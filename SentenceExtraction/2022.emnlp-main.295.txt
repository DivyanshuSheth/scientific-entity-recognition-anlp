
Ke Bai, Guoyin Wang, Jiwei Li, Sunghyun Park,
Sungjin Lee, Puyang Xu, Ricardo Henao, Lawrence CarinDuke UniversityAmazonZhejiang UniversityKAUST
Abstract
Open world classification is a task in natural
language processing with key practical rele-
vance and impact. Since the open or unknown
category data only manifests in the inference
phase, finding a model with a suitable deci-
sion boundary accommodating for the identi-
fication of known classes and discrimination
of the open category is challenging. The per-
formance of existing models is limited by the
lack of effective open category data during the
training stage or the lack of a good mecha-
nism to learn appropriate decision boundaries.
We propose an approach based on adaptive
negative samples (ANS) designed to generate
effective synthetic open category samples in the
training stage and without requiring any prior
knowledge or external datasets. Empirically,
we find a significant advantage in using auxil-
iary one-versus-rest binary classifiers, which
effectively utilize the generated negative sam-
ples and avoid the complex threshold-seeking
stage in previous works. Extensive experiments
on three benchmark datasets show that ANS
achieves significant improvements over state-
of-the-art methods.
1 Introduction
Standard supervised classification assumes that all
categories expected in the testing phase have been
fully observed while training, i.e., every sample is
assigned to one known category as illustrated in
Figure 1(a). This may not always be satisfied in
practical applications, such as dialogue intention
classification, where new intents are expected to
emerge. Consequently, it is desirable to have a
classifier capable of discriminating whether a given
sample belongs to a known or an unknown category,
e.g., the red samples in Figure 1(a). This problem
can be understood as a (C+ 1) classification prob-
lem, where Cis the number of known categories
and the additional category is reserved for open
(unknown) samples. This scenario is also knownFigure 1: Illustration of previous methods and our pro-
posed one. C0,C1, and C2are three categories. The
boundary is used to discriminate known and open (un-
known) categories. (a) Boundary learned by supervised
learning. (b) Optimal decision boundary. (c) ADB
method which has a closed decision boundary, but may
capture irrelevant points. (d) Proposed ANS method.
as multi-class open set recognition (Scheirer et al.,
2014).
To discriminate the known from the open sam-
ples during inference, it is necessary to create
a clear classification boundary that separates the
known from the open category. However, the
lack of open category samples during training
makes this problem challenging. Current research
in this setting mainly focuses on two directions.
The first direction mainly estimates a tighter de-
cision boundary between known classes to allow
for the possibility of the open category. Existing
works in this direction include the Local Outlier
Factor (LOF) (Breunig et al., 2000; Zhou et al.,
2022; Zeng et al., 2021), Deep Open Classification
(DOC) (Shu et al., 2017) and Adaptive Decision
Boundary (ADB) (Zhang et al., 2021b). LOF and
ADB calibrate the decision boundary in the feature
space while DOC does it in the probability space.
The second direction deals with learning a bet-
ter feature representation to make the boundary-
seeking problem easier. In this direction, Deep-4378Unk (Lin and Xu, 2019a) and SEG (Yan et al.,
2020) added constraints to the feature space,
SCL (Zeng et al., 2021) and Zhou et al. (2022)
fine-tuned the feature extractor backbone with con-
trastive learning. Zhan et al. (2021) considered in-
troducing open samples from other datasets as neg-
ative, and Shu et al. (2021) generated samples with
contradictory meanings using a large pretrained
model. The latter two deliver large performance
gains.
These improvements demonstrate the signif-
icance of negative samples in determining the
boundary between the known and open categories.
To accomplish the same in the absence of addi-
tional datasets or knowledge, we propose a novel
negative-sample constraint and employ a gradient-
based method to generate pseudo open category
samples. As shown in Figure 1(d), negative sam-
ples are adaptively generated for each category to
closely bound each category.
Given the generated negative samples, we then
empirically find that using auxiliary one- versus -rest
binary classifiers can better capture the boundary
between the known and the open category, relative
to a(C+ 1) -way classifier (Zhan et al., 2021),
where all the open categories, possibly distributed
in multiple modes or arbitrarily scattered over the
feature space, are categorized into one class.
Specifically, we first learn a C-category classi-
fier on known category data. Then for each known
category, we learn an auxiliary binary classifier,
treating this category as positive and others as neg-
ative. During inference, one sample is recognized
as open if all the binary classifiers predict it as neg-
ative, thus not belonging to any known category
Our main contributions are summarized below:
•We propose a novel adaptive negative-sample-
generation method for open-world classifica-
tion problems without the need for external
data or prior knowledge of the open categories.
Moreover, negative samples can be added to
existing methods and yield performance gains.
•We show that synthesized negative samples
combined with auxiliary one-versus-rest bi-
nary classifiers facilitate learning better deci-
sion boundaries and requires no tuning (cali-
bration) on the open category threshold.
•We conduct extensive experiments to show
that our approach significantly improves over
previous state-of-the-art methods.2 Related Work
Boundary Calibration The classical local outlier
factor (LOF) (Breunig et al., 2000) method is a
custom metric that calculates the local density devi-
ation of a data point from its neighbors. However,
there is not a principled rule on how to choose the
outlier detection threshold when using LOF. Zeng
et al. (2021); Zhou et al. (2022) added open cat-
egory data into the validation set to estimate or
grid-search the proper threshold. So motivated,
Bendale and Boult (2016) fit the output logits of
the classifier to a Weibull distribution, but still use
a validation set that contains the open category to
select the confidence threshold. Further, Shu et al.
(2017) employed one-versus-rest binary classifiers
and then calculates the threshold over the confi-
dence score space by fitting it to a Gaussian distri-
bution. This method is limited by the often inac-
curate (uncalibrated) predictive confidence learned
by the neural network (Guo et al., 2017). Adap-
tive decision boundary (Zhang et al., 2021b), il-
lustrated in Figure 1(c), was recently proposed to
learn bounded spherical regions for known cate-
gories to contain known class samples. Though
this post-processing approach achieves state-of-the-
art performance, it still suffers from the issue that
the tight-bounded spheres may not exist or cannot
be well-defined in representation space. Due to the
fact that high-dimensional data representations usu-
ally lie on a low-dimensional manifold (Pless and
Souvenir, 2009), a sphere defined in a Euclidean
space can be restrictive as a decision boundary.
Moreover, the issue can be more severe if certain
categories follow multimodal or skewed distribu-
tions.
Representation Learning DeepUnk (Lin and Xu,
2019a) trains the feature extractor with Large Mar-
gin Cosine Loss (Wang et al., 2018). SEG (Yan
et al., 2020) assumes that the known features fol-
low the mixture Gaussian distribution. Zeng et al.
(2021) and Zhou et al. (2022) applied supervised
contrastive learning (Chen et al., 2020) and fur-
ther improve the representation quality by using
k-nearest positive samples and negative samples
collected from the memory buffer of MOCO (He
et al., 2020). Getting a better representation trained
with known category data only is complementary
to our work, since a better pretrained backbone can
further improve our results. Recent works found
that it may be problematic to learn features solely
based on the known classes; thus, it is crucial to pro-4379
vide samples from unknown classes during training.
Specifically, Zhan et al. (2021) creates negative
samples with mixup of training data and examples
from an external dataset. Shu et al. (2021) gener-
ates open class samples using BART (Lewis et al.,
2019) and external text entailment information.
3 Methodology
Problem Definition Suppose we are given a
training dataset D={(x, y),(x, y), . . . ,
(x, y)}consisting of Nexamples, where x
is an input text sequence and yits correspond-
ing label, which belongs to a predefined set
L={l, l, . . . , l}withCcategories, thus y∈
L,∀i∈[N], where [N]:= [1, . . . , N ]. In this
paper, we use [·]to represent index sequence. In
an open world classification setting, the goal is to
learn a model which categorizes a test instance to
either one of the predefined Ccategories or as an
open category. In practice, the open category is
denoted as a unique category l.
3.1 Known Category Classification
Following the setting suggested in Lin and Xu
(2019b); Zhang et al. (2021b), we use BERT (De-
vlin et al., 2018) as our feature extractor f(·).
For each input text sequence x, where xis repre-
sented as a sequence of tokens [t, t, . . . , t], we
take the average of features zof each token tex-
tracted from the BERT output layer as the sentence
representation z. - The training of f(·)is for-mulated as a multi-category classification problem
by minimizing the cross-entropy loss L:
L(ψ, ψ) = (1)
−/summationdisplaylogexp(f(z))
/summationtextexp(f(z)),
where f(·)is a classifier that takes zas input
and the output dimension is the number of known
categories C.f(z)represents the output logit
for the j-th category. A well-trained feature extrac-
torf(·)and a high-quality classifier f(·)
can extract representative features of each category
and ensure good performance on the classification
of known categories during the inference stage.
3.2 Open Category Recognition
Once the classifier for the known categories is
available, the task is to recognize samples from
the open category versus the ones from known
categories. As mentioned above, directly using
the known category classifier f(·)can result in
poor performance (Hendrycks and Gimpel, 2016),
while using a (C+ 1) category classifier setting
is complicated due to the need to find proper sam-
ples to obtain a suitable decision boundary for the
open category (Scheirer et al., 2012; Liang et al.,
2018; Shu et al., 2021). In this work, building
upon ideas from one-class classification (Schölkopf
et al., 2001; Ruff et al., 2018) and one- vs-all sup-
port vector machines (Rifkin and Klautau, 2004),4380we propose a one- versus -rest framework via train-
ing simple binary classifiers for each predefined
category. Based on this framework, we then intro-
duce an effective open-sample generation approach
to train these classifiers in Section 3.3.
We build an auxiliary one- versus -rest binary clas-
sifier for each known category, and take m-th cat-
egory as an example to illustrate. Given a text
sequence x, we use the BERT pretrained with clas-
sification loss as the feature extractor f(·)to
extract features z∈Rto be fed to the binary
classifiers, where dis the dimension of the feature
space. Each category is provided with a binary
classifier denoted as g(z) :R→R, such that
ifg(z)>0then the input text xbelongs to the
m-th category or vice versa belongs to any of the
other categories. We parameterize the entire binary
classification framework for the m-th category as
θ= (ψ, θ).
To learn each binary classifier g(·)from
training data D, we first construct a positive set
{x,x, . . . ,x}using data points with label l
fromDand a negative set{ˆx,ˆx, . . . , ˆx}
by data points not in category lbut also from D.
The total number of samples within category mis
N, andN−Nis the number of remaining sam-
ples in D. Each binary classifier is optimized by
minimizing the binary cross-entropy loss function
L:
L(θ) =/summationdisplaylog(1 + exp( −g(x)))
+/summationdisplaylog(1 + exp( g(ˆx))). (2)
During the inference phase, we have
ˆy=/braceleftigg
open, if g(x)<0,∀m∈[C];
known ,otherwise .
We assign a testing example to the open category
lif it is detected as unknown in all Cbinary clas-
sifiers. Otherwise, we pass the example to the
known classifier to categorize the known categories.
The entire inference pipeline is illustrated in Fig-
ure 2(b).
3.3 Adaptive Negative Sampling (ANS)
In practice, it is problematic to learn a good bi-
nary classifier with only the aforementioned neg-
ative samples from D. The sample space of the
open category is complicated, considering that new-
category samples could originate from differenttopics and sources, relative to the known classes.
So motivated, some existing methods introduce
additional external data as negative samples.
To alleviate the issue associated with the lack
of real negative samples, we propose to synthesize
negative samples ˜x. Considering that it is hard
to create actual open-category text examples, we
choose to draw virtual text examples ˜zin the fea-
ture space (Miyato et al., 2016; Zhu et al., 2019).
Compared with the token space of text, the fea-
ture space is typically smoother (Bengio et al.,
2013), which makes it convenient to calculate gra-
dients (Wang et al., 2021).
For all known samples in a category l, points
that are away from these samples can be recognized
asnegative to classifier g(·). The entire feature
spaceRcontains essentially an uncountable set of
such points, among which we are mostly interested
in those near the known samples. Consequently,
capturing these points will be helpful to character-
ize the decision boundary.
To give a mathematical description of these
points, we assume that data usually lies on a
low-dimensional manifold in a high-dimensional
space (Pless and Souvenir, 2009). The low-
dimensional manifold can be viewed as a local-
Euclidean space, thus we can use the Euclidean
metric to measure distances locally for each known
dataz. Under this assumption, the set of pseudo
negative samples N(r)forz, which we call adap-
tive synthesized open set , can be described as fol-
lows,
N(r) =:/braceleftig
˜z:r≤ ∥˜z−z∥;∥˜z−z∥≤γ·r,
∀j:y=m/bracerightig
, (3)
where ris the distance radius and γ >1is a hy-
perparameter. Note that each known sample zhas
an associated adaptive synthesized open set. As
defined above, this set is subject to two inequalities.
The first keeps synthesized samples away from all
known samples within category m. The second
implies that the synthesized samples should not be
too far from the known samples. An intuitive geo-
metric interpretation is that when j=i, the space
implied by these two constraints is a spherical shell
with inner radius rand outer radius γ·r.
To get the radius r, we first calculate the co-
variance matrix Σofzusing known samples from
category mand choose r, s.t.r≤/radicalbig
2 Tr(Σ) and
γr≥/radicalbig
2 Tr(Σ) . This is under the consideration4381that/radicalbig
2 Tr(Σ) is the average Euclidean distance
between random samples drawn from a distribution
with covariance matrix Σ.
The estimation is supported by the following
proposition,
Proposition 1 The expectation of the euclidean
distance between random points sampled from dis-
tribution with covariance matrix Σis smaller than/radicalbig
2 Tr(Σ) , i.e.
E/radicalbig
∥x−y∥≤√
2 Tr Σ (4)
The proof can be found in supplementary. In
our experiments, we fix γ= 2 andr= 8. The
choice of ris relevant to the covariance matrix
of the features in representation space. The de-
tailed justification for our selection is provided in
Appendix A.3. Ablation studies (Figure 3) show
that model performance is not very sensitive to the
chosen r.
Binary Classification with ANS According to
Equation 3, each sample from a known category
mcontributes an adaptive synthesized set of open
samples N(r). The classifier g(·)is expected to
discriminate them as negative. The corresponding
objective function is the binary cross-entropy loss,
L(θ) =/summationdisplaylog(1 + exp( g(˜z))),
where ˜zis sampled from N(r)andNis the total
number of known samples with category l. How-
ever, there exist uncountably many points in N(r).
Randomly sampling one example from N(r)is
not effective. Alternatively, we choose the most
representative ones that are hard for the classifier
to classify it as negative . Consistent with this in-
tuition, the max(·)operator is added to select the
most challenging synthetic open sample distributed
inN(r).
L(θ) = (5)
/summationdisplaymaxlog(1 + exp( g(˜z))).
Finally, the complete loss accounting for open
recognition is summarized as:
L=L+λL, (6)
where λis a regularization hyperparameter.Algorithm 1
Directly minimizing the objective function in
Equation 6 subject to the constraint in Equation 3 is
challenging. In the experiments, we adopt the pro-
jected gradient descend-ascend technique (Goyal
et al., 2020) to solve this problem.
Projected Gradient Descend-Ascent We use
gradient descent to minimize the open recognition
lossLand gradient ascent to find the hardest
synthetic negative samples ˜z. The detailed steps
are summarized in Algorithm 1. As illustrated in
Figure 2(c), the sample ˜z=˜z+ϵdirectly derived
from gradient ascent (line 11 of Algorithm 1) might
be out of the constraint area N(r). We then project
to the closest ˜zwithin the constraint such that
˜z= arg min∥˜z−u∥,∀u∈ N(r)(Boyd
et al., 2004). Unfortunately, direct search within
N(r)defined in Equation 3 requires complex com-
putation over entire training data D. Based on
our assumption that the training samples lie on a
low-dimensional manifold and the empirical obser-
vation that ˜zis always closest to the corresponding
positive point zrelative to other positive points,
N(r)can be further relaxed to the sphere shell
around sample z:N(r) ={˜z:r≤ ∥˜z−z∥≤
γ·r}. We can then directly find the synthetic
negative sample via a projection along the radius
direction, i.e.,˜z=˜z+α, where αis
adjusted to guarantee ˜z∈ N(r):
α=

1, ifr≤ ∥˜z−z∥≤γ·r,ifγ·r≤ ∥˜z−z∥,if∥˜z−z∥≤r4382
In the future, we would like to consider relaxing
these constraints by only considering the nearest k
points instead of all the points within a category.
4 Experiments
We conduct experiments on three datasets: Bank-
ing, CLINC and Stackoverflow. Details and exam-
ples of the datasets are found in Appendix A.4.
Task Design We apply the same settings as Shu
et al. (2017) and Lin and Xu (2019a). For each
dataset, we sample 25%,50%,75% categories ran-
domly and treat them as the known category . Any
other categories out of the known categories are
grouped into the open category . In the training
and validation set, only samples within the known
category are kept. All the samples in the testing set
are retained, and the label of samples belonging to
open categories is set to l. Importantly, samples
from the open category are never exposed to the
model in the training and validation process.
Evaluation Metrics The model needs to iden-
tify the samples with the open category, as well as
classify the known samples correctly. Following
Shu et al. (2017); Zhang et al. (2021b), we useaccuracy and macro F1 as our evaluation metrics.
The accuracy measures the overall performance,
considering that open-world classification can be
treated as a C+ 1classification problem. F1 is a
binary classification metric mainly used for evaluat-
ing the performance of open category recognition.
The F1-score reported in this paper is the mean
of the macro F1-score per category (including the
open category), where the positive category is the
corresponding one and negatives are all the other
categories. F1-known is the average over F1-score
of all known categories. F1-open is the F1-score of
the open category.
Experimental setting We use the BERT-base-
uncased model to initialize the feature extractor
fand freeze the first ten layers of BERT during
training. Note that all results are the mean of ten
trials with different random seeds. Other experi-
mental details are included in Appendix A.5.
4.1 Results
Table 1 compares our approach with previous state-
of-the-art methods using accuracy and F1. Our
implementation is based on Zhang et al. (2021a).4383
The baselines include threshold finding methods,
MSP (Hendrycks and Gimpel, 2016), DOC (Shu
et al., 2017), OpenMax (Bendale and Boult, 2015),
ADB (Zhang et al., 2021b); and feature learn-
ing methods, DeepUnk (Lin and Xu, 2019a); and
negative data generation method SelfSup (Zhan
et al., 2021). We did not include results from
ODIST (Shu et al., 2021) because their method
relies on MNLI-pretrained BART, which is not cur-
rently public and their model performance drops
dramatically if not coupled with ADB. Note that
SelfSup uses additional datasets, without which
the accuracy on 50% CLINC drops from 88.33 to
83.12.
Our approach performs better than most pre-
vious methods, even better than the method us-
ing additional datasets, with the greatest improve-
ment on CLINC. This is in accordance with Self-
Sup (Zhan et al., 2021), which also benefits the
most on CLINC by adding negative samples. This
implies that our synthesized negative samples are
of high quality and could possibly be used as extra
datasets in other methods.
The average performance gain in these three
datasets decreases as the known category ratio in-
creases, i.e., compared to the strongest baseline
ADB, our accuracy improvements in the three
datasets are 5.08,3.11,1.42under the setting of
25%,50%,75%. With more known categories
available, the more diverse the known negative sam-
ples will be, allowing the model to better capture
the boundaries of the positive known categories
while reducing the impact of synthetic samples.
The comparison with baselines on F1-open and
F1-known can be found in Appendix A.3.
4.2 Discussion
Synthesized Negative is Beneficial for a Vari-
ety of Structures To investigate the contribution
of the synthesized samples and the structure of
one-versus -rest, we performed experiments adding
the synthesized samples to two well-known base-
lines, MSP (Hendrycks and Gimpel, 2016) and
ADB (Zhang et al., 2021b) as shown in Table 3.
Specifically, the C-way classifier in MSP and ADB
is replaced by a ( C+ 1)-way classifier, with an
extra head for the synthesized negative samples.
See Appendix A.7 for details.
We observe that performance increases on all
baselines with synthesized negative samples. The
synthesized samples behave like data augmentation,
leading to better representation of positive samples.
Further, synthesized samples benefit one- versus -
rest the most. The difference, we believe, stems
from the model’s flexibility on boundary learning.
The open category may contain sentences with var-
ious themes, making it difficult for a single head
of a (C+ 1)-way classifier to catch them all. This
one-versus -rest flexibility comes at the cost of more
classifier parameters. However, compared to the
huge feature extractor BERT, the number of addi-
tional parameters is relatively small. Distillation
techniques can be used to build a smaller model if4384
necessary, for instance, where there are thousands
of known categories.
Adaptive Negatives Samples Generation Our
adaptive negative sample generation consists of
three modules (a) adding Gaussian noise to the
original samples (line 8in Algorithm 1). (b) gra-
dient ascent (line 10∼11) (c) projection (line
13∼14). We add each module to the baseline in
turn to study their importance. The baseline exper-
iment uses the vanilla one- versus -rest framework
described in Section 3.2, without the use of synthe-
sized negative samples. Experiments are conducted
on CLINC as shown in Table 2.
The following describes our findings from each
experiment: ( i) Adding samples with noise as neg-
ative alleviates the overconfidence problem of the
classifier and improves the results significantly.
The noise level needs to be designed carefully since
small noise blurs the distinction between known
and open, while large noise is ineffective.
(ii) Constraining synthesized samples to N(r)
improves performance by keeping synthesized sam-
ples from being too close or too far away from
positive known samples.
(iii) Adding a gradient ascent step further en-
hances performance. The improvement over the
previous step is marginal. Our hypothesis is that
the calculated gradient could be noisy, since the
noise we add is isotropic and may be inconsistent
with (outside of) the manifold of the original data.
Radius rAnalysis In the adaptive negative sam-
ple generation process, the radius rand multiplier
γare two hyperparameters that determine the up-
per and lower bounds of the distance between the
synthesized sample and the known sample. To in-
vestigate the impact of radius, we fix γto 2 andincrease the rfrom 1to256. Note that 8is our
default setting.
As illustrated in Figure 3, the performance grad-
ually drops when the radius rincreases, because
the influence of the synthesized negative examples
reduces as the distance between them and the posi-
tive samples grows. When the radius rdecreases,
the classifier may be more likely to incorrectly cate-
gorize positive as negative because the synthesized
negative samples are closer to the known positives,
resulting in a decrease in accuracy and F1 score
on the banking and CLINC datasets. However, the
performance on Stackoverflow improves. We hy-
pothesize that there is a better data-adaptive way to
estimate the radius rto improve the performance
even further, for example, using knearest neighbor
instead of all the data in a category. We leave this
as interesting future direction.
In summary, we observe that the performance is
affected by the radius, but comparable results can
be obtained for a wide value range. They are all bet-
ter than the vanilla one- versus -rest baseline, which
lacks the generated negative samples. The accuracy
of baselines on Banking, CLINC and Stackover-
flow is 58.09,71.80and64.58, respectively.
Visualization Figure 4 shows the t-SNE repre-
sentation of the features extracted from the sec-
ond hidden layer of one- versus -rest classifier g.
Randomly chosen three known categories, each
corresponds to a one- versus -rest classifier, yield
three figures. The known positive/negative sam-
ples (blue) are clustered because the features are
extracted from a pretrained C-way classifier Sec-
tion 3.1. Open samples (pink) are scattered, some
of which overlap with the known positives (see the
middle figure). Our synthesized negatives work
as expected; they are close to the known positives4385and bridge the gap between the positive and other
known categories.
5 Conclusions
We have introduced ANS, a pseudo open category
sample generation approach for open-world classi-
fication problems. The generation process is free
from extra datasets or prior knowledge. The synthe-
sized samples are effective for improving existing
methods. Combined with one- versus -rest frame-
work, significant improvements are observed on
three benchmarks. The gradient-based negative
sample generation proposed can be applied to other
NLP tasks such as out-of-scope discovery, which
we leave as future work.
6 Limitations
First, both in terms of the number of training sam-
ples and the length of sentences, all the datasets that
we employ in this study are relatively small and
short. It may not scale well for long texts. Second,
further work should be done on the model’s ability
to handle increasingly complicated data. The train-
ing samples from three benchmark datasets might
be too sample; many input sentences even include
the category descriptions, as shown in Tabel A.1.
7 Acknowledgement
This research was supported by DARPA, DOE,
NIH, ONR and NSF.
References43864387A Appendix
A.1 Ethical Consideration
The topics of the three datasets we use in this paper
are relatively simple, covering only the information
needed for classification (check Table A.1). The
category labels are either everyday intentions or
technical terms in computer science. There are no
potentially sensitive topics or contents that we are
aware of. All three datasets have been published
and are included in our appendix.
A.2 Related work: Adversary Augmentation
The gradient descend-ascend technique has been
used successfully in adversarial attacks (Madry
et al., 2018; Zeng et al., 2021); however, it differs
from ours in terms of motivation and loss formula-
tion.
Madry et al. (2018) sought for samples that were
similar to the training sample but had a substantial
loss given the paired label. The addition of gen-
erated samples during training may strengthen the
model’s resistance to adversarial attacks by avoid-
ing inputs that are indistinguishable from genuine
data but improperly categorised. The associated
optimization formula is
minE[maxl(θ,x+δ, y)], (7)
where y,xare the training data, and lcould be
any classification model parameterized by θ.Sis
the an adversarial perturbation lball.
Our work targets on shrinking the decision
boundary. We need to treat the samples with pos-
itive labels in a specific region N(r)(defined in
Equation 3) as negative, i.e.
minE[ maxl(θ,z+δ,−1)], (8)
where zis the positive sample from dataset D,l
is a binary classifier with parameter δ. This equa-
tion behaves the same as Equation 5.
A.3 Explanation on Radius r
Proposition 2 The expectation of the euclidean
distance between random points sampled from dis-
tribution with covariance matrix Σis smaller than/radicalbig
2 Tr(Σ) , i.e.
E/radicalbig
∥x−y∥≤√
2 Tr Σ (9)Proof: Given that we are measuring the distance
between samples drawn from the same distribution,
we could subtract a constant value from both vari-
ables and assume that the distribution’s expectation
is zero. If xandyare random variables indepen-
dently sampled from distribution with covariance
matrix Σand zero mean, we could have:
E(∥x−y∥) =/summationdisplayE(x−2xy+y)
= 2/summationdisplay(E(x)−E(x)E(y))
= 2/summationdisplay(E(x)
= 2 Tr(Σ)
For a random variable Z, Jensen’s inequality
gives us
E(√
Z)≤/radicalbig
E[Z] (10)
The combination of the two equations above
proves the proposition,
E(∥x−y∥)≤/radicalbig
2 Tr(Σ)
In experiment, we choose the mean of the last
layer of BERT as the latent representation z∈
R. When calculating the trace, only the variance
of each dimension, are required. On three datasets,
the predicted distance per category falls primarily
into[8,12], we fix r= 8for all the experiments.
For each positive point, we could sample several
adaptive negative samples. The distance between
the synthesized negative and the chosen positive is
determined by r. Meanwhile, we can also calculate
the distance between the synthesized negative and
other positive known samples.
We find that even when the radius is set to be
less than the average distance, the synthesized neg-
ative samples have a much greater distance to other
known points. In theory, known samples are on
a low-dimensional manifold, whereas synthesized
points are in a high-dimensional space, and the
probability of sampled points falling into the man-
ifold is zero. We calculated the distance between
the synthesized sample and other known samples
in the same category empirically, and discovered
that their distances are nearly twice the average
distance.
A further ablation study on different options of
the radius can be found in the main context, where
we have comparisons over different radius, i.e.,
r∈ {1,4,8,16,64}.4388
A.4 Dataset
All three datasets are in English. The label dis-
tributions of each dataset are balanced. Banking
(Casanueva et al., 2020) is a data set for intent de-
tection in the banking domain with 77 fine-grained
categories. It contains 13,083 samples in total.
We follow the original splitting and sampling, i.e.,
9,003, 1,000, and 3,080 for training, validation and
testing respectively.
CLINC (Larson et al., 2019) is an intent classifi-
cation dataset originally designed for out-of-scope
detection. It owns 22,500 in-scope samples and
1,200 out-of-scope samples that existed only in the
testing phase. The divisions on the in-scope ones
follow the original training, validation and testing
splitting, i.e., 15,000, 3,000 and 5,700.
StackOverflow (Xu et al., 2015) consists of 20categories of technical question titles. We utilize
the prepossessed version with 12,000 for training,
2,000 for validation and 6,000 for testing.
The statistics of the datasets are summarized in
Table A.2. We also provide raw data samples of
each dataset in Table A.1 for an intuitive under-
standing of the open world recognition task.
A.5 Experimental Details
All experiments are executed on a single NVIDIA
Titan Xp GPU with 12,196M memory. All the
experiments are done on NVIDIA X
Known Category Classification The classifier
fis a fully connected neural network composed
of one hidden layer with ReLU activation function.
The hidden size is 768.
The learning rate of the transformer part and
non-transformer part are 5e−5and1e−4respec-
tively. The total number of training epochs is 100.
Learning rate decay and early stops are applied.
Training on this part takes about 10-20 minutes,
depending on when the early stopping is triggered.
one- versus -rest Binary Classification During
the training of one- versus -rest structure, we fix the4389parameters of the feature extractor ψ.
For the one- versus -rest module, the feature z
is chosen as the mean of the output of the BERT
model’s last layer. The classifier is a fully con-
nected three-layer neural network with ReLU as
the activation function. The numbers of hidden neu-
rons are (256,64), respectively. Dropout is added
per hidden layer. The learning rate of the classifier
is1e−3for Banking and CLINC, 3e−4for Stack-
overflow. The total number of epochs for each
classifier is min(C,20)to avoid overfitting. γis
0.5.
Currently, we train each one- versus -rest classi-
fier individually, and this takes roughly a minute
per head. As a result, the total time grows linearly
with the number of known categories. Parallel train-
ing of multiple heads can increase efficiency if nec-
essary.
Model Size The parameters of the BERT back-
bone model and the ovr classifier are approximately
109 million and 0.2 million, respectively, implying
that the maximum number of parameters from one-
versus -rest is only about one-fifth of BERT (75%
CLINC).
Reproducibility Checklist: hyper-parameter
search We didn’t include results from the val-
idation set considering there is a huge gap between
the current validation set and test set; test set con-
tains open category samples while the validation
set does not.
It is difficult to study the hyper-parameter set-
ting because we lack an effective validation set
with open category samples and the testing set is
unavailable during training. To solve this, we con-
struct a "pseudo dataset" by selecting a subset as
"sub-known" from all known categories and treat-
ing others as "sub-open". Taking 50% CLINC as an
example, we take a quarter of the known category
as "sub-known" and the others as "sub-open". We
discover that the rules we developed using these
synthesized datasets can be transferred to formal ex-
periments. We choose the proper hyper-parameter
according to F1.
The hyperparameters we manually tried include
training epoch ( 10,20), learning rate ( 1e−3,1e−4
for the classifier head, 1e−4,5e−5,1e−5for
BERT, note that the learning rate of the classifier
head is always larger than BERT). The ablation
study and the analysis on radius rcan be found in
Figure 3 and Appendix A.3. The hyper-parametersin gradient ascend are not sensitive to the final
experiments.
A.6 More results
F1-known and F1-open The definition of F1-
known and F1-open can be found in Section 4.
Table A.3 shows the comparisons between the base-
lines and ours.
Reproducibility Checklist: Differences on
Datasets During this process, we found that
dataset CLINC is the robustest to change of hyper-
parameters while dataset Stackoverflow is the
weakest. A similar observation also shows in Zeng
et al. (2021). It works the best on CLINC and worst
on Stackoverflow.
We hypothesize that the difference comes from
the quality of the raw data. As shown in Table Ta-
ble A.1, the category of the input in Stackoverflow
is usually included in the original sentence and we
name them “easy”. Rare sentences do not follow
to this rule and we call them “hard”. This leads to
an observation in empirical experiments that the
number of training epochs should be controlled in a
limited range; otherwise many open category sam-
ples would be wrongly categorized to the known
category.
This is in consistent with the finding in noisy
labeling classification. The neural network will
first fit the clean label before overfitting the noisy
labeled samples. Under the current setting, the
“hard” corresponds to the noisy sample. The one-
versus -rest will first fit the easier one, followed
by the harder one. When the harder one is classi-
fied correctly, many open categories could also be
classified into this known category.
ADB (Zhang et al., 2021b) avoids this problem
by working directly on the pre-trained features. It
can statistically filter out the influence of noisy sam-
ples. Though ADB does not require extra hyper-
parameter tuning, we found that the position of
features extracted from the model has an impact on
the final performances.
In summary, the differences between datasets are
an intriguing topic that merits further investigation
in the future.
Reproducibility Checklist: Standard Deviation
As shown in Table A.4, larger known category ra-
tios are more likely to be associated with lower
variance; this is to be expected because more sam-
ples make the training more stable.4390
A.7 Ablation Study: Synthesized Negative
Samples on other Structures
Negative sample generation for MSP and ADB
follow the same process as ours, except that the
gradient ascend is removed. The complete version
is left for future work.
MSP with negative sampling The original MSP
is a C-way classifier ftrained with cross-entropy
loss during the training. In inference, the confi-
dence score p(x) = Softmax (f(x))is first
calculated. If the category with maximum proba-bility pis lower than 0.5, the corresponding input
is recognized as “open” category. Otherwise, this
sample belongs to the category m,i.e.,
ˆy=/braceleftigg
open, if max( p(x))<0.5
arg max p(x),otherwise .
In MSP with negative settings, an extra category
lis added for synthesized negatives. The inference4391now becomes
ˆy=

open, if max( p(x))<0.5
open, if arg max( p(x)) == l
arg max p(x),otherwise
Note that our MSP with synthesized negatives dif-
fers from (Zhan et al., 2021) in two aspects, ( i),
different ways to choose the negative samples. ii),
their work added synthesized negative samples to
the validation set, while ours uses the origin valida-
tion set.
ADB with negative sampling ADB training has
two steps. The first step is to learn a good feature
extractor using C-way classifier. The second step
is to learn the boundary of each category in the
pre-trained feature space.
Our modification is the first step. We replace the
origin classifier with a C+ 1-way classifier. The
extra head is designed for the synthesized negative
samples. The inference step is kept the same as the
original method.4392