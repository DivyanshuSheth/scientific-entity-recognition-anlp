
Letitia ParcalabescuMichele CafagnaLilitta Muradjan
Anette FrankIacer CalixtoAlbert GattHeidelberg University, Department of Computational LinguisticsUniversity of Malta, Institute of Linguistics and Language TechnologyNew York UniversityILLC, University of AmsterdamUtrecht University, Department of Information and Computing Sciences
Abstract
We propose V ALSE ( Vision AndLanguage
Structured Evaluation), a novel benchmark de-
signed for testing general-purpose pretrained
vision and language (V&L) models for their
visio-linguistic grounding capabilities on spe-
ciﬁc linguistic phenomena . V ALSE offers
a suite of six tests covering various linguis-
tic constructs. Solving these requires models
to ground linguistic phenomena in the visual
modality, allowing more ﬁne-grained evalua-
tions than hitherto possible. We build V ALSE
using methods that support the construction of
valid foils, and report results from evaluating
ﬁve widely-used V&L models. Our experi-
ments suggest that current models have consid-
erable difﬁculty addressing most phenomena.
Hence, we expect V ALSE to serve as an impor-
tant benchmark to measure future progress of
pretrained V&L models from a linguistic per-
spective , complementing the canonical task-
centred V&L evaluations.
1 Introduction
General-purpose pretrained vision and language
(V&L) models have gained notable performance on
many V&L tasks (Lu et al., 2019; Tan and Bansal,
2019; Li et al., 2019; Chen et al., 2020; Li et al.,
2020a; Su et al., 2020). As a result, V&L research
has changed its focus from task-speciﬁc architec-
tures to ﬁne-tuning large V&L models.
Current benchmarks give a good perspective
on model performance on a wide range of V&L
tasks (Cao et al., 2020; Lourie et al., 2021; Li
et al., 2021), but the ﬁeld is only starting to assess
why models perform so well and whether models
learn speciﬁc capabilities that span multiple V&L
tasks . Speciﬁcally, we lack detailed understand-
ing of the extent to which such models are able to
ground linguistic phenomena—from morphosyn-
tax to semantics—in the visual modality (Bernardiand Pezzelle, 2021). For example, recent evidence
suggests that models are insensitive to linguistic
distinctions of verb-argument structure (Hendricks
and Nematzadeh, 2021) and word order (Cirik et al.,
2018; Akula et al., 2020).
Our work addresses this gap with V ALSE (Vi-
sion And Language Structured Evaluation), a
benchmark for V&L model evaluation compris-
ing six tasks, or ‘pieces’, where each piece has the
same structure: given a visual input, a model is
asked to distinguish real captions from foils, where
a foil is constructed from a caption by altering a
word or phrase that realizes a speciﬁc linguistic
phenomenon , e.g., semantic number of nouns, verb
argument structure, or coreference. V ALSE uses a
resource-lean diagnostic setup that dispenses with
large-scale annotation (e.g., of bounding boxes),
and builds on existing high-quality image caption-
ing and VQA data. V ALSE is designed to lever-
age the existing prediction heads in pretrained (or
ﬁnetuned) V&L models; for that reason, our bench-
mark does not include any re-training and can be
interpreted as a zero-shot evaluation. We build test
data for each piece so as to safeguard against the
possibility of models exploiting artefacts or statis-
tical biases in the data, a well-known issue with
highly parameterised neural models pretrained on
large amounts of data (Goyal et al., 2017; Mad-
hyastha et al., 2018; Kaﬂe et al., 2019). With this
in view, we propose novel methods to guard against
the emergence of artefacts during foiling.
Our main contributions are:
i)We introduce V ALSE, a novel benchmark
aimed at gauging the sensitivity of pre-trained
V&L models to foiled instances.
ii)We cover a wide spectrum of basic linguistic
phenomena affecting the linguistic andvisual
modalities: existence, plurality, counting, spa-
tial relations, actions, and entity coreference.
iii)We investigate novel strategies to build valid
foils that include automatic andhuman valida-8253tion. We balance word frequency distributions
between captions and foils, and test against
pretrained models solving the benchmark uni-
modally by relying only on text. We employ
masked language modeling (MLM) in foil cre-
ation and semantic inference for validating
foils, and ﬁnally collect human annotations
for the entire benchmark.
iv)We establish initial experimental results for
pretrained V&L models of diverse architec-
tures on V ALSE. The overall weak perfor-
mance of these models indicates that the time
is ripe for a novel, reliable foiling dataset tar-
geting the visual grounding capabilities of
V&L models through the lens of linguistic
constructs.
2 Background and Related work
Pretrained V&L models learn to combine vision
and language through self-supervised multitask
learning. Tasks include multimodal masked model-
ing—where words in the text and object labels or re-
gions in the image are masked out, then predicted—
andimage-sentence alignment , whereby a model
learns to predict whether an image and a text corre-
spond. Major architectures are single- and dual-
stream multimodal transformers: single-stream
models concatenate word and image features, and
encode the resulting sequence with a single trans-
former stack; dual-stream models use distinct trans-
former stacks to handle visual and textual inputs,
and additional layers (e.g. co-attention) to fuse
these into multimodal features.
Benchmarking V&L models V&L models (Li
et al., 2019; Lu et al., 2019; Tan and Bansal, 2019;
Lu et al., 2020; Li et al., 2020b; Kim et al., 2021)
are commonly evaluated on V&L tasks such as
VQA (Goyal et al., 2017), visual reasoning (Suhr
et al., 2019), or image retrieval (Lin et al., 2014;
Plummer et al., 2015).
Given how well transformer-based models per-
form across unimodal and multimodal tasks, re-
search efforts have recently started to address what
makes them so effective, and to what extent they
learn generalisable representations. Techniques
to address these questions in unimodal and multi-
modal V&L contexts include: adversarial examples
(Jia and Liang, 2017; Jia et al., 2019); investigationof the impact of bias, be it linguistic (Gururan-
gan et al., 2018), visual semantic (Agarwal et al.,
2020), or socio-economic (Garg et al., 2019); and
the use of linguistically-informed counterfactual
and minimally-edited examples (Levesque et al.,
2012; Gardner et al., 2020). A trend within the
latter research line that is speciﬁc to V&L mod-
els is vision-and-language foiling (Shekhar et al.,
2017b; Gokhale et al., 2020; Bitton et al., 2021;
Parcalabescu et al., 2021; Rosenberg et al., 2021),
where the idea is to create counterfactual (i.e.,
foiled ) and/or minimally edited examples by per-
forming data augmentation on captions (Shekhar
et al., 2017b,a) or images (Rosenberg et al., 2021).
Since most V&L models are pretrained on some
version of the image-text alignment task, it is pos-
sible to test their ability to distinguish correct from
foiled captions (in relation to an image) in a zero-
shot setting. The construction of foils can serve
many investigation purposes. With V ALSE, we
target the linguistic grounding capabilities of V&L
models , focusing on pervasive linguistic phenom-
ena that span multiple tokens , described in §3.1–
§3.6. At the same time, we ensure that our data
is robust to perturbations and artefacts by i) con-
trolling for word frequency biases between cap-
tions and foils, and ii) testing against unimodal
collapse , a known issue of V&L models (Goyal
et al., 2017; Madhyastha et al., 2018), thereby pre-
venting models from solving the task using a single
input modality. The issue of neural models exploit-
ing data artefacts is well-known (Gururangan et al.,
2018; Jia et al., 2019; Wang et al., 2020b; He et al.,
2021) and methods have been proposed to uncover
such effects, including gradient-based, adversar-
ial perturbations or input reduction techniques (cf.
Wallace et al., 2020). Yet, these methods are still
not fully understood (He et al., 2021) and can be
unreliable (Wang et al., 2020b).
Our work is related to Gardner et al. (2020),
who construct task-speciﬁc contrast sets for NLU.
However, our focus is on modelling linguistic phe-
nomena instead of tasks, and we construct carefully
curated, balanced, single foils from valid instances
that we select from multiple multimodal datasets.
3 Constructing the V ALSE benchmark
We resort to a musical analogy to describe V ALSE:
Vision And Language Structured Evaluation is
composed of 6 pieces , each corresponding to a
speciﬁc linguistic phenomenon (see Table 1 for an8254
overview). Each piece consists of one or more in-
struments designed to evaluate a model’s ability to
ground that speciﬁc linguistic phenomenon.
All instruments are built by applying foiling func-
tions (FFs) speciﬁc to the linguistic phenomenon
under study. FFs take a correct caption as input and
change a speciﬁc part to produce a foiled caption
(orfoil). We design FFs such that the sentences
they produce fail to describe the image, while still
being grammatical and otherwise valid sentences.
Of course, a foiled caption may be less likely
than the original caption from which it was pro-
duced, and such unwarranted biases can be eas-
ily picked up by overparameterised V&L models.
Moreover, an automatic FF may fail to produce a
foil that contradicts the image, for example by alter-
ing the original caption to yield a near-synonymous
one, or one that is entailed by the original caption.
For phenomena that make it difﬁcult to control
these crucial properties of foils, we apply addi-
tional ﬁlters: i) some FFs make use of strong LMs
to propose changes to captions, so that the gener-
ated foils are still high-probability sentences; ii)
we use state-of-the-art natural language inference
(NLI) methods to detect cases where there is an
entailment between caption and foil, and ﬁlter out
such foils from the dataset (see §4 for discussion).
As a ﬁnal measure, we employ human annotators
to validate all generated testing data in V ALSE.
V ALSE data is sourced from existing V&Ldatasets. Below, we describe each piece and its
instruments, and the corresponding task setup in
V ALSE. For each instrument, we follow the same
procedure: i) we identify captions that contain in-
stances of the targeted linguistic phenomenon; ii)
we apply a FF that automatically replaces the ex-
pression with a variant that contradicts the original
expression’s visual content, thereby constructing
one or more foils from each target instance in the
original caption, as discussed in §4; we then iii)
subject the obtained foils to various ﬁlters, with the
aim of distilling a subset of valid andreliable foils
that cannot be easily tricked by a new generation
of highly parameterised pretrained V&L models.
3.1 Existence
Theexistence piece has a single instrument and tar-
gets instances with existential quantiﬁers . Mod-
els need to differentiate between examples i) where
there is no entity of a certain type or ii) where one
or more of these entities are visible in an image.
We use the Visual7W visual question answering
dataset (Zhu et al., 2016) and source its ‘how many’
examples, building a pool of those whose answers
are numerals (0, 1, 2, etc.). We use templates to
transform question and answer ﬁelds into a declara-
tive statement that correctly describes what can be
seen in the image, e.g. ‘Q: How many animals are
shown? A: 0’!‘There are 0 animals shown’. We
then transform these statements into an existential8255statement. In the example above, we replace the nu-
meral by the word ‘no’ to create a correct caption
(‘There are no animals shown’) and remove the
numeral altogether to create a foil (‘There are ani-
mals shown’). The existence piece has 505 image–
caption–foil tuples after manual validation, out of
534 candidates (cf. §4), and captions/foils are bal-
anced: 50% of the (correct) captions originally
have answer 0, and the remaining have answer 1 or
greater. Full details are provided in A.1.
3.2 Plurality
The plurality piece has a single instrument, con-
cerned with semantic number . It is intended to
test whether a model is able to distinguish between
noun phrases denoting a single entity in an im-
age (‘exactly one ﬂower’), versus multiple entities
(‘some ﬂowers’). The dataset consists of 851 vali-
dated instances out of 1000 generated candidates
(cf. §4), evenly divided between cases where the
caption contains a plural NP, foiled by replacing it
with a singular ( pl2sg : ‘some ﬂowers’!‘exactly
one ﬂower’), or conversely, the caption contains a
singular which is foiled by replacing it with a plural
(sg2pl ). Foil candidates were generated from the
COCO 2017 validation set (Chen et al., 2015). Full
details about the foil construction and our measures
against introducing biases with quantiﬁers such as
‘exactly one’, are provided in A.2.
3.3 Counting
The counting piece has three instruments: bal-
anced ,adversarial andsmall numbers . All in-
stances are statements about the number of entities
visible in an image . The model needs to differenti-
ate between examples where the speciﬁc number of
entities in the associated image is correct or incor-
rect, given the statement. Similarly to the existence
piece, we use the Visual7W VQA dataset (Zhu
et al., 2016) and source its ‘how many’ examples
whose answers are numerals (0, 1, 2, etc.). We use
templates to transform question and answer ﬁelds
into a declarative statement describing the image
and create foils by replacing the numeral in the
correct statement by another numeral.
All three instruments are designed to show
whether models learn strategies that generalize be-
yond the training distribution, and to what extent
a model exploits class frequency bias.Incount-
ing balanced we cap the number of examples toa maximum per class and make sure correct and
foil classes are balanced, so that models that ex-
ploit class frequency bias are penalized. In count-
ing adversarial we ensure that all foils take class
n2f0;1;2;3g, whereas all correct captions take
classm2fmjm4g. Biased models are ex-
pected to favour more frequent classes. Since small
numbers are naturally the most frequent, models
that resort to such biases should perform poorly on
this adversarial test set. Counting small numbers
is a sanity check where all correct captions and
foils have class n2f0;1;2;3g, and caption/foil
classes are balanced. Since models likely have
been exposed to many examples in this class set
and all such classes are high-frequency, with this in-
strument we disentangle model performance from
class exposure. Counting balanced, adversarial,
and small numbers have 868 (1000), 691 (756),
and 900 (1000) instances after (before) manual val-
idation, respectively (cf. §4). For details, see A.3.
3.4 Spatial relations
The relations piece has a single instrument and
focuses on the ability of models to distinguish be-
tween different spatial relations. Foils differ from
the original caption only by the replacement of
a spatial preposition. As with plurals, the data
was sourced from the COCO 2017 validation split.
To create foils, we ﬁrst identiﬁed all preposition
sequences in captions (e.g., ‘in’, ‘out of’). Foils
were created by masking the prepositions and using
SpanBERT (Joshi et al., 2020) to generate candi-
dates of between 1–3 words in length. We keep
SpanBERT candidates, which are spans whose
lengths vary from 1 to 3, if they differ from the orig-
inal preposition sequence, but exist in the dataset.
There are 535 instances after manual validation out
of 614 proposed instances (cf. §4), and we ensure
that prepositions are similarly distributed among
captions and foils. Full details are provided in A.4.
3.5 Actions
The actions piece has two instruments: i) action
replacement and ii) actant swap . They test a
V&L model’s capability to i) identify whether an
action mentioned in the text matches the action
seen in the image (e.g., ‘a man shouts /smiles at a
woman’), and ii) correctly identify the participants
of an action and the roles they play (e.g., is it the
man who is shouting or is it the woman, given the
picture in Table 1?).
The SWiG dataset (Pratt et al., 2020) contains8256504 action verbs, and we generate captions and
foils from SWiG annotations of semantic roles and
their ﬁllers. For the action replacement piece, we
exchange action verbs with other verbs from SWiG
that ﬁt the linguistic context as suggested by BERT.
For the actant swap, we swap role ﬁllers in the role
annotations, hence generating action descriptions
with inverted roles. Action replacement and actant
swap have 648 (779) and 949 (1042) instances after
(before) manual validation, respectively (cf. §4).
See A.5 for full details.
3.6 Coreference
The coreference piece aims to uncover whether
V&L models are able to perform pronominal coref-
erence resolution. It encompasses cases where i)
the pronoun has a noun (phrase) antecedent and
pronoun and (noun) phrase are both grounded in
the visual modality (‘ A woman is driving a motor-
cycle. Is shewearing a helmet?’), and cases where
ii) the pronoun refers to a region in the image or
even to the entire image (‘Is this outside?’).
We create foils based on VisDial v1.0 (Das et al.,
2017) with images from MSCOCO (Lin et al.,
2014). VisDial captions and dialogues are Q&A se-
quences. We select image descriptions of the form
[Caption. Question? Yes/No. ] where the ques-
tion contains at least one pronoun. When foiling,
we exchange the answer from yestonoand vice-
versa (see Table 1). We ensure a 50-50% balance
between yes / no answers.
The coreference piece consists of two instru-
ments: coreference standard originating from the
VisDial train set and a small coreference clean set
from the validation set, containing 708 (916) and
104 (141) examples after (before) manual valida-
tion, respectively (cf. §4).See A.6 for full details.
4Reliable construction of valid foils
In V ALSE, an instance consisting of an image-
caption-foil triple is considered valid if: the foil
minimally differs from the original caption; the foil
does not accurately describe the image; and inde-
pendent judges agree that the caption, but not the
foil, is an accurate description of the image. We
consider a foiling method to be more reliable the
more it ensures that a generated foil does not sub-
stantially differ from a human caption regarding
distributional and plausibility bias, and cannot be
easily solved unimodally.In this section, we discuss automatic and man-
ual means to reliably construct valid foils. In this
context, two types of bias are especially worthy of
note: distributional bias (§4.1) and plausibility bias
(§4.2). In §4.3 we discuss how we apply a natu-
ral language inference model to ﬁlter examples in
our data pipeline, and §4.4 show how we manually
validate all examples in our benchmark. Random
samples from the ﬁnal version of each instrument
are shown in Tab. 6–11.
4.1 Mitigating distributional bias
A ﬁrst form of bias is related to distributional imbal-
ance between captions and foils (e.g., certain words
or phrases having a high probability only in foils).
Previous foiling datasets exhibit such imbalance,
enabling models to solve the task disregarding the
image (Madhyastha et al., 2019). To mitigate this
problem, for each phenomenon and throughout our
data creation process, we ensure that the token fre-
quency distributions in correct and foiled captions
are approximately the same (cf. App. A and E).
4.2 Countering plausibility bias
A second form of bias may arise from automatic
procedures yielding foils that are implausible or un-
natural, which can facilitate their detection. Often,
V ALSE pieces can be safely foiled by simple rules
(e.g., switching from existence to non-existence,
or from singular to plural or vice versa). However,
with spatial relations andactions , a foil could be
deemed unlikely given only the textual modality
and independently of the image, e.g., ‘a man stands
under /ona chair’. Such plausibility biases may
be detected by large language models that incorpo-
rate commonsense knowledge (Petroni et al., 2019;
Wang et al., 2020a), and we expect future V&L
models to exhibit similar capabilities.
To ensure that foiled and correct captions are
similarly plausible, we use language models such
as BERT (Devlin et al., 2019) and SpanBERT
(Joshi et al., 2020) to suggest replacements in our
foiling functions. Additionally, in the case of spa-
tial relations and plurals, we also apply a grammat-
icality ﬁlter using GRUEN (Zhu and Bhat, 2020).
GRUEN was originally proposed to automatically
score generated sentences based on discourse-level
and grammatical properties. We use only the gram-
maticality component of GRUEN, and retain only
foil candidates with a grammaticality score 0:8.
Furthermore, we evaluate unimodal, language-
only models on V ALSE to verify whether our8257benchmark could be solved by a multimodal model
with strong linguistic capacities in unimodal col-
lapse , whereby a model silently relies on a single
modality within which biases are easier to exploit
(Goyal et al., 2017; Shekhar et al., 2019a). By eval-
uating V ALSE with unimodal models, we establish
a baseline that V&L models should exceed if we
are to expect true multimodal integration.
4.3 Filtering foils with NL Inference
When constructing foils, we need to ensure that
they failto describe the image. To test this au-
tomatically, we apply natural language inference
(NLI) with the following rationale: We consider an
image and its caption as a premise and its entailed
hypothesis, respectively (a similar rationale is ap-
plied in the visual entailment task; Xie et al., 2019).
In addition, we consider the caption as premise and
thefoil as its hypothesis . If a NLI model predicts
the foil to be entailed (E) by the caption, it cannot
be a good foil since by transitivity it will give a
truthful description of the image. By contrast, if
the foil is predicted to contradict (C) or to be neu-
tral (N) with respect to the caption, we take this as
an indicator of a valid (C) or a plausible (N) foil.
We use the NLI model ALBERT (Lan et al.,
2020) ﬁnetuned on the task (see Appendix C for
details). Filtering with NLI was initially applied
torelations, plurals andactions , on the grounds
that foils in these pieces may induce substantive
changes to lexical content.Following automatic
labelling of caption-foil pairs, we manually vali-
dated a sample labelled as E, C or N. For relations
(N= 30 ), labels were found to be near 100% accu-
rate with only 2 (0.06%) errors overall. For plurals
(N= 60 , 50% sg2pl and 50% pl2sg ), the er-
ror rate was also low, with 0 errors for C, 33%
errors for E and 11% errors for N. Here, a number
of entailment errors were due to odd formulations
arising from the automatic foiling process, whereas
no such oddities were observed for C. We therefore
include only foils labelled C in the ﬁnal relations
and plurals pieces. For actions , the model labelledcontradictions very accurately (0% error) but was
erroneous up to 97.1% for E, meaning that a large
number of valid foils would be spuriously excluded.
To avoid reducing the dataset too much, we did not
use NLI ﬁltering for actions, but relied on human
annotation as a ﬁnal validity check.
4.4 Manual evaluation of generated foils
As a ﬁnal step, the data for each instrument was
submitted to a manual validation. For each instance,
annotators were shown the image, the caption and
the foil. Caption and foil were numbered and dis-
played above each other to make differences more
apparent, with differing elements highlighted in
boldface (Fig. 2, App. E). Annotators were not in-
formed which text was the caption and which was
the foil, and captions appeared ﬁrst (numbered 1)
50% of the time. The task was to determine which
of the two texts accurately described what could be
seen in the image. In each case, annotators had a
forced choice between ﬁve options: a) the ﬁrst, but
not the second; b) the second, but not the ﬁrst; c)
both of them; d) neither of the two; and e) I cannot
tell.
Each item was annotated by three individuals.
The validation was conducted on Amazon Mechan-
ical Turk with a ﬁxed set of annotators who had
qualiﬁed for the task. For details see App. E. For
the ﬁnal version of V ALSE, we include instances
which passed the following validation test: at least
two out of three annotators identiﬁed the caption,
but not the foil, as the text which accurately de-
scribes the image. Across all instruments, 87.7%
of the instances satisﬁed this criterion (min 77.3%;
max 94.6%), with 73.6% of instances overall hav-
ing a unanimous (3/3) decision that the caption,
but not the foil, was an accurate description. We
consider these ﬁgures high, suggesting that the au-
tomatic construction and ﬁltering procedures yield
foils which are likely to be valid, in the sense dis-
cussed in §4 above.
We compute inter-annotator agreement for each
instrument (Tab. 5). On the valid subset, agreement
is low to medium (Krippendorff’s : min=0.23,
max=0.64, mean=0.42, sd=0.12). We note that
there is considerable variation in the number of an-
notations made by individuals, and is computed
over 5 categories. Hence, this result cannot be
straightforwardly interpreted as a ceiling of human
performance for V ALSE. However, is higher for
pieces on which models also perform better (e.g.8258existence, Foil-It!; cf. §5).
5 Benchmarking with V ALSE
We propose V ALSE as a task-independent, zero-
shot benchmark to assess the extent to which mod-
els learn to ground speciﬁc linguistic phenomena as
a consequence of their pretraining (or ﬁne-tuning).
V ALSE is built in the spirit of approaches such
as Checklist (Ribeiro et al., 2020), including pairs
consisting of captions and minimally edited foils.
The only requirement to evaluate a model on
our benchmark is: i)to have a binary classiﬁcation
head to predict whether an image-sentence pair is
foiled, or ii)to predict an image-sentence matching
score between the image and the caption vs. the foil,
returning the pair with the highest score. Systems
reporting results on V ALSE are expected to report
any data used in model training prior to testing on
V ALSE, for comparability.
5.1 Benchmark Metrics
We employ ﬁve metricsfor evaluation: over-
allaccuracy (acc) on all classes (foil and cor-
rect); precision (p) measuring how well mod-
els identify the correct examples; foil precision
(p) measuring how well foiled cases are identi-
ﬁed; pairwise ranking accuracy (acc), which
measures whether the image-sentence alignment
score is greater for a correct image-text pair than
for its foiled pair; and area under the receiver
operating characteristic curve (AUROC), which
measures how well models distinguish correct vs.
foiled examples across different prediction thresh-
olds.accis more permissive than accas it accepts
model predictions if the score for a foil is lower
than the caption’s score. Our main metrics are AU-
ROC andacc.accgives results for a pair himage,
captioniandhimage, foili. Both AUROC and acc
are well suited to evaluate minimally-edited pairs
as neither uses a classiﬁcation threshold. As for p
andp, since these are competing metrics where
naively increasing one can decrease the other, we
report the smaller of the two as an indicator of
how informed model predictions are. Since all in-
struments are implemented as a balanced binary
classiﬁcation, the random baseline is always 50%.
5.2 V&L models
We benchmark ﬁve V&L models on V ALSE: CLIP
(Radford et al., 2021), LXMERT (Tan and Bansal,2019), ViLBERT (Lu et al., 2019), ViLBERT 12-
in-1 (Lu et al., 2020), and VisualBERT (Li et al.,
2019). These models have different architectures
and are pretrained on a variety of tasks with differ-
ent training data. We also benchmark two unimodal
text-only models, GPT1 (Radford et al., 2018) and
GPT2 (Radford et al., 2019). See Appendix D for
details on all these models used in our evaluation.
Unimodal models GPT1 and GPT2 are autore-
gressive language models pretrained on English
text. We test whether V ALSE is solvable by these
unimodal models by computing the perplexity of
the correct and foiled caption and predicting the
entry with the lowest perplexity . If the perplexity
is higher for the foil, we take this as an indication
that the foiled caption may suffer from plausibility
bias or other linguistic biases (cf. §4.2).
5.3 Experiments and Results
We test V&L and unimodal models on V ALSE in a
zero-shot setting, and also evaluate on a number of
correct captions and foils from the FOIL it! dataset
(Shekhar et al., 2017b) (cf. App. A.7 for details).
All results are listed in Table 2.
Unimodal results For most instruments, uni-
modal results are close to random and hence do
not signal strong linguistic or plausibility biases.
One exception is the original FOIL it! dataset, in
line with Madhyastha et al. (2019)’s ﬁndings. Also
the spatial relations (77.2%), action replacement
(66.8%) and actant swap (76.9%) instruments sug-
gest plausibility biases in foils. Such biases are
hard to avoid in automatic foil generation for ac-
tions due to the verb arguments’ selectional restric-
tions, which are easily violated when ﬂipping role
ﬁllers, or replacing the verb. Similar considerations
hold for relations: though SpanBERT proposals are
intended to aid selection of likely replacements for
prepositions, plausibility issues arise with relatively
rare argument-preposition combinations.
While these might be the ﬁrst instruments in
V ALSE to be solved in the future, current V&L
models struggle to detect even blatant mismatches
of actant swap, e.g., ‘A ball throws a tennis player.’
For V ALSE, the unimodal scores will serve as a
baseline for the pairwise accuracy of V&L models.
Multimodal results The best zero-shot results
are achieved by ViLBERT 12-in-1 with the high-
est scores across the board, followed by ViLBERT,8259
LXMERT, CLIP,and ﬁnally VisualBERT. The
latter obtains high pbut very low pvalues—
reﬂected in the min(p;p)scores—indicating that
VisualBERT learned a heuristic that does not gen-
eralise (see Hendricks and Nematzadeh, 2021, for
similar observations with other models). We hy-
pothesise that this is due to the way image-sentence
alignment is framed in VisualBERT’s pretraining:
the model expects an image and a correct sen-
tencec, and predicts whether a second sentence
cis a match.During pretraining candcare
likely to differ in many ways, whereas in our set-
ting, they are nearly identical. This may bias the
model against predicting foils, which would raise
the valuep.
Instruments centered on individual objects like
existence and the FOIL it! dataset are almost solved
by ViLBERT 12-in-1, highlighting that models are
capable of identifying named objects and their pres-
ence in images. However, none of the remaining
pieces can be reliably solved in our adversarial
foiling settings: i) distinguishing references to sin-
gle vs. multiple objects or counting them in animage (plurality and counting); ii) correctly classi-
fying a named spatial relation between objects in
an image (relations); iii) distinguishing actions and
identifying their participants, even if supported by
preference biases (actions); or, iv) tracing multiple
references to the same object in an image through
the use of pronouns (coreference).
Correct vs. foil precision pandpshow that
V&L models struggle to solve the phenomena in
V ALSE. When a model achieves high precision on
correct captions pthis is often at the expense of
very low precision on foiled captions p(cf. ViL-
BERT), or vice-versa (cf. VisualBERT). This sug-
gests that such models are insensitive to V ALSE’s
inputs: models that almost always predict a match
will inﬂatepat the expense of p.min(p;p)
reveals that VisualBERT and ViLBERT perform
poorly and below random baseline, and LXMERT
close to or below it. ViLBERT 12-in-1 performs
strongly on existence, well on counting, but strug-
gles on plurality, spatial relations, coreference, and
actions. These tendencies we see reﬂected in our
main metrics, accand AUROC.
6 Conclusions and Future Work
We present the V ALSE benchmark to help the com-
munity improve V&L models by hard-testing their
visual grounding capabilities through the lens of lin-8260guistic constructs. Our experiments show that V&L
models identify named objects and their presence
in images well (as shown by the existence piece),
but struggle to ground their interdependence and re-
lationships in visual scenes when forced to respect
linguistic indicators. We encourage the commu-
nity to use V ALSE for measuring progress towards
V&L models capable of true language grounding.
Furthermore, V ALSE could be used as an indirect
assessment of datasets, as models could be evalu-
ated before and after training or ﬁne-tuning to see
if a dataset helps models improve on any of the
aspects tested by V ALSE.
V ALSE is designed as a living benchmark. As
future work we plan to extend it to further linguistic
phenomena, and to source data from diverse V&L
datasets to cover more linguistic variability and
image distributions.
Acknowledgments
IC has received funding from the European Union’s
Horizon 2020 research and innovation programme
under the Marie Skłodowska-Curie grant agree-
ment No 838188. AG and MC are supported by the
European Union’s Horizon 2020 research and in-
novation Programme under the Marie Skłodowska-
Curie grant agreement No 860621. This collabora-
tion was facilitated by the Multi3Generation COST
Action CA18231.
References8261826282638264A Benchmark creation
A.1 Existence
Theexistence piece has a single instrument and tar-
gets instances with existential quantiﬁers . Mod-
els need to differentiate between examples i) where
there is no entity of a certain type or ii) where there
is one or more of these entities visible in an image.
Data sources We use the Visual7W visual ques-
tion answering dataset (Zhu et al., 2016) to source
examples, starting with the ‘how many’ questions
in Visual7W and building a pool of those whose
answers are numerals (e.g., 0, 1, 2, etc.). We use
the templates from Parcalabescu et al. (2021) to
transform question and answer ﬁelds into a declara-
tive statement that correctly describes what can be
seen in the image, e.g., ‘Q: How many animals are
shown? A: 0’!‘There are 0 animals shown’.
Foiling method Let us usex=‘There are N
animals shown’ as a running example for a cor-
rect caption, where Nis a number. If N > 0, we
simply remove Nfrom the sentence, effectively
creating the statement 9xor ‘There are animals
shown’. IfN= 0, we replace Nby ‘no’, creating
the statement:9xor ‘There are no animals shown’.
If necessary, we ﬁx singular–plural agreement. To
create data with balanced correct and foil classes,
we select 50% of our examples from those where
the correct answer is originally 0, and the remain-
ing50% from those where the correct answer is
any other number (e.g., 1, 2, etc.). To create foils,
we then simply convert the statement from 9xto
:9x, and vice-versa.
A.2 Plurality
The plurality piece has a single instrument, con-
cerned with semantic number , that is, the distinc-
tion between single entities in an image (‘exactly
one ﬂower’) and multiple instances of the same
type (‘some ﬂowers’). In this piece, foil candidates
are created either by converting a singular NP and
its coreferents to a plural, or vice versa.
Data sources The data was sourced from the val-
idation split of the COCO 2017 dataset (Chen et al.,
2015). Captions are only foiled if their length after
tokenization with the pretrained BERT tokenizer
is of 80 tokens or less. This is done to minimise
the risk that captions and foils need to be truncatedto accommodate the input speciﬁcations of current
pretrained V&L models.
Foiling method Foiling is done in two directions:
singular-to-plural ( sg2pl ) or plural-to-singular
(pl2sg ). Given a caption, NP chunking is applied
to identify all non-pronominal NPs. In the sg2pl
case, a foiled version of a caption containing a sin-
gular NP is created by pluralising the head noun.
We automatically identify anaphoric expressions
coreferring to the singular NP within the caption
and pluralise them in the same way. For NPs which
are subjects of copular VPs or VPs with an auxil-
iary requiring subject-verb number agreement (e.g.
‘NisV’), we also pluralise the verb. Note that
this procedure creates a potential foil for every sin-
gular NP in the caption; thus, more than one foil
candidate can be created for each instance in the
source dataset.In the pl2sg case, the same pro-
cedure is carried out, but turning a plural NP, as
well as its coreferents, into a singular. We generate
all foil candidates using the Checklist framework
(Ribeiro et al., 2020), within which we implement
our procedures for data perturbation.
An important consideration, especially in the
pl2sg case, is that singularising an NP in a foil
can still be truth-preserving. Speciﬁcally, a caption
with a plural NP, such as ‘A small copper vase with
some ﬂowers in it’, arguably still entails the ver-
sion with the singular ‘(. . . ) a ﬂower ’. As a result,
the singular version may still correctly be judged
to match the image. One way around this problem
is to insert a quantiﬁer in the singular NP which
makes it explicit that exactly one instance and no
more is intended (e.g. ‘ exactly one ﬂower’). This
may however result in a biased dataset, with such
singular quantiﬁers acting as signals for singular
foils and enabling models to solve the task with
no grounding in the visual information. We avoid
this by adopting a uniform strategy for both sg2pl
andpl2sg . We determine two singular quantiﬁers
(‘exactly one N’ and ‘a single N’) and two plural
quantiﬁers (‘some N’, ‘a number of N’). When a
foil candidate is generated, we alter the original NP
by inserting one of the two quantiﬁers matching
its semantic number, and generate a foil with one8265of the two quantiﬁers for the other number. In the
foregoing example, we end up with ‘A small copper
vase with some ﬂowers / exactly one ﬂower in it.’
After generating all candidate foils, in both direc-
tions, we use the GRUEN pretrained model (Zhu
and Bhat, 2020) to score the foils for grammat-
icality. We only keep foils with a score 0:8,
and run each foil-caption pair through the NLI
model described in Section 4.3, keeping only pairs
whose predicted label is contradiction , for an ini-
tial candidate set of 1000 cases (500 sg2pl and
500pl2sg ), of which 851 (85.1%) are considered
valid following manual validation (see §4.4). Fig-
ure 4 shows the distribution of nouns in captions
and foils, before and after the validation. Note that
the validation process does not result in signiﬁcant
change to the distributions.
A.3 Counting
The counting piece comes in three instruments:
balanced ,adversarial andsmall numbers . All
three instruments include instances with statements
about the number of entities visible in an image .
The model needs to differentiate between exam-
ples where the speciﬁc number of entities in the
associated image is correct or incorrect, given the
statement.
All three instruments are designed to show
whether models learn strategies that generalize be-
yond the training distribution, and to what extent
a model exploits class frequency bias.Incount-
ing balanced we cap the number of examples to
a maximum per class and make sure correct/foil
classes are balanced, so that models that exploit
class frequency bias are penalized. In counting
adversarial we make sure that all foils take class
n2f0;1;2;3g, whereas all correct captions take
classn2fnjn4g. Biased models are ex-
pected to favour more frequent classes and these
correspond to smaller numbers, therefore models
that resort to such biases should perform poorly on
this adversarially built test. Instrument counting
small numbers is a sanity check where all correct
captions and foils have class n2f0;1;2;3g, and
caption/foil classes are balanced. Models likely
have been exposed to many examples in this class
set, so with this instrument we assess model per-
formance certain it does not suffer from (class)
exposure bias.Data sources We use the Visual7W visual ques-
tion answering dataset (Zhu et al., 2016) and source
its ‘how many’ examples, building a pool of those
whose answers are numerals (e.g., 0, 1, 2, etc.). We
use the templates from Parcalabescu et al. (2021) to
transform question and answer ﬁelds into a declara-
tive statement that correctly describes what can be
seen in the image.
Foiling method We create foils by directly re-
placing the numeral in the correct caption by an-
other numeral. When creating foils we make sure
that the class distribution for correct and foiled cap-
tions are approximately the same, i.e., there are a
similar number of correct and foiled examples in
each class in each instrument. The only exception
is the counting adversarial instrument, where the
classes used in correct and foiled captions are dis-
joint, i.e.,n2f0;1;2;3gandn2fnjn4g,
respectively. See Figure 3 for a visualisation of
these distributions.
A.4 Spatial relations
Therelations piece has one instrument and focuses
on the ability of models to distinguish between dif-
ferent spatial relations, as expressed by preposi-
tions. Foils therefore consist of captions identical
to the original except for the replacement of a spa-
tial preposition.
Data sources Data was sourced from the COCO
2017 validation split (Chen et al., 2015). To gen-
erate foil candidates, we ﬁrst extracted from the
original COCO captions all the sequences consist-
ing of one or more consecutive prepositions (e.g.,
‘on’ or ‘out of’). Foils are generated by detecting
these preposition spans, and replacing them with
another preposition span attested in the list.
Foiling method To generate foils, we mask the
preposition span in an original caption, and use
SpanBERT (Joshi et al., 2020), a pretraining
method based on BERT (Devlin et al., 2019).
The advantage of SpanBERT over BERT is that in
a masked language modelling context, with masks
spanning more than a single word, SpanBERT pre-
dicts sequences and takes into account their joint
probability, whereas BERT trained with standard
Masked Language Modelling can only predict sin-
gle tokens independently. With SpanBERT, we8266generate replacements of between 1 and 3 tokens
in length, in each case retaining only the best pre-
diction out of the top kwhich matches one of the
preposition sequences in the pre-extracted list.
After all candidates are generated, we apply
GRUEN (Zhu and Bhat, 2020) to score the foils for
grammaticality, and further apply the NLI model
descibed in Section 4.3 to label the entailment rela-
tionship between caption and foil pairs. From the
resulting data, we sample as follows: i) we keep
only caption-foil pairs labelled as contradiction ,
where the GRUEN grammaticality score is 0:8;
ii) for every caption-foil pair sampled where pis
replaced with q, we search for another caption-foil
pair whereqis replaced with p, if present. This
strategy yields a roughly balanced dataset, where
no single preposition or preposition sequence is
over-represented in captions or foils.
These processes result in an initial set of 614
cases, of which 535 (87.1%) are selected following
manual validation described in §4.4.
Figure 3 shows proportions in captions and foils
of the prepositions. E.g.: ‘A cat plays with a pocket
knife on / underneath a table.’
As with plurals, we implement procedures
for foil candidate generation by extending the
perturb functionality in Checklist (Ribeiro et al.,
2020).
A.5 Actions
Theaction piece consists of two instruments: i) ac-
tion replacement and ii) actant swap . They are
testing a V&L model’s capability of i) identifying
whether an action mentioned in the textual modal-
ity matches the action seen in the image or not
(e.g. ‘a man shouts /smiles at a woman’) and ii)
correctly identifying the participants of an action
and the roles they are playing in it (e.g., given the
picture in Table 1: is it the man or the woman who
shouts?).
Data source For creating interesting foils with di-
verse actions, we focus on the SWiG dataset (Pratt
et al., 2020) that comprises 504 action verbs anno-
tated with semantic roles and their ﬁllers, which are
grounded in images of the imSitu dataset (Yatskar
et al., 2016). We generate English captions for
the images using SimpleNLG (Gatt and Reiter,
2009). For generation we use the speciﬁed ac-tion verb , the realized FrameNet semantic roles
and their annotated ﬁller categories (see Table 1
forshout :A : man, A : woman),
and generate short captions, with realization of two
roles in active form. We apply various ﬁlters to
ensure high quality of the generated captions using
diverse metricsand manual checks through AMT
crowdsourcing.
Foiling method When creating the action re-
placement instrument, we need to make sure that
the action replacement suits the context. We pro-
pose action replacements with BERT (Devlin et al.,
2019) that need to satisfy three conditions: 1) the
proposed action verbs originate from the SWiG
dataset – otherwise new verbs are introduced on
the foil side only, which may induce biases; 2) the
frequency distribution of action verbs on the cap-
tion and on the foil side is approximately the same
(cf. Figure 4); 3) we constrain the replacement
verbs to be either antonyms of the original verb
or at least not synonyms, hyponyms or hypernyms
to the original, according to WordNet (Fellbaum,
1998) in order to avoid situations where replace-
ments are almost synonymous to the original action.
Theactant swap instrument is based on the origi-
nal image annotations, but swaps the two role ﬁllers
(e.g., ‘A woman shouts at the man.’ for the image
in Table 1). To avoid agreement mistakes, we gen-
erate these foils using the inverted role ﬁllers as
input.
We plot caption and foil word frequency distribu-
tions for action replacement in Figure 4. We do not
plot statistics for the actant swap instrument since
by construction it cannot suffer from distributional
bias since caption and foil contain the same words
up to a permutation .
A.6 Coreference
The coreference piece consists of two pieces:
coreference standard andcoreference clean . It
aims to uncover whether V&L models are able to
perform pronoun coreference resolution. The coref-
erence phenomenon encompasses both cases where
i) the pronoun refers to a noun (phrase) and both
the pronoun and the (noun) phrase are grounded8267in the visual modality (e.g. ‘ A woman is driving a
motorcycle. Is shewearing a helmet?’), and cases
where ii) the pronoun refers directly to a region in
the image or even to the whole image (e.g. ‘A man
is sitting on a bench. Is this outside?’).
Data source We source the data from VisDial
v1.0 (Das et al., 2017), which contains images
from MSCOCO (Lin et al., 2014), their captions
and dialogues about the images in form of Q&A
sequences. To ensure that the coreference phe-
nomenon is present in the [ Caption. Question?
Yes/No. ] formulations, we check whether pronouns
are present in the question . The list of pronouns
and their frequencies in our train-val-test splits are
represented in Figure 1.
Thecoreference standard instrument contains
916 data samples (708 are valid) from the Vis-
Dial’s training set. The data of coreference clean
instrument consisting of 141 samples (104 are
valid), originates from VisDial’s validation set.
With models that have been trained on VisDial,
we would be in the situation where models are
tested on their training data. Therefore we also
have the coreference clean instrument based on
the validation set of VisDial to test models safely.
Unfortunately, we cannot use VisDial’s test set be-
cause the required question-answers annotations
necessary for foiling are withheld.
Foiling method When foiling, we take the im-
age description of the form [ Caption. Question?
Yes/No. ] and exchange the answer: yes→noand
vice-versa (see example in Table 1). This way, we
keep the full textual description including pronoun
and noun (phrase) intact, hence ensuring that the
coreference phenomenon is present and valid in the
foil too, and rely on the model to interpret afﬁr-
mation and negation correctly. Note that we rely
on the capability of models to correctly interpret
negation also in the existence piece (cf. §3.1).
Arguably, coreference is the most difﬁcult phe-
nomenon to foil in V ALSE. Especially in cases
where pronouns refer to a noun (phrase) (e.g.,
‘A woman is driving a motorcycle. Is shewear-
ing a helmet? Yes.’), exchanging the pronoun with
another pronoun would generate incoherent and un-
likely sequences(e.g., ‘ A woman is driving a mo-
torcycle. Is hewearing a helmet?’), and exchanging
it with a noun phrase would furthermore break the
pronoun coreference phenomenon because there
would be no pronoun anymore (e.g., ‘ A woman is
driving a motorcycle. Is the man wearing a hel-
met?’). Therefore when foiling the coreference
piece, we aim to keep the original description in-
tact for ensuring the preservation of the coreference
phenomenon. Hence we rely on the answers con-
taining yesornoand exchange afﬁrmative to
negative answers and vice-versa.
A.7 FOIL it! data
We include an additional piece in V ALSE consist-
ing of 1000 randomly sampled entries from the
FOIL it! dataset (Shekhar et al., 2017b). Each
entry in FOIL it! consists of an MSCOCO (Lin
et al., 2014) image and a foiled caption where a
noun phrase depicting an object visible in the im-
age was replaced by a semantically related noun
phrase. Since examples in the FOIL it! dataset are
linked to MSCOCO, we use these links to retrieve
one correct caption from the ﬁve captions available
for the image, and create an image–caption–foil
triple. From the original 1000 entries, 943 have
been validated by our manual annotation proce-
dure (in Appendix E). Please refer to Shekhar et al.
(2017b) for more details.
B Evaluation metrics
We evaluate pretrained V&L models on V ALSE
using accuracy (acc), the overall accuracy on all
classes; precision orpositive predictive value (p),
which measures the proportion of correctly identi-
ﬁedcorrect captions ; and foil precision ornegative
predictive value (p), which measures the propor-
tion of correctly identiﬁed foiled examples; pair-
wise ranking accuracy acc, computed using the
image-sentence alignment score that the model
assigns to correct and foiled image-text pairs; and8268area under the receiver operating characteris-
tic curve (AUROC)—a classic metric used in ma-
chine learning classiﬁcation problems—which in
our case measures how well models distinguish
correct vs. foiled examples across different predic-
tion thresholds. The AUROC has a probabilistic
interpretation and can be understood as the prob-
ability that a model will assign a higher score to
a randomly chosen correct example relative to a
randomly chosen foil.
Withacc, a prediction is considered successful,
if given an image ( i) paired with a correct ( c) versus
a foil (f) text, the score of the positive/correct pair
is greater than that of the foiled pair.
acc=PPs(i;c;f )
jCj+jFj;
s(i;c;f ) =(
1;if(i;f)(i;c);
0;otherwise,
whereCis the set of correct image-caption pairs
(i;c), andFis the set of foils for the pair ( i;c).
The pairwise accuracy accis important for
two reasons: First, it enables V&L models to be
evaluated on V ALSE without a binary classiﬁcation
head for classifying image-sentence pairs as correct
or foiled. For example, CLIP (Radford et al., 2021)
is a model that computes a score given an image-
sentence pair. This score can be used to compare
the scores of a correct image-sentence pair and the
corresponding foiled pair. By contrast, a model
like LXMERT (Tan and Bansal, 2019) has a binary
image-sentence classiﬁcation head and can predict
a correct pair independently of the foiled pair (and
vice-versa). Second, accenables the evaluation of
unimodal models on V ALSE, as motivated in §4.2.
In Table 4, we show results for all models investi-
gated according to all above-mentioned metrics.
C Filtering methods
NLI ﬁltering For NLI ﬁltering we make use of
theHuggingFace (Wolf et al., 2020) implementa-
tion of ALBERT (xxlarge-v2) that was already ﬁne-
tuned on the concatenation of SNLI (Bowman et al.,
2015), MultiNLI (Williams et al., 2018), FEVER-
NLI (Nie et al., 2019) and ANLI datasets (Nie
et al., 2020). The model is the best performing on
the ANLI benchmark leaderboardand it achieves
90% accuracy on MultiNLI devset.D Vision & Language and Unimodal
Models
In Table 3 we summarise the ﬁve V&L models used
in our experiments, their architecture, pretraining
tasks and data, and ﬁne-tuning tasks (if any).
CLIP CLIP (Radford et al., 2021) is composed
of two transformer-based text and an image en-
coders. These are jointly trained on 400M image-
text pairs through contrastive learning for predict-
ing high scores for paired image-text examples and
low scores when image-text samples are not paired
in the dataset. CLIP has shown zero-shot capa-
bilities in e.g. object classiﬁcation, OCR, activity
recognition (Radford et al., 2021). Goh et al. (2021)
have shown the existence of multimodal neurons
in CLIP, responding to the same topic regardless of
whether it is represented in an image, drawing or
handwritten text. We use CLIP’s image-text align-
ment scores for benchmarking on V ALSE: Given
an image, we compare whether CLIPpredicts
higher image-text similarity for the correct or for
the foiled caption.
LXMERT LXMERT (Tan and Bansal, 2019) is
a dual-stream transformer model combining V&L
through cross-modal layers. It is pretrained on
MSCOCO (Lin et al., 2014) and on multiple VQA
datasets for (i) multimodal masked word and object
prediction, (ii) image-sentence alignment, i.e., de-
termining whether a text corresponds to an image
or not, and (iii) question-answering. For bench-
marking on V ALSE, we use LXMERT’simage-
sentence alignment head.
ViLBERT and ViLBERT 12-in-1 ViLBERT
(Lu et al., 2019) is a BERT-based transformer archi-
tecture that combines V&L on two separate streams
by co-attention layers. It is pretrained on Google
Conceptual Captions (Sharma et al., 2018) on (i)
multimodal masked word and object prediction;
and (ii) image-sentence alignment. ViLBERT 12-
in-1 (Lu et al., 2020) further ﬁnetuned a ViLBERT
model checkpoint on 12 different tasks including
VQA, image retrieval, phrase grounding and oth-
ers.We use the image-sentence alignment head
of the publicly available model checkpoints for8269
ViLBERTand ViLBERT 12-in-1.
VisualBERT VisualBERT (Li et al., 2019) is
also a BERT-based transformer. Its single-stream
architecture encodes image regions and linguis-
tic features via a transformer stack, using self-
attention to discover the alignments between the
two modalities. VisualBERT is pretrained on
MSCOCO captions (Chen et al., 2015) on twotasks: (i) masked language modelling, and (ii)
sentence-image prediction. The latter is framed
as an extension of the next sentence prediction task
used with BERT. Inputs consist of an image and
a caption, with a second caption which has a 50%
probability of being random. The goal is to deter-
mine if the second caption is also aligned to the
image. In our experiments, we use the publicly
available implementation of VisualBERT.
GPT-1 and GPT-2 – Unimodal models GPT1
(Radford et al., 2018) and GPT2 (Radford et al.,8270
2019) are transformer-based autoregressive lan-
guage models pretrained on English data through
self-supervision. We test whether our benchmark is
solvable by these unimodal models by computing
the perplexity of the correct sentence and compare
it to the perplexity of the foiled sentence. In case
the computed perplexity is higher for the foil than
for the correct sentence, we assume that the cor-
rectly detected foiled caption may possibly suffer
from a plausibility bias (as described in section
4.2) or from other biases (e.g. a model’s preference
towards afﬁrmative or negative sentences).
E Mechanical Turk Annotation and
Evaluation
Setup The validation study was conducted on all
the data for each instrument in V ALSE, as well
as for the FOIL it! data (Shekhar et al., 2019b).
Each instance consisted of an image, a caption and
a foiled version of the caption, as shown in Fig-
ure 2. Annotators received the following general
instructions:
You will see a series of images, each
accompanied by two short texts. Your
task is to judge which of the two texts
accurately describes what can be seen in
the image.
Each instance was accompanied by the caption
and the foil, with the ordering balanced so that thecaption appeared ﬁrst 50% of the time. In each
instance, the caption and foil were placed above
each other, with the differing parts highlighted in
bold. Annotators were asked to determine which
of the two sentences accurately describes what can
be seen in the image? In each case, they had to
choose between ﬁve options: (a) the ﬁrst, but not
the second; (b) the second, but not the ﬁrst; (c) both
of them; (d) neither of the two; and (e) I cannot tell.
We collected three annotations for each instance,
from three independent workers.
Annotator selection We recruited annotators
who had an approval rating of 90% or higher on
Amazon Mechanical Turk. We ran an initial, pre-
selection study with 10 batches of 100 instances
each, in order to identify annotators who under-
stood the instructions and performed the task ade-
quately. The pre-selection batches were ﬁrst man-
ually annotated by the authors, and we identiﬁed
‘good’ annotators based on the criterion that they
preferred the caption to the foil at least 70% of
the time. Based on this, we selected a total of 63
annotators. Annotators were paid $0.05 per item
(i.e. per HIT on Mechanical Turk).
Results Table 5 shows, for each instrument, the
number of instances in total, as well as the pro-
portion of instances which we consider valid , that
is, those for which at least two out of three anno-
tators chose the caption, but not the foil , as the8271
text which accurately describes the image. We also
show the number of instances for which annotators
unanimously (3/3) chose the caption.
Annotator agreement As shown in Table 5, the
proportion of valid instances in each instrument
was high, ranging from 73.8% to 94.6%, with most
instruments having annotators choose the caption
well over 80% of the time. The table also shows
two inter-annotator agreement statistics, both com-
puted using Krippendorff’s : over all the data
in a given instrument, and over the valid subset
only. On the valid subset, agreement is higher, and
ranges from 0.3 to 0.6 (mean = 0.42; sd=0.12).
There is a signiﬁcant positive correlation between
the percentage of valid instances per instrument
and thevalue (Spearman’s = 0:75;p < : 05).
The low to medium agreement suggested by the 
range is due to two factors: ﬁrst, the statistic is com-
puted over the entire pool of annotators, of whom
there were signiﬁcant diversions in the amount of
annotations they computed (e.g. some workers an-
notated fewer than 5 HITs); furthermore, the agree-
ment is computed over 5 categories (see above).
Given these factors, the inter-annotator agreement
results should be treated with caution, and are not
straightforwardly interpretable as an index of hu-
man performance on V ALSE - in particular, the
validation task (with 5 categories) was framed dif-
ferently from the benchmark (which is binary).
Bias check While measures were taken to con-
trol for distributional bias between captions andfoils in the different pieces of V ALSE (cf. §4.1), it
is possible that sub-sampling after manual valida-
tion could reintroduce such biases. To check that
this is not the case, we compare the word frequency
distributions between captions and foils in the orig-
inal pieces, and the word frequency distribution of
the manually validated set. We report the Jensen-
Shannon divergence and the number of words that
differ between caption and foil in Table 5. The
foil-caption word frequency distributions can be
inspected in Figures 3 and 4. The Jensen-Shannon
(JS) divergence is deﬁned as:
JS(fkc) =r
KL(fkm) +KL(ckm)
2
wherefis the normalized word frequency for foils,
cthe normalized word frequency for captions, m
is the point-wise mean of fandc, andKLis the
Kullback-Leibler divergence.
As Table 5 shows, the JS-divergence between
caption and foil distributions remains the same, or
changes only marginally (compare columns JS-div
andJs-div valid , where #Lexical Items indicates the
number of lexical/phrasal categories in the relevant
distributions). This indicates that no signiﬁcant
bias was introduced as a result of subsampling after
manual validation.827282738274827582768277827882798280