
Qi Jia, Yizhu Liu, Haifeng Tang, Kenny Q. ZhuShanghai Jiao Tong University, Shanghai, ChinaMeituan, Shanghai, ChinaChina Merchants Bank Credit Card Center, Shanghai, ChinaJia_qi@sjtu.edu.cn ,liuyizhu@meituan.comthfeng@cmbchina.com ,kzhu@cs.sjtu.edu.cn
Abstract
Curriculum learning has shown promising im-
provements in multiple domains by training
machine learning models from easy samples to
hard ones. Previous works which either design
rules or train models for scoring the difﬁculty
highly rely on task-speciﬁc expertise, and can-
not generalize. Inspired by the “easy-to-hard”
intuition, we propose to do in-sample curricu-
lum learning for natural language generation
tasks. Our learning strategy starts training the
model to generate the last few words, i.e., do
sequence completion, and gradually extends to
generate the whole output sequence. Compre-
hensive experiments show that it generalizes
well to different tasks and achieves signiﬁcant
improvements over strong baselines.
1 Introduction
Curriculum learning (CL) proposed by Bengio et al.
(2009) provides performance improvements on a
number of machine learning tasks. It mimics the
learning process of humans by training models with
samples in a more meaningful order, i.e., from the
easy ones to the hard ones. Therefore, ranking
training samples by difﬁculty lies in the core of CL,
which is also the key challenge when it’s applied
to natural language generation (NLG) tasks.
Previous work on CL for NLG focuses on mea-
suring the difﬁculty of training samples in two
ways. One is to resort to human-crafted rules
based on various linguistic features and human
observations (Liu et al., 2018; Kocmi and Bo-
jar, 2017). The other uses models either trained
from outside data or the same data but in previous
epochs/steps (Zhou et al., 2020; Kumar et al., 2019;
Shen and Feng, 2020). Either way seeks to produce
a numeric score for each training sample relying
on domain expertise so that it can be ranked, mak-
ing it difﬁcult to generalize to different tasks. Forexample, summarization focuses more on generat-
ing concise outputs while style transfer emphasizes
style changes. So the former should pay attention
to the ratio between the lengths of the output and
the input (the more compressed the more difﬁcult),
while the latter should focus on differences in style
between the input and output (the more different
the more difﬁcult). Designing a comprehensive
or universal scoring function is difﬁcult or even
impossible under this deﬁnition of CL.
In this paper, we propose an alternative to
sample-wise CL, which we call in-sample CL
(ICL). ICL re-orders the learning sequence within
the sample. One particular ICL re-ordering strat-
egy which we ﬁnd effective is to predict the last
few tokens given a long preﬁx ﬁrst from the origi-
nal output, and then gradually increase the number
of tokens at the end while shortening the preﬁx,
to create an easy-to-hard training order. Such a
curriculum learning strategy focuses more on the
difﬁculty of language generation itself, leading to
a better generalization ability among tasks.
Actually, we are not the ﬁrst to propose the idea
of ICL. Liang et al. (2021) introduced the notion
of “token-wise curriculum learning(TCL)”. Illus-
trations of TCL, ICL and the traditional CL are
shown in Figure 1. Their work considers gener-
ating the ﬁrst few tokens in the output sequence
to be easier than generating a longer sequence in
the output. Based on this idea, they proposed a
“hard” version of TCL that creates training samples
of increasing output length by cutting the sentences
short. In this way, TCL is similar to data augmenta-
tion with incomplete and even “incorrect” samples,
while our ICL considers each training sample in
full length. A “soft” version of TCL that places
decaying weights on the end tokens instead of cut-
ting short is introduced as a mitigation to avoid in-
complete samples, which was proved to uniformly
outperform the “hard” version.
To validate the advantage of ICL, we conduct ex-11937
tensive experiments on a range of natural language
generation tasks, including reading comprehension,
dialogue summarization, style transfer, question
generation and news summarization, with different
backbone models, such as BART, UniLM and GPT-
2. The results show the favorable performance of
ICL over the strong baselines.
In a word, our contributions are:
•We propose an improved in-sample curricu-
lum learning strategy for text generation by
doing sequence completion (Section 2.1).
•We propose a novel ICL learning algorithm
(Section 2.2). Together with our sequence
completion ICL curriculum, it achieves signif-
icant improvements over the strong baselines
on different NLG tasks, demonstrating strong
generalization ability (Section 3).
•Our approach can be combined with tradi-
tional CL for further performance gains (Sec-
tion 4.3).
2 Approach
We present an ICL strategy in the context of the
vanilla sequence-to-sequence (Seq2Seq) training
objective with a detailed learning algorithm.
2.1 ICL by Sequence Completion
Today, NLG tasks are generally solved by Seq2Seq
models, especially the pre-trained language mod-
els. Vanilla Seq2Seq models are trained to predict
the outputY={y,...,y}given the input Xby
minimizing the negative log-likelihood:
L=−1
n/summationdisplaylogP(y|y,X) (1)
Traditional CL manipulates the selection of training
pair(X,Y )from easier pairs to harder ones for
different tasks with this vanilla loss function.In contrast, ICL digs into the output sequence
itself and exploits the difﬁculty of language gener-
ation within each training sample. We segment Y
into two sub-sequences by a cutting point c, where
1≤c≤n. The sub-sequence before cis called
thepreﬁx , and the one after (and including) cis
thetarget . According to the Shannon Information
Theory, the entropy goes down when more related
information is given. Thus, the difﬁculty of the
sequence completion task that generates the target
will decrease when a longer preﬁx is given. In other
words, we can manipulate cto vary the difﬁculty
of samples during training.
Based on this intuition, we modify the vanilla
loss as:
L=−1
n−c+ 1/summationdisplaylogP(y|y,X)(2)
i.e., givenXand the preﬁx as inputs to the en-
coder and decoder respectively, we only calculate
the loss for predicting the target. At the beginning
of the training process, we use a larger cto train the
model to predict the target with only the last few
words. Then, we gradually decrease c, until the
preﬁx reduces to an empty sequence. In this way,
the model grows stronger with more difﬁcult gen-
eration objectives and learns to generate the whole
output in the end. An illustration is in Figure 2.119382.2 ICL Algorithm
Since the output length varies from sample to sam-
ple, it’s hard to set cas a constant for all samples.
If so, samples with short outputs will be neglected
whencis large at the beginning, and the model
will eventually bias to training samples with long
outputs as they are shown more times. In light of
this, we proposed to determine csample by sample
relative to their output lengths.
We deﬁne a start point p and a stride sfor
controllingc, where 0≤p,s≤1. The train-
ing process starts with:
c=n×p (3)
After each epoch or a number of updating steps,
we validate the model on the validation set. If
the performance on the validation set no longer
increases, we introduce a more difﬁcult generation
task by removing sfromp:
p=/braceleftBigg
p−s,ifp>s
0, else
and updatecby Equation 3. The training process
terminates until there are no improvements on the
validation set with cequaling 0. More details are
included in Algorithm 1.
Algorithm 1 The ICL training algorithm.3 Experiment
In this section, we ﬁrst present the experimental
setups for different tasks. Then, we show the quan-
titative and qualitative results together with com-
prehensive analysis and case studies.
3.1 Experimental Setups
We did experiments on ﬁve commonly-researched
natural language generation tasks as follows:
Reading comprehension is the task that answer-
ing questions about a piece of text. We use the
DREAM dataset (Sun et al., 2019) where questions
are about corresponding dialogues and the answer
is a complete sentence in natural language. We
neglect the negative choices in the original dataset
and formulate it as a NLG task. We adopt the
pre-trained language model BART(Lewis et al.,
2020) as the baseline. The generated answers are
evaluated by BLEU scores (Papineni et al., 2002)
widely used for QA systems, together with Meteor
and Rouge-L F1 (Fabbri et al., 2021). We eval-
uate the model after each training epoch and the
early-stop patience will be added 1 if there is no
improvement in the perplexity on the validation set.
The training process terminates when the early-stop
patience equals or is larger than 3. During the in-
ference, the minimum and maximum output length
are set to 5 and 100, with no_repeat_ngram_size=3,
length_penalty=1.0 and num_beams=4.
Dialogue summarization is to generate a con-
cise summary covering the salient information in
the input dialogue. The preceding model BART
has shown to be a strong baseline for this task.We
experiment with SAMSum dataset (Gliwa et al.,
2019) for daily-chat dialogues. The generated sum-
maries are evaluated by comparing with the refer-
ence through evaluation metrics, including Rouge-
1/2/L F1 scores (Lin, 2004), Meteor (Banerjee and
Lavie, 2005) and BertScore F1. The parameters are
the same as reading comprehension, except that the
early-stop is activated if there is no improvement
according to the Rouge-2 F1 score.
Style transfer preserves the semantic meaning
of a given sentence while modifying its style, such
as positive to negative, formal to informal, etc. We
adopt the Shakespeare author imitation dataset (Xu
et al., 2012), containing William Shakespeare’s
original plays and corresponding modernized ver-
sions. Krishna et al. (2020) proposed to do unsuper-
vised style transfer by training paraphrase models11939
based on the GPT-2 language model (Radford et al.,
2019). We re-implemented their approach STRAP.
Evaluation metrics include transfer accuracy(ACC),
semantic similarity(SIM), Fluency(FL) and two ag-
gregation metrics, i.e., geometric averaging(GM)
and their proposed J(·)metric. In the training
stage, we evaluate the model after updating every
500 steps. The perplexity on the validation set is
used to activate the early-stop which equals 3. The
inference is done as default.
Question generation (Zhou et al., 2017) aims at
generating a question given an input document and
its corresponding answer span. SQuAD 1.1 (Ra-
jpurkar et al., 2016) is generally used for evalua-
tion. We adopt the data split as in (Du et al., 2017)
and ﬁne-tune the pre-trained UniLM (Dong et al.,
2019) as the strong baseline. Generated questions
are evaluated by metrics including BLEU-1/2/3/4,
Meteor and Rouge-L with the provided scripts. The
model is evaluated every 1000 steps and the early-
stop equaling 5 is associated with the perplexity on
the validation set. Other parameters are unchanged
following the ofﬁcial guideline.
News summarization differs from dialogue
summarization where the input is a document in-
stead of a dialogue. We adopt the same strong
baseline BART and evaluation metrics as dialogue
summarization. Experiments are done with CN-
NDM dataset (Hermann et al., 2015) consisting of
news articles and multi-sentence summaries. The
model is evaluated every 3000 steps and the early-
stop equaling 3 is associated with the Rouge-2 on
the validation set. During the inference, the mini-
mum and maximum output length is set to 45 and
140 respectively, with no_repeat_ngram_size=3,
length_penalty=2.0 and num_beams=4.
A summary of these tasks is in Table 1 andthe speciﬁc packages we adopted are in the Ap-
pendix. For fair comparisons, we re-implement
these baselines on our machine. Then, we fur-
ther arm them with different in-sample curriculum
settings without changing corresponding hyper-
parameters. Speciﬁcally, we distinguish Liang et al.
(2021)’s work and our method in detail from two
aspects, including the curriculum criterion denoted
by SG or SC and the training algorithm denoted
by TCL or ICL, which results in the following 4
combinations:
•TCL-SG : the token-wise curriculum learning
algorithm(TCL) with sub-sequence genera-
tion(SG) criterion proposed by Liang et al.
(2021) with their best soft setting. The hyper-
parameters are set as γ= 0.7andα= 25
following the original paper.
•TCL-SC : we modiﬁed the TCL-SG by incor-
porating our sequence completion(SC) crite-
rion in Section 2 with the hard settingwhere
λ= 0.1following the original paper.
•ICL-SG : we implemented the SG criterion by
using our ICL algorithm in Section 2 which
calculating the loss with 1≤t≤cin (2).
•ICL-SC : our ﬁnal approach. Both TCL-SC
and ICL-SG are ablations for it. The settings
of newly introduced p andsare speciﬁed
and discussed in Section 4.2.
All of the approaches are trained with the same
max training epochs with the early-stop for prevent-
ing from over-ﬁtting. The experiments are done on
a single RTX 3090 with 24G GPU memory. The re-
sults are averaged over three runs. We open-source
all of codes and results at https://github.com/
JiaQiSJTU/InsampleCurriculumLearning .11940
3.2 Automatic Evaluations on Different Tasks
The performances on different NLG tasks are
shown in Table 2. These tasks not only focus on
solving different problems, but also has a various
amount of training data as well as output lengths
according to Table 1. Besides, the basic models are
also different, including BART, GPT-2 and UniLM.
Our approach ICL-SC achieves signiﬁcant im-
provements over the strong baselines among dif-
ferent tasks on most evaluation metrics, which
shows that our method not only works well, but
also has strong generalization abilities. It should be
noted that GM and J are two comprehensive evalu-
ation metrics for style transfer, with our approach
topping the ranks with signiﬁcant improvements.
To disentangle factors of learning curriculum
and training algorithms, we conduct variations of
ICL-SC for detailed comparisons to TCL-SG. Moreobservations are as follows.
∗-SC outperforms∗-SG for both training algo-
rithms, showing that our proposed sequence com-
pletion curriculum is a more effective way of do-
ing curriculum learning within a single sample.
The only exception is that ICL-SG performs bet-
ter than ICL-SC for news summarization in Table
2e. The reason is that multi-sentence summaries
in CNNDM are more extractive and cover differ-
ent salient information in each sentence. Human
agreement on salient information is relatively low
as shown in Table 3. Consequently, the preﬁx of a
summary can also be a reasonable and more con-
cise reference summary with one or more complete
sentences. The nature of ∗-SG happens to take
advantage of this property.
ICL-∗is better than TCL- ∗with better per-
formance and less computational costs. For TCL
training algorithm adopted in Liang et al. (2021), it
separates the whole training process into curricu-
lum and ordinary training. The curriculum length is
an important hyper-parameter that is required to be
estimated by ﬁnishing the training of the baseline
model and computing the number of steps it takes
to reach approximately 70% of ﬁnal scores. It inten-
sively aggravates the computational costs. Besides,
this estimation rule can not generalize well to dif-
ferent tasks (More in Appendix). We choose to set
curriculum steps to 2 or 3 epochs, approximately
to the same amount of samples with different dif-
ﬁculty levels in ICL-SC. Taking dialogue summa-
rization as an example, TCL-SG takes around 15.67
epochs (6 for the curriculum step estimation, 3 for
curriculum and 6.67 for ordinary training) while
our ICL-SC takes only 11.67 epochs to get the ﬁnal
results (More in Appendix). In a word, our ICL- ∗
do the curriculum and ordinary training in a uni-
ﬁed manner, requiring less computational costs in
total. Moreover, ICL- ∗moves to the next difﬁculty
level after the model has fully been trained on that
judging by the performance on the validation set,
which is more similar to the education process in
real life and leads to better results.
3.3 Human Evaluations
To further prove the improvement of our approach,
we asked three proﬁcient English speakers from
Asia for human evaluation. 100 samples from the
test set of each task are randomly selected, ignoring
the ones with totally same generations among three
models, including the vanilla model, TCL-SG and11941ICL-SC. The original input, reference output and
three generations are shown to annotators together,
while the order of the three generations is unknown
and different among samples. 3-point Likert Scale
is adopted for scoring each generation (Gliwa et al.,
2019), where [5, 3, 1] represent excellent, moderate
and disappointing results respectively. The average
scores and annotator agreements are in Table 3.
The Fleiss Kappa on the ﬁrst four tasks indicates
moderate agreements. It shows the promising im-
provement of ICL-SC over the vanilla model and
TCL-SG, which is consistent with the conclusion
based on automatic metrics. The poor agreement
on news summarization reﬂects the diverse con-
cerns of summarization from different annotators.
The drop of TCL-SG over the baseline on style
transfer is apparent. Although TCL-SG achieves
signiﬁcant improvements in accuracy, the gener-
ated contents with less semantic similarities and
poor ﬂuency are not preferred by annotators. Ex-
amples will be discussed in Section 3.4.
3.4 Case Studies
We show some cases in Table 4.
In the ﬁrst case from reading comprehension,
our ICL-SC reasoned correctly while the base-
line model raised a wrong answer. TCL-SG also
answered incorrectly by merging both keywords.
Such ability is not suitable for generating a precise
answer. In contrast, ICL-SC successfully incorpo-
rated more salient information in a single sentence
for dialogue summarization, which performs better
than both baselines. The vanilla model did poorly
on coreference resolution among dialogue utter-
ances and generated “this” without a clear referent.
ICL-SC also generated a more accurate question in
Table 4d compared with strong baselines, although
it’s not the same as the reference.
For transferring style from modern to Shake-
speare’s style, the model generated results are all
acceptable while ICL-SC performs slightly better
for being more polite. Both TCL-SG and ICL-
SC even generated the more professional word
“prithee” which is widely used in Shakespeare’s11942time. A bad case is the second case of Table 4c.
ICL-SC didn’t make any improvements over the
baseline. TCL-SG even got out of control.
Generated summaries in Table 4e cover differ-
ent parts of information in the original document.
The vanilla output is just a reordering of the ﬁrst
three sentences. ICL-SC did better by omitting too
detailed content compared to the two baselines.
In a word, the results show that ICL-SC can
capture the characteristics of different tasks
and do better language modeling . Besides, by
comparing the improvements among these ﬁve
tasks with different output length, we conclude that
ourICL-SC is more competitive with tasks hav-
ing shorter outputs. Long outputs, such as sum-
maries in news summarization, bring additional
difﬁculties on the arrangement of multiple salient
contents and cross-sentence relations, which can’t
be well solved with such a simple in-sample cur-
riculum and will be considered in the future.
4 Analysis
For a better understanding of ICL-SC, we did com-
prehensive ablation studies and combined it with
the traditional CL. The experiments in this section
are done on dialogue summarization, which is rep-
resentative due to the medium output length.
4.1 Ablations on the Training Strategy
To examine the design of decreasing the preﬁx for
ICL-SC, we introduce the alternatives as follows:
•Decrease refers to the Algorithm 1. Taking
p = 0.6ands= 0.3as an example, the
preﬁx percentage pvaries as 0.6→0.3→
0.0during training.
•Increase means that we gradually increase
the length of preﬁx by increase pfollowing
0.0→0.3→0.6.
•Random is that we randomly pick pfrom the
set{0.0,0.3,0.6}in this example.
The results are shown in Table 5, with Decrease
ranking ﬁrst and Increase ranking the worst. De-
crease signiﬁcantly outperforms other ablations,showing that our sequence completion criterion of
shrinking the preﬁx does work by means of learn-
ing from easy to hard.
4.2 Parameter Search of the Starting Point
and the Stride
To better understand how the ICL-SC manipulates
the difﬁculty of samples during the training process,
we further did experiments on different settings of
two newly-introduced hyper-parameters p and
s. The results are in Figure 3.
We can see that the performance drops with ei-
ther a too large or too small p. The former one
starts training with only predicting the last 1 or 2
tokens according to the average length of reference
output shown in Table 1. Most of the time, they are
punctuation marks that do not carry any important
semantic information, leading to a bad warm-up.
The latter one requires the model to predict more
than half of the output, which are too difﬁcult as a
beginning learning target. Besides, a larger p
which is divisible by sis more competitive.
The trend is the same for using different stride
values. The performance drops with sequaling 0.1
or 0.6. The smaller ones lead to too tiny changes,
which not only excessively prolongs the required
training time but also leads to server outﬁtting on
the training set. The larger ones greatly enlarge the
gap between training targets which degrades to 0.0
directly. It also harms the performances.
In a word, the training should start with a
medium difﬁculty training objective and the gap
between training objectives shouldn’t be too large.
Both parameters are closely related to the out-
put length of different tasks. We suggest using
(p= 0.6,s= 0.3) for NLG tasks with multi-
sentence outputs, and ( p = 0.5,s= 0.5) for
NLG tasks with single-sentence outputs. All of our
experiments are done based on this guideline.119434.3 Combinations with the Traditional CL
Since our ICL-SC is orthogonal to sample-wise CL
and designing an appropriate sample-wise curricu-
lum is not easy, we choose dialogue summarization
as a representative task, design several traditional
CL strategies empirically, and further apply our
ICL-SC on top of them for comparisons. 4differ-
ent traditional CL strategies are as follows:
•Input length (InLen) refers to the number of
tokens in the input dialogue. The longer a
dialogue is, the more complex a sample is.
•Output length (OutLen) is the number of
tokens in a reference summary, which is also
proportional to the difﬁculty of a sample.
•Compression ratio (CompR) equals the out-
put length divided by the input length. More
compressed training pairs are harder.
•Abstractiveness (Abstr) represents the per-
centage of novel words in the reference sum-
mary which are not in the dialogue. We mea-
sure it by Rouge-2 recall, which is inversely
proportional to the difﬁculty level.
The results based on the ordered training sam-
ples according to these intuitive CL strategies are
shown in Table 6. It shows that only InLen im-
proves the vanilla model, but it still lags behind the
pure ICL-SC. Other strategies failed mainly due
to the low data quality at the beginning or the end
of training. Taking Abstr as an example, samples
with the highest Rouge-2 recall are gathered at the
beginning where their inputs and outputs are al-
most the same. This leads to a bad initialization for
models learning the summarization ability.
Besides, some strategies are incompatible, such
as OutLen and CompR. Samples with the shortest
output length are always too compressed. There-
fore, developing a comprehensive score for a betterranking is difﬁcult. It should be also noticed that
most of these strategies are designed for summa-
rization, which are not suitable for generalization.
In a word, it’s hard to develop a comprehensive
strategy for one task or a uniﬁed strategy for dif-
ferent NLG tasks with traditional CL. ICL-SC not
only outperforms these CL strategies, but also im-
proves them when easily combined.
5 Related Work
Natural language generation has received great
attention with deep neural networks, especially
pre-trained language models. It refers to the task
where expected outputs for different purposes are
in natural language (Dong et al., 2022). The inher-
ent characteristic of having more than one correct
output given the same input is the core challenge
of solving this kind of task, especially for evalua-
tion (Singh et al., 2018).
Curriculum learning (Bengio et al., 2009)
boost models’ performances in a range of machine
learning areas (Liu et al., 2021; Varshney et al.,
2022) by reordering the training samples. It meets
great obstacles when applying to NLG tasks as it’s
hard to evaluate the difﬁculties of training sam-
ples. Different rules are developed for different
tasks (Platanios et al., 2019; Chang et al., 2021).
For example, (Liu et al., 2018) measures the com-
plexity of question-answering pairs from the view
of frequency and grammar simply for answers.
(Kocmi and Bojar, 2017) focuses more on POS
features and the length of translation pairs. Other
works utilize additional models or targeting models
in the previous training step (Zhang et al., 2018).
Shen and Feng (2020) reorder samples by the ac-
curacy from an independent emotion classiﬁer for
response generation. However, such salient fea-
tures do not always exist or can be well classiﬁed.
There is also work (Zhou et al., 2020) using either
the reference perplexity or generations evaluated by
corresponding metrics for ranking during training,
while these scores are not ideal due to the one-to-
many characteristic of NLG. Thus, designing a CL
strategy generalizing well for NLG is difﬁcult.
Instead of ﬁguring out the oracle scoring func-
tion for training samples, we propose to measure
the language generation difﬁculty within a sample.
Liang et al. (2021) did something similar though
their approach amounts to data augmentation by
doing sub-sequence generation, which is not ex-
actly curriculum learning. We, on the other hand,11944train on the original sample with a decreasing preﬁx
length and thus learn from easy to hard.
6 Conclusion
This paper deﬁnes a kind of curriculum learning
strategy for NLG tasks called in-sample curriculum
learning (ICL) by manipulating the difﬁculty of
training within a training sample instead of ranking
among samples. We propose the ICL algorithm
with the sequence completion curriculum which
boosts the performance of strong baselines on a
wide range of tasks, showing the effectiveness and
strong generalization ability of our approach. More
training strategies under ICL digging the inherent
difﬁculties of generating a language sequence are
expected in the future.
Limitations
One limitation of our approach is that in-sample
curriculum learning methods (both TCL-SG and
ICL-SC) always incur extra overhead during train-
ing compared with the vanilla model shown in Ta-
ble 7. Nevertheless, the inference time of different
approaches is the same as the vanilla model. In a
word, it’s worthwhile because (1) ICL-SC can per-
form signiﬁcantly better than both baselines with-
out additional computational requirements during
inference in real applications; (2) ICL-SC doesn’t
rely on task-speciﬁc expertise and has strong gen-
eralization ability.
Due to the limited computational resources, we
were unable to do experiments on machine trans-
lation. According to the implementation details
in Liang et al. (2021), all of their machine transla-
tion experiments were done on 32G NVIDIA V100
GPUs which are much more powerful than a single
RTX 3090. Even for the low resource setting with
around 133K to 612K training samples, they used
dynamic batching with 4096 maximum tokens and
trained for 60 epochs. This will either lead to an
out-of-memory error or take us several weeks or
even months to get the results of a single run on ourmachine. Instead, we tried our best to cover a range
of representative natural language generation tasks
and corresponding datasets with different charac-
teristics, such as sizes and output lengths (Table
1).
Acknowledgments
This work was generously supported by the CMB
Credit Card Center & SJTU joint research grant,
and Meituan-SJTU joint research grant.
References1194511946
A Packages used for Baselines
The packages we adopted to re-implement the base-
line are listed as follows:
Reading Comprehension
•Dataset: https://github.com/nlpdata/
dream/tree/master/data
•Baseline Code: https://github.com/
huggingface/transformers
•Evaluation Metric: https://github.
com/tensorflow/nmt/blob/master/nmt/
scripts/bleu.py
Dialogue Summarization
•Dataset: https://arxiv.org/src/1911.
12237v2/anc/corpus.7z
•Baseline Code: https://github.com/
huggingface/transformers
•Evaluation Metric: https://github.
com/pltrdy/files2rouge ; https:
//github.com/Yale-LILY/SummEval
Style Transfer
•Dataset: https://github.
com/martiansideofthemoon/
style-transfer-paraphrase
•Baseline Code: https://github.
com/martiansideofthemoon/
style-transfer-paraphrase
•Evaluation Metric: https://
github.com/martiansideofthemoon/
style-transfer-paraphrase
Question Generation
•Dataset: https://github.com/microsoft/
unilm/tree/master/unilm-v1•Baseline Code: https://github.
com/microsoft/unilm/tree/master/
unilm-v1
•Evaluation Metric: https://github.
com/microsoft/unilm/tree/master/
unilm-v1
News Summarization
•Dataset: https://drive.google.com/
file/d/0BzQ6rtO2VN95a0c3TlZCWkl3aU0/
view?resourcekey=
0-toctC3TNM1vffPCZ7XT0JA
•Baseline Code: https://github.com/
huggingface/transformers
•Evaluation Metric: https://github.
com/pltrdy/files2rouge ; https:
//github.com/Yale-LILY/SummEval
B Preliminary Studies on TCL
Preliminary studies on dialogue summarization for
TCL under different settings are shown in Table 8.
We can see that the “soft” setting does help the TCL
with sub-sequence generation curricula, which is
consistent with the results in Liang et al. (2021).
Results are opposite for TCL with our proposed
sequence completion curricula. The “soft” setting
considering the loss from preﬁx tokens actually
hurts the intuition that “the shorter the target is, the
easier the tasks is”. As a result, SC-hard performs
better than SC-soft.
Experiments on the sensitivity of curriculum step
in TCL-SG (Liang et al., 2021) are in Table 9. It
consistently has improvements on dialogue sum-
marization compared with the baseline. However,
the performances also vary a lot with different
curriculum steps, especially on R1, Meteor and
BertScore. The estimation rule proposed in Liang
et al. (2021) of computing the number of steps it
takes to reach approximately 70% of ﬁnal scores
doesn’t perform well for dialogue summarization.
So, we choose to set curriculum steps to 3 epochs11947for dialogue summarization and news summariza-
tion, and 2 epochs for reading comprehension and
style transfer, which not only achieve better results,
but also are fairer for comparisons. For news sum-
marization, we still adopted their estimation rule
and trained with 5200 curriculum steps.11948ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
It is the section after the conclusion and before the references.
/squareA2. Did you discuss any potential risks of your work?
Not applicable. We propose a method for better natural language generation.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3.1
/squareB1. Did you cite the creators of artifacts you used?
Section 3.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
All of the datasets are publicly available and the source link for downloading them are in the
Appendix A. We will only release the codes and results for our work (Section 3.1).
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 3.1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. We adopted the widely-used publicly available datasets.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. We are not a dataset paper. We provided necessary information about the datasets in
Section 3.1. More information please refer to their original dataset paper.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 3.1, Table 1
C/squareDid you run computational experiments?
Section 3.2
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 3.1 and Limitations11949/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3.1 and Section 4.2
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3.1
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 3.1 and Appendix A
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3.3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Section 3.3
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. We had student volunteers to do the human evaluation.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. The volunteers knew how the data would be used before doing the human evaluation.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. We did not collect new datasets, only a simple human evaluation.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Section 3.3 for human evaluation.11950