
Ikuya YamadaKoki WashioHiroyuki ShindoYuji MatsumotoStudio OusiaRIKENMegagon LabsNAIST
ikuya@ousia.jp kwashio@megagon.ai shindo@is.naist.jp
yuji.matsumoto@riken.jp
Abstract
We propose a global entity disambiguation
(ED) model based on BERT (Devlin et al.,
2019). To capture global contextual informa-
tion for ED, our model treats not only words
but also entities as input tokens, and solves
the task by sequentially resolving mentions
to their referent entities and using resolved
entities as inputs at each step. We train the
model using a large entity-annotated corpus
obtained from Wikipedia. We achieve new
state-of-the-art results on five standard ED
datasets: AIDA-CoNLL, MSNBC, AQUAINT,
ACE2004, and WNED-WIKI. The source code
and model checkpoint are available at https:
//github.com/studio-ousia/luke .
1 Introduction
Entity disambiguation (ED) refers to the task of
assigning mentions in a document to correspond-
ing entities in a knowledge base (KB). This task is
challenging because of the ambiguity between men-
tions (e.g., “World Cup”) and the entities they refer
to (e.g., FIFA World Cup or Rugby World Cup).
ED models typically rely on local contextual infor-
mation based on words that co-occur with the men-
tion and global contextual information based on
the entity-based coherence of the disambiguation
decisions. A key to improve the performance of ED
is to effectively combine both local and global con-
textual information (Ganea and Hofmann, 2017;
Le and Titov, 2018).
In this study, we propose a global ED model
based on BERT (Devlin et al., 2019). Our model
treats words and entities in the document as in-
put tokens, and is trained by predicting randomly
masked entities in a large entity-annotated corpus
obtained from Wikipedia. This training enables
the model to learn how to disambiguate masked
entities based on words and non-masked entities.
At the inference time, our model disambiguatesFigure 1: The inference procedure of our model with
the input text “Messi played in the World Cup.” Given
mentions (“Messi” and “World Cup”), our model se-
quentially resolves them to their referent entities, and
uses the resolved entities as contexts at each step.
mentions sequentially using words and already re-
solved entities (see Figure 1). This sequential infer-
ence effectively accumulates the global contextual
information and enhances the coherence of disam-
biguation decisions (Yang et al., 2019).
We conducted extensive experiments using
six standard ED datasets, i.e., AIDA-CoNLL,
MSNBC, AQUAINT, ACE2004, WNED-WIKI,
and WNED-CWEB. As a result, the global con-
textual information consistently improved the per-
formance. Furthermore, we achieved new state
of the art on all datasets except for WNED-
CWEB. The source code and model check-
point are available at https://github.com/
studio-ousia/luke .
2 Related Work
Transformer-based ED. Several recent stud-
ies have proposed ED models based on Trans-
former (Vaswani et al., 2017) trained with a large
entity-annotated corpus obtained from Wikipedia
(Broscheit, 2019; Ling et al., 2020; Févry et al.,
2020; Cao et al., 2021; Barba et al., 2022).
Broscheit (2019) trained an ED model based on
BERT by classifying each word in the document
to the corresponding entity. Similarly, Févry et al.
(2020) addressed ED using BERT by classifying
mention spans to the corresponding entities. Ling3264
et al. (2020) trained BERT by predicting entities
using the document-level representation. Cao et al.
(2021) addressed ED by training BART (Lewis
et al., 2020) to generate referent entity titles of tar-
get mentions in an autoregressive manner. Barba
et al. (2022) formulated ED as a text extraction
problem; they fed the document and candidate en-
tity titles to BART and Longformer (Beltagy et al.,
2020) and disambiguated a mention in the docu-
ment by extracting the referent entity title of the
mention. However, unlike our model, these models
addressed the task based only on local contextual
information.
Treating entities as inputs of Transformer. Re-
cent studies (Zhang et al., 2019; Yamada et al.,
2020; Sun et al., 2020) have proposed Transformer-
based models that treat entities as input tokens to
enrich their expressiveness using additional infor-
mation contained in the entity embeddings. How-
ever, these models were designed to solve general
NLP tasks and not tested on ED. We treat entities
as input tokens to capture the global context that is
shown to be highly effective for ED.
ED as sequential decision task. Past studies
(Yang et al., 2019; Fang et al., 2019) have solved
ED by casting it as a sequential decision task to
capture global contextual information. We adopt a
similar method with an enhanced Transformer ar-
chitecture, a training task, and an inference method
to implement the global ED model based on BERT.
3 Model
Given a document with Nmentions, each of which
hasKentity candidates, our model solves ED by
selecting a correct referent entity from the entity
candidates for each mention.3.1 Model Architecture
Our model is based on BERT and takes words and
entities (Wikipedia entities or the [MASK] entity).
The input representation of a word or an entity is
constructed by summing the token, token type, and
position embeddings (see Figure 2):
Token embedding is the embedding of the cor-
responding token. The matrices of the word and
entity token embeddings are represented as A∈
RandB∈R, respectively, where His
the size of the hidden states of BERT, and Vand
Vare the number of items in the word vocabulary
and that of the entity vocabulary, respectively.
Token type embedding represents the type of to-
ken, namely word ( C) or entity ( C ).
Position embedding represents the position of the
token in a word sequence. A word and an entity
appearing at the i-th position in the sequence are
represented as DandE, respectively. If an entity
mention contains multiple words, its position em-
bedding is computed by averaging the embeddings
of the corresponding positions (see Figure 2).
Following Devlin et al. (2019), we tokenize the
document text using the BERT’s wordpiece tok-
enizer, and insert [CLS] and[SEP] tokens as the
first and last words, respectively.
3.2 Training Task
Similar to the masked language model (MLM) ob-
jective adopted in BERT, our model is trained by
predicting randomly masked entities. Specifically,
we randomly replace some percentage of the enti-
ties with special [MASK] entity tokens and then
trains the model to predict masked entities.
We adopt a model equivalent to the one used to
predict words in MLM. Formally, we predict the
original entity corresponding to a masked entity by3265
applying softmax over all entities:
ˆ y=softmax (Bm+b) (1)
m=layernorm/parenleftbig
gelu(Wh+b)/parenrightbig
(2)
where h∈Ris the output embedding corre-
sponding to the masked entity, W∈Ris a
matrix, b∈Randb∈Rare bias vectors,
gelu(·)is the gelu activation function (Hendrycks
and Gimpel, 2016), and layernorm (·)is the layer
normalization function (Lei Ba et al., 2016).
3.3 ED Model
Local ED Model. Our local ED model takes
words and N[MASK] tokens corresponding to the
mentions in the document. The model then com-
putes the embedding m∈Rfor each [MASK]
token using Eq. (2)and predicts the entity using
softmax over the Kentity candidates:
ˆ y=softmax (Bm+b), (3)
where B∈Randb∈Rconsist of the en-
tity token embeddings and the bias corresponding
to the entity candidates, respectively. Note that B
andbare the subsets of Bandb, respectively.
Global ED Model. Our global ED model re-
solves mentions sequentially for Nsteps (see Al-
gorithm 1). First, the model initializes the entity of
each mention using the [MASK] token. Then, for
each step, it predicts an entity for each [MASK] to-
ken, selects the prediction with the highest probabil-
ity produced by the softmax function in Eq. (3), and
resolves the corresponding mention by assigning
the predicted entity to it. This model is denoted as
confidence-order . We also test a model that selects
mentions according to their order of appearance in
the document and denote it by natural-order .
3.4 Modeling Details
Our model is based on BERT (Devlin et al.,
2019). The parameters shared with BERT are ini-
tialized using BERT, and the other parameters are
initialized randomly. We treat the hyperlinks in
Wikipedia as entity annotations and randomly mask
30% of all entities. We train the model by maximiz-
ing the log likelihood of entity predictions. Further
details are described in Appendix A.
4 Experiments
Our experimental setup follows Le and Titov
(2018). In particular, we test the proposed
ED models using six standard datasets: AIDA-
CoNLL (CoNLL) (Hoffart et al., 2011), MSNBC,
AQUAINT, ACE2004, WNED-CWEB (CWEB),
and WNED-WIKI (WIKI) (Guo and Barbosa,
2018). We consider only the mentions that re-
fer to valid entities in Wikipedia. For all datasets,
we use the KB+YAGO entity candidates and their
associated ˆp(e|m)(Ganea and Hofmann, 2017),
and use the top 30 candidates based on ˆp(e|m).
For the CoNLL dataset, we also test the perfor-
mance using PPRforNED entity candidates (Per-
shina et al., 2015). We report the in-KB accuracy
for the CoNLL dataset and the micro F1 score (av-
eraged per mention) for the other datasets. Further
details of the datasets are provided in Appendix C.
Furthermore, we optionally fine-tune the model
by maximizing the log likelihood of the ED pre-
dictions ( ˆ y) using the training set of the CoNLL
dataset with the KB+YAGO candidates. We mask
90% of the mentions and fix the entity token em-
beddings ( BandB) and the bias ( bandb).3266
The model is trained for two epochs using AdamW.
Additional details are provided in Appendix B.
4.1 Results
Table 1 and Table 2 present our experimental
results. We achieve new state of the art on
all datasets except the CWEB dataset by outper-
forming strong Transformer-based ED models,
i.e, Broscheit (2019), Ling et al. (2020), Févry
et al. (2020), Cao et al. (2021), and Barba et al.
(2022).Furthermore, on the CoNLL dataset,
our confidence-order model trained only on our
Wikipedia-based corpus outperforms Yamada et al.
(2016) and Ganea and Hofmann (2017) trained on
its in-domain training set.
Our global models consistently perform better
than the local model, demonstrating the effective-
ness of using global contextual information even
if local contextual information is captured using
expressive BERT model. Moreover, the confidence-
order model performs better than the natural-order
model on most datasets. An analysis investigating
why the confidence-order model outperforms thenatural-order model is provided in the next section.
The fine-tuning on the CoNLL dataset signifi-
cantly improves the performance on this dataset
(Table 1). However, it generally degrades the per-
formance on the other datasets (Table 2). This sug-
gests that Wikipedia entity annotations are more
suitable than the CoNLL dataset to train general-
purpose ED models.
Additionally, our models perform worse than
Yang et al. (2018) on the CWEB dataset. This is
because this dataset is significantly longer on aver-
age than other datasets, i.e., approximately 1,700
words per document on average, which is more
than three times longer than the 512-word limit
that can be handled by BERT-based models includ-
ing ours. Yang et al. (2018) achieved excellent
performance on this dataset because their model
uses various hand-engineered features capturing
document-level contextual information.
4.2 Analysis
To investigate how global contextual information
helps our model to improve performance, we manu-
ally analyze the difference between the predictions
of the local, natural-order, and confidence-order
models. We use the fine-tuned model using the
CoNLL dataset with the YAGO+KB candidates.
Although all models perform well on most men-
tions, the local model often fails to resolve men-
tions of common names referring to specific entities
(e.g., “New York” referring to New York Knicks).
Global models are generally better to resolve such
difficult cases because of the presence of strong
global contextual information (e.g., mentions refer-3267ring to basketball teams).
Furthermore, we find that the confidence-order
model works especially well for mentions that re-
quire a highly detailed context to resolve. For ex-
ample, a mention of “Matthew Burke” can refer
to two different former Australian rugby players.
Although the local and natural-order models incor-
rectly resolve this mention to the player who has
the larger number of occurrences in our Wikipedia-
based corpus, the confidence-order model success-
fully resolves this by disambiguating its contextual
mentions, including his teammates, in advance. We
provide detailed inference sequence of the corre-
sponding document in Appendix D.
4.3 Performance for Rare Entities
We examine whether our model learns effective em-
beddings for rare entities using the CoNLL dataset.
Following Ganea and Hofmann (2017), we use the
mentions of which entity candidates contain their
gold entities and measure the performance by di-
viding the mentions based on the frequency of their
entities in the Wikipedia annotations used to train
the embeddings.
As presented in Table 3, our models achieve en-
hanced performance for rare entities. Furthermore,
the global models consistently outperform the local
model both for rare and frequent entities.
5 Conclusion and Future Work
We propose a new global ED model based on BERT.
Our extensive experiments on a wide range of ED
datasets demonstrate its effectiveness.
One limitation of our model is that, similar to
existing ED models, our model cannot handle en-
tities that are not included in the vocabulary. In
our future work, we will investigate the method to
compute the embeddings of such entities using a
post-hoc training with an extended vocabulary (Tai
et al., 2020).
References32683269Appendix for “Global Entity
Disambiguation with BERT”
A Details of Proposed Model
As the input corpus for training our model, we use
the December 2018 version of Wikipedia, compris-
ing approximately 3.5 billion words and 11 million
entity annotations. We generate input sequences by
splitting the content of each page into sequences
comprising ≤512words and their entity annota-
tions (i.e., hyperlinks). The input text is tokenized
using BERT’s tokenizer with its vocabulary con-
sisting of V= 30,000words. Similar to Ganea
and Hofmann (2017), we create an entity vocabu-
lary consisting of V= 128 ,040entities, which are
contained in the entity candidates in the datasets
used in our experiments.
Our model consists of approximately 440 mil-
lion parameters. To reduce the training time, the
parameters that are shared with BERT are initial-
ized using BERT. The other parameters are initial-
ized randomly. The model is trained via iterations
over Wikipedia pages in a random order for seven
epochs. To stabilize the training, we update only
those parameters that are randomly initialized (i.e.,
fixed the parameters initialized using BERT) at
the first epoch, and update all parameters in the
remaining six epochs. We implement the model
using PyTorch (Paszke et al., 2019) and Hugging
Face Transformers (Wolf et al., 2020), and the train-
ing takes approximately ten days using eight Tesla
V100 GPUs. We optimize the model using AdamW.
The hyper-parameters used in the training are de-
tailed in Table 4.
B Details of Fine-tuning on CoNLL
Dataset
The hyper-parameters used in the fine-tuning on
the CoNLL dataset are detailed in Table 5. We se-
lect these hyper-parameters from the search space
described in Devlin et al. (2019) based on the accu-
racy on the development set of the CoNLL dataset.
A document is split if it is longer than 512 words,
which is the maximum word length of the BERT
model.
C Details of ED Datasets
The statistics of the ED datasets used in our experi-
ments are provided in Table 6.
D Example of Inference by
Confidence-order Model
Figure 3 shows an example of the inference per-
formed by our confidence-order model fine-tuned
on the CoNLL dataset. The document is obtained
from the test set of the CoNLL dataset. As shown
in the figure, the model starts with unambiguous
player names to recognize the topic of the docu-
ment, and subsequently resolves the mentions that
are challenging to resolve.
Notably, the model correctly resolves the men-
tion “Nigel Walker” to the corresponding former
rugby player instead of a football player, and the
mention “Matthew Burke” to the correct former
Australian rugby player born in 1973 instead of3270
the former Australian rugby player born in 1964.
This is accomplished by resolving other contextual
mentions, including their colleague players, in ad-
vance. These two mentions are denoted in red in
the figure. Note that our local model fails to resolve
both mentions, and our natural-order model fails to
resolve “Matthew Burke.”3271