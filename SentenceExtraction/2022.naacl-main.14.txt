
Zijie Zeng, Xinyu Li, Dragan Gaševi ´candGuanliang Chen
Centre for Learning Analytics, Monash University
Melbourne, Victoria, Australia
{Zijie.Zeng, Xinyu.Li, Dragan.Gasevic, Guanliang.Chen}@monash.edu
Abstract
Deep Learning (DL) techniques have been in-
creasingly adopted for Automatic Text Scoring
in education. However, these techniques often
suffer from their inabilities to explain and jus-
tify how a prediction is made, which, unavoid-
ably, decreases their trustworthiness and hin-
ders educators from embracing them in prac-
tice. This study aimed to investigate whether
(and to what extent) DL-based graders align
with human graders regarding the important
words they identify when marking short an-
swer questions. To this end, we ﬁrst conducted
a user study to ask human graders to manually
annotate important words in assessing answer
quality and then measured the overlap between
these human-annotated words and those identi-
ﬁed by DL-based graders (i.e., those receiving
large attention weights). Furthermore, we ran
a randomized controlled experiment to explore
the impact of highlighting important words de-
tected by DL-based graders on human grading.
The results showed that: (i) DL-based graders,
to a certain degree, displayed alignment with
human graders no matter whether DL-based
graders and human graders agreed on the qual-
ity of an answer; and (ii) it is possible to fa-
cilitate human grading by highlighting those
DL-detected important words, though further
investigations are necessary to understand how
human graders exploit such highlighted words.
1 Introduction
Automatic Text Scoring refers to the task of apply-
ing computational techniques to score written text
based on certain grading criteria (Alikaniotis et al.,
2016). Since its inception (Page, 1966), Automatic
Text Scoring has been actively investigated and ap-
plied to assist educators in scoring student-written
text, e.g., short answer questions and essays, which
are often referred to as Automated Short Answer
Scoring (ASAS) (Brew and Leacock, 2013) and Es-
say Scoring (Rodriguez et al., 2019). Driven by thegreat success of Deep Learning (DL) techniques
in various NLP tasks, researchers have endeavored
to apply them to construct ASAS systems in re-
cent years (Xia et al., 2020; Sung et al., 2019b,a),
some of which displayed performance compara-
ble to human graders. For instance, (Xia et al.,
2020) showed that the average performance of an
attention-based bidirectional LSTM model could
be up to 0.71 (measured by the metric Quadratic
Weighted Kappa) in the ASAS competition orga-
nized by the Hewlett Foundation, which can be
deemed as achieving a substantial agreement with
human graders.
Though being effective, DL-based ASAS sys-
tems have been widely plagued by the inability to
explain how the quality of an answer is graded.
The lack of understanding the underlying work-
ing mechanism of these systems, beyond question,
may stop educators from adopting them in teach-
ing practice as there remain concerns that the use
of such ASAS systems might unintentionally en-
courage students to produce formulaic writings,
i.e., writing that is often lengthy and involve com-
plex words, but not much quality content (Wil-
son et al., 2021; Chen and Cheng, 2008; Wang
et al., 2013). Inspired by the research efforts in the
broader NLP communities, i.e., those focusing on
dissecting complex deep neural net architectures
and explaining how they work (Serrano and Smith,
2019; Wiegreffe and Pinter, 2019; Jain and Wal-
lace, 2019; Xie et al., 2020; Sun et al., 2021), in
the study presented in this paper we aimed to gain
a better understanding of how DL-based ASAS
systems work. Speciﬁcally, we investigated (i) the
alignment between DL-based graders and human
graders in terms of the words they think are im-
portant in the task of Short Answer Scoring and
(ii) whether the important words identiﬁed by DL-
based graders can be of use to human graders in the
marking process. Formally, this study was guided
by the following two Research Questions:191RQ1 To what extent do DL-based graders align
with human graders regarding the words that
are important in assessing answer quality?
RQ2 Can the important words identiﬁed by DL-
based graders be used to facilitate human
graders to perform marking?
We conducted two user studies to answer RQ1
and RQ2. For RQ1, with the dataset provided by
the Hewlett Foundation, we constructed relatively
simple but effective BERT-based graders (i.e., cou-
pling BERT with a single classiﬁcation layer for
prediction), whose performances were comparable
to those reported in recent studies (Xia et al., 2020;
Surya et al., 2019). To locate words that were es-
sential in determining answer quality, we extracted
weights allocated to different input words in the
self-attention layers of BERT. Then, we ran a user
study to ask human graders to not only score an
answer but also annotate the words they believed
were important in contributing to or hurting the an-
swer quality. We measured the alignment between
BERT-based graders and human graders with the
aid of the Jaccard coefﬁcient. Building upon the
results of RQ1, we further implemented a random-
ized controlled trial to investigate whether display-
ing the important words identiﬁed by BERT-based
graders can help human graders improve marking
accuracy and efﬁciency to answer RQ2.
In summary, our work contributes to the research
on Automated Text Scoring with the following
main ﬁndings: (i) text spans contained in an answer
that increase the answer quality, compared to those
decreasing answer quality, are more likely to be
identiﬁed by human graders; (ii) there exists a cer-
tain level of alignment between DL-based graders
and human graders regarding the words they think
are important in assessing answer quality no mat-
ter whether they agree on the quality score of an
answer; and (iii) the important words detected by
DL-based graders can be potentially used to facili-
tate human grading, though more research efforts
are required to understand how these words are to
be utilized by human graders.
2 Related Work
2.1 Automated Short Answer Scoring
As a sub-branch of Automatic Text Scoring, ASAS
aims to leverage statistical and machine learning
techniques to assess the quality of short answers
authored by students in education (Burrows et al.,2015; Xia et al., 2020). Given its important role
played in supporting educators to scale up their
teaching practices (e.g., to meet the need of mark-
ing up to tens of thousands of answers submitted
by students and provide informative feedback in a
Massive Open Online Course (Pappano, 2012) in
a relatively short amount of time), ASAS has been
drawing attention from researchers since its incep-
tion (Page, 1966). Typically, ASAS can be tackled
as either a classiﬁcation problem (Xia et al., 2020)
or a regression problem (Sahu and Bhowmick,
2020). As surveyed in (Bonthu et al., 2021), the
approaches used to tackle ASAS often fall into two
categories. One is based on traditional machine
learning techniques such as SVM (Gleize and Grau,
2013; Mohler et al., 2011; Higgins et al., 2014),
K-means (Sorour et al., 2015), Linear Regres-
sion (Nau et al., 2017; Heilman and Madnani, 2015;
Higgins et al., 2014), and Random Forests (Higgins
et al., 2014; Ramachandran et al., 2015; Ishioka
and Kameda, 2017), all of which heavily rely on the
input of manually-crafted features. For example,
Sultan et al. (2016) devised a set of features which
were based on a lexical similarity (i.e., similarities
between words identiﬁed by a paraphrase database
(Ganitkevitch et al., 2013)) and monolingual align-
ment (Sultan et al., 2014), and input the designed
features to a ridge regression model to obtain the
score of an answer. The other category is based
on DL techniques, such as Bi-LSTM (Xia et al.,
2020; Kim et al., 2018) and BERT (Sung et al.,
2019b), which, in contrast to traditional machine
learning approaches, often demonstrate superior
performance without the need to engineer human-
crafted features. For instance, Sung et al. (2019b)
proposed a ﬁne-tuned BERT model for short an-
swer scoring, which outperformed human experts
in classifying short answers collected in the subject
of psychology.
It has been documented that the use of certain
Automated Text Scoring systems in education tends
to promote formulaic writing among students, i.e.,
producing lengthy and complex but not quality con-
tent (Wilson et al., 2021; Chen and Cheng, 2008;
Wang et al., 2013). As a result, there remain con-
cerns about the ability of these ASAS systems in
supporting teachers and instructors (Wilson et al.,
2021). To alleviate this issue, some studies were
proposed to investigate how ASAS systems work
to reach a decision (Higgins et al., 2014), which
mainly focused on the ASAS systems powered by192traditional machine learning approaches. As an ex-
ample, Higgins et al. (2014) demonstrated that in-
cluding syntactically-informed features could boost
the predictive performance of an ASAS model, en-
hancing the model’s ability to identify high-quality
responses written by students. To our knowledge, it
remains largely unexplored the interpretation abil-
ity and reliability of ASAS systems powered by
up-to-date DL techniques. An exception is pre-
sented by (Manabe and Hagiwara, 2021), in which
a toolkit named EXPATS is introduced to enable
people to visualize not only models based on tradi-
tional machine learning techniques but also those
based on DL techniques as well as the predictions
produced by these models. Our work distinguished
itself from previous studies by collecting human-
annotated data to inspect how an answer was evalu-
ated by an ASAS system, i.e., comparing the over-
lap between the important answer words identiﬁed
by DL-based graders and human graders, to shed
light on the extent to which DL-based graders acted
like human graders in the marking process.
2.2 Interpretability of Deep Learning Models
in NLP
The interpretability and explainability of a predic-
tive model have been widely acknowledged as an
essential factor in helping human users understand
the validity of a prediction and decide whether to
adopt the model for practical use (Jacovi and Gold-
berg, 2020; Belinkov et al., 2020). In this strand of
research, one common method is called test-based
(Li et al., 2015; Jain and Wallace, 2019; Sun et al.,
2020; Lei et al., 2016), which interprets a predic-
tion by identifying relevant parts of input data that
drive the prediction, e.g., words contained in a long
sentence that play major roles in determining the
overall sentimental polarity of the sentence. When
it comes to the application of DL-models equipped
with attention mechanism (Bahdanau et al., 2014)
for NLP tasks, researchers often regard the weights
assigned by the attention layer to different parts
of input text as indicators of their importance to
the model prediction (Mohankumar et al., 2020;
Yang et al., 2016; Wang et al., 2016). However,
it remains disputable to use attention weights to
measure the importance of input text (Wiegreffe
and Pinter, 2019; Serrano and Smith, 2019; Jain
and Wallace, 2019). For instance, by manipulating
attention weights in well-trained models to analyze
their inﬂuences upon predictions in text classiﬁ-cation, Serrano and Smith (2019) concluded that
attention weights can only noisily predict the over-
all importance of different input text to a model
prediction and thus should not be regarded as an
optional measure for strict importance ranking. On
the contrary, Wiegreffe and Pinter (2019) claimed
that the feasibility of attention-as-explanation de-
pends on the concrete deﬁnition of explanation. In
line with this claim, when inspecting a Transformer-
based model for non-factoid question answering,
Bolotova et al. (2020) extracted the self-attention
weights assigned to words in an answer to mea-
sure their importance. Similarly, Zou and Ding
(2021) analyzed the self-attention weights in three
Transformer-based models to investigate whether
these models displayed human-like attention in the
task of goal-directed reading comprehension. Sim-
ilar to these studies, we treated the self-attention
weights in a BERT-based model as proxies to re-
veal the importance of different input text, but in
the task of ASAS.
2.3 Human Grading
It has been documented that human grading can be
affected by various factors, e.g., students’ ethnic-
ity (Hinnerich et al., 2011; Van Ewijk, 2011) and
gender (Protivínský and Münich, 2018), and the
order of answers to be graded Yen et al. (2020). For
instance, Protivínský and Münich (2018) showed
that teachers’ grading was biased towards female
students in subjects of mathematics and the native
language (Czech), and the observed grading dif-
ference was likely due to the different levels of
non-cognitive skills (e.g., engagement in the class-
room) displayed by female and male students. In
a different vein, Yen et al. (2020) demonstrated
that human graders spent much less time if they
were presented with answers sorted according to
their similarities with the marking rubric. Previous
research on text reading showed that highlighting
can facilitate people to remember and comprehend
reading materials (Fowler and Barker, 1974; Lorch,
1989; Lorch et al., 1995; Dodson et al., 2017; Sil-
vers and Kreiner, 1997). Inspired by this, we were
interested in investigating whether it can facilitate
human graders to score answer quality by high-
lighting words that DL-based graders identiﬁed as
important in ASAS.1933 Methods
This study was approved by the Human Research
Ethics Committee at Monash University (Project
ID30074 ). In the following, we ﬁrst describe the
task and dataset based on which we examined the
alignment between DL-based graders and human
graders, followed by the construction of the DL-
based graders. Then, we detailed the setup of the
two user studies we implemented to answer RQ1
and RQ2.
3.1 Task, Dataset, and DL-based Graders
This study focused on the ASAS challenge orga-
nized by the Hewlett Foundation in Kaggle, whose
dataset contains over 17,000answers written by
students of Grade 10 in the US to 10 different
question prompts. The subjects of these question
prompts include such subjects as science, biol-
ogy, and English. Notice that each of the question
prompts and the corresponding collected answers
have their own unique characteristics, e.g., differ-
ent marking rubrics, scoring scale (some are on
[0,2]and the others are on [0,3]), whether rele-
vant source material was provided, and the average
length of answers (ranging from 40 to 60 words). It
is worth noting that each of the answers contained
in the original dataset was double-scored, i.e., be-
ing rated by two independent human graders, and
we denoted these scores as Ground-truth Score1
andGround-truth Score2 , respectively, in this study.
As speciﬁed in the challenge, Ground-truth Score1
is the ﬁnal score an answer received and also the
score that a model should aim to predict. As for
Ground-truth Score2, it can be used as a measure
of reliability. For instance, researchers can measure
the agreements (i) between a model and Ground-
truth Score1 and (ii) between Ground-truth Score1
and Ground-truth Score2, and then calculate the
difference between the two agreements to gain a
rough understanding of the gap between the con-
structed model and human graders.
In line with previous studies (Xia et al., 2020;
Surya et al., 2019; Sung et al., 2019b), we tackled
ASAS as a classiﬁcation problem, i.e., classifying
answers to different quality groups. Inspired by
the great success of BERT (Devlin et al., 2018) in
various NLP tasks, we also used it to construct
DL-based graders to score an answer automati-
cally. The model structure is relatively simple,i.e., we only coupled BERT with a single classi-
ﬁcation layer for prediction and then adapted the
model to capture the unique characteristics of this
task by ﬁne-tuning on the graded answers. Given
the unique characteristics of the different question
prompts contained in the dataset, we decided to
build a BERT-based grader for each of the question
prompts. For each question prompt, we randomly
split the answers in the ratio of 8:1:1 as training, val-
idation, and testing sets. The details of the construc-
tion process are provided in Appendix A. As sug-
gested by the challenge requirement, we used the
metric Quadratic Weighted Kappa (QWK) to mea-
sure the performance of DL-based graders, which
ranged from 0.660 to 0.891 for the 10 question
prompts. As our goal was to investigate that, when
a DL-based model was able to achieve a substantial
level of performance in assessing answer quality,
whether and to what extent it aligned with human
graders in the marking process. Thus, we chose
only the three question prompts (i.e., Prompt 1, 5,
and 6, which were all graded on a scale of [0,3])
in which DL-based graders achieved the best pre-
diction performance (i.e., with QWK 0.831, 0.860,
and 0.891, respectively) for the two user studies, as
described below.
3.2 Study One
For RQ1, we designed a study to collect answer
annotations from human graders, i.e., important
words or text spans that human graders believe to
increase or decrease the quality of an answer.
Participants. For Study 1, we recruited a total of
20 participants (7 females, 13 males), all of whom
had received at least a master’s degree, were pro-
ﬁcient in English, and were employed by Monash
University. In particular, all the participants had
certain years of prior teaching experience, i.e., 13
participants less than 3 years, four had 3 ∼5 years,
and three more than 5 years of experience. All
participants were informed of the purpose of this
study (and also Study 2) and signed consent forms
before participating in the studies.
Study Setup. For each question prompt, we ran-
domly selected ﬁve answers from each quality level
(i.e., scores in the scale of [0,3]) from the testing
data set, which resulted in a total of 60 answers. In
particular, we developed a grading system to allow
participants to not only score an answer but also an-
notate the words or text spans that they thought im-
portant in determining the quality levels. When an-194notating an answer, the participants were required
to annotate not only the text spans that increasing
answer quality but also those decreasing answer
quality, which were correspondingly denoted as
Positive andNegative text spans in later analysis.
We provided the screenshots of the grading system
in Appendix C. Each of the 20 participants was
required to attend a 90-minute session to grade 30
answers so that we collected a total of 600 assess-
ment scores and annotations from our participants.
Every answer was graded by 10 participants. In
particular, we assigned the selected answers to the
participants in a way that each participant was re-
quired to score answers of every quality level. After
completion, we compensated each participant with
a gift card worth $75 AUD for their time (i.e., $50
AUD per hour, which is comparable to the hourly
rate for people with a master’s degree in Australia).
Procedure. To ensure the quality of the collected
data, we expected the participants to undertake ade-
quate training to understand how they should mark
before moving to the actual answer scoring and
annotation. Therefore, the grading system we de-
veloped provided two modes for participants, i.e.,
Practice for pre-task training and Actual Task for
actual data collection. Only after ﬁnishing the ac-
tivities scheduled in Practice , the participants were
allowed to start the Actual Task . Both Practice and
Actual Task required a participant to evaluate an an-
swer by following the steps described below. The
main difference lied in the sources from which the
presented answers were selected, i.e., validation set
forPractice and testing set for Actual Task .
(1) Material Reading. A participant was asked
to read a prompt, an article relevant to the
prompt (if available in the original dataset),
marking rubric, and exemplar answers with
scores assigned by human graders (i.e.,
Ground-truth Score1).
(2) Pre-task questionnaire. The participant
was asked to indicate their familiarity, in-
terestingness, and perceived difﬁculty of the
question prompt by answering three ques-
tions on a rating scale of [1,5], which were
provided in the Appendix B.
(3) Answer assessment. An answer was pre-
sented for the participant to rate its quality.
Note that the marking rubric was displayed
along with the to-be-graded answer to facili-
tate the participant to mark.(4) Answer annotation. The participant was
instructed to highlight words and text spans
that they identiﬁed as important in determin-
ing answer quality.
Grading Alignment Measurement. Here, we in-
troduce how we detected the words that DL-based
graders paid attention to in the marking process
and further measured the overlap between these
words and those annotated by the human partici-
pants in Study 1. Similar to the work by Bolotova
et al. (2020), we extracted the words by calculating
weights assigned to each token in the self-attention
layers of the adopted BERT model. Speciﬁcally, for
a token tcontained in an answer Ans, its attention
weight is calculated as follows:
Atn_Score (t) =/summationtextw
|Ans|(1)
Here,|Ans|denotes the length of Ans, and
wdenotes the attention weight assigned by to-
kenito token t, which can be retrieved and cal-
culated in the attention layers of BERT. In more
detail, there were 12 attention heads in the adopted
BERT model; we ﬁrst averaged the weights con-
tained in these attention heads and then retrieved
win the averaged attention head. In brief, for
a token in an answer, we summed up the attention
weights from each token contained in the answer
to the target token and then normalized the sum
by the answer length to obtain the importance of
the target token. Then, we could select the top
K words with the largest attention weights as the
set of important words identiﬁed by BERT-based
graders. As suggested by Bolotova et al. (2020), we
determined the value of K with the aid of a linear re-
gression model, which took the length of an answer
as the only input to predict the number of important
words it should output. The regression model was
trained based on the human-annotated data we col-
lected in Study 1. Then, we followed the approach
adopted by Bolotova et al. (2020) and measured
their grading alignment by calculating the Jaccard
coefﬁcient between the two sets of important words
identiﬁed by a human grader and a BERT-based
grader. We also followed Qu et al. (2019) to ex-
clude stop words from the set of important words
before calculating the Jaccard coefﬁcient. The stop
words were detected with the aid of NLTK toolkit.
We ﬁrst calculate the Jaccard coefﬁcients over all195<answer ,human grader >data pairs collected in
this study and then averaged them as the ﬁnal mea-
sure of the alignment between BERT-based graders
and the human graders.
3.3 Study Two
For RQ2, we conducted a randomized controlled
experiment to investigate whether human grading
could be facilitated by observing highlighted im-
portant words detected by DL-based graders.
Participants. We randomly invited 10 out of the
20 participants who had taken part in Study 1 for
Study 2. As for the prior teaching experience, seven
out of the 10 participants were with less than 3
years, two were between 3 to 5 years, and one with
more than 5 years of experience.
Study Setup. The 10 participants were randomly
assigned to two groups, i.e., 5 in the control group
and 5 in the experimental group. Only the partici-
pants in the experimental group were displayed
with the highlighted words detected by BERT-
based graders. We randomly selected 36 answers
from the testing data for participants to mark. It
is worth noting that these answers were unseen to
the participants in Study 1, and these answers cov-
ered all quality levels in all question prompts. Each
participant was required to attend a 45-minute ses-
sion to assess a total of 24 answers. Each answer
was assessed by the same number of participants
from the control group and the experimental group.
Similar to Study 1, every participant was required
to score answers of all quality levels. After com-
pletion, each participant received a gift card worth
$40 AUD to compensate for their time.
Procedure. The procedure of Study 2 only con-
sisted of two main steps from the procedure of
Study 1 and the participants did not need to receive
pre-task training again, as described below.
(1) Material Reading. Similar to Study 1, we
presented participants with all relevant ma-
terials to help them get familiar with the task
requirement.
(2) Answer Assessment. In this step, the par-
ticipants in the control group were displayed
with answers without any highlighted words.
As for the answers presented to participants
in the experimental group, the important
words contained in these answers, i.e., those
detected by applying DL-based graders as
described in Section 3.2, were highlighted.It is noteworthy that the participants were
informed about (i) the nature of these high-
lighted words (i.e., important words de-
tected by DL-based graders) and (ii) the
reliability of these highlighted words (i.e.,
some might be helpful for assessing answer
quality while the others are not). We used
the linear regression constructed in Section
3.2 to determine the number of words that
should be highlighted to a participant.
(3) Post-task questionnaire. This step was
only for the participants in the experimental
group after completing the whole study. The
participants were asked to answer two ques-
tions on a rating scale of [1,5]with regard
to the usefulness of the highlighted words
for marking.
With the completion of the study, we compared
the grading performance of the participants in the
two groups from two perspectives. One is accu-
racy, which was calculated as the QWK between
the participant-provided scores and the ground-
truth scores. Here, we regarded that a participant-
provided score matched the ground-truth data if it
matched either Ground-truth Score1 or Ground-
truth Score2 in the original dataset; otherwise,
we simply regarded Ground-truth Score1 as the
ground-truth. The other is efﬁciency, which was
calculated as the average amount of time that par-
ticipants spent in scoring answers. The amount of
time that a participant used in assessing an answer
was computed as the time difference between the
moment when the participant entered the screen
of the grading system to assess the answer and the
moment when the participant clicked to move to
grade the next answer.
4 Results
Recall that all answers used in this study were as-
sessed on a grading scale of [0,3]. For the fol-
lowing analysis, we aggregated and denoted the
answers whose Ground-truth Score1 is 0 or 1 in the
original dataset as Low quality , and those of score
2 or 3 as High quality .
4.1 Results on RQ1
Table 1 details the fractions of answers with differ-
ent types of annotations, i.e., positive/negative text
spans that increased/decreased answer quality. We
observed that 89.3% answers were annotated by our196
participants, among which 74.5% received positive
annotations while only 42.5% received negative
annotations. This indicates that text spans con-
tributing to answer quality were more likely to be
determined and identiﬁed by human graders than
those lowering answer quality. This is corroborated
by the results in both high-quality and low-quality
answer categories. For instance, the fraction of an-
swers with positive and negative annotations were
95.3% and 32.7%, respectively. We can make sim-
ilar observations in low-quality answers, but the
difference was only 1.4%.
Table 2 describes the alignment between BERT-
based graders and human graders involved in Study
1, i.e., the Jaccard coefﬁcients between the two sets
of the important words identiﬁed by them when
performing marking. As a baseline for comparison,
we selected important words based on randomly-
assigned weights and measured their overlap with
the important words annotated by human graders
(as detailed in the column RANDOM ). By comparing
the results of RANDOM andALL, we can conclude
that there existed some agreement between BERT-
based graders and human graders. When consid-
ering both positive and negative annotations, the
alignment reached the value of 0.252 (about 69%
more than that of randomly-annotated important
words, i.e., 0.149). Also, we noticed that the align-
ment in high-quality answers was more than that
in lower-quality answers (0.275 vs. 0.224). When
considering only positive/negative annotations for
measurement, there was a higher level of alignment
in positive annotations than negative annotations
(0.251 vs. 0.174). This suggests that DL-based
graders and human graders were more likely to
agree with each other regarding what makes a good
answer than what makes a bad answer. This ﬁnding
is related to what we observed in Table 1, i.e., hu-
man graders made more positive annotations than
negative annotations in Study 1. These ﬁndings
together imply that, to a certain degree, there do
exist similarities between the BERT-based graders
and human graders in the grading process.It should be noted that (i) only 40 out of the 60
answers used in Study 1 were correctly assessed
by BERT-based graders and (ii) only 342 out of the
600 (about 57%) collected assessment scores pro-
vided by our participants were in agreement with
the ground-truth (i.e., matching to either Ground-
truth Score1 or Ground-truth Score2 in the original
dataset). To further investigate grading alignment
in different conditions (e.g., BERT-based graders
and human graders simultaneously failed to assess
the quality of an answer), we further divided the
collected data into two groups for analysis, i.e.,
G1consisting of answers for which human graders
and BERT-based graders simultaneously delivered
correct/incorrect answer scores and G2consisting
of the other data for which either human graders
or BERT-based graders failed to produce correct
assessments, as detailed in Table 2. The G1align-
ment was similar to those calculated by taking
all answers into account (i.e., the ALL column).
To our surprise, there also exists an overall align-
ment of 0.248 between BERT-based graders and
human graders in G2. This means, even in the situ-
ations where human graders were able to correctly
assess the quality of an answer but BERT-based
graders failed to do so (or the opposite), there is
some overlap between the human-annotated and
DL-detected important words. This implies that, in
certain cases, even human graders and DL-based
graders disagreed on the score value of an answer,
they did reach a partial agreement on the words that
were important for assessing the answer’s quality.
4.2 Results on RQ2
It should be pointed out that 25 out of the 36 se-
lected answers for our participants to grade in Study
2 were accurately scored by BERT-based graders.
The reasons we also included the inaccurately-
scored answers are two-fold. Firstly, our ultimate
goal is to use important words located by BERT-
based graders to facilitate human grading in prac-
tice. As a DL-based grader is unlikely to derive
correct predictions all the time in real-world scenar-
ios, it is very likely that some of the DL-identiﬁed
plausibly-important words will be presented to hu-
man graders in certain cases. Secondly, as demon-
strated in Study 1, when BERT-based graders dis-
agreed with human graders regarding an answer’s
quality, they still had moderate overlap between
the important words they identiﬁed. Thus, we were
interested in investigating whether human graders197
could be facilitated by observing (not necessarily
correct) important words identiﬁed by BERT-based
graders. The grading performances of the partici-
pants are given in Table 3, i.e., QWK for accuracy
and the average amount of time spent in scoring an
answer for efﬁciency. The results showed that, by
highlighting important words detected by BERT-
based graders, we could facilitate human grading
in terms of grading accuracy. However, contrary
to our expectation, the Experimental participants
spent roughly 12 more seconds in scoring an an-
swer than the Control participants. This is proba-
bly because, as being informed that the highlighted
words were identiﬁed by a DL-based model and
not necessarily useful in discerning answer quality,
the participants allocated more time in scrutiniz-
ing the answer text to judge its quality. This was
partially supported by their responses to questions
in the post-task questionnaire, i.e., an average rat-
ing of only 3.0 (out of 5.0) was reported regarding
whether the participants considered the highlighted
words as actually important, but an average rat-
ing of 3.8 (out of 5.0) was reported regarding the
usefulness of the highlighted words for marking.
5 Discussions and Conclusion
Given the increasing popularity of DL-based Au-
tomatic Text Scoring systems in education, this
work investigated the interpretability of these sys-tems regarding their alignment with human graders
in the task of ASAS. Through two well-designed
user studies, we demonstrated that (i) there exists
certain alignment between DL-based graders and
human graders in terms of the important words they
identiﬁed for assessing answer quality; and (ii) it is
possible to utilize the important words detected by
DL-based graders to facilitate human grading.
Though several interesting ﬁndings have been
enabled, it is not the time yet to advocate that
we should incorporate such DL-identiﬁed impor-
tant words into ASAS systems to facilitate human
grading due to the following limitations in our
study. Firstly, as observed in Study 1, DL-based
graders displayed certain agreement with human
graders, even in the circumstances of being un-
able to correctly assess the quality of an answer.
It remains unknown what types of words (or con-
cepts) DL-based graders agree/disagree on with
human graders. For example, would it be possi-
ble that both types of graders tend to hold similar
misconceptions (e.g., identifying the same type
of plausibly-correct words)? This calls for future
research to characterize and analyze the different
types of important answer spans identiﬁed by DL-
based graders and human graders under different
circumstances. By doing this, we can potentially
derive more insights on how to further improve the
prediction performance of DL-based graders. Sec-
ondly, and more importantly, it remains unknown
the sense-making process the human graders follow
to make use of the DL-detected important words.
As showed in Study 2, the human graders in the
Experimental group neither agreed nor disagreed
that the DL-highlighted words were important, but
they considered the DL-highlighted words were of
value and delivered better grading accuracy (at the
cost of a lower grading efﬁciency). Would it be
the case that the highlighted words provoke more198in-depth thinking and understanding of an answer
among human graders and thus enabling them to
better assess the answer quality? This motivates
us to conduct further studies to investigate human
graders’ marking process in using DL-highlighted
words in the future.
References199200A BERT-based Grader Construction
We constructed our DL-based graders based on
the well-known pre-trained language model BERT
(Devlin et al., 2018). Similar to (Sung et al.,
2019b), we implemented the BERT-based graders
by adopting the pre-trained bert-base-cased model
(12 layers, 768 neurons in each hidden layer, 12
attention heads, and a total of 110M parameters)
with a sequence classiﬁcation layer on top pro-
vided by the Python package Transformers. Then,
we ﬁne-tuned the whole model on the training set
of each question prompt. It should be noted that
only the student-authored answers were used as
input to train the model. During model training,
the batch size was set to 16, and we selected the
number of epoch from {1,2,3,4,5}and the learn-
ing rate from{1e−5,5e−5,1e−4}. For the
optimizer, we used Adam with decoupled weight
decay (i.e., AdamW). To determine the best val-
ues for the hyperparameters mentioned above, we
trained the models with different combinations of
parameter values and selected the best model based
on its performance on the validation set for each
question prompt. The run time for ﬁne-tuning
each BERT-based grader is roughly 1.5 hours (5
epochs, with NVIDIA Tesla P4 GPU). Table 4 de-
tails the parameter choices for each constructed
model and their corresponding performance (mea-
sured by QWK). By comparing QWK1 and QWK2
reported in Table 4, the difference between which
can be regarded as the gap between human graders
and the constructed BERT-based graders, we can
conclude that the BERT-based graders achieved
a substantial level of predictive performance in
most of the question prompts. Note that the
codes for constructing the models can be accessed
viahttps://github.com/douglashiwo/
AttentionAlignmentASAS .
B Pre-/Post-task Questionnaires
In Study 1, before scoring answers to a question
prompt, the participants were required to answer
the following three questions to indicate their in-
terest, familiarity, and perceived difﬁculty of the
question prompt on a rating scale of [1,5]:
•To what extent are you interested in the
prompt topic? (with 1 being not interested
at all and 5 being very interested)201
•To what extent are you familiar with the
prompt topic? (with 1 being not familiar at all
and 5 being very familiar)
•How would you describe the difﬁculty level
of the prompt topic to students of Grade 10 in
the US (i.e., the second year in high school)?
(with 1 being very easy and 5 being very difﬁ-
cult)
In Study 2, the participants in the Experimental
group were required to answer the following two
questions to share their opinions regarding the high-
lighted important words detected by BERT-based
graders on a rating scale of [1,5]:
•To what extent do you think the text high-
lighted by our automatic model grader are
actually “important” in determining the qual-
ity of an answer? (with 1 being not important
at all and 5 being very important)
•To what extent do you think the text high-
lighted by our automatic model grader helps
you grade answers? (with 1 being not helpful
at all and 5 being very helpful)
C The Developed Grading System
Figure 1 shows the screen of the developed grading
system in which participants scored an answer in
Study 1. Figure 2 shows the screen of the devel-
oped grading system in which participants anno-
tated an answer after assigning a score value to the
answer in Study 1. Figure 3 shows the screen of thedeveloped grading system in which participants in
the Experimental group scored an answer in Study
2; the screen that participants in the Control group
is similar to Figure 3 but without any answer words
being highlighted.202203204205