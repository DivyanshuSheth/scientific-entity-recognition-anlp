
Wei Tang, Benfeng Xu, Yuyue Zhao, Zhendong Mao, Yifeng Liu
Yong Liao,Haiyong XieUniversity of Science and Technology of China, Anhui, ChinaCCCD Key Lab of Ministry of Culture and Tourism, Anhui, ChinaNational Engineering Laboratory for Risk Perception and Prevention (RPP), Beijing, China
{weitang, benfeng, yyzha0}@mail.ustc.edu.cn, zdmao@ustc.edu.cn, liuyifeng3@cetc.com.cn
{yliao, hxie}@ustc.edu.cn
Abstract
Relational triple extraction is challenging for
its difficulty in capturing rich correlations be-
tween entities and relations. Existing works
suffer from 1) heterogeneous representations
of entities and relations, and 2) heteroge-
neous modeling of entity-entity interactions
and entity-relation interactions. Therefore, the
rich correlations are not fully exploited by
existing works. In this paper, we propose
UniRel to address these challenges. Specif-
ically, we unify the representations of enti-
ties and relations by jointly encoding them
within a concatenated natural language se-
quence, and unify the modeling of interactions
with a proposed Interaction Map, which is
built upon the off-the-shelf self-attention mech-
anism within any Transformer block. With
comprehensive experiments on two popular re-
lational triple extraction datasets, we demon-
strate that UniRel is more effective and compu-
tationally efficient. The source code is available
at https://github.com/wtangdev/UniRel.
1 Introduction
Relational Triple Extraction (RTE) aims to iden-
tify entities and their semantic relations jointly. It
extracts structured triples in the form of <subject-
relation-object> from raw texts in an end-to-end
manner, and is a crucial task towards automatically
constructing large-scale knowledge bases (Nayak
et al., 2021).
Early works try to solve RTE tasks in a pipelined
fashion involving two sub-tasks (Zelenko et al.,
2003; Chan and Roth, 2011), where entities are
firstly recognized, and relations are then assigned
for each extracted entity pair. Therefore, such meth-
ods fail to capture the implicit correlation between
these two isolated sub-tasks and are thus prone to
propagated errors (Li and Ji, 2014).
Instead, many researchers (Miwa and Bansal,
2016; Zheng et al., 2017; Zeng et al., 2018) seekFigure 1: We leverage semantic information to unify
the representation of entities and relations. Relational
triples are extracted by modeling the entity-entity inter-
action (blue dashed line) and entity-relation interaction
(red solid line) in a unified way.
to jointly extract <s-r-o> triples in an end-to-end
manner. For example, Wei et al. (2020) propose a
cascaded network that identifies subjects first and
then recognizes corresponding objects for relations.
Zheng et al. (2021a) decompose RTE into three sub-
tasks. Wang et al. (2020) and Shang et al. (2022)
extract relational triples in one stage to eliminate
exposure bias. In general, these works accomplish
end-to-end extraction by various ways to factor-
ize and re-assemble the label space of relational
triples. However, very often, there exist rich infor-
mative correlations between entities and relations,
and these correlations can hardly be captured by
the superficial constraints applied in label space
only. Specifically, existing works fall short of two
aspects: 1) heterogeneous representations between
entities and relations; 2) ignorance of interactive
dependencies between entity-level interactions and
relation-level interactions.
For representations, existing works mainly focus
on how to better capture the contextual information
of entities, while ignoring the equally important
semantic meaning of relations. Generally, rela-7087tions are simply represented as atomic label ids
indicating a specific dimension in the newly initial-
ized classifier (Wei et al., 2020; Wang et al., 2020;
Zheng et al., 2021a), resulting in its heterogene-
ity with language model augmented entity repre-
sentations. Such heterogeneity prevents models
from capturing the intrinsic correlations between
entities and relations in the semantic space. For
instance, from the semantic meaning of the relation
is_capital_of , we can infer that triples involving
are related to locations: the subject is supposed to
be a city, while the object should be a country. We
argue that it is important to build unified represen-
tations for both entities and relations.
For interactions, we exemplify the interdepen-
dence between the entity-entity interactions and
entity-relation interactions in Figure 1. We can eas-
ily determine that London andUKare correlated in
the prerequisite of given the interactions of ( Lon-
don-is_capital_of ) and ( UK-is_capital_of ).
However, existing works either enumerate all
entity-relation-entity triples (Wang et al., 2020;
Shang et al., 2022), which suffer from huge predic-
tion space, or model them with separate modules
(Wei et al., 2020; Zheng et al., 2021a; Li et al.,
2021) respectively for named entity recognition
(NER) and relation extraction (RE), resulting in
neglect of interdependencies. In general, the ab-
sence of unified modeling of interactions limits
these methods from fully utilizing their interdepen-
dencies for better extraction.
In this paper, we propose UniRel with Unified
Representation andInteraction to resolve both the
heterogeneity of representations and the absence of
interaction dependencies. We first encode both re-
lation and entity into meaningful sequence embed-
dings to construct unified representations. Based on
the semantic definition, candidate relations are first
converted to natural language texts, and form a con-
secutive sequence together with the input sentence.
We then apply a Transformer-based Pre-trained
Language Model (PLM) to encode the sequence
while intrinsically capturing their informative cor-
relations. We then propose a novel solution for Uni-
fied Interaction , where we simultaneously model
the entity-entity interactions and entity-relation in-
teractions in one single Interaction Map by lever-
aging the off-the-shelf self-attention mechanism
inside Transformer. Besides, benefiting from the
design of Interaction Map, UniRel preserves the ad-
vantages for end-to-end extraction and is superiorin computational efficiency.
We conduct comprehensive experiments on two
popular datasets: NYT (Riedel et al., 2010) and
WebNLG (Gardent et al., 2017), and achieve a new
state-of-the-art. We summarize our contributions
as follows:
•We propose unified representations of enti-
ties and relations by jointly encoding them
within a concatenated natural language se-
quence, which fully exploits their contextu-
alized correlations and leverages the semantic
knowledge learned from LM pre-training.
•We propose unified interactions to capture the
interdependencies between entity-entity inter-
actions and entity-relation interactions. This
is innovatively achieved by the proposed In-
teraction Map built upon the off-the-shelf self-
attention mechanism within any Transformer
block.
•We show that UniRel achieves the new state-
of-the-art for RTE tasks, while preserving the
superiority of being end-to-end and computa-
tionally efficient.
2 Related Work
Early works (Zelenko et al., 2003; Chan and Roth,
2011) apply pipeline approaches that divide rela-
tional triple extraction into two isolated sub-tasks:
first do named entity recognition to extract all enti-
ties, and then apply relation extraction to identify
relations for each entity pair. However, these meth-
ods always suffer from error propagation for failing
to capture the implicit correlation between these
two isolated sub-tasks.
To tackle these issues, recent researches focus
on jointly extracting entities and relations. Pre-
vious feature-based joint models (Yu and Lam,
2010; Miwa and Sasaki, 2014; Li and Ji, 2014;
Ren et al., 2017) require complex feature engineer-
ing and heavily depend on NLP tools. Researchers
propose neural network-based joint models to elim-
inate hand-craft features. Miwa and Bansal (2016)
propose a model to jointly learn entities and re-
lations through parameter sharing. Zheng et al.
(2017) transform RTE into a sequence tagging prob-
lem, which unifies the annotation role of entities
and relations.
Despite their success, most models cannot deal
with complex scenarios where one sentence con-7088sists of multiple overlapping relational triples shar-
ing a single entity (SingleEntityOverlap, SEO) or
an entity pair (EntityPairOverlap, EPO). To handle
the problem, researchers (Zeng et al., 2018, 2019;
Nayak and Ng, 2020; Ye et al., 2021) propose gen-
erative models that view triple as a token sequence.
Some works (Wang et al., 2020; Ren et al., 2021;
Shang et al., 2022) introduce methods to extract
triples in one-stage but suffer from huge prediction
space. Other researchers (Wei et al., 2020; Yuan
et al., 2020; Zheng et al., 2021b; Li et al., 2021; Wu
and Shi, 2021) decompose RTE into different sub-
tasks, but learn the interaction between sub-tasks
only by input sharing, or falling into the cascade
error. PFN (Yan et al., 2021) proposes a partition
filter network to fuse the task representation of
NER and RE, but still models entity-entity interac-
tions and entity-relation interactions with separate
modules. In this work, UniRel unifies the modeling
of the two kinds of interactions in one single Inter-
action Map to fully capture their interdependencies
and is superior in computational efficiency.
More recently, Xu et al. (2022) propose EmRel
that explicitly introduce relation representation to
leverage the rich interactions across relations, en-
tities, and context. However, it still suffers from
heterogeneity between entities and the newly ini-
tialized embeddings of relations. Some approaches
(Han et al., 2021; Chen and Li, 2021; Chen et al.,
2022) introduce prompt-tuning to extract relation
with semantic information. They transform the rela-
tion extraction task into a masked language model-
ing problem. However, such methods focus on the
simple scenario of sentence-level relation classifi-
cation without capturing the correlations between
entities and relations. In terms of technical designs,
SSAN (Xu et al., 2021) also delves into the self-
attention layer within the Transformer to model
structural interaction, but it makes extra adaptions
for standard self-attention mechanism and focuses
on document-level RE tasks. Compared to these ap-
proaches, our work aims to extract relational triples
in complex scenarios where rich intrinsic correla-
tions exist between entities and relations. In this
paper, we unify the representations and interactions
to fully exploit the correlations to extract entities
and relations jointly.
3 Methodology
In this section, we present our model in detail. We
first introduce the problem formulation in Section3.1. Then, we introduce the Unified Representa-
tion and the Unified Interaction in Section 3.2 and
Section 3.3, respectively. Finally, we present the
details of training and decoding in Section 3.4.
3.1 Problem Formulation
Given a sentence X={x, x,···, x}with
Ntokens, the goal of joint relational triple ex-
traction is to identify all possible triples T=
[(s, r, o)]from X, where s,r,prepre-
sent the subject, the object, and their relation, re-
spectively, and Lis the number of triples. The
subject and object are entity mentions E=
{e, e,···, e}from sentence X, k is the number
of entities. The relation is from pre-defined relation
setR={R, R,···, R}withMtypes. Note
that entities and relations might be shared among
triples.
3.2 Unified Representation
We first convert relations in the schema to natural
language texts to represent them in the same form
as the input sentence. For clarity, the conversion is
performed through a verbalizer with human-picked
words. The relation word is basically the most in-
formative word within the label name that preserves
its semantics, for example, “founders” for relation
“/business/company/founders ”. We then input
the concatenation of the input sentence and the nat-
ural language texts of relations to a Transformer-
based PLM encoder. We use BERT (Devlin et al.,
2019) as the PLM in this work. The inputs are then
transferred to a sequence of input embeddings by
searching the corresponding ids from the embed-
ding table.
T=Concat (T, T) (1)
H=E[T] (2)
where H∈Ris the input embedding
vector. dis the embedding size. TandTare
the input ids of the input sentence and the relations,
respectively. Eis the embedding table in BERT.
After obtaining the input embeddings, the en-
coder captures the correlations between each input
word with the self-attention mechanism. To be spe-
cific, Transformer-based PLMs comprise stacked
Transformer (Vaswani et al., 2017) layers consist-
ing of multiple attention heads. Each head applies
three separate linear transformations to transform
the input embeddings Hto query, key, and value
vectors Q,K,V, and then computes the attention7089
weights between all pairs of words by Softmax-
normalized dot production of QandK, which then
is fused with Vas follows:
Attention (Q, K, V ) =softmax (QK
√d)V. (3)
Each Transformer layer generates token embed-
dings from the previous layer’s output with the
self-attention mechanism. We denote the Has the
output of the i-the Transformer layer. As we take
both entities and relations into the input embedding
H, theH, encoded by such a deep Transformer
network, fully captures the rich intrinsic correla-
tions between entities and relations. The above
steps make the representations of entities and rela-
tions unified into one semantic embedding vector
Hwith rich correlational information.
3.3 Unified Interaction
The interactions between the triple elements, Entity-
Entity Interaction and Entity-Relation Interac-
tion, can be directly used to extract relational
triples. As shown in Figure 2, Triple ( London ,
is_capital_of ,UK) can be determined if know-ing the interactions between ( London -UK), (Lon-
don-is_capital_of ), and ( UK-is_capital_of ).
Motivated by this, we design an Interaction Map to
model the two kinds of interactions simultaneously.
3.3.1 Entity-Entity Interaction
Entity-entity interaction is defined for identifying
entity pairs that can be used to form valid rela-
tional triples. Given two entities eandefrom
sentence X, we regard entity pair (e, e)are in-
teracted only when there exists relation rthat can
be formed as valid triples together with them, and
both(e, r, e)and(e, r, e)are allowed. For ex-
ample, in Figure 2, ( Holmes -London ), as well as
(London -Holmes ), are supposed to be interacted
for the existing triple ( Holmes ,lives in ,London ),
while unrelated for ( Holmes -capital ) since no
valid triple consists of them. Formally, we define
the entity-entity interaction indicator function I(·)
as follows:
I(e, e) =

True (e, r, e)∈Tor
(e, r, e)∈T,∃r∈R
False otherwise,
(4)7090I(e, e) =I(e, e), as entity-entity interaction
is symmetrical.
3.3.2 Entity-Relation Interaction
Entity-relation interaction recognizes correlated en-
tities for each relation. Given a relation r, we re-
gard entity eas interacting with rwhen existing
triples consisting of eas either subject or object and
ras the relation. As the relation is directional, we
define entity-relation interaction asymmetrically to
distinguish subject entities and object entities, as
shown in the upper right part (Subject-Relation)
and lower left part (Relation-Object) of the map
in Figure 2, respectively. For instance, the inter-
action value of ( London -is_capital_of ) is sup-
posed to be True because of the valid triple ( Lon-
don,is_capital_of ,UK), while False for (UK
-is_capital_of ) since it is impossible for UK
to be the subject of the relation is_capital_of .
We formally define the indicator function I(·)of
entity-relation interaction as follows:
I(e, r) =/braceleftigg
True (e, r, o )∈T,∃o∈E
False otherwise,(5)
I(r, e) =/braceleftigg
True (s, r, e )∈T,∃s∈E
False otherwise,(6)
where I(e, r)andI(r, e)are defined for identi-
fying entity eas subject and object for relation r,
respectively.
3.3.3 Interaction Discrimination
Transformer layers bring powerful deep correlation
capturing ability to BERT. Therefore, As shown in
Figure 2, we comprise the two kinds of interactions
into one single Interaction Map, which is in the
same form as the attention map computed by the
Transformer layer. Then we directly take the last
Transformer layer of BERT for Interaction Discrim-
ination. As the Interaction Map is not restricted
to a normalized matrix, after obtaining Q,Kfrom
H, the embeddings generated by the last layer of
BERT, we average the dot production of QandK
of all heads and directly apply the sigmoid function
to obtain the results. The detailed operations are as
follows:
I=sigmoid (1
T/summationdisplayQK√d), (7)
where I∈Ris the interaction ma-
trix corresponding to the Interaction Map. Tisthe number of heads. WandWare trainable
weights. We consider I(·)valid when the value of
I(·)exceeds threshold σ.
UniRel captures the interactive dependencies as
the entity-entity interactions and entity-relation in-
teractions are simultaneously modeled in one sin-
gle Interaction Map. Besides, with Unified Inter-
action, the prediction space is narrowed down to
O((N+M)), which is much smaller than the
most recent work, OneRel, which predicts triples
in the complexity of O(N×M×N).
3.4 Training and Decoding
The binary cross entropy loss is used for training:
L=−1
(N+M)/summationdisplay/summationdisplay(IlogI
+(1−I) log(1 −I)),(8)
where Iis the ground truth matrix of the Interac-
tion Map.
For decoding, we first recognize all valid subject
entities and object entities for each relation from
entity-relation interactions I(the green box and
blue box in the lower right part of Figure 2). Then
we enumerate all candidate entity pairs for each
relation pruned by entity-entity interactions I(the
red box in the lower right part of Figure 2).
Such a decoding method can address the com-
plex scenarios with overlapping patterns containing
EntityPairOverlap (EPO) and SingleEntityOverlap
(SEO) for all arrangements of the entities and rela-
tions are taken into account. As shown in Figure 2,
the extracted triples contain SEO triples: ( Holmes ,
lives_in ,UK) and ( Holmes ,lives_in ,London ),
and the EPO triples: ( UK,contains ,London ) and
(London ,is_capital_of ,UK).
4 Experiments
4.1 Datasets and Evaluation
We evaluate the proposed method on two widely
used benchmark datasets NYT (Riedel et al., 2010)
and WebNLG (Gardent et al., 2017). NYT dataset
is produced by distant supervision from New York
Times articles which has 24 predefined relations.
WebNLG was first created for natural language
generation task and is adapted to relational triple
extraction by Zeng et al. (2018), which has 171
predefined relations. The statistics of the datasets
are shown in Table 2. We evaluate our method
on standard data splitting and report the standard7091ModelNYT WebNLG
Prec. Rec. F1 Prec. Rec. F1
NovelTagging (Zheng et al., 2017) 62.4 31.7 42.0 52.5 19.3 28.3
CopyRE (Zeng et al., 2018) 61.0 56.6 58.7 37.7 36.4 37.1
GraphRel (Fu et al., 2019) 63.9 60.0 61.9 44.7 41.1 42.9
OrderCopyRE (Zeng et al., 2019) 77.9 67.2 72.1 63.3 59.9 61.6
CasRel (Wei et al., 2020) 89.7 89.5 89.6 93.4 90.1 91.8
TPlinker (Wang et al., 2020) 91.3 92.5 91.9 91.7 92.0 91.9
PRGC (Zheng et al., 2021a) 93.3 91.9 92.6 94.0 92.1 93.0
R-BPtrNet (Chen et al., 2021) 92.7 92.5 92.6 93.7 92.8 93.3
PFN (Yan et al., 2021) - - 92.4 - - 93.6
TDEER (Li et al., 2021) 93.0 92.1 92.5 93.8 92.4 93.1
GRTE (Ren et al., 2021) 92.9 93.1 93.0 93.7 94.2 93.9
EmRel (Xu et al., 2022) 91.7 92.5 92.1 92.7 93.0 92.9
OneRel (Shang et al., 2022) 92.8 92.9 92.8 94.1 94.4 94.3
UniRel 93.5 94.0 93.7 94.8 94.6 94.7
UniRel 93.1 93.2 93.1 75.1 68.6 71.7
UniRel 92.6 93.7 93.1 93.5 94.4 93.9
micro Precision (Prec.), Recall (Rec.), and F1-score
on test set following the same setting as Zeng et al.
(2019).
Our model is implemented based on Pytorch.
We optimize the parameters by Adam (Kingma
and Ba, 2015) using learning rates 3e-5 and 5e-5
for NYT and WebNLG, respectively. The learn-
ing rates are searched in {3e-5, 5e-5, 7e-5}. We
also conduct weight decay (Loshchilov and Hutter,
2019) with a rate of 0.01. The batch size is 24/6 for
NYT/WebNLG and trained for 100 epochs. We use
cased BERT-basewith 108M parameters as the
PLM and set the max length of the input sentence
to 100 to keep in line with previous work. The size
of attention head dis 64. The threshold σis set as
0.5. We tune the parameters on the development set.
Our experiments are conducted on one NVIDIA
V100 32GB GPU.
Dataset Train TestOverlapping Pattern
Normal SEO EPO SOO
NYT 56195 5000 3266 1297 978 45
WebNLG 5019 703 245 457 26 844.2 Results
For comparison, We employed twelve strong mod-
els as baselines consisting of the SOTA models
PRGC (Zheng et al., 2021a), PFN (Yan et al., 2021),
TDEER (Li et al., 2021), GRTE (Ren et al., 2021)
and OneRel (Shang et al., 2022). We take the ex-
periment results from the original papers of these
baselines directly.
Table 1 shows the results of our model against
other baseline methods on all datasets. Many pre-
vious baselines achieve F1-score of over 90% on
both datasets, especially on WebNLG, which al-
ready exceed human-level performance. UniRel
achieves +0.7% and +0.4% improvements over F1-
scores on NYT and WebNLG and outperforms all
the baselines in terms of all the evaluation metrics,
which shows the superiority of our model.
To further study the ability to handle the over-
lapping problem and extracting multiple triples,
following previous works (Wei et al., 2020; Wang
et al., 2020; Zheng et al., 2021a; Shang et al., 2022),
we conduct further experiments on different types
subsets of NYT.
As shown in Table 3, the results indicate the
effectiveness of our model in complex scenarios.
Our model exceeds almost all the baselines in the
Normal class and three overlapping patterns. Es-
pecially in SEO and EPO, the most common over-7092Model Normal SEO EPO SOO L = 1 L= 2 L= 3 L= 4 L≥5
CasRel 87.3 91.4 92.0 77.0 88.2 90.3 91.9 94.2 83.7
TPlinker 90.1 93.4 94.0 90.1 90.0 92.8 93.1 96.1 90.0
PRGC 91.0 94.0 94.5 81.8 91.1 93.0 93.5 95.5 93.0
R-BPtrNet 90.4 94.4 95.2 - 89.5 93.1 93.5 96.7 91.3
PFN 90.2 95.3 94.1 - 90.5 92.9 93.7 96.3 92.6
TDEER 90.8 94.1 94.5 - 90.8 92.8 94.1 95.9 92.8
GRTE 91.1 94.4 95.0 - 90.8 93.7 94.4 96.2 93.4
OneRel 90.6 95.1 94.8 90.8 90.5 93.4 93.9 96.5 94.2
UniRel 91.695.395.289.891.594.394.596.694.2
lapping situations, our model achieves the high-
est performance, and the results are robust, which
demonstrates the advantage of UniRel in process-
ing overlapping triples. It can be seen that our
model also makes improvements for almost all
kinds of sentences regarding the number of triples.
From simple situation (L = 1) to complex case
(L= 3), UniRel still brings improvements, which
shows the robustness of our model. In general, this
experiment shows the power of our model in com-
plex scenarios. We attribute the effectiveness to
the captured rich interactions between entities and
relations by the introduced Interaction Map, which
is essential for solving the complex overlapping
triple problem.
5 Analysis
5.1 Ablation Study
In this section, we conduct ablation experiments on
NYT and WebNLG datasets to study the effective-
ness of the proposed Unified Representation and
Unified Interaction as reported in Table 1.
5.1.1 Effect of the Unified Representation
To study the effectiveness of Unified Representa-
tion, instead of assigning meaningful words for
each relation, we use the placeholder [unused]
of BERT to represent relations as marked as
UniRel in Table 1. Same as meaningless label
ids, the embeddings of [unused] tokens are ran-
domly initialized at the fine-tuning stage without
augmenting the meaningful semantic information
learned from pre-training. We can see a perfor-
mance decay in terms of all the evaluation metrics
on both datasets without Unified Representation,which indicates the importance of the semantic in-
formation for relational triple extraction.
We also notice the significant performance de-
crease of UniRel on the WebNLG dataset. We
think it is because the WebNLG dataset has much
fewer training data but defines far more relations
compared to the NYT dataset. Such a contradiction
makes many relations have few samples for train-
ing in the WebNLG dataset. And it is hard for a
model to learn the deep semantic information with
few examples from zero.
To validate our assumption, we further analyze
the performance of UniRel and UniRel on
relations with different orders of magnitude sam-
ples in training data on the WebNLG dataset. As
shown in Figure 3, UniRel performs well on
relations with much samples ( ≥1000 ), but is lim-
ited when the number of samples decreases, which
confirms our assumption. In contrast, UniRel main-
tains a good performance level in the face of differ-
ent numbers of samples. Especially with extremely
few samples ( ≤10), UniRel performs at the same
level as much samples ( ≥1000 ) , which further
demonstrates the effectiveness of Unified Repre-
sentation.
5.1.2 Effect of the Unified Interaction
To study the influence of Unified Interaction, we
take the relation sequence out of the input sen-
tence to model the two kinds of interactions in a
separate manner, and denoted as UniRel in
Table 1. Specifically, we first obtain the sequence
embeddings of the input sentence and the natural
language texts of relations severally with the same
BERT encoder. Then we apply two transform lay-
ers to get the query and key of the concatenation of7093
the two embeddings. Finally, the Interaction Map
is outputted by doing dot production of query and
key.
AsUniRel takes entity and relation as in-
dividual inputs, the deep Transformer network can
only independently model the correlations inside
entity pair and relation pair without capturing the
interdependencies between entity-entity interac-
tions and entity-relation interactions. As shown
in Table 1, UniRel has marked performance
degradation on both datasets compared to UniRel,
which demonstrates the unified interaction’s ef-
fectiveness. We further analyze the performances
of UniRel and UniRel on different interac-
tion types. As shown in Figure 4, not only entity-
relation, the F1-score of entity-entity also decreases
without simultaneously modeling the interactions,
which proves the interdependencies between the
two kinds of interactions, and UniRel takes benefits
from modeling them in a unified way.
5.2 Computational Efficiency
Table 4 shows the comparison results of the compu-
tational efficiency between UniRel and some recent
high-performance models. We report training and
inference time on both NYT and WebNLG datasets.
In this experiment, we follow previous works andModelTraining Time Inference Time
NYT WebNLG NYT WebNLG
CasRel 1142 105 35 37
TPLinker 2951 810 48 57
PRGC 3632 498 13 13
OneRel 2998 186 20 21
UniRel 967 119 12 14
set the batch size to 6/1 for training/inference. All
the compared models are tested in the same hard-
ware environment as declared in Section 4.1.
UniRel shows a conspicuous computational ef-
ficiency performance in both training and infer-
ence time. Specifically, Compared to the SOTA
model OneRel, on the NYT dataset, UniRel ob-
tains 3 ×and 1.7 ×faster in the stage of training
and inference, respectively. We think the reason is
that UniRel (O(N+M))has a smaller prediction
space than OneRel (O(N×M×N)). Although
CasRel performs similarly to ours regarding train-
ing time, UniRel obtains more than 2.6 ×speedups
in the inference time. We attribute the efficiency
to the design of the Interaction Map, which allows
UniRel to narrow down the prediction space and di-
rectly leverage the off-the-shelf self-attention mech-
anism within the Transformer block.70945.3 Visualization
We visualize the Interaction Map to see how it
works for relational triple extraction. As shown in
Figure 5, the red box represents the entity-entity
interaction. The blue box and green box represent
the entity-relation interaction for the subject and
the object, respectively. From the map, we can
extract all six relational triples: ( Yunnan ,country ,
China ), (China ,administrative_divisions ,
Yunnan ), (Thailand ,contains ,Chiang Mai ), (Yu-
nan,Contains ,Jinghong ), (China ,Contains ,
Jinghong ), and ( China ,Contains ,Yunnan ).
6 Conclusion
In this work, we propose UniRel to fully leverage
the rich correlations between entities and relations
by resolving the heterogeneity. Unified Represen-
tation eliminates the representation’s heterogeneity
by encoding both entity and relation into mean-
ingful sequence embeddings. Unified Interaction
eliminates the interaction’s heterogeneity by simul-
taneously modeling entity-entity interactions and
entity-relation interactions in one single Interaction
Map. UniRel produces significant improvements
over competitive baselines. We give a comprehen-
sive analysis to further justify our design.
Limitations
There are two limitations we want to discuss in this
section:
•First, for clarity, we select the correspond-
ing words for each relation in a manual way,
which would be sophisticated for schema with
much relations. We will next to try to design
an auto-verbalizer for relations.
•Second, the evaluation benchmarks (NYT and
WebNLG) for joint relational triple extrac-
tion are produced with much annotated train-
ing data, which is expensive for real applica-
tion. The performance of our model in low-
resource scenario needs to be validated. How-
ever, existing benchmarks for low-resource
(Han et al., 2018; Gao et al., 2019) are lim-
ited to the simple scenario of sentence-level
relation classification. We would like to ex-
plore the the idea of unified representation and
unified interaction towards joint RTE in low
resource scenario in the future.Acknowledgements
This work is supported by the National Key
Research and Development Program of China
(2021YFC3300500).
References70957096
A Supplemental Experiments
A.1 Influence of Random Seeds
As shown in Table 5 , we conduct experiments with
5 different random seeds, and the results and their
improvements are rather robust. We get an aver-
aged performance of 93.62 ±0.07/94.52 ±0.12 on
NYT/WebNLG with 5 runs in total, which robustly
outperforms the previous SOTA methods.
Dataset 2019 2021 2022 2023 2024
NYT 93.6 93.7 93.6 93.7 93.5
WebNLG 94.4 94.7 94.5 94.4 94.6
A.2 Influence of Relation Numbers
To study how the number of relations Minfluences
the performance of UniRel, as shown in Figure 6,
we conduct ablation experiments with different M.
We control the number of sentences of each relation
in the range of [1500, 1700) to keep each relation
has similar signals in each experiment.
From Figure 6, we can observe that UniRel has
relatively stable improvements when the number of
relations increases. For example, UniRel achieves
both +5.6% improvements when M= 3andM=
9. The results show the effectiveness of UniRel is
relatively stable with different numbers of relations.
A.3 Relation-Word Mapping
For clarity, we convert relations to natural language
texts with human-picked words. The picked word7097is basically the most informative word within the
label name that preserves its semantics, for exam-
ple, “founders” for “/business/company/founders”.
We don’t exploit any special selection strategy and
need no formal annotation. For exceptional cases
where the relation’s labels are very similar, we sim-
ply resort to alternative words ("part” and "sec-
tion”) or capitalization ("part” and "Part”) to make
a distinction. This provides a semantic-aware ini-
tialization and the model will continue to optimize
them. As a result, the performance of UniRel is
very robust to choices of words. As shown in Ta-
ble 7, we conduct experiments with two different
relation-word mappings of the NYT dataset, and
the results show UniRel is robust to the mapping
words.
A.4 Extended to Multi-token Entity Setting
With minimum adaptation, we can extend UniRel
to the multi-token entity setting. The adaption pro-
cesses are as follows: 1) repeat Interaction Map
twice respectively for head and tail token of en-
tity span to identify (subject-head, relation, object-
head) and (subject-tail, relation, object-tail). 2) add
a third Interaction Map between head-tail tokens
to identify (head, relation, tail). 3) decode triples
by linking the head and tail tokens of entities w.r.t.
each relation.
As shown in Table 6, UniRel achieves SOTA
performance under the multi-token entity setting,
which further indicates the effectiveness of the pro-
posed methods.
ModelNYT WebNLG
Prec. Rec. F1 Prec. Rec. F1
NovelTagging 32.8 30.6 31.7 52.5 19.3 28.3
MultiHead 60.7 58.6 59.6 57.5 54.1 55.7
ETL-span 85.5 71.7 78.0 84.3 82.0 83.1
RSAN 85.7 83.6 84.6 80.5 83.8 82.1
PRGC 93.5 91.9 92.7 89.9 87.2 88.5
TPLinker 91.4 92.6 92.0 88.9 84.5 86.7
GRTE 93.4 93.5 93.4 92.3 87.9 90.0
OneRel 93.2 92.6 92.9 91.8 90.3 91.0
UniRel 93.7 93.2 93.4 91.8 90.5 91.17098Relation Mapping A Mapping B
/business/company/advisors advisors counselor
/business/company/founders founders creator
/business/company/industry industry sector
/business/company/major_shareholders holding shareholder
/business/company/place_founded founded establish
/business/company_shareholder/major_shareholder_of shareholder holder
/business/person/company company corporation
/location/administrative_division/country country state
/location/country/administrative_divisions administrative administration
/location/country/capital capital Capital
/location/location/contains contains include
/location/neighborhood/neighborhood_of neighbor neighbour
/people/deceased_person/place_of_death death Death
/people/ethnicity/geographic_distribution geographic Geographic
/people/ethnicity/people people People
/people/person/children children Children
/people/person/ethnicity ethnicity ethnic
/people/person/nationality nationality national
/people/person/place_lived lived live
/people/person/place_of_birth birthplace birth
/people/person/profession profession career
/people/person/religion religion Religion
/sports/sports_team/location location Location
/sports/sports_team_location/teams teams Teams
Prec./Recall/F1. 93.5/94.0/93.7 93.9/93.4/93.67099