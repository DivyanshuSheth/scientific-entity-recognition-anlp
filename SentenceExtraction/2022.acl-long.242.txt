
Zhihui Guo, Pramod Sharma, Andy Martinez, Liang Du, Robin Abraham
Microsoft Corporation
Abstract
Molecular representation learning plays an es-
sential role in cheminformatics. Recently, lan-
guage model-based approaches have gained
popularity as an alternative to traditional expert-
designed features to encode molecules. How-
ever, these approaches only utilize a single
molecular language for representation learning.
Motivated by the fact that a given molecule can
be described using different languages such
as Simplified Molecular Line Entry System
(SMILES), the International Union of Pure
and Applied Chemistry (IUPAC), and the IU-
PAC International Chemical Identifier (InChI),
we propose a multilingual molecular embed-
ding generation approach called MM-Deacon
(multilingual molecular domain embedding
analysis via contrastive learning). MM-Deacon
is pre-trained using SMILES and IUPAC as two
different languages on large-scale molecules.
We evaluated the robustness of our method
on seven molecular property prediction tasks
from MoleculeNet benchmark, zero-shot cross-
lingual retrieval, and a drug-drug interaction
prediction task.
1 Introduction
Drug discovery process involves screening of mil-
lions of compounds in the early stages of drug
design, which is time consuming and expensive.
Computer-aided drug discovery can reduce the time
and cost involved in this process via automating
various cheminformatics tasks (Kontogeorgis and
Gani, 2004; Xu et al., 2017; Winter et al., 2019).
Traditional methods to encode molecules such
as fingerprint generation rely heavily on molecu-
lar fragment-level operations on top of molecule
graph constructed by molecular atoms and bonds
(Burden, 1989; Bender and Glen, 2004; V ogt and
Bajorath, 2008; Muegge and Mukherjee, 2016).
An example of such methods is Morgan fingerprint,
also known as Extended-Connectivity Fingerprint
(ECFP) (Morgan, 1965; Rogers and Hahn, 2010),where a fixed binary hash function is applied on
each atom and its neighborhood. These kinds of
approaches focus on local features, hence they may
not capture global information.
In addition to molecule graph, a given molecule
can also be described with different languages
such as Simplified Molecular Line Entry System
(SMILES), the International Union of Pure and
Applied Chemistry (IUPAC), and the IUPAC Inter-
national Chemical Identifier (InChI). Particularly,
SMILES is widely used to represent molecule struc-
tures as ASCII strings (Weininger, 1988; Favre and
Powell, 2013) at an atom and bond level. IUPAC
nomenclature, on the other hand, serves the pur-
pose of systematically naming organic compounds
by basic words that indicate the structure of the
compound and prioritize on functional groups to
facilitate communication (Panico et al., 1993). Fig.
1 shows a comparison of SMILES and IUPAC char-
acteristics for the same molecule. The SMILES
string is created by traversing the molecule graph,
where each letter in the SMILES string (such as
C, F , N, O in Fig. 1) corresponds to an atom on
the graph, and other characters represent positions
and connectivity. However, IUPAC names are akin
to a natural language, and morphemes in the IU-
PAC name (like fluoro, prop, en, yl in this example)
often represent specific types of substructure on
the molecule graph, which are also responsible for
characteristic chemical reactions of molecules.
Advances in natural language processing (NLP)
have been very promising for molecule embed-
ding generation and molecular property prediction
(Xu et al., 2017; Gómez-Bombarelli et al., 2018;
Samanta et al., 2020; Koge et al., 2021; Honda
et al., 2019; Shrivastava and Kell, 2021; Goh et al.,
2017; Schwaller et al., 2019; Payne et al., 2020;
Aumentado-Armstrong, 2018). It is important to
note that all of the methods mentioned above work
with SMILES representation only. Therefore, the
underlying chemical knowledge encoded in the em-3441
bedding is restricted to a single language modality.
Transformer models trained with self-supervised
masked language modeling (MLM) loss (Vaswani
et al., 2017) in chemical domain (Wang et al., 2019;
Chithrananda et al., 2020; Elnaggar et al., 2020;
Rong et al., 2020; Schwaller et al., 2021; Bagal
et al., 2021) have also been used for molecular
representation learning. However, pre-training ob-
jectives like MLM loss tend to impose task-specific
bias on the final layers of Transformers (Carlsson
et al., 2020), limiting the generalization of the em-
beddings.
In recent years, contrastive learning has been
successful in multimodal vision and language re-
search (Radford et al., 2021; Meyer et al., 2020;
Shi et al., 2020; Cui et al., 2020; Chen et al., 2021;
Alayrac et al., 2020; Akbari et al., 2021; Lee et al.,
2020; Liu et al., 2020). Radford et al. (2021) used
image-text pairs to learn scalable visual representa-
tions. Carlsson et al. (2020) showed the superiority
of contrastive objectives in acquiring global (not
fragment-level) semantic representations.
In light of these advances, we propose
MM-Deacon ( multilingual molecular domain
embedding analysis via contrastive learning), a
molecular representation learning algorithm built
on SMILES and IUPAC joint training. Trans-
formers are used as base encoders in MM-Deacon
to encode SMILES and IUPAC, and embeddings
from encoders are projected to a joint embedding
space. Afterwards, a contrastive objective is used
to push the embeddings of positive cross-lingual
pairs (SMILES and IUPAC for the same molecule)
closer together and the embeddings of negativecross-lingual pairs (SMILES and IUPAC for dif-
ferent molecules) farther apart. Here instead of us-
ing SMILES and IUPAC for sequence-to-sequence
translation (Rajan et al., 2021; Krasnov et al., 2021;
Handsel et al., 2021), we obtain positive and nega-
tive SMILES-IUPAC pairs and contrast their em-
beddings at the global molecule level rather than
the fragment level. Different molecule descriptors
are thus integrated into the same joint embedding
space, with mutual information maximized across
distinct molecule languages.
We pre-train MM-Deacon on 10 million
molecules chosen at random from the publicly
available PubChem dataset (Kim et al., 2016) and
then use the pre-trained model for downstream
tasks. Our main contributions are as follows:
•We propose MM-Deacon, a novel approach
for utilizing multiple molecular languages to
generate molecule embeddings via contrastive
learning.
•To the best of our knowledge, we are the first
to leverage mutual information shared across
SMILES and IUPAC for molecule encoding.
•We conduct extensive experiments on a vari-
ety of tasks, including molecular property pre-
diction, cross-lingual molecule retrieval, and
drug-drug interaction (DDI) prediction, and
demonstrate that our approach outperforms
baseline methods and existing state-of-the-art
approaches.
2 Molecule pre-training
Deep learning tasks commonly face two challenges:
first, dataset size is often limited, and second, an-
notations are scarce and expensive. A pre-training
scheme can benefit downstream tasks by leverag-
ing large-scale unlabeled or weakly labeled data.
Such pre-training and fine-tuning frameworks have
recently sparked much interest in the molecular
domain (Hu et al., 2019; Samanta et al., 2020;
Chithrananda et al., 2020; Rong et al., 2020; Shri-
vastava and Kell, 2021; Xue et al., 2021; Zhu et al.,
2021; Wang et al., 2021; Liu et al., 2021). Exist-
ing pre-training methods can be divided into three
categories based on the models used: pre-training
with graph neural networks (GNNs), pre-training
with language models, and pre-training with hybrid
models.
Pre-training with GNNs. GNNs are a popular
choice for molecule encoding that regard atoms as3442nodes and bonds as edges. Hu et al. (2019) pre-
trained GNNs on 2 million molecules using both
node-level and graph-level representations with at-
tribute masking and structure prediction objectives.
MolCLR (Wang et al., 2021) used subgraph-level
molecule data augmentation scheme to create pos-
itive and negative pairs and contrastive learning
to distinguish positive from negative. GraphMVP
(Liu et al., 2021) was pre-trained on the consistency
of 2D and 3D molecule graphs (3D graphs formed
by adding atom spatial positions to 2D graphs) and
contrastive objectives with GNNs.
Pre-training with language models. Language
models are widely used to encode SMILES for
molecular representation learning. Xu et al. (2017)
reconstructed SMILES using encoder-decoder
gated recurrent units (GRUs) with seq2seq loss,
where embeddings in the latent space were used for
downstream molecular property prediction. Chem-
berta (Chithrananda et al., 2020) fed SMILES
into Transformers, which were then optimized by
MLM loss. FragNet (Shrivastava and Kell, 2021)
used encoder-decoder Transformers to reconstruct
SMILES and enforced extra supervision to the la-
tent space with augmented SMILES and contrastive
learning. X-Mol (Xue et al., 2021) was pretrained
by taking as input a pair of SMILES variants for
the same molecule and generating one of the two
input SMILES as output with Transformers on 1.1
billion molecules.
Pre-training with hybrid models. Different
molecule data formats can be used collaboratively
to enforce cross-modality alignment, resulting in
the use of hybrid models. For example, DMP
(Zhu et al., 2021) was built on the consistency of
SMILES and 2D molecule graphs, with SMILES
encoded by Transformers and 2D molecule graphs
encoded by GNNs.
Unlike other molecule pre-training methods,
MM-Deacon is multilingually pre-trained with lan-
guage models using pairwise SMILES and IUPAC.
Compared with using molecule graphs with GNNs,
IUPAC names encoded by language models bring
in a rich amount of prior knowledge by basic words
representing functional groups, without the need
for sophisticated graph hyperparameter design.
3 Method
MM-Deacon is a deep neural network designed for
SMILES-IUPAC joint learning with the goal of con-
trasting positive SMILES-IUPAC pairs from nega-
tive pairs and thus maximizing mutual information
across different molecule languages. SMILES and
IUPAC for the same molecule are regarded as posi-
tive pairs, while SMILES and IUPAC for different
molecules are considered negative. Transformer
encoders with multi-head self-attention layers are
utilized to encode SMILES and IUPAC strings. Em-
beddings from the encoders are pooled globally and
projected to the joint chemical embedding space.
MM-Deacon is pre-trained on a dataset of 10 mil-
lion molecules chosen at random from PubChem.
3.1 Tokenizer
We use a Byte-Pair Encoding (BPE) tokenizer for
SMILES tokenization, as is shown by Chithrananda
et al. (2020) that BPE performed better than regex-
based tokenization for SMILES on downstream
tasks. For IUPAC name tokenization, a rule-based
regex (Krasnov et al., 2021) that splits IUPAC
strings based on suffixes, prefixes, trivial names,
and so on is employed. The input sequence length
statistics as well as the top 20 most frequent tokens
in the SMILES and IUPAC corpora are displayed
in Figs. 9 and 10 (Appendix A).
3.2 Model architecture
As illustrated in Fig. 2, MM-Deacon takes
SMILES and IUPAC strings as the input to sep-
arate branches. The input text string sis tokenized
and embedded into a numeric matrix representation
xwithin each branch, and the order of the token list
is preserved by a positional embedding p. Then x
andpare ingested by an encoder block φthat con-
sists of 6 layers of Transformer encoder. A Trans-3443former encoder has two sub-layers, a multi-head
attention layer and a fully-connected feed-forward
layer. Each sub-layer is followed by a residual con-
nection and layer normalization to normalize input
values for all neurons in the same layer (Vaswani
et al., 2017; Ba et al., 2016). The multi-head atten-
tion layer acquires long-dependency information
by taking all positions into consideration. We then
use a global average pooling layer ρto integrate
features at all positions and a projection layer ϕ
to project the integrated feature vector to the joint
embedding space. Thus the final embedding zofx
can be expressed as,
z=ϕ(ρ(φ(x+p))). (1)
The maximum input token sequence length is set
to 512. For each of the 6 Transformer encoder lay-
ers, we choose the number of self-attention heads
as 12 and hidden size of 768. The projection layer
ϕprojects the vector from length of 768 to 512
to make the representation more compact. Thus
z∈R.
3.2.1 Contrastive loss
Our goal is to align pairs of language modalities in
the joint embedding space by maximizing mutual
information of positive pairs and distinguishing
them from negative pairs. For this purpose, we use
InfoNCE (Oord et al., 2018; Alayrac et al., 2020;
Radford et al., 2021) as the contrastive loss. We
do not construct negative pairs manually. Instead,
during training, we obtain negative pairs in mini-
batches. Using a minibatch of NSMILES-IUPAC
pairs from Nmolecules as input, Npositive pairs
andN−Nnegative pairs can be generated within
the correlation matrix of NSMILES strings and N
IUPAC strings. More specifically, the only positive
pair for i-th SMILES is i-th IUPAC, while the re-
maining N−1IUPAC strings form negative pairs
withi-th SMILES. Therefore, the InfoNCE loss for
i-th SMILES is,
L=−log(exp(sim(z, z)/τ)Pexp(sim(z, z)/τ)),(2)
where slandiprepresent SMILES and IUPAC
respectively. sim()is the pairwise similarity func-
tion that employs cosine similarity in this work. τ
is the temperature. Likewise, the loss function for
i-th IUPAC is,
L=−log(exp(sim(z, z)/τ)Pexp(sim(z, z)/τ)).(3)
As a result, the final loss function is as follows,
L=1
2NXXL. (4)
We pre-train MM-Deacon on 80 V100 GPUs for
10 epochs (15 hours in total) with a 16 batch size on
each GPU using AdamW optimizer with a learning
rate of 10. The temperature τis set as 0.07 as in
(Oord et al., 2018).
3.3 Downstream stage
Knowledge gained during pre-training can be trans-
ferred to downstream tasks in different ways. Fig.
3 lists two situations that make use of pre-trained
MM-Deacon in the downstream stage.
MM-Deacon fine-tuning: A task-specific clas-
sification/regression head can be attached to pre-
trained MM-Deacon and the system as a whole can
be tuned on downstream task datasets.
MM-Deacon fingerprint: Pre-trained MM-
Deacon is frozen. An input molecule is embed-
ded as MM-Deacon fingerprint for zero-shot explo-
rations (such as clustering analysis and similarity
retrieval) and supervised tasks with the help of an
extra classifier.
4 Experiments
MM-Deacon was evaluated on seven molecular
property prediction tasks from MoleculeNet bench-3444mark (Wu et al., 2018), zero-shot cross-lingual re-
trieval, and a drug-drug interaction (DDI) predic-
tion task.
4.1 Molecular property prediction
MoleculeNet benchmark provides a unified frame-
work for evaluating and comparing molecular ma-
chine learning methods on a variety of molecular
property prediction tasks ranging from molecular
quantum mechanics to physiological themes, and
is widely acknowledged as the standard in the re-
search community (Hu et al., 2019; Chithrananda
et al., 2020; Xue et al., 2021; Zhu et al., 2021;
Wang et al., 2021; Liu et al., 2021). Four classifica-
tion datasets and three regression datasets from the
MoleculeNet benchmark were utilized to evaluate
our approach.
Data. The blood-brain barrier penetration (BBBP),
clinical trail toxicity (ClinTox), HIV replication
inhibition (HIV), and side effect resource (SIDER)
datasets are classification tasks in which molecule
SMILES strings and their binary labels are pro-
vided in each task. Area Under Curve of the Re-
ceiver Operating Characteristic curve (ROC-AUC)
is the performance metric in which the higher the
value, the better the performance. For datasets
with multiple tasks like SIDER, the averaged ROC-
AUC across all tasks under the same dataset is
reported. The fractions of train/val/test sets for
each classification task are 0.8/0.1/0.1 with Scaf-
fold split. Note that data split using molecule scaf-
folds (two-dimensional structural frameworks) re-
sults in more structurally distinct train/val/test sets,
making it more challenging than random split (Wu
et al., 2018). The water solubility data (ESOL), free
solvation (FreeSolv), and experimental results of
octabol/water distribution coefficient (Lipophilic-
ity) datasets are all regression tasks to predict nu-
meric labels given molecule SMILES strings. Root
Mean Square Error (RMSE) is used as the evalua-
tion metric in which the lower the value, the better
the performance. As recommended by Molecu-
leNet, random split that divides each dataset into
0.8/0.1/0.1 for train/val/test sets is employed. The
results on validation set are used to select the best
model. To maintain consistency with MoleculeNet,
we ran each task three times, each time with a dif-
ferent data split seed, to obtain the mean and stan-
dard deviation (std) of the metric. Details of each
dataset such as the number of tasks and molecules
it contains are displayed in Table 1.
Model. We utilized the model shown in Fig.
3(a) in which a linear layer serving as the task-
specific head was added to pre-trained MM-Deacon
SMILES branch for fine-tuning (IUPAC branch
was removed). Cross-entropy loss was employed
for classification tasks and MSE loss was employed
for regression tasks. Hyperparameter tuning was
performed using grid search with possible choices
listed in Table 5 (Appendix B). Each task was opti-
mized individually.
Results. Table 2 shows the mean and std re-
sults for each dataset. The first half of the table
displays results imported from MoleculeNet (Wu
et al., 2018), while the second section shows the
results from MM-Deacon and other state-of-the-art
molecular pre-training and fine-tuning approaches.
MLM- [CLS] denotes our implementation of a
Chemberta (Chithrananda et al., 2020) variant that
uses the same Transformer settings as MM-Deacon
SMILES branch, pre-trained with MLM loss on
10M molecules, and fine-tuned through [CLS]
token with the same downstream setting as MM-
Deacon. MM-Deacon exceeds the performance of
traditional machine learning methods like random
forest (RF) and task-specific GNNs reported in
MoleculeNet work by a significant margin for most
of the tasks. When compared to other pre-training
based approaches, MM-Deacon outperforms the
existing state-of-the-art approaches in four of the
seven datasets and is comparable in the remaining
three, with major improvements on ClinTox and
FreeSolv.
All pre-training based methods were pre-trained
on millions of molecules, with the exception
of GraphMVP, which was pre-trained on 50K
molecules. The requirement that molecules have
both 2D and 3D structure information available at3445
the same time to be qualified has limited the scala-
bility of GraphMVP. MM-Deacon and MLM- CLS
both used 6 layers of Transformer blocks to pro-
cess SMILES. For each task, MM-Deacon, which
was pre-trained with both SMILES and IUPAC,
outscored MLM- CLS, which was pre-trained with
SMILES only. MM-Deacon and DMP performed
comparably on the four classification tasks, while
DMP used 12 layers of Transformer blocks for
SMILES and a 12-layer GNN to encode a molecule
2D graph, which is nearly twice the size of MM-
Deacon model.
Moreover, we found that BBBP test set is sig-
nificantly more challenging than the validation set,
which is consistent with the results published in
the MoleculeNet paper (Wu et al., 2018). The sub-
stantially high accuracy X-Mol achieved on the
BBBP dataset could be due to either the 1.1 bil-
lion molecules they utilized for pre-training or a
different dataset division approach they employed.
4.2 Zero-shot cross-lingual retrieval
In addition to conducting fine-tuning on super-
vised tasks like molecular property prediction, pre-
trained MM-Deacon can be employed directly in
large-scale zero-shot analysis. Zero-shot cross-
lingual retrieval operates on top of MM-Deacon
fingerprint generated by pre-trained MM-Deacon
given molecule SMILES or IUPAC as input. This
task enables the retrieval of similar molecules
across languages without the need for translation,
and it can also be used to evaluate the learnedagreement in the joint embedding space between
SMILES and IUPAC representations.
Data. 100K molecules were randomly chosen
from PubChem dataset after excluding the 10 mil-
lion molecules used for MM-Deacon pre-training.
SMILES and IUPAC strings are provided for each
molecule. We used average recall at K (R@1 and
R@5) to measure the percentage of the ground
truth that appears in the top K retrieved molecules.
Model. Pre-trained MM-Deacon was used for MM-
Deacon fingerprint generation, as shown in Fig.
3(b). As a result, each SMILES and IUPAC string
was encoded as MM-Deacon SMILES fingerprint
and IUPAC fingerprint respectively. Cosine simi-
larity between a query and molecules in the search
candidates was used to determine the ranking.
Results. Fig. 4 shows the outcomes of SMILES-to-
IUPAC and IUPAC-to-SMILES retrieval in terms
of recall. We not only performed retrieval directly
on the entire 100K molecules, but also reported
the results on smaller groups of molecules (100,
10K) to get a more thorough picture of the retrieval
performance. MM-Deacon gets a R@5 above 85%
for both types of cross-lingual retrieval even while
executing retrieval on 100K molecules. More-
over, Figs. 5 and 6 show an example of SMILES-
to-IUPAC retrieval and an example of IUPAC-to-
SMILES retrieval respectively.
Additional retrieval examples for scenarios
where the performance is difficult to be quantified,
such as retrieval queried by a free combination of3446
tokens and unilingual retrieval are included in Ap-
pendix C.
4.3 DDI prediction
The effectiveness of combining MM-Deacon finger-
prints with a task-specific classifier for supervised
learning was tested on a DDI prediction task. The
objective of this task is to predict whether or not
any two given drugs have an interaction.
Data. The DDI dataset (Zhang et al., 2017) used
here includes 548 drugs, with 48,584 known inter-
actions, and 101,294 non-interactions (may contain
undiscovered interactions at the time the dataset
was created). We obtained the SMILES and IU-
PAC names for each drug from PubChem. Strat-
ified 5-fold cross-validation with drug combina-
tion split was utilized. The evaluation metrics are
Area Under the ROC Curve (AUC), Area Under
the Precision-Recall Curve (AUPR), precision, and
recall, with AUPR serving as the primary metric
(Zhang et al., 2017).
Model. MM-Deacon fingerprints of paired drugs
are concatenated and fed into a multi-layer percep-
tron (MLP) network implemented by scikit-learn
(Pedregosa et al., 2011) for binary classification.
Three different types of fingerprints are used for
MM-Deacon: SMILES, IUPAC, and concatenated
SMILES and IUPAC fingerprints. The MLP has
one hidden layer with 200 neurons. ReLU activa-
tion and a learning rate of 10are used.
Results. As shown in Table 3, MM-Deacon out-
performs other methods in terms of AUPR, preci-
sion and recall, with the maximum AUPR obtained
when SMILES and IUPAC fingerprints were con-
catenated as input feature set. Ensemble models
(Zhang et al., 2017) included extra bioactivity re-
lated features in addition to drug structural proper-
ties. DPDDI (Feng et al., 2020) encoded molecule
graph with GNNs, from which latent features were
concatenated for pairs of drugs and ingested into a
deep neural network.3447
Table 4 shows the top 20 most potential interac-
tions predicted by MM-Deacon (concat) in the non-
interaction set (false positives), 13 out of which are
confirmed as true positives by DrugBank. While,
the number is 7/20 for ensemble models (Zhang
et al., 2017).
5 Discussions
After being pre-trained on 10 million molecules,
MM-Deacon showed outstanding knowledge trans-
fer capabilities to various downstream scenarios
(Fig. 3) where a pre-trained model could be used.
The competitive performance on seven molecu-
lar property prediction tasks from MoleculeNet
benchmark demonstrated the effectiveness of the
pre-trained MM-Deacon when adopting a network
fine-tuning scheme as shown in Fig. 3(a). The eval-
uation results of zero-shot cross-lingual retrieval
further revealed that MM-Deacon SMILES and IU-
PAC fingerprints shared a substantial amount of
mutual information, implying that an IUPAC name
can be used directly without first being translated
to SMILES format as chemists have done in the
past. The DDI prediction task showed that MM-
Deacon also allows directly using embeddings in
the joint cross-modal space as molecular finger-
prints for downstream prediction tasks, which is a
widely used strategy in cheminformatics.
MM-Deacon profited from the alignment of two
molecule languages with distinct forms of nomen-
clatures, as opposed to the baseline MLM- [CLS]
model, which was pre-trained on SMILES represen-
tation only. Furthermore, we looked at molecule-
level and token-level alignments of MM-Deacon to
untangle the outcome of cross-lingual contrastive
learning.
5.1 Molecule-level alignment
We used centered kernel alignment (CKA) (Ko-
rnblith et al., 2019) with RBF kernel to compare
representations between different layers. In Fig.
7(a), the representations of 6 Transformer layers
and the final projection layer were compared be-
tween MM-Deacon SMILES and IUPAC branches,
where the representations differ in shallow layers,
while reach a high level of alignment in deeper lay-
ers. In Fig. 7(b), both the MM-Deacon SMILES
branch and MLM- [CLS] model take SMILES as
the input, therefore the shallow layers have a high
alignment score, while the representation varies
as the network grows deeper. Fig. 7 shows that
MM-Deacon aligned SMILES and IUPAC repre-
sentations effectively, and that molecular represen-
tations trained with SMILES and IUPAC differs
from representations trained only on SMILES.
5.2 Token-level alignment
The cosine similarity matrix of MM-Deacon fin-
gerprints between tokens from the IUPAC corpus
and tokens from the SMILES corpus is shown in
Fig. 8. The table in Fig. 8 lists IUPAC tokens
expressed in SMILES language, and the heat map
demonstrates that there exists a good token-level
alignment between SMILES and IUPAC.3448
6 Conclusion
In this study, we proposed a novel method for multi-
lingual molecular representation learning that com-
bines mutual information from SMILES-IUPAC
joint training with a self-supervised contrastive loss.
We evaluated our approach for molecular property
prediction, zero-shot cross-lingual retrieval, and
DDI prediction. Our results demonstrate that the
self-supervised multilingual contrastive learning
framework holds enormous possibilities for chem-
ical domain exploration and drug discovery. In
future work, we plan to scale MM-Deacon pre-
training to larger dataset sizes, as well as investi-
gate the applicability of MM-Deacon to other types
of molecule languages.
Acknowledgements
We would like to thank Min Xiao and Brandon
Smock for some insightful discussions.
References344934503451A Data statistics
The distributions of SMILES and IUPAC sequence
lengths in the training set are shown in Fig. 9 in
log scale for the y axis.
Fig. 10 displays the top 20 most frequent al-
phabetic tokens in SMILES and IUPAC corpora.
Token C, which simply denotes a carbon atom, ap-
pears nearly 20% of the time in SMILES language.
On the other hand, the frequency of IUPAC tokens
is quite evenly distributed, with the prefixes methyl
andphenyl as well as the suffix ylfrom the alkyl
functional group being the top 3 most common
tokens.B Hyperparameter tuning
Table 5 lists the search space of hyperparameter tun-
ing for seven molecular property prediction tasks
from MoleculeNet benchmark for MM-Deacon and
MLM- [CLS] . We employed grid search to find
the best hyperparameters. Each task was optimized
individually.
C Extra zero-shot retrieval examples
An example of cross-lingual retrieval using a free-
form IUPAC query is shown in Fig. 11. The IUPAC
query thiolamide , a combination of tokens thiol
andamide , does not exist in the IUPAC corpus
(is not a substring of any IUPAC name). When
searching on top of MM-Deacon fingerprints, all of
the retrieved molecules have the features of atom S,
NandC=O . That is, the semantic meaning of the
query is captured.
In addition to cross-lingual retrieval, unilingual
similarity retrieval is also supported, while its per-
formance is difficult to be quantified. Figs. 12 and
13 show an example of SMILES-to-SMILES re-3452
trieval and IUPAC-to-IUPAC retrieval respectively
using MM-Deacon fingerprints.3453