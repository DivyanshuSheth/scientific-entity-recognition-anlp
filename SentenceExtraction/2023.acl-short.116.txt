
Yufei Li, Xiao Yu, Yanchi Liu, Haifeng Chen, Cong LiuUniversity of California, RiversideStellar CyberNEC Labs America{yli927,congl}@ucr.edu ,xyu@stellarcyber.ai ,{yanchi,haifeng}@nec-labs.com
Abstract
Jointly extracting entity pairs and their rela-
tions is challenging when working on distantly-
supervised data with ambiguous or noisy la-
bels. To mitigate such impact, we propose
uncertainty-aware bootstrap learning , which
is motivated by the intuition that the higher
uncertainty of an instance, the more likely
the model confidence is inconsistent with the
ground truths. Specifically, we first explore
instance-level data uncertainty to create an
initial high-confident examples. Such subset
serves as filtering noisy instances and facili-
tating the model to converge fast at the early
stage. During bootstrap learning, we propose
self-ensembling as a regularizer to alleviate
inter-model uncertainty produced by noisy la-
bels. We further define probability variance
of joint tagging probabilities to estimate inner-
model parametric uncertainty, which is used
to select and build up new reliable training in-
stances for the next iteration. Experimental
results on two large datasets reveal that our ap-
proach outperforms existing strong baselines
and related methods.
1 Introduction
Joint extraction involves extracting multiple types
of entities and relations between them using a sin-
gle model, which is necessary in automatic knowl-
edge base construction (Yu et al., 2020). One way
to cheaply acquire a large amount of labeled data
for training joint extraction models is through dis-
tant supervision (DS) (Mintz et al., 2009). DS
involves aligning a knowledge base (KB) with an
unlabeled corpus using hand-crafted rules or logic
constraints. Due to the lack of human annotators,
DS brings a large proportion of noisy labels, e.g.,
over 30% noisy instances in some cases (Mintz
et al., 2009), making it impossible to learn useful
features. The noise can be either false relations due
to the aforementioned rule-based matching assump-
tion or wrong entity tags due to limited coverage
over entities in open-domain KBs.Existing distantly-supervised approaches model
noise relying either on heuristics such as reinforce-
ment learning (RL) (Nooralahzadeh et al., 2019;
Hu et al., 2021) and adversarial learning (Chen
et al., 2021), or pattern-based methods (Jia et al.,
2019; Shang et al., 2022) to select trustable in-
stances. Nevertheless, these methods require de-
signing heuristics or hand-crafted patterns which
may encourage a model to leverage spurious fea-
tures without considering the confidence or uncer-
tainty of its predictions.
In response to these problems, we propose
UnBED —Uncertainty-aware Bootstrap learning
for joint Extraction on Distantly-supervised data.
UnBED assumes that 1) low data uncertainty in-
dicates reliable instances using a pre-trained lan-
guage model (PLM) in the initial stage, 2) model
should be aware of trustable entity and relation la-
bels regarding its uncertainty after training. Our
bootstrap serves uncertainty as a principle to miti-
gate the impact of noise labels on model learning
and validate input sequences to control the num-
ber of training examples in each step. Particularly,
we quantify data uncertainty of an instance accord-
ing to its winning score (Hendrycks and Gimpel,
2017) and entropy (Shannon, 1948). We define
averaged maximum probability that is estimated
by a joint PLM over each token in a sequence
to adapt previous techniques in joint extraction
scheme. Instances with low data uncertainty are
collected to form an initial subset, which is used
to tune the joint PLM tagger and facilitate fast
convergence. Then, we define parametric uncer-
tainty in two perspectives—inter-model and inner-
model uncertainty. The former is quantified by self-
ensembling (Wang and Wang, 2022) and serves as
a regularizer to improve model robustness against
noisy labels during training. The latter is captured
by probability variance in MC Dropout (Gal and
Ghahramani, 2016) for selecting new confident in-
stances for the next training iteration. Such two-1349fold model uncertainties reinforce with each other
to guide the model to iteratively improve its robust-
ness and learn from reliable knowledge.
2 Related Work
Joint Extraction Methods Joint extraction de-
tects entities and their relations using a single
model, which effectively integrates the informa-
tion from both sources and therefore achieves bet-
ter results in both subtasks compared to pipelined
methods (Zheng et al., 2017). For example, uni-
fied methods tag entities and relation simultane-
ously, e.g., (Zheng et al., 2017) proposes a novel
tagging scheme which converts joint extraction to
a sequence labeling problem; (Dai et al., 2019)
introduces query position and sequential tagging
to extract overlapping relations. Such methods
avoid producing redundant information compared
to parameter-sharing neural models (Miwa and
Bansal, 2016; Gupta et al., 2016), and require no
hand-crafted features that are used in structured
systems (Yu et al., 2020).
To address the challenge of learning from DS,
pre-trained transformers (e.g., BERT, GPT-2) have
gain much attention. They model strong expressive
context-aware representations for text sequence
through multiple attention layers, and achieve state-
of-the-art performance on various NLP tasks (Rad-
ford et al., 2019; Devlin et al., 2019; Li et al., 2022).
They can be cheaply fine-tuned to solve different
downstream tasks including NER and RC. Specifi-
cally, BERT is trained on large English corpus us-
ing masked language modeling. The multi-head at-
tention weights indicate interactions between each
pair of words and its hidden states integrate seman-
tic information of the whole sentence, which are
used to decode different tagging results.
Uncertainty Methods Uncertainty generally
comes from two sources—aleatoric uncertainty and
epistemic uncertainty. The former is also referred
to as data uncertainty, describing noise inherent
in the data generation. Methods mitigating such
uncertainty include data interpolation (Dong et al.,
2018), winning score, and temperature scale (Guo
et al., 2017). The latter is also called model uncer-
tainty, describing whether the structure choice and
model parameters best describe the data distribu-
tion. One main solution to mitigate model uncer-
tainty is Bayesian Neural Network (BNN) (Klein
et al., 2017) that puts a prior distribution on its
weights. To save computational cost, Monte Carlo
dropout is proposed as an approximation of vari-
ational Bayesian inference (Gal and Ghahramani,
2016), realized by training models with dropout
layers and testing with stochastic inference to
quantify probability variance. Besides BNN, self-
ensembling (Wang and Wang, 2022) which mea-
sures the outputs variance between models with
the same architecture has been shown effective to
reduce parametric uncertainty across models.
3 Joint Extraction Model
Tagging Scheme For an input sequence X=
{x, ..., x}, we tag nsequences according to dif-
ferent query position pfollowing (Dai et al., 2019).
Ifpis the start of an entity (query entity e), the
sequence is an instance. The entity type is labeled
atpand other entities ewhich have relationship
with the query entity are labeled with relation types
re. The rest of tokens are labeled “O” (Outside),
meaning they do not correspond to the query entity.
Accordingly, we convert joint extraction into a to-
ken classification task and extract relation triplets
{e, re, e}in each instance, as shown in Figure 1.
Position-Attentive Encoder we use BERT (De-
vlin et al., 2019) to encode a sentence Xinto token-
level representations h={h, ..,h}, where h∈
Ris ad-dimensional vector corresponding to the
i-th token in X. For each query p, self-matching is
applied to calculate the position-attention a∈R
between token at pand each token at target posi-
tiont, which compares the sentence representations
against itself to collect context information (Tan
et al., 2018). The produced position-aware repre-
sentation c∈Ris an attention-weighted sen-
tence vector c=ah. Finally, we concatenate
handcto generate position-aware and context-1350aware representations u= [h|c].
CRF Decoder (Lafferty et al., 2001) For each
position-aware representation u, we first learn a
linear transformation z=W u∈Rto repre-
sent tag scores for the t-th token. Here Cis the
number of distinct tags. For an instance with la-
belsy={y, ..., y}, the decoding score s(z,y)
is the sum of transition score from tag yto tag
yplus the input score z. The conditional prob-
ability p(y|z)is the softmax over s(z,y)for all
possible label sequences y. We maximize the log-
likelihood of correct tag sequences during training
L=/summationtextlogp(y|z).
4Uncertainty-Aware Bootstrap Learning
Motivation One of the main challenges in boot-
strap learning is to evaluate the “correctness” of a
labeled instance. We consider this problem from an
uncertainty perspective and assume instances with
lower uncertainty are more likely to be correctly
labeled. In this section, we first propose instance-
level data uncertainty which is used to filter noisy
examples and build an initial subset. Then, we
introduce our two-fold model uncertainties which
helps iteratively mitigate DS effect and build up
trustable examples during bootstrap learning.
4.1 Data Uncertainty
Presenting examples in an easy-to-hard order at dif-
ferent training stages can benefit models (Platanios
et al., 2019; Zhou et al., 2020), we propose data
uncertainty as a way to quantify the “hardness” of
an instance. To better estimate the data uncertainty,
we use pre-trained language models (PLMs) to gen-
erate tag probability for each token in a sequence.
Our intuition is that higher uncertain inputs are
“harder” to be generated by a PLM, as it already
has rationales of language. Accordingly, we pro-
pose two data uncertainties, which can be used
individually or combined together:
Winning Score (WS) The maximum softmax
probability reflects data uncertainty of an input
(Hendrycks and Gimpel, 2017). Given an input in-
stanceI={x, ..., x}, we define data uncertainty
u(I)as the minus averaged token classification
winning score:(1)
Entropy Shannon entropy (Shannon, 1948) is
widely used to reflect information uncertainty. Wepropose data uncertainty u(I)as the averaged
token classification entropy:
(2)
We filter out examples with high uncertainty
scores and build an initial subset with “simple” ex-
amples. At the early training stage, a model is not
aware of what a decent distribution P(y|x)should
be, thus data uncertainty facilitates it to converge
fast by tuning on a fairly “simple” subset.
4.2 Model Uncertainty
In our bootstrap learning, we define model uncer-
tainty, i.e., epistemic uncertainty (Kendall and Gal,
2017), to measure whether model parameters can
best describe the data distribution following (Zhou
et al., 2020). A small model uncertainty indicates
the model is confident that the current training
data has been well learned (Wang et al., 2019).
We adopt Monte Carlo Dropout (Gal and Ghahra-
mani, 2016) to approximate Bayesian inference
which captures inner-model parametric uncertainty.
Specifically, we perform Kforward passes through
our joint model. In each pass, part of network neu-
ronsθare randomly deactivated. Finally, we yield
Ksamples on model parameters { ˆθ, ...,ˆθ}. We
use the averaged token classification Probability
Variance (PV) (Shelmanov et al., 2021) over all
tags for instance I:(3)
where Var[.]is the variance of distribution over
theKpasses following the common settings in
(Dong et al., 2018; Xiao and Wang, 2019). Accord-
ingly, model is aware of its confidence over each
instance and how likely the label is noisy.
4.3 Training Strategy
Uncertainty-Aware Loss Besides MC Dropout
which measures parametric uncertainty within
a model, we also consider mitigating paramet-
ric uncertainty between models to stabilize the
weights during training. Specifically, we use self-
ensembling (He et al., 2020; Wang and Wang,
2022) to calculate the loss between the same mod-
els to improve model robustness and reduce the
label noise effect on model performance.1351Algorithm 1 Bootstrap Learning
Input: Original dataset D={(I, y)}, two
joint models f,fwith parameters θ,θ;Compute data uncertainty u(I)for each in-
stance IinD;Initial dataset C ← Select data pairs (I, y)
such that u(I)< τfromD;forepoch e= 1, ...do Train f,fonCusing Eq. (5); Calculate model uncertainty u(θ)onD;C ← Select data pairs (I, y)such that
u(I;θ)< τfromD;
We create another joint model with identical
framework, e.g., architecture, loss functions, hy-
perparameters, and compute a self-ensemble loss
Lto minimize the difference between two outputs
from the two models regarding the same inputs:
L=/summationdisplay
KL(f(I;θ), f(I;θ)) (4)
where KL(.)is the Kullback-Leibler divergence
between two probabilistic distributions, θ,θde-
note the parameters of first and second models. We
formulate our final uncertainty-aware objective L
as the sum of CRF and self-ensemble loss:
L=L+αL (5)
where αdenotes the weight of self-ensembling,
andLmeans the token classification loss.
Bootstrap Learning Procedure To mitigate the
DS effect on model performance, we propose a two-
fold bootstrap learning strategy (see Algorithm 1).
Specifically, we first apply data uncertainty to fil-
ter “harder” examples and redistribute a reliable
initial training data M. Then, we iteratively feed
examples following an easy-to-hard order to the
model. In each training iteration, we regularize the
joint model with self-ensembling loss to reduce the
impact of noisy labels on model parameters. Then
we use probability variance to select new confident
training instances Dthat can be explained by the
model as the next training inputs. The more certain
examples are selected, the more likely the model
will learn beneficial information and will converge
faster. We repeat the above procedure until the F1
score on the validation set converges.5 Experiments
5.1 Setup
We evaluate the performance of UnBED on two
datasets, NYT and Wiki-KBP. The NYT (Riedel
et al., 2010) dataset collects news from New York
Times and its training data is automatically labeled
by DS. We use the revised test dataset (Jia et al.,
2019) that is manually annotated to ensure quality.
The Wiki-KBP (Ling and Weld, 2012) dataset col-
lects articles from Wikipedia. Its training data is
labeled by DS (Liu et al., 2017), and the test set is
manually annotated (Ellis et al., 2013).
We compare UnBED with the following base-
lines: ARNOR (Jia et al., 2019), a pattern-based
method to reduce noise for distantly-supervised
triplet extraction. PURE (Zhong and Chen, 2021),
a pipeline approach that uses pre-trained BERT
entity model to first recognize entities and then
employs a relation model to detect underlying re-
lations. FAN (Hao et al., 2021), an adversarial
method including a transformers encoder to reduce
noise for distantly-supervised triplet extraction.
Evaluation We evaluate the extracted triplets for
each sentence based on Precision (Prec.), Recall
(Rec.), and F1. A triplet {e, re, e}is marked
correct if the relation type re, two entities e,eare
all correct. We build a validation set by randomly
sampling 10% sentences from the test set.
Implementation Details We use Hugging Face
bert-large-uncased (Devlin et al., 2019) pre-trained
model as backbone. For ARNOR, the hidden vector
size is set to 300. In regularization training, we find
optimal parameters αas 1 for both datasets. We
implement UnBED and all baselines in PyTorch,
with Adam optimizer, initial learning rate 10,
dropout rate 0.1, and batch size 8. For initial subset
configuration, we choose data uncertainty threshold
0.5. For bootstrap learning, an empirical model
uncertainty threshold is set to 0.6 with the best
validation F1.
5.2 Overall Results
As shown in Table 1, UnBED significantly out-
performs all baselines in precision and F1 met-
ric. Specifically, UnBED achieves 8% F1 im-
provement on NYT (3% on Wiki-KBP) over
denoising approaches—ARNOR and FAN. Our
approach also outperforms baselines using pre-
trained transformers (PURE and FAN), showing
that uncertainty-aware bootstrap learning effec-
tively reduces the impact of noisy labels.1352MethodNYT Wiki-KBP
Prec. Rec. F1 Prec. Rec. F1
ARNOR (Jia et al., 2019) 0.588 0.614 0.600 0.402 0.471 0.434
PURE (Zhong and Chen, 2021) 0.536 0.664 0.593 0.395 0.433 0.413
FAN (Hao et al., 2021) 0.579 0.646 0.611 0.391 0.467 0.426
UnBED-WS 0.662 0.730 0.694 0.429 0.501 0.462
UnBED-Entropy 0.651 0.741 0.693 0.422 0.509 0.461
5.3 Further Analysis
We analyze the functionality of different compo-
nents in Figure 2. We observe that both the entropy-
PV and vanilla-PV outperform the baseline (joint
model directly trained on the original DS dataset)
in terms of F1 (5 ∼7% increase), demonstrating
the effect of filtering noisy labels and selecting
trustable instance using probability variance. Be-
sides, self-ensembling further enhances the perfor-
mance in later training stage (2 ∼4 F1 increase),
proving that mitigating the inter-model uncertainty
benefits model robustness against noisy labels.
6 Conclusions
In this paper, we propose a novel uncertainty-
aware bootstrap learning framework for distantly-
supervised joint extraction. Specifically, we define
data uncertainty in generally token classification to
filter out highly-error-prone instances and build an
initial high-confident subset, which is used to tune
the joint extraction model for fast convergence. We
then propose a two-fold bootstrap learning proce-
dure which iteratively mitigates the DS impact on
model robustness and selects new trustable train-
ing instances. Experimental results on two bench-
mark datasets show that UnBED significantly out-performs other denoising techniques.
Limitations
In this work we propose an uncertainty-aware
bootstrap learning framework for joint extraction.
Though it achieves state-of-the-art performance
compared to other denoising techniques, UnBED
requires large training resources considering the
ensemble loss calculated between two large PLMs
and the probability variance calculated on the PLM
joint extraction model. In our future work, we hope
to incorporate pruning techniques during training to
improve the efficiency. We will also consider more
complex relations between entities, e.g., relations
beyond the sentence boundary, to fit in real-world
information extraction scenarios.
Acknowledgements
This work was supported by NSF CNS 2135625,
CPS 2038727, CNS Career 1750263, and a Darpa
Shell grant.
References1353135413551356ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7
/squareA2. Did you discuss any potential risks of your work?
We study open-domain information extraction for researches in this area
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
No response.1357/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 5
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
No response.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 5
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.1358