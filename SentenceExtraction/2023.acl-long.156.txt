
Ziqing Yang, Yiming Cui, Xin Yao, Shijin WangState Key Laboratory of Cognitive Intelligence, iFLYTEK Research, Beijing, ChinaResearch Center for SCIR, Harbin Institute of Technology, Harbin, ChinaiFLYTEK AI Research (Central China), Wuhan, China{zqyang5,ymcui,xinyao10,sjwang3}@iflytek.comymcui@ir.hit.edu.cn
Abstract
Pre-trained language models achieve superior
performance but are computationally expensive.
Techniques such as pruning and knowledge dis-
tillation have been developed to reduce their
sizes and latencies. In this work, we propose a
structured pruning method GRAIN (Gradient-
based Intra-attention pruning), which performs
task-speciﬁc pruning with knowledge distilla-
tion and yields highly effective models. Differ-
ent from common approaches that prune each
attention head as a whole, GRAIN inspects and
prunes intra-attention structures, which greatly
expands the structure search space and enables
more ﬂexible models. We also propose a gra-
dient separation strategy that reduces the inter-
ference of distillation on pruning for a better
combination of the two approaches. Experi-
ments on GLUE, SQuAD, and CoNLL 2003
show that GRAIN notably outperforms other
methods, especially in the high sparsity regime,
and achieves 6∼7×speedups while main-
taining 93%∼99% performance. Under ex-
treme compression where only 3%transformer
weights remain, the pruned model is still com-
petitive compared to larger models.
1 Introduction
Transformer-based (Vaswani et al., 2017) pre-
trained language models (PLMs) have achieved
great success and become the backbones of various
natural language processing tasks. However, PLMs
are computationally expensive and slow in infer-
ence due to their large sizes, which limits their ap-
plications in real-world scenarios. Hence, a grow-
ing interest has been in developing compression
and acceleration methodologies for PLMs.
A common approach to model compression is
structured pruning, which compresses the model
by removing groups of consecutive parameters,
namely the pruning units. In applying structuredFigure 1: A comparison of GRAIN and other distillation
and pruning methods on MNLI-m development set at
different model sizes. More details are in Section 5.
pruning on PLMs, recent works have investigated
removing units such as hidden dimensions in feed-
forward layers, attention heads in the multi-head
attention (Michel et al., 2019; Li et al., 2022), and
coarse-grained units such as multi-head attention
layers and feed-forward layers (Xia et al., 2022).
However, these pruning units only span a small
space of model structures and limit the exploration
for better structures. For example, in the pruning of
BERT(Devlin et al., 2019), which contains 144
attention heads, the possible choices of attention
heads for the pruned model are limited. Block
Pruning (Lagunas et al., 2021) extends pruning
units by considering blocks in the weight matrices,
but Block Pruning is not a fully structured pruning
method and can not achieve large speedups.
In this work, we propose GRAIN (Gradient-
based Intra-attention pruning), a structured prun-
ing method that prunes PLMs with ﬁner pruning
units. In the following, we present the method from
three aspects: pruning units, pruning algorithm,
and training objectives.
Pruning Units Unlike attention heads pruning
where the pruning unit is a single head, we propose
intra-attention pruning, which inspects and prunes2775the structures inside attention heads. Intra-attention
pruning greatly expands the search space of model
structures, making the resulting models more likely
to ﬁnd better structures. However, directly apply-
ing intra-attention pruning yields fragmented mod-
els, i.e., models with many small heads. The frag-
mented models have relatively large latencies on
devices like GPUs. To overcome the shortcom-
ing, we introduce structure regularization, which
encourages prioritizing speciﬁc units for pruning.
Structure regularization helps generate more regu-
lar structures and achieve lower latencies.
Pruning Algorithm Pruning algorithms decide
which units to be removed. We adapt the gradient-
based pruning algorithm (Michel et al., 2019) for
intra-attention pruning. Gradient-based pruning
is a light-weighted method that estimates the im-
portance of the pruning units with gradient-based
scores and then prunes the least important ones.
In addition, we conduct the pruning in an itera-
tive manner (Zhu and Gupta, 2018), i.e., the model
is gradually pruned during ﬁne-tuning. The itera-
tive approach has been employed in combination
with pruning algorithms such as Movement Prun-
ing (Sanh et al., 2020) and Magnitude Pruning (Zhu
and Gupta, 2018), but few works have combined
it with gradient-based pruning. We ﬁnd that itera-
tive gradient-based pruning is especially effective
despite its simplicity.
Training Objectives As another common ap-
proach to model compression, knowledge distil-
lation offers highly effective training objectives
(Jiao et al., 2020). Pruning with distillation ob-
jective shows improved performance (Sanh et al.,
2020; Xia et al., 2022). However, in gradient-based
pruning, the distillation objectives may disturb the
estimation of importance scores. We propose a
gradient separation strategy that uses different gra-
dients for model optimization and importance score
estimation. We show that this method leads to bet-
ter performance.
GRAIN performs task-speciﬁc pruning without
additional pre-training or data augmentation. In the
experiments, we compare GRAIN with strong prun-
ing and distillation baselines on GLUE, SQuAD,
and CoNLL 2003. GRAIN notably outperforms the
comparable methods in the high-sparsity regime.
A demonstration of the results on MNLI is shown
in Figure 1. While keeping 5% parameters in trans-
formers, GRAIN maintains 93%∼99% perfor-
mance of BERTand6∼7×speedups acrossdifferent tasks. Furthermore, GRAIN still achieves
competitive results even under extreme compres-
sion where only 3%transformer weights remain.
2 Related Work
A growing number of works have been devoted to
the compression and acceleration of PLMs. Most
of the works have combined multiple techniques.
Knowledge Distillation (Hinton et al., 2015) is
a training technique that trains a student model to
mimic the outputs and intermediate representations
of the teacher model (Sun et al., 2019). DistilBERT
(Sanh et al., 2019) and TinyBERT (Jiao et al., 2020)
are both small BERT-like models distilled with gen-
eral and task-speciﬁc distillation. MobileBERT
(Sun et al., 2020) and KroneckerBERT (Tahaei
et al., 2022) have designed novel structures for
student models. Chen et al. (2021) proposes to
extract a subnetwork from the teacher and then per-
form distillation. AutoTinyBERT (Yin et al., 2021)
combine distillation with neural architecture search
to ﬁnd optimal hyperparameters. DynaBERT (Hou
et al., 2020) apply task-speciﬁc distillation and can
ﬂexibly adjust the model size. In this work, we only
apply task-speciﬁc distillation, which consumes
fewer resources.
Structured Pruning on PLMs remove differ-
ent types of units from the models, like attention
heads (Michel et al., 2019), FFN hidden dimen-
sions (Liang et al., 2021), blocks of weights (Lagu-
nas et al., 2021), MHA layers or FFN layers (Xia
et al., 2022). Many works combine pruning with
other methods. Wang et al. (2020) presents a struc-
tured pruning approach with low-rank factorization
of weight matrices. McCarley (2019) and Xia et al.
(2022) apply pruning with knowledge distillation.
In this work, we apply matrix factorization on the
embeddings and use distillation and pruning to re-
duce the size of transformers.
Unstructured Pruning removes each weight in-
dividually based on its magnitude (Han et al., 2015;
Zhu and Gupta, 2018; Gordon et al., 2020), or the
score computed by ﬁrst-order (Sanh et al., 2020;
Louizos et al., 2017) or second-order (Kurtic et al.,
2022) method. Unstructured pruning yields higher
sparsity models but is hard to speed up without
specialized devices for sparse matrix operations. In
this work, we only consider structured pruning.
Besides model compression, another group of
acceleration methods is dynamic inference, where
the computation cost is determined at test time (Fan2776et al., 2020; Liu et al., 2020; Xin et al., 2020). Liu
et al. (2021) and Shen et al. (2022) have proposed
to integrate model compression with dynamic in-
ference. We do not consider dynamic inference in
this work and leave it for future work.
3 Preliminaries
3.1 Transformers
A Transformer block (Vaswani et al., 2017) is
mainly composed of a multi-head attention (MHA)
layer and a feed-forward network (FFN) layer.
LetX∈Rbe the input sequence, where
nis the length, and dis the hidden size. An
attention head is parameterized by the matrices
W,W,W,W∈R. Its output is
Att(X) = softmax/parenleftBig
QK/√
d/parenrightBig
VW,(1)
wheredis head size, and iis the head index. An
MHA layer contains N=d/dattention heads
MHA( X) =/summationdisplayAtt(X). (2)
Following the MHA layer is the feed-forward net-
work layer. It consists of two linear layers and a
GeLU activation (Hendrycks and Gimpel, 2016)
FFN(X) =GeLU (X·W)·W, (3)
where W∈R,W∈R, anddis the
intermediate hidden size. Typically d>d.
A transformer block contains other components,
such as LayerNorm and residual connection, but
they only take up a few parameters.
3.2 Gradient-based Pruning
Gradient-based pruning (Michel et al., 2019) de-
ﬁnes the importance score of a pruning unit was
the variation of the loss with respect to the unit:
IS(w) =E/vextendsingle/vextendsingle/vextendsingle/vextendsingle∂L(x)
∂ww/vextendsingle/vextendsingle/vextendsingle/vextendsingle, (4)
whereXis the data distribution. The term in the
absolute value is the ﬁrst-order Taylor approxima-
tion of the lossLaroundw= 0. To apply (4)in
PLM pruning, wshould be set accordingly. For
example, by setting wtoW, Equation (4)gives
the importance score of the head h; by settingwtothei-th row of W, Equation (4)gives the impor-
tance score of the i-th FFN hidden dimension. A
lower importance score implies that the loss is less
sensitive to the unit. The pruning units are sorted
and then pruned in the order of increasing scores.
4 Methodology
GRAIN performs task-speciﬁc intra-attention prun-
ing together with knowledge distillation. The
overview of GRAIN is depicted in Figure 2. Fol-
lowing previous works, we only include the en-
coder in counting the model size unless otherwise
speciﬁed. We refer to the size of the pruned model
relative to the unpruned model as model density :
model density =SizeOf( pruned model )
SizeOf( original model ).
Sparsity is equal to one minus model density.
4.1 Intra-attention Pruning
4.1.1 Intra-attention Pruning Units
FFN hidden dimensions and attention heads are
common pruning units in PLM pruning studies.
These pruning units have been treated as atomic in
structured pruning. However, attention heads in-
clude ﬁner pruning units and are not really atomic.
Equation (2)shows that the output of an MHA
layer is the sum of individual heads, so different
heads can be pruned independently. To be spe-
ciﬁc, We can remove the rows of the matrices
W,W,W,Wto reduce head size. Fur-
ther, from Equation (1), we see that the output di-
mensions of W,Wand the input dimensions
ofW,Wcan be different. It gives another
freedom to set the dimensions of attention heads.
Based on the above observation, we introduce
two kinds of intra-attention pruning units: query
units, namely the rows of W,W; and value
units, namely the rows of W,W. We keep
FFN hidden dimensions but discard attention heads
as the pruning units since the intra-attention prun-
ing units are more structurally fundamental. Each
pruning unit takes 2dparameters. The new set of
pruning units greatly expands the structure space.
In the actual implementation (Wolf et al., 2020),
the parameters of all heads in an MHA layer
are gathered and stored in four large matrices
W,W,W,W∈R. The parameters
of thei-th head are stored in the rows (i,i+d).
We prune query and value units from large matri-
ces by removing corresponding rows. The pruning
units are illustrated in the right part of Figure 2.2777
4.1.2 Structure Regularization
Since intra-attention pruning removes the units in-
side attention heads, it tends to generate models
with many small heads of different sizes, but the
total number of heads can still be large. We refer to
this kind of structure as fragmented (see the upper
panel in Figure 6 for an example). The fragmented
structure has low efﬁciency on devices like GPUs
since there are still many attention modules left in
the model, and these heads are hard to parallelize.
To remedy this, we introduce Structure Reg-
ularization (StructReg for short) to encourage
generating less fragmented structures. Intuitively,
to avoid small heads, the pruning process should
ﬁrst prune the units in the small heads and make
them empty, which can then be safely removed.
To be general, we deﬁne D(M,W)as the den-
sity of a set of pruning units Win moduleM,
i.e., the ratio of the remaining units in M. The
regularized importance score of a unit w∈W is:
IS(w) =IS(w)·tanh(D(M,W)/α),(5)
whereαis the regularization strength. The lower
the density of the units in M, the lower the reg-
ularized scores of the units. Hence, the units in
low-density modules will be pruned with priority
until all the units in Mhave been pruned, leaving
fewer low-density modules in the pruned model.
StructReg can be applied on different levels by
choosing different Ms andWs. We apply it to
intra-attention structures. We set Mto each atten-
tion head andWto the value units in M. Headswith fewer value units will be pruned with priority
until empty, resulting in fewer small heads.
4.2 Knowledge Distillation
Distillation Objectives Knowledge distillation
provides effective objectives for transferring knowl-
edge from a large model to a small model. The
most simple distillation objective involves a cross-
entropy loss between the student’s and the teacher’s
prediction probabilities
L=p·logp, (6)
whereTandSdenote teacher andstudent respec-
tively, and p=softmax (z/τ)is the scaled proba-
bility with temperature τand logits z. By integrat-
ing logits distillation with hidden layer representa-
tion distillation (Jiao et al., 2020; Sun et al., 2020),
the performance of knowledge distillation can be
further improved:
L =/summationdisplayMSE(HW,H),(7)
whereIis the set of layer index pairs, H(i>0)
is the hidden states from the i-th transformer block
(His the output from the embedding layer), and
Wis a trainable linear mapping. We employ the
sum ofLandL as the total loss.
Gradient Separation When applying distilla-
tion with gradient-based pruning, the hidden layer
matching lossL should be treated carefully.2778In gradient-based pruning, the units are pruned
based on how signiﬁcantly they affect the model
predictions. Thus, the importance score should be
calculated solely from the cross-entropy loss, and
we should avoid the gradients from other losses like
L affecting the estimation of the importance
scores. Therefore, we propose to use the gradient
fromLfor model optimization and importance
score computation, while using the gradient from
L only for model optimization. We call this
strategy gradient separation (GS). The gradient
ﬂows of different losses are illustrated in Figure 2.
4.3 Iterative Gradient-based Pruning
Iterative Pruning Similar to Sanh et al. (2020),
we take an iterative approach to prune the model,
i.e., the model size is gradually reduced during
ﬁne-tuning. We denote the total training steps as
Nand the current step as i. The model is pruned
to the density s(t)at every step, where s(t)is the
density scheduler as a function of the training per-
centaget=i/N∈[0,1]. We will give the exact
form ofs(t) shortly. Notice that in the standard
gradient-based pruning, the importance score is es-
timated from all the examples in the dataset X(see
Equation (4)). It would be impractical to estimate
the score at every step. Therefore we deﬁne an
exponentially smoothed importance score IS(w)
which can be computed efﬁciently during training
and used for pruning at step i:
IS(w) =β·IS(w) + (1−β)·IS(w),(8)
where IS(w)is the importance score of the pruning
unitwcalculated with a single batch at step i, and
βis the smoothing factor. The smoothed score
avoids the large variance and leads to more stability.
Equation (8)can also be applied on the regularized
score simply by replacing IS (w)with IS(w).
Scheduling Following Zhu and Gupta (2018), we
use a cubic density scheduler s(t)


1 0 ≤t<p
s+ (1−s)(1−)p≤t≤p
s p<t≤1.
The complete process can be divided into three
stages, as depicted in Figure 3. The ﬁrst stage is
the warm-up stage. We train the student model for
Npsteps with the distillation objective, where 0<
p<1is a hyperparameter. In the second stage,
we gradually prune the model with distillation for
N(p−p)steps. The model density sdecreases
from the initial density ( 100% ) to the target density
sfollowing the schedule. In the last stage, the
model structure is ﬁxed, and we continually train
the model with distillation to recover performance
(Sanh et al., 2020; Zhu and Gupta, 2018). The
three stages take place consecutively, and the whole
process is done in a single run of ﬁne-tuning.
4.4 Embedding Factorization
The pruning mentioned above reduces the param-
eters in the transformers, while another large frac-
tion of the parameters stored in the word embed-
ding matrix is untouched. We apply singular value
decomposition (SVD) to reduce the embedding
size. SVD decomposes the word embedding ma-
trixE∈RasE=UΣV, whereqis the vo-
cabulary size and dis the hidden size, U∈R,
V∈RandΣis a diagonal matrix composed of
singular values. Ecan be approximated as Eby
selecting top rsingular values and corresponding
rrows from UandV
E≈E=UΣV=WV, (9)
where W∈RandU∈R. The original
embedding Eis now replaced by WandV. The
embedding size is reduced from qdto(q+d)r.
Embedding factorization has little effect on la-
tencies but signiﬁcantly reduces model sizes. Some
works (Xia et al., 2022; Lagunas et al., 2021) do not
prune embeddings. We also conduct experiments
without embedding factorization for comparison.
We name this setting as GRAIN w/o EF .
5 Experiments
5.1 Experiment Setup
Datasets We evaluate our approach on machine
reading comprehension SQuAD 1.1 (Rajpurkar2779
et al., 2016), named entity recognition CoNLL
2003 (Tjong Kim Sang and De Meulder, 2003),
and four classiﬁcation tasks (SST-2, QNLI, MNLI,
and QQP) that have relative large training data from
GLUE benchmark (Wang et al., 2018). Details are
summarized in Appendix B. We report the results
on the development sets of GLUE and SQuAD and
the results on the test set of CoNLL 2003.
Training Settings We use BERTas the back-
bone model.We ﬁrst ﬁne-tune the teachers for
each task, then train and prune the students follow-
ing the procedure in Section 4.3. The target model
densities range from 3%to20%. We list the model
size and the total size (with embeddings and classi-
ﬁers) for reference. We report the mean score of 3
runs with different random seeds. See Appendix A
for training details and costs.
Baselines We compare our proposed methodwithCoFi (Xia et al., 2022), Block Pruning (La-
gunas et al., 2021), TinyBERT(Jiao et al., 2020)
andDynaBERT (Hou et al., 2020). We also list
the results of AutoTinyBERT (Yin et al., 2021)
andMobileBERT (Sun et al., 2020). However,
they are not directly comparable to GRAIN since
they have been distilled from different teacher mod-
els and pre-trained extensively, consuming much
more computation. Following Xia et al. (2022), we
re-implement TinyBERTand DynaBERT without
task-speciﬁc data augmentation for a fair compari-
son. We also re-implement CoFi and Block Pruning
with their public code, and choose Hybrid Filled
approach as the Block Pruning baseline. We use the
same teachers in training for GRAIN, TinyBERT,
CoFi, and Block Pruning.
5.2 Main Results
In Figure 1 and Figure 4, we show the scores
of GRAIN and the baseline methods on various
downstream tasks with model densities ranging2780
from 3%to20%. Table 1 summarizes the de-
tailed results at densities 5% and 3%.We see
that GRAIN outperforms baselines in the majority
of tasks on a wide range of model sizes. GRAIN
outperforms TinyBERTand Block Pruning on
all tasks and outperforms CoFi except on SST-2
at relatively high density. Especially, in the low-
density regime, GRAIN exhibits notable advan-
tages over other methods. Under extreme com-
pression at density 3%, GRAIN (2.6M) can match
TinyBERT (4.7M) and CoFi (4.7M) on most tasks,
despite having fewer parameters. In addition, com-
pared to MobileBERT and AutoTinyBERT, which
require general pre-training and use different teach-
ers than GRAIN’s, although not directly compa-
rable, GRAIN shows promising results with less
computation.
In Table 1, we show the results of GRAIN with-
out embedding factorization ( GRAIN w/o EF ).
One can see that the pruned models do not always
beneﬁt from having large embeddings. On SQuAD,
the factorized embedding leads to improved perfor-
mance, while on SST-2, a large embedding matrix
is better. However, the gaps at model density 5%
are closer than those at model density 3%, indicat-
ing that embedding factorization has more minor
impacts on larger pruned models.
We also measure the latency of GRAIN and ﬁnd
that GRAIN achieves competitive speedups when
compared with other methods. Please refer to Ap-
pendix D for more details.
To summarize the above, GRAIN is efﬁcient
and effective for compressing pre-trained language
models on a wide range of downstream tasks.
5.3 Ablation Study
We apply ablations on GRAIN w/o EF to study the
effect of each component, as listed in Table 2.
Firstly, The impact of removing StructReg varies
depending on the task, with performance either
increasing or decreasing. We defer the detailed
discussion on StructReg to Section 5.4.
Secondly, we remove gradient separation (Grad-
Sep), so the importance scores are inﬂuenced by
gradients from both L andL. The perfor-
mance on different tasks drops more or less, and
SQuAD is most notably affected. The results indi-
cate that the gradients from the hidden layer loss
L have an impact on the pruning process, and
it would be more beneﬁcial to exclude it from the
estimation of importance scores.
Thirdly, we remove the hidden layer loss L ,
so knowledge distillation only optimizes the cross-
entropy objective L. The performance drops
signiﬁcantly, showing the necessity to use both
objectives for obtaining effective pruned models.
Lastly, we investigate if gradient-based pruning
is necessary and effective. To ablate gradient-based
pruning, we generate random scores instead of
gradient-based scores at each pruning step and keep
all other settings unchanged, so the models are ran-
domly pruned. The results are displayed in the last
line in Table 2. The random structures resulted in
inferior results, proving the superiority of the struc-
tures found by gradient-based pruning. Thus both
pruning and distillation are crucial components.
5.4 Analysis
We ﬁrst compare the effects of different pruning
units. Then we look into the structures of pruned
models to better understand our method.
Attention Heads Pruning Intra-attention prun-
ing allows larger structure search space and more
ﬂexible models, but is intra-attention pruning more2781
effective compared to attention heads pruning in
practice? To answer the question, we conduct com-
parative attention heads pruning experiments.
We follow the GRAIN procedure, except for set-
ting the pruning units to be attention heads and
FFN hidden dimensions. The structure regular-
ization strength is set to 0, and the target model
density is set to 5%. Since each attention head
has more parameters than each FFN hidden dimen-
sion, the importance scores of attention heads and
FFN hidden dimensions are not directly compara-
ble, so attention heads and FFN hidden dimensions
can not be globally sorted and pruned.Hence,
we sort and prune the two kinds of units indepen-
dently and we have the freedom to set their den-
sities as long as the model density is ﬁxed to 5%.We experiment with ﬁve groups of (FFN,Heads )
density,and the results are shown in Table 3. In-
tra+FFN denotes pruning with intra-attention units.
Heads+FFN denotes pruning with attention heads.
Heads+FFN reaches its best performance when its
(FFN, Heads) density is close to the (FFN, Heads)
density of Intra+FFN, but Intra+FFN still outper-
forms Heads+FFN at different (FFN, Heads) densi-
ties. The results imply that intra-attention pruning
is more effective than attention heads pruning.
Model Structures As we stated previously, intra-
attention pruning tends to yield fragmented struc-
tures, which hinder running efﬁciency. We apply
structure regularization (StructReg) to encourage
generating models with less fragmented units. To
get an intuitive understanding, Figure 6 shows the
structures of the models pruned with and without
StructReg at model density 5%on QNLI.We ﬁrst
notice that with intra-attention pruning, attention
heads take more diverse structures since the num-
ber of query and value units can differ. The model
pruned without StructReg holds 95 attention heads,
where most heads contain only a few query or value
units. The average query and value units per head
are 9.8 and 8.2, respectively. With StructReg, the
model holds only 25 attention heads, and the aver-
age numbers of query and value units per head are
28.6 and 28.5. The number of heads is signiﬁcantly
reduced. We also ﬁnd FFN layers are more severely
pruned than attention heads, consistent with results
in Xia et al. (2022).
Speed and Performance We next study the im-
pacts of StructReg on speed and performance. We
evaluate the latency with batch size 128 and se-
quence length 512 on an NVIDIA M40 GPU for all
tasks. The results are shown in Figure 5. The2782latency of BERTis around 3840 ms, far be-
yond the plots’ range. The pruned models without
StructReg only achieve about 4×speedup. As the
regularization strength αincreases from 0to0.3,
the latency decreases monotonically. At α= 0.3
(the leftmost marker in each plot), models achieve
6∼7×speedups, notably faster than the unregu-
larized ones. The task performance is also affected
by StructReg. As αincreases from 0 to 0.3, the
QNLI accuracy drops by 0.6%, while SQuAD F1
increases by 0.4%. There is no uniform trend in
performance across different tasks. Nevertheless,
compared to the gains in speedups, the variances in
performance are marginal.
6 Conclusion
This paper proposes GRAIN, a gradient-based
structured pruning method that expands the struc-
ture search space by pruning with intra-attention
structures. We provide a structure regularization
strategy that encourages ﬁnding regular structures
and helps achieve lower latencies. We also com-
bine pruning with distillation. We propose to sep-
arate the gradients from different losses to reduce
the interference. GRAIN is computationally efﬁ-
cient since it does not require pre-training or data
augmentation. Experiments show that GRAIN
achieves impressive high performance and outper-
forms other methods at different model densities
on various natural language understanding tasks
and meanwhile maintains competitive speedups.
Limitations
Inference Speed At the same model size, the la-
tencies of GRAIN on different tasks are relatively
large compared to the methods like CoFi and Tiny-
BERT. This is because GRAIN generates models
with different head size, and the computation of
these heads are not parallelized. Thus the resulting
models are slower than the models with uniform at-
tention structures. This problem could be relieved
by introducing model structure regularization at a
higher level or by some engineering techniques,
such as merging heads with the same or similar
size into a large matrix to increase parallelism.
Backbone Models GRAIN is designed for
transformer-based models. Although the trans-
former is one of the most popular building blocks
of NLP models, there are many other promising
structures. The effectiveness of GRAIN on model
compression is possibly correlated with hardwarelottery or software lottery (Hooker, 2020). In ad-
dition, we have only tested our method with the
standard multi-head attention mechanism. Trans-
planting GRAIN to other attention mechanisms is
possible, but the effectiveness has yet to be tested.
Acknowledgements
This work is supported by the National Key Re-
search and Development Program of China (Grant
No. 2022YFC3303504).
References278327842785A Reproducibility and Training Costs
Hyperparameters We summarize the hyperpa-
rameters of our experiments in Table 4. We use
AdamW optimizer (Loshchilov and Hutter, 2019).
The learning rate is scheduled with 10% warm-up
steps followed by a linear decay.
Training Environment All the training experi-
ments are conducted on a single NVIDIA V100
GPU. The PyTorch (Paszke et al., 2019) version is
1.8.1, the CUDA version is 10.2, and Transformers
(Wolf et al., 2020) version is 4.10.0.
Training Costs It takes about 15 hours to ﬁnish
training on MNLI and QQP, 11 hours on SQuAD,
5 hours on QNLI, 3 hours on SST-2, and 1 hour on
CoNLL 2003.
Hyperparameter Value
peak learning rate3e-5 (GLUE)
3e-5 (SQuAD)
1e-4 (CoNLL 2003)
number of epochs20 (GLUE)
20 (SQuAD)
40 (CoNLL 2003)
batch size 32
temperature τ 8
start of pruning p 0.2
end of pruning p 0.4
smoothing factor β 0.998
regularization strength α 0.3
reduced embedding size r 192
B Dataset Statistics
The details of the datasets are shown in Table 5.
C Structures of Pruned Models
Table 6 summarizes the structures of the pruned
models on different tasks at model density 5%.
D Inference Speed vs. Performance
Figure 7 shows the latency of GRAIN and other
methods on various tasks. All the measurements
are conducted under the same environment (see
the paragraph Speed and Performance in Section
5.4). The structure regularization strength αis0.3.
GRAIN achieves competitive speedups comparable
to other methods.Task Train Size Metric # Labels
English Task
QNLI 105k Acc 2
MNLI 393k Acc 3
QQP 364k Acc 2
SST-2 67k Acc 2
SQuAD 88k F1 N/A
CoNLL 2003 14k F1 9
Chinese Task
OCNLI 50k Acc 3
TNEWS 53k Acc 15
CMRC 2018 10k F1 N/A
DRCD 27k F1 N/A
E More Results
E.1 Pruning RoBERTa
We conduct GRAIN with RoBERTa-base (Liu et al.,
2019) on the same set of tasks and use the same
hyperparameters as those in Table 4. The results
of GRAIN with BERT and RoBERTa at differ-
ent model densities are shown in Table 7. The
pruned RoBERTa outperforms pruned BERT at
high densities, but at low densities, BERT surpasses
RoBERTa on some tasks.
E.2 Experiments on Chinese Tasks
Due to the limited availability of results on model
compression methods for Chinese tasks, we present
the results of GRAIN on several Chinese tasks, pro-
viding a useful reference point for related works.
We evaluate GRAIN on the following Chinese
tasks: OCNLI (Hu et al., 2020), an original Chinese
natural language inference task; TNEWS (Xu et al.,
2020), a short text classiﬁcation task for news;
CMRC 2018 (Cui et al., 2019) and DRCD (Shao
et al., 2019), two representative span-extraction
Chinese machine reading comprehension tasks.
The details of the datasets are shown in Table 5.
The learning rate is 1e-4 for CMRC 2018 and
DRCD, 2e-5 for OCNLI and TNEWS; the number
of epochs is 40 for CMRC 2018 and DRCD, 20 for
OCNLI and TNEWS. Other hyperparameters are
the same as those in Table 4. The teacher model is
Chinese-RoBERTa-wwm-ext (Cui et al., 2021).
We report the mean score of 3 runs for each
task using different random seeds. The results are
shown in Table 8.278627872788ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
The section after Conclusion.
/squareA2. Did you discuss any potential risks of your work?
This work presents a general compression method, which is not tied to particular applications.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1 Introduction.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 5.1 and Appendix B
/squareB1. Did you cite the creators of artifacts you used?
Section 5.1 and Appendix B
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The licenses for each artifact can be found in the original paper or the repository on GitHub.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Reader may refer to the original papers of the artifacts.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix B
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 5.1 and Appendix A2789/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix A
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5.1
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.2790