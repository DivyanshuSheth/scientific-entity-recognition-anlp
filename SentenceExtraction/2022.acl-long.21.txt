
Yuwei ZhangHaode ZhangLi-Ming ZhanXiao-Ming Wu
Albert Y.S. Lam
Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R.
University of California, San Diego
Fano Labs, Hong Kong S.A.R.
zhangyuwei.work@gmail.com
{haode.zhang, lmzhan.zhan}@connect.polyu.edu.hk
csxmwu@comp.polyu.edu.hk, albert@fano.ai
Abstract
New intent discovery aims to uncover novel
intent categories from user utterances to ex-
pand the set of supported intent classes. It
is a critical task for the development and ser-
vice expansion of a practical dialogue system.
Despite its importance, this problem remains
under-explored in the literature. Existing ap-
proaches typically rely on a large amount of
labeled utterances and employ pseudo-labeling
methods for representation learning and clus-
tering, which are label-intensive, inefficient,
and inaccurate. In this paper, we provide new
solutions to two important research questions
for new intent discovery: (1) how to learn se-
mantic utterance representations and (2) how
to better cluster utterances. Particularly, we
first propose a multi-task pre-training strat-
egy to leverage rich unlabeled data along with
external labeled data for representation learn-
ing. Then, we design a new contrastive loss
to exploit self-supervisory signals in unlabeled
data for clustering. Extensive experiments on
three intent recognition benchmarks demon-
strate the high effectiveness of our proposed
method, which outperforms state-of-the-art
methods by a large margin in both unsupervised
and semi-supervised scenarios. The source
code will be available at https://github.
com/zhang-yu-wei/MTP-CLNN .
1 Introduction
Why Study New Intent Discovery (NID)? Recent
years have witnessed the rapid growth of conversa-
tional AI applications. To design a natural language
understanding system, a set of expected customer
intentions are collected beforehand to train an in-
tent recognition model. However, the pre-defined
intents cannot fully meet customer needs. This
implies the necessity of expanding the intent recog-
nition model by repeatedly integrating new intents
discovered from unlabeled user utterances (Fig. 1).Figure 1: New Intent Discovery.
To reduce the effort in manually identifying un-
known intents from a mass of utterances, previous
works commonly employ clustering algorithms to
group utterances of similar intents (Cheung and
Li, 2012; Hakkani-Tür et al., 2015; Padmasundari,
2018). The cluster assignments thereafter can ei-
ther be directly used as new intent labels or as
heuristics for faster annotations.
Research Questions (RQ) and Challenges.
Current study of NID centers around two basic
research questions: 1) How to learn semantic ut-
terance representations to provide proper cues for
clustering? 2) How to better cluster the utterances?
The study of the two questions are often interwoven
in existing research. Utterances can be represented
according to different aspects such as the style of
language, the related topics, or even the length
of sentences. It is important to learn semantic ut-
terance representations to provide proper cues for
clustering. Simply applying a vanilla pre-trained
language model (PLM) to generate utterance repre-
sentations is not a viable solution, which leads to
poor performance on NID as shown by the experi-
mental results in Section 4.2. Some recent works
proposed to use labeled utterances of known intents256for representation learning (Forman et al., 2015;
Haponchyk et al., 2018; Lin et al., 2020; Zhang
et al., 2021c; Haponchyk and Moschitti, 2021),
but they require a substantial amount of known
intents and sufficient labeled utterances of each
intent, which are not always available especially
at the early development stage of a dialogue sys-
tem. Further, pseudo-labeling approaches are often
exploited to generate supervision signals for repre-
sentation learning and clustering. For example, Lin
et al. (2020) finetune a PLM with an utterance simi-
larity prediction task on labeled utterances to guide
the training of unlabeled data with pseudo-labels.
Zhang et al. (2021c) adopt a deep clustering method
(Caron et al., 2018) that uses k-means clustering
to produce pseudo-labels. However, pseudo-labels
are often noisy and can lead to error propagation.
Our Solutions. In this work, we propose a sim-
ple yet effective solution for each research question.
Solution to RQ 1: multi-task pre-training. We
propose a multi-task pre-training strategy that takes
advantage of both external data and internal data
for representation learning. Specifically, we lever-
age publicly available, high-quality intent detection
datasets, following Zhang et al. (2021d), as well
as the provided labeled and unlabeled utterances
in the current domain, to fine-tune a pre-trained
PLM to learn task-specific utterance representa-
tions for NID. The multi-task learning strategy en-
ables knowledge transfer from general intent detec-
tion tasks and adaptation to a specific application
domain. Solution to RQ 2: contrastive learning
with nearest neighbors. We propose to use a con-
trastive loss to produce compact clusters, which is
motivated by the recent success of contrastive learn-
ing in both computer vision (Bachman et al., 2019;
He et al., 2019; Chen et al., 2020; Khosla et al.,
2020) and natural language processing (Gunel et al.,
2021; Gao et al., 2021; Yan et al., 2021). Con-
trastive learning usually maximizes the agreement
between different views of the same example and
minimize that between different examples. How-
ever, the commonly used instance discrimination
task may push away false negatives and hurts the
clustering performance. Inspired by a recent work
in computer vision (Van Gansbeke et al., 2020), we
introduce neighborhood relationship to customize
the contrastive loss for clustering in both unsuper-
vised (i.e., without any labeled utterances of known
intents) and semi-supervised scenarios. Intuitively,
in a semantic feature space, neighboring utterancesshould have a similar intent, and pulling together
neighboring samples makes clusters more compact.
Our main contributions are three-fold.
•We show that our proposed multi-task pre-
training method already leads to large perfor-
mance gains over state-of-the-art models for
both unsupervised and semi-supervised NID.
•We propose a self-supervised clustering
method for NID by incorporating neighbor-
hood relationship into the contrastive learning
objective, which further boosts performance.
•We conduct extensive experiments and abla-
tion studies on three benchmark datasets to
verify the effectiveness of our methods.
2 Related Works
New Intent Discovery. The study of NID is still in
an early stage. Pioneering works focus on unsuper-
vised clustering methods. Shi et al. (2018) lever-
aged auto-encoder to extract features. Perkins and
Yang (2019) considered the context of an utterance
in a conversation. Chatterjee and Sengupta (2020)
proposed to improve density-based models. Some
recent works (Haponchyk et al., 2018; Haponchyk
and Moschitti, 2021) studied supervised clustering
algorithms for intent labeling, yet it can not handle
new intents. Another line of works (Forman et al.,
2015; Lin et al., 2020; Zhang et al., 2021c) inves-
tigated a more practical case where some known
intents are provided to support the discovery of
unknown intents, which is often referred to as semi-
supervised NID.
To tackle semi-supervised NID, Lin et al. (2020)
proposed to first perform supervised training on
known intents with a sentence similarity task and
then use pseudo labeling on unlabeled utterances
to learn a better embedding space. Zhang et al.
(2021c) proposed to first pre-train on known in-
tents and then perform k-means clustering to as-
sign pseudo labels on unlabeled data for represen-
tation learning following Deep Clustering (Caron
et al., 2018). They also proposed to align clusters
to accelerate the learning of top layers. Another
approach is to first classify the utterances as known
and unknown and then uncover new intents with
the unknown utterances (Vedula et al., 2020; Zhang
et al., 2021b). Hence, it relies on accurate classifi-
cation in the first stage.
In this work, we address NID by proposing a
multi-task pre-training method for representation257learning and a contrastive learning method for clus-
tering. In contrast to previous methods that rely
on ample annotated data in the current domain for
pre-training, our method can be used in an unsuper-
vised setting and work well in data-scarce scenarios
(Section 4.3).
Pre-training for Intent Recognition. Despite
the effectiveness of large-scale pre-trained lan-
guage models (Radford and Narasimhan, 2018;
Devlin et al., 2019; Liu et al., 2019; Brown et al.,
2020), the inherent mismatch in linguistic behav-
ior between the pre-training datasets and dialogues
encourages the research of continual pre-training
on dialogue corpus. Most previous works proposed
to pre-train on open domain dialogues in a self-
supervised manner (Mehri et al., 2020; Wu et al.,
2020; Henderson et al., 2020; Hosseini-Asl et al.,
2020). Recently, several works pointed out that
pre-training with relavant tasks can be effective
for intent recognition. For example, Zhang et al.
(2020) formulated intent recognition as a sentence
similarity task and pre-trained on natural language
inference (NLI) datasets. Vuli ´c et al. (2021); Zhang
et al. (2021e) pre-trained with a contrastive loss on
intent detection tasks. Our multi-task pre-training
method is inspired from Zhang et al. (2021d) which
leverages publicly available intent datasets and un-
labeled data in the current domain for pre-training
to improve the performance of few-shot intent de-
tection. However, we argue that the method is more
suitable to be applied for NID due to the natural
existence of unlabeled utterances.
Contrastive Representation Learning. Con-
trastive learning has shown promising results in
computer vision (Bachman et al., 2019; Chen et al.,
2020; He et al., 2019; Khosla et al., 2020) and
gained popularity in natural language processing.
Some recent works used unsupervised contrastive
learning to learn sentence embeddings (Gao et al.,
2021; Yan et al., 2021; Kim et al., 2021; Giorgi
et al., 2021). Specifically, Gao et al. (2021); Yan
et al. (2021) showed that contrastive loss can avoid
an anisotropic embedding space. Kim et al. (2021)
proposed a self-guided contrastive training to im-
prove the quality of BERT representations. Giorgi
et al. (2021) proposed to pre-train a universal
sentence encoder by contrasting a randomly sam-
pled text segment from nearby sentences. Zhang
et al. (2021e) demonstrated that self-supervised
contrastive pre-training and supervised contrastive
fine-tuning can benefit few-shot intent recognition.Zhang et al. (2021a) showed that combining a con-
trastive loss with a clustering objective can improve
short text clustering. Our proposed contrastive loss
is tailored for clustering, which encourages utter-
ances with similar semantics to group together and
avoids pushing away false negatives as in the con-
ventional contrastive loss.
3 Method
Problem Statement. To develop an intent recog-
nition model, we usually prepare a set of expected
intents Calong with a few annotated utterances
D ={(x, y)|y∈ C}for each intent. Af-
ter deployed, the system will encounter utterances
D={x|y∈ {C,C}}from both pre-
defined (known) intents Cand unknown intents
C. The aim of new intent discovery (NID) is
to identify the emerging intents CinD.
NID can be viewed as a direct extension of out-of-
distribution (OOD) detection, where we not only
need to identify OOD examples but also discover
the underlying clusters. NID is also different from
zero-shot learning in that we do not presume ac-
cess to any kind of class information during train-
ing. In this work, we consider both unsupervised
and semi-supervised NID, which are distinguished
by the existence of D , following Zhang et al.
(2021c).
Overview of Our Approach. As shown in
Fig. 2, we propose a two-stage framework that
addresses the research questions mentioned in
Sec. 1. In the first stage, we perform multi-task
pre-training (MTP) that jointly optimizes a cross-
entropy loss on external labeled data and a self-
supervised loss on target unlabeled data (Sec. 3.1).
In the second stage, we first mine top- Knearest
neighbors of each training instance in the embed-
ding space and then perform contrastive learning
with nearest neighbors (CLNN) (Sec. 3.2). After
training, we employ a simple non-parametric clus-
tering algorithm to obtain clustering results.
3.1 Stage 1: Multi-task Pre-training (MTP)
We propose a multi-task pre-training objective that
combines a classification task on external data
from publicly available intent detection datasets
and a self-supervised learning task on internal data
from the current domain. Different from previous
works (Lin et al., 2020; Zhang et al., 2021c), our
pre-training method does not rely on annotated data
(D ) from the current domain and hence can258
be applied in an unsupervised setting.
Specifically, we first initialize the model with
a pre-trained BERT encoder (Devlin et al., 2019).
Then, we employ a joint pre-training loss as in
Zhang et al. (2021d). The loss consists of a cross-
entropy loss on external labeled data and a masked
language modelling (MLM) loss on all available
data from the current domain:
L=L(D ;θ)|{z}+L(D ;θ)|{z },(1)
where θare model parameters. For the super-
vised classification task, we leverage an exter-
nal public intent dataset with diverse domains
(e.g., CLINC150 (Larson et al., 2019)), denoted
asD , following Zhang et al. (2021d). For
the self-supervised MLM task, we use all avail-
able data (labeled or unlabeled) from the current
domain, denoted as D .
Intuitively, the classification task aims to learn
general knowledge of intent recognition with anno-
tated utterances in external intent datasets, while
the self-supervised task learns domain-specific se-
mantics with utterances collected in the current
domain. Together, they enable learning semantic
utterance representations to provide proper cues for
the subsequent clustering task. As will be shown
in Sec. 4.3, both tasks are essential for NID.
For semi-supervised NID, we can further utilize
the annotated data in the current domain to con-
duct continual pre-training, by replacing D
in Eq. 1 to D . This step is not included in
unsupervised NID.3.2 Stage 2: Contrastive Learning with
Nearest Neighbors (CLNN)
In the second stage, we propose a contrastive learn-
ing objective that pulls together neighboring in-
stances and pushes away distant ones in the em-
bedding space to learn compact representations
for clustering. Concretely, we first encode the ut-
terances with the pre-trained model from stage 1.
Then, for each utterance x, we search for its top-
Knearest neighbors in the embedding space using
inner product as distance metric to form a neigh-
borhood N. The utterances in Nare supposed to
share a similar intent as x. During training, we
sample a minibatch of utterances B={x}. For
each utterance x∈ B, we uniformly sample one
neighbor xfrom its neighborhood N. We then
use data augmentation to generate ˜xand˜xforx
andxrespectively. Here, we treat ˜xand˜xas
two views of x, which form a positive pair. We
then obtain an augmented batch B={˜x,˜x}
with all the generated samples. To compute con-
trastive loss, we construct an adjacency matrix A
forB, which is a 2M×2Mbinary matrix where
1indicates positive relation (either being neighbors
or having the same intent label in semi-supervised
NID) and 0 indicates negative relation. Hence, we
can write the contrastive loss as:
l=−1
|C|Xlogexp( sim(˜h,˜h)/τ)Pexp( sim(˜h,˜h)/τ),
(2)259L=1
2MXl, (3)
where C≡ {A= 1|j∈ {1, ...,2M}}denotes
the set of instances having positive relation with
˜xand|C|is the cardinality. ˜his the embedding
for utterance ˜x.τis the temperature parameter.
sim(·,·)is a similarity function (e.g., dot product)
on a pair of normalized feature vectors. During
training, the neighborhood will be updated every
few epochs. We implement the contrastive loss
following Khosla et al. (2020).
Notice that the main difference between Eq. 2
and conventional contrastive loss is how we con-
struct the set of positive instances C. Conventional
contrastive loss can be regarded as a special case
of Eq. 2 with neighborhood size K= 0 and the
same instance is augmented twice to form a positive
pair (Chen et al., 2020). After contrastive learning,
a non-parametric clustering algorithm such as k-
means can be applied to obtain cluster assignments.
Data Augmentation. Strong data augmenta-
tion has been shown to be beneficial in contrastive
learning (Chen et al., 2020). We find that it is inef-
ficient to directly apply existing data augmentation
methods such as EDA (Wei and Zou, 2019), which
are designed for general sentence embedding. We
observe that the intent of an utterance can be ex-
pressed by only a small subset of words such as
“suggest restaurant” or “book a flight”. While it is
hard to identify the keywords for an unlabeled utter-
ance, randomly replacing a small amount of tokens
in it with some random tokens from the library will
not affect intent semantics much. This approach
works well in our experiments (See Table 5 RTR).
Advantages of CLNN. By introducing the no-
tion of neighborhood relationship in contrastive
learning, CLNN can 1) pull together similar in-
stances and push away dissimilar ones to obtain
more compact clusters; 2) utilize proximity in
the embedding space rather than assigning noisy
pseudo-labels (Van Gansbeke et al., 2020); 3) di-
rectly optimize in the feature space rather than
clustering logits as in Van Gansbeke et al. (2020),
which has been proven to be more effective by
Rebuffi et al. (2020); and 4) naturally incorporate
known intents with the adjacency matrix.
4 Experiment
4.1 Experimental Details
Datasets. We evaluate our proposed method
on three popular intent recognition benchmarks.
BANKING (Casanueva et al., 2020) is a fine-
grained dataset with 77intents collected from
banking dialogues, StackOverflow (Xu et al.,
2015) is a large scale dataset collected from online
queries, M-CID (Arora et al., 2020) is a smaller
dataset collected for Covid-19 services. We choose
CLINC150 (Larson et al., 2019) as our external
public intent dataset in stage 1 due to its high-
quality annotations and coverage of diverse do-
mains. The dataset statistics are summarized in
Table 1. We use the same splits of BANKING
and StackOverflow as in Zhang et al. (2021b). De-
tails about dataset splitting are provided in the Ap-
pendix.
Experimental Setup. We evaluate our proposed
method on both unsupervised and semi-supervised
NID. Notice that in unsupervised NID, no labeled
utterances from the current domain are provided.
For clarity, we define two variables. The propor-
tion of known intents is defined as |C|/(|C|+|C|)
and referred to as “ known class ratio ( KCR )”, and
the proportion of labeled examples for each known
intent is denoted as “ labeled ratio ( LAR )”. The la-
beled data are randomly sampled from the original
training set. Notice that, KCR = 0means unsuper-
vised NID, and KCR >0means semi-supervised
NID. In the following sections, we provide experi-
mental results for both unsupervised NID and semi-
supervised NID with KCR ={25%,50%,75%}
and LAR ={10%,50%}.
Evaluation Metrics. We adopt three popular
evaluation metrics for clustering: normalized mu-
tual information (NMI), adjusted rand index (ARI),
and accuracy (ACC).
Baselines and Model Variants. We summa-
rize the baselines compared in our experiments for
both unsupervised and semi-supervised NID. Our260
implementation is based on Zhang et al. (2021b).
•Unsupervised baselines. (1) GloVe-KM
and (2) GloVe-AG are based on GloVe (Pen-
nington et al., 2014) embeddings and then
evaluated with k-means (MacQueen et al.,
1967) or agglomerative clustering (Gowda,
1984) respectively. (3) BERT-KM applies k-
means on BERT embeddings. (4) SAE-KMadopts k-means on embeddings of stacked
auto-encoder. (5) Deep Embedding Clustering
(SAE-DEC) (Xie et al., 2016) and (6) Deep
Clustering Network (SAE-DCN) (Yang et al.,
2017) are unsupervised clustering methods
based on stacked auto-encoder.
•Semi-supervised baselines. (1) BERT-KCL
(Hsu et al., 2018) and (2) BERT-MCL (Hsu
et al., 2019) employs pairwise similarity task
for semi-supervised clustering. (3) BERT-
DTC (Han et al., 2019) extends DEC into
semi-supervised scenario. (4) CDAC+ (Lin261
et al., 2020) employs a pseudo-labeling pro-
cess. (5) Deep Aligned Clustering (DAC)
(Zhang et al., 2021c) improves Deep Clus-
tering (Caron et al., 2018) by aligning clusters
between iterations.
•Our model variants include MTP and MTP-
CLNN, which correspond to applying k-
means on utterance representations learned
in stage 1 and stage 2 respectively. Further,
we continue to train a DAC model on top of
MTP to form a stronger baseline MTP-DAC
for semi-supervised NID.
Implementation. We take pre-trained bert-base-
uncased model from Wolf et al. (2019)as our
base model and we use the [CLS] token as the
BERT representation. For MTP, we first train un-
til convergence on the external dataset, and then
when training on D , we use a development
set to validate early-stopping with a patience of
20epochs following Zhang et al. (2021c). For
contrastive learning, we project a 768-d BERT em-
bedding to an 128-d vector with a two-layer MLP
and set the temperature as 0.07. For mining near-
est neighbors, we use the inner product methodprovided by Johnson et al. (2017). We set neigh-
borhood size K= 50 for BANKING and M-CID,
andK= 500 for StackOverflow, since we empiri-
cally find that the optimal Kshould be roughly half
of the average size of the training set for each class
(see Section 4.4). The neighborhood is updated
every 5epochs. For data augmentation, the random
token replacement probability is set to 0.25. For
model optimization, we use the AdamW provided
by Wolf et al. (2019). In stage 1, the learning rate
is set to 5e. In stage 2, the learning rate is set
to1efor BANKING and M-CID, and 1efor
StackOverflow. The batch sizes are chosen based
on available GPU memory. All the experiments
are conducted on a single RTX-3090 and averaged
over10different seeds. More details are provided
in the Appendix.
4.2 Result Analysis
Unsupervised NID. We show the results for un-
supervised NID in Table 2. First, comparing the
performance of BERT-KM with GloVe-KM and
SAE-KM, we observe that BERT embedding per-
forms worse on NID even though it achieves better
performance on NLP benchmarks such as GLUE,
which manifests learning task-specific knowledge
is important for NID. Second, our proposed pre-262
training method MTP improves upon baselines by
a large margin. Take the NMI score of BANKING
for example, MTP outperforms the strongest base-
line SAE-DCN by 14.38%, which demonstrates
the effectiveness of exploiting both external public
datasets and unlabeled internal utterances. Further-
more, MTP-CLNN improves upon MTP by around
5%in NMI, 10% in ARI, and 10% in ACC across
different datasets.
Semi-supervised NID. The results for semi-
supervised NID are shown in Table 3. First, MTP
significantly outperforms the strongest baseline
DAC in all settings. For instance, on M-CID,
MTP achieves 22.57% improvement over DAC in
NMI. Moreover, MTP is less sensitive to the pro-
portion of labeled classes. From KCR = 75% to
KCR = 25% on M-CID, MTP only drops 8.55%
in NMI, as opposed to about 21.58% for DAC. The
less performance decrease indicates that our pre-
training method is much more label-efficient. Fur-
thermore, with our proposed contrastive learning,
MTP-CLNN consistently outperforms MTP and
the combined baseline MTP-DAC. Take BANK-
ING with KCR = 25% for example, MTP-CLNN
improves upon MTP by 4.11% in NMI while sur-
passing MTP-DAC by 2.63%. A similar trend canbe observed when LAR = 50% , and we provide
the results in the Appendix.
Visualization. In Fig. 3, we show the t-SNE
visualization of clusters with embeddings learned
by two strongest baselines and our methods. It
clearly shows the advantage of our methods, which
can produce more compact clusters. Results on
other datasets can be found in the Appendix.
4.3 Ablation Study of MTP
To further illustrate the effectiveness of MTP, we
conduct two ablation studies in this section. First,
we compare MTP with the pre-training method
employed in Zhang et al. (2021c), where only in-
ternal labeled data are utilized for supervised pre-
training (denoted as SUP).In Fig. 4, we show
the results of both pre-training methods combined
with CLNN with different proportions of known
classes. Notice that when KCR = 0there is no pre-
training at all for SUP-CLNN. It can be seen that
MTP-CLNN consistently outperforms SUP-CLNN.
Furthermore, the performance gap increases while
KCR decreases, and the largest gap is achieved263
when KCR = 0. This shows the high effectiveness
of our method in data-scarce scenarios.
Second, we decompose MTP into two parts: su-
pervised pre-training on external public data (PUB)
and self-supervised pre-training on internal unla-
beled data (MLM). We report the results of the two
pre-training methods combined with CLNN as well
as MTP in Table 4. We can easily conclude that ei-
ther PUB or MLM is indispensable and multi-task
pre-training is beneficial.
4.4 Analysis of CLNN
Number of Nearest Neighbors. We conduct an ab-
lation study on neighborhood size Kin Fig. 5. We
can make two main observations. First, although
the performance of MTP-CLNN varies with dif-
ferent K, it still significantly outperforms MTP
(dashed horizontal line) for a wide range of K.
For example, MTP-CLNN is still better than MTP
when K= 50 on StackOverflow or K= 200 on
BANKING. Second, despite the difficulty to search
forKwith only unlabeled data, we empirically
find an effective estimation method, i.e. to choose
Kas half of the average size of the training set
for each class. It can be seen that the estimated
K≈60on BANKING and K≈40on M-CID
(vertical dashed lines) lie in the optimal regions,
which shows the effectiveness of our empirical es-
timation method.
Exploration of Data Augmentation. We com-
pare Random Token Replacement (RTR) used in
our experiments with other methods. For instance,
dropout is applied on embeddings to provide data
augmentation in Gao et al. (2021), randomly shuf-
fling the order of input tokens is proven to be effec-
tive in Yan et al. (2021), and EDA (Wei and Zou,
2019) is often applied in text classification. Further-
more, we compare with a Stop-words Replacement
(SWR) variant that only replaces the stop-words
with other random stop-words so it minimally af-
fects the intents of utterances. The results in Table 5
demonstrate that (1) RTR and SWR consistently
outperform others, which verifies our hypothesis
in Section 3.2. (2) Surprisingly, RTR and SWR
perform on par with each other. For simplicity,
we only report the results with RTR in the main
experiments.
5 Conclusion
We have provided simple and effective solutions
for two fundamental research questions for new
intent discovery (NID): (1) how to learn better ut-
terance representations to provide proper cues for
clustering and (2) how to better cluster utterances in
the representation space. In the first stage, we use
a multi-task pre-training strategy to exploit both
external and internal data for representation learn-
ing. In the second stage, we perform contrastive
learning with mined nearest neighbors to exploit
self-supervisory signals in the representation space.
Extensive experiments on three intent recognition
benchmarks show that our approach can signifi-
cantly improve the performance of NID in both
unsupervised and semi-supervised scenarios.
There are two limitations of this work. (1) We
have only evaluated on balanced data. However, in
real-world applications, most datasets are highly
imbalanced. (2) The discovered clusters lack inter-
pretability. Our clustering method can only assign a
cluster label to each unlabeled utterance but cannot
generate a valid intent name for each cluster.
6 Acknowledgments
We would like to thank the anonymous reviewers
for their valuable comments. This research was
supported by the grants of HK ITF UIM/377 and
PolyU DaSAIL project P0030935 funded by RGC.264References265266267A Experimental Details
A.1 Datasets
In this section, we provide more details about the
datasets. The development sets are prepared to
exclude no unknown intents.
•BANKING (Casanueva et al., 2020) is a fine-
grained intent detection dataset in which 77
intents are collected for banking dialogue sys-
tem. The dataset is splitted into 9,003, 1,000
and 3,080 for training, validation, and test sets
respectively.
•StackOverflow (Xu et al., 2015) is a large
scale dataset for online questioning which con-
tains 20 intents with 1,000 examples in each
class. We split the dataset into 18,000 for
training, 1,000 for validation, and 1,000 for
test.
•M-CID (Arora et al., 2020) is a small scale
dataset for cross-lingual Covid-19 queries.
We only use the English subset of this dataset
which has 16 intents. We split the dataset into
1,220 for training, 176 for validation, and 349
for test.
•CLINC150 (Larson et al., 2019) consists of
10 domains across multiple unique services.
We use 8 domainsand remove the out-of-
scope data. We only use this dataset during
training stage 1.
A.2 Implementation
The batch size is set to 64 for stage 1 and 128 for
stage 2 in all experiments to fully utilize the GPU
memory. In stage 1, we first train until convergence
on external data and then train with validation on
internal data. In stage 2, we train until convergence
without early-stopping.
A.3 More Experimental Results
The results on semi-supervised NID when LAR =
50% are shown in Table 6. It can be seen that
our methods still achieve the best performance in
this case. In Fig. 6 and Fig. 7, we show the t-
SNE visualization of clusters on BANKING and
M-CID with embeddings learned by two strongest
baselines and our methods. Again, it shows that
our methods can produce more compact clusters.268269