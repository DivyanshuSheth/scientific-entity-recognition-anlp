
Jiahao Cao, Rui Liu, Huailiang Peng, Lei Jiang, Xu Bai
Institute of Information Engineering, Chinese Academy of Sciences
School of Cyber Security, University of Chinese Academy of Sciences
{caojiahao,liurui3221,penghuailiang,jianglei,baixu}@iie.ac.cn
Abstract
Aspect-based sentiment analysis (ABSA) is a
fine-grained sentiment classification task. Most
recent efforts adopt pre-trained model to clas-
sify the sentences with aspects. However, the
aspect sentiment bias from pre-trained model
brings some noise to the ABSA task. Besides,
traditional methods using cross-entropy loss are
hard to find the potential associations between
sentiment polarities. In this work, we analyze
the ABSA task from a novel cognition perspec-
tive: humans can often judge the sentiment of
an aspect even if they do not know what the
aspect is. Moreover, it is easier to distinguish
positive and negative sentiments than others for
human beings because positive and negative are
two opposite sentiments. To this end, we pro-
pose a no-aspect differential sentiment (NADS)
framework for the ABSA task. We first design
a no-aspect template by replacing the aspect
with a special unbiased character to eliminate
the sentiment bias and obtain a stronger rep-
resentation. To better get the benefits from
the template, we adopt contrastive learning be-
tween the no-aspect template and the original
sentence. Then we propose a differential sen-
timent loss instead of the cross-entropy loss to
better classify the sentiments by distinguishing
the different distances between sentiments. Our
proposed model is a general framework and can
be combined with almost all traditional ABSA
methods. Experiments on SemEval 2014 show
that our framework is still able to predict the
sentiment of the aspect even we don’t konw
what the aspect is. Moreover, our NADS frame-
work boosts three typical ABSA methods and
achieves state-of-the-art performance.
1 Introduction
Aspect-based sentiment analysis (ABSA) (Jiang
et al., 2011) aims to identify the sentiment polarity
(i.e., negative, neutral, or positive) of each specific
aspect term in a piece of text (Hou et al., 2021;Dai et al., 2021; Li et al., 2021). For example,
in“The food is great, but the service is terrible” ,
the sentiment towards “food” is positive while the
sentiment towards “service” is negative. We need
to predict the sentiments of different aspect terms
in a sentence.
Previous works usually employ pre-trained
model to extract the embedding of the concate-
nation of the sentence and the aspect term. In
this way, the attention mechanism in pre-trained
model enhances the connection between aspect and
its context (Tang et al., 2016; Song et al., 2019).
The experiment results verify its appealing perfor-
mance. However, the pre-trained model on large-
scale raw corpora tends to internalize aspects’ in-
trinsic attributes (Huang et al., 2020) and brings
some noise to the ABSA task. For example, for
the sentence “Desserts include flan and sopaip-
illas” , a typical model BERT-SPC (Song et al.,
2019) based on BERT (Devlin et al., 2019) tends to
classify the sentiment towards “Desserts” as posi-
tive, while the label is neutral. This is because in
pre-trained corpora, “Desserts” often appears with
words that contain positive sentiment, causing the
word “Desserts” to internalize positive sentiment
as well. Moreover, traditional text classification
methods using the cross-entropy loss have some
shortcomings. On the one hand, the cross-entropy
loss suffers from lacking of robustness to noisy
labels (Zhang and Sabuncu, 2018) and the possi-
bility of poor margins (Elsayed et al., 2018). On
the other hand, the cross-entropy loss ignores the
potential relationships between different sentiment
polarities. Meanwhile, the non-smooth anisotropic
semantic space induced by pre-trained model (Li
et al., 2020) also brings difficulty in distinguishing
potential relationships between sentiments.
To tackle these problems, we analyze the ABSA
task from a human cognition perspective. People
often pay attention to the learning strategy and fea-
ture representation in many NLP tasks, but ignore1599
the organization of concepts between human and
artificial intelligence. Intuitively, human can still
perform well in the ABSA task without knowing
the meaning of aspect. As shown in Figure 1, in
“I had a delicious shrimp. ” , maybe we don’t know
what the “shrimp” is (it is a kind of food), we can
also easily classify the sentiment polarity of this
word as positive. Because we can judge the senti-
ment of the aspect through its context. Moreover, in
human perception, “positive” and“negative” are
two completely opposite sentiments and “neutral”
sentiment is in between. The distance between
“positive” and“neutral” is obviously closer than
“positive” and“negative” .
Inspired by the human cognition, we propose the
no-aspect template differential sentiment (NADS)
framework. We first design a no-aspect template by
replacing the aspect term in the sentence with the
special sentiment unbiased character “ < aspect > ”
and utilize the contrastive learning between the no-
aspect template and the original sentence. In this
way, we can not only eliminate the sentiment bias
in original sentence, but also learn a wider range
of sentence patterns as shown in Figure 1 to en-
hance the robustness of our framework. Moreover,
it helps our NADS framework to judge the senti-
ment of the aspect without knowing the specific
meaning of it, just like human beings. Then, in
order to reduce the semantic loss caused by the spe-
cial character “ < aspect > ”, we utilize the masked
aspect prediction to keep the original semantic in-
formation. Morever, we design the differential sen-
timent loss to find the different distances between
different sentiments and better distinguish different
sentiments. Our main contributions are:
•We propose a no-aspect template and utilize
the no-aspect contrastive learning to consider
a wider range of sentence patterns and elim-
inate the sentiment bias in the aspect embed-
ding. This also enables our model to predict
the sentiment of the aspect without knowing
what the aspect is, just like human beings.•We design the differential sentiment loss to
help us better distinguish different distances
between different sentiments. Moreover, our
differential sentiment loss can make samples
with the same sentiment as close as possible,
and samples with different sentiments as far
away as possible.
•Experiments on SemEval 2014 show that our
model enhances the performance of three typ-
ical ABSA methods and achieves new state-
of-the-art. Additionally, the experiments on
an aspect robustness test set ARTS show our
NADS model can greatly improve the robust-
ness of the model.
2 Related Work
Aspect-based sentiment analysis is a fine-grained
sentiment classification task. Recently, some works
on ABSA have focused on leveraging syntactic
knowledge through syntactic trees. (Wang et al.,
2020) reshaped the syntactic tree with aspect terms
as the center and utilized a relational graph atten-
tion network to encode the new tree structure for
sentiment prediction. (Hou et al., 2021) combined
the dependency relations from different parses be-
fore applying GNNs over the resulting graph.
Another trend utilizes various attention mech-
anisms to find the semantic relation of an aspect
and its context (Tan et al., 2019; Li et al., 2018;
Fan et al., 2018; Huang et al., 2018). Attention
mechanism helps to focus on the context related to
aspect and shield the irrelevant context. Besides,
some works tried to integrate syntax tree and at-
tention mechanism. Most recent work (Li et al.,
2021) utilized a mutual biaffine attention mecha-
nism to fuse syntactic information and semantic
information from syntactic tree.
In parallel, the pre-trained language model
BERT (Devlin et al., 2019) has achieved remark-
able performance in many NLP tasks. Experiments
show that using BERT in ABSA can achieve better
results (Li et al., 2021; Zhang et al., 2019) than
using static word embeddings such as Word2vec
(Mikolov et al., 2013) and GloVe (Pennington et al.,
2014). However, (Wang et al., 2021) showed that
the sentiment bias of aspect caused by the pre-
trained model may perplex the ABSA task. They
utilized external sentiment knowledge SentiWord-
Net (Esuli and Sebastiani, 2006) to extract prior
three-categorical sentiment for aspect terms. Then
they proposed an adversarial network to eliminate1600
the prior sentiment of aspect terms. However, it is
not known whether the aspect sentiment polarity
labeled from SentiWordNet is consistent with the
sentiment bias in the pre-trained model. In addi-
tion, previous works using cross-entropy loss also
ignored the potential associations between different
sentiment polarities.
In this paper, we propose a no-aspect template
and utilize contrastive learning to eliminate sen-
timent bias and learn a wider range of sentence
patterns to improve the robustness of the model.
Moreover, we design the differential sentiment loss
to better distinguish the different distances between
different sentiments and cluster the same sentiment.
3 Preliminaries
In this section, we use causal inference (Pearl et al.,
2000; Robins, 2003) to illustrate the theoretical ba-
sis of our framework. We illustrate the traditional
ABSA methods and our NADS framework by us-
ing a causal graph described in Figure 2. Causal
graph reflects the causal relationship between vari-
ables and we use “ →” denotes the direct effect.
For the ABSA task, the factors affecting sentiment
prediction include the specific aspect term Athat
we need to predict and the context of the aspect C.
BothAandCare important to ABSA task because
Ccontains the sentiment information and we need
to know which aspect Ato predict sentiment for.
In traditional methods’ causal graph as shown
in Figure 2 (a), the context and aspect capture the
direct effect of sentiment via C→YandA→Y.
The fusing information captures the indirect effect
ofAandConYvia the M,i.e., A, C →M→Y.
The predicted result that Ywould obtain if Ais set
toaandCis set to cas:
Y=Y(A=a, C=c, M =m) (1)
where m=Mdenotes the information about
the fusion of aspect aand context c. According tothis formula, traditional methods well consider the
role of the aspect and its context in the ABSA task.
However, in human cognition, the specific meaning
of aspect does not affect people’s judgment of its
sentiment. Traditional methods ignored the aspect
sentiment bias which makes aspect have a direct
impact on the prediction results YviaA→Y. It
may cause ABSA models to suffer from the spuri-
ous correlation between aspect and sentiment, and
thus fail to conduct effective reasoning.
In our NADS framework, we propose to exclude
aspect sentiment bias effect on A→Yin ABSA
as shown in Figure 2 (b). We utilize a special
character “ < aspect > ” that without sentiment
bias to replace the original aspect in the sentence
and employ the masked aspect prediction to keep
the original semantic information of the sentence.
We get the sentiment prediction Yas:
Y=Y(A=a, C=c, M =m)(2)
where a=“< aspect > ” and m=M. In
this way, we eliminate the direct impact of aspect’s
sentiment bias on the prediction results and keep
the original semantic information.
4 Proposed NADS
In the ABSA task, given a sentence S=
{ω, ω, ..., ω, ..., ω, ..., ω}and an aspect
termA={ω, ω, ..., ω}, the purpose is
to predict the sentiment polarity of Ain this S. As
shown in Figure 3, our NADS framework consists
of three parts. We first propose no-aspect template
and utilize contrastive learning between the no-
aspect template and original sentence to consider a
wider range of sentence patterns and eliminate the
sentiment bias in the aspect embedding. Then, in
order to make the sentence with the special char-
acters “ < aspect > ” keep the original semantic
information, we utilize the masked aspect predic-
tion. Finally we design the differential sentiment
loss to learn the different distances between differ-
ent sentiment polarities. We elaborate the details
of our proposed NADS.
4.1 No-aspect Contrastive Learning
For each {S, A}pair, we utilize a special character
“< aspect > ” without sentiment bias to replace the
whole aspect term Ain the sentence. We denote
the no-aspect template as T:
T=Replace ({S|A=a}, < aspect > )(3)1601
To better use the information from no-aspect
template and regularize pre-trained anisotropic em-
bedding space, we utilize contrastive learning be-
tween the original sentence and no-aspect template.
Specifically, for each sentence-aspect pair (s, a),
we denote the positive sentence as:
s=T (4)
where Tis the no-aspect template of s. Thus
we get a positive instance (s, < aspect > )for
(s, a). We obtain feature representation for each
sentence-aspect pair and positive instance through
the encoder f(·):
h=f(s, a) (5)
h=f(s, < aspect > ) (6)
where handhdenote the feature representation
of original sentence-aspect pair and positive in-
stance. In our NADS framework, we utilize BERT
to get the embedding of each sentence-aspect pair
by inputting the concatenation of the aspect term
and the sentence. For other models we use their
methods as the encoder to get the embedding for
each pair (s, a). We denote all other sentences
in the mini-batch as negative instances, so the con-
trastive loss is:
L=−loge
Σe(7)
where τis the temperature hyperparameter and
sim(·)is the cosine similarity. Nis the batch size.By comparing the original sentence with the no-
aspect template, we eliminate the sentiment bias
caused by the aspect terms in the original sentence
and learn not only the information of a sentence,
but also the information of a group of sentence
patterns. This helps us to improve the robustness of
the model. Moreover, contrastive learning helps us
regularize pre-trained anisotropic embedding space
to prepare for differential sentiment loss.
4.2 Masked Aspect Prediction
In section 4.1, we utilize the “ < aspect > ” to con-
struct the no-aspect template. However, we think
that directly using a special character “ < aspect > ”
that does not exist in the pre-trained model may
cause trouble to remain the semantic information.
Therefore, we utilize masked aspect prediction for
the special characters “ < aspect > ” to keep the
original semantics. Specifically, we mask the as-
pects by using “ < aspect > ” and predict the orig-
inal aspect terms in the position of “ < aspect > ”
in our ABSA training dataset. According to (Hong
et al., 2021), our purpose is to train the embedding
of “< aspect > ” to keep the complete semantic
information. For our NADS framework, we de-
note the embedding of “ < aspect > ” position as
h. We feed hinto a softmax layer to
predict the original aspect:
/hatwideY=softmax (Wh+b) (8)
where the Wandbare trainable parameters, /hatwideY
indicates the predict probability of the aspect word
at its position. We get the masked aspect prediction
loss through the cumulative of log-likelihood on1602predictions of each “ < aspect > ” position:
L=−Σlogp(/hatwideY) (9)
Specially, we only predict the position of “ <
aspect > ” in sentences. The masked aspect
prediciton helps us keep the original semantic infor-
mation of the sentence after replacing the aspect.
4.3 Differential Sentiment Loss
After regularizing pre-trained anisotropic embed-
ding space by using the contrastive learning be-
tween original and no-aspect template, we design
our differential sentiment loss to better distinguish
different sentiments. We first embed our labels into
the same size of h. We convert positive, neutral
and negative sentiment labels into label embed-
dings L={l, l, l}. The distance between
the sentence-aspect pair embedding hand label
embedding lis:
d(h, l) = 1−hl
∥h∥·∥l∥(10)
For each sentence-aspect pair embedding h, the
distance between hand its label lshould be closer
than other label embeddings in L. Thus, we utilize
a triplet loss to make hcloser to the right label
embedding land further to the other label embed-
dings. For each h, the positive instance is the label
embedding land the negative instances are the
other label embeddings in L. Moreover, in human
cognition, the distances between different senti-
ments are different. Thus, we set a specific margin
for each negative instance to better distinguish the
different distances between different sentiments.
Our differential sentiment loss is given as:
L=Σmax(d(h, l)
−d(h, l) +m(l, l),0)(11)
where m(l, l)is the specific margin for label l
andl. According to human cognition, we de-
note that positive and negative sentiments should
have the same distance to neutral sentiment and the
distance between positive and negative is further.
Thus, we set m(pos, neu ) = m(neg, neu )and
m(pos, neg )> m(pos, neu )in our model. Com-
pared with the cross-entropy loss, our differential
sentiment loss can better classify the sentiments
through distinguishing the differences between sen-
timents. Moreover, our differential sentiment lossDataset Division #Pos #Neu #Neg
LaptopTrain 976 455 851
Test 337 167 128
RestaurantTrain 2164 637 807
Test 727 196 196
can jointly train the model and label embeddings
and make our framework converge faster.
In order to judge the sentiment polarity of the
sentence-aspect pair, we utilize cosine similarity to
construct our scoring function:
S(h, l) =hl
∥h∥·∥l∥(12)
where his the embedding of the sentence-aspect
pair, and lis the embedding of the label. We take
thelwith the largest score as our prediction result.
Our training goal is to minimize the following
total objective function:
L=L+λL+λL (13)
where λandλare the weights of contrastive
leanrning loss and masked aspect prediction loss.
5 Experiments
5.1 Datasets
We evaluate our model on two ABSA task pub-
lic datasets: Restaurant and Laptop reviews from
SemEval 2014 Task 4 (Pontiki et al., 2014). We
remove several examples with “conflict” sentiment
polarity labels in the reviews. Table 1 shows the
statistics of these datasets.
5.2 Baseline Methods
We compare our NADS with state-of-the-art base-
lines. The models are described as follows.
1)BERT-SPC (Song et al., 2019) utilizes BERT
to encode the sentence-aspect pair as : "[CLS] sen-
tence [SEP] aspect [SEP]" and gets the embedding
of “[CLS]”. Our NADS framework utilizes BERT-
SPC as the encoder.
2)AEN+BERT (Song et al., 2019) utilizes BERT
and several attention layers to encoder sentence-
aspect pair. The embedding of the sentence and the
embedding of the aspect are obtained respectively.
3)CapsNet+BERT (Jiang et al., 2019) combines
the BERT and capsule networks in ABSA.1603Models Strategy14Rest 14Lap
Accuracy Macro-F1 Accuracy Macro-F1
CapsNet+BERT (Jiang et al., 2019) Ori 85.09 77.75 78.21 73.34
BERT-ADA (Rietzler et al., 2020) Ori 87.14 80.05 79.19 74.18
SDGCN-BERT (Zhao et al., 2020) Ori 83.57 76.47 81.35 78.34
R-GAT+BERT (Wang et al., 2020) Ori 86.60 81.35 78.21 74.07
DGEDT+BERT (Tang et al., 2020) Ori 86.30 80.00 79.80 75.60
Ori 84.46 76.98 78.99 75.03
BERT-SPC (Song et al., 2019) Noasp 81.77 70.81 75.47 69.65
Unite 84.45 77.40 78.16 73.06
Ori 87.49 82.09 82.12 79.13
NADS Noasp 87.04 81.77 81.01 77.69
Unite 87.58 81.73 81.96 78.87
Ori 83.12 73.76 79.93 76.31
AEN+BERT (Song et al., 2019) Noasp 80.70 68.86 77.06 72.41
Unite 80.97 71.65 78.16 74.39
Ori 84.00 75.88 81.33 77.78
AEN+NADS Noasp 86.51 80.16 80.22 76.88
Unite 86.68 79.69 81.48 78.07
Ori 87.13 81.16 81.80 78.10
DualGCN+BERT (Li et al., 2021) Noasp 81.95 72.42 77.53 73.49
Unite 84.90 77.24 78.64 74.43
Ori 87.49 82.07 82.75 79.95
DualGCN+NADS Noasp 86.86 81.23 81.49 78.02
Unite 87.67 82.59 82.75 79.72
4)BERT-ADA (Rietzler et al., 2020) utilizes do-
main data to enhance the BERT and then uses task
data to make supervised fine-tuning.
5)SDGCN+BERT (Zhao et al., 2020) employs
graph convolution network for sentences with mul-
tiple aspects.
6)R-GAT+BERT (Wang et al., 2020) proposes an
aspect-oriented tree and encodes new dependency
trees with a relational GAT.
7)DGEDT+BERT (Tang et al., 2020) proposes
a mutual biaffine module to jointly consider the
representations learned from Transformer and the
GNN model over the dependency tree.
8)DualGCN+BERT (Li et al., 2021) utilizes de-
pendency tree to extract syntax information and
self attention to extract semantic information.
Moreover, in order to prove the effectiveness
of our NADS framework, we adopt our NADS to
BERT-SPC, AEN+BERT and DualGCN+BERT.
5.3 Implementation Details
We utilize the bert-base-uncased English version.
Following the DualGCN (Li et al., 2021), we useLAL-Parser (Mrini et al., 2019) to get dependency
tree for DualGCN+NADS. We randomly initialize
the embedding of three sentiments and we set the
λ= 0.4,λ= 0.1during our training. The differ-
ent margins (m(pos, neu ), m(pos, neg ))are set to
(0.4, 0.6), (0.4, 0.6) for the laptop and restaurant
datasets for our NADS framework. During train-
ing, we use AdamW as the optimizer and set the
learning rate to 2×10. We train the model up
to 15 epochs with a batch size of 16.
5.4 Comparison Results
We utilize the accuracy and macro-averaged F1-
score to evaluate ABSA task. In order to better
predict the correct sentiment in the test, we adopt
three different ways to test our NADS framework.
1)Original test : utilizing the concatenation of
the original aspect and the original sentence as the
input and extract the embedding for prediction.
2)Noasp test : utilizing the concatenation of the “ <
aspect > ” and the no-aspect template as the input
of encoder. This test method can help us judge
whether the model can correctly predict sentiment1604without knowing the specific meaning of the aspect
like human beings.
3)Unite test : using both Original test mode and
Noasp test mode to get the scores of each label for
each sentence-aspect pair and sum the scores of
same label after normalization.
Table 2 shows our main experimental results. As
we can see, our NADS framework outperforms all
baselines on laptop and restaurant datasets, and
the performance of the three traditional models:
BERT-SPC, AEN+BERT and DualGCN+BERT
has been improved after adding our NADS frame-
work. Our NADS outperforms the BERT-SPC by
3.03%/3.13% on Restaurant/Laptop. The result
demonstrates that our NADS framework effectively
utilizes the way of human cognitive and plays a
better role in the ABSA task. Compared with tradi-
tional methods, our no-aspect template eliminates
the sentiment bias of aspect and learns more in-
formation of a group of sentence patterns, which
can reduce the noise caused by aspect sentiment
bias and enhance the robustness of our framework.
Moreover, our differential sentiment loss can better
classify the sentiment through distinguishing the
difference in these three different sentiment polar-
ities after contrastive learning. The experiments
on three traditional methods also show that our
framework well fits most of the existing models
and boosts their performance.
In parallel, according to the experimental re-
sults of Noasp test, the performance of traditional
methods drops significantly without knowing what
aspect is. However, our NADS framework can
still perform well without knowing the aspect just
like human beings. In Noasp test mode, BERT-
SPC drops 2.69%/3.52% on Restaurant/Laptop.
By contrast, our NADS framework drops only
0.45%/1.11% on Restaurant/Laptop. We also
find AEN+NADS increases 2.51% on Restaurant
dataset, while the AEN+BERT drops 2.42%. This
shows that our NADS can still perform well with-
out knowing the specific meaning of the aspect.
Comparing these three test modes, we can also find
that the Unite test mode can achieve the most stable
result in different models.
5.5 Ablation Study
In order to further study the role of different mod-
ules in our framework, we conduct extensive abla-
tion experiments. The results are shown in Table
3. NADS w/o NOASP denotes that we only utilize
the original sentence and remove the contrastive
learning. Without contrastive learning between
no-aspect template and original sentence, the senti-
ment bias of aspect perplexes the prediction result
and more importantly, differential sentiment loss
will not work because of the anisotropic in BERT
model. Therefore, its performance is degraded on
both two datasets. NADS w/o MAP means that we
remove the masked aspect prediction module so
that we may lose the original semantic information
of the sentence. NADS w/o DS indicates that we
utilize the cross-entropy loss function instead of
our differential sentiment loss. Without differential
sentiment loss, the model cannot find the different
distances between sentiments. Experiments show
that every module is indispensable in our NADS
framework.
5.6 Selection of Margin
We experiment with different margins in the differ-
ential sentiment loss. In our framework, we only
consider the m(pos, neu )andm(pos, neg ). Fig-
ure 4 shows the accuracy of different m(pos, neg )
when we set m(pos, neu ) = 0.4in our three meth-
ods based on our NADS framework on Laptop
dataset. As we can see, the accuracy increases first
and then decreases in the process of m(pos, neg )
gradually increasing. The three models perform
best when the m(pos, neg )is set to 0.6, 0.5 and
0.7. This experiment shows that the distance be-1605
tween positive and negative is indeed farther than
that between positive and neutral. It proves the
effectiveness of our differential sentiment loss.
5.7 Sentiment Bias Elimination
In order to better understand the ability of our
NADS framework to eliminate sentiment bias, we
find several examples whose labels are neutral and
show their prediction results in different models
in Table 4, where P, N, O represent positive, neg-
ative, and neutral sentiments. We highlight the
aspect words in red. We can see that our NADS
framework outperforms all other models. For the
aspect “ steak ” in the first sample, previous methods
ignore the positive sentiment bias of “ steak ” and
incorrectly predict the sentiment as positive. In
contrast, our NADS eliminates the positive senti-
ment bias through no-aspect template and predicts
the correct sentiment as neutral. Moreover, we also
show the distribution of bad cases in Figure 5. The
bad cases of neutral aspects terms in our NADS
framework are significantly less than BERT-SPC.
This proves the effectiveness of our NADS frame-
work in eliminating sentiment bias. However, there
are still some neutral aspect terms in our framework
that are incorrectly predicted as shown in Table 4.
A possible reason is that there may be other words
in a sentence carrying the sentiment bias in addition
to the current aspect.
5.8 Robustness Study
In order to verify the robustness of our NADS, we
test the robustness score of our framework on the
Aspect Robustness Test Set (ARTS) (Xing et al.,
2020). The datasets enrich 14Lap and 14Rest ac-
cording to three strategies: reverse the original sen-
timent of the target aspect (REVTGT), perturb the
sentiments of the non-target aspects (REVNON)
and generate more non-target aspect terms that have
opposite sentiment polarities to the target (AD-
DDIFF). They take the original sentence and the
three variants as an unit. Only if the original sen-
tence and all variants are correct, the unit is correct.
Calculate the accuracy of the units in the datasets
as the final Aspect Robustness Score (ARS).
We compare the ARS of the three models be-1606fore adding our NADS framework and after adding
NADS. The results in Table 5 show that the ARS
of the model has been significantly improved after
adding our NADS framework. Our NADS frame-
work adding DualGCN performs significantly bet-
ter than other models with 21.33% and 21.93%
decline on Restaurant and Laptop. This shows that
our framework utilizing human cognition has better
robustness than other models.
Moreover, we utilize the three test modes to test
on the ARTS as shown in Table 6. As we can see,
the AEN+NADS model with 57.39% and 44.18%
decline on Restaurant and Laptop when using Orig-
inal test mode. However, with 26.51% and 22.34%
decline when utilizing Noasp test mode. In the
overall scheme, the Noasp test mode and Unite test
mode can get a more stable result than the Original
test mode on the ARTS. Utilizing no-aspect tem-
plate in test may be a more stable robustness test
method.
6 Conclusion
In this paper, we propose a NADS framework
which is more in line with human cognition for
the ABSA task. Our NADS framework utilizes
no-aspect contrastive learning to eliminate the sen-
timent bias of aspects and enhance the sentence
representations. In addition, we construct a dif-
ferential sentiment loss to better classify the sen-
timents through distinguishing the different dis-
tances between sentiment polarities. Extensive ex-
periments show that our NADS framework boosts
three typical ABSA methods and outperforms base-
lines. Moreover, our NADS framework can still
perform well even we don’t know what the aspect
is. The test on the robustness dataset shows that
our NADS framework significantly improves the
robustness of the model.
Acknowledgements
This paper is supported by Pilot Projects of Chinese
Academy of Sciences (NO.XDC02030000).
References160716081609