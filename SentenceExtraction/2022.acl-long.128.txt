
Zhihong Shao, Minlie Huang
The CoAI group, DCST, Tsinghua University, Institute for Artiﬁcial Intelligence;
State Key Lab of Intelligent Technology and Systems;
Beijing National Research Center for Information Science and Technology;
Tsinghua University, Beijing 100084, China
szh19@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn
Abstract
Open-domain questions are likely to be open-
ended and ambiguous, leading to multiple
valid answers. Existing approaches typically
adopt the rerank-then-read framework, where
a reader reads top-ranking evidence to pre-
dict answers. According to our empirical
analysis, this framework faces three problems:
ﬁrst, to leverage a large reader under a mem-
ory constraint, the reranker should select only
a few relevant passages to cover diverse an-
swers, while balancing relevance and diver-
sity is non-trivial; second , the small read-
ing budget prevents the reader from access-
ing valuable retrieved evidence ﬁltered out by
the reranker; third , when using a generative
reader to predict answers all at once based
on all selected evidence, whether a valid an-
swer will be predicted also pathologically de-
pends on the evidence of some other valid an-
swer(s). To address these issues, we propose to
answer open-domain multi-answer questions
with a recall-then-verify framework, which
separates the reasoning process of each answer
so that we can make better use of retrieved
evidence while also leveraging large mod-
els under the same memory constraint. Our
framework achieves state-of-the-art results on
two multi-answer datasets, and predicts signif-
icantly more gold answers than a rerank-then-
read system that uses an oracle reranker.
1 Introduction
Open-domain question answering (V oorhees, 1999;
Chen et al., 2017) is a long-standing task where a
question answering system goes through a large-
scale corpus to answer information-seeking ques-
tions. Previous work typically assumes that there
is only one well-deﬁned answer for each question,
or only requires systems to predict one correct an-
swer, which largely simpliﬁes the task. However,
humans may lack sufﬁcient knowledge or patience
Table 1: An example of open-domain multi-answer
questions. We display only a subset of valid answers.
In fact, [You Don’t Know Jack] can also be a song.
to frame very speciﬁc information-seeking ques-
tions, leading to open-ended and ambiguous ques-
tions with multiple valid answers. According to
Min et al. (2020b), over 50% of a sampled set of
Google search queries (Kwiatkowski et al., 2019)
are ambiguous. Figure 1 shows an example with at
least three interpretations. As can be seen from this
example, the number of valid answers depends on
both questions and relevant evidence, which chal-
lenges the ability of comprehensive exploitation of
evidence from a large-scale corpus.
Existing approaches mostly adopt the rerank-
then-read framework. A retriever retrieves hun-
dreds or thousands of relevant passages which are
later reranked by a reranker; a generative reader
then predicts all answers in sequence conditioned
on top-ranking passages. With a ﬁxed memory
constraint, there is a trade-off between the size of
the reader and the number of passages the reader
can process at a time. According to Min et al.
(2021), provided that the reranker is capable of1825selecting a small set of highly-relevant passages
with high coverage of diverse answers, adopting a
larger reader can outperform a smaller reader us-
ing more passages. However, as shown by Section
4.4, this framework is faced with three problems:
ﬁrst, due to the small reading budget, the reranker
has to balance relevance and diversity, which is
non-trivial as it is unknown beforehand that which
answers should be distributed with more passages
to convince the reader and which answers can be
safely distributed with less to save the budget for
the other answers; second , the reader has no access
to more retrieved evidence that may be valuable
but is ﬁltered out by the reranker, while combining
information from more passages was found to be
beneﬁcial to open-domain QA (Izacard and Grave,
2021b); third , as the reader predicts answers in se-
quence all at once, the reader learns pathological
dependencies among answers, i.e., whether a valid
answer will be predicted also depends on passages
that cover some other valid answer(s), while ideally,
prediction of a particular answer should depend on
the soundness of associated evidence itself.
To address these issues, we propose to answer
open-domain multi-answer questions with a recall-
then-verify framework. Speciﬁcally, we ﬁrst use an
answer recaller to predict possible answers from
each retrieved passage individually; this can be
done with high recall, even when using a weak
model for the recaller, but at the cost of low preci-
sion due to insufﬁcient evidence to support or refute
a candidate. We then aggregate retrieved evidence
relevant to each candidate, and verify each candi-
date with a large answer veriﬁer. By separating the
reasoning process of each answer, our framework
avoids the problem of multiple answers sharing a
limited reading budget, and makes better use of re-
trieved evidence while also leveraging strong large
models under the same memory constraint.
Our contributions are summarized as follows:
•We empirically analyze the problems faced by
thererank-then-read framework when dealing
with open-domain multi-answer QA.
•To address these issues, we propose to answer
open-domain multi-answer questions with a
recall-then-verify framework, which makes
better use of retrieved evidence while also
leveraging the power of large models under
the same memory constraint.
•Our framework establishes a new state-of-the-art record on two multi-answer QA datasets
with signiﬁcantly more valid predictions.
2 Related Work
Open-domain QA requires question answering sys-
tems to answer factoid questions by searching
for evidence from a large-scale corpus such as
Wikipedia (V oorhees, 1999; Chen et al., 2017).
The presence of many benchmarks has greatly pro-
moted the development of this community, such as
questions from real users like NQ (Kwiatkowski
et al., 2019) and WQ (Berant et al.,
2013), and trivia questions like Quasar-T (Dhingra
et al., 2017) and TriviaQA (Joshi et al., 2017). All
these benchmarks either assume that each question
has only one answer with several alternative sur-
face forms, or only require a system to predict one
valid answer. A typical question answering system
is a pipeline as follows: an efﬁcient retriever re-
trieves relevant passages using sparse (Mao et al.,
2021; Zhao et al., 2021) or dense (Karpukhin et al.,
2020; Xiong et al., 2021; Izacard and Grave, 2021a;
Khattab et al., 2021) representations; an optional
passage reranker (Asadi and Lin, 2013; Nogueira
and Cho, 2019; Nogueira et al., 2020) further nar-
rows down the evidence; an extractive or genera-
tive reader (Izacard and Grave, 2021b; Cheng et al.,
2021) predicts an answer conditioned on retrieved
or top-ranking passages. Nearly all previous work
focused on locating passages covering at least one
answer, or tried to predict one answer precisely.
However, both Kwiatkowski et al. (2019) and
Min et al. (2020b) reported that there is genuine
ambiguity in open-domain questions, resulting in
multiple valid answers. To study the challenge of
ﬁnding all valid answers for open-domain ques-
tions, Min et al. (2020b) proposed a new bench-
mark called A QA where questions are anno-
tated with as many answers as possible. In this new
task, the passage reranker becomes more vital in
thererank-then-read framework, particularly when
only a few passages are allowed to feed a large
reader due to memory constraints. This is because
the reranker has to ensure that top-ranking passages
are highly relevant and also cover diverse answers.
Despite state-of-the-art performance on A QA
(Min et al., 2021), according to our empirical anal-
ysis, applying the rerank-then-read framework to
open-domain multi-answer QA faces the following
problems: balancing relevance and diversity is non-
trivial for the reranker due to unknown effect on the1826performance of the subsequent reader; when using
a large reader under a ﬁxed memory constraint, the
small reading budget prevents it from making use
of more retrieved evidence that is valuable but ﬁl-
tered out; when using a generative reader to predict
all answers in sequence based on all selected evi-
dence, it learns pathological dependencies among
answers. To address these issues, we propose to
tackle this task with a recall-then-verify framework,
which separates the reasoning process of each an-
swer with a higher level of evidence usage while
also leveraging large models under the same mem-
ory constraint.
Some previous work argued that a reader can be
confused by similar but spurious passages, result-
ing in wrong predictions. Therefore, they proposed
answer rerankers (Wang et al., 2018a,b; Hu et al.,
2019; Iyer et al., 2021) to rerank top predictions
from readers. Our framework is related to answer
reranking but with two main differences. First, a
reader typically aggregates available evidence and
already does a decent job of answer prediction even
without answer reranking; an answer reranker is
introduced to ﬁlter out hard false positive predic-
tions from the reader. By contrast, our answer
recaller aims at ﬁnding possible answers with high
recall, most of which are invalid. Evidence focused
on each answer is then aggregated and reasoned
about by our answer veriﬁer. It is also possible to
introduce another model analogous to an answer
reranker to ﬁlter out false positive predictions from
our answer veriﬁer. Second, answer reranking typ-
ically compares answer candidates to determine
the most valid one, while our answer veriﬁer se-
lects multiple valid answers mainly based on the
soundness of their respective evidence but without
comparisons among answer candidates.
3 Task Formulation
Open-domain multi-answer QA can be formally
deﬁned as follows: given an open-ended question
q, a question answering system is required to make
use of evidence from a large-scale text corpus C
and predict a set of valid answers fa;a;:::;ag.
Questions and their corresponding answer sets are
provided for training.
Evaluation To evaluate passage retrieval and
reranking, we adopt the metric MR @kfrom
(Min et al., 2021), which measures whether the
top-kpassages cover at least kdistinct answers
(ornanswers if the total number of answers nisless thank). To evaluate question answering per-
formance, we follow (Min et al., 2020b) to use F1
score between gold answers and predicted ones.
4 Rerank-then-Read Framework
In this section, we will brieﬂy introduce the rep-
resentative and state-of-the-art rerank-then-read
pipeline from (Min et al., 2021) for open-domain
multi-answer questions, and provide empirical anal-
ysis of this framework.
4.1 Passage Retrieval
Dense retrieval is widely adopted by open-domain
question answering systems (Min et al., 2020a). A
dense retriever measures relevance of a passage to
a question by computing the dot product of their
semantic vectors encoded by a passage encoder and
a question encoder, respectively. Given a question,
a set of the most relevant passages, denoted as B
(jBjjCj ), is retrieved for subsequent processing.
4.2 Passage Reranker
To improve the quality of evidence, previous work
(Nogueira et al., 2020; Gao et al., 2021) ﬁnds it ef-
fective to utilize a passage reranker, which is more
expressive than a passage retriever, to rerank re-
trieved passages, and select the kbest ones to feed
a reader for answer generation ( k <jBj). With
a ﬁxed memory constraint, there is a trade-off be-
tween the number of selected passages and the size
of the reader. As shown by (Min et al., 2021),
with good reranking, using a larger reader is more
beneﬁcial. To balance relevance and diversity of
evidence, Min et al. (2021) proposed a passage
reranker called JPR for joint modeling of selected
passages. Speciﬁcally, they utilized T5-base (Raf-
fel et al., 2020) to encode retrieved passages fol-
lowing (Izacard and Grave, 2021b) and decode the
indices of selected passages autoregressively using
a tree-decoding algorithm. JPR is designed to seek
for passages that cover new answers, while also
having the ﬂexibility to select more passages cov-
ering the same answer, especially when there are
less thankanswers for the question.
4.3 Reader
A reader takes as input the top-ranking passages,
and predicts answers. Min et al. (2021) adopted a
generative encoder-decoder reader initialized with
T5-3b, and used the fusion-in-decoder method from
(Izacard and Grave, 2021b) which efﬁciently ag-1827gregates evidence from multiple passages. Speciﬁ-
cally, each passage is concatenated with the ques-
tion and is encoded independently by the encoder;
the decoder then attends to the representations of
all passages and generates all answers in sequence,
separated by a [SEP] token.
4.4 Empirical Analysis
To analyze performance of the rerank-then-read
framework for open-domain multi-answer ques-
tions, we built a system that resembles the state-
of-the-art pipeline from (Min et al., 2021) but with
two differences. First, we used the retriever from
(Izacard and Grave, 2021a). Second, instead of us-
ing JPR, we used an oracle passage reranker (OPR):
a passagepis ranked higher than another passage
pif and only if 1) pcovers some answer while
pcovers none 2) or both pandpcover or fail
to cover some answer but phas a higher retrieval
score. Following (Min et al., 2021), we retrieved
jBj=100 Wikipedia passages, k=10 of which were
selected by the reranker. Table 2 shows model per-
formance on a representative multi-answer dataset
called A QA (Min et al., 2020b). Compared
with JPR, OPR is better in terms of reranking, with
similar question answering results.
Though 3,670 diverse gold answers are covered
by OPR on the dev set, the reader predicts only
1,554 of them. Our empirical analysis and ﬁndings
are detailed as follows.
(1) To leverage a large reader under a ﬁxed mem-
ory constraint, a reranker should select only a few
highly-relevant passages to cover diverse answers,
while balancing relevance and diversity is non-
trivial. As shown by Figure 1a (bottom), the num-
ber of selected supporting passagesof predicted
gold answers has a widespread distribution. There
may be cases where redundant false positive evi-
dence is selected and can be safely replaced with
passages that cover other gold answers. However,
it is non-trivial for the reranker to know beforehand
whether a passage is redundant, and how many or
which supporting passages of an answer are strong
enough to convince the reader.
(2) Multiple answers sharing a small reading
budget prevents a reader from using more evidence
that may be valuable but is ﬁltered out by the
reranker. Due to the shared reading budget, it is
inevitable that some answers are distributed with
less supporting passages. As shown by Figure 1a,
a gold answer covered by OPR but missed by the
reader generally has signiﬁcantly less supporting
passages fed to the reader (3.13 on average) than
a predicted gold answer (5.08 on average), but not
because of lacking available evidence. There is
more evidence in retrieved passages for missed an-
swers but ﬁltered out by the reranker. As shown by
Figure 1b, OPR has a much lower level of evidence
usage for missed answers.
(3) As the reader predicts answers all at once
conditioned on all selected passages, whether a
valid answer will be predicted also pathologically
depends on evidence of some other valid answer(s),
which partly accounted for the large number of1828
gold answers missed by the reader. For veriﬁca-
tion, we attacked OPR’s reader on the dev set of
A QA as follows: a question is a target if and
only if 1) it has a gold answer covered by OPR
but missed by the reader 2) and it has a predicted
gold answer whose supporting passages cover no
other gold answer; a successful attack on a targeted
question means that a missed answer is recovered
after removing a subset of supporting passages of
some predicted answerwithout removing any sup-
porting passage of the other gold answers.
There are 179 targeted questions; for 43.6% of
them, we successfully recovered at least one missed
gold answer. Figure 3 shows the success rate break-
down on the number of answers covered by the
reader’s input, indicating that predictions tend to
be brittle when the reader is fed with many diverse
supporting passages.
One possible explanation of the pathological de-
pendencies is that the reader implicitly comparesthe validity of answer candidates and predicts the
most likely ones. However, for 40.0% of success-
fully attacked questions, according to OPR, sup-
porting passages of recovered missed answers are
more relevant than those removed passages of pre-
dicted answers. Notably, Min et al. (2020b) also
had a similar observation on another rerank-then-
read pipeline, i.e., it is hard to argue that the pre-
dicted answers are more likely than the missed
ones.
5 Recall-then-Verify Framework
5.1 Overview
To avoid the issues faced by the rerank-then-read
framework, we propose a recall-then-verify frame-
work, which separates the reasoning process of
each answer so that answers (1) can be individ-
ually distributed with maximum supporting pas-
sages allowed on the same hardware (2) and are
predicted mainly based on their own evidence. Fig-
ure 2 shows our framework. Speciﬁcally, we ﬁrst
guess possible answers based on retrieved passages
using an answer recaller, an evidence aggregator
then aggregates evidence for each answer candi-
date, and ﬁnally, an answer veriﬁer veriﬁes each
candidate and outputs valid ones.
5.2 Answer Recaller
Our answer recaller, based on T5, is trained to pre-
dict all gold answer(s) in sequence (separated by a
[SEP] token) from each retrieved positive passage
p2B that cover some gold answer(s). We also
train the recaller to predict the “irrelevant” token
given a negative passage so that the recaller can
ﬁlter out negative candidates; the number of neg-
atives per positive used for training is denoted as
. The set of answer candidates recalled dur-
ing inference is denoted as A=f^a;^a;:::;^ag.1829Though a passage may not contain strong enough
evidence to support an answer, by exploiting se-
mantic clues in the question and the passage (e.g.,
the answer type), it is sufﬁcient for even a weak
model to achieve high recall. However, this is at the
cost of low precision, which necessitates answer
veriﬁcation based on more supporting passages.
5.3 Evidence Aggregator
We aggregate evidence for each answer candidate
from retrieved passages, which can be formulated
as a reranking task, i.e., to rerank retrieved passages
according to their relevance to a question-candidate
pair, and select top-ranking ones for answer veriﬁ-
cation. Our evidence aggregator resembles OPR:
for a speciﬁc candidate ^a, we encode the question-
candidate pair with the retriever’s question encoder;
a passagepis ranked higher than another passage
pif and only if 1) pcovers ^awhilepdoes not
2) or bothpandpcover or fail to cover ^abut
the semantic vector of pis closer to that of the
question-candidate pair. We denote the top- krele-
vant passages of ^aasE.
5.4 Answer Veriﬁer
Given a candidate ^aand its evidenceE, our an-
swer veriﬁer, based on T5-3b, predicts whether ^a
is valid, using the fusion-in-decoder method from
(Izacard and Grave, 2021b). Each passage from E
is concatenated with the question and the candidate,
and is encoded independently; the decoder then at-
tends to the representations of all passages and is
trained to produce the tokens “right” or “wrong”
depending on whether the encoded candidate is
valid or not. During inference, we compute the va-
lidity score of a candidate by taking the normalized
probability assigned to the token “right”:
Candidates with their validity scores higher than a
thresholdwill be produced as ﬁnal predictions.
6 Experiments
6.1 Datasets
We conducted experiments on two multi-answer
QA datasets, whose statistics are shown in Table 3.WQSP (Yih et al., 2016) is a semantic parsing
dataset for knowledge base question answering,
where answers are a set of entities in Freebase.
Following (Min et al., 2021), we repurposed this
dataset for textual QA based on Wikipedia.
A QA(Min et al., 2020b) originates from NQ
(Kwiatkowski et al., 2019), where questions are an-
notated with equally valid answers from Wikipedia.
6.2 Baselines
We compare our recall-then-verify system with two
state-of-the-art rerank-then-read systems.
R (Gao et al., 2021) selects 100 top-ranking
passages from 1,000 retrieved passages, and pre-
dicts answers with a reader based on BART
(Lewis et al., 2020). It also has a round-trip pre-
diction mechanism, i.e., to generate disambiguated
questions based on predicted answers, which are
re-fed to the reader to recall more answers.
JPR (Min et al., 2021) is a passage reranker which
jointly models selected passages. With improved
reranking performance, Min et al. (2021) selected
only 10 passages from 100 retrieved passages, and
used a reader based on T5-3b which is much larger
and more powerful than R ’s reader, while
requiring no more memory resources than R .
6.3 Implementation Details
Our retrieval corpus is the English Wikipedia from
12/20/2018. We ﬁnetuned the dense retriever from
(Izacard and Grave, 2021a) on each multi-answer
dataset. The answer recaller and the answer veriﬁer
were initialized with T5-3b; both were pre-trained
on NQ and then ﬁnetuned on each multi-answer
dataset.was 0.1 when ﬁnetuning the recaller.
We retrieved 100 passages for a question, and
veriﬁed each candidate with k=10 passages. The
thresholdfor veriﬁcation was tuned on the dev
set based on the sum of F1 scores on all questions
(F1 (all)) and questions with multiple answers (F1
(Multi)); the best on WQSP/A QA are1830
0.8/0.5, respectively. Experiments with different
model choices for the recaller and different
values of,kandare shown in Section 6.5.
Memory Constraint: Min et al. (2021) consid-
ered a ﬁxed hardware and trained a reader with
the maximum number of passages. We follow this
memory constraint, under which a reader/veriﬁer
based on T5-3b can encode up to 10 passages each
of length no longer than 360 tokens at a time.
6.4 QA Results
Due to candidate-aware evidence aggregation and
a ﬁxed sufﬁcient number of passages distributed
to each candidate, our recall-then-verify frame-
work can make use of most retrieved support-
ing passages (see our improvements over OPR
in Figure 1b). With a higher level of evidence
usage, our recall-then-verify system outperforms
state-of-the-art rerank-then-read baselines on both
multi-answer datasets, which is shown by Table
4. Though focused on multi-answer questions, our
framework is also applicable to single-answer sce-
nario and achieves state-of-the-art results on NQ.
Please refer to the Appendix for more details.
6.5 Ablation Study
In this section, we present ablation studies on
A QA. Please refer to the Appendix for results
on WQSP, which lead to similar conclusions.
6.5.1 Answer Recalling
Model Choices for the Answer Recaller As
shown by Table 5, though T5-base is commonly
recognized as a much weaker model than T5-3b,
a recaller based on T5-base can achieve a high
coverage of gold answers, leading to competitive
end-to-end performance on the test set.
Necessity of Veriﬁcation To investigate whether
the recaller has the potential to tackle multi-answer
questions alone, we tuned the precision of the re-
caller by varying . As shown in Table 5, with
increased, the recaller learns to recall answers
more precisely but still signiﬁcantly underperforms
the overall recall-then-verify system. It is likely
that the recaller is trained on false positive pas-
sages, which may mislead the recaller to be over-
conservative in ﬁltering out hard negative passages.
By contrast, using more evidence for veriﬁcation is
less likely to miss true positive evidence if there is
any for a candidate, thus not prone to mislead the
veriﬁer.
Reducing Answer Candidates Though only us-
ing our recaller for multi-answer QA falls short, the
recaller can be trained to shrink down the number
of candidates so that the burden on the veriﬁer can
be reduced. As shown by Table 5, a small value
ofhelps reduce answer candidates without
signiﬁcantly lowering recall.
6.5.2 Answer Veriﬁcation1831
Effect ofkFigure 4 shows the beneﬁt of using
more evidence for veriﬁcation. As kincreases from
1 to 10, there is a signiﬁcant boost in F1 scores.
Effect ofAs shown by Figure 4a, the balance be-
tween recall and precision can be controlled by :
a lowerleads to higher recall and may beneﬁt per-
formance on questions with multiple answers. With
k=10, our system outperforms the previous state-of-
the-art system for a wide range of . As shown by
Figure 4b, under the best setups ( k=10,=0.5), our
system predicts 31.7% and 34.1% more gold an-
swers than the system using OPR on all questions
and questions with multiple answers, respectively.
Dependencies among Answers Despite being
candidate-aware, aggregated evidence Ecan also
include supporting passages of some other gold
answer(s). We therefore investigated how answer
veriﬁcation is affected by the evidence of the other
gold answers. Speciﬁcally, we attacked the veriﬁer
as follows: a question-candidate pair is a target if
and only if 1) the candidate ^ais a gold answer and
2) the aggregated evidence Eincludes at least one
supporting passage of some other gold answer(s)
that do not cover ^a; we removed an arbitrary subset
of supporting passages of the other gold answer(s)
at a timewithout removing any supporting pas-
sages of ^a, and recorded the worst changes of the
predicted validity scores of ^a. As shown by Figure
5, the changes are small, indicating that missed
gold candidates with low scores are not mainly
suppressed by some other answer(s), and that pre-
dicted gold candidates with high scores are veriﬁedmainly based on their associated evidence.
6.6 Error Analysis
Among 3,288 recalled gold answers on the dev
set of A QA, the answer veriﬁer misses 1,242
of them and outputs 1,323 wrong predictions. We
manually analyzed 50 random samples, 25 of
which are missed gold answers and 25 are wrong
predictions. Table 6 reports our analysis.
For 76% of missed gold answers, our evidence
aggregator actually aggregates straightforward true
positive evidence. Among these missed answers
with straightforward evidence, 58% of them have
validity scores higher than 0.2 but lower than the
threshold 0.5. We attacked the veriﬁer on missed
gold answers with their validity scores below 0.2
as in Section 6.5.2, and found that the maximum
change of predicted scores on average is small
(+0.04), indicating that the low scores can not be
attributed to the negative distraction by the other
gold answer(s). We conjecture that, as it is difﬁcult
even for human annotators to ﬁnd all valid answers
to an open-domain question (Min et al., 2020b), the
veriﬁer was trained to refute false negative candi-
dates, resulting in unexpected low scores on some
straightforward valid answers.
Notably, 80% of our “wrong” predictions turn
out to be false negatives: 52% of “wrong” pre-
dictions are semantically equivalent to some an-
notated answer but are superﬁcially different (Si
et al., 2021); 28% of “wrong” predictions are unan-
notated false negatives. Therefore, it is likely that
our system is underrated.
6.7 Inference Efﬁciency
In this section, we analyze the time complexity
of our framework during inference, make com-
parisons with the state-of-the-art rerank-then-read
framework JPR, and show how to reduce the com-
putation cost of a recall-then-verify system.
For convenience, we denote the encoder length
and decoder length as LandL, respectively.1832
Recaller vs. Reranker The time complexity of
answer recalling is O(jBj(L+LL+L)),
while that of passage reranking is O(jBjL+k
jBjL+k). As encoding dominates computation
cost (whose time complexity is O(jBjL)), given
the same model size and jBj, the time complexity
of answer recalling and passage reranking is at the
same level.
Veriﬁer vs. Reader The time complexity of an-
swer veriﬁcation is O(jAj(kL+kL)), while
that of the reader is O(kL+LkL+L).
As the reader decodes a sequence of length Lin
an autoregressive way, while the decoding length
of the veriﬁer is only 1, the ratio between the in-
ference time of the veriﬁer and that of the reader
should be much less than jAj.
Evidence Aggregator Evidence aggregation is sig-
niﬁcantly faster than answer recalling and veriﬁ-
cation, as representations of Wikipedia passages
are pre-computed. The time complexity is O(jAj
(L+jBjlogk))whereLcomes from encoding
a question-candidate pair with the retriever’s ques-
tion encoder, andjBjlogkcomes from selecting
the top-krelevant passages for a candidate.
One can adjust the computation cost of a recall-
then-verify system, depending on how much infer-
ence efﬁciency is valued over precision and recall,
by (1) choosing a recaller model of proper time
complexity, (2) tuningto adjust the expected
number of candidates jAjneeded for veriﬁcation,
(3) or tuning the number of passages kused for
veriﬁcation.
Table 7 shows QA performance and inference
efﬁciency of our systems with different conﬁgu-
rations. Replacing T5-3b with T5-base for the re-
caller is signiﬁcantly faster in answer recalling, but
is much less precise and produces more answer
candidates with the same , which increases the
burden on the veriﬁer and thus may fail to reduce
the overall computation cost if is not raised.By also increasing and choosing a smaller k,
as shown by the last row of Table 7, the overall
time needed to answer a question on the dev set of
A QA can be reduced to 1.88 sec on a single
V100 GPU while also obtaining state-of-the-art F1
scores (50.7/38.2). By contrast, the rerank-then-
read system from Min et al. (2021) using a T5-base
JPR (k=10) and a T5-3b reader is estimated to take
1.51 sec per questionwith F1 scores of 48.5/37.6.
7 Conclusion
In this paper, we empirically analyze the prob-
lems of the rerank-then-read framework for open-
domain multi-answer questions, and propose the
recall-then-verify framework, which separates the
reasoning process of each answer so that 1) we
can have a higher level of evidence usage 2) and
predicted answers are mainly based on associated
evidence and are more robust to distraction by ev-
idence of the other gold answer(s), 3) while also
leveraging large models under the same memory
constraint. On two multi-answer datasets, our
framework signiﬁcantly outperforms rerank-then-
read baselines with new state-of-the-art records.
Acknowledgements
This work was supported by the National Science
Foundation for Distinguished Young Scholars (with
No. 62125604) and the NSFC projects (Key project
with No. 61936010 and regular project with No.
61876096). This work was also supported by the
Guoqiang Institute of Tsinghua University, with
Grant No. 2019GQG1 and 2020GQG0005.
Ethical Considerations
To address problems of the rerank-then-read frame-
work for open-domain multi-answer QA, we pro-1833pose a recall-then-verify framework that will hope-
fully beneﬁt information-seeking users with an en-
hanced ability of comprehensive exploitation of
evidence from a large-scale corpus. As our pre-
dictions are veriﬁed with textual knowledge, our
system itself would not raise new signiﬁcant ethical
concerns. All the datasets as well as the retrieval
corpus in our experiments have been widely used
for research purposes, and to our knowledge, do
not have any attached privacy and ethical issues.
References18341835A Implementation Details
A.1 Retriever
Our retrieval corpus is the English Wikipedia from
12/20/2018, where articles are split into 100-word
passages. Both OPR and our recall-then-verify
system share the same passage retriever, which was
initialized with the checkpoint released by (Izacard
and Grave, 2021a) and was ﬁnetuned on each multi-
answer dataset following DPR (Karpukhin et al.,
2020). Speciﬁcally, for each question, we retrieved
100 passages with Izacard and Grave (2021a)’s
checkpoint; for each gold answer a, we treated
top-6 retrieved passages covering aas positives,
and top-30 retrieved passages covering no gold
answer as hard negatives. During ﬁnetuning, batch
size was set to 128; each question in a batch was
paired with one random positive passage and two
random hard negatives.
Table 8 shows the performance of our retriever.
Our retriever underperforms DPR, the retriever of
JPR (Min et al., 2021), in terms of MR @5
and MR @10. As DPRhas not been re-
leased, it is unknown whether DPRstill covers
more gold answers than our retriever when retriev-
ing 100 passages.
A.2 Answer Recaller & Answer Veriﬁer
Our answer recallers used an encoder length of 240
and a decoder length of 40; they were ﬁrst pre-
trained on NQ for 10 epochs and then ﬁnetuned on
WQSP/A QA for 80/20 epochs with early
stopping. Batch size was set to 320. We trained
the recallers to decode gold answers covered by
a given positive passage (following the order they
appear in the passage) and output the “irrelevant”
token given a negative passage. Our best system
adopts the recaller trained with =0.1 because
compared with =0, the recaller trained with1836=0.1 shrinks down nearly half of answer can-
didates without a signiﬁcant drop in recall.
Our answer veriﬁers used an encoder length
of 280; they were ﬁrst pre-trained on NQ for 3
epochs and then ﬁnetuned on WQSP/A QA
for 30/10 epochs with early stopping. Batch
size was set to 320 for k=1 and set to 64 for
k2f5;10g. The number of invalid answers used
for training was set to 10 times the number of
valid answers. The best threshold was chosen
fromf0:3;0:4;0:5;0:6;0:7;0:8;0:9gbased on F1
scores on the dev set.
We used a ﬂat learning rate of 1e-5 with 500
warm-up steps. All experiments were conducted
on a single machine with eight V100 GPUs.
B Experiments
B.1 Single-Answer QA Result
Though our framework focuses on multi-answer
questions, we also experimented on NQ to demon-
strate that our framework is applicable to single-
answer scenario without suffering from low pre-
cision. Speciﬁcally, for each question, we only
output the candidate with the highest validity score.
As shown by Table 9, we slightly outperform pre-
vious state-of-the-art rerank-then-read systems.
B.2 Ablation Study on WQSP
B.2.1 Answer Recalling
Table 10 shows the results of recallers on WQSP,
which were trained with different models and .
In summary, a weak model sufﬁces to recall an-
swers with high coverage. Using a large and strong
model for the recaller beneﬁts precision, but it is
still difﬁcult for the recaller alone to answer open-
domain multi-answer questions, which necessitates
answer veriﬁcation based on more associated evi-
dence. However, an answer recaller can help reduce
the burden on the answer veriﬁer by conservatively
ﬁltering out negative candidates.
Though a recall-then-verify system with a re-
caller based on T5-base signiﬁcantly outperforms
JPR on WQSP, it lags behind the system with a
T5-3b recaller on F1 (all) on the test set. We con-
jecture that this is because with the same =0.1,
a T5-base recaller obtains lower recall (69.5/61.4)
than a T5-3b recaller (72.1/63.3); a T5-base recaller
may need an even lower value of to obtain a
higher coverage of gold answers.
B.2.2 Answer Veriﬁcation
As shown by Figure 6, F1 scores on WQSP
are insensitive to a wide range of , while a lower
is helpful to predict more gold answers.
C Error Analysis
Table 11 reports our error analysis on the dev set
of A QA.1837Missed Gold Answers > Evidence is wrong (24%)
Question: Who does brooke davis have a baby with?
Gold Answers: Julian Baker
Missed Gold Answer: Julian Baker
Evidence: Brooke Davis is happier than ever; preparing to marry Julian Baker ... The Scott family are
expecting their second child and Haley feels the baby will be a girl ...
Explanation: Evidence is insufﬁcient to infer Brooke Davis has a baby with Julian Baker.
Missed Gold Answers > Evidence is right and straightforward (76%)
Question: What’s the most points scored in an nba game?
Gold Answers: 370; 153; 162; 100; 186
Missed Gold Answer: 162
Evidence: The 1971-72 team holds franchise records in wins (69), most points scored, and largest
margin of victory; both of the latter came in the team’s 63 point win versus Golden State (162-99).
Wrong Predictions > Predictions are true negatives (20%)
Question: When did the song lost boy come out?
Gold Answers: February 12, 2015; January 2015; 4 December 2015; May 9, 2016; 2015; 2017;
November 17, 2017
Prediction: 20 December 2011
Evidence: “The Lost Boy” was written by Holden in 2011 ... Holden recorded it and released as a
charity single on 20 December 2011 ...
Explanation: “Lost Boy” and “The Lost Boy” are different songs.
Wrong Predictions > Predictions are superﬁcially-different false negatives (52%)
Question: How much sports are there in the winter olympics?
Gold Answers: ﬁfteen; 86; 98; seven; 102
Prediction: 15
Evidence: ... the Winter Olympics programme features 15 sports.
Wrong Predictions > Predictions are unannotated false negatives (28%)
Question: How much did it cost rio to host the olympics?
Gold Answers: US$11.6 billion; US$13,100,000,000
Prediction: USD 4.6 billion
Evidence: Indirect capital costs were “not” included, such as for road ... Rio Olympics’ cost of USD
4.6 billion compares with costs of USD 40-44 billion for Beijing 2008 ...1838