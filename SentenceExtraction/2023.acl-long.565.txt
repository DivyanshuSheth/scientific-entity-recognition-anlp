
Jin Myung Kwak, Minseon Kim, Sung Ju Hwang
KAIST, DeepAuto
{kwak.jinmyung, minseonkim, sjhwang82}@kaist.ac.kr
Abstract
Transformer-based Language Models (LMs)
have achieved impressive results on natural lan-
guage understanding tasks, but they can also
generate toxic text such as insults, threats, and
profanity, limiting their real-world applications.
To overcome this issue, a few text generation
approaches aim to detoxify toxic texts using
additional LMs or perturbations. However, pre-
vious methods require excessive memory, com-
putations, and time which are serious bottle-
necks in their real-world application. To ad-
dress such limitations, we propose an effec-
tive yet efficient method for language detox-
ification using an attribute-discriminative la-
tent space. Specifically, we project the latent
space of an original Transformer LM onto a
discriminative latent space that well-separates
texts by their attributes using a projection block
and an attribute discriminator. This allows the
LM to control the text generation to be non-
toxic with minimal memory and computation
overhead. We validate our model, Attribute-
Discriminative Language Model (ADLM) on
detoxified language and dialogue generation
tasks, on which our method significantly out-
performs baselines both in performance and
efficiency.
1 Introduction
Pre-training language models (LMs) on large-scale
web text corpora (i.e., Common Crawl and Open-
WebTextCorpus (Gokaslan and Cohen, 2019)) has
significantly improved their language generation
performances (Radford et al., 2019; Yang et al.,
2019; Dai et al., 2019; Shoeybi et al., 2019; Li
et al., 2020; Brown et al., 2020), by allowing them
to learn meaningful relations between words. How-
ever, since the models are trained on massive web-
crawled text data which is not exhaustively filtered,Figure 1: Memory and computational efficiency vs.
Exp. Max Toxicity. Comparison of toxicity of the gen-
erated texts between previous language detoxification
methods and ours, on the number of model parameters
and inference time per 100 generated texts with a single
GPU. Toxicity is calculated on random-10K prompts
from RealToxicityPrompts (Gehman et al., 2020). Our
model achieves the best language detoxification perfor-
mance while being time- and memory- efficient.
they are prone to generating unexpected and unde-
sired texts (Sheng et al., 2019; Wallace et al., 2019)
which are often also inappropriate (See Table 1).
Specifically, LMs trained on unfiltered texts
can randomly generate racial slurs, sexually ex-
plicit and violent expressions, which are highly
toxic (Groenwold et al., 2020; Luccioni and Vi-
viano, 2021; Xu et al., 2021; Dale et al., 2021).
This is one of the main obstacles in deploying pre-
trained LMs to real-world applications (e.g., con-
versational agents). Furthermore, as demonstrated
in Gehman et al. (2020); Baheti et al. (2021); Dale
et al. (2021), LMs are prone to generating toxic
language even from the non-toxic prompts or con-
texts. One simple and straightforward approach to
tackle this problem is to eliminate the toxic and
biased texts by detecting them from the training
dataset (Zhou et al., 2021; Zampieri et al., 2019).
However, as the size of LMs increases, the training
corpora have also expanded enormously (Brown
et al., 2020; Du et al., 2021). Thoroughly removing
or filtering out all toxic words or sentences from
such a large-scale corpus and retraining the LM
from scratch, could be costly and impractical (Ben-10149
der et al., 2021).
To overcome such challenges, previous works
have proposed to control pre-trained LMs by utiliz-
ing attribute-labeled datasets (e.g., toxic and non-
toxic). They modify the decoding process either
by adversarially perturbing the LM with a toxi-
city discriminator (Dathathri et al., 2020) or us-
ing additional finetuned LMs on targeted attribute
data to suppress toxic logits and amplify non-toxic
logits of the base LMs (Krause et al., 2021; Liu
et al., 2021a). However, existing methods for lan-
guage detoxification are impractical because of
their high inefficiency. The perturbation-based
method (Dathathri et al., 2020) slows down the in-
ference time of the original GPT-2 (Radford et al.,
2019) by 40 times due to the high cost of gradient
computation. While the methods of Krause et al.
(2021) and Liu et al. (2021a) are as fast as GPT-
2, both additionally require auxiliary LMs to shift
the logits toward those of non-toxic texts, which is
memory inefficient.
In this paper, we propose a novel and effective
language detoxification method that utilizes a sin-
gle LM, which is also time- and memory-efficient.
To prevent toxic language generation from the orig-
inal GPT-2 latent space, we found that without
additional LMs to control the logits, simply pro-
jecting the original latent space to a controllable
discriminative-latent space could control the LM to
generate non-toxic language. Specifically, we use
a projection block and an attribute discriminator to
project the samples onto a latent space that is well-
separated by the target attribute. We refer to this
model as an Attribute-Discriminative LM (ADLM)
(Figure 2).
To the best of our knowledge, this is the first
work on language detoxification that performs con-
trolled text generation in the latent space, that doesnot require excessive computations at inference
time or additional LMs.
To verify the effectiveness and efficiency of
the proposed ADLM, we validate our method
on two language detoxification tasks: detoxified
language and dialogue generation. With 10K
random prompts from the RealToxicityPrompts
dataset (Gehman et al., 2020), we conduct a generic
language modeling experiment for detoxification.
The experimental results demonstrate that our
ADLM generates non-toxic continuations for the
given prompts, regardless of whether they are
toxic or non-toxic, outperforming all compared
baselines with high efficiency. On the language
detoxification task for dialogue generation (Ba-
heti et al., 2021; Sun et al., 2022), our ADLM
generates safer responses than baselines on Toxi-
Chat and DiaSafety datasets. Lastly, to further
show the general applicability of our method to
any attribute-controlled text generation tasks, we
validate ADLM on a sentiment-controlled text gen-
eration task (Socher et al., 2013) on which our
model also achieves impressive performance (Ap-
pendix D). Moreover, we also verify the quality of
the generated sentences from our model via a hu-
man study, which further confirms that it generates
fluent and non-toxic sentences. In summary, our
contributions are as follows:
•We propose a novel LM for language detoxifi-
cation, with a projected attribute-discriminative
latent space learned by training a discriminator
to classify texts by their attributes.
•We introduce a time- and memory-efficient lan-
guage detoxification method using our attribute-
discriminative language model (ADLM), which
does not require excessive computational over-
head at inference time or memory (Figure 1).10150
•Our method largely outperforms existing meth-
ods on both generic language detoxification and
real-world dialogue detoxification tasks.
2 Related Work
Pre-trained language models (LMs) (Radford et al.,
2019; Shoeybi et al., 2019; Gao et al., 2020; Brown
et al., 2020; Du et al., 2021) mostly concentrate on
human-like text generation focusing on the struc-
tures of the generated texts, rather than on the con-
tent, are not innately controllable. To design LMs
that can generate texts with desired properties, addi-
tional modifications are necessary (Yu et al., 2017;
Hu et al., 2017; Ziegler et al., 2019). Story genera-
tion (Fan et al., 2018; Guan et al., 2020), attribute
(e.g., sentiment, topic, or emotion) controlled gen-
eration (Yang and Klein, 2021; Khalifa et al., 2021;
Chan et al., 2021; Liu et al., 2021b) and summa-
rization (Chu and Liu, 2019) are active topics of re-
search on controlled text generation. While the lit-
erature on controlled text generation is vast, in this
paper, we mainly focus on methods for language
detoxification, as it has been a critical problem in
deploying LMs to real-world applications (Gehman
et al., 2020).
The simplest methods to tackle language detox-
ification is to either pre-train LMs on the datasets
which only contain desired attributes as done by
Domain-Adaptive Pretraining (DAPT) (Gururan-
gan et al., 2020) or conditionally prepend a prefix
ahead of each text as done by Conditional Trans-
former Language (CTRL) (Keskar et al., 2019)
and Attribute conditioning (ATCON) (Gehman
et al., 2020). Since these approaches utilize a
single attribute token in front, controlling the se-
quences does not work well. When these mod-
els are exposed to toxic texts in the pre-taining
phase, it becomes more difficult to perform con-
trolled language generation. Another approachfor tackling the language detoxification problem
is to train auxiliary LMs to guide the base LM
in the decoding phase. Generative Discrimina-
tor (GeDi) (Krause et al., 2021) employs an AT-
CON model as the discriminator, and Decoding-
time Experts (DExperts) (Liu et al., 2021a) uses
two experts and anti-expert LMs, each of which is
a DAPT model trained only on the toxic or non-
toxic subset of the dataset. However, such auxiliary
LM approaches are highly memory-inefficient. On
the other hand, Plug-and-Play Language Model
(PPLM) (Dathathri et al., 2020) employs a single
LM and utilizes an attribute discriminator to gener-
ate gradient perturbations towards the specified at-
tributes. However, during inference, it takes signif-
icantly more time as it samples each word through
multiple backward passes. In contrast, our method
only requires a single LM and overcomes the mem-
ory and computational efficiency issues present in
existing methods while achieving superior perfor-
mance.
3 Method
In this section, we describe a novel language detoxi-
fication method using our Attribute- Discriminative
Language Model (ADLM ), which can efficiently
perform controlled text generation for a given at-
tribute using a projected discriminative-latent vec-
tor. In Section 3.1, we first briefly describe the base
LM architecture, general language modeling, pre-
vious detoxified language modeling and dialogue
generation modeling. Then, in Section 3.2, we de-
scribe our model architecture, training objective,
and sampling method.
3.1 Background
Language models. A Language Model (LM) pre-
dicts the next words for a given text sequence
by learning the joint probability distribution over
words in given texts (Bengio et al., 2003; Mikolov10151et al., 2010). An LM can be trained either in an au-
toregressive or autoencoder manner to learn the dis-
tributed representations of words. The autoregres-
sive approaches (Radford et al., 2019; Keskar et al.,
2019; Dai et al., 2019; Kitaev et al., 2020; Yang
et al., 2019) learn to predict the next word given the
sequence of previously generated words, whereas
autoencoder approaches (Devlin et al., 2019; Lan
et al., 2020; Liu et al., 2019; Sanh et al., 2019;
Clark et al., 2020) learn to anticipate the missing
or masked words utilizing bidirectional contexts.
In this paper, we use an autoregressive LM, GPT-
2 (Radford et al., 2019), as our base model. A GPT-
2 is composed of a Transformer and a head layer.
The Transformer (Vaswani et al., 2017) consists of
multiple blocks, each of which is composed with
a position-wise feed-forward network, multi-head
self-attention, and layer normalization. The Trans-
former encodes the contextual embedding of the
given input sequence xwhere i:jdenotes i
through jtoken in the sequence. The head layer
is a linear layer that predicts the logit ( o) of the
possible next tokens xbased on the hidden states
h= [h, h, . . . , h]∈Rwhich are
the outputs of the Transformer layers. Formally,
we can define an LM succinctly as follows:
h=Transformer (x;θ),
o=Head(h;θ),(1)
where o∈R,|V|is the vocabulary size, θand
θare Transformer’s and head layer’s parameters,
respectively.
General language model. In generic language
modeling, the initially given input sequence is
called as a prompt x= (x, . . . , x)and
the text sequence generated following it is called
acontinuation x= (x, . . . , x). The goal
of language modeling is then generating coherent
continuation xto the preceding prompt x.
P(x/vextendsingle/vextendsinglex) =/productdisplayP(x/vextendsingle/vextendsinglex),(2)
where Pis the softmax function that calculate prob-
ability of next tokens from the input x. The
model learns the distribution of the next token x
conditioned on the previously generated tokens,
using the chain rule of probability as Equation 2.
Detoxified language model. The detoxified lan-
guage modeling could be considered as a controlledattribute text generation task, but always have to
generate non-toxic attribute sequences even from
the toxic prompts. This, referred to as language
detoxification, is a challenging problem that re-
quires strong attribute control while preserving the
fluency of the LM. For language detoxification, the
objective is to learn to generate texts toward the
desired attribute a(i.e., nontoxic) as follows:
x= (x,x, . . . , x),
P(x/vextendsingle/vextendsinglex,a) =/productdisplayP(x/vextendsingle/vextendsinglex,a),(3)
where xdenotes the continuation that corre-
sponds to the desirable attribute a. The objective
is to learn the distribution of the sequence x
conditioned on ain an autoregressive manner.
Dialogue generation model. In the dialogue gen-
eration, the input sequence is referred to as the
context and the generated sequence is referred to
as the response . The dialogue generation model
learns to generate context-related human alike re-
sponses. Since the dialogue generation models
interact with users, language detoxification is an
essential task for their real-world application. Sim-
ilar to the detoxified language model, the dialogue
generation model learns the distribution of the re-
sponse sequence xconditioned on the attribute
aand the context sequence x, with an LM.
3.2 Attribute-Discriminative Language Model
Previously, the language detoxification was only
applied at decoding time using additional LMs or
by perturbing the LM, which is further trained
on each attribute dataset to guide the logits of
the pre-trained large base LM. However, they are
computation- and memory-inefficient, and thus we
propose a novel single-LM approach for language
detoxification which uses a latent space to control
the attributes of the generated texts. Specifically,
we learn a projected latent embedding space in
which the texts are well-discriminated by their at-
tributes, and use it to control the attribute of gen-
erated text sequences. We discuss the ADLM’s
architecture, objective, and the sampling method in
the following paragraphs.
Model architecture. Our model consists of a sin-
gle LM, a projection block, and an attribute discrim-
inator (Figure 3a). The projection block, ProjB , is
a single Transformer block, which learns to project10152
the original latent space onto a discriminative la-
tent space that embeds the attribute information.
The attribute is embedded onto a discriminative la-
tent space through a single token embedding layer,
AttEmb , followed by a projection block, ProjB , as
follows:
h=Transformer (x;θ),
z=AttEmb (a;θ),
h=ProjB (h, z;θ),
o=Head(h;θ),(4)
where θandθare the parameters of each compo-
nent. The projected contextual embeddings h
conditioned on attribute embeddings zaare ob-
tained by prepending ztohand pass them
intoProjB .
To learn a discriminative latent space h
where the contextualized word embeddings are
well separated by their attributes, we use an at-
tribute discriminator ( Disc ):
y=Disc(h;θ), (5)
where y∈Ris the output logit which predict-
ing the attribute a,|A|is the cardinality of the
attribute set, and θis the parameters of the dis-
criminator. The module performs summation ofhto condense the overall representation and
then pass the summed vector into a single affine
layer to determine the corresponding attribute a.
The discriminator classifies the h, which will
render the newly constructed latent space to be an
attribute-discriminative latent (See Figure 2).
Training objective. We further jointly train the
components of ADLM in an end-to-end manner.
Let us denote the dataset |D|={X, A}, where
x∈Xis a training text sequence and a∈Ais
its corresponding attribute label, and the set of the
model parameters is θ={θ, θ, θ}. Throughout
the paper, we freeze all the layers of Transformer
andHead and only train set of parameters θ, as
shown in Figure 3.
Our training objective consists of three terms.
The first objective is the autoregressive LM loss
for conditional language modeling, which learns to
reconstruct the given input text xconditioned on
the prompt xand the attribute a:
L(θ) =−/summationdisplay/summationdisplaylogP(x/vextendsingle/vextendsinglex,a),(6)
where Tis the total length of the iinput x. The
second objective directly enforces the projected
embeddings to be attribute-discriminative:
L(θ) =−/summationdisplaylogP(a/vextendsingle/vextendsingleh). (7)
Lastly, we also propose a regularizer for the pro-
jected latent space to preserve the relationship be-
tween the word embeddings in the original latent
space, to alleviate the potential negative impact of
strong detoxification on fluency. To this end, we
apply Elastic Weight Consolidation (EWC) (Kirk-
patrick et al., 2017) regularization often used for
continual learning that uses Fisher information ma-
trix to put higher regularization weights on the
update of more important parameters:
L(θ) =−/summationdisplayλ
2F(θ−θ), (8)
where jis the index referring the j-th parameter of
θuniquely identified by the number of parameters
|θ|,θis the parameters of ProjB trained without
the discriminator, Fis the Fisher information ma-
trix applying more weights on useful parameters10153learned from the θ, and λis a scale controlling
the preservation of θtoθ.
Our final combined objective aims to minimize
the sum of the two cross-entropy loss terms and an
EWC regularizer term as follows:
arg minL=L+L +L.(9)
Minimizing the total loss ( L) together allows our
ADLM to control the attributes of the generated
texts in the latent space.
Sampling. Our model constrains the logits of text
generation to use the vocabulary toward the desired
attribute. We can obtain different types of attribute
logits from the attribute-discriminative latent space
of ADLM, which uses much less memory during
the inference compared to the previous methods.
Our model computes both types of logits o,¬o
for the text generation based on the attributes such
as the desired (non-toxic; a) and undesired (toxic;
¬a) attribute as shown in Figure 3b. Each logit is
computed as follows:
o=Head(ProjB (h, z)),
¬o=Head(ProjB (h, z)).(10)
The non-toxic logits ( o) would have a high prob-
ability on non-toxic tokens, and toxic logits ( ¬o)
would have high probability on toxic tokens. From
this difference of probability, the tokens which have
greater probability in toxic logits than non-toxic
logits can be presumed as toxic tokens which could
lead to the generation of toxic texts. Therefore, ev-
ery generation of token, we compute the difference
between the logits, ∆o=o− ¬o, to suppress
the tokens that shows higher probability in toxic
logits as follows:
o=/braceleftbiggo+α∆o ∆o<0
o ∆o≥0, (11)
where ois final logits of our decoding, and αis
a constant value of suppressing scale, which is
empirically determined.
4 Experiments
To validate our ADLM, we conduct two detoxifi-
cation experiments: the language generation task
on RealToxicityPrompts (Gehman et al., 2020)
and dialogue generation task on ToxiChat (Baheti
et al., 2021) and DialogueSafe (Sun et al., 2022).
Further, we show the general applicability of ourmethod to attribute-controlled language generation
on a sentiment-controlled text generation task (Ap-
pendix D). In this section, we will discuss the ex-
perimental setup and results for two tasks. For
more detailed explanation of the experimental se-
tups, please refer to Appendix B.1. The code is
available at https://github.com/jin8/ADLM .
4.1 Detoxification for Language Generation
Baselines. We compare against the following
baselines for generic language detoxification tasks,
using GPT-2 as the base language model.All com-
pared models, including ours, are trained on Jigsaw
Unintended Bias in Toxicity Classification Kaggle
challenge datasetand evaluated on random 10K
prompts from RealToxicityPrompts (Gehman et al.,
2020). The training dataset is imbalanced between
non-toxic comments (91M tokens) and toxic com-
ments(10M tokens), as mentioned in Liu et al.
(2021a). To address this skewed distribution, we
apply class weightsto balance the update losses in
Equation 6 and 7 to our model. The details of the
hyperparameters used for each model are provided
in Appendix B.2.
•Domain-adaptive pre-training (DAPT; Guru-
rangan et al. (2020)): This baseline further
trains the LM on the dataset with desired at-
tributes (e.g., non-toxic corpus).
•Attribute conditioning (ATCON; Gehman
et al. (2020)): This baseline learns the distri-
bution of the generated texts conditioned on the
task-specific control codes (e.g., toxic or non-
toxic) prepend to the texts.
•Plug-and-play language models (PPLM;
Dathathri et al. (2020)): This baseline consists
of a classifier that backpropagates the gradients
to the LM multiple times to generate texts with
desired attributes. Due to the high computational
cost, we only sample 10 sentences per prompt as
Gehman et al. (2020) setting.
•Generative discriminators (GeDi; Krause et al.
(2021)): GeDi utilizes additional LM that is
trained with ATCON (Gehman et al., 2020) to
guide the base LM in the decoding time. GeDi
weighs the attribute probability from ATCON
using the Bayes rule on logits of the base LM.
•Decoding-time Experts (DExperts; Liu et al.
(2021a)): DExperts employs expert (non-toxic10154
DAPT (Gururangan et al., 2020)) and anti-expert
(toxic DAPT (Gururangan et al., 2020)) LMs to
guide the base LM at the decoding time. DEx-
perts add expert’s logit and subtract anti-expert’s
logit on the base LM’s logit to detoxify.
Automatic Evaluation. To validate our language
detoxification method, we evaluate the toxicity of
the generated texts using it, as well as the efficiency.
Moreover, we examine the diversity of the gener-
ated texts. To automatically measure the toxicity
of the generated texts, we utilize Perspective API
that returns the toxicity scores of given texts and
further details are provided in Appendix A. To mea-
sure diversity, we calculate the mean of distance
n-grams (Li et al., 2016) that is normalized by the
total text length.
The results in Table 2 show that ADLM largely
outperforms baselines in the language detoxifica-
tion performance. Compared to GeDi, ADLM can
lower the toxicity of the generated texts to 0.28
with a significantly smaller number of parameters
(1/7) and×2faster inference time. Moreover, our
model is able to generate more diverse texts com-
pared to those generated by baselines.
Ablation study. We examine the effect of each
component of our ADLM, i.e., architectural design,
dataset design, and training modules, in Table 3.
We observe that balancing the toxic and non-toxic
data is the most important factor to construct a
well discriminative latent space. Moreover, when
we utilize a discriminator, our model is able to
discriminate the texts more effectively along with
the attribute embedding tokens which supports our
hypothesis that obtaining a well-discriminated pro-
jected latent space is the key factor to success in
detoxification.
Analysis of toxicity types. We further examine
which types of toxic texts are highly suppressed by
our model compared to GPT-2. As shown in Fig-
ure 4, our model suppresses all types of the toxic
level of the generated texts compare to baselines.
Notably, ADLM successfully suppresses toxicity
on the threat type, which DExperts fail to detoxify.
The threat is one of the frequent types of toxic sen-
tences that GPT-2 generates with the highest prob-
ability (0.624). This explains why DExperts is vul-10155
nerable to threats , Since DExperts eventually em-
ploy the original latent space of GPT-2 and thus can-
not significantly change its language generation be-
havior. On the other hand, our ADLM modifies the
original latent space into attribute-discriminative
ones, and thus can effectively suppress them. An-
other notable point is that all models, including
ADLM, cannot handle flirtations well. However,
by checking the generated examples, we found that
the perspective API assign high flirtation scores on
sentences with words such as women, her, she, like,
etc. appear, which results in misclassifications of
sentences that do not contain any flirting contexts
since they are commonly used words.
4.2 Detoxification for Dialogue Generation
Baselines. For detoxified dialogue generation
task, we use DialoGPT (Zhang et al., 2020) as
a baseline language model. We compare against
the DialoGPT, DAPT, and ATCON which is the
baseline introduced in Baheti et al. (2021) for dia-
logue generation on ToxiChat (Baheti et al., 2021)
and DiaSafety (Sun et al., 2022). The details of the
hyperparameters used for each model are provided
in Appendix B.2.
Automatic Evaluation. To validate dialogue
detoxification performance, we evaluate responses
by the percentages of bad words and offensiveness
using classifiers which predict the degree of toxic-
ity and types of toxic sentences (Baheti et al., 2021;
Sun et al., 2022). Further, we also test the stance of
the responses, which tells whether they agree with
the context or not. Table 4 shows that our model
better suppresses the toxic responses compared to
the baselines. We further examine our methods
on another dialogue toxic dataset: DiaSafety. As
shown in Figure 5, our method generates more safe
responses for different categories of toxic dialogues.
The results on both datasets show that our method
achieves consistent language detoxification perfor-
mance on dialogue generation tasks for diverse
categories of toxic languages, effectively suppress-
ing the toxicity of the generated responses even
when the model is exposed to toxic data, which is
essential to real-world dialogue application.
4.3 Perplexity of Detoxified Texts
To examine the quality of the generated texts, per-
plexity (PPL) is frequently used as an automatic
evaluation measure of fluency (refer Appendix A
for more details). However, since strong detoxifi-
cation methods may generate texts that largely dis-
agree with ones in the test dataset (i.e. generating
non-toxic continuation for toxic prompts), higher
PPL is somewhat inevitable. As shown in Table 5,
our model generates around twice more non-toxic
continuations from toxic prompts with as much
as 46.75% reduced toxicity compared to baselines,
but yields 109.05% higher PPL compared to that of
DExperts. However, the increased PPL mostly re-
sults from generating incoherent text sequences to
avoid toxic language generation for toxic prompts,
and the increased PPL does not necessarily imply10156
that the quality of the generated texts is degraded.
This is clearly shown by the results of the human
study (Figure 6), where the participants ranked the
fluency of the language generated by our method
higher, while its toxicity lower.
4.4 Human Evaluation of Generated Texts
Although we demonstrate the effectiveness of our
method with automatic evaluation, in language gen-
eration, human judgment is the the most important
measurement. Thus, we performed a human evalu-
ation of generated texts using our method, by com-
paring it to ones generated by the best-performing
baselines, DExperts and GeDi (Figure 6). We evalu-
ate the toxicity of generated texts and the quality of
the generated texts, e.g. grammatical correctness,
coherent topic, and overall fluency, by recruiting
45 participants on Mechanical Turk. The details
are provided in Appendix B.3.
The results show that our model is considered to
have the best detoxification performance even by
human judgments (lower the better) with p <0.05
in paired t-test. Notably, our model is evaluated
to have better fluency over the baselines (higher
the better). The texts generated by our model are
evaluated to be grammatically correct and fluent
compared to those generated by GeDi and DEx-
perts with p-value of less than 0.05 in paired t-test.
As for coherency, there was no difference among
the compared models, with p >0.05. These re-
sults reconfirm that our model generates fluent and
detoxified texts.
5 Conclusion
In this paper, we proposed a novel and an effec-
tive attribute-controllable language model, ADLM,
for efficient language detoxification. Our ADLM
learns an attribute-discriminative latent space with
a projection Transformer layer on top of the origi-
nal pretrained LM and attribute discriminator thatdifferentiate texts by their attributes. Ours is shown
to be effective for detoxifying texts for both lan-
guage and dialogue generation tasks, outperform-
ing all baselines in automatic and human evalu-
ation, without requiring large computational and
memory overhead unlike existing methods that use
multiple LMs or additional computations.
Broader Impact and Ethical Impact
Recent Transformer-based LMs are prone to gen-
erating toxic texts such as insults, threats, and pro-
fanities. Therefore, ensuring safety in language
generation is a crucial task that is necessary for
their deployments to real-world applications. We
achieve this goal with an efficient solution that does
not require multiple LMs or further pretraining on
a large refined corpus, which is computationally
expensive. However, even with our techniques, the
language model is not guaranteed to be completely
safe and may generate toxic language, albeit at a
significantly lower rate. Furthermore, when the
toxic prompts are provided, the model may gener-
ate incoherent sequences to avoid toxic generation,
which leads to reduced fluency compared to that of
the original language model. Yet, this is a general
limitation of detoxified language modeling, which
cannot be avoided unless the provided prompts are
rephrased into non-toxic prompts while maintain-
ing their semantic meaning. In addition to develop-
ing a safe LMs, it is essential to address the issue of
LM hallucination, which refers to the generation of
factually incorrect texts. While our paper does not
focus on this aspect, ensuring both safety and fac-
tual valid generation of texts is vital for real-world
applications of LMs.
Acknowledgement
This work was supported by the Institute of Infor-
mation & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea
government (MSIT) (No.2020-0-00153) and Insti-
tute of Information & communications Technol-
ogy Planning & Evaluation (IITP) grant funded by
the Korea government (MSIT) (No.2019-0-00075,
Artificial Intelligence Graduate School Program
(KAIST)). We thank Jihoon Tack, Hayeon Lee
and Seul Lee for providing helpful feedbacks and
suggestions in preparing an earlier version of the
manuscript. We also thank all participants of our
human evaluation for their effort and time.10157References101581015910160Appendix
Language Detoxification with
Attribute-Discriminative Latent Space
In this supplementary material, we provide the de-
tails of our approach and results that were not cov-
ered in the main paper due to limited space. The
appendix is organized as follows:
Appendix A. We organize the terminologies that
are used in the paper.
Appendix B. We elaborate the experiment setup
in more details on the datasets and the baseline
models.
Appendix C. We elaborate the training and infer-
ence details when we train our ADLM.
Appendix D. We demonstrate the results of sen-
timent control tasks, ablation experiments, and ex-
amples of generating samples.
A Terminology
Here, we will describe a more detailed description
of the terminology we used in the manuscript.
Attribute. The characteristic of the sentence in
terms of toxicity. Toxic and non-toxic are types of
attributes in the toxicity task.
Latent space. We denote the hidden space be-
tween the head layer of language model and Trans-
former as a latent space.
Toxicity. The score of being harmful or unpleas-
ant in the provided texts. Toxicity is scored from 0
to 1.0. A sentence with a score of larger than 0.5
is considered as toxic. The sentence with a score
smaller than 0.5 is considered as non-toxic.
Type of toxic. The Perspective APIdetects the
toxic sentence with 8 different types, e.g., profanity,
sexually explicit, identity attack, flirtation, threat,
insult, severe toxicity, toxicity . The results that are
calculated in the main manuscript are based on the
score of the toxicity .
Toxicity probability. Toxicity probability is the
probability of generating toxic sentences from 25
generations. The probability to generate toxic
sentences ( ≥0.5) in 25 generations from single
prompts. If there are five sentences that have a
score larger than 0.5 in the results of 25 genera-
tions, toxicity probability is 1/5 = 0 .2.Expectation of max toxicity. Expectation Max
Toxicity (Exp. Max Toxicity) is calculated by the
mean of max toxicity from 25 generations. The
average value of toxicity of the largest score in 25
generations in the evaluation set.
Fluency Fluency is the measurement of how flu-
ent the continuation is. Automatic evaluation of
fluency is calculated based on GPT-2 xl. Fluency
is measured as the perplexity of generated output
to GPT-2 xl and the targeted models.
Diversity Diversity is the measurement of how
diverse words are generated from the models. Auto-
matic evaluation of diversity is computed by count-
ing the unique n-grams normalized by the total
length of text. Dist-1, Dist-2, Dist-3 stand for val-
ues of 1-gram, 2-grams, 3-grams, respectively.
B Experimental Setup
B.1 Dataset
Toxicity dataset. For the train set, we use a
dataset from Jigsaw Unintended Bias in Toxicity
Classification Kaggle challenge. The dataset
is annotated by humans. We denote toxic class
datasets that are greater than 50% annotator choose
the comments as toxic examples. For the non-toxic
class dataset, we use comments that none of the
annotators choose as toxic. The toxic and non-
toxic classes consist of 160K comments and 1.4M
comments, respectively. Since we need to control
our hidden states, we duplicate toxic comments as
large as the size of non-toxic comments to balance
between the non-toxic comments to format a stable
representation.
For the evaluation set, we use several subset from
the RealToxicityPrompts dataset(Gehman et al.,
2020). 100K dataset is total evaluation prompts
from RealToxicityPrompts. Random 10K prompts
are random samples of 5K toxic prompts and
5K non-toxic prompts from RealToxicityPrompts
dataset (Liu et al., 2021a). We sample 25 continua-
tions from the single prompt with 0.9 probability in
sampling. Temperature is set as 1 and max length
of continuation is set as 20.
Toxicity dataset for dialogue generation. We
train our model on the Reddit conversation dataset
from Baheti et al. (2021). Each conversation con-
sists of a title, post, and response with offensive10161and stance labels indicating whether it is a toxic or
conforming comment. The toxichat dataset is split
into train, dev, and test splits with 1400, 300 and
300 threads.
We evaluate our models on the DiaSafety
dataset(Sun et al., 2022) that to protect human
users and promote fairness and social justice. The
DiaSafety dataset is collected from social media
platforms and generated texts from language mod-
els. It consists of five categories: offending user,
risk ignorance, unauthorized expertise, toxicity
agreement, and bias opinion. The DiaSafety dataset
is split into train, dev, and test with 8.8K, 1.1K and
1.1K context-response pairs.
B.2 Baseline
DAPT. For the language detoxification task,
DAPT is further trained on the non-toxic corpus,
OpenWebText (Gokaslan and Cohen, 2019). The
results of DAPT (small) are from Gehman et al.
(2020) which is evaluated on 10K RealToxici-
tyPrompts.
ATCON. ATCON is a model that learn the dis-
tribution of the generated text by conditioning
on the given control codes that are specific for
each task. For language detoxification task, the
text is prepended with control codes:/angbracketleftbig
toxic/angbracketrightbig
and/angbracketleftbig
nontoxic/angbracketrightbig
. The results of ATCON is evalu-
ated on 10K RealToxicityPrompts (Gehman et al.,
2020).
PPLM. PPLM consists of a classifier that back-
propagates the gradients to the LM to generate texts
with desired attributes multiple times. Because of
the high computational cost of this model, 10 sen-
tences are sampled from single prompts. For the
language detoxification task, the results of PPLM
are reported results from Gehman et al. (2020) on
random 10K prompts RealToxicityPrompts. The
model is GPT-2 medium-based.
GeDi. GeDi is a model that guides the gener-
ation of each token by determining the attribute
probability of given text which can be obtained
by the Bayes rule normalizing over two attribute-
conditional distribution of next tokens. To this end,
they use two LM: base and discriminator. The dis-
criminator LM is trained as ATCON which learns
the attribute conditional-distributions and the baseLM focuses on generation with the guidance of the
discriminator LM. For the language detoxification
task, the results of GeDi are evaluated on random
10K prompts from RealToxicityPrompts. We uti-
lized the provided model from Krause et al. (2021)
which is GPT-2 medium-based.
DExperts. Under the concept of expert and anti-
expert, DExperts use three LMs: base expert, and
anti-expert. The expert and anti-expert are respec-
tively, trained on a specific subset in the dataset:
toxic and non-toxic texts in the language detox-
ification task and positive and negative texts in
the sentiment-controlled task. DExperts use both
logits from experts which support the base LM to
suppress and to amplify logit values so that the
base LM samples desired vocabularies. For the lan-
guage detoxification task, the results of DExperts
are evaluated on random 10K prompts from Re-
alToxicityPrompts. We reproduced the DExperts
with small experts which is GPT-2 small based
where the toxic performance was the best among
the other sizes of GPT-2.
B.3 Human evaluation
We clearly informed the participants regarding hu-
man evaluation and conducted the survey as shown
in Figure 7. We recruited a total of 45 participants
from both Amazon Mechanical Turk and school,
and we compensated them with $ 10 per survey.
We compare against DExperts, and GeDi for this
experiment, which is the best two performing base-
line by the automatic evaluation. We first randomly
choose 20 prompts each from the random-10K sub-
set. Then, we also randomly select one of the gener-
ated continuations among 25 generations for each
prompt and show the generated texts by our model,
DExperts, and GeDi in random order.
Therefore, for language detoxification, 45 partic-
ipants evaluated 60 continuations with i) toxicity,
ii) grammatical fluency, iii) topic coherency, and iv)
overall fluency. For each question, the participants
scored from 1 to 5 on whether provided continua-
tion is toxic or fluent. For the results, we average
the score of all 20 sequences for each question.
We provided the standard of the score for each
question. For toxicity, scores 1, 3, and 5 mean not
toxic at all, feel toxic, and very toxic (contains toxic
words), respectively. For grammatical correctness,
score 1, 2, 3, 4, and 5 stands for grammatically
poor, weak, understandable, minor mistake, and
good. For topic coherency, scores 1, 3, and 5 are a10162totally different topic, similar topic but not fluent,
and good coherency, respectively. For fluency, the
score 1, 2, 3, 4, and 5 are does not make any sense,
weak, limited, understandable, and good.
As shown in Figure 6, our model is 2.24, 3.60,
3.00, and 3.39 for toxicity, grammatical correctness,
coherency, and fluency, respectively. In sum, our
model generates texts that are less than feel toxic,
with a few minor mistakes in grammar, similar
topic texts but not fluent, and weak fluency.
C ADLM Details
C.1 Modeling Details
We use GPT-2 from HuggingFace Transformers
version 4.2.0 (Wolf et al., 2020), implemented
in the PyTorch framework. For RealToxici-
tyPrompts (Gehman et al., 2020), our ADLM is
trained with 128 block sizes, 32 batch sizes per
GPU, 5elearning rate, and 3 epochs. Same set-
ting is used for sentiment-controlled text genera-
tion. Since the sizes of training datasets differ in
dialogue generation tasks, the hyperparameters are
empirically determined. For ToxiChat (Baheti et al.,
2021), our ADLM and baselines are trained with
32 batch sizes per GPU, 2elearning rate and
three epochs. For DiaSafety (Sun et al., 2022), our
ADLM and baselines are trained with eight batch
sizes per GPU, 2elearning rate and five epochs.
The block sizes of both dialogue datasets are not
truncated unless they exceed 512. For all datasets,
we set λas 0.1 for EWC loss and use AdamW op-
timizer with 1eepsilon and a linear scheduler.
Trainings are performed on a single NVIDIA RTX
2080 Ti or Quradro RTX 8000.
C.2 Generation
For RealToxicityPrompts (Gehman et al., 2020)
and sentiment-controlled text generation, we set
the same setting in generation for all baselines and
our models, except for PPLM (Dathathri et al.,
2020). We perform a total of 25 generations on
each prompt. The max length of generated sen-
tences is 20. For PPLM (Dathathri et al., 2020),
we generate 10 generations on each prompt due to
computational costs. For our generation, we set
αto 4.0 for the language detoxification task. For
dialogue generations, the generation setup is differ-
ent. For ToxiChat (Baheti et al., 2021), the models
generate until the end-of-token appears or until the
max sequence threshold is 500. The αis set to 1.5.
Lastly, for DiaSafety (Sun et al., 2022), the max10163length of a generation is set to 128 and the αis set
to1.5. All the generations use nucleus sampling
with0.9top-p probability and 1.0temperature scal-
ing for the softmax.
C.3 Suppress visualization
The ADLM model is able to control the genera-
tion of toxic and non-toxic language through the
use of a discriminative latent space. Detoxification
is achieved by suppressing toxic words through
the utilization of both toxic and non-toxic logits.
The effectiveness of our proposed method was vali-
dated through experimental results, and the result-
ing word distribution was qualitatively analyzed.
As an example, when the prompt "Aloy is your
robotic-wilderness prodigy, who sets off to find out
why machines are that" was given, the word distri-
bution of the toxic, non-toxic, and suppressed logits
is obtained as following Figure 8. The following
figure illustrates that toxic words are suppressed
and positive words are emphasized. By applying
our suppression method, we can observe that nega-
tive words such as "bad," "stupid," and "evil" are
suppressed, while positive words like "good" and
"powerful," which were not present in the non-toxic
logits, appear at the top.
D Experiments
D.1 Sentiment-Controlled Text Generation
Sentiment dataset. For sentiment-controlled text
generation task, we train our model on sentiment
movie review dataset from Stanford Sentiment Tree-
bank (SST-5) (Socher et al., 2013). Each review
in the dataset is rated on a scale from 1 to 5 (very
negative to very positive). The reviews with rat-
ings 4 to 5 are assigned as positive reviews and
ratings 1 to 2 are assigned as negative reviews. For
the evaluation set, there are 2.5K prompts for each
sentiment that is provided from Liu et al. (2021a)
which is obtained from OWTC (Gokaslan and Co-
hen, 2019).
Baselines. For sentiment-controlled text gener-
ation, the positive and negative DAPT (Gururan-
gan et al., 2020) models have been independently
trained on each subset of SST-5 dataset. Similar to
ATCON, CTRL (Keskar et al., 2019) which uses
"Reviews Rating: 5.0" and"Reviews Rating:
1.0" as control code are used. The results of DAPT,
CTRL, GeDi, PPLM and DExperts on sentiment-
controlled text generation task are reported values
from Liu et al. (2021a).
Automatic Evaluation. To guarantee that our
method is generally applicable to any controllable
text generation tasks, we further validate our model
on sentiment-controlled text generation problem.
To this end, we consider the problem of generating
continuations which has opposite semantics from
the given prompts (e.g., positive continuation for
negative prompts). For automatic evaluation, to
validate whether the generated text matches with
targeting sentiment, we use HuggingFace’s senti-
ment analysis classifier (Wolf et al., 2020).
The results in Table 6 show that our model
achieves impressive performance on controlled text
generation as well. This suggests that our method
is applicable to any attribute-controlled text gener-
ation tasks.
D.2 Ablation experiment
To evaluate fluency, we measure the mean perplex-
ity of the continuations according to the GPT-2
XL model. We conduct the ablation experiment α10164
in Eq. 11 and λin Eq. 8. As shown in Figure 9,
when alpha decreases and lambda increases, the
toxicity increases while the perplexity decreases.
The toxicity control performance and fluency are
in somewhat a trade-off relationship, and we can
increase and decrease them at the expense of the
others by controlling the values αandλ.
D.3 Generation examples
The Table 7 and Table 8 are the examples generated
from our model for language detoxification task.
The Table 9 and Table 10 are the examples gen-
erated from our model for dialogue detoxification
task on ToxiChat dataset.1016510166101671016810169ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.10170/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.10171