
Satwik BhattamishraArkil PatelVarun KanadePhil BlunsomUniversity of OxfordMila and McGill UniversityCohere
{satwik.bmishra, varun.kanade, phil.blunsom}@cs.ox.ac.uk arkil.patel@mila.quebec
Abstract
Despite the widespread success of Transform-
ers on NLP tasks, recent works have found
that they struggle to model several formal lan-
guages when compared to recurrent models.
This raises the question of why Transformers
perform well in practice and whether they have
any properties that enable them to generalize
better than recurrent models. In this work,
we conduct an extensive empirical study on
Boolean functions to demonstrate the follow-
ing: (i) Random Transformers are relatively
more biased towards functions of low sensitiv-
ity. (ii) When trained on Boolean functions,
both Transformers and LSTMs prioritize learn-
ing functions of low sensitivity, with Trans-
formers ultimately converging to functions of
lower sensitivity. (iii) On sparse Boolean func-
tions which have low sensitivity, we find that
Transformers generalize near perfectly even in
the presence of noisy labels whereas LSTMs
overfit and achieve poor generalization accu-
racy. Overall, our results provide strong quan-
tifiable evidence that suggests differences in the
inductive biases of Transformers and recurrent
models which may help explain Transformer’s
effective generalization performance despite
relatively limited expressiveness.
1 Introduction
Transformers (Vaswani et al., 2017) have sup-
planted recurrent models across a range of NLP
tasks (Liu et al., 2019; Brown et al., 2020). In
particular, effective large-scale pretrained models
have predominantly been Transformer-based mod-
els and have found application in other areas such
as computer vision and protein folding. Given the
irrefutable importance of understanding these ar-
chitectures, a significant effort has been devoted
to analyze the inner workings of large-scale pre-
trained Transformers. However, the cause behind
the difference in performance between Transform-
ers and recurrent models has largely been unclear.A line of work has attempted to understand neu-
ral sequence models through the lens of formal
language theory. These works have sought to un-
derstand the expressive power of these architectures
and identify differences in their ability to general-
ize across various formal languages. A notable
result by Hahn (2020) showed that Transformers
are limited in their ability to express the P
languagewhile it is well known that small-sized
RNNs can express such languages. Across empiri-
cal studies, Transformers have been found to per-
form worse or comparably to LSTMs in almost all
formal languages previously considered in the liter-
ature (Bhattamishra et al., 2020a; Delétang et al.,
2022; Chiang and Cholak, 2022). In particular,
Transformers have been shown to struggle with
theP language and certain other regular lan-
guages. This leads to a natural question: Why do
Transformers perform so well in practice if they are
arguably less expressive and perform worse than
LSTMs across certain formal languages?
Although recurrent models such as LSTMs have
been shown to perform better on formal languages
such as P , we find that they struggle to gen-
eralize well on several sparse Boolean functions
such as S P . We find a clear con-
trast between the generalization abilities of Trans-
formers and LSTMs on various k- Boolean
functions which have low sensitivity. Addition-
ally, through extensive empirical analysis, we pro-
vide strong evidence to suggest differences in the
bias towards low complexity functions between
Transformers and recurrent models. Based on our
results, we hypothesize that one of the reasons be-
hind Transformer’s practical effectiveness could be
that they are more biased towards simple functions
in comparison to recurrent models which may lead
to better generalization.
In particular, we focus on a complexity measure5767
called sensitivity (Kahn et al., 1989), which mea-
sures how likely it is that a function value changes
due to a ‘small’ change in input. Sensitivity is
related to several other complexity measures; func-
tions with low sensitivity have low Kolmogorov
complexity,simpler Fourier spectra, and can be
represented by decision trees of small depths. The
relationship between sensitivity and generalization
has also been previously studied in the literature
(Novak et al., 2018; Franco, 2006).While mea-
sures such as Kolmogorov complexity are uncom-
putable, sensitivity can be tractably estimated and
extensions of sensitivity can be used to estimate
the complexity of functions in more realistic NLP
tasks (Hahn et al., 2021).
Our Contributions. We investigate the bias in
(a)parameter space by analyzing randomly initial-
ized models, and (b) learning procedure by exam-
ining the sensitivity of models during the training
process. Motivated by our findings indicating dif-
ferences between the biases of Transformers and
LSTMs, we evaluate their performance on func-
tions of low sensitivity.
(i)We demonstrate that random Transformers
are significantly more likely to represent functions
of lower sensitivity than recurrent models when
the weights are sampled uniformly or according to
Normal distribution (see Figure 1, bottom right).
When the weights are initialized following practi-
cal strategies (such as Xavier normal), then botharchitectures are likely to have low sensitivity with
Transformers having relatively lower sensitivity.
(ii)We show that both Transformers and LSTMs
learn functions of increasing sensitivity when
trained on a set of Boolean functions as well as
practical datasets such as sentiment classification
(see Figure 1, top right). For Boolean functions,
Transformers converge to functions of lower sensi-
tivity in comparison to LSTMs when both models
achieve near-zero training error.
(iii) On various k- Boolean functions,
we find that Transformers generalize near-perfectly
even in the presence of noise in the training data
whereas LSTMs severely overfit and obtain poor
generalization performance (see Figure 1, left).
Auxiliary Results. Although not the primary
focus of the paper, we explore relations between
sensitivity and generalization in Appendix D. In
particular, we show how sensitivity can be used as
a capacity measure to derive generalization bounds.
Additionally, we explore the correlation between
sensitivity and generalization gap for LSTMs and
Transformer-based models on sentiment classifica-
tion tasks. We also conduct experiments with three
other complexity measures in Appendix B.1.
2 Related Work
Random Neural Networks. One approach to ex-
plaining deep learning’s unexpected generalization
performance has been to study the inductive biases
of random neural networks. Several prior works
have shown theoretically (Palma et al., 2019) and5768empirically (Valle-Perez et al., 2019) that random
untrained feedforward networks are biased towards
‘simple’ functions. Valle-Perez et al. (2019) showed
that considering the distribution over functions gen-
erated via random neural networks as a prior leads
to better PAC-Bayesian generalization bounds than
traditional ones. Several works (Mingard et al.,
2019; Wilson and Izmailov, 2020; Lee et al., 2017)
have argued using heuristic methods that the induc-
tive biases in random neural networks can be used
to understand the properties of trained networks.
Additionally, there is empirical and theoretical evi-
dence (Oymak and Soltanolkotabi, 2019) that neu-
ral networks trained with SGD usually converge
close to the initialization point. Hence, understand-
ing the properties of random neural networks is
imperative to understand their generalization abil-
ities.In Section 4.1, we study the complexities
of random Transformers and recurrent models and
investigate the differences between them.
Formal Languages and Sequence Models. In the
past few years, a strand of workprimarily in the
NLP community has attempted to understand neu-
ral sequence models’ capabilities and inner work-
ings by analyzing them on formal languages, e.g.
(Suzgun et al., 2019b; Sennhauser and Berwick,
2018). Given the recent success of Transformers,
several works have sought to investigate them via
the lens of formal languages. Hahn (2020) theo-
retically showed the limitations of Transformers
in recognizing languages like Parity and Dyck-2.
While Transformers are expressive enough to rep-
resent the P language for bounded lengths
(Chiang and Cholak, 2022), multiple works have
observed that they struggle to generalize well on
Parity and other regular languages when tested em-
pirically (Bhattamishra et al., 2020a; Chiang and
Cholak, 2022; Delétang et al., 2022). In contrast to
this, we show that when evaluated on some simpler
variants of these formal languages, Transformers
generalize near perfectly whereas LSTMs achieve
poor generalization performance.
3 Background and Preliminaries
3.1 Sensitivity of Boolean Functions
We will work with a complexity measure called
Boolean Sensitivity which has been widely studied
in computational complexity (Kahn et al., 1989;Ambainis et al., 2014). Sensitivity can be seen
as a discrete analog (Gopalan et al., 2016) of the
‘smoothness’ of a continuous function which mea-
sures how gradually a function changes locally. For
Boolean functions defined over the Hamming cube,
sensitivity captures how many neighbours of a par-
ticular input have different outputs. Formally, the
sensitivity of a Boolean function f:{0,1}→
{±1}at input x∈ {0,1}is defined as
s(f, x) =/summationdisplayI[f(x)̸=f(x)], (1)
where Idenotes the indicator function and x=
(x, . . . , x,1−x, x, . . . , x)is the same as
xat every coordinate or bit except the i-th one.
The maximum sensitivity of a function fis defined
asms(f) = maxs(f, x). The average
sensitivity (also referred to as total influence) of
a Boolean function measures the average of the
sensitivity of the function across all inputs x∈
{0,1}and is defined as
s(f) =E[s(f, x)] =1
2/summationdisplays(f, x).(2)
See that 0≤s(f)≤ms(f)≤n. To compare
across inputs of different lengths, in our experi-
ments we will normalize the average sensitivity
across length S(f) =s(f)which can also be
interpreted as,
S(f) = Pr[f(x)̸=f(x)] (3)
where [n] ={1, . . . , n }and the sampling is over
uniform distribution over the domains.
Parity. The Parity function over {0,1}is de-
fined as P (x):= (−1). For any input
x∈ {0,1}, the function P has value +1if
the number of ones in the input is even and has
value−1otherwise. The sensitivity of the Par-
ity function is the maximum among all functions
since changing any bit of any input changes the
function value. Hence, for P over{0,1},
s(P ) =nandS(P ) = 1 .
Sparse Boolean functions. Another class of
functions are the k- functions (also referred
to ask-juntas) where the function value depends on
at most kcoordinates of the input. More formally,
a function f:{0,1}→ {± 1}isk- if5769there exist indices 1≤i< i< . . . < i≤
nand a function g:{0,1}→ {± 1}, such
that for every x∈ {0,1},f(x, x, . . . , x) =
g(x, x, . . . , x). Let SPARSE- (k, n)be the
class of k- functions on inputs of length n
that depend on at most kbits. It is easy to see that,
for any f∈SPARSE- (k, n), the average sensitiv-
itys(f)≤k(and hence S(f)≤).
When k≪n,SPARSE- (k, n)can be seen as
a subclass of all Boolean functions with low av-
erage sensitivity. Other functions with low av-
erage sensitivity can also be approximated with
k- functions using Friedgut’s Junta The-
orem (O’Donnell (2021), Page 269). The max-
imum average sensitivity s(f) = kis attained
byS P denoted fwhich is the
Parity over a subset of kcoordinates. A sparse
parity function foverS⊆[n], s.t.|S|=k
is+1if the number of ones in the coordinates S
is odd and −1otherwise. Other Boolean functions
such as sparse majority can be defined similarly.
The majority function fover{0,1}is+1if
the number of ones in the input is greater than the
number of zeros and is −1otherwise. Similarly,
the sparse majority function fis the majority
function over coordinates S⊆[n], s.t.|S|=k.
Parities (and Sparse Parities) are an important class
of Boolean functions since any Boolean function
can be represented as a linear combination of a set
of Parity functions.
4 Sensitivity Experiments
In this section, we conduct various experiments to
investigate the differences in the bias of Transform-
ers and RNNs towards functions of low sensitiv-
ity.From here onward, whenever sensitivity is
mentioned, we will refer to the length normalized
version of average sensitivity Sdefined in Eq. (3).
The first part of this section deals with analyzing
the sensitivity of random Transformers and RNNs
while the second part investigates the sensitivity of
models trained to fit random Boolean functions.
4.1 Sensitivity of Randomly Initialized Models
We seek to understand the landscape of the com-
plexity of functions in the parameter space of Trans-
formers and RNNs. Let us assume that the parame-
ter space Θof our models is bounded, i.e. all the
parameters (weights) take some value within some
bounded range [−B, B]. A particular realization of
the parameters with values in [−B, B]leads to the
model being a function from {0,1}→ {0,1}. We
begin with a simple question: Out of all the param-
eterizations in the parameter space of Transformers
(or RNNs), if we select one uniformly at random,
then how likely is it to have low sensitivity?
Setup. In all our experiments, we consider bi-
nary classifiers with Transformers and RNN-based
architectures. By Transformer, we refer to the
encoder-only version of the original Transformer
architecture (Vaswani et al., 2017) as used in mod-
els such as BERT (Devlin et al., 2019). The model
takes a sequence of tokens along with a [CLF] to-
ken as input. The final classification is done based
on the output vector of the [CLF] token. For re-
current models, we consider LSTMs (Hochreiter
and Schmidhuber, 1997), GRUs, and RNNs with
tanh activation. Most of the results in the main
paper pertaining to recurrent models are based on
experiments with LSTMs and we discuss when the
behaviour is different for other recurrent models.
In our experiments, we explore four strategies
to sample random networks: Uniform, Gaussian,
Xavier uniform, and Xavier normal initialization.
In uniform sampling, each parameter (weights and
biases) is assigned a value by uniformly sampling
in[−10,10]. Similarly, for Gaussian initializa-
tion, each parameter is assigned by sampling from
N(0, σ)where we set sigma as 10. Xavier normal
(Glorot and Bengio, 2010) initialization is the one5770
that is more commonly used in practice to train
these models. All the weights are initialized with
N(0, σ)where the standard deviation σ=d
where dis the number of hidden units. All the in-
put embedding vectors and positional embedding
vectors are initialized with N(0,1)which is the
default scheme in PyTorch (Paszke et al., 2019).
For input lengths greater than 10, we estimate the
sensitivity of each model by computing the aver-
age over a sampled set of bit strings. We sample
10k bit strings and compute the average sensitivity
across the samples. For each hyperparameter con-
figuration, we sample 75-1000 different models to
estimate their sensitivity depending on the compu-
tational costs associated with it. For most of the
results reported in the main paper, we consider bit
strings of length 20. But we also experiment with
lengths ∈ {5,7,10,15,20,50,100,200}.
Results. Figure 2 (upper row) and Figure 3 (left)
shows the distribution of sensitivity for uniformly
initialized Transformers and LSTMs. The distribu-
tion for Transformers is heavily skewed towards
functions of very low sensitivity in comparison to
LSTMs. The pattern holds across Gaussian initial-
ization as well (see Figure 1, bottom right). For
initialization strategies used in practice such as
Xavier normal and Xavier uniform, we find that
both Transformers and LSTMs have low sensitivity
(see Figure 2, lower row and Figure 3, right) with
Transformers having relatively lower average sen-
sitivity. Refer to Section B.3 in the Appendix for
results with Xavier uniform initialization.
Although we primarily discuss results with sen-
sitivity in the main paper, similar experiments with
other complexity measures are presented in Ap-
pendix B.1. Further experiments exploring the
change in distribution across the number of lay-
ers, width, and lengths for both architectures are
presented in Appendix B.3.
Discussion. These results imply that lower sen-
sitivity functions are over-represented in the pa-
rameter space of Transformers. If every Boolean
function f:{0,1}→ {0,1}would have had
equal representation in the parameter space of the
model, then the distribution would have concen-
trated around 1/2. A learning algorithm based
on a random search over the parameter space is
more likely to encounter functions of low sensitiv-
ity. Note that, while recurrent models have higher
sensitivity than Transformers, they are still lower
than randomly sampling a Boolean function.
Why randomly initialized models? Each ran-
domly initialized Transformer or RNN when re-
stricted to domain {0,1}represents one of the
2Boolean functions finF. The distribution
overFinduced by randomly initialized models
can be seen as their prior P(f). Given a set of
(training) examples S={(x, y), . . . , (x, y)},
letP(f|S)denote the probability of sampling f
conditioned on the event that the sampled function
is consistent with S(matches all the input-output
mappings in S). We can apply Bayes’ rule to calcu-
late the posterior P(f|S) =P(S|f)P(f)/P(S)
using the prior P(f), the likelihood P(S|f), and
the marginal likelihood P(S). Since we condition
onfbeing consistent with S(zero training error),
the likelihood P(S|f) = 1 if∀x∈S,f(x) =y
and0otherwise. Let U(S)denote the set of all
functions f∈ F which are consistent with S. Note
that, given a fixed training set S, since P(S)is con-
stant, the probability over the choice of f∈U(S)
ultimately depends on the prior P(f). In practice,
we do not fit the training set by sampling models
to find one that is consistent with the training set.
However, recent work (Mingard et al., 2021) has
shown that P(f|S)≈P(f|S)across a range
of neural architectures and data sets. The SGD-
based posterior P(f|S)denotes the probabil-5771ity that a neural network converges on function
fwhen trained to fit S. Hence, our results sug-
gest that for Transformers, P(f|S)would be con-
centrated on low-sensitivity functions and conse-
quently, P(f|S)could be biased towards low-
sensitivity functions as well.
4.2 Models learn functions of increasing
sensitivity
In this section, we investigate the sensitivity of
functions learned during the training process when
Transformers and LSTMs are trained to fit datasets
of Boolean strings with random labels.
Setup. We create datasets of size 1k each by
uniformly sampling bit strings of length 40. The
label for each input string is assigned randomly
(+1or−1with probability 1/2). All the weights
of the models are initialized with Xavier normal ini-
tialization and the biases are initialized with zero
vectors. We consider Transformers and LSTMs
across various hyperparameter configurations with
a similar number of parameters. We train the mod-
els until they reach zero training error and estimate
the sensitivity of the models at every epoch. We
conduct the experiments over 20different datasets
with100runs for Transformers and LSTMs each.
Sensitivity during training. We find that both
Transformers and LSTMs gradually learn functions
of increasing sensitivity with Transformers con-
verging to functions of much lower sensitivity than
LSTMs (refer to Figure 1, top right). We observe
similar behavior when the models are trained on
various sparse Boolean functions including sparse
parities. Even though sensitivity is defined over
Boolean functions, we explore a few natural exten-
sions to estimate the sensitivity of models trained
on real datasets such as sentiment classification. On
two sentiment classification datasets, namely SST
and IMDB, we found similar observations where
both Transformers and LSTMs seem to incremen-
tally learn functions of increasing sensitivity. See
Appendix C for more details.
Discussion. Even if sequence models such
as Transformers or LSTMs are capable of rep-
resenting arbitrary functions, our results suggest
that they prioritize learning simpler patterns first.
These results echo prior observations that indicate
feedforward-like neural networks trained with SGD
learn functions of increasing complexity (Nakkiran
et al., 2019; Arpit et al., 2017). Rahaman et al.
(2019) find that ReLU neural networks learn func-
tions of lower frequency modes first. Functions
with lower average sensitivity also have a lower
frequency and hence these observations are closely
connected. More importantly, average sensitivity
can be naturally extended to real data which allows
us to empirically explore this for text data.
Sensitivity upon convergence. For Transform-
ers and LSTMs trained until 0%training error, we
estimate the sensitivity of functions learned by the
models. We create 15 datasets and for each dataset,
we compute the sensitivity of 100trained models.
The combined distribution of the sensitivity of the
models across all datasets is shown in Figure 4. We
observe that Transformers consistently learn func-
tions of lower sensitivity in comparison to LSTMs.
This supports our hypothesis that for Transformers
the parameter search via algorithms such as Adam
is more likely to find functions of lower sensitivity
that fit the training set as opposed to LSTMs.
5 Experiments on Sparse Boolean
Functions
Our results in the previous section indicate that rel-
ative to LSTMs, random Transformers are biased
towards low-sensitivity functions and Transform-
ers are biased towards learning Boolean functions
of low sensitivity. Motivated by this difference
in bias, we conduct experiments geared towards
answering the following question: Is there any dif-
ference between the ability of Transformers and
LSTMs to learn sparse Boolean functions which
have low sensitivity?5772
5.1 Setup.
Boolean Functions. We focus on k-
Boolean functions which have low sensitivity when
k≪n(refer to Section 3 for definition). We
first consider certain Boolean functions which are
widely studied in the analysis of Boolean functions.
The first one is S P which can be in-
terpreted as the k- variation of standard par-
ity. We denote an instance of S P
asP -(n, k)where ndenotes the length of
the input string and kdenotes the number of rel-
evant bits. We denote an instance of standard
P asP -nwhere ndenotes the length
of the input string and the output is computed based
on the number of ones in all indices. Learning
P -(n, k)with gradient-based methods has
well-known hardness results −requiring at least
ncomputational steps to find the correct target
function (Kearns, 1998). The other two Boolean
functions we consider are sparse majorities (de-
noted by M-(n, k)) and the dictator function (de-
noted by D-n). The output of the dictator func-
tion depends only on a single input bit, making
it arguably one of the simplest Boolean functions
with very low sensitivity. In M-(n, k), the output
for a string of length nis determined by whether
the number of ones is greater than the number of
zeros in the krelevant indices.
The second set of Boolean functions we
consider is random k- functions (de-
noted by J -(n, k)). For each instance of
J -(n, k), the function is determined by ran-
domly choosing kindices and assigning labels to
each of the 2distinct inputs randomly.
Noisy Labels. We also conduct experiments to
examine the ability of the models to learn in the
presence of noise. In these experiments, labels oftraining data are flipped with a certain probability
η. Thus, about 1−ηfraction of the training data is
clean and ηfraction of the training data has incor-
rect labels. The validation set is clean without any
modifications. The goal is to investigate whether a
model is robust to noise during the training process.
Training Details. The training and validation
sets are created by uniformly sampling bit strings
over{0,1}. In our experiments, we consider
Transformers with 1-6 layers, 4-8 heads and width
(usually referred to as d_model ) within 8-128. We
consider Transformers with both learnable and ab-
solute positional encodings. For LSTMs, we con-
sider up to 6 layers and widths (also referred to as
hidden_size ) within 8-256. The size of the token
embeddings is kept the same as the width. We also
consider the presence of learnable positional em-
beddings as a hyperparameter. We use batch sizes
of 100 and 500 in all our experiments and tune
across learning rates ∈ {1e-1, 5e-2, . . ., 1e-6}. For
each dataset, we extensively tune the models across
various hyperparameters, details of which are pro-
vided in Appendix G.
5.2 Experiments
Parities. ForP -(40,4)andP -40, we
create 5different datasets and report the results
based on the maximum accuracy achieved on un-
seen test data. The train set consists of 30k samples
and the validation sets contain 10k samples.
We observe a stark contrast between the perfor-
mance of Transformers and LSTMs on different
forms of parity tasks. We find that Transform-
ers struggle to fit and generalize on P -40
while LSTMs easily (across a range of hyperparam-
eters) generalize well on them. On the other hand,
perhaps surprisingly, on P -(40,4), we find
that while Transformers generalize well, LSTMs
severely overfit and achieve poor validation accu-
racy. Although LSTMs achieve 100% training ac-
curacy over the training data, their validation ac-
curacy does not move far beyond the chance level
(50%). Figure 5 depicts the training and validation
accuracy curves for Transformers and LSTMs on
P -(40,4)task. We find similar behaviour for
LSTMs even with learnable positional embeddings.
Robustness to noise. OnS P
datasets, we find that Transformers are surprisingly
robust to noise. When the training data contains
5%-20% noise ( η), Transformers achieve perfect
generalization accuracy with training accuracy con-5773
verging at 1−η. In some cases, after training for a
large number of iterations post-convergence, Trans-
formers begin to overfit on the noise. This observa-
tion echoes a similar finding in Tänzer et al. (2022)
where they observed such behaviour while finetun-
ing large pretrained models for sequence tagging
tasks in the presence of noise. The training and
validation accuracy curves are provided in Figure
6. The behaviour of recurrent models is the same
as in the previous scenario with clean data: they
overfit on the training data while achieving chance
level validation accuracy. Additional results on
P -(n, k)across different dataset sizes, task
variations, as well as exploring phenomena such
as phase transitions and grokking are provided in
Appendix E.
We observe this pattern across other sparse
Boolean functions such as sparse majority and
dictator functions as well. For sparse major-
ity datasets M-(n,5), we consider lengths
n∈ {50,75,100,200}and for dictator func-
tions D-n, we consider lengths n∈
{100,200,300,500,700}. We experiment with
various rates of noise (10 - 30%). While LSTMs do
generalize well up to certain lengths, they achieve
poor validation accuracy (<75%) as the lengths go
higher. At the same time, they obtain 100% train-
ing accuracy on all the datasets. The validation
accuracies of LSTMs are reported in Figure 7. In
contrast, Transformers achieve near-perfect gener-
alization even in the presence of significant noise.
Random k-sparse functions. For
J -(n, k), we experiment with var-
ious datasets for J -(n,5) with
n∈ {30,50,80,150,200}. For lengths n <150,
we find that LSTMs generalize well on some
of the J -(n,5)functions. However, in
the presence of 10% noise (i.e., η= 0.1), theirperformance degrades sharply. We create 10
datasets for J -(50,5)with η= 0.1, and
similar to previous scenarios, LSTMs struggle to
generalize well (>75%) whereas Transformers are
able to generalize perfectly on all the datasets (see
Figure 1, top middle). However, even when the
validation accuracies of LSTMs were below 75%,
their training accuracy reached 100% indicating
that they overfit on the training data. Figure 1
(bottom left) shows the training and validation
curves of LSTMs on the 10datasets.
Sensitivity During Training. We observe that
onk- functions, both Transformers and
LSTMs learn functions of increasing sensitivity.
However, when LSTMs overfit and reach zero train-
ing error, they converge to functions of much higher
sensitivity than that of the target function (see Fig-
ure 16. Since Transformers generalize perfectly,
their sensitivity matches that of the target function.
6 Clarifications
(1)Do our results imply that Transformers can
learn any k- functions with small (practi-
cal) number of examples? No. For small lengths
(n= 50 ) and k= 3, we could enumerate and
verify that they are able to learn all functions
in the presence of 10% noise. However, as the
length nand the number of relevant bits kgrow,
Transformers struggle to perform well. Given the
computational hardness associated with learning
S P , the task becomes much more
difficult with the increase in nandk. Forn= 100
andk= 5, we were not able to obtain good gener-
alization performance with Transformers.
(2)Do Transformers never overfit on k-
functions? They do overfit when the size of the
training data is very small. For S P
withn= 40 andk= 4, it is perhaps surprising
that Transformers learn the correct function even
with as little as 2500 training examples in less than
10000 computational steps. However, for training
sets of size 1000 , Transformers overfit across all
runs. Additionally, with training sets of size 5000 -
10000 , Transformers with higher depths overfit in
some cases. See Appendix E for more details.
(3)Does the low sensitivity bias of Transformer
(Section 4) explain their good generalization
performance on k- functions such as
S P ?No. Our findings in Section 4
motivated us to compare the performance of Trans-
formers and LSTMs on functions of low sensitiv-5774ity such as k- functions. While the bias
towards low sensitivity functions and strong per-
formance on various k- functions could be
related, it is not a direct explanation for their per-
formance on k- . For S P , it
is natural to expect Transformers to follow some
mechanism along the lines presented in Barak et al.
(2022) for FFNs trained with SGD. However, the
exact details are unclear, and more importantly,
why and how LSTMs overfit is unclear as well.
(4)Are Transformers performing better than
LSTMs because of learnable positional embed-
dings? This seems unlikely since we found that
Transformers with absolute positional encoding
also generalize well on sparse parities (see Figure
18). Moreover, we found that LSTMs with learn-
able positional embeddings also fail to generalize
on sparse parities and behave similarly to Figure 5.
(5) Do LSTMs never succeed in learning
S P from data? They do succeed for
smaller lengths. For lengths up to 20, we find that
both Transformers and LSTMs are able to learn
P and S P . However, for
higher lengths, Transformers struggle to fit P
and LSTMs begin to overfit on S P .
For length n= 20 andk= 4, we could robustly
find that even LSTMs without positional embed-
dings succeeded in learning sparse parities. On the
other hand, for n= 40 andk= 4, we robustly
found that LSTMs with learnable positional em-
beddings overfit and achieve poor generalization
performance. Transformers were able to generalize
well in the presence of noise across various hyper-
parameters for S P withn= 40 and
k= 4. Our goal is not to identify the exact class of
functions that Transformers can learn in practice.
The key result is the juxtaposition of the perfor-
mance between Transformer and LSTMs across
various k- functions.
(6)Do Transformers work effectively in practice
primarily due to their simplicity bias? It is hard to
answer this question. In our work, we try to high-
light concrete differences between Transformers
and LSTMs with respect to certain properties which
have close connections to generalization. While
these properties could partially be the reason be-
hind their good generalization performance, it is
also possible that they are ubiquitous in practice
because they effectively model long-distance de-
pendencies and can be trained efficiently.7 Discussion and Final Remarks
A natural question that arises from our results is
whether Transformers are performing better be-
cause the tasks are more suited to their architecture.
Perhaps yes. One could argue that a number of reg-
ular languages that Transformers struggle to learn
(Bhattamishra et al., 2020a; Delétang et al., 2022)
are more suited to recurrent architecture. Trans-
formers have been shown to perform poorly on
languages that require modular counting. DFAs,
which are often considered to be formal abstrac-
tions of recurrent models, can represent these more
efficiently. For instance, languages like standard
parity can be represented with a two-state DFA
while representing sparse parities would require a
larger number of states. In contrast, for circuits
that have recently been related to Transformers
(Hao et al., 2022; Merrill et al., 2022), representing
sparse parities would be easier than representing
standard parity. Our results indicate that previous
works might have overestimated the performance
of LSTMs by considering regular languages which
are more suited for autoregressive architectures.
The question of which formal languages are
more closely associated with practical tasks is not
entirely clear. Prior works on analysis with for-
mal languages have primarily followed Chomsky
hierarchy owing to the conjecture that natural lan-
guages are mildly context-sensitive. While regular
languages such as P have high sensitivity
(S= 1), practical tasks are often structured and
have typically much lower sensitivity (Hahn et al.,
2021). In tasks such as sentiment analysis, the label
often depends on a sparse subset of input tokens.
When practical text datasets such as SST are la-
belled with random noise, then it can be shown that
their sensitivity would be concentrated around 1/2.
As shown in Fig. 23, models take much longer to fit
such datasets whereas, in the case of the true labels,
they only need a few epochs to fit the dataset.
Our results indicate that while Transformers per-
form poorly on certain regular languages, they gen-
eralize more effectively than recurrent models on
various sparse Boolean functions. Moreover, we
showed that random Transformers as well as those
trained with gradient-based algorithms are biased
towards functions of low sensitivity. Our results
add to the body of work that suggests that there is
a form of implicit regularization in the procedure
used to train neural models which prevent them
from overfitting despite their incredible capacity.5775Acknowledgments
We would like to thank Michael Hahn, Ard Louis,
Kabir Ahuja, anonymous reviewers, and our col-
leagues at the University of Oxford for helpful dis-
cussions and for providing valuable feedback.
Limitations
A general limitation of this line of work is that most
of the results are primarily confined to artificial
datasets. Although such formal languages provide
us with a controlled setting and clarity regarding
the precise nature of the problem, the relation to
practical tasks remains unclear. Hence, while our
results highlight the contrast in the performance
between the two types of architectures, its precise
implications on real-world tasks remain unclear.
There are two negative results that do not support
our hypothesis. (a) All the experiments discussed
in the main paper are on strings of fixed lengths.
We conducted some experiments on tasks with vari-
able length sequences which in some sense have
low sensitivity. The tasks can be seen as a variable
length extension of sparse parities and sparse ma-
jorities. Unlike the fixed length setting, we found
both LSTMs and Transformers perform similarly
on those tasks. See Section E.1 in the Appendix for
more details. (b) Although we found Transformers
to consistently converge to low sensitivity func-
tions in the case of Boolean functions, we did not
find similar behaviour on sentiment classification
datasets such as SST and IMDB (see Section C).
A caveat with empirical studies such as this is
that the results depend on the hyperparameters and
other aspects of the experimental setup. While
we have tried to be as thorough as possible with
hyperparameter tuning, there is always a chance
that the results or behaviour could differ for some
hyperparameter.
Ethics Statement
We have extensively discussed the limitations of
our work in the previous section. We use two exist-
ing datasets, SST (Socher et al., 2013) and IMDB
(Maas et al., 2011), which are publicly available
and commonly used in NLP research. We syntheti-
cally generate datasets of formal languages which
does not require ethical consideration. We have
discussed the experimental details and computa-
tional budget in detail in Appendix G. The research
presented in this paper focuses on analysing the in-
ductive biases of Transformers and LSTMs basedon experiments on formal languages and subse-
quently we believe that our work does not raise any
ethical concerns.
References5776577757785779A Roadmap
The appendix is organized as follows.
•In Section B, we report and discuss additional
results on the complexity of random models.
•In Section C, we investigate the sensitivity
of models on real data. In particular, we
demonstrate that models learn functions of
increasing sensitivity on sentiment classifica-
tion datasets such as SST and IMDB.
•In Section D, we discuss some additional re-
sults relating sensitivity and generalization.
•In Section E, we present additional experi-
ments investigating the ability of Transform-
ers and LSTMs to learn sparse boolean func-
tions.
•In Section F, we present some experiments to
show that both Transformers and LSTMs can
easily fit practical datasets even when they are
labelled randomly.
•In Section G, details of implementation and
experimental setup are discussed which are
relevant for the reproducibility of the results.
•In Section H, we discuss some additional
works related to our paper.
B Complexity of Random Models
In this section, we discuss additional results re-
lated to the complexity of random Transformers
and LSTMs. We present results with additional
complexity measures, initialization strategies, and
variations across hyperparameters.
B.1 Additional Measures
As discussed in Section 3, sensitivity is related
to several other complexity measures. Since it is
more tractable to estimate sensitivity as opposed
to certain other measures, we primarily focused on
estimating and comparing sensitivity in the main
paper. We explore three other complexity mea-
sures which have been previously explored in the
literature to compute the complexity of functions
represented by neural models. The measures are
defined as follows:
1.SOP (Size of Boolean Expression) : This mea-
sure computes the size of the smallest Booleanexpression in Sum-of-Product form that rep-
resents the function. In order to compute this
for a neural network over {0,1}, we compute
the output of the model over all 2inputs and
then use standard libraries (SymPy (Meurer
et al., 2017)) to find the Boolean expression.
The size indicates the number of operators and
operands in the smallest expression. Since the
problem of minimizing Boolean expressions
is NP-complete, the runtime grows exponen-
tially, and hence, we can only compute this
up to length 10for several samples of ran-
dom models. This measure was explored in
Valle-Perez et al. (2019).
2.Entropy : This measure takes the output la-
bels for all 2inputs and simply computes the
entropy over the labels. This is a weak mea-
sure and primarily indicates how imbalanced
the label set is. This measure was explored
in Mingard et al. (2019); Valle-Perez et al.
(2019).
3.CSR (Critical Sample Ratio) : This measure
computes the fraction of inputs for which the
function label changes at a small fixed dis-
tance from the inputs (Arpit et al., 2017). For
discrete inputs such as {0,1}, CSR can be
seen as the fraction of inputs for which the
function label changes at a Hamming distance
of1. This was also explored in Valle-Perez
et al. (2019).
Figure 9 shows the distribution of different com-
plexity measures and scatter plots depicting rela-
tions among them. The measures are computed
for random Transformers and LSTMs with weights
sampled uniformly between -10 and 10. The mea-
sures are computed for sequences of length 7with
200ksamples of models. We take Transformers
and LSTMs with depth ∈ {1,2,4,8}and width
(d_model/hidden_size) ∈ { 8,32,64,256,768}.
We take an equal number of samples for each hy-
perparameter. Figure 8 shows the distribution of
SOP based on 50k samples for a fixed hyperpa-
rameter configuration of Transformer and LSTM.
It includes a 1-layer LSTM with width 64and a
4-layer Transformer with width 64.
As can be seen in Figure 9, there exists signifi-
cant correlation between sensitivity and other mea-
sures. Note that, high sensitivity functions will
always have high entropy and high CSR but the5780
converse is not true. Functions with maximum en-
tropy can also have low sensitivity. For instance,
the dictator function has maximum entropy (since
half the inputs have label 1 and the other half have
label 0) while having very low sensitivity. Sim-
ilarly, CSR can be seen as a weaker version of
sensitivity.
B.2 Why Sensitivity?
Sensitivity can be seen as a discrete analog
(Gopalan et al., 2016) of the ‘smoothness’ of a
continuous function which measures how gradu-
ally a function changes locally. Functions of higher
sensitivity can be considered more complex since
the function value can be changed by changing any
of a large subset of bits whereas functions of lower
sensitivity depend on fewer bits and their function
value can be determined based on a small number
of input coordinates. Sensitivity measures are also
polynomially related to several other notions of
complexity such as the depth of a decision tree, cer-
tificate complexity, and the degree of the Fourier
expansion of Boolean functions (see Ambainis et al.
(2014) for more details). The correlation between
generalization and a different notion of sensitivity
has been demonstrated in Novak et al. (2018) for
computer vision models. The relation between gen-
eralization and a variant of Boolean sensitivity has
even been explored over a decade ago by Franco
(2006). More recently, Hahn et al. (2021) extend
the notion of block sensitivity to incorporate vari-
able length sequences and propose it as a measure
to estimate the difficulty of various NLP tasks.
B.3 Additional Sensitivity Results
The distribution of the sensitivity of Transformers
and LSTMs initialized with Xavier uniform dis-
tribution are given in Figure 11 respectively. ForGaussian initialization, the weights are sampled
with mean 0 and σ= 10 . For Xavier uniform
initialization all the values in weight matrices are
sampled uniformly between −danddwhere
dis the number of hidden units. The values in the
bias vectors are set to zero and the ones in the input
embedding vectors are sampled from N(0,1).
Finding Parity. For strings of length 5, the total
number of possible functions is 2. If Boolean
functions are sampled uniformly, then the proba-
bility of picking the P function is less than
1 in two billion. However, on uniformly sampling
10million LSTMs of depth 2and hidden size 8,
we found that the probability of finding one that
represents P is 1 in 30,000. Hence, it is
over 60,000 times more likely to find P func-
tion by sampling LSTMs than randomly sampling
Boolean functions. This indicates that the parame-
ter space of recurrent models such as LSTMs has
a significant representation of P functions
which might help explain why it is easier for them
to learn P . On the other hand, for Trans-
formers, we did not find a single sample which
represented P based on 10 million samples.
Change across hyperparameters. For uniform
sampling, a general observation for both the archi-
tectures is that the likelihood of higher sensitiv-
ity functions increases with the number of layers
(see Figure 15, left and Figure 10), however, even
for Transformers with depth 12, the distribution is
heavily skewed towards low sensitivity functions
in comparison to recurrent models with depth 1.
Unlike recurrent models, the sensitivity of Trans-
formers decreases when the width of the model
is increased (see Figure 15, middle). For Trans-
formers, the average sensitivity decreases with the
increase in the length of the strings (see Figure 15,
right), whereas for LSTMs, it remains quite high
even for lengths up to 200.
For LSTMs with uniform sampling, the change
in sensitivity across different widths (hidden_size)
and lengths is provided in Figure 10. As can be
seen, the sensitivity of LSTMs does not signifi-
cantly reduce across higher lengths and widths,
unlike Transformers.
While it is not entirely clear why random Trans-
formers are relatively more biased towards low
complexity functions, we observe that they behave
similar to hard-attention Transformers upon inspec-
tion of attention weights. Recent works (Hao et al.,
2022; Hahn, 2020) have shown that hard-attention57815782
Transformers can only represent functions in AC
(which contain functions that can be represented
by constant depth AND/OR circuits). Since AC
circuits can only represent functions of low average
sensitivity (O’Donnell, 2021), it might help explain
why random Transformers have low sensitivity.
C Sensitivity During Learning Sentiment
Classification
In this section, we discuss experiments on mea-
suring the sensitivity of Transformers and LSTMs
when trained on the sentiment classification task.
C.1 Experimental Setup
Datasets. We experiment with two sentiment clas-
sification datasets: SST (Socher et al., 2013) and
IMDB (Maas et al., 2011). For SST, we train on
the full train set of size 67349 examples and evalu-
ate both sensitivity and validation accuracy on the
validation set of size 872 examples. For IMDB,
we preprocess the dataset to only include sentences
of length up to 500. This leads to a train set of
size 22156. The validation set consists of 8939 ex-
amples randomly sampled from the test set. Since
the sentences in IMDB dataset are of much longer
lengths, in order to save compute, we evaluate sen-
sitivity of models on a dataset of size 894 examples
randomly sampled from the test set.
Sensitivity Metrics. Boolean sensitivity as de-
fined in Section 3 cannot be directly applied to
sequences of variable length and larger vocabulary.
As an alternative, we compute certain proxy met-
rics which measure how likely it is for the function
value to change due to a change in one token of
the input sequence. To that end, we design three
simple metrics to measure the sensitivity of models
trained on sentiment classification:
1.Word Label-Sensitivity: For each word in the
sentence (one word at a time), we replace it n
times with a word sampled randomly from thevocabulary and measure the average (over n)
number of times the predicted label changes.
We sum this value for all the words in the
sentence and normalize the value by its length.
2.Word Softmax-Sensitivity: For each word in
the sentence (one word at a time), we replace
itntimes with a word sampled randomly from
the vocabulary and measure the average (over
n)L2-distance between the predicted softmax
normalized output vector before and after the
replacement. Again, we sum this value for all
the words in the sentence and normalize by its
length.
3.Embedding Label-Sensitivity: For each word
in the sentence (one word at a time), we add
Gaussian noise with mean 0and variance σ
to its embedding ndifferent times and mea-
sure the average (over n) number of times the
predicted label changes. We sum this value
for all the words in the sentence and normalize
by its length.
For all metrics, the final score is obtained by
averaging across all the examples in the dataset. In
all our experiments, we set n= 10,andσ= 15 .
Hyperparameter Details. For both
Transformers and LSTMs, we vary the
number of layers ∈ { 1,2}, learning rate
∈ {0.0001,0.0003,0.0005}, and model width
(d_model/hidden_size) {128,256}. We set the
batch size as 128and the FFN size as twice the
width. For LSTMs, we keep the embedding
size the same as the hidden size. Both models
are trained with Adam (Kingma and Ba, 2014)
optimization and using Dropout regularization
with probability 0.2.
Results. Figure 12 shows the word softmax-
sensitivity for both models across different iter-
ations of training for SST and IMDB datasets.
The word label-sensitivity and embedding label-
sensitivity for SST is provided in Figure 13.
We find that across all three measures, both Trans-
formers and LSTMs learn functions of increasing
sensitivity where they prioritize learning functions
of lower sensitivity first. We found ‘word label-
sensitivity’ and ‘word softmax-sensitivity’ to cor-
relate well with generalization gap (i.e., the differ-
ence between train accuracy and test accuracy).
Since the measures are very similar, there is a
strong correlation between the two measures them-
selves. We did not find any non-trivial correlation5783
between ‘embedding label-sensitivity’ and gener-
alization gap. Note that unlike random and sparse
Boolean functions, on real datasets, we did not find
Transformers converging to functions with lower
sensitivity.
D Sensitivity and Generalization
D.1 Sensitivity as Capacity Measure
We show how maximum sensitivity can be used as
a capacity measure to derive generalization bounds.
Capacity measures such as the VC Dimension are
a classical approach to derive sample complexities
and probabilistic upper bounds for the test error of
a classification model.
LetF:{0,1}→ {± 1}be a class of functions
such that the maximum sensitivity for any function
f∈ Fis upper bounded by kwhere 0≤k≤
n. Any function fwith a maximum sensitivity k
can be uniquely determined by its values on any
Hamming ball of radius 2kin{0,1}(Gopalan
et al., 2016). This can be used to upper bound the
size of the function class |F| ≤2(). Since
the VC Dimension (denoted VCD ) of a class of
functions Fis upper bounded by log|F|, we have
that,
VCD(F)≤/parenleftbiggn
≤2k/parenrightbigg
≤/parenleftbiggn+ 2k
2k/parenrightbigg
(4)≤/parenleftbigge(n+ 2k)
2k/parenrightbigg
=O(n)
Letf∈ Fbe a target function and ˆf∈ F
be a hypothesis produced by a learning algorithm.
LetL(ˆf, f) = E[I[ˆf(x)̸=f(x)]]be
the true error between fandˆf. Similarly, let
ˆL(ˆf, f) =/summationtextI[ˆf(x)̸=f(x)]be the empir-
ical error on a sample set S. Then using Equation
(4) and basic properties of VC dimension (Mohri
et al. (2018)), we can upper bound the distance
of the true error Lfrom the sample error ˆLusing
maximum sensitivity.
Proposition D.1. For any δ >0, with probability
at least 1−δ, the following holds for any function
f,ˆf∈ F,
L(ˆf, f)≤ˆL(ˆf, f) +/radicaligg
cnlog+ 8 log
m
(5)
where c >0is some constant. Functions with
low maximum sensitivity can be learned with bet-
ter sample efficiency. Functions with low average
sensitivity can also be learned efficiently when the
data generating distribution is uniformly distributed
over the input (O’Donnell (2021), Sec 3.4).
D.2 Sensitivity and Generalization Gap
The correlation between sensitivity and general-
ization has previously been studied for networks
trained on Boolean functions (Franco, 2006) and
image datasets (Novak et al., 2018). We examine
the relation between simple variants of sensitivity
described in Section C and generalization.
We train various models on SST dataset until
convergence and then compare sensitivity with gen-
eralization gap. The generalization gap is simply
the difference between the train error and test er-
ror; higher gap indicates overfitting. We plot the5784
word label-sensitivity and word softmax-sensitivity
(defined in Section C) for Transformers, LSTMs,
and a pretrained Large Language Model (RoBERTa
(Liu et al., 2019)) against the generalization gap
(see Figure 14). We observe positive correlation
between the measures and generalization gap indi-
cating that when sensitivity is higher, the models
are more likely to overfit and achieve poorer gen-
eralization performance. Large language models
such as RoBERTa have a lower sensitivity while
achieving better test accuracies than Transformers
and LSTMs trained from scratch.
E Additional Experiments on Sparse
Boolean Functions
Standard Parity. The training curves for LSTMs
on standard parity are provided in Figure 17. The
models are trained on datasets of size 20k where
the input strings are of length 30. Similar to Trans-
formers on S P , we observe phase
transitions for LSTMs on standard P task.
Sparse Parities. The results on sparse parities with
length n=40 and k=4 relevant bits for Transformers
with absolute positional encodings are provided in
Figure 18. We find that Transformers with absolute
positional encodings are able to generalize well
onS P task and exhibit grokking on
relatively larger datasets (30k samples) in compar-
ison to models with learnable positional embed-
dings. For Transformers trained with learnable
encoding, we robustly observe grokking on smalldatasets. Figure 19 depicts the training curves for
Transformers trained on datasets of size 5k.
Overfitting. We found that Transformers overfit on
training data when the sample size is too low (see
Figure 20). Apart from that, for datasets of certain
sizes, we find that while Transformers with depth
up to 6generalize well, those with much higher
depths ( >8) overfit across several runs.
Effect of Regularization on LSTMs. We explore
the effect of dropout and L2 regularization on train-
ing with LSTMs. While training on sparse parities,
we find that increasing regularization increases the
convergence time but the model still overfits and
converges to a function with higher sensitivity than
the target function. Upon further increasing regu-
larization, the model fails to fit the training data.
Mixed Parity. To explore the difference in bias be-
tween Transformers and LSTMs, we conduct a sim-
ple experiment described as follows. We create a
dataset of size 15k called ‘Mixed Parity’ where half
of the examples are labelled as standard P
(label is determined by all bits) and the other half is
labelled as S P with4relevant bits.
The inputs are of length 30and the first bit deter-
mines whether the input is labelled according to
standard P function (when the first bit is 1)
or as a S P function (when the first
bit is 0).
We train Transformers and LSTMs (with learn-
able positional encodings) of depth 2and width
64across various learning rates ∈[0.01,0.00001]
on the Mixed Parity dataset. We find that LSTMs
obtain 100% training accuracy on the dataset (see
Figure 21, right); LSTMs validation accuracy on
theP task is near 100% whereas it is 50% on
theS P task. In contrast, the train-
ing accuracy of Transformers converges around
75% (see Figure 21, left); their validation ac-
curacy on the P task is 50% whereas on
S P they achieve near 100% vali-
dation accuracy.
Convergence time vs Sample Size. For Trans-
formers trained on S P , we con-
duct experiments to compare the number of com-
putational steps required to successfully learn
S P with the size of the dataset it
is trained on. We consider length n=40 and k=4,
and create datasets of five different sizes ({5k,
25k, 50k, 100k and 250k}). For each dataset,
we train a Transformer of depth 2 and width 1285785
across 100 different initializations with learning
rate∈ {0.0001,0.0005}and with batch size 500.
We consider each iteration as a computational step.
We report the median, minimum and maximum
steps for each dataset in Figure 22. We find that
neural networks such as Transformers can suc-
cessfully learn S P with relatively
small number of computational steps on small-
sized training sets. It is perhaps surprising that
forS P withn=40 and k=4, Trans-
formers can successfully generalize well with less
than20000 computational steps on over 75 out of
100 runs.
Phase Transitions. As reported in Barak et al.
(2022), we observe phase transitions on Parity tasks
where the training and validation accuracies do not
change for a large number of training iterations
and then abruptly reach near-perfect accuracy in
a few iterations (see Figure 6). This phenomenon
was observed for feedforward networks (FFNs) and
Transformers in Barak et al. (2022) and theoreti-
cally explained for ReLU FFNs trained with SGD.
We observe another such behaviour for LSTMs
onP -n(see Figure 17 for training curves
onP -30). For both LSTMs and Transform-
ers, we were unable to get them to generalize well
with SGD on either S P or standard
P . Both the architectures seem to succeed
with the Adam optimizer (Kingma and Ba, 2014).
Grokking. Another interesting phenomenon we
observe in Transformers is that in some cases the
training accuracies start increasing gradually with
no change in the validation accuracy. After some
iterations, the validation accuracy increases and
matches the training accuracy. We reliably ob-
served this phenomenon while training Transform-
ers with absolute positional encodings across train-
ing sets of various sizes (see Figure 18) and while
training with learnable encodings on small-sized5786training sets (see Figure 19). Similar observations
for grokking (Power et al., 2022) were made in
Barak et al. (2022) for ReLU FFNs trained on
small-sized training sets.
E.1 Experiments on Variable Length Inputs
We conducted some additional experiments on
tasks with variable length inputs. These tasks are
simple extensions of sparse parities and majorities
to variable length input and have (in an informal
sense) low sensitivity.
Task. LetVP -(n, k)denote the exten-
sion of P -(n, k)to variable length sequences.
A function in VP -(n, k)is defined over
sequence of {0,1,2}where the total number of 0s
and1s are exactly n, along with krelevant indices
which determine the label. The input distribution
is such that there could be token 2between any
zeros and ones with some probability. The tokens
2however do not influence the output of the func-
tion and are merely constructed to vary the input
lengths. The label is determined by removing all
the tokens 2from the input and applying the reg-
ular P -(n, k)over the remaining string over
{0,1}.
For illustration, for f∈P -(4,2), where
S={1,3}, for an input ‘ 1001 ’, the function
f(1001) = 1 since the number of 1s in posi-
tion 1 and 3 is odd. For a similar function ˆf∈
VP -(4,2), here are some examples on var-
ious inputs: ˆf(2102202221) = ˆf(12002212) =
ˆf(122020 21222) = f(1001) = 1 . The function
VM-(n, k)is defined similarly, where it takes
an input string over {0,1,2}and the label is de-
termined by removing all 2s and applying regular
M-(n, k)on the remaining string over {0,1}.
Results. Contrary to the fixed length set-
ting, we observe that both Transformers and
LSTMs perform similarly on these tasks. For both
VP -(n, k)andVM-(n, k)we exper-
iment with various mean lengths and variances with
k= 5. The general behaviour is that both Trans-
formers and LSTMs generalize well when the tasks
over short sequences ( <40forVP -(n, k)
and<100forVM-(n, k)). However as the
lengths of the input go beyond that, both architec-
tures do not generalize well.
In comparison to LSTMs, Transformers only per-
formed better when the variance of the lengths of
the inputs was very low. An interesting observation
about Transformers is that they only seemed to gen-
eralize well with positional masking (also referred
to as causal masking) along with positional encod-
ings. Their performance was notably worse with
only positional encodings (learnable or absolute).
These results do not support the hypothesis
posed in Section 1 and we intend to explore this
further in the future.
F Fitting Randomly Labelled Data
We conduct some experiments to examine the abil-
ity of LSTMs and Transformers to fit random noise.
The capacity of a class of functions to fit ran-
dom noise is often theoretically measured as its
Rademacher complexity. Given the incredible ex-
pressive power of neural networks, measures such
as Rademacher complexity lead to vacuous gener-
alization bounds. One assumption was that, despite
their capacity, deep neural networks trained with
gradient-based methods can only learn a small sub-
set of such functions. The work of Zhang et al.
(2021) demonstrated that large feedforward-like
networks trained with gradient-based methods are
able to fit random noise on image datasets. We con-
duct similar experiments to evaluate the ability of
sequence models to fit noise on text data. We con-5787
sider the SST dataset (Socher et al., 2013) as used
in the GLUE benchmark. The training data con-
tains approximately 65k samples and we label each
sample either +1or−1randomly (with probability
1/2 each).
Figure 23 depicts the training curves for Trans-
formers and LSTMs. We find that both the models
are able to conveniently fit the training set near-
perfectly. For both the models, the training takes
significantly more number of iterations/epochs in
comparison to training on the original dataset with
true labels which only takes a few epochs.
G Implementation Details
Our implementation of Transformer is based on
Rush (2018). For various recurrent models such
as RNNs, GRUs, and LSTMs, we use PyTorch’s
standard implementation (Paszke et al., 2019).
For each dataset, we extensively tune across sev-
eral hyperparameters and report the results based
on the best-performing models. Table 1 lists the
hyperparameters used for tuning the models for
Boolean function experiments in Section 5. We use
a grid search procedure to tune the hyperparame-
ters. The models were trained with cross-entropy
loss. For all our results, we used Adam Optimizer
and tuned the learning rates. We also tried SGD
with weight decay but could not get either Trans-
formers or LSTMs to perform well on parities,
sparse parities, or random k-sparse functions.
Compute. All our experiments were conducted
using 16 NVIDIA Tesla V100 GPUs each with
16GB memory. Since the datasets are synthetic
and relatively smaller than practical datasets, most
training runs took 10-30mins on a single GPU. The
larger expenditure was on tuning LSTMs to find
whether any of the hyperparameters succeed. Some
experiments with LSTMs on P -(40,4)for
over 100k steps took 3 hours and across multiple
hyperparameters took ≈400 GPU hours in total.
The experiments conducted for Figure 22 took sim-
ilar amount of GPU hours ( ≈300). The rest of the
experiments took less than 10% of this time. The
experiments conducted in Section 4 with random
models were not as compute intensive.
H Additional Related Work
Formal Languages and Recurrent Models. For
recurrent models, analysis on formal languages
dates back to a few decades ago (see Kolen and
Kremer (2001)). Several works have examined
the ability of RNNs and LSTMs to recognize vari-
ous context-free and counter languages (Gers and
Schmidhuber, 2001; Weiss et al., 2018; Suzgun
et al., 2019b), the most prominent one being the
Dycklanguages (Skachkova et al., 2018; Bhat-
tamishra et al., 2020b). Connections between
RNNs and finite state automata have been explored
for a long time (Goudreau et al., 1994; Korsky and5788Hyperparameter Bounds Transformer | LSTM
D_model/Hidden Size [16, 128] | [8, 256]
Heads [4, 8 ]
Number of Layers [1, 6] | [1, 6]
Learning Rate [1e-2, 1e-5]
Position Encoding Scheme [Learnable, Absolute]
Berwick, 2019). Prior works have also sought to
extract finite state automata from recurrent models
trained on regular languages (see Wang et al. (2018)
for a survey). Connections between LSTMs and
counter automata have also been established empiri-
cally (Suzgun et al., 2019a) and theoretically (Mer-
rill et al., 2020). More recently, multiple works
have investigated the ability of Transformers to
recognize various regular, context-free (Ebrahimi
et al., 2020; Yao et al., 2021; Bhattamishra et al.,
2020b), and mildly context-sensitive languages
(Wang, 2021).
Neural Networks and Parities. Prior works can
be divided into two categories. One set of works
focuses on the Parity language containing strings of
arbitrary length that can be represented by a 2-state
DFA. The other set examines the P -(n, k)
problem with strings over {0,1}where the output
depends on a subset of bits. The P -(n, k)
problem has been widely studied in learning the-
ory and has several well-understood properties. On
theP -(n, k), Barak et al. (2022) theoreti-
cally analyze the ability of feedforward-like net-
works trained with SGD and conduct experiments
with various architectures including Transformers.
Some of our results corroborate their findings and
we empirically explore the phenomenon further.
Preliminary experiments along this direction were
also explored in Edelman et al. (2022) where they
showed that Transformers can efficiently express
k- functions.
Since variable length Parity can be represented
by a 2-state DFA, small-sized RNNs can effi-
ciently represent them and several works have
found LSTMs to generalize well when tested em-
pirically (Schwarzschild et al., 2021). On the other
hand, Transformers are limited in their ability to
express such a language (Hahn, 2020). While they
can express them for bounded lengths (Chiang
and Cholak, 2022), they have been found to strug-gle when tested empirically (Bhattamishra et al.,
2020a; Delétang et al., 2022; Chiang and Cholak,
2022). Anil et al. (2022) explore length general-
ization abilities of large language models on Parity.
Finding P and other languages by uniformly
initializing weights was also explored in the 1990s
(see Chap. 13 Kolen and Kremer (2001)) for older
versions of recurrent architectures which are not
used anymore.5789ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
4,5
/squareB1. Did you cite the creators of artifacts you used?
1,2,4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Our use of existing artifacts pertains to usage of the Transformer and LSTM models and the SST and
IMDB datasets which are all open-sourced and usage is allowed without restriction.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
see section 4,5
C/squareDid you run computational experiments?
4,5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
H5790/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4,5,H
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
See result ﬁgures.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
H
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.5791