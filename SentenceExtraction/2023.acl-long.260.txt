
Chenzhengyi LiuJie HuangKerui Zhu Kevin Chen-Chuan Chang
University of Illinois at Urbana-Champaign, USA
{cl115, jeffhj, keruiz2, kcchang}@illinois.edu
Abstract
In this paper, we propose DimonGen , which
aims to generate diverse sentences describing
concept relationships in various everyday sce-
narios. To support this, we first create a bench-
mark dataset for this task by adapting the exist-
ing CommonGen dataset. We then propose a
two-stage model called MoREE to generate the
target sentences. MoREE consists of a mixture
of retrievers model that retrieves diverse con-
text sentences related to the given concepts, and
a mixture of generators model that generates di-
verse sentences based on the retrieved contexts.
We conduct experiments on the DimonGen task
and show that MoREE outperforms strong base-
lines in terms of both the quality and diversity
of the generated sentences. Our results demon-
strate that MoREE is able to generate diverse
sentences that reflect different relationships be-
tween concepts, leading to a comprehensive
understanding of concept relationships.
1 Introduction
Concepts are mental representations of classes or
categories of objects, events, or ideas, distinguished
by shared characteristics that set them apart from
other things. For instance, the concept of “dog”
represents a class of animals that share character-
istics such as being four-legged, having fur, and
being domesticated. These concepts are crucial in
helping us understand and communicate about the
world around us.
To fully grasp concepts, it is important to un-
derstand the relationships between them. Re-
searchers have proposed using generated sentences
as a means to model these relationships more ef-
fectively (Lin et al., 2020; Huang et al., 2022a,c;
Huang and Chang, 2022b). For example, Common-
Gen (Lin et al., 2020) aims to generate coherentFigure 1: An example of DimonGen. The input is a
pair of concepts and the output is a set of sentences that
capture different ways in which these concepts interact.
sentences that describe everyday scenarios involv-
ing specific sets of common concepts, while Open
Relation Modeling (Huang et al., 2022a) generates
informative sentences that describe relationships
between concepts/entities.
However, in real-world scenarios, concepts often
refer to broad classes, and their relationships can be
complex. This can make it challenging to summa-
rize these relationships through a single sentence.
For example, “dog” and “sheep” are both animal
concepts, but while “dogs” can herd “sheep”, they
can also attack them. A single sentence would not
accurately convey this complexity, leading to an
insufficient understanding. Additionally, this ap-
proach can also introduce bias, particularly when
concepts are related to sensitive topics such as gen-
der or race. For instance, the statement “ women are
better suited for caregiving roles than men.” is a
biased statement.
To mitigate the above issues, we propose a new
task called DimonGen: Diversified Generative
Commonsense Reasoning . The task involves gen-
erating diverse sentences that describe the relation-
ships between two given concepts, such as the ex-
ample shown in Fig. 1 of the concept pair “dog”
and “sheep”. This helps build a comprehensive and
diverse understanding of the relationships between
the concepts in various everyday scenarios.
DimonGen is a challenging task because it re-
quires generating reasonable scenarios for a given
pair of concepts without any context. This requires
a deep understanding of relational and common-
sense knowledge about the concepts. Additionally,
the target outputs must reflect diverse relationships4719between the input concepts. Previous approaches
to generating diverse content have used sampling
from a designed vocabulary distribution (Holtzman
et al., 2020; Meister et al., 2022; Fan et al., 2018)
or encoding inputs to various latent variables (Zhao
et al., 2017; Cao and Wan, 2020). However, these
methods introduce diversity only at the generation
stage which may not be suitable for the DimonGen
task as it relies on the semantic information from
the input contexts.
To overcome the challenges, we propose
MoREE :Mixture ofRetrieval- Enhanced Experts ,
a two-stage method that utilizes external knowl-
edge to generate diverse relationship sentences. In
the first stage, MoREE retrieves diverse context sen-
tences related to the given concepts using a mixture
of retrievers model based on the Mixture of Experts
(MoE) model (Shen et al., 2019). In the second
stage, MoREE generates diverse relationship sen-
tences conditioned on the retrieved contexts using
a mixture of generators model. An Expectation-
Maximization (EM) based matching algorithm is
proposed to combine the two stages. By extracting
diverse contexts from corporas before generation,
MoREE aims to improve the diversity and quality
of the generated relationship sentences.
We build a benchmark dataset for DimonGen by
adapting the existing CommonGen benchmark (Lin
et al., 2020) and conduct both quantitative and qual-
itative experiments on the dataset. The results indi-
cate that our proposed MoREE model outperforms
well-designed baselines in terms of both the qual-
ity and diversity of the generated sentences. For
example, in the automatic evaluation, our method
gains over 2%in the BLEU-4 score for quality
and around 5%inSelf-BLEU-4 for diversity. And
in our human evaluation, the annotated score (up
to5) for quality increases from 3.77 to 4.21, and
for diverse increases from 3.65 to 3.94. We also
conduct detailed ablation studies and case studies
to further verify the effectiveness of our proposed
method. Overall, the results suggest that MoREE
can generate diverse sentences that reflect relation-
ships between concepts from multiple and varied
perspectives.
2 DimonGen: Diversified Generative
Commonsense Reasoning
We propose a task called DimonGen that aims to
generate diverse sentences that describe the rela-
tionships between a pair of concepts from differentperspectives. The task is defined as a diverse con-
strained text generation task, where the input is a
pair of concepts (i.e., x={e,e}) and the out-
put is a set of sentences Y={y, . . . , y}that
include both concepts and are diverse in terms of
their content (an example is illustrated in Fig. 1).
To solve the above task, we propose a two-stage
retrieval-enhanced method named MoREE, which
consists of a mixture of retriever and generator
models (Fig. 2). This method is based on the Mix-
ture of Experts (MoE) model, which will be re-
viewed in the following section before introducing
MoREE.
2.1 Base Model: Mixtrue of Experts for
Diverse Text Generation
The Mixture of Experts (MoE) is an ensemble tech-
nique that was originally designed to increase the
capacity of a model (Jacobs et al., 1991; Jordan
and Jacobs, 1994). It consists of several expert
models that share the same network architecture,
but have different probabilities of being assigned to
the same training examples. This means that each
expert model is exposed to a different subset of
the training data, and the MoE ensemble combines
them to achieve optimal performance.
In recent years, MoE has been adapted for text-
generation tasks to improve diversity in the gen-
eration stage (Shen et al., 2019; Cho et al., 2019).
Since the mixture base models are trained on differ-
ent subsets of the training data, they can learn differ-
ent aspects of the input, leading to a diverse set of
generations during the inference phase. Formally,
for each training example (x,y)where y∈ Y is
a relation description, if there are nexpert models
with a set of latent variables Z={z, . . . , z}
as identifiers, the likelihood of the MoE model is
formulated as the following marginal likelihood:
p(y|x;θ) =/summationdisplayp(z|x;θ)p(y|z,x;θ),(1)
where θrepresents the model weights.
To promote diversity among the different expert
models, the training examples are split into subsets
with distinct elements, and each expert model is
trained on one subset. This training process is done
through a hard-EM algorithm as follows (Shen
et al., 2019; Yu et al., 2022):
•E-step: for each training example (x,y), se-
lect the expert model z∈ Z that maximizes4720the posterior probability p(z|x,y;θ)using cur-
rent model weights θwith the equation z=
arg maxp(y,z|x;θ).
•M-step: update the model weights θthrough the
gradients ∇logp(y,z|x;θ)of selected expert
model z.
The hard-EM algorithm is performed by iterat-
ing these two steps. It should be noted that this
algorithm can be easily applied to a batch learning
algorithm by updating the model weights for each
batch during the M-step. Finally, by assuming a
uniform prior of expert models, the loss function
could be formulated as
L=E[min−logp(y|z,x;θ)].(2)
2.2 MoREE: Mixture of Retrieval-Enhanced
Experts
The DimonGen task poses a significant challenge
as it requires relational commonsense reasoning
and the generation of diverse content with the min-
imal input information. Traditional methods for
encouraging diverse text generation focus on intro-
ducing diversity in the generation stage through di-
versified decoding or sampling mechanisms (Meis-
ter et al., 2022; Fan et al., 2018; Zhao et al., 2017;
Cao and Wan, 2020). However, these methods are
not suitable for the DimonGen task due to the lim-
ited input information and the need for diversified
relational reasoning. Our experiments in Sec. 3.3
show that even with powerful pre-trained language
models, these methods struggle to solve this task.
To address this challenge, we propose a diversi-
fied retrieval-enhanced method named Mixture of
Retrieval- Enhanced Experts (MoREE). Our over-
all framework is illustrated in Fig. 2 which consists
of two stages. In the first stage, we use a mixture
of retrievers model to extract several sets of diverse
context sentences as auxiliary inputs to help with
the generation process. In the second stage, we use
a mixture of generators model to generate diverse
outputs and propose a matching algorithm to assign
the appropriate contexts to the target outputs.
2.2.1 Retrieval Stage
To better understand the relationships between
given concepts, we introduce a retrieval stage to
gather context sentences from external corpora C.
Given an input concept pair x={e,e}, we aimto retrieve several diversified sets of relational con-
texts{S, . . . ,S}, where S={s, . . . , s}is a
set of context sentences containing x.
We train the retriever models on a binary clas-
sification task. Given a candidate sentence from
external knowledge corpora s∈ C, we concate-
nate it with the input xand use it as input:
x=[CLS] x[SEP] s[SEP]
x=e[SEP] e,(3)
where [CLS] and [SEP] are special tokens in pre-
trained language models. The model’s task is to pre-
dict a label yfrom [0,1], indicating the confidence
of the candidate sentence being a true relational
context for the input concepts. For each input, we
use its target output sentences in the dataset as
positive examples and randomly sample the same
number of negative examples from its retrieved
candidate sentences.
To extract diversified contexts for each input
concept pair, we introduce the mixture of experts
(MoE) method into the retriever model. Since inde-
pendently parameterizing each expert may cause an
overfitting problem, we follow the weight-sharing
schema in Shen et al. (2019) with a unique iden-
tifier to solve this issue. To make the MoE mod-
els more easily understood by pre-trained language
models, for each expert model, we design its unique
identifier as latent variables z=z, . . . , zwhich
is a randomly sampled prefix token sequence in the
model vocabulary. Once an expert is chosen, we
could train the model by concatenating the latent
variable and input concepts with contexts as the
final input:
x=z[CLS] x[SEP] s[SEP] . (4)
We apply the hard-EM algorithm (Shen et al.,
2019; Yu et al., 2022) to train our mixture of re-
trievers model. For each iteration, at E-step, we
assign the expert model to each input; at M-step,
we update all the expert models with the assigned
inputs. With this process, the total training loss
turns into an expectation form:
L=E[min−logp(y|x;θ)].(5)
However, during experiments, we find the bi-
nary classification problem has obvious patterns,
and simply applying the hard-EM algorithm may
lead to a severe overfitting problem (i.e., one ex-
pert always predicts one class label). To solve this4721
problem, we propose a regularization term based
on Jenson Shannon divergence (Sibson, 1969) to
penalize the output probability distribution over
different labels among experts. Given the output
probability distribution of nexperts {P, . . . , P},
the regularization loss is calculated as an average
of the Kullback-Leibler (KL) distances between
each distribution and the distribution center:
L=1
n/summationdisplayD(P||1
n/summationdisplayP), (6)
where D(·||·)is the KL divergence.
The final loss function for our mixture of retriev-
ers model is a weighted sum of the two:
L=L+αL, (7)
where αis a hyperparameter to balance the two
losses.
2.2.2 Generation Stage
At the generation stage, we fine-tune a mixture of
generators model to generate qualified and diver-
sified relationship sentences with retrieved sets of
contexts. Given an input concept pair x={e,e}
and several diversified sets of context sentences
{S, . . . ,S}from the retrieval stage, our goal is
to generate a set of relationship sentences ˆY=
{ˆy, . . . , ˆy}.
For each input concept pair xand each set of
its context sentences S={s, . . . , s}, we con-
catenate all of their input token sequences with
the expert’s latent variable zto construct the final
input as
x=z[CLS] x[SEP] s[SEP] . . .s.(8)
By applying the same method for all sets of re-
trieved sentences, we can obtain ndifferent context-
aware inputs {x, . . . , x}.However, the retrieved contexts are not present
in the original dataset and thus, there is no explicit
link between the target outputs and the retrieved
contexts. To address this issue, we propose a match-
ing algorithm based on a hard-EM algorithm sim-
ilar to the one used in the MoE process. For each
input, we evaluate its compatibility with each tar-
get output by calculating the posterior probability
p(y|x;θ)using the current generator model’s
parameters θ. The context-aware input is then as-
signed to the target output with the highest score:
y= arg maxp(y|x;θ). (9)
In the training phase, we use Eq. (9)to construct
training examples at E-step and then use these ex-
amples to fine-tune a mixture of generators model
at M-step. In the inference phase, for each input,
we feed all the diversified context-aware inputs into
the generator model to generate diverse results.
3 Experiments
3.1 Dataset Construction and Analysis
We construct our DimenGen benchmark dataset
by combining the CommonGen dataset (Lin et al.,
2020), which contains high-quality descriptive sen-
tences for everyday relations between input con-
cepts, and ConceptNet (Speer et al., 2017), a se-
mantic graph with nodes representing concepts and
edges indicating the category of the relationship be-
tween them. To build our dataset, we first cluster all
pairs of input concepts present in the CommonGen
dataset and collect their corresponding relational
sentences as target relationship sentences. We then
verify the informativeness and correctness of the
dataset using ConceptNet. Specifically, we ensure
that each concept set in every target relationship
sentence contains a path between the input concept4722
pair on ConceptNet, verifying the existence of a se-
mantic relationship between the concepts described
by a chain of category names. This approach helps
us establish the semantic relationships between the
input concepts in a systematic manner and ensures
that the generated sentences contain coherent and
meaningful relationships.
To encourage diversity, given a target set of gen-
erations for an input concept pair, we first em-
bed each target sentence into latent space with
Sentence-BERT (Reimers and Gurevych, 2019)
model and calculate the cosine similarity for each
pair of them. Next, we filter out the generations
that have pair-wise cosine similarity higher than a
pre-set threshold p= 0.75in our experiments. For
each input concept pair, we limit its target refer-
ences within 3∼5. This is because if the number
is too small, it is difficult for the model to learn
the diversity in the references, while if the number
is too large, the models will be trained in a biased
manner towards some input concept pairs.
To help evaluate the generalization ability, fol-
lowing CommonGen (Lin et al., 2020), we explic-
itly control the ratio of unseen concept composi-
tions between input concepts in test examples and
target outputs in training examples. Table 1 shows
the basic statistics of the dataset. We totally extract
16212 examples in our dataset with 15263 ,665,
and1181 split for training, dev, and test. The ratio
of unseen concept compositions is 92% and98%
for dev and test respectively. The highly unseen
concept compositions make the DimonGen task a
difficult problem to solve, which requires the model
to be capable of generalized reasoning ability.
For the diversity, the average numbers of target
relationship sentences for each example are 4.13,
3.71, and 3.38for training, dev, and test sets re-
spectively. It is also noted that there are over 41%
examples that have 5target outputs. The high ratio
of examples with 5target references not only con-
tributes to increasing the models’ ability to generate
diverse outputs but also helps to build comprehen-
sive evaluation metrics.3.2 Experimental Setup
Baselines. Since we are targeting the DimonGen
task with many references for each input, we com-
pare with several strong baseline models with di-
verse text generation capabilities. Generally, previ-
ous works introduce the diversity at the generation
stage by either sampling the next word by a prob-
ability distribution (Sampling-based methods) or
incorporating mixture components in the generator
model (MoE-based methods). Different from previ-
ous works, our MoREE model introduces diversity
by extracting the diverse contexts from the external
corpora at the retrieval stage.
•Sampling-based methods. Sampling methods
create diverse outputs at the inference phase of
the generation stage. These methods sample
the next token with a designed probability dis-
tribution of the vocabulary, rather than simply
maximizing the likelihood. We compare with
three strong sampling-based methods: Top-k
sampling (Fan et al., 2018) truncates the sam-
pling pool by keeping only the top-k candidates
for each token in the generation. Top-p sam-
pling (Holtzman et al., 2020) cuts off the next-
token sampling pool from a threshold of the prob-
ability mass. Typical sampling (Meister et al.,
2022) constrains the generated words to expected
information content by shifting the truncation set
with a conditional entropy of prior content.
•MoE-based methods. MoE-based methods in-
troduce diversity at the training phase of the gen-
eration stage by using diverse latent variables.
We compare with two of them: MoE (Shen et al.,
2019) is the vanilla MoE model for diverse text
generation we discussed in Sec. 2.1. MoKGE (Yu
et al., 2022) incorporates commonsense knowl-
edge from an external graph and uses the MoE
model to generate diverse outputs. Compared
to our model, MoKGE also extracts information
from external knowledge, but it only introduces
diversity at the generation stage.
Implementation. In our proposed method, we
utilize external corpora from V ATEX (Wang
et al., 2019), ActivityNet (Krishna et al., 2017),
SNLI (Bowman et al., 2015), and MNLI (Williams
et al., 2018) for retrieval purposes. These datasets
comprise high-quality descriptive sentences and
are widely employed in commonsense benchmark-
ing tasks. We retrieve all sentences containing4723
both input concepts to create a candidate pool.
In cases where there are insufficient candidates,
we substitute the concepts with the smallest co-
sine similarity in each sentence, according to their
Word2Vec (Mikolov et al., 2013) embeddings. We
use pre-trained Roberta models (Liu et al., 2019) as
its base model to rank and select the candidates in
the retrieval stage. For the generation stage, we use
the pre-trained BART model (Lewis et al., 2020)
as the base model for all baseline methods and our
proposed method for a fair comparison. We require
each method to generate k= 3 relationship sen-
tences in our experiments because the minimum
reference sentences’ number in the dataset is 3.
We use Huggingface’s Transformers (Wolf et al.,
2020) to implement the code and perform a grid
search to find the best hyper-parameters for all base-
line methods. Our models were trained by one
NVIDA RTX A40 GPU card with about 4-5 hours
of training on the DimonGen dataset.
Metrics. To evaluate the performance of our pro-
posed DimonGen task, we use three different eval-
uation metrics: quality, pairwise diversity, and cor-
pus diversity.
•Quality metrics. For quality evaluation, we use
bothN-gram-based metrics such as BLEU (Pa-
pineni et al., 2002) and ROUGE (Lin, 2004), as
well as the concept overlapping rate ( Success
Rate) between the input and generated sentences.
We make a slight modification for the DimonGen
task by first requiring the model to generate a set
of top- kcandidates, then evaluating the quality
between each generated candidate and the target
references. The best candidate with the high-
est score is chosen and its score is used for the
quality metrics.
•Pairwise diversity. To measure pairwise diver-
sity, we compute the average score of N-gram-
based evaluation metrics between all pairs of gen-erations in the generated candidate set. The lower
the average score is, the higher the evaluated pair-
wise diversity will be. These metrics are named
Self-BLEU andSelf-ROUGE (Zhu et al., 2018).
•Corpus diversity. To evaluate the corpus diver-
sity of the generated text, we use two widely-used
metrics: Distinct- n(Li et al., 2016) and Entropy-
n(Zhang et al., 2018). Distinct- nis computed by
taking the ratio of the number of unique n-grams
to the total number of n-grams in the generated
sentences. On the other hand, Entropy- ncal-
culates the average uncertainty of the n-gram
distribution within one generation, providing an
estimate of the diversity of the generated text.
3.3 Experimental results
The experimental results in Table 2 show that our
proposed MoREE model outperforms all five base-
line models in both quality and diversity metrics
on the DimonGen task. Specifically, our method
achieves a 2%improvement in BLEU-4 compared
to other baseline models in terms of quality, and
outperforms the strong baseline MoKGE model by
around 5%inSelf-BLEU-4 for pairwise diversity
and4%indistinct-4 for corpus diversity. These
results demonstrate the superior diverse generation
capabilities of our proposed method.
Additionally, the results show that MoE-
based methods have a significant advantage over
sampling-based methods in terms of diversity, with
an approximate 5%improvement in Self-BLEU-
4and6%indistinct-4 . Furthermore, retrieving
from external corpora improves performance on
concept-related evaluation metrics, as shown by
the superior success rate of the MoKGE model
and MoREE model compared to the vanilla MoE
model. Our MoREE method specifically achieves a
4%gain in this metric, indicating the effectiveness
of the mixture retriever in extracting high-quality
contexts to assist diverse generations.4724
3.4 Ablation Study
In order to gain a deeper understanding of our pro-
posed two-stage framework, we conduct an abla-
tion study by removing different components of
our method and comparing the results. Specifi-
cally, we remove the MoE module from the re-
trieval stage, remove the proposed regularization
term for training MoE retrievers, and replace the
EM-based matching algorithm for the generation
stage with random selection. Table 3 displays the
results, revealing the following insights:
•For the retrieval stage, employing a mixture of
retrievers improves both quality and diversity.
When using a single retriever model with MoE
generators, the BLEU-4 score drops from 19.06
to16.91, and the Self-BLEU-4 score increases
from 24.85to27.77. This suggests incorporating
diversity into the retrieval stage with a mixture
of retrievers can enhance diverse commonsense
reasoning capabilities.
•For the retrieval stage, the proposed regulariza-
tion term significantly boosts diversity. Without
the regularization term in the loss function during
the training process, the Self-BLEU-4 score in-
creases from 24.85to29.40. This demonstrates
that our proposed regularization term helps the
retriever balance the distribution of different mod-
els, which in turn improves the diversity of the
retrieved contexts and generations.
•For the generation stage, the proposed match-
ing algorithm greatly enhances both quality and
diversity. Specifically, our proposed EM-basedmatching algorithm for matching retrieved con-
texts to target output gains over 2%on the BLEU-
4score and 6%on the distinct-4 score com-
pared to random selection. This indicates that
the matching algorithm can effectively assign ap-
propriate contexts to generations, improving the
quality and diversity of the generations.
3.5 Human Evaluation
In order to understand the effectiveness of our pro-
posed MoREE method, we conduct the human
evaluation by asking three annotators to assign
grades (up to 5) of the generated relationship sen-
tences. We randomly sample 100examples from
the test set of the DimonGen dataset and compare
our method with the typical sampling and MoKGE
methods. Following Yu et al. (2022), we design
three evaluation dimensions: quality, diversity, and
grammar & fluency (gra & flu).
The human evaluation results in Tab. 4 shows
that the DimonGen dataset receives high scores for
quality and diversity, indicating that the majority of
examples in the dataset are well-written and diverse.
Our proposed MoREE method outperforms the two
baseline methods in terms of quality and diversity
and achieves similar scores for grammar and flu-
ency. This demonstrates that our method is able
to effectively capture the complex relationships be-
tween concepts in real-world scenarios while also
generating a variety of unique and accurate rela-
tionship sentences.
3.6 Case study
Table 5 illustrates some generation examples for in-
put concept pairs {“dog”, “sheep” }and{“airport”,
“way”}with different methods, which shows that:
•For the input pair “dog” and “sheep”, the genera-
tions produced by the baseline methods contain
some unreasonable outputs, such as “Sheep and
dogs are grazing in a meadow.” In contrast, our
proposed MoREE method generates more rea-
sonable and diverse outputs, such as “The dog is4725
heading sheep with a farmer nearby” and “A dog
is chasing a flock of sheep.”
•For the input pair “airport” and “way”, the base-
line methods tend to generate plain and repeti-
tive outputs, such as “Passengers make their way
through the airport.” In contrast, our proposed
MoREE method can accurately capture the re-
lationships between concepts, for example, “A
plane is on its way to the airport.”
4 Related Work
Generative relational reasoning attempts to
generate a coherent sentence involving a pair or
a set of concepts/entities (Lin et al., 2020; Huang
et al., 2022a,c; Huang and Chang, 2022b). For in-
stance, Lin et al. (2020) introduce CommonGen ,
which aims to generate a coherent sentence that de-
scribes an everyday scenario involving a given set
of common concepts. Huang et al. (2022a) propose
Open Relation Modeling , which aims to generate
an informative sentence describing relationships
between concepts. However, these methods do not
consider the diversity of possible relationships that
can exist between concepts, leading to a limited un-
derstanding of relationships between the concepts.
Incorporating diversity at inference phrase is
achieved by sampling methods. Instead of selecting
the next token based on maximum likelihood (Fre-
itag and Al-Onaizan, 2017), tokens are sampled
from a probability distribution of the vocabulary.
For example, Fan et al. (2018) reduces the sampling
pool by keeping only the top-k candidates for each
token in the generation. Holtzman et al. (2020) lim-
its the next-token sampling pool by a threshold of
the probability mass. Meister et al. (2022) restrict
the generated words to expected information con-
tent by shifting the truncation set with a conditionalentropy of prior content. While these methods re-
duce the training effort for neural models, they are
criticized for the low quality of generations (Zhang
et al., 2021).
Incorporating diversity at the training phase is
achieved through diverse model structures. Specifi-
cally, Zhao et al. (2017) propose a conditional varia-
tional autoencoder-based framework to embed each
input into a latent distribution. Cao and Wan (2020)
construct their model based on a conditional gener-
ative adversarial network with a diversity loss term.
Shen et al. (2019) and Cho et al. (2019) utilize a
mixture of experts model to encourage diverse out-
puts from different expert models. Among previous
works, Yu et al. (2022)’s method is most similar
to ours. They propose to first extract common-
sense knowledge from external knowledge graphs
and then use an MoE model to generate diverse
outputs. While this work considers incorporating
external knowledge to improve generation quality,
it falls short in increasing diversity due to the naive
retriever shared among all the generators.
5 Conclusion
While previous approaches have used generated
sentences to model concept/concept relationships,
these methods often rely on a single sentence and
can be insufficient or biased in conveying the com-
plexity of these relationships. To address this issue,
we propose DimonGen, a task for generating di-
verse sentences that describe concept relationships
in various everyday scenarios. To solve the pro-
posed task, we design a two-stage model called
MoREE, which combines a mixture of retriever
and generator models. Our experimental results
demonstrate the effectiveness of MoREE in gen-
erating coherent and diverse sentences to describe
concept relationships in everyday scenarios.4726Limitations
Our proposed DimonGen task involves generating
several diverse sentences to describe the relation-
ships between concepts. However, it does not take
into account the number of relationships between
different concept pairs. This can lead to problems
when applying the model trained on the DimonGen
dataset to other unseen concept pairs. For exam-
ple, some concepts may have a small number of
relationships, and asking the model to generate a
greater number of diverse relationships may lead to
hallucinations which can be misleading when us-
ing the generative model for educational purposes.
We leave this as a future work for the research
community.
Additionally, the performance of the MoREE
model is heavily dependent on the quality of the
external corpora used in the retrieval stage. If the
corpora do not contain any relevant information
for the input concepts, the MoREE model will per-
form similarly to a vanilla MoE model. An alter-
native approach is to retrieve information from the
Web (Huang et al., 2022b; Lazaridou et al., 2022).
Last, it should be noted that the base models used
in this study were relatively small. Recent studies
have demonstrated that large language models pos-
sess superior reasoning abilities compared to their
smaller counterparts (Wei et al., 2022; Huang and
Chang, 2022a). Future work on exploring the diver-
sified generative commonsense reasoning ability of
large language models is encouraged.
Acknowledgements
This material is based upon work supported by
the National Science Foundation IIS 16-19302 and
IIS 16-33755, Zhejiang University ZJU Research
083650, IBM-Illinois Center for Cognitive Com-
puting Systems Research (C3SR) and IBM-Illinois
Discovery Accelerator Institute (IIDAI), gift grants
from eBay and Microsoft Azure, UIUC OVCR
CCIL Planning Grant 434S34, UIUC CSBS Small
Grant 434C8U, and UIUC New Frontiers Initiative.
Any opinions, findings, and conclusions or recom-
mendations expressed in this publication are those
of the author(s) and do not necessarily reflect the
views of the funding agencies.
References472747284729ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
In Section 6
/squareA2. Did you discuss any potential risks of your work?
In Section 6
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
In Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
In Section 2 and Section 3
/squareB1. Did you cite the creators of artifacts you used?
In Section 1, Section 2, and Section 3
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
In Section 1, Section 3, and the submitted code folder
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
In Section 1, Section 3, and the submitted code folder
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
In Section 1, Section 3, and the submitted code folder
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
In Section 1, Section 3, and the submitted code folder
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 3, and the submitted code folder
C/squareDid you run computational experiments?
Section 3
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 34730/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Section 3
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 3, and the submitted code folder
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Section 3, and the submitted code folder
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Section 3, and the submitted code folder
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Section 34731