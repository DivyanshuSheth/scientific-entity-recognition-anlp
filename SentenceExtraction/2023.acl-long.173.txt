
Weiqi Wu, Chengyue Jiang, Yong Jiang, Pengjun Xie, Kewei TuSchool of Information Science and Technology, ShanghaiTech UniversityShanghai Engineering Research Center of Intelligent Vision and ImagingDAMO Academy, Alibaba Group, China
{wuwq,jiangchy,tukw}@shanghaitech.edu.cn
{yongjiang.jy,chengchen.xpj}@alibaba-inc.com
Abstract
Ontological knowledge, which comprises
classes and properties and their relationships, is
integral to world knowledge. It is significant to
explore whether Pretrained Language Models
(PLMs) know and understand such knowledge.
However, existing PLM-probing studies focus
mainly on factual knowledge, lacking a system-
atic probing of ontological knowledge. In this
paper, we focus on probing whether PLMs store
ontological knowledge and have a semantic un-
derstanding of the knowledge rather than rote
memorization of the surface form. To probe
whether PLMs know ontological knowledge,
we investigate how well PLMs memorize: (1)
types of entities; (2) hierarchical relationships
among classes and properties, e.g., Person is a
subclass of Animal andMember of Sports Team
is a subproperty of Member of ; (3) domain and
range constraints of properties, e.g., the subject
ofMember of Sports Team should be a Person
and the object should be a Sports Team . To
further probe whether PLMs truly understand
ontological knowledge beyond memorization,
we comprehensively study whether they can
reliably perform logical reasoning with given
knowledge according to ontological entailment
rules. Our probing results show that PLMs can
memorize certain ontological knowledge and
utilize implicit knowledge in reasoning. How-
ever, both the memorizing and reasoning per-
formances are less than perfect, indicating in-
complete knowledge and understanding.
1 Introduction
Pretrained Language Models (PLMs) have or-
chestrated impressive progress in NLP across
a wide variety of downstream tasks, including
knowledge-intensive tasks. Previous works pro-
pose that PLMs are capable of encoding a signif-
icant amount of knowledge from the pretraining
corpora (AlKhamissi et al., 2022), and determine
to explore the kinds of knowledge within PLMs.Figure 1: (a) An example of an ontological knowledge
graph. (b) Potential manual and soft prompts to probe
the knowledge and corresponding semantics. Instances
are replaced by pseudowords in reasoning experiments
to mitigate potential interference from model memory.
Existing probing works mainly focus on factual
knowledge associated with instances (Petroni et al.,
2019; Jiang et al., 2020; Safavi and Koutra, 2021).
Meanwhile, although classes (concepts) have raised
some research interest (Bhatia and Richie, 2020;
Peng et al., 2022; Lin and Ng, 2022), there is no
systematic study of ontological knowledge.
Ontological knowledge models the world with a
set of classes and properties and the relationships
that hold between them (Nilsson, 2006; Kumar
et al., 2019). It plays a vital role in many NLP
tasks such as question answering by being injected
into (Goodwin and Demner-Fushman, 2020) or em-
bedded outside deep neural networks (Wang et al.,30802017). Therefore, it is essential to explore whether
PLMs can encode ontological knowledge and have
a semantic understanding of the knowledge rather
than rote memorizing its surface form.
In this paper, we first probe PLM’s memorization
of ontological knowledge. Specifically, as shown in
Figure 1(a), we construct memorization tests about
(1) Types of entities. Entities can be categorized
into classes, as Lionel Messi is a Person and Ar-
gentina National Football Team is a Sports Team .
(2) Hierarchical relationships between classes, e.g.,
Person is a subclass of Animal . (3) Hierarchical
relationships between properties, e.g., Member of
Sports Team is a subproperty of Member of . (4)
Domain constraints of properties. It specifies in-
formation about the subjects to which a property
applies. For example, the subject of Member of
Sports Team should be an instance of Person . (5)
Range constraints of properties. Similar to domain,
range specifies information about the object of a
property, such as the object of Member of Sports
Team should be an instance of Sports Team . Exper-
iments prove that PLMs store a certain amount of
ontological knowledge.
To further examine whether PLMs understand
ontological knowledge, we investigate if PLMs can
correctly perform logical reasoning that requires
ontological knowledge. Illustrated in Figure 1(b),
given the fact triple (Lionel Messi, Member of
Sports Team , Argentina National Football Team)
along with property constraints, we can perform
type inferences to conclude that Lionel Messi is
aPerson , and Argentina National Football Team
is aSports Team . We comprehensively investigate
the reasoning capability of PLMs over ontological
knowledge following six entailment rules. Exper-
iments show that PLMs can apply implicit onto-
logical knowledge to draw conclusions through
reasoning, but the accuracy of their reasoning falls
short of perfection. This observation suggests that
PLMs possess a limited understanding of ontologi-
cal knowledge.
In summary, we systematically probe whether
PLMs know and understand ontological knowledge.
Our main contributions can be summarized as fol-
lows: (1) We construct a dataset that evaluates the
ability of PLMs to memorize ontological knowl-
edge and their capacity to draw inferences based
on ontological entailment rules. (2) We compre-
hensively probe the reasoning ability of PLMs by
carefully classifying how ontological knowledgeis given as a premise. (3) We find that PLMs can
memorize certain ontological knowledge but have
a limited understanding. We anticipate that our
work will facilitate more in-depth research on onto-
logical knowledge probing with PLMs. The code
and dataset are released at https://github.com/
vickywu1022/OntoProbe-PLMs .
2 Benchmark Construction
In this section, we present our methodology for
ontology construction and the process of generat-
ing memorizing and reasoning tasks based on the
ontology for our probing analysis.
2.1 Ontology Building
Class We use DBpedia (Auer et al., 2007) to
obtain classes and their instances. Specifically,
we first retrieve all 783 classes in DBpedia, then
use SPARQL (hommeaux, 2011) to query their in-
stances using the type relation and superclasses
using the subclass-of relation. We sample 20
instances for each class.
Property Properties are collected based on DB-
pedia and Wikidata (Vrande ˇci´c and Krötzsch, 2014)
using the following pipeline: (1) Obtain properties
from Wikidata and use subproperty of (P1647) in
Wikidata to find their superproperties. (2) Query
the domain and range constraints of the properties
using property constraint (P2302) in Wikidata. (3)
Align the Wikidata properties with DBpedia proper-
ties by equivalent property (P1628) . (4) Query the
domain and range constraints of the properties in
DBpedia. (5) Cleanse the collected constraints us-
ing the above-collected class set as vocabulary. We
choose 50 properties with sensible domain, range
and superproperties.
2.2 Construction of Memorizing Task
The memorizing task consists of five subtasks, each
probing the memorization of an ontological rela-
tionship: (1) TP: types of a given instance, (2)
SCO : superclasses of a given class, (3) SPO : su-
perproperties of a given property, (4) DM: domain
constraint on a given property, and (5) RG: range
constraint on a given property. Every subtask is for-
mulated as a cloze-completion problem, as shown
in Figure 1(b). Multiple correct answers exist for
TP, SCO, and SPO, which form a chain of classes
or properties. There is only one correct answer
for DM and RG, as it is not sound to declare an
expanded restriction on a property. For instance,3081
Animal is too broad as the domain constraint of
the property Member of Sports Team (P54) , hence
applying Person as the domain.
We construct the dataset for each subtask using
the ontology built in Sec. 2.1 and reserve 10 sam-
ples for training and 10 for validation to facilitate
few-shot knowledge probing. The statistics of the
dataset for each subtask are shown in Table 1.
2.3 Construction of Reasoning Task
We construct the reasoning task based on the entail-
ment rules specified in the Resource Description
Framework Schema (RDFS). We propose six sub-
tasks, each probing the reasoning ability following
a rule listed in Table 2. For rule rdfs2/3/7, we de-
sign a pattern for each property to be used between
a pair of instances, e.g., "[X] is a player at [Y] ."
forMember of Sports Team , where [X] and [Y] are
the subject and object, respectively.
Each entailment rule describes a reasoning pro-
cess:P∧P|=H, where P,Pare the premisesandHis the hypothesis. Similar to the memo-
rizing task, we formulate the reasoning task as
cloze-completion by masking the hypothesis (see
Figure 1(b)). Premises are also essential to the
reasoning process and can be:
•Explicitly Given : The premise is explicitly in-
cluded in the input of the model, and inferences
are made with natural language statements.
•Implicitly Given : The premise is not explicitly
given but memorized by the model as implicit
knowledge. The model needs to utilize implicit
knowledge to perform inferences, which relieves
the effect of context and requires understanding
the knowledge.
•Not Given : The premise is neither explicitly
given nor memorized by the model. It serves as
a baseline where the model makes no inference.
Hence, there exist 3×3different setups for two
premises. It is a refinement of the experimental
setup used by Talmor et al. (2020), which only dis-
tinguishes whether a premise is explicitly included
in the input. We determine the memorization of a
premise by the probing results of the memorizing
task, which will be elaborated in Sec. 3.2.3.
3 Probing Methods
We investigate encoder-based PLMs (BERT (De-
vlin et al., 2019) and RoBERTa (Liu et al., 2019))
that can be utilized as input encoders for various
NLP tasks. Prompt is an intuitive method of our
probing task as it matches the mask-filling nature3082
of BERT. We use OpenPrompt (Ding et al., 2022),
an open-source framework for prompt learning that
includes the mainstream prompt methods, to facili-
tate the experiments.
3.1 Probing Methods for Memorization
3.1.1 Prompt Templates
Manual Templates Manual prompts with
human-designed templates written in discrete
language phrases are widely used in zero-shot
probing (Schick and Schütze, 2021) as PLMs
can perform tasks without any training. Manual
templates are designed for all the ontological
relationships in our task, as shown in Table 3.
Soft Templates One of the disadvantages of man-
ual prompts is that the performance can be signifi-
cantly affected by perturbation to the prompt tem-
plates (Jiang et al., 2020). A common alternative is
to use soft prompts that consist of learnable soft to-
kens (Liu et al., 2021; Li and Liang, 2021) instead
of manually defined templates. The soft prompts
we use for ontological relationships are also shown
in Table 3. To probe using soft prompts, we tune
randomly initialized soft tokens on the training set
with the PLMs parameters being frozen. Detailed
training setups are listed in Appendix A.
3.1.2 Candidates Scoring
Given a candidate cwhich can be tokenized into
ntokens c, c, . . . , c, such that c∈V, i=
{1, . . . , n }, n≥1, where Vis the vocabulary of
the model, it is scored based on the log probabil-
ity of predicting it in the masked prompt. We can
either use ndifferent [MASK] tokens or the same
[MASK] token to obtain the log probability of
each composing token c, and then compute thelog probability of the candidate c. For simplicity,
we use a single [MASK] token when illustrating
our prompts.
Multiple Masks For a candidate cconsisting
ofntokens, we use n[MASK] tokens in the
masked input, with the ith [MASK] token denoted
as[MASK ]. The candidate probability can be
computed by three different pooling methods: (1)
mean : the average of log probabilities of compos-
ing tokens (Klein and Nabi, 2020), (2) max: the
maximum log probability of all composing tokens,
(3)first: the log probability of the first compos-
ing token. Formally, the score sof candidate cis
computed as:
ˆs= log ( p([MASK ]=c))
s=Pooling (ˆs,ˆs, . . . , ˆs)
Single Mask We use one single [MASK] token
to obtain an independent prediction of each token.
The log probability of each composing token c
equals the log probability of recovering cin the
same [MASK], and the candidate is scored with
the proposed pooling methods.
ˆs= log ( p([MASK ] =c))
3.1.3 Metrics
We rank the candidates by their log probability
scores and use the top K Recall (R@K) and Mean
Reciprocal Rank (MRR) as our evaluation metrics.
Since MRR only evaluates the ability to retrieve the
first ground truth, we additionally take the average
rank of all gold labels as the final rank when com-
puting mean reciprocal rank to evaluate models’
ability to retrieve all the ground truths and denote3083it as MRR. Formally, MRRis defined as:
MRR=1
n/summationdisplay1/(1
|G|/summationdisplayrank(g))
where nis the number of samples in the dataset
andGis the gold label set of the ith sample.
3.2 Probing Methods for Reasoning
We explain how we concatenate the premises and
hypothesis in the textual input, exclude the models’
memory of hypotheses and split a set of premises
based on how well the knowledge they represent
is memorized by the model. We follow the candi-
date scoring methods proposed in Sec. 3.1.2 and
evaluation metrics in Sec. 3.1.3.
3.2.1 Prompt Templates
Apart from the prompt templates for our concerned
ontological relationships introduced in Sec. 3.1.1,
we further add conjunction tokens between the
premises and hypothesis, which can be either man-
ually designed or automatically tuned.
Manual Conj. As in Figure 1(b), we use a con-
junctive adverb therefore between the premises and
hypothesis. It is kept when there is no premise
explicitly given in the input to exclude the effect
of the template on probing results under different
premise settings.
Soft Conj. We can also use soft conjunctions
by adding a soft token between premises explic-
itly given in the input and a soft token between
the premises and the hypothesis. Therefore, the
input would be " P<s4>P<s5>H". The soft
templates used in P,PandHare loaded from
the learned soft prompts in memorizing tasks and
finetuned together with soft conjunctions.
3.2.2 Reasoning with Pseudowords
When testing the reasoning ability of PLMs, we
replace the specific instances, classes, and proper-
ties in the hypothesis prompt with pseudowords to
prevent probing the memorization of hypotheses.
Pseudowords (Schütze, 1998; Zhang and Pei, 2022;
Goodwin et al., 2020) are artificially constructed
words without any specific lexical meaning. For
example, the reasoning prompt for the transitivity
of subclass (i.e., rule rdfs9) is "[X] is a person. Per-
son is an animal. Therefore, [X] is a particular
[MASK] .", where [X] is a pseudoword.
Inspired by (Karidi et al., 2021), we obtain pseu-
dowords for PLMs by creating embeddings withoutspecial semantics. Specifically, we sample embed-
dings at a given distance from the [MASK] token,
as the [MASK] token can be used to predict all
the words in the vocabulary and appear anywhere
in the sentence. The sampling distance dis set
to be smaller than the minimum L2 distance be-
tween [MASK] and any other tokens in the static
embedding space. Formally:
d=α·min∥z−z∥
where zis the static embedding of token tand
α∈(0,1)is a coefficient. Moreover, we require
that the distance between two pseudowords is at
least the sampling distance dto ensure they can be
distinguished from each other.
3.2.3 Classifying Premises: Memorized or not
To determine whether a premise is memorized by
the model when it is not explicitly given in the
input, we employ a classifying method based on
the rank of the correct answer in the memorizing
task to sort and divide the premise set. The first
half of the premise set is regarded as memorized,
and the second half is not.
Each rule consists of two premises and we clas-
sify them separately. For P, which involves knowl-
edge of subclass, subproperty, domain or range
tested in the memorizing task, we can leverage pre-
viously calculated reciprocal rank during the evalu-
ation. Premises are then sorted in descending order
by the reciprocal rank. We conduct the same tests
onP, which involves knowledge of pseudowords,
to examine model predispositions towards specific
predictions and classify whether Pis memorized
or not. Finally, we form our test set by combining
premises according to the entailment rule and how
each premise is given.
4 Results and Findings
In this section, we introduce the performance of
PLMson the test sets of memorizing and reason-
ing tasks, and analyze the results to posit a series
of findings. We then analyze the effectiveness of
different prompts. Detailed experimental results
can be found in Appendix C.
4.1 Memorizing Task
The baseline model used for the memorizing task
is a frequency-based model which predicts a list3084
of gold labels in the training set based on the fre-
quency at which they appear, followed by a random
list of candidates that are not gold labels in the train-
ing set. It combines prior knowledge and random
guesses and is stronger than a random baseline.
The experimental results of the memorizing task
are summarized in Table 4, from which we can
observe that: (1) The best performance of PLMs
is better than the baseline on every task except for
DM. On DM, the baseline achieves higher MRR.
If taking all three metrics into account, the best
performance of PLMs still surpasses the perfor-
mance of the baseline. (2) Except for DM, BERT
models achieve much better performance than the
baseline in all subtasks and all metrics. Taking an
average of the increase in each metric, they out-
perform the baseline by 43–198%. Only BERT-
base-uncased and BERT-large-cased outperform
the baseline in DM by a small margin of 1% and
7%. (3) RoBERTa models generally fall behind
BERT, showing a 38–134% improvement com-
pared with the baseline except for DM. (4) Despite
a significant improvement from the baseline, the
results are still not perfect in all subtasks.
PLMs can memorize certain ontological knowl-
edge but not perfectly. Based on the above ob-
servation, we can conclude that PLMs have a cer-
tain memory of the concerned ontological rela-tionships and the knowledge can be accessed via
prompt, allowing them to outperform a strong base-
line. It proves that during pretraining, language
models learn not only facts about entities but also
their ontological relationships, which is essential
for a better organization of world knowledge. How-
ever, the memorization is not perfect, urging further
efforts on ontology-aware pretraining.
Large models are not necessarily better at
memorizing ontological knowledge. According
to Petroni et al. (2019), models with larger sizes
appear to store more knowledge and achieve better
performance in both knowledge probing tasks and
downstream NLP tasks. However, as shown in Ta-
ble 4, BERT-large-uncased is worse than its smaller
variant under most circumstances, and RoBERTa-
large is worse than RoBERTa-base in TP and DM.
It demonstrates that the scale of model parame-
ters does not necessarily determine the storage of
ontological knowledge.
4.2 Reasoning Task
We fix the usage of multiple masks and mean-
pooling in the reasoning experiments as they gen-
erally outperform other settings in the memorizing
task (see Appendix B). We take an average of the
MRR metrics using different templates and illus-
trate the results of BERT-base-cased and RoBERTa-3085
base in Figure 2. With neither premise given,
the rank of the ground truth is usually low. It
shows that models have little idea of the hypothe-
sis, which is reasonable because the information of
pseudowords is probed. With premises implicitly
or explicitly given, especially P, the MRR met-
rics improve in varying degrees. Moreover, results
show that BERT-base-cased has better reasoning
ability with our concerned ontological entailment
rules than RoBERTa-base.
PLMs have a limited understanding of the
semantics behind ontological knowledge. To
reach a more general conclusion, we illustrate the
overall reasoning performance in Figure 3 by av-
eraging over all the entailment rules and PLMs,
and find that: (1) When Pis explicitly given in
the input text, models are able to significantly im-
prove the rank of gold labels. As Pcontains the
ground truth in its context, it raises doubt about
whether the improvement is obtained through logi-
cal reasoning or just priming (Misra et al., 2020).
(2) Explicitly giving Pintroduces additional to-
kens that may not be present in gold labels, making
P/P=EX/EXworse than P/P=EX/IM.
(3) When premises are implicitly given, the MRR
metrics are higher than when they are not given. It
implies that, to some extent, PLMs can utilize the
implicit ontological knowledge and select the cor-
rect entailment rule to make inferences. (4) How-
ever, none of the premises combinations can give
near-perfect reasoning performance (MRR metrics
close to 1), suggesting that PLMs only have a weak
understanding of ontological knowledge.
Paraphrased properties are a challenge for lan-
guage models. In Figure 2(d), the premise Pof
rule rdfs7 contains a paraphrased version of the3086ground truth, which is the manually-designed pat-
tern of a particular property. Compared with rule
rdfs5 shown in Figure 2(c), where Pcontains the
surface form of the correct property, the MRR of
BERT-base-cased of rdfs7 decreases by 23%, 49%
and 29% when Pis explicitly given and Pis
not, implicitly and explicitly given, respectively.
Though the MRR of RoBERTa-base of rdfs7 in-
creases when Pis not given, it decreases by 40%
and 15% when Pis implicitly and explicitly given.
This suggests that PLMs fail to understand the se-
mantics of some properties, thus demonstrating a
limited understanding of ontological knowledge.
4.3 Effectiveness of Prompts
In this section, we discuss how prompt templates af-
fect performance. In the memorizing task, Table 4
shows that using soft templates generally improves
the performance of memorizing tasks, in particular
TP, SCO and SPO. It suggests that it is non-trivial
to extract knowledge from PLMs.
Meanwhile, only a few models perform better
with soft templates on DM and RG with a relatively
marginal improvement. This could be explained
by the fact that both the manual templates and se-
mantics of domain and range constraints are more
complex than those of other relationships. There-
fore, it is difficult for models to capture with only
three soft tokens. We also note that RoBERTa mod-
els appear to benefit more from soft templates than
BERT models, probably due to their poor perfor-
mance with manual templates.
Trained soft templates for each relation barely
help with reasoning, though. In Figure 4, we sum-
marize the performance by averaging across dif-
ferent models and reasoning tasks and find that it
is the trained conjunction token which improves
the performance of reasoning rather than the soft
templates that describe ontological relationships. It
might be inspiring that natural language inference
with PLMs can be improved by adding trainable
tokens as conjunctions instead of simply concate-
nating all the premises.
5 Preliminary Evaluation of ChatGPT
After we finished the majority of our probing exper-
iments, ChatGPT, a decoder-only model, was pub-
licly released and demonstrated remarkable capa-
bilities in commonsense knowledge and reasoning.
Therefore, we additionally perform a preliminary
probe of the ability of ChatGPT to memorize and
understand ontological knowledge.
Since ChatGPT is a decoder-only model, we
employ a distinct probing method from what is
expounded in Sec. 3. Instead of filling masks, we
directly ask ChatGPT to answer multiple-choice
questions with 20 candidate choices and evaluate
the accuracy.
5.1 Probing for Memorization Ability
For memorization probing, we use the finest-
grained gold label as the correct answer and ran-
domly sample 19 negative candidates to form the
choice set. Take the TP task as an example, we
query the GPT-3.5-turbo API with the prompt
"What is the type of Lionel Messi? (a) soccer
player, (b) work, (c) ..." followed by remaining
candidates. We sample 500 test cases for the TP
and SCO tasks and use the complete test sets for
the other tasks.
For comparison, we also conduct the experi-
ments using BERT-base-uncased, a generally com-
petitive PLM in memorizing and understanding on-
tological knowledge, with manual prompts and the
identical candidate subset. The results presented in
Table 5 indicate that ChatGPT outperforms BERT-3087
base-uncased significantly in most of the memoriz-
ing tasks associated with ontological knowledge.
5.2 Probing for Reasoning Ability
Since we cannot input embeddings in the GPT-
3.5-turbo API, we use XandYto represent pseu-
dowords as they are single letters that do not convey
meanings. However, ChatGPT cannot generate any
valid prediction without sufficient context regard-
ing these pseudowords. Therefore, Pneeds to be
explicitly provided to describe the characteristics
or relations of the pseudowords. We then explore
the ability of ChatGPT to select the correct an-
swer from 20 candidates with different forms of
P. In this task, Pis regarded as memorized if the
model can correctly choose the gold answer from
the given 20 candidates in the memorizing task.
Based on the results presented in Table 6, Chat-
GPT demonstrates high accuracy when Pis ei-
ther implicitly or explicitly given, suggesting its
strong capacity to reason and understand ontolog-
ical knowledge. Due to a substantial disparity in
the knowledge memorized by ChatGPT compared
to other models (as shown in section 5.1), their
performance is not directly comparable when Pis
not given or implicitly given. Therefore, we only
compare ChatGPT and BERT-base-uncased when
Pis explicitly given. Results show that ChatGPT
significantly outperforms BERT-base-uncased in
explicit reasoning (97.1% vs. 88.2%).
6 Related Work
Knowledge Probing Language models are
shown to encode a wide variety of knowledge af-
ter being pretrained on a large-scale corpus. Re-
cent studies probe PLMs for linguistic knowl-
edge (Vuli ´c et al., 2020; Hewitt and Manning,
2019), world knowledge (Petroni et al., 2019; Jiang
et al., 2020; Safavi and Koutra, 2021), action-
able knowledge (Huang et al., 2022), etc. via
methods such as cloze prompts (Beloucif and Bie-
mann, 2021; Petroni et al., 2020) and linear clas-
sifiers (Hewitt and Liang, 2019; Pimentel et al.,2020). Although having explored extensive knowl-
edge within PLMs, previous knowledge probing
works have not studied ontological knowledge sys-
tematically. We cut through this gap to investigate
how well PLMs know about ontological knowledge
and the meaning behind the surface form.
Knowledge Reasoning Reasoning is the process
of drawing new conclusions through the use of
existing knowledge and rules. Progress has been
reported in using PLMs to perform reasoning tasks,
including arithmetic (Wang et al., 2022; Wei et al.,
2022), commonsense (Talmor et al., 2019, 2020;
Wei et al., 2022), logical (Creswell et al., 2022)
and symbolic reasoning (Wei et al., 2022). These
abilities can be unlocked by finetuning a classifier
on downstream datasets (Talmor et al., 2020) or
using proper prompting strategies (e.g., chain of
thought (CoT) prompting (Wei et al., 2022) and
generated knowledge prompting (Liu et al., 2022)).
This suggests that despite their insensitivity to nega-
tion (Ettinger, 2020; Kassner and Schütze, 2020)
and over-sensitivity to lexicon cues like priming
words (Helwe et al., 2021; Misra et al., 2020),
PLMs have the potential to make inferences over
implicit knowledge and explicit natural language
statements. In this work, we investigate the ability
of PLMs to perform logical reasoning with implicit
ontological knowledge to examine whether they
understand the semantics beyond memorization.
7 Conclusion
In this work, we systematically probe whether
PLMs encode ontological knowledge and under-
stand its semantics beyond the surface form. Ex-
periments show that PLMs can memorize some
ontological knowledge and make inferences based
on implicit knowledge following ontological entail-
ment rules, suggesting that PLMs possess a certain
level of awareness and understanding of ontologi-
cal knowledge. However, it is important to note that
both the accuracy of memorizing and reasoning is
less than perfect, and the difficulty encountered by
PLMs when processing paraphrased knowledge is
confirmed. These observations indicate that their
knowledge and understanding of ontology are lim-
ited. Therefore, enhancing the knowledge and un-
derstanding of ontology would be a worthy future
research goal for language models. Our exploration
into ChatGPT shows an improved performance in
both memorizing and reasoning tasks, signifying
the potential for further advancements.3088Limitations
The purpose of our work is to evaluate the ontolog-
ical knowledge of PLMs. However, a sea of classes
and properties exist in the real world and we only
cover a selective part of them. Consequently, the
scope of our dataset for the experimental analysis is
limited. The findings from our experiments demon-
strate an imperfect knowledge and understanding
obtained by the models, indicating a tangible room
for enhancement in both ontological knowledge
memorization and understanding and a need for a
better ability to address paraphrasing. These ob-
servations lead us to contemplate refining the exist-
ing pretraining methods to help language models
achieve better performance in related tasks.
Ethics Statement
We propose our ethics statement of the work in this
section: (1) Dataset. Our data is obtained from DB-
pedia and Wikidata, two publicly available linked
open data projects related to Wikipedia. Wikidata
is under the Creative Commons CC0 License, and
DBpedia is licensed under the terms of the Cre-
ative Commons Attribution-ShareAlike 3.0 license
and the GNU Free Documentation License. We
believe the privacy policies of DBpediaand Wiki-
dataare well carried out. We inspect whether our
dataset, especially instances collected, contains any
unethical content. No private information or of-
fensive topics are found during human inspection.
(2) Labor considerations. During dataset construc-
tion, the authors voluntarily undertake works re-
quiring human efforts, including data collection,
cleansing, revision and design of property patterns.
All the participants are well informed about how
the dataset will be processed, used and released.
(3) Probing results. As PLMs are pretrained on
large corpora, they may give biased results when
being probed. We randomly check some probing
results and find no unethical content in these sam-
ples. Therefore, we believe that our study does not
introduce additional risks.
Acknowledgement
This work was supported by the National Natu-
ral Science Foundation of China (61976139) and
by Alibaba Group through Alibaba Innovative Re-
search Program.References30893090
A Experimental Setup
We train soft tokens for 100 epochs with AdamW
optimizer. The learning rate is set to 0.5 and a linear
warmup scheduler is used. Since both the mem-
orizing and reasoning task can be formulated as
a multi-label classification problem, we use BCE-
WithLogitsLoss or NLLLoss as our loss function
in the memorizing task to report the better results
given by one of these two and select a better train-
ing objective. Therefore, we fix the loss function
to BCEWithLogitsLoss in the reasoning task.
For pseudowords, we set the coefficient αto 0.5
and sample 10 pairs of pseudowords for each entail-
ment rule as we at most need two pseudowords to
substitute the subject and object instances respec-
tively, and report the averaged performance as the
final result.
B Multi-token Prompting Methods
In the main body of the paper, we discuss the im-
pact of different prompts on the performance of
knowledge probing and reasoning. In this section,
we continuously discuss the impact of other prompt
settings by comparing the averaged performance.
B.1 Number of [MASK] Tokens
To support multi-token candidate scoring, we use
multiple [MASK] tokens or one single [MASK] to-
ken to predict with masked language models. The
comparison between the two methods is shown in
Figure 5, by averaging the performance of all the
memorizing tasks and models. We can observe
that single [MASK] prediction achieves better ac-
curacy (R@1) with a negligible tiny margin but
worse performance in other metrics. Therefore,
using multiple [MASK] tokens to obtain predic-
tion by forward pass inference is more sensible and
achieves better results.3091
B.2 Pooling Methods
Three pooling methods are proposed when comput-
ing the probability of a candidate that can be tok-
enized into multiple subtokens. The mean-pooling
method is usually used in multi-token probing.
Furthermore, we introduce max-pooling and first-
pooling, which retain the score of only one impor-
tant token. They can exclude the influence of prepo-
sitions, e.g., by attending to mean ortransportation
when scoring the candidate mean of transportation ,
but at the cost of other useful information. We are
interested in whether it is better to consider the
whole word or focus on the important part.
Figure 6 shows that mean-pooling, as a classical
method, is much better than the other two pool-
ing methods. Besides, first-pooling gives clearly
better results than max-pooling, which is possi-
bly caused by the unique information contained
in the headword (usually the first token). Con-
sider candidates volleyball player ,squash player
andgolf player , the conditional log probability of
token player might be higher, but the candidates
are distinguished by their headwords. In summary,
mean-pooling obtains the best results with the most
comprehensive information.
B.3 Loss Functions
As mentioned in Appendix A, we try two loss func-
tions in the memorizing task. (1) The Binary Cross
Entropy With Logits Loss (BCEWithLogitsLoss) is
a common loss function for multi-label classifica-
tion which numerically stably combines a Sigmoid
layer and the Binary Cross Entropy Loss into one
layer. All examples are given the same weight
when calculating the loss. (2) The Negative Log
Likelihood Loss (NLLLoss) is a loss function for
multi-class classification. However, we can convert
the original multi-label problem to a multi-class
one by sampling one ground truth at a time to gen-
erate multiple single-label multi-class classification
cases. As can be seen from Figure 7, using BCE-
WithLogitsLoss as the loss function achieves better
results than using NLLLoss. Hence, in subsequent
reasoning experiments, we stick to the classical
loss for multi-label classification.
C Experimental Results
C.1 Task Examples
In order to enhance the clarity of the experiments,
we have compiled a list in Table 7 that includes task3092
prompts as well as the top five predicted candidate
words generated by BERT-base-cased. The table
consists of examples with successful predictions
for all correct answers (SPO, RG), examples with
partial correct answers predicted (TP, SCO), and
examples where the correct answer is not predicted
within the top five candidates (DM).
C.2 Memorizing Results
The complete results of the memorizing task are
reported in Table 8, 9, 10, 11 and 12.
C.3 Reasoning Results
We report the MRR Metric of BERT-base-
uncased, BERT-large-cased, BERT-large-uncased
and RoBERTa-large in Figure 8. It is generally con-
sistent with the two models reported in the main
body of the paper and the macro-averaged perfor-
mance across different PLMs, so consistent conclu-
sions can be drawn.3093309430953096309730983099ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitation section
/squareA2. Did you discuss any potential risks of your work?
Ethical section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
2
/squareB1. Did you cite the creators of artifacts you used?
2
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Ethical section
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
2
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Ethical section
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
2
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
2
C/squareDid you run computational experiments?
4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
We focus on investigating whether PLMs know and understand ontological knowledge using models
from the huggingface. We do not pay extra attention to the computational budget or computing
infrastructure.3100/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix A
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
2
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Ethical section
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Ethical section
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
2, Ethical section
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
2
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
As the authors undertake the annotation work, reported demographic and geographic characteristics
maybe violate the anonymous submission policy.3101