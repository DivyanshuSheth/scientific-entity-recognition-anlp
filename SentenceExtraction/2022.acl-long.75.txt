
Yichu Zhou
School of Computing
University of Utah
flyaway@cs.utah.eduVivek Srikumar
School of Computing
University of Utah
svivek@cs.utah.edu
Abstract
Given the prevalence of pre-trained contextual-
ized representations in today’s NLP, there have
been many efforts to understand what infor-
mation they contain, and why they seem to be
universally successful. The most common ap-
proach to use these representations involves
fine-tuning them for an end task. Yet, how
fine-tuning changes the underlying embedding
space is less studied. In this work, we study the
English BERT family and use two probing tech-
niques to analyze how fine-tuning changes the
space. We hypothesize that fine-tuning affects
classification performance by increasing the
distances between examples associated with
different labels. We confirm this hypothesis
with carefully designed experiments on five dif-
ferent NLP tasks. Via these experiments, we
also discover an exception to the prevailing wis-
dom that “fine-tuning always improves perfor-
mance”. Finally, by comparing the representa-
tions before and after fine-tuning, we discover
that fine-tuning does not introduce arbitrary
changes to representations; instead, it adjusts
the representations to downstream tasks while
largely preserving the original spatial structure
of the data points.
1 Introduction
Pre-trained transformer-based language mod-
els (e.g., Devlin et al., 2019) form the basis of
state-of-the-art results across NLP. The relative
opacity of these models has prompted the devel-
opment of many probes to investigate linguistic
regularities captured in them (e.g., Kovaleva et al.,
2019; Conneau et al., 2018; Jawahar et al., 2019).
Broadly speaking, there are two ways to use
a pre-trained representation (Peters et al., 2019):
as a fixed feature extractor (where the pre-trained
weights are frozen), or by fine-tuning it for a
task. The probing literature has largely focused
on the former (e.g., Kassner and Schütze, 2020;
Perone et al., 2018; Yaghoobzadeh et al., 2019;Krasnowska-Kiera ´s and Wróblewska, 2019; Wal-
lace et al., 2019; Pruksachatkun et al., 2020; Agha-
janyan et al., 2021). Some previous work (Mer-
chant et al., 2020; Mosbach et al., 2020b; Hao
et al., 2020) does provide insights about fine-tuning:
fine-tuning changes higher layers more than lower
ones and linguistic information is not lost during
fine-tuning. However, relatively less is understood
about how the representation changes during the
process of fine-tuning and why fine-tuning invari-
ably seems to improve task performance.
In this work, we investigate the process of fine-
tuning of representations using the English BERT
family (Devlin et al., 2019). Specifically, we ask:
1.Does fine-tuning always improve perfor-
mance?
2.How does fine-tuning alter the representation
to adjust for downstream tasks?
3.How does fine-tuning change the geometric
structure of different layers?
We apply two probing techniques—classifier-
based probing (Kim et al., 2019; Tenney et al.,
2019) and D P (Zhou and Srikumar,
2021)—on variants of BERT representations that
are fine-tuned on five tasks: part-of-speech tag-
ging, dependency head prediction, preposition su-
persense role & function prediction and text clas-
sification. Beyond confirming previous findings
about fine-tuning, our analysis reveals several new
findings, briefly described below.
First, we find that fine-tuning introduces a di-
vergence between training and test sets , which is
not severe enough to hurt generalization in most
cases. However, we do find one exception where
fine-tuning hurts the performance; this setting also
has the largest divergence between training and test
set after fine-tuning (§4.1).
Second, we examine how fine-tuning changes
labeled regions of the representation space. For
a representation where task labels are not linearly
separable, we find that fine-tuning adjusts it bygrouping points with the same label into a small
number of clusters (ideally one), thus simplifying
the underlying representation. Doing so makes it
easier to linearly separate labels with fine-tuned
representations than untuned ones (§4.2). For a
representation whose task labels are already lin-
early separable, we find that fine-tuning pushes the
clusters of points representing different labels away
from each other, thus introducing large separating
regions between labels. Rather than simply scal-
ing the points, clusters move in different directions
and with different extents (measured by Euclidean
distance). Overall, these clusters become distant
compared to the untuned representation. We con-
jecture that the enlarged region between groups
admits a bigger set of classifiers that can separate
them, leading to better generalization (§4.3).
We verify our distance hypothesis by investi-
gating the effect of fine-tuning across tasks. We
observe that fine-tuning for related tasks can also
provide useful signal for the target task by altering
the distances between clusters representing differ-
ent labels (§4.4).
Finally, fine-tuning does not change the higher
layers arbitrarily. This confirms previous findings.
Additionally, we find that fine-tuning largely pre-
serves the relative positions of the label clusters,
while reconfiguring the space to adjust for down-
stream tasks (§4.5). Informally, we can say that
fine-tuning only “slightly” changes higher layers.
These findings help us understand fine-tuning
better, and justify why fine-tuned representations
can lead to improvements across many NLP tasks.
2 Preliminaries: Probing Methods
In this work, we probe representations in the BERT
family during and after fine-tuning. First, let us
look at the two supervised probes we will employ:
a classifier-based probe (e.g., Tenney et al., 2019;
Jullien et al., 2022) to assess how well a represen-
tation supports classifiers for a task, and D -
P (Zhou and Srikumar, 2021) to analyze the
geometry of the representation.
2.1 Classifiers as Probes
Trained classifiers are the most commonly used
probes in the literature (e.g. Hewitt et al., 2021;
Whitney et al., 2021; Belinkov, 2021). To under-
stand how well a representation encodes the labelsfor a task, a probing classifier is trained over it,
with the embeddings themselves kept frozen when
the classifier is trained.
For all our experiments, we use two-layer neural
networks as our probe classifiers. We use grid-
search to choose the best hyperparameters. Each
best classifier is trained five times with different
initializations. We report the average accuracy and
its standard deviation for each classifier.
The hidden layer sizes are selected from
{32,64,128,256} × { 32,64,128,256}, and the
regularizer weight from the range 10to10.
All models use ReLUs as the activation func-
tion for the hidden layer and are optimized by
Adam (Kingma and Ba, 2015). We set the maxi-
mum number of learning iterations to 1000 . We use
scikit-learn v0.22 (Pedregosa et al., 2011)
for these experiments.
Classifier probes aim to measure how well a
contextualized representation captures a linguistic
property. The classification performance can help
us assess the effect of fine-tuning.
2.2 D P : Probing the Geometric
Structure
Classifier probes treat the representation as a black
box and only focus on the final task performance;
they do not reveal how fine-tuning changes the un-
derlying geometry of the space. To this end, we
useD P (Zhou and Srikumar, 2021),
a recently proposed technique which analyzes em-
beddings from a geometric perspective. We briefly
summarize the technique and refer the reader to the
original work for details.
For a given labeling task, D P returns
a set of clusters such that each cluster only contains
the points with the same label, and there are no
overlaps between the convex hulls of these clusters.
Any decision boundary must cross the regions be-
tween the clusters that have different labels (see
in Figure 1). Since fine-tuning a contextualized
representation creates different representations for
different tasks, it is reasonable to probe the rep-
resentation based on a given task. These clusters
allow us to measure three properties of interest.
Number of Clusters : The number of clusters indi-
cates the linearity of the representation for a task. If
the number of clusters equals the number of labels,
then examples with the same label are grouped into△
△△
△△
△△
△△
△△
△△
△△
△△
△△
△
one cluster; a simple linear multi-class classifier
will suffice. If, however, there are more clusters
than labels, then at least two clusters of examples
with the same label can not be grouped together
(as in Figure 1, right). This scenario calls for a
non-linear classifier.
Distances between Clusters : Distancesbetween
clusters can reveal the internal structure of a rep-
resentation. By tracking these distances during
fine-tuning, we can study how the representation
changes. To compute these distances, we use the
fact that each cluster represents a convex object.
This allows us to use max-margin separators to
compute distances. We train a linear SVM (Chang
and Lin, 2011) to find the maximum margin separa-
tor and compute its margin. The distance between
the two clusters is twice the margin.
Spatial Similarity : Distances between clusters can
also reveal the spatial similarity of two representa-
tions. Intuitively, if two representations have sim-
ilar relative distances between clusters, the repre-
sentations themselves are similar to each other for
the task at hand.
We use these distances to construct a distance
vector vfor a representation, where each element
vis the distance between the clusters of a pair
of labels. With nlabels in a task, the size of v
is. This construction works only when the
number of clusters equals the number of labels (i.e.,
the dataset is linearly separable under the represen-
tation). Surprisingly, we find this to be the case for
most representations we studied. As a measure of
the similarity of two representations for a labeling
task, we compute the Pearson correlation coeffi-
cient between their distance vectors. Note that this
coefficient can also be used to measure the similar-
ity between two labeled datasets with respect to the
same representation. We exploit this observation
to analyze the divergence between training and test
sets for fine-tuned representations (§4.1).
3 Experimental Setup
In this section, we describe the representations and
tasks we will encounter in our experiments.
3.1 Representations
We investigate several models from the BERT fam-
ily (Devlin et al., 2019; Turc et al., 2019). These
models all share the same basic architecture but
with different capacities, i.e., different layers and
hidden sizes. Table 1 summarizes the models we
investigate in this work. All of these models are
for English text and uncased.
For tokens that are broken into subwords by the
tokenizer, we average the subword embeddings for
the token representation. We use the models pro-
vided by HuggingFace v4.2.1 (Wolf et al., 2020),
and Pytorch v1.6.0 (Paszke et al., 2019) for our
experiments.
3.2 Tasks
We instantiate our analysis of the BERT models on
a diverse set of five NLP tasks, which covers syn-
tactic and semantic predictions. Here, we briefly
describe the tasks, and refer the reader to the origi-
nal sources of the data for further details.
Part-of-speech tagging (POS) predicts the part-
of-speech tag for each word in a sentence. The
task helps us understand if a representation cap-
tures coarse grained syntactic categorization. We
use the English portion of the parallel universal
dependencies treebank (ud-pud, Nivre et al., 2016).
Dependency relation (DEP) predicts the syntac-
tic dependency relation between two tokens, i.e.(w andw). This task can help us under-
stand if, and how well, a representation can charac-
terize syntactic relationships between words. This
task involves assigning a category to a pair of to-
kens. We concatenate their contextualized repre-
sentations from BERT and treat the concatenation
as the representation of the pair. We use the same
dataset as the POS task for dependencies.
Preposition supersense disambiguation involves
two categorization tasks of predicting preposition’s
semantic role ( PS-role )andsemantic function ( PS-
fxn). These tasks are designed for disambiguating
semantic meanings of prepositions. Following the
previous work (Liu et al., 2019), we only train and
evaluate on single-token prepositions from Streusle
v4.2 corpus (Schneider et al., 2018).
Text classification , in general, is the task of catego-
rizing sentences or documents. We use the TREC-
50dataset (Li and Roth, 2002) with 50 semantic
labels for sentences. As is the standard practice, we
use the representation of the [CLS] token as the
sentence representation. This task can show how
well a representation characterizes a sentence.
3.3 Fine-tuning Setup
We fine-tune the models in §3.1 on the five tasks
from §3.2 separately.The fine-tuned models
(along with the original models) are then used to
generate contextualized representations. The prob-
ing techniques described in §2 are applied to study
both original and fine-tuned representations.
Our preliminary experiments showed that the
commonly used 3-5epochs of fine-tuning are in-
sufficient for the smaller representations, such as
BERT, and they require more epochs. We fine-
tuned all the representations for 10epochs except
BERT, which we fine-tuned for the usual three
epochs. Note that the fine-tuning phase is sepa-
rate from the classifier training phase for probing;
for the probe classifiers, we train two-layer neural
networks (described in §2.1) from scratch on both
original and fine-tuned representations, ensuring
a fair comparsion between them.
4 Observations and Analysis
In this section, we will use classifier probes to ex-
amine if fine-tuning always improves classifier per-formance (§4.1). Then we propose a geometric
explanation for why fine-tuning improves classi-
fication performance using D P (§4.2
and §4.3). Next, we will confirm this geomet-
ric explanation by investigating cross-task fine-
tuning (§4.4). Finally, we will analyze how fine-
tuning changes the geometry of different layers of
BERT(§4.5).
4.1 Fine-tuned Performance
It is commonly accepted that the fine-tuning im-
proves task performance. Does this always hold?
Table 2 summarizes the relevant observations from
our experiments. Appendix C presents the com-
plete fine-tuning results.
Fine-tuning diverges the training and test set.
In Table 2, the last column shows the spatial sim-
ilarity between the training and test set for each
representation. We apply D P on the
training and test set separately. The spatial simi-
larity is calculated as the Pearson correlation co-
efficient between the distance vectors of training
and test set (described in §2). We observe that after
fine-tuning, all the similarities decrease, implying
that the training and test set diverge as a result of
fine-tuning. In most cases, this divergence is not
severe enough to decrease the performance.
There are exceptions, where fine-tuning hurts
performance. An interesting observation in Ta-
ble 2 is that BERT does not show the im-
provements on the PS-fxn task after fine-tuning,
which breaks the well-accepted impression that
fine-tuning always improve the performance. How-
ever, only one such exception is observed across all
our experiments (see Appendix C). It is insufficient
to draw any concrete conclusions about why this is
happening. We do observe that BERT shows
the smallest similarity ( 0.44) between the training
and test set after fine-tuning on PS-fxn task. We
conjecture that controlling the divergence between
the training and test sets can help ensure that fine-
tuning helps. Verifying or refuting this conjecture
requires further study.
4.2 Linearity of Representations
Next, let us examine the geometry of the represen-
tations before and after fine-tuning using D -
P and counting the number of clusters. We
will focus on the overwhelming majority of cases
where fine-tuning does improve performance.
Smaller representations require more complex
classifiers. Table 3 summarizes the results. For
brevity, we only present the results on BERT.
The full results are in Appendix C. We observe
that before fine-tuning, small representations (i.e.,
BERT) are non-linear for most tasks. Although
a non-linearity does not imply poor generalization,
it represents a more complex spatial structure, and
requires a more complex classifier. This suggests
that to use small representations (say, due to limited
resources), it would be advisable to use a non-linear
classifier rather than a simple linear one.
Fine-tuning makes the space simpler. In Ta-
ble 3, we observe that the number of clusters de-
creases after fine-tuning. This tells us that after fine-
tuning, the points associated with different labels
are in a simpler spatial configuration. The same
trend holds for TREC-50 (Table 4), even when the
final representation is notlinearly separable.
4.3 Spatial Structure of Labels
To better understand the changes in spatial struc-
ture, we apply D P toevery intermedi-
ate representation encountered during fine-tuning.
Here, we focus on the BERT. Since all repre-
sentations we considered are linearly separable,
the number of clusters equals the number of labels.
As a result, each cluster exclusively corresponds to
one label. Going ahead, we will use clusters and
labels interchangeably.
Fine-tuning pushes each label far away from
each other. This confirms the observation of
Zhou and Srikumar (2021), who pointed out that
the fine-tuning pushes each label away from each
other. However, they use the global minimum dis-
tance between clusters to support this argument,
which only partially supports the claim: the dis-
tances between some clusters might increase de-
spite the global minimum distance decreasing.
We track the minimum distance of each label to
all other labels during fine-tuning. We find that all
the minimum distances are increasing. Figure 2
shows how these distances change in the last layer
of BERTfor the PS-role and POS tagging tasks.
Appendix D includes the plots for all tasks. For
clarity, we only show the three labels where the
distance increases the most, and the three where it
increases the least. We also observe that although
the trend is increasing, the minimum distance asso-
ciated with a label may decrease during the course
of fine-tuning, e.g., the label S in PS-role task,
suggesting a potential instability of fine-tuning.
To further see how labels move during the fine-
tuning, we track the centroids of each cluster. We
select three closest labels from the POS tagging
task and track the paths of the centroids of each
label cluster in the last layer of BERTduring
the fine-tuning. Figure 3 (right) shows the 2D PCA
projection of these paths. We observe that before
fine-tuning, the centroids of all these three labels
are close to each other. As fine-tuning proceeds,
the centroids move around in different directions,
away from each other.
We conclude that fine-tuning enlarges the gaps
between label clusters and admits more classifiers
consistent with the labels, allowing for better gen-
eralization. Note that neither the loss nor the op-
timizer explicitly mandates this change. Indeed,since the labels were originally linearly separable,
the learner need not adjust the representation at all.
4.4 Cross-task Fine-tuning
In §4.3, we hypothesized that fine-tuning improves
the performance because it enlarges the gaps be-
tween label clusters. A natural inference of this
hypothesis is that the process may shrink the gaps
between labels of an unrelated task, and its perfor-
mance can decrease. In this subsection, we investi-
gate how fine-tuning for one task affects another.
We fine-tune the BERTon PS-role and POS
tagging tasks separately and use the fine-tuned
models to generate contextualized representations
for the PS-fxn task. Our choice of tasks in this
experimental design is motivated by the observa-
tion that PS-role and PS-fxn are similar tasks that
seek to predict supersense tags for prepositions.
On the other hand, POS tagging can adversely af-
fect the PS-fxn task because POS tagging requires
all the prepositions to be grouped together (label
ADP ) while PS-fxn requires different prepositions
to be far away from each other. We apply D- P on both representations to analyze the
geometric changeswith respect to PS-fxn.
The effects of cross-task fine-tuning depends on
how close two tasks are. The third and fourth
columns of Table 5 indicate the number of labels
whose minimum distance is increased or decreased
after fine-tuning. The second column from the right
shows the average distance change over all labels,
e.g. fine-tuning on POS results in the minimum dis-
tances of the PS-fxn labels decreasing by 1.68on
average. We observe that fine-tuning on the same
dataset (PS-fxn) increases the distances between
labels (second row), which is consistent with ob-
servations from §4.3; fine-tuning on a similar task
also increases the distances between clusters (third
row) but to a lesser extent. However, fine-tuning on
a “opposing” task decreases the distances between
clusters (last row). These observations suggest that
cross-task fine-tuning could add or remove infor-
mation from the representation, depending on how
close the source and target task are.
Small distances between label clusters indicate
a poor performance. Based on our conclusion
in §4.3 that a larger gap between labels leads to bet-
ter generalization, we expect that the performance
of PS-fxn after fine-tuning on PS-role would be
higher than the performance after fine-tuning on
POS tagging. To verify this, we train two-layer
neural networks on PS-fxn task using the represen-
tations that are fine-tuned on PS-role and POS tag-
ging tasks. Importantly, we do not further fine-tune
the representations for PS-fxn. The last column
of Table 5 shows the results. Fine-tuning on PS-
fxn enlarges gaps between allPS-fxn labels, which
justifies the highest performance; fine-tuning on
PS-role enlarges gaps between some labels in PS-
fxn, leading to a slight improvement; fine-tuning
on POS tags shrinks the gaps between alllabels in
PS-fxn, leading to a decrease in performance.
In summary, based on the results of §4.2, §4.3
and §4.4, we conclude that fine-tuning injects or
removes task-related information from representa-
tions by adjusting the distances between label clus-
terseven if the original representation is linearly
separable (i.e., when there is no need to change the
representation). When the original representation
does not support a linear classifier, fine-tuning tries
to group points with the same label into a small
number of clusters, ideally one cluster.
4.5 Layer Behavior
Previous work (Merchant et al., 2020; Mosbach
et al., 2020b) showed that during fine-tuning, lower
layers changed little compared to higher layers. In
the following experiments, we confirm their find-
ings and further show that: (i) fine-tuning does
not change the representation arbitrarily, even for
higher layers; (ii) an analysis of the changes of dif-
ferent layers by a visual comparison between lower
and higher layers. Here, we focus on the POS tag-
ging task with BERT. Our conclusions extend
to other tasks, whose results are in Appendix E.Higher layers do not change arbitrarily. Al-
though previous work (Mosbach et al., 2020b)
shows that higher layers change more than the
lower layers, we find that higher layers still remain
close to the original representations. To study the
dynamics of fine-tuning, we compare each layer
during fine-tuning to its corresponding original pre-
trained one. The spatial similarity between two rep-
resentations is calculated as the Pearson correlation
coefficient of their distance vectors as described
in §2. Intuitively, a classifier learns a decision
boundary that traverses the region between clusters,
which makes the distances between clusters more
relevant to our analysis (as opposed to the spatial
structure of points within each cluster).
Figure 4 shows the results for all four tasks.
To avoid visual clutter, we only show the plots for
every alternate layer. For the higher layers, we find
that the Pearson correlation coefficient between the
original representation and the fine-tuned one is
surprisingly high (more than 0.5), reinforcing the
notion that fine-tuning does not change the repre-
sentation arbitrarily. Instead, it attempts to pre-
serve the relative positions the labels. This means
the fine-tuning process encodes task-specific in-
formation, yet it largely preserves the pre-trained
information encoded in the representation.
The labels of lower layers move only in a small
region and almost in the same directions. The
unchanged nature of lower layers raises the ques-
tion: do they not change at all? To answer this
question, for every label, we compute difference
between its centroids before and after fine-tuning.
Figure 5 shows the PCA projection in 2D of these
difference vectors. For brevity, we only present the
plots for every alternative layer. A plot with all lay-
ers can be found in Appendix E. We observe that
the movements of labels in lower layers concentrate
in a few directions compared to the higher layers,
suggesting the labels in lower layers do change, but
do not separate the labels as much as higher layers.
Also, we observe that the labels INTJ andSYM
have distinctive directions in the lower layers.
Note that, in Figure 5, the motion range of lower
layers is much smaller than the higher layers. The
projected two dimensions range from −1to3and
from−3to3for layer two, while for layer 12 they
range from −12to13and−12to8, suggesting
that labels in lower layers only move in a small
region compared to higher layers. Figure 3 shows
an example of this difference. Compared to the
layer 12 (right) paths, we see that the layer 1 paths
(left) traverse almost the same trajectories, which
is consistent with the observations from Figure 5.
5 Discussion
Does fine-tuning always improve performance?
Indeed, fine-tuning almost always improves task
performance. However, rare cases exist where fine-
tuning decreases the performance. Fine-tuning in-
troduces a divergence between the training set and
unseen examples (§4.1). However, it is unclear
how this divergence affects the generalization abil-
ity of representations, e.g. does this divergence
suggest a new kind of overfitting that is driven by
representations rather than classifiers?
How does fine-tuning alter the representation
to adjust for downstream tasks? Fine-tuning al-
ters the representation by grouping points with thesame label into small number of clusters (§4.2) and
pushing each label cluster away from the others
(§4.3). We hypothesize that the distances between
label clusters correlate with the classification per-
formance and confirm this hypothesis by investigat-
ing cross-task fine-tuning (§4.4). Our findings are
surprising because fine-tuning for a classification
task does not need to alter the geometry of a rep-
resentation if the data is already linearly separable
in the original representation. What we observe
reveals geometric properties that characterize good
representations. We do not show theoretical analy-
sis to connect our geometric findings to representa-
tion learnability, but the findings in this work may
serve as a starting point for a learning theory for
representations.
How does fine-tuning change the underlying ge-
ometric structure of different layers? It is es-
tablished that higher layers change more than the
lower ones. In this work, we analyze this behavior
more closely. We discover that higher layers do not
change arbitrarily; instead, they remain similar to
the untuned version. Informally, we can say that
fine-tuning only “slightly” changes even the higher
layers (§4.5). Nevertheless, our analysis does not
reveal why higher layers change more than the
lower layers. A deeper analysis of model parame-
ters during fine-tuning is needed to understand the
difference between lower and higher layers.
Limitations of this work. Our experiments use the
BERT family of models for English tasks. Given
the architectural similarity of transformer language
models, we may be able to extrapolate the results to
other models, but further work is needed to confirm
our findings to other languages or model archi-
tectures. In our analysis, we ignore the structure
within each cluster, which is another information
source for studying the representation. We plan to
investigate these aspects in future work. We make
our code available for replication and extension by
the community.
6 Related Work
There are many lines of work that focus on an-
alyzing and understanding representations. The
most commonly used technique is the classifier-
based method. Early work (Alain and Bengio,
2017; Kulmizev et al., 2020) starts with using linear
classifiers as the probe. Hewitt and Liang (2019)
pointed out that a linear probe is not sufficient
to evaluate a representation. Some recent workalso employ non-linear probes (Tenney et al., 2019;
Eger et al., 2019). There are also efforts to in-
spect the representations from a geometric persepc-
tive (e.g. Ethayarajh, 2019; Mimno and Thompson,
2017), including the recently proposed D -
P (Zhou and Srikumar, 2021), which we use
in this work. Another line of probing work designs
control tasks (Ravichander et al., 2021; Lan et al.,
2020) to reverse-engineer the internal mechanisms
of representations (Kovaleva et al., 2019; Wu et al.,
2020). However, in contrast to our work, most stud-
ies (Zhong et al., 2021; Li et al., 2021; Chen et al.,
2021) focused on pre-trained representations, not
fine-tuned ones.
While fine-tuning pre-trained representations
usually provides strong empirical perfor-
mance (Wang et al., 2018; Talmor et al., 2020),
how fine-tuning manage to do so has remained an
open question. Moreover, the instability (Mosbach
et al., 2020a; Dodge et al., 2020; Zhang et al., 2020)
and forgetting problems (Chen et al., 2020; He
et al., 2021) make it harder to analyze fine-tuned
representations. Despite these difficulties, previous
work (Merchant et al., 2020; Mosbach et al.,
2020b; Hao et al., 2020) draw valuable conclusions
about fine-tuning. This work extends this line of
effort and provides a deeper understanding of how
fine-tuning changes representations.
7 Conclusions
In this work, we take a close look at how fine-
tuning a contextualized representation for a task
modifies it. We investigate the fine-tuned represen-
tations of several BERT models using two probing
techniques: classifier-based probing and D -
P . First, we show that fine-tuning introduces
divergence between training and test set, and in
at least one case, hurts generalization. Next, we
show fine-tuning alters the geometry of a repre-
sentation by pushing points belonging to the same
label closer to each other, thus simpler and better
classifiers. We confirm this hypothesis by cross-
task fine-tuning experiments. Finally, we discover
that while adjusting representations to downstream
tasks, fine-tuning largely preserves the original spa-
tial structure of points across all layers. Taken
collectively, the empirical study presented in this
work can not only justify the impressive perfor-
mance of fine-tuning, but may also lead to a better
understanding of learned representations.Acknowledgments
We thank the ARR reviewers and the Utah NLP
group for their constructive feedback. This work
is partially supported by NSF grants #1801446
(SaTC) and #1822877 (Cyberlearning), and a gen-
erous gift from Verisk Inc.
References
A Fine-tuning Details
In this work, we fine-tune all tasks and representa-
tions using HuggingFace library. We use a linear
weight schduler with a learning rate of 3e, which
uses 10% of the total update steps as the warmup
steps. The same schduler is used for all tasks. All
the models are optimized by Adam (Kingma and
Ba, 2015) with batch size of 32. All the fine-tuning
is run on a single Titan GPU. The best hidden-layer
sizes for each task are shown in Table 7.
B Summary of Tasks
In this work, we conduct experiments on five NLP
tasks, which are chosen to cover different usages of
the representations we study. Table 6 summarizes
these tasks.
C Probing Performance
Table 7 shows the complete table of probing results
in our experiments. The last column is the spa-
tial similarity between the training set and test set.
Some entries are missing because the similarity can
only be computed on the representations that are
linearly separable for the given task.
D Dynamics of Minimum Distances
Figure 6 shows the dynamics of minimum distances
for labels on all four tasks. For clarity, we only
present the distances for the three labels where the
distances increase the most and the three where it
decreases the most.
E PCA Projections of the Movements
Figures 7–10 show the PCA projections of the dif-
ference vector between the centroids of labels be-
fore and after fine-tuning based on BERT.
F Cluster Number Revision
We discovered a bug in the implementation of D- P which causes the merging to stop
early while the remaining clusters are still merge-
able. The main paper (Table 3, Table 4, and Ta-
ble 7) has been updated to report the correct results.
Table 8 shows the original results.
This bug does not change the natural of the lin-
earity of datasets and representations. All the find-
ings from original experiments remain the same.
This bug only affects the number of clusters when
the representation is non-linear for a given task.