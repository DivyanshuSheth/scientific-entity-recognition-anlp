
Fatemehsadat Mireshghallah, Kartik Goyal, Archit Uniyal
Taylor Berg-Kirkpatrick,Reza ShokriUniversity of California San Diego,Toyota Technological Institute at Chicago (TTIC)University of Virginia,National University of Singapore
[fatemeh, tberg]@ucsd.edu ,
kartikgo@ttic.edu,a.uniyal@virginia.edu,reza@comp.nus.edu.sg
Abstract
The wide adoption and application of Masked
language models (MLMs) on sensitive data
(from legal to medical) necessitates a thorough
quantitative investigation into their privacy
vulnerabilities. Prior attempts at measuring
leakage of MLMs via membership inference
attacks have been inconclusive, implying
potential robustness of MLMs to privacy at-
tacks. In this work, we posit that prior attempts
were inconclusive because they based their
attack solely on the MLM’s model score. We
devise a stronger membership inference attack
based on likelihood ratio hypothesis testing
that involves an additional reference MLM to
more accurately quantify the privacy risks of
memorization in MLMs. We show that masked
language models are indeed susceptible to
likelihood ratio membership inference attacks:
Our empirical results, on models trained on
medical notes, show that our attack improves
the AUC of prior membership inference attacks
from 0.66to an alarmingly high 0.90level.
1 Introduction
BERT-based encoders with Masked Language Mod-
eling (MLM) Objectives (Devlin et al., 2018; Liu
et al., 2019) have become models of choice for use
as pre-trained models for various Natural Language
Processing (NLP) classification tasks (Wang et al.,
2018; Zhang et al., 2019; Rogers et al., 2020) and
have been applied to diverse domains such as dis-
ease diagnosis, insurance analysis on financial data,
sentiment analysis for improved user experience,
etc (Yang et al., 2020; Gu et al., 2021; Lee et al.,
2020). Given the sensitivity of the data used to train
these models, it is crucial to conceive a framework
to systematically evaluate the leakage of training
data from these models (Shokri, 2022; Carlini et al.,
2019; Murakonda and Shokri, 2020; Mireshghallah
et al., 2020), and limit the leakage. The conventional
way to measure the leakage of training data from
machine learning models is by performing mem-bership inference attacks (Shokri et al., 2017; Nasr
et al., 2021), in which the attacker tries to determine
whether a given sample was part of the training data
of the target model or not. These attacks expose the
extent of memorization by the model at the level of
individual samples. Prior attempts at performing
membership inference and reconstruction attacks on
masked language models have either been inconclu-
sive (Lehman et al., 2021), or have (wrongly) con-
cluded that memorization of sensitive data in MLMs
is very limited and these models are more private
than their generative counterparts (e.g., autoregres-
sive language models) (Vakili and Dalianis, 2021;
Jagannatha et al., 2021; Nakamura et al., 2020).
We hypothesize that prior MLM attacks have
been inconclusive because they rely solely on the
target model’s (model under attack) loss on each
individual sample as a proxy for how well the model
has memorized that sample. If the loss is lower than
a threshold, the sample is predicted to be a member
of the training set. However, the target model’s
loss includes confounding factors of variation like
the intrinsic complexity of the sample – and thus
provides a limited discriminative signal for mem-
bership prediction. This scheme has either a high
false-negative rate (with a conservative threshold)
– classifying many hard-to-fit samples from the
training set as non-members, or a high false-positive
rate (with a generous threshold) – failing to identify
easy-to-fit samples that are not in the training set.
Reference-based likelihood ratio attacks, on the
other hand, when applied to certain probabilistic
graphical models and classifiers, have been shown
to alleviate this problem and more accurately dis-
tinguish members from non-members (Murakonda
et al., 2021; Ye et al., 2021). In such attacks, instead
of the loss of the model under attack, we look at the
ratio of the likelihood of the sample under the target
model and a reference model trained on samples
from the underlying population distribution that
generates the training data for the target model.8332This ratio recalibrates the test statistic to explain
away spurious variation in model’s loss for different
samples due to the intrinsic complexity of the
samples. Unlike most other models (e.g., generative
models), however, computing the likelihood of
MLMs is not straightforward. In this paper, we
propose a principled framework for measuring
information leakage of MLMs through likelihood
ratio-based membership inference attacks and
perform an extensive analysis of memorization in
such models. To compute the likelihood ratio of the
samples under the target and the reference MLMs,
we view the MLMs as energy-based probabilistic
models (Goyal et al., 2022) over the sequences.
This enables us to perform powerful inference
attacks on conventionally non-probabilistic models
like masked language models.
We evaluate our proposed attack on a suite
of masked clinical language models, follow-
ing (Lehman et al., 2021). We compare our attack
with the baseline from the prior work that relies
solely on the loss of the target model (Yeom et al.,
2018; Song and Raghunathan, 2020; Jagannatha
et al., 2021). We empirically show that our
attack improves the AUC from 0.66to0.90on the
ClinicalBERT-Base model, and achieves a true
positive rate (recall) of 79.2%(for a false positive
rate of 10%), which is a substantial improvement
over the baseline with 15.6%recall. This shows
that, contrary to prior results, masked language
models are significantly susceptible to attacks
exploiting the leakage of their training data. In low
error regions (at 1%false positive rate) our attack
is51×more powerful than the prior work .
We also present analyses of the effect of the
size of the model, the length of the samples, and
the choice of the reference model on the success
of the attack. Finally, we attempt to identify
features of samples that are more exposed (attack
is more successful on), and observe that samples
with multiple non-alphanumeric symbols (like
punctuation) are more prone to being memorized.
We provide instructions on how to request access
to the data and code in Appendix A.2.1.
2 Membership Inference Attacks
In this section, we first formally describe the mem-
bership inference attack, how it can be conducted
using likelihood ratio tests and how we apply the
test for masked language models (MLMs) which do
not explicitly offer an easy-to-compute probability
distribution over sequences. Finally, we describe allthe steps in our attack, as summarized in Figure 1.
2.1 Problem Formulation
LetMdenote a model with parameters θthat
have been trained on data set D, sampled from the
general population distribution p. Our goal is to
quantify the privacy risks of releasing Mfor the
members of training set D.
We consider an adversary who has access to the
target model M. We assume this adversary can
train a (reference) model Mwith parameters θ
on independently sampled data from the general
population p. In a Membership Inference Attack
(MIA), the objective of the adversary is to create
a decision rule that determines whether a given
sample swas used for training M. To test the
adversary, we perform the following experiment.
We sample a datapoint sfrom either the general
population or the training data with a 0.5probability,
and challenge the adversary to tell if sis selected
from the training set (it is a member) or not (it is a
non-member) (Murakonda et al., 2021). The pre-
cision of the membership inference attack indicates
the degree of information leakage from the target
model about the members of its training set. We
measure the adversary’s success using two metrics:
(1) the adversary’s power (the true positive rate),
and (2) the adversary’s error (the false positive rate).
2.2 Likelihood Ratio Test
Before discussing our proposed attack for MLMs
in the next section, we summarize the likelihood
ratio test here which forms the core of our approach.
A likelihood ratio test distinguishes between a null
hypothesis and an alternative hypothesis via a test
statistic based on the ratio of likelihoods under the
two hypotheses. Prior work demonstrated an MIA
attack based on the likelihood ratio to be optimal
for probabilistic graphical models (Bayesian net-
works) (Murakonda et al., 2021). Given a sample s
from the training data of the target model, the adver-
sary aims at distinguishing between two hypotheses:
1.Null hypothesis ( H): The target sample s
is drawn from the general population p,
independently from the training set D.
2.Alternative hypothesis ( H): The target
sample sis drawn from the target model’s
training set D.
The goal of hypothesis testing is to find whether
there is enough evidence to reject Hin favor
ofH. We use a likelihood ratio for this purpose8333
which involves comparison of the likelihood of the
target sample under the settings for HandH
respectively. For H, we already have access to
the target model, which is parameterized by θand
trained on D. ForH, we require access to a model
trained on the general population. As mentioned ear-
lier, the adversary has access to a reference model
parameterized by θ. Therefore, the likelihood
ratio test is characterized by the following statistic:
L(s)=log/parenleftbiggp(s;θ)
p(s;θ)/parenrightbigg
(1)
The Likelihood Ratio (LR) test is a comparison
of the log-likelihood ratio statistic L(s)with a
threshold t. IfL(s)≤t, then the adversary rejects
H(decides in favor of membership of s∈D);
otherwise the adversary fails to reject H. We
discuss the details of selecting the threshold and
quantifying the attack’s success in Section 2.4.
2.3 Likelihood Ratio Test for MLMs
Performing a likelihood ratio test with masked
language models is difficult because these models
do not explicitly define an easy-to-compute
probability distribution over natural language
sequences. Following prior work (Goyal et al.,
2022), we alternatively view pre-trained MLMs
as energy-based probability distributions on se-
quences, allowing us to directly apply the likelihood
ratio formalism. An energy-based sequence model
defines the probability distribution over the space
of possible sequences Sas:
p(s;θ)=e
Z,
where E(s;θ)refers to the scalar energy of
a sequence sthat is parametrized by θ, and
Z=/summationtextedenotes the intractablenoramlization constant. Under this framework, the
likelihood ratio test statistic (Eq. 1) is:
L(s)=log/parenleftbiggp(s;θ)
p(s;θ)/parenrightbigg
=log/parenleftigg
e
Z/parenrightigg
−/parenleftigg
loge
Z/parenrightigg
=−E(s;θ)−log(Z)+E(s;θ)+log( Z)
=E(s;θ)−E(s;θ)+constant
Above, we make use of the fact that for two fixed
models (i.e., target model θ, and reference model
θ), the intractable term log(Z)−log(Z)is
a global constant and can be ignored in the test.
Therefore, computation of the test statistic only
relies on the difference between the energy values
assigned to sample sby the target model M, and
the reference model M.
In practice, we cast a traditional MLM as an
energy-based language model using a slightly
different parameterization than explored by Goyal
et al. (2022). Since the training of most MLMs (in-
cluding the ones we attack in experiments) involves
masking 15% of the tokens in a training sequence,
we define our energy parameterization on these
15% chunks. Specifically, for a sequence of length
T, and the subset size l=⌈0.15×T⌉, we consider
computing the energy with the set Cconsisting of
all/parenleftbig/parenrightbig
combinations of masking patterns.
E(s;θ)=−1
|C|/summationdisplay/summationdisplaylog/parenleftbig
p(s|s;θ)/parenrightbig
(2)
where sis the sequence swith the lpositions in
Imasked. Computing this energy, which involves
running |C|=/parenleftbig/parenrightbig
forward passes of the MLM, is
expensive. Hence, we further approximate this
parametrization by summing up over Krandom
masking patterns where K≪|C| .8334
2.4 Quantifying the Privacy Risk
Given the form of the likelihood ratio test statistic
(Eq. 2) and energy function formulation for MLM
likelihood (Eq. 2), we conduct the attack as follows
(shown in Figure 1):
1.Given a sample swhose membership we want
to determine, we calculate its energy E(s;θ)
under the model under attack ( M) using Eq. 2.
We calculate the energy E(s;θ)under the ref-
erence model. Using Eq. 1, we compute the test
statistic L(s)by subtracting the two energies.
2.We compare L(s)to a threshold t, and if
L(s)≤t, we reject the null hypothesis ( H)
and mark the sample as a member. Otherwise,
we mark it as a non-member.
Choosing the threshold. The threshold determines
the (false positive) error the adversary is willing to
tolerate in the membership inference attack. Thus,
for determining the threshold t, we select a false
positive rate α, and empirically compute tas the
corresponding percentile of the likelihood ratio
statistic over random samples from the underlying
distribution. This process is visualized in Figure 2a.
We empirically estimate the distribution of the test
statistic L(x)using all the sequences xdrawn from
the general population distribution. This yields
the distribution of Lunder the null hypothesis. We
then select the threshold such that the tolerance of
attack’s error i.e. the rate at which attack falsely
classifies the population data as “members” is α%.
Quantifying the Privacy Risk. The attacker’s
success (i.e. the privacy loss of the model) can be
quantified using the relation between the attack’spower (the true positive rate) versus its error (the
false positive rate). Higher power for lower errors
indicates larger privacy loss. To compare two
attack algorithms (e.g., our method versus the target
model loss based methods), we can compute their
power for all different error values, which can be
illustrated in an ROC curve (as in Figure 2b and
Figure 4). This enables a complete comparison
between two attack algorithms. The Area Under
the Curve (AUC) metric for each attack provides
an overall threshold independent evaluation of the
privacy loss under each attack.
3 Experimental Setup
We conduct our experiments using the pre-
processed data, and pre-trained models provided
by Lehman et al. (2021). We use this medical-based
setup as medical notes are sensitive and leakage of
models trained on notes can cause privacy breaches.
In this section, we briefly explain the details of our
experimental setup. Appendix A.2 provides more
details. Table 1 provides a summary.
3.1 Datasets
We run our attack on two sets of target samples, in
both of which the “members” portion is sampled
from the training set ( D) of our target models,
which is the MIMIC-III dataset. The non-members,
however, are different. For the results shown under
“MIMIC”, the non-members are a held-out subset
of the MIMIC data that was not used in training.
For i2b2, the non-members are from a different (but
similar) dataset, i2b2. Below we elaborate on each
of these datasets (full detail in Appendix A.2.2).
Both the datasets require a license for access, so we
cannot show examples of the training data.
MIMIC-III. The target models we attack are
trained on the pseudo re-identified MIMIC-III
notes which consist of 1,247,291electronic health
records (EHR) of 46,520patients.
i2b2. This dataset was curated for the i2b2
de-identification of protected health information
(PHI) challenge in 2014 (Stubbs and Özlem
Uzuner, 2015). We use this dataset as a secondary
non-member dataset since it is similar in domain
to MIMIC-III (both are medical notes), is larger in
terms of size than the held-out MIMIC-III set, and
has not been used as training data for our models.
3.2 Models
Target Models. We perform our attack on 4
different pre-trained ClinicalBERT models, that are
all trained on MIMIC-III, but with different training8335
procedures, summarized in Table 1 under Models.
Reference Models. We use Pubmed-BERT
trained on pre-processed PubMed texts containing
around 4000M words extracted from PubMed
ASCII code version (Peng et al., 2019) as our
main domain-specific reference model, since its
training data is similar to MIMIC-III in terms of
domain, however, it does not include MIMIC-III
training data. We also use the standard pre-trained
bert-base-uncased as a general-domain
reference model for ablating our attack.
3.3 Baselines
We compare our results with a popular prior method,
which uses the loss of the target model as a signal to
predict membership (Yeom et al., 2018; Jayaraman
et al., 2021; Ye et al., 2021). We show this baseline
asModel loss in our tables. This baseline could have
two variations, based on the way its threshold is cho-
sen: (1) µthreshold (Jagannatha et al., 2021), which
assumes access to the mean of the training data loss,
µand uses it as the threshold for the attack, and (2)
population threshold (pop. thresh.) which calcu-
lates the loss on a population set of samples (samples
that were not used in training but are similar to train-
ing data), and then selects the threshold that would
result in a 10% false positive rate on that population.
3.4 Metrics
Area Under the ROC Curve (AUC). The ROC
curve is a plot of power (true positive rate) versus
error (false positive rate), measured across different
thresholds t, which captures the trade-off between
power and error. Thus, the area under the ROC
curve (AUC) is a single, threshold-independent
metric for measuring the strength of the attack. Fig-
ure 2b shows how we obtain the ROC curve. AUC
=1implies that the attacker can correctly classify
all target samples as members or non-members.
Precision and Recall. We set α=10% as the false
positive rate and choose the threshold accordingly,
as shown in Fig. 2a. For precision, we measure
the percentage of samples correctly inferred as
members of the training set out of the total number
of target samples inferred as members by the attack.
For recall, we measure the percentage of samples
correctly inferred as members of the training set
out of the total number of target samples that are
actually members of the training set.
4 Results
In this section, we discuss our experimental results
and main observations. First, we explore the overall
performance improvement of our approach over
baselines. Later, we analyze the effectiveness of
our approach across several factors of variation that
have an effect on the leakage of the model. (e.g.
length of samples, model size, including names8336
etc.) Finally, we explore correlations between
samples that are deemed to be exposed by our
approach. We provide further studies and ablations
on choosing a lower false-positive rate of 1%, using
different energy formulation and changing target
sequence lengths in Appendix sections A.3.1, A.3.2,
and A.3.3, respectively.
4.1 Comparison with Baseline
Table 2 shows the metrics for our attack and the
baseline’s on both sample and patient level, with
held-out MIMIC-III and i2b2 medical notes used
as non-member samples (Figure 4 shows the ROC
curve). The table shows that our method signifi-
cantly outperforms the target model loss-based base-
lines (Jagannatha et al., 2021; Yeom et al., 2018),
which threshold the loss of the target model based
on either the mean of the training samples’ loss
(µ), or the population samples’ loss. Our attack’s
improvement over the baselines is more apparent in
the case where both the members and non-members
are from MIMIC-III. This case is harder for the
baselines since members and non-members are
much more similar and harder to distinguish if we
only look at the loss of the target model. Our attack,
however, is successful due to the use of a reference,
which helps magnify the gap in the behavior of the
target model towards members and non-members,
thereby teasing apart similar samples.
We can also see that in terms of precision/recall
trade-off, our attack has a consistently higher recall,
with an average higher precision. Population loss
based thresholding ((A) w/ Pop. thresh.) has the
lowest recall of 15.6%, which is due to members
and non-members achieving similar losses from
the target model due to their similarity. This is also
shown in Figure 3b. In Figure 3a, however, we
see a distinct separation between the member and
non-member histogram distributions when we use
the ratio statistic L(s)as the test criterion for our
attack. This results in the estimation of a useful
threshold that correctly classifies the blue line sam-
ple as a member, as opposed to using only the target
model loss (Figure 3b). Finally, we observe that all
the metrics have higher values on the patient-level
attack, compared to sample-level, for both our
attack and the baselines. This is due to the higher
granularity of the patient level attack, as it makes the
decision based on an aggregate of multiple samples.
4.2 Effect of Sample Length and Model Size
Tables 3 and 4 show the metrics for our attack and
the baseline broken down based on the length of the
target sample, and the size and training epochs of
the target model, respectively. In Table 3, the target
model is same as that of Table 2, ClinicalBERT-base.
Short samples are those that have between 10to
20tokens, and long samples have 20to60tokens.8337
We can see that both the baseline and our attacks
show more leakage for long sentences than they
do for short sequences, which could be due to the
longer sentences being more unique and thus being
more likely to provide a discriminative signal for a
sequence-level decision. Table 4 shows the attacks
mounted on the four models from Table 1. We
see that leakage on all the models is very similar,
however, the AUC on Large++ is consistently
higher than on Base, which hints at the observation
made by (Carlini et al., 2021b) that larger models
tend to have a higher capacity for memorization.
4.3 Effect of Changing the Reference Model
Table 5 studies how changing the reference model
would affect the success of the attack. Here, Pubmed
is the reference model that is used in the previous
experiments, and BERT-base is Huggingface’s
pre-trained BERT. We observe that the attack using
BERT-base performs well, but is worse than using
Pubmed, especially in terms of recall (true positive
rate). The main reason behind this is the domain
overlap between the Pubmed reference model and
the model under attack. An ideal reference model
for this attack would be trained on data from a
domain that is similar to that of the target model’s
training set so as to better characterize the intrinsic
complexity of the samples. On the other hand, a ref-
erence model trained on a different data distribution
(in this case Wikipedia) would give the same score
to easy and difficult samples, thereby decreasing
the true positive rate (recall), as shown in the table.
4.3.1 Effect of Inserting Names
Table 6 shows results for attacking the name inser-
tion model (Lehman et al., 2021), shown as Base-b,
where the patient’s first and last name are prepended
to each training sample. We see that our attack’s
performance is better on the name-insertion model,
compared to the base model, whereas the baseline at-
tack performs worse (in the sample-level scenario).
We hypothesize that this is due to the “difficulty”
of the samples. Adding names to the beginning
of each sample actually increases the entropy of
the dataset overall, since in most cases they don’t
have a direct relation with the rest of the sentence
(except for very few sentences that directly state a
person’s disease), therefore they might as well be
random. This makes these sentences more difficult
and harder to learn, as there is no easy pattern.
Hence, on average, these sentences have higher loss
values (2.14 for name inserted samples, vs. 1.61 for
regular samples). However, for the non-members,
since they don’t have names attached to them, the
average loss is the same (the 10% FPR threshold
is 1.32), and that is why the attack performs poorly
on these samples, as most of the members get
classified as non-members. For our attack, since
we use the reference, we are able to tease apart such
hard samples as they are extremely less likely given
the reference than they are given the target model.83384.4 Correlations between Memorized Samples
To evaluate whether there are correlations between
samples that have high leakage based on our attack
(i.e. training samples that are successfully detected
as members), we conduct an experiment. In this
experiment, we create a new train and test dataset,
by subsampling the main dataset and selecting
5505 and7461 samples, respectively. We label the
training and test samples based on whether they are
exposed or not, i.e. whether the attack successfully
detects them as training samples or not, and get
2519 and 3 283samples labeled as “memorized”,
for the train and test set. Since our goal is to see if
we can find correlations between the memorized
samples of the training set and use those to predict
memorization on our test set, we create features
for each sample, and then use those features with
the labels to create a simple logistic regression
classifier that predicts memorization.
Table 7 shows these results in terms of precision
and recall for predicting if a sample is “memorized"
or not, with different sets of features. The first 4
rows correspond to individual handcrafted feature
sets: (A) the number of digits in the sample, (B)
length of a sample (in tokens), (C) the number of
non-alphanumeric characters (this would be char-
acters like ’*’, ’-’, etc.). (D) corresponds to feature
sets that are obtained by encoding the tokenized
sample by the frequency of each of its tokens, and
then taking the 3 least frequent tokens’ frequencies
as features (the frequency comes from a frequency
dictionary built on the training set). We can see
that among the hand-crafted features, (C) is most
indicative, as it counts the characters that are more
out-of-distribution and are possibly not determined
by grammatical rules or consistent patterns. (C) and
(D) concatenated together perform slightly better
than (C) alone, which could hint at the effect fre-
quency of tokens and how common they are could
have on memorization. We also get a small improve-
ment over these by concatenating (B), (C), and (D),
which shows the length has a slight correlation too.
5 Related Work
Prior work on measuring memorization and leakage
in machine learning models can be classified into
two main categories: (1) membership inference
attacks and (2) training data extraction attacks.
Membership inference. Membership Inference
Attacks (MIA) try to determine whether or not
a target sample was used in training a target
model (Shokri et al., 2017; Yeom et al., 2018).
These attacks can be seen as privacy risk analysis
tools (Murakonda and Shokri, 2020; Nasr et al.,
2021; Kandpal et al., 2022), which help reveal how
much the model has memorized the individual sam-
ples in its training set, and what the risk of individual
users is (Nasr et al., 2019; Long et al., 2017; Salem
et al., 2018; Ye et al., 2021; Carlini et al., 2021a).
A group of these attacks rely on behavior of shadow
models to determine the membership of given
samples (Jayaraman et al., 2021; Shokri et al., 2017).
Song and Shmatikov mounts such an attack on
LSTM-based text-generation models, Mahloujifar
et al. mounts one on word embedding, Hisamoto
et al. applies it to machine translation and more
recently, Shejwalkar et al. mounts it on transformer-
based NLP classification models. Mounting such
attacks is usually costly, as their success relies
upon training multiple shadow models on different
partitionings of shadow data, and access to adequate
shadow data for training such models.
Another group of MIAs relies solely on the
loss value of the target sample, under the target
model, and thresholds this loss to determine
membership (Jagannatha et al., 2021; Yeom et al.,
2018). Song and Raghunathan mount such an
attack on word embedding, where they try to infer
if given samples were used in training different
embedding models. Jagannatha et al. (2021), which
is the work closest to ours, uses a thresholding
loss-based attack to infer membership on MLMs.
Our approach instead incorporates a reference
model by using an energy-based formulation to8339mount a likelihood ratio based attack and achieves
higher AUC as shown in the results.
Training data extraction. Training data extrac-
tion quantifies the risk of extracting training data
by probing a trained language model (Salem et al.,
2020; Carlini et al., 2019; Zanella-Béguelin et al.,
2020; Carlini et al., 2021b, 2022; Nakamura et al.,
2020). One such prominent attacks on NLP models
is that of Carlini et al. (2021b), where they take
more than half a million samples from different
GPT-2 models, sift through the samples using a
membership inference method to find samples that
are most likely to have been memorized. Lehman
et al. (2021) mount the same data extraction attack
on MLMs, but their results are inconclusive as
to how much MLMs memorize samples. They
also mount other types of attacks, where they try
to extract a person’s name given their disease,
or disease given name, but in all their attacks,
they only use signals from the target model and
consistently find that a frequency-based baseline
(i.e. one that would always guess the most frequent
name/disease) is more successful.
6 Conclusions
In this paper, we introduce a principled membership
inference attack based on likelihood ratio testing
to measure the training data leakage of Masked
Language Models (MLMs). In contrast to prior
work on MLMs, we rely on signals from both
the model under attack and a reference model to
decide the membership of a sample. This enables
performing successful membership inference
attacks on data points that are hard to fit, and
therefore cannot be detected using the prior work.
We also perform an analysis of why these models
leak, and which data points are more susceptible
to memorization. Our attack shows that MLMs
are significantly prone to memorization. This
work calls for designing robust privacy mitigation
algorithms for such language models.
Limitations
Membership inference attacks form the foundation
of privacy auditing and memorization analysis in
machine learning. As we show in this paper, and as
it is shown in the recent work (Carlini et al., 2021a;
Ye et al., 2021), these attacks are very efficient in
identifying privacy vulnerabilities of models with
respect to individual data records. However, for a
thorough analysis of data privacy, it is not enough torely only on membership inference attacks. We thus
would need to extend our analysis to reconstruction
attacks and property inference attacks.
Ethics Statement
We use two datasets in this paper, MIMIC-III and
i2b2, both of which contain sensitive data and can
only be accessed by requestand after agreeing
to the data usage and confidentiality termsand
passing proper training for ethical and privacy-
preserving use of the data. For reproduction of our
results, code will be made available only by request
and for research purposes, only to researchers who
provide proof of authorized access to the datasets
(by forwarding the access granted emails from
MIMIC-III and i2b2 to the first author).
To protect models against membership inference
attacks, like the one proposed in this work, differen-
tially private training algorithms (Abadi et al., 2016;
Chaudhuri et al., 2011) can be used, as they are theo-
retically designed to protect the membership of each
data record individually. Other methods such as
adversarial training (Mireshghallah et al., 2021) and
personally identifiable information scrubbing (Der-
noncourt et al., 2017) can also be used, however,
they do not provide the worst-case guarantees that
differential privacy does (Brown et al., 2022).
Acknowledgements
The authors would like to thank the anonymous
reviewers and meta-reviewers for their helpful
feedback. We also thank our colleagues at the
UCSD Berg Lab and NUS for their helpful
comments and feedback.
References834083418342
A Appendix
A.1 Notations
To summarize and clarify the notations used in the
paper for explaining our attack, we added Table 8.
A.2 Detailed Experimental Setup
A.2.1 Code and Data Access
We use two datasets in this paper, MIMIC-
III and i2b2, both of which contain sen-
sitive data and can only be accessed
by request through https://mimic.
mit.edu/docs/gettingstarted/
and https://portal.dbmi.hms.
harvard.edu/projects/n2c2-nlp/ ,
and after agreeing to the data usage and confidential-
ity termsand passing proper training for ethical and
privacy-preserving use of the data. For reproduction
of our results, code will be made available only by re-
quest and for research purposes, only to researchers
who provide proof of authorized access to the
datasets (by forwarding the access granted emails
from MIMIC-III and i2b2 to the first author).
A.2.2 Datasets
We run our attack on two sets of target samples, one
we denote by “MIMIC” and the other by “i2b2” in
the results (both medical notes). For both of these,
the “members” portion of the target samples is from
the training set ( D) of our target models, which is
the MIMIC-III dataset. However, the non-members
are different. For the results shown under MIMIC,
the non-members are a held-out subset of the
MIMIC data that was not used in training. For i2b2,
the non-members are from a different (but similar)
dataset, i2b2. Below we elaborate on this setup and
each of these datasets.
MIMIC-III The target models we attack
are trained (by Lehman et al.) on the pseudo
re-identified MIMIC III notes which consist of1,247,291 electronic health records (EHR) of
46,520patients. These records have been altered
such that the original first and last names are
replaced with new first and last names sampled from
the US Census data. Only 27,906of these patients
had their names explicitly mentioned in the EHR.
For the attack’s target data, we use a held-out
subset of MIMIC-III consisting of 89patients
(4072 sample sequences) whose data was not used
during training of the ClinicalBERT target models
and use them as “non-member” samples. For
“member” samples, we take a 99patient subsample
of the entire training data, and then subsample 4072
sample sequences from that (we do this 10times
for each attack and average the results over), so that
the number of member and non-member samples
are the same and the target pool is balanced.
i2b2 This dataset was curated for the i2b2
de-identification of protected health information
(PHI) challenge in 2014 (Stubbs and Özlem
Uzuner, 2015). We use this dataset as a secondary
non-member dataset, since it is similar in domain
to MIMIC-III (both are medical notes), is larger in
terms of size than the held-out MIMIC-III set, and
has not been used as training data for our models.
We subsample 99patients from i2b2, consisting of
18561 sequences, and use them as non-members.
The population data that we use to evaluate
the distribution of likelihood ratio over the null
hypothesis (which is used to compute the threshold)
is disjoint with the non-member set that we use to
evaluate the attack. We randomly select 99patients
from the i2b2 dataset for this purpose.
A.2.3 Target Models
We perform our attack on 5different pre-trained
models, that are all trained on MIMIC-III, but with
different training procedures:
ClinicalBERT (Base) BERT-base architecture
trained over the pseudo re-identified MIMIC-III
notes for 300k iterations for sequence length 128
and for 100k iterations for sequence length 512.
ClinicalBERT++ (Base++) BERT-base architec-
ture trained over the pseudo re-identified MIMIC III
notes for 1M iterations at a sequence length of 128.
ClinicalBERT-Large (Large). BERT-large
architecture trained over the pseudo re-identified
MIMIC-III notes for 300k iterations for sequence
length 128 and for 100k iterations for sequence
length 512.8343ClinicalBERT-large++ (Large++) BERT-large
architecture trained over the pseudo re-identified
MIMIC III notes for 1M iterations at a sequence
length of 128.
ClinicalBERT-b ((Base-b), Name Insertion).
Same training and architecture as ClincalBERT-
base, but that the patient’s surrogate name is
prepended to the beginning of every sentence.
This model is used to identify the effect of name-
insertion on the memorization of BERT-based
models (Lehman et al., 2021).
A.2.4 Computational Resources
For this paper, we did not train any models, so the
GPU training time is 0hours. However, for getting
the likelihoods, we ran inference on the sequences
in our target samples pool. For that, we used an
RTX2080 GPU with 11GB of memory for 18 hours.
A.3 Further Studies
A.3.1 Lower False Positive Rate
All the threshold-dependant results (precision and
recalls) in Section 4 are reported with a threshold
set for having α=10% false positive rate (using the
mechanism shown in Figure 2a). In this section, we
want to look at lower false positive rates, like we do
in Figure 5, and see how well our attack does when
precision is very important to us and we do not want
to get any false positives. These results are shown in
Table 10. (This table corresponds with Table 4 from
Section 4.2 and the sample-level part of Table 6.)
We can see that compared to Table 10, as we are
decreasing the false positive rate, the performance
gap between our attack and the baseline increases
drastically, showin that our attack performs really
well under tight false positive rate constraints. Note
that AUC is not threshold dependant therefore it has
the same value in both tables.
A.3.2 Using Normalized Energy
In the paper, we use E(s;θ), as shown in Equation 2
for finding the likelihood ratio. In other words,
for finding the likelihood ratio, we basically
calculate the loss of the target model and reference
model (using 15% masking and averaging over 10
times) on the given sequence s, and subtract them.
However, another way to approach this problem of
calculating likelihood ratio is to use the normalized
energy (instead of the loss) as introduced in (Goyal
et al., 2022), instead of the loss. For calculating
the normalized energy, we mask each token in the
sequence, one token at a time (instead of 15%), cal-
culated the loss, and average over all the tokens. Tosee how this energy does, we have used it to mount
our attack, and we show the results in Table 11.
Compared to using loss (Table 9) it seems like
the AUC for normalized energy is higher overall,
A.3.3 Results for Short Sequences
All the results in Section 4 are reported for long
sequences (more than 20 tokens long), except those
reported in Table 3, where we ablate the privacy risks
for sequences of different lengths. In this section,
for the sake of completion, we are reporting results
for short sequences as well as long sequences, for all
the five models we study. These results are shown in
Table 9. (This table corresponds with Table 4 from
Section 4.2 and the sample-level part of Table 6.)
A.3.4 Qualitative Comparison with Baseline
Figures 3a and 3b show histogram visualizations
ofL(s)(likelihood ratio statistic) and model loss,
over the target sample pool (i.e. mixture of members
and non-members from the MIMIC dataset), respec-
tively. Here the target model is ClinicalBERT-Base.
The blue line represents a target query sample drawn
from the member set. The point of these visualiza-
tions is to show a case in which a sample is misclassi-
fied as a non-member by the model loss baseline, but
is correctly classified as a member using our attack.
The member and non-member distributions’
histograms are shown via light blue and light
orange bars respective. The red line indicates the
threshold that is selected such that α=0.1, i.e.10%
false positive rate. In Figure 3a we see a distinct
separation between the member and non-member
histogram distributions when we use L(s)as the
test criterion for our attack. This results in the
estimation of a useful threshold that correctly
classifies the blue line sample as a member. In
contrast, the baseline attack by (Jagannatha et al.,
2021), in Figure 3b based solely upon the target
model’s loss leads to a high overlap between
the member and non-member histograms which
leads to a threshold that misclassifies the sample
represented by the blue line. These histograms
show that the reference model used in our method
helps in getting a sense of how hard each sample
is in general, and puts each point in perspective.
A.3.5 ROC Curve Magnified
In Figure 5 We have plotted Figure 4 from the
results, but with logarithmic x-axis, to zoom in on
the low false positive rate section and really show
the differences between our attack and the baseline.8344A.4 Extended Related Works
Since our work proposes an attack for quantifying
leakage of masked language models (MLMs),
based on the likelihood ratio, there are two lines of
work that are related to ours: (1) work surrounding
attacks/leakage on machine learning models
(2) work on calculating sequence likelihood for
MLMs. Prior work on measuring memorization
and leakage in machine learning models, and
specifically NLP models can itself be classified into
two main categories: (1) membership inference
attacks and (2) training data extraction attacks.
Below we discuss each line of work in more detail.
Membership inference. Membership Inference
Attacks (MIA) try to determine whether or
not a target sample was used in training a target
model (Shokri et al., 2017; Yeom et al., 2018). These
attacks be seen as privacy risk analysis tools (Mu-
rakonda and Shokri, 2020; Nasr et al., 2021;
Kandpal et al., 2022), which help reveal how much
the model has memorized the individual samples in8345
its training set, and what the risk of individual users
is (Nasr et al., 2019; Long et al., 2017; Salem et al.,
2018; Ye et al., 2021; Carlini et al., 2021a) A group
of these attacks rely on behavior of shadow models
(models trained on data similar to training, to mimic
the target model) to determine the membership of
given samples (Jayaraman et al., 2021; Shokri et al.,
2017). In the shadow model training procedure the
adversary trains a batch of models m,m,...,mas
shadow models, with data from the target user. Then,
it trains m,m...,mwithout the data from the tar-
get user and then tries to find some statistical dispar-
ity between these models (Mahloujifar et al., 2021).
Shadow-based attacks have been mounted on NLP
models as well: (Song and Shmatikov, 2018) mounts
such an attack on LSTM-based text-generation
models, (Mahloujifar et al., 2021) mounts one on
word embedding, (Hisamoto et al., 2020) applies
it to machine translation and more recently, (She-
jwalkar et al., 2021) mounts it on transformer-based
NLP classification models. Mounting such attacks
is usually costly, as their success relies upon
training multiple shadow models, and access to
adequate shadow data for training such models.
Another group of MIAs relies solely on the loss
value of the target sample, under the target model,
and thresholds this loss to determine member-
ship (Jagannatha et al., 2021; Yeom et al., 2018).
Song and Raghunathan mount such an attack on
word embedding, where they try to infer if given
samples were used in training different embedding
models. Jagannatha et al., which is the work closest
to ours, uses a thresholding loss-based attack to infer
membership on MLMs. Although our proposed
attack is also a threshold-based one, it is differentfrom prior work by: (a) applying likelihood ratio
testing using a reference model and (b) calculating
the likelihood through our energy function formu-
lation. These two components cause our attack to
have higher AUC, as shown in the results.
We refer the reader to the framework introduced
by (Ye et al., 2021) that formalizes different
membership inference attacks and compares their
performance on benchmark ML tasks.
Training data extraction. Training data extrac-
tion quantifies the risk of extracting training data
by probing a trained language model (Salem et al.,
2020; Carlini et al., 2019; Zanella-Béguelin et al.,
2020; Carlini et al., 2021b, 2022; Nakamura et al.,
2020). The most prominent of such attacks, on NLP
models is that of Carlini et al. (2021b), where they
take more than half a million samples from different
GPT-2 models, sift through the samples using a
membership inference method to find samples that
are more likely to have been memorized, and finally,
once they have narrowed down the samples to 1800,
they check the web to see if such samples might
have been in the GPT-2 training set. They find
that over 600 of those 1800 samples were verbatim
training samples. Lehman et al. (2021) mount the
same data extraction attack on MLMs, but their
results are somehow inconclusive as to how much
MLMs memorize samples, as only 4% of generated
sentences with a patient’s name also contain one of
their true medical conditions. They also mount other
type of attacks, where they try to extract a person’s
name given their disease, or disease given name, but
in all their attacks, they only use signals from the
target model and consistently find that a frequency-
based baseline (i.e. one that would always guess the8346most frequent name/disease) is more successful.8347