
Orion Weller*
Johns Hopkins UniversityKevin Seppi
Brigham Young UniversityMatt Gardner
Microsoft Semantic Machines
Abstract
Transfer learning (TL) in natural language pro-
cessing (NLP) has seen a surge of interest in re-
cent years, as pre-trained models have shown
an impressive ability to transfer to novel tasks.
Three main strategies have emerged for mak-
ing use of multiple supervised datasets during
ﬁne-tuning: training on an intermediate task
before training on the target task (STILTs), us-
ing multi-task learning (MTL) to train jointly
on a supplementary task and the target task
(pairwise MTL), or simply using MTL to train
jointly on all available datasets (MTL). In
this work, we compare all three TL methods in
a comprehensive analysis on the GLUE dataset
suite. We ﬁnd that there is a simple heuristic
for when to use one of these techniques over
the other: pairwise MTL is better than STILTs
when the target task has fewer instances than
the supporting task and vice versa. We show
that this holds true in more than 92% of ap-
plicable cases on the GLUE dataset and vali-
date this hypothesis with experiments varying
dataset size. The simplicity and effectiveness
of this heuristic is surprising and warrants addi-
tional exploration by the TL community. Fur-
thermore, we ﬁnd that MTLis worse than
the pairwise methods in almost every case. We
hope this study will aid others as they choose
between TL methods for NLP tasks.
1 Introduction
The standard supervised training paradigm in NLP
research is to ﬁne-tune a pre-trained language
model on some target task (Peters et al., 2018; De-
vlin et al., 2018; Raffel et al., 2019; Gururangan
et al., 2020). When additional non-target super-
vised datasets are available during ﬁne-tuning, it is
not always clear how to best make use of the sup-
porting data (Phang et al., 2018, 2020; Liu et al.,
2019b,a; Pruksachatkun et al., 2020a). Althoughthere are an exponential number of ways to com-
bine or alternate between the target and supporting
tasks, three predominant methods have emerged:
(1) ﬁne-tuning on a supporting task and then the tar-
get task consecutively, often called STILTs (Phang
et al., 2018); (2) ﬁne-tuning on a supporting task
and the target task simultaneously (here called pair-
wise multi-task learning, or simply MTL); and (3)
ﬁne-tuning on all Navailable supporting tasks and
the target tasks together (MTL,N > 1).
Application papers that use these methods gener-
ally focus on only one method (Søgaard and Bingel,
2017; Keskar et al., 2019; Glavas and Vuli ´c, 2020;
Sileo et al., 2019; Zhu et al., 2019; Weller et al.,
2020; Xu et al., 2019; Chang and Lu, 2021), while
a limited amount of papers consider running two.
Those that do examine them do so with a limited
number of conﬁgurations: Phang et al. (2018) ex-
amines STILTS and one instance of MTL, Chang-
pinyo et al. (2018); Peng et al. (2020); Schröder
and Biemann (2020) compare MTL with MTL,
and Wang et al. (2018a); Talmor and Berant (2019);
Liu et al. (2019b); Phang et al. (2020) use MTL
and STILTs but not pairwise MTL.
In this work we perform comprehensive experi-
ments using all three methods on the 9 datasets in
the GLUE benchmark (Wang et al., 2018b). We
surprisingly ﬁnd that a simple size heuristic can be
used to determine with more than 92% accuracy
which method to use for a given target and support-
ing task: when the target dataset is larger than the
supporting dataset, STILTS should be used; oth-
erwise, MTL should be used (MTLis almost
universally the worst of the methods in our experi-
ments). To conﬁrm the validity of the size heuristic,
we additionally perform a targeted experiment vary-
ing dataset size for two of the datasets, showing
that there is a crossover point in performance be-
tween the two methods when the dataset sizes are
equal. We believe that this analysis will help NLP
researchers to make better decisions when choosing272
a TL method and will open up future research into
understanding the cause of this heuristic’s success.
2 Experimental Settings
Dataset Suite To conduct this analysis, we
chose to employ the GLUE dataset suite, following
and comparing to previous work in transfer learn-
ing for NLP (Phang et al., 2018; Liu et al., 2019b).
Training Framework We use Huggingface’s
transformers library (Wolf et al., 2019) for access-
ing the pre-trained encoder and for the base training
framework. We extend this framework to combine
multiple tasks into a single PyTorch (Paszke et al.,
2017) dataloader for MTL and STILTs training.
Many previous techniques have been proposed
for how to best perform MTL (Raffel et al., 2019;
Liu et al., 2019b), but a recent paper by Got-
tumukkala et al. (2020) compared the main ap-
proaches and showed that a new dynamic approach
provides the best performance in general. We im-
plement all methods described in their paper and
experimented with several approaches (sampling
by size, uniformity, etc.). Our initial results found
that dynamic sampling was indeed the most effec-
tive on pairwise tasks. Thus, for the remainder
of this paper, our MTL framework uses dynamic
sampling with heterogeneous batch schedules. Forconsistency, we train the STILTs models using the
same code, but include only one task in the dat-
aloader instead of multiple. The MTLsetup uses
the same MTL code, but includes all 9 GLUE tasks.
We train each model on 5 different seeds to con-
trol for randomness (Dodge et al., 2020). For the
STILTs method, we train 5 models with different
seeds on the supporting task and then choose the
best of those models to train with 5 more random
seeds on the target task. For our ﬁnal reported
numbers, we record both the average score and
the standard deviation, comparing the MTL ap-
proach to the STILTs approach with a two-sample
t-test. In total, we train 985 = 360 different
MTL versions of our model, 5 MTLmodels, and
95 + 95 = 90 models in the STILTs setting.
Model We use the DistilRoBERTa model (pre-
trained and distributed from the transformers li-
brary similarly to the DistilBERT model in Sanh
et al. (2019)) for our experiments, due to its strong
performance and efﬁciency compared to the full
model. For details regarding model and compute
parameters, see Appendix A. Our purpose is notto
train the next state-of-the-art model on the GLUE
task and thus the absolute scores are not imme-
diately relevant; our purpose is to show how the
different methods score relative to each other . We
note that we conducted the same analysis in Fig-273
ure 1 for BERT and found the same conclusion (see
Appendix D), showing that our results extend to
other pre-trained transformers.
3 Results
We provide three different analyses: a comparison
of pairwise MTL vs STILTs, experiments varying
dataset size to validate our ﬁndings, and a compari-
son of pairwise approaches vs MTL.
MTL vs STILTs We ﬁrst calculate the abso-
lute score matrices from computing the MTL and
STILTs method on each pair of the GLUE dataset
suite, then subtract the STILTs average score ma-
trix from the MTL one (Figure 1). Thus, this shows
the absolute score gain for using the MTL method
instead of the STILTs method (negative scores in-
dicate that the STILTs method was better, etc.).
However, this matrix does not tell us whether
these differences are statistically signiﬁcant; for
this we use a two-sample t-test to compare the
mean and standard deviation of each method for
a particular cell. Scores that are statistically sig-
niﬁcant are color coded green (if STILTs is better)
or blue (if MTL is better), whereas they are coded
grey if there is no statistically signiﬁcant difference.
We note that although some differences are large
(e.g. a 9 point difference on (WNLI, STS-B)) the
variance of these results is high enough that there
is no statistically signiﬁcant difference between the
STILTs and MTL score distributions.We order the datasets in Figure 1 by size, to
visually illustrate the trend. The number of green
cells in a row is highly correlated with the size of
the dataset represented by that row. For example,
MNLI is the largest and every cell in the MNLI
row is green. QQP is the 2nd largest and every cell
in its row is also green, except for (QQP, MNLI).
The smallest dataset, WNLI, has zero green cells.
We can summarize these results with the follow-
ing size heuristic: MTL is better than STILTs
when the target task has fewer training in-
stances than the supporting task and vice versa.
In fact, if we use this heuristic to predict which
method will be better we ﬁnd that it predicts 49/53
signiﬁcant cells, which is equivalent to 92.5% accu-
racy. To more clearly visualize which cells it fails
to predict accurately, those four cells are indicated
with red text. We note that this approach does not
hold on the cells that have no statistically signiﬁ-
cant difference between the two methods: but for
almost every signiﬁcant cell, it does.
Unfortunately, there is no clear answer to why
those four cells are misclassiﬁed. Three of the four
misclassiﬁed cells come when using the MRPC
dataset as the target task, but there is no obvious
reason why it fails on MRPC. We recognize that
this size heuristic is not an absolute law, but merely
a good heuristic that does so with high accuracy:
there are still other pieces to this puzzle that this
work does not consider, such as dataset similarity.
Dataset Size Experiments In order to validate274
the size heuristic further we conduct controlled
experiments that alter the amount of training data
of the supporting task to be above and below the
target task. We choose to test QNLI primary with
MNLI supporting, as they should be closely related
and thus have the potential to disprove this heuristic.
We subsample data from the supporting task so that
we have a proportion Kof the size of the primary
task (whereK2f1=3;1=2;1;2;3g). By doing so,
we examine whether the size heuristic holds while
explicitly controlling for the supporting task’s size.
Other than dataset size, all experimental parameters
are the same as in the original comparison (§2).
We also test whether these results hold if the size
of the primary dataset is changed (e.g., perhaps
there is something special about the current size
of the QNLI dataset). We take the same pair and
reduce the training set of QNLI in half, varying
MNLI around the new number of instances in the
QNLI training set as above (e.g. 1/3rd, 1/2, etc.).
The results of these two experiments are in Fig-
ure 2. We can see that as the size of the supporting
dataset increases, MTL becomes more effective
than STILTs. Furthermore, we ﬁnd that when both
datasets are equal sizes the two methods are statis-
tically similar, as we would expect from the size
heuristic (Support Task Proportion =1.0).
Thus, the synthetic experiments corroborate our
main ﬁnding; the size heuristic holds even on con-
trolled instances where the size of the training sets
are artiﬁcially manipulated.
Pairwise TL vs MTL We also experiment
with MTLon GLUE (see Appendix B for im-
plementation details). We ﬁnd that the average
pairwise approach consistently outperforms the
MTLmethod, except for the RTE task (Table 1)
and using the best supporting task outperforms
MTLin every case (Pairwise Oracle). Thus, al-
though MTLis conceptually simple, it is not the
best choice w.r.t. the target task score: on a randomdataset simply using STILTs or MTL will likely
perform better. Furthermore, using the size heuris-
tic on the average supplementary task increases the
score by 5 points over MTL(78.3 vs 73.3).
4 Related Work
A large body of recent work (Søgaard and Bingel,
2017; Vu et al., 2020; Bettgenhäuser et al., 2020;
Peng et al., 2020; Poth et al., 2021) exists that ex-
amines when these transfer learning methods are
more effective than simply ﬁne-tuning on the target
task. Oftentimes, these explanations involve recog-
nizing catastrophic forgetting (Phang et al., 2018;
Pruksachatkun et al., 2020b; Wang et al., 2018a)
although recent work has called for them to be re-
examined (Chang and Lu, 2021). This paper is or-
thogonal to those, as we examine when you should
choose MTL or STILTs, rather than when they are
more effective than the standard ﬁne-tuning case
(in fact, these strategies could be combined to pre-
dict transfer and then use the best method). As
our task is different, theoretical explanations for
how these methods work in relation to each other
will need to be explored in future work. Potential
theories suggested by our results are discussed in
Appendix C, and are left to guide those efforts.
5 Conclusion
We examined the three main strategies for transfer
learning in natural language processing: training
on an intermediate supporting task to aid the target
task (STILTs), training on the target and supporting
task simultaneously (MTL), or training on multiple
supporting tasks alongside the target task (MTL).
We provide the ﬁrst comprehensive comparison be-
tween these three methods using the GLUE dataset
suite and show that there is a simple rule for when
to use one of these techniques over the other. This
simple heuristic, which holds true in more than 92%
of applicable cases, states that multi-task learning275is better than intermediate ﬁne tuning when the
target task is smaller than the supporting task and
vice versa. Additionally, we showed that these pair-
wise transfer learning techniques outperform the
MTLapproach in almost every case.
References276277
A Training and Compute Details
We use the hyperparameters given by the trans-
former library example on GLUE as the default
for our model (learning rate of 2e-5, batch size of
128, AdamW optimizer (Kingma and Ba, 2014),
etc.). We train for 10 epochs, checkpointing every
half an epoch and use the best model on the de-
velopment set for the test set scores. We train on
a mix of approximately 10 K80 and P100 GPUs
for approximately two weeks for the main experi-
ment, using another week of compute time for the
synthetic experiments (§3). Our CPUs use 12-core
Intel Haswell (2.3 GHz) processors with 32GB of
RAM.
B Pairwise Approaches vs MTL
Experimental Setup We use MTLwith three
different sampling methods: uniform sampling,
sampling by dataset size, and dynamic sampling.
To illustrate the difference between MTLand
the pairwise methods, we show the average score
across all supplementary tasks for MTL and
STILTs. We also show the average score found
by choosing MTL or STILTs using the size heuris-
tic as Ave. S.H. . Finally, we report the score from
the best task using the best pairwise method, which
we call the Pairwise Oracle . The results are shown
in Table 2.
Results Although dynamic sampling was more
effective for the pairwise tasks, we ﬁnd that dy-
namic sampling was worse than sampling by size
when using MTL on all nine datasets (top half of
Table 2).However, when the MTLmethod is compared
to the pairwise methods, it does not perform as well
(bottom half of Table 2). We see that the Pairwise
Oracle, which uses the best supplementary task for
the given target task, outperforms all methods by
a large margin. Thus, although MTLis concep-
tually simple, it is not the best choice with respect
to target task accuracy. Furthermore, if you could
predict which supplementary task would be most
effective (Pairwise Oracle, c.f. Section 4, Vu et al.
(2020); Poth et al. (2021), etc.), you would be able
to make even larger gains over MTL.
C Theories for Transfer Effectiveness
Previous work often invokes ideas such as catas-
trophic forgetting to describe why STILTs or MTL
does or does not improve over the basic ﬁne-tuning
case (Phang et al., 2018; Pruksachatkun et al.,
2020b; Wang et al., 2018a). However, as our work
provides a novel comparison of MTL vs. STILTs
there exists no previous work that shows how these
methods differ in any practical or theoretical terms
(e.g. does MTL or STILTs cause more catastrophic
forgetting of the target task). Furthermore, previous
explanations for why the STILTs method works has
been called into question (Chang and Lu, 2021),
leaving it an open research area.
A naive explanation for our task would be to
think that when the target task is larger, STILTs
should be worse because of catastrophic forgetting,
whereas MTL would still have access to the sup-
porting task. However, for STILTs this catastrophic
forgetting would mainly effect the supporting task
performance, not the target task performance, mak-
ing that explanation unlikely in some contexts (e.g.
when the tasks are not closely related). One poten-
tial explanation based on our results is that a small
supporting task is best used to provide a good ini-278tialization for a larger target task (e.g. STILTs)
while a large supporting task used for initialization
would change the weights too much for the small
target task to use effectively (thus making MTL the
more effective strategy for a larger supporting task).
Another explanation could be that a larger target
task does not beneﬁt from MTL (and perhaps is
harmed by it, e.g. catastrophic interference) and
therefore, STILTs is more effective - while MTL is
more effective for small target tasks. However, all
of these explanations also fail to take into account
task relatedness, which likely also plays a role in
the theoretical explanation (although even that too,
has been called into question with Chang and Lu
(2021)).
We thus note that there are a myriad of possi-
ble explanations (and the answer is likely a com-
plex combination of possible explanations), but
these are out of the scope of this work. Our work
aims to show what happens in practice, rather than
proposing a theoretical framework. As theoretical
explanations for transfer learning are still an active
area of research, we leave them to future work and
provide this empirical comparison to guide their
efforts and the current efforts of NLP researchers
and practitioners.
D Alternate Model: BERT
We conduct the same analysis as Figure 1 with
the BERT model and ﬁnd similar results (Figure 3,
thus showing that our results transfer to other pre-
trained transformer models. We follow previous
work in using two different pre-trained models for
our analysis (Talmor and Berant, 2019; Phang et al.,
2018).
E Additional Background Discussion
In this section we will show how the size heuristic
is supported by and helps explain the results of
previous work in this area. Although this section
is not crucial to the main result of our work, we
include it to help readers who may not be as
familiar with the related work . We examine two
works in depth and then discuss broader themes of
related work.
BERT on STILTs Phang et al. (2018) This
work deﬁned the acronym STILTs, or Supplemen-
tary Training on Intermediate Labeled-data Tasks ,
which has been an inﬂuential idea in the commu-
nity (V oskarides et al., 2019; Yan et al., 2020; ClarkModel RTE accuracy
GPT!RTE 54.2
GPT!MNLI!RTE 70.4
GPT!{MNLI, RTE} 68.6
GPT!{MNLI, RTE}!RTE 67.5
et al., 2020). To determine the effect of the inter-
mediate training, the authors computed the STILTs
matrix of each pair in the GLUE dataset. As our
model and training framework are different from
their methodology, we cannot compare our matrix
with the absolute numbers in their matrix. However,
at the end of Section 4 in their paper, they conduct
an experiment with MTL and compare the results
to their STILTs matrix (their experimental results
are reproduced in Table 3 for convenience). Their
analysis uses MNLI as the supporting task and RTE
as the target task, trying MTL, STILTs, MTL+ﬁne-
tuning, and only ﬁne-tuning on RTE. Their results
show that STILTs provides the highest score, with
all MTL varieties being worse. From this they con-
clude that MTL is worse than STILTs.
How does this compare to our results? In Fig-
ure 1 we see that our results also show that the
STILTs method is better than the MTL method for
the (RTE, MNLI) pair, showing that our results are
consistent with those in the literature. Furthermore,
we ﬁnd that this is one of the 4 signiﬁcant cells
in our matrix where the size heuristic does not ac-
curately predict the best method. It is unfortunate
that the task they decided to pick happened to be
one of the anomalies. Thus, our paper extends and
completes their results with more rigor.
MultiQA Talmor and Berant (2019) MultiQA
showed that using MTL on a variety of question-
answering (QA) datasets made it possible to train
a model that could outperform the current SOTA
on those QA datasets. They used an interesting
approach to MTL, pulling 15k examples from each
of the 5 major datasets to compose one new “MTL"
task, called Multi-75K. They then show results for
STILTs transfer on those same datasets along with
the MTL dataset (their data is reproduced with
new emphasis in Appendix E Table 4 for conve-279
nience). We note that this STILTs-like transfer
with the “MTL" dataset is an equivalent method
to doing MTL and then ﬁne-tuning on the target
task, reminiscent of the third example in Phang et al.
(2018) (Table 3, GPT !{MNLI, RTE}!RTE, c.f.
Appendix E).
How does this relate to our results? The size
heuristic says that MTL is better than STILTs when
the target task has fewer training instances. In
the MultiQA paper the size of each training set is
artiﬁcially controlled to be the same number (75k
instances), thus our size heuristic would say that the
methods should be comparable. Although no error
bounds or standard deviations are reported in their
paper (which makes the exact comparison difﬁcult),
we see that the MTL approach performs equal or
better on almost half of the datasets. Thus, although
the MultiQA paper is not strictly comparable to our
work due to their training setup (the MTL+ﬁne
tuning), their results agree with our hypothesis as
well.
For convenience, Table 4 from Talmor and Be-rant (2019) is reproduced here in the appendix.
The top half contains the results using the DocQA
model while the bottom half uses BERT. Note that
both model’s Multi-75K scores perform approxi-
mately similar to the STILTs methods, which is
expected given that they are the same size. TQA-
G and TQA-W come from the same dataset. As
stated in the body of this paper, no standard devi-
ation is reported in the MultiQA paper and thus it
is hard to know whether the difference in results
are statistically signiﬁcant. Even if all results were
statistically signiﬁcant, which is highly unlikely,
each of the Multi-75K models perform equal or
better on 2 of the 6 tasks, which is not statistically
different from random.
Combining All Tasks Our results using MTL
showed that although MTLis conceptually easy
(just put all the datasets together) it does not lead
to the best performance. We ﬁnd similar results in
Wang et al. (2018a), where in their Table 3 they
show that the STILTs approach outperforms the280SQuAD NewsQA SearchQA TQA-G TQA-W HotpotQA
SQuAD - 33.3 39.2 49.2 34.5 17.8
NewsQA 59.6 - 41.6 44.2 33.9 16.5
SearchQA 57 31.4 - 57.5 39.6 19.2
TQA-G 57.7 31.8 49.5 - 41.4 19.1
TQA-W 57.6 31.7 44.4 50.7 - 17.2
HotpotQA 59.8 32.4 46.3 54.6 37.4 -
Multi-75K 59.8 33.0 47.5 56.4 40.4 19.2
SQuAD - 41.2 47.8 55.2 45.4 20.8
NewsQA 72.1 - 47.4 55.9 45.2 20.6
SearchQA 70.2 40.2 - 57.3 45.5 20.4
TQA-G 69.9 41.2 50.0 - 46.2 20.8
TQA-W 71.0 39.2 48.4 55.7 - 20.9
HotpotQA 71.2 39.5 48.6 56.6 45.6 -
Multi-75K 71.5 42.1 48.5 56.6 46.5 20.4
MTLapproach for all but one task. Additionally,
in the follow up work from the initial STILTs paper
(Phang et al., 2020) they ﬁnd that although MTL
has a slightly higher average performance in the
cross-lingual setting, it is worse than the pairwise
approach in 75% of the evaluated tasks.
The current literature (and our work) seems to
suggest that naively combining as many tasks as
possible may not be the best approach. However,
more work is needed to understand the training
dynamics of MTL.
Combining Helpful Tasks In this paper, we
only examine the difference between pairwise
MTL, STILTs or MTL, due to time and space.
Although it is possible that our heuristic may ex-
trapolate to transfer learning with more than two
tasks, computing the power set of the possible task
combinations for MTL and STILTs would be ex-
tremely time and resource intensive. We leave it to
future work to examine how the size heuristic may
hold when using more than two datasets at a time.
Additionally, there may be further value in com-
puting this power set: Changpinyo et al. (2018)
showed that taking the pairwise tasks that proved
beneﬁcial in pairwise MTL and combining them
into a larger MTL set (an “Oracle" set) oftentimes
provides higher scores than pairwise MTL. Explor-
ing which subsets of tasks provide the best transfer
with which method would be valuable future work.Dataset Size in TL Dataset size has been used
often in transfer learning techniques (Søgaard and
Bingel, 2017; Pruksachatkun et al., 2020a; Poth
et al., 2021). Our size heuristic, although related,
focuses on a different problem: whether to use
MTL or STILTs. Thus, our work provides addi-
tional insight into how the size of the dataset is
important for transfer learning.
Fine-tuning after MTL Many papers that use
MTLalso perform some sort of ﬁne-tuning af-
ter the MTL phase. Since ﬁne-tuning after MTL
makes the MTL phase an intermediate step, it essen-
tial combines the STILTs and MTL methods into a
single STILTs-like method. However, whether ﬁne-
tuning after MTL is better than simply MTL is still
controversial: for example, Liu et al. (2019b), Raf-
fel et al. (2019), and Talmor and Berant (2019) say
that ﬁne-tuning after MTL helps but Lourie et al.
(2021) and Phang et al. (2018) say that it doesn’t.
However, Raffel et al. (2019) is the only one whose
experiments include multiple random seeds, giving
more credence to their results. However, due to the
difference of opinion it is unclear which method is
actually better; we leave this to future work.
F GLUE Dataset Sizes and References
To give credit to the original authors and to provide
the exact sizes, we provide Table 5.281282