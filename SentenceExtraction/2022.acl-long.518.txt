
Raphael Schumann
Computational Linguistics
Heidelberg University, GermanyStefan Riezler
Computational Linguistics & IWR
Heidelberg University, Germany
{rschuman|riezler}@cl.uni-heidelberg.de
Abstract
Vision and language navigation (VLN) is a
challenging visually-grounded language un-
derstanding task. Given a natural language
navigation instruction, a visual agent interacts
with a graph-based environment equipped with
panorama images and tries to follow the de-
scribed route. Most prior work has been con-
ducted in indoor scenarios where best results
were obtained for navigation on routes that are
similar to the training routes, with sharp drops
in performance when testing on unseen environ-
ments. We focus on VLN in outdoor scenarios
and find that in contrast to indoor VLN, most
of the gain in outdoor VLN on unseen data is
due to features like junction type embedding or
heading delta that are specific to the respective
environment graph, while image information
plays a very minor role in generalizing VLN to
unseen outdoor areas. These findings show a
bias to specifics of graph representations of ur-
ban environments, demanding that VLN tasks
grow in scale and diversity of geographical en-
vironments.
1 Introduction
Vision and language navigation (VLN) is a chal-
lenging task that requires the agent to process nat-
ural language instructions and ground them in a
visual environment. The agent is embodied in the
environment and receives navigation instructions.
Based on the instructions, the observed surround-
ings, and the current trajectory the agent decides
its next action. Executing this action changes the
position and/or heading of the agent within the en-
vironment, and eventually the agent follows the
described route and stops at the desired goal loca-
tion. The most common evaluation metric in VLN
is the proportion of successful agent navigations,
called task completion (TC).While early work on grounded navigation was
confined to grid-world scenarios (MacMahon et al.,
2006; Chen and Mooney, 2011), recent work has
studied VLN in outdoor environment consisting
of real-world urban street layouts and correspond-
ing panorama pictures (Chen et al., 2019). Recent
agent models for outdoor VLN treat the task as a
sequence-to-sequence problem where the instruc-
tions text is the input and the output is a sequence
of actions (Chen et al., 2019; Xiang et al., 2020;
Zhu et al., 2021b). In contrast to indoor VLN (An-
derson et al., 2018; Ku et al., 2020), these works
only consider a seen scenario , i.e., the agent is
tested on routes that are located in the same area
as the training routes. However, studies of indoor
VLN (Zhang et al., 2020) show a significant per-
formance drop when testing in previously unseen
areas.
The main goal of our work is to study outdoor
VLN in unseen areas , pursuing the research ques-
tion of which representations of an environment
and of instructions an agent needs to succeed at this
task. We compare existing approaches to a new ap-
proach that utilizes features based on the observed
environment graph to improve generalization to un-
seen areas. The first feature, called junction type
embedding, encodes the number of outgoing edges
at the current agent position; the second feature,
called heading delta, encodes the agent’s heading
change relative to the previous timestep. As our
experimental studies show, representations of full
images do not contribute very much to successful
VLN in outdoor scenarios beyond these two fea-
tures. One reason why restricted features encoding
junction type and heading delta are successful in
this task is that they seem to be sufficient to en-
code peculiarities of the graph representation of the
environments. Another reason is the current restric-
tion of outdoor environments to small urban areas.
In our case, one dataset is the widely used Touch-
down dataset introduced by Chen et al. (2019), the7519other dataset is called map2seq and has recently
been introduced by Schumann and Riezler (2021).
The map2seq dataset was created for the task of
navigation instructions generation but can directly
be adopted to VLN. We conduct a detailed analy-
sis of the influence of general neural architectures,
specific features such as junction type or heading
delta, the role of image information and instruction
token types, to outdoor VLN in seen and unseen
environments on these two datasets.
Our specific findings unravel the contributions of
these features on several VLN subtasks such as ori-
entation, directions, stopping. Our general finding
is that current outdoor VLN suffers a bias towards
urban environments and to artifacts of their graph
representation, showing the necessity of more di-
verse datasets and tasks for outdoor VLN.
Our main contributions are the following:
•We describe a straightforward agent model
that achieves state-of-the-art task completion
and is used as a basis for our experiments.
•We introduce the unseen scenario for outdoor
VLN and propose two environment-dependent
features to improve generalization in that set-
ting.
•We compare different visual representations
and conduct language masking experiments
to study the effect in the unseen scenario.
•We adopt the map2seq dataset to VLN and
show that merging it with Touchdown im-
proves performance on the respective test sets.
2 VLN Problem Definition
The goal of the agent is to follow a route and stop
at the desired target location based on natural lan-
guage navigation instructions. The environment is
a directed graph with nodes v∈Vand labeled
edges (u, v)∈E. Each node is associated with a
360panorama image pand each edge is labeled
with an angle α. The agent state s∈Scon-
sists of a node and the angle at which the agent
is heading: (v, α|u∈N), where N
are all outgoing neighbors of node v. The agent
can navigate the environment by performing an ac-
tiona∈ {FORWARD ,LEFT,RIGHT ,STOP}at each
timestep t. The FORWARD action moves the agent
from state (v, α)to(u, α), where (u, u)
is the edge with an angle closest to α. The
RIGHT andLEFT action rotates the agent towards
the closest edge angle in clockwise or counterclock-
wise direction, respectively: (v, α). Given a
starting state sand instructions text x, the agent
performs a series of actions a, ..., auntil the
STOP action is predicted. If the agent stops within
one neighboring node of the desired target node
(goal location), the navigation was successful. The
described environment and location finding task
was first introduced by (Chen et al., 2019) and we
will also refer to it as "outdoor VLN task" through-
out this paper.
3 Model Architecture
In this section we introduce the model that we use
to analyze navigation performance in the unseen
and seen scenario for outdoor VLN. The architec-
ture is inspired by the cross-modal attention model
for indoor VLN (Krantz et al., 2020). First we give
a high level overview of the model architecture and
rough intuition. Afterwards we provide a more7520formal description.
As depicted in Figure 1, the model follows a
sequence-to-sequence architecture where the input
sequence is the navigation instructions text and the
output is a sequence of agent actions. At each de-
coding timestep, a new visual representation of the
current agent state within the environment is com-
puted, where the agent state is dependent on the
previously predicted actions. The decoder RNN
has two layers where the first encodes metadata
and a visual representation. The second RNN layer
encodes a contextualized text and visual represen-
tation and eventually predicts the next action.
The intuition behind the model architecture is to
firstly accumulate plain observations available at
the current timestep and entangle them with previ-
ous observations in the first recurrent layer. Based
on these observations, the model focuses attention
to certain parts of the instructions text and visual
features which are again entangled in the second
recurrent layer. Thus, we use the acronym ORAR
(observation-recurrence attention-recurrence) for
the model.
In detail, the instructions encoder embeds and
encodes the tokens in the navigation instructions se-
quence x=x, ..., xusing a bidirectional LSTM
(Graves et al., 2005):
ˆx=embedding (x)
((w, ..., w), z) =Bi-LSTM (ˆx, ...,ˆx),
where w, ..., ware the hidden representations for
each token and zis the last LSTM cell state. The
visual encoder, described in detail below, emits a
fixed size representation ¯pof the current panorama
view and a sequence of sliced view representations
¯p, ...,¯p. The state z of the cell in the first
decoder LSTM layer is initialized using z. The
input to the first decoder layer is the concatenation
(⊕) of visual representation ¯p, previous action em-
bedding ¯a, junction type embedding ¯n, and
heading delta d. The output of the first decoder
layer,
h=LSTM([¯a⊕¯n⊕d⊕¯p]),
is then used as the query of multi-head atten-
tion (Vaswani et al., 2017) over the text encoder.
The resulting contextualized text representation c
is then used to attend over the sliced visual repre-
sentations:
c=MultiHeadAttention (h,(w, ..., w))
c=MultiHeadAttention (c,(¯p, ...,¯p)).The input and output of the second decoder layer
are
h =LSTM([¯t⊕h⊕c⊕c]),
where ¯tis the embedded timestep t. The hidden rep-
resentation h of the second decoder LSTM
layer is then passed through a feed forward network
to predict the next agent action a.
3.1 Visual Encoder
At each timestep tthe panorama at the current
agent position is represented by extracted visual
features. We slice the panorama into eight pro-
jected rectangles with 60field of view, such that
one of the slices aligns with the agent’s heading.
This centering slice and the two left and right
of it are fed into a ResNet pretrainedon Ima-
geNet (Russakovsky et al., 2015). We consider
two variants of ResNet derived panorama features.
One variant extracts low level features from the
fourth to last layer ( 4th-to-last ) of a pretrained
ResNet-18 and concatenates each slice’s feature
map along the width dimension, averages the 128
CNN filters and cuts out 100 dimensions around
the agents heading. This results in a feature matrix
of100×100(¯p, ...,¯p). The full procedure is
described in detail in Chen et al. (2019) and Zhu
et al. (2021b). The other variant extracts high level
features from a pretrained ResNet-50’s pre-final
layer for each of the 5 slices: ¯p, ...,¯p. Each slice
vector ¯pis of size 2,048resulting in roughly the
same number of extracted ResNet features for both
variants, making a fair comparison. Further, we use
thesemantic segmentation representation of the
panorama images. We employ omnidirectional se-
mantic segmentation (Yang et al., 2020) to classify
each pixel by one of the 25 classes of the Map-
illary Vistas dataset (Neuhold et al., 2017). The
classes include e.g. car, truck, traffic light, vegeta-
tion, road, sidewalk. See Figure 1 bottom right for
a visualization. Each panorama slice ( ¯p, ...,¯p) is
then represented by a 25 dimensional vector where
each value is the normalized area covered by the
corresponding class (Zhang et al., 2020). For either
feature extraction method, the fixed sized panorama
representation ¯pis computed by concatenating the
slice features ¯p, ...,¯pand passing them to a feed
forward network.7521
3.2 Junction Type Embedding
The junction type embedding is a feature that we
introduce to better analyze generalization to unseen
areas. It embeds the number of outgoing edges of
the current environment node and is categorized
into {2, 3, 4, >4}. It provides the agent information
about the type of junction it is positioned on: a
regular street segment, a three-way intersection, a
four way intersection or an intersection with more
than four outgoing streets. We want to point out
that the number of outgoing edges isn’t oracle infor-
mation in the environment described in Section 2.
The agent can rotate left until the same panorama
view is observed and thus counting the number
of outgoing edges by purely interacting with the
environment. But it is clear that the feature lever-
ages the fact that the environment is based on a
graph and it would not be available in a continuous
setting (Krantz et al., 2020).
3.3 Heading Delta
As described in Section 2, the environment defined
and implemented by Chen et al. (2019) only al-
lows states where the agent is heading towards
an outgoing edge. As a consequence the environ-
ment automatically rotates the agent towards the
closest outgoing edge after transitioning to a new
node. The environment behavior is depicted in Fig-ure 2a) for a transition between two regular street
segments. However, as depicted in Figure 2b), a
problem arises when the agent is walking towards
a three-way intersection. The automatic rotation
introduces unpredictable behavior for the agent and
we hypothesis that it hinders generalization to un-
seen areas. To correct for this environment artifact,
we introduce the heading delta feature dwhich
encodes the change in heading direction relative to
the previous timestep. The feature is normalized
to(−1,1]where a negative value indicates a left ro-
tation and a positive value indicates a right rotation.
The magnitude signals the degree of the rotation
up to 180.
4 Data
We use the Touchdown (Chen et al., 2019) and the
map2seq (Schumann and Riezler, 2021) datasets
in our experiments. Both datasets contain human
written navigation instructions for routes located in
the same environment. The environment consists
of 29,641 panorama images from Manhattan and
the corresponding connectivity graph.
4.1 Touchdown
The Touchdown dataset (Chen et al., 2019) for vi-
sion and language navigation consists of 9,326
routes paired with human written navigation in-
structions. The annotators navigated the panorama
environment based on a predefined route and wrote
down navigation instructions along the way.
4.2 Map2seq
The map2seq (Schumann and Riezler, 2021)
dataset was created for the task of navigation in-
structions generation. The 7,672 navigation instruc-
tions were written by human annotators who saw a
route on a rendered map, without the corresponding
panorama images. The annotators were told to in-
clude visual landmarks like stores, parks, churches,
and other amenities into their instructions. A differ-
ent annotator later validated the written navigation
instructions by using them to follow the described
route in the panorama environment (without the
map). This annotation procedure allows us to use
the navigation instructions in the map2seq dataset
for the vision and language navigation task. We are
the first to report VLN results on this dataset.
4.3 Comparison
Despite being located in the same environment, the
routes and instructions from each dataset differ in7522multiple aspects. The map2seq instructions typi-
cally include named entities like store names, while
Touchdown instructions focus more on visual fea-
tures like the color of a store. Both do not include
street names or cardinal directions and are written
in egocentric perspective. Further, in map2seq the
agent starts by facing in the correct direction, while
in Touchdown the initial heading is random and
the first part of the instruction is about orientating
the agent ("Turn around such that the scaffolding
is on your right"). A route in map2seq includes a
minimum of three intersections and is the shortest
path from the start to the end location.In Touch-
down there are no such constraints and a route can
almost be circular. The routes in both datasets are
around 35-45 nodes long with some shorter outliers
in Touchdown. On average instructions are around
55 tokens long in map2seq and around 89 tokens
long in Touchdown.
5 Experiments
We are interested in the generalization ability to
unseen areas and how it is influenced by the two
proposed features, types of visual representation,
navigation instructions and training set size. Along-
side of the results in the unseen scenario, we report
results in the seen scenario to interpret performance
improvements in relation to each other. All exper-
imentsare repeated ten times with different ran-
dom seeds. The reported numbers are the average
over the ten repetitions. Results printed in bold
are significantly better than non-bold results in the
same column. Significance was established by a
paired t-teston the ten repetition results and a
p-value ≤0.05without multiple hypothesis cor-
rections factor. Individual results can be found in
the Appendix.
5.1 Data Splits
To be able to compare our model with previous
work, we use the original training, development
and test split (Chen et al., 2019) for the seen sce-
nario on Touchdown. Because we are the first to
use the map2seq data for VLN we create a new split
for it. The resulting number of instances can be
seen in the left column of Table 1. For the unseen
scenario, we create new splits for both datasets. We
separate the unseen area geographically by drawing
a boundary across lower Manhattan (see Figure 3).
Development and test instances are randomly cho-
sen from within the unseen area. Routes that are
crossing the boundary are discarded. The right col-
umn of Table 1 shows the number of instances for
both splits. Additionally, we merge the two datasets
for both scenarios. This is possible because both
datasets are located in the same environment and
the unseen boundary is equivalent.
5.2 Training Details
We train the models with Adam (Kingma and Ba,
2015) by minimizing cross entropy loss in the
teacher forcing paradigm. We set the learning rate
to 5e-4, weight decay to 1e-3 and batch size to
64. After 150 epochs we select the model with
the best shortest path distance (SPD) performance
on the development set. We apply dropout of 0.3
after each dense layer and recurrent connection.
The multi-head attention mechanism is regularized7523
by attention dropout of 0.3 and layer normaliza-
tion. The navigation instructions are lower-cased
and split into byte pair encodings (Sennrich et al.,
2016) with a vocabulary of 2,000 tokens and we
use BPE dropout (Provilkov et al., 2020) during
training. The BPE embeddings are of size 32 and
the bidirectional encoder LSTM has two layers of
size 256. The feed forward network in the visual
encoder consists of two dense layers with 512 and
256 neurons, respectively, and 64 neurons in case of
using semantic segmentation features. The embed-
dings that encode previous action, junction type,
and step count are of size 16. The two decoder
LSTM layers are of size 256 and we use two atten-
tion heads. Training the full model takes around 3
hours on a GTX 1080 Ti.
5.3 Model Comparison
We compare the ORAR model to previous works.
Because these works only report results for the
seen scenario on Touchdown, we evaluate those for
which we could acquire the code, on the map2seq
dataset and the unseen scenario. The models RCon-
cat(Mirowski et al., 2018; Chen et al., 2019),
GA(Chaplot et al., 2018; Chen et al., 2019) and
ARC (Xiang et al., 2020) use an LSTM to encode
the instructions text and a single layer decoder
LSTM to predict the next action. They differ in how
the text and image representations are incorporated
during each timestep in the decoder. As the namesuggests, in RConcat the two representations are
concatenated. GAuses gated attention to compute
a fused representation of text and image. ARC uses
the hidden representation of the previous timestep
to attend over the instructions text. This contextu-
alized text representation is then concatenated to
the image representation. They further introduce
ARC+l2s which cascades the action prediction into
a binary stopping decision and a subsequent di-
rection classification. The VLN-Transformer (Zhu
et al., 2021b) uses pretrained BERT (Devlin et al.,
2019) to encode the instructions and VLN-BERT
(Majumdar et al., 2020) to fuse the modalities.
5.4 Metrics
We use task completion ( TC) as the main perfor-
mance metric. It represents the percentage of suc-
cessful agent navigations (Chen et al., 2019). We
further report normalized Dynamic Time Warp-
ing (nDTW ) which quantifies agent and gold trajec-
tory overlap for all routes (Ilharco et al., 2019). The
shortest path distance ( SPD) is measured within the
environment graph from the node the agent stopped
to the goal node (Chen et al., 2019).
6 Results & Analysis
The two upper sections of Table 2 show the results
of the ORAR model introduced in Section 3 in
comparison to other work. While the model sig-7524
nificantly outperforms all previous work on both
datasets, our main focus is analyzing generalization
to the unseen scenario. It is apparent that the type
of image features influences agent performance and
will be discussed in the next section. The bottom
section of Table 2 ablates the proposed heading
delta and junction type features for the best mod-
els. Removing the heading delta feature has little
impact in the seen scenario, but significantly re-
duces task completion in the unseen scenario of
the map2seq dataset. Surprisingly, the feature has
no impact in the unseen scenario of Touchdown.
We believe this is a consequence of the different
data collection processes. Touchdown was specifi-
cally collected for VLN and annotators navigated
the environment graph, while map2seq annotators
wrote instructions only seeing the map. Removing
the junction type embedding leads to a collapse
of task completion in the unseen scenario on both
datasets. This shows that without this explicit fea-
ture, the agent lacks the ability to reliably identify
intersections in new areas.
6.1 Visual Features
Table 3 shows results for different types of visual
features in the unseen scenario. We compare high
level ResNet features (pre-final), low level ResNet
features (4th-to-last), semantic segmentation fea-
tures and using no image features. For the ResNet
based features, the low level 4th-to-last features
perform better than pre-final on both datasets. On
map2seq the no image baseline performs on par
with models that have access to visual features.
When we remove the junction type embedding,
the task completion rate drops significantly, which
shows that the agent is not able to reliably locate
intersections from any type of visual features.
6.2 Sub-task Oracle
The agent has to predict a sequence of actions in
order to successfully reach the goal location. In
Touchdown this task can be divided into three sub-
tasks (see Section 4). First the agent needs to ori-
entate itself towards the correct starting heading.
Next the agent has to predict the correct directions
at the intersections along the path. The third sub-
task is stopping at the specified location. Providing
oracle actions (during testing) for two of the three
sub-tasks lets us look at the completion rate of
the remaining sub-task. Table 4 shows the com-
pletion rates for each of the three sub-tasks when
using ResNet pre-final, 4th-to-last and no image
features. In the seen scenario we can observe that
the pre-final features lead to the best performance
for the directions task. The 4th-to-last features on
the other hand lead to the best orientation task per-
formance and the stopping task is not influenced
by the choice of visual features. In the unseen
scenario 4th-to-last features again provide best ori-
entation task performance but no image features
lead to the best performance for the directions task.
This shows that the ResNet 4th-to-last features are
primarily useful for the orientation sub-task and ex-
plains the discrepancy of the no image baseline on
Touchdown and map2seq identified in the previous
subsection. In the Appendix we use this knowledge
to train a mixed-model that uses 4th-to-last features7525
for the orientation sub-task and pre-final/no image
features for directions and stopping.
6.3 Token Masking
To analyze the importance of direction and object
tokens in the navigation instructions, we run mask-
ing experiments similar to Zhu et al. (2021a), ex-
cept that we mask the tokens during training and
testing instead of during testing only. Figure 4
shows the resulting task completion rates for an
increasing number of masked direction or object
tokens. From the widening gap between masking
object and direction tokens, we can see that the
direction tokens are more important to successfully
reach the goal location. Task completion nearly
doesn’t change when masking object tokens, indi-
cating that they are mostly ignored by the model.
While task completion significantly drops when di-
rection tokens are masked, the agent still performs
on a high level. This finding is surprising and in
dissent with Zhu et al. (2021a) who report that task
completion nearly drops to zero when masking di-
rection tokens during testing only. We believe that
in our setting (masking during testing and train-
ing), the model learns to infer the correct directions
from redundancies in the instructions or context
around the direction tokens. Besides the general
trend of lower performance on the unseen scenario,
we can not identify different utilization of object ordirection tokens in the seen and unseen scenario.
6.4 Merged Datasets
We train the ORAR full model on the merged
dataset (see Section 5.1). Model selection is per-
formed on the merged development set but results
are also reported for the individual test sets of
Touchdown and map2seq. For comparison with
models trained on the non-merged datasets, the
first row of Table 5 shows the best results of Ta-
ble 2. Training on the merged dataset signifi-
cantly improves nDTW and task completion across
both datasets and scenarios. This shows that both
datasets are compatible and the merged dataset can
further be used by the VLN community to evaluate
their models on more diverse navigation instruc-
tions. Despite being trained on twice as many in-
stances, the no image baseline still performs on par
on map2seq unseen. From this we conclude that the
current bottleneck for better generalization to un-
seen areas is the number of panorama images seen
during training instead of number of instructions.
7 Related Work
Natural language instructed navigation of embod-
ied agents has been studied in generated grid en-
vironments that allow a structured representation
of the observed environment (MacMahon et al.,
2006; Chen and Mooney, 2011). Fueled by the ad-
vances in image representation learning (He et al.,
2016), the environments became more realistic by
using real-world panorama images of indoor loca-
tions (Anderson et al., 2018; Ku et al., 2020). Com-
plementary outdoor environments contain street
level panoramas connected by a real-world street
layout (Mirowski et al., 2018; Chen et al., 2019;
Mehta et al., 2020). Agents in this outdoor en-
vironment are trained to follow human written
navigation instructions (Chen et al., 2019; Xiang
et al., 2020), instructions generated by Google
Maps (Hermann et al., 2020), or a combination
of both (Zhu et al., 2021b). Recent work focuses
on analyzing the navigation agents by introduc-
ing better trajectory overlap metrics (Jain et al.,
2019; Ilharco et al., 2019) or diagnosing the perfor-
mance under certain constraints such as uni-modal
inputs (Thomason et al., 2019) and masking direc-
tion or object tokens (Zhu et al., 2021a). Other
work used a trained VLN agent to evaluate auto-
matically generated navigation instructions (Zhao
et al., 2021). An open problem in indoor VLN is7526
the generalization of navigation performance to pre-
viously unseen areas. Proposed solutions include
back translation with environment dropout (Tan
et al., 2019), multi-modal environment representa-
tion (Hu et al., 2019) or semantic segmented im-
ages (Zhang et al., 2020). Notably the latter work
identifies the same problem in the Touchdown task.
8 Conclusion
We presented an investigation of outdoor vision
and language navigation in seen and unseen envi-
ronments. We introduced the heading delta feature
and junction type embedding to correct an arti-
fact of the environment and explicitly model the
number of outgoing edges, respectively. Both are
helpful to boost and analyze performance in the
unseen scenario. We conducted experiments on
two datasets and showed that the considered visual
features poorly generalize to unseen areas. We con-
jecture that VLN tasks need to grow in scale and
diversity of geographical environments and naviga-
tion tasks.
Acknowledgments
The research reported in this paper was supported
by a Google Focused Research Award on "Learn-
ing to Negotiate Answers in Multi-Pass Semantic
Parsing".
References752775287529
A Architecture Ablation
We perform ablation studies on the ORAR full
model in the seen scenario to measure the impact
of individual architecture components. As seen in
Table 6, removing the second decoder RNN layer
or BPE dropout results in a decrease of six and
three task completion points, respectively. The
largest drop in performance is observed when re-
moving the text attention mechanism. This again
shows the importance of attention over the encoder
in sequence-to-sequence models. Removing the
image attention mechanism on the other hand does
not affect task completion on the map2seq dataset.
B Mixed-Model
The findings in Section 6.2 inspire us to modify
the ORAR model to use distinct visual features for
the orientation and directions/stopping task. The
orientation task is equivalent to the very first ac-
tion prediction by the agent. Thus we modify the
model architecture to use the ResNet 4th-to-last
features (+text representation) to predict the first
action and then start the recurrent prediction of the
remaining actions with a different set of visual fea-
tures (pre-final for the seen scenario and no image
features for the unseen scenario). The results for
thisORAR mixed model trained on the merged
dataset are shown in Table 7. We only test it on
Touchdown because map2seq does not have the
orientation task. The mixed model significantly
outperforms the single visual feature model on the
Touchdown seen test set but unfortunately shows
no improvement in the unseen scenario.
C Additional Metrics and Individual
Runs
We present the results of the individual repeti-
tions and additional metrics for the main results
in Table 2 and the results on the merged datasetin Table 5. The additional metrics are success
weighted normalized Dynamic Time Warping (Il-
harco et al., 2019) and shortest-path distance (Chen
et al., 2019).753075317532