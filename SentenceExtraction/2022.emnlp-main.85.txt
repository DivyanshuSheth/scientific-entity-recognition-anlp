
Wenbin An, Feng Tian, Ping Chen, Siliang Tang, Qinghua Zheng, QianYing WangSchool of Automation Science and Engineering, Xi’an Jiaotong UniversitySchool of Computer Science and Technology, MOEKLNNS Lab, Xi’an Jiaotong UniversityDepartment of Engineering, University of Massachusetts BostonCollege of Computer Science and Technology, Zhejiang UniversityLenovo Research
wenbinan@stu.xjtu.edu.cn, {fengtian,qhzheng}@mail.xjtu.edu.cn
ping.chen@umb.edu, siliang@zju.edu.cn, wangqya@lenovo.com
Abstract
Novel category discovery aims at adapting mod-
els trained on known categories to novel cat-
egories. Previous works only focus on the
scenario where known and novel categories
are of the same granularity. In this paper,
we investigate a new practical scenario called
Fine-grained Category Discovery under Coarse-
grained supervision (FCDC). FCDC aims at
discovering ﬁne-grained categories with only
coarse-grained labeled data, which can adapt
models to categories of different granularity
from known ones and reduce signiﬁcant la-
beling cost. It is also a challenging task
since supervised training on coarse-grained cat-
egories tends to focus on inter-class distance
(distance between coarse-grained classes) but
ignore intra-class distance (distance between
ﬁne-grained sub-classes) which is essential
for separating ﬁne-grained categories. Con-
sidering most current methods cannot trans-
fer knowledge from coarse-grained level to
ﬁne-grained level, we propose a hierarchical
weighted self-contrastive network by build-
ing a novel weighted self-contrastive mod-
ule and combining it with supervised learn-
ing in a hierarchical manner. Extensive ex-
periments on public datasets show both ef-
fectiveness and efﬁciency of our model over
compared methods. Code and data are avail-
able at https://github.com/Lackel/
Hierarchical_Weighted_SCL .
1 Introduction
Discovering novel categories based on some known
categories has attracted much attention in both Nat-
ural Language Processing (Zhang et al., 2021; Zhao
et al., 2021) and Computer Vision (Zhong et al.,
2021; Han et al., 2019). Previous works assume
that novel categories are of the same granularity (or
of the same class hierarchy level) as known cate-
gories. However, in real-world scenarios, novel cat-
egories can be more ﬁne-grained sub-categories of
known ones (e.g., sports and tennis). A typical ap-Figure 1: An example of proposed FCDC task (ﬁne-
grained label names need to be assigned by experts).
plication of this scenario is when data analysts want
to perform more ﬁne-grained analysis on data with
only coarse-grained annotations, where re-labeling
ﬁne-grained categories can be time consuming and
labour intensive. For example, in the intent de-
tection ﬁeld, discovering more ﬁne-grained user
intents can help to provide better services to cus-
tomers, but labeling ﬁne-grained intent categories
is often much more difﬁcult than labeling coarse-
grained ones, since ﬁne-grained annotation often
requires higher expertise. To meet this requirement,
we investigate a new scenario named Fine-grained
Category Discovery under Coarse-grained supervi-
sion (FCDC). As shown in Figure 1, FCDC needs
models to discover ﬁne-grained categories (e.g.,
tennis and music) based only on coarse-grained
(e.g., sports and arts) labeled data which are easier
and cheaper to obtain.
In addition to being in line with above practical
needs, FCDC is also a challenging task. Firstly,
performing FCDC requires models to increase
intra-class distance to ensure ﬁne-grained separa-
bility with only coarse-grained supervision. How-
ever, coarse-grained classiﬁcation only focuses on
inter-class distance and does not care about intra-
class distance (Bukchin et al., 2021), so samples
with the same coarse-grained labels will be close
to each other and hard to be separated in ﬁne-
grained feature space. Secondly, since ﬁne-grained
differentiation depends on correct coarse-grained
classiﬁcation, FCDC also requires models to con-1314trol inter-class distance to ensure coarse-grained
separability. Although increasing intra-class dis-
tance can contribute to ﬁne sub-classes separabil-
ity, it will also decrease inter-class distance, which
can result in overlapping between different coarse-
grained classes and therefore lead to misclassiﬁca-
tion. So how to control and coordinate inter-
class and intra-class distance to ensure both
coarse-grained and ﬁne-grained separability is
the core challenge of FCDC .
To address above challenges and transfer knowl-
edge from coarse-grained level to ﬁne-grained
level, we propose a hierarchical weighted self-
contrastive network. By performing different ex-
periments on each layer of BERT, Jawahar et al.
(2019) ﬁnd bottom layers of BERT capture more
surface features and top layers capture more high-
level semantic features, which means BERT can
extract features of different granularities from shal-
low to deep (Xu et al., 2021). Inspired by this phe-
nomenon, the core motivation of our model is to
learn coarse-grained knowledge by shallow layers
of BERT and learn more ﬁne-grained knowledge
by the rest of deep layers hierarchically. This moti-
vation is not only consistent with the feature extrac-
tion process of BERT, but also corresponding with
the shallow-to-deep learning process of humans.
Speciﬁcally, we use given coarse-grained labels to
train shallow layers of BERT to learn some sur-
face knowledge, then we propose a weighted self-
contrastive module to train deep layers of BERT
to learn more ﬁne-grained knowledge based on the
learned surface knowledge.
To ensure both coarse-grained and ﬁne-grained
separability, we further propose a weighted self-
contrastive module to better coordinate inter-class
and intra-class distance in the ﬁne-grained feature
space. Speciﬁcally, given a query sample, we ﬁrstly
propose a weighting strategy by weighting differ-
ent negative samples to control both inter-class
and intra-class distance. Then we propose a self-
contrastive strategy to generate positive samples
to coordinate inter-class and intra-class distance
to avoid the overlapping between different coarse-
grained classes. We further verify effectiveness and
efﬁciency of our model both theoretically (Section
3.2.4) and experimentally (Section 4.5).
The main contributions of our work can be sum-
marized as threefold:
•We propose to investigate a practical scenario
called Fine-grained Category Discovery un-der Coarse-grained supervision (FCDC), we
further propose a hierarchical model to learn
ﬁne-grained knowledge from shallow to deep
to facilitate the FCDC task.
•To better coordinate inter-class and intra-class
distance, we propose a novel weighted self-
contrastive module to ensure both coarse-
grained and ﬁne-grained separability.
•Extensive experiments on public datasets
show that our model signiﬁcantly advances
best compared methods with a large margin
and gets double training efﬁciency than state-
of-the-art contrastive learning methods.
2 Related work
2.1 Contrastive learning
Contrastive Learning (CL) aims at grouping simi-
lar samples closer and separating dissimilar sam-
ples far from each other in a self-supervised way
(Jaiswal et al., 2021), which has gained popular-
ity in both Natural Language Processing (NLP)
(Wu et al., 2020) and Computer Vision (CV) (Chen
et al., 2020). A critical point for CL is to build
high-quality positive and negative samples. The
simplest way to construct negative samples is to
use other in-batch data as negatives (Chen et al.,
2017). Further, He et al. (2020) built a dynamic
queue with momentum-updated encoder to keep
consistency of representations of negatives. How-
ever, these methods consider all negatives equally
important, which may lose discriminative informa-
tion of different negatives. As for positive samples,
in CV , one common way is taking two different
transformations of the same image as the query and
positive sample (Dosovitskiy et al., 2014). And in
NLP, augmentation techniques such as word dele-
tion (Meng et al., 2021), adversarial attack (Yan
et al., 2021) and dropout (Gao et al., 2021) were
proposed to generate positives. Although there are
some recent works (Bae et al., 2021) using out-
puts from different levels of a network as positives,
we have totally different motivations: they aim at
providing more high-quality positives for represen-
tation learning while we aim at better adjusting
intra-class and inter-class distance.
2.2 Novel Category Discovery
With data volume increases, novel categories espe-
cially novel ﬁne-grained categories may be intro-
duced into datasets (Mekala et al., 2021). To dis-1315
cover novel categories without human annotation,
most previous work adopted clustering methods
and transfer learning methods to generate pseudo
labels for unlabeled data to train their models (Zhan
et al., 2020). For example, Zhang et al. (2021)
proposed an alignment strategy to perform Deep-
Cluster (Caron et al., 2018) to discover novel cat-
egories. Ge et al. (2020) proposed a mutual mean
teaching network to reﬁne noisy pseudo labels to
perform unsupervised person re-identiﬁcation. Re-
cently, Two similar tasks as FCDC are proposed.
Bukchin et al. (2021) proposed to perform ﬁne-
grained image classiﬁcation under coarse-grained
supervision with angular contrastive learning, and
they performed this task in a few-shot learning way
which needs extra ﬁne-grained labels for each cate-
gories. Mekala et al. (2021) proposed to perform
ﬁne-grained text classiﬁcation with coarse-grained
annotations, but they need extra ﬁne-grained label
hierarchy and corresponding label names to assist
in the task. These two tasks both rely on extra
ﬁne-grained knowledge from human annotations,
which is usually unavailable when novel categories
appear in real-world applications. Comparatively,
our FCDC is a category discovery task which does
not require ﬁne-grained knowledge and is more
adapted to real world scenarios.
2.3 Problem Formulation
Denote byY ={C,C,...,C}a set of
coarse-grained classes. The training set of FCDC is
a set of textsD ={D,D,...,D}with their
coarse-grained labels {c,c,...,c}, where c∈
Y . Different from previous tasks (Bukchinet al., 2021; Mekala et al., 2021) where the ﬁne-
grained label setY ={F,F,...,F}is al-
ready known, FCDC assumes that we do not have
any prior knowledge about ﬁne-grained labels. So
FCDC requires models to perform clustering meth-
ods (e.g., K-Means) to discover ﬁne-grained clus-
tersYwithD. Since performing clustering
will assign each input with a speciﬁc cluster assign-
ment, FCDC can also classify inputs into proper
ﬁne-grained categories {f,f,...,f}. Although
the number of ﬁne-grained clusters Kcan be es-
timated with various methods from the clustering
area, we assume it is known in FCDC following pre-
vious similar works (Zhang et al., 2021; Bukchin
et al., 2021) to make a fair comparison.
3 Proposed Approach
As shown in Figure 2, our model mainly contains
three components: BERT, Dynamic Queue and
Momentum BERT. BERT is used to extract both
coarse-grained and ﬁne-grained features. Dynamic
Queue can store more negative samples grouping
by their coarse-grained labels following Bukchin
et al. (2021). Momentum BERT is used to update
representations of samples in Dynamic Queue. In-
spired by the "shallow to deep" learning process of
humankind and the ability of pre-trained models to
extract features from coarse-grained to ﬁne-grained
(Jawahar et al., 2019; Xu et al., 2021), a core
motivation of our model is to learn ﬁne-grained
knowledge in a progressive way. Speciﬁcally, our
model can learn coarse-grained knowledge at shal-
low layers under coarse-grained supervision and1316learn more ﬁne-grained knowledge at deep layers
with the proposed weighted self-contrastive learn-
ing.
3.1 Supervised Learning
We ﬁrstly perform supervised learning on Trans-
former layer Lof BERT to learn coarse-grained
knowledge. Given the i-thdocumentDwith its
coarse-grained label c, we use all token embed-
dings from the L-th layer of BERT as its shallow
features. Then we apply a mean-pooling layer to
get its shallow feature representation h:
h=mean -pooling (BERT(D)) (1)
whereh∈Ris the hidden state of feature
representations, his the dimension of hidden rep-
resentations. Then we perform supervised learning
with cross entropy loss on coarse-grained labels to
get supervised loss Lat layer L:
z=σ(Wh+b) (2)
L=−1
N/summationdisplaylogexp((z))/summationtextexp((z))(3)
wherez∈Ris the output logits, Mis the
number of coarse-grained classes. σis the Tanh
activation function, W∈Randb∈R
are learnable weights and bias terms, respectively.
(z)is the j-th element of output logits z.
3.2 Weighted Self-contrastive Learning
As shown in Figure 3, denote the coarse-grained
inter-class and intra-class distance by d and
d, respectively. Supervised learning on coarse-
grained labels can ensure d/greatermuch0but will also
maked≈0, which can bring difﬁculties for
ﬁne-grained categorization. So how to increase
d to ensure separability of ﬁne-grained sub-
classes is a severe challenge. Meanwhile, increas-
ingd without restraint will result in overlap-
ping between different coarse-grained classes and
therefore lead to misclassiﬁcation. So how to con-
straindto ensure the proper classiﬁcation on
coarse-grained classes is the other challenge. In
summary, our total goal can be described as:
0/lessmuchd<d/lessmuchd (4)
whered is a threshold to ensure that sam-
ples fall into proper coarse-grained classes.
To achieve above objectives, we propose a
weighted self-contrastive module by introducing a
novel generation strategy for positive samples and
a weighting strategy for negative samples.
3.2.1 Negative Key Generation
Given the i-thdocumentD, we use all token em-
beddings from the output layer of BERT as its deep
features. Then we apply a mean-pooling layer to
get its deep feature representation h∈R:
h=mean -pooling (BERT(D)) (5)
In-batch negative keys Givenhwith its coarse-
grained label cas a query q, we treat shal-
low and deep features of other in-batch samples
as its in-batch negative keys, where k(i) =
{h,h}. In this way, we can increase
distance between different samples so that satis-
fyingd/greatermuch0andd/greatermuch0. To satisfy
d/greatermuchd, we propose a weighting strategy
by giving more weights to samples with different
coarse-grained labels as the query qto further in-
crease their distance. So kcan be divided into
two groups according to the coarse-grained labels:
k(i) ={k∈k(i) :c/negationslash=c} (6)
k (i) ={k∈k(i) :c=c} (7)
Momentum negative keys To provide more neg-
ative keys, we build a momentum BERT and a set
of dynamic queues {Q}to store previous sam-
ples grouped by their coarse-grained labels follow-
ing Bukchin et al. (2021), where Mis the number of
coarse-grained classes. Speciﬁcally, given hwith
its coarse-grained label cas a query, we treat sam-
ples from the queue Qas its momentum negative
keys:
k(i) ={k∈Q} (8)
Feature representations of samples in dynamic
queues are extracted by momentum BERT, and
parameters of momentum BERT are updated in
a momentum way (He et al., 2020). At the end
of each iteration, the dynamic queues will be up-
dated by adding novel samples and removing the1317earliest samples. Since samples in k(i)have the
same coarse-grained labels as the query, they are
much harder to be separated and beneﬁcial to better
representation learning.
The overall negative keys for the query his :
k(i) ={k(i),k (i),k(i)} (9)
3.2.2 Positive Key Generation
By weighting different negative samples, we can
satisfy the condition 0/lessmuchd/lessmuchd . But
increasingdwithout restraint will violate the
conditiond<d and make some sam-
ples fall into incorrect coarse-grained classes. To
solve this problem, we propose a self-contrastive
strategy by treating shallow features of a query as
its positive key. Speciﬁcally, given the deep feature
representation hfor documentDas a query, we
treathas its positive key:
k(i) =h (10)
As shown in Figure 3, after supervised learning on
coarse-grained labels at layer L,hcan be very
close to the class center of c, so pullinghclose to
hwill also pull hclose to the class center of c.
In this way, we can increase dwith restraint and
satisfy the condition d< d without
computing the speciﬁc value of d . Another
advantage of our self-contrastive strategy is that we
can get double training efﬁciency than traditional
data augmentation-based methods (Wu et al., 2020;
Gao et al., 2021) since we only need to perform
forward and backward propagation only once to get
and update both queries and positive keys (Section
5.2).
3.2.3 Weighted Self-contrastive Loss
Given the query hwith its positive key k(i)
and negative keys k(i), the overall loss of our
weighted self-contrastive module is:
L =/summationdisplay−loge
/summationtextα/summationtexte
(11)
whereα∈{α,α,α}are weighting fac-
tors for different negative keys, sim(h,h)is co-
sine similarityandτis a temperature hy-
perparameter.
By weighting different negative keys and select-
ing shallow features as the positive key, our model
can satisfy the goal in Inequation 4 and provideconditions for subsequent ﬁne-grained categoriza-
tion.
3.2.4 Theoretical Analysis
The effectiveness of our weighted self-contrastive
learning compared with traditional contrastive
learning from the gradient perspective is analyzed
below.
Self-contrastive Strategy Compared with tradi-
tional contrastive loss which only aims at grouping
queries and their transformations closer, our self-
contrastive strategy aims at pulling queries and
their shallow features closer:
sim(h,h) :=sim(h,h) +1
τ(12)
Sinceτis positive, the positive similarity will
increase and hwill be grouped closer to h. Af-
ter supervised learning on coarse-grained labels
at layer L,hcan be close to the class center
ofc, so pulling hcloser tohwill also pull
hcloser to the class center of c. So our Self-
Contrastive strategy can guarantee queries fall into
correct coarse-grained categories and get double
training efﬁciency since we only need to perform
forward and backward propagation only once to
get and update both queries and positive keys.
Weighting Strategy Since negatives with the
same coarse-grained labels as queries have larger
gradients (Wang and Liu, 2021), traditional con-
trastive loss will push these negatives farther from
queries than those with different coarse-grained la-
bels as queries, which leads to d<dand
is opposite of what we expect to solve the FCDC
task. To mitigate this limitation, we propose a
weighting strategy to give more weights to samples
with different coarse-grained labels as the query to
further increase their distance:
sim(h,h) :=sim(h,h)−α·P (13)
P=1
τ·e
/summationtextα/summationtexte(14)
By increasing the weighting factor αfor nega-
tives with different coarse-grained labels as queries,
the corresponding similarity will decrease faster.
So negatives with different coarse-grained labels
from queries will be pushed farther than those with
the same coarse-grained labels as queries, which
can guarantee d<d for the FCDC task.1318
Dataset |C| |F| # Train # Test
CLINC 10 150 18,000 1,000
WOS 7 33 8,362 2,420
HWU64 18 64 8,954 1,031
3.3 Overall Loss
To further guarantee samples to be classiﬁed into
proper coarse-grained categories, we also add su-
pervised learning on coarse-grained labels at the
output layer. So the overall loss for our hierarchical
weighted self-contrastive network is:
L=L+γL+γL (15)
whereLis the cross entropy loss at the output
layer.γandγare weighting factors.
After representation learning, we simply perform
the non-parametric clustering method K-Means to
discover ﬁne-grained categories based on features
extracted by the output layer of BERT.
4 Experiments
4.1 Datasets
To evaluate effectiveness of our model, we conduct
experiments on three public datasets. Statistics of
three datasets can be found in Table 1.
CLINC is an intent classiﬁcation dataset released
by Larson et al. (2019).
Web of Science (WOS) is a paper classiﬁcation
dataset released by Kowsari et al. (2017).
HWU64 is a personal assistant query classiﬁcation
dataset released by Liu et al. (2021).
4.2 Implementation Details
We use the pre-trained BERT model (bert-base-
uncased) implemented by Pytorch (Wolf et al.,
2020) as our backbone and adopt most of its sug-
gested hyper-parameters. We use the cuml li-
brary (Raschka et al., 2020) to perform K-Means
on GPU to speed up calculations. We use the
AdamW optimizer with 0.01 weight decay. Gra-
dient clipping is also used with the norm 1.0. For
hyper-parameters, temperature τis set to 0.1, layer
Lis set to 8, and the weighting factors αfor
{k(i),k (i),k(i)}are set to {1.4, 1.0,
1.0}, weighting factors {γ,γ}are set to {0.001,
0.008}. The training batch size is set to 128, andthe testing batch size is set to 64. The momentum
queue size for each coarse-grained category is set
to 128, and the momentum factor for Momentum
BERT is set to 0.9. The hidden dimension his 768,
the learning rate is set to 5e, the dropout rate is
set to 0.1. The training epoch is set to 20.
For a fair comparison, we use the same BERT
model as ours to extract features for all compared
methods and adopt hyper-parameters in their origi-
nal papers.
4.3 Compared Methods
Baselines We perform FCDC with BERT in un-
supervised and coarse-supervised way as baselines.
Self-supervised Methods DeepCluster (Caron
et al., 2018) and DeepAligned (Zhang et al., 2021)
are self-supervised methods using self-training
techniques and achieve state-of-the-art results in
many category discovery tasks. Ancor (Bukchin
et al., 2021) is a self-supervised method designed
for few-shot ﬁne-grained classiﬁcation with coarse-
grained labels. SimCSE (Gao et al., 2021) and
Delete One Word (Wu et al., 2020) are contrastive
learning methods in NLP with different data aug-
mentation techniques.
Self-supervised + Cross Entropy To investigate
the inﬂuence of coarse-grained supervision on com-
pared models, we further add cross entropy loss on
coarse-grained labels Lto their loss function.
4.4 Evaluation Metrics
We use ﬁne-grained labels as ground truth to eval-
uate model performance on testing sets. Since no
ﬁne-grained knowledge is available for the FCDC
task, we need to perform clustering to discover
ﬁne-grained categories. Clustering performance
can reﬂect the quality of discovered ﬁne-grained
clusters (more compact clusters usually mean better
discovered categories). And classiﬁcation perfor-
mance can reﬂect the semantic overlap between
discovered clusters and real categories.
To evaluate clustering performance, we use two
broadly used external evaluation metrics. Adjusted
Rand Index (ARI) is used to evaluate the degree of
agreement between cluster assignments and ground
truth. And Normalized Mutual Information (NMI)
is used to evaluate the mutual information between
cluster assignments and ground truth. To evaluate
classiﬁcation performance, we use metric Accu-
racy (ACC), which is obtained from Hungarian al-
gorithm (Kuhn, 1955) to align cluster assignments
and ground truth.1319
MethodsCLINC WOS HWU64
ACC ARI NMI ACC ARI NMI ACC ARI NMI
Unsupervised 33.38 16.42 63.46 32.32 18.21 47.12 33.66 16.88 56.78
Coarse Supervised 45.91 32.27 75.04 39.42 33.67 61.60 42.41 33.74 71.57
DeepCluster 26.40 12.51 61.26 29.17 18.05 43.34 29.74 13.98 53.27
DeepAligned 29.16 14.15 62.78 28.47 15.94 43.52 29.14 12.89 52.99
SimCSE 40.22 23.57 69.02 25.87 13.03 38.53 24.48 8.42 46.94
Anchor 45.60 33.11 75.23 41.20 37.00 65.42 37.34 34.75 74.99
Delete One Word 47.11 31.28 73.39 24.50 11.68 35.47 21.30 6.52 44.13
DeepCluster + CE 30.28 13.56 62.38 38.76 35.21 60.30 41.73 27.81 66.81
DeepAligned + CE 42.09 28.09 72.78 39.42 33.67 61.60 42.19 28.15 66.50
Anchor + CE 44.44 31.50 74.67 39.34 26.14 54.35 32.90 30.71 74.73
Delete One Word + CE 47.87 33.79 76.25 41.53 33.78 61.01 35.13 31.84 74.88
SimCSE + CE 52.53 37.03 77.39 41.28 34.47 61.62 34.04 31.81 74.86
Ours 74.67 64.77 89.14 63.64 51.55 72.46 58.45 48.20 78.66
4.5 Main Results
Model performance on ﬁne-grained categories
are reported in Table 2. From the results we can
draw following conclusions. Our model signiﬁ-
cantly outperforms other compared methods across
all datasets. We contribute reasons of better per-
formance of our model to following two points.
Firstly, we propose a hierarchical architecture to
learn ﬁne-grained knowledge from shallow to deep,
which is consistent with the feature extraction pro-
cess of BERT and the shallow-to-deep learning pro-
cess of humans. Secondly, we propose a weighted
self-contrastive module to coordinate inter-class
and intra-class distance so that we can better learn
both coarse-grained and ﬁne-grained knowledge.
Self-training methods perform badly on all datasets
and evaluation metrics since they rely on abun-
dant labeled data to generate high-quality pseudo
labels for unlabeled data. Contrastive learning
methods perform better than self-training methods
since they do not need ﬁne-grained labels to ini-
tialize their models. However, their performance
is still much worse than ours since they cannot
fully utilize given coarse-grained labels to control
inter-class and intra-class distance between sam-
ples. We also ﬁnd that model performance of most
compared methods increases with the addition of
coarse-grained supervision, which means coarse-
grained supervision can boost model performance
on ﬁne-grained tasks.
Our model performance on coarse-grained
Model CLINC WOS
Coarse Supervised 98.58 91.86
Ours 98.71 91.45
Model ACC ARI NMI
ALL 74.67 64.77 89.14
- Momentum 73.38 64.47 88.91
-L 72.93 63.72 88.06
- Weighting 71.75 62.99 88.47
- Self-Contrast 53.21 40.05 75.36
categories are reported in Table 3. From the table
we can see that our model gets similar classiﬁca-
tion accuracy as the upper-bound coarse-supervised
BERT, which means that our model can control
not only intra-class distance to ensure ﬁne-grained
separability, but also inter-class distance to ensure
coarse-grained variability.
5 Discussion
5.1 Ablation Study
To investigate contributions of different compo-
nents to our model, we compare the performance
of our model with its variants on the CLINC dataset.
As shown in Table 4, removing different compo-1320
nents will affect model performance more or less,
which indicates the effectiveness of different com-
ponents of our model. Removing Momentum En-
coder has minimal impact, since our model is in-
sensitive to the number of negative samples. Re-
moving weighting strategy or cross entropy loss at
shallow layers also hurt model performance since
they can help to learn coarse-grained knowledge
and lay foundation for learning ﬁne-grained knowl-
edge. Above all, removing self-contrastive strategy
results in a signiﬁcant decrease, since it is respon-
sible for controlling intra-class and inter-class dis-
tance.
5.2 Training Efﬁciency
In this section, we compare the training efﬁciency
of our model with contrastive methods SimCSE
and Delete One Word on the CLINC dataset. We
test all methods using the BERT base model trained
on the same hardware platform (an AMD EPYC
CPU 7702 and a RTX 3090 GPU) with the batch
size 128. Average results over 100 epochs are
shown in Figure 4. Compared with SimCSE and
Delete One Word, our model gets double train-
ing efﬁciency both when adding or removing Mo-
mentum Encoder, which beneﬁts from our self-
contrastive strategy. Speciﬁcally, our model uti-
lizes shallow features of queries as positive keys,
which only needs to perform forward and backward
propagation once to get and update both queries
and positive keys.
5.3 Visualization
We further visualize the learned embeddings of our
model and SimCSE using t-SNE on the CLINC
dataset in Figure 5. Our model can separate differ-
ent coarse-grained categories with a larger margin
than SimCSE (Top in Figure 5), which beneﬁts
from our strategy of combining supervised learn-
ing and contrastive learning in a hierarchical way.
Furthermore, our model can also separate different
ﬁne-grained categories with a larger margin (Bot-
tom in Figure 5), which beneﬁts from the weighted
self-contrastive module. In summary, our model
can better control both inter-class and intra-class
distance between samples to facilitate the FCDC
task than traditional contrastive learning methods.
5.4 Choices of Land weighting factors
Effect of Shallow Layer LThe inﬂuence of the
choice of shallow layer Lon model performance
is shown in Figure 6. Our model achieves the best
performance when L=8. In this way, our model
can learn coarse-grained knowledge at shallow
layers ( L<8) and provide enough model capacity
to learn ﬁne-grained knowledge at deeper layers
(L>8), which is consistent with the feature extrac-
tion process of BERT (Jawahar et al., 2019).
Effect of Weighting Factors We investigate the
inﬂuence of the ratio β=α/α in Figure 7
(We ﬁxedα= 1since it has little inﬂuence). As
analyzed in Section 3.2.4, by giving more weights
to negatives with different coarse-grained labels
as queries ( β > 1), our weighting strategy can
keep these negatives further away from queries and
guaranteed<d . On the contrary, when
β < 1, negatives with the same coarse-grained
labels as queries will be further away from the
queries, which can hurt our model performance.1321
6 Conclusion
In this paper, we investigate a novel task named
Fine-grained Category Discovery under Coarse-
grained supervision (FCDC), which can reduce sig-
niﬁcant labeling cost and adapt models to novel cat-
egories of different granularity from known ones.
We further propose a hierarchical weighted self-
contrastive model to approach the FCDC task by
better controlling intra-class and inter-class dis-
tance. By performing supervised and contrastive
learning on shallow and deep layers of pre-trained
models, our model can learn ﬁne-grained knowl-
edge from shallow to deep with only coarse-grained
supervision. Extensive experiments on public
datasets show that our approach is more effective
and efﬁcient than compared methods.
Limitations
The limitations of our method lies in two aspects.
Firstly, following previous works, we need to know
the number of ﬁne-grained clusters Kas prior
knowledge, which is usually difﬁcult to get in real-
world scenarios. Secondly, our method cannot pre-
dict semantic meanings (e.g., label names) of dis-
covered ﬁne-grained categories, which is also an
unexplored question in the ﬁeld of novel categorydiscovery.
Acknowledgements
This work was supported by National Key
Research and Development Program of China
(2020AAA0108800), National Natural Science
Foundation of China (62137002, 61721002,
61937001, 61877048, 62177038, 62277042). In-
novation Research Team of Ministry of Edu-
cation (IRT_17R86), Project of China Knowl-
edge Centre for Engineering Science and Technol-
ogy. MoE-CMCC “Artiﬁcal Intelligence” Project
(MCM20190701), Project of Chinese academy of
engineering “The Online and Ofﬂine Mixed Ed-
ucational ServiceSystem for ’The Belt and Road’
Training in MOOC China”. “LENOVO-XJTU” In-
telligent Industry Joint Laboratory Project.
References13221323