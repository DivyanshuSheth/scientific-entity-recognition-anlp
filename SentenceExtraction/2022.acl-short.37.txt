
Peiyi Wang, Liang Chen, Tianyu Liu, Damai Dai,
Yunbo Cao, Baobao Chang, Zhifang SuiKey Laboratory of Computational Linguistics, Peking University, MOE, ChinaTencent Cloud Xiaowei
wangpeiyi9979@gmail.com; leo.liang.chen@outlook.com
{rogertyliu, yunbocao}@tencent.com
{daidamai, chbb, szf}@pku.edu.cn
Abstract
Abstract Meaning Representation (AMR) pars-
ing aims to translate sentences to semantic rep-
resentation with a hierarchical structure, and
is recently empowered by pretrained sequence-
to-sequence models. However, there exists a
gap between their ﬂat training objective (i.e.,
equally treats all output tokens) and the hi-
erarchical AMR structure, which limits the
model generalization. To bridge this gap, we
propose a Hierarchical Curriculum Learning
(HCL) framework with Structure-level (SC)
and Instance-level Curricula (IC). SC switches
progressively from core to detail AMR seman-
tic elements while IC transits from structure-
simple to -complex AMR instances during
training. Through these two warming-up pro-
cesses, HCL reduces the difﬁculty of learning
complex structures, thus the ﬂat model can bet-
ter adapt to the AMR hierarchy. Extensive
experiments on AMR2.0, AMR3.0, structure-
complex and out-of-distribution situations ver-
ify the effectiveness of HCL.
1 Introduction
Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) parsing aims to translate a
natural sentence into a directed acyclic graph. Fig-
ure 1(a) illustrates an AMR graph where nodes
represent concepts, e.g., ‘die-01’ and ‘soldier’,
and edges represent relations, e.g., ‘:ARG1’ and
‘:quant’. AMR has been exploited in the down-
stream NLP tasks, including information extraction
(Rao et al., 2017; Wang et al., 2017; Zhang and Ji,
2021), text summarization (Liao et al., 2018; Hardy
and Vlachos, 2018) and question answering (Mitra
and Baral, 2016; Sachan and Xing, 2016).
The powerful pretrained encoder-decoder mod-
els, e.g., BART (Lewis et al., 2020), have been
successfully adapted to the AMR parsing and be-
came the mainstream and state-of-the-art meth-Figure 1: The AMR (sub-)graphs of the sentence “Nine
of the twenty soldiers died”. The deeper sub-graphs
contain more sophisticated semantics compared with
shallower ones.
ods (Bevilacqua et al., 2021). Through directly
generating the linearized AMR graph (e.g., Fig-
ure 1(a)) from the sentence, these sequence-to-
sequence methods (Xu et al., 2020b; Bevilacqua
et al., 2021) circumvent the complex data pro-
cessing pipeline and can be easily optimized com-
pared with transition-based or graph-based meth-
ods (Naseem et al., 2019; Lee et al., 2020; Lyu and
Titov, 2018; Zhang et al., 2019a,b; Cai and Lam,
2020; Zhou et al., 2021b). However, there exists
a gap between the ﬂat sentence-to-AMR training
objectiveand AMR graphs, since sequence-to-
sequence models deviate from the essence of graph
representation. Therefore, it is difﬁcult for sequen-
tial generators to learn the inherent hierarchical
structure of AMR (Zhou et al., 2021b).
Humans usually adapt to difﬁcult tasks by deal-
ing with examples gradually from easy to hard, i.e.,
Curriculum Learning (Bengio et al., 2009; Platan-
ios et al., 2019; Su et al., 2021; Xu et al., 2020a). In-
spired by human behavior, we propose a hierarchi-333
cal curriculum learning framework with two curric-
ular strategies to help the ﬂat pretrained model pro-
gressively adapt to the hierarchical AMR graph. (1)
Structure-level Curriculum (SC) . AMR graphs
are organized in a hierarchy where the core se-
mantic elements stay closely to the root node (Cai
and Lam, 2019). As depicted in Figure 1, the con-
cepts and relations that locate in the different layers
of the AMR graph correspond to different levels
of abstraction in terms of the semantic represen-
tation. Motivated by the human learning process,
i.e.,core concepts ﬁrst, then details , SC enumer-
ates all AMR sub-graphs with different depths, and
deals with them in order from shallow to deep. (2)
Instance-level Curriculum (IC) . Our preliminary
study in Figure 3 shows that the performance of
the vanilla BART baseline would drop rapidly as
the depth of AMR graph grows, which indicates
that handing deeper AMR hierarchy is more difﬁ-
cult for pretrained models. Inspired by the human
cognition, i.e., easy ones ﬁrst, then hard ones , we
propose IC which trains the model by starting from
easy instances with a shallower AMR structure and
then handling hard instances.
To sum up: (1) Inspired by the human learn-
ing process, i.e., core concepts ﬁrst andeasy in-stances ﬁrst , we propose a hierarchical curriculum
learning (HCL) framework to help the sequence-to-
sequence model progressively adapt to the AMR
hierarchy. (2) Extensive experiments on AMR2.0,
AMR3.0, structure-complex and out-of-distribution
situations verify the effectiveness of HCL.
2 Methodology
We formulate AMR parsing as a sequence-to-
sequence transformation. Given a sentence x=
(x; :::; x), the model aims to generate a lin-
earized AMR graph y= (y; :::; y). As shown
in Figure 1(a), following Bevilacqua et al. (2021),
the AMR graph is linearized by the DFS-based
linearization method with special tokens to indi-
cate variables and parentheses to mark visit depth.
Speciﬁcally, variables of AMR nodes are set to a
series of special tokens <R0>, ..., <Rk> (more de-
tails of linearization are included in Appendix A).
In this paper, we propose a hierarchical curriculum
learning framework (Figure 2) with the structure-
and instance-level curricula to help the ﬂat model
progressively adapt to the structured AMR graph.
2.1 Structure-level Curriculum
Motivated by learning core concepts ﬁrst , we pro-
pose Structure-level Curriculum (SC). AMR graphs
are organized in a hierarchy where the core seman-
tics stay closely to the root (Cai and Lam, 2019),
thus SC divides all AMR sub-graphs into Nbuck-
ets according to their depths fS:i= 1;2; :::; Ng,
where Scontains AMR sub-graphs with the depth
i. As shown in Figure 2(a), SC has Ntraining
episodes, and each episode consists of Tsteps.
In each step of the i-th episode, the training sched-
uler samples a batch of examples from buckets
fS:jigto train the model. When parsing334
a sentence into a sub-graph with the depth d, we
append a special string “parse to dlayers” to the
input sentence, and replace the start token of the
decoder with an artiﬁcial token <d>, so the model
can perceive layers that need to be parsed.
2.2 Instance-level Curriculum
Inspired by learning easy instances ﬁrst , we pro-
pose Instance-level Curriculum (IC). Figure 3
shows AMR graphs with deeper layers can be re-
garded as harder instances for the ﬂat pretrained
model, thus IC divides all AMR graphs into M
buckets according to their depths fI:i=
1; :::; Mg, where Icontains AMR graphs with the
depth i. As shown in Figure 2(b), IC has Mtrain-
ing episodes, and each episode consists of Tsteps.
In each step of the i-th episode, the training sched-
uler samples a batch of examples from buckets
fI:jigto train the model. Speciﬁcally, we
ﬁrst use SC and then IC to train the model, since SC
(follows learning core semantics ﬁrst) is for AMR
sub-graphs, which can be regarded as a warming-
up stage of IC (obeys learning easy instances ﬁrst),
which is for AMR full graphs.
3 Experiments
Datasets and Evaluation Metrics We evalu-
ate our hierarchical curriculum learning frame-
work on two popular AMR benchmarks, AMR2.0
(LDC2017T10) and AMR3.0 (LDC2020T02).
Please refer to the Appendix B for details of two
benchmarks. Following Bevilacqua et al. (2021),
we use the scores (Cai and Knight, 2013)and the ﬁne-grained evaluation metrics (Damonte
et al., 2017)to evaluate the performances.
Experiment Setups Our implementation is
based on Huggingface’s transformers library (Wolf
et al., 2020) and the open codebase of Bevilac-
qua et al. (2021). We use BART-large as our
sequence-to-sequence model the same as Bevilac-
qua et al. (2021). We utilizes RAdam (Liu et al.,
2020) as our optimizer with the learning rate 3e-
5. The batch size is 2048 graph linearization to-
kens with the gradient accumulation 10. Dropout
is set to 0:25and beam size is 5. The train-
ing steps Tis1000 andTis500. After the
curriculum training, the model is trained for 30
epochs on the training set. We use cross-entropy
as our loss function. We train our model on a
single NVIDIA TESLA V 100GPU with 32GB
memory. We adopt the same post-processing pro-
cess as Bevilacqua et al. (2021). Our code and
model are available at https://github.com/
Wangpeiyi9979/HCL-Text2AMR .
Main Results We compare our method with pre-
vious approaches in Table 1. As is shown, on
AMR2.0 and AMR3.0, our hierarchical curriculum
learning model achieves 84:30:1and83:70:1 scores, and outperforms Bevilacqua et al.
(2021) 0:5and 0:7 scores, respectively.
For the ﬁne-grained results, our model achieves
the best performance in 6out of 8metrics on both
AMR2.0 and AMR3.0, which shows the effective-335
ness of our method. Although Cai and Lam (2020)
outperforms our model in Neg. and Wiki. on
AMR2.0, they adopt a complex process, which
may hurt the model generalization ability. Bevilac-
qua et al. (2021) outperforms slightly our model
in Conc. and Wiki. on AMR3.0. However, these
metrics are unrelated to the AMR structure that our
HCL focuses on.
4 Analysis
Structure Beneﬁt In order to explore the ef-
fectiveness of our HCL framework for the struc-
tured AMR parsing. We divide the ﬁne-grained
F1 scores into 2categories, “structure-dependent”
(unlabelled, re-entrancy and SRL) and “structure-
independen” (the left 5metrics). Please refer
to Appendix C for the reason for this division.
As shown in Table 1, compared with Bevilac-
qua et al. (2021) (also a sequence-to-sequence
model based on BART-large), our method achieves
2:97and 2:83average F1 scores improvement on
3structure-dependent metrics on AMR2.0 and
AMR3.0, respectively, which proves HCL helps
the ﬂat sequence-to-sequence model better adapt to
AMR with the hierarchical and complex structure.
Hard Instances Beneﬁt Figure 4 shows the per-
formances of our HCL and Bevilacqua et al. (2021)
(SPRING) at different layers. As is shown, as the
number of layers increases, HCL exceeds SPRING
greater, which shows our HCL helps the model
better handle hard instances.In addition, to some
extend, out-of-distribution (OOD) instances can be
regarded as hard instances, thus we also consider
the OOD situation. Bevilacqua et al. (2021) pro-
pose the OOD evaluation for AMR parsers. Follow-
ing Bevilacqua et al. (2021), we train our model on
the training dataset of AMR2.0, and then evaluate
it on 3 OOD test datasets, BIO, TLP and News3.
Please refer to Appendix B for details of OOD
datasets. As shown in Table 3, our method out-
performs Bevilacqua et al. (2021) on all 3 OOD
datasets, which shows our HCL framework can also
improve the generalization ability of the model.
Ablation Study To illustrate the effect of our pro-
posed curricula. We conduct ablation studies by
removing one curriculum at a time. Table 2 shows
the scores on both AMR2.0 and AMR3.0.
As shown in Table 2, we can see both curricula are
conducive to the performance of the model, and
they are complementary to each other. Speciﬁcally,
the structure-level curriculum (SC) is more effec-
tive than the instance-level curriculum (IC). We
think the reason is that SC constructs AMR sub-
graphs for training, which enhances the model’s
ability to perceive the AMR hierarchy.
5 Conclusion
In this paper, we propose a Hierarchical Curricu-
lum Learning (HCL) framework for sequence-
to-sequence AMR parsing, which consists of
Structure-level Curriculum (SC) and Instance-level
Curriculum (IC). inspired by human cognition, SC
follows the principle of learning the core concepts
of AMR ﬁrst, and IC obeys the rule of learning
easy instances ﬁrst. SC and IC train the model on
different hierarchies (AMR sub-graphs and AMR
full graphs). Extensive experiments on AMR2.0,
AMR3.0, structure-complex and out-of-distribution
situations verify the effectiveness of HCL.336Acknowledgement
The authors would like to thank the anonymous re-
viewers for their thoughtful and constructive com-
ments, and Bevilacqua et al. (2021) for their high-
quality open codebase. This paper is supported by
the National Key Research and Development Pro-
gram of China under Grant No. 2020AAA0106700,
the National Science Foundation of China under
Grant No.61936012 and 61876004, and NSFC
project U19A2065.
References337338A Linearzation
As shown in Figure 5, following Bevilacqua et al.
(2021), the AMR graph is linearized by the DFS-
based linearization method according to the edge
order (‘:ARG0’!‘:ARG1’!‘:ARG2’). Variables
of the AMR graph are set to a series of special
tokens <R0>, <R1>, <R2>, <R3>, <R4> , and
the depth is marked by parentheses.
B Datasets
B.1 In-domain Distribution
AMR2.0 (LDC2017T10) contains 36;521,
1;368and 1;371sentence-AMR pairs in training,
development and testing sets, respectively.
AMR3.0 (LDC2020T02) is larger than AMR2.0
in size, which contains 55;635,1;722and 1;898
sentence-AMR pairs for training development and
testing set, respectively. AMR3.0 is a superset of
AMR2.0.
B.2 Out-domain Distribution
BIO is a test set of the Bio-AMR corpus, consist-
ing of 500instances.
TLP is a AMR dataset annoated on the children’s
novel The Little Prince (version 3.0), consisting of
1;562instances.
New3 is a sub-set of AMR3.0, which is not in-
cluded in the AMR2.0 training set, consisting of
527instances.
C Fine-grained Metric Division
There are 8ﬁne-grained AMR metrics: (1) Unla-
beled : Smatch score computed on the predicted
graphs after removing all edge labels. (2) No
WSD. : Smatch score while ignoring Propbanksenses (e.g., duck-01 vs duck-02). (3) Named Ent. :
F-score on the named entity recognition (:name
roles). (4) Wikiﬁcation : F-score on the wikiﬁ-
cation (:wiki roles). (5) Negation : F-score on
the negation detection (:polarity roles). (6) Con-
cepts : F-score on the concept identiﬁcation task.
(7)Reentrancy : Smatch computed on reentrant
edges only, e.g., the edges of node ‘I’ in Figure A.
(8)SRL : Smatch computed on :ARG-i roles only.
We only regard Unlabeled, Reentrancy and SRL
as “structure-dependent” metrics, since: (1) Unla-
beled does not consider any edge labels, and only
considers the graph structure. (2) Reentrancy is a
typical structure feature for the AMR graph. With-
out reentrant edges, the AMR graph is reduced to a
tree. (3) SRL denotes the core-semantic relation of
the AMR, which determines the core structure of
the AMR. (4) As described above, all other metrics
have little relationship with the structure.
D Case Study
Figure 6 shows a case study (we omit some de-
tails of AMR graphs for a more clear description).
As is illustrated, our method achieves the right
AMR for the input sentence. However, the AMR
parsed by the SPRING model (depth:5) is shal-
lower than the gold AMR (depth:9), and their struc-
tures are also different (e.g., the root of the gold
AMR and the SPRING parsed AMR are ‘possible-
01’ and ‘and’, respectively). This case intuitively
shows our HCL framework can help the model
better handle the hard instance with complex struc-
ture.339