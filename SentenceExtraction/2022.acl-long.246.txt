
Swaroop MishraArindam MitraNeeraj VarshneyBhavdeep Sachdeva
Peter ClarkChitta BaralAshwin KalyanArizona State UniversityMicrosoft ResearchAllen Institute for AI
Abstract
Given the ubiquitous nature of numbers in
text, reasoning with numbers to perform sim-
ple calculations is an important skill of AI
systems. While many datasets and models
have been developed to this end, state-of-
the-art AI systems are brittle; failing to per-
form the underlying mathematical reasoning
when they appear in a slightly different sce-
nario. Drawing inspiration from GLUE (Wang
et al., 2018) that was proposed in the con-
text of natural language understanding, we
propose NGLUE, a multi-task benchmark
that evaluates the performance of AI systems
on eight different tasks, that at their core re-
quire simple arithmetic understanding. We
show that this benchmark is far from being
solved with neural models including state-of-
the-art large-scale language models perform-
ing signiﬁcantly worse than humans (lower
by 46.4%). Further, NGLUE promotes
sharing knowledge across tasks, especially
those with limited training data as evidenced
by the superior performance (average gain of
3.4% on each task) when a model is jointly
trained on all the tasks as opposed to task-
speciﬁc modeling. Finally, we hope that
NGLUE will encourage systems that per-
form robust and general arithmetic reasoning
within language, a ﬁrst step towards being able
to perform more complex mathematical rea-
soning.
1 Introduction
Reasoning with numbers is an important skill that
occurs in various day-to-day scenarios and not
surprisingly, numbers are ubiquitous in textual data.
To train AI reasoning systems that can perform
simple mathematical reasoning, many tasks have
been proposed (Dua et al., 2019b; Ravichander
et al., 2019; Koncel-Kedziorski et al., 2016).
Despite these efforts, current state-of-the-art AIOriginal Word Problem
John had 5 apples. He gave 3 to Peter. How
many apples does John have now?
Fill In The Blanks Format
John had 5 apples. He gave 3 to Peter. John has
apples now.
NLI Format
Premise: John had 5 apples. He gave 3 apples to
Peter. Hypothesis: John has 2 apples now. Does
the hypothesis entail, contradict or is neutral to
the premise?
Comparison Format
John had 5 apples. He gave 3 to Peter. Who has
more apples?
Figure 1: A system that can robustly perform numeric
reasoning over language should be able to solve prob-
lems such as the above, regardless of how the problem
is posed. However, we observe existing systems are
brittle; producing inconsistent solutions to such minor
stylistic variations.
systems are brittle and fail when problems involv-
ing similar mathematical reasoning is posed in a
slightly different manner. For instance, presenting
a word problem in a different manner as shown in
ﬁg. 1, while hardly affecting human performance,
is sufﬁcient to confuse state-of-the-art AI systems.
This brittleness in reasoning indicates that the
models latch on to spurious signals in the speciﬁc
dataset resulting in “solving” the dataset while
not truly understanding the underlying reasoning
skill of simple arithmetic. Further, we believe that
building AI systems that can truly understand and
apply simple arithmetic reasoning is a mandatory
ﬁrst step towards successfully tackling complex3505mathematical reasoning skills (Saxton et al., 2019;
Hendrycks et al., 2020, 2021).
NumGLUE. To this end, we propose NGLUE ,
a multi-task benchmark consisting of eight
different tasks that at their core test for arithmetic
reasoning skills. For example, as discussed in ﬁg. 1,
tasks can involve word problems presented in a
slightly different manner or can involve additional
reasoning strategies like commonsense reasoning
or reading comprehension to be combined with the
core skill of simple arithmetic. Our benchmark
consists of four new tasks in addition to four
existing ones; with 100Kproblems spread
across eight differet tasks. The motivation behind
NGLUE is similar to GLUE (Wang et al.,
2018, 2019), a multi-task benchmark that aimed
at models that demonstrated superior language
understanding by learning the underlying linguistic
features. NGLUE is designed with goal of
progressing towards AI systems that are capable
of performing arithmetic reasoning in a general
setting; achieving superior performance on our
benchmark requires the ability to correctly identify
and perform the underlying arithmetic reasoning
without relying on task or dataset-speciﬁc signals.
Finally, we hope that NGLUE will encourage
systems that perform robust and general numeric
reasoning within language, a ﬁrst step towards
being able to perform more complex mathematical
reasoning.
Contributions.
1. We introduce NGLUE– a multi-task bench-
mark consisting of eight different tasks, includ-
ing 4 new ones, whose solution at its core re-
quires an understanding of simple arithmetic.
2.We demonstrate that NGLUE is a challeng-
ing benchmark even for state-of-the-art large
scale language models, obtaining poor scores
not only in zero or few shot settings but also after
ﬁne-tuning. This indicates a fundamental barrier
for AI systems; one that needs to be breached
before complex mathematical challenges can be
successfully tackled.
3.Finally, we propose a memory-augmented neu-
ral model to demonstrate the utility of such a
multi-task meta dataset. Our proposed model
when trained on the entirety of NGLUE ob-
tains an average improvement of 3.4% on each
task as opposed to task-speciﬁc training – in-dicating that joint training leads to beneﬁcial
transfer owing to the common theme of arith-
metic reasoning.
2 Related Work
Datasets for Numerical reasoning. Quantitative
reasoning has been a challenging problem for a
long time. Small question answering datasets were
proposed to understand the quantitative aspect
of natural language such as the template-based
dataset which solved questions with equations
as parameters (Kushman et al., 2014), addition-
subtraction dataset (Hosseini et al., 2014) and
arithmetic problems dataset (Koncel-Kedziorski
et al., 2015). Difﬁculty of questions were increased
in subsequent datasets (Roy and Roth, 2016),
(Upadhyay et al., 2016). Later, larger datasets
were created to facilitate deep learning research
(Ling et al., 2017; Dua et al., 2019b). Several other
maths datasets have been proposed to improve
explainability (Amini et al., 2019), diversity
(Miao et al., 2020), scale information in language
embeddings (Zhang et al.) and hardness of math
questions (Hendrycks et al., 2021).
One of the motivations behind creating this
benchmark is to test for simple arithmetic reason-
ing independent of the context or the presentation
style of the problem. Further, To the best of
our knowledge, our work is the ﬁrst to consider
multiple tasks in the numerical reasoning space.
Multi-Task Benchmarks. With increased
success of deep learning based models on indi-
vidual tasks, there has been a signiﬁcant push
both in the NLP community and in the broader
AI community towards general purpose models
that excel at multiple tasks. Naturally, various
benchmarks and challenges that test for such
understanding have been proposed. For instance,
the BAbI dataset (Weston et al., 2015), GLUE
(Wang et al., 2019) and the subsequent harder
SuperGLUE (Wang et al., 2019) were proposed
to both evaluate and drive progress in language
understanding via shared linguistic knowledge
across tasks. McCann (McCann et al., 2018) build
a multi-task dataset via a novel approach – format-
ting each task as that of question-answering. In the
more restricted setting of reading comprehension,
Dua et al. (2019a) and Downey and Rumshisky
build a meta-dataset that spans multiple domains3506and reasoning skills.
Multi-task Models. With the growing inter-
est towards models that go beyond speciﬁc
datasets, various neural models that can perform
mutliple tasks have been proposed. When the
underlying reasoning is similar – eg. commonsense
reasoning, problem decomposition or linguistic
understanding – it has been found that training on
multi-task datasets yields more robust and accurate
models. For instance, the Multi-task Question
Answering Network (McCann et al., 2018), T5
(Raffel et al., 2019), GPT3 (Brown et al., 2020)
and GPT3-Instruct models aim to build general
purpose language models that are capable of
transferring linguistic understanding across tasks.
A similar approach is taken by Khashabi et al.
(2020) in the setting of question-answering and
Lourie et al. (2021) in the scope of commonsense
reasoning. Further, Muppet (Aghajanyan et al.,
2021) adds an additional step of pre-ﬁnetuning
between pretraining and ﬁnetuning that improves
generalization to multiple tasks.
3 NGLUE
As mentioned previously, our NGLUE bench-
mark consists of both new and already existing
arithmetic reasoning tasks. We ﬁrst begin by
introducing the novel datasets curated by us before
providing a brief overview of existing tasks that
are part of NGLUE . Finally, in this section, we
provide an analysis of the datasets demonstrating
that it contains interesting and diverse linguistic
and mathematical properties.
NGLUE Benchmark. Our proposed
NGLUE benchmark is a collection of eight
different tasks that together include 100K
questions. The tasks may either be self-contained
or require additional background knowledge
(e.g.commonsense reasoning) to arrive at the
ﬁnal solution; however, all the tasks, at their
core, involve arithmetic reasoning. Table 1
shows an example question belonging to each
task along with indicating the total number
of data points associated with each task. It is
important to note that tasks are imbalanced with
only400examples for Task 1 and nearly 50K
questions under Task 5. While we could have
under-sampled the questions to create a balanced
suite, we retain the imbalanced dataset in orderto mimic the real world – for instance, arithmetic
word problems are more abundant as opposed
to word problems that may require common-
sense reasoning in addition to arithmetic reasoning.
Data Partition and Evaluation. We ran-
domly partition data in each task into training
(70%), development (10%) and test (20%) sets . In
the case of reading comprehension tasks (Task 5
and 6), we assign all questions corresponding to a
passage to the same split – we do this in order to
discourage any data leakage and thereby, allowing
models to potentially rely on memorization to
arrive at the correct answer.
For each task, we report the F1 measure
and as an aggregate measure of performance on
theNGLUE benchmark similar to Dua et al.
(2019b), we report the (unweighted) average of the
F1 scores corresponding to each task.
3.1 Novel Datasets
The novel tasks proposed as part of NGLUE are
a combination of both freshly collected data and
intelligent modiﬁcations of already existing
datasets. The four novel arithmetic reasoning tasks
introduced are as follows:
Task 1: Commonsense + Arithmetic Rea-
soning. Consider the following question – How
many faces do 10 dice have? Answering this not
only requires simple arithmetic i.e.multiplying the
number of faces in a die by ten but also requires
knowing that a standard die has six faces. We
collect this dataset by ﬁrst asking the annotator
to write down a numerical commonsense fact
(e.g.a human has 2 hands, a day has 24 hours etc.)
and then use frame a question that requires using
this numerical fact as part of a simple arithmetic
calculation.
Task 2: Domain Speciﬁc + Arithmetic Reason-
ing. How many units of hydrogen are required
to produce 10 units of water? This question,
similar to the previously introduced task of
arithmetic reasoning questions, requires additional
domain-speciﬁc knowledge – speciﬁcally, that each
unit of water contains two units of hydrogen. We3507
curate a dataset of such questions that require both
domain-speciﬁc knowledge and arithmetic rea-
soning motivated by the ﬁnding that QA systems
perform poorly on the ARC dataset Clark et al.
(2018) consisting of grade-school level science
questions. Speciﬁcally, the dataset collected by us
requires understanding of a small set of chemistry
(conservation of mass in chemical reactions) and
physics principles ( speed{).
Task 3: Commonsense + Quantitative Compar-
ison. A golf ball weighs 40g and a baseball weighs
150g. Which has a higher gravitational force?
Answering this question requires both knowing
that mass is directly proportional to gravitational
force and a numerical comparison via subtraction.
We collect such quantitative comparison questions
by using the QuaRel dataset (Tafjord et al., 2019)
containing questions from diverse ﬁelds such as
physics and economics as the starting point. The
annotator chooses a subset of these questions that
involve numerically comparable quantities (for
instance, in this example, mass of the objects
involved) to create the required task of quantitative
comparison questions.
Task 4: Fill-in-the-blanks Format. Unlike the
previously proposed tasks that require external in-formation (e.g.commonsense knowledge) in addi-
tion to simple arithmetic reasoning, this task is self-
contained but a stylistic variant of existing math
word problems. We source word problems from
the Arithmetic Word Problem repository (Roy and
Roth, 2016, 2017, 2018) and convert them into the
ﬁll-in-the-blanks format. For an example of such a
conversion, refer to ﬁg. 1.
3.2 Existing Datasets
We now review existing datasets while discussing
any modiﬁcations made when including them
inNGLUE . In general, for all the datasets
included, we perform a ﬁltering step to clean
and control for the quality of the data points
being included. This step includes – a) discarding
questions that do not have answer annotations b)
eliminating questions with high lexical overlap
with the remainder of the dataset and c) ﬁxing
any type mismatches present in the data (e.g.“7.0
students”Ñ“7 students”).
Task 5: Reading Comprehension (RC) +
Explicit Numerical Reasoning. We select a
subset from the DROP (Dua et al., 2019b) dataset
to create this task. Speciﬁcally, the selected
questions involve reading comprehension and
numerical reasoning but importantly, the required3508answer is also a number.
Task 6: Reading Comprehension (RC) +
Implicit Numerical Reasoning. Consider the
following question based on a relevant passage –
Which state has the highest income tax rate? Here,
while the ﬁnal answer is a name, arriving at it
requires performing comparison (i.e.subtraction).
We classify such questions in the DROP dataset as
a separate task in NGLUE.
Task 7: Quantitative NLI EQUATE (Ravichan-
der et al., 2019) introduces quantitative NLI
questions that require simple arithmetic calcula-
tions to be performed in order to accurately classify
the relationship between the provided premise and
the hypothesis. As noted in ﬁg. 1, many word
problems can also be easily converted to this
format and is therefore, a diverse and interesting
task for evaluating arithmetic reasoning skills of
AI systems.
Task 8: Arithmetic Word Problems Finally, we
arrive at one of the earliest and extensively studied
class of arithmetic reasoning problems i.e.word
problems. The speciﬁc dataset included as part of
ourNGLUE benchmark is a combination of
multiple datasets proposed by Koncel-Kedziorski
et al. (2016), (Koncel-Kedziorski et al., 2015) and
Kushman et al. (2014). Further, to ensure that the
benchmark as a whole is diverse, we eliminate
questions that have a high sentence similarity with
questions from the ﬁll-in-the-blanks task.
3.3 Data Quality Analysis:
In order to ensure a high-quality test set, three in-
dependent annotators evaluate each question in the
test set across all tasks. A tiny porton of the data
marked as invalid or with disagreement between
the annotators was excluded, resulting in a veriﬁed,
high-quality NGLUE evaluation suite. We also
perform a variety of analysis and ﬁnd that the novel
question tasks we created (task 1-4) have higher
quality than the existing question tasks since they
have higher average vocabulary (number of unique
words per number of samples), higher number of
unique nouns, verbs and other POS tags and have
less semantic textual similarity among each other
(indicating lower repetition). Detailed analysis can
be found in the supplementary material: Data Qual-
ity Analysis of NGLUE.4 Experiments
In this section, we establish multiple baselines on
our benchmark and discuss their performance.
4.1 Baselines
We evaluate several baselines on our benchmark
– (i) Heuristic, (ii) Zero-shot, (iii) Few-shot, (iv)
Fine-tuning and (v) Human. We use two kinds
of model architectures (i) Neuro-symbolic, a
memory augmented novel architecture that extends
Numnet+v2 (Ran et al., 2019) and (ii) End-to-end,
GPT3 (Brown et al., 2020).
Architectures. In the multi-task setting where the
same model is trained on all the NGLUE tasks,
we use Reading Comprehension (RC) as the
common format – converting each task to RC
format via a set of hand-coded rules. In addition
to being capable of faithfully representing all
the constituent tasks, the RC format also allows
us to inject additional context in the IR setting
without affecting the rest of the pipeline. On
the other hand, GPT3 being a generative model
does not require such modiﬁcations. Importantly,
note that both models are inputted the exact same
information for the multi-task experiments.
Heuristic Baselines with Task Oracle. For
this baseline, we assume a task oracle that knows
the task a particular question belongs (in a multi-
task setting) – we use this to make our heuristic
baselines more competitive. The ﬁrst heuristic
baseline is random : we randomly select one of the
options in case the question has multiple options
(task 3 and 7), a number between 0 to 100 for
questions having a numerical answer and a random
entity present in the passage for questions having a
text segment from the passage as the answer. In
themajority baseline, we select the most frequent
answer for each task such as "Entailment" for
NLI questions and similarly, the most frequent
number for questions having numerical answer
and the major entity present in the passage for
questions having span based answer. As the task
information is known, we include these baselines
under task-speciﬁc baselines when discussing
results.3509
Zeroshot and Fewshot Baselines. We use
GPT3 (Brown et al., 2020) and the more recent
GPT3-Instruct. We have two types of few shot
baseline (i) task speciﬁc and (ii) multi task. In
case of task speciﬁc fewshot baseline, instances
of the same task are used as in-context examples
(Brown et al., 2020) whereas in case of multitask
few shot baseline, instances from all tasks are
used to condition the model. Multitask fewshot is
naturally a harder setting as it is task-agnostic. We
use default parameters in GPT3 and GPT3-Instruct.
In few-shot setting, we experiment after feeding as
many examples as it can ﬁt within the tokensize.
For few shot experiments, we randomly selectexamples and averaged the results over 5 runs.
Fine-tuning Baselines. We ﬁrst consider
variations of the ﬁne-tuning baselines in the
context of our neuro-symbolic model, Ex-NumNet.
We use it as bias-checking baseline – to ensure
that solving the benchmark correctly requires
considering all of the information presented to
it. To this end, we evaluate the performance of
our model when ﬁnetuned only on the question
(Q-only) or the context (C-only). Next, we present
task-speciﬁc and multi-task baselines where
Ex-NumNet is ﬁne-tuned on individual tasks and
the entire NGLUE benchmark respectively.
With the goal of addressing the data imbalance
across the tasks, we include an oversampling3510
baseline that oversamples data from tasks with
limited data so as to ensure that the model sees the
same number of examples from each constituent
task.
In addition, we propose a new architectural
modiﬁcation to Ex-NumNet. Noting that our base-
line model Ex-NumNet does not take into account
external knowledge, we create a new enhanced
architecture in the form of a memory-augmented
model that does Information Retrieval (IR) (Khot
et al., 2019) with respect to a knowledge base
we create, MATH KB to identify the needed
knowledge. This is inspired by the observation
that formula book and mathematical knowledge
make the task easier for humans while solving
math questions of various types. We then use this
knowledge in the Ex-NumNet setting. Figure 3
illustrates our approach which leverages our newly
created knowledge base MATH KB . Conditional
IR model is different from the regular IR model in
the sense that, IR is performed only for questions
of task 1 , 2 and 4, since they require external
knowledge to get answered. More details about the
model and the IR process can be found in supple-
mentary material: Proposed Memory-Augmented
Model (A.5 and A.6).
Finally, we discuss ﬁne-tuning baselines in
the context of end-to-end models, speciﬁcally
GPT3. We ﬁnetune the GPT3-13B model (for
which the ﬁnetuning capability has been recentlyprovided by OpenAI) in the multi-task setting i.e.
the desired setting of the NGLUE benchmark.
Human Baseline. Human baseline was cal-
culated on 100 test set samples of each task (81 of
Task 1) by averaging the scores of four annotators.
5 Results and Discussion
Table 2 shows the performance of various baseline
models on the test set of our benchmark. Note
that the performance of all baseline models is
signiﬁcantly lesser than the human baseline (Figure
2). We now discuss various insights based on these
results.
Does the benchmark contain bias that a
model can exploit? A challenging dataset
requires the model to ideally consider all the
information provided to it before arriving at an
answer. To ensure that this is indeed the case, we
perform ablations where only one portion of the
input is provided i.e. either the question or the
context. Both these “bias-checking” baselines
perform poorly even in task-speciﬁc setting –
indicating that both the benchmark and constituent
tasks are challenging.
Which Tasks are Hard to Solve? Our re-
sults show that task 1 which requires numerical
commonsense knowledge, is the hardest task to
solve. Similarly, tasks 2, 4 and 8 appear to be3511comparatively harder from the rest. One pattern
among these tasks is that all of them expect the
answer to be numeric. Numeric answer requires
accurate calculation. So, models might have
difﬁculty in learning the task directly from data.
This hypothesis is also justiﬁed from the slight
drop in human performance in these tasks..
On the other hand, task 7 has the best performance
among all. Further, we see that performance on
task 6 is slightly better than task 5 – although
both tasks are sourced from the same dataset, we
observe that models answer span based questions
better as compared to numeric answers. Relatively
higher performance for task 3 suggests that models
ﬁnd it easier to answer in an MCQ setting.
Does IR Help? Results show that knowl-
edge help in improving performance of tasks 1,
2 and 4 – where indeed, external knowledge like
commonsense or domain-speciﬁc knowledge is
needed in addition to arithmetic reasoning to arrive
at the correct answer. However, task 3 is an excep-
tion to this trend and in fact registers a drop in the
score when provided with (unnecessary) additional
information; we ﬁnd that this shortcoming is
ﬁxed when using conditional information retrieval
(CIR) which in fact leads to the strongest baseline
presented in this work.
Does Oversampling help overcome data
imbalance across tasks? Even though oversam-
pling results in higher performance in certain
tasks (in comparison with the multitask baseline),
speciﬁcally the ones with smaller training data, it
results in signiﬁcant drop in performance in the
other extreme, i.e tasks with bigger training data.
Also, it never performs better than the Conditional
IR module in multitask setting.
5.1 Error Analysis
We now present an analysis of the errors made
by our baselines to indicate potential avenues for
future research.
We analyze errors associated with 50 sam-
ples each of the 8 tasks and ﬁnd that there are
mainly 4 categories of error models make: (1)
producing invalid output (e.g. answering text
where the answer is supposed to be a number,
answering a text different from the classes allowed
in a classiﬁcation problem), (2) copying a number
from the question instead of calculating the answer,
(3) incorrect calculation – this can be due to
multiple reasons including (i) using an incorrect
operation e.g. subtraction in place of addition,
(ii) incorrect parsing of numbers or (iii) incorrect
knowledge of numerical commonsense facts. (4)
producing redundant text after producing correct
answer. Based on error distribution in Table 3,
we observe that the majority of errors come from
incorrect calculation. Further, GPT3 is better than
Ex NumNet+v2 in producing valid outputs, but it
produces more redundant text.
Future Directions: Bigger model, more
data or : : :?Table 2 shows that ﬁne-tuned
GPT3-13B outperforms other baselines on task 1,
2 and 3. Recall that these tasks require external
knowledge and perhaps, this is the reason why
GPT3, already pre-trained on a diverse web-scale
text corpus has an edge over other baselines on
these tasks. In case of the smaller Ex-NumNet, it is
interesting that multitask baselines are higher than
the single task baselines by 3.4% on average and
that information retrieval helps in tasks that require
external knowledge. Also notice that, GPT-3 is
better on smaller datasets and NumNet is better
on large datasets. This may indicate that GPT-3
is a better few-shot learner but not necessarily a
better many-shot learner. This non-overlapping
performance of GPT-3 and Ex-numnet, end-to-end
and neuro-symbolic models respectively, indicates
that a potential future direction for research is to
combine the best of both the models.
6 Conclusion
We propose NGLUE , a multi-task benchmark
to test for arithmetic understanding. Our bench-
mark consists of eight tasks including four new
ones. While some of the tasks require external
knowledge like commonsense or domain-speciﬁc
information in addition to arithmetic reasoning,
some are self-contained e.g. arithmetic word prob-
lems. Further, we demonstrate that our benchmark3512is far from being solved – with state-of-the-art large
scale models achieving considerably lower perfor-
mance than humans. This indicates that current
AI systems are incapable of performing simple
arithmetic reasoning in a general setting – indicat-
ing a fundamental hurdle towards AI systems that
understand complex mathematical concepts like
differential equations or combinatorics. Finally,
we present various baselines including a novel ar-
chitecture (memory augmented Ex-NumNet) that
demonstrate the advantages of various modeling
choices (e.g. end-to-end vs neuro-symbolic mod-
els). Speciﬁcally, we show that training in the
multi-task setting leads to meaningful sharing of
knowledge across tasks as evidenced by an average
gain of 3.4% on tasks compared to task-speciﬁc
modeling. Finally, we hope that our benchmark
not only leads to AI systems that are capable of
performing simple arithmetic reasoning in a fairly
general setting but also results in progress towards
more complex mathematical reasoning capability.
Acknowledgements
We thank OpenAI for providing academic access
to the GPT3 API, the Aristo team at AI2 for help-
ful input, the Beaker team for their support with
experiments and the anonymous reviewers for their
insightful feedback. The support of DARPA SAIL-
ON, DARPA CHESS program is gratefully ac-
knowledged.
Ethical Considerations
We have veriﬁed that all licenses of source datasets
used in this paper allow for their use, modiﬁca-
tion, and redistribution in a research context. The
dataset will be distributed in a manner similar to
SuperGLUE (Wang et al., 2019) i.e. give full credit
assignment to the original data and task creators.
References351335143515A Supplemental Material
A.1 NGLUE vs Other Datasets:
As ﬁgure 4 shows, we select each task from one of
the clusters of numerical reasoning datasets (except
the multi-model reasoning cluster since we wanted
to limit our dataset to text only).
A.2 Construction of NGLUE :
Figure 5 and 6 illustrate detailed data creation pro-
cess for task 1, task 2, task 3 and task 4 questions
with the help of an example for each task. We fol-
low the same procedure for creating other examples
within the task.
A.3 GPT3-Instruct’s Response
We used GPT3-Instruct on various forms of a sim-
ple arithmetic question. An expert did tuning of
various parameteres such as temperature, stop con-
dition, presence penalty, engine, maximum token
size. However, GPT3-Instruct still could not solve
the basic aritmetic questions reliabily (Figures 7-
11).
A.4 Data Quality Analysis of NumGLUE
In this section, we discuss various linguistic and
statistical properties of our benchmark; ones
that we believe result in the quality, diversity
and challenging nature (Gururangan et al., 2018;
Mishra et al., 2020; Mishra and Sachdeva, 2020;
Swayamdipta et al., 2020; Arunkumar et al., 2020)
of the proposed NGLUE benchmark.
Vocabulary Size. First, we calculate vocab-
ulary size of each task by ﬁnding the number
of unique words across all questions. Since our
dataset is unbalanced in terms of question task,
we ﬁnd the average vocabulary size by dividing
vocabulary size with number of data in that task.
Which Data has Higher Average Vocabu-
lary? As illustrated in Figure 12a, most of the
tasks belonging to the novel dataset category
have relatively better average vocabulary size.
This implies questions in those tasks have less
repetitiveness. Furthermore, we expand our
vocabulary analysis to understand Figure 12a
better. We dive deep to analyze different parts
of speech. Figure 12b summarises our analysis.
Most of the novel datasets have more average
number of nouns, verbs and adjectives implying
there are more varieties of entities, actions andattributes. This further means that datasets belong-
ing to the novel category are more diverse in nature.
Sentence Similarity Analysis We extend
our analysis to reinforce our inference from the
word vocabulary analysis. We ﬁnd Semantic
Textual Similarity (STS) of a sentence with every
other sentence.
Which Data Consists of Most Dissimilar
Sentences? As depicted by Figure 12c-12f, most
questions in QuaRel have high similarity value
with other questions indicating the repetitiveness
of data. Same is true for majority of EQUATE
data. DROP also has high similarity among
questions. However, similarity among questions in
our dataset is signiﬁcantly less. Some similarity
boxes can be seen in the chart. They are mostly
due to task 2 data, and partly due to task 3 data.
Lesser similarity implies that our dataset is far
less repetitive than others. Also, the repetition in
our dataset is sparse and is not equally distributed
among the whole dataset unlike others. This way,
our dataset is more diverse.
Note that question in Task 2 have lower vo-
cabulary and further, a higher similarity as well.
As a small set of chemistry and physics principles
are used to generate questions, the result is a fairly
templated or uniform-looking dataset – leading to
the observed reversal of trends in this particular
task.
A.5 Ex-NumNet
Figure 13 illustrates our baseline model: Ex-
NumNet. This contains a Reading Comprehen-
sion Converter module which converts each task of
question to reading comprehension format. Figure
14 illustrates various examples of how each task
of questions get converted to the reading compre-
hension format. We add a task converter module
to detect task of a question. We design task con-
verter heuristically based on the features associated
with questions (e.g. NLI contains "Sentence 1"
and "Sentence 2" whereas completion contains a
blank). We convert each of the tasks to RC format.
For NLI questions, we use the premise sentence
as passage, hypothesis as the question and append
the string “Entailment, contradiction or neutral?”3516
to the question so that it has a span based answer.
For other questions, we tokenize the question string
into its constituent sentences and use a heuristic ap-
proach to split the question string into passage and
question. Furthermore, for option based questions,
we append all the options at the end of the question.A.6 Proposed Memory-Augmented Model
Figure 13 illustrates our baseline model Ex-
NumNet. We add an IR mechanism as described
in Algorithm 1 and illustrated in Figure 3 of the3517
main paper. As mentioned in the ‘Baselines’ sub-
section (Experiments section) of the main paper,
we convert each task to RC format in our baseline
and append the knowledge retrieved using IR from
MATH KB at the end of the passage. In our exper-
iments, we use the following hyperparameters in
the IR process: Z50,v10,th0:75and
b0:1.
Formalization LetDrepresents dataset, srep-
resents sample, Krepresent the MATH KB ,vrepre-
sents the number of knowledge statements retrieved
for each sample, this the cut off STS (Semantic
Textual Similarity) value above which knowledge
statements are treated redundant and removed, bis
the reduction we do iteratively on thuntilvstate-
ments remain.
We create a knowledge base, MATH KB by
accumulating all tasks of external knowledge
which are needed to solve questions of various
tasks (e.g. human has 2 hands, cow has 4 legs,
there are 24 hours in a day etc..). We also add
math formulae required to solve questions in our
benchmark (e.g. the formula of speed in terms
of distance and time). We add alll these in the
form of plain text separated by new line. We
use Elasticsearch to retrieve relevant knowledge
sentences. We further ﬁlter them using a heuristic
threshold of relevance. We append this knowledge
in the beginning of the passage so that continuity is
not broken between passage and question. Figure
3 of the main paper illustrates our approach.Algorithm 1: Our Information Retrieval
Approach
A.7 Hyper Parameters Used
All the experiments were ran with the following
hyper parameters, batch size was kept at 16 where
as the eval batch size was 5. The maximum number
of epoch ran for the experiments were 5 with the
warm-up kept at 0.06. The learning rate used was
1.5e-5 and the weight decay was 0.01.
All above hyper parameters were selected using
a grid search; we kept rest of the hyper parameters
unaltered. All the experiments were performed on
"TeslaV100-SXM2-16GB", with which the model
takes 24hrs to train on nearly 100k samples.
A.8 Additional Examples
We provide additional examples of task 1, 2, 3 and 4
questions here to better illustrate the novel datasets
we have created as part of our NGLUE.351835193520352135223523