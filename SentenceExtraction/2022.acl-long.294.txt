
Nan YuandMeishan ZhangandGuohong FuandMin ZhangSchool of Computer Science and Technology, Soochow University, ChinaInstitute of Computing and Intelligence, Harbin Institute of Technology (Shenzhen), ChinaInstitute of Artiﬁcial Intelligence, Soochow University, China
nyu@stu.suda.edu.cn, mason.zms@gmail.com,
{ghfu,minzhang}@suda.edu.cn
Abstract
Pre-trained language models (PLMs) have
shown great potentials in natural language pro-
cessing (NLP) including rhetorical structure
theory (RST) discourse parsing. Current PLMs
are obtained by sentence-level pre-training,
which is different from the basic processing
unit, i.e. element discourse unit (EDU). To this
end, we propose a second-stage EDU-level pre-
training approach in this work, which presents
two novel tasks to learn effective EDU represen-
tations continually based on well pre-trained
language models. Concretely, the two tasks
are (1) next EDU prediction (NEP) and (2) dis-
course marker prediction (DMP). We take a
state-of-the-art transition-based neural parser
as baseline, and adopt it with a light bi-gram
EDU modiﬁcation to effectively explore the
EDU-level pre-trained EDU representation. Ex-
perimental results on a benckmark dataset show
that our method is highly effective, leading a
2.1-point improvement in F1-score. All codes
and pre-trained models will be released pub-
licly to facilitate future studies.
1 Introduction
Discourse analysis based on rhetorical structure
theory (RST) has received increasing interest in
the natural language processing (NLP) community
(Yu et al., 2018; Liu et al., 2019a; Kobayashi et al.,
2020; Zhang et al., 2020; Guz and Carenini, 2020;
Koto et al., 2021; Zhang et al., 2021), which orga-
nizes discourse output through a well-deﬁned tree
structure. Figure 1 shows an example of an RST
constituent tree, where the leaf nodes are element
discourse units (EDUs). Given an EDU sequence,
RST discourse parsing aims to automatically con-
struct a hierarchical constituent tree.Figure 1: An example of RST discourse tree. e,e,e,
e,e, andeare EDUs. elab ,attr ,circ andsame
are relations. NS,SN, and NNare nuclearities.
The shift-reduce transition-based model has
been widely adopted in RST discourse parsing (Yu
et al., 2018; Mabona et al., 2019), building the con-
stituent tree incrementally with multiple steps by
a sequence of actions. These models take EDU-
level features as inputs to score transition actions
at each step. Recently, neural network models have
achieved state-of-the-art performance for this task
by using sophisticated-designed neural modules
(Yu et al., 2018; Liu et al., 2019a; Mabona et al.,
2019; Zhang et al., 2020; Kobayashi et al., 2020).
In particular, the contextualized pre-trained lan-
guage models (PLMs) such as XLNet (Yang et al.,
2020) is able to achieve an impressive performance,
resulting in F1-score gains of more than 3 points
according to previous studies (Koto et al., 2021;
Zhang et al., 2021; Nguyen et al., 2021) and our
preliminary ﬁndings.
Although great successes have been observed by
the contextualized PLMs (Peters et al., 2018; De-
vlin et al., 2019; Liu et al., 2019b), an apparent mis-
match in the basic processing units exists between
the EDU-level RST parsing and the sentence-level
contextualized language modeling, which might be
unable to fully explore the pre-training paradigm.
Several previous studies have been investigated
to address the mismatch between the target tasks4269and the standard language model pre-training, e.g.,
SpanBERT (Joshi et al., 2020) for extractive ques-
tion answering, BART (Lewis et al., 2020), and
T5 (Raffel et al., 2020) for sequence-to-sequence
(seq2seq) generation, and all these studies achieve
improved performances for their target tasks.
In this study, we investigate a second-stage EDU-
level pre-training based on the above observation.
Concretely, we conduct pre-training from a PLM
with two EDU-level tasks in the second stage. The
ﬁrst task is next EDU prediction (NEP), which
is inspired by next sentence prediction (NSP) in
BERT (Devlin et al., 2019) learning, substituting
the sentences with EDUs. The second task is dis-
course marker prediction (DMP), which is also
inspired by the masked language modeling (MLM)
in BERT (Devlin et al., 2019) learning, substitut-
ing the masked words with the masked discourse
markers. To fully utilize contextualized pre-trained
representations, we adapt a transition-based neural
RST parser that exploits BiEDU representations
with regard to the basic encoding unit instead of
the standard single-EDU manner.
We conduct experiments on RST discourse tree-
bank (RST-DT) (Carlson et al., 2001) to evaluate
the proposed model. First, we derive BiEDU rep-
resentations directly from PLMs, and thus build
a very strong transition-based neural RST parser.
Then, we examine the proposed second-stage EDU-
level pre-training approach. Experimental results
show that the two second-stage pre-training tasks
improve RST parsing greatly, and their combina-
tion leads to further increases. Our ﬁnal model
achieves the top performance among all the models
reported in the literature.
In summary, our contributions are as follows:
•We present a second-stage EDU-level pre-
training approach to address the inconsistency
between the EDU-level RST parsing and the
sentence-level contextualized language model-
ing, aiming for a better pre-training paradigm
for RST parsing.
•We suggest BiEDU-based representations for
neural RST parsing to exploit well pre-trained
language models more effectively.
•We advance the state-of-the-art RST parsing
performance.
2 Second-Stage EDU-Level Pre-training
In this section, we introduce the proposed second-
stage EDU-level pre-training approach. It has two
EDU-level pre-training tasks, termed by NEP and
DMP, respectively. NEP requiries EDU pairs as
inputs, and predicts whether each EDU pair is ad-
jacent. DMP requiries EDU sequences as inputs,
and predicts the masked discourse marker between
two adjacent EDUs.
2.1 Next EDU Prediction (NEP)
NEP is inspired by NSP in BERT (Devlin et al.,
2019) learning. NSP is a binary sentence-level
classiﬁcation task, which determines whether two
sentences are continous. It integrates rich inter-
sentence context features into BERT and thus has
a positive effect on several downstream classiﬁca-
tion tasks, such as PDTB-style discourse relation
classiﬁcation (Shi and Demberg, 2019) and Stan-
ford Natural Language Inference (SNLI) (Bowman
et al., 2015). RST parsing involves the classiﬁca-
tion between two subtrees (a single EDU can also
become a subtree), which is highly similar to above
downstream tasks. Therefore, we believe that a
similar second-stage pre-training task is effective
for RST parsing. Considering that the basic in-
puts of RST parsing are EDUs, we substituting the
sentences with EDUs.
We reimplement a SOTA EDU seg-
menter(Muller et al., 2019) and use it to
segment large-scale unlabeled texts. Based on
EDU segmentation data, we apply NEP to PLM.
Figure 2 shows an overview of NEP. We sample
the continuous EDU pairs as positive instances,
and the non-continuous EDU pairs as negative
instances. It should be noted that these positive
and negative instances are sampled on the same
scale. When the these instances are ready, we use
Equation 4 to pack each EDU pair and calculate its
corresponding EDU representation. Then we use a4270
linear layer to calculate the binary score:
y=Wx+b(1)
whereWandbare the model parameters of
the linear layer, and yindicates whether the two
EDUs are continous. We adopt a cross entropy
function as the training objective of NEP.
2.2 Discourse Marker Prediction (DMP)
We further adopt DMP to pre-train PLMs in the
second stage based on the following considera-
tion. Pitler et al. (2009) point out that if discourse
markers (Schiffrin, 1987) exist in PDTB-style dis-
course parsing, the classiﬁcation of discourse rela-
tion types become easier. RST parsing aims to clas-
sify the relationship between two discourse frag-
ments. By analogy, discourse markers can also
make RST parsing easier.
The framework of DMP is shown in Figure 3.
The input of DMP is an EDU sequence. We only
mask the ﬁrst word in each EDU that starts with a
discourse marker.Then we use Equations 4 and 5
to obtain EDU representations of the masked EDU
sequence. Finally, we feed them into a linear layer
to calculate the discourse marker score:
y=Wh+b(2)
whereWandbare the model parameters of
the linear layer, and yis the score distribution of
the discourse markers. We also use a cross entropy
function as the training objective of DMP.e e e e e eattr -SN
3 Transition-based Neural RST Parser
We adopt a transition-based neural RST parser to
evaluate the second-stage EDU-level pre-training
approach. The model has two key components,
termed by a transition system and a neural network
model, respectively. The transtion system, mainly
borrowed from Yu et al. (2018), formalizes RST
parsing into action sequence predictions, and the
neural model yields EDU representations and out-
puts action sequences.
3.1 Transition System
As shown in Figure 4, our transition system con-
sists of states and actions. A state has two parts,
namely a stack stores partially parsed subtrees and
a queue stores un-parsed EDUs. The initial state is
an empty state, and the ﬁnal state represents a full
RST discourse tree. A action controls the transition
of states. There are three kinds of actions:
•A shift action pops the ﬁrst EDU of the queue
and pushes it into the stack. It can only be
executed when the queue is not empty.
•A reduce action combines the top two sub-
trees of the stack into a new subtree with a
unclearity label and a relation label. It can
only be executed if there are more than two
subtrees are in the stack.
•A pop root action pops a full discourse tree
from the stack, and the parsing process is com-
pleted. It can only be executed when the queue
is empty, and only one element is in the stack.
In summary, the transition system converts a tree
construction into a sequence of action predictions.
By performing the actions, a RST discourse tree is
constructed incrementally. Concretely, given the
example in Figure 1, we perform actions “shift,4271
shift, reduce- attr -SN, shift, shift, shift, reduce-
elab -NS, shift, reduce- same -NN, reduce- circ -SN,
reduce- elab -SN, pop root” to construct a full RST
discourse tree step by step.
3.2 Neural Network Model
The Vanilla Representation We use PLM to en-
code each text, obtaining single-EDU represen-
tations. Concretely, given a text that has been
segmented into EDUs ee, a special symbol
[CLS] is placed at the beginning of each EDU.
Then each EDU is tokenized by byte pair encoding
(BPE) (Sennrich et al., 2016), and encoded by PLM
to obtain contextualized word piece embeddings.
Finally, for each EDU, we choose the following
representation of [CLS] to represent it:
e=[CLS];tt
x;xx=PLM (e)
x=x(3)
where [CLS];ttare word pieces,
x;xxare word piece embeddings,
andxis the single-EDU representation.
Extension with BiEDU The vanilla EDU-based
representation exploits the information by treated
an EDU as the ﬁrst segmentation type, leaving its
segmentation type unused. Here, we make an ex-
tension by using BiEDU representations. Each
input unit is packaged by the current EDU as well
as the previous EDU jointly, forming as BiEDU.
Then [CLS] is placed before the ﬁrst EDU and
[SEP] before the second EDU. We also use BPE
to tokenize it and use a PLM for encoding. We stillchoose the representation of [CLS] to represent
each EDU as follow:
(e;e) =[CLS]t;[SEP]t
xx;xx=PLM (e;e)
x=x(4)
where [CLS]t;[SEP]tare tokens,
xx;xxare word piece embed-
dings, andxis the BiEDU representation.
BiLSTM Encoding Furthermore, we fol-
low Koto et al. (2021), using BiLSTM to obtain
high-level EDU representations:
hh=BiLSTM (xx) (5)
wherehhare ﬁnal EDU representations. In
addition, we follow Zhang et al. (2021) and Koto
et al. (2021), using paragraph features to further
enhance the high-level representations.
Decoder The decoder part predicts the next-step
action based on a given state. We follow Yu et al.
(2018), selecting the three subtrees ( s,s,s) at
the top of the stack and the ﬁrst EDU ( q) in the
queue to represent the current state. We calcu-
late the subtree representation by the average of its
EDU representations. We concatenate three subtree
representations ( h,h,h) and an EDU repre-
sentation (h), and input them into a linear layer
to calculate the score distribution of the action:
y=W(hhhh) +b (6)
whereW,bare model parameters and is a con-
catenation operation. During the inference, at each
step, we exploit the highest-scored action as the
output. When actions are ready, we perform them
to construct the coresponding RST discourse tree
step by step according to the transition system in-
troduced in Section 3.1.
Training We adopt a cross-entropy loss plus with
lregularization term as an objective function to
train our RST parser. Given a state, we obtain ac-
tion scores according to the neural network model
and compute the probability of the gold action by
softmax. Finally, we feed it into the objective func-
tion for loss calculation as follows:
p= softmax(y)
L() = log(p[a]) +jjjj
2(7)4272whereais the gold-standard action of the i-th step,
is a set of model parameters of our RST parser,
andis thelregularization factor. We use Adam
algorithm (Kingma and Ba, 2015) to optimize the
model parameters of our neural network model.
4 Experiments
4.1 Settings
Datasets To show the proposed model is com-
parable with previous state-of-the-art systems for
RST parsing, we conduct experiments on RST-
DT(Carlson et al., 2001). It is a standard bench-
mark dataset for this task, which is collected from
the Wall Street Journal news. It has been divided
into training and test sets, which have 347 dis-
courses and 38 discourses, respectively. We ran-
domly select 35 discourses from the training set to
develop our model. The original RST-DT contains
78 ﬁne-grained discourse relations. Most of previ-
ous studies simplify these ﬁne-grained discourse
relations to 18 coarse-grained relations. To facili-
tate comparison with previous studies, we also use
18 simpliﬁed coarse-grained relations.
To show the domain generalization capability of
our proposed RST parser to unseen domain articles,
we test it on the georgetown university multilayer
(GUM) corpus. It contains small-scale articles
annotated based on RST in several domains, such
as news, ﬁction, conversations, and etc. For more
details, one can refer to their paper (Zeldes, 2017).
The training corpus for second-stage EDU-level
pre-training contains unlabeled large-scale col-
lected from a English Wikipedia corpus. Although
using a unlabeled news corpus may lead to greater
improvements, we ﬁnd that using a Wikipedia cor-
pus is sufﬁcient to provide new SOTA results.
Evaluation We use the evaluation recommended
by Morey et al. (2017), which attaches nuclearity
and relation labels to non-leaf trees to eliminate
redundant evaluations. The evaluation includes
four metrics, termd by Span, Nuclearity, Relation,
and Full, respectively. Span evaluates the skele-
ton of the discourse tree. Nuclearity evaluates the
discourse tree with nuclearity labels. Relation eval-
uates the discourse tree with relation labels. Full
evaluates the complete discourse tree with nuclear-
ity and relation labels.Hyper-parameters There are several hyper-
parameters in our proposed second-stage EDU-
level pre-training approach and RST parser. In
NEP, the learning rate of PLM is set to 5e-6, and
the learning rate of the other model parameters is
set to 1e-3. The batch size is set to 50. The max-
imum norm of gradient clipping is set to 1. The
maximum tranining epoch number is set to 10. In
DMP, the learning rate of PLM is set to 1e-6, and
the learning rate of the other model parameters is
set to 1e-4. The batch size is set to 1. The output
hidden size of LSTM is set to 200. The settings of
maximum training iteration number and the norm
of gradient cliping are the same as NEP.
The hyper-parameters of our RST parser are
tuned based on the preliminary results on the devel-
opment set. The hidden size of all neural layers is
set to 200. The dropout is set to 0.25. The learning
rate of PLM is set to 2e-5, and the learning rate of
other model parameters is set to 1e-3. The maxi-
mum norm of gradient clipping is set to 1, and the
maximum training iteration number is set to 20.
We use transformers library (Wolf et al., 2020)
to implement PLM and use PyTorch (Paszke et al.,
2019) to implement other neural network modules.
4.2 Development Results
We conduct several development experiments to
show the important factors that inﬂuence the per-
formance of our RST parser.
Different Pre-trained Language Models First,
we test our proposed RST parser based on several
publicly available PLMs such as BERT (Devlin
et al., 2019), RoBERTa (Liu et al., 2019b), XL-
Net (Yang et al., 2020), SpanBERT (Joshi et al.,
2020), and DeBERTa (He et al., 2020). The max
input length of BERT, RoBERTa, SpanBERT, and
DeBERTa is 512 tokens. Therefore, we extend
them with BiEDU to better exploit these PLMs.
Since XLNet has no input length limit, we do not
need to apply BiEDU extension to our XLNet RST
parser. Table 1 shows the performances of with
different PLMs. We ﬁnd that our BiEDU exten-
sion is able to further improve the performances
of these PLM-based RST parsers. The SpanBERT
RST parser achieves worst performance among
these RST parsers. It is probably because that the
basic processing units of SpanBERT learning are
not matched with RST parsing. The XLNet RST
parser achieves the best performance among these
RST parser. Therefore, following experiments are4273NEP505152
DMP50515253
0k 30k 60k 120k 240k
conducted based on the XLNet RST parser.
Unlabeled Article Size We study how the unla-
beled articles size in second-stage EDU-level pre-
training inﬂuences the performance of our RST
parser. First, we apply NEP to PLMs. As shown in
Figure 6, the performance of our RST parser shows
a similar trend when increasing the size of unla-
beled articles to perform DMP based pre-training.
When the size of the unlabeled articles reaches 30k,
the Full metric reaches its peak. Therefore, we use
30k unlabeled articles in NEP.
Then, we adopt DMP to further pre-train the
PLM part of our RST parser in the second stage.
As can be seen from Figure 6, the performance of
our RST parser ﬁrst increases and then decreases as
the size of the unlabeled articles as the size the un-
labeled articles gradually increases from 0 to 240k.
When the size of the unlabeled articles reaches
120k, the Full metric reaches its peak. Therefore,
we use 120k unlabeled articles in DMP. Above ex-
perimental results show that we do not need an
ultra large-scale unlabeled corpus for our proposed
second-stage EDU-level pre-training approach.
4.3 Final Results
As shown in Table 2, we report main results on
the RST-DT test set. Our proposed RST parser
achieves 73.4 on the Span metric, 63.3 on the Nu-
clearity metric, 52.4 on the Relation metric, and
51.4 on the Full metric, exceeding most of the
previous state-of-the-art systems. When we ap-
ply second-stage EDU-level pre-training to XLNet,
it achieves 76.4 on the Span metric and 66.1 on
the Nucleairty metric, resulting a Full metric im-
provement 53.5 - 51.4 = 2.1. The Span, nuclearity,
and relation metrics have similar tendencies as well.
In addition, we implement a top-down RST parser,
and also enhance it with using our proposed second-
stage EDU-level pre-training approach. We ﬁnd
that the proposed approach is able to improve the
performance of top-down RST parser as well.
We compare our proposed RST parser with previ-
ous state-of-the-art systems. Feng and Hirst (2014)
propose a linear-chain conditional random ﬁeld
(CRF) parser. Ji and Eisenstein (2014) adopt a
statistical transition-based parser with a represen-
tation learning. Surdeanu et al. (2015) employ a
perceptron and a logistic regression to parse a text.
Li et al. (2016) propose a hierarchical neural parser
with attention. Joty et al. (2015) propose an intra-
sentential and multi-sentential parser. Hayashi et al.
(2016) reimplement the HILDA parser (Heilman
and Sagae, 2015), using a linear SVM classiﬁca-
tion to parse a text from the bottom up. Braud
et al. (2016) present a BiLSTM RST parser with
multi-task learning. Braud et al. (2017) propose a
neural greedy parser with cross-lingual recourse.
Yu et al. (2018) propose a transition-based neural
parser, and further enhance it with hidden-layer vec-
tors extracted from a neural syntax parser. Mabona
et al. (2019) propose a generative RST parser with
beam search. Zhang et al. (2020) propose a top-4274
down neural parser. Koto et al. (2021) propose
a transformer top-down parser with dynamic or-
cale. Nguyen et al. (2021) propose a seq2seq neural
parser based on point network. Koto et al. (2021)
propose a sequence labelling parser with dynamic
oracle. Zhang et al. (2021) propose a neural top-
down parser with adversarial learning. As shown
in Table 2, our transition-based XLNet RST parser
achieves the best performance among the systems
studied on the Span and the Nuclearity metrics.
We ﬁnd that the Relation and the Full metrics of
our RST parser are lower than that of Zhang et al.
(2021). It is probably because that our proposed
second-stage EDU-level pre-training approach only
requires predicted EDU segmentation, lacking the
information of predicted RST discourse trees.
4.4 Analysis
In this section, we conduct several analysis experi-
ments from different aspects to better understand
the proposed RST parser.
Ablation Studies Here we conduct several abla-
tion experiments to examine the effectiveness of
our proposed second-stage EDU-level pre-training
approach and paragraph features. As shown in Ta-
ble 3, we ﬁnd that NEP and DMP are effective for
RST discourse parsing. NEP improves our XLNet
RST parser by an increase of 52.1 - 51.4 = 0.7 on
the Full metric. The tendency of DMP is similar to
NEP, obtaining an increase of 52.6 - 51.4 = 0.8 on
the Full metric. Our proposed model can be further
improved when two EDU-level tasks are applied to
XLNet, resulting the Full metric improvement 53.5
- 51.4 = 2.1. In addition, the paragraph features
is also effective for RST discourse parsing, which
results the overall improvements.
Effect of EDU Segmentation Performance As
mentioned earlier, the second-stage EDU-level pre-
training approach requires EDU segmentation pro-
duced by a supervised EDU segmenter. Predicted
EDU segmentation could have errors, which may
propagate into RST parsing. Here we examine how+NEP+DMP51525354
0 (0.0%) 10 (57.0%)
100 (80.5%) 300 (96.0%)
the performance of the supervised EDU segmenter
inﬂuence the performance of RST parsing. The full
EDU segmenter is trained on 300 discourses. We
retrain two weaker EDU segmenters on 10 and 100
discourses. Figure 7 shows the RST parsing perfor-
mances with different EDU segmenters. We ﬁnd
that the EDU segmentation performance inﬂuences
the RST parsing quality, indicating the importance
of correct EDU segmentation.
Analysis by Number of EDUs in Subtrees As
mentioned earlier, NEP predicts whether each EDU
pair is continous, and it is able to integrate rich
inter-EDU context features into PLMs. Therefore,
it is expected that the introduce of NEP may bring
better improvements for the spans containing more
EDUs. As such, here we investigate the beneﬁt by
using NEP. Table 4 shows the comparison results.
We ﬁnd that performances are improved signiﬁ-
cantly when spans contains more EDUs.
Effect of Different Sampling Strategies Forther-
more, we examine how different EDU pair sam-
pling strategies inﬂuence RST discourse parsing.
The training set of NEP is sampled from a large-
scale unlabeled corpus. We sample the continuous
EDU pairs as the positive instances and the non-
continuous EDU pairs as the negative instances.
The difﬁculty of NEP changes depending on how
the non-continuous EDU pairs are sampled. Here
we compare four strategies of sampling the non-
continuous EDU pairs: from a sentence, two adja-
cent sentences, two sentences in an article, and two4275
different articles, respectively. Table 5 shows the
comparsion results. We ﬁnd that these sampling
strategies do not make difference to RST parsing.
Analysis by Number of Discourse Markers As
mentioned earlier, DMP predicts the masked dis-
course markers of an EDU sequence and discourse
markers are essential cues for RST parsing. There-
fore, it is expected that the introduce of DMP may
bring better performance for the spans containing
discourse markers. As such, here we investigate
the beneﬁt by adopting DMP. Table 6 shows the
performance of our XLNet RST parser with DMP
and without DMP. The performances are improved
signiﬁcantly when spans contain discourse markers,
which is consistent with our intuitions.
Effect of Different Masking Strategies Then
we change the masking strategy in DMP to show
how different masking strategies inﬂuences RST
parsing. We use a random word set to replace the
discourse marker set in DMP. The number of ran-
dom words is the same as the number of discourse
markers. Compared with discourse markers, these
random words may be unable to offer key cues
for discourse parsing. As shown in Table 7, the
masking discourse markers strategy leads to per-
formance improvement, and the masking random
words strategy leads to slight performance degra-
dation. Therefore, it is thus clear that discourse
markers are useful for RST parsing.
Result on GUM Corpus Finally, we test our pro-
posed RST parser on the GUM corpus (Zeldes,
2017) to show the domain generalization capability
of our proposed RST parser. As shown in Table 8,
the performance of our XLNet RST parser declines
signiﬁcantly for these out-of-domain articles, es-
pecially in conversation and vlog domains. By
using our proposed second-stage EDU-level pre-
training approach, the performance of the XLNet
RST parser can be improved in academic, con-
versation, textbook, and whow domains signiﬁ-
cantly, and the performances declines slightly in
bio, speech, and vlog domains. Therefore, there is
still a lot of room for improvement in the general-
ization ability of our proposed RST parser.
5 Related Work
RST discourse parsing is an important task in the
NLP community, which has been studied since
early (Soricut and Marcu, 2003). Early studies
adopt statistical models for this task, using human-
designed discrete features (Hernault et al., 2010;
Feng and Hirst, 2012; Joty et al., 2013; Feng and
Hirst, 2014; Heilman and Sagae, 2015; Wang et al.,
2017). Recently, several neural network models
show great promising for this task (Braud et al.,
2016, 2017; Liu and Lapata, 2017; Yu et al., 2018;
Mabona et al., 2019; Zhang et al., 2020; Guz and
Carenini, 2020). With PLMs such EMLo (Pe-
ters et al., 2018), BERT (Devlin et al., 2019),
XLM-RoBERTa (CONNEAU and Lample, 2019),
and XLNet (Yang et al., 2020), these neural RST
parsers report high competitive performances (Liu
et al., 2019a; Lin et al., 2019; Liu et al., 2020;
Kobayashi et al., 2020; Zhang et al., 2021; Nguyen
et al., 2021). We follow the line of these studies,
using neural networks to perform RST parsing.
Recently, several studies aim to alleviate the mis-
match between pre-trained language models and
target tasks. Joshi et al. (2020) use a span masked
language modeling to pre-train a language model
for extraction question answering. Lewis et al.
(2020) propose a pre-training approach for text
generation tasks, which maps corrupt documents4276
to the original. Raffel et al. (2020) propose an uni-
ﬁed text-to-text pre-training framework for several
NLP tasks. Our work mainly inspired by above
studies. In this paper, we propose a second-stage
EDU-level pre-training approach to alleviate the
mismatching between EDU-level RST parsing and
sentence-level language modeling.
There are several studies have shown that pe-
sudo data is useful for RST parsing. Huber and
Carenini (2019) use pesudo RST discourse trees to
train a RST parser, which generated by distant su-
pervision on a sentiment classiﬁcation. Kobayashi
et al. (2021) improve RST parsing with large-scale
sliver agreement subtrees, which is produced by a
well trained RST parser. Zhang et al. (2021) train a
top-down RST paser with predicted RST discourse
trees. Above approaches requires a well trained
RST parser to generate pesudo RST discourse trees.
In this work, the generation of our pesudo data
merely requires an EDU segmenter and discourse
markers, without using a well trained RST parser
to further generate pesudo RST discourse trees.
6 Conclusion and Future Work
We proposed a second-stage EDU-level pre-
training approach for PLM-based RST discourse
parser, reducing the mismatch between the EDU-
level RST discourse parsing and the pre-training of
sentence-level contextualized language modeling.
In addition, we extended our RST discourse parser
with a light bi-gram EDU modiﬁcation, ﬁnding that
it is able to exploit PLMs more effectively. Exper-
iments on RST-DT (Carlson et al., 2001) showed
that the proposed approach can bring signiﬁcantly
better performance for RST discourse parsing. We
further conducted several experimental analysis to
better understand the proposed approach.
The results on the RST-DT (Carlson et al., 2001)and the GUM (Zeldes, 2017) corpora suggest two
possibilities for future research. First, although
the XLNet RST parser obtains signiﬁcantly im-
provements when the second-stage EDU-level pre-
training approach is adopted, the Relation and the
Full metrics of our RST parser are still lower than
the best system. Future research might extend the
second-stage EDU-level pre-training task, using
pesudo RST discourse trees. Second, the general-
ization ability of our proposed RST parser needs
to be improved in multi-domain scenarios. So in
future we may continue to explore the issue of do-
main adapation in RST parsing on the basis of the
second-stage EDU-level pre-training framework.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their constructive comments, which
help to improve the paper. This work was supported
by National Natural Science Foundation of China
under grants 62076173, U1836222, and 62176180.
References4277427842794280