
Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, Caiming Xiong
Language Technologies Institute, Carnegie Mellon University
Salesforce AI Research
prakharg@cmu.edu, {wu.jason,wenhao.liu,cxiong}@salesforce.com
Abstract
Fact-checking is an essential tool to mitigate
the spread of misinformation and disinforma-
tion. We introduce the task of fact-checking
in dialogue, which is a relatively unexplored
area. We construct DF, a testing bench-
mark dataset of 22,245 annotated conversa-
tional claims, paired with pieces of evidence
from Wikipedia. There are three sub-tasks in
DF: 1) Verifiable claim detection task
distinguishes whether a response carries verifi-
able factual information; 2) Evidence retrieval
task retrieves the most relevant Wikipedia snip-
pets as evidence; 3) Claim verification task pre-
dicts a dialogue response to be supported, re-
futed, or not enough information. We found
that existing fact-checking models trained on
non-dialogue data like FEVER (Thorne et al.,
2018) fail to perform well on our task, and
thus, we propose a simple yet data-efficient
solution to effectively improve fact-checking
performance in dialogue. We point out unique
challenges in DF such as handling the
colloquialisms, coreferences and retrieval am-
biguities in the error analysis to shed light on
future research in this direction.
1 Introduction
Misinformation online can have deleterious con-
sequences to our society, especially during public
health crises like the COVID-19 pandemic. False
and outdated information can be spread not only
by humans but also by automatic agents as gen-
erative models have shown remarkable progress
recently (Adiwardana et al., 2020; Xu et al., 2021).
These systems are not perfect, as they can either
generate hallucinated and imperfect information,
or they can be abused to automatically generate
false claims and spread misinformation at a mas-
sive scale. Fact verification tools are thus necessary
in the current information age to tackle the spread
of misinformation propagated.
Figure 1: Dialogue fact-checking involves predicting
if a response should be considered a Verifiable claim,
followed by finding relevant evidence, and finally pre-
dicting if the it is S , R or NEI.
Fact-checking was introduced in Wang (2017);
Thorne et al. (2018) and since then a growing body
of research has explored and suggested various
tasks and resources to address the challenges in this
area. Fact-checking has been explored in medium
such as Wikipedia passages, tables, social media
and news articles (Guo et al., 2021; Bekoulis et al.,
2021). In dialogue domain, related work either
focus on evaluating factual consistency (Honovich
et al., 2021; Qin et al., 2021) or consistent response
generation (Rashkin et al., 2021; Shuster et al.,
2021). However, due to lack of publicly available
benchmarks, fact checking is still underexplored in
the dialogue domain.
Verifying factual correctness of claims in dia-
logue poses new challenges to both dataset con-
struction and modeling. Claims in existing datasets
are from formal sources such as news articles and
they are generally succinct and formal. In contrast,
claims in dialogue are often informal and sparse in
factual content. Furthermore, dialogue utterances
often include personal opinions, slang, and col-
loquialisms which need to be distinguished from
factual information. Another challenge in dialogue
fact-checking is that ellipsis and coreference occur
frequently which make utterances incomplete and
ambiguous (DeVault and Stone, 2007). Although
humans can easily understand utterances with refer-3785ences or absent information based on the dialogue
context and their reasoning skills, a fact-checking
system may need to model this behavior explicitly.
We introduce the task of fact-checking in dia-
logue and propose an evaluation dataset, D-
F. An example is shown in Figure 1. D-F has three sub-tasks: 1) Verifiable claim
detection aims to distinguish responses that do not
contain verifiable factual information, such as “I
haven’t been but want to!” in Figure 1. 2) Evidence
retrieval involves selecting the most relevant knowl-
edge snippets from Wikipedia which can verify the
response. 3) Claim verification aims to classify if
a response is supported, refuted, or does not have
enough information to verify the response given
the dialogue history and the retrieved evidence.
DF consists of both human-written and
machine-generated claims based on the Wizard of
Wikipedia (Dinan et al., 2019) dialogue dataset.
Each response claim and its evidence sentences
from Wikipedia are annotated by crowd workers
and we perform rigorous quality checks on the
annotations. For fact verification, we propose cre-
ation of weakly-supervised training data by lever-
aging techniques such as negation, entity swapping,
language model mask-and-fill, and knowledge-
grounded generation. We establish baseline model
performance on this task, and point out the weak-
nesses of fact-checking models. Our analysis show
that this is a non-trivial task with challenges remain-
ing for future work. We hope that future work can
leverage this dataset as a fact-checking benchmark
or for development of automatic consistency met-
rics, and advance the state-of-the art in knowledge-
grounded dialogue generation and evaluation.
2 Related Work
Fact Verification The spread of false informa-
tion online has led to a growing body of research
exploring automatic fact-checking. Thorne et al.
(2018) and subsequent works (Wenhu Chen et al.,
2020; Jiang et al., 2020; Nørregaard and Derczyn-
ski, 2021; Aly et al., 2021) introduced fact ex-
traction and verification datasets verifiable against
pieces of evidence from Wikipedia articles. Fact-
checking has been explored in a variety of medi-
ums such as Wikipedia based claims (Schuster
et al., 2021), claims over tables (Aly et al., 2021),
scientific claims (Wadden et al., 2020), and so-
cial media claims (Nakov et al., 2021). However,
fact-checking in dialogue is still an underexplored
area. Kim et al. (2021) explored fact-checking forcolloquial claims, curated by converting FEVER
claims into colloquial style. Although closely re-
lated to our work, colloquial claims is not a dia-
logue dataset, only contains verifiable claims, and
does not have dialogue contexts for claims. In
DF, on the other hand, both evidence re-
trieval and claim verification are more challenging
as they require resolving ambiguities and corefer-
ences from the dialogue context.
Consistency in Dialogue Neural dialogue sys-
tems grounded on knowledge sources such as
Wikipedia (Dinan et al., 2019), knowledge
graphs (Wu et al., 2019) or snippets from the inter-
net (Komeili et al., 2021) have garnered interest in
recent years. Despite generating plausible and en-
gaging responses, existing models still hallucinate
invalid information (Roller et al., 2021). Ensuring
safety and consistency in dialogue response genera-
tion is thus an actively explored area (Rashkin et al.,
2021; Shuster et al., 2021). Some recent works
have proposed evaluation metrics and benchmarks
for factual consistency in knowledge grounded re-
sponse generation (Honovich et al., 2021; Dziri
et al., 2021). Our work instead focuses on fact-
checking in dialogue for both human and machine-
generated responses, and involves additional tasks
of verifiable claim detection and evidence retrieval.
Synthetic datasets Synthetic dataset construction
has been shown to improve robustness of evaluation
models (Gupta et al., 2021; Ghazarian et al., 2021)
and improve the complexity of test sets (Sakaguchi
et al., 2021; Feng et al., 2021). Synthetic claims
have been explored in fact-checking to create hard
test sets. Several participants in the FEVER 2.0
breakers phase (Niewinski et al., 2019; Hidey et al.,
2020; Atanasova et al., 2020) proposed approaches
for automatically generated adversarial claims. Re-
cently, Jiang et al. (2020) created complex multi-
hop claims using word substitutions, Saakyan et al.
(2021) used Bert based token-infilling to created
refuted claims, and Schuster et al. (2021) created
synthetic revisions to Wikipedia sentences to im-
prove fact-checking robustness. Our work also
introduces techniques to create synthetic claims in
the context of dialogue fact-checking.
3 Task Background
Let a conversation context consist of a list of utter-
ances C={u, u, ..., u}. The task is to per-
form fact-checking on the last utterance of the3786conversation u, henceforth called claim c. Fact-
checking claims in conversations is a pipeline that
consists of several steps. First, the system needs
to decide whether a response is V or
N-V . We define them as follows:
N-V :The claim contains no veri-
fiable factual information. It includes claims with
personal opinions or personal information. V- :The claim contains at least one factual
information verifiable against a background corpus
(Wikipedia in this task).
Next, the system should retrieve documents from
the background corpus and select relevant evidence
sentences from the documents. Finally, the system
should predict whether the claim belongs to one
of the following three categories: S :
The response contains factual information which
is valid in light of the evidence. R :The
response contains factual information which is in-
valid in light of the evidence. NE I- (NEI): The response contains fac-
tual information which can not be validated (sup-
ported or refuted) with the evidence.
V claims can be S ,R- , orNEI, and N-V claims are
always NEI. We leverage the Wizard of Wikipedia
(WoW) dataset (Dinan et al., 2019) as the base to
build this task. WoW is a knowledge-grounded
open-domain dialogue dataset with conversations
between two speakers - a wizard who has access to
background Wikipedia documents to deliver knowl-
edge carrying responses, and an apprentice who
plays the role of a curious learner. For each turn u,
the wizard is shown a set of articles Kretrieved
from Wikipedia. The wizard either chooses a rel-
evant knowledge sentence kfrom the set K, or
chooses a no sentence used option to construct a re-
sponse. For our fact-checking task, we additionally
need claims which belong to R andNEI
categories. We next describe the methodologies
used to create claims from the valid and test splits
of the WoW dataset.
4 Dataset Construction and Annotation
We use two approaches to create claim responses
forDF: 1) Automatically generated claims,
and 2) Human written claims to emulates claims
created by dialogue systems and humans respec-
tively. All claims are further annotated by crowd
workers on Amazon Mechanical Turk (Mturk).4.1 Automatically Generated Claims
In this approach, we use automatic methods to cre-
ate claims for all categories either from scratch or
by mutating the responses in WoW dataset.
4.1.1 Methods for claim generation
Negation We use the 42 rule-based transforma-
tions from Thorne et al. (2019) which apply to
verb phrases of the claims to convert them to their
negated versions by adding words like “not” or
“no”. It typically creates R claims.
Substitution We perform three types of substitu-
tions: For 1) Context and knowledge-based entity
substitution, we first run SpaCy NER tagging (Hon-
nibal and Montani, 2017) on a response ufrom
WoW. We then swap an entity in the response u
with an entity from either its conversation context
Cor its background knowledge articles set K. An
entity is only swapped if it is present in k, the orig-
inal knowledge sentence to avoid swaps which do
not change the facts. Entities are swapped within
their types. For 2) Sense-based substitution, we
swap an entity in uwith an entity with a similar
“sense” returned from the sense2vec (Trask et al.,
2015) library. For 3) Adjective substitution, we
substitute adjectives in a claim (ignoring adjectives
related to emotions, such as “happy”) with their
WordNet (Miller, 1998) antonyms (for example
bestis replaced with worst ). These operations typi-
cally create R claims.
Mask-and-Fill This method generates claims in
two stages: 1) Mask salient words from the origi-
nal claims, and 2) Substitute those words with their
alternates using a language model. For masking
salient words in the original response claims, we
follow the procedure from Thorne and Vlachos
(2021) and use the Neutrality Masker model from
Shah et al. (2020). It predicts the tokens which
upon masking are likely to cause a label flip from
S toNEI. For step 2) we first train a
T5-base model (Raffel et al., 2020) on the WoW
dataset on the task of infilling masked tokens con-
ditioned on evidence sentences. For training, the
input sequence consists of concatenated evidence
sentence k, dialogue context C, and the gold re-
sponse with masked spans at random positions, and
the output is the gold response. The model is thus
trained to infill a masked response based on the
provided evidence and the dialogue context. For
generating response claims which belong to R- orNEI categories, we use the following3787types of evidence sentences to condition the in-
filling: a) empty evidence, b) evidence sentences
selected randomly from the knowledge article set
Kbelonging to the original response, and c) ev-
idence sentences from a Wikipedia article of an
entity retrieved using sense2vec based on its sim-
ilarity with the entities in the original response.
Conditioning on such evidence lead to generation
of claims which have factual details inconsistent
with the original evidence.
Generation We fine-tune one of the best chit-chat
dialogue systems, Blenderbot model (Roller et al.,
2021), on the WoW dataset. The model takes the
concatenation of the knowledge sentence kand
the dialogue context Cas input and it is trained to
predict the tokens of the gold response. To generate
new response claims, we condition the model on
the three types of evidence described in the Mask-
and-Fill approach. We use a high temperature (1.5)
and nucleus sampling (Holtzman et al., 2020) with
p= 0.9during decoding to encourage the model
to generate unexpected and non-contextual entities
in the responses.
Final claim set creation Our target is to create a
challenging and diverse test set for dialogue fact-
checking. Using the aforementioned methods of
claim generation, we get a set R={r, r, ..., r}
of response claims for a dialogue context C. To
select a final set of claims, we first remove any re-
sponses which do not have at least 3 words different
from other responses in R, then filter out less flu-
ent claims whose GPT-2 (Radford et al., 2019) per-
plexity scores are higher than 1.1 times the average
perplexity scores of the responses in R. We then
score the response claims using existing state-of-
the-art models related to our task: namely Dialogue
NLI (Welleck et al., 2019), Dialogue contradiction
detection (Nie et al., 2021), FEVER based fact ver-
ification (Schuster et al., 2021) and fact-checking
on colloquial claims (Kim et al., 2021). For each
model, we calculate the entropy of the scores pre-
dicted for each label and rank the claims in R
based on the sum of the entropy of the scores of all
the models, which gives an estimate of the confu-
sion or difficulty in classifying the claims. The top
4 responses from the ranked list are chosen as the
final set of response claims for that context.
4.1.2 Evidence set creation
For each claim, a set of evidence sentences is first
automatically created and then labelled by crowd
workers. We first extract a set of named entitiesand noun phrases nfrom the following sources:
the claim c, the dialogue context C, the original
response ufor the dialogue context in WoW, and
the title of the knowledge articles Kshown to the
wizard for u. We use the MediaWiki APIto find
a set of relevant Wikipedia pages Pforn. We
then create a set of candidate sentences with the
first 10 sentences of each page in P. Finally, we
use two methods - SpaCy’s word2vec similarity
and BM25 similarityto rank the top 10 evidence
sentences using each method. We then combine
the non-overlapping evidence from both methods
to create the final evidence set efor each claim c.
We add the knowledge sentence kassociated with
the original response in the WoW dataset if it is not
already present in e.
4.1.3 Claim and Evidence Annotation
We carry out the annotations of the claims and ev-
idence on the Mturk platform in 3 rounds. The
screenshot of the annotation UI is shown in Fig-
ure 3 of the Appendix. In each round a worker
sees the claim c, its dialogue context C, and its
associated evidence sentences e. Workers have to
perform 3 tasks: First, they select if the claim is
V orN-V . Second, they
select one or more evidence sentences related to the
response claim. In case the set of evidence shown
is not enough to decide the label of the response,
or if they choose NEI, they are instructed to search
Wikipedia and add relevant additional evidence sen-
tences in the interface. For NEI claims they are in-
structed to add evidence sentences which are most
related to the claim. Third, they choose the cate-
gory of the response - S , R , or
NEI. For N-V claims, NEI is auto-
selected. Since automatically created responses
can have grammatical or coherence related issues,
in the first round of labeling, annotators are asked
to edit a response to make it appropriate to the con-
text if needed, or mark a response as incoherent, in
which case it is removed from further rounds (We
dropped 5% of incoherent claims). In the second
and third rounds we gather 2 additional annotations
for each claim. We select the label which has the
majority vote among the set of 3 annotations across
all rounds. The evidence set for each claim is the
union of evidence annotated in any of the rounds.
Note that this mechanism can miss relevant evi-3788
dence sometimes due to either retrieval errors in
evidence set creation, or insufficient search of evi-
dence or incorrect evidence annotation by workers.
4.2 Human Written Claims
Our dataset also consists of human written claims
to cover lexical and stylistic patterns present in
human-human conversations. The annotation is
carried out in 3 rounds. In the first round , we in-
struct crowd workers to write V factual
responses conditioned on dialogue context and a
set of evidence sentences for a pre-specified la-
bell- one of S , R , orNEI.
Workers were provided detailed examples and in-
structions for the task such as “Avoid using nega-
tion words such as do not, no for Refuted claims”
(Appendix C). The evidence set for each claim
is constructed using the method described in sec-
tion 4.1.2. In the second round , we use the claim
labeling interface from section 4.1.3 to gather la-
bels for the claims collected in the first round. For
any claim which is not labeled in the second round
with the original label l, we gather a third round
of annotations. If the label in the third round does
not match l, we drop that claim from the dataset.
We drop about 7% of the human written claims.
4.3 Dataset Statistics
We present the dataset statistics in Table 1. The
dataset consists of balanced S andR- claims. Test set contains claims for 3,760
dialogue contexts with an average of 3.1 claims per
context, and validation contains claims for 3,738
contexts with an average of 2.8 claims per context.
The average number of tokens per claim is 22.0 in
test set and 20.0 in validation set. Average number
of evidence per claim is 1.3 in the test set and 1.1 in
the validation set. We show some sample instancesin Table 13 in the Appendix.
4.4 Quality Control
Annotators : We hire workers on Mturk with with
at least 5000 HITS done and an acceptance rate
of 95% or above. Workers have to first pass a
qualification test where they are shown the task
instructions, label definitions, and multiple exam-
ples and the explanations for each label. Then they
are asked to label or write 12 claims. Using these
qualification tests, we get a final set of 87 workers
for the main data collection stage (Appendix C).
Quality checks Annotations were carried out in
batches over multiple weeks. We examined random
samples to provide feedback to workers. Workers
with poor annotations were either asked to retake
a new qualification test or removed from further
batches. We recollected annotations for data an-
notated by removed workers. We provide tooltips
and examples during annotation, and we also added
automatic checks to alert workers about issues such
as too short responses, no evidence selected, and
copy-pasting evidence sentences as claims.
Data validation To evaluate inter-annotator agree-
ment, we collected 2 extra rounds of annotations
for 1200 claims for both automatically generated
and human written claims, which is 10% of the
data. Krippendorff’s alpha value for category la-
bels was 0.68 for human written claims and 0.58 for
automatically generated claims, denoting moderate
agreement. Krippendorff’s alpha for V
versus N-V was 0.49, with a low-
to-moderate agreement. The lower agreement is
due to some claims like “Guns N’ Roses was the
greatest rock band of all time.”, where it is difficult
to judge if this is a personal opinion or a verifiable
fact. In such conflicts, workers would still typically
correctly label such ambiguous claims as NEI.
Lexical Biases Following Schuster et al. (2019),
we measure the Local Mutual Information (LMI)
to measure the correlation between bigrams in the
claims ( w) and the categories l, defined as follows:
LMI (w, l) =p(w, l)log
. We present
the top bigrams in R claims and their LMI
value in Table 2. The top bigrams in DF do
not include obvious negations such as “do not”, “is
not”, are mostly topical in nature, and the p(l/w)
value is low with the Refute label. Investigating
generated and written claims separately, we found
that bigrams such as “does not, only one, did not,
are not” had higher p(l/w)in written claims com-3789
pared to generated claims for R category,
although their LMI values were not high. Finally,
there is significant overlap between the top bigrams
for different categories, suggesting an absence of
obvious lexical biases in the dataset.
5 Experiments
We propose new baselines and compare with ex-
isting models for three sub-tasks in dialogue fact-
checking - 1) Verifiable claim detection, 2) Evi-
dence retrieval, and 3) Claim verification.
5.1 Verifiable Claim Detection
We propose three simple baselines for verifiable
claim detection. 1) Lexical overlap calculates the
maximum word overlap between a claim and all
evidence sentences after removing punctuation and
stopwords using SpaCy. 2) DNLI uses the probabil-
ity of the neutral class from the Dialogue Natural
Language Inference model (Welleck et al., 2019).
3)Lexical+DNLI uses the sum of scores of both
baselines and Random predicts each class with 50%
probability. For all baselines, we mark a response
asV orN-V based on a
threshold value selected using validation data. We
present the accuracy and individual F1 scores for
both classes in Table 3. Lexical+DNLI performs
the best and all baselines have low F1 scores for
N-V claims.
5.2 Evidence Retrieval
Evidence retrieval consists of two steps: 1) Docu-
ment Retrieval, 2) Evidence Sentence selection.5.2.1 Document Retrieval
We test two methods for document retrieval:
The first one is WikiAPI, which retrieves
Wikipedia pages and is used in past fact-checking
work (Hanselowski et al., 2018; Stammbach and
Neumann, 2019; Liu et al., 2020). It uses the Al-
lenNLP constituency parser (Gardner et al., 2018)
to extract potential entities from the claims. Then
it feeds the entities as queries through the Me-
diaWiki APIand returns up to three Wikipedia
pages per query. For each Wikipedia page, we
query the KILT (Petroni et al., 2021) knowledge
source to get the first 5 paragraphs of the page.
We create two versions of this method: a) Wiki-
ctxwhich concatenates the last two turns of the
dialogue context with the response claim before
document retrieval and b) Wiki-claimonly - which
uses just the claim. The second method is Dense
Passage Retrieval (DPR) (Karpukhin et al., 2020),
a dual encoder based model which retrieves doc-
uments using BERT (Devlin et al., 2019) trained
by metric learning. We create three versions of
this method: a) DPR-original , which uses the orig-
inal DPR trained on question-answering tasks, b)
DPR-WoWft-claimonly , which is fine-tuned on the
WoW dataset to retrieve documents relevant to a
query composed only of a response claim, and c)
DPR-WoWft-ctx , which is also fine-tuned on WoW
dataset but uses both the context as well as the re-
sponse as a query (training details are provided in
Appendix B). For DPR-based methods we retrieve
the top 100 documents. A document is relevant if
it contains a gold evidence sentence.
We present the document recall results in Table 4.
WikiAPI methods outperform DPR-based methods.
Both methods show better performance when dia-
logue context is used in retrieval. DPR is typically
able to retrieve documents with the correct topic
but often fails to retrieve a relevant evidence sen-
tence. Entity linking is crucial for fact-checking3790
in dialogue and WikiAPI is able to leverage that
capability for better performance.
5.2.2 Evidence Sentence Selection
In evidence sentence selection, a final set of top
kevidence sentences are chosen from the set of
documents Dretrieved in the previous step for
claim c. First, we create a candidate evidence sen-
tence set Sby taking the union of all sentences
inD. We fine-tune a Bert-base model for rank-
ing the candidate sentences in S. The model is
trained to predict -1 for irrelevant evidence and 1
for relevant evidence for a given claim. We use
the context-response pairs from the WoW dataset
for training the model. Besides using randomly
selected evidence sentences, to create hard nega-
tive examples for training, we also chose sentences
from the set of articles Kshown to the wizard
during WoW data collection. These sentences are
close in content and topic to the gold evidence sen-
tence and form hard negative candidates for the
model. At test time, we use the evidence sentences
in the top krank with a score of more than 0. Simi-
lar to document retrieval, we created two versions
of the model: 1) Ret-with-context, and 2) Ret-only-
claim, based on whether the last two utterances
of the dialogue context were included in the input
to the BERT model. We present the performance
of the models in Table 5 for two of the best per-
forming document retrieval models Wiki-ctx and
DPR-WoWft-ctx. We find that recall@5 values for
both models are higher when dialogue context is
added as an input with the claim.
5.3 Claim Verification
In claim verification, a claim cis classified as S- ,R , orNEI given a context Candevidence sentences set S.
5.3.1 Baselines
DNLI (Welleck et al., 2019) Dialogue NLI dataset
contains sentence pairs labeled as entailment, neu-
tral, or contradiction derived from dialogues. En-
tailment maps to S , neutral maps to
NEI, and contradiction maps to R in our
task. We train a Bert-base model on their training
set of 310,110 data points.
DECODE (Nie et al., 2021) Dialogue Contradic-
tion Detection dataset contains both human-human
and human-bot contradictory dialogues. The train
set contains 27,948 data points with two labels
contradiction and non-contradiction. We train a
Bert-base model with the last two utterances of the
context and the response as input to the model.
VitaminC (Schuster et al., 2021) VitaminC is a
large-scale fact verification dataset which is based
on contrastive claim-evidence pairs created from
Wikipedia edits. They train models that avoid
claim-only biases and are more sensitive to changes
in the evidence. We use their ALBERT-base model
finetuned on FEVER (Thorne et al., 2018) and their
VitaminC dataset.
Colloquial (Kim et al., 2021) It contains collo-
quial claims converted from FEVER dataset claims
into colloquial style. It has 410k colloquial claim-
evidence pairs in the training set and is well aligned
to our task because of its colloquial nature. We fine-
tune a Bert-base model on this dataset.
CorefBert-Colloquial (Ye et al., 2020) is one of
the best performing models on FEVER and is de-
signed to better capture and represent the corefer-
ence information. We use their model which uses
kernel graph attention network (KGAT) (Liu et al.,
2020) and fine-tune it on Colloquial claims.
Aug-WoW We propose a novel model which is
trained on weakly supervised training data. D-
F is meant to be used only for validation and
test, and we do not train a model on DF to
avoid creating a model which can simply learn to
solve the dataset instead of the task. Instead, we
leverage the techniques described in section 4.1.1
to create synthetic training data for each category of
claims. For S claims, we use the claim-
evidence pair from the original WoW dataset. We
use the Lexical baseline from section 5.1 to filter
out Non-Verifiable claims, which leads to 46,934
S claims. We follow the methods Nega-
tionandSubstitution from section 4.1.1 to create
38,895 R claims. We create NEI claims3791
using two methods: 1) For every context-claim-
evidence triplet, we substitute the evidence with
random unrelated evidence. 2) We use the Genera-
tionapproach from section 4.1.1 to condition the
generation on random evidence. We select a sub-
set of 40,000 NEI claims from the two approaches.
We fine-tune the Colloquial baseline model on this
synthetic dataset. The input to the model is the
sequence of the last 2 context utterances separated
by [EOT] token, followed by the claim.
For all Bert-based models, all evidence sentences
are concatenated together. More details about train-
ing the baselines are provided in Appendix B.
5.3.2 Results
Table 6 summarizes the results for claim verifi-
cation on the test set. N-V claims
are included in the NEI category. We experiment
with three evidence retrieval settings - 1) Oracle
Evidence, where we use gold evidence, 2) Wiki-
Evidence, where we use Wiki-ctx for document
retrieval and Ret-with-context for evidence selec-
tion, and 3) DPR-Evidence, where we use DPR-
WoWft-ctx for document retrieval and Ret-with-
context for evidence selection. We set the max-imum evidence to 5. In all three settings, Aug-
WoW outperforms baselines and the performance
of all baselines drops when retrieved evidence is
used compared to when oracle evidence is used.
This indicates that evidence retrieval is an impor-
tant step for this task. Even with oracle evidence,
none of the models achieve an accuracy higher
than 70%, which leaves abundant opportunity for
future improvements. Colloquial baseline is the
closest to Aug-WoW since it has been trained on
conversation-like colloquial claims. Although Col-
loquial and CorefBert-Colloquial perform better
than VitaminC with oracle evidence, the contrastive
nature of VitaminC helps it perform better with re-
trieved evidences.
In Table 8, we present the claim verification re-
sults on the Test set using oracle evidence on Gen-
erated and Written claims separately. The perfor-
mance of all models is lower on Generated claims
compared to Written claims. This is expected
since as we mentioned in “Final claim set creation”
in section 4.1.1, the Generated claims were cho-
sen from a larger candidate claims set based on
the difficulty of existing models to classify those
claims. Thus Generated claims in DF are
more challenging. Furthermore, Aug-WoW’s per-
formance is high on both types of claims, however,
the gain in its performance on Written claims is
higher on Written claims compared to Generated
claims.
In Table 7, we present the claim verification re-
sults on the test set with Aug-WoW model abla-
tions. In Aug-WoW-noctx we do not concatenate
the dialogue context, and in Aug-WoW-BertLarge
we use the Bert-Large model as base architecture.3792
Aug-WoW-noctx is comparable to Aug-WoW, and
has slightly lower performance with Oracle evi-
dence. Although Aug-WoW-BertLarge performs
better with oracle evidence, it is more sensitive
to the evidence quality and performs poorly with
retrieved evidence.
To test if a model that relies solely on claims
and no evidence can leverage lexical biases in
the claims to obtain good performance on D-
F, we train a model Aug-WoW-claimonly with
no evidence included during training and testing.
Aug-WoW-claimonly achieves 33.2% accuracy and
28.9% macro F1 score on the DF test set.
Thus, a model can not exploit lexical cues in the
claims of DF to obtain good performance.
We report performance on a two-way classifica-
tion experiment in Appendix A (Table 12) where
we combine R andNEI into a single class
named N-S .
5.3.3 Discussion
We present sample dialogue contexts, claims, ora-
cle evidence for the claims along with model pre-
dictions in Table 9. We found that models tend to
incorrectly predict a R orNEI response
asS when there is significant overlap
between the evidence and the claim while ignoring
the semantics. The first example illustrates this
point where the presence of terms “biathlon” and
“cross country skiing” misleads some models to
predict S incorrectly. Similarly, models
predict S orR for a NEI claimdue to word overlap between claim and evidence,
as shown in the second example. Models also often
fail to perform complex and commonsense-based
reasoning during verification. In the third example,
although humans can reason that the claim is R- by the evidence, all models fail to correctly
classify the claim. Finally, models struggle with
lexical biases and separating the colloquial part of a
claim from its factual parts. In the fourth example,
although there is significant overlap between the
claim and the evidence, models are fooled by the
presence of the word “not one of”, and predict a
S claim as R .
6 Conclusion
We propose a new benchmark, DF, for fact-
checking in dialogue created based on grounded
dialogues from the Wizard-of-Wikipedia dataset.
Besides human-written response claims, we also
create synthetic claims with operations such as con-
tradiction, infilling and substitutions. We hire quali-
fied crowd workers to annotate responses into N-
V , , , orNE- I categories along with cor-
responding evidence. We point out empirically
that existing fact-checking models trained on non-
dialogue data fail to perform well on our task. We
demonstrate how to leverage automatically gener-
ated responses as weak supervised signals to im-
prove performance. We hope that DF can
facilitate fact-checking, and consistency modeling
and evaluation research in the dialogue community.3793Ethical Considerations & Broader Impact
In this paper, we study the problem of fact-
checking in dialogue. The DF benchmark
dataset proposed in this work could be helpful in
creation of more accurate automatic fact checking
systems and metrics, and ultimately creation of di-
alogue systems which are more faithful to factual
knowledge and are thus more trustworthy. Auto-
matic fact-checking of dialogue could be useful
in many real-life scenarios where conversations
need to be properly monitored to avoid spread of
misinformation and disinformation, and where the
conversation participants are needed to be given
accurate information. However, DF bench-
mark only covers a specific domain with Wikipedia
as background knowledge. Furthermore, even with
our best efforts to ensure high quality and accu-
racy, the dataset might still contain incorrect labels
and biases in some instances. This could pose a
risk if models that are evaluated or built using this
benchmark are used in domains not covered by
the dataset or if they leverage evidence from un-
reliable or biased resources. Thus the proposed
benchmark should not be treated as a universal tool
for all domains and scenarios. In our work, we
mitigate this risk by using the trusted source of
Wikipedia for evidence and by curating hard train-
ing and testing instances using automated genera-
tion approaches. Considerable additional work is
needed to improve the scope, coverage and validity
of fact-checking systems and metrics, but our work
provides a cautious yet concrete step towards devel-
oping fact checking systems for dialogue. training
and testing instances using automated generation
approaches.
References379437953796
A Supplementary Results
We present the claim verification results on the val-
idation set in Table 10. The trend in performance
is similar to the trend observed in the test set re-
ported in 6. In our human studies discussed in
subsection Data validation of section 4.4, we ob-
serve that workers confuse between R and
NEI labels. Furthermore, there are cases where the
workers can miss finding an evidence which refutes
a claim on Wikipedia and label the claim as NEI
even though they are instructed to find and verify
a claim by visiting Wikipedia. Similar findings
were reported in other fact-checking tasks (Jiang
et al., 2020). Hence we perform another experi-
ment where we combine R andNEI into
a single class, and name it N-S . We
present the claim verification results on test set for
this setting in Table 12. The performance of all
baselines is higher since the task is transformed to
a 2-way classification task from a 3-way classifi-
cation task. Aug-WoW performs the best in this
setting.
In Section5.3.2, we discuss results where N-
V claims are included in the NEI cate-
gory. In Table 11, we present the results for 3-way
classification on test set where N-V
claims with NEI-P labels are removed,
that is, only Verifiable claims are kept for NEI la-
belled claims. The trends in results are similar to
the ones observed in Table 6.
We show the confusion matrix of our Aug-WoW
model in Figure 2. Aug-WoW has the lowest per-
formance on NEI claims and highest confusion
between NEI and Refuted classes.
B Implementation Details
First we discuss the implementation details for
claim generation techniques in section 4.1.1. For
Negation we use the implementation from fever-2
baseline(Thorne et al., 2019). For the T5 model
inMask-and-Fill and Blenderbot model in Gen-
eration approach, we use the models and training
scripts available in the Hugging Face’s Transform-
ers repository. Blenderbot was finetuned on full
WoW training dataset with batch size of 40.
We next discuss the implementation details for
the document retrieval methods. For WikiAPI
method, Kim et al. (2021) pointed out that Wiki-
API method naively retrieves documents related
to filler words such as “I”, “Yes”, “They” etc. fre-
quently. In our implementation of WikiAPI we
mitigate this issue by filtering out such colloquial
phrases by using a manually created stopwords list.
We remove the stopwords from the candidate set
of entities on which MediaWiki API is called. Our
experiments showed significant improvement in
the quality of the returned documents. For DPR,
we use the wiki_dpr dataset available in the Hug-3797
ging Face Datasets libraryfor document retrieval.
It contains 21M passages from wikipedia along
with their DPR embeddings. The wikipedia arti-
cles are split into multiple, disjoint text blocks of
100 words as passages. We retrieve top 100 docu-
ments per claim. We finetune the claim encoders
forDPR-WoWft-claimonly andDPR-WoWft-ctx us-
ing the original DPR implementation. The orig-
inal biencoder was trained on natural questions
dataset. We only fine-tune the question encoder
of the DPR model. DPR training data consists of
positive, random negatives and hard negative pairs.
For positive claim-evidence document pairs, we
use the response-knowledge sentence pairs in the
original WoW dataset, where we filter out N-
V claims using the Lexical baseline
from section 5.1. For hard negatives, we follow the
instructions in the DPR repository and mine hard
negatives using the original DPR index and encoder
(facebook/dpr-question_encoder-single-nq-base) it-
self. Specifically, we use DPR to retrieve top 2
evidences per claim and use them as a hard nega-
tive if they are not the same as the original knowl-
edge sentence for the claim in the WoW dataset.
We finetune the base DPR encoder on the afore-
mentioned constructed data and convert only the
question encoder checkpoints into Hugging Face
model format. Since the Wikipedia version used
for evidence in WoW dataset (and hence in Dial-Fact evidence), and Hugging Face’s wiki_dpr (used
for document retrieval in our experiments) are dif-
ferent, even if WikiAPI and DPR methods retrieve
a correct document, it might not exactly match the
evidence we picked up from WoW dataset due to
wording changes and edits between the two ver-
sions of Wikipedia pages. Therefore we relax the
requirements from exact document matching to
partial matching. That is, we assume a retrieved
document matches a gold document if either the
initial half or final half of the retrieved document
matches the gold evidence document’s half.
We next discuss the implementation details for
the models for claim verification 5.3. For VitaminC,
we use the tals/albert-base-vitaminc-fever model
available in their repo. We finetune CorefBERT-
base for CorefBERT and use the official code from
the authors. We train AugWoW and Colloquial
models using the code from the VitaminC repo
on a machine with 4 NVIDIA A100 GPUs and
train batch size of 100. We use the validation set
performance for model selection.
C AMT Instructions
We present the screenshot of the annotation inter-
face is shown in Figure 3. Workers were paid an
avergae of $8-10 per hour across all tasks. For3798
the claim labelling task, workers were told that
they will be shown a conversation between two
speakers, some previously created responses to the
conversation, and some Wikipedia knowledge snip-
pets related to the response (which we will call
evidence henceforth). They will label some dia-
logue responses which could belong to one of the
3 categories mentioned below.
Supported : The response should exclusively use
factual information which can be verified by the
given evidence sentences and is correct or true in
light of the evidence. A response is verifiable if
evidence could be retrieved from Wikipedia, which
decreases the uncertainty about the truthfulness (or
falsehood) of the statement.Example 1:
• Context: I think Jazz is an American creation!
•Evidence: Jazz has roots in West African cul-
tural and musical expression, and in African-
American music traditions including blues and
ragtime, as well as European military band mu-
sic.
•Response: Its roots include African-American
music traditions including blues and ragtime
•Explanation: Response is natural and can be ver-
ified from the evidence.
Example 2:3799
•Context: What are the three different waterfalls
Niagra is made from? Can you please share with
me?
•Evidence: From largest to smallest, the three
waterfalls are the Horseshoe Falls, the American
Falls, and the Bridal Veil Falls.
•Response: The three waterfalls are the Horseshoe
Falls, the American Falls and the Bridal Veil
Falls.
•Explanation: Response is natural and can be ver-
ified from the evidence as all facts mentioned are
correct.
Refuted : The response contains factual informa-
tion which is “incorrect” or “false” in light of the
evidence, that is it contradicts the evidence. The
response should be marked refuted if even a small
part of the response is incorrect.
Example 1:
• Context: I think Jazz is an American creation!
•Evidence: Jazz has roots in West African cul-
tural and musical expression, and in African-
American music traditions including blues and
ragtime, as well as European military band mu-
sic.
•Response: Its roots include American music tra-
ditions including blues and ragtime•Explanation: Roots are African-American, not
American.
Example 2:
•Context: What are the three different waterfalls
Niagra is made from? Can you please share with
me?
•Evidence: From largest to smallest, the three
waterfalls are the Horseshoe Falls, the American
Falls and the Bridal Veil Falls.
•Response: The three waterfalls are the Horseshoe
Falls, the American Falls and the Sommer Falls.
•Explanation: One of the falls is incorrect based
on the evidence.
Not Enough Information : The response can not
be verified (supported or refuted) with Wikipedia
evidence. Moreover, for this response, it is allowed
to use information/knowledge that might not be
available in Wikipedia but you assume to be general
knowledge, e.g. that 90s refers to the time span
from 1990 to 1999.
Example 1:
• Context: I think Jazz is an American creation!
•Evidence: Jazz has roots in West African cul-
tural and musical expression, and in African-
American music traditions including blues and3800ragtime, as well as European military band mu-
sic.
•Response: Jazz is now played in all parts of the
world except Russia.
•Explanation: The response is not a personal opin-
ion and the provided evidence can’t be used to
verify the stated fact.
Example 2:
•Context: What are the three different waterfalls
Niagra is made from? Can you please share with
me?
•Evidence: From largest to smallest, the three
waterfalls are the Horseshoe Falls, the American
Falls and the Bridal Veil Falls.
•Response: I think three waterfalls all intersect
multiple times. I am trying to remember the
names.
•Explanation: The stated fact can not be verified
from the evidence.
We ask workers to do the following:
• Read the context carefully and if writing or edit-
ing a response, write minimum of 9 words.
•The label should be exclusively based on the
response and the selected evidence sentences.
We ask workers to NOT do the following:
•While writing or editing a response please avoid
typos and mis-spelling as much as possible.
•While writing or editing a response, do not use
“know-it-all” phrases such as "did you know" in
your responses - e.g., the response "did you know
that the Berlin Wall was demolished in 1989"
will not be accepted.
Personal/generic response : We give workers
some examples of personal response. The response
should not make any factual claim that could be
verified using Wikipedia or any knowledge source.
It can contain facts that are personal opinions or
background of the speaker, but no fact pertinent to
encyclopedic knowledge. The response should be
a good follow-up to the conversation.
Example 1:
•Context: I do not understand why some people
enjoy hunting.•Evidence: Hunting is the practice of killing or
trapping animals.
•Response 1: I enjoy going out in the woods to
hunt animals.
•Response 2: Wow interesting. I have mostly used
hunting as a means of pest control.
•Explanation: Even if hunting can be used as pest
control, it is a personal detail or opinion here.
Example 2:
•Context: It would be perfect to have a family
member involved in choosing foster care.
•Evidence: Usually children are taken care of by
their parents, legal guardians or siblings.
•Response: Very true, that is why I think it is best
when parents or or legal guardians take care of
their children, because they are they only ones
that love the children.
•Explanation: Although part of the response is
present in the evidence, this is a subjective opin-
ion of the speaker.
To start the final task, we ask workers to read
the dialogue, the corresponding responses, and the
Wikipedia knowledge provided (links and pieces
of evidence).
•For each provided response, mark them as SUP-
PORTED, REFUTED, or NOT ENOUGH IN-
FORMATION.
•if the response consists of only personal opinions
or personal information with no verifiable fac-
tual information, please mark the corresponding
checkbox.
•Please read the instructions and examples in the
link above carefully.
•If you select the SUPPORTED or REFUTED
option, you must click at least one checkbox
as evidence or copy-and-paste sentences from
Wikipedia links.
•For NEI, you would generally need to verify
the facts in the responses by visiting and search-
ing Wikipedia pages and pasting any related evi-
dence.
•Please edit and correct the responses if they con-
tain any grammatical or spelling mistakes.3801