
Mengting HuYike WuHang GaoYinhao BaiShiwan ZhaoCollege of Software, Nankai UniversitySchool of Journalism and Communication, Nankai UniversityInstitute for Public Safety Research, Tsinghua University
{mthu, wuyike}@nankai.edu.cn, gaohang@mail.tsinghua.edu.cn
yinhao@mail.nankai.edu.cn, zhaosw@gmail.com
Abstract
Recently, aspect sentiment quad prediction
(ASQP) has become a popular task in the field
of aspect-level sentiment analysis. Previous
work utilizes a predefined template to para-
phrase the original sentence into a structure
target sequence, which can be easily decoded
as quadruplets of the form ( aspect category ,
aspect term ,opinion term ,sentiment polarity ).
The template involves the four elements in a
fixed order. However, we observe that this so-
lution contradicts with the order-free property
of the ASQP task, since there is no need to fix
the template order as long as the quadruplet
is extracted correctly. Inspired by the observa-
tion, we study the effects of template orders
and find that some orders help the generative
model achieve better performance. It is hy-
pothesized that different orders provide various
views of the quadruplet. Therefore, we propose
a simple but effective method to identify the
most proper orders, and further combine mul-
tiple proper templates as data augmentation to
improve the ASQP task. Specifically, we use
the pre-trained language model to select the or-
ders with minimal entropy. By fine-tuning the
pre-trained language model with these template
orders, our approach improves the performance
of quad prediction, and outperforms state-of-
the-art methods significantly in low-resource
settings.
1 Introduction
The aspect sentiment quad prediction (ASQP) task,
aiming to extract aspect quadruplets from a review
sentence, becomes popular recently (Zhang et al.,
2021a; Cai et al., 2021). The quadruplet consists of
four sentiment elements: 1) aspect category (ac) in-
dicating the aspect class; 2) aspect term (at) which
is the specific aspect description; 3) opinion termFigure 1: An example sentence is paraphrased into a tar-
get sequence with a fixed-order template (Zhang et al.,
2021a). Our approach employs special markers to form
free-order templates and generates multiple target se-
quences. Ois the i-th order permutation of the four
elements.
(ot)which is the opinion expression towards the as-
pect; 4) sentiment polarity (sp) denoting the senti-
ment class of the aspect. For example, the sentence
“The service is good and the restaurant is clean. ”
contains two quadruplets ( service general ,service ,
good ,positive ) and ( ambience general ,restaurant ,
clean ,positive ).
To extract aspect sentiment quadruplets, Zhang
et al. (2021a) propose a new paradigm which trans-
forms the quadruplet extraction into paraphrase
generation problem. With pre-defined rules, they
first map the four elements of ( ac,at,ot,sp) into
semantic values ( x,x,x,x), which are then
fed into a template to obtain a nature language
target sequence. As shown in Figure 1, the origi-
nal sentence is “re-writen” into a target sequence
by paraphrasing. After fine-tuning the pre-trained
language model (Raffel et al., 2020) in such a
sequence-to-sequence learning manner, the quadru-
plets can be disentangled from the target sequence.
Though promising is this paradigm, one issue is
that the decoder of the generative pre-trained lan-
guage model (Raffel et al., 2020) is unidirectional
(Vinyals et al., 2015), which outputs the target se-7889quence from the beginning of the sequence to its
end. Thus four elements of a quadruplet are mod-
eled in a fixed order {x→x→x→x}.
Yet ASQP is not a typical generation task. There is
no need to fix the element order of the quadruplet as
long as it can be extracted accurately. Aspect sen-
timent quadruplet has the order-free property, sug-
gesting that various orders, such as {x→x→
x→x}and{x→x→x→x}, are
all correct.
In light of this observation, our curiosity is trig-
gered: Does the order of the four elements impact
the generative pre-trained language models’ perfor-
mances? Thus we conduct a pilot experiment. The
four elements are concatenated with commas, thus
we could switch their orders in a flexible manner
and obtain order permutations. It is found that some
template orders can help the generative model per-
form better. Even only concatenating with commas,
some orders outperform the state-of-the-art.
It is hypothesized that different orders provide
various views of the quadruplet. Therefore, we pro-
pose a simple but effective method to identify the
most proper orders, and further combine multiple
proper templates as data augmentation to improve
the ASQP task. Concretely, we use the pre-trained
language model (Raffel et al., 2020) to select the
orders with minimal entropy. Such template orders
can better promote the potential of the pre-trained
language model. To jointly fine-tune these template
orders together, inspired by Paolini et al. (2021),
we design special markers for the four elements, re-
spectively. The markers help to disentangle quadru-
plets by recognizing both the types and their values
of the four elements from the target sequence. In
this way, the template orders do not need to be fixed
in advance.
In summary, the contributions of this work are
three-fold:
•We study the effects of template orders in the
ASQP task, showing that some orders perform
better. To the best of our knowledge, this work
is the first attempt to investigate ASQP from
the template order perspective.
•We propose to select proper template orders by
minimal entropy computed with pre-trained
language models. The selected orders are
roughly consistent with their ground-truth per-
formances.
•Based on the order-free property of thequadruplet, we further combine multiple
proper templates as data augmentation to im-
prove the ASQP task. Experimental results
demonstrate that our approach outperforms
state-of-the-art methods and has significant
gains in low-resource settings.
2 Preliminaries on Generative ASQP
2.1 Paraphrase Generation
Given a sentence x, aspect sentiment quad predic-
tion (ASQP) aims to extract all aspect-level quadru-
plets{(ac, at, ot, sp )}. Recent paradigm for ASQP
(Zhang et al., 2021a) formulates this task as a para-
phrase generation problem. They first define pro-
jection functions to map quadruplet (ac, sp, at, ot )
into semantic values (x, x, x, x). Con-
cretely, 1) aspect category acis transformed
into words, such as x=“service general” for
ac=“service#general” ; 2) if aspect term atis ex-
plicit, x=at, otherwise x=“it”; 3) if opinion
termotare explicitly mentioned, x=ot, other-
wise it is mapped as “NULL” if being implicitly
expressed; 4) the sentiment polarity sp∈{posi-
tive,neutral ,negative }, is mapped into words with
sentiment semantics { great ,ok,bad}, respectively.
With the above rules, the values can better ex-
ploit the semantic knowledge from pre-trained lan-
guage model. Then the values of quadruplet are
fed into the template, which follows the cause and
effect semantic relationship.
xisxbecause xisx. (1)
It is worth noting that if a sentence describes
multiple quadruplets, the paraphrases are concate-
nated with a special marker [SSEP ]to obtain the
final target sequence y.
2.2 Sequence-to-Sequence Learning
The purpose of paraphrasing is consistent with
the typical sequence-to-sequence problem. The
encoder-decoder model is leveraged to “re-write”
the original sentence xinto the target sequence
y. Assume the parameter is θ, the overall objec-
tive is to model the conditional probability p(y|x).
Specifically, at the t-th time step, the decoder out-
putyis calculated with the input xand the previ-
ous outputs y, formulating as below.
p(y|x,y) = softmax( Wy)(2)
where Wmaps yinto a vector, which can rep-
resent the probability distribution over the whole
vocabulary set.7890
During training, a pre-trained encoder-decoder
model, i.e. T5 (Raffel et al., 2020), is chosen to
initialize the parameter θand fine-tuned with mini-
mizing the cross-entropy loss.
L(x,y) =−/summationdisplaylogp(y|x,y) (3)
where nis the length of the target sequence y.
3 A Pilot Experiment
As Eq. (1) displayed, this template forms a fixed
order of four elements. Our curiosity is whether
the quadruplet’s order affects the performance of
sequence-to-sequence learning. Therefore, we con-
duct a pre-experiment. By only concatenating with
commas , four elements can also be transformed
into a target sequence. The orders can be switched
in a more flexible way, compared with Eq. (1).
There will be 4! = 24 permutations. During in-
ference, quadruplets can be recovered by splitting
them with commas. Based on the pre-experimental
results, we have the following observations.
Template order affects the performances of
sequence-to-sequence learning. Part of the ex-
perimental results on two datasets, i.e. Rest15 and
Rest16 , are shown in Table 1. It is observed that
onRest15 , the F1score ranges from 45.94% to
48.17%. Similarly, on Rest16 , theF1score ranges
from 57.09% to 59.20%. We draw an empirical
conclusion that the template order also matters for
the ASQP task. Moreover, a template has various
performances on different datasets, which is hard
to say some order is absolutely good.
The performances of each element are con-
nected to its position. We further investigate the
F1scores on each of the four elements. Given the
24permutations, there are 6templates for each
element at each position. For example, there are
6templates {(., ., x, .)}ofxat position 3. In
Figure 2, we show the average F1scores of the six
templates of each element at each position. We
can see that the performances of the four elements
have different trends with the positions. The F1
scores of xandxboth degrade when they are
gradually placed backwards. Compared with the
other three elements, xis more stable on different
positions while xhas the worst performance in
the first position. In addition to positions, it can
also be observed that xandxachieve higher F1
scores compared with xandx, showing various
difficult extents of four elements.
4 Methodology
As analyzed in the previous section, the template or-
der influences the performances of both the quadru-
plet and its four elements. It is hypothesized
that different orders provide various views of the
quadruplet. We argue that combining multiple tem-
plate orders may improve the ASQP task via data
augmentation. However, using all 24permutations
significantly increases the training time, which is
inefficient. Therefore, we propose a simple method
to select proper template orders by leveraging the
nature of the pre-trained language model (i.e. T5).
Then for the ASQP task, these selected orders are
utilized to construct the target sequence yto fine-
tune the T5 model.
Specifically, given an input sentence xand
its quadruplets {(ac, at, ot, sp )}, following Zhang7891
et al. (2021a), we map them into semantic values
{(x, x, x, x)}. As shown in Figure 3, our
approach is composed of two stages, next which
will be introduced in detail.
4.1 Selecting Template Orders
Inspired by (Yuan et al., 2021; Lu et al., 2022), we
choose template orders by evaluating them with the
pre-trained T5. As shown in Figure 3, given an in-
putxand it quadruplets, we construct all 24 target
sequences with multiple order mapping functions
O, where i∈[1,24]. An example Ois shown
below.
O(x, x, x, x) =xxxx (4)
where the four values are concatenated with a sim-
ple space, without any other tokens such as com-
mas, in a specific order O. In this way we can
reduce the impact of noisy tokens, but focus more
on the order. We also introduce the special sym-
bol[SSEP ]if there are multiple quadruplets in a
sentence. Given multiple template orders, multiple
target sequences yare constructed for an input x.
Then we evaluate these target sequences with
the entropy computed by the pre-trained T5. Here
yis also fed into the decoder as teacher forcing
(Williams and Zipser, 1989). The output logits p
of the decoder are utilized to compute the entropy.
E(y|x) =−1
n/summationdisplay/summationdisplayplogp (5)
where nis length of target sequence and and |V|is
the size of the vocabulary set.
Given the whole training set T, we have
(x,{y})for each instance by constructingtemplate orders. Specifically, we design the fol-
lowing two template selection strategies.
Dataset-Level Order (DLO) To choose the
dataset-level orders, we compute a score for each
order on the whole training set.
S=1
|T |/summationdisplayE(y|x) (6)
where Sdenotes the average entropy of all in-
stances for the template order O. Then by ranking
these scores, template orders with smaller values
are chosen.
Instance-Level Order (ILO) Different instances
have various contexts and semantics, and tend to
have their own proper template orders. Therefore,
we also design to choose orders at the instance level.
Similarly, the template orders of each instance with
small values are chosen based on Eq. (5).
4.2 Fine-tuning with Selected Orders
Multiple template orders provide various views of
a quadruplet. However, to train them jointly, an
issue arises. If the four values are concatenated
with a comma or only a blank space, the value type
could not be identified during the inference. For
example, when the output sequence “food quality,
pasta, delicious, great” is recovered to a quadru-
plet, the machine does not know the element types.
Therefore, to deal with this issue, we design special
markers to represent the structure of the informa-
tion (Paolini et al., 2021). The markers for x,
x,x,xare[AC],[AT],[OT],[SP], respectively.
Given an order, the target sequence is constructed:
y=O([AC]x,[AT]x,[OT]x,[SP]x)
= [AT]x[AC]x[SP]x[OT]x
Now we can train multiple orders together, mean-
while the quadruplet can be recovered by these
special markers during the inferences. Note that
previous data augmentation methods usually de-
sign multiple inputs for one label, such as word
deletion and replacement (Gao et al., 2021), obtain-
ing multiple input sentences. While our method
constructs multiple labels for one input sequence.
This is beneficial from the ASQP task’s property
when using generation-based models.
5 Experiments
5.1 Datasets
We conduct experiments on two public datasets,
i.e. Rest15 andRest16 (Zhang et al., 2021a).7892
These two datasets originate from SemEval tasks
(Pontiki et al., 2015, 2016), which are gradually an-
notated by previous researchers (Peng et al., 2020;
Wan et al., 2020). After alignment and completion
by Zhang et al. (2021a), each instance in the two
datasets contains a review sentence, with one or
multiple sentiment quadruplets. The statistics are
presented in Table 3.
5.2 Implementation Details
We adopt T5-base (Raffel et al., 2020) as the pre-
trained generative model. The pre-trained parame-
ters are utilized to initialize the model, which is ex-
ploited to calculate template orders’ entropy with-
out updating any parameters. After selecting order
with minimal entropy, we fine-tune T5 with the
constructed training samples. The batch size is set
to 16. In the pilot experiment, the hyper-parameters
are set following Zhang et al. (2021a). The learn-
ing rate is set to 3e-4. During the inference, greedy
decoding is chosen to generate the output sequence.
The number of training epochs is 20 for all experi-
ments. For the proposed approaches, since multiple
template orders are combined, we set the learning
rate as 1e-4 to prevent overfitting. During the infer-
ence, we utilize the beam search decoding, with the
number of beam being 5, for generating the output
sequence. All reported results are the average of 5fixed seeds.
5.3 Compared Methods
To make an extensive evaluation, we choose the
following strong baseline methods.
•HGCN-BERT+BERT-Linear HGCN (Cai
et al., 2020) aims to jointly extract acandsp.
Following it, BERT extracts atandot(Li et al.,
2019). Finally, stacking a linear layer (BERT-
Linear) forms the full model.
•HGCN-BERT+BERT-TFM The final stacked
layer in the above model is changed to a trans-
former block (BERT-TFM).
•TASO-BERT-Linear TAS (Wan et al., 2020)
is proposed to extract ( ac,at,sp) triplets. By
changing the tagging schema, it is expanded into
TASO (TAS with Opinion). Followed by a linear
classification layer, the model is named as TASO-
BERT-Linear.
•TASO-BERT-CRF TASO is followed with a
CRF layer, named as TASO-BERT-CRF.
•Extract-Classify-ACOS (Cai et al., 2021) It is
a two-stage method, which first extracts atand
otfrom the original sentence. Based on it, acand
spare obtained through classification.
•GAS (Zhang et al., 2021b) It is the first work to
deal aspect level sentiment analysis with genera-
tive method, which is modified to directly treat
the sentiment quads sequence as the target se-
quence.
•Paraphrase (Zhang et al., 2021a) It is also a
generation-based method. By paraphrasing the
original sentence, the semantic knowledge from7893
the pre-trained language model can be better ex-
ploited.
5.4 Experimental Results
5.4.1 Overall Results
Experimental results of various approaches are re-
ported in Table 2. The best scores on each metric
are marked in bold. It is worth noting that for our
two approaches, i.e. ILO and DLO, the default
template orders are selected with the top-3 minimal
entropy from all permutations.
We observe that ILO and DLO achieve the
best performances compared with strong baselines.
Specifically, comparing with Paraphrase, the ab-
solute improvement of ILO is +2.12% (+4.51%
relatively) F1score on Rest15 dataset. ILO outper-
forms Paraphrase by +1.86% (+3.21% relatively)
F1score on Rest16 dataset. This validates the
effectiveness of our template-order data augmen-
tation, which provides more informative views for
pre-trained models. Our method exploits the order-
free property of quadruplet to augment the “output”
of a model, which is different from the previous
data augmentation approaches.
5.4.2 Ablation Study
To further investigate the strategies for selecting
template orders, an ablation study is conducted.
The results are shown in Table 4. As aforemen-
tioned, the default setting of ILO and DLO is to
select top-3 template orders with minimal entropy.
The model variants also select top-3 template or-
ders, but with maximal entropy and random sam-
pling. We observe that using minimal entropy con-
sistently outperforms the other two strategies. This
verifies that our strategy is effective, and the se-
lected template orders can better promote the po-
tential of T5 on solving the ASQP task.
Moreover, we investigate the distribution of the
chosen template orders. Firstly, we sort all the 24
template orders by their F1scores in ascending or-
der based on the results of the pilot experiment (see
Appendix). As depicted in Figure 4, the horizon-
tal axis represents the template index i∈[1,24].
The template order at index 1 has the worst per-
formance while index 24 the best. We then count
the number of each template index which is se-
lected by ILO. We observe that by minimal en-
tropy, more performant template orders (e.g., index
i∈[17,24]) are selected compared with using
maximal entropy. On the contrary, we also see
that ILO chooses less poorly-performed template
orders (e.g., index i∈[1,5]) than ILO(Entropy
Max). This verifies that using minimal entropy, we
can select performant template orders. The obser-
vations are similar in DLO, which are presented in
the appendix.
5.4.3 Low-Resource Scenario
To further explore the performances of the pro-
posed method in low-resource settings, we design
an experiment which uses only 25% of the original
training data to train the model. The experimen-
tal results are shown in Table 5. Our approach
achieves significant improvements compared with
the state-of-the-art. Specifically, ILO(top-10) out-7894
performs Paraphrase by +4.48% (+12.24% rela-
tively) and +4.36% (+9.01% relatively) F1scores
onRest15 andRest16 , respectively. This further
verifies our hypothesis that different orders provide
various informative views of the quadruplet while
combining multiple orders as data augmentation
can improve the model training, especially in low-
resource settings.
We also plot the F1score curves by setting differ-
ent top- kvalues (see Figure 5). It can be seen that
under the two settings, i.e. full and 25% training
data, ILO both outperforms Paraphrase. Compar-
ing the two settings, ILO achieves more signif-
icant improvements under the low-resource sce-
nario. This observation is in line of expectation.
When the training data is adequate, selecting tem-
plate orders with top-3 is enough. When the train-
ing data is limited, model can obtain more gains by
setting large k. It also shows that our data augmen-
tation is friendly for real applications which have
limited labeled data.
5.4.4 Effects of Special Marker
Since we design four special markers for the four
elements to jointly train models with multiple tem-
plates, we investigate the differences in using other
symbols. The templates below are chosen for com-
parison. T2andT3are inspired by Chia et al.
(2022), which annotate the type of information by
specific words.
•T1:[AT]x[OT]x[AC]x[SP]x
•T2: aspect term: xopinion term: xaspect
category: xsentiment polarity: x
•T3: Aspect Term: xOpinion Term: x
Aspect Category: xSentiment Polarity: x
•T4:x,x,x,x
The evaluation results of the above four tem-
plates are reported in Table 6. Firstly, by compar-
ingT4with others, it can be seen that marking
the types of four elements are effective for gen-
erative ASQP. A possible reason is that marking
with either special symbols or specific words helps
to demonstrate the structured information (Paolini
et al., 2021). Secondly, T1achieves the best per-
formances on almost all evaluation metrics. Such
special markers can avoid overlapping words with
sentences. For example, the sentence “Service is
not what one would expect from a joint in this price
category. ” contains the word “category” , which
is overlapped with the type indicator aspect cate-
gory inT2. It is shared between sentence and type
markers through word embeddings, and might lead
to negative effects.
5.4.5 Error Analysis
We further investigate some error cases. Two exam-
ple cases are presented in Figure 6. We observe that
ILO can generate quadruplets in different orders
with the help of special markers. By recognizing
the special markers, the quadruplets can be disen-
tangled from the target sequence.
The two examples demonstrate that some cases
are still difficult for our approach. The first ex-
ample contains an implicit aspect term, which is7895
mapped into “it”. Its opinion term, i.e. “go some-
where else” , also expresses negative sentiment po-
larity implicitly. This case is wrongly predicted.
As for the second one, its gold label consists of two
quadruplets. Our method only predicts one quadru-
plet, which does not match either quadruplet. This
example also describes aspect terms implicitly for
different aspect categories, i.e. “restaurant general”
and“restaurant miscellaneous” . In summary, sen-
tences with implicit expressions and multiple as-
pects are usually tough cases. This observation is
also consistent with the results from the pilot ex-
periment. As shown in Figure 2, the F1scores of
aspect term and opinion term are much worse than
other two elements.
6 Related Work
6.1 Aspect-Level Sentiment Analysis
Aspect-level sentiment analysis presents a research
trend that deals with four elements gradually in a
finer-grained manner (Zhang et al., 2022). Analyz-
ing sentiment at the aspect level begins from learn-
ing the elements separately (Pontiki et al., 2014).
To name a few, some works have been proposed
to classify sentiment polarity given the mentioned
aspect, either aspect category (Hu et al., 2019) or
aspect term (Zhang and Qian, 2020). Other works
extract aspect term (Ma et al., 2019), classify aspect
category (Bu et al., 2021). The four elements are
not solely existing, which actually have strong con-
nections with each other. Therefore, researchers
focus on learning them jointly, such as aspect sen-
timent pair (Zhao et al., 2020; Cai et al., 2020) or
triplet (Chen and Qian, 2020; Mao et al., 2021).
Recently, learning four elements simultaneously
sparks new research interests. Two promising di-
rections have been pointed out by researchers. Cai
et al. (2021) propose a two-stage method by extract-ing the aspect term and opinion term first. Then
these items are utilized to classify aspect category
and sentiment polarity. Another method is based
on generation model (Zhang et al., 2021a). By para-
phrasing the input sentence, the quadruplet can be
extracted in an end-to-end manner. In this work,
we follow the generative direction and consider the
order-free property of the quadruplet. To the best
of our knowledge, this work is the first to study
ASQP from the order perspective.
6.2 Data Augmentation
Data augmentation has been widely adopted in both
the language and vision fields. We formulate the in-
put and output of a model as XandY, respectively.
Previous data augmentation can be divided into
three types. The first type is augmenting the input
X. For example, image flipping, rotation and scal-
ing all change Xto seek improvements (Shorten
and Khoshgoftaar, 2019). In the text tasks, back
translation (Sugiyama and Yoshinaga, 2019) can
also generate pseudo pairs through augmenting X.
The main idea is that changing Xdoes not affects
its ground-truth label Y. Secondly, both XandY
are augmented. A promising work is mixup (Zhang
et al., 2018), which constructs virtual training ex-
amples base on the prior knowledge that linear
interpolations of feature vectors should lead to lin-
ear interpolations of the associated targets. Despite
it is intuitive, it has shown effectiveness in many
tasks (Sun et al., 2020).
The third one is augmenting Y. One recent work
proposes virtual sequence as the target-side data
augmentation (Xie et al., 2022) for sequence-to-
sequence learning. It deals with typical generation
tasks, which are closely connected with the order
of words. Different from it, we exploit the charac-
teristic of the generative ASQP task. Order permu-
tations still provide ground-truth labels. Then we
think that different orders are just similar to seeing
a picture from different perspectives, i.e. different
views. Therefore, combining multiple template or-
ders can prevent the model from being biased to
superficial patterns, and help it to comprehensively
understand the essence of the task.
7 Conclusion
In this work, we study aspect sentiment quad pre-
diction (ASQP) from the template order perspec-
tive. We hypothesize that different orders provide
various views of the quadruplet. In light of this hy-7896pothesis, a simple but effective method is proposed
to identify the most proper orders, and further com-
bine multiple proper templates as data augmenta-
tion to improve the ASQP task. Specifically, we
use the pre-trained language model to select the
orders with minimal entropy. By fine-tuning the
pre-trained model with these template orders, our
model achieves state-of-the-art performances.
Limitations
Our work is the first attempt to improve the ASQP
task by combining multiple template orders as
data augmentation. Despite state-of-the-art per-
formance, our work still have limitations which
may guide the direction of future work.
Firstly, we use the entropy to select the proper
template orders. The smaller entropy value indi-
cates that the target sequence is better fitting with
the pre-trained language model. However, there
may be other criteria for template order selection
which can better fine-tune the pre-trained language
model to support the ASQP task.
Secondly, in the experiment, we simply select
the top- ktemplate orders for data augmentation.
This can be treated as a greedy strategy for the com-
bination. However, each of the top- korders may
not supplement well to each other. More advanced
strategies may be designed to select template orders
for data augmentation.
Thirdly, we only consider augmenting the target
sequences in the model training, while augmenting
both the input and out sequences may bring more
performance improvement.
Acknowledgements
We sincerely thank all the anonymous review-
ers for providing valuable feedback. This work
is supported by the key program of the Na-
tional Science Fund of Tianjin, China (Grant No.
21JCZDJC00130), the Basic Scientific Research
Fund, China (Grant No. 63221028), the National
Science and Technology Key Project, China (Grant
No. 2021YFB0300104).
References78977898
A Appendix
A.1 Software and Hardware
We use Pytorch to implement all the models
(Python 3.7). The operating system is Ubuntu
18.04.6. We use a single NVIDIA A6000 GPU
with 48GB of RAM.
A.2 Full Pilot Experimental Results
The full experimental results of template permuta-
tions are presented in Table 7, Table 8, Table 9 and
Table 10. The results of each table are sorted by F1
scores in an ascending order.
A.3 Results of DLO
Since the proposed DLO choose templates for the
whole training set, we plot the ranking position of
each template order in Figure 7. Here the horizon
axis indicates that the template indexes which are
ordered by F1score from Table 9 and Table 10 in
an ascending order. Then we can see that DLO
can choose better-performed templates, where the
ranking positions of O, i∈[17,24]are small. In
contrary, DLO(Entropy Max) selects template or-
ders that performed worse.78997900