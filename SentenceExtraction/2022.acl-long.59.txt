
Shuai Zhang, Yongliang Shen, Zeqi Tan, Yiquan Wu, Weiming Lu
College of Computer Science and Technology, Zhejiang University
{zsss, luwm}@zju.edu.cn
Abstract
Named entity recognition (NER) is a funda-
mental task to recognize specific types of en-
tities from a given sentence. Depending on
how the entities appear in the sentence, it can
be divided into three subtasks, namely, Flat
NER, Nested NER, and Discontinuous NER.
Among the existing approaches, only the gener-
ative model can be uniformly adapted to these
three subtasks. However, when the generative
model is applied to NER, its optimization ob-
jective is not consistent with the task, which
makes the model vulnerable to the incorrect
biases. In this paper, we analyze the incorrect
biases in the generation process from a causal-
ity perspective and attribute them to two con-
founders: pre-context confounder and entity-
order confounder. Furthermore, we design
Intra- and Inter-entity Deconfounding Data
Augmentation methods to eliminate the above
confounders according to the theory of back-
door adjustment. Experiments show that our
method can improve the performance of the
generative NER model in various datasets.
1 Introduction
Named entity recognition (NER) is a task aimed at
identifying distinct and independent entities from a
given text while classifying them into predefined
types. As a fundamental work in Natural Language
Processing (NLP), its research facilitates the ap-
plication of many downstream tasks (Ganea and
Hofmann, 2017; Miwa and Bansal, 2016; Shen
et al., 2021b). In previous work (Sang and Meul-
der, 2003; Pradhan et al., 2013a; Doddington et al.,
2004; Kim et al., 2003; Karimi et al., 2015), three
kinds of different NER subtasks were raised (as
shown in Figure 1), which are Flat NER, Nested
NER and Discontinuos NER.
The existing NER methods can be divided into
three main categories, including labeling-basedFigure 1: Examples involving flat, nested and discontin-
uous NER types, Entities are highlighted with colored
markers
.
(Ju et al., 2018; Straková et al., 2019), span-
based (Luan et al., 2019a; Shen et al., 2021a) and
generative-based (Straková et al., 2019; Paolini
et al., 2021; Yan et al., 2021a) methods. Non-
generative methods have different problems when
applied to all three different subtasks: the labeling-
based methods need to design different tagging
schema for various types (Ratinov and Roth, 2009;
Metke-Jimenez and Karimi, 2016; Straková et al.,
2019; Dai et al., 2020) while the span-based meth-
ods suffers from ambiguity of boundary when ap-
plied to discontinuous task. Although generative-
based methods are able to model all NER subtasks
uniformly (Yan et al., 2021a), the training objective
differ significantly from NER task due to the au-
toregressive generation mannner, resulting in some
incorrect biases learned by the model during the
training process.
From a causal perspective, the incorrect bi-
ases stem from two confounders: pre-context con-
founder and entity-order confounder. Pre-context
confounder means that the model is affected by
pre-context words that may be extra-entity words
when generating a particular entity word. For exam-
ple, in S3 of Figure 1, the autoregressive generation
mannner causes the model to generate the word "fa-
tigue" of the entity "muscle fatigue" conditioned on
the extra-entity words "muscle" and"pain" . This
causes the model to mistakenly establish dependen-808
cies between the intra-entity word "fatigue" and
the extra-entity words "muscle" and"pain" , while
ignoring the dependency between the intra-entity
words "muscle" and"fatigue" . Therefore, when
only the entity "muscle fatigue" is in the input sen-
tence, the model cannot predict the entity accu-
rately and completely due to the learned incorrect
dependency bias. Entity-order confounder refers
to the fact that the model is affected by a predeter-
mined order of entities when generating an entity
sequence. The entities in a sentence are essentially
a set structure without decoding order among them.
In contrast, the generative NER model pre-specifies
the decoding order of entities, which introduces
incorrect bias and ignores the bidirectional depen-
dency between entities. As in S1 of Figure 1, after
fixing the set of entities as "Stallone" →"Rocky"
→"Rambo" , the model only models the unidirec-
tional dependency of "Rambo" on"Stallone" and
"Rocky" , without considering the reverse depen-
dency of "Stallone" on"Rocky" and"Rambo" . In
this case, if "Rambo" is decoded first, it is difficult
for the model to decode the other two entities "Stal-
lone" and"Rocky" due to the lack of the reverse
dependency.
We can formulate the causalities in the process of
entity sequence generation with a Structural Causal
Model (SCM). As illustrated in Figure 2, the direct
links denote the causality between the two nodes:
cause→effect. X→Yrepresents the generation
process of the target sequence, which can be di-
vided into two cases according to the location of
the generated words: intra-entity generation and
inter-entity generation. In the former case, Nde-
notes the pre-context words, which can affect the
generation of the next word ( N→Y). While
in the latter case, Ndenotes the entity decoding
order, and can affect the generation of the next en-
tity (N→Y). In both cases, the representationof input Xis contaminated by the backdoor path
X←N→Y. Therefore, Nis a confounder for
theX→Yprocess which introduces a incorrect
bias to the model.
In order to eliminate the bias caused by con-
founders Nin both cases, we designed the Intra-
and Inter-entity Deconfounding Data Augmenta-
tion method from the theory of backdoor adjust-
ment. Our contributions are as follows:
•We analyzed the incorrect bias of the gener-
ative model on the NER task from a causal
perspective, concluding that the pre-context
confounder and the entity-order confounder
are the main causes of the bias.
•Based on the backdoor adjustment theory,
we designed the Intra- and Inter-entity De-
confounding Data Augmentation methods to
remove the pre-context confounder and the
entity-order confounder, respectively, to elimi-
nate the incorrect bias of the generative model
on the NER task.
•Experiments on three kinds of NER tasks
show that our proposed method can de-bias
the generative NER model and thus improve
the model performance.
2 Prerequisite
For subsequent analysis, in this section we first
illustrate how the NER task is modeled as a genera-
tive task, after which we illustrate the training and
inference process of the generative model.
2.1 Problem Definition
The three kinds of NER tasks can all be for-
mulated as follows, given an input sentence of
ltokens x={x, x, ..., x}, the target se-
quence y={[ss], E,···, E,[ee]}, where E=
{[s], y, ..., y,[e]}is a word sequence of entity
e,Mdenotes the number of the entities, Edenotes
the length of the entity, [ss]and[ee]are the start
and end tags for the sequence, [s]and[e]are the
start and end tags for the entity, and yis the j-th
word of i-th target entity.
2.2 Generative Model
In general, given an input sentence x, the gen-
erative model will return a sequence consisting
of a collection of entities arranged in fixed or-
dery={[ss], E,···, E,[ee]}. To this end,809we first computes the hidden vector representation
H=h, ..., hof the input via a multi-layer trans-
former encoder:
H=Encoder (x, ..., x) (1)
where each layer of Encoder( ·) is a transformer
block with multi-head attention mechanism.
After the input sentence is encoded, the decoder
predicts the output token-by-token according to the
sequential inputs’ hidden vectors. At the step iof
generation, the self-attention decoder predicts the
i-thtoken yin the linearized form and decoder
statehas:
y, h=Decoder ([H;h, ..., h], y)(2)
where each layer of Decoder( ·) is a transformer
block that contains self-attention with decoder hid-
den state hand cross-attention with encoder state
H.
Specifically, the optimization objective of the
generated model is to maximize the conditional
probability of the entire output sequence p(y|x),
which is progressively combined by the probability
of each step p(y|y, x):
p(y|x) =Yp(y|y, x) (3)
3 The Proposed Solution
In the above, we have analyzed that the bias in
the traditional generative NER model P(Y|X)
is introduced by two kinds of confounders: the
pre-context confounder and the entity-order con-
founder. Now we need to perform the deconfound-
ing using backdoor adjustment to obtain a debi-
ased model P(Y|do(X)). Deconfounding seeks
the true causal effect of one variable on another,
and it is appealing to the objective of NER: given
a sentence X, we hope Yextracted by the model
being faithful only to the content of the input X
itself. And the backdoor adjustment promotes the
posterior probability P(Y|do(X))from passive ob-
servation to active intervention as shown below:
P(Y|do(X)) =XP(Y|X, n)P(n)(4)
where nis the stratum for the confounder N. This
encourages the model to maximize P(Y|X, n)forevery stratum n, only subject to a prior P(n)lis-
tening to no one, and hence the model is decon-
founded.
In the next sections, we apply Equation 4 to
design two data augmentation (DA) methods, Intra-
entity Deconfounding DA and Inter-entity Decon-
founding DA, for the pre-context confounder and
the entity-order confounder, respectively.
3.1 Intra-entity Deconfounding DA
We first focus on the generation of words inside
the entity. The autoregressive decoder needs to de-
code the word at the current step conditioned on
the pre-context words, i.e., the already generated
word sequence. The pre-context words may be in
other entities that are not associated with the entity
currently being generated. Thus it will learn the
wrong dependencies and bring in bias to the model.
From the SCM in Figure 2, the pre-context words
are the confounder in the generation of words in-
side the entity, causing the spurious correlation
X←N→Yto mislead the model from the true
objective X→Y.
Next we implement Intra-entity Deconfounding
by data augmentation to eliminate pre-context con-
founder. As the backdoor adjustment shown in
Equation 4, we stratify the confounder N, pre-
context words, and train the model on each stratum.
To avoid the influence of other entity words, we
split the target sequences of the samples by entity
and construct separate target sequences for each
entity. Specifically, we randomly sample a context
word [CW]of an entity efrom Xand concate-
nate it in front of the entity as a target sequence Y,
denoted as:
{[CW], y, y,···, y}
whereEdenotes the length of the entity e. If there
areMentities in a sentence X, we can construct
Maugmented samples (X, Y). It is worth noting
that, compared to the target sequence Y of the orig-
inal sample, the target sequence in the augmented
sample does not contain tags denoting the begin-
ning and end of the sequence, i.e., [ss]and[ee].
This is to tell the model to generate only a single
entity on the augmented sample instead of all the
entities, as a way to prevent the model trained by
the augmented samples from exiting early in the
practical prediction.8103.2 Inter-entity Deconfounding DA
Another generation case is that after the current en-
tity is generated, the model is expected to generate
the first word of the next entity. In traditional gen-
erative NER models (Paolini et al., 2021; Yan et al.,
2021a), the target sequence is fixed in the order
of entities, for example, Yan et al. (2021a) pre-
specified entity order according to the occurrence.
However, entities are essentially set structures and
the decoding sequence is not supposed to be fixed.
A pre-specified entity order can make the optimiza-
tion target inconsistent with the task and introduce
an incorrect bias to the model. As shown in the
SCM of Figure 2, entity order is the confounder
Nwho affects the generation X→Ythrough the
backdoor path X←N→Y.
According to Equation 4, we design an Inter-
entity Deconfounding data augmentation to elim-
inate entity-order confounder. Similar to Section
3.1, we construct augmented samples by sampling
from all possible entity orders. Specifically, for the
original sample (X,Y), we keep the last entity of
its target sequence fixed and permute the order of
the other entities. The target sequence Yof the
augmented sample can be represented as:
{[ss],Perm ( E,···, E), E,[ee]}
where Perm( ·)represents the permutation opera-
tion. During the training, we only compute the loss
for the first token of the last entity, while the other
entities are fed directly to the decoder as decoded
sequences.
3.3 Constrained Prediction
As the model uses a token-by-token approach for
prediction, in order to reduce the search space and
the impact of exposure bias, we restrict the model
to generating only tokens from the original sen-
tence at generation time, and control the entire
generation process by limiting tokens that can be
generated at each step.
Specifically, we add special start and end tokens
for the generation of each entity and the generation
of the whole sequence. At the time of prediction,
the generation of the sequence must start from the
sequence start token, and the generation of the en-
tity must start from the entity start token, and when
the end token of the entity is generated, the next
token that could be generated can only be the se-
quence end token and entity start token. Also, when
generating each entity, we restrict the category ofthe entity to be generated only after the entity is
generated, and the category can only be followed
by the [e].
4 Experiments
In this section, we first describe the dataset we
used, then we present related implementation de-
tails and experimental results, after which we make
an analysis based on the experimental results.
4.1 Datasets
As same as (Yan et al., 2021b), to show that our pro-
posed method can be used in various NER subtasks,
we conducted experiments on eight datasets.
4.1.1 Flat NER Datasets
We selected the CoNLL2003 (Sang and Meul-
der, 2003) and OntoNotes (Pradhan et al., 2013b)
datasets to do the experiments of Flat NER sub-
task. For CoNLL2003, we follow (Lample et al.,
2016; Yu et al., 2020) to train our model on the con-
catenation of the train and development sets. For
OntoNotes, we use the same train, development
and test splits as (Pradhan et al., 2012; Yu et al.,
2020).
4.1.2 Nested NER Datasets
For Nested NER subtask, we adopt ACE2004 (Dod-
dington et al., 2004), ACE2005 and Genia datasets
(Kim et al., 2003). In experiment conducted on
ACE2004 and ACE2005, we use the same data
split as (Lu and Roth, 2015; Muis and Lu, 2017; Yu
et al., 2020), the ratio between train, development
and test is 8:1:1. For Genia, we follow (Wang et al.,
2020b; Shibuya and Hovy, 2020) to use five types
of entities and split the train, development and test
as 8.1:0.9:1.0.
4.1.3 Discontinuous NER Datasets
We follow (Dai et al., 2020) to use CADEC (Karimi
et al., 2015), ShARe13 (Pradhan et al., 2013a) and
ShARe14 (Mowery et al., 2014) datasets to do our
experiment. Since only the Adverse Drug Events
(ADEs) entities include discontinuous annotation,
only this kind of entity is considered. (Karimi et al.,
2015; Metke-Jimenez and Karimi, 2016; Tang et al.,
2018).
4.2 Implementation Details
Because of the use of special tokens, we use the pre-
trained language model T5 (Raffel et al., 2020) as
our encoder-decoder generative architecture. The811ModelCoNLL2003 OntoNotes
Prec. Rec. F1 Prec. Rec. F1
(Clark et al., 2018)[GloVe300d] - - 92.6 - - -
(Peters et al., 2018)[ELMo] - - 92.22 - - -
(Akbik et al., 2019)[Flair] - - 93.18 - - -
(Straková et al., 2019)[BERT-Large] - - 93.07 - - -
(Yamada et al., 2020)[RoBERTa-Large] - - 92.40 - - -
(Li et al., 2020b)[BERT-Large] 92.47 93.27 92.87 91.34 88.39 89.84
(Yu et al., 2020)[BERT-Large] 92.85 92.15 92.5 89.92 89.74 89.83
(Yan et al., 2021b)(BPE)[BART-Large] 92.60 93.22 92.96 90.00 89.52 89.76
Ours[T5-Base](Without-De) 92.68 93.49 93.08 89.58 90.71 90.14
Ours[T5-Base](Intra-De) 92.78 93.51 93.14 89.77 91.07 90.42
Ours[T5-Base](Inter-De) 92.68 93.57 93.12 89.75 91.02 90.38
ModelACE2004 ACE2005 Genia
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
(Luan et al., 2019b)[ELMO] - - 84.7 - - 82.9 - - 76.2
(Straková et al., 2019)[BERT-Large] - - 84.33 - - 83.42 - - 76.44
(Shibuya and Hovy, 2020)[BERT-Large] 85.23 84.72 84.97 83.30 84.69 83.99 77.46 76.65 77.05
(Li et al., 2020b)[BERT-Large] 85.83 85.77 85.80 85.01 84.13 84.57 81.25 76.36 78.72
(Yu et al., 2020)[BERT-Large] 85.42 85.92 85.67 84.50 84.72 84.61 79.43 78.32 78.87
(Wang et al., 2020a)[BERT-Large] 86.08 86.48 86.28 83.95 85.39 84.66 79.45 78.94 79.19
(Yan et al., 2021b)(BPE)[BART-Large] 86.69 83.83 85.24 82.08 83.44 82.75 78.15 79.06 78.60
Ours[T5-Base](Without-De) 86.19 83.76 84.96 83.23 86.25 84.71 80.11 76.92 78.49
Ours[T5-Base](Intra-De) 86.36 84.54 85.44 83.31 86.56 84.90 81.04 77.21 79.08
Ours[T5-Base](Inter-De) 86.53 84.06 85.28 82.92 87.05 84.93 80.66 76.45 78.50
ModelCADEC ShARe13 ShARe14
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
(Metke-Jimenez and Karimi, 2016) 64.4 56.5 60.2 - - - - - -
(Tang et al., 2018) 67.8 64.9 66.3 - - - - - -
(Dai et al., 2020)[ELMo] 68.9 69.0 69.0 80.5 75.0 77.7 78.1 81.2 79.6
(Yan et al., 2021b)(BPE)[BART-Large] 69.45 70.51 69.97 82.07 76.45 79.16 75.88 84.37 79.90
Ours[T5-Base](Without-De) 71.34 70.54 70.94 79.03 78.03 78.53 77.06 83.41 80.11
Ours[T5-Base](Intra-De) 71.35 71.86 71.60 81.09 78.13 79.58 77.88 83.77 80.72
Ours[T5-Base](Inter-De) 70.44 71.65 71.04 81.31 76.75 78.96 77.51 83.27 80.29
T5 pre-trained model provides 100 default sentinel
tokens for unsupervised training, here we use these
special tokens to control the sequence generation
process for avoiding the occupation of real tokensin the word list. Specifically, we use <extra_id_2>
and<extra_id_3> to represent [s]and[e],<ex-
tra_id_0> and<extra_id_1> to represent [ss]and
[ee],<extra_id_11> to<extra_id_30> to represent812different NER categories, and <extra_id_50> to
mark the sample of inter-entity deconfounding sam-
ples. In addition, we use the AdamW (Loshchilov
and Hutter, 2019) optimizer with a linear learning
rate schedule (with peak learning rate of 1e-4). For
simplicity, we assume that entities are unique, and
for words with referential relations, such as "we" ,
which appears frequently in ACE2005, we tag each
"we" with a different label in a sentence such as
"we_1" ,"we_2" , ... to distinguish them from each
other.
4.3 Results
4.3.1 Comparision between Baselines
For simplicity of comparison, we use the results
reproduced by (Yan et al., 2021b) on the dataset
with different subtasks. Moreover, since we con-
ducted the experiments on the subtoken-level, we
only kept the experimental results of BPE in (Yan
et al., 2021b). As can be seen from Tables 1 to 3,
our model achieves similar or even better results
on all three subtasks than the model in (Yan et al.,
2021b). This may be caused by the fact that we use
a different pre-trained model and not use pointer
mechanism. Compared with other non-generative
models, same as (Yan et al., 2021b), our method
achieves comparable results with models focusing
on only one subtask of NER on most of datasets,
for exceptional cases, (Akbik et al., 2019) in Table
1 tags tokens at token-level; (Wang et al., 2020a) in
Table 2 classifies candidate span, which integrates
information of all subtokens in span, and is based
on span-level; while our model only focuses on
subtoken, which is based on subtoken-level.
4.3.2 Analysis of Intra-entity Deconfounding
In comparing the results of Without-De and Intra-
De in Table 1-3, we can see that when intra-entity
deconfounding are performed, the model has dif-
ferent degrees of improvement in all datasets. It
is worth noting that the selection method we used
to do augmentation differs slightly from dataset to
dataset. Specifically, in each dataset we select enti-
ties considering on occurrence frequency, nesting
status and character length of entity, in particular,
we kick out some special entities that have referen-
tial relationships with others.
4.3.3 Analysis of Inter-entity Deconfounding
In comparing the results of Without-De and Inter-
De in Table 1-3, we can see that when inter-entity
deconfounding are performed, the model also have
different degrees of improvement in all datasets.
Here, it is worth noting that when selecting the
sample for inter-entity deconfounding, we select
samples based on factors with which the order con-
founder is most likely to have impact, such as the
minimum order of last entity in the whole training
dataset and whether it is easy to perform permuta-
tion such as the number of target entities. Besides,
we have not select all samples for augmentation,
and the results in Table 1-3 may not be the best.8134.4 Robustness Testing
To verify the effectiveness of the two data augmen-
tation methods we designed for de-confounding,
we conducted robustness testing experiments on
CoNLL03, ACE04 and CADEC, respectively.
The pre-context confounder introduce error bias
into the model by incorrectly relying on prefix se-
quences during entity sequence generation in the
training phase. To verify the effectiveness of our
Intra-entity Deconfounding Data Augmentation
method in eliminating the pre-context confounder,
we designed robustness testing experiments. In
decoding, we randomly sample several words as
pre-context sequences, and then require the model
to continue decoding the entities. The experimental
results are shown in Table 4. We can observe that
the performance of both the baseline model and the
Intra-entity Deconfounding model have different
degrees of degradation after the attack of random
fixed pre-context. However, the relative perfor-
mance degradation of the Intra-entity Deconfound-
ing model is less, and the ∆F1 on ACE04, CADEC
and CoNLL are improved by+1.44%, +1.21% and
+0.43% relative to the baseline model. This indi-
cates that after Intra-entity Deconfounding Data
Augmentation, the model can eliminate the pre-
context confounder to some extent.
We also verify the robustness of the Inter-
entity Deconfounding Data Augmentation method
against the entity-order confounder. We first ran-
domly sample kentities as the prefix of the decod-
ing sequence, and then let the model continue to
generate entities. For convenience, we choose a
sample of the test set with the number of entities
greater than kfor evaluation, and we do not con-
sider the krandomly sampled correct entities in
our evaluation. In our experiments, k= 4. From
Table 5, we can observe that the performance of
both models decreases after the attack of random
entity order. However, after deconfounding the en-
tity sequences by the Inter-entity Deconfounding
Data Augmentation method, the model degrada-
tion is reduced, and the ∆F1 on ACE04, CADEC,
and CoNLL are improved by +0.49%, +0.71%, and
+0.19% relative to the baseline model. This in-
dicates that the Inter-entity Deconfounding Data
Augmentation method we designed can enhance
the robustness of the model to cope with random
entity order when generating entity sequences, i.e.,
the entity-order confounder are eliminated to some
extent.5 Related Work
5.1 NER Task
The existing models can be basically divided into
sequence labeling formulation, span-based formu-
lation and generative-based formulation. Among
them, the sequence labeling formulation was ear-
lier applied to solve the NER problem (McCal-
lum and Li, 2003; Collobert et al., 2011; Huang
et al., 2015; Chiu and Nichols, 2016; Lample et al.,
2016; Straková et al., 2019; Yan et al., 2019; Li
et al., 2020a). After Nested NER and Discontinu-
ous NER were discovered and raised, inspired by
the successful application of sequence labeling for-
mulation on Flat NER subtask, Metke-Jimenez and
Karimi (2016); Muis and Lu (2017) attempted to
extend this approach to the new subtasks. Others
chose a different path, based on the characteristics
of Nested NER, Xu et al. (2017); Wang and Lu
(2019); Yu et al. (2020) try to traverse all possible
spans and do classification at the span-level. Shen
et al. (2021a) try to reduce the number of candidate
spans and Tan et al. (2021) make the left and right
boundaries of the candidate spans completely un-
fastened. In addition, in order to apply span-based
formulation to the Discontinuous NER, the concept
of hypergraph was introduced to efficiently repre-
sent spans (Lu and Roth, 2015; Katiyar and Cardie,
2018; Muis and Lu, 2016).
Although sequence labeling formulation and
span-based formulation can be applied to different
subtasks separately, these formulations are diffi-
cult to be applied to them simultaneously. Among
them, sequence labelling formulation needs to de-
sign different tagging schema for different NER
subtasks (Ratinov and Roth, 2009; Metke-Jimenez
and Karimi, 2016; Straková et al., 2019; Dai et al.,
2020), while span-based formulation needs to sacri-
fice a certain degree of performance. For example,
span-based methods need to set a maximum span
length to avoid the number of candidate spans to
be traversed (Xu et al., 2017; Luan et al., 2019b;
Wang and Lu, 2018), since it is impossible to enu-
merate all possible spans, which is quadratic to the
length of the sentence and fragment numbers of
discontinuous entity.
Contrary to sequence-labeling and span-based
formulation, generative-based formulation can be
used to model these subtasks in a unified manner
because it can generate variable-length sequences
(Yan et al., 2021b). However, since the generative
model uses autoregressive generation, its optimiza-814tion objective differs significantly from the extrac-
tion objective of the NER task, which results in the
model being influenced by some confounders and
thus reduces the performance of model.
5.2 Causal Inference
Causal inference is a science that studies the rela-
tionship between correlation and causality. It is not
only an explanatory framework, but also a way to
provide solutions to achieve desired goals by pursu-
ing causal effects (Pearl et al., 2016; Fenton et al.,
2020). So far, it has been achieved greatly success
in various domains such as psychology, politics and
epidemiology for years (Mackinnon et al., 2007;
Luke, 2015; Alves et al., 2014). Recently, causal
inference has also attracted increasing attention
in nature language process for improving model’s
performance in various ways. For example, Gard-
ner et al. (2020) constructs counterfactual samples
by manually rewriting the rules, and Garg et al.
(2019) frames counterfactual samples by heuristi-
cally replace some keywords. Compared to them,
our method offers a fundamental way to remove the
confounder in training phase for generative mod-
els which is applied to various tasks of essentially
non-sequential problem.
6 Conclusion
In this paper, we analyze two kinds of confounder
that generative models arised when applied to NER
and use backdoor adjustment methods in causal
inference to perform deconfounding. Specifically,
for pre-context confounder and entity-order con-
founder, we respectively design Intra-entity and
Inter-entity De-confounding Data Augmentation
methods. Experiments show that the performance
of the model improves on all datasets after decon-
founding. In the future, we will continue to explore
the application of causal inference to other tasks.
Acknowledgments
This work is supported by the Key Research and
Development Program of Zhejiang Province, China
(No. 2021C01013), the Chinese Knowledge Center
of Engineering Science and Technology (CKCEST)
and MOE Engineering Research Center of Digital
Library.
References815816817818