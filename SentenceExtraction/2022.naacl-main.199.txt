
Prasetya Ajie Utama Joshua BambrickNaﬁse Sadat Moosavi Iryna GurevychBloomberg, London, United KingdomUKP Lab, Technical University of Darmstadt, GermanyDepartment of Computer Science, The University of She ﬃeld
{putama,jbambrick7}@bloomberg.net
Abstract
Neural abstractive summarization models are
prone to generate summaries which are fac-
tually inconsistent with their source docu-
ments. Previous work has introduced the task
of recognizing such factual inconsistency as a
downstream application of natural language in-
ference (NLI). However, state-of-the-art NLI
models perform poorly in this context due to
their inability to generalize to the target task.
In this work, we show that NLI models can be
eﬀective for this task when the training data
is augmented with high-quality task-oriented
examples. We introduce F alsesum , a data gen-
eration pipeline leveraging a controllable text
generation model to perturb human-annotated
summaries, introducing varying types of fac-
tual inconsistencies. Unlike previously intro-
duced document-level NLI datasets, our gen-
erated dataset contains examples that are di-
verse and inconsistent yet plausible. We show
that models trained on a F alsesum -augmented
NLI dataset improve the state-of-the-art perfor-
mance across four benchmarks for detecting
factual inconsistency in summarization.
1 Introduction
Recent advances in conditional text generation and
the availability of large-scale datasets have given
rise to models which generate highly ﬂuent ab-
stractive summaries (Lewis et al., 2019; Zhang
et al., 2019). However, studies indicate that such
models are susceptible to generating factually in-
consistent outputs, i.e., where the content of the
summary is not semantically entailed by the in-
put document (Kryscinski et al., 2019; Goodrich
et al., 2019). This motivates a new line of research
for recognizing factual inconsistency in generated
summaries (Kryscinski et al., 2020; Pagnoni et al.,
2021; Wang et al., 2020; Fabbri et al., 2021).This factual consistency problem is closely re-
lated to the task of natural language inference (NLI)
whereby a hypothesis sentence is classiﬁed as ei-
ther entailed, neutral, or contradicted by a given
premise sentence (Condoravdi et al., 2003; Dagan
et al., 2006; Bowman et al., 2015). Using an in-
put document as the premise and a corresponding
generated summary as the hypothesis, earlier so-
lutions have adopted out-of-the-box NLI models
to detect factual inconsistency, albeit with limited
success (Falke et al., 2019; Kryscinski et al., 2020).
This poor performance largely stems from the
fact that most NLI datasets are not designed to
reﬂect the input characteristics of downstream
tasks (Khot et al., 2018). Such datasets may not
always capture the kinds of entailment phenom-
ena which naturally arise from neural abstractive
summarization. More importantly, there is also a
discrepancy in terms of the input granularity, i.e.,
the premises in this consistency classiﬁcation task
consist of multi-sentence documents while com-
mon NLI datasets use single-sentence premises.
In this work, we introduce Falsesum , a data
generation pipeline that produces NLI examples
consisting of documents paired with gold sum-
maries as positive examples and automatically
generated inconsistent summaries as negative
examples. We propose a novel strategy to train a
text generation model to render false summaries
of a given document using only supervision from
an existing summarization dataset (Nallapati
et al., 2016). In addition, our generator supports
switchable input control codes to determine the
type of factual error exhibited in the generated
output. This design allows Falsesum to compose
diverse and naturalistic outputs which more closely
resemble the inconsistent summaries generated by
summarization models (Maynez et al., 2020). This
contrasts with previous solutions (e.g., Kryscinski
et al., 2020; Yin et al., 2021), which synthesize
NLI examples using rule-based transformations2763
or language model-based replacements, limiting
their diversity and ability to reﬂect realistic factual
errors in summarization. Overall, our contributions
in this paper are the following:
First, we present a novel training pipeline to
create a text generation model which takes as input
a pair of a document and a corresponding gold
summary. It then perturbs the summary such that it
is no longer factually consistent with the original
document. Our strategy obviates the need for
explicit examples of inconsistent summaries, using
only an existing summarization dataset. We use
this model to generate a large-scale NLI dataset
for the task of recognizing factually inconsistent
summaries. The resultant dataset consists of pairs
with documents as the premise and naturalistic
summaries as the hypotheses, each labeled as
either entailment ornon-entailment .
Second, we demonstrate the utility of our gen-
erated data for augmenting existing NLI datasets.
We show that on four benchmark datasets, NLI
models trained on Falsesum -augmented data out-
perform those trained on previous document-level
NLI datasets. We conduct an analysis to show that
Falsesum -generated summaries are plausible and
hard to distinguish from human-written summaries.
Lastly, we show that the improvement over the
benchmarks is largely attributable to the diversity
of factual errors that F alsesum introduces.
2 Related Work
This work is related to the growing body of re-
search into factual consistency and hallucination
in text generation models, particularly for summa-rization (Cao et al., 2018). Research has found that
around 30% of summaries generated by abstractive
summarization models contain information which
is inconsistent with the source document (Kryscin-
ski et al., 2019). This motivates the development
of an automatic approach to assess factual consis-
tency in generated summaries, in addition to the
benchmark datasets to measure the progress in this
task (Falke et al., 2019; Kryscinski et al., 2020;
Pagnoni et al., 2021; Fabbri et al., 2021).
Earlier work by Goodrich et al. (2019) proposes
to use an information extraction model to extract
relation tuples from the ground-truth summary text
and the generated summary and then count the over-
lap as the measure of factuality. Eyal et al. (2019);
Durmus et al. (2020); Wang et al. (2020) use a
question-answering model to detect factual incon-
sistency by matching the predicted answers using
the document and the summary as the context.
Concurrently, researchers have drawn a connec-
tion between factual consistency and natural lan-
guage inference (NLI), observing that all infor-
mation in a summary should be entailed by the
source document. While this approach enables the
summary to be directly evaluated without ﬁrst ex-
tracting its intermediate semantic structure, earlier
attempts were largely unsuccessful. Falke et al.
(2019) use the probabilities assigned to the entail-
ment label by NLI models to re-rank the summary
candidates given by beam search but found no im-
provement in the consistency errors. Kryscinski
et al. (2020) evaluate out-of-the-box NLI models
on the task of inconsistency detection in a binary
classiﬁcation setting and show that the performance
is only slightly better than majority voting.
In the same paper, Kryscinski et al. (2020) pro-2764pose FactCC, a synthetic NLI data generation pro-
cess which applies a set of transformation rules to
obtain examples of inconsistent summaries (e.g.,
sentence negation, entity swapping). They demon-
strate that the resulting NLI model performs well
on realistic test cases which are obtained by manu-
ally annotating the output of several summarization
models. This highlights the importance of NLI
examples beyond sentence-level granularity and
which more closely resemble the input characteris-
tics of the downstream tasks (Mishra et al., 2021).
While the FactCC model is moderately e ﬀec-
tive for detecting factual inconsistency, subsequent
work indicates that it only performs well on easier
test cases, where highly extractive summaries (i.e.,
those with high lexical overlap between a summary
and the source document) tend to be factually con-
sistent and more abstractive summaries are likely to
be inconsistent (Zhang et al., 2020). Furthermore,
Goyal and Durrett (2021) show that the synthetic
and rule-based nature of FactCC leads to lack of
diversity of consistency error types and it poorly
aligns with the error distribution found in more
abstractive summaries.
Falsesum addresses these limitations using con-
trolled natural language generation to construct an
NLI dataset which better targets the summarization
domain. Inspired by the recent work on control-
lable generation (Keskar et al., 2019; Ross et al.,
2021), we employ a generation model conditioned
on an input code which controls the type of consis-
tency errors induced. We further use the generated
document-level NLI examples for augmentation
and show that NLI models can beneﬁt from the
additional data without hurting their existing infer-
ence ability (Min et al., 2020).
3 F alsesum Approach
3.1 Design Overview
Falsesum takes as an input a source document D
and a corresponding reference summary S. The
framework then preprocesses andformats Dand
Sand feeds them into a generation model G
which outputs a factually inconsistent summary
S. For each summarization example, we then
have both positive (entailment) and negative (non-entailment) NLI tuples (D,S,Y=1),(D,S,Y=
0), which consist of a document-level premise, a
summary sentence, and the consistency label ( 1
indicates entailment).
Falsesum aims to produce a naturalistic S
which is contrastive with respect to its correspond-
ingS. This means that SandSshould be in-
distinguishable in their surface characteristics (e.g.,
style, length, vocabularies) and only di ﬀer in their
factual consistency with respect to D. This ensures
that the resulting NLI model learns the correct no-
tion of factual consistency rather than discriminat-
ing based on surface features (McCoy et al., 2019).
In addition to naturalness, we consider the diversity
of the consistency error types exhibited by S. We
follow the consistency error typology introduced
by Maynez et al. (2020), which categorizes con-
sistency errors as either intrinsic , i.e., errors due
to incorrect consolidation of information from the
source document, or extrinsic , i.e., errors due to
assuming new information not directly inferable
from the contents of the source document.
As illustrated in Figure 1, a generation model
Gis trained to imitate the consistency mistakes
of summarization models. Speciﬁcally, it gener-
ates perturbed summaries by either (1)incorrectly
inserting pieces of information from the source doc-
ument into random spans of the original summary;
or(2)amending pieces of information in the sum-
mary by hallucinating new “facts” not present in
the source document.
To this end, the framework identiﬁes (♦i)what
information or “facts” in the source document are
available to the generator; and (♦ii)where the in-
correct information can be inserted into the gold
summary, which is indicated by span masking . We
obtain both by subsequently performing input pre-
processing andformatting steps (§3.2 and §3.3).
Next, we deﬁne the following seq2seq task to
train the modelG: “Given (♦i)a list of shuﬄedand
formatted pieces of information extracted from
source document and gold summary and (♦ii)a
partially masked gold summary, ﬁll in the blanks
and generate the original gold summary.” Note
that using gold summaries means that we can apply
the existing summarization corpus to train Gto
generate more coherent and plausible sentences.
3.2 Input Preprocessing
Following Goodrich et al. (2019), “facts” in the
source document and the gold summary are de-2765ﬁned as an open information extraction (OpenIE)
tuple, which represents the predicate and argument
structures found in a sentence. We denote each re-
lation tuple as (arg,pred,..., arg), where predi-
cate pred describes the event ( what happened) and
its complementing semantic arguments argrep-
resent the who,to whom ,where , orhow of the
event. Predicates are usually the main verb of a
clause. Both predicates and their arguments consist
of spans of tokens (Fader et al., 2011).
We use an OpenIE implementation of Pred-
Patt (White et al., 2016; Zhang et al., 2017), a
pattern-based framework for predicate-arguments
extraction.As illustrated in the top half of Fig-
ure 2, we extract the relation tuples from each
source document and its corresponding reference
summaries. To minimize the risk of Ginadvertently
generating consistent summaries, we corrupt each
extracted “fact” by removing one randomly chosen
argument from each tuple. For instance, OpenIE
may extract the following tuple from a sentence:
(Jo,plans to give,Alex,apples)
We then randomly choose applesto be re-
moved from the tuple. We additionally lemmatize
the dependency root word of each argument and
predicate span, e.g., plans to give⇒plan to give .
This forces the model to learn to correct for gram-
maticality by inﬂecting the spans when inserting
them to the masked spans. Once all such spans
are extracted and processed, they are grouped and
shuﬄedinto two lists (predicates and arguments).
3.3 Input Formatting
LetP=(PRED,..., PRED)andA=(ARG, ...,
ARG)be the unordered lists of extracted predi-
cates and arguments from a source document D
and the summary sentence S. Additionally, we
assume a masked summary sentence M(described
later), derived from S, and a control code vari-
able c∈{intrinsic ,extrinsic}. GeneratorG
is trained to compute p(S|P,A,M,c). As illus-
trated in the bottom half of Figure 2, we encode all
the conditional variables into the following format: P; A; c; M
In the following, we describe the key steps in the
input formatting process:
Step 1: Span Removal Initially, PandAin-
clude predicate and argument spans from the orig-
inal summary which may be used to reconstruct
S. However, at testtime we remove these “gold”
spans from the two lists to force the Gto make con-
sistency mistakes. The removal is also done when
training the model for control code extrinsic to
trainGto predict plausible unseen spans.We sum-
marize the di ﬀerent input formatting in Table 1.
Step 2: Span Reduction To encourageGto
generate ﬁne-grained errors (Pagnoni et al., 2021;
Goyal and Durrett, 2021), we also train it to hal-
lucinate incorrect modiﬁers into spans from Pand
A. To this end, we randomly drop adjectives and
adverbs from 10% of the gold predicate and argu-
ment spans. For instance, an argument span “re-
cently elected prime minister” will be reduced to
“minister”. This teaches the model to generate the
remaining part of the span given only the context
provided in the formatted input.
Step 3: Control Code To control the type of
consistency errors generated by G, we append the
string “ code: ” followed by either “ intrinsic ”
or “extrinsic ” into the input tokens. The code is
determined randomly with equal probability of 0.5.2766
Once the code is chosen, we perform the remaining
formatting steps accordingly (see Table 1).
Step 4: Summary Masking We derive masked
summary Mby replacing the spans of randomly
selected predicates and arguments with a special to-
ken<span_i> , where i=0is reserved for the pred-
icate, and i>0for their arguments. These tokens
control where the incorrect information should be
inserted by the generator model into the original
summary (see Table 1).
3.4 Training F alsesum
We run the Falsesum data generation pipeline on
thetrain split of the CNN /DailyMail corpus (Her-
mann et al., 2015), originally collected for ques-
tion answering, but subsequently reformulated for
summarization by Nallapati et al. (2016). This
dataset contains English news documents paired
with human-written summaries, each consisting of
multiple sentences. We break the summaries down
such that each Falsesum example consists of the
document text and a single sentence summary. We
then run the preprocessing andformatting steps
on each document-summary pair. The resulting
pairs of formatted input and target output are sub-
sequently split into train and test sets which consist
of 394,774 and 262,692 instances, respectively.We use the T5-base model (Ra ﬀel et al., 2020)
as generatorGand ﬁne-tune it on the seq2seq task
described in §3.1. The NLI examples are produced
by running the ﬁne-tuned generator on the prepro-
cessed and formatted test split.This renders an
equal number of positive and negative examples.
In our experiments, we randomly sample 100,000
Falsesum examples to augment the NLI dataset.
4 Experimental Settings
Our experiments aim to demonstrate the e ﬀective-
ness of Falsesum -generated document-level exam-
ples for NLI dataset augmentation. We evaluate
the downstream performance of the NLI models
by testing them against several benchmarks for
determining the factual inconsistency of generated
summaries. In this section, we describe the training
setup of the NLI models, including the model and
both the sentence- and document-level datasets.
4.1 Training
NLI models We train several NLI models by
ﬁne-tuning RoBERTa-base (Liu et al., 2019)
oneither the original or the augmented MNLI
dataset (Williams et al., 2018). The MNLI dataset
consists of 392,702 train instances, each labeled2767as either “ entailment ”, “neutral ”, or “ contradic-
tion”. To enable the application of NLI data to this
factual consistency task, we use a binary formula-
tion of NLI, where the “neutral” and“contradic-
tion” labels are combined into “non-entailment” .
The document-level inputs are formatted similarly
to sentence-level examples, i.e., the document
premise Dand summary hypothesis ( SorS)
are concatenated and a special classiﬁcation token
([CLS] ) is used (Devlin et al., 2019).
Document-level NLI datasets We conduct aug-
mentation comparisons with several multi-sentence
NLI datasets which obtain examples from news or
summarization domains. We consider the follow-
ing datasets: ANLI (Nie et al., 2020), a paragraph-
level NLI dataset collected via an iterative and
adversarial human-in-the-loop annotation proto-
col. It consists of mostly Wiki data but also in-
cludes a small portion of news text; DocNLI (Yin
et al., 2021), a document-level NLI dataset con-
taining multi-sentence premise and hypothesis sen-
tences, collected by converting QA examples to
NLI instances (Demszky et al., 2018) and replac-
ing words and sentences in news summaries us-
ing a language model; FactCC (Kryscinski et al.,
2020), a large-scale dataset speciﬁcally generated
for training summary factual correctness classiﬁ-
cation models. The positive examples in FactCC
are obtained by backtranslating a random sentence
from a CNN /DailyMail news story, while nega-
tive examples are obtained by perturbing the sen-
tence using predeﬁned rules, e.g., entity swapping.
For fair comparison, we sample 100,000 examples
from each augmentation dataset in our experiments.
4.2 Benchmark Datasets
We evaluate these NLI models on four benchmark
datasets to classify the factual consistency of ab-
stractive summaries. These datasets di ﬀer in terms
of the annotation protocol, the granularity of the
summaries (single- or multi-sentence), the sum-
marization corpus used, and the models used to
generate the summaries that are annotated. The
tasks are formulated as a binary classiﬁcation with
the labels “consistent” and“inconsistent” . We
evaluate NLI models on these tasks by mapping the
predicted label “entailment” to“consistent” and
“non-entailment” to“inconsistent” . The bench-
marks datasets are detailed in the following:
FactCC In addition introducing a synthetic train-
ing dataset for the task, Kryscinski et al. (2020)introduce a manually annotated test set. It contains
1,431 document and single-sentence summary pairs
generated by various neural abstractive summariza-
tion models trained on CNN /DailyMail corpus.
Ranksum Falke et al. (2019) formulate the fac-
tual consistency problem in summarization as a
ranking task. They introduce a dataset consist-
ing of 107 documents, each paired with a set of
ﬁve ranked summary candidates obtained from the
beam search of a summarization model. Given the
manually annotated consistency label on summary
candidates, the task is to re-rank the list such that
the top-1 summary is factually consistent.
Summeval Fabbri et al. (2021) introduce a com-
prehensive benchmark for factual consistency de-
tection in summarization. It includes summaries
generated by seven extractive models and sixteen
abstractive models, which are judged by three an-
notators using a 5-point Likert scale.
QAGS The dataset collected by Wang et al.
(2020) consists of 239 test set instances from
XSUM (Narayan et al., 2018) and 714 instances
from CNN /DailyMail.Each instance consists of
a pair of a source document and a single-sentence
summary, which is labeled via majority voting on
three annotators’ labels.
5 Results and Discussion
5.1 Main Results
Performance on FactCC, QAGS, and SummEval is
measured using balanced accuracy, which is suit-
able for class imbalanced settings, since the factu-
ally consistent label is the majority in some bench-
mark datasets. It is deﬁned as the average recall
of the two classes, such that majority label voting
obtains only a 50% score. To measure ranking per-
formance in Ranksum, we calculate the average
Precision@1, which computes the fraction of times
a factually consistent summary is ranked highest
on each test instance. We perform ﬁve training
runs for each setup using di ﬀerent random seeds
and take the mean to address performance instabil-
ity (Reimers and Gurevych, 2017).2768
From the results in Table 2, we observe the
following: (1)Models trained on sentence-level
MNLI datasets perform poorly when evaluated
directly on document-level benchmarks, even af-
ter we increase the maximum input token length
from 128 to 512;(2)This limitation can be
alleviated by the sentence-wise prediction strat-
egy ( [split-doc] MNLI-128),which achieves
66.63. Note, however, that this improvement comes
at the expense of compute cost which is multi-
plied by a signiﬁcant factor; (3)DocNLI and ANLI
perform poorly even though they contain longer
premise sentences, indicating that the length mis-
match may not be the primary issue; (4)Falsesum
obtains substantial improvement over the previous
state-of-the-art FactCC, despite being derived from
the same summarization dataset (CNN /DailyMail).
This indicates that Falsesum provides higher qual-
ity examples and includes more types of entailment
phenomena that occur naturally in this task.
5.2 Ablation Analysis on F alsesum Data
We perform an ablation analysis to study how
each component of our data generation pipelinecontributes to the ﬁnal performance. We ﬁrst re-
move the contrastive property of the Falsesum data
by randomly including only either the positive
(D,S,Y=1)ornegative (D,S,Y=0)NLI
examples obtained from a single (D,S)pair. Next,
we ﬁlter out the negative NLI instances that are
generated using either intrinsic orextrinsic
code. We refer to the three ablated datasets as
−contrastive ,−intrinsic and−extrinsic ,
respectively. We set the sampled training size to
100,000 for the three ablation setups and aggregate
the results from ﬁve training runs.
Table 3 shows the performance of the ablated
models. We observe that removing contrastive
pairs in the augmented training data results in a
1.06% drop on the overall benchmarks score. We
also see that removing intrinsic error examples
results in the highest performance loss, −5.03%
compared to−2.22% by−extrinsic . This is ex-
plained by the fact that intrinsic consistency errors
are more dominant on benchmarks that are built
on the CNN /DailyMail corpus (Goyal and Dur-
rett, 2021). We conclude that all the above prop-
erties are important for the overall improvements
obtained by F alsesum .
5.3 Fine-grained Evaluation
Previous work has shown that NLI models are
prone to relying on fallible heuristics which asso-
ciate lexical overlap with entailment labels (McCoy
et al., 2019). In the factual consistency task, this
corresponds to models associating highly extractive
summaries with the “consistent” label. This raises
a question about whether Falsesum data alleviates
this tendency in the resulting NLI models.
To answer this question, we partition the FactCC
annotated test examples into ﬁve ordered sub-
sets based on the lexical overlap between their2769
summary hypothesis and the source document
premise. We deﬁne an overlap score using the
normalized coverage anddensity summary extrac-
tiveness scores introduced by Grusky et al. (2018).
Both measures have the range [0.0,1.0], where
density =1.0indicates that all words in a sum-
mary are also present in the source document and
normalized coverage =1.0indicates that the sum-
mary is obtained by copying a continuous frag-
ment of the source document. We then deﬁne
overlap =normalized coverage ×density .
Figure 3 shows the comparison of FactCC and
Falsesum augmentation performance across vary-
ing lexical overlap scores. We see that Falsesum
performs better on all subsets of the FactCC test
set with the greatest performance gap appearing
on the 0.9overlap subset. Upon closer inspection,
we see that the FactCC model makes mostly false
positive classiﬁcation errors on this subset, i.e., it
tends to predict highly extractive summaries as
“consistent”, leading to near majority voting perfor-
mance of 50%.Falsesum , on the other hand, better
discriminates the factual consistency of examples
without over-relying on lexical overlap.
5.4 Data Quality Analysis
We conduct both manual and automatic quality
evaluation of the Falsesum -generated dataset. First,
we sample 200 generated negative examples and
manually verify whether (i) the perturbed sum-
mary Sis indeed factually inconsistent; (ii) the
type of consistency error follows the speciﬁed con-
trol code; (iii) the incorrect “fact” is inserted at
the speciﬁed missing span. Following Kryscinski
et al. (2020), the authors perform this annotation
to avoid high disagreement by crowd annotators in
this task (Falke et al., 2019). The results in Table 4
show that about 86% of intrinsic 81% of extrinsic
generated error examples are factually inconsistent,
which happen due to several reasons, e.g., gen-
erator model chooses a span from the list that is
similar to the original span, or generator model
correctly guesses the original missing span. This
further suggests that pre-trained language models
such as RoBERTa-base can be robust against the
induced label noise and can still learn a performant
classiﬁer. WhileGalmost always inserts the incor-
rect “fact” at the speciﬁed positions, we observe
that it often fails to follow the speciﬁed extrinsic
code correctly. We suspect that this is because the
model prefers the easier task of copying the input
over generating novel phrases.
Following Gururangan et al. (2018), we also
evaluate the naturalness of the generated dataset.
We train an NLI model using positive examples
from CNN /DailyMail and Falsesum -generated neg-
ative examples. The model receives no premise so
must distinguish between entailed and non-entailed
hypotheses using semantic plausibility or spuri-
ous surface features, e.g., grammatical mistakes
or ﬂuency errors. The relatively low accuracy of
these models on Falsesum data (shown in Table 5)
suggests that, compared to FactCC and DocNLI,
Falsesum -generated summaries are relatively hard
to distinguish from the gold ones.2770Conclusion
NLI models present a promising solution for au-
tomatic assessment of factual consistency in sum-
marization. However, the application of existing
models for this task is hindered by several chal-
lenges, such as the mismatch of characteristics be-
tween their training dataset and the target task data.
This mismatch includes the di ﬀerence in terms of
the input granularity (sentence vs. document level
premises) and the types of (non-)entailment phe-
nomena that must be recognized.
In this work, we present Falsesum , a data gener-
ation pipeline which renders large-scale document-
level NLI datasets without manual annotation. Us-
ing our training strategy, we demonstrate that it is
possible to learn to generate diverse and naturalis-
tic factually inconsistent (non-entailed) summaries
using only existing (entailed) consistent summaries
for training. We show that the resultant data is ef-
fective for augmenting NLI datasets to improve the
state-of-the-art performance across four summary
factual inconsistency benchmarks.
Acknowledgments
We would like to thank Marco Ponza, Marco Fis-
cato, Umut Topkara and other colleagues from
Bloomberg AI for the thoughtful discussion and
feedback throughout this project. We also thank
Leonardo Ribeiro for comments on the earlier ver-
sion of this work and the anonymous reviewers
for their constructive feedback. The authors af-
ﬁliated with UKP were supported by the German
Research Foundation through the research training
group “Adaptive Preparation of Information from
Heterogeneous Sources” (AIPHES, GRK 1994 /1)
and by the German Federal Ministry of Education
and Research and the Hessian State Ministry for
Higher Education, Research and the Arts within
their joint support of the National Research Center
for Applied Cybersecurity ATHENE.
References277127722773A Hyperparameters
Generator model We train a T5-base model
for three epochs with batch size of 24 using the
AdamW optimizer. We set the maximum source
token length to 256 and the target token length to
42. We use a learning rate of 3eand ﬁx the ran-
dom seed to 11. For decoding, we set the minimum
and maximum sequence length to 10 and 60, re-
spectively. We sample using beam search with a
beam of size two. We additionally set the repetition
penalty to 2.5 and the length penalty to 1.0.
Classiﬁcation model We train RoBERTa-base
models on augmented and original MNLI datasets
for three epochs with a batch size of 32. The learn-
ing rate is set to 1e, while the maximum input
token length is set to either 128 or 512. We use the
following random seeds for the ﬁve training runs:
11, 12, 13, 14, and 15.
B Aggregating Predictions
We follow Falke et al. (2019) to adapt out-of-the-
box MNLI models to document-level input by per-
forming a sentence-wise prediction before aggre-
gating the output. Given a document Dconsisting
of sentences d,..., d, and a multi-sentence sum-
mary Sconsisting of s,..., s, we aggregate the
probability scores given by the classiﬁer model F
on each d,spair. The aggregated consistency
scoreσ(D,S) is given by:
σ(D,S)=1
m/summationdisplaymaxF(d,s)
This means that it is su ﬃcient for a summary sen-
tence to be factually consistent given only a single
entailing sentence in the source document. We then
take the average scores across the summary sen-
tences since each of them needs to be entailed by
the source document. We use a similar aggregation
method to evaluate augmented MNLI models on
multi-sentence summaries from the Summeval and
Ranksum benchmarks.
C F alsesum Details
In the preprocessing steps, we only perform the
predicate and argument span extraction on the ﬁrst
15 sentences for computational e ﬃciency. For train-
ing, this is not an issue since the gold spans from
the reference summary are included in the input.
Additionally, we may extract multiple OpenIE re-
lation tuples from each sentence. To avoid havingoverlapping spans from a single input, we randomly
select two tuples from each sentence.
D Falsesum Examples
We include more examples of generated NLI in-
stances in Table 6. We also include cases where
Falsesum inadvertently generates factually consis-
tent summaries in Table 7. Lastly, we show several
examples of the formatted input and the generated
output at testtime in Table 8.277427752776