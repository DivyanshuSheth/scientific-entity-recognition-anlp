
Yiwei Li, Shaoxiong Feng, Bin Sun, Kan Li
School of Computer Science, Beijing Institute of Technology
{liyiwei,shaoxiongfeng,binsun,likan}@bit.edu.cn
Abstract
Generative dialogue models suffer badly from
the generic response problem, limiting their
applications to a few toy scenarios. Recently,
an interesting approach, namely negative train-
ing, has been proposed to alleviate this prob-
lem by reminding the model not to gener-
ate high-frequency responses during training.
However, its performance is hindered by two
issues, ignoring low-frequency but generic re-
sponses and bringing low-frequency but mean-
ingless responses. In this paper, we propose a
novel negative training paradigm, called nega-
tive distillation, to keep the model away from
the undesirable generic responses while avoid-
ing the above problems. First, we introduce a
negative teacher model that can produce query-
wise generic responses, and then the student
model is required to maximize the distance
with multi-level negative knowledge. Em-
pirical results show that our method outper-
forms previous negative training methods sig-
niﬁcantly.
1 Introduction
In the past few years, data-driven response gener-
ation (Vinyals and Le, 2015; Shang et al., 2015;
V ougiouklis et al., 2016) has achieved impressive
performance, drawing continuously increasing at-
tention from academia and industry. Convention-
ally, with the guidance of maximum likelihood
estimation (MLE), neural dialogue models are ex-
pected to maximize the probability of generating
the corresponding reference given any query. Un-
fortunately, due to the many-to-one phenomenon
(see Table 1), a characteristic of the dialogue task
(Csáky et al., 2019), these models are prone to pro-
duce safe but generic responses (e.g., I don’t know
(Li et al., 2016)), which sets an obstacle for the
generative dialogue system to be deployed widely.
Some researchers tried to redesign the objectiveof models to meet the requirement of diverse re-
sponses instead of MLE, such as MMI (Li et al.,
2016), AdaLabel (Wang et al., 2021), and IAT
(Zhou et al., 2021). Besides, several studies (Ku-
likov et al., 2019; Holtzman et al., 2020) proposed
more advanced decoding strategies to alleviate the
problem of generic responses. Indeed, the above
methods boost the diversity of responses by remind-
ing the model what should be said.
However, inspired by negative training (Kim
et al., 2019; Ma et al., 2021), we argue that it is also
necessary to tell the dialogue model what not to say.
To alleviate the problem of generic responses, He
and Glass (2020) negatively updates the parameters
when identifying the high-frequency responses. Li
et al. (2020a) punishes the behaviors of generating
repetitive or high-frequency tokens by using the
unlikelihood objective (Welleck et al., 2020).
Although the negative-training based methods
enhance the diversity of responses, there still exists
two drawbacks: First, they regard high-frequency
tokens or utterances as negative candidates. How-
ever, the high-frequency response problem is only
a sub-problem of the generic response problem (He
and Glass, 2020). It means that the responses that
are low-frequency but generic will escape from pun-
ishment. Even worse, we have observed that some
generic responses followed by a low-frequency but
meaningless subsequence can avoid being identi-
ﬁed as high-frequency, which inevitably sacriﬁces
the ﬂuency of responses (see Analysis). Second,
these methods ignore the implicit negative knowl-
edge in neural networks that characterizes negative
candidates at multiple levels. We contend that it
is more effective to conduct negative training with
richer information (e.g., hierarchical representa-
tion).
To tackle the above problems and further im-
prove the diversity of responses, we propose a
novel negative training paradigm called Negative
Distillation (ND). Conventional knowledge distil-407Query Response Entropy Score
1: Oh, honey, you made a mistake. I don’t know how to do it. 8.61
2: Would you like regular car wash package? I don’t know what you mean. 8.75
3: I’m looking for the airport. No, sorry. I don’t know . 10.69
Can you tell me how to get there?
4: That’s cathy. She is pretty, isn’t she? Well, I don’t know . 12.14
She likes dancing. Ask her to dance.
5: It doesn’t matter. You gotta ﬁnd what she’s I don’t know . . . 6.82
interested in and go with that.
lation (KD) (Hinton et al., 2015; Jiao et al., 2020)
takes the teacher as a positive role model and in-
duces the student to imitate. Differing from that,
we train the teacher as a negative role model and
remind the student to get rid of those bad behaviors.
Speciﬁcally, we ﬁrst collect a negative training
set by using a ﬁltering method called Source En-
tropy (Csáky et al., 2019). This ﬁltering method can
retrieve all many-to-one cases of the raw dataset.
Note that the “one” is usually a generic response.
Then, we train a dialogue model on the above sub-
set as the negative teacher. Given queries, the nega-
tive teacher can provide a set of negative candidates
(i.e., generic and dull responses) that the student
is prone to generate, which avoids the ﬁrst draw-
back mentioned before. Therefore, the student ob-
tains query-wise bad behaviors for Negative Distil-
lation . To conduct the negative update holistically,
we design two negative objectives, including soft
unlikelihood loss on the prediction layer and re-
verse square error on the intermediate layer. In this
way, the negative distillation fully exploits multi-
level negative knowledge to force the student to
generate non-generic responses.
Our contributions are summarized as follows:
•We propose a novel and effective negative
training paradigm called Negative Distillation .
It constructs query-wise generic responses as
the negative candidates.
•We design two negative objectives to utilize
multi-level information to further boost the
performance of negative distillation.
•We perform extensive experiments and de-
tailed analysis to verify the effectiveness ofthe negative distillation framework and the
superiority compared with previous negative
training methods.
2 Method
In this section, we ﬁrst introduce the negative
teacher, then describe the negative distillation on
the prediction layer and the intermediate layer, re-
spectively, and ﬁnally present the progressive opti-
mization objective. Algorithm 1 shows the whole
training details.
2.1 Background
Dialogue Generation with MLE TakeQ=
{q,q,...,q}andR={r,r,...,r}as the
(query, response ) pair, where TandTrepresent
the length of query and response, respectively. The
generative dialogue model aims to learn a condi-
tional probability distribution p(R|Q). The maxi-
mum likelihood estimation (MLE) is usually used
to train the model, which can also be expressed as
minimizing the negative log-likelihood:
L=−/summationdisplaylogp(r|r,Q). (1)
Considering one characteristic of the dialogue task,
i.e., allowing the response to be varied, the many-
to-one phenomenon occurs in the dialogue corpora
frequently. However, with the MLE-based training,
this phenomenon will cause the model to produce
generic responses.
Unlikelihood Training Unlikelihood (UL) loss
(Welleck et al., 2020) is proposed for the model to
address the problem of undesirable behaviors (e.g.,408repetitive or high-frequency tokens). It forces the
model to minimize the probability of generating
negative candidates, which is formulated as:
L=−/summationdisplay/summationdisplay
log(1−p(r|r,Q)), (2)
whereCconsists of negative candidates (e.g.,
overuse frequent words) that are also a sub-set of
the vocabulary.
Knowledge Distillation The traditional knowl-
edge distillation (KD) usually transfers useful
knowledge from a large and strong teacher net-
workTto a small student network S. The distilla-
tion loss is used to align the soften predictions of
the teacher and the student, denoted as f(x)and
f(x):
L=/summationdisplayL/parenleftbig
f(x),f(x)/parenrightbig
, (3)
whereL(·)is a measurement function that calcu-
lates the distance of different probability distribu-
tions,xis the input text, and Ddenotes the training
set.
In this work, we replace the positive teacher in
vanilla KD with a negative teacher, aiming to pro-
vide negative knowledge for the student to conduct
negative training and avoid undesirable behaviors.
2.2 Negative Teacher
To improve the diversity of responses, the dialogue
model should be told which responses are generic.
For negative distillation, a negative teacher is re-
quired to produce possible generic responses given
any query. In this work, we adopt the widely used
Transformer (Vaswani et al., 2017) as the underly-
ing model for both teacher and student. We intro-
duce the Source Entropy ﬁltering method (Csáky
et al., 2019) to identify and collect the many-to-
one cases for the negative training set. The source
entropy is deﬁned as:
H(r,D) =−/summationdisplayp(q|r) logp(q|r),(4)
wherep(q|r)is the conditional probability calcu-
lated based on the relative frequency of ( query,
response ) pairs,ris a response, qis the query
corresponding to the response r, andDrepresentsthe raw training set. A higher source entropy in-
dicates that the response rcorresponds to more
queries, i.e., the many-to-one problem is serious.
We select the top 50%dialogue pairs (q,r)with
a high source entropy as the negative training set
D, which contains a much higher proportion of
generic responses than the raw training set.
After that, we train the teacher Non the negative
training setDby Equation 1. The teacher will
naturally produce generic responses for any input
query. More importantly, it will provide richer
negative knowledge for the student, including soft
logits in the prediction layer and implicit features
in the intermediate layers.
2.3 Negative Distillation
In this section, we conduct the negative distillation
for the student based on the multi-level negative
knowledge.
ND for Prediction Layer The soften logits in
the prediction layer contain more information than
the ground-truth labels, such as the similarity be-
tween labels (Wang et al., 2021). Therefore, con-
ventional KD transfers knowledge by narrowing
the gap between the probability distributions of the
teacherTand the student S:
L=−/summationdisplay/summationdisplayp(r=k|r,Q)
·logp(r=k|r,Q). (5)
As for negative distillation, the extra knowledge in
soften logits of the negative teacher reﬂects how to
generate dull responses based on the input query.
Therefore, we propose a soft unlikelihood loss to
maximize the distance between the predictions of
the negative teacher Nand the student S:
L=−/summationdisplay/summationdisplayp(r=k|r,Q)
·log (1−p(r=k|r,Q)),(6)
wherepandpare calculated by:
p=exp (z/t)/summationtextexp (z/t), (7)
wheretis a temperature coefﬁcient that is used to
soften the probability distribution over words.409It should be emphasized that previous nega-
tive training methods only use the high-frequency
words or phrases with one-hot representation as
the targets, which ignores the rich information ex-
isting in the soften logits (e.g., the generic words
have similar probabilities). In the Analysis section,
we demonstrates the superiority of soften logits
compared with hard targets (i.e., one-hot represen-
tation).
ND for Intermediate Layer In addition to the
output knowledge from the prediction layer, there
is also some implicit knowledge embedded in the
intermediate layers, such as hidden states and at-
tention matrices. To keep the student away from
undesirable behaviors (i.e., producing generic re-
sponses) more effectively, we further consider the
above knowledge into negative distillation. Speciﬁ-
cally, the distance between features of the negative
teacher and the student should also be increased.
In this work, we propose a new measurement func-
tion, called mean reverse square error (MRSE), to
calculate this distance:
L (A,B) =1
n/summationdisplayexp,(8)
whereAandBare the feature matrices of the
negative teacher and the student, respectively, and
nis the number of elements of each matrix.
Due to the responses generating in the decoding
phrase, we only conduct negative distillation on
the intermediate layers of the decoder. For each
decoder layer, the negative distillation objective of
hidden states is deﬁned as:
L=L (H,H), (9)
whereHandHare the output hidden states of
theldecode layer of NandS, respectively.
As the attention weights can learn substantial
linguistic knowledge (Clark et al., 2019), it is ben-
eﬁcial for the student to further conduct negative
distillation on the attention matrices, which is com-
puted as follows:
A=QK
√d, (10)
Attention (Q,K,V) = softmax( A)V,(11)
whereQ,K, andVare the matrices of queries,
keys, and values, respectively, and dis a scalingfactor. Following Jiao et al. (2020), the attention
matrixAis chosen to calculate the distance rather
than its softmax version softmax( A). Similar to
Equation 9, the negative distillation objective of
attention matrices is formulated as:
L=L (A,A)), (12)
whereAandAare the attention matrices of the
ldecoder layer of NandS, respectively.
Algorithm 1 Negative Distillation
Input:D: The raw training set; H: The Source
Entropy ﬁltering method; NandS: The nega-
tive teacher and the student.% Collection of negative training set.[Data_entropy]←Calculate_data_entropy( D,
H) using Eq.4Index_list←Sort([Data_entropy])D←Extract_top_data(D, Index_list, 50%)% Training of negative teacher.repeat OptimizeNby minimizingL(N)on
Dusing Eq. 1until Convergence% Negative distillation.repeat OptimizeSby minimizingL(S)onDusing
Eq. 13until Convergence
Output:S: The trained student.
2.4 Progressive Optimization
The overall loss, combining the above negative
distillation objectives and the MLE objective, is
denoted as:
L= (1−α)L+
α(L+/summationdisplay
L+/summationdisplay
L), (13)
whereαis a hyper-parameter that balances the im-
portance of supervised learning and negative distil-
lation. For negative distillation, it would be better
that the student has the ability to say something
before it is reminded of what not to say. Thus, we
perform a progressive distillation that ﬁrst warms
up the negative distillation ratio and then colds it
down gradually. Inspired by the derivative of sig-
moid function:
σ(z) =σ(z)(1−σ(z)) =e
(e+ 1),(14)410
which shows a trend of gradual rise-fall, we deﬁne
the balance coefﬁcient αas:
α=λ∗e
(e+ 1), (15)
whereλcontrols the peak value and zis calculated
by:
z(s) =β∗(s−γ), (16)
wheresis the training step, and βandγcontrol the
telescopic and translation transformation, respec-
tively.
3 Experiments
3.1 Datasets
In our experiments, two widely used dialogue
datasets are employed to evaluate the proposed
method: DailyDialog , which collects conversa-
tions that are similar to human daily communica-
tion (Li et al., 2017b), and OpenSubtitles , which
consists of large-scale dialogues extracted from
movie subtitles (Tiedemann, 2009). In this work,
we focus on the single-turn dialogue generation,
thus we pre-process these two datasets into the
(query, response) pairs. Table 2 provides the statis-
tics of both datasets.
3.2 Experimental Settings
We take the Transformer-based sequence-to-
sequence model (Vaswani et al., 2017) as the un-
derlying model for all approaches. Following the
settings of Transformer in Csáky et al. (2019), both
encoder and decoder contain 6 layers, in which the
self-attention module has 8 attention heads and the
number of feed-forward units is 2048. The size
of hidden states is set to 512 and the dimension
is 64 for query, key, and value. Please refer to
Appendix A for more details.
For the proposed approach, both the negative
teacher network and the student network have the
same settings in terms of the network architecture
and hyper-parameters. λin Equation 15 is set to
4, making the peak value equal to 1. γis 25600
andβis6/γ. For the temperature coefﬁcient t, we
simply set it to 1.3.3 Baselines
We compare the proposed negative distillation
(ND) approach with the standard Transformer, two
existing negative training approaches and two extra
diversity improving approaches:
•Standard The vanilla Transformer-based
sequence-to-sequence model with the MLE-
based training (i.e., the cross-entropy based
loss).
•NT(Negative Training) (He and Glass, 2020)
During training, it ﬁrst counts the frequency of
all generated utterances and then conducts the
negative update based on the high-frequency
utterances.
•UL(Unlikelihood Training) (Li et al., 2020a)
Different from NT, it calculates the frequency
of all generated words instead of utterances
and penalizes the high-frequency words by
introducing an unlikelihood loss term.
•CV AE (Zhao et al., 2017) A dialogue re-
sponse generation model using conditional
V AE to improve the diversity of generated re-
sponses.
•FACE (Jiang et al., 2019) It uses the
frequency-aware cross-entropy loss to tackle
the low-diversity problem.
All the baselines are performed with the same
architecture and hyper-parameters as ours. Fol-
lowing He and Glass (2020); Li et al. (2020a), we
use greedy search as the decoding strategy for all
baselines and our method. We also evaluate the
performance with beam search (size 5) and obtain
similar results (see 3.6 for details). Details for base-
lines is describes in Appendix B.
3.4 Automatic Evaluation
Metrics To evaluate whether negative distillation
can effectively reduce the generic responses, we
adopt Dist-{1,2,3} (distinct) (Li et al., 2016) to re-
ﬂect the lexical diversity of the generated responses.
It is a widely used metric that counts the proportion
of unique unigrams/bigrams/trigrams. LF(low-
frequency token ratio) (Li et al., 2020b) further
measures the diversity of responses by calculating
the ratio of low-frequency words in the generated
responses. The threshold of low frequency is set
to 100. Besides, it is necessary to verify whether411
the models can ensure consistency while improv-
ing diversity. So we use KL-{1,2} (KL divergence)
(Csáky et al., 2019), which measures the distribu-
tion distance between the generated and the ground-
truth responses, to reﬂect how well a model can ap-
proximate the ground-truth unigrams/bigrams dis-
tributions. BLEU (Chen and Cherry, 2014) is also
reported and it measures n-gram overlap between
the generated and the ground-truth references.
Results Table 3 shows the results obtained at the
lowest point of the validation loss. We can see
that our approach outperforms all baselines in di-
versity ( Dist andLF) by a signiﬁcant margin on
both datasets, demonstrating that NDcan effec-
tively alleviate the generic response problem by
using multi-level negative information. The KL
andBLEU scores of NDare close to or better
thanStandard , which veriﬁes that our method can
maintain the consistency of responses while im-
proving its diversity. To some extent, both NT
andULimprove the diversity of words, especially
for trigrams, but the low LFscores indicate that
they reduce the high-frequency words but fail to
increase the number of low-frequency’s. What’s
worse, BLEU andKL-2 scores of above two and
CV AE sharply decline. It suggests that previous
negative training approaches and other methods for
diversity enhancement may harm the consistencyand ﬂuency of responses dramatically, which is not
in line with the goals of the dialogue system. Our
method obtains similar results with beam search.
Please refer to 3.6 for details.
3.5 Human Evaluation
Apart from automatic evaluations, we conduct hu-
man evaluations to further verify the effectiveness
of our method than previous negative training meth-
ods. We randomly select 50 samples from the test
set of DailyDialog, and three well-educated anno-
tators are invited to judge which of the responses
generated by NDand baselines is better (i.e., win,
tie or loss) in terms of informativeness, relevance,
and ﬂuency. Informativeness reﬂects how much the
information related to the query is contained in the
generated response. Relevance reﬂects how likely
the generated response is coherent to its query. Flu-
ency reﬂects how likely the generated response is
produced by human.
Table 4 summarizes the human evaluation results.
We can see that the proposed approach is overall
better than all baselines. Speciﬁcally, NDachieves
better performance than Standard in terms of infor-
mativeness and relevance, and remains competitive
in ﬂuency. Compared with both NTandUL, our
approach shows signiﬁcant advantages, especially
in ﬂuency. It indicates that their punishment for
high-frequency tokens or utterances will lead to412a serious non-ﬂuency and inconsistency problem.
We use Fleiss’s kappa (Fleiss, 1971) to measure the
inter-annotator agreement.
3.6 Experimental Analysis
We conduct extensive analysis on DailyDialog to
investigate the effectiveness of the negative distilla-
tion in more details.
Ablation study We study the effects of different
negative distillation objectives by ablating the pre-
diction layer distillation (w/o L), the attention
distillation (w/oL), the hidden state distillation
(w/oL), and the whole negative distillation (w/o
L, i.e. Standard). The results in Table 5 show
that all three proposed negative distillation objec-
tives are useful for improving the diversity. The
signiﬁcant decline in w/o Lindicates that the
negative information in intermediate layers is very
important for ND. w/oLis better than w/oL,
attributing to the more abundant information in
hidden states.
Does source entropy work? To verify whether
thesource entropy ﬁltering method can collect the
generic responses, we select the top 50% and the
bottom 50% of the sorted training set as Dand
D, respectively. Then we train NandNon the
corresponding sub-sets. From Table 6, we can see
thatNoutperforms Nin all the diversity-related
metrics, indicating the effectiveness of source en-
tropy .
Can the negative knowledge be transferred?
We takeNandNas the negative teachers for
the students SandS, respectively. Then we con-
duct negative distillation on both SandS. Theresults in Table 7 demonstrate that Sobtains more
gains in diversity than S, indicatingSgets rid of
more negative knowledge. It can be further veriﬁed
by the results of previous analysis that Nhas more
negative knowledge than N.
Study of soft target To evaluate the superiority
of soft targets for negative distillation, we sample
responses (i.e., hard target) by greedy search on
the predictions of negative teachers for compari-
son. The results in Table 9 show that ND with
soft targets can diversify the responses more ef-
fectively, demonstrating the advantages of richer
negative information (e.g., the similarity between
labels) in soft targets. What’s more, we randomly
select responses from the negative training set D
as negative targets. The sharp decline in perfor-
mance proves that the negative teacher can produce
targeted generic responses.
Effect of progressive distillation In order to ver-
ify the effectiveness of progressive negative distil-
lation, we conduct negative distillation with ﬁxed
α. The value is obtained by calculating the average
ofαin Equation 15 across the convergence steps.
The results in Table 8 demonstrate that the progres-
sive distillation policy can help the student exploit
negative knowledge more effectively. Besides, note
that ND with ﬁxed αalso outperforms the Standard
model.413
Evaluation results with beam search He and
Glass (2020) and Li et al. (2020a) choose greedy
decoding due to its simplicity and higher diversity
than beam decoding. However, we ﬁnd that both
NTandULtend to generate long but non-ﬂuent
and incoherent responses. So we conduct beam
search with adding the length penalty. Table 10
summarizes the results and it shows that both two
baselines get better KLandBLEU scores than
using greedy search due to shorter responses. ND
outperform baselines on all the metrics, conﬁrming
the effectiveness of our method.
Case study Table 11 shows some cases gener-
ated by the proposed method and baselines. Stan-
dard prefers generic and meaningless responses.
Both NTandULtend to generate a short generic
sentence followed by a incoherent and non-ﬂuent
subsequence. In contrast, NDcan produce diverse
and coherent responses.
4 Related work
Diversity Dialogue Learning There are two
lines of work for solving the generic response prob-lem: One line promotes the diversity from posi-
tive view, which is outside of our work. Specially,
previous work includes MMI (Li et al., 2016),
GAN (Li et al., 2017a; Zhang et al., 2018), CV AE
(Zhao et al., 2017), BT (Su et al., 2020), FACE
(Jiang et al., 2019), AdaLabel (Wang et al., 2021),
IAT (Zhou et al., 2021), and Nucleus Sampling
(Holtzman et al., 2020). The other line allevi-
ates the generic response problem using negative
training. He and Glass (2020) regards frequent re-
sponse problem as a sub-problem of the generic
response problem and conduct negative update for
the high-frequency responses during training. Li
et al. (2020a) focuses on high-frequency tokens
rather than tokens and punishes them by using the
unlikelihood objective (Welleck et al., 2020). Both
of them handle the generic response problem only
from the angle of reducing frequency, thus can not
capture all the features of generic replies.
Negative Training for Dialogue Learning Neg-
ative training for retrieval-based dialogue learning
has been previously extensively studied (Humeau
et al., 2020; Nugmanova et al., 2019), while we
focus on the dialogue generation in this work. He
and Glass (2020) uses negative training to prevent
generic and malicious responses in dialogue mod-
els. Li et al. (2020a) generalizes unlikelihood to
dialogue generation for improving repetition, speci-
ﬁcity and coherence. Lagutin et al. (2021) proposes
implicit unlikelihood training to minimize repeti-
tion. Our work proposes a new negative training
paradigm aimed at improving the diversity of di-
alogue responses while avoiding the problem of
poor consistency and ﬂuency of previous work.
5 Conclusion
We present a novel negative training paradigm to
improve the diversity of dialogue responses. It
formulates the conventional negative training as
a knowledge distillation process, which is rarely
explored before. The negative teacher can produce414the corresponding generic and dull responses given
any query, which naturally avoids problems that
hinder previous negative training methods. Besides,
we further boost the performance of negative distil-
lation by exploiting richer information, i.e., multi-
level features. Extensive experiments validate the
superiority of our proposed method compared with
prior negative training work.
A limitation of our work is that we only focus
on the generic response problem. For future work,
we will extend the proposed negative distillation to
handle other generation problems, such as incon-
sistency and lacking personas or emotions.
Acknowledgements
We would like to thank the anonymous review-
ers for their constructive comments. This work is
supported by Beijing Natural Science Foundation
(No.4222037, L181010) and National Natural Sci-
ence Foundation of China (No.61972035). Kan Li
is the corresponding author.
References415416
A Details for Implementations
Here are some implementation details of our exper-
iments. Dropout (Srivastava et al., 2014) is used for
the self-attention module, the feed-forward layer,
and the activation layer, and the rate of all three is
set to 0.1. We also use label smoothing (Szegedy
et al., 2016) and the smoothing value is 0.1. The
batch size is set to 256. We use the Adam optimizer
(Kingma and Ba, 2015) and employ the warm-up
(He et al., 2016) trick to adjust the learning rate dur-
ing training. The warm-up steps sare 128000
and 256000 for DailyDialog and OpenSubtitles,
respectively. The learning rate is computed as fol-
lows:
lr=2·min(,√)
√d, (17)
wherelris the learning rate at the sstep of train-
ing andd is the size of hidden states. We
implement all approaches with Pytorch 1.7, and
conduct all experiments on RTX 3090.
B Baselines
ForNT, the threshold ris set to 1% and the
weight coefﬁcient λis set to 1 as the authors’suggestion. For UL, we search the mixing hyper-
parameterαin[1,10,100,1000] and 1000 is se-
lected for its best performance. Both NTandUL
are reﬁned on the well-trained Standard model.
ForCA VE , we set the latent size with patience
to 256 and 64 for DailyDialog and OpenSubtitles,
respectively. And for FACE , we use the "output
frequency" and "pre-weight" version as the author
suggested.
We also compare the proposed method (ND)
with AdaLabel (Wang et al., 2021), although AdaL-
abel alleviates the generic response problem from
the perspective of target regularization instead of
negative training. The results in Table 12 conﬁrms
the superior performance of our method for improv-
ing the diversity of generated responses.417418