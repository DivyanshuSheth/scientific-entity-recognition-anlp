
Omer Goldman, David Guriel, Reut Tsarfaty
Bar-Ilan University
{omer.goldman,davidgu1312}@gmail.com,reut.tsarfaty@biu.ac.il
Abstract
In the domain of Morphology, Inﬂection is a
fundamental and important task that gained a
lot of traction in recent years, mostly via SIG-
MORPHON’s shared-tasks. With average ac-
curacy above 0.9 over the scores of all lan-
guages, the task is considered mostly solved
using relatively generic neural seq2seq models,
even with little data provided. In this work,
we propose to re-evaluate morphological in-
ﬂection models by employing harder train-test
splits that will challenge the generalization ca-
pacity of the models. In particular, as op-
posed to the naïve split-by-form, we propose
a split-by-lemma method to challenge the per-
formance on existing benchmarks. Our exper-
iments with the three top-ranked systems on
the SIGMORPHON’s 2020 shared-task show
that the lemma-split presents an average drop
of 30 percentage points in macro-average for
the 90 languages included. The effect is most
signiﬁcant for low-resourced languages with
a drop as high as 95 points, but even high-
resourced languages lose about 10 points on
average. Our results clearly show that general-
izing inﬂection to unseen lemmas is far from
being solved, presenting a simple yet effective
means to promote more sophisticated models.
1 Introduction
In recent years, morphological (re)inﬂection tasks
in NLP have gained a lot of attention, most notably
with the introduction of SIGMORPHON’s shared
tasks (Cotterell et al., 2016, 2017, 2018; Vylomova
et al., 2020) in tandem with the expansion of Uni-
Morph (McCarthy et al., 2020), a multi-lingual
dataset of inﬂection tables. The shared-tasks sam-
ple data from UniMorph includes lists of triplets
in the form of (lemma, features, form) for many
languages, and the shared-task organizers maintain
standard splits for a fair system comparison.
The best-performing systems to-date in all inﬂec-
tion shared-tasks are neural sequence-to-sequence
models used in many NLP tasks. An LSTM-basedmodel won 2016’s task (Kann and Schütze, 2016),
and a transformer came on top in 2020 (Canby
et al., 2020). In 2020’s task the best model achieved
exact-match accuracy that transcended 0.9 macro-
averaged over up to 90 languages from various lan-
guage families and types. This trend of high results
recurred in works done on data collected indepen-
dently as well (e.g. Malouf, 2017, Silfverberg and
Hulden, 2018, inter alia).
Interestingly, the averaged results of 2020’s
shared-task include languages for which very lit-
tle data was provided, sometimes as little as a
couple of hundreds of examples. This has led
to a view considering morphological inﬂection a
relatively simple task that is essentially already
solved , as reﬂected in the saturation of the results
over the year and the declining submissions to the
shared tasks.This also led the community to grav-
itate towards works attempting to solve the same
(re)inﬂection tasks with little or no supervision
(McCarthy et al., 2019; Jin et al., 2020; Goldman
and Tsarfaty, 2021).
However, before moving on we should ask our-
selves whether morphological inﬂection is indeed
solved or may the good performance be attributed
to some artifacts in the data. This was shown to be
true for many NLP tasks in which slight modiﬁca-
tions of the data can result in a more challenging
dataset, e.g., the addition of unanswerable ques-
tions to question answering benchmarks (Rajpurkar
et al., 2018), or the addition of expert-annotated
minimal pairs to a variety of tasks (Gardner et al.,
2020). A common modiﬁcation is re-splitting the
data such that the test set is more challenging and
closer to the intended use of the models in the
wild (Søgaard et al., 2021). As the performance
on morphological inﬂection models seems to have
saturated on high scores, a similar rethinking of the
data used is warranted.864In this work we propose to construct more dif-
ﬁcult datasets for morphological (re)inﬂection by
splitting them such that the test set will include no
forms of lemmas appearing in the train set. This
splitting method will allow assessing the models
in a challenging scenario closer to their desired
function in practice, where training data usually
includes full inﬂection tables and learning to inﬂect
the uncovered lemmas is the target.
We show, by re-splitting the data from task 0 of
SIGMORPHON’s 2020 shared-task, that the pro-
posed split reveals a greater difﬁculty of morpho-
logical inﬂection. Retesting 3 of the 4 top-ranked
systems of the shared-task on the new splits leads to
a decrease of 30 points averaged over the systems
for all 90 languages included in the shared-task.
We further show that the effect is more prominent
for low-resourced languages, where the drop can
be as large as 95 points, though high-resourced
languages may suffer from up to a 10 points drop
as well. We conclude that in order to properly as-
sess the performance of (re)inﬂection models and
to drive the ﬁeld forward, the data and related splits
should be carefully examined and improved to pro-
vide a more challenging evaluation, more reﬂective
of their real-world use.
2 (Re)inﬂection and Memorization
Inﬂection and reinﬂection are two of the most dom-
inant tasks in computational morphology. In the
inﬂection task, the input is a lemma and a feature-
bundle, and we aim to predict the respective in-
ﬂected word-form. In reinﬂection , the input is an
inﬂected word-form along with its features bun-
dle, plus a feature-bundle without a form, and
we aim to predict the respective inﬂected-form
for the same lemma. The training input in SIG-
MORPHON’s shared-tasks is a random split of the
available (lemma,form,features) triplets such that
no triplet occurring in the train-set occurs in the
test-set.
In such a setting, models can short-cut their way
to better predictions in cases where forms from
the same lemma appear in both the train and test
data. This may allow models to memorize lemma-
speciﬁc alternations that make morphological in-
ﬂection a challenging task to begin with. Consider
for example the notoriously unpredictable German
plurality marking, where several allomorphs areassociated with nouns with no clear rule govern-
ing the process. Kind , for example, is pluralized
with the sufﬁx -erresulting in Kinder tagged as ;. Assuming a model saw this example in
the train set it is pretty easy to predict Kindern for
the same lemma with;features,but without
knowledge of the sufﬁx used to pluralize Kind the
predictions Kinden andKinds are just as likely.
3 Related Work
Many subﬁelds of NLP and machine learning in
general suggested hard splits as means to improve
the probing of models’ ability to solve the under-
lying task, and to make sure models do not simply
employ loopholes in the data.
In the realm of sentence simpliﬁcation, Narayan
et al. (2017) suggested the WS dataset,
where models are required to split and rephrase
complex sentences associated with a meaning rep-
resentation over a knowledge-base. Aharoni and
Goldberg (2018) found that some facts appeared
in both train and test sets and provided a harder
split denying models the ability to use memorized
facts. Aharoni and Goldberg (2020) also suggested
a general splitting method for machine translation
such that the domains are as disjoint as possible.
In semantic parsing, Finegan-Dollak et al. (2018)
suggested a better split for parsing natural lan-
guage questions to SQL queries by making sure
that queries of the same template do not occur in
both train and test, while Lachmy et al. (2021) split
their H data such that any one visual pat-
tern used for the task cannot appear in both train
and test. Furthermore, Loula et al. (2018) adver-
sarially split semantic parsing for navigation data
to assess their models’ capability to use composi-
tionality. In spoken language understanding Arora
et al. (2021) designed a splitting method that will
account for variation in both speaker identity and
linguistic content.
In general, concerns regarding data splits and
their undesired inﬂuence on model assessments led
Gorman and Bedrick (2019) to advocate random
splitting instead of standard ones. In reaction, Sø-
gaard et al. (2021) pointed to the ﬂaws of random
splits and suggested adversarial splits to challenge
models further. Here we call for paying attention
to the splits employed in evaluating morphological
models, and improve on them.865
4 Experiments
In order to better assess the difﬁculty of morpho-
logical inﬂection, we compare the performances of
3 of the top-ranked system at task 0 (inﬂection) of
SIGMORPHON’s 2020 shared-task. We examined
each system on both the the standard (form) split
and the novel (lemma) split.
When re-splitting,we kept the same proportions
of the form-split data, i.e. we split the inﬂection ta-
bles 70%, 10% and 20% for the train, development
and test set. In terms of examples the proportions
may vary as not all tables are of equal size. In prac-
tice, the averaged train set size in examples terms
was only 3.5% smaller in the lemma-split data, on
average.4.1 The Languages
SIGMORHPON’s 2020 shared-task includes
datasets for 90 typologically and genealogically
diverse languages from 14 language families. The
languages are varied along almost any typological
dimension, from fusional to agglutinative, small
inﬂection tables to vast ones. They include mostly
preﬁxing and mostly sufﬁxing languages with rep-
resentation of inﬁxing and circumﬁxing as well.
The languages vary also in use, including widely-
used languages such as English and Hindi and mori-
bund or extinct languages like Dakota and Middle
High German.
4.2 The Models
We tested the effects of lemma-splitting on our own
LSTM-based model as well as 3 of the 4 top-ranked
systems in the shared task.
Base LSTM We implemented a character-based
sequence-to-sequence model which consists of a
1-layer bi-directional LSTM Encoder and a 1-layer
unidirectional LSTM Decoder with a global soft
attention layer (Bahdanau et al., 2014). Our model
was trained for 50 epochs with no model selection.
Base trm-single The shared-task’s organizers
supplied various baselines, some based on a trans-
former architecture that was adapted for character-
level tasks (Wu et al., 2021).All baseline models
include 4 encoder and 4 decoder layers, consist-
ing of a multi-head self-attention layer and 2 feed-
forward layers, equipped with a skip-connection.
In every decoder layer a multi-head attention layer
attends to the encoder’s outputs. The network was
trained for 4,000 warm-up steps and up to 20,000
more steps, each step over a batch of size 400. The
model was examined with and without augmented
data, trained separately on each language or each
language family. One of the baseline setups, train-
ing a model per language without augmented data,
made it to the top 4 systems and we include it here.866DeepSpin Peters and Martins (2020) submitted a
recurrent neural network – dubbed DeepSpin-02.
The system is composed of 2 bi-directional LSTM
encoders with bi-linear gated Attention (Luong
et al., 2015), one for the lemma characters and
one for the features characters, and a unidirectional
LSTM Decoder for generating the outputs. The in-
novation in the architecture is the use of sparsemax
(Martins and Astudillo, 2016) instead of softmax
in the attention layer.
CULing Liu and Hulden (2020)’s system is also
based on the transformer architecture, with hyper-
parameters very similar to base trm-single .Their
innovation is in restructuring the data such that the
model learns to inﬂect from any given cell in the
inﬂection table rather than solely from the lemma.
4.3 Results
Table 1 summarizes our main results. We clearly
see a drop in the performance for all systems, with
an average of 30 points. The table also shows that
splitting the data according to lemmas allows dis-
cerning between systems that appear to perform
quite similarly on the form-split data. The best
system on the lemma-split data, DeepSpin-02, out-
performs the second-ranked CULing system by
about 13 points with both baseline systems per-
forming signiﬁcantly worse. The results in terms
of averaged edit distance show the same trends.
DeepSpin-02 emerges victorious also in Table
2, where results are broken down by language fam-
ily. The table shows that DeepSpin-02 is the best
performer over all language families when data is
lemma-split, in contrast to the mixed picture over
the form-split data.
The average performance per language family
seems to be controlled by training data availability.
For example, Germanic languages show average
drop of 23 points, while for Niger-Congo languages
the drop is 39 points on average.
In order to further examine the relation between
the amount of training data and drop in perfor-
mance we plotted in Figure 1 the drop per sys-
tem and per language against the size of the avail-
able train data, color-coded to indicate systems. It
shows that the major drops in performance that
contributed the most to the overall gap between
the splits are in those low-resourced language. Re-
markably, for some systems and languages the drop
can be as high as 95 points. On the other hand, on
high-resourced languages with 40,000 training ex-
amples or more, all systems didn’t lose much. The
analysis also shows the advantage of DeepSpin-02
in the lower-resourced settings that made it the best
performer overall.
When color-coding the same broken-down data
for linguistic family membership rather than sys-
tem, as we do in Figure 2, it becomes clear that
there is no evidence for speciﬁc families being eas-867ier for inﬂection when little data is provided. The
ﬁgure does show the remarkable discrepancy in
annotation effort, as the high-resourced languages
mostly belong to 2 families: Germanic and Uralic.
5 Discussion
We proposed a method for splitting morphologi-
cal datasets such that there is no lemma overlap
between the splits. On the re-split of SIGMOR-
PHON’s 2020 shared-task data, we showed that all
top-ranked systems suffer signiﬁcant drops in per-
formance. The new split examines models’ general-
ization abilities in conditions more similar to their
desired usage in the wild and allows better discern-
ing between the systems in order to point to more
promising directions for future research — more
so than the original form-split data on which all
systems fared similarly. The new splitting method
is likely to lead to more sophisticated modeling,
for instance, in the spirit of the model proposed by
Liu and Hulden (2021). The suggested move to a
harder split is not unlike many other NLP tasks, in
which challenging splits are suggested to drive the
ﬁeld forward. We thus call for morphological stud-
ies to carefully attend to the data used and expose
the actual difﬁculties in modelling morphology, in
future research and future shared tasks.
Acknowledgements
This research was funded by the European Re-
search Council under the European Union’s Hori-
zon 2020 research and innovation programme,
(grant agreement No. 677352) and by a research
grant from the ministry of Science and Technology
(MOST) of the Israeli Government, for which we
are grateful.
References868869870