
Zhenwen Liang,Jipeng Zhang, and Xiangliang ZhangUniversity of Notre Dame, {zliang6, xzhang33}@nd.eduHong Kong University of Science and Technology, jzhanggr@conect.ust.hk
Abstract
Math word problem (MWP) solving is an im-
portant task in question answering which re-
quires human-like reasoning ability. Analogi-
cal reasoning has long been used in mathemat-
ical education, as it enables students to apply
common relational structures of mathematical
situations to solve new problems. In this paper,
we propose to build a novel MWP solver by
leveraging analogical MWPs, which advance
the solver’s generalization ability across dif-
ferent kinds of MWPs. The key idea, named
analogy identification , is to associate the ana-
logical MWP pairs in a latent space, i.e., encod-
ing an MWP close to another analogical MWP,
while moving away from the non-analogical
ones. Moreover, a solution discriminator is
integrated into the MWP solver to enhance
the association between the representations of
MWPs and their true solutions. The evalua-
tion results verify that our proposed analogical
learning strategy promotes the performance of
MWP-BERT on Math23k over the state-of-the-
art model Generate2Rank , with 5 times fewer
parameters in the encoder. We also find that
our model has a stronger generalization ability
in solving difficult MWPs due to the analogical
learning from easy MWPs.
1 Introduction
Math word problem (MWP) solving has attracted
considerable attention in recent years. Currently,
MWP solver design focuses on generating an equa-
tion towards an unknown quantity, with an input
problem description, as shown in Figure 1. Build-
ing such a successful solver is quite challenging, as
it requires mathematical understanding and multi-
step reasoning abilities to transform the implied
logic behind the problem into a mathematical equa-
tion composed of operators and quantities. With
the emergence of deep learning, MWP has been
effectively studied by Seq2Seq models (Xie and
Sun, 2019; Zhang et al., 2020b) and pre-trained
language models (PLMs) (Tan et al., 2021; Li et al.,Figure 1: This figure shows three examples of math
word problems. We can find mathematical analogy
across different topics (P1 and P2) and different dif-
ficulty levels (P2 and P3).
2021; Huang et al., 2021; Liang et al., 2022), which
treat it as a translation task from the natural lan-
guage description of a problem to its solution in a
mathematical equation.
In educational science, analogical reasoning
(Novick and Holyoak, 1991; Bernardo, 2001) has
long been considered as a crucial skill in human
problem-solving. In education and training of math-
ematical reasoning, researchers (Stacey et al., 1982)
interpret it as “a dynamics process which, by en-
abling us to increase the complexity of ideas we
can handle, expands our understanding.” For ex-
ample, the MWP P1 and P2 in Figure 1 are in
different situations, but both aim to calculate the
available quantity of a target object based on known
resources, leading to the same solution structure.
The MWP P2 and P3 both focus on the knowledge
of ‘number of units = money / unit price’, while
the MWP P3 has more reasoning steps. Gaining
the skill of generating the solution of P2 to address
P1 and P3 is important for students, as well as for
automated MWP solvers. A widely accepted defi-9454nition of analogical reasoning is mapping the pro-
cess of mathematization between different domains
(V osniadou, 1988). We thus propose a strategy of
associating the mathematization process of analog-
ical MWP pairs, e.g., P1 and P2, P2 and P3. The
key idea is to encode the analogical problems close
in the latent representation space, while encoding
the non-analogical problems distantly. The intro-
duction of analogical learning in MWP solving can
then alleviate the limitations of existing methods
such as simple-to-difficult generalization (Zhang
et al., 2020b; Jie et al., 2022) and topic-to-topic
generalization (Hong et al., 2021).
While the analogical MWPs have close represen-
tations, they should be also more associated with
their corresponding true solutions than wrong so-
lutions. We thus encode the solution as well, and
design a module named solution discrimination to
enhance the association of an MWP and its correct
solution. The enhancement is powered by generat-
ing hard negative samples that are wrong solutions
but similar to the correct solution. Negative sam-
pling has been proved effective in training strong
MWP solvers (Li et al., 2021; Liang and Zhang,
2021). For example, Li et al. (2021) used negative
examples to calculate a contrastive loss and find
patterns across MWPs, but the negative examples
are retrieved from the given training set, which
could be bounded by the dataset size and quality.
Liang and Zhang (2021) bridged the problem space
and solution space, by building a teacher module to
help the solver match problems with their ground
truth solutions and distinguish them from randomly
generated negative solutions. However, the nega-
tive samples in their approach are randomly manip-
ulated variants of the ground truth equation, and
are thus limited in their shapes and fail to consider
the hardness of negative samples. Therefore, we
design a gradient-based manipulation method to
find hard negative samples (i.e., wrong solutions
that are difficult to be distinguished from a cor-
rect solution). In the meantime, to make the form
of negative samples less bounded by the ground
truth, we introduce more randomness during the
generation of negative samples.
Overall, our proposed MWP solver is novel on
considering the analogy among MWPs. It contains
ananalogy identification module which aims to
transfer mathematization knowledge and skills to
generalize the solution to analogical MWPs. More-
over, a solution discrimination module is incorpo-rated to enable the solver to bridge problems and
their ground truth solutions. Our proposed solver
is a flexible framework and can have any MWP
encoder-decoder plugged in. The experimental re-
sults show that training representative baselines
(e.g., GTS (Xie and Sun, 2019) and MWP-BERT
(Liang et al., 2022)) by our analogical strategy can
further improve their accuracy. Comparing to the
current best solver on Math23k, Generate&Rank
(Shen et al., 2021), we make MWP-BERT achieve
higher accuracy but with 5 times fewer encoder
parameters. Ablation studies, qualitative analyses
and case studies also demonstrate the effectiveness
of our designed solver framework, and verify its
generalization ability in solving difficult MWPs
due to the analogical learning from easy MWPs.
2 Related Works
2.1 Math Word Problem Solving
The topic of automated math word problem solv-
ing was raised in 1960s (Bobrow, 1964). At the
beginning, researchers established rules and tried
to match every problem with a certain solving rule
(Bakman, 2007). Then, statistical and machine
learning methods (Shi et al., 2015) were widely ap-
plied and sometimes integrated with semantic pars-
ing approaches (Koncel-Kedziorski et al., 2015).
Inspired by the great success of deep learning
and Seq2Seq models, a Seq2Seq solver with an
encoder-decoder structure was proposed and out-
performed transitional methods, after which many
other Seq2Seq solvers (Wang et al., 2018, 2019;
Liu et al., 2019) were proposed. Later, GTS (Xie
and Sun, 2019) replaced the sequential decoder
with a novel tree-based decoder and achieved great
performance. Most following papers (Zhang et al.,
2020b; Wu et al., 2020; Lin et al., 2021; Wu et al.,
2021; Liang and Zhang, 2021) after GTS focused
on the improving of encoder part without touching
the tree-based decoder. Recently, due to the de-
velopment of pre-trained language models (PLMs)
(Devlin et al., 2019; Cui et al., 2020), the accu-
racy of MWP-BERT (Liang et al., 2022) firstly sur-
passes human performance (Wang et al., 2019). Be-
sides accuracy improvement, there are many works
exploring some other aspects of MWP solvers, e.g.,
teacher-student distillation (Zhang et al., 2020a),
auxiliary training tasks (Qin et al., 2021), situation
model (Hong et al., 2021). For more comprehen-
sive reviews of MWP solver, please refer to these
surveys (Zhang et al., 2019; Faldu et al., 2021).94552.2 Analogical Learning in NLP
The k-nearest neighbor retrieval has been a popular
approach deployed in machine translation (He et al.,
2021; Jiang et al., 2021) since the born of kNN-MT
(Khandelwal et al., 2021). It is simple yet effective
in terms of improving the translation performance
and explainability. This idea was quickly adopted
by other NLP research such as building language
models (Liu et al., 2022) and text-generation (Li
et al., 2022). There is one recent study in MWP
solving that shows the benefits of retrieval-based
analogical learning (Huang et al., 2021). The ba-
sic idea is to set up a memory module for storing
the problems that have been learned. For a new
given problem, the solver retrieves similar prob-
lems from the memory based on measuring the
cosine similarities of the encoded problems and
lets the solution of the retrieved problem partici-
pate decoding. However, this approach has several
limitations. Firstly, the retrieved problems with
high cosine similarity may not be analogical to the
given problem. They have similar representation
vectors perhaps because they have common topic
words in the problem description, but their reason-
ing steps are completely different. These literally
similar but non-analogical problems can mislead
the decoder give a wrong answer. Secondly, with
the increasing size of the training dataset/memory
module, the solver will suffer from long search
time.
We design our analogical reasoning solver with
ananalogy identification module, which takes ad-
vantage of only the problems whose mathematical
analogy is confirmed by their solution. Therefore,
we have no top- kselection issues. Moreover, un-
like (Huang et al., 2021) that introduces the analog-
ical problems in decoding, we use them for improv-
ing problem understanding (encoding). Analogical
problem pairs are mapped close in a dense region,
and non-analogical pairs are separated in different
regions (as demonstrated later in Figure 3). This ad-
justed representation space facilitates the decoding
process and leads to more correct results.
3 Approach
3.1 Problem Definition
We aims to train a math word problem solver
that receives the problem description Xas input
and then generates an equation-shaped solution
Y={y, y, ..., y}with length n. All equation
solutions are transformed into a binary tree rep-resentation as proposed in (Xie and Sun, 2019)
and sequantialized as their pre-order traversal, thus
there exists no parenthesis in Y. The vocabulary
ofYcontains two parts, operators and numbers.
In the solution tree, operator nodes are always par-
ents of numbers, and the number nodes have to
be leaf nodes, as shown in Figure 2. Specifically,
the vocabulary of operators Vcontains 5 opera-
tors{+,−,∗, /,ˆ}and the vocabulary of numbers
V may vary across different problems. We ap-
ply number mapping (Wang et al., 2017) to replace
the numbers in Ywhich can be found in Xwith
placeholders, thus the length of V depends on
how many numbers appear in X. Besides, there are
some useful constants like πwhich are not shown
inXbut used in Yto get a solution, we also in-
clude them in V.
3.2 Backbone Solvers
Our target is to design a model that can solve MWP
by analogical reasoning. Therefore, the model is in
fact a flexible framework that can be implemented
with any existing MWP solvers. We select GTS
(Xie and Sun, 2019) and MWP-BERT (Liang et al.,
2022) as backbone solvers and plug them into our
novel framework. GTS uses GRU (Cho et al., 2014)
as the encoder and MWP-BERT has a BERT en-
coder that is continually pre-trained on MWP cor-
pus. Both of them apply tree-based decoder and
two models can be found at open-sourced.
3.3 Analogy Identification
School students are often taught with example prob-
lems and tested by analogical problems in exams
for evaluating their problem-solving skills. As
stated in (Gentner and Holyoak, 1997), how to
bridge analogy among problems is one of the key
abilities in mathematical problem-solving. To build
an automated MWP solver, we first target to also
bridge analogy among problems. Thus we design a
module named analogy identification for address-
ing two issues, 1) how to find analogical problems;
and 2) how to bridge the analogy among problems.
Finding analogical problems Analogical prob-
lems usually have associated mathematization pro-
cesses. In other words, they can be solved by run-
ning a similar series of mathematical operators on
their own numbers, e.g., the problem 1 and 2 in9456
Figure 1. For analogical problems with different
difficulty levels, e.g., the problem 2 and 3 in Figure
1, the more difficult one can often be simplified
into an easier one. For example, ‘ buys 4 pencils
at$0.5each ’ in problem 3 can be simplified as
‘spends 4∗0.5 = $2 on pencils ’, which matches
well with ‘ spent $4to buy pencils ’ in problem 2.
By this simplification, we can see that the problem
2 and 3 both focus on addressing ‘number of units
= money / unit price’. To find these analogical
problems, we first try to understand how a solution
tree is generated by a decoder.
Generally, an MWP is solved by a top-down so-
lution tree, where the nodes closer to the top reflect
more about the ultimate goal, i.e., the root of the
tree. And lower nodes achieve sub-goals by local
calculation. Therefore, analogical problems tend
to have the same top nodes, which typically man-
ifest similar required knowledge or solving skills.
For example, the solution tree of problem 1 and
problem 2 in Figure 2 have the same top nodes of
running “/” and “-”. To find analogical problems,
we prune a solution tree in a bottom-up manner, re-
moving the bottom nodes and only keeping the top
nodes. Note that we only consider operator nodes
because they imply hidden logic (Yang et al., 2022)
that different MWPs may share in common, while
number nodes represent quantities varying across
different MWPs. In this way, all solution trees aresimplified to have fewer nodes. In our experimen-
tal implementation, we collect analogical problem
pairs if they have the same top-1 operator (the same
root node), or the same top-1 and top-2 operators
(the same root node and its left children following
the pre-order transversal). For an instance, prob-
lem 2 and problem 3 in Figure 2 are an analogical
pair (a positive pair) since they have common root
node “/” and its left child “-”. We consider our
operator matching method as a specific kind of
measurement of similarity/analogy, incorporating
analogies in (a) different topics and (b) different
difficulty levels by matching outermost operators
in the solution tree. There are two kinds of existing
analogy identification methods - 1. Finding prob-
lems with similar semantics. 2. Finding problems
with similar solutions (trees). However, the former
method neglects the topic-topic analogy, and the
latter method neglects the difficulty-level analogy.
Bridging the analogy For one analogical pair
of problems ( XandX), they should be mapped
close in a representation space, because they share
similar knowledge and skills for being solved. To
bridge their analogy, we maximize their analogy
score which is defined by:
s=σ(MLP ([en(X) :en(X)])),(1)
where en(X)yields the problem representation
forXby the problem encoder, [:]denotes vector9457concatenation, and σis a sigmod function. The
multi-layer perceptrons (MLP) and the encoder
en()are trained jointly to associate the analog-
ical pair. For two non-analogical problems, their
analogy score can be calculated in the same way,
and should be minimized.
Then the loss function for bridging the analogy
can be defined by cross-entropy:
L=−(log(s) +log(1−s)) (2)
where sis the score for positive pairs and sis for
negative. In each iteration, we randomly sample
our positive and negative pairs that are used to
update the MLP and encoder.
3.4 Solution Discrimination
Besides the analogical learning part that groups ana-
logical problems together, we also design a solution
discrimination module to make the solver associate
MWPs stronger to their ground truth solutions. We
define a discrimination score that measures the as-
sociation of a problem Xwith its ground truth
solution Y:
s=Dis([en(X), en(Y)]) (3)
where en(Y)is the representation of equation Y
encoded by a gate-recurrent-unit (GRU) (Chung
et al., 2014). The discriminator can be empirically
performed by a bi-linear similarity function with-
out bias. In other words, the output of Dis(A, B)
isAWB where Wis a learnable matrix. For the
problem X, we can also have a wrong solution
(negative sample) Y. A discrimination score
¯scan be calculated by Eq.(3) as well. Then, this
solution discriminator can be trained to minimize
swhile maximizing ¯s. In other words, the true
solution is pulled close to the problem, while the
wrong solution is pushed away from the problem.
The discriminator is updated once in each mini-
batch. Although the discriminator is trained from
scratch, it is stable in converging, as it only en-
codes solutions where the vocabulary only contains
placeholders for numbers and operators.
Negative Solution Generator For generating
negative solutions, any manipulation of the ground
truth solution can lead to a negative one, as imple-
mented in (Liang and Zhang, 2021). However, the
random modification neglects the importance of
tokens in the solution equation. Although all neg-
ative solutions ultimately lead to a wrong answer,the roles they play in minimizing loss functions
and serving as contrastive examples to a positive
true solution are at different levels of importance.
Our goal is to find variants of the ground truth
solution as hard negative samples, which only ma-
nipulate the most vulnerable (important) token. In
fact, this target is similar to evasion attack (Carlini
and Wagner, 2017) on texts, i.e., maximum effect
and minimum manipulation. Therefore, borrowing
the idea from white-box evasion attack, we regard
the token with the largest gradient as the most im-
portant and vulnerable one:
y=argmax(∇Dis([en(X), en(Y)]).(4)
After getting the most important y, the next step is
to find the replacement token of it. Different from
(Liang and Zhang, 2021) which generates it ran-
domly, we notice that the vocabulary for Yis in a
small size. Also, the token type has to be consistent
after replacement (operator or number). Therefore,
it is not costly to find all possible alternatives of
token yand make them all as negative equations.
Ifyis an operator, we have 4 alternatives out of
{+,−,∗, /,ˆ}, leading to 4 negative equations. If
yis a number, the choices of replacement will de-
pend on the size of V which is usually less than
5. In this way, we make full use of the result of gra-
dient checking and have considered all possibilities
of token replacement to form negative solutions.
Additional Negatives The above-generated neg-
ative equations are similar to the ground truth solu-
tion, since most part of it has not been changed. To
generate diverse negative samples that have loose
connections to the ground truth, we also randomly
select a solution from the training set as a neg-
ative equation, subject to the vocabulary of that
solution being a subset of the current solution vo-
cabulary. Because the output vocabulary may vary
in different problems, this restriction ensures the
generation of reasonable negative samples. Fur-
thermore, we execute the same procedure on this
random negative solution, i.e., finding and manip-
ulating the most vulnerable token to obtain more
negative samples. To sum up, the negative equation
generator of the solution discriminator module will
generate two groups of negative samples which are
variants of the ground truth solution and a random
solution with its variants. After the discriminator
Dis gets well-trained by cross-entropy loss, we can
construct a new loss term that provides extra super-
vision to the encoder of the MWP solver, which is9458similar to the adversarial loss in the generative ad-
versarial network (GAN) (Goodfellow et al., 2014):
L=−logP(s= 1|Dis([en(X), en(Y)])).
(5)
The solution discriminator thus enhances the asso-
ciation between problem representations and their
ground truth equation representations.
3.5 Model Training
There are three components in our proposed frame-
work. (a) An analogy identification module that
is jointly trained with the encoder-decoder solver,
which plays the role of an auxiliary task. (b) A
solution discrimination module that is separately
trained with the solver, which ensures the consis-
tency between MWP and its true solution, and pro-
vides guidance when the encoder-decoder part is
trained. (c) The solver is trained by the summation
of Seq2Seq loss, analogy identification loss (Equa-
tion 2), and solution discrimination loss (Equation
5), where the Seq2Seq loss is the negative log-
probability of generating Ygiven X:
L=−logP(Y|X). (6)
The steps of the training pipeline is in Alg. 1.
Firstly, we randomly sample some positive pairs
and negative pairs according to the introduction of
analogical identification in Section 3.3. Secondly,
we calculate the gradient as shown in Equation (4),
find all vulnerable tokens, and generate all nega-
tive equations for solution discrimination. Then
we train the discriminator with a cross-entropy loss
with ground truth as the positive sample and manip-
ulated solutions as the negative samples (line 3-5).
Finally, we train the analogy identification module
and the solver with the joint loss (line 6).
4 Experimental Results
In this section, we firstly introduce the used
datasets, evaluation metrics and baselines in this
paper. Then we conduct an accuracy comparison
between our solvers and all baseline methods. Next,
our ablation studies show the contribution from
different modules. We also analyze the solver ac-
curacy under different solution lengths to evaluate
generalization ability. Visualization of different
groups of MWPs shows that our solver has better
MWP representation ability. Case studies can be
found in appendix.Algorithm 1 Training Pipeline
Input : Problem X, Solution Y, Analogy Identifi-
cation Module θ, Solution Discrimination Module
γ, Encoder-Decoder Module δ
Parameter : Loss Weights λ, λ
Output : Well-trained θ,γandδforX,Yin the training set doX, X←Sample positive/negative
problems from the training set by analogy
identification.y←Get the most important token in Yby
gradient as shown in Eq. (4).Y←Replace ywith all possible tokens
and get the negative solution set. Train the discriminator γwith cross-entropy
loss. Train the analogy identification module (i.e.,
MLP) γand encoder-decoder module δwith
a joint loss: L=L+λL+λL.end forreturn θ,γ,δ
4.1 Datasets
Math23k Math23k (Wang et al., 2017) is the
most commonly used Chinese dataset in MWP
solving. It contains 23,162 problems with 21,162
training problems, 1,000 validation problems and
1,000 testing problems. We use the value accuracy
as the evaluation metric, which checks whether the
equation solution given by the model leads to the
ground truth value.
MathQA MathQA (Amini et al., 2019) is an En-
glish mathematical problems dataset at GRE level.
The original MathQA dataset is annotated in a dif-
ferent way from Math23k with many pre-defined
operations. Also, some problems in the dataset suf-
fer from the low-quality issue. Many efforts (Tan
et al., 2021; Li et al., 2021; Jie et al., 2022) have
been paid to clean and filter the MathQA dataset.
In our experiment, we follow the latest version (Jie
et al., 2022) of MathQA dataset where 4 arithmetic
operators {+,−,∗, /}are included in this subset.
After data filtering, the training set contains 16191
training MWPs and 1605 testing samples. We also
use the value accuracy as our evaluation metric for
this dataset.
4.2 Baselines
The baselines used in this paper can be divided
into RNN-based and PLM-based. For RNN-based9459Math23k MathQA #E
RNN-solvers
GTS 75.6 68.07.2M
GTS-teacher 76.5 68.57.2M
Graph2Tree 77.4 69.59.0M
EEH-G2T 78.5 − 9.9M
GTS+A&D 78.2 69.67.2M
PLM-solvers
REAL 82.3 − 110M
BERT-CL 83.2 73.8102M
MWP-BERT 84.6 77.2102M
Deductive Reasoner 85.1 78.6102M
Generate&Rank 85.4 − 610M
MWP-BERT+A&D 85.6 79.6103M
solvers, we select GTS (Xie and Sun, 2019) and
Graph2Tree (Zhang et al., 2020b) which are the
most commonly used baselines in other papers. Be-
sides, we include GTS-teacher (Liang and Zhang,
2021) which pairs up the problems and solutions
and checks them with a discriminator, and EEH-
G2T (Wu et al., 2021) is the best existing RNN-
based solver on Math23k. For PLM-based solvers,
we choose the retrieval-based analogical solver
REAL (Huang et al., 2021), contrastive learning
solver BERT-CL (Li et al., 2021) that determine
analogies based on sub-trees in the solution, Deduc-
tive Reasoner (Jie et al., 2022) and Generate&Rank
(Shen et al., 2021). To our knowledge, Gener-
ate&Rank is known as the best solver on Math23k
(85.6% accuracy) and Deductive Reasoner per-
forms the best on MathQA (78.6%).
4.3 Implementation Details
Our model is trained and evaluated under Pytorch
Framework with a single NVIDIA Tesla V100
GPU. The training iteration follows the steps in
Alg. 1 where λ= 0.01 and λ= 0.001, which are
selected by grid search from [0.0001, 0.001, 0.01,
0.1, 1]. We train the solvers with 160 epochs using
a batch size of 32. The learning rate for RNN-based
models and PLM-based models is set to 0.001 and0.00005, which is halved every 30 epochs. One-
epoch training takes about 10 minutes and 30 min-
utes on GTS-based and BERT-based solvers, re-
spectively. AdamW (Kingma and Ba, 2014) op-
timizer is used with default hyper-parameters in
Pytorch. Dropout (Srivastava et al., 2014) proba-
bility of 0.5 and 0.1 is used for GTS-encoder and
BERT-encoder to avoid potential overfitting. Five-
beam search is applied to produce better solutions.
For more details, please refer to our code.
4.4 Comparative Experiments
In this part, we compare our model with the above
baselines in terms of the value accuracy and the
number of parameters in the encoder, as shown
in Table 1. On Math23k benchmark, the perfor-
mance of GTS gets considerably raised from 75.4%
to 78.2%, which is comparable to the best RNN
solver EEH-G2T with fewer parameters in the en-
coder. For PLM-based solvers, our proposed train-
ing method boosts the accuracy of MWP-BERT
from 84.6% to 85.6%, achieving state-of-the-art
performance with much fewer parameters than
Generate&Rank. For MathQA dataset, with our
pipeline, the accuracies of GTS and MWP-BERT
both get improved and the improved MWP-BERT
solver outperforms all other competitors. For some
baselines, we failed to re-produce their methods
and get a reasonable accuracy on MathQA, thus we
leave them empty as many previous papers (Zhang
et al., 2020b; Li et al., 2021; Liang et al., 2022) did.
4.5 Ablation Studies
There are two major modules in our framework, i.e.,
analogy identification and solution discrimination.
For the former part, we separately test the influence
of top- k(k≤3)nodes and find that only top-1
and top-2 have contributions. The reason for the
ineffectiveness of top-3 nodes could be that only
a small number of MWPs contain more than 3
operators (shown in Table 3), and fewer MWPs
would have 3 same operators at the top. Therefore,
there will be a considerately limited number of
analogical pairs for top-3 MLP. For the solution
discrimination module, we evaluate the influence
of gradient-guided token selection (we use random
token selection in its ablation) and the additional
negative solutions that are randomly drawn out of
the training set. The results in Table 2 verify the
contribution of every individual component to the9460Analogy Identification (A) Solution Discrimination (D)Acc.Top-1
MLPTop-2
MLPTop-3
MLPGradient
guidedExtra
Negatives
MWP-BERT ✗ ✗ ✗ ✗ ✗ 84.7
✓ ✗ ✗ ✗ ✗ 84.9
MWP-BERT+A ✓ ✓ ✗ ✗ ✗ 85.3
✓ ✓ ✓ ✗ ✗ 85.1
✗ ✗ ✗ ✓ ✗ 85.0
✗ ✗ ✗ ✗ ✓ 85.1
MWP-BERT+D ✗ ✗ ✗ ✓ ✓ 85.3
MWP-BERT+A&D ✓ ✓ ✗ ✓ ✓ 85.6
#OP Pct.GTS MWP-BERT
w/o w w/o w
1 35.4% 84.986.190.190.7
2 44.0% 80.681.787.088.0
3 14.1% 70.771.362.471.3
4 3.3% 50.060.660.678.8
50.6% 38.238.250.050.0
proposed training pipeline.
4.6 Analysis of Long-tail Problem Solving
In Table 3, the column Pct. gives the percentage of
MWPs with different solution lengths. We can see
that the distribution of MWPs follows a long-tail
distribution where the simple MWPs with short so-
lutions are the head, and the difficult MWPs with
long solutions are the tail. We find that the ac-
curacy on the tails gets more increased than the
accuracy on the head with the assistance of our
method. Although the overall accuracy improve-
ment on Math23k is about 1% as shown in Table
2, our method makes the solver learn from simple
MWPs and generalize to difficult MWPs, bringing
a significant accuracy boost on tail problems.
4.7 Visualization
To demonstrate the ability of grouping analogical
problems in the representation space, we visual-
ize them by T-SNE (Van der Maaten and Hinton,
2008) and check the clustering phenomenon as did
in (Liang and Zhang, 2021; Li et al., 2021). We
randomly select 150 problems from Math23k in
4 classes, where each class represents a group of
MWPs whose solutions has the same root nodes
out of{+,−,∗, /}. Then we visualize them in Fig-
ure 3. For vanilla MWP-BERT, many boundary
points are not well separated. With the analogy
identification, the number of confusing boundary
points gets lower. With the solution discrimination
module, the inter-class distance is increased and
the green cluster is well separated from the others.
With both, different clusters are more separated and9461
the distribution of each cluster gets denser. This
analysis demonstrates that our model effectively
improves problem representation learning.
Case Study We show two cases in Figure 4 to
demonstrate the potency of our proposed method.
We select two problems that MWP-BERT (Liang
et al., 2022) failed to solve from the Math23k test-
ing set. We can find that MWP-BERT solver is
influenced by analogical problems in the training
set and generates an identical solution as the so-
lution of the second analogical problem. The po-
tential reason is that these MWPs are semantically
similar and the model fails to distinguish them. In
our approach, we design the solution discrimina-
tion module to strength the association between the
problems and ground truth solutions. Our solver
correctly generates the equation through the help
of the designed module.
5 Conclusion
We propose a novel analogical training pipeline for
math word problem solvers, considering the gener-
alization among analogical MWPs and the associa-
tion between ground truth solutions and problem
descriptions. In this way, problems that need sim-
ilar solving skills are grouped together to supportanalogical learning, and solvers are trained to focus
on ground truth solutions. The comparative study
shows that our method with MWP-BERT outper-
forms other baselines on Math23k and MathQA,
and is much lighter (with fewer parameters). It
can also solve more difficult problems due to the
analogical reasoning. We believe our work would
facilitate the MWP research community and inspire
more studies about the analogy in mathematical
question answering.
Limitations
Commonsense Knowledge As mentioned in
(Lin et al., 2020; Liang et al., 2021), MWP solving
in the real-word scenario requires many common-
sense knowledge, e.g., 1km = 1000m and one day =
24 hours. When these commonsense constants are
not explicitly given in the problem description, our
MWP solver has no chance to solve problems that
require them. A future direction could be injecting
commonsense knowledge into MWP solvers.
Acknowledgement
The research work is partially supported by the
Internal Asia Research Collaboration Grant, Uni-
versity of Notre Dame. Thanks for all reviewers
for their valuable comments.9462References94639464