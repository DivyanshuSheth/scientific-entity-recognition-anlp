
Hung-yi Lee
National Taiwan UniversityShang-Wen Li
Amazon AINgoc Thang Vu
University of Stuttgart
Abstract
Deep learning has been the mainstream tech-
nique in natural language processing (NLP)
area. However, the techniques require many
labeled data and are less generalizable across
domains. Meta-learning is an arising ﬁeld
in machine learning studying approaches to
learn better learning algorithms. Approaches
aim at improving algorithms in various as-
pects, including data efﬁciency and generaliz-
ability. Efﬁcacy of approaches has been shown
in many NLP tasks, but there is no systematic
survey of these approaches in NLP, which hin-
ders more researchers from joining the ﬁeld.
Our goal with this survey paper is to offer
researchers pointers to relevant meta-learning
works in NLP and attract more attention from
the NLP community to drive future innovation.
This paper ﬁrst introduces the general concepts
of meta-learning and the common approaches.
Then we summarize task construction settings
and application of meta-learning for various
NLP problems and review the development of
meta-learning in NLP community.
1 Introduction
Recently, deep learning (DL) based natural lan-
guage processing (NLP) has been one of the re-
search mainstreams and yields signiﬁcant perfor-
mance improvement in many NLP problems. How-
ever, DL models are data-hungry. The downside
limits such models’ application to different do-
mains, languages, countries, or styles because col-
lecting in-genre data for model training are costly.
To address the challenges, meta-learning tech-
niques are gaining attention. Meta-learning,
or Learning to Learn, aims to learn better
learning algorithms, including better parameter
initialization (Finn et al., 2017), optimization
strategy (Andrychowicz et al., 2016; Ravi and
Larochelle, 2017), network architecture (Zophand Le, 2017; Zoph et al., 2018; Pham et al.,
2018a), distance metrics (Vinyals et al., 2016;
Gao et al., 2019a; Sung et al., 2018), and be-
yond (Mishra et al., 2018). Meta-learning allows
faster ﬁne-tuning, converges to better performance,
yields more generalizable models, and it achieves
outstanding results for few-shot image classiﬁ-
caition (Triantaﬁllou et al., 2020). The beneﬁts
alleviate the dependency of learning algorithms on
labels and make model development more scalable.
Image processing is one of the machine learning
areas with abundant applications and established
most of the examples in the previous survey papers
on meta-learning (Hospedales et al., 2021; Huis-
man et al., 2021).
On the other hand, there are works showing ben-
eﬁts of meta-learning techniques in performance
and data efﬁciency via applying meta-learning to
NLP problems. Please refer to Tables 2 and 3 in the
appendix for NLP applications improved by meta-
learning. Tutorial (Lee et al., 2021b) and Work-
shop (Lee et al., 2021a) are organized at ACL 2021
to encourage exchange and collaboration among
NLP researchers interested in these techniques. To
facilitate more NLP researchers and practitioners
beneﬁting from the advance of meta-learning and
participating in the area, we provide a systematic
survey of meta-learning to NLP problems in this pa-
per. There is another survey paper on meta-learning
in NLP (Yin, 2020). While Yin (2020) describes
meta-learning methods in general, this paper fo-
cuses on the idea of making meta-learning success-
ful when applied to NLP and provides a broader
review of publications on NLP meta-learning. This
paper is organized as below.
•A brief introduction of meta-learning back-
grounds, general concepts, and algorithms in
Section 2.
•Common settings for constructing meta-
learning tasks in Section 3.666•Adaptation of general meta-learning ap-
proaches to NLP problems in Section 4.
•Meta-learning approaches for special topics,
including knowledge distillation and life-long
learning for NLP applications in Section 5.
Due to space constraints, we will not give too many
detailed descriptions of general meta-learning tech-
niques in this survey paper. For general concepts
of meta-learning, we encourage readers to read the
previous overview paper (Yin, 2020; Hospedales
et al., 2021; Huisman et al., 2021).
2 Background Knowledge for Meta
Learning
The goal of machine learning (ML) is to ﬁnd a
functionf(x)parametrized by model parameters
θfor inference from training data. For machine
translation (MT), the input xis a sentence, while
f(x)is the translation of x; for automatic speech
recognitoin (ASR), xis an utterance, while f(x)
is the transcription; In DL, θare the network pa-
rameters, or weights and biases of a network. To
learnθ, there is a loss function l(θ;D), where Dis
a set of paired examples for training,
D={(x,y),(x,y),...,(x,y)}, (1)
wherexis function input, yis the ground truth,
andKis the number of examples in D. The loss
functionl(θ;D)is deﬁned as below:
l(θ;D) =/summationdisplayd(f(x),y). (2)
whered(f(x),y)is the “distance” between the
function output f(x)and the ground truth y. For
classiﬁcation problem, d(.,.)can be cross-entropy;
for regression, it can be L1/L2 distance. The fol-
lowing optimization problem is solved to ﬁnd the
optimal parameter set θfor inference via minimiz-
ing the loss function l(θ;D).
θ= arg minl(θ;D). (3)
In meta-learning, what we want to learn is a
learning algorithm. The learning algorithm can
also be considered as a function, denoted as F(.).
The input of F(.)is the training data, while the
output of the function F(.)is the learned model pa-
rameters, orθin (3). The learning algorithm F(.)
is parameterized by meta-parameters φ, which iswhat we want to learn in meta-learning. If F(.)
represents gradient descent for deep network, φcan
be initial parameters, learning rate, network archi-
tecture, etc. Different meta-learning approaches
focus on learning different components. For ex-
ample, model-agnostic meta-learning (MAML) fo-
cuses on learning initial parameters (Finn et al.,
2017), which will be further descried in Section 4.1.
Learning to Compare methods like Prototypical
Network (Snell et al., 2017) in Section 4.2 learn
the latent representation of the inputs and their dis-
tance metrics for comparison. Network architec-
ture search (NAS) in Section 4.3 learns the network
architecture (Zoph and Le, 2017; Zoph et al., 2018;
Pham et al., 2018a).
To learn meta-parameters φ,meta-training tasks
T are required.
T ={T,T,...,T}, (4)
where Tis a task, and Nis the number of tasks
inT. Usually, all the tasks belong to the same
NLP problem; for example, all the Tare QA but
from different corpora, but it is also possible that
the tasks belong to various problems. Each task T
includes a support set Sand a query set Q. Both
SandQare paired examples as Din (1). The
support set plays the role of training data in typical
ML, while the query set can be understood as the
testing data in typical ML. However, to not confuse
the reader, we use the terms support and query sets
in the context of meta-learning instead of training
and testing sets.
In meta-learning, there is a loss function
L(φ;T), which represents how “bad” a learn-
ing algorihtm paramereized by φis on T.
L(φ;T)is the performance over all the tasks
inT,
L(φ;T) =/summationdisplayl(θ;Q). (5)
The deﬁnition of the function l(.)above is the same
as in (2).l(θ;Q)for each task Tis obtained
as below. For each task TinT, we use a
support set Sto learn a model by the learning
algorihtmF. The learned model is denoted as θ,
whereθ=F(S). This procedure is equivalent
to typical ML training. We called this step within-
task training . Thenθis evaluated on Qto obtain
l(θ;Q)in (5). We called this step within-task
testing . One execution of within-task training and667followed by one execution of within-task testing is
called an episode .
The optimization task below is solved to learn
meta-parameteres φ.
φ= arg minL(φ;T). (6)
Ifφis differentiable with respect to L(φ;T),
then we can use gradient descent to learn meta-
parameters; if not, we can use reinforcement learn-
ing algorithm or evolutionary algorithm. Solv-
ing (6) is called cross-task training in this pa-
per, which usually involves running many episodes
on meta-training tasks. To evaluate φ, we need
meta-testing tasks T, tasks for evaluating algo-
rithms parameterized by meta-parameters φ. We
docross-task testing onT, that is, running an
episode on each meta-testing task to evaluate algo-
rithms parameterized by meta-parameters φ.
In order to facilitate the reading of our paper, we
summarize the most important terminologies and
their meanings in Table 1 in the appendix.
3 Task Construction
In this section, we discuss different settings of
constructing meta-training tasks T and meta-
testing tasks T.
3.1 Cross-domain Transfer
A typical setting for constructing the tasks is based
on domains (Qian and Yu, 2019; Yan et al., 2020; Li
et al., 2020a; Park et al., 2021; Chen et al., 2020b;
Huang et al., 2020a; Dai et al., 2020; Wang et al.,
2021b; Dingliwal et al., 2021; Qian et al., 2021).
In this setting, all the tasks, no matter belonging
toT orT, are the same NLP problems. In
each task T, the support set Sand the query set
Qare from the same domain, while different tasks
contain the examples from different domains. In
each task, the model is trained on the support set
of a domain (usually having a small size) and eval-
uated on the query set in the same domain, which
can be considered as domain adaptation . From the
meta-training tasks T, cross-task training ﬁnds
meta-parameters φparameterizing the learning al-
gorithmF. With a sufﬁcient number of tasks in
T, cross-task training should ﬁnd a suitable φ
for a wide range of domains, and thus also workswell on the tasks in Tcontaining the domains
unseen during cross-task training. Hence, meta-
learning can be considered as one way to improve
domain adaptation . If the support set in each task
includes only a few examples, the meta-learning
has to ﬁnd the meta-parameters φthat can learn
from a small support set and generalize well to the
query set in the same domain. Therefore, meta-
learning is considered one way to achieve few-shot
learning .
The cross-domain setting is widespread. We
only provide a few examples in this subsection.
In MT, each meta-training task includes the doc-
uments from a speciﬁc domain (e.g., news, laws,
etc.), while each meta-testing task also contains
documents from one domain but not covered by the
meta-training tasks (e.g., medical records) (Li et al.,
2020a). For another example, both meta-training
and meta-testing tasks are DST. The meta-training
tasks include hotel booking, ﬂight ticket booking,
etc., while the testing task is taxi booking (Huang
et al., 2020a; Wang et al., 2021b; Dingliwal et al.,
2021). Domain has different meanings in different
NLP problems. For example, in speech process-
ing tasks, the domains can refer to accents (Winata
et al., 2020b; Huang et al., 2021) or speakers (Kle-
jch et al., 2019; Wu et al., 2021b; Huang et al.,
2022).
3.2 Cross-lingual Transfer
If we consider different languages as different do-
mains, then the cross-lingual transfer can be re-
garded as a special case of cross-domain transfer.
Suppose each task contains the examples of an
NLP problem from one language, and different
tasks are in different languages. In this case, cross-
task training ﬁnds meta-parameters φfrom the
languages in T, and cross-task testing evaluate
the meta-parameters φon new langauges in T.
This setting aims at ﬁnding the learning algorithm
F(.)that works well on the NLP problem of
any language given the support set of the language.
Cross-language settings have been applied to NLI
and QA in X-MAML (Nooralahzadeh et al., 2020),
documentation classiﬁcation (van der Heijden et al.,
2021), dependency parsing (Langedijk et al., 2021),
MT (Gu et al., 2018), and ASR (Hsu et al., 2020;
Winata et al., 2020a; Chen et al., 2020d; Xiao et al.,
2021).
For the meta-learning methods aiming at learn-
ing the initial parameters like MAML (will be intro-668duced in Section 4.1), the network architecture used
in all tasks must have the same network architec-
ture. A uniﬁed network architecture across all tasks
is not obvious in cross-lingual learning because the
vocabularies in different tasks are different. Be-
fore multilingual pretrained models are available,
uniﬁed word embeddings across languages are re-
quired. Gu et al. (2018) uses the universal lexical
representation to overcome the input-output mis-
match across different languages. Recently, by
using multilingual pretrained models as encoders,
such as M-BERT (Devlin et al., 2019) or XLM-
R (Conneau et al., 2020), all languages can share
the same network architecture (Nooralahzadeh
et al., 2020; van der Heijden et al., 2021).
3.3 Cross-problem Training
Here the meta-training and meta-testing tasks can
come from different problems. For example, the
meta-training tasks include MT and NLI, while
the meta-testing tasks include QA and DST. The
cross-problem setting is not usual, but there are
still some examples. In Bansal et al. (2020a),
the meta-training tasks are the GLUE benchmark
tasks (Wang et al., 2018), while the meta-testing
tasks are NLP problems, including entity typing,
NLI, sentiment classiﬁcation, and various other
text classiﬁcation tasks, not in the GLUE. All the
meta-training and meta-testing tasks can be for-
mulated as classiﬁcation but with different classes.
In Indurthi et al. (2020), the meta-training tasks are
MT and ASR, while the meta-testing task is speech
translation (ST). CrossFit is a benchmark corpus
for this cross-problem setting (Ye et al., 2021).
The intrinsic challenge in the cross-problem
setting is that different NLP problems may need
very different meta-parameters in learning algo-
rithms, so it may be challenging to ﬁnd uniﬁed
meta-parameters on the meta-training tasks that
can generalize to meta-testing tasks. In addition,
the meta-learning algorithms learning initial pa-
rameters such as MAML require all the tasks to
have a uniﬁed network architecture. If different
problems need different network architecture, then
the original MAML cannot be used in the cross-
problem setting. LEOPARD (Bansal et al., 2020a)
and ProtoMAML (van der Heijden et al., 2021) are
the MAML variants that can be used in the classi-
ﬁcation tasks with different class numbers. Both
approaches use the data of a class to generate the
class-speciﬁc head, so only the parameters of the
head parameter generation model are required. The
head parameter generation model is shared across
all classes, so the network architecture becomes
class-number agnostic. On the other hand, recently,
universal models for a wide range of NLP prob-
lems have been emgered (Raffel et al., 2019; Chen
et al., 2021; Ao et al., 2021). We believe the de-
velopment of the universal models will intrigue the
cross-problem setting in meta-learning.
3.4 Domain Generalization
Traditional supervised learning assumes that the
training and testing data have the same distribution.
Domain shift refers to the problem that a model
performs poorly when training data and testing
data have very different statistics. Domain adapta-
tion in Section 3.1 uses little domain-speciﬁc data
to adapt the model. On the other hand, domain
generalization techniques attempt to alleviate the
domain mismatch issue by producing models that
generalize well to novel testing domains.
Meta-learning can also be used to realize domain
generalization by learning an algorithm that can
train from one domain but evaluate on the other. To
simulate the domain generalization scenario, a set
of meta-training tasks are constructed by sampling
data from different domains as the support and
query sets. With the meta-training tasks above,
cross-task training will ﬁnd the meta-parameters φ
that work well on the scenario where the training
(support) and testing (query) examples are from
different domains. Fig. 1 shows how to construct
tasks for domain generalization and compares the
construction with the cross-domain transfer setting.
The setting has been used to improve the domain
generalization for semantic parsing (Wang et al.,6692021a) and language generalizationfor sentiment
classiﬁcation and relevance classiﬁcation (Li et al.,
2020c).
3.5 Task Augmentation
In meta-learning, it is critical to have a large num-
ber of diverse tasks in the meta-training tasks T
to ﬁnd a set of meta-parameters φthat can gener-
alize well to the meta-testing tasks. However, con-
sidering the setting in the previous subsections, dif-
ferent tasks contain examples in various domains,
language, or even NLP problems, so a large and di-
verseT are often not available. In typical ML,
data augmentation comes in handy when data is
lacking. In meta-learning, augmenting tasks is sim-
ilarly understood as data augmentation in ML. Data
augmentation becomes task augmentation because
the “training examples” in meta-learning are a col-
lection of tasks. Task augmentation approaches
in meta-learning can be categorized into two main
directions: a) Inventing more tasks (without human
labeling efforts) to increase the number and diver-
sity of the meta-training tasks T. b) Splitting
training data from one single dataset into homoge-
nous partitions that allow applying meta-learning
techniques and therefore improve the performance.
NLP-speciﬁc methods have been proposed in both
categories.
Inventing more tasks The main question is
how to construct a massive amount of tasks ef-
ﬁciently. There is already some general task aug-
mentation approahces proposed for general meta-
learning (Yao et al., 2021a; Ni et al., 2021; Ra-
jendran et al., 2020; Yao et al., 2021b). Here
we only focus on NLP-speciﬁc approaches. In-
spired from the self-supervised learning, Bansal
et al. (2020b) generates a large number of cloze
tasks, which can be considered as multi-class clas-
siﬁcation tasks but obtained without labeling ef-
fort, to augment the meta-training tasks. Bansal
et al. (2021) further explores the inﬂuence of un-
supervised task distribution and creates task distri-
butions that are inductive to better meta-training
efﬁcacy. The self-supervised generated tasks im-
prove the performance on a wide range of different
meta-testing tasks which are classiﬁcation prob-
lems (Bansal et al., 2020b), and it even performs
comparably with supervised meta-learning meth-
ods on FewRel 2.0 benchmark (Gao et al., 2019b)
on 5-shot evaluation (Bansal et al., 2021).Generating tasks from a monolithic corpus
Many tasks can be constructed with one monolithic
corpus (Huang et al. (2018); Guo et al. (2019);
Wu et al. (2019); Jiang et al. (2019); Chien and
Lieow (2019); Li et al. (2020b); MacLaughlin et al.
(2020); Wang et al. (2020a); Pasunuru and Bansal
(2020); Xu et al. (2021a); Murty et al. (2021)).
First, the training set of the corpus is split into sup-
port partition, D, and query partition, D. Two
subsets of examples are sampled from DandD
as the support set, S, and query set, Q, respectively.
In each episode, model parameters θare updated
withS, and then the losses are computed with the
updated model and Q. The meta-parameters φ
are then updated based on the losses, as the meta-
learning framework introduced in Section 2. The
test set of the corpus is used to build Tfor eval-
uation. As compared to constructing T from
multiple relevant corpora, which are often not avail-
able, building T with one corpus makes meta-
learning methodology more applicable. Besides,
results obtained from one corpus are more compa-
rable with existing NLP studies. However, only
using a single data stream makes the resulting mod-
els less generalizable to various attributes such as
domains and languages.
How to sample the data points to form a task
is the key in such category. In NAS research in
Section 4.3, the support and query sets are usually
randomly sampled. Learning to Compare in Sec-
tion 4.2 splits the data points of different classes in
different tasks based on some predeﬁned criteria.
There are some NLP-speciﬁc ways to construct the
tasks. In Huang et al. (2018), a relevance function
is designed to sample the support set Sbased on its
relevance to the query set Q. In Guo et al. (2019),
a retrieval model is used to retrieve the support set
Sfrom the whole dataset. DReCa (Murty et al.,
2021) applies clustering on BERT representations
to create tasks.
4 Meta-Learning for NLP Tasks
This section shows the most popular meta-learning
methods for NLP and how they ﬁt into NLP tasks.
Due to space limitations, only the major trends are
mentioned. Please refer to Table 2 and 3 in the
appendix for a complete survey.6704.1 Learning to Initialize
In typical DL, gradient descent is widely used to
solve (3). Gradient descent starts from a set of
initial parameters θ, and then the parameters θ
are updated iteratively according to the directions
of the gradient. There is a series of meta-learning
approaches targeting at learning the initial parame-
tersθ. In these learn-to-init approaches, the meta-
parametersφto be learned are the initial parameters
θfor gradient descent, or φ=θ. MAML (Finn
et al., 2017) and its ﬁrst-order approximation, FO-
MAML (Finn et al., 2017), Reptile (Nichol et al.,
2018), etc., are the representative approaches of
learn-to-init. We surveyed a large number of papers
using MAML-based approaches to NLP applica-
tions in the last three years and summarized them
in Table 4 in the appendix.
Learning to Initialize v.s. Self-supervised
Learning The learn-to-init approaches aim at
learning a set of good initial parameters. On the
other hand, self-supervised approaches like BERT
also have the same target. There is a natural ques-
tion: are they complementary? Based on the sur-
vey in Table 4 in the appendix, it is common to
use the self-supervised models to “initialize” the
meta-parameters φin learn-to-init approaches. To
ﬁnd the optimal φin (5), gradient descent is used
as well, and thus the “initial parameters for initial
parameters”, or φis required. A self-supervised
model usually serves the role of φ, and the learn-
to-init approaches further update φto ﬁndφ.
Learn-to-init and self-supervised learning are
complementary. The self-supervised objectives are
different from the objective of the target NLP prob-
lem, so there is a “learning gap”. On the other hand,
learn-to-init approaches learn to achieve good per-
formance on the query sets of the meta-training
tasks, so it directly optimizes the objective of the
NLP problems. The beneﬁt of self-supervised
learning is that it does not require labeled data,
while labeling is still needed to prepare the exam-
ples in meta-training tasks.
Learning to Initialize v.s. Multi-task Learning
Multi-task learning is another way to initialize
model parameters, which usually serves as the
baseline of learn-to-init in the literature. In multi-
task learning, all the labelled data from the meta-
training tasks is put together to train a model. That
is, all the support sets Sand query sets Qin
the meta-training tasks T are put together as atraining set D, and the loss (3) is optimized to ﬁnd a
parameterθ. Thenθis used as initial parameters
for the meta-testing tasks.
Both multi-task learning and meta-learning lever-
age the examples in the meta-training tasks, but
with different training criteria. Learn-to-init ﬁnds
the initial parameters suitable to be updated by up-
dating the model on the support sets and then eval-
uating it on the query sets. In contrast, multi-task
learning does not consider that the initial parame-
ters would be further updated at all during training.
Therefore, in terms of performance, learn-to-init
is usually shown to be better than multi-task learn-
ing (Dou et al., 2019; Chen et al., 2020b). On
the other hand, in terms of training speed, meta-
learning, which optimizes (5), is more computation-
ally intensive than multi-task learning optimizing
(3).
Three-stage Initialization Since learn-to-init,
multi-task, self-supervised learning all have their
pros and cons, they can be integrated to draw on
the strong points of each other. A common way to
integrate the three approaches is “three-stage ini-
tialization” as below. a) First, initialize a model
by self-supervised learning, which leverages un-
labeled data. Its objective is usually not directly
related to the target NLP problem. b) Then, multi-
task learning is used to ﬁne-tune the self-supervised
model. The objective of multi-task learning is
the target NLP problem but does not consider the
update procedure in gradient descent. c) Finally,
learn-to-init, which ﬁnds the initial parameters suit-
able for update, is used to ﬁne-tune the multi-task
model.
Learn-to-init is chosen to be the last stage be-
cause its training objective is closest to the target
of looking for good initial parameters, but it is
the most computationally intensive method, and
thus it is only used to change the model a little bit.
The three-stage initialization has been tested in sev-
eral works (Nooralahzadeh et al., 2020; Wu et al.,
2021b; van der Heijden et al., 2021; Langedijk
et al., 2021), but it does not always improve the
performance (Wu et al., 2021b; van der Heijden
et al., 2021).
Challenges Learn-to-init is an essential
paradigm for few-shot learning and usually
achieves outstanding results in the few-shot
learning benchmarks of image classiﬁcation (Tri-671antaﬁllou et al., 2020). However, it has fallen short
of yielding state-of-the-art results on NLP few-shot
learning benchmarks (Ye et al., 2021; Chen et al.,
2022; Bragg et al., 2021). For example, on the
cross-task few-shot learning benchmark, CrossFit,
simple multi-task learning outperforms existing
learn-to-init in many cases (Ye et al., 2021). One
possible reason is meta-learning methods are
susceptible to hyper-parameters and even random
seeds (Antoniou et al., 2019). Hence, it is difﬁcult
to obtain decent performance without exhaustively
tuning hyperparameters. The research about
developing more stable learn-to-init methods may
lead to more practical real-world applications for
the approaches. There is a study about stabilizing
the cross-task training of learn-to-init methods by
reducing the variance of gradients for NLP (Wang
et al., 2021b).
4.2 Learning to Compare
Learning to Compare methods are widely applied
to NLP tasks. Among many others, we ﬁnd appli-
cations of Learning to Compare methods in text
classiﬁcation (Yu et al., 2018; Tan et al., 2019;
Geng et al., 2019; Sun et al., 2019b; Geng et al.,
2020), sequence labeling (Hou et al., 2020; Oguz
and Vu, 2021), semantic relation classiﬁcation (Ye
and Ling, 2019; Chen et al., 2019a; Gao et al.,
2019a; Ren et al., 2020), knowledege completion
(Xiong et al., 2018; Wang et al., 2019b; Zhang et al.,
2020; Sheng et al., 2020) and speech recognition
(Lux and Vu, 2021) tasks.
Most of the proposed methods are based on
Matching Network (Vinyals et al., 2016), Prototyp-
ical Network (Snell et al., 2017) and Relation Net-
work (Sung et al., 2018), and extend these architec-
tures in two aspects: a) how to embed text input in a
vector space with/without context information, and
b) how to compute the distance/similarity/relation
between two inputs in this space. Since these ques-
tions have had deep roots in the computation lin-
guistics research for many years (Sch ¨utze, 1992;
Manning and Schutze, 1999), Learning to Com-
pare methods is one of the most important methods
among other meta-learning methods in the context
of NLP despite their simplicity. Notably, to date,
such family of methods is mainly applied to classi-
ﬁcation tasks.
4.3 Neural Network Architecture Search
Neural network architecture search (NAS) is an-
other common meta-learning technique appliedto NLP including language modeling (WikiText-
103 (Merity et al., 2017), PTB (Mikolov et al.,
2010)), NER (CoNLL-2003 (Sang and De Meul-
der, 2003)), TC (GLUE (Wang et al., 2019a)),
and MT (WMT’14 (Bojar et al., 2014)). As dis-
cussed in Section 3.5, these techniques are often
trained/evaluated with a single, matched dataset,
which is different from other meta-learning ap-
proaches.
Moreover, in contrast to conventional NAS meth-
ods that focus on learning the topology in an indi-
vidual recurrent or convolutional cell, NAS meth-
ods have to be redesigned in order to make the
search space suitable for NLP problems, where
contextual information often plays an important
role. Jiang et al. (2019) pioneers the application
of NAS to NLP tasks beyond language modeling
(NER in this case), and improves differentiable
NAS by redesigning its search space for natural
language processing. Li et al. (2020b) extends
the search space of NAS to cover more RNN ar-
chitectures and allow the exploring of intra- and
inter-token connection to increase the expressibil-
ity of searched networks. As the popularity of pre-
trained language models (PLM) grows in NLP area,
researchers also apply NAS to discover better topol-
ogy for PLM such as BERT. Wang et al. (2020a)
introduces Hardware-Aware Transformers (HAT)
to search Transformer architecture optimized for
inference speed and memory footprint in different
hardware platforms. NAS-BERT (Xu et al., 2021b)
and AdaBERT (Chen et al., 2020a) explores task-
agnostic and task-dependent network compression
techniques with NAS respectively. EfﬁcientBERT
(Dong et al., 2021) applies NAS to search for more
efﬁcient architecture of feed-forward network that
is suitable for edge device deployment.
To show the efﬁcacy of NAS, we summarize
the performance of several state-of-the-art NAS
approaches on GLUE benchmarks (Wang et al.,
2019a) in Table 5 in the appendix. These ap-
proaches are applied to BERT to discover ar-
chitectures with smaller sizes, faster inference
speed, and better model accuracy. For com-
parison, performance from original and manu-
ally compressed BERT models is also presented.
The results show that the BERT architecture im-
proved by NAS yields performance competitive
to BERT (c.f., 82.3 from EfﬁcientBERT vs 82.5
from BERT) and is 6.9x smaller and 4.4x faster.
The searched architecture also outperforms man-672ually designed, parameter- and inference-efﬁcient
model (MobileBERT) at similar size and speed.
These results suggest the efﬁcacy of NAS in dis-
covering more efﬁcient network architectures. As
NLP researchers continue to design even larger
PLMs while the need of deployment on edge de-
vices grows, we expect there will be increasing
investment in innovating NAS techniques to make
PLM networks more compact and accelerate infer-
ence.
Challenges The main bottleneck for NAS be-
ing widely applied is the prohibitive requirement
in computation resources for architecture search.
Approaches such as Efﬁcient Neural Architecture
Search (ENAS, Pham et al. (2018b)) and Flexible
and Expressive Neural Architecture Search (FE-
NAS, Pasunuru and Bansal (2020)) are proposed
to improve the search efﬁciency. As PLMs usually
have bulky sizes and slow training speed, search
efﬁciency is even more critical when applying NAS
to PLM. Weight-sharing techniques are often ap-
plied to accelerate searching (Wang et al., 2020a;
Dong et al., 2021; Xu et al., 2021b).
4.4 Meta-learning for Data Selection
Multi-linguality, multi-task, and multi-label see
many impacts on NLP problems due to the diver-
sity of human languages. To learn models with bal-
anced performance over attributes (e.g., languages,
tasks, labels), a common approach is to weight the
training examples for data selection to learn models
with balanced performance over the attributes, and
it is a natural assumption that meta-learning tech-
niques derive more generalizable weighting than
manually tuned hyperparameters. For example, Wu
et al. (2019) add another gradient update step wrap-
ping the conventional classiﬁer update for training
meta-parameters that controls the weight when ag-
gregating losses from different labels to update clas-
siﬁer’s parameters. In addition to gradient update,
meta-learned weights are also applied directly to
training examples for data selection to address the
issue of noisy labeling. Shu et al. (2019) propose a
technique to jointly learn a classiﬁer and a weight-
ing function, where a conventional gradient update
for the classiﬁer and a meta-learning update for
the weighting is performed alternatively. The func-
tion weights examples to mitigate model overﬁtting
towards biased training data caused by corrupted
labels or class imbalance. Zheng et al. (2021) apply
a similar framework but extend the weighting witha label correction model. Both techniques show
improvement over SOTA in text classiﬁcation with
biased training data.
Additionally, as the progress in the research of
pre-training and transfer learning, there is a trend
of leveraging datasets in multiple languages, do-
mains, or tasks to jointly pre-train models to learn
transferable knowledge. A meta-learned data se-
lector can also help in this scenario by choosing
examples that beneﬁt model training and transfer-
ability. For instance, Wang et al. (2020b) investi-
gate the common challenge of imbalanced train-
ing examples across languages in multilingual MT,
which is conventionally addressed by tuning hyper-
parameters manually to up-sample languages with
less resources. The authors propose Differentiable
Data Selection (DDS) to parameterize the sampling
strategies. DDS is trained with episodes and REIN-
FORCE algorithm to optimize parameters of sam-
pler and MT models in an alternating way for the
MT models to converge with better performance
across languages. Pham et al. (2021) formulate
data sampling for multilingual MT as a problem of
back-translation to generate examples of parallel
utterances from unlabeled corpora in target lan-
guage. The back-translation is jointly trained with
MT models to improve translation result through
better distribution of training examples and data
augmentation. Tarunesh et al. (2021) further study
knowledge transferring across tasks and languages.
The authors combine Reptile and DDS to meta-
learn samplers with six different languages (en, hi,
es, de, fr, and zh) and ﬁve different tasks (QA,
NLI, paraphrase identiﬁcation, POS tagging, and
NER) and demonstrate competitive performance
on XTREME multilingual benchmark dataset (Hu
et al., 2020).
5 Meta-learning beyond Accuracy
In the previous sections, meta-learning is used to
obtain better evaluation metrics for NLP applica-
tions. This section illustrates how meta-learning
can improve NLP applications from more aspects
beyond performance.
5.1 Learn to Knowledge Distillation
Knowledge distillation method was proposed in
(Hinton et al., 2015). The main goal is to transfer
knowledge from a so-called teacher model, e.g., a
vast neural network trained with a lot of training
data, to a more compact student model, e.g., a neu-673ral network with much less trainable parameters.
The main weaknesses of this method are as follows:
a) the number of teacher models is ﬁxed to one that
could limit the power of the transferring process; b)
the teacher model is not optimized for the transfer-
ring process and c) the teacher model is not aware
of the student model during the transferring pro-
cess. Meta-learning methods can be applied to
partially ﬁx these issues. The high-level idea is
to increase the number of teacher models and the
number of student models and consider each pair
of a teacher model and a student model as a task
in the meta-learning framework. By doing so, we
can train a meta teacher model that works better
than a single teacher model (Pan et al., 2020), and
we can optimize the transferring process and force
the teacher model to be aware of the student model
(Zhou et al., 2022).
5.2 Learn to Life-long learning
This subsection discusses how to use meta-learning
to improve lifelong learning (LLL) (Chen and Liu,
2018). The real world is changing and evolving
from time to time, and therefore machines natu-
rally need to update and adapt to the new data they
receive. However, when a trained deep neural net-
work is adapted to a new dataset with a different
distribution, it often loses the knowledge previously
acquired and performs the previous seen data worse
than before. This phenomenon is called catas-
trophic forgetting (McCloskey and Cohen, 1989).
There is a wide range of LLL approaches aiming
for solving catastrophic forgetting (Parisi et al.,
2019). Among them, the following directions ap-
ply meta-learning:
Meta-learning for Regularization-based LLL
methods Regularization-based LLL methods
aim to consolidate essential parameters in a model
when adapting models with new data (Kirkpatrick
et al., 2017; Zenke et al., 2017; Schwarz et al.,
2018; Aljundi et al., 2018; Ehret et al., 2021). Meta-
learning targets “how to consolidate” and has some
successful examples in NLP applications. Knowl-
edgeEditor (De Cao et al., 2021) learns the parame-
ter update strategies that can learn the new data and
simultaneously retain the same predictions on the
old data. KnowledgeEditor has been applied to fact-checking and QA. Editable Training (Sinitsin et al.,
2020) employs learn-to-init approaches to ﬁnd the
set of initial parameters, ensuring that new knowl-
edge can be learned after updates without harming
the performance of old data. Editable Training
empirically demonstrates the effectiveness on MT.
Meta-learning for Data-based LLL Methods
The basic idea of data-based methods is to store a
limited number of previously seen training exam-
ples in memory and then use them for empirical
replay, that is, training on seen examples to re-
cover knowledge learned (Sprechmann et al., 2018;
de Masson d 'Autume et al., 2019; Sun et al., 2019a)
or to derive optimization constraints (Lopez-Paz
and Ranzato, 2017; Li and Hoiem, 2017; Saha and
Roy, 2021). A hurdle for data-based approaches
is the need to store an unrealistically large number
of training examples in memory to achieve good
performance. To achieve sample efﬁciency, Oba-
muyide and Vlachos (2019a); Wang et al. (2020c);
Wu et al. (2021a) uses meta-learning to learn a
better adaptation algorithm that recovers the knowl-
edge learned with a limited amount of previously
seen data. Experiments on text classiﬁcation and
QA benchmarks validate the effectiveness of the
framework, achieving state-of-the-art performance
using only 1% of the memory size (Wang et al.,
2020c).
6 Conclusion
This paper investigates how meta-learning is used
in NLP applications. We review the task construc-
tion settings (Section 3), the commonly used meth-
ods including learning to initialize, learning to com-
pare and neural architecture search (Section 4), and
highlight research directions that go beyond im-
proving performance (Section 5). We hope this
paper will encourage more researchers in the NLP
community to work on meta-learning.
References674675676677678679680A Appendx681682683684