
Linlu QiuPeter ShawPanupong PasupatPaweł Krzysztof Nowak
Tal LinzenFei ShaKristina ToutanovaGoogle ResearchNew York Univeristy
Abstract
Generic unstructured neural networks have
been shown to struggle on out-of-distribution
compositional generalization. Compositional
data augmentation via example recombina-
tion has transferred some prior knowledge
about compositionality to such black-box neu-
ral models for several semantic parsing tasks,
but this often required task-speciﬁc engineer-
ing or provided limited gains.
We present a more powerful data recombi-
nation method using a model called Compo-
sitional Structure Learner (CSL). CSL is a
generative model with a quasi-synchronous
context-free grammar backbone, which we in-
duce from the training data. We sample recom-
bined examples from CSL and add them to the
ﬁne-tuning data of a pre-trained sequence-to-
sequence model (T5). This procedure effec-
tively transfers most of CSL’s compositional
bias to T5 for diagnostic tasks, and results
in a model even stronger than a T5-CSL en-
semble on two real world compositional gen-
eralization tasks. This results in new state-of-
the-art performance for these challenging se-
mantic parsing tasks requiring generalization
to both natural language variation and novel
compositions of elements.
1 Introduction
Compositional generalization refers to the ability to
generalize to novel combinations of previously ob-
served atoms .For example, we may ask a model
to interpret the instruction “jump twice”, when the
atoms “jump” and “twice” were each observed sep-
arately during training but never in combination
with each other (Lake and Baroni, 2018).
Improving compositional generalization is seen
as important for approaching human-like language
understanding (Lake et al., 2017; Battaglia et al.,
Figure 1: An overview of our method for compositional
data augmentation with CSL, a generative model with a
QCFG backbone, which is automatically induced from
the training data. We show a notional set of original and
synthetic examples mapping utterances to programs.
2018) and is practically signiﬁcant for real world
applications, where models deployed in the wild
often need to interpret new combinations of ele-
ments not well-covered by expensive and poten-
tially skewed annotated training data (Herzig and
Berant, 2019; Yin et al., 2021).
Generic neural sequence-to-sequence models
have improved substantially and reached high lev-
els of performance, particularly when combined
with large-scale unsupervised pretraining and siz-
able in-distribution labeled data. However, these
models often perform poorly on out-of-distribution
compositional generalization tasks (Lake and Ba-
roni, 2018; Furrer et al., 2020; Shaw et al., 2021).4341In contrast, specialized architectures with dis-
crete latent structure (Chen et al., 2020; Liu et al.,
2020; Nye et al., 2020; Herzig and Berant, 2021;
Shaw et al., 2021) have made strides in compo-
sitional generalization, but without task-speciﬁc
engineering or ensembling, the gains have been
limited to synthetic semantic parsing tasks. Al-
though following SCAN (Lake and Baroni, 2018)
some increasingly realistic synthetic tasks such
as CFQ (Keysers et al., 2020) and COGS (Kim
and Linzen, 2020) have been created, and several
approaches achieve good performance on these
tasks, the out-of-distribution generalization abil-
ity of state-of-the-art models on real-world, non-
synthetic tasks is still far from sufﬁcient (Shaw
et al., 2021; Yin et al., 2021).
Given their different strengths and weaknesses,
it is compelling to combine the compositional bias
of such specialized models with the greater ﬂexi-
bility and ability to handle natural language vari-
ation that characterizes generic pre-trained neural
sequence-to-sequence models. One method for this
isdata augmentation . For example, Jia and Liang
(2016) generate new training examples using exam-
ple recombination via induced high-precision syn-
chronous grammars, resulting in improvements on
in-distribution and compositional splits of seman-
tic parsing tasks. Another example is GECA (An-
dreas, 2020), a more general data augmentation ap-
proach that does not require task-speciﬁc assump-
tions. GECA achieved further gains on a larger va-
riety of tasks, but provided limited improvements
on some compositional generalization challenges.
We present a compositional data augmentation
approach that generalizes these earlier methods.
Training examples are recombined using the Com-
positional Structure Learner (CSL) model, a gener-
ative model with a (quasi-)synchronous context-
free grammar (QCFG) backbone, automatically
induced from the training data. As illustrated in
Figure 1, CSL is used to sample synthetic train-
ing examples, and the union of original and syn-
thesized examples is used to ﬁne-tune the T5
sequence-to-sequence model (Raffel et al., 2020).
CSL is more generally applicable than the method
of Jia and Liang (2016), employing a generic gram-
mar search algorithm to explore a larger, higher-
coverage space of possible grammars. Unlike
GECA, CSL can re-combine examples recursively
and also deﬁnes a probabilistic sampling distribu-
tion over input-output pairs.CSL builds on the NQG model of Shaw et al.
(2021), a discriminative parsing model over an in-
duced QCFG backbone, which Shaw et al. (2021)
proposed to ensemble with T5. Like NQG, CSL
can, on its own, address a variety of composi-
tional generalization diagnostic tasks on synthetic
datasets and achieves high precision (but limited
recall) on non-synthetic compositional generaliza-
tion tasks, leading to overall gains when ensembled
with T5. However, CSL offers several signiﬁcant
improvements over NQG, allowing it to efﬁciently
address a wider range of datasets (see §3.1). Ad-
ditionally, unlike NQG which is a discriminative
model assigning probabilities to outputs ygiven
inputsx, CSL is a generative model which ad-
mits sampling from a joint probability distribution
p(x,y). This enables the creation of new input-
output training examples.
Empirically, augmenting the training data for
T5 with samples from CSL transfers most of
CSL’s compositional bias to T5 for diagnostic tasks
(SCAN and COGS), and outperforms a T5+CSL
ensemble on non-synthetic compositional general-
ization tasks deﬁned by compositional splits of
GeoQuery (Zelle and Mooney, 1996) and SM-
CalFlow (Andreas et al., 2020; Yin et al., 2021),
resulting in new state-of-the-art performance on
these splits.
2 Background and Motivation
In this section, we discuss the problem setting com-
mon to the compositional generalization evalua-
tions we study, and propose some general assump-
tions that motivate our proposed method.
Problem Setting Consider a training dataset D
consisting of input-output pairs /angbracketleftx,y/angbracketright∈X×Y ,
whereXis the set of valid inputs and Yis the
set of valid outputs. We assume that /angbracketleftx,y/angbracketright∈D
are sampled from a source distribution p(x,y).
Our model will be evaluated on inputs from a tar-
getdistribution p(x,y). We make an assumption
that the conditional distribution of ygivenxis un-
changed between source and target distributions;
i.e.,p(y|x) =p(y|x), which is also a standard
assumption for domain adaptation evaluations un-
der covariate shift. Any or all of the following
may be true: p(x,y)/negationslash=p(x,y),p(x)/negationslash=p(x),
p(y)/negationslash=p(y), andp(x|y)/negationslash=p(x|y).4342
What differentiates our setting from other forms
of distribution shift is the added assumption that
the source and target distributions share common
“atoms” (see §1). In order to translate this intu-
itive notion of atom sharing into formal conditions,
we deﬁne a general class of models termed deriva-
tional generative models , based on representing
atoms as functions which can be recombined via
function application. As a modeling hypothesis, we
will assume that the training and evaluation distri-
butions can be modeled by derivational generative
models that share a common set of functions, but
may vary in how they assign probability to deriva-
tions formed by recombining these functions.
Derivational Generative Models A deriva-
tional generative model deﬁnes a distribution
p(x,y)over input-output pairs. The model con-
tains a set of functions, G, and a distribution over
derivations . A derivation zcan be viewed as a tree
of functions fromGwhich derives some element
/llbracketz/rrbracket=/angbracketleftx,y/angbracketright∈X×Y determined by recursively
applying the functions in z.An example deriva-
tion is shown in Figure 2.
GivenX,Y, andG, we can generate a set Zof
possible derivations. We deﬁne some shorthands
for important subsets of Zfor givenxandy:
Z={z∈Z|/llbracketz/rrbracket=/angbracketleftx,y/angbracketright}
Z={z∈Z|∃y∈Y,/llbracketz/rrbracket=/angbracketleftx,y/angbracketright}
A derivational generative model also consists of
some probability distribution p(z)over the set of
derivationsZ, which we assume to be parameter-
ized byθ. We deﬁne p(x,y)in terms ofp(z)
as:
p(x,y) =/summationdisplayp(z), (1)and therefore:
p(y|x) =/summationtextp(z)
/summationtextp(z), (2)
forp(x)>0.
Discussion In general, we are interested in a set
of functions that captures some knowledge of how
the parts of inputs correspond to parts of outputs.
If we can recover some approximation of the under-
lying set of functions, G, givenD, then we could
sample derivations consisting of new combinations
of functions that are not observed in D. This could
potentially help us improve performance on the
target distribution, since we assume that the set
of functions is unchanged between the source and
target distributions, and that what is varying is the
distribution over derivations.
However, even assuming Gcan be exactly re-
covered givenDis not sufﬁcient to ensure that we
can correctly predict the most likely ygivenxac-
cording to the true p(y|x)(shared between source
and target distributions) for x∼p(x).We must
also assume that there exists a parameterization
ofp(z)such that when we estimate ˆθgivenD,
p(y|x)sufﬁciently approximates the true p(y|x)
forx∼p(x). We hypothesize that conditional in-
dependence assumptions with respect to how p(z)
decomposes across the function applications in
zcan be helpful for this purpose. In particular,
such assumptions can enable “reusing” conditional
probability factors across the exponential space of
derivations, potentially improving transfer to the
target distribution.
With this intuition in mind, in §3we pro-
pose a speciﬁc class of functions for Gbased
on(quasi-)synchronous context-free grammars, as
well as a factorization of p(z)with strong condi-
tional independence assumptions.
3 Proposed Method
As shown in Figure 1, our method consists of two
stages. First, we induce our generative model, CSL,
from training data (§3.1). Second, we sample syn-
thetic examples from the generative model and use
them to augment the training data for a sequence-
to-sequence model (§3.2).4343
3.1 Compositional Structure Learner (CSL)
CSL can be viewed as a derivational generative
model, as deﬁned in §2, where the set Gof recur-
sive functions is deﬁned by a (quasi-)synchronous
context free grammar (QCFG).
We ﬁrst describe the grammar formalism and
the parameterization of our probabilistic model.
Then we describe our two-stage learning procedure
for inducing a grammar and learning the model
parameters. CSL builds on the NQG model of
Shaw et al. (2021), with several key differences
discussed in the following sections:
•Unlike NQG, which is discriminative, CSL is
agenerative model that admits efﬁcient sam-
pling from the joint distribution p(x,y).
•CSL enables a more expressive set of gram-
mar rules than NQG.
•CSL offers a more computationally efﬁcient
and parallelizable grammar induction algo-
rithm and a more efﬁcient parametric model,
allowing the method to scale up to larger
datasets such as SMCalFlow-CS.
See section 4.4 for experiments and analysis com-
paring the components of CSL and NQG.
3.1.1 Grammar Formalism
An example QCFG derivation is shown in Figure 3,
and the notation for QCFGs is reviewed in Ap-
pendix B.1. Notably, the correspondence between
rules over input and output strings in QCFGs is
akin to a homomorphism between syntactic and
semantic structures, commonly posited by formal
theories of compositional semantics (Montague,
1970; Janssen and Partee, 1997). We restrict our
grammars to have only a single unique nonterminal
symbol,NT. In constrast to standard synchronous
context-free grammars (SCFGs), our grammars canbequasi-synchronous (Smith and Eisner, 2006) be-
cause we allow a one-to-many alignment between
non-terminals.Unlike the formalism of Shaw et al.
(2021), which limited rules to contain ≤2non-
terminals, in the current work the maximal num-
ber of non-terminals is a conﬁgurable parameter,
which enables inducing grammars with higher cov-
erage for certain datasets.
3.1.2 Probabilistic Model
We factorize the probability of a derivation in terms
of conditional probabilities of sequentially expand-
ing a rule from its parent. Formally, let rdenote a
rule expanded from its parent rule r’sNTnon-
terminal (or a special symbol at the root of the
derivation tree).We assume conditional indepen-
dence and factorize the probability of zas:
p(z) =/productdisplayp(r|r,i) (3)
This non-terminal annotation with context from the
tree is akin to parent annotation or other structure
conditioning for probabilistic context-free gram-
mars (Johnson, 1998; Klein and Manning, 2003).
Using independent parameters for each combi-
nation of a rule, its parent, and non-terminal index
may lead to overﬁtting to the training set, limiting
our ability to generalize to new combinations of
rule applications that are needed for compositional
generalization. We therefore factor this distribution
using a soft clustering into a set of latent statesS
representing parent rule application contexts:
p(r|r,i) =/summationdisplayp(r|s)p(s|r,i) (4)
wherep(s|r,i)∝eandp(r|s)∝e
and theθs are scalar parameters.The number of
context states|S|is a hyperparameter. While these
conditional independence assumptions may still be
too strong for some cases (see Appendix C.2), we
ﬁnd them to be a useful approximation in practice.4344We also optionally consider a task-speciﬁc output
CFG, which deﬁnes valid output constructions.
3.1.3 Learning Procedure
A principled method to estimate GandθgivenD
would be to ﬁnd the MAP estimate based on some
prior,p(G,θ), that encourages compositionality:
arg maxp(G,θ)×/productdisplayp(x,y) (5)
However, since optimizing Gandθjointly is
computationally challenging, we adopt a two-stage
process similar to that of Shaw et al. (2021). First,
we learn an unweighted grammar using a surro-
gate objective for the likelihood of the data and
a compression-based compositional prior that en-
courages smaller grammars that reuse rules in
multiple contexts, inspired by the Minimum De-
scription Length (MDL) principle (Rissanen, 1978;
Grunwald, 2004). We describe the induction ob-
jective and algorithm in §3.1.4. Second, given
an unweighted grammar G, we optimize the pa-
rametersθby maximizing the log-likelihood of
p(x,y), as deﬁned by Eq. 1, using the Adam
optimizer (Kingma and Ba, 2015).
3.1.4 Grammar Induction Algorithm
Our method for inducing a QCFG is based on that
of Shaw et al. (2021), but with several modiﬁca-
tions, which improve the computational scalability
of the algorithm as well as the precision and cov-
erage of the induced grammar. We analyze the
relative performance of the two algorithms in §4.4.
Objective The main idea of the grammar induc-
tion objective, L(G), is to balance the size of the
grammar with its ability to ﬁt the training data:
L(G) =/summationdisplay|α|+|β|−c(α,β),(6)
where|·|is a weighted count of terminal and non-
terminal tokens (the relative cost of a terminal vs.
nonterminal token is a hyperparameter) and
c(α,β) =kln ˆp(α|β) +kln ˆp(β|α)(7)wherekandkare hyperparameters and ˆp(α|β)
is equal to the fraction of examples /angbracketleftx,y/angbracketright∈D
whereα“occurs in”xout of the examples where
β“occurs in”y, and vice versa for ˆp(β|α).The
correlation between αandβas measured by the
ˆpterms provides a measure related to how well
the rule ﬁts the training data. We use sampling to
optimize the computation of ˆpfor larger datasets.
While conceptually similar to the objective used
by NQG, we found that CSL’s objective is more
efﬁcient to compute and can be more effective at
penalizing rules that lead to lower precision.
Initialization To initializeGwe add a rule
NT→ /angbracketleftx,y/angbracketrightfor every/angbracketleftx,y/angbracketright ∈ D . We also
optionally add a set of seed rules, such as NT→
/angbracketleftx,x/angbracketrightwhere a terminal or span xis shared between
the input and output vocabularies. For details on
seed rules used for each dataset, see Appendix A.
Greedy Algorithm Following the initialization
of the set of rulesG, we use an approximate parallel
greedy search algorithm to optimize L(G), while
maintaining the invariant that all examples in D
can be derived byG.
At each iteration, the algorithm considers each
rulerin the current grammar in parallel. The algo-
rithm determines a (potentially empty) set of candi-
date actions for each r. Each candidate action con-
sists of adding a new rule to the grammar that can
be combined with an existing rule to derive r, en-
ablingrto be removed. Certain candidate actions
may enable removing other rules, too. The algo-
rithm then selects the candidate action that leads to
the greatest improvement in the induction objective,
if any action exists that leads to an improvement.
The selected actions are then aggregated and exe-
cuted, resulting in a new set of rules. The algorithm
continues until no further actions are selected, or a
maximum number of steps is reached. The detailed
implementation of the greedy algorithm is detailed
in Appendix B.
3.1.5 Inference Procedure
While the primary goal of CSL is to be used to sam-
ple new examples for data augmentation (discussed4345next), we can also use CSL as a discriminative pars-
ing model, by using a variant of the CKY algorithm
to ﬁnd the highest scoring derivation zthat maxi-
mizes Eq. 3 for a given input x. We then output the
corresponding yif it can be derived by the given
output CFG, or if no output CFG is provided.
3.2 Data Augmentation
We synthesize a conﬁgurable number of examples
by sampling from the learned generative model,
CSL.To generate a synthetic example (x,y), we
use forward sampling: we start from the single NT
symbol and sample recursively to expand each non-
terminal symbol with a rule, based on p(r|r,i)
deﬁned by Eq. 4.
Given that generic sequence-to-sequence models
perform poorly on length extrapolation (Newman
et al., 2020), we optionally bias our sampling to
favor deeper derivations. We achieve this by adding
a biasδ>0toθfor any rulerthat contains more
nonterminals than our conﬁgurable threshold.
We ﬁne-tune T5 on the union of the original
training data and the synthesized data. Following
Jia and Liang (2016), we ensure an approximately
equal number of original and synthesized examples
are used for training. We achieve this by replicating
original or synthetic examples as needed.
4 Experiments and Analysis
In this section, we comparatively evaluate and an-
alyze our main proposed method, T5+CSL-Aug.,
which uses CSL to generate examples for augment-
ing the training data of T5.
4.1 Datasets
We evaluate our approach on both synthetic bench-
marks designed for controlled assessments of com-
positional generalization, and non-synthetic eval-
uations, which introduce the additional challenge
of handling natural language variation. For exam-
ple, some words in the test data might never appear
during training. Further details on datasets and
preprocessing are in Appendix A.
SCAN The SCAN dataset contains navigation
commands paired with action sequences. We con-
sider three compositional data splits from Lake andBaroni (2018): the jump andturn left splits (where
a new primitive is used in novel combinations), and
thelength split. We also consider the MCD splits
from Keysers et al. (2020) created by making the
distributions of compositional structures in training
and test data as divergent as possible.
COGS The COGS dataset (Kim and Linzen,
2020) contains sentences paired with logical forms.
We use the generalization test set, which tests
generalization to novel linguistic structures. As
SCFGs cannot handle logical variables (Wong and
Mooney, 2007), we convert the outputs into equiv-
alent variable-free forms.
GeoQuery GeoQuery (Zelle and Mooney, 1996;
Tang and Mooney, 2001) contains human-authored
questions paired with meaning representations. We
report results on the standard data split as well
as three compositional splits based on those intro-
duced in Shaw et al. (2021): the template split
(where abstract output templates in training and
test data are disjoint (Finegan-Dollak et al., 2018)),
theTMCD split (an extension of MCD for non-
synthetic data), and the length split.
SMCalFlow-CS Yin et al. (2021) proposed a
compositional skills split of SMCalFlow (Andreas
et al., 2020) that contains single-turn sentences
from one of two domains related to creating cal-
endar events or querying an org chart, paired with
LISP programs. The single-domain (S) test set
has examples from a single domain, while the
cross-domain (C) test set has sentences that require
knowledge from both domains (e.g., “create an
event with my manager”). Only a small number
of cross-domain examples (8, 16, or 32) are seen
during training.
4.2 Baselines
Our primary goal is to evaluate T5+CSL-Aug.
in comparison to T5 and T5+GECA, a method
augmenting training data with GECA which is
prior state of the art for data augmentation (An-
dreas, 2020).Details and hyperparameters for the4346
GECA experiments are available in Appendix C.4.
We also compare with representative prior state-
of-the-art methods. For SCAN, NQG-T5 (Shaw
et al., 2021) is one of several specialized mod-
els that achieves 100% accuracy across multiple
splits (Chen et al., 2020; Liu et al., 2020; Nye et al.,
2020; Herzig and Berant, 2021). For COGS, we
show results from LeAR (Liu et al., 2021), the pre-
viously reported state-of-the-art on COGS.We
also report new results for NQG-T5 on COGS. For
GeoQuery, we report results for NQG-T5and
SpanBasedSP (Herzig and Berant, 2021) on the
GeoQuery splits we study.For SMCalFlow-CS,
we show the strongest previously reported results
by Yin et al. (2021), which include the coarse2ﬁne
(C2F) model of Dong and Lapata (2018) as a
baseline, as well C2F combined with the span-
supervised (SS) attention method of Yin et al.
(2021). We found it was not computationally feasi-
ble to run NQG-T5 on SMCalFlow.
4.3 Main Results
The results are shown in Table 1. For synthetic
datasets, the induced grammars have high cover-
age, making the CSL model highly effective for
data augmentation. When we use CSL to gener-
ate additional training data for T5 (T5+CSL-Aug.),
the performance of T5 improves to nearly solvingSCAN and achieving state-of-the-art on COGS.
For non-synthetic tasks, T5+CSL-Aug. leads to
new state-of-the-art accuracy on all compositional
splits. However, performance is slightly worse on
the single-domain splits of SMCalFlow-CS. Based
on error analysis in Appendix C.9, we ﬁnd a signiﬁ-
cant degree of inherent ambiguity for the remaining
errors on the single-domain split, which may con-
tribute to this result.
Using CSL for data augmentation outperforms
using GECA on SCAN and GeoQuery. We did not
ﬁnd it computationally feasible to run GECA on
COGS or SMCalFlow-CS. On some splits, using
GECA to augment the training data for T5 can lead
to worse performance, as GECA can over-generate
incorrect examples. We provide further analysis
comparing CSL and GECA in Appendix C.4.
4.4 Analysis and Discussion
The performance of T5+CSL-Aug. is dependent
on the CSL grammar backbone, parametric model,
and data sampling details. We analyze the accuracy
of T5+CSL-Aug. in relation to CSL’s coverage, and
summarize the impact of the design choices in CSL
that depart from prior work.
Performance Breakdown CSL provides analy-
ses for and can only sample inputs covered by its
grammarx∈X, which is often a strict subset
of all possible utterances. It is therefore interest-
ing to see how data augmentation impacts T5’s
performance on covered and non-covered inputs.
In Table 2, we analyze the relative performance
of T5, CSL, and combinations of T5 and CSL us-
ing ensembling and data augmentation, for non-
synthetic compositional splits, partitioning inputs
based on whether they are covered by CSL (the
same analysis for all splits can be found in Ap-4347
pendix C.1). An ensemble model can help T5 only
whenx∈X, but we can see from Table 2 that
data augmentation improves model performance
even on inputs not covered by the grammar. For
example, for the GeoQuery Length split, perfor-
mance on non-covered inputs improves from 35.3
to 60.9. This means that T5 is generalizing from
the sampled data ( x∈X) tox /∈X.
Comparison with NQG We cannot compare us-
ing CSL for data augmentation directly with using
its closely related predecessor NQG (Shaw et al.,
2021) for data augmentation, as NQG is a discrimi-
native parsing model and not a probabilistic genera-
tive model that enables sampling new examples.
However, we include comparisons of the novel
components of CSL relative to the related com-
ponents of NQG in the following sections, which
analyze CSL’s grammar induction algorithm and
parametric model.
Grammar Induction The grammar induction al-
gorithm of CSL is signiﬁcantly more scalable than
that of NQG, enabling more than 90% decrease
in runtime for GeoQuery, and enabling induction
to scale to larger datasets such as SMCalFlow-CS.
CSL can also induce higher coverage grammars
than NQG in some cases, while maintaining high
precision. For example, for COGS, the grammar
induced by CSL can derive 99.9% of the evaluation
set while the grammar induced by NQG can only
derive 64.9%. Appendix C.3 contains further analy-
sis comparing the grammar induction algorithms of
CSL and NQG. Of course, the grammars induced
by CSL can still lack coverage for some datasets,
as shown in Table 2. We analyze the limitations of
QCFGs in Appendix C.5.Parameteric Model We ﬁnd that the simple pa-
rameteric model of CSL performs comparably in
terms of parsing accuracy to the BERT-based dis-
criminative model of NQG given the same gram-
mar (see Appendix C.3). It is also more scalable be-
cause it does not require the computation of a parti-
tion function. The variable number of state clusters
(§3.1.2) provides a powerful knob for tuning the
amount of context sensitivity (see Appendix C.2)
to sufﬁciently ﬁt the training data while also extrap-
olating to out-of-distribution compositions. We be-
lieve further improvements to the parametric model
(e.g. using pre-trained representations) have strong
potential to improve overall accuracy.
Sampling Results on most splits are signiﬁcantly
improved by using CSL’s parametric model com-
pared to sampling uniformly from the induced
grammar (Appendix C.6), pointing to a potential
source of gains over unweighted augmentation ap-
proaches like GECA. However, for SMCalFlow-
CS, a higher sampling temperature can improve
performance, especially on the 8-shot split, as it
leads to>15times the number of cross-domain
examples being sampled, given their low percent-
age in the training data. Determining improved
methods for biasing the sampling towards exam-
ples most relevant to improving performance on
the target distribution is an important direction. In
Appendix C.7 we explore a setting where we as-
sume access to unlabeled examples from the target
distribution, and use these to update the parametric
model. We ﬁnd that this improves sample efﬁciency
with respect to the number of sampled synthetic
examples, but can have minimal effect when a suf-
ﬁciently large number of examples can be sampled.
We believe this is a promising research direction.43485 Related Work
Grammar Induction Before the trend towards
sequence-to-sequence models, signiﬁcant prior
work in semantic parsing explored inducing
SCFG (Wong and Mooney, 2006, 2007; An-
dreas et al., 2013) and CCG (Zettlemoyer and
Collins, 2005, 2007; Kwiatkowksi et al., 2010;
Kwiatkowski et al., 2013; Artzi et al., 2014) gram-
mars of the input-output pairs. SCFGs have
also been applied to machine translation (Chiang,
2007; Blunsom et al., 2008; Saers et al., 2013).
Compression-based objectives similar to ours have
also been applied to CFG induction (Grünwald,
1995). Recently, the method of Kim (2021) learns
neural parameterized QCFG grammars, which can
avoid the pitfalls in coverage of lexicalized gram-
mars such as the ones we learn; however the
approach can be computationally demanding for
longer input-output pairs.
Data Augmentation Data augmentation has
been widely used for semantic parsing and re-
lated tasks (Jia and Liang, 2016; Andreas, 2020;
Akyürek et al., 2021; Wang et al., 2021b; Zhong
et al., 2020; Oren et al., 2021; Tran and Tan, 2020;
Guo et al., 2020, 2021). Jia and Liang (2016) per-
form data recombination using an induced SCFG
but their approach requires domain-speciﬁc heuris-
tics. GECA (Andreas, 2020) provides a more gen-
eral solution, which we analyzed in §4.4. The
method of Akyürek et al. (2021) is appealing be-
cause it can learn data recombinations without com-
mitting to a grammar formalism, although gains
were limited relative to symbolic methods. The re-
combination approach of Guo et al. (2020) demon-
strates gains for translation tasks but is not as ef-
fective as GECA for semantic parsing tasks. Other
approaches leverage a forward semantic parser
and a backward input generator with some vari-
ants (Wang et al., 2021b; Zhong et al., 2020; Tran
and Tan, 2020; Guo et al., 2021), but most of these
approaches do not explicitly explore the compo-
sitional generalization setting. Oren et al. (2021)
propose an approach to sample more structurally-
diverse data to improve compositional generaliza-
tion, given a manually speciﬁed SCFG.
Compositional Generalization Beyond data
augmentation, many approaches have been pursued
to improve compositional generalization in seman-
tic parsing, including model architectures (Li et al.,
2019; Russin et al., 2019; Gordon et al., 2020; Liuet al., 2020; Nye et al., 2020; Chen et al., 2020;
Zheng and Lapata, 2020; Oren et al., 2020; Herzig
and Berant, 2021; Ruiz et al., 2021; Wang et al.,
2021a), different Transformer variations (Csordás
et al., 2021; Ontanón et al., 2021), ensemble mod-
els (Shaw et al., 2021), intermediate representa-
tions (Herzig et al., 2021), meta-learning (Lake,
2019; Conklin et al., 2021; Zhu et al., 2021), and
auxiliary objectives to bias attention in encoder-
decoder models (Yin et al., 2021; Jiang and Bansal,
2021). Also, Furrer et al. (2020) compared pre-
trained models with specialized architectures.
6 Conclusion
We showed that the Compositional Structure
Learner (CSL) generative model improves the state
of the art on compositional generalization chal-
lenges for two real-world semantic parsing datasets
when used to augment the task training data for
the generic pre-trained T5 model. Data augmenta-
tion using CSL was also largely sufﬁcient to distill
CSL’s knowledge about compositional structures
into T5 for multiple synthetic compositional gen-
eralization evaluations. While CSL has limitations
(notably, the QCFG formalism is not a good ﬁt
for all phenomena in the mapping of natural lan-
guage to corresponding logical forms), our experi-
ments suggest the strong potential of more power-
ful probabilistic models over automatically induced
latent structures as data generators for black-box
pretrained sequence-to-sequence models.
Acknowledgements
We thank Nitish Gupta, Ming-Wei Chang, William
Cohen, Pengcheng Yin, Waleed Ammar, the
Google Research Language team, and the anony-
mous reviewers for helpful comments and discus-
sions.
Ethical Considerations
This paper proposed methods to improve composi-
tional generalization in semantic parsing. While we
hope that improvements in compositional general-
ization would lead to systems that generalize better
to languages not well represented in small train-
ing sets, in this work we have only evaluated our
methods on semantic parsing datasets in English.4349References4350435143524353Appendix
The appendix is organized into three sections:
•Appendix A contains dataset and preprocess-
ing details.
•Appendix B contains modeling details and
hyperparameters.
•Appendix C contains additional experiments
and analysis.
A Dataset and Preprocessing Details
In this section we detail preprocessing for each
dataset. Dataset sizes are reported in Table 3. We
show examples of each dataset in Table 4, with ex-
amples of the corresponding induced QCFG rules
in Table 5. For each dataset, we report exact match
accuracy. We note that all datasets include English
language data only; evaluating and extending our
method for other languages is an important future
direction. We use the same dataset preprocessing
for the T5, T5+GECA, NQG-T5, and T5+CSL-
Aug. results we report.
SCAN We did not perform any preprocessing
for SCAN. Grammar induction does not use any
seed rules, and we do not assume a CFG deﬁning
valid output constructions, as the outputs consist
of action sequences, not executable programs or
logical forms.COGS For COGS, as QCFGs do not support
logical variables (Wong and Mooney, 2007), we
mapped the original logical forms to a variable-free
representation, with an example shown in Table 4.
The mapping is deterministic and reversible, and is
akin to the use of other variable-free logical forms
for semantic parsing such as FunQL (Kate et al.,
2005) or Lambda-DCS (Liang, 2013). An alterna-
tive but potentially more complex solution to han-
dling logical variables in outputs would be to use
an extension of SCFGs, such as λ-SCFG (Wong
and Mooney, 2007).
We deﬁne an output CFG based on the deﬁnition
of this variable-free representation. To minimize
the linguistic prior, we did not distinguish the types
of primitives (e.g., nouns vs verbs); they all belong
to the same CFG category. We use a set of seed
rules of the form NT→/angbracketleftx,x/angbracketrightwherexis a token
found in a training output, and xisxor an inﬂected
form ofxfound in a training input (e.g., for x=
“sleep”, we add NT→/angbracketleftsleep,sleep/angbracketrightandNT→
/angbracketleftslept,sleep/angbracketright). These/angbracketleftx,x/angbracketrightpairs were identiﬁed
by running the IBM I alignment model (Brown
et al., 1993) on the training data.
GeoQuery We use the same variant of
FunQL (Kate et al., 2005) as Shaw et al. (2021),
with entities replaced with placeholder values.
We generate new length, template, and TMCD
splits following the methodology of Shaw et al.
(2021), so that we could evaluate our method on
dev sets, which the original splits did not include.
Speciﬁcally, for the length split, we randomly
split the test set of the original length split into
a dev set of 110 examples and a test set of 330
examples. To reduce variance, we created 3 new
template and TMCD splits with different random
seeds, with (approximately, in the case of template
splits) 440 training examples, and 440 examples
that are then randomly split into a 110 dev set and
330 test set. For the TMCD splits, we changed
the atom constraint slightly, based on the error
analysis in Shaw et al. (2021) which found that
a disproportionate amount of the errors on the
TMCD test set were in cases where an “atom”
was seen in only a single context during training.
To create a fairer evaluation of compositional
generalization, we strengthen the atom constraint
such that every atom in the test set must be seen at
least 2 times in the training set. Additionally, as
several function symbols in FunQL can be used
with and without arguments, and these usages4354
are semantically quite different, we treat function
symbols used with different numbers of arguments
as different atoms.
We deﬁne an output CFG based on the deﬁnition
of the FunQL operators and the primitive types in
the geobase database. We use a set of seed rules of
the formNT→/angbracketleftx,x/angbracketrightwherexoccurs in both the
input and output of a training example.
SMCalFlow-CS To construct SMCalFlow-CS,
Yin et al. (2021) ﬁltered out examples that require
conversational context. We heuristically ﬁltered
out 22 more training examples whose programs
contain string literals that are not in the inputs.
We use the original LISP programs provided
with the dataset as the output representation. We
extract seed rules for string literals and numbers
that are copied from inputs to outputs, such as per-son names and meeting subjects. We add 5 seed
rules with a single non-terminal on the input side
that enable “hallucinating” various program frag-
ments. We construct an output CFG based on the
bracketing of LISP programs and a mapping of
argument slots to nonterminals.
B Modeling Details
B.1 QCFG Background and Notation
Synchronous context-free grammars (SCFGs) have
been used to model the hierarchical mapping be-
tween pairs of strings in areas such as compiler the-
ory (Aho and Ullman, 1972) and multiple natural
language tasks, e.g., machine translation (Chiang,
2007) and semantic parsing (Wong and Mooney,
2006; Andreas et al., 2013). SCFGs can be viewed
as an extension of context-free grammars (CFGs)4355
thatsynchronously generate strings in what we will
refer to as an input and output language. We write
SCFG rules as NT→/angbracketleftα,β/angbracketright, whereNT is a non-
terminal symbol, and αandβare strings of non-
terminal and terminal symbols.
An SCFG rule can be viewed as two CFG rules,
NT→αandNT→β, with a pairing between
the occurrences of non-terminal symbols in αand
β. This pairing is indicated by assigning each non-
terminal inαandβan index∈N. Non-terminals
sharing the same index are called linked . Following
convention, we denote the index for a non-terminal
using a boxed subscript, e.g. NT.
B.2 Model Parameterization Details
Here we provide the complete deﬁnition of the
p(r|r,i)andp(s|r,i)terms introduced in
§ 3.1.2:
p(s|r,i) =e
/summationtexte(8)
p(r|s) =e
/summationtexte(9)
where the θs are scalar parameters. For
SMCalFlow-CS, where the number of induced
rules is large, we approximate the denominator in
Eq. 9 by only considering rules used in derivations
in the same batch during training.
B.3 Grammar Induction Algorithm Details
In this section we describe the detailed implementa-
tion of the grammar induction algorithm introduced
in § 3.1.4.At each step, we process each rule rinGin
parallel. First, using a variant of the CKY algo-
rithm, we check if we can just remove rwithout
violating the invariant that all examples in Dcan
be derived byG. If so, we simply remove the rule
ras this will always decrease L(G). Otherwise,
we determine a set of candidate actions ,A, where
an actiona∈Aconsists of a rule to add, r, and
a set of rules to remove, R . We determine
Ausing the UNIFY operation described in Fig-
ure 4. Speciﬁcally, we consider each rule returned
byUNIFY(r,r)(whereris any other rule in G)
as a potential rule to add, r. The corresponding
setR then consists of rand any other rule
that we determine can be removed if ris added,
without violating the above invariant.
If a CFG deﬁning valid outputs is provided for
the task, we ensure that the output string in rcan
be generated by the given CFG, for some replace-
ment of the nonterminal symbols with nonterminals
from the output CFG, using a variant of the CKY
algorithm.
Given these candidate actions, A, we select:
a= arg max−L(EXEC(G,a))
where EXEC(G,a)is an operation that returns a
new set of rules (G∪r)\R .
We then aggregate over the actions, a, se-
lected for each rule in G, choosing an action only if
it improves the objective. Each action is executed
by settingG← EXEC(G,a). The algorithm
completes if no action was selected or if we reach
a conﬁgurable number of steps.
We optionally partition the dataset into a con-
ﬁgurable number of equally sized partitions based
on example length. We then run the algorithm
sequentially on each partition, starting with the par-
tition containing the shortest examples. During
initialization, we only add rules for examples in the
ﬁrst partition. We then add rules corresponding to
examples in the next partition once the algorithm
completes on the current partition.
B.4 Hyperparameters
We performed a limited amount of hyperparameter
tuning based on performance on development sets.
As our goal is to develop models that generalize
well across multiple types of distribution shifts, we
strove to use the same hyperparameters for each
split within a dataset.4356
Grammar Induction For grammar induction,
we selected various conﬁguration options by in-
specting the data for each dataset, such as the maxi-
mum number of nonterminals in a rule and whether
we allow repeated nonterminal indexes. We evalu-
ated several conﬁgurations for kandkin Eq. 7
and the relative cost of terminal vs. nonterminal
symbols referenced in Eq. 6, which we will refer
to here ask, during the development of our algo-
rithm. The selected hyperparameters are listed in
Table 6.
Parameteric Model For the CSL parame-
teric model, we selected a learning rate from
[0.01,0.05,0.1]. We selected a number of context
states|S|from [32,64], except for SCAN where
we analyzed a larger number of context states on
the MCD splits, as discussed in Appendix C.2. For
SCAN we selected |S|= 2 for all splits, except
for MCD1 and MCD3, where we found |S|= 4
to give more consistent performance on the dev
sets. This was the only case where we used differ-
ent hyperparameters for different splits of the same
dataset.
Sampling We sampled 100,000synthetic exam-
ples for all datasets. We provide some analysis of
the effect of this in Appendix C.7. For COGS, we
biased sampling to increase the number of longer
examples (as discussed in § 3.2) by setting the bias
δ= 6, and otherwise used δ= 0. We limited the
maximum recursion depth to 5 for SCAN, 10 for
SMCalFlow, and 20 for GeoQuery and COGS.
T5 Fine-Tuning We started with the same con-
ﬁguration for ﬁne-tuning T5 as Shaw et al.
(2021). We similarly selected a learning rate from
[1e,1e,1e]for each dataset. We use learn-
ing rate of 1efor SCAN and SMCalFlow and
1efor GeoQuery and COGS.
B.5 Training Details
We train the CSL model on 8 V100 GPUs, which
takes less than 1.5 hours for all splits. We ﬁne-tuneT5 on 32 Cloud TPU v3 coresfor 10,000 steps,
which takes less than 6 hours for all splits.
C Additional Analysis
C.1 Performance Breakdown
We extend the performance breakdown analysis of
§4.4 to all splits, with results reported in Table 7.
C.2 Varying Context Sensitivity
Varying the number of context states |S|can vary
the degree of context sensitivity in the CSL model.
This can be important because we want our model
to be able to accurately model p(y|x), which we
assume is shared between the source and target
distributions, but we also want to sample new in-
putsxthat may have low probability under the
source distribution due to the novel compositions
they contain.
As a step towards understanding the trade-offs re-
lated to context sensitivity, we compute the average
logp(x,y)andlogp(y|x)according to CSL mod-
els with different number of context clusters |S|.
The results are reported in Table 8. The results also
let us compare logp(x) = logp(x,y)−logp(y|x).
A constraint on the number of context states
|S|is in some ways similar to a constraint on the
number of nonterminal symbols in a conventional
SCFG. Notably, for SCAN, writing a SCFG that
unambiguously maps inputs to outputs requires
2 unique nonterminal symbols, and we observe
that, similarly,|S|≥ 2is required to reach 100%
accuracy on the dev set. We also observe that while
the models with larger |S|ﬁt the training set better,
the log likelihood of the dev sets is highest with
|S|in the range between 2 and 4, indicating that
the optimal place on the tradeoff curve is not at the
extremes. We also note that there is some variance
across the different splits for the optimal number of
types, with some values leading to less than optimal
modeling of p(y|x).
It is also worth noting that, regardless of the
number of context states, the structural conditional
independence assumptions in our model can be
too strong, harming the accuracy of modeling
p(y|x). For example, consider a rule for coordi-
nation, NT→/angbracketleftNTand NT,NT∧NT/angbracketright.
In our model, we cannot condition the expansion
ofNTon the corresponding expansion of NT
or the parent context in which the coordination rule4357
is applied, in order to capture notions of type agree-
ment. Such limitations are similar to the limitations
of conventional PCFGs to sufﬁciently model struc-
tural dependencies for syntactic parsing (Klein and
Manning, 2003).
In general, better understanding and optimizing
the trade-offs related to context sensitivity for com-
positional generalization is an important direction
for future work.
C.3 Comparing CSL and NQG
CSL and NQG of Shaw et al. (2021) vary across
several dimensions, as the two systems use dif-
ferent grammar induction algorithms and different
model parameterizations. Here we compare the twoapproaches across both dimensions independently.
Grammar Induction As discussed in §3.1.4, the
largest set of changes to the CSL algorithm from
that of NQG were to improve the scalabilty of the
induction algorithm, as both algorithms scale super-
linearly in both dataset size and the length of input
and output strings. The runtime of grammar induc-
tion on the GeoQuery standard split on a standard
workstation CPU is around 15 minutes for NQG,
and<1minute for CSL. More importantly, we
did not ﬁnd it feasible to run NQG for SMCalFlow-
CS, while CSL enables grammar induction to be
completed within 10 hours with parallelization.
CSL also supports QCFG rules with >2nonter-4358
minals while NQG does not. We found allowing
up to 4nonterminals can improve the coverage
of induced grammars for COGS and SMCalFlow-
CS, with some example rules shown in Table 5 in
Appendix A. Notably, for COGS, the induction al-
gorithm of NQG induced a grammar that can only
derive 64.9%of the test set.
Model Parameterization We compare the per-
formance of CSL’s simple generative model with
that of the span-based model of NQG which uses a
BERT-Base encoder in Table 9. Both models use
thesame grammar induced via the CSL algorithm.
Overall, the models perform comparably, despite
CSL having far fewer parameters (e.g. for Geo-
Query, CSL has only 51,200 parameterswhile
NQG has over 110M parameters as it includes a
BERT-Base encoder), not leveraging pre-trained
neural networks, and being a generative model to
support sampling (in contrast to NQG which is a
discriminative model). Incorporating pre-trained
neural components into a model such as CSL could
be a promising future direction.
For SMCalFlow-CS, given a grammar induced
by CSL, we found that it can be computationally
infeasible to train a discriminative NQG model, due
to the need to compute the partition function which
sums over all possible derivations of the input. As a
generative model, CSL avoids the need to compute
a partition function during training.
C.4 Comparison with GECA
Hyperparameters For SCAN, the reported re-
sults in Table 1 use a window size of 1. Using
a window size of 2 improves performance on the
MCD split (24.9% vs. 22.8%) but hurts perfor-
mance on the other splits. For GeoQuery, we used
the default window size of 4 for the GeoQuery ex-
periments. We attempted to run GECA on COGS
and SMCalFlow-CS also using the default hyperpa-
rameters and did not ﬁnd it to be computationally
tractable. The algorithm’s iteration over templates
and fragments can become prohibitive for larger-
scale datasets.
Analysis From Table 1 we see that augmenting
the training data using CSL outperforms GECA
across both synthetic and non-synthetic evaluations.
GECA relies on the simple assumption that frag-
ments are interchangeable if they appear in the
same context. It is restricted by a pre-deﬁned win-
dow size for fragments, and does not support recur-
sion. Figure 5 compares differences in the sets of
derivable synthetic examples for a notional set of
training examples. In this case, CSL can derive a
much larger set of recombinations by inducing the
ruleNT→/angbracketleftNTandNT,NTNT/angbracketright, which
can be applied recursively.
C.5 Limitations of QCFGs
The mapping from inputs to outputs in SCAN,
COGS, and GeoQuery are all well supported by
QCFGs. However, grammars were used to gener-
ate the data for SCAN and COGS, so this is per-
haps not surprising. While GeoQuery inputs were
written by humans, the distribution of queries in
the dataset is inﬂuenced by the capabilities of the
underlying execution engine based on logic pro-
gramming; the dataset has a large number of nested
noun phrases in inputs that map directly to nested4359FunQL clauses in outputs.
SMCalFlow The induced grammars have rela-
tively low coverage on SMCalFlow, as shown by
%in Table 7, although they are still sufﬁcient
to improve the performance of T5. One reason for
the low coverage is that inputs in SMCalFlow often
reference speciﬁc names, locations, and meeting
subjects, such as “setup up a sales meeting with
Sam and his manager” where “sales meeting” and
“Sam” must be copied to the output program as
string literals. Sequence-to-sequence models with
copy mechanisms or shared input-output vocabu-
laries can handle such copying, but the QCFGs
induced by our method do not support generaliza-
tion to such novel tokens. Extending the method
to support such string copying could signiﬁcantly
improve coverage.
Another reason for the low coverage is that the
mismatch between the nesting of prepositional
phrases in the input (e.g., “at NT” and “with NT”)
and the corresponding clauses in the output pro-
gram tree makes it difﬁcult to induce QCFG rules
that enable recombination of different prepositional
phrases in different contexts.
The induced QCFGs are also limited in other
cases, such as their inability to “distribute” over
groupings correctly. Since the training data
only contains example such as /angbracketleftJennifer and her
boss, (person = “Jennifer”) (FindManager (per-
son= “Jennifer”))/angbracketright, the induced rule NT →/angbracketleftNT
and her boss, (person = “NT”) (FindManager (per-
son= “NT”))/angbracketright, the induced grammar cannot cor-
rectly generate test examples like /angbracketleftJennifer and Elli
and their bosses, (person = “Jennifer”) (person =
“Elli”) (FindManager (person= “Jennifer”)) ( Find-
Manager (person= “Elli”)) /angbracketright.
CFQ We also evaluated the feasibility of our ap-
proach to improve T5 performance on CFQ (Key-
sers et al., 2020), a popular synthetic dataset for
evaluating compositional generalization. We found
it was challenging to induce QCFGs with rea-
sonable coverage for CFQ. First, the SPARQL
queries in CFQ contain variables, which are not
well supported by QCFGs (Wong and Mooney,
2007). Additionally, the mapping from queries to
SPARQL in CFQ requires notions of commutativ-
ity (both “M0 edited and directed M1” and “M0
directed and edited M1” will be mapped to “M0
ns:ﬁlm.director.ﬁlm M1 . M0 ns:ﬁlm.editor.ﬁlm
M1”) and distributivity (edited in “edited M1 andM2” will appear twice in “?x0 ns:ﬁlm.editor.ﬁlm
M1 . ?x0 ns:ﬁlm.editor.ﬁlm M2”) that are also not
well supported by QCFGs. Such limitations can
potentially be partially overcome by desigining in-
termediate representations for CFQ (Furrer et al.,
2020; Herzig et al., 2021), but a complete solution
likely requires an extension to the class of allow-
able rules inGbeyond those a QCFG formalism
supports, such as better support for variables (Wong
and Mooney, 2007) and the ability to apply rewrit-
ing rules to generated output strings.
C.6 Sampling Temperature
In this section we study the impact of the paramet-
ric model on data augmentation. To do this, we
consider varying the sampling temperature, applied
toθprior to normalization. We compare the ac-
curacy of T5+CSL-Aug. for different temperatures
for SMCalFlow-CS, and also with sampling from a
uniform distribution rather than using the CSL pa-
rameteric model for all splits, which can be viewed
as using temperature =∞.
Table 10 shows that using the parameteric model
outperforms uniform sampling by a large margin
on most splits. However, for SMCalFlow-CS, in-
creasing the sampling temperature can lead to im-
proved performance. To help understand why in-
creasing temperature improves performance on the
SMCalFlow-CS cross-domain splits, we computed
the number of single-domain examples and cross-
domain examples in the 100,000 sampled synthetic
examples. Sampling from uniform distribution gen-
erates on average 17,764 cross-domain examples
comparing with sampling from CSL which gener-
ates on average 1,114 cross-domain examples. The
signiﬁcant larger number of synthetic cross-domain
examples might explain the improvement on cross-
domain performance when increasing sampling
temperature, especially on the 8-shot split, given
the small number of cross-domain examples in the
original training data.
C.7 Semi-Supervised Learning
If we have unlabeled data from our target distribu-
tion, consisting of inputs only, we can incorporate
this data when training our generative model in a
straightforward way. Here we propose a new ex-
periment to evaluate a method for semi-supervised
learning that leverages such unlabeled examples.
We assume that the unlabeled inputs from the de-
velopment set are available during training.4360
Experiment Setting In this setting, conceptually,
when optimizing θ, we want to maximize both the
likelihood of p(x,y)forDand the marginal like-
lihoodp(x)for the unlabeled data. As the latter
requires marginalizing over derivations for xand
all possible outputs, and this can be a large set to
sum over, we approximate this by ﬁrst labeling
the unlabeled data following the inference proce-
dure described in §3.1.5 using the generative model
trained only onD, which can be interpreted as a
hard-EM approach. During this process, we dis-
card any unlabeled that cannot be derived given G.
We then re-train the generative model on both sets
of data following the standard procedure, duplicat-
ing the “unlabeled” data a conﬁgurable number of
times to achieve the desired ratio to the original
labeled data.
We evaluate our CSL + T5-Aug. in this setting
using unlabeled data from the development set. We
use the SCAN MCD splits as they have dev sets
available. We also evaluate performance on the
GeoQuery splits. We compare the performance
with and without unlabeled data using 1,000 and
100,000 synthetic examples.
Results Results are reported in Table 11. In-
corporating unlabeled examples leads to improve-
ments when sampling only 1,000 examples, but
leads to minimal improvements when sampling
100,000 examples. We did not ﬁnd positive results
based on initial experiments for SMCalFlow-CS,
likely due to the low coverage of the induced gram-
mars on the target examples (see Table 7), as the
method we evaluated cannot leverage unlabeled
examples that are not covered by the induced gram-
mar.
C.8 GeoQuery Variance
The variance of T5+CSL-Aug. for GeoQuery is
reported in Table 12.
C.9 SMCalFlow-CS Error Analysis
We sampled 20 prediction errors for T5+CSL-Aug.
from single-domain and cross-domain development
sets respectively. We found a large number of er-
rors are due to ambiguous and inconsistent anno-
tations. Table 13 shows some examples of such
errors. First, the subject string is determined incon-
sistently for training and testing examples. Second,
the same source can be mapped to different tar-
gets which express the same meaning. Third, some
examples require additional context to generate
the correct output. Among the errors we sample,
around 60% of single-domain errors and around
35% of cross-domain errors fall into these three
types. In addition, for the cross-domain examples,
T5+CSL-Aug. sometimes struggles with nesting
programs in a correct way when examples require
querying an org chart for more than one people as
discussed in Appendix C.5.43614362