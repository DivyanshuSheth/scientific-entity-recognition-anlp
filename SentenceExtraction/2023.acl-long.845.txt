
Julia MendelsohnRonan Le BrasYejin ChoiMaarten Sap
/envel⌢pejuliame@umich.edu /gl⌢bedogwhistles.allen.ai
Abstract
Warning: content in this paper may be upset-
ting or offensive to some readers.
Dogwhistles are coded expressions that simul-
taneously convey one meaning to a broad audi-
ence and a second one, often hateful or provoca-
tive, to a narrow in-group; they are deployed
to evade both political repercussions and algo-
rithmic content moderation. For example, in
the sentence “ we need to end the cosmopolitan
experiment ,” the word “ cosmopolitan ” likely
means “ worldly ” to many, but secretly means
“Jewish ” to a select few. We present the first
large-scale computational investigation of dog-
whistles. We develop a typology of dogwhis-
tles, curate the largest-to-date glossary of over
300 dogwhistles with rich contextual informa-
tion and examples, and analyze their usage in
historical U.S. politicians’ speeches. We then
assess whether a large language model (GPT-
3) can identify dogwhistles and their mean-
ings, and find that GPT-3’s performance varies
widely across types of dogwhistles and targeted
groups. Finally, we show that harmful content
containing dogwhistles avoids toxicity detec-
tion, highlighting online risks of such coded
language. This work sheds light on the theo-
retical and applied importance of dogwhistles
in both NLP and computational social science,
and provides resources for future research in
modeling dogwhistles and mitigating their on-
line harms.
1 IntroductionFigure 1: Schematic of how dogwhistles work, based
on Henderson and McCready (2018) with the exam-
ple of cosmopolitan . First, a speaker simultaneously
communicates the dogwhistle message and their per-
sona (identity). The in-group recovers both the message
content and speaker persona, enabling them to arrive at
the coded meaning (e.g. Jewish ). The out-group only
recognizes the message’s content and thus interprets it
literally. This literal meaning also provides the speaker
with plausible deniability; if confronted, the speaker can
claim that they solely intended the literal meaning.
Cosmopolitan andinner city are examples of
dogwhistles, expressions that “send one message
to an out-group and a second (often taboo, contro-
versial, or inflammatory) message to an in-group”
(Henderson and McCready, 2018). Many listen-
ers would believe that Hawley is simply criticizing
well-traveled or worldly people, but others recog-
nize it as an attack on the Jewish people. Similarly,
many assume that Ryan is discussing issues within
a geographic location, but others hear a pernicious
stereotype of Black men as lazy. Crucially, Hawley
and Ryan can avoid alienating the out-group by
maintaining plausible deniability : they never ex-
plicitly say “Jewish” or “Black”, so they can reject
accusations of racism (Haney-López, 2014).
Because dogwhistles can bolster support for par-15162ticular policies or politicians among the in-group
while avoiding social or political backlash from
the out-group, they are a powerful mechanism of
political influence (Mendelberg, 2001; Goodin and
Saward, 2005). For example, racist dogwhistles
such as states’ rights andlaw and order were part
of the post-Civil Rights Republican Southern Strat-
egy to appeal to white Southerners, a historically
Democratic bloc (Haney-López, 2014). Despite po-
larization and technology that enables message tar-
geting to different audiences, dogwhistles are still
widely used by politicians (Haney-López, 2014;
Tilley et al., 2020) and civilians in online conversa-
tions (Bhat and Klein, 2020; Åkerlund, 2021).
Beyond political science, research on dogwhis-
tles is urgent and essential for NLP, but they remain
a challenge to study. Dogwhistles are actively and
intentionally deployed to evade automated content
moderation, especially hate speech detection sys-
tems (Magu et al., 2017). They may also have harm-
ful unseen impacts in other NLP systems by infil-
trating data used for pretraining language models.
However, researchers face many difficulties. First,
unless they are a part of the in-group, researchers
may be completely unaware of a dogwhistle’s ex-
istence. Second, dogwhistles’ meanings cannot
be determined by form alone, unlike most overt
hateful or toxic language. Rather, their interpre-
tation relies on complex interplay of different fac-
tors (context, personae, content, audience identities,
etc.; Khoo, 2017; Henderson and McCready, 2018,
2019; Lee and Kosse, 2020), as illustrated in Fig-
ure 1. Third, since their power is derived from
the differences between in-group and out-group in-
terpretations, dogwhistles continuously evolve in
order to avoid being noticed by the out-group.
We establish foundations for large-scale compu-
tational study of dogwhistles by developing theory ,
providing resources, and empirically analyzing
dogwhistles in several NLP systems. Prior work
largely focuses on underlying mechanisms or polit-
ical effects of dogwhistle communication (Albert-
son, 2015; Henderson and McCready, 2018) and
typically considers a very small number of dog-
whistles (often just one). To aid larger-scale efforts,
we first create a new taxonomy that highlights both
the systematicity and wide variation in kinds of
dogwhistles (§2.1). This taxonomy characterizes
dogwhistles based on their covert meanings, style
and register, and the personae signaled by their
users. We then compile a glossary of 340 dogwhis-tles, each of which is labeled with our taxonomy,
rich contextual information, explanations, and real-
world examples with source links (§2.2-2.3). As
this glossary is the first of its kind, we highlight
its value with a case study of racial dogwhistles in
historical U.S. Congressional Speeches (§3).
We then apply our taxonomy and glossary to
investigate how dogwhistles interact with existing
NLP systems (§4). Specifically, we evaluate the
ability of large language models (i.e. GPT-3) to
retrieve potential dogwhistles and identify their
covert meanings. We find that GPT-3 has a lim-
ited capacity to recognize dogwhistles, and perfor-
mance varies widely based on taxonomic features
and prompt constructions; for example, GPT-3 is
much worse at recognizing transphobic dogwhis-
tles than racist ones. Finally, we show that hateful
messages with standard group labels (e.g. Jewish )
replaced with dogwhistles (e.g. cosmopolitan ) are
consistently rated as far less toxic by a commer-
cially deployed toxicity detection system (Perspec-
tive API), and such vulnerabilities can exacerbate
online harms against marginalized groups (§5).
This work highlights the significance of dog-
whistles for NLP and computational social sci-
ence, and offers resources for further research in
recognizing dogwhistles and reducing their harm-
ful impacts. Our glossary, code, results, GPT-
3 outputs, and a form for adding new dogwhis-
tles to our glossary are all available at: https:
//dogwhistles.allen.ai .
2 Curating a dogwhistle glossary
2.1 Taxonomy
Based on prior work and our own investigations,
we craft a new taxonomy (Figure 2). We categorize
dogwhistles by register, type, and persona.
Register We label all dogwhistles as either part
of a formal/offline orinformal/online register.
Formal/offline dogwhistles originated in offline
contexts or are likely to appear in statements by
mainstream political elites (e.g. family values ). The
informal/online register includes dogwhistles that
originated on the internet and are unlikely to be
used in political speech (e.g. cuckservative ).
Type I Henderson and McCready (2018) distin-
guish dogwhistles into two types: Type I dogwhis-
tles covertly signal the speaker’s persona but do
not alter the implicatures of the message itself,
while Type II dogwhistles additionally alter the15163
message’s implied meaning. We extend this typol-
ogy to highlight the wide variety of dogwhistles,
which has important consequences for building a
theory of dogwhistles as well as future computa-
tional modeling. We identify three subcategories
of “only persona-signaling” (Type I) dogwhistles:
symbols (including emojis, abbreviations, and im-
agery), self-referential terms for members of the
in-group, and dogwhistles that require specialized
knowledge from a shared in-group culture .
Type II Dogwhistles with an “added message
meaning” (Type II) tend to fall into two subcate-
gories: they name a concept or serve as a substitute
for a target group label. We further divide con-
cepts into policies (titles for initiatives with covert
implications, such as law and order ),values that
the in-group purports to uphold, expressions whose
covert meanings are grounded in in-group humor ,
andother concepts , which are often coded names
for entities that are not group labels (e.g. the New
World Order conspiracy theory is antisemitic but
does not name or describe Jewish people).
Dogwhistles serve as target group labels in three
ways. Many are stereotype-based, whose interpre-
tations rely on pre-existing associations between
the dogwhistle and target group; we separate these
intostereotype-based target group labels , which
directly name the target group (e.g. cosmopoli-
tan), while stereotype-based descriptors are less
direct but still refer to the target group (e.g. inner-
city). Others have an arbitrary or phonetic rela-
tionship to the group label; these are commonly
used to evade content moderation, such as “Opera-
tion Google” terms invented by white supremacists
on 4chan to replace various slurs (Magu et al.,
2017; Bhat and Klein, 2020). The final subcat-
egory, Bogeyman , includes names of people orinstitutions taken to represent the target group (e.g.
George Soros ↔Jewish , orWillie Horton ↔Black ).
Persona Persona refers to the in-group identity
signalled by the dogwhistle. Figure 2 lists some per-
sonae, but this is an open class with many potential
in-groups. There is considerable overlap in mem-
bership of listed in-groups (e.g. white supremacists
are often antisemitic), so we label persona based
directly on explanations from sources referenced in
our glossary (as described in 2.2). Drawing upon
third-wave sociolinguistics, personae are not static
labels or stereotypes; rather, people actively con-
struct and communicate personae through linguistic
resources, such as dogwhistles (Eckert, 2008).
2.2 Gathering dogwhistles
We draw from academic literature, media cover-
age, blogs, and community-sourced wikis about
dogwhistles, implicit appeals, and coded language.
Since academic literature tends to focus on a small
set of examples, we expanded our search to media
coverage that identifies dogwhistles in recent polit-
ical campaigns and speeches (e.g. Burack, 2020)
or attempts to expose code words in hateful on-
line communities (e.g. Caffier, 2017). During our
search, we found several community-sourced wikis
that provided numerous examples of dogwhistles,
particularly the RationalWiki “Alt-right glossary”,
“TERF glossary”, and “Code word” pages.
2.3 Glossary contents
Our glossary contains 340 English-language dog-
whistles and over 1,000 surface forms (morpho-
logical variants and closely-related terms), mostly
from the U.S. context. Each dogwhistle is labeled15164
with its register, type, and signaled persona, an ex-
planation from a linked source, and at least one
example with linguistic, speaker, situational, and
temporal context included, as well as a link to the
example text. Table 1 shows one glossary entry for
the transphobic dogwhistle sex-based rights .
Antisemitic, transphobic, and racist (mostly anti-
Black but sometimes generally against people of
color) dogwhistles are the most common, with over
70 entries for each persona. The glossary includes
dogwhistles with other personae, such as homo-
phobic, anti-Latinx, Islamophobic, anti-vax, and
religious. See Table A.1 in the Appendix for glos-
sary statistics across register, type, and persona.
Because dogwhistles continuously evolve, we in-
tend for this resource to be a living glossary and
invite the public to submit new entries or examples.
3 Case study: racial dogwhistles in
historical U.S. Congressional speeches
We showcase the usefulness of our glossary, with
a diachronic case study of racial dogwhistles in
politicians’ speeches from the U.S. Congressional
Record (Gentzkow et al., 2019; Card et al., 2022) to
analyze the frequency of speeches containing racist
dogwhistles from 1920-2020. For this case study,
we simply identify glossary terms based on regular
expressions and do not distinguish between covert
and literal meanings of the same expressions. We
also measure how ideologies of speakers using dog-
whistles changed over time using DW-NOMINATE
(Poole and Rosenthal, 1985), a scaling procedurethat places politicians on a two dimensional map
based on roll call voting records, such that ideo-
logically similar politicians are located near each
other (Carroll et al., 2009; Lewis et al., 2023). We
consider the first dimension of DW-NOMINATE,
which corresponds to a liberal-conservative axis.
As shown in Figure 3, dogwhistle use began
to increase during the Civil Rights Era, following
the 1954 Brown vs. Board of Education Supreme
Court decision mandating racial integration of pub-
lic schools. This aligns with qualitative accounts of
the Republican Southern Strategy: because explicit
racism was no longer acceptable, politicians turned
to dogwhistles to make the same appeals implicitly,
particularly aiming to gain the support of white
voters in the Southern United States (Mendelberg,
2001). Their frequency continued to increase from
the 1970s through the 1990s, paralleling Haney-
López (2014)’s account of dogwhistles during the
Nixon, Reagan, Bush Sr., and Clinton presiden-
cies. Since the 1990s, the frequency of racial dog-
whistles has fluctuated but remained high. Like
Haney-López (2014), we qualitatively observe that
the dogwhistles invoked post-9/11 have shifted to-
wards being more Islamophobic and anti-Latinx
rather than exclusively anti-Black. We caution that
this case study and Figure 3 do not make novel
claims; rather, our goal is to show that even a naive
application of our glossary illustrates qualitatively
well-established historical patterns in U.S. politics.
Figure 4 shows how the average ideologies of
speakers who use particular dogwhistles ( property
rights ,thug,welfare reform ,hardworking Amer-
icans , and Willie Horton ) have shifted over time,
and reveals interesting insights into the evolution
and lifecycle of dogwhistles. Most racial dogwhis-
tles in the U.S. Congressional Speeches have be-
come increasingly associated with more conserva-
tive speakers over time. However, the inflection
point when speaker ideologies shift varies across
dogwhistles, suggesting that they emerged as dog-
whistles at different points. For example, property
rights became increasingly associated with more
conservative speakers since the 1960s, while the
average ideology of speakers using welfare reform15165
did not change until the 1990s.
Willie Horton presents an interesting example. In
his 1988 presidential campaign, George Bush ran a
television advertisement featuring Willie Horton, a
Black man convicted of rape and murder while on
prison furlough (Mendelberg, 2001). The ad was so
powerful among white voters that it propelled Bush
to victory, but shortly afterwards was explicitly
called out as racist (Haney-López, 2014). We see
this pattern in Figure 4: in 1988, Willie Horton was
associated with extremely conservative speakers,
but quickly became more liberal, and Willie Horton
no longer functioned as a successful dogwhistle.
4 Recognition of dogwhistles in GPT-3
We conduct two experiments to assess if a large lan-
guage model, GPT-3 (Brown et al., 2020), can rec-
ognize dogwhistles. First, we interrogate whether
GPT-3 is able to identify covert meanings of dog-
whistles from our glossary, an ability that would
be instrumental in understanding the breadth of on-
line bigotry. Second, we measure GPT-3’s ability
tosurface dogwhistles, motivated by the fact that
dogwhistles are often intentionally obscured from
researchers which makes it impossible to ensure
that a manual search is complete or comprehensive.
Since GPT-3 is trained on large portions of internet
data, it may be able to reveal a more comprehen-
sive and diverse set of dogwhistles than manual
efforts. Finally, we present a small manual analysis
ofin-context dogwhistle recognition using our
glossary examples and GPT-4 as a proof of concept
for future work.
4.1 Identifying covert meanings
To gauge which types of in-group language GPT-3
might have seen during training, we examine if it
is able to identify dogwhistles’ covert meanings.
Experimental setup We create prompts that vary
in (1) which of the five definitions of dogwhistles
is provided, if any, and (2) if the prompt includes
a secret cue. For each input term, we construct
12 prompts (6 definitions including none, and each
with or without a secret cue). Consider the example
below, with a definition, secret cue, dogwhistle and
GPT-3’s response . Prompts with no secret clue
simply exclude the word secretly . We generate
responses for each dogwhistle in the glossary and
consider multiple forms for some, such as actual
emojis as well as descriptions. We thus test 480
variants with 12 prompts for each, leading to 5,760
generations.15166
Results The first author manually evaluated
whether or not each of the 5,760 GPT-3 generations
contains the covert meaning for each dogwhistle.
80.3% of dogwhistles had their covert meanings
identified in at least one generation. Overall, 56.0%
generations contained the correct covert meaning
for dogwhistles that are part of the formal/offline
register, but just 29.4% for dogwhistles in the infor-
mal/online register. We refer readers to Appendix
A.2 (Figure A.2) for more details about register-
based variation and examples of dogwhistles for
which GPT-3 performed particularly well or poorly.
The specific prompt form strongly impacts GPT-
3’s ability to generate covert meanings (Table 2).
Without a definition or secret cue, covert meanings
are identified in just 8.5% of generations. Including
both a definition and secret cue improves GPT-3’s
performance over 5-fold, with dogwhistles’ covert
meanings identified in 54.3% of generations.
We observe wide variation in GPT-3’s ability to
identify covert meanings across personae. Among
the most represented personae in our glossary
(at least 100 generations for each), GPT-3 has
the lowest recognition of transphobic dogwhis-
tles, the highest recognition of homophobic and
Islamophobic dogwhistles, with antisemitic, white
supremacist, and racist dogwhistles in the mid-
dle (Appendix Table A.3). There is also variation
in performance by dogwhistle type and the spe-
cific definition provided; we refer the reader to
Appendix A.2 and Figure A.3 for more details.
4.2 Surfacing dogwhistles
In addition to evaluating if GPT-3 can identify dog-
whistles’ covert meanings, we assess GPT-3’s abil-
ity to surface dogwhistles in text generation.
Experimental setup We construct a series of
prompts that begin with one of five definitions of
dogwhistles from prior work (Table A.2). The def-
inition is followed by a question or request for
examples (see Appendix A.1 for more prompting
details). In the following example, the definition ismarked in blue, the request in purple, and GPT-3’s
response is highlighted in yellow .
Evaluation We use our glossary as a proxy to
measure precision and recall of GPT-3’s ability to
surface dogwhistles because an exhaustive ground-
truth set of dogwhistles does not exist. We cal-
culate recall as the proportion of dogwhistles in
our glossary that were also surfaced at least once
by GPT-3. For precision, the authors manually
inspect candidates appearing in at least 4% of GPT-
3 text generations for generic ,white supremacist ,
racist ,antisemitic ,Islamophobic , and transphobic
prompt types. Because our glossary is not exhaus-
tive, this method yields conservative estimates (see
Appendix A.1 for more evaluation details).
Precision Results We find that GPT-3 does have
the ability to surface dogwhistles when prompted
to do so, but caution that such results are imperfect
and require manual verification. The most common
errors involve explicit mentions of groups in stereo-
types or conspiracy theories ( Jews are behind the
9/11 attacks ) or phrases that may accompany dog-
whistles but are not dogwhistles themselves ( I’m
not racist but... ). Precision in dogwhistle surfacing
varies across prompt types; while the average pre-
cision over all six prompt types is 66.8%, scores
range from just 50% for transphobic dogwhistle
prompts to 91.3% for generic prompts (Figure A.1).
Recall Results GPT-3 surfaced 153 of 340 dog-
whistles in our glossary (45%). We observe sig-
nificant differences by register: GPT-3 surfaced
69.4% of formal/offline dogwhistles but just 12.9%
ofinformal/online dogwhistles. Despite its ability
to generate emojis and other symbols, GPT-3 did
not surface any symbols or emojis from our glos-
sary except for the antisemitic triple parentheses
“((())) ”.
Figure 5 shows GPT-3 surfacing recall results
by both register and in-group personae. We show
results for the five most-frequent personae repre-
sented in our glossary. Recall of dogwhistles in the15167
informal/online register is low across the board. For
the formal/offline register, recall is considerably
higher although it varies widely across personae.
As with precision, GPT-3 has the lowest perfor-
mance for transphobic dogwhistles, surfacing just
44.8% of formal/offline transphobic dogwhistles.
For formal/offline antisemitic dogwhistles, recall is
considerably higher but far from perfect at 71.7%.
GPT-3 has 80.3% and 83.3% recall of racist and
white supremacist dogwhistles, respectively, and
full 100% recall of Islamophobic dogwhistles.
4.3 Identifying dogwhistles in context
Our experiments show that LLMs have some ability
to identify covert meanings of a given dogwhistle
specified in the prompt (Section 4.1) and gener-
ate dogwhistles (Section 4.2). We now consider
a natural follow-up question: can LLMs correctly
identify dogwhistles in real-world texts?
Fully addressing this question would require a
larger dataset including a variety of linguistic con-
texts per dogwhistle, documents containing no dog-
whistles, and documents with dogwhistle expres-
sions that do not carry the dogwhistle meaning.
We leave such a rigorous analysis for future work.
For now, we present a small manual analysis with
ChatGPT/GPT-4 (OpenAI, 2023), using several
real-world examples from our glossary as a proof
of concept for in-context dogwhistle recognition.
We prompt GPT-4 with the instructions below.
Appendix Table A.5 shows the exact glossary exam-
ple and GPT-4 output text for the dogwhistles cos-
mopolitan (elite) (antisemitic), inner-city (racist),#IStandWithJKRowling , (transphobic), and
did you see Kyle? (white supremacist).
Using the examples presented in Section 1, GPT-
4 correctly identifies the dogwhistles cosmopolitan
(elite) andinner city , and generates a correct ex-
planation for their covert meanings. However, the
model does not work as well for the other examples.
For the example containing #IStandWithJKRowl-
ing, GPT-4 correctly identifies that text covertly
signals transphobia through support of JK Rowling,
but does not select this hashtag as the dogwhis-
tle. On the other hand, GPT-4 correctly identifies
the dogwhistle in a tweet from JK Rowling,
and correctly relates this symbol to the women’s
suffrage movement, but does not capture the ap-
propriation of this symbol to covertly communi-
cate transphobia. Finally, GPT-4 misses both the
dogwhistle and the precise covert meaning for did
you see Kyle? (“see Kyle” sounds similar to the
Nazi slogan “Sieg Heil”); while the model still ul-
timately identifies covert white supremacy, it gen-
erates a false explanation connecting the glossary
example to this persona.
5 Dogwhistles and toxicity detection
Beyond evaluating language models’ ability to rec-
ognize dogwhistles, we seek to understand how
dogwhistles affect the decisions that NLP systems
make, and how this has downstream implications
for content moderation and online safety. We begin
to address this with a study of how dogwhistles are
handled by a widely-deployed toxic language de-
tection system, Google/Jigsaw’s Perspective API.
Perspective API scores a text between 0 and 1 for
a range of attributes (e.g. toxicity, identity attack,
profanity ), representing the estimated probability
that a reader would perceive the text to contain
that attribute. Perspective API’s models are mul-
tilingual BERT-based models distilled into single-
language convolutional neural networks for faster
inference, and are trained on annotated data from
online forums. We refer readers to the Perspective
API Model Cards for more details.15168
Experimental setup We consider 237 hateful
sentence templates from HateCheck (Röttger et al.,
2021), a test suite for bias in hate speech detec-
tion, that contain placeholders for identity terms
(group referents) in either adjectival, singular nom-
inal, or plural nominal forms. We fill filled with a
standard group label, a slur, or a dogwhistle in the
corresponding grammatical form requested by the
template. For this experiment, we consider racist
(mostly anti-Black), antisemitic, and transphobic
terms, as these personae are the most common in
our glossary (see Tables A.7 and A.8 for a sample
of sentence templates and group label terms, re-
spectively). We feed our resulting 7,665 sentences
to Perspective API to get scores for toxicity ,severe
toxicity , and identity attack .
Results Hateful sentences are rated as less toxic,
less severely toxic, and less identity-attacking when
dogwhistles are used instead of standard group la-
bels or slurs (Figure 6). This pattern holds for all
three personae (Appendix Figure A.4).
Interestingly, mean toxicity scores for slurs are
lower than for standard group labels, especially for
antisemitic slurs. We observe relatively wide varia-
tion in Perspective API’s ratings depending on the
specific choice of slur. For example, sentences con-
taining the N-word are almost always rated as more
toxic than the same sentences containing Black or
Black people . Lower toxicity ratings for other slurs,
such as the highly derogatory antisemitic K-word
may be because, similar to dogwhistles, Perspec-
tive API does not recognize that these terms referto identity groups. However, deeper analysis of
slurs is outside the scope of the current work.
6 Discussion & Conclusion
We lay the groundwork for NLP and computational
social science research on dogwhistles by develop-
ing a new taxonomy and glossary with rich contex-
tual information and examples. We demonstrate
our glossary’s utility in a case study of historical
U.S. Congressional speeches, where our quantita-
tive analysis aligns closely with historical accounts.
We further use our glossary to show that GPT-3
has some, but limited, ability to retrieve dogwhis-
tles and recognize their covert meanings. Finally,
we verify that dogwhistles readily evade Perspec-
tiveAPI’s toxicity detection. We now turn to several
implications of this work, highlighting potential fu-
ture directions across disciplines.
Dogwhistles and toxic language Dogwhistles
are closely related to other forms of subtle biases
studied in NLP, such as implicit hate speech and
symbols (Magu et al., 2017; Magu and Luo, 2018;
ElSherief et al., 2018, 2021; Qian et al., 2019;
Caselli et al., 2020; Menini et al., 2021; Arviv
et al., 2021; Botelho et al., 2021; Wiegand et al.,
2021a,b; Hartvigsen et al., 2022), microaggressions
(Breitfeller et al., 2019), dehumanization (Mendel-
sohn et al., 2020), propaganda (Da San Martino
et al., 2020), condescension (Pérez-Almendros
et al., 2020), and stereotypes (Nangia et al., 2020;
Sap et al., 2020; Nadeem et al., 2021).
However, dogwhistles are distinct from toxic lan-
guage in several important ways. First, although of-
ten implicitly abusive, they are not exclusively hate-
ful; for example, wonder-working power covertly
signals the speaker’s Evangelical Christian iden-
tity (Albertson, 2015). Second, dogwhistles are
characterized by dual meanings, wherein different
sub-audiences interpret the exact same message dif-
ferently (Henderson and McCready, 2018). Third,
dogwhistles’ true meanings are intentionally hid-
den from the out-group (Saul, 2018). Nevertheless,
because dogwhistles are often deployed specifically
to avoid hate speech detection and other content
moderation tools, NLP researchers should consider
how dogwhistles highlight a vulnerability in ex-
tant language technologies, which ultimately puts
people’s safety and well-being at risk.
We show that hateful speech using dogwhistles
evades toxicity detection, and is one way that NLP
systems (unintentionally) perpetuate harms against15169marginalized groups. This finding is not surprising,
as prior work shows that toxicity detection often
fails on subtle language (Han and Tsvetkov, 2020;
Hartvigsen et al., 2022), but underscores the need
for toxicity and hate speech detection models to
be able to flag hateful dogwhistles. One poten-
tial approach to improve such models could be to
train them to recognize dogwhistles in naturally-
occurring in-group contexts (starting with model-
ing contextual factors; Zhou et al., 2023). More
broadly, content moderation pipelines should take
context into account and consider mechanisms to
identify when a dogwhistle has potentially negative
consequences. Beyond toxicity detection, future
work ought to consider the impact of dogwhistles
in a broader range of NLP tasks, such as bias miti-
gation or story generation.
How do LLMs know about dogwhistles? Our
findings regarding GPT-3’s ability to surface and
identify dogwhistles’ covert meanings are probably
driven by the contents of the training data. GPT-3’s
training data likely includes right-wing extremist
content, as has been shown with its predecessor
GPT-2 (Gehman et al., 2020), which may result
in high performance for dogwhistles from these
in-groups. Or perhaps the model is simply memo-
rizing articles or social media posts that explicitly
call out certain expressions as dogwhistles. Fu-
ture work could evaluate if large language models
can learn dogwhistles’ covert meanings from in-
context usage alone by experimentally controlling
for whether or not these terms are explicitly ex-
posed as dogwhistles in the training data.
Moreover, we find that GPT-3’s performance
varies widely across target groups. Transphobic
dogwhistles are notably difficult for GPT-3 to sur-
face and identify. Perhaps this is because the model
is trained on fewer data from transphobic communi-
ties compared to other in-groups considered in this
work. Furthermore, transphobic dogwhistles may
be less frequent in the training data because many
have emerged relatively recently. Another reason
may be formatting: transphobic dogwhistles are of-
ten emoji-based and appear in social media screen
names and profile bios rather than in posts them-
selves. We hope that future work will investigate
the links between language models’ knowledge of
dogwhistles and training data.
Potential of LLMs for dogwhistle research Be-
yond the risks presented by current NLP technolo-
gies, we wish to highlight the potential benefits ofusing NLP to advance dogwhistle research. Even
though LLMs’ performance is likely due to vast
training data, and even then, their outputs require
manual verification, our experiments with GPT-3
demonstrate that LLMs have some ability to sur-
face dogwhistles and explain their covert meanings.
This is particularly valuable as dogwhistles are in-
tentionally hidden from out-group members, and
out-group researchers may have no other way to
access this information. There is thus a unique op-
portunity for LLMs to assist dogwhistle research,
and political content analysis more broadly.
Bridging large-scale analysis and mathematical
models Our work builds foundations for large-
scale computational analysis of dogwhistles in real-
world political discourse. We diverge from prior
quantitative dogwhistle research, which focuses
on mathematically modeling the process underly-
ing dogwhistle communication using probabilis-
tic, game-theoretic, deep learning, and network-
based approaches on simulation data (Smaldino
et al., 2018; Dénigot and Burnett, 2020; Hender-
son and McCready, 2020; Breitholtz and Cooper,
2021; Smaldino and Turner, 2021; Xu et al., 2021;
Hertzberg et al., 2022; van der Does et al., 2022).
We are optimistic about future research synthesiz-
ing these two strands of work to address many of
the challenges presented by dogwhistles. For exam-
ple, future work could use our resources along with
these mathematical models to develop systems that
can automatically detect dogwhistle usages, emer-
gence of new dogwhistles, or decline of older terms
as dogwhistles due to out-group awareness.
Implications for social science research Under-
standing dogwhistles at scale has vast implications
across disciplines, so we develop resources useful
for both NLP and social science researchers. We
provide the most comprehensive-to-date glossary
of dogwhistles and demonstrate through our case
study how this resource can be used to analyze po-
litical speeches and other corpora, such as social
media posts and newspaper articles. Dogwhistles
have mostly been studied using primarily qualita-
tive methods (Moshin, 2018; Åkerlund, 2021) and
experiments (Albertson, 2015; Wetts and Willer,
2019; Thompson and Busby, 2021), and we hope
that by facilitating quantitative content analysis,
our resources can add to dogwhistle researchers’
methodological repertoires.151707 Limitations
This work represents an initial push to bring dog-
whistles to the forefront of NLP and computational
social science research, and as such, has many lim-
itations. Our glossary is the most comprehensive
resource to date (to the best of our knowledge) but
aims to document a moving target, as dogwhistles
continuously emerge or fall out of use due to out-
group awareness. We aim to make this resource a
“living glossary” and encourage others to submit
new entries or examples. We further encourage
future research to develop models to automatically
detect the emergence of new dogwhistles.
Another major limitation in this work is that we
identify as out-group members for nearly all dog-
whistles in the glossary and have an adversarial
relationship with many of the communities stud-
ied (e.g. white supremacists). Although our work
would ideally be validated by members of the in-
groups, they have very little incentive to share this
information, as that would damage the dogwhistle’s
utility as a tool for covert in-group communication.
This work, like most prior work, is limited in that
we operationalize dogwhistles as a static binary;
we assume each term either does or does not have
a dogwhistle interpretation and is categorically in-
cluded or excluded from our glossary and analyses.
In reality, dogwhistles are far more complicated
constructs. For example, Lee and Kosse (2020)
characterize dogwhistles along two dimensions:
the size of their in-group and the degree to which
their usage is conventionalized. Other axes of vari-
ation may include the level of out-group awareness,
and the social and political risks of backlash to the
communicator if the dogwhistle interpretation is
exposed. It is even possible that audience mem-
bers who hear a dogwhistle further recirculate it
even if they themselves do not recognize the covert
meaning (Saul, 2018). We hope future work will
consider multifaceted and continuous measures of
“dogwhistleness" that account for such nuances.
Finally, the current work is limited in the scope
of dogwhistles considered: they are all in English
with the vast majority coming from the U.S. polit-
ical and cultural contexts. However, dogwhistles
are prominent across cultures (Pal et al., 2018; Åk-
erlund, 2021) and we hope that future work will
consider other languages and cultures, especially
involving researchers who have high awareness of
or expertise in non-U.S political environments.8 Ethical Implications
We caution readers about several potential ethi-
cal risks of this work. First is the risk of readers
misusing or misunderstanding our glossary. We
emphasize that dogwhistles are extremely context-
dependent, and most terms in the glossary have
benign literal meanings that may be more common
than the covert dogwhistle meanings. For example,
many entities from the financial sector have been
used as antisemitic dogwhistles (e.g. the Federal
Reserve ,bankers ) but their primary usage has no
antisemitic connotations.
Relatedly, some glossary entries include terms
that originate from the target group but were ap-
propriated by the dogwhistles’ in-group. Examples
include the appropriation of goy(a Yiddish word
for non-Jewish people) as an antisemitic in-group
signal, and baby mama (originally from African
American English) as a racist dogwhistle. As with
hate speech detection (Sap et al., 2019), there is a
risk of social bias in dogwhistle detection.
As we have discussed throughout this work, dog-
whistle researchers face a challenge with no exhaus-
tive ground truth and an unknown search space. We
anticipate our glossary being a helpful resource for
this reason, but because we also lack such exhaus-
tive ground truth, there are bound to be biases in the
representation of dogwhistles in our glossary. The
current version of the glossary may exclude groups
and thus lead to worse performance in dogwhis-
tle detection, toxic language detection, and other
downstream NLP tasks.
Our glossary also includes real-world examples
of how each dogwhistle is used. This presents
a privacy risk, which we mitigate by prioritizing
examples from public figures or examples from
anonymous social media accounts whenever possi-
ble. We do not release personal information of any
speaker who is not a well-known public figure.
Finally, we do not pursue any computational
modeling or prediction of dogwhistle usages in this
work, but see it as a natural direction for future
work. However, we caution researchers to con-
sider dual-use issues in doing so. Many people use
coded language in order to avoid censorship from
authoritarian regimes (Yang, 2016) and marginal-
ized groups may also use coded language for their
own safety (Queen, 2007). When building com-
putational models, we urge researchers to mitigate
this dual-use risk as much as possible.15171Acknowledgements
We thank Ceren Budak, Yulia Tsvetkov, and audi-
ences at Text as Data 2022 (TADA) and New Ways
of Analyzing Variation 50 (NWA V) for their helpful
feedback on an earlier version of this work. We also
thank the anonymous reviewers for their comments
and suggestions. J.M. gratefully acknowledges sup-
port from the Google PhD Fellowship.
References151721517315174A Appendix
A.1 Details for dogwhistle surfacing
We create 51 total request formulations that ask for
generic examples of dogwhistles (n=17), dogwhis-
tles that target specific social groups (n=25), and
dogwhistles that are used by certain personae/in-
groups (n=9). For each prompt, we also consider
three spelling variations of “dogwhistle”: dogwhis-
tle,dog-whistle , and dog whistle . Exact prompt
text can be found in our project repository.
To encourage GPT-3 to generate a list, we con-
clude all prompts with a newline token followed
by “1.”. All prompts were provided to a GPT-3
Instruct model ( text-davinci-002 ) with de-
fault hyperparameters except for max_tokens= 256,
temperature= 0.7, and num_outputs= 5(5 genera-
tions per prompt). The resulting texts are strings
that take the form of an enumerated list. To ag-
gregate and compare surfaced dogwhistles across
each text completion, we post-process by: split-
ting by newline characters, removing enumeration
and other punctuation, converting all outputs to
lowercase, lemmatizing each surfaced term with
SpaCy, and removing definite articles that precede
generated dogwhistles. We then aggregate over all
generations to determine how often each dogwhis-
tle is surfaced for each in-group.
In calculating precision of dogwhistle surfacing,
we mark each of the 154 candidate terms as true
positives if they appear in the glossary. Some sur-
faced dogwhistles were marked as “correct” if they
were closely related to a dogwhistle entry in our
glossary, even if the exact term did not appear. Ex-
amples include national security ,identity politics ,
the swamp ,tax relief , and patriot . However, this is
still a conservative estimate because our glossary is
not exhaustive. GPT-3 surfaces a number of terms
that potentially have dogwhistle usages but were
not covered by our glossary, and thus not included
in our precision estimates. Examples of these terms
include names of Muslim political organizations
(Hezbollah ,Hamas ,Muslim Brotherhood ) and Sec-
ond Amendment rights . Figure A.1 shows variation
in precision of dogwhistle surfacing across prompt
types (in-groups and generic prompting).
A.2 Details for identifying covert meaning
Variation across registers We identify varia-
tion in GPT-3’s ability to identify dogwhistles’
covert meanings based on prompt features, dog-15175
whistle register, and the interaction between the
two. Figure A.2 shows that including the definition
in prompts consistently improves GPT-3’s covert
meaning identification for both formal and informal
dogwhistles. However, including the secret cue has
minimal effect for informal dogwhistles, and only
leads to substantial improvement for identifying
formal dogwhistles’ covert meanings.
Variation across personae There is significant
variation in GPT-3’s performance across personae,
as can be seen in Table A.3.
Variation across dogwhistle types GPT-3’s per-
formance varies widely across dogwhistle types in
our taxonomy (§2.1; Fig. 2). GPT-3 has the lowest
performance for humor-based and arbitrary target
group label dogwhistles, and the highest perfor-
mance for representatives (Bogeymen), phonetic-
based target group labels, and policies (Table A.4).15176
Variation across dogwhistle definitions Only
19.1% of GPT-3 generations include the correctcovert meaning when prompted with no dogwhis-
tle definition. Prompting GPT-3 with any of the
five dogwhistle definitions greatly improved perfor-
mance over no definition provided, but the extent
varied, with the Merriam-Webster definition yield-
ing the lowest improvement (43.8%) and Wikipedia
yielding the highest (54.3%) (Table A.6). The boost
in performance by adding the secret cue depends
on the specific definition used; the secret cue has a
bigger effect when using the Merriam-Webster and
Albertson (2015) definitions (Figure A.3).
Where does GPT-3 perform poorly? Most un-
recognized dogwhistles are part of the informal
register, especially symbols (e.g. the transphobic
spiderweb orcherry emojis). Other unrecognized
dogwhistles include “Operation Google” terms (e.g.
Skype ,Yahoo ), more recent terms (e.g. Let’s Go
Brandon ), and several antisemitic and transphobic
dogwhistles whose covert meanings are especially
context-dependent (e.g. adult human female ,XX,
(Wikipedia) early life ,fellow white people ). Unrec-
ognized formal dogwhistles tend to be extremely
subtle and nuanced (e.g. Dred Scott as a con-
servative anti-abortion dogwhistle) or are highly-
conventionalized phrases that may be far more com-
monly used without the covert implicature (e.g. the
antisemitic dogwhistle poisoning the well ).15177
Where does GPT-3 perform well? GPT-3 read-
ily identifies Islamophobic dogwhistles (e.g. rad-
ical Islam ,Barack Hussein Obama ), many anti-
semitic conspiracy theories (e.g. Khazars ,Kalergi
Plan,Israel Lobby ), some racist dogwhistles whose
covert meanings are more widely discussed among
the out-group (e.g. inner-city ,ghetto ,thug,war
on drugs ,black-on-black crime ), some alt-right
memes (e.g. Pepe the Frog ), and conservative po-
litical rhetoric (e.g. balance the budget ,judicial
activism ,job creators ).
A.3 Details for toxicity detection
Table A.7 shows a sample of the 237 hateful sen-
tence templates used from HateCheck (Röttger
et al., 2021). We used the subset of HateCheck
templates labeled as hateful and containing a place-
holder for an identity term in either adjectival, sin-
gular nominal, or plural nominal form. We filled
in these placeholders with either a standard group
label, dogwhistle, or slur for three personae: an-
tisemitic, racist, and transphobic. Specific terms
used can be found in Table A.8.
For all personae and toxicity measures provided
by Perspective API, dogwhistles are rated as less
toxic than slurs and standard group labels (Figure
A.4. Interestingly, except for the N-word, Perspec-
tive seems to not recognize the extreme offensive-
ness of most slurs, and thus toxicity scores for
hateful sentences containing slurs are similar to or
lower than scores for the same hateful sentences
containing standard group labels.15178ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7
/squareA2. Did you discuss any potential risks of your work?
Section 8
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Sections 0 and 1
/squareA4. Have you used AI writing assistants when working on this paper?
Yes, we used ChatGPT to rephrase a single sentence from the abstract to repeat the same point at the
end of the introduction.
B/squareDid you use or create scientiﬁc artifacts?
Section 2
/squareB1. Did you cite the creators of artifacts you used?
Congressional records data (Section 3), Glossary sources (Section 2), GPT-3 (Section 4), and Hate
speech templates + models (Section 5)
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Our glossary and taxonomy will be open and available to the public (Section 2)
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Yes, in Sections 2, 5, and 8
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Data released is anonymized with no identifying information
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 2 and Section 8
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 2
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Not applicable. We did not train our own model, and just used GPT-3 with the OpenAI API15179/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Yes, section 4 and Appendix A.1 and A.2
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Yes, Sections 3-5, with additional statistics in A.1-A.3 in the appendix
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Yes, Section 4 and Appendix A.1-A.2
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.15180