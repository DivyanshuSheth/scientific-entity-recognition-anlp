
Shuhao Gu, Bojie Hu, Yang FengKey Laboratory of Intelligent Information Processing,
Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)University of Chinese Academy of SciencesTencent Minority-Mandarin Translation, Beijing, China
{gushuhao19b,fengyang}@ict.ac.cn, bojiehu@tencent.com
Abstract
This paper considers continual learning of
large-scale pretrained neural machine transla-
tion model without accessing the previous train-
ing data or introducing model separation. We
argue that the widely used regularization-based
methods, which perform multi-objective learn-
ing with an auxiliary loss, suffer from the mis-
estimate problem and cannot always achieve
a good balance between the previous and new
tasks. To solve the problem, we propose a two-
stage training method based on the local fea-
tures of the real loss. We first search low forget-
ting risk regions, where the model can retain
the performance on the previous task as the pa-
rameters are updated, to avoid the catastrophic
forgetting problem. Then we can continually
train the model within this region only with the
new training data to fit the new task. Specif-
ically, we propose two methods to search the
low forgetting risk regions, which are based
on the curvature of loss and the impacts of the
parameters on the model output, respectively.
We conduct experiments on domain adapta-
tion and more challenging language adaptation
tasks, and the experimental results show that
our method can achieve significant improve-
ments compared with several strong baselines.
1 Introduction
The current large-scale pretrained neural machine
translation (NMT) models, such as GMNMT (John-
son et al., 2017), mBART50-nn (Tang et al., 2020),
and M2M-100 (Fan et al., 2021), are generally
trained with large amounts of data from different
domains and languages, so the model can learn
good semantic representation and mapping relation-
ship at the same time. Based on such models, we
hope that they can continually acquire new knowl-
edge from new translation tasks, e.g., new domainsFigure 1: An illustration to indicate the difference be-
tween the regularization-based method and our method.
Rdenotes the task loss. θdenotes the model parame-
ters.→and←denote the parameter update direction
during continual learning. [θ, θ]denotes the low
forgetting risk regions.
and languages, while preserving the previously
learned knowledge, i.e., continual learning (CL).
However, catastrophic forgetting is the biggest chal-
lenge of continual learning, which means the model
will forget the previously learned knowledge while
learning new knowledge (Gu and Feng, 2020). One
solution to avoid forgetting is to mix the previous
and the new training data to train the model jointly.
However, considering that the training data of the
large-scale pre-trained NMT model is usually very
large, this method will bring more training con-
sumption and consume more energy. Besides, we
sometimes cannot access the previous data due to
data privacy or storage limitation.
To solve this problem, many researchers have
made their attempts. Some work avoids catas-
trophic forgetting by constructing pseudo data and
mixing them with the new task data for joint train-
ing (Kim and Rush, 2016; Ko et al., 2021; Liu et al.,17072021). However, these methods must require that
the previous and new tasks are similar, and can not
be directly applied to tasks with large differences,
such as learning a totally different new language.
Some work introduces extra task-specific param-
eters and only updates these parameters with the
new task data (Bapna and Firat, 2019; Escolano
et al., 2021). On the one hand, this will increase
the size of the model. On the other hand, the task-
specific parameters lead to model separation, which
makes the model must know which task the input
sentence belongs to, otherwise, the translation qual-
ity will degrade significantly (Aharoni and Gold-
berg, 2020). In contrast, the regularization-based
methods (Khayrallah et al., 2018; Thompson et al.,
2019) don’t have such limitations and are more
flexible in practice. These methods perform multi-
objective learning with an extra penalty item on
the parameters, which aims to approximate the real
loss on the previous task, and is usually in quadratic
form. However, as illustrated in Figure 1 (a) and
(b), the approximated loss (the red dashed line)
is convex and symmetric, but the real loss (the
red solid line) is not necessarily so in most cases,
which may lead to the under or over constraint
problems. Besides, the multi-objective learning
can only make the parameters converge to a region,
where the gradients produced by the two losses are
equal in size and opposite in direction, i.e., gradi-
ents reach balance. However, this does not guar-
antee that the values of the two losses are small in
this region, which means the model may still suffer
from the forgetting problem.
Instead of performing multi-objective learning
with the approximated loss, we propose a two-stage
continual learning method based on the local fea-
tures of the real loss around the initial parameters.
In the first stage, we aim to search the low forget-
ting risk regions as the parameter update regions,
where the performance of the previous task will
not degrade significantly when the parameters are
updated. Therefore, we can constrain the parame-
ters to such regions to avoid catastrophic forgetting.
In the second stage, the parameters will be freely
updated in the searched regions only guided by the
gradients of the new task, while the update outside
these regions will be forbidden, as illustrated in
Figure 1 (c). To achieve this, we can use some data
related to the previous task to help us find such
regions, e.g., the validation set data of the previous
translation task, which is usually small-scale andeasy to obtain. We propose two methods to search
the low forgetting risk regions. The first method is
to find out which parameters have the least impact
on the previous task loss. We use the curvature of
the loss curve as an indicator and the parameter
with small curvature indicates that the loss curve
is relatively flat, so the parameters can be updated
more greedily. The second method is based on
the impacts of parameters on the model output of
the previous task. We define an objective function,
which tries to maximize the size of update regions
as much as possible while restricting the change
of model output on the previous task, to help the
model learn the update regions automatically. We
conduct experiments on the domain adaptation and
more challenging language adaptation tasks, and
the results show that our method can achieve signif-
icant improvements compared with several strong
baselines.
Our contribution can be summarized as follows:
•We propose two methods to search the low for-
getting risk regions of neural machine trans-
lation without accessing the previous training
data.
•We conduct experiments on the domain adap-
tation task and the more challenging language
adaptation task of continual learning, and our
method can bring significant improvements.
•We prove that our method can also achieve
good performance when combined with part
of the original training data.
2 Background
In this section, we report the background knowl-
edge used in this paper: the Transformer (Vaswani
et al., 2017) model, multilingual NMT, Hessian
matrix, and Fisher information matrix.
2.1 Transformer
We denote the input sequence of symbols as x=
(x, . . . , x)and the ground-truth sequence as y=
(y, . . . , y). The transformer model is based on
the encoder-decoder architecture. The encoder is
composed of Nidentical layers. Each layer has two
sublayers. The first is a multi-head self-attention
sublayer, and the second is a fully connected feed-
forward network. Both of the sublayers are fol-
lowed by a residual connection operation and a
layer normalization operation. The input sequence1708xwill be first converted to a sequence of vectors
E= [E[x];. . .;E[x]]where E[x]is the
sum of word embedding and position embedding of
the source word x. Then, this sequence of vectors
will be fed into the encoder, and the output of the
N-th layer will be taken as source state sequences.
We denote it as H. The decoder is also composed
ofNidentical layers. In addition to the same kind
of two sublayers in each encoder layer, the cross-
attention sublayer is inserted between them, which
performs multi-head attention over the output of
the encoder. The final output of the N-th layer
gives the target hidden states S= [s;. . .;s],
where sis the hidden states of y. We can get
the predicted probability of the j-th target word
conditioned by the source sentence and the j−1
previous target words. The model is optimized by
minimizing a cross-entropy loss of the ground-truth
sequence with teacher forcing:
L=−1
J/summationdisplaylogp(y|y,x;θ),(1)
where Jis the length of the target sentence and θ
denotes the model parameters.
2.2 Multilingual Translation
The multilingual neural machine translation
(MNMT) model can translate between multiple
different languages with a single model (Johnson
et al., 2017). Following Liu et al. (2020), we add
a particular language id token at the beginning of
the source and target sentences, respectively, to
indicate the language.
2.3 Hessian and Fisher Information Matrices
The Hessian Matrix For a twice-differentiable loss
L, the Hessian matrix is the matrix of second-order
derivatives of the loss function with respect to the
weights, mathematically expressed as H=∇L.
Intuitively, its role is to express the curvature of
the loss around a given point θ. The smaller the
value, the "flatter" the loss is around the given point
θ, and vice versa. The flatter the region around θ,
the smaller the loss change when the value of θis
changed.
The Fisher Information Matrix The Fisher infor-
mation matrix Fof the model’s conditional distri-
bution Pis defined as:
F= E[∇logp(x,y)∇logp(x,y)]
(2)Intuitively, the role of the Fisher matrix is very
similar to that of the Hessian Matrix. If θis an
accurate set of parameters for the model, we can
approximate the Hessian matrix at θwith the Fisher
information matrix (Ly et al., 2017).
In practice settings, we can approximate the
Fisher information matrix by replacing the model
distribution Pwith the empirical training distri-
bution ˆQ:
ˆF= E[∇logp(x,y)∇logp(x,y)]
=1
N/summationdisplay∇logp(y|x)∇logp(y|x)
(3)
3 Method
The goal of our method is to preserve the previ-
ously learned knowledge while adapting to the new
tasks efficiently. To achieve this, we propose a
two-stage training method. In the first stage, we try
to depict the local features of the large-scale pre-
trained NMT model around its initial parameters
θ. Specifically, we want to find a region around
θwith low forgetting risk, so we can retain the
performance of the model on the previous task as
the parameters are updated. We can constrain the
parameters within this region so that the model
will not suffer from severe forgetting problem. In
the second stage, the parameters are updated com-
pletely guided by the gradients produced by the
new training data within this region, which can
mitigate the under-fitting problem on the new task.
3.1 Search Parameter Update Region
We propose two methods to search the low forget-
ting risk (LFR) regions around the initial parame-
tersθ. For the first method, we hope to find out
which parameters to update have the least impact
on the model loss of the previous task. To achieve
this, we propose to use curvature as an indicator to
help find the parameter update regions. For the sec-
ond method, we determine the LFR regions based
on the impact of parameters on the model output of
the previous task. We propose an objective function
to help the model learn the regions.
Curvature-based Method Intuitively, the cur-
vature of the loss function measures how fast the
loss changes as the parameter changes around the
initial parameters θ. Therefore, the parameters
with small curvature can be safely updated without
causing forgetting. As described in Section 2.3, the1709Hessian matrix is used to represent the curvature of
the model loss, but it is almost impossible to obtain
the exact Hessian matrix in practice. Therefore, we
use the Fisher information matrix to approximate
the Hessian matrix with Equation 3. Noting that
we cannot access the previous training data, we
use a small-scale validation set related to the previ-
ous task to compute the Fisher information matrix,
which will be described in the experimental part.
After getting the Fisher information matrix, we
fix the top ρ%parameters of each module with
large values, and then set the update regions for the
rest of the parameters as [θ, θ], which are
defined as:
θ=θ−λ∗ |θ|;
θ=θ+λ∗ |θ|,(4)
where θdenotes the parameter values of the pre-
trained NMT model, and λis a hyper-parameter to
control the size of the update region.
Output-based Method In this method, we hope
that the model can automatically learn the update
region of parameters based on the impact of param-
eters on the model output of the previous task. In-
tuitively, preserving the previously learned knowl-
edge requires that the model output on the previous
task should not change significantly after parameter
update. Meanwhile, we also hope that the update
regions should be as large as possible because the
large regions will give the model more capacity to
learn new tasks. Following the above intuition, we
can define the learning objective:
L(θ) =1
N/summationdisplayKL(p(y|x, θ)||p(y|x, θ))
−α
M/summationdisplay(θ−θ),(5)
where Ndenotes the amount of the training exam-
ple,KLdenotes the KL-divergence, αis a hyper-
parameter to control the ratio of the two terms, and
Mdenotes the total amount of the model parame-
ters. The first term of the above objective function
let the model output on the previous task stay as
close as possible to the pre-trained model, which
will discourage the parameter to change. While the
second term encourages the model parameters to
change more greedily, and maximize the size of re-
gions as much as possible. These two items can be
regarded as performing adversarial learning during
learning the parameter update regions. Similar tothe curvature-based method, we use a small-scale
validation set related to the previous task instead of
the data of the new task as the training data.
After this learning process, we can get the up-
dated model parameters θ, then we define the up-
date region [θ, θ]as:
θ= min( θ, θ);
θ= max( θ, θ).(6)
3.2 Hard-Constrained Training
After finding the parameter update regions, we con-
tinually train the model parameters within this re-
gion to learn the new translation tasks, i.e., to find:
θ= arg min/summationdisplayL(y|x, θ),
s.t. θ≤θ≤θ,(7)
whereDdenotes the training data of the new task.
One may suspect that updating parameters only in
a constrained region may lead to the insufficient
ability of the model to fit the new translation tasks.
However, considering the over-parameterization
of the large-scale NMT model and the fact that
many parameters have not learned sufficient knowl-
edge (Hoefler et al., 2021), the hard-constrained
training can give model the capability to adapt to
new tasks in most cases, even though the update
range of parameters is limited. The experimental
results will also prove this.
4 Experiments
In this work, we perform continual learning on two
tasks: the domain adaptation task and the more
challenging language adaptation task. In the do-
main adaptation task, the language pairs of the
new training data are already supported by the pre-
trained NMT model, and the goal is to enable the
model to support the translation of new domains.
In the language adaptation task, the goal is to en-
able the model to support the translation of new
languages, which are not seen during pretraining.
Under the above two scenarios, we hope to retain
the translation ability of the pre-trained MNMT
model on the original translation task.
4.1 The Pre-trained NMT Model
In the experiments, we use the mBART50-nn (Tang
et al., 2020) model as our pre-trained NMT model,
which is available in the fairseq library. The1710Task Train Valid Test
Multilingual
Translationxx↔En / 997 1012
Domain
Adaptation
(De→En)IT 0.22M
2000 2000Law 0.47M
Medical 0.25M
Subtitles 0.5M
Koran 18K
Language
AdaptationEl↔En 1M997 1012Sk↔En 1M
mBart50-nn is a many-to-many multilingual NMT
model which can support the translation between
50 different languages. The encoder and decoder
of mBART50-nn have 12 layers, respectively. The
dimensionality of the model is set as 1024, and
the attention module has 16 attention heads. The
dimensionality of the embedding layer and hidden
states is set as 1024, while the inner-layer of the
feed-forward network has dimensionality as 4096.
The attention module has 16 attention heads both
in the encoder and decoder. Besides, the model has
a shared source-target vocabulary of about 250k
tokens, and the model uses learned positional em-
beddings with the max token length set as 1024.
4.2 Data Preparation
Multilingual Translation Task We test the per-
formance of the model on the original translation
task before and after continual learning, to verify
whether the methods can preserve the previously
learned knowledge. To achieve this, we use the
FLORES-101 test sets (Goyal et al., 2021), which
are extracted from English Wikipedia and cover a
variety of different topics and domains. We test
the translation performance of other 49 languages
to and from English. For our method, we also use
the FLORES-101 validation sets to compute the
empirical Fisher information matrix and search the
parameter update regions.
Domain Adaptation Task For the domain adap-
tation task, we use the data set proposed by Koehn
and Knowles (2017) to simulate a diverse multi-
domain setting. The data set includes parallel text
in German and English, both of which have already
been supported by the mBART50-nn model. The
text is mainly from five diverse domains: IT, Law,Medical, Subtitles, and Koran, which are available
in OPUS (Aulamo and Tiedemann, 2019). We use
the new split released by Aharoni and Goldberg
(2020), and perform German to English transla-
tion (De →En) for this task. It should be noted
that the De →En translation task has already been
supported by the mBART50-nn model.
Language Adaptation Task For the language
adaptation task, we adapt the model to support the
Greek↔English (El ↔En) and Slovak ↔English
(Sk↔En) translation directions. Greek is from an
unseen language family and uses an unseen alpha-
bet, which is very different from all the languages
supported by mBART50-nn. In contrast, Slovak is
from the same language family as Czech, which
is already supported by the mBART50-nn model,
so it is more familiar to the model. Therefore,
we use them as the new languages because this
can simulate most cases where we need language
adaptation. We use the training data from OPUS-
100 (Zhang et al., 2020) to train the model, and the
validation/test sets from FLORES-101 to choose
the checkpoint and test the performance.
Following Tang et al. (2020), we use the sen-
tencepiece (Kudo and Richardson, 2018) model,
which was trained using monolingual data for 100
languages from XLMR, to process all the above
data.
4.3 Systems
We use the open-source toolkit called Fairseq-
py(Ott et al., 2019) as our Transformer system.
The following systems can be divided into two cat-
egories. The first category only focuses on either
the previous task or the new task.
•Scratch : This system is trained from scratch only
with the training data from the new translation task.
•mBART50-nn (Tang et al., 2020): The large scale
pretrained NMT model. All the following systems
are implemented based on this model.
•mBART50-nn + Language-Specific Embedding
(LSE) (Berard, 2021): We insert a new language-
specific embedding layer for the new languages and
fine-tune these parameters with the new training
data for 20k steps. The original parameters are kept
fixed during training. For the language adaptation
task, we use this system as the baseline and other
methods are implemented based on this system.
•Fine-tuning (Luong and Manning, 2015): This
system is trained based on the pretrained model
only with the new training data.1711
•Mixed-FT : We mix the small-scaled validation
sets related to the previous task, i.e., the FLORES-
101 validation sets of the languages supported by
mBART50-nn, with the new training data to train
the model jointly. We use the temperature-based
sampling function to oversample the validation
datasets and the temperature is set as 20(Arivazha-
gan et al., 2019).
The second category contains several continual
learning methods, which aim to get a good balance
between previous and new tasks.
•Knowledge Distillation (KD) (Dakwale and
Monz, 2017): Besides minimizing the training loss
of the new task, this method also minimizes the
cross-entropy between the output distribution of
the mBART50-nn model and the network. The
final objective is:
L(θ) =L(θ) +αKL (p||p),(8)
where αis the hyper-parameter which controls the
contribution of the two parts.
•L2-Reg (Barone et al., 2017): This method addsa L2-norm regularizations on the parameters:
L(θ) =L(θ) +α
M/summationdisplay(θ−θ),(9)
where idenotes the i-th parameter.
•EWC (Thompson et al., 2019): This method mod-
els the importance of the parameters with Fisher
information matrix and puts more constrains on the
important parameters to let them stay close to the
original values. The training objective is:
L(θ) =L(θ) +α
M/summationdisplayF(θ−θ),
(10)
where Fdenotes the modeled importance for the
i-th parameter.
•FT-xattn (Gheini et al., 2021): This method only
fine-tunes the cross-attention parameters, which
can also mitigate the forgetting problem.
•LFR-CM : This system is implemented as the
curvature-based method. We set ρ%as75% andλ
as0.1in the main experiments. We put more results1712about the hyperparameters in the next section and
Appendix.
•LFR-OM : This system is implemented as the
output-based method. We set the hyper-parameter
as1. Other training details are listed below.
Training Details We set dropout as 0.3and
attention-dropout as 0.1. We employ the Adam
optimizer with β= 0.9andβ= 0.98. We use
the inverse square root learning scheduler and set
thewarmup _steps = 4000 . For the curvature-
based method (LFR-CM), we set ρ%as75% and
λas0.2. For the output-based method (LFR-OM),
we set lr= 2e−4for the domain adaptation
task,lr= 4e−5for the language adaptation, and
train the model 5k steps to search the parameter
update region. During continual learning, we set
lr= 5e−4and train the model 30k steps for the
domain adaptation task; we set lr= 5e−5and
train the model 50k steps for the language adapta-
tion task. We fix all the norm layers of the model
in both of the two tasks. In the language adaptation
task, we also fix the original and new embedding
layers, which we find can also help alleviate the
forgetting problem. All the systems are trained on
8 A100 GPUs with the update frequency 2. The
max token is 1024 for each GPU. Besides, we use
beam search with the size of 4 and length penalty
as 1 during decoding.
4.4 Main Results
The final translation is evaluated using the 4-gram
case-sensitive BLEU (Papineni et al., 2002) with
theSacreBLEU tool (Post, 2018). Following Goyal
et al. (2021), we report the SentencePiece BLEU
which uses a SentencePiece tokenizer (SPM) with
256K tokens and then BLEU score is computed on
the sentence-piece tokenized text.
The main results of the domain adaptation task
are in Table 2. Compared with the Scratch system,
Fine-Tuning can greatly improve the performance
of domain adaptation tasks, but it also suffers from
catastrophic forgetting on the multilingual transla-
tion task. Besides, we observe that the forgetting
problem in En →xx directions is more severer. Af-
ter analyzing the model output, we find that the
model cannot output other languages except for
English for all previous translation directions. This
may be because the target language of the domain
adaptation task is only English. Among all the con-
tinual learning methods, our method can get the
best overall performance. The main results of the
language adaptation task are in Table 3. By adding
and updating the new language-specific embed-
dings (mBART50-nn+LSE), we can achieve good
results except for the En →El directions. Greek is
quite different from the previous languages, so it
is more difficult for the model to learn it as the
target language only by the existing knowledge.
Just like the domain adaptation task, our method
outperforms other continual learning methods.
5 Analysis
5.1 Effects of Different Hyper-parameters
For the regularization-based methods, the hyper-
parameter αcontrols the performance trade-off
between the previous and new tasks. The larger
the hyper-parameters αis, the less decline of the
BLEU on the original task will be, and the less
improvement of the new task performance will be.
As for our method, the proportion of model pa-1713
rameters to be pruned has a similar effect. erasing
more neurons will bring better results in the new
task, but will also lead to more degradation in the
original task. To better show the full performance
trade-off, we conduct experiments with different
hyper-parameters. We compare our method with
the L2-Reg and EWC systems. For the L2-Reg
and EWC method, we vary αfrom 0.001to1. For
the curvature-based method, we fix ρ%as75%
and vary λfrom 0.1to1. For the learning-based
method, we vary the learning rate from 1e−5to
1e−4, because we find that adjusting the learn-
ing rate is more effective than changing the hyper-
parameter. The detailed settings of the hyper-
parameters are put in the Appendix. The results are
shown in Figure 2. It shows that our method outper-
forms L2-Reg and EWC at all the operating points
significantly. Besides, it also shows that the output-
based method is better than the curvature-based
method.
5.2 Mixed-Training with Previous Data
In this work, we try to preserve all the previously
learned knowledge and consider the situation that
performing continual learning without access to
any previous training data. But in practice, it is
more likely that we just want to preserve some
specific knowledge, e.g., for some specific lan-
guages, and we have some training data from or
related to the previous training task. To prove the
effectiveness of our method under this scenario,
we conduct further experiments on the language
adaptation task. We choose Chinese (Zh ↔En),
French (Fr ↔En), and German (De ↔En) from the
languages supported by mBART50-nn as our target
languages, on which we want to retain the trans-
lation performance. Then we collect the corre-
sponding training data from the OPUS-100 dataset,
which is much smaller in quantity than the previous
training data of mBART50-nn. Last we continu-
ally train the model with the mixed data. We test
the BLEU scores on the supervised directions and
the zero-shot directions between the previous lan-
guages (Zh, Fr, De) and the new languages (El,
Sk). The results are put in the Table 4, and the
detailed hyper-parameter settings are put in the
Appendix. We find that the regularization-based
method fails to deal with this scenario, and they are
even worse than the vanilla Fine-tuning method,
which indicates that the soft constraints they put
on the parameters are harmful to the model when
some previous data is available. Compared with the
fine-tuning method, our method can further reduce
the forgetting problem with the previous training
data and achieve better overall performance.
5.3 Sequential Language Adaptation
In this experiment, we perform the language adap-
tation task in the sequential training scenario. We
first train the model with the EL ↔En data and
then with the Sk ↔En data. For the L2-Reg and
EWC methods, we use the model after training
with the EL ↔En data as the pretrained model for
the Sk↔En task to compute the regularization loss.
We recompute the Fisher information matrix for the
EWC and LFR-CM method after the El ↔En train-
ing stage. Besides, we also recompute the update
regions after the training of El ↔En for the Sk ↔En
task. The detailed hyper-parameter settings are
put in the Appendix. We report the final results
in Table 5. The results show that our method still
outperforms other methods under this scenario.
6 Related Work
Recent work on continual learning of NMT can be
divided into three categories: data memory based
method, task-specific module based method, and
regularization based method.
Data Memory Based Method The data mem-
ory based methods need to retain part or all of the1714training data of the previous task. Chu et al. (2017)
fine-tune the pretrained model with the mix of the
previous and new data. Bapna and Firat (2019) pro-
pose an n-gram level retrieval approach to find the
useful context to help translation. Xu et al. (2020)
propose to use the similar translations from trans-
lation memory to boost the performance. Liu et al.
(2021) utilize a bilingual dictionary to generate
mixed-language sentences. Compared with these
methods, our method doesn’t need the previous
data and thus is more flexible in practice.
Task-specific Module Based Method The task-
specific module based methods need to assign the
model parameters to different tasks. Bapna and
Firat (2019) injects domain-specific adapter mod-
ules into each layer of the pretrained model, then
they fix the model and only train the new adapters.
Based on this, here are also some work (Zeng et al.,
2018, 2019; Gu et al., 2019; Cao et al., 2021) that
adds different kinds of task-specific modules to
the model. Besides, some work tries to divide the
model into different parts based on the parameter
importance on each task (Gu et al., 2021; Liang
et al., 2020). Although they don’t increase the
model, these methods actually divide the model
into different task-specific parts, which makes the
model need to know which task the input comes
from. Compared with these methods, our method
doesn’t introduce model separation explicitly.
Regularization Based Method The regulariza-
tion based methods mitigate forgetting by intro-
ducing an additional penalty term in the learning
objective. Khayrallah et al. (2018) and Thomp-
son et al. (2019) add regularization terms to let
the model parameters stay close to their original
values. Dakwale and Monz (2017) minimize the
cross-entropy between the output distribution of
the pretrained model and the fine-tuned model. Dif-
ferent from the above work, our method constrains
the model parameters within low forgetting risk
regions to avoid the forgetting problem.
Besides, our work is also related to unstructured
model pruning (See et al., 2016; Zhu and Gupta,
2018; Frankle and Carbin, 2019), because we both
aim to search and operate on the unimportant pa-
rameters. The difference is that our method is to
find an update region around the initial parameters,
while model pruning should directly remove some
parameters, that is, set them to zero.7 Conclusion
In this work, we propose a continual learning
method for NMT by constraining model parame-
ters within low forgetting risk regions. We propose
two methods to find such regions, the first is based
on the curvature of the loss function and the second
is based on the model output of the previous task.
Then we can continually train the model parame-
ters within this region. The experimental results
on the domain adaptation and language adaptation
tasks prove that our method can achieve significant
improvements over several strong baselines.
Limitations
Because our method does not introduce additional
parameters or use the previous data, the overall per-
formance of our method is weaker than the data
memory based methods and task-specific module
based methods. It is difficult for our method to
achieve the same performance as the Fine-tuning
method in the new task without causing catas-
trophic forgetting.
Acknowledgements
We thank all the anonymous reviewers for their
insightful and valuable comments.
References171517161717
A Appendix
A.1 The mBART50-nn Model
The mBart50-nn model is a many-to-many multilin-
gual NMT model which can support the translation
between 50 different languages. The encoder and
decoder of mBART50-nn have 12 layers. The di-
mensionality of the model is set as 1024, while
the inner-layer of the feed-forward network has di-
mensionality as 4096. The attention module has
16 attention heads both in the encoder and decoder.
Besides, the model has a shared source-target vo-
cabulary of about 250k tokens, and the model uses
learned positional embeddings with the max token
length set as 1024.
A.2 Hyper-parameter Settings for the
Analysis
In this section, we report the detailed hyper-
parameter settings in our experiments.
For the main experiments, we set αas1for the
KD method, 0.01for the L2-Reg method, and 0.05
for the EWC method. We set ρ%as75% andλ
as0.1for the LFR-CM method, and αas1for the
LFR-OM method.
For the experiments studying the effects of dif-
ferent hyper-parameters (Section 5.1), we tried the
following hyper-parameters:
•L2-Reg ( α): 0.001, 0.005, 0.01, 0.05, 0.1, 0.5,
1.
• EWC ( α): 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5.
• LFR-CM ( λ): 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.
•LFR-OM ( lr):1e−5,2e−5,3e−5,4e−
5,5e−5,1e−4.
For the mixed-training with previous data ex-
periments (Section 5.2), we set αas0.01for the
L2-Reg and EWC methods, λas0.4for the LFR-
CM method, and lras1e−4for the LFR-OM
method.
For the sequential language adaptation experi-
ments, we set αas0.5for the L2-Reg and EWC
methods, λs0.05for the LFR-CM method, and lr
as2e−5for the LFR-OM method.1718