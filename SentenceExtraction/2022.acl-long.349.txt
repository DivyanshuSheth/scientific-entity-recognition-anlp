
Jia-Chen Gu, Chao-Hong Tan, Chongyang Tao, Zhen-Hua Ling,
Huang Hu, Xiubo Geng, Daxin JiangNational Engineering Research Center for Speech and Language Information Processing,
University of Science and Technology of China, Hefei, ChinaMicrosoft, Beijing, China
{gujc,chtan}@mail.ustc.edu.cn ,zhling@ustc.edu.cn ,
{chotao,huahu,xigeng,djiang}@microsoft.com
Abstract
Recently, various response generation models
for two-party conversations have achieved
impressive improvements, but less effort has
been paid to multi-party conversations (MPCs)
which are more practical and complicated.
Compared with a two-party conversation
where a dialogue context is a sequence of
utterances, building a response generation
model for MPCs is more challenging, since
there exist complicated context structures
and the generated responses heavily rely
on both interlocutors (i.e., speaker and
addressee) and history utterances. To address
these challenges, we present HeterMPC, a
heterogeneous graph-based neural network for
response generation in MPCs which models
the semantics of utterances and interlocutors
simultaneously with two types of nodes
in a graph. Besides, we also design six
types of meta relations with node-edge-type-
dependent parameters to characterize the
heterogeneous interactions within the graph.
Through multi-hop updating, HeterMPC can
adequately utilize the structural knowledge
of conversations for response generation.
Experimental results on the Ubuntu Internet
Relay Chat (IRC) channel benchmark show
that HeterMPC outperforms various baseline
models for response generation in MPCs.
1 Introduction
Enabling dialogue systems to converse naturally
with humans is a challenging yet intriguing prob-
lem of artiﬁcial intelligence and has attracted
increasing attention due to its promising potentials
and alluring commercial values (Kepuska and
Bohouta, 2018; Berdasco et al., 2019; Zhou et al.,
2020). A large number of researchers have focused
on building dialogue generation models with var-
ious neural networks. At ﬁrst, researchers mostlyFigure 1: Illustration of a graphical information ﬂow
in an MPC. Pink rectangles denote utterances and blue
circles denote interlocutors. Each solid line represents
the “ replied-by " relationship between two utterances.
Each dashed line indicates the speaker of an utterance.
focused on dialogues between two participants
(Shang et al., 2015; Serban et al., 2016; Wen et al.,
2017; Young et al., 2018). Recently, researchers
have paid more attention to a more practical and
challenging scenario involving more than two
participants, which is well known as multi-party
conversations (MPCs) (Ouchi and Tsuboi, 2016;
Zhang et al., 2018; Le et al., 2019; Hu et al., 2019b;
Wang et al., 2020b; Gu et al., 2021). Utterances
in a two-party conversation are posted one by one
between two interlocutors, constituting a sequential
information ﬂow. Different from that, utterances
in an MPC can be spoken by anyone and address
anyone else in this conversation, which constitutes
agraphical information ﬂow as shown in Figure 1.
Although sequence-to-sequence (Seq2Seq) mod-
els (Sutskever et al., 2014; Serban et al., 2016) are
effective at modeling sequential dialogues, they
fall short of modeling graph-structured ones. To
overcome this drawback, Hu et al. (2019b) ﬁrst
proposed a graph-structured network (GSN) to
encode utterances based on the graph topology
rather than the sequence of their appearances.
The graph established in GSN was homogeneous,
where nodes represented only utterances. How-
ever, interlocutors are also important components
of MPCs. There exist complicated interactions
between interlocutors, and between an utterance5086and an interlocutor. Furthermore, when passing
messages over a graph, a bidirectional information
ﬂow algorithm was designed for GSN. Since
both the forward and backward information ﬂows
employed the same model structure and parame-
ters, this algorithm cannot distinguish the “ reply "
or “replied-by " relations between two connected
utterance nodes. Also, information ﬂows along
both directions are independently propagated, so
that a graph node cannot be jointly updated at a
single propagation step.
On account of above issues, we propose a hetero-
geneous graph-based neural network for response
generation in MPCs, named HeterMPC. First, a
heterogeneous graph is designed which employs
two types of nodes to represent utterances and
interlocutors respectively. Different from previous
methods that built a homogeneous graph modeling
only utterances, utterances and interlocutors are
modeled simultaneously in HeterMPC, so that
the complicated interactions between interlocutors,
between utterances, and between an interlocutor
and an utterance can be explicitly described. In
order to characterize the heterogeneous attention
over each ( source, edge, target ) triple, model
parameters dependent on both types of nodes and
edges are introduced when calculating attention
weights and passing messages. Speciﬁcally, we
introduce six types of meta relations for modeling
different edges including “ reply ” and “ replied-
by” between two utterances, “ speak ” and “ spoken-
by” between an utterance and a speaker, and
“address ” and “ addressed-by ” between an utterance
and an addressee. With these node-edge-type-
dependent structures and parameters, HeterMPC
can better utilize the structural knowledge of
conversations for node representation and response
generation than conventional homogeneous graphs.
Finally, Transformer is employed as the backbone
of HeterMPC and its model parameters can be
initialized with PLMs to take advantage of the
recent breakthrough on pre-training.
We evaluate HeterMPC on the Ubuntu Internet
Relay Chat (IRC) channel benchmark released by
Hu et al. (2019b). Experimental results show that
HeterMPC outperforms GSN (Hu et al., 2019b),
GPT-2 (Radford et al., 2019), BERT (Devlin et al.,
2019) and BART (Lewis et al., 2020) by signiﬁcant
margins in terms of both automated and human
evaluation metrics, achieving a new state-of-the-art
performance for response generation in MPCs.In summary, our contributions in this paper are
three-fold: 1) To the best of our knowledge, this
paper is the ﬁrst exploration of using heteroge-
neous graphs for modeling conversations; 2) A
Transformer-based heterogeneous graph architec-
ture is introduced for response generation in MPCs,
in which two types of nodes, six types of meta re-
lations, and node-edge-type-dependent parameters
are employed to characterize the heterogeneous
properties of MPCs; 3) Experimental results show
that our proposed model achieves a new state-
of-the-art performance of response generation in
MPCs on the Ubuntu IRC benchmark.
2 Related Work
Multi-Party Conversation Existing methods on
building dialogue systems can be generally catego-
rized into generation-based (Shang et al., 2015;
Serban et al., 2016; Wen et al., 2017; Young
et al., 2018; Zhang et al., 2020) or retrieval-based
approaches (Lowe et al., 2015; Wu et al., 2017;
Zhou et al., 2018; Tao et al., 2019a,b; Gu et al.,
2019, 2020). In this paper, we study the task of
response generation in MPCs, where in addition
to utterances, interlocutors are also important
components who play the roles of speakers or
addressees. Previous methods have explored
retrieval-based approaches for MPCs. For example,
Ouchi and Tsuboi (2016) proposed the dynamic
model which updated speaker embeddings with
conversation streams. Zhang et al. (2018) proposed
speaker interaction RNN which updated speaker
embeddings role-sensitively. Wang et al. (2020b)
proposed to track the dynamic topic in a conver-
sation. Gu et al. (2021) proposed jointly learning
“who says what to whom" in a uniﬁed framework by
designing self-supervised tasks during pre-training.
On the other hand, Hu et al. (2019b) explored
generation-based approaches by proposing a graph-
structured network, the core of which was an
utterance-level graph-structured encoder.
Heterogeneous Graph Neural Network Early
studies on graph neural networks (GNNs) focused
on homogeneous graphs where a whole graph is
composed of a single type of nodes. However,
graphs in real-world applications usually come
with multiple types of nodes, namely heteroge-
neous information networks (HINs) or heteroge-
neous graphs (Sun and Han, 2012). Recently,
researchers have attempted to extend GNNs to
model heterogeneity. For example, Zhang et al.5087(2019) adopted different RNNs for different types
of nodes to integrate multi-modal features. Wang
et al. (2019) extended graph attention networks by
maintaining different weights for different meta-
path-deﬁned edges. Hu et al. (2020) proposed
heterogeneous graph Transformer (HGT) to model
heterogeneity by maintaining dedicated representa-
tions for different types of nodes and edges. In
addition, heterogeneous graphs have also been
applied to many NLP tasks, such as multi-hop
reading comprehension (Tu et al., 2019), text
classiﬁcation (Hu et al., 2019a) and document
summarization (Wang et al., 2020a).
Previous studies have veriﬁed the superiority of
modeling MPCs with homogeneous graphs consid-
ering only utterances. We claim that it is indeed
necessary to model a complex information ﬂow
in MPCs shown in Figure 1 with a heterogeneous
graph, since a homogeneous one cannot explicitly
model the relationships of multiple utterances
spoken by or addressing an interlocutor. Nowadays,
HINs have been widely used in many NLP tasks.
To the best of our knowledge, this paper makes
the ﬁrst attempt to build a heterogeneous graph-
based neural network considering utterances and
interlocutors simultaneously for response gener-
ation in MPCs. In addition, we introduce many
task-speciﬁc modelings for MPCs such as graph
construction and node updating which will be
elaborated in the model section.
3 Problem Formulation
The task of response generation in MPCs is to
generate an appropriate response rgiven the
conversation history, the speaker of a response, and
which utterance the response is going to reply to,
which can be formulated as:
r= argmaxlogP (rjG)
= argmaxXlogP (rjGr):(1)
Here, Gis a heterogeneous graph containing
both history conversation and the response to be
generated. The speaker and addressee of the
response are known and its contents are masked.
The response tokens are generated in an auto-
regressive way. randrstand for the k-th
token and the ﬁrst (k 1)tokens of response r
respectively.jrjis the length of r.
We will introduce how to construct the graph
and how to model the probability in Eq. (1) given
the built graph in the next section.
4 HeterMPC Model
HeterMPC adopts an encoder-decoder architecture
consisting of stacked encoder and decoder layers
for graph-to-sequence learning (Yao et al., 2020).
The graph encoder is designed to capture conver-
sation structures and output the representations of
all nodes in a graph that are fed to the decoder for
response generation.
4.1 Graph Construction
A heterogeneous graph is constructed to explic-
itly model the complicated interactions between
interlocutors, between utterances, and between an
interlocutor and an utterance in an MPC. This graph
models utterances and interlocutors by consider-
ing them as two types of nodes under a uniﬁed
framework. Given an MPC instance composed of
Mutterances and Iinterlocutors, a heterogeneous
graphG(V;E)is constructed. Speciﬁcally, Vis a
set ofM+Inodes. Each node denotes either an
utterance or an interlocutor. E=fegis a
set of directed edges. Each edge edescribes the
connection from node pto nodeq.
Inspired by Sun et al. (2011, 2012), we introduce
six types of meta relations { reply ,replied-by ,speak ,
spoken-by ,address ,addressed-by } to describe
the directed edge between two graph nodes as
illustrated in Figure 2. Speciﬁcally, if an utterance
represented by node nreplies another utterance
represented by node m, the edgee=reply
and the reversed edge e=replied-by . If an
utterance represented by node mis spoken by an
interlocutor represented by node i,e=speak
ande=spoken-by . If an utterance represented
by nodenaddresses an interlocutor represented by5088
nodei,e=address ande=addressed-by .
In other cases, e=NULL indicating that there
is no connection between these two nodes. Note
that it is necessary to distinguish the bidirectional
edges between every two nodes that indicate the
active and passive tense information respectively.
4.2 Node Initialization
In HeterMPC, each node is represented as a vec-
tor. These vectors are ﬁrst initialized individually
without considering graph edges.
Utterances When encoding utterances, a [CLS]
token is inserted at the start of each utterance,
denoting the utterance-level representation. Be-
sides, a [SEP] token is also inserted at the end
of each utterance (Devlin et al., 2019). Then each
utterance is encoded individually by stacked Trans-
former encoder layers through the self-attention
mechanism to derive the contextualized utterance
representations.The output of a Transformer
encoder layer is used as the input of the next layer.
Readers can refer to Vaswani et al. (2017) for
details of Transformer. Formally, the calculation
for an utterance at the l-th Transformer layer is
denoted as:
H= TransformerEncoder( H); (2)wherem2 f1;:::;Mg,l2 f0;:::;L 1g,
Ldenotes the number of Transformer layers for
initialization, H2R,kdenotes the
length of an utterance and ddenotes the dimension
of embedding vectors.
Interlocutors Different from an utterance com-
posed of a sequence of tokens, an interlocutor
is directly represented with an embedding vector.
Interlocutors in a conversation are indexed accord-
ing to their speaking order and the embedding
vector for each interlocutor is derived by looking up
an order-based interlocutor embedding table (Gu
et al., 2020) that is updated during end-to-end
learning. The ﬁrst interlocutors in all conversation
sessions share the same embedding vector in the
interlocutor embedding table, so do all the second
interlocutors.Thus, this order-based embedding
table can be shared across the training, validation
and testing sets, and there is no need to estimate an
embedding vector for each speciﬁc interlocutor in
the dataset.
4.3 Node Updating
As shown in Figure 3, the initialized node repre-
sentations are updated by feeding them into the
built graph for absorbing context information (Kipf
and Welling, 2017; Velickovic et al., 2018; Yun5089et al., 2019). We calculate heterogeneous atten-
tion weights between connected nodes and pass
messages over the graph in a node-edge-type-
dependent manner, inspired by introducing param-
eters to maximize feature distribution differences
for modeling heterogeneity (Schlichtkrull et al.,
2018; Wang et al., 2019; Zhang et al., 2019; Hu
et al., 2020). After collecting the information from
all source nodes to a target node, a node-type-
dependent feed-forward network (FFN) followed
by a residual connection (He et al., 2016) is em-
ployed to aggregate the information. Then, in order
to let each token in an utterance have access to the
information from other utterances, an additional
Transformer layer is placed for utterance nodes
speciﬁcally. Ldenotes the number of iterations
for updating both utterance and interlocutor nodes.
4.3.1 Heterogeneous Attention
Since the representations of two types of nodes are
initialized in different ways, node-type-dependent
linear transformations are ﬁrst applied to node
representations before attention calculation so that
the two types of nodes share similar feature distri-
butions (Wang et al., 2019; Hu et al., 2020). Mean-
while, each of the six relation types is assigned
a separate linear projection so that the semantic
relationship between two connected nodes can be
accurately described when calculating attention
weights. The forward and backward information
ﬂows between them can also be distinguished.
Formally, let the triple ( s, e, t ) denote an edge e
connecting a source node sto a target node t. The
representations of the source and target nodes at the
l-th iterationare denoted as handh, serving as
akey(K) vector and a query (Q) vector of attention
calculation respectively. Then, the heterogeneous
attention weight w(s;e;t )before normalization
for this triple is calculated as:
k(s) =hW+b; (3)
q(t) =hW+b; (4)
w(s;e;t ) =k(s)Wq(t)p
d: (5)
Here,(s);(t)2{UTR, ITR} distinguish utter-
ance ( UTR ) and interlocutor ( ITR) nodes. Eqs. (3)
and (4) are node-type-dependent linear transfor-
mations. Eq. (5) contains an edge-type-dependent
linear projection Wwhereis an adaptivefactor scaling to the attention. All W2R
andb2Rare parameters to be learnt.
4.3.2 Heterogeneous Message Passing
When passing the message of a source node
that serves as a value (V) vector to a target
node, node-edge-type-dependent parameters are
also introduced considering the heterogeneous
properties of nodes and edges. Mathematically:
v(s) =hW+b; (6)
v(s) =v(s)W; (7)
where v(s)is the passed message and all W2
Randb2Rare parameters to be learnt.
4.3.3 Heterogeneous Aggregation
For a target node, the messages passed from all
its connected source nodes need to be aggregated.
A softmax function is applied to normalize the
attention weights and then the messages from all
source codes are summarized as:
h=Xsoftmax(w(s;e;t ))v(s); (8)
whereS(t)denotes the set of source nodes for the
target node t. Then the summarized message h
is aggregated with the original node representation
husing a node-type-dependent FFN followed by
a residual connection (He et al., 2016) as:
h=FFN(h) +h; (9)
where the output his used as the input of the
next iteration of node updating. One iteration can
be viewed as a single-step information propagation
along edges. When stacking Literations, a node
can attend to other nodes up to Lhops away.
A speciﬁc consideration on utterance nodes is
that the tokens except [CLS] in an utterance
have no access to other utterances during the
node updating process introduced above. To
overcome this disadvantage and derive more con-
textualized utterance representations, an additional
Transformer layer (Vaswani et al., 2017) is further
placed for utterance nodes as shown in Figure 3. In
detail, at the l-th iteration, the representations of an
utterance node before and after node updating, i.e.,
handh, are concatenated and then compressed
by a linear transformation as:
^h= [h;h]W+b; (10)5090
where W2Randb2Rare
parameters. Then, ^hreplaces the representation
of[CLS] (i.e.,h) in the sequence representations
of the whole utterance. Finally, the updated
sequence representations are fed into the addi-
tional Transformer layer for another round of
intra-utterance self-attention, so that the context
information learnt by the [CLS] representation
can be shared with other tokens in the utterance.
4.4 Decoder
The decoder is composed of a stack of identical
layers as shown in Figure 4. We follow the standard
implementation of Transformer decoder to generate
responses. In each decoder layer, a masked self-
attention operation is ﬁrst performed where each
token cannot attend to future tokens to avoid in-
formation leakage. Furthermore, a cross-attention
operation over the node representations of the
graph encoder output is performed to incorporate
graph information for decoding. It is notable that a
residual connection along with layer normalization
is followed by each attention operation.
5 Experiments
5.1 Datasets
We evaluated our proposed method on the Ubuntu
IRC benchmark used in Hu et al. (2019b). The data
processing script provided by Hu et al. (2019b)
was employed to derive the dataset.In this dataset,
both speaker and addressee labels were included
for each utterance in a session. When testing, thespeaker and addressee information was both given
for response generation, i.e., the system knew who
would speak next and which utterance should be
responded to following the graph structure. It
contained 311,725/5,000/5,000 dialogues in the
training/validation/testing sets respectively.
5.2 Baseline Models
We compared our proposed methods with as many
MPC models as possible. Considering that there
are only a few research papers in this ﬁeld, sev-
eral recent advanced models were also adapted
to provide sufﬁcient comparisons. Finally, we
compared with the following baseline models: (1)
RNN-based Seq2Seq (Sutskever et al., 2014) took
all utterances except the target utterance to generate
as input, which were sorted according to their
posting time and concatenated. Thus, structured
conversations were converted into sequential ones.
Seq2Seq modeling with attention was performed
as that in Sutskever et al. (2014); Bahdanau et al.
(2015) on the concatenated utterances. (2) Trans-
former (Vaswani et al., 2017) took the same input
utterances as those used for the Seq2Seq model. (3)
GPT-2 (Radford et al., 2019) was a uni-directional
pre-trained language model. Following its original
concatenation operation, all context utterances and
the response were concatenated with a special
[SEP] token as input for encoding. (4) BERT
(Devlin et al., 2019) concatenated all context
utterances and the response similarly as those for
GPT-2. To adapt BERT for response generation, a
special masking mechanism was designed to avoid
response information leakage during encoding.
Concretely, each token in the context utterances
attended to all tokens in the context utterances,
while each token in the response cannot attend to
future tokens in the utterance. (5) GSN (Hu et al.,
2019b) achieved the state-of-the-art performance
on MPCs. The core of GSN was an utterance-
level graph-structured encoder. (6) BART (Lewis
et al., 2020) was a denoising autoencoder using a
standard Tranformer-based architecture, trained by
corrupting text with an arbitrary noising function
and learning to reconstruct the original text. In
our experiments, a concatenated context started
with <s> and separated with </s> were fed into the
encoder, and a response were fed into the decoder.
5.3 Evaluation Metrics
To ensure all experimental results were comparable,
we used the same automated and human evaluation5091
metrics as those used in previous work (Hu et al.,
2019b). Hu et al. (2019b) used the evaluation pack-
age released by Chen et al. (2015) including BLEU-
1 to BLEU-4, METEOR and ROUGE, which was
also used in this paper.Human evaluation was
conducted to measure the quality of the generated
responses in terms of three independent aspects: 1)
relevance, 2) ﬂuency and 3) informativeness. Each
judge was asked to give three binary scores for a
response, which were further summed up to derive
the ﬁnal score ranging from 0 to 3.
5.4 Training Details
Model parameters were initialized with pre-trained
weights of bert-base-uncased released by Wolf
et al. (2020). The AdamW method (Loshchilovand Hutter, 2019) was employed for optimization.
The learning rate was initialized as 6:25e-5and
was decayed linearly down to 0. The max gradient
norm was clipped down to 1:0. The batch size
was set to 16with 8gradient accumulation steps.
The maximum utterance length was set to 50.
The number of layers for initializing utterance
representations Lwas set to 9, and the number
of layers for heterogeneous graph iteration Lwas
set to 3.LandLwere validated on the validation
set. The number of decoder layers Lwas set to 6,
achieving the best performance out of {2, 4, 6, 8}
on the validation set. The strategy of greedy search
was performed for decoding. The maximum length
of responses for generation was also set to 50. All
experiments were run on a single GeForce RTX
2080 Ti GPU. The maximum number of epochs
was set to 15, taking about 40 hours. The validation
set was used to select the best model for testing. All
code was implemented in the PyTorch framework
and are published to help replicate our results.
5.5 Evaluation Results
In our experiments, BERT and BART were selected
to initialize HeterMPC. HeterMPC denoted
that the utterance encoder was initialized with
BERT and the decoder was randomly initialized.
HeterMPC denoted the encoder and decoder5092were initialized by those of BART respectively.
Automated Evaluation Table 1 presents
the evaluation results of HeterMPC,
HeterMPC and previous methods on the
test set. Each model ran four times with identical
architectures and different random initializations,
and the best out of them was reported. We ran the
code released by Hu et al. (2019b) to reproduce the
results of GSN for a fair comparison.The results
show that both HeterMPC and HeterMPC
outperformed all baselines in terms of all metrics.
HeterMPC outperformed GSN by 2.38%
BLEU-1 and 0.44% BLEU-4, and outperformed
GPT-2 by 2.24% BLEU-1 and 0.48% BLEU-4.
HeterMPC outperformed GSN by 2.03%
BLEU-1 and 0.52% BLEU-4, and outperformed
GPT-2 by 1.89% BLEU-1 and 0.56% BLEU-4.
Furthermore, HeterMPC outperformed BERT
by 1.71% BLEU-1 and 0.52% BLEU-4, and
HeterMPC outperformed BART by 1.01%
BLEU-1 and 0.54% BLEU-4, illustrating the
importance of modeling MPC structures.
To further verify the effectiveness of our pro-
posed methods, ablation tests were conducted as
shown in Table 1. First, all nodes or edges were
considered equivalently by employing the same
linear transformations in Eqs. (3) to (9) for all node
or edge types without distinguishing them. The
drop in performance illustrates the effectiveness
of the node-edge-type-dependent parameters. On
the other hand, interlocutor nodes were removed
out of a graph and only the meta relations of reply
andreplied-by were left. The drop in performance
illustrates the importance of modeling interactions
between utterances and interlocutors, and the
effectiveness of the heterogeneous architecture.
Human Evaluation Table 2 presents the human
evaluation results on a randomly sampled test
set. 200 samples were evaluated and the order of
evaluation systems were shufﬂed. Three graduate
students were asked to score from 0 to 3 (3 for
the best) and the average scores were reported.
The Fleiss’s kappa value (Fleiss, 1971) for each
model was also reported, indicating the inter-judge
moderate agreement during evaluation. It can
be seen that HeterMPC and HeterMPC
achieved higher subjective quality scores than the
baselines. Their kappa values were also higher than
the BERT and BART baselines, respectively.
5.6 Analysis
The impact of numbers of iterations ( L).
Figure 5 illustrates how the performance of
HeterMPC changed with respect to different
numbers of iterations ( L) on the test set. It can
be seen that the performance of HeterMPC
was signiﬁcantly improved as Lincreased at
the beginning, which shows the effectiveness of
incorporating the contextual information between
nodes. Then, the performance was stable and
dropped slightly. The reason might be that models
begin to overﬁt due to a larger set of parameters.
The impact of conversation length.Figure 6
illustrates how the performance of HeterMPC
changed according to the test samples with differ-
ent session lengths. As the session length increased,
the performance of HeterMPC dropped less
than that of BERT, showing superiority of our
method on dealing with longer conversations.5093
Case Study. Case studies were conducted by
randomly sampling two MPC instances as shown
in Table 3. Given the conversation graph of
the ﬁrst case, the response to generate addresses
I.2. Thus, the information relevant to I.2 should
be collected. We can see that “ gparted ” in the
ﬁrst utterance is two hops away from I.2 (the
ﬁrst utterance is replied by the second utterance
which is spoken by I.2), and this word in the
fourth utterance and “ install gparted ” in the third
utterance are both one hop away from I.2 (thesetwo utterances directly address I.2). The responses
generated by HeterMPC and HeterMPC
both contain these keywords, showing that it
can capture the conversation graph information
accurately and generate a human-like response.
However, due to the lack of the interlocutor
information and the conversation structure, GSN
generated an irrelevant response. BERT generated
a response which seems replying to the third
utterance. Although BART captured “ gparted ”,
it failed to handle the action “ install ”. In the
second case, we can see that the responses gen-
erated by GSN, BERT and BART are general and
useless while HeterMPC and HeterMPC
can still generate a suitable response. Due to the
complicated interactions between utterances and
interlocutors, the conversation ﬂow might be led
by some unnecessary information, which shows
the importance of making models aware of the
conversation structure.
Robustness. The addressee labels are important
for constructing a graph used in HeterMPC. This
kind of label is commonly available in real life
such as “A@B” labels in group chatting, Twitter,
Reddit and various forums that denote speaker A
talking to addressee B. However, addressee labels
of a part of utterances are missing in the existing
MPC datasets since a speaker may forget to specify
an addressee. HeterMPC is robust since utterances
without addressee labels can be assigned with a
general addressee label “ To all interlocutors ”. We
leave evaluation on other datasets in future work.
6 Conclusion
We present HeterMPC to model complicated in-
teractions between utterances and interlocutors in
MPCs with a heterogeneous graph. Two types of
graph nodes and six types of edges are designed.
Node-edge-type-dependent parameters are intro-
duced for better utilizing the structural knowledge
of conversations during node updating. Results
show that HeterMPC outperforms baselines by
signiﬁcant margins, achieving a new state-of-the-
art performance for response generation in MPCs
on the Ubuntu IRC benchmark. In the future, we
will explore better ways of maximizing feature
distribution differences to model heterogeneity.
Acknowledgements
We thank anonymous reviewers for their valuable
comments.5094References509550965097