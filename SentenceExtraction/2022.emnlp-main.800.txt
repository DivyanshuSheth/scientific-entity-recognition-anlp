
Jihyuk Kim
Yonsei University
jihyukkim@yonsei.ac.krSeung-won Hwang
Seoul National University
seungwonh@snu.ac.kr
Seoho Song and Hyeseon Ko, and Young-In Song
NA VER Corp
{song.seoho, hyeseon.ko, song.youngin}@navercorp.com
Abstract
This paper studies how to enhance the doc-
ument representation for the bi-encoder ap-
proach in dense document retrieval. The bi-
encoder, separately encoding a query and a
document as a single vector, is favored for high
efficiency in large-scale information retrieval,
compared to more effective but complex archi-
tectures. To combine the strength of the two,
the multi-vector representation of documents
for bi-encoder, such as ColBERT preserving
all token embeddings, has been widely adopted.
Our contribution is to reduce the size of the
multi-vector representation, without compro-
mising the effectiveness, supervised by query
logs. Our proposed solution decreases the la-
tency and the memory footprint, up to 8- and 3-
fold, validated on MSMARCO and real-world
search query logs.
1 Introduction
Information Retrieval (IR) aims to rank documents
by their relevance scores to the given query, where
neural IR models have achieved state-of-the-art
performances by fine-tuning large language models.
These models, encoding document dto query qinto
vectors for soft matching, can be categorized into
cross-encoder jointly encoding qandd, and bi-
encoder separately encoding the two into single
vectors.
Our target scenario is commercial-scale IR, re-
quiring online query latency to a web-scale corpus,
where bi-encoders are generally preferred for lower
latency. Meanwhile, as a hybrid of the two, a sin-
gle vector representation can be enriched by multi-
vector representation, such as ColBERT (Khattab
and Zaharia, 2020) preserving all token embed-
dings, for higher effectiveness. With multi-vector
bi-encoder, once query and document vectors are
indexed on memory, relevant documents can beefficiently identified, by a nearest neighbor (NN)
search on the index (Johnson et al., 2019). Col-
BERT, by fully preserving all tokens in document
representation, sacrifices on-memory efficiency ,
for enriching document representation.
query-aware query-agnostic
(cost: H/L)
document Ours (L) D-Cluster
IDCM (H) ME-BERT
query ColBERT-PRF (H)
Table 1: Taxonomy of query and document representa-
tion approaches.
Our research question is thus: Can we improve
memory efficiency, without compromising the ef-
fectiveness of ColBERT? To answer this question,
our hypothesis is that: Given the differential contri-
bution of terms in a document regarding relevant
queries, a few query-relevant terms may be enough
to match the queries, and non-relevant terms can
be pruned out, to decrease index overhead. The
closest existing work is D-Cluster (Tang et al.,
2021), where kcentroids of document tokens are
considered relevant, and their token embeddings
are preserved, as highly relevant terms. This work
is categorized as “query-agnostic” in Table 1, as
actual queries formulated for this (or, similar) doc-
ument in the training dataset, are not used.
Our distinction is selecting kterms with query-
awareness. Existing query-aware term selection,
such as IDCM (Hofstätter et al., 2021), waits for
users to formulate query, then select query-relevant
terms, though incurring high query latency. An-
other example is pseudo-relevance feedback (PRF):
For the related goal of selecting terms to expand
queries, ColBERT-PRF (Wang et al., 2021b) also
waits for the query. Both leverage relevance feed-
backs from user query or PRF, respectively, but11639incur prohibitive online overhead for our target sce-
nario of commercial-scale IR.
Our distinction is to capture query-aware rele-
vance at indexing time, not to incur evaluation-time
overhead. A key distinction is using a pseudo-
query, by supervising a pseudo-query extractor
from query logs, in contrast to query-agnostic un-
supervised approaches, e.g., D-Cluster. This can be
viewed as finding a low-cost solution in the query-
aware space– marked as Ours (L) in Table 1.
Our proposed pseudo-query extractor, following
the intuition of D-Cluster and PRF, treats a subset
of document as candidates of a pseudo-query. Our
distinction is, based on q-d pairs seen from query
log, to supervise such extraction, during indexing
time, for which we address three research questions
as follows:
RQ 1. How can we obtain supervision for train-
ing a pseudo-query extractor? To answer RQ
1, we should address the limitation of most bench-
mark datasets (e.g., MSMARCO), annotating rele-
vance coarsely, at document-level. Though some
dataset annotates term-level relevance (Hofstätter
et al., 2020), its scale is often limited due to annota-
tion overhead, and often not sufficient for training
and used only for evaluation. Our contribution is
obtaining term-level supervision from the coarse
labels, by utilizing the late interaction layer of Col-
BERT which measures the contribution of each
term to document-level relevance.
RQ 2. Would pseudo-query extractor preserve
relevance? Our hypothesis, stated as RQ 2, is
that only a few terms in each document contribute
to query-document relevance, which we extract
as “pseudo-queries”. Our contribution is guiding
document encoder before indexing, to keep only
the terms relevant to pseudo-queries (as done in
cross-encoder with real queries). Our pseudo-query
extractor is thus trained to closely predict the rele-
vance with the real query. We validated our hypoth-
esis in both public benchmark and real-life search
engine.
RQ 3. Finally, would pseudo-query extrac-
tor generalize? Though increasing efficiency,
pseudo-query terms extracted at indexing phase
without query, may overfit to seen query-document
pairs in the training dataset. We validate ours gen-
eralizes well for unseen queries and documents,
by comparing term-level relevance score predicted
with manual annotation, and also automaticallycomparing scores with those from oracle and gen-
eralizable heuristics.
We show ours contributes to perform comparably
to ColBERT-PRF, with 8-fold and 3-fold decrease
in latency and memory footprint, using both public
benchmark MSMARCO passage ranking tasks and
real-world search engine queries.
2 Preliminaries and Related Work
This section motivates the importance of query-
awareness for document understanding, then com-
pares and contrasts with existing approaches.
2.1 Motivation
In Table 2, we present, as a running example, a
relevant q-dpair from MSMARCO dataset. When
not knowing a relevant query (e.g., during index-
ing), the entire document terms are equally im-
portant in representation. However, knowing the
query, we can identify highly relevant terms in the
document, e.g., bold-faced terms in Table 2. Our
distinction is (a) identifying highly relevant terms
before query is known, for (b) enriching document
encoding to match query better.
2.2 Related Work
With the goal of capturing relevant terms be-
tween q-d, our work is built upon relevance
model (Lavrenko and Croft, 2017), where the un-
derlying relevance model Rgenerates terms in both
qanddby modeling the probability on each term
w, i.e.,p(w|R). Since the true relevance model R
is unknown, Ris approximated as ˜R. Depending
on target scenarios, wcan be either added to the
given query (§2.2.1) or extracted from each docu-
ment in a corpus (§2.2.2), and correspondingly the
approximation ˜Ralso differs.
2.2.1 Adding wtoq
Asqis often short, relevant terms may be ab-
sent from q. Query expansion aims to expand q,
where missing relevant terms can be sampled from
p(w|R).11640Given q, to approximate p(w|R), pseudo-
relevance feedback documents can be leveraged
as˜R:
p(w|R)≈/summationdisplayp(w, d|q). (1)
For example, ColBERT-PRF first retrieves top-
ranked documents by ColBERT (Khattab and Za-
haria, 2020) retriever as the pseudo-relevance feed-
back, then finds the expansion terms as centroid
terms in the documents, by applying k-means clus-
tering on all token embeddings. Once augmenting
qwith the expansion terms, ColBERT-PRF repeats
the same retrieval, but now using a better query.
While improving performance, this sacrifices ef-
ficiency, as it requires two rounds of retrieval using
the given qand the expanded q, respectively. In-
stead, with the goal of devising an efficient retriever,
we adopt the relevance model for term extraction
fromd. Different from queries, long documents
often produce high recall but low precision on rel-
evant terms. Therefore, in this work, we aim to
increase precision by pruning non-relevant terms
ind, so as to improve the efficiency with little sac-
rifice on the effectiveness.
2.2.2 Extracting wfrom d
For extracting wfromd, we categorize two scenar-
ios depending on whether qis given or not.
Query-agnostic selection When targeting extrac-
tion at indexing time, qis not given, such that Rcan
be approximated by ˜R={d}, i.e., the singleton
ofd, and terms that summarize ˜Rcan can get high
probability. For example, ME-BERT (Luan et al.,
2021) selects first- kterms, by leveraging positional
bias (Hofstätter et al., 2020) which has been effec-
tive for extractive summarization tasks (See et al.,
2017). As an another example, D-Cluster (Tang
et al., 2021) selects centroid terms, by applying
k-means clustering on document term vectors.
Alternatively, Rcan be approximated by the
entire collection, so that terms that discriminate
relevant documents well can get high probabil-
ity. For this purpose, inverse document frequency
(IDF) (Lassance et al., 2022) can be used, by lever-
aging the fact that rare query terms can discrim-
inate the given document from most of the other
documents.
Query-aware selection Query-agnostic ap-
proach would select terms that are uniformly
relevant to the entire document terms. For example,among bold terms in Table 2, query-agnostic
approach can select flight duration which is related
to many terms in d, e.g., flight ,hours ,minutes ,
speed , while cannot select terms like Cairo, Egypt .
This motivates query-aware selection, delaying
term selection until evaluation time, when qis
given. Targeting second-stage re-ranking for docu-
ment ranking tasks, IDCM (Hofstätter et al., 2021)
first selects a few relevant passages in a document
to the given query using a bi-encoder, and then,
computes the relevance of the document using a
cross-encoder based on only the selected passages.
While having an advantage of query-awareness,
this sacrifices evaluation-time efficiency, violating
our goal. Our distinction is introducing query-
awareness to the indexing-time, so as to achieve
efficiency. We argue that indexing-time selection
suffices when properly trained via our proposed
supervision, and empirically show that our pro-
posed retriever achieves comparable performance
to IDCM with significantly lower latency.
2.2.3 Application of identifying wfrom d
Though our primary goal is to improve the rank-
ing effectiveness, identifying query terms in dcan
also help document owners with better organiz-
ingdby envisioning relevant queries in d. For
example, Pickens et al. (2010) analyze the retriev-
ability of documents (Azzopardi and Vinay, 2008)
via reverted indexing which is a query-centric
view of inverted indexing. Aiming to increase the
transparency of search engines, Li et al. (2022)
study identifying exposing queries for documents
by which the document creators better understand
search queries that surface their documents. As in
the prior work, our pseudo-query extractor can im-
prove the transparency of our search engine by pro-
viding pseudo-query terms (which are readable by
humans in contrast to dense vectors in D-Cluster),
and potentially improve the retrievability by en-
abling the document owners to modify contents to
expose query terms of their interest in their doc-
uments. Different from the previous studies tar-
geting mainly document owners, our distinction
is increasing the ranking performance of search
engines, to also satisfy users of search engines.
3 Proposed: Query-Aware Selection at
Indexing Time
To enable query-aware selection at indexing time,
we leverage existing relevant query logs for each
d. However, in practice, most of the documents11641in a corpus do not have query logs. Therefore, we
train a pseudo-query extractor to predict relevant
query terms from d, by designing supervision for
the training based on the relevant q-dpairs from
the query logs. At indexing time, the pseudo-query
extractor selects relevant terms from both seen and
unseen documents, by envisioning real, relevant
queries.
Our extractor and retriever are built upon Col-
BERT (§3.1) which models term-level relevance
between q-d. By leveraging soft matches between
terms in q/d, we obtain supervisions on p(w|R)for
training pseudo-query extractor (§3.2).
3.1 ColBERT
ColBERT preserves all token embeddings to mea-
sure the relevance score between q-d, denoted by
rel(q, d), as sum of maximum similarity of each
query token qover document token d:
h=WBERT ({x+p+e})(2)
h=WBERT ({x+p+e})(3)
rel(q, d) =/summationdisplaymaxhh, (4)
where x∈R,p∈R, and e∈R
denote token, position, and segment embedding
in BERT, respectively, W∈Rtransforms
768-dimension BERT features into 128-dimension
features for efficiency, and |q|,|d|denote the num-
ber of tokens in qanddrespectively. Note that,
the max-pooling in Eq 4 plays an important role in
increasing ranking performance. For example, re-
placing max-pooling by mean-pooling significantly
decreases the ranking performance (Khattab and
Zaharia, 2020), which motivates our approach of
keeping only a few highly relevant terms.
During training, Wand parameters for BERT
are optimized to maximize rel(q, d)of an an-
notated relevant document dand minimize the
scores of a set of non-relevant documents {d}, by
using noise-contrastive learning objective L(Mnih
and Kavukcuoglu, 2013):
L=−loge
e+/summationtexte.(5)
Though most of the documents in a corpus are
non-relevant to q, to avoid trivial negatives, we
use top-1000 ranked documents by BM25 as hard
negatives, and for sample-efficient training, we also
use in-batch negatives (Karpukhin et al., 2020).3.2 Ours: Pseudo-Query-Aware ColBERT
Our distinction is to preserve only highly relevant
terms that much contribute to rel(q, d)in Eq 4, so
as to reduce memory overhead of index and online
evaluation latency, without losing effectiveness. To
capture the relevant terms, we introduce a pseudo-
query extractor and how we supervise the extractor
to predict term score bfor each document term d,
by which the pseudo-query terms can be extracted
as top- kterms according to b. We leverage the ex-
tracted pseudo-query terms for both pseudo- query-
aware relevance modeling, or PQA-Relevance and
encoding d, orPQA-Encoding . As we integrate
our pseudo-query extractor with ColBERT, we
henceforth denote our retriever by Pseudo- Query-
Aware ColBERT , or PQA-ColBERT.
Pseudo-query extractor Given only the
document-level annotations on relevance, it is
non-trivial to obtain supervision for training the
pseudo-query extractor. To obtain term-level
supervision, we propose to leverage term-level soft
matches of ColBERT. Specifically, based on the
observation that only a few terms in dcontribute
torel(q, d)after max-pooling in Eq 4, we treat
those terms, denoted by ¯q, as pseudo-query terms
ofd. During training, our pseudo-query extractor
is supervised to assign p(w|R) = 1 for any term w
in¯qandp(w|R) = 0 for the others. Note that, ¯q
fully preserves relevance of dtoq, asrel(q, d)
is unchanged once we retain ¯qfrom d. When
multiple qare annotated to d, we take the union
of the ¯qfor each q, to obtain the supervision on
p(w|R).
Given this supervision, we train a prediction
layer fthat predicts the probability bfor each
dto be ¯q:b=σ(f(h)), where σdenotes the
sigmoid function and f(·)consists of two fully-
connected layers with ReLU (Hahnloser et al.,
2000) activation function. As the objective func-
tion, we employ binary cross entropy on b.
During inference, for each din the corpus, we
retain top- kterms regarding b,i.e., pseudo-query
terms q={q}. In the following paragraphs,
we propose to leverage qfor both efficient rele-
vance modeling and effective encoding.
PQA-Relevance While building upon ColBERT,
our distinction is enabling efficient indexing and
relevance modeling, where only {q}(k≪ |d|)
are involved in rel(q,·), and accordingly in index-11642ing, that is, rel (q, d)is approximated by rel (q,q):
rel(q, d)≈rel(q,q) (6)
=/summationdisplaymaxhh,(7)
Our results in §4 show that ours incurs 1/3 of
index memory overhead and evaluation latency
of ColBERT-PRF, but perform comparably to
ColBERT-PRF.
PQA-Encoding While adopting the bi-encoder
design, our distinction is reflecting differential con-
tribution of each dduring encoding d, to give
more emphasis on query-relevant terms, as in cross-
encoder approaches.
Analogous to cross-encoder approaches, we pro-
vide our encoder with query context, as if qis given
viaq. Specifically, we add a binary indicator
s= 1(d∈q), where s∈ {0,1}, to segment
embeddings in BERT by replacing ein Eq 2 with
e. By differentiating qand the other tokens in d
through e, the document encoder can give more
emphasis on qwhich has full responsibility for
relevance prediction (Eq 7).
For training PQA-Encoder, our goal is to fully
preserve q-drelevance using only qind. We
thus design the training objective as the degree of
preserving rel(q, d)(Eq 4) with rel(q,q)(Eq 7),
which can be implemented by Kullback–Leibler
divergence with normalized relevance scores:
L=D(p||p), (8)
where p=softmax (rel(q,·)).
4 Experiments
In this section, we empirically validate the effective-
ness of our PQA-ColBERT, with respect to ranking
performance on passage ranking task. We report
results for a single run.
4.1 Dataset
We train and evaluate our PQA-ColBERT using
benchmark datasets MSMARCO (Nguyen et al.,
2016) and TREC-DL (Craswell et al., 2020, 2021),
on both passage ranking and document ranking
tasks. For training, we use MSMARCO datasets,and, for evaluation, both MSMARCO and TREC-
DL datasets.
In addition, we construct RealQ , where relevant
q-dpairs are extracted from query clicks from real
users. By not requiring human annotation, this
dataset increases test queries by 100-fold from MS-
MARCO. Also, it captures realistic queries that
are much shorter than queries from MSMARCO,
where the average length is 3.15 and 6.37 for RealQ
and MSMARCO, respectively. We leave details of
the dataset construction process in A.1.
Finally, to further analyze the generalization abil-
ity to unseen q/d, we also evaluate the zero-shot
ranking performance: Retrievers are trained on
the MSMARCO Passage Ranking dataset and eval-
uated on TREC-COVID (V oorhees et al., 2021)
dataset which contains medical or pathological q/d
terms and thus is used as an out-of-domain evalua-
tion dataset.
As evaluation metrics, for MSMARCO and Re-
alQ that provide the binary relevance, we report
MRR@10 on top-10 ranked documents. For TREC-
DL datasets and TREC-COVID dataset that pro-
vide graded relevance ranging from 1 to 4, we re-
port NDCG@10, following the convention of orig-
inal paper for graded relevance. For other datasets
with binary annotation, we follow their convention
to report Recall@1000 (denoted by R@1k) on top-
1000 ranked documents. To compare efficiency,
we also report index size and average latency per
query.
4.2 Baselines
To validate the effectiveness of our pseudo-query-
aware indexing, we compare our method to existing
low-cost retrievers that use either single vector or
multiple vectors for each document, along with
high-cost retrievers such as ColBERT (Khattab and
Zaharia, 2020) and ColBERT-PRF (Wang et al.,
2021b) that use all document terms for indexing.
As a baseline that uses single-embedding, we
adopt ANCE (Xiong et al., 2021) that uses [CLS]
token embedding from BERT encoder. For base-
lines that use multi-embeddings, we adopt ME-
BERT (Luan et al., 2021) that uses first ktokens
embeddings, and D-Cluster (Tang et al., 2021) that
uses centroid embeddings from k-means clustering
on token embeddings in a given passage. For a
fair comparison to D-Cluster, we set k= 24 for
the passage ranking task and k= 48 for the doc-
ument ranking task, to have the same index size.11643
For example, for the passage ranking task, since
D-Cluster uses four centroid vectors for each pas-
sage having 768 dimension per centroid, k= 24
matches the size of entire document embeddings
(i.e.,4×768 = 24 ×128). Similarly, since D-
Cluster uses eight centroid vectors for document
ranking task, k= 48 matches the size. In addi-
tion, we also compare two query-agnostic term
selection methods applied on ColBERT proposed
by Lassance et al. (2022), that use ColBERT’s
document token embeddings of ktokens selected
based on leading positions or largest IDF, denoted
by ColBERT-First and ColBERT-IDF, respectively.
Lassance et al. (2022) used 50 tokens, i.e.,k= 50 ,
for both the passage and the document ranking task,
paying more costs than the other low-cost baselines
and ours.
4.3 Training details
In this section, we explain training details for train-
ing PQA-Encoder. We adopt the same architecture
and model configuration to those of the original
implementation of ColBERT. For query/document
length, we set 32 and 180, respectively, as in the
original implementation. We first train ColBERT
using the official train triples for MSMARCO for
300k steps, and then fine-tune PQA-ColBERT for
500k steps. For constructing hard negative pas-
sages dused in fine-tuning, we adopt the same
strategy used in D-Cluster. In D-Cluster, however,
the hard negative passages are periodically updated
for every Tsteps using the retriever being trained,
consuming huge training time costs (e.g., 10 hours
on a single GPU for each update). Instead, weupdate the hard negatives only once, after 100k
steps. For inference, we follow the same indexing-
retrieval pipeline to that of ColBERT, consisting
of FAISS indexing (Johnson et al., 2019) and NN
search. We explain more details in A.2.
4.4 Evaluation result
Passage Ranking Task In Table 3, we present
the end-to-end retrieval performance of our PQA-
ColBERT along with that of baselines, on passage
ranking tasks.
Among ColBERT and ColBERT-PRF, which re-
quire the full-term indexing, ColBERT-PRF shows
better performance overall. This indicates the ben-
efits of adding relevant terms to the given query.
However, as enriching the query representation at
evaluation time, ColBERT-PRF significantly sac-
rifices the efficiency on the latency. We show
pseudo-query terms from PQA-ColBERT, can also
further enhance such high-cost retriever, when we
can afford increases in index size and query la-
tency, which we denote as PQA-ColBERT-H (ours)
for high-latency scenario. More precisely, PQA-
ColBERT-H (ours) uses top- kterms selected by our
extractor in addition to the all document terms, to
emphasize the contribution of the pseudo-query
terms to the term matching, achieving the best
MRR@10 on the MSMARCO Dev set and the best
Recall@1k on TREC datasets.
Among low-cost approaches, ours is the only
approach performing comparably to ColBERT, de-
creasing query latency and index size by a factor
of 8 and 3, respectively.11644
In Figure 1, we further compare results of our
PQA-ColBERT with different kvalues, along with
D-Cluster, while exploring the optimal value for k.
Specifically, we compare memory footprints for in-
dexing passages (y-axis) and ranking performance
(x-axis) on MSMARCO Passage Ranking dataset.
Compared to D-Cluster, our PQA-ColBERT with
k= 12 already shows better MRR@10 with much
smaller index size, showing the effectiveness of our
pseudo-query extractor and pseudo-query-aware
document representations. For PQA-ColBERT
with different kvalues, while increasing kleads
to better ranking performance, the gain decreased
at higher k, and k= 48 shows the best perfor-
mance. This indicates that only a few terms in a
passage contribute to top-ranking results and our
pseudo-query extractor accurately extracts those.
Document Ranking Task In Table 4, we com-
pare retrievers on document ranking task, using
MSMARCO Dev and TREC-DL 2019 datasets.
In addition to retriever baselines, we also report
the ranking performance of the cross-encoder re-
ranker IDCM, to compare query-aware document
term selection. Note that, IDCM has an advantage
of leveraging the given query for both term selec-
tion and encoding, while sacrificing efficiency. In
contrast, we select pseudo-query terms before the
real query is given, and separately encode docu-
ments by adopting bi-encoder approach, to devise
an efficient, scalable retriever.
Among low-cost retrievers (presented in the “L”
section at Table 4), our PQA-ColBERT outper-
forms all baselines, on both the two datasets. Com-
pared to ColBERT (a high-cost retriever presented
in the “H” section), while achieving comparable
performance, ours decreases the index size and the
query latency by a factor of 8.5 and 6.4, respec-
tively. By paying additional cost to further increase
the ranking effectiveness, our PQA-ColBERT-H
outperforms ColBERT, as in the passage rank-
ing tasks. Meanwhile, though IDCM reranker
shows strong performance on TREC dataset, as re-
encoding documents for each given query, IDCM
still shows higher latency than ours. Furthermore,
ours even outperforms IDCM on MSMARCO
datasets.
In Table 5, we compare ColBERT with PQA-
ColBERT on RealQ using more realistic queries.
Since real queries in RealQ are much shorter than11645
those in benchmark datasets, missing any of the
query terms from document representation leads
to significant loss of relevance score. We thus pre-
serve more terms for indexing than the benchmark
datasets, by setting k= 65 . Ours captures query-
relevant terms from d, showing comparable perfor-
mance to that of ColBERT using all tokens.
4.5 Analysis on Pseudo-Query Extractor
In this section, we validate the effectiveness of our
term-level supervision in diverse settings, using hu-
man and automatic evaluation. For baselines, we
adopt two query-agnostic extractors, which select
the first- kterms and top- kIDF terms. We present
selected pseudo-query terms for an example docu-
ment in Appendix A.4.
Human Evaluation For manual annotations, we
employ FiRA dataset (Hofstätter et al., 2020),
where multiple human annotators produce term-
level relevance labels on relevant query-document
pairs from TREC-DL 2019 Document Ranking
dataset. To evaluate our supervision ( qin §3.2),
for each document term, we measure the average
number of positive judgements for term relevance.
For fair comparisons, we set different kfor our pre-
dictions ( qin §3.2) and baselines, by the number
of gold query-relevant terms in qfor each docu-
ment. Our proposed supervision qoutperforms
the others, indicating the effectiveness of the su-
pervision. The prediction from our pseudo-query
extractor qis the second best, indicating our ex-
tractor indeed learns to extract relevant query terms
from documents.
In the following paragraph, for more comprehen-
sive analysis, instead of the limited amounts human
annotated labels, we use qas gold query-relevant
term, which can be obtained automatically, to scale
up evaluation.
Automatic Evaluation To increase the reliabil-
ity of the automatic evaluation, more precisely the
reliability of the oracle q, instead of FiRA, we use
MSMARCO Dev Passage ranking dataset that pro-
vides much larger amounts of annotations for train-
ing ColBERT by which the oracle qis obtained.
For 7433 labeled documents in MSMARCO Dev
Passage ranking dataset with diverse characteris-
tics, we compare distributions, on the position and
IDF scores of query-relevant terms, from our se-
lection ( q) with that of our oracle selection ( q).
We set selection size kidentical to that of oracle
annotation for each document.
In Figure 2(a), we can observe the distribution
of position from our extracted term closely resem-
bles that of oracle distribution. The distribution is
known to left-skewed, known as lead bias, which
explains why First- kheuristic is often effective and
generalizable (Hofstätter et al., 2020). However,
this heuristic was less effective compared to our
supervised selection, and we observe similar trends
for IDF distribution (Figure 2(b)). Those obser-
vations explain the better ranking performance of
our PQA-ColBERT compared to ColBERT-First
and ColBERT-IDF (Table 3 and Table 4). For fur-
ther qualitative and quantitative comparisons to the
two selection methods, see Appendix A.5 and A.4,
respectively.
4.6 Generalization to unseen qandd
In addition to the in-domain ranking performance,
to compare the generalization ability of retriev-
ers, we also evaluate the out-of-domain ranking
performance using TREC-COVID dataset where
standard dense retrievers such as DPR (Karpukhin
et al., 2020) are known to not generalize well, un-
derperforming BM25 (Thakur et al., 2021). Results
are reported in Table 7. For PQA-ColBERT, we11646
setk= 36 considering that the average length of
documents in TREC-COVID is in-between that of
MSMARCO Passage of using k= 24 and MS-
MARCO Document of using k= 48 .
Most of the dense retrievers including PQA-
ColBERT underperform BM25 (presented with red-
colored performance), showing the shared weak-
ness on the generalization ability given the train-
test difference. Meanwhile, titles in documents
provide strong inductive bias in many datasets (Jin
et al., 2002) including TREC-COVID, and our
pseudo-query extractor is flexible enough to lever-
age such prior, in a way of giving the priority to
title terms over other terms. Specifically, when we
force our extractor to retain title terms and then ex-
tract top- kpseudo-query terms within the remain-
ing budget ( i.e.,k), denoted by “+ prioritizing title”,
our PQA-ColBERT shows the best performance,
significantly outperforming BM25.
On the other hand, we stress that solely relying
on titles is insufficient, given missing salient terms
out of the titles. As an ablation study, we com-
pare ColBERT-First(denoted by Ablin Table 7)
which preserves title terms as much as possible,
since a title is placed at the beginning of a docu-
ment, and uses the same index size to ours: Though
maximally enjoying title terms, ColBERT-First still
underperforms BM25 and even our PQA-ColBERT
that do not leverage such prior, indicating that many
salient terms exist also at the latter part of docu-ments. In contrast, PQA-ColBERT successfully
captures such terms, being synergetic to the prior
on titles.
5 Conclusion
We studied enhancing document representation
of bi-encoder, by balancing the benefit of the
single-vector (of memory-efficient indexing) and
the multi-vector (of preserving all relevant query
terms) representation. Specifically, inspired by
relevance model, we supervise a pseudo-query
extractor by using query logs of relevant query-
document pairs in the training dataset, to extract
query-relevant terms at indexing time. Our pro-
posed model is validated on MSMARCO and Re-
alQ benchmarks, for both passage and document
retrieval tasks. Our model achieves comparable
ranking performance to multi-vector retriever and
even to cross-encoder ranker, with significantly
better memory-efficiency for indexing and lower
latency for inference.
Acknowledgements
This work was supported by [search engine train-
ing optimization using deep learning] funded by
NA VER corporation.
Limitations
Targeting real-world retrieval, we employ MS-
MARCO and RealQ datasets which provide large-
scale relevance annotations on diverse queries.
However, both datasets have annotation biases in-
herent in most of the retrieval datasets.11647Because of the extremely large number of docu-
ments in a corpus, relevance annotations are given
only to a few documents, while the others are
often assumed to be non-relevant. For example,
the user clicks as a relevance proxy often suffer
from the ranking bias (also known as position bias),
where clicks are often only examined on the top-
ranked (or top-positioned) documents by a sys-
tem (Joachims et al., 2017). For MSMARCO, rel-
evance annotations are biased to the documents
that have exact matching terms to queries (Xiong
et al., 2021). Such annotation biases have been a
bottleneck for both training (Prakash et al., 2021;
Kim et al., 2022) and evaluation (Arabzadeh et al.,
2022).
To alleviate such biases, one can design the an-
notation process in an interactive way between the
annotators and a target retriever, such that the pool
of documents presented to the annotators dynam-
ically change as the system evolves (Wang et al.,
2021a), which we leave as future work.
References11648
A Appendices
A.1 Dataset construction process for RealQ
To evaluate models on open-domain, realistic docu-
ments, we newly construct dataset, namely RealQ,
from commercial search engines X. The construc-
tion process is as follows.
We first sampled about 1M queries randomly
from the recent 3 month Web search logs of X,
satisfying the following constraints:
•A query should consist of more than 2 words.
•It has to appear at least 10 times in each
month.
• It should not be a navigational query.
The first constraint was introduced to exclude
a single-term query which does not have any con-
textual word of a query term. The remaining two11649constraints intend to rule out other factors than
semantic relevance, such as recency or types of
information, which affect in determining relevance
between a query and a document as much as pos-
sible. A pre-built statistical search intent classifier,
which is being deployed in X Web search, is used
to identify a navigational query. Then, we extracted
top 3 most clicked documents for each of sampled
queries from the same log data and regards them
as positive documents. To filter noisy clicks, we
consider only a satisfactory click, determined by
several heuristics considering dwell time and other
user behavior pattern after clicks.
To sample negative documents, we first retrieve
top 1,000 documents from X search engine, which
indexes multi-billions of Web pages. To increase
the relevance gap between positive, clicked docu-
ments and negative ones, we use simple BM25 as
a ranking scheme instead of X’s original ranking
model utilizing a vast amount of different ranking
signals. Then, 20 documents are randomly selected
from the collected 1,000 documents and finally
regarded as a negative samples for the query. In
the case of test queries, we sampled 100 negative
documents, instead of 20 documents, to enable to
measure a recall@K metrics at a sufficiently large
K. Note that it inevitably incurs a risk that many
false-negative documents can be included in the
dataset. However, we assume that such potentially
false-negative documents would be less relevant
than a positive document which received satisfac-
tory clicks from users.
For the document preprocessing, we identified
and extracted a title and body text using a HTML
parser for each of samples, and regarded their con-
catenation as a text content of a document. In this
step, a document which contains less than 20 words
or a root page of a web site was filtered out in both
positive and negative samples. As a result, we
collect total 21,829,598 (query, document) pairs
for 1,135,453 queries for training, and 10,935,449
pairs for 139,274 queries for testing. In the experi-
ments, we used 20% of samples randomly chosen
from both positive and negative pairs, to reduce
training time required to learn a ColBERT model.
The average number of positive documents for each
query is 2.32. We present the detailed statistics on
our dataset in Table 8.
A.2 Implementation details
For MSMARCO, we initialized the encoder with
bert-base-uncased checkpoint from Transform-
ers (Wolf et al., 2020) library. The maximum se-
quence length of document and query are 180 and
32, respectively. We trained our model for 500K
steps using AdamW (Loshchilov and Hutter, 2019)
optimizer with learning rate 3e-6 and batch size 36.
For RealQ, the encoder was initialized with pre-
trained ELECTRA (Clark et al., 2020) with 12
layers and 4 attention heads. The model was pre-
trained on web corpus in dominant language used
in X. The maximum sequence length of document
and query are set to 180 and 16, respectively. Then
we train our model with a batch size 560 for 30
epochs.
A.3 Computational budgets
In Table 9, we present the computational budgets
used in our experiments with the number of total
parameters.
A.4 Qualitative Analysis on Pseudo-Query
Extractor
In Figure 3, we present a relevant query-document
pair from MSMARCO Dev Passage Ranking
dataset, along with selected terms within the docu-
ment by different strategies.11650
According to writing conventions, salient con-
tents often appear at the beginning of the docu-
ment (Hofstätter et al., 2020). By leveraging such
tendency, First-k strategy successfully selects two
query-relevant terms, “ammonium” and “salts”.
However, this obviously misses useful terms po-
sitioned beyond the leading terms, e.g., “soluble”.
Another useful feature for the extraction is col-
lection frequency (IDF) which often indicates the
importance of a term within a corpus. For example,
Top-k IDF strategy selects discriminative, query-
relevant terms such as “ammonium” and “hex-
achloroplainate”. Note that, “hexachloroplainate”
enables the document to be matched to other rel-
evant yet unannotated queries, e.g., “Does ammo-
nium hexachloroplainate soluble in water?”. Nev-
ertheless, Top-k IDF fails to capture “salt” and
“soluble”, as these may appear frequently in other
documents, producing low IDF values. Different
from the two strategies which use predefined fea-
tures, our pseudo-query extractor learns, during
training, useful features including the two afore-
mentioned features, capturing all query-relevant
terms such as “ammonium”, “salt”, and “soluble”.
A.5 Quantitative Analysis on Pseudo-Query
Extractor
By devising our pseudo-query extractor, our goal is
to maximally preserve relevant query terms given a
fixed budget for the document indexing ( i.e.,k). In
Figure 4, by using MSMARCO Dev Passage Rank-
ing dataset, we evaluate how much the document-
level relevance can be preserved using the selected
pseudo-query terms, compared to using all terms
for computing relevance. Specifically, we measure
the degree of preservation on relevance scores from
the pre-trained ColBERT using all tokens in each
document (i.e., rel(q, d)in Eq 4) to the scores us-
ing selected pseudo-query tokens (i.e., rel(q,¯q)in
Eq 7), depending on the number of selected tokens.
To analyze the generalizability of pseudo-query ex-
tractors, we present results on seen and unseen doc-
uments that have and do not have observed query
logs in the training dataset, respectively. As base-
lines to our extractor, we compare with the two
query-agnostic selection methods extracting lead-
ingkterms and top-IDF terms, denoted by First- k
and Top- kIDF in Figure 4, respectively. Given
the goal of achieving both memory-efficiency and11651the ranking effectiveness, promising curves should
bow towards the top-left corner.
Our pseudo-query extractor much quickly con-
verges to the upper bound than the others. For
example, our extractor preserves 95% of the rele-
vance score rel(q, d)using less than 10 tokens (13%
of entire document tokens on average), while base-
line extractors use approximately 20 (ColBERT-
First) and 30 (ColBERT-IDF) tokens to achieve the
same preservation ratio. Furthermore, the gap be-
tween performance on seen and unseen documents
is marginal, indicating that ours generalize well to
unseen documents.11652