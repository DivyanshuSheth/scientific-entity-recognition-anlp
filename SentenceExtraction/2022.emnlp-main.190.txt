
Kelong Mao, Zhicheng Dou, Hongjin Qian, Fengran Mo, Xiaohua Cheng, Zhao CaoGaoling School of Artificial Intelligence, Renmin University of ChinaUniversité de Montréal, Québec, CanadaHuawei Poisson Lab
{mkl,dou}@ruc.edu.cn
Abstract
Conversational search provides users with nat-
ural and convenient new search experience. Re-
cently, conversational dense retrieval has shown
to be a promising technique to realize conver-
sational search. However, as conversational
search systems have not been widely deployed,
it is hard to get large-scale real conversational
search sessions and relevance labels to support
the training of conversational dense retrieval.
To tackle this data scarcity problem, previous
methods focus on developing better few-shot
learning approaches or generating pseudo rele-
vance labels, but the data they use for training
still heavily rely on manual generation.
In this paper, we present ConvTrans , a data
augmentation method that can automatically
transform easily-accessible web search ses-
sions into conversational search sessions to fun-
damentally alleviate the data scarcity problem
for conversational dense retrieval. ConvTrans
eliminates the gaps between these two types of
sessions in terms of session quality and query
form to achieve effective session transforma-
tion. Extensive evaluations on two widely used
conversational search benchmarks, i.e., CAsT-
19 and CAsT-20, demonstrate that the same
model trained on the data generated by Con-
vTrans can achieve comparable retrieval perfor-
mance as it trained on high-quality but expen-
sive artificial conversational search data.
1 Introduction
Conversational search systems support multi-turn
interaction with users through conversations to
meet various information needs. Such a new search
paradigm is more natural, convenient, and user-
friendly, which is expected to hugely improve users’
search experience in the future (Culpepper et al.,
2018). Different from traditional ad-hoc search
systems, conversational search systems have to un-
derstand the user’s real search intent of the currentFigure 1: Left: An example of a web search session
(WSS) (orange) and a conversational search session
(CSS) (blue) sharing similar search intents shift. Right:
A high-level illustration of ConvTrans.
turn based on the history conversation context, fac-
ing a more complex search intent understanding
problem (Gao et al., 2022).
Recently, inspired by the success of dense re-
trieval (Karpukhin et al., 2020), conversational
dense retrieval has also been developed for con-
versational search, whose core is to train a con-
versational session encoder to directly encode the
whole conversational search session (i.e., the cur-
rent query combined with the history conversation
context) into an embedding to implicitly represent
the real search intents. Although conversational
dense retrieval shows to be promising, unfortu-
nately, its development is severely hindered by the
lack of data problem. As conversational search
systems have not been widely deployed in prac-
tice, it is hardly to obtain real large-scale conversa-
tional search sessions and corresponding relevance
labels to support its training (Yu et al., 2021). Ex-
isting methods try to mitigate this data scarcity
obstacle by developing better few-shot learning ap-
proaches (Yu et al., 2021; Mao et al., 2022) or gen-
erating pseudo relevance labels (Lin et al., 2021).
Nevertheless, the conversational search sessions2935and oracle rewrites that they use for training still
rely heavily on manual generation, leading to lim-
ited data volume and weak scalability.
Given that the real large-scale conversational
search data is naturally unavailable at present and
the high cost of artificial data, we propose to lever-
age the data from another domain, i.e., web search
sessions that can be easily collected from existing
web search engines, to help train the conversational
session encoder. A web search session contains a
series of queries and click behaviors of a user in a
short period of time, and the click behaviors can be
used as relevance labels. It reflects the user’s multi-
turn interaction with the web search engine, which
is similar to the multi-turn interaction in the conver-
sational search session to some extent. As shown
in Fig. 1 (left), both the web search session and
the conversational search session can share similar
search intents shift. These similarities make the
web search session be a promising data source for
simulating the conversational search process.
However, directly using raw web search sessions
may not be effective for training the conversational
session encoder due to the following undesirable
gaps. First, as shown on the left side of Figure 1,
queries in web search sessions are often keyword-
based and basically self-contained, while the con-
versational queries are expressed with natural lan-
guage and may have some linguistic phenomena,
such as ellipsis and co-reference (we call it conver-
sational natural language-based ). Second, due to
the different forms of search engines, the search
behaviors in the two types of sessions are not com-
pletely consistent. From the view of conversational
search, the search topic shift in the raw web search
sessions tends to be more cluttered with more noise,
and the session length can be too short or too long
because of inaccurate session segmentation. These
significant differences in the query form and the ses-
sion quality are likely to make the conversational
session encoder ineffectively trained, resulting in
unsatisfactory retrieval performance.
To address these gaps, in this paper, we pro-
pose a data augmentation method called Con-
vTrans , which can automatically transform easily-
accessible web search sessions into conversational
search sessions to fundamentally alleviate the data
scarcity problem for conversational dense retrieval.
The right side of Fig. 1 illustrates the idea of such
data transformation. Specifically, to facilitate the
improvement of the session quality towards con-versational search, we first reorganize the raw web
search session into a heterogeneous session graph,
whose nodes and edges stand for queries and query
relations, respectively. This graph has three types
of edges, including response-induced ,topic-shared ,
andtopic-changed to simulate three common query
(turn) relations in conversational search. Mean-
while, we further add more useful queries retrieved
from the whole session database into the graph
to enrich its diversity. Then, each query in the
graph is transformed into a conversational natural
language-based one using a specific conversational
query rewriter composed of two T5 models (Raffel
et al., 2020). Finally, we perform a random walk
sampling on this session graph to generate pseudo
conversational search sessions.
By bridging web session search and conversa-
tional search, our ConvTrans largely expands avail-
able data sources and reduces human efforts to gen-
erate new conversational search data, which would
also be helpful to solve the cold start problem in
deploying real conversational search systems for
industry. We apply ConvTrans on MS MARCO
search sessions to generate a new dataset and use
it to train the conversational session encoder. Ex-
tensive evaluations on two widely used conversa-
tional search datasets, i.e., CAsT-19 and CAsT-20,
show that the same model trained on the dataset
generated by ConvTrans can achieve comparable
retrieval performance as it trained on high-quality
but expensive artificial conversational search data.
Our contributions are summarized as:
•We propose ConvTrans, a data augmentation
method which can automatically transform
web search sessions into pseudo conversa-
tional search sessions to alleviate the data
scarcity problem in conversational search.
•In ConvTrans, we develop a graph-based re-
organization method and a specific conversa-
tional query rewriter to effectively narrow the
gap between the two types of sessions in terms
of session quality and query form.
•We conduct extensive experiments to validate
the effectiveness of the conversational search
sessions generated by ConvTrans for conver-
sational dense retrieval.
2 Related Work
Conversational search. One of the most unique
challenges for conversational search is to recover2936the users’ real information need from the complex
conversation context. To solve this problem, there
are mainly two types of methods for conversational
search, which are query reformulation and con-
versational dense retrieval. Query reformulation
methods (Yu et al., 2020; V oskarides et al., 2020;
Lin et al., 2020; Vakulenko et al., 2021; Wu
et al., 2021) change conversational search into
ad-hoc search by using a query rewriting model
to generate an explicit de-contextualized query,
while conversational dense retrieval methods (Yu
et al., 2021; Lin et al., 2021; Mao et al., 2022;
Krasakis et al., 2022; Kim and Kim, 2022; Li et al.,
2022) are to encode the conversation context into a
high-dimensional space to perform dense retrieval.
In contrast, the two-stage query reformulation
methods can make better use of the existing ad-hoc
search resources, but the integrated conversational
dense retrieval methods tend to have lower search
latency and are easier to be directly optimized
towards retrieval tasks (Lin et al., 2021; Gao
et al., 2022). Instead of proposing new learning
algorithms or model architectures, our work is
to study data augmentation for supporting the
training of conversational dense retrieval.
Data augmentation for conversational search.
A tough problem hindering the development of
conversational search is that we are very short
of conversational search data, such as conversa-
tional search sessions, relevance labels, and ora-
cle rewrites. Data augmentation, as an effective
method to alleviate data scarcity problems, has
raised attention from the conversational search
community. Specifically, Yu et al. (2020) are the
first to leverage web search sessions for conversa-
tional search. They propose a rule-based method
and a self-learn method to transform the web search
sessions for training a GPT-based query rewriting
model. However, different from our ConvTrans,
their work only considers the differences in query
forms between these two types of sessions, which
is far from enough to build effective conversational
search sessions for conversational dense retrieval.
Besides, they directly filter out a large number of
keyword-based queries, which fails to make full
use of the web search sessions. Lin et al. (2021)
employ a strong text ranking model to generate
pseudo relevance labels for conversational dense
retrieval, but they still rely on existing artificial
conversational search sessions and oracle rewrites.Recently, Dai et al. (2022) propose a dialog in-
painting method which treats each sentence of a
document as a answer and utilize a T5 model to
complete the question part to transform a document
into a dialog. While the amount of data generated
can be very large, this method limits all answers
in each conversation (dialog) are come from the
same passage, which is not the real case of conver-
sational search. The generated conversation is also
not originated from users’ real search behaviors.
3 Preliminary
3.1 Conversational Dense Retrieval
In conversational search, the multi-turn interaction
between the user and the search engine constitutes
a whole conversational search session S=
{(u, p)}, where uandpare the utterance
(i.e., conversational query) and the system response
passage in the k-th turn, respectively. nis the ses-
sion length. For the k-th turn, the target of con-
versational search is to find the passage pfrom
a collection Pfor the utterance uunder its con-
versation context C={(u, p)}. Conversa-
tional dense retrieval realizes it by encoding the
sub-session of the current turn (i.e., the current ut-
terance ucombined with its conversation context
C) and passages into a unified space for matching:
s=CSE(u, C), (1)
p=PE(p), p∈P, (2)
where CSE and PE stand for the conversational ses-
sion encoder and the passage encoder respectively.
The dot product of the conversational session em-
bedding sand the passage embedding pis used
as the retrieval score. Similar to dense retrieval, the
negative log likelihood ranking loss is a common
choice for training:
L=−loge
e+/summationtexte,(3)
where pandpare the relevant and irrelevant
passages for the current turn k. Since there is no ob-
vious difference between ad-hoc search and conver-
sational search from the view of the passage side,
the passage encoder is often directly reused from a
well-trained ad-hoc passage encoder to save compu-
tation cost (Yu et al., 2021; Lin et al., 2021). Corre-
spondingly, the conversational session encoder will
be also initialized as a well-trained ad-hoc query
encoder. In training, the parameters of the passage2937encoder are frozen and only the conversational ses-
sion encoder will be trained.
3.2 Our Motivation
One of the most serious obstacles to conversational
dense retrieval is the lack of training data. Cur-
rently, most conversational search sessions and
query-passage relevance labels are generated by
humans, which are limited and expensive. To fun-
damentally alleviate this data scarcity problem, in
this work, we propose to leverage the web search
session to help the model training. We choose the
web search session because it is not only easily-
accessible and large-scale but also reflects the
multi-turn interaction of users with the search en-
gine, which is similar to the conversational search
session to some extent. Moreover, a large number
of users’ click behaviors can be used as relevance
labels. However, there still exist significant dif-
ferences in the query form and the session quality
between them, making the raw web search sessions
not suitable to be directly used as training data for
conversational dense retrieval. In the following, we
present ConvTrans which tackles the above prob-
lems to generate effective pseudo conversational
search sessions from raw web search sessions.
4 Methodology
Formally, for a raw web search session S=
{(q, p)}, where qis the k-th ad-hoc query
andpis the user’s clicked passage for q, our Con-
vTrans is to generate new pseudo conversational
search sessions Sbased on S, and the rel-
evance labels of Sare directly inherited from
S. We show an overview of ConvTrans in Fig. 2
and two concrete examples in Appendix C. On
the whole, ConvTrans consists of three key steps,
including session graph construction, query trans-
formation, and conversational session generation.
We elaborate on them in the following.
4.1 Session Graph Construction
To facilitate the improvement of the session quality
towards conversational search, we first reorganize
the raw web search session into a heterogeneous
session graph, whose nodes and edges stand for
queries and query relations, respectively.
Query relation determination. We define three
types of query relations that are common in conver-
sational search, including: (1) response-induced :A query is induced by the response passage of an-
other query. (2) topic-shared : Two queries share
the same main search topic but are on different
sub-topics. (3) topic-changed : Two queries are on
different main search topics. Specifically, we use
the following rulesto determine the query relation
and weight between any two queries qandq:
•qhas the response-induced relation to qif
more than half of terms of qare in p, where
pis a sentence of the q’s response passage p
(i.e.,|q∩p|>). The weight is defined
as|q∩p|to encourage their similarities. If
multiple sentences of the passage pcan sat-
isfy the condition, we choose the one with the
largest weight.
•Otherwise, qhas the topic-shared relation to
qif more than half of terms of qare in q(i.e.,
|q∩q|>). The weight is defined as
to encourage the differences between them on
the basis of exceeding a similarity threshold.
•Otherwise, qhas the topic-changed relation
toqand the weight is set to a constant.
Notably, the response-induced relation is priori-
tized because it is harder to be satisfied in the raw
session data. The requirement of terms overlap
between qandp(orq) in the first two rules is to
inspire ellipsis and co-reference for the subsequent
query transformation (Section 4.2).
Graph construction. We sequentially process
each query in a raw web search session to construct
the session graph. Specifically, the first query q
is first added to the graph as the current central
node . Then, we collect a query set RI(q)which
contains queries that have the response-induced
relation to qfrom the remaining queries of this
session. Then we add RI(q)to the graph by es-
tablishing a response-induced edge from qto each
query of RI(q). Similarly, then we collect another
query set TS(q)from the remaining queries and
addTS(q)to the graph with topic-shared edges
from qto the queries of TS(q). Then, we add
the next query qto the graph by connecting it to
the current central node (i.e., q) with the topic-
changed edge from qtoq(note that if qhas
been already in the graph, the next query will be
q, and so on). Finally, qbecomes the new current2938
central node and we repeat the above process to
add all queries of the session to the graph.
However, due to the poor quality of the raw
web search session, only very few queries actu-
ally satisfy the response-induced ortopic-shared
relation in the same session. To solve this problem,
we resort to the “collaborative power” to enrich
the diversity of the graph that improves its quality.
Specifically, for qwe not only find its RI(q)and
TS(q)in the remaining queries of the session but
also additionally retrieve satisfactory queries from
the whole session database. Finally, we reduce
the noise of the graph by only keeping the Top-5
response-induced and Top-5 topic-shared edges
with the largest weights for each central node, but
the queries originally in the raw session will be
retained first.
4.2 Query Transformation
To eliminate the gap between query forms, we pro-
pose a conversational query rewriter to transform
the ad-hoc queries in the graph into conversational
natural language-based ones. Our conversational
query rewriter is composed of two T5 models (Raf-
fel et al., 2020) named NL-T5 and CNL-T5.
4.2.1 NL-T5
NL-T5 is to transform the keyword-based query
into a natural language (NL)-based query. For
a natural language-based query q, we employ a
keyword extraction method on it to approximately
get its corresponding keyword-based query q,
and use the (q,q)pair to train NL-T5. Weuse Quora Question Pairsdataset, which contains
404,302 natural language-based queries, to build
the training pairs. For inference, we simply ap-
ply the trained NL-T5 on query nodes that are
not natural-language based to transform them into
natural-language based ones.
4.2.2 CNL-T5
CNL-T5 is to further transform the NL-based query
into a conversational natural language (CNL)-based
one. Since the query nodes have overlapped terms
with their central query nodes or passages, they
have the potential to be rewritten as conversational
queries. Therefore, we apply CNL-T5 on NL-based
query nodes except for those central query nodes
to transform them into CNL-based queries. Specifi-
cally, for a NL-based query qlinked to its central
query node qwith the topic-shared edge, we get
its corresponding CNL-based query qby:
q=T5([CLS] ◦q◦[SEP]◦q◦[SEP] ),(4)
where ◦is the concatenation operation. For q
which is linked to its central query node qwith
theresponse-induced edge, we get its qby:
q=T5([CLS] ◦q◦[SEP]◦p◦[SEP] ).(5)
Notably, an important merit of our CNL-T5
is that, in inference, its input is much shorter
than the existing query rewriters for conversational
search (Yu et al., 2020; Lin et al., 2020), whose
inputs include all of the previous turns. In contrast,2939we only need to refer to a short context (i.e., qor
p), largely reducing the transformation difficulty.
For training, we use the conversational search
sessions of CANARD (Elgohary et al., 2019), a
high-quality human-written conversational query
reformulation dataset, as the training data. Similar
to Yu et al. (2020), we input the concatenation of
the oracle queries of the previous turns and the
current turn into CNL-T5, and train it to generate
the conversational query of the current turn.
4.3 Conversational Session Generation
Finally, we develop a specific random walk sam-
pling algorithm to generate pseudo conversational
search sessions based on the query-transformed ses-
sion graph. Algorithm 1 shows the pseudo-code of
the whole random walk sampling process. Specif-
ically, we start from the first central node qand
sample it as the first utterance. Then, we sample
at most wconversational queries which are linked
toqwith topic-shared edges. Then, we continue
to sample 0 or 1 conversational query which is
linked to qwith the response-induced edge. Then,
we move to the next central node along the topic-
changed edge and repeat the above process. The
intuition of such a random walk order is that users
are likely to change to other search topics after
raising a query induced by the last response. The
whole sampling process stops when the session
length meets the pre-defined maximum number of
turnsTor all query nodes have been sampled.
5 Experimental Setup
5.1 Datasets and Evaluation Metrics
We collect the raw web search sessions from the
MS MARCO Conversational Search DEV dataset.
The dataset is created by joining public queries
into private sessions based on embedding similarity,
containing 75,193 sessions and 408,389 queries in
total. The relevance labels are extracted from the
MS MARCO Passage Ranking dataset.
To evaluate the CSE’s retrieval effectiveness af-
ter training, we conduct experiments on two widely
used conversational search evaluation datasets:
CAsT-19 (Dalton et al., 2020) and CAsT-20 (Dal-
ton et al., 2021). These two datasets are manually
generated and labeled by the experts of the TRECConversational Assistance Track. There are 50
and 25 search-oriented conversations in CAsT-19
and CAsT-20, respectively. Each conversation has
around 10 conversational queries (turns) and each
conversational query has a corresponding man-
ual oracle rewrite. Compared with CAsT-19, the
queries in CAsT-20 can refer to the previous an-
swers and thus is more difficult. The passage collec-
tion contains around 38 million passages. Follow-
ing Dalton et al. (2021), we use MRR, NDCG@3,
and Recall (at cutoff 20 and 100) as the evaluation
metrics. NDCG@3 is the primary metric as pre-
scribed by TREC CAsT. All significance tests are
conducted with paired t-tests at p < 0.05 level.
5.2 Baselines
We compare ConvTrans with the following base-
lines: (1) AutoRewriter (Yu et al., 2020): It con-
tains two query rewriting methods: Rule-based
andSelf-Learn , to transform question-like web
search sessions into conversational search sessions
and use them to train CSE using the same rank-
ing loss. (2) CQE(Lin et al., 2021): It employs
a strong ranking model to generate pseudo rele-
vance labels for the existing CANARD (Elgohary
et al., 2019) dataset which contains 5,644 human-
written conversational search sessions and uses
them to train CSE using the same ranking loss.
(3)ConvDR-CANARD (Yu et al., 2021): It uses the
conversational search sessions and the oracle query
rewrites of the CANARD dataset to train CSE us-
ing a knowledge distillation loss. The CSE used in
all compared methods are initialized with the same
ANCE (Xiong et al., 2021) checkpoint, which is
a state-of-the-art ad-hoc query encoder. We also
show the results of using the not fune-tuned ANCE
as well as only using the current query (i.e., Raw) or
the current oracle query (i.e., Oracle ) as the model
input for reference.
5.3 Implementations
We use KeyBERTas the keyword extraction
method in ConvTrans. The sampling threshold
wis set to 3 and the maximum number of turns T
is set to 10. We sample one conversational search
session for each raw web search session. The con-
versational session encoder is trained 1 epoch for
ConvTrans using the ranking loss (Eq. 3) with the
Adam optimizer in batch size 16 and learning rate2940
5e-7. The inputto the conversational session en-
coder (except for ANCE-Raw and ANCE-Oracle)
is the concatenation of the current utterance u, the
last response passage p, and all previous utter-
ances u, with [CLS] in the head and [SEP]
to separate turns. The maximum input sequence
length is 512 and the maximum length of pis
384. More implementation details are provided in
Appendix B and the released code.
6 Main Results
Table 1 reports the performance comparisons
among various methods and we have the following
main observations:
(1) ConvTrans significantly outperforms the
other baselines w.r.t. Recall. In particular, on
the more difficult dataset CAsT-20, ConvTrans
achieves 7.1% and 8.7% relative improvements
over the second-best results w.r.t. Recall@20 and
Recall@100, respectively. In terms of MRR and
NDCG@3, ConvTrans also achieves the best per-
formance on CAsT-19 and the second-best perfor-
mance on CAsT-20, only worse than CQE. Note
that the training sessions of ConvTrans and Au-
toRewriter are transformed from web search ses-
sions while the training sessions of CQE and
ConvDR-CANARD are human-written. This re-
sult demonstrates that the training sessions gen-
erated by ConvTrans are comparably effective as
the human-written ones and are significantly better
than those of AutoRewriter.
(2) We find that the five compared methods all
obtain large boosts in retrieval performance overthe zero-shot ANCE, indicating that the original
ad-hoc query encoder is not effective to solve the
complex conversational query understanding prob-
lem. It is necessary to generate appropriate conver-
sational search sessions to empower it for conver-
sational search.
(3) ConvDR-CANARD and ConvDR achieve
relatively strong performance, especially on MRR
and NDCG@3, demonstrating that the knowledge
distillation loss is also very effective, but a draw-
back is that it needs expensive oracle rewrites.
(4) The numbers of training sessions for different
methods are different and ConvTrans has the most
training sessions. The reason is: For the two Au-
toRewriter methods, although they share the same
original web search sessions with ConvTrans, they
can only use those question-like queries, and thus
they finally have fewer available sessions. For CQE
and ConvDR-CANARD, their numbers of sessions
are strictly limited by the CANARD dataset which
only has 5,644 sessions. We emphasize that the
ability to freely leverage large-scale raw web search
sessions is also an important advantage of our Con-
vTrans. We further show the study of the impact of
the training data size in Section 7.
7 Impact of Training Data Size
To study the impact of the amount of training data,
we test the performance of the baseline methods
and ConvTrans with 20% to 100% of training ses-
sions on CAsT-20. Results are shown in Fig. 3.
We find that with the increase in the amount
of training data, the retrieval performance shows
an upward trend, but this trend gradually becomes
slower. This indicates that although increasing the
amount of training data can basically improve the2941
model effectiveness, simply increasing it may en-
counter performance bottlenecks. It is necessary to
improve the data quality for conversational dense
retrieval. In contrast, the performance growth rate
of ConvDR-CANARD decreases the slowest with
the increase of data volume. The better “sustainabil-
ity” of ConvDR-CANARD is probably due to its
use of high-quality manual oracle rewrites. Unfor-
tunately, we find that ConvTrans becomes incom-
petent to significantly improve the model perfor-
mance when the data volume reaches around thirty
thousand sessions. This indicates that, although
ConvTrans finally achieves decent performance,
the quality of the generated sessions may still have
a gap with the manually generated data and can be
further improved.
Besides, to remove the influence of the train-
ing data size for comparisons, we randomly select
the same number of training sessions (i.e., 5,644)
for all these methods. As shown in Fig. 3, our
ConvTrans still significantly outperforms the two
AutoRewriter methods and only performs slightly
worse than CQE (0.283 v.s 0.289 w.r.t. NDCG@3)
whose sessions are human-written, further demon-
strating the effectiveness of the training sessions
generated by ConvTrans.
8 Ablation Studies
In this section, we investigate the effects of the
session graph construction and the query transfor-
mation of ConvTrans.
Specifically, we build three ablations: (1)
Direct , which directly uses the raw web search
sessions as the training data. (2) Direct + SG ,
which builds session graph to refine the session
quality but does not perform query transformation.
(3)Direct + QT , which only performs query trans-
formation on the raw web search sessions. Fig. 4
shows the performance comparisons among these
ablations. We find that Direct can only achieve
similar performance to the zero-shot ANCE base-
line, demonstrating that the raw sessions are not
suitable for being the direct training data of the
conversational session encoder. After incorporat-
ing the session graph ( Direct + SG ) or the query
transformation step ( Direct + QT ), we observe
significant improvements over Direct , verifying
that our session graph and query transformation are
effective to refine the session quality and transform
the query form for the raw web search sessions.
However, these two ablations still perform much
worse than the complete ConvTrans. This is rea-
sonable since solving only one type of data gap
still leads to an out-of-distribution problem for the
generated training data, so it is necessary to include
both of these two steps in ConvTrans to generate
more realistic conversational search sessions.
9 Investigation of Using Different Session
Generation Strategies
As introduced in Section 4.3, we will sample both
topic-shared andresponse-induced conversational
queries for each central query (node). In this sec-
tion, we investigate the influence of topic-shared
andresponse-induced query relations on the ef-
fectiveness of the generated conversational search2942sessions. Specifically, we build two variants: (1)
ConvTrans-TS , which only samples topic-shared
queries. (2) ConvTrans-RI , which only samples
response-induced queries. Results are shown in
Table 2. We find that only sampling one type of
query will result in performance degradation. In
contrast, ConvTrans-TS shows much better perfor-
mance than ConvTrans-RI, indicating that the topic-
shared relation is more important and more com-
mon than the response-induced relation in terms of
the CAsT datasets. This is also consistent with the
actual situation of the two CAsT datasets. In Partic-
ular, ConvTrans-TS achieves similar performance
to ConvTrans on CAsT-19, but it still performs
worse than ConvTrans on CAsT-20. This is be-
cause the CAsT-20 dataset has response-induced
queries while the CAsT-19 dataset does not.
10 Conclusion
In this work, we propose ConvTrans, a data aug-
mentation method which can transform large-scale
web search sessions into effective conversational
search sessions to support the training of conver-
sational dense retrieval. To mitigate the gaps in
terms of the session quality and the query form
between the two types of search sessions, we de-
sign a graph-based re-organization method and a
two-step conversational query rewriter. The pseudo
conversational search sessions are generated by per-
forming a tailored random walk on the constructed
session graph. We run extensive experiments on
two CAsT datasets. Results demonstrate that Con-
vTrans outperforms state-of-the-art data augmen-
tation baselines for conversational search and can
support the model to achieve comparable retrieval
performance as using the expensive artificial con-
versational search data.
Limitations and future work. Our work demon-
strates the feasibility of transforming web search
sessions for helping the training of conversational
dense retrieval. However, we only consider three
coarse-grained query relation types (i.e., response-
induced ,topic-shared , and topic-changed ) in this
work. In practice, the real conversational search
sessions can have more diverse query shifts, such
as returning to previous main search topics and
referring to more earlier response passages. Study-
ing better ways of session graph construction to
handle more complex conversational search behav-
iors would be a good improvement to this work.
Another limitation of this work is that, since weadded some queries and adjusted the order of some
queries for the raw web search sessions, their rel-
evance labels may also change to some extent be-
cause of changed search session context. But the
relevance labels of the generated conversational
search sessions are still directly inherited from the
raw web search sessions in this work. Therefore,
it would be better to develop a method to further
check and improve the relevance labels. Finally,
as pointed out in Section 7, ConvTrans still has a
large improvement space for generating more pow-
erful sessions that can be continuously absorbed by
the model to finally achieve better retrieval perfor-
mance.
Acknowledgments
Zhicheng Dou is the corresponding author. This
work was supported by the National Natural Sci-
ence Foundation of China No. 61872370, Bei-
jing Outstanding Young Scientist Program NO.
BJJWZYJH012019100020098, the Fundamental
Research Funds for the Central Universities, the Re-
search Funds of Renmin University of China NO.
22XNKJ34, Public Computing Cloud, Renmin Uni-
versity of China, and Intelligent Social Governance
Platform, Major Innovation & Planning Interdisci-
plinary Platform for the “Double-First Class” Ini-
tiative, Renmin University of China. The work was
partially done at Beijing Key Laboratory of Big
Data Management and Analysis Methods, and Key
Laboratory of Data Engineering and Knowledge
Engineering, MOE.
References29432944Appendix
A Pseudo-Code for Conversational
Session Generation
Algorithm 1 Conversational Session Generation
Require: The session graph G, the max number
of turns T, a sampling threshold w.Initialize the conversational search session list
S= [], the current central query node c=q.while len(S)≤Tandcis not null doS.append( c)n=randint (0, w), sampling nqueries
from topic-shared (G, c)intoS.n=randint (0,1), sampling nqueries
from response-induced (G, c)intoS.c=topic-changed (G, c)end whileS=S[:T]Output S
B More Detailed Experimental Setup
We provide a more detailed introduction of the
datasets, implementation, and hyper-parameter set-
tings of ConvTrans and baseline methods in this
section.
B.1 Datasets Details
The statistics of the two CAsT datasets and the CA-
NARD dataset are shown in Table 3. In CAsT-19,
173 queries in 20 test conversations have relevance
judgments. In CAsT-20, most queries have rele-
vance judgments. The 38 million passages are from
MS MARCO (Nguyen et al., 2016) and TREC
Complex Answer Retrieval (CAR) (Dietz et al.,
2017). All of the conversational search sessions
and oracle rewrites of CANARD (i.e., training, de-
velopment, and test sets) are used in our work.B.2 More Implementation Details
B.2.1 ConvTrans
In the session graph construction, we resort to other
web search sessions to obtain more satisfactory
queries to refine the graph. Specifically, as the size
of the whole session database is very large, we
build a term-query inverted index for fast retrieving
queries that have the topic-shared relation to qto
expand TS(q). For expanding RI(q), we narrow
down the range of candidate queries from the whole
session database to the queries ever raised after
users clicked on passage p.
When extracting keywords to build the (q, q)
pairs for training NL-T5, the parameters of Key-
BERT are set to: “msmarco-bert-base-dot-v5” ,
keyphrase_ngram_range = (1,2),top_n = 5. The
extracted keywords are arranged according to their
original order in the query (i.e., q) to form the
keyword-based query ( q). NL-T5 and CNL-T5
are based on the HuggingFace T5-base model.
B.2.2 Baselines
AutoRewriter. Their original paper also uses the
MS MARCO Conversational Search DEV dataset
as the raw web search sessions. Therefore, we
directly obtain the transformed conversational
search sessions of Rule-based and Self-Learn
from their original repositoryand use them to
train the conversational session encoder. After
parameter fine-tuning, we finally train 1 epoch for
the conversational session encoder with batch size
16 and learning rate 1e-7.
CQE. In this work, the only difference between
CQE and ConvTrans is that CQE uses the CA-
NARD dataset with pseudo relevance labels while
ConvTrans uses generated conversational search
sessions to train the conversational session encoder.
We use the pseudo relevance labels released by
their official repository. For CQE, we train the
conversational session encoder with batch size
16 and learning rate 1e-6. We use the CAsT-19
training data as the validation set following their
original paper.
ConvDR-CANARD. We use their official code
for implementation and follow their original2945settings (i.e., 1e-5 learning rate, 16 batch size, and
1 training epoch) to train ConvDR-CANARD.
ANCE. ANCE is a zero-shot baseline for reference
and is also the initial state of the conversational
session encoder, which can be downloaded here.
C Case Studies
In this section, we present two concrete examples
of session transformation of ConvTrans. The
maximum number of turns Tis 10.
Example #1
The input raw web search session:
q: a lamborghini cost
q: how much is a tesla
q: key west weather
q: what is nicki minaj name
q: hotels in houston airport
The generated conversational search session:
u: How much is a lamborghini cost? (from q)
u: How much is the car insurance?
u: How much is a tesla? (from q)
u: How much does it cost to replace its batteries?
u: How much electricity can it generate?
u: What is the key west weather? (from q)
u: How long does it take to drive to there
u: What does the temperature feel like?
u: What is nicki minaj name? (from q)
u: What are her tour dates 2015?
Example #2
The input raw web search session:
q: what is metoprolol succinate
q: what does adipex do
q: clonazepam a benzo
q: is does trazodone help
q: what is hydrochlorothiazide
The generated conversational search session:
u: What is metoprolol succinate? (from q)
u: Does it cause dry mouth?
u: What does a beta blocker?
u: What does adipex do? (from q)
u: How long can a doctor write a prescription
for?u: Is clonazepam a benzo? (from q)
u: Does trazodone help? (from q)
u: Does it help release serotonin?
u: What is hydrochlorothiazide? (from q)
u: Can it cause leg cramps?2946