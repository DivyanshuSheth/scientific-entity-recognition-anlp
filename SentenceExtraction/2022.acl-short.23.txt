
Chao ZhaoWenlin YaoDian Yu
Kaiqiang SongDong YuJianshu Chen
zhaochao@cs.unc.edu
{wenlinyao,yudian,riversong,dyu,jianshuchen}@tencent.comUNC Chapel Hill, Chapel Hill, NCTencent AI Lab, Bellevue, WA
Abstract
Comprehending a dialogue requires a model to
capture diverse kinds of key information in the
utterances, which are either scattered around or
implicitly implied in different turns of conver-
sations. Therefore, dialogue comprehension re-
quires diverse capabilities such as paraphrasing,
summarizing, and commonsense reasoning. To-
wards the objective of pre-training a zero-shot
dialogue comprehension model, we develop
a novel narrative-guided pre-training strategy
thatlearns by narrating the key information
from a dialogue input. However, the dialogue-
narrative parallel corpus for such a pre-training
strategy is currently unavailable. For this rea-
son, we first construct a dialogue-narrative par-
allel corpus by automatically aligning movie
subtitles and their synopses. We then pre-train a
BART model on the data and evaluate its perfor-
mance on four dialogue-based tasks that require
comprehension. Experimental results show that
our model not only achieves superior zero-shot
performance but also exhibits stronger fine-
grained dialogue comprehension capabilities.
The data and code are available at https:
//github.com/zhaochaocs/Diana .
1 Introduction
Dialogue comprehension (Sun et al., 2019; Cui
et al., 2020) aims to capture diverse kinds of key in-
formation in utterances, which are either scattered
around or implicitly implied in different turns of
conversations. Therefore, it requires different ca-
pabilities such as paraphrasing (Falke et al., 2020),
summarizing (Gliwa et al., 2019), and common-
sense reasoning (Arabshahi et al., 2021). Recent ad-
vances in pre-trained language models (PLMs) (De-
vlin et al., 2019; Radford et al., 2019) have been
applied to the problem (Jin et al., 2020; Liu et al.,
2021). However, these PLMs are generally pre-
trained on formal-written texts, which are differentfrom dialogue data in nature. Specifically, dia-
logues are composed of colloquial languages from
multi-speakers, and utterances usually have com-
plex discourse structures (Afantenos et al., 2015).
Therefore, applying these models directly to dia-
logue comprehension, especially in low-resource
settings, is sub-optimal.
To learn better dialogue representations, recent
studies have designed several dialogue-specific pre-
training objectives such as speaker prediction (Qiu
et al., 2021), utterance prediction (Chapuis et al.,
2020), response selection (Wu et al., 2020), and
turn order restoration (Zhang and Zhao, 2021).
These methods, albeit improve over the vanilla
PLMs, usually rely on surface-level dialogue in-
formation. In particular, they still fail to train the
models to explicitly learn the aforementioned capa-
bilities which are critical for dialogue comprehen-
sion (e.g., linguistic knowledge, world knowledge,
and commonsense knowledge). Furthermore, it
was not able to incorporate knowledge beyond di-
alogue (e.g., non-verbal communications between
speakers, as well as time and location information),
which are also crucial for dialogue comprehension.
To pre-train a zero-shot dialogue comprehension
model with the aforementioned features, we de-
velop a novel generative pre-training strategy that
learns by narrating the key information from a
dialogue input (see Figure 1 for an example). In
particular, the generated narrative text is supposed
to not only (i) paraphrase the gists of the dialogue
but also (ii) carry certain inferred information (e.g.,
the time and location of a scene and relations be-
tween speakers) that are not explicitly mentioned in
the dialogues. Learning to narrate such information
helps the model to learn varied lexical, syntactic,
and semantic knowledge of dialogue. It also en-
hances the model’s ability to infer extra information
beyond the literal meaning within dialogues, which
will benefit the model’s capability of dialogue com-
prehension.212
However, the learning-by-narrating strategy
would require a dialogue-narrative parallel cor-
pus, which, to our best knowledge, is not pub-
licly available. For this reason, we first create
DIANA , a large-scale dataset with ( DIAlogue,
NArrative) pairs automatically collected from sub-
titles of movies and their corresponding plot syn-
opses. We consider dialogues from movie subtitles
as they are close to daily human-to-human conver-
sations (Zhang and Zhou, 2019). In addition, the
movie synopses include rich narrative information,
which is helpful for dialogue comprehension. After
data collection and strict quality control, we ob-
tain a dataset with 243K (dialogue, narrative) pairs
written in English. As the automatic data construc-
tion procedure is language-independent, it can be
applied to low-resource languages as well.
We then pre-train a BART model (Lewis et al.,
2020) on the constructed corpus with the proposed
learning-by-narrating strategy, and evaluate it on
four dialogue-based tasks that require comprehen-
sion. In zero-shot settings, our pre-trained model
outperforms the BART baseline on all tasks by a
large margin (e.g., +8.3%on DREAM (Sun et al.,
2019)), demonstrating the success of our approach.
The contributions of this paper are three-fold:
•We propose a novel learning-by-narrating pre-
training strategy for dialogue comprehension;
•We release DIANA , a new large-scale
dialogue-narrative parallel corpus;
•Experiments show that our pre-trained dia-
logue comprehension model achieves superior
zero-shot performance on a variety of down-
stream tasks.
2 DIANA: A Dialogue-Narrative Corpus
In this section, we describe the procedure to create
the dialogue-narrative parallel dataset.2.1 Data Collection and Segmentation
We collect 47,050 English subtitles of movies and
TV episodes released from Opensubtitle (Lison
et al., 2018) and their corresponding synopses from
online resources such as Wikipedia and TMDB. To
link the subtitle and synopsis of the same movie or
TV episode, we require a subtitle and a synopsis to
have the same title and the release year, as well as
a high overlap rate ( >50%) on role names.
The subtitle and synopsis of a movie are too long
for a PLM. To facilitate pre-training, we split both
the subtitle and synopsis into smaller segments and
align the related segments from each part to shorter
(dialogue, narrative) pairs. We split subtitles using
the time interval δbetween utterances and split a
synopsis into sentences. We set δ= 5s.
2.2 Data Alignment
We aim to align the dialogue sessions {d, . . . , d}
and narrative segments {s, . . . , s}with maxi-
mum global similarity to form (dialogue, narrative)
pairs. For each dialogue session d, the goal is to
find its corresponding narrative segment s.
Inspired by (Tapaswi et al., 2015) in which the
narrative in a synopsis follows the timeline of a
movie or a TV episode, we develop a dynamic
time warping method to find the globally optimal
alignment score. During aligning, some narrative
segments contain information beyond the dialogue,
so they cannot be aligned to any dialogue session.
We therefore allow our algorithm to skip at most k
narrative segments during alignment searching:
whereA(i, j)denotes the optimal alignment score
of the first inarrative segments and the first jdi-
alogue sessions. S(s, d)is the text similarity
between sandd.
We compare the performance of three text simi-
larity measures: Jaccard similarity, Rouge-1F, and213
TF-IDF. In consideration of time efficiency, we
don’t apply more advanced neural methods. We
compare these similarity measures on MovieNet
dataset (Huang et al., 2020), which provides a man-
ual alignment between the segments of subtitles
and synopses of 371 movies.We evaluate the per-
formance of each similarity measure by alignment
accuracy, a.k.a, the percentage of dialogue sessions
that are correctly aligned to the corresponding nar-
rative segment. As shown in Table 1, TF-IDF per-
forms best among all similarity measures. We also
find that a narrative-wise Lnormalization of the
TF-IDF can further improve the alignment accu-
racy. It helps to penalize the similarity of (d, s)
when shas high similarity with many dialogues
(e.g., when scontains common words or protag-
onists’ names.) We therefore choose the normal-
ized TF-IDF as our similarity function. We further
analyze the errors during alignment and find that
85.94% of errors happen because the dialogue ses-
sion is aligned to the previous or next segment of
the gold narrative segment. It indicates that most of
the errors happen locally. Figure 2 shows an exam-
ple from MovieNet, where the red line and the blue
line indicate the gold alignment and the predicted
alignment via normalized TF-IDF, respectively. It
shows that the two lines are generally well over-
lapped except for some local discrepancies.
2.3 Quality Control
After data alignment, each narrative segment scan
be aligned to multiple dialogues. To consider the
local alignment errors, we also merge the aligned
dialogues of sandsto the dialogues of s.
Some of these dialogues may not be relevant to s.
To select the relevant dialogues, we use a greedy
method to incrementally select dialogues until the
rouge-F score between the narrative and the se-
lected dialogues doesn’t increase. After selection,
we concatenate the selected dialogues and preserve
their relative position. We finally obtain around 1.5
Million (dialogue, narrative) pairs.
To further improve the quality of data, we fil-
ter out pairs where the dialogue and the narra-
tive are irrelevant. To evaluate the relevance, we
use two automatic measures: Coverage and Den-
sity (Grusky et al., 2018). Low Coverage and Den-
sity indicate that the narrative text is either too
abstractive or irrelevant to the dialogue. We thus
only select the pairs with Coverage >0.5and
Density >1. After this strict quality control, we
obtain 243K (dialogue, narrative) pairs as the final
DIANA dataset, which is a high-quality subset of
the original dataset. The average length of the dia-
logue and the narrative are 58 tokens and 18 tokens,
respectively.
2.4 Analysis of Knowledge Type
To analyze what types of knowledge are included
in DIANA, we randomly sample 100 instances and
manually categorize the relation between dialogue
and the corresponding narrative text into seven
knowledge types. We show the percentage of each
knowledge type in parentheses and in Figure 3 as
well. The knowledge types are:
•Summarizing (39%): The narrative text sum-
marizes multiple utterances as a concise state-
ment to reflect the salient event or information
of the dialogue.
•Visual/Audial (17%): The narrative text pro-
vides extra visual or audial information of the
dialogue, such as the location of the dialogue,
the speakers’ actions, and ambient sounds.
•Paraphrasing (14%): The narrative text re-
states speakers’ utterances using other words.
•Text Matching (9%): The narrative text is di-
rectly copied from the utterances of speakers.214
•Implicit (10%): The narrative text provides
extra information that is not explicitly men-
tioned in the dialogue.
•Causal (6%): The narrative text describes the
cause and effect relationship between events.
•Interpersonal (5%): The narrative text re-
veals the relationships between speakers.
Among these knowledge types, Summarizing
andVisual/Audial are the two most frequent ones.
They are followed by Paraphrasing andText Match-
ing, which contribute to 23% in total. It also
shows that narratives use paraphrasing more of-
ten than copying. Additionally, DIANA contains
three higher-level knowledge types that require the
awareness of real-world commonsense and more
complicated inference such as implicit knowledge,
causal relationships, and interpersonal relation-
ships. The diverse knowledge types in DIANA
indicate the benefit of this dataset for dialogue com-
prehension and other downstream tasks as well.
3 Pre-training: Learning-by-Narrating
During pre-training, we aim to inject the knowl-
edge contained in DIANA into pre-trained models.
One option is to ask the model to distinguish be-
tween a correct narrative and an incorrect narrative
via a classification objective. However, it requires
carefully designing additional non-trivial negative
(dialogue, narrative) pairs. Therefore, we propose
to directly generate a narrative text from the given
dialogue by maximizing the generative probability:
p(y|x;θ) =Yp(y|y,x;θ),(2)
where xare dialogue texts and yare narrative texts.
There are two main advantages of using the gen-
erative objective. First, it can fully leverage the nar-
rative information from each token of the narrativetext with no need to construct negative pairs. Sec-
ond, the pre-trained model can be directly applied
to both generative and discriminative downstream
tasks without further fine-tuning. For discrimina-
tive tasks, we calculate the probability of each can-
didate according to Equation 2 and choose the most
probable candidate as the predicted answer.
4 Experiments
In this section, we evaluate the performance of the
pre-trained model on four downstream tasks that
require dialogue comprehension.
4.1 Setting
We use BART, a state-of-the-art sequence-to-
sequence model, as our baseline model.We use
its released checkpoint and further pre-train the
model on DIANA. During pre-training, we con-
catenate the utterances as the input and update the
parameters to maximize the probability of the corre-
sponding narrative. We use Adam as the optimizer,
and we set the learning rate and weight decay to
3×10and 0.01, respectively. Following previous
studies that suggest that a larger batch size helps
pre-training, we set the batch size to 1024 and pre-
train the model for 1,000 steps.
4.2 Tasks
We evaluate our model’s ability of dialogue com-
prehension on four downstream tasks. DREAM
(Sun et al., 2019) aims to read a dialogue and se-
lect the correct answer from options of a dialogue-
related question. To make the task similar to our
pre-training task, we follow previous work (Chen
et al., 2021) to train a T5 model to convert each
(question, answer) pair to a statement. PCMD (Ma
et al., 2018) is a passage completion task. Given a
dialogue and a passage that describes the dialogue,
a query is created by replacing a character mention
with a variable x, and the model needs to recover
the character mention. VLEP (Lei et al., 2020)
aims to select the most probable future event given
the dialogue of the current event and two candi-
dates of future events. SAMSum (Gliwa et al.,
2019) is a dialogue summarization task to create a
concise abstractive summary for a dialogue. The
first three are discriminative tasks, and SAMSum
is a generative task. None of the source dialogues
in these tasks are included in DIANA.215
We evaluate the model performance on these
tasks under the zero-shot setting. For discrimina-
tive tasks, we convert each test instance with K
answer candidates as K(dialogue, narrative) pairs.
Given the dialogue as input, we evaluate the con-
ditional probability of each narrative according to
Equation 2 and choose the most probable narrative
as the predicted answer. We use accuracy (ACC) as
the evaluation metric for discriminative tasks and
ROUGE for the summarization task.
We compare our pre-trained model (Narrator)
with strong pre-trained baselines such as GPT-2,
RoBERTa, and BART. To investigate the impact
of the pre-training objective, we compare with 1)
BART-DIAL-DE: the original BART de-noising
objectives, which is trained on the dialogue part
of DIANA; and 2) BART-CNN-CLS: a classifica-
tion objective, which is trained using the CNNDM
dataset (See et al., 2017) to distinguish between
positive and negative summaries based on the doc-
uments. Negative summaries are obtained from
DocNLI (Yin et al., 2021) by replacing the words,
entities, and sentences of positive summaries. We
also investigate the quality of DIANA by compar-
ing it with two large summarization datasets: CN-
NDM and CRD3 (Rameshkumar and Bailey, 2020).
We pre-train BART to generate the summaries of
these datasets from the corresponding documents
and refer to the models as BART-CNN-GEN and
BART-CRD3-GEN. Besides the zero-shot models,
we list the supervised results finetuned on BART
(BART-FT) as a reference for the upper bound.
4.3 Results
Results are shown in Table 2. Our observations
are as follows. (i) When compared with vanilla
PLMs, Narrator outperforms GPT-2, RoBERTa,
and BART, demonstrating that the learning-by-
narrating pre-training objective can improve the
model’s ability of dialogue comprehension. (ii)
When compared with different pre-training tasks,
Narrator outperforms BART-DIAL-DE, and BART-
CNN-GEN outperforms BART-CNN-CLS. This
indicates that the narrative-guided generative ob-
jective is more effective than the de-noising objec-
tive and the discriminative objective. (iii) When
compared with different pre-training data, Narra-
tor achieves better performance on all tasks com-
pared with BART-CNN-GEN and BART-CRD3-
GEN, demonstrating that DIANA is a more helpful
resource for dialogue comprehension.
We further analyze what types of knowledge are
enhanced during pre-training. To this end, we test
Narrator on a subset of the DREAM test set, which
includes annotated knowledge types released along
with the DREAM dataset. As shown in Table 3,
compared with the vanilla BART, Narrator achieves
better performance on all knowledge types except
Arithmetic, which is not covered in DIANA. The
performance gain indicates that the narrative pre-
training contributes the most to the knowledge re-
lated to paraphrasing and matching. It also benefits
from other knowledge types that require various
reasoning abilities such as commonsense reasoning
and logic reasoning.
5 Conclusion
We propose a learning-by-narrating strategy to pre-
train a zero-shot dialogue comprehension model.
We first construct a dialogue-narrative dataset
named DIANA, which contains 243K (dialogue,
narrative) pairs obtained by automatically aligning
movie subtitles with their corresponding synopses.
We then pre-train a dialogue comprehension model
based on DIANA and evaluate its performance on
four downstream tasks that require dialogue com-
prehension abilities. Experiments show that our
model outperforms strong pre-trained baselines,
demonstrating that the learning-by-narrating strat-
egy is a promising direction for dialogue compre-
hension. We also hope that DIANA will promote
future research in related areas.216References217218