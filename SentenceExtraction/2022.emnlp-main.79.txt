
Yash Kumar Lal
Stony Brook UniversityNiket Tandon
Allen Institute for AITanvi Aggarwal
Stony Brook UniversityHorace Liu
Stony Brook University
Nathanael Chambers
US Naval AcademyRaymond Mooney
University of Texas at AustinNiranjan Balasubramanian
Stony Brook University
Abstract
Answering questions in narratives about why
events happened often requires commonsense
knowledge external to the text. What aspects
of this knowledge are available in large lan-
guage models? What aspects can be made ac-
cessible via external commonsense resources?
We study these questions in the context of an-
swering questions in thedataset
using as a source of relevant com-
monsense relations. We analyze the effects of
model size (variants and ) along with
methods of injecting knowledge ( )
into these models. Results show that the
largest models, as expected, yield substantial
improvements over base models and injecting
external knowledge helps models of all sizes.
We also find that the format in which knowl-
edge is provided is critical, and that smaller
models benefit more from larger amounts of
knowledge. Finally, we develop an ontol-
ogy of knowledge types and analyze the rel-
ative coverage of the models across these cat-
egories.
1 Introduction
Humans reason about events in narratives by mak-
ing inferences about why those events happen.
The recently introduceddataset tests
for this capability by posing why questions over
events in simple narratives (Lal et al., 2021). An-
swering these often requires commonsense knowl-
edge (CSK) that is not explicitly stated as part
of the narratives. Indeed, QA models built over
standard base sized models fare poorly, especially
where the answer is not stated in the narrative.
There are two broad avenues for incorporating
the necessary commonsense knowledge for this
task — using larger language models (e.g. (Raffel et al., 2020)) and leveraging exter-
nal knowledge resources. The former can be seenFigure 1: This paper systematically studies how to uti-
lize commonsense knowledge to answer why-questions
and their interaction with models Mof different sizes.
as an implicit approach, where we tap knowledge
that is acquired via language modeling and gen-
eral QA task pretraining. The latter is an ex-
plicit approach where we inject knowledge from
a resource as part of the context. We start by
asking three questions that can inform future re-
search along these avenues: (1)What aspects of
commonsense knowledge are already accessible
to larger language models? (2)What aspects can
be made accessible by injecting information from
relevant knowledge sources? (3)What kinds of
knowledge remains inaccessible?
For thetask, we explore the util-
ity of(Bosselut et al., 2019; Hwang
et al., 2021) as a knowledge source. is a
transformer-based model that generates common-
sense inferences about events that it has learned
from ATOMIC (Sap et al., 2019; Hwang et al.,
2021) and ConceptNet (Speer et al., 2017). How-
ever, the automatically generated knowledge may
contain incorrect or irrelevant inferences.
We start by exploring multiple ways of inte-
grating this kind of knowledge into a QA model.
First, we experiment with the best way of select-
ingrelations from that should be added
to model input. We find that adding diverse types1204of relations helps the most. Next, we investigate
various ways to integrate this knowledge into a
model’s input. While relations are usu-
ally words or phrases, converting them to sen-
tences using simple verbalization templates works
the best. Finally, we analyze the amount of exter-
nal knowledge needed by models of various sizes.
Smaller models benefit more from a larger amount
of knowledge while larger models do well with
less external knowledge.
These findings are used to build models of mul-
tiple sizes that can use external commonsense
knowledge for thetask. We use
diverse types of information from con-
verted into fluent sentences as part of the model
inputs. For small models, we supply more com-
monsense knowledge to boost model performance
while larger models are given less knowledge.
To analyze the relative merits of all these ap-
proaches, we manually categorized the Why ques-
tions according to the types of knowledge needed
to answer them. We find that most questions tar-
get Consequence, Goal seeking, Desire, and Re-
actionary knowledge types. We categorize the rest
as Other and analyze the performance of different
models across these knowledge categories. Our
analysis shows that models seem to particularly
lack the ability to understand and utilize ‘Goal
seeking’ knowledge.
In summary, our contributions are:
1. A systematic analysis of different aspects of
injecting commonsense knowledge for an-
swering why questions and their interaction
with models of different sizes
2. Developing an approach based on this analysis
to achieve a new state-of-the-art result on thedataset, and an addition of human
judgments for answers to it
3. An analysis of types of knowledge that are not
adequately captured by current models.
2 Overview: Task and Models
This section gives an overview of the data and
evaluation scheme, and defines a formulation to
describe the model configurations we investigated.
2.1 Task(Lal et al., 2021) is a dataset of 30k
questions and free-form answers concerning why
characters in short English narratives perform the
actions described. It is built on the ROCStoriescorpus (Mostafazadeh et al., 2016). The ques-
tions are created by applying templates over events
described in the narratives, and the answers are
crowd-sourced from MTurk. Each question has 3
(possibly different) human answers. The dataset
contains both explicit-answer questions ( ;
there is a possible answer to the question in the
narrative) and implicit-answer questions ( ;
the answer is not in the narrative, so external
knowledge and/or reasoning is needed). Dataset
statistics are presented in Table 9, and an example
can be found in Table 11.
2.2 Model Setup
For this task, we investigate a variety of model
configurations that add commonsense knowledge
to the input. The inputs to a given model follow
the format:
question: Q[sep] context: C[sep]G(CSK)
(1)
where Qrepresents the question and Cdenotes
the context, CSK stands for the external common-
sense knowledge being used, the function Gindi-
cates the input format of this CSK, nrepresents
the number of CSK statements being used and Ω
stands for the way the relevant knowledge is se-
lected from all available knowledge.
For our experiments, we primarily use the T5
family of models (Raffel et al., 2020). T5 is a text-
to-text model, which means it can be trained on
arbitrary tasks involving textual input and output.
T5 has achieved SOTA on many natural language
understanding (NLU) tasks, including free-form
question answering. We use HuggingFace (Wolf
et al., 2020) for our models.
Small models We start with base-sized models,
which we refer to as small models. This class of
models is the most readily accessible and works
with smaller compute resources. Lal et al. (2021)
showed that small models struggle with answering
why questions about events in narratives. Prior
work (Bi et al., 2019; Xu et al., 2021b) has
shown that adding relevant knowledge from exter-
nal sources helps models answer contextual ques-
tions. For our investigation, we focus on,
which is a 220 million parameter model.
Large models It has been shown that, as the size
of the model increases, the ability of these models
to perform NLP tasks improves. With the increase
in the number of parameters, these models are bet-
ter endowed with certain types of knowledge due1205
to pretraining (Petroni et al., 2019). To investi-
gate the performance of large models, we use the model. This 11 billion parameter model
requires significant compute resources.
Very large models Brown et al. (2020) showed
that very large models have the ability to per-
form very well on a variety of natural language
understanding tasks even in zero- and few-shot
settings. Furthermore, PaLM (Chowdhery et al.,
2022) and LaMDA (Thoppilan et al., 2022) have
shown that these models can achieve compara-
ble performance to state-of-the-art finetuned mod-
els even when used in zero- and few-shot settings
(Wei et al., 2022). For our experiments, we use
the API by OpenAI to run zero-shot experi-
ments. has around 175 billion parameters.
2.3 External Knowledge
We use (Hwang et al., 2021) as our
source of external commonsense knowledge to in-
tegrate pertinent information into the models. This
knowledge is represented through CSK in Equa-
tion 1. Such autogenerated knowledge may con-
tain incorrect or irrelevant inferences.
For the sentence in the narrative used to create
a question, we generate 3 relation phrases of dif-
ferent types from . We focus on relation
types (see Table 10 for full list of relation types
used) about people (social interaction) and events
(event-centered). also provided a score for
each generated relation.
When investigating the best approach to using , we need a ranking of the relations ac-
cording to their relevance to the associated story
and question. We calculate the BertScore (Zhang*et al., 2020) between the output for each relation
and all gold answers for a question. The resulting
list of relations sorted by the described BertScore
value is considered to be the gold ranking. We
hypothesize that this is the kind of knowledge the
model needs to answer the question correctly.
2.4 Human Evaluation Metric
We use the human evaluation templates and
MTurk settings provided by Lal et al. (2021) to
collect judgments for models’ predicted answers
on the hidden test set. We asked the annotators
whether, given the story and its associated ques-
tion, the answer shown to them was valid. Each
answer is evaluated by 3 annotators on a 5-point
Likert scale (-2 to 2). We use the average Lik-
ert score over all answers as a performance met-
ric (Liddell and Kruschke, 2018). The maximum
score possible is 2, and the minimum is -2.
Running human evaluation is expensive and
time-consuming. Additionally, slight variants of
most large models tend to generate similar an-
swers (when using beam search) for many ques-
tions. In order to improve time and cost efficiency,
we implement a caching mechanism to re-use pre-
vious annotator judgments for the same answer for
a question in a particular story. We have built
a cache of ∼7000 model-generated answers with
human evaluations and make it available so that
human evaluation on this dataset is easier in the
future. More details are in Appendix A.12063 Empirical Insights into Knowledge
Integration
We instantiate the abstract model formulation de-
scribed in Equation 1 with various knowledge inte-
gration approaches. We ask three questions about
injecting external knowledge into models to im-
prove why question answering. Our findings influ-
ence the choices we make when building the best
possible model. We use the small and large mod-
els for our investigations in this section. Examples
of each variant are shown in Table 1.
3.1 What Knowledge to Inject?
For each question, is used to retrieve a
list of possible commonsense relations across sev-
eral types. Each relational inference comes with
a score provided by , but which of these
best aids answering the why-question is an open
question. This section investigates how to select
which to use ( Ωin Equation 1). We thus hard-code
n= 3 and use ( G=verbal. ) to explore Ω. The
relations are verbalized according to the templates
presented in Table 10. More details about G
can be found in §3.2.
Intuitively, we want the external knowledge to
help produce human-like answers. To this end, we
calculate the BertScore of each inference
to human answers and use this as a gold ranking
for the external knowledge we want to add.
•Ω= (original) First, we use the
scores from in descending order as
ranks for the relations. The QA model in-
put is augmented with the top nrelations ac-
cording to these scores. Although using the ranking is the most straightforward
way to select relevant information, Table 2
shows that this approach performs poorly on
Precision@k metrics.
•Ω= (pre-trained )We start by using an off-the-shelf
pretrained ranking model: the msmarco-
distilbert-dot-v5 model available on Hug-gingFace. The question and the narrative
concatenated with the "[SEP]" symbol is
treated as the query, and the associated rela-
tional inferences are treated as the documents
in this setting. We compute the cosine simi-
larity between the query and the inferences
to rank the latter. As shown in Table 2, this
significantly improves the P@k performance
over ranking just using scores
•Ω= (fine-tuned) We
finetuned the prior pretrained ranking model
to produce "silver" ranked relations as com-
pared to the gold ranking. We use sepa-
rate query and document encoders, each with
frozen embeddings. The word embeddings
are mean-pooled to obtained sentence-level
representations for both the questions and the
relations. We compute the cosine similarity
between the query and inferences to
rank the latter. We use the pairwise ranking
loss function with the aim of optimizing Pre-
cision@5 for the ranking. To do this, we gen-
erate pairs using positive examples for ranks
1 to 4, and use the other relations to generate
negative examples. Table 2 shows that this
finetuned ranking model is clearly the best for
selecting inferences to augment our
QA models. More details about the ranking
model are available in Appendix B. Going
forward, we use to refer
to this model.
•Ω= Table 1 illustrates
that the top scoring inferences according to often involve the same relation types.
Relational inferences of the same type are
often semantically similar. We hypothesize
that models would benefit more from di-
verse knowledge rather than similar, redun-
dant knowledge. Therefore, we filter the list
of inferences to retain only the top
inference for each relation, according to its score. Finally, we take the top scor-
ing relations from this filtered list.
Finding 1: Using ranked inferences
helps. Table 3 shows the results for all possible
ways of using inferences. We see that us-
ing external knowledge in any form, even rank-
ing by scores, helps compared to mod-
els without added knowledge. Furthermore, using1207
the top inferences from the reranking model im-
proves over just using the top inferences according
to . We see that selecting diverse relations
helps the most; but using the reranking
model helpsthe most. However, the Di-
verse andReranked models per-
form similarly across both model sizes.
3.2 How to Express the Knowledge to Inject?
Task-specific knowledge can be used in different
ways (Sahand Sabour, 2021; Xu et al., 2021a). We
investigate several ways of integrating the knowl-
edge from (G(.)in Equation 1) into the
models for. We use n= 3 and
Ω = Diverse- for these experiments.
•G: This format uses special tokens
(<info> and </info>) for inferences and re-
lation types.
•G :This format adds a "\n" token after
each inference and its relation type encapsu-
lated inside <info> tags. Each of these infer-
ences is additionally separated by "\n".
•G :Prior work (He et al., 2021; Arab-
shahi et al., 2021) has shown that it helps to
add external information in a fluent natural
language. Motivated by this, we verbalize the
inferences according to their relation type us-
ing the templates presented in Table 10.
See Fig. 5 for examples of these input formats.
Finding 2a: Relations as fluent sentences helps.
Table 4 shows that commonsense in any format
improves performance. Verbalizing rela-
tions helps the most. Models are able to process
this extra information better when it is expressed
as fluent natural language sentences.
Finding 2b: Separator used is important. Prior
work (Khashabi et al., 2020) highlighted the im-
portance of separator tokens. In long texts such as
ours, it helps the model distinguish between dif-
ferent portions of the input. We found that a clear
separator token ( sep =\n) informs the model
about the input segments and thus improves the
performance of both small (performance
improves from 0.36 to 0.58) and large models (improvement from 0.99 to 1.21). Results are
presented in Table 14.
3.3 How much Knowledge to Inject?
We also investigate how the amount of knowledge
added ( nin Eq. 1) affects the performance of the
model. We set Ω = Diverse- and use
G .
Finding 3: Larger models need less knowledge.
Table 5 shows the effects of adding different num-
bers of relations. Adding 5 relations helps
the most, while does best with 3 relations.
3.4 Injecting knowledge with prompts
To extend upon the insights of Finding 3, we
also experiment with a very large model ( ),
which performs well on many NLP problems but
may still exhibit a lack commonsense (Bender and
Koller, 2020). With the right prompts , very large
models have been shown to work well even in a
zero-shot setting (Ouyang et al., 2022) because
they may already encode much of the information
needed to perform the task. We prompt with
the narrative context ( N), the question ( Q) and1208the knowledge ( CSK ) and the model autoregres-
sively generates a sequence. Our use of knowledge
in the prompt is a form of “prompt engineering”,
where ’s behavior is modified by enhancing
the prompt (Le Scao and Rush, 2021). We enhance
the input by simply injecting commonsense that
nudges the model towards the correct answer. See
Table 12 for examples of different prompts.
Unlike finetuning, in a zero-shot setting, the
model has no opportunity to learn when and how
to apply CSK . Therefore, it is imperative to inject
CSK into the prompt in the best possible manner.
We experimented with providing CSK before N
(prefix), after N(postfix), and finally by inserting
CSK after the sentence from which Qwas cre-
ated (infix). Infix injection works best because it
allows the model to encode the sentence of interest
with a richer context. Postfix injection forces the
autoregressive model to pay more attention to po-
tentially noisy CSK , and prefix injection leads to
the knowledge being often ignored perhaps due to
the distance from the question. Thus, we chose in-
fix injection as the preferred prompting approach.
4 Distilling the Empirical Insights: TheApproach
Finally, we combine our findings to create theapproach. We use it to build the best
possible models of all sizes — small, large and
very large — for thetask. From Find-
ings 1-2, we use sep =\n,G andΩ =
Diverse- . From Finding 3, we use n= 5
for the small models, n= 3for large models, and
n= 1for very large models.
Injecting knowledge helps. For each scale of
model under investigation, we compare versions
with and without external knowledge. Table 6
shows the overall human evaluation numbers on
the hidden test set of thedataset as
calculated according to §2.4.
Injecting external knowledge helps the small
models the most. While overall performance im-
proves, the biggest improvement is on implicit
questions, where the answer is not available in the
narrative. This shows that such external knowl-
edge can significantly fill gaps in small models.
Additonally, we find that external knowledge
improves the performance of very large models
( ) more than it does for large models (). This can be due to various reasons. First, is used in a zero-shot setting while the oth-
ers are finetuned. Second, it is possible that very
large models have a greater capacity to use exter-
nal information.
Scale matters. Table 6 indicates that just in-
creasing the scale of the model results in a large
performance boost (5x higher than the previous
SOTA Lal et al. (2021), which achieves 0.36 on
Full and -0.27 on on the Avg Likert metric
§2.4). Judiciously adding knowledge helps across
all model sizes. Large models outperform small
models, even when small models are augmented
with external knowledge. Interestingly, adding ex-
ternal, relevant commonsense knowledge still sig-
nificantly helps large and very large models cor-
rectly answer questions. and aug-
mented with knowledge achieve the best perfor-
mance on this dataset and come very close to hu-
man performance on the Avg Likert metric.
4.1 Have models actually reached human
performance?
To investigate this, we compare scores for humans
and models on the spectrum of the Likert scale.
Table 7 suggests that (very) large models are
unable to maintain peak performance consistently.1209
For a Likert score of +2, they outperform humans:
0.44 vs. 0.52 for . Figure 3 shows an exam-
ple where the model answer is judged by humans
to be better than a human answer. However, unlike
humans, this performance is inconsistent. Models
generate more answers given scores 0, -1, and -2.
Figure 3 also shows an example where the model
generates a terrible answer that is rated -2, a score
that no human answer is ever given. This is in line
with Bender and Koller (2020): large models are
on topic, but can be unclear or fail to make sense.
To make the comparison with human perfor-
mance clearer, similar to (Lal et al., 2021), we
collapse the 5-point average Likert into a binary
measure of accuracy (only scores of 1 and 2 are
counted as correct). On this binary accuracy met-
ric, as shown in Table 6, humans are almost per-
fect, with an accuracy of 99%. However, the
best model ( with knowledge) only achieves
87%, indicating that there is still significant room
for improvement on this task.
5 Analysis
To better understand the strengths and weaknesses
of these models, we defined an ontology for the
types of knowledge that are required to answer
TellMeWhy questions. We identified five cate-
gories of questions, and then labeled the CaTeRs
subset of TellMeWhy, for which the gold answers
already have human evaluation judgments.12105.1 Question distribution in subset
The categories are:
•Consequence (30.2%) : an event happened
as a consequence of another event.
•Goal-seeking (29.1%) : an agent performed
an action as an intermediate step to a goal.
•Reactionary (25.4%) : an agent performed
an action as a reaction to another event.
•Desire (8.8%) : an agent performed an action
to accomplish an inherent goal.
•Other (6.5%) : types of knowledge that do
not fall into the categories above.
Examples of each type can be found in Figure 6.
Since implicit answer questions ( ) require
knowledge outside the text, we analyze them to
study the gaps in the models’ understanding and
identify possible areas for improvement. Fig-
ure 2 presents binary accuracy of all models across
question types.
To quantify the differences across models, we
compute a failure probability for each category,
i.e., the probability of an incorrectly answered
question (Avg 5pt Likert score of the model an-
swer to the question <1) belonging to a given cat-
egory. We compute this by dividing the number of
incorrectly answered questions of that knowledge
type by the total number of wrong answers. We
measure the differences in these failure probability
distributions across models using Jensen-Shannon
Divergence (JSD).
5.2 Reasoning that Models Lack
Figure 2 shows that small models are unable to
reason adequately about all knowledge types, and
adding external knowledge boosts performance
across the board, particularly for ‘Consequence’
questions. As the model size increases, it’s un-
derstanding of each type also increases. However,
without knowledge, there is a huge gap in the per-
formance of even the largest models when com-
pared to humans across all categories, showing
that understanding all of the aspects of an event
needed to answer why questions is hard.
The JSD of the failure probability distributions
forand across categories is only
0.13and and is0.14. This suggests
only a small difference in the knowledge types
these models fail to capture.
5.3 Where Knowledge Helps
Figure 2 shows how adding knowledge helps
for questions of different categories. Exter-
nal knowledge consistently helps models of all
sizes, except that it hurts significantly for ‘Reac-
tionary’ questions. For ‘Consequence’ questions,
adding CSK pushes(a 110M parameter
model) to close to (11B parameter model)
even though the latter is finetuned without CSK.
Adding CSK improves the most on ‘Goal-
seeking’ questions. Figure 4 shows an example
where knowledge helps.
6 Related Work
6.1 Knowledge Bases
Knowledge bases (KBs) are a reliable source of
world facts and relationships between common
concepts. They can be constructed through semi-
automated extraction over text (Speer et al., 2017;
Tandon et al., 2017) or through crowdsourcing
(Sap et al., 2019).
Petroni et al. (2019) show that, instead of these
approaches, pretraining language models on text
already endows them with certain types of factual
knowledge that helps them do well on QA tasks.
More recently, a popular approach is to fine-tune
a language model on existing KBs, to general-
ize their knowledge and pay attention to the con-
text, e.g., (Bosselut et al., 2019; Hwang
et al., 2021) generates context-relevant common-
sense knowledge. It is a fine-tuned language
model over ATOMIC and ConceptNet KBs. Sim-
ilarly, ParaCOMET (Gabriel et al., 2021) is a lan-
guage model fine-tuned for discourse knowledge
by fine-tuning over ROCStories, thus it generates
relations consistent with an input narrative.12116.2 Incorporating External Knowledge
Model outputs have been improved through com-
monsense injection using regularization at training
time (Guan et al., 2020) or simply by appending to
the input (Lewis et al., 2020; Talmor et al., 2020).
There are two key challenges in using external
sources. The first is figuring out what knowledge
to use and the second is determining how to effec-
tively integrate it into the end task.
Some recent research injects triples into sen-
tences in order to create domain-specific knowl-
edge (Liu et al., 2020; Wang et al., 2020). Huang
et al. (2019) incorporate commonsense knowledge
directly into training data. Feng et al. (2020) lever-
age relations from ConceptNet using structured re-
lational attention to perform multi-hop QA. How-
ever, there is still uncertainty on the proper way
to use external knowledge to solve commonsense
reasoning problems (Zhang et al., 2020).
ERNIE (Zhang et al., 2019) is an enhanced lan-
guage representation model trained using large-
scale corpora and knowledge graphs that shows
significant improvements on various knowledge-
driven tasks. Xiong et al. (2020) propose a weakly
supervised pretraining objective, which explicitly
forces the model to incorporate knowledge about
real-world entities to perform entity-related QA
tasks. KGLM (Logan et al., 2019) is a neural lan-
guage model with mechanisms for selecting and
copying facts from a knowledge graph that are rel-
evant to the context.
KagNet (Lin et al., 2019) grounds a QA pair
in CommonsenseQA (Talmor et al., 2019) from
the semantic space to the knowledge-based sym-
bolic space as a schema graph, uses a KG-aware
module to focus on it, and scores answers with
graph representations. Lv et al. (2020) propose
a graph-based contextual representation learning
and inference module to better use graph in-
formation for commonsense QA. Shwartz et al.
(2020) generate and integrate background knowl-
edge from pretrained LMs to develop an unsu-
pervised framework for multiple-choice common-
sense tasks. Generated knowledge prompting elic-
its and integrates knowledge from language mod-
els using task-specific, human-written, few-shot
demonstrations so as to improve performance on
commonsense reasoning tasks (Liu et al., 2021).7 Conclusion
Answering why questions requires several forms
of commonsense knowledge. This paper inves-
tigates different aspects of incorporating external
knowledge to improve this process. We discover
several empirical insights on how to incorporate
external knowledge. By incorporating these in-
sights, our approach,, successfully uses
external knowledge to help models of all sizes
(small, large, and very large) answer why ques-
tions better; but they still fall short of human per-
formance. Questions that involve implicit infer-
ences are harder for all the models, and require
modeling innovations. Our investigation opens up
interesting questions, such as learning when and
how to add external knowledge, in order to further
close the gap with humans.
8 Limitations
Reproducing our experiments for requires
extensive compute resources, including TPUs,
while running zero-shot experiments with
requires access to the paid OpenAI API.
Few-shot prompting and in-context learning are
also popular ways of using models like for
various tasks. We leave the exploration of such
methods forfor future work.
Our investigation into adding external common-
sense to help NLP models perform better is lim-
ited to just one dataset that focuses on why ques-
tion answering. Indeed, such knowledge is also re-
quired to enhance other reasoning capabilities of a
model. It would be interesting to see how our find-
ings would transfer to other tasks. as a source of knowledge is limited in
the quality and relevance of information it can
provide. Studying other sources of commonsense
knowledge would be another productive area for
future work.
Finally, human evaluation of free-form model
answers is expensive and time-consuming. Even
though our current answer cache is fairly sizeable,
there is non-trivial time and expense involved in
following the evaluation suggested by Lal et al.
(2021).
Acknowledgements
This material is based on research that is sup-
ported in part by the Air Force Research Labora-
tory (AFRL), DARPA, for the KAIROS program1212under agreement number FA8750-19-2-1003 and
in part by the National Science Foundation under
the award IIS #2007290. The authors would like to
thank the anonymous reviewers and the area chair
for their feedback on this work. We would also
like to thank Jierui Li for her suggestions on the
camera ready version.
References121312141215
A Caching for Human Evaluation
In order to improve time and cost efficiency, we
implement a caching mechanism to re-use previ-
ous annotator judgments for the same answer for a
question in a particular story. For this purpose, we
save all the human judgments for a (question, an-
swer, story) triple. For all model predictions, we
first check if a (question, answer, story) tripleis
already present in the cache. If it is, we use the
old judgments for it. If not, we gather validity an-
notations for it using human evaluation and add
them to the cache for future use. We have built up
a cache of ∼7000 model-generated answers and
will release it so that it becomes easier to perform
human evaluation on this dataset in the future.
B Building the Ranking Model
Configuration P@3 P@5 P@10
Q: narrative + ques 0.32 0.41 0.57
D: relation phrase
Q: ques 0.25 0.34 0.50
D: relation phrase
Here, we provide details into our experiments
with finetuning the pretrained msmarco-distilbert-dot-v5 model to rank the relations associ-
ated with a context and question.
We experimented with different formulations of
queries and documents. Table 8 shows the ranking
precision scores of the pretrained model for dif-
ferent configurations. We select the first one for
finetuning as it has a higher P@k suggesting that
adding the context after a question is most helpful.
We used Adam optimizer with a learning rate of
1e-05 and weight decay of 1e-04. The batch size
was 1 and we used Precision@5 scores to select
the best finetuned model.
C Hyperparameters
C.1
For, we train the model with batch size
16, learning rate 5e-5 and maximum answer length
30. We vary the source length from 75 to 450 ac-
cording to the amount of external knowledge be-
ing injected into the input context. The model is
trained until the dev loss fails to improve for 3 it-
erations. Training usually takes 7-8 hr on 1 Titan
Xp GPU.
C.2
For training themodel, we followed a de-
fault set of hyperparameters that are recommended
in (Raffel et al., 2020). model has 110M parameters with
24-layers, 1024-hidden-state, 4096 feed-forward
hidden-state, and 16 attention heads.
model has 11B parameters with 24-layers, 1024-
hidden-state, 65,536 feed-forward hidden-state,
128 attention heads. We use TPU (v3-8) on google
cloud platform. It takes 6 hours in average to train
the model.
C.3
We used a temperature of 0.0 for all the experi-
ments to select the most likely token at each step,
as this setting allow for reproducibility.1216
The frequency penalty penalizes new tokens
based on existing frequency in text so far, while
the presence penalty sets the model’s likelihood to
talk about novel topics.
Split # stories # questions
Train 7558 23964
Dev 944 2992
Test 944 3099
Hidden Test 190 464
Total 9,636 30,519
D Automatic Metrics
Table 13 shows the values for various automatic
metrics for different models we built. We adapt
the evaluation script released by Lal et al. (2021)
to obtain these numbers.Story: Sandra got a job at the zoo. She loved
coming to work and seeing all of the animals.
Sandra went to look at the polar bears during her
lunch break. She watched them eat fish and jump
in and out of the water. She took pictures and
shared them with her friends.
Question: Why did Sandra go to look at the po-
lar bears during her lunch break?
Ans: she wanted to take some pictures of them.
Story: Cam ordered a pizza and took it home.
He opened the box to take out a slice. Cam dis-
covered that the store did not cut the pizza for
him. He looked for his pizza cutter but did not
find it. He had to use his chef knife to cut a slice.
Question: Why did Cam order a pizza?
Ans: Cam was hungry.12171218Full Impl
Small ()w/o sep. 0.36 -0.27
w/ sep. 0.58 0.02
Large ( )w/o sep. 0.99 0.6
w/ sep. 1.21 0.971219