
Haoyu Cao, Jiefeng Ma, Antai Guo, Yiqing Hu, Hao Liu, Deqiang Jiang,
Yinsong Liu, Bo RenTencent YouTu LabUniversity of Science and Technology of China
{rechycao,ankerguo,hooverhu,ivanhliu,dqiangjiang}@tencent.com
jfma@mail.ustc.edu.cn {jasonysliu,timren}@tencent.com
Abstract
Document Information Extraction (DIE) has
attracted increasing attention due to its various
advanced applications in the real world. Al-
though recent literature has already achieved
competitive results, these approaches usually
fail when dealing with complex documents
with noisy OCR results or mutative layouts.
This paper proposes Generative Multi-modal
Network (GMN) for real-world scenarios to ad-
dress these problems, which is a robust multi-
modal generation method without predefined
label categories. With the carefully designed
spatial encoder and modal-aware mask mod-
ule, GMN can deal with complex documents
that are hard to serialized into sequential or-
der. Moreover, GMN tolerates errors in OCR
results and requires no character-level annota-
tion, which is vital because fine-grained anno-
tation of numerous documents is laborious and
even requires annotators with specialized do-
main knowledge. Extensive experiments show
that GMN achieves new state-of-the-art per-
formance on several public DIE datasets and
surpasses other methods by a large margin, es-
pecially in realistic scenes.
1 Introduction
Document Information Extraction (DIE) aims to
map each document to a structured form consistent
with the target ontology ( e.g., database schema),
which has recently become an increasingly impor-
tant task. Recent research (Xu et al., 2020, 2021;
Wang et al., 2021a; Zhang et al., 2020; Li et al.,
2021a) has achieved competitive results for infor-
mation extraction in the idealized scenario with
accurate OCR results, word-level annotations, and
serialized document words in reading order. These
methods regard DIE as a Sequence Labeling (SL)
task. Given OCR results of a document image, the
traditional sequence labeling method first serializesFigure 1: Examples in public DIE benchmarks with
practical problems. The three rows from top to bottom in
each sub-figure are 1) input images, 2) raw intermediate
tags and final results generated by the SL method, and
3) results of our GMN method, respectively. The error
parts are marked in red, while the correct parts are in
green. Best viewed in color.
words in reading order then classifies each input
word into predefined categories.
As shown in Figure 1, multiple challenging prob-
lems for practical document understanding still ex-
ist in realistic scenes. 1) Document serialization
requires pre-composition processing, which is diffi-
cult in real scenarios with ambiguous word orders.
One entity may be incorrectly divided into multi-
ple entities when the input sequences are sorted
by coordinates. 2) OCR results are usually noisy
because of inevitable recognition errors. 3) The
volume of keys in practical scenarios is generally
substantial and expanded frequently. Existing se-
quence labeling methods could not identify unde-
fined keys. 4) While facing duplicated values,
collecting word-level annotations is necessary for
sequence labeling methods. However, this is diffi-
cult in practical scenarios since they are costly and
labor-intensive.
To address the limitations mentioned above, we
propose a robust information extraction method3768named Generative Multi-modal Network (GMN)
for practical document understanding. Unlike se-
quence labeling methods that label each input word
with a predefined category, we regard DIE as a
translation task that translates source OCR results
to a structured format (like key-value pairs in this
paper). We use UniLM (Dong et al., 2019) as
the basic model structure, which is a transformer-
based pre-trained network that can handle both
natural language understanding (NLU) and natural
language generation (NLG) tasks simultaneously.
Conditioned on a sequence of source words, GMN
generates one word at each time step to compose a
series of key-value pairs in them.
Regarding the sequence serialization problem, a
novel two-dimensional position embedding method
is proposed while the original one-dimensional po-
sitional embedding in the transformer is removed
because all information in document understand-
ing can be acquired from 2D layouts. In this
manner, GMN bypasses the serialization problem.
Furthermore, benefiting from the large-scale self-
supervised pre-training processed on a vast docu-
ment collection, GMN can correct OCR errors com-
monly encountered in practical scenarios. More-
over, using a weakly supervised training strategy
that utilizes only key information sequences as su-
pervision, GMN needs no word-level annotations
that are indispensable in traditional sequence label-
ing methods like LayoutLM (Xu et al., 2020) and
StructuralLM (Li et al., 2021a).
Experiments illustrate that the proposed GMN
model outperforms several SOTA pre-trained mod-
els on benchmark datasets, including SROIE and
CORD. The contributions of this paper are summa-
rized as follows:
1)We present GMN tailored for the DIE task,
which is more applicable for practical scenar-
ios, including lack of word-level annotations,
OCR errors as well as various layouts.
2)We propose a layout embedding method and
multi-modal Transformer in a decoupling
manner, which jointly models interactions be-
tween multiple modalities and avoids the read-
ing order serialization.
3)Experiments on public DIE datasets demon-
strate that the proposed method not only
achieves a substantial performance improve-
ment but also generalizes well to data under
practical scenarios with unseen keys.2 RELATED WORKS
Traditional methods (Esser et al., 2012; Schus-
ter et al., 2013; Riloff, 1993) on DIE tasks rely
heavily on predefined rules, templates, and hand-
crafted features, giving rise to difficulty in gen-
eralizing to unseen documents. With the devel-
opment of deep learning technology, document
information extraction methods have recently im-
proved substantially in both performance and ro-
bustness. These deep learning-based methods can
be classified into three categories: textual content-
based methods, multi-modal-based methods, and
pre-trained Transformer-based methods.
Textual content-based methods . Palm et al.
(2017); Sage et al. (2019) adopt the idea from nat-
ural language processing and use recurrent neural
networks (RNN) to extract entities of interest from
documents. However, they discard the layout in-
formation during the text serialization, which is
crucial for document understanding.
Multi-modal-based methods . Some works
(Katti et al., 2018; Hwang et al., 2021) take the
layout information into consideration and try to
reconstruct character or word segmentation of the
document. Katti et al. (2018) encode each doc-
ument page as a two-dimensional grid of char-
acters that represents text representation with a
two-dimensional layout. Yu et al. (2020); Ma-
jumder et al. (2020); Zhang et al. (2020); Wang
et al. (2021b) further integrate image embeddings
for better feature extraction. Yu et al. (2020); Tang
et al. (2021) represent documents by graphs, with
nodes representing word segments and edges ei-
ther connecting all the nodes or only spatially near
neighbors. Convolutional or recurrent mechanisms
are then applied to the graph for predicting the field
type of each node. However, due to the lack of
large-scale pre-training, the robustness and accu-
racy of the model are relatively limited.
Pre-trained Transformer-based methods . Re-
cently, pre-trained models (Devlin et al., 2019; Liu
et al., 2019) show effective knowledge transferabil-
ity with large-scale training data and various self-
supervised tasks. LayoutLM (Xu et al., 2020) first
proposes a document-level pre-training framework
that semantic and layout information are jointly
learned. LayoutLM V2 (Xu et al., 2021) further
improves the LayoutLM model by integrating the
image information in the pre-training stage. Li et al.
(2021a) propose the StructuralLM pre-training ap-
proach to exploit text block information. Methods3769
mentioned above all use one-dimensional position
embeddings to model the word sequence, even with
two-dimensional layout embeddings are involved,
so that the reading order serialization in the doc-
ument is required, which is challenging or even
impossible due to the complex and diverse layout
in the real world. What’s more, they are all based
on the classification of each input text segment to
predefined labels, which means fine-grained an-
notations are indispensable and lack the ability to
correct error OCR results.
On the contrary, the proposed GMN relies on a
two-dimensional position embedding to bypass the
serialization process and cross-modality encoders
in a decoupling manner to model the layout infor-
mation and the relative position of a word within a
document simultaneously.
3 METHODOLOGY
In this section, we first introduce the overall ar-
chitecture of GMN, followed by illustrating multi-
modal feature extraction, generative pre-training
model with multi-modal decoupling in detail, re-
spectively.
3.1 Overall Architecture
GMN aims at constructing an enhanced
Transformer-based translator architecture for
DIE for converting the document to structured,machine-readable data. An overview of the
architecture is as shown in Figure 2. It mainly
consists of two parts: the multi-modal feature
extraction module and stacked cross-modality
module named MD-Bert (Modal Decoupling
Bert), which simultaneously serves as encoder and
decoder following the design of UniLM.
The whole process can be summarized as 1)
Multi-modality embeddings of source inputs are
extracted through an advanced OCR engine and
a small CNN; 2) The extracted features from dif-
ferent modalities are fused as “multi-modal em-
beddings” through MD-Bert along with memory
updating for each layer at each time step; 3) Next,
MD-Bert output the encoding results by applying
token prediction on multi-modal embeddings; 4)
Finally, MD-Bert recursively generates structured
results by taking multi-modal embeddings and ac-
cumulative memory as inputs until a terminator
[SEP] is predicted.
3.2 Multi-Modal Feature Extraction
Based on the multi-modal information, including
semantics, layout, and vision, we propose a unified
layout embedding method named Cord2Vec which
simultaneously encodes sequence information and
spatial information to avoid complex reading order
serialization.37703.2.1 Semantic Embedding
Intuitively, semantic contents are reliable signals to
extract valuable information. The semantic content
of each text fragment is acquired from the results of
the OCR engine for practical application scenarios.
After text fragments are acquired and tokenized,
the start indicator tag [BEG] is added in front of
the input token sequence, and the end indicator tag
[SEP] is also appended to the end. Extra padding
tag [PAD] is used to unify the length of sequence
with predefined batch length L. In this way, we can
get the input token sequence Sas
Here, trefers to i-th token in OCR texts. More-
over, though the sequence length of the input token
sequence is fixed during training, GMN can handle
variable lengths when making the inference due to
the novel positional embedding method.
3.2.2 Layout Embedding
DIE task is a typical two-dimensional scene in
which relative positions of words are essential ev-
idence. While the reading order serialization is
challenging, we propose Cord2Vec , a unified em-
bedding method that fully utilizes spatial coordi-
nates rather than one-dimensional sequence order
information to bypass this problem.
As for the source input part, we normalize and
discretize all coordinates to the integer in the range
of[0, α], here αis the max scale which is set to
1000 in our experiment. Then corner coordinates
and edge lengths of each text fragment are gained
using corresponding bounding boxes. In order to
enhance the tokens’ interaction in the same box,
two tuples (x, x, w),(y, y, h)are used to rep-
resent the layout information. Here, (x, y)and
(x, y)are the top-left and bottom-right coordi-
nates of each token, and wis the average width of
tokens in the same box while hrepresenting box
height. Such embedding represents both the layout
and the word order information. As for target to-
kens generated by GMN which does not have the
real coordinate, the Cord2Vec assumes each token
is tied in the grid of [W, H]with row-first
principle, and each token occupies a pixel with a
width and height of 1. After the layout information
is acquired, we use two embedding layers to em-
bed x-axis features and y-axis features separately
as stated in Equation 2.Here, PosEmb2DandPosEmb2Dare the posi-
tion embedding function which takes coordinate
as input. Each input element is embedded sepa-
rately and then added together with an element-
wise function. Note that the placeholder such
as [PAD] can be treated as some evenly divided
grids, so their bounding box coordinates are easy
to calculate. An empty bounding box X=
(0,0,0), Y= (0,0,0)is attached to [PAD], and
X= (0, w, w ), Y= (0, h, h)is attached to
other special tokens including [BEG] and [SEP].
3.2.3 Visual Embedding
We use ResNet-18 (He et al., 2016) as the back-
bone of the visual encoder. Given a document page
image I, it is first resized to W∗Hthen fed into
the visual backbone. After that, the feature map is
scaled to a fixed size by average-pooling with the
width being W/n and height being H/n ,nis the
scaling scale. Finally, RoI Align (He et al., 2017)
is applied to extract each token’s visual embedding
with a fixed size. The visual embedding of the i-th
token is denoted by v∈(v, v, v, . . . , v). For
source input, visual embedding can be represented
as
Here, Posstands for the position of i-th token, and
ConvNet is a convolutional neural network serving
as feature extractor in terms of input image, and
then ROIAlign takes the image feature and location
as input, and extracts the corresponding image fea-
tures. Note that the [BEG] token represents the full
image feature, and the other special tokens, as well
as output tokens, are attached to the default null
image feature.
3.3 Generative Pre-training Model
3.3.1 Model Structure
In order to learn more general features and make
full use of the pre-training data, we propose a
unified encoder-decoder module named MD-Bert
which is composed of stacked hierarchical multi-
modal Transformer encoders. The context of input
tokens is from OCR result during the encoding
stage, while already decoded tokens are also in-
cluded in the decoding stage. To solve this prob-
lem, inspired by UniLM, we use masking to control
which part of context the token should attend to
when computing its contextualized representation,
as shown in Figure 3. The input features of se-
mantics, layout and computer vision are mapped to
hidden states by: F=/braceleftbig
f, . . . , f/bracerightbig
,F=3771
/braceleftbig
f, . . . , f/bracerightbig
,F=/braceleftbig
f, . . . , f/bracerightbig
,F=/braceleftbig
f, . . . , f/bracerightbig
,a linear mapping is as follows:
where matrices W∈R,W∈
R,W∈R,W∈R
are used to project features into hidden-state
inddimensions. MD-Bert takes Fand
memory Mof history state as input, gener-
ates output and update memory Mstep by
step. where, F∈ {F, F, F, F}, M∈
{M, M, M, M},Mcontains the history
state of each layer and previous embeddings of
the model. In the first timestep, Mis initialized
from scratch, input Tmeans the full OCR result
and MD-Bert acts as bi-directional encoder. For
timestep t, where t∈[1, m], the model takes the
output of the previous timestep or [BEG ]as in-
put, and outputs the current result, MD-Bert acts
as uni-directional decoder.
3.3.2 Cross-Modality Encoder
Traditional multi-modal models usually fuse dif-
ferent modal features by adding or concatenating
them together, which inevitably introduces the un-
wanted correlations between each other during self
attention procedure, e.g.word-to-position, image-
to-word. However, these correlations are harmful
to strengthening the model’s capability as different
data modalities are practically orthogonal, thus we
need to design a customized pipeline for each one.We propose MAMM (modal-aware mask module)
encoder, a hierarchical structure multi-modal Trans-
former model in a decoupling manner that jointly
models different modalities. As a consequence, the
feature embedding decoupling is decomposed into
three embeddings in GMN. The MAMM module
follows the design of a basic module in BERT, but
replaces the multi-head attention with modal-aware
multi-head attention.
It also contains feed-forward (FF) layers, resid-
ual connections, and layer normalizations (LN),
meanwhile, parameters of modals are not shared.
When feeded with different modal content such as
semantics, layout and computer vision, MAMM
first calculates each modal’s attention score sepa-
rately, then added these attention scores together
to get a fusion score, finally use this fusion score
to apply masking and following operations on se-
mantic content. As shown in Figure 3, let F=
{f, . . . , f}be the encoded feature in the l-th
layer.Fis the vector of the input features as
mentioned in Equation 4. Features output by the
next layer Fcan be obtained via:
where f (·)is the modal-aware mask function
defined as
where q(·),k(·),v(·)are linear transformation lay-
ers applied to the proposals’ feature, which repre-
sent the query, key and value in attention mecha-
nism accordingly. Benefited from the parameters’
sharing among layers with regard to X, Y and Vi-
sion, GMN has comparable weights as Bert. Sym-
boldis the number of attention headers for nor-
malization, and the f is the Mask operation
which controls the attention between each token.
In GMN, we apply full attention on all OCR input
tokens, and input tokens of the model for the output
structural sequence can attend to the whole inputs
as well as tokens that have been decoded which are
like auto-regressive encoder. Finally, Fcan be
obtained by Fvia a feed-forward sub-layer
composed of two fully-att connected layers of func-
tionf(·). Hierarchically stacked layers form the
multi-modal encoder.37723.3.3 Pre-training Method
Similar to UniLM, three cloze tasks including Uni-
directional LM, Bidirectional LM and Sequence-
to-Sequence LM are used in the GMN. Meanwhile,
we propose NER-LM for better entity correlation
extraction. The whole loss function is defined as,
L=L +L+L +L (9)
In a cloze task, we randomly choose some Word-
Piece (Wu et al., 2016) tokens in the input, and re-
place them with the special token [MASK ]. Then,
we feed their corresponding output vectors com-
puted by the Transformer network into a softmax
classifier to predict the masked token. The parame-
ters of GMN are learned to minimize cross-entropy
loss, which is computed using the predicted tokens
and the original tokens.
NER-LM is an extension of sequence-to-
sequence LM for better integrity constraints on
the entity. Given the source segment which in-
cludes entity values s, s, and the correspond-
ing entity types n, nas well as some back-
ground sentence e.g. b, b, we form the in-
put format as A “ [BEG ]sbsb[SEP ]” and B
“[BEG ]nsns[SEP ]”. Each token in A can ac-
cess all others of A, while each token in B can
access all tokens of A as well as the preceded to-
kens in B. The target entity in B is masked for
prediction during training.
4 EXPERIMENTS
4.1 Dataset
4.1.1 Pre-training Dataset
Our model is pre-trained on the IIT-CDIP Test Col-
lection 1.0 (Lewis et al., 2006), which contains
more than 6 million documents, with more than
11 million scanned document images. Moreover,
each document has its corresponding text and meta-
data stored in XML files which describe the prop-
erties of the document, such as the unique iden-
tity and document labels. And the NER-LM is
pre-trained on the Enron Email Dataset (Klimt and
Yang, 2004), which contains 0.5 million emails gen-
erated by employees of the Enron Corporation. We
follow the organization of the letter and generate
the content on the image. The structured informa-
tion in the letters acts as an entity, such as subject,
date, etc.4.1.2 Fine-tuning Datasets
We conduct experiments on three real-world public
datasets, FUNSD-R, CORD and SROIE.
The FUNSD-R Dataset. FUNSD (Jaume et al.,
2019) is a public dataset of 199 fully annotated
forms, which is composed of 4 entity types ( i.e.
Question, Answer, Header and Other). The origi-
nal dataset has both semantic entity extraction (EE)
and semantic entity linking (EL) tasks. It’s note-
worthy that the linking between different entities
are complicated, one header entity may have link-
ing to several question entities with more answer
entities linked.
To better evaluate the system performance in
the multi-key scenario, we relabel the dataset in
key-value pairs format to tackle EE and EL tasks si-
multaneously. We named the new dataset FUNSD-
R, which contains 1,421 keys for training and 397
keys for testing. Meanwhile, there are 267 keys in
the test set that have not appeared in the training
set. FUNSD-R will be released soon.
The CORD Dataset. The CORD (Park et al.,
2019) dataset contains 800 receipts for the training
set, 100 for the validation set and 100 for the test
set. The dataset defines 30 fields under 4 categories
and the task aims to label each word to the right
field.
The SROIE Dataset. SROIE (Huang et al.,
2019) dataset contains 626 receipts for training
and 347 receipts for testing. Each entity of the re-
ceipt is annotated with pre-defined categories such
as company, date, address, and total.
To further investigate the capacity of our pro-
posed method under more challenging scenarios,
we expand “SROIE” and “CORD” datasets to
“SROIE-S” and “CORD-S” by shuffling the order
of text lines and keep the box coordinates to simu-
late complex layouts. The evaluation metric is the
exact match of the entity recognition results in the
F1 score.
4.2 Implementation Details
Model Pre-training . We initialize the weight of
GMN model with the pre-trained UniLM base
model except for the position embedding layer and
visual embedding layer. Specifically, our BASE
model has the same architecture: a 12-layer Trans-
former with 768 hidden sizes, and 12 attention
heads. For the LARGE setting, our model has a
24-layer Transformer with 1,024 hidden sizes and
16 attention heads, which is initialized by the pre-3773trained UniLM LARGE model. For unidir-LM and
bidir-LM methods, we select 15% of the input to-
kens of sentence A for prediction. We replace these
masked tokens with the [MASK ]token 80% of
the time, a random token 10% of the time, and an
unchanged token 10% of the time. For seq-to-seq
LM and NER-LM, we select 15% tokens of the
sentence B. The target of the token is the next to-
ken. Then, the model predicts the corresponding
token with the cross-entropy loss.
In addition, we also add the two-dimensional
position embedding and visual embedding for pre-
training. Considering that the document layout
may vary in different page sizes, we scale the ac-
tual coordinate to a “virtual" coordinate: the actual
coordinate is scaled to have a value from 0 to 1,000,
and rescale the images to the size of 512×512.
We train our model on 64 NVIDIA Tesla V100
32GB GPUs with a total batch size of 1,024. The
Adam optimizer is used with an initial learning rate
of 5e-5 and a linear decay learning rate schedule.
Task-specific Fine-tunings . We evaluate the
model following the typical fine-tuning strategy
and update all parameters in an end-to-end way
on task-specific datasets. We arrange the source
OCR result from top to bottom and left to right.
In addition, we add the “ [DSEP ]” as the separa-
tor between text detection boxes. In SROIE and
CORD datasets, we construct the target key-value
pairs in a certain order due to the keys being lim-
ited. ( i.e.company, date, address, total). In the
FUNSD dataset, we organize the target key-value
pairs from top to bottom and left to right. We add
the “:” as the separator between key and value
and “[DSEP ]” as the separator between key-value
pairs. The max source length parameter is set to
768 in the SROIE and CORD datasets and 1536 in
the FUNSD-R datasets, so input sequences below
max length will be padding to the same length. The
model is trained for 100 epochs with a batch size of
48 and a learning rate of 5e-5. Note that, the annota-
tions of all GMN results are the weakly-supervised
label of sentence-level while other methods use
word-level annotations.
4.3 Comparison to State-of-the-Arts
We compare our method with several state-of-the-
arts on the FUNSD-R, SROIE and CORD bench-
marks. We use the publicly available PyTorch mod-
els for BERT, UniLM and LayoutLM in all the
experiment settings. The results of PICK (Yu et al.,
2020), MatchVIE (Tang et al., 2021), BROS (Hong
et al., 2021), StrucTexT (Li et al., 2021b), SPADE
(Hwang et al., 2021) and DocFormer (Appalaraju
et al., 2021) are obtained from the original papers.
Results under scene with larger amount of
keys. Table 1 shows the model results on the
FUNSD-R dataset which is evaluated using entity-
level precision, recall and F1 score. In the case
of a large number of key categories, especially in
the case that some categories have not appeared
in the training set, the method based on sequence
labeling yield, neither the Bert model, which only
contains text modality nor the LayoutLM which
also contains layout and visual modalities.
The best performance is achieved by the
GMN , where a significant improvement
is observed compared to other methods. Note that,
67.25% of keys have not appeared in the training
set, This illustrates that the generative method in
GMN is suitable for scenes with a large number of
keys.
Results with Ground Truth Setting. Under
this setting, the ground truth texts are adopted as
model input. As shown in Table 2 and Table 3,
even using weakly supervised labels, our approach
shows excellent performance on both SROIE and
CORD, and yields new SOTA results, which in-
dicates that GMN has a powerful representational
capability and can significantly boost the perfor-
mance on DIE tasks.3774
Results with End-to-End Setting. We adopt
Tesseract as OCR engine to get the OCR result
of public datasets. It’s worth noting that there
are exist some OCR errors, the sequence labeling
method can not handle, but in our GMN, the match-
ing process between OCR results and ground truth
is avoided thanks to the novel layout embedding
method in an end-to-end training setting. The per-
formances are shown in Table 4. Our method shows
new state-of-the-art performance benefits from the
ability to error correction. A detailed analysis of it
is introduced in case studies B.
Results with Position Shuffle Setting. In order
to verify the robustness of our two-dimensional
embedding method, we apply a shuffling operation
on boxes of the test dataset. As shown in Table 5,
compared with models that have one-dimensional
position embeddings, our method is more robust to
input disruption with a big gap.
4.4 Ablation Study
An ablation study is conducted to demonstrate the
effects of different modules in the proposed model.
We remove some components to construct several
comparable baselines on the CORD dataset. The
statistics are listed in Table 6.
The “GMN w/o MAMM” means using the
same multi-modal feature as LayoutLM. Compared
with LayoutLM, MAMM brings about 1.82% im-
provement of F1, which verifies the validation of
MAMM. The “GMN w/o Image” means removing
the image feature extraction. Experiment results
show that visual modality can also improve the
performance. Moreover, with NER-LM consid-
ered, the performance of information extraction
increases to 97.45%. Extended experiments includ-
ing attention visualization analysis and case studies
can refer to Appendix A and B.
5 CONCLUSION
In this work, we propose a Generative Multi-modal
Network (GMN) for practical document informa-
tion extraction. Since GMN is a generation method
including no pre-defined label category, it supports
scenes that contain unknown similar keys and toler-
ates OCR errors, meanwhile requires no character-
level annotation. We conduct extensive experi-
ments on publicly available datasets to validate
our method, experimental results demonstrate that
GMN achieves state-of-the-art results on several
public DIE datasets, especially in the practical sce-
narios. Though our GMN significantly outperforms
other DIE models, there still exists potential to be
exploited as regard to practical scenarios. In or-
der to cope with complicated layout information as
well as ambiguous semantic representations, we ar-
gue that more attention should be paid to the modal-
ity embedding and interaction strategy, which has
more opportunity to handle such difficult cases.3775References3776
Appendix
A Attention Visualization
To further explore what context information is fo-
cused by our GMN, we visualize the attention map
of the multi-head Transformer, as shown in Figure
4. The input tokens of the model are marked in
black and the decoding results are marked in or-
ange, while X-axis represents the attended tokens.3777
As shown in Figure 3, we use the Mask operator
to control the attention between each token. The
input OCR tokens can attend to each other but the
output tokens can only be attended to the already
decoded tokens. Consequently, the upper right area
of the attention map has no active response, and
the area in the lower right corner shows a stepped
pattern.
We can observe that the semantic attention mech-
anism plays an important role in modeling local
dependence. In semantic attention, the input OCR
tokens mainly focus on themselves and their nearby
semantically relevant parts. In contrast, decoded
tokens mostly focus on the counterparts in the origi-
nal tokens, showing a reasonable alignment. Mean-
while, layout and visual attention mechanisms fo-
cus on more global information, complementing
the semantic attention mechanism.B Case studies
The motivation behind GMN is to tackle the prac-
tical DIE tasks. To verify this, we show some
examples of the output of LayoutLM and GMN,
as shown in Figure 5. In the sub-figure A and B,
GMN successfully corrects the recognition error
of OCR results thanks to semantic learning on a
large-scale corpus. In the sub-figure C~F, GMN
accurately generates the key-value pairs with com-
plex layouts and ambiguous contexts thanks to the
novel position embedding method, in comparison
LayoutLM is unable to merge the value entities cor-
rectly. It’s noteworthy that the sub-figures G and
H are failed cases, which are caused by semantic
obfuscation and reasonable complement to missing
character. These examples show that GMN is capa-
ble of correcting OCR errors and predicting more
accurately in practical scenarios.3778