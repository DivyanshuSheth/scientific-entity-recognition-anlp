
Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
Abstract
In-context learning, where pre-trained language
models learn to perform tasks from task ex-
amples and instructions in their contexts, has
attracted much attention in the NLP commu-
nity. However, the ability of in-context learn-
ing is not fully exploited because language
models are not explicitly trained to learn in
context. To this end, we propose PICL ( Pre-
training for In-Context Learning), a framework
to enhance the language models’ in-context
learning ability by pre-training the model on
a large collection of “intrinsic tasks” in the
general plain-text corpus using the simple lan-
guage modeling objective. PICL encourages
the model to infer and perform tasks by con-
ditioning on the contexts while maintaining
task generalization of pre-trained models. We
evaluate the in-context learning performance of
the model trained with PICL on seven widely-
used text classification datasets and the S -
N I benchmark, which
contains 100+ NLP tasks formulated to text
generation. Our experiments show that PICL
is more effective and task-generalizable than
a range of baselines, outperforming larger lan-
guage models with nearly 4x parameters. The
code is publicly available at https://github.
com/thu-coai/PICL .
1 Introduction
Pre-trained language models (PLMs; Han et al.,
2021; Qiu et al., 2020) have shown strong abili-
ties of learning and performing unseen tasks con-
ditioning on several task examples or instructions
in its context, which is called in-context learning
(ICL; Brown et al., 2020). Compared to conven-
tional fine-tuning methods, ICL adapts PLMs to
downstream tasks only through inference, with-
out parameter updates, which is computationally
cheaper in practice and is closer to general AI.However, PLMs trained on massive corpora to
predict the next word given previous words are not
explicitly taught to learn in the context. This makes
ICL a surprising emergent ability but also indicates
that the ICL ability of PLMs is not fully exploited.
Garg et al. (2022) has shown that by directly train-
ing to do ICL in a meta-learning paradigm, models
show strong performance on learning simple func-
tion classes in the context. In practical NLP scenar-
ios, previous works (Min et al., 2022b; Chen et al.,
2022b) also enhance the ICL performance by meta-
fine-tuning PLMs on a large collection of down-
stream tasks and evaluating them on unseen tasks.
However, the low diversity of human-annotated
downstream tasks restricts the performance of the
meta-tuned model. Direct training on downstream
tasks also brings undesired bias on specific input
formats, label spaces, or domains, which hurts the
generalization of PLMs.
To enhance the ICL ability while maintaining
generalization, we propose PICL ( Pre-training for
In-Context Learning), a framework that exploits
the PLM’s ICL ability by pre-training models on
data automatically constructed from the general
plain-text corpus. Our framework is based on a
simple observation that many paragraphs in the
text documents contain “intrinsic tasks”. As shown
in the left part of Figure 1, each paragraph in the
document contains an intrinsic task. When doing
language modeling on each paragraph, models im-
plicitly perform the corresponding intrinsic tasks
simultaneously. This shares a similar idea with
the prompt-learning paradigm (Liu et al., 2021),
where downstream data examples from NLP tasks
are transformed into text sequences, and the model
learns to perform the original tasks when trained
on the text sequences with language modeling. Dif-
ferent from the downstream data, text paragraphs
contain more diverse intrinsic tasks and have lit-
tle bias on input formats, label spaces, or domains
because they are free-form texts from the large-4849
scale general corpus. By gathering and concatenat-
ing paragraphs with the same intrinsic tasks (right
part of Figure 1), we can construct a meta-training
dataset to pre-train the model to perform the intrin-
sic task conditioning on paragraphs in the context,
and thereby improve the ICL ability.
We adopt a retrieval-based approach to gather
paragraphs sharing the same intrinsic tasks from a
general corpus. We first train an encoder to repre-
sent each paragraph in a vector space where para-
graphs with the same intrinsic task have close em-
beddings. The encoder is trained with contrastive
learning (Khosla et al., 2020) on a collection of
downstream datasets by taking examples from the
same tasks as positive pairs and those from dif-
ferent tasks as negative pairs. Then, treating any
paragraph in the corpus as a query, we retrieve the
paragraphs with close representations to the query,
namely, sharing the same intrinsic task with the
query. Finally, we concatenate the query and the
retrieved paragraphs to get a pre-training instance.
Note that although we use downstream datasets, the
model is trained on instances constructed from the
general corpus, which ensures its generalization.
We evaluate the ICL performance of the
model pre-trained with PICL on seven widely-
used text classification datasets and S -
N I (Wang et al., 2022), a
benchmark whose test split contains more than 100
tasks formulated into text generation. Empirical re-
sults show the effectiveness of PICL, enabling the
model to reach or even outperform larger models
with nearly 4x parameters. Besides, we find that
the PICL-trained model is more generalizable onvarious tasks than previous meta-fine-tuning meth-
ods. We also conduct extensive experiments to
analyze several key factors of PICL.
2 Method
We first present an overview of PICL and then
describe the details in the following sections. As
shown in the right part of Figure 1, we construct the
pre-training instances from a corpus Cconsisting
of paragraphs split from full documents by “ \n”.
For each paragraph zinC, we first use a retriever
Rto find kparagraphs {z, z,···, z}sharing
the same intrinsic task (Sentiment Analysis) with
z. Then the retrieved paragraphs are treated as
demonstrations and concatenated with zto form a
pre-training instance: z⊕z⊕ ··· ⊕ z⊕z.
Finally, we adopt a language modeling objective to
pre-train the model on the constructed instances.
In this way, the pre-training stage can be re-
garded as a meta-training process, where the model
learns to solve the intrinsic task in zconditioning
on its context z⊕z⊕ ··· ⊕ z. Since Cis a
large-scale general corpus, it contains a variety of
intrinsic tasks and little domain bias, which ensures
the generalization of the pre-trained model.
2.1 Retriever
The main component of the retriever Ris a task-
semantics encoder Ethat represents a text para-
graph as a d-dimensional vector in a space V,
where paragraphs with the same intrinsic tasks have
similar representations. We define the similarity
between two paragraphs zandzusing the dot
product of their representations: E(z)·E(z).4850
Encoder We use RoBERTa (Liu et al., 2019)
as the base model of E. The output vector is com-
puted by averaging the last-layer representation of
each token in the input paragraph.
Retrieval We approximate that paragraphs
whose representations are close to each other in
Vshare the same intrinsic task. Therefore, for ev-
ery paragraph zinC,Rsearches for kparagraphs
with embeddings closest to E(z):
We employ the FAISS library (Johnson et al., 2019)
for efficient searching.
Contrastive Training We adopt contrastive
learning (Khosla et al., 2020; Karpukhin et al.,
2020) to train the task-semantics encoder E. As
shown in Figure 2, we take two paragraphs with
the same intrinsic task as positive pairs and those
from different tasks as negative pairs. However,
the annotation of a paragraph’s intrinsic task is usu-
ally unavailable. To this end, we use a collection
of downstream NLP datasets from various tasks
whose examples are converted into text sequences
with human-written prompts to train E. In this
way, treating each text sequence as a paragraph, we
can regard the corresponding downstream task as
the intrinsic task annotation. We assume that the
instances from all downstream tasks form a dataset
D. For each z∈ D, we have a positive instance
zsharing the same task with zand a set N(z)
consisting of negative instances with different tasksthanz, the loss function takes the form:
Positive and Negative Instances For each z∈
D, we randomly sample a positive instance zbe-
longing to the same task with zfromD\{z}.
As shown in Figure 2, N(z)contains two kinds
of negative instances: (1) Easy Negatives z
sampled from Dand belonging to different tasks
thanz. (2) Hard Negatives zsharing the same
prompt with zbut containing mismatched tasks.
For instance, in Figure 2, we apply the prompt from
the sentiment task to the summarization task to cre-
ate the hard negative instance z. This prevents
the model from hacking the contrastive objective
using prompts like “Guess the sentiment” and learn-
ing a trivial pattern matching but forces the model
to extract task semantics from the whole paragraph.
2.2 Data Construction
For each paragraph z∈ C, we concatenate the re-
trieved paragraphs {z, z,···, z}=R(z)with
zto get a pre-training instance z⊕z⊕ ··· ⊕
z⊕z. To improve the quality of the constructed
data, we derive an approach to filter out instances
that are less informative to ICL. We consider the
following score to measure the informativeness of
an instance based on the perplexity difference of
the paragraphs in the instance before and after they
are concatenated as a sequence:
where | · |is the length of a sequence and P(·)is
the language modeling probability based on any
uni-direct PLMs. Given a manually set thresh-
oldδ, we retain the instances that satisfy s > δ .
This criterion leverages the original ICL ability of
the PLM. If concatenating the paragraphs results
in lower perplexity, they are more correlated and
may be more informative for ICL. We finally con-
struct a pre-training corpus containing Ninstances
C ={z⊕z⊕,···,⊕z⊕z}.
2.3 Pre-Training
We pre-train the model with auto-regressive lan-
guage modeling on C . Unlike previous
works (Min et al., 2022b; Chen et al., 2022b),
which only compute the language modeling loss on4851the label tokens, we compute the loss on the whole
sequence. There are two reasons for this choice.
First, the intrinsic tasks are already in the natural
language format, and it is unnecessary to split the
input and the label. Second, we argue that com-
puting loss on the whole sequence ensures a large
token number in a forward batch, which is critical
to maintaining the basic in-weights ability (Chan
et al., 2022). Therefore, the loss function is:
where θis the parameters of the model. In addi-
tion, we find that adding a language modeling loss
L(θ)on the original full documents before be-
ing split into paragraphs benefits the performance.
Therefore, the final optimization objective is:
where we set α= 0.5in our main experiments.
3 Experimental Setup
3.1 Pre-training Data
We merge OWT(Gokaslan et al., 2019),
WC (Foundation, 2022), and BC-(Zhu et al., 2015) to construct the pre-training
data, where full documents are split into para-
graphs by “ \n”. The corpus Cconsists of 80M
paragraphs, totaling about 30GB. For each para-
graph, we search for k= 20 demonstrations and
concatenate them until 1024 tokens, the maximum
input length constraint of the language model we
used. This ensures that the model sees various
demonstration numbers during pre-training. We
use GPT2-Large (Radford et al., 2019) to compute
P(·)in Equation 3 and set δ= 0.0for filtering.
More details of data processing and statistics are
shown in Appendix A.
3.2 Baselines
We consider four baselines in our experiments:
•VanillaICL directly prompts a PLM with the
concatenation of training examples to do ICL.
•ExtraLM further pre-trains the PLM on the orig-
inal full documents before being split into para-
graphs with the language modeling objective.
•Self-Sup (Chen et al., 2022a) designs four self-
supervised pre-training objectives, including
Next Sentence Generation, Masked Word Predic-
tion, Last Phrase Prediction, and Classification,to enhance the ICL performance. We conduct
the self-supervised pre-training on our merged
corpus for a fair comparison.
•MetaICL (Min et al., 2022b) meta-trains the
model on a large collection of downstream
human-annotated datasets for learning to learn
in context. The meta-training instances are con-
structed by concatenating several training exam-
ples in each dataset to a single text sequence. We
replicate the method on the training set of our
task-semantics encoder for a fair comparison.
3.3 Evaluation
We evaluate the model trained with PICL on two
kinds of downstream tasks.
Few-Shot Text Classification We consider seven
widely-used text classification datasets, includ-
ing SST-2 (Socher et al., 2013), SST-5 (Socher
et al., 2013), Subj (Pang and Lee, 2004),
MR (Pang and Lee, 2005), RTE (Dagan et al.,
2006), CB (De Marneffe et al., 2019), and AG-
News (Zhang et al., 2015) to evaluate the few-shot
ICL performance of the trained models (see Ap-
pendix B.1 for more details). Note that these tasks
are not included in the training set of the task-
semantics encoder. We randomly sample 4 or 8
demonstrations from the official training sets of
each dataset. Effects of other demonstration num-
bers can be found in Section 4.3. We compute the
average accuracy scores on at most 1000 samples
from the validation split of each dataset across five
random seeds for selecting demonstrations.
Instruction Following To test the generalization
of PICL, we also evaluate the trained model on
a larger range of tasks with more free-form in-
puts, including both human instructions and few-
shot examples. We use the test split of S -
N I (Wang et al., 2022) as
the benchmark and exclude the tasks that appear
in the training set of the task-semantics encoder,
resulting in 105 evaluation tasks (see Appendix
B.2 for a full list of tasks). Each task is specified
with a human-written instruction and two or three
demonstrations. We follow Wang et al. (2022) to
formulate all tasks to the text generation format
and score the outputs with ROUGE-L (Lin, 2004).
3.4 Settings
Retriever We train the task-semantics encoder
on 37 tasks (see Appendix C) using up to 10K
examples per task. To enhance generalization, we4852
apply multiple prompts from PromptSource (Bach
et al., 2022) to one example and use 320 prompts in
all. We use the in-batch negative trick (Chen et al.,
2020) to compute the contrastive loss. We set the
learning rate to 5×10, the batch size to 64, and
construct 4 hard negatives for each instance. The
encoder is trained from RoBERTafor 1 epoch.
Language Model We test PICL based on the
770M GPT2-Large (Radford et al., 2019) unless
otherwise specified. Results on larger models can
be found in Appendix E.1. To save computa-
tional resources, we train the model from its pre-
trained checkpoints. We also test the VanillaICL
performance of larger models, including GPT2-
xLarge (Radford et al., 2019) (1.5B) and GPT-
Neo(Black et al., 2021) (2.7B) for reference.
Pre-Training We set the maximum learning rate
to1×10and use the “inverse square root” sched-
uler (Vaswani et al., 2017) with 1000 steps warmup.
The model sees 131K tokens in a step and is pre-
trained for 100K steps. It takes less than a day to
finish pre-training on 64 V100 32G GPUs.
4 Results
4.1 Few-Shot Text Classification
Table 1 shows the results of few-shot text classifi-
cation, from which we have 3 observations.
First , among the baselines with 770M parame-
ters, simply further training the model on our cor-
pus with language modeling improves the perfor-
mance (ExtraLM). This is likely due to the higherdomain diversity of our corpus. MetaICL is helpful
on most datasets, which verifies the effectiveness of
meta-training for ICL. Self-Sup fails to bring bene-
fits on most datasets against VanillaICL, probably
because the constrained label space of the Clas-
sification training task (only contains “True” and
“False”) brings bias to the model’s output. This
emphasizes the importance of using training objec-
tives with little bias.
Second , we observe that the PICL-trained model
outperforms the baselines with the same model
sizes by a large margin on most datasets across
different shots, verifying the effectiveness of PICL.
An exception is RTE, where MetaICL performs
the best. We speculate the reason is that some
training tasks of MetaICL share the same label
space with RTE (“Yes”/“No”), such as paraphrase
identification. Min et al. (2022c) has shown that the
label space plays a vital role in ICL, which explains
the good performance of MetaICL on RTE.
Thrid , comparing models across different sizes,
we find that increasing the model parameters boosts
the performance, but PICL enables the 770M
model to beat a 2.7B counterpart. This indicates
that the ICL ability can be enhanced not only
through scaling up the parameters. Improving the
structure of the pre-training data is also beneficial.
In Appendix E.1, we can see that PICL is also
effective when applied to a 1.5B model.
4.2 Instruction Following
The results on S -N I
are shown in Table 2. We can see that PICL4853
achieves higher overall instruction following per-
formance than the baselines, outperforming a larger
model with about 4x parameters.
In Figure 3, we compare the per-task perfor-
mance of PICL and MetaICL because they share
the most similar setting where human-annotated
downstream datasets are used. We observe that
PICL outperforms MetaICL on about 3/4 of evalu-
ation tasks, indicating that compared to fine-tuning
directly on downstream tasks, pre-training on in-
trinsic tasks constructed from the general plain-text
corpus brings better ICL ability and ensures higher
generalization performance across a broad range of
tasks (see Appendix E.2 for more details).
Most tasks where MetaICL beats PICL belong to
text classification whose output spaces are “Yes/No”
or “True/False”. This matches the second obser-
vation in Section 4.1, where MetaICL predicts
“Yes/No” well because of training on tasks that
share the same label spaces. On the other hand,
PICL performs much better on text generation, or
tasks whose output spaces share the same seman-
tics with “Yes/No” but use label words not in the
training tasks of MetaICL (e.g., “Correct/Wrong”).
This indicates that direct training on downstream
datasets causes overfitting to specific labels. There
are also tasks where PICL performs similarly to
MetaICL, such as reasoning and word analogy. Wenotice that the improvements of PICL and MetaICL
on these tasks are also marginal against VanillaICL
probably because these tasks rely more on the “in-
weights learning” ability (Chan et al., 2022), rather
than in-context learning.
4.3 Analysis
Effect of Retriever We compare different ap-
proaches to retrieve paragraphs and test the final
model performance. We try randomly selecting
paragraphs (Random), retrieving using the non-
parametric approach (BM25), encoding each para-
graph with the original pre-trained encoder as it is
(RoBERTa), or using the encoder for sentence simi-
larity (Reimers and Gurevych, 2019) (SRoBERTa).
We also study different numbers of hard negatives
(0, 1, 4) and downstream tasks (7, 24, 37) to train
the task-semantics encoder in PICL. From the re-
sults in Table 3, we can see that all retrieval meth-
ods except Random bring improvements against
VanillaICL on both text classification and instruc-
tion following settings, indicating that improving
the coherence of the paragraphs in the pre-training
data benefits ICL. Using the task-semantics en-
coder in PICL achieves the best performance, show-
ing the importance of retrieving paragraphs based
on task semantics rather than word overlap or sen-
tence meanings. Comparing different settings to
train the task-semantics encoder, we observe that
increasing the number of hard negatives and train-
ing tasks improves the final performance. This is in
line with previous works (Karpukhin et al., 2020;
Chen et al., 2020; He et al., 2020) that more chal-
lenging hard negatives benefit contrastive learning.
Effect of Demonstration Numbers Training
with PICL brings two benefits: (1) PLMs learn
a format where demonstrations from the same task
are concatenated as the prefix, which is beneficial
when the model is evaluated under the same num-
ber of demonstrations. (2) PLMs learn a better4854
ability to infer and perform tasks from the context,
even when the demonstration numbers in evalua-
tion and pre-training do not match. To differentiate
these effects, we conduct pre-training on instances
containing only 4, 8, or 16 demonstrations and test
the trained models under different text classifica-
tion shots. Results in Figure 4 show that when
pre-trained with different demonstration numbers,
the models generalize well to unseen demonstra-
tion numbers in evaluation, achieving similar per-
formance with the default setup where the model
sees various demonstration numbers in pre-training
(PICL-default). This indicates that the models learn
more than the input formats in PICL.
Effect of Filtering We try different threshold val-
uesδfor filtering and report the scores on text clas-
sification tasks in Figure 5(a), while controlling the
sizes of the constructed pre-training data the same.
We find that δ= 0 yields the best performance,
which means we retain an instance if and only if
the perplexity of individual paragraphs is higher
than that of the concatenated sequence (Equation
3). For lower δ, the pre-training data contain too
many uninformative instances for ICL. For larger δ,
we speculate that the filtering process relies on the
original GPT2-Large too much. Since we also pre-
train based on GPT2-Large, the filtering process
reduces the signals in the constructed data beyond
the base model’s ability.
Effect of Full Documents In Figure 5(b), we
report the model performance on text classifica-
tion tasks when using different choices of α, which
controls the proportion of the full-document data.
We find that balancing the constructed and full-
document data performs the best ( α= 0.5). When
αis too large, the model is trained mostly on our
constructed data and overfits its bias inevitably in-
troduced by the task-semantics encoder in the data
construction process. When αis too small, our
method degenerates into ExtraLM.
Effect of Data Amount We study the size ef-
fect of the corpus used to construct the pre-training
data in PICL and report the performance on text
classification tasks in Figure 6(a). We conduct the
data construction on 0.01%, 0.1%, 1%, and 10% of
the original 80M paragraphs (100%) and pre-train
models for at most 100K steps until the validation4855
losses begin to increase. From the results, we con-
clude that when the corpus is small, pre-training
with the constructed data hurts the performance
because the search library is too small to find para-
graphs sharing the same intrinsic tasks. Training
on small data for multiple epochs also causes over-
fitting. When the corpus contains more than 80K
paragraphs (0.1%), adding more data constantly im-
proves the performance, which is consistent with
the scaling law (Kaplan et al., 2020).
Data Comparison We compare the usefulness
of different pre-training data to enhance the ICL
ability. In addition to the final model performance,
we borrow the thoughts for designing the filtering
criterion in Section 2.2 to measure the usefulness of
a pre-training instance by computing the perplexity
using a reference large PLM: GPT-J (Wang and
Komatsuzaki, 2021) with 6B parameters. Lower
perplexity means the correlation within the instance
is higher and is intuitively more helpful for enhanc-
ing the ICL ability. In Figure 6(b), we show the
perplexity and the final model performance of 3 pre-
training data: original full documents before being
split into paragraphs (Full Doc), concatenation of
randomly selected paragraphs (Rand ⊕z), and
the concatenated same-intrinsic-task paragraphs
gathered using the retrieval method in PICL be-
fore filtering (R(z)⊕z). We can see that thedata constructed by retrieval has much lower per-
plexity and correspondingly yields higher accuracy
scores, which verifies its usefulness. In Appendix
F, we present several examples of the retrieved
paragraphs and the corresponding intrinsic tasks.
5 Related Work
In-Context Learning Recently, in-context learn-
ing (ICL), where models perform tasks simply con-
ditioning on instructions or the concatenation of
examples in the context (Brown et al., 2020), has
been found promising for using PLMs in various
application scenarios. To this end, there emerge
many works to improve the ICL performance by
calibrating the model predictions (Zhao et al., 2021;
Han et al., 2022; Holtzman et al., 2021; Min et al.,
2022a), selecting or reordering demonstrations (Ru-
bin et al., 2022; Liu et al., 2022; Lu et al., 2022),
designing pre-training tasks (Chen et al., 2022a),
and breaking the context length limits (Hao et al.,
2022). However, the underlying mechanism of ICL
is poorly understood (Min et al., 2022c). Therefore,
some works propose mathematical frameworks to
reveal how ICL works (Xie et al., 2021; Olsson
et al., 2022; Elhage et al., 2021), or investigate
the pre-training data to explain ICL’s good perfor-
mance (Chan et al., 2022; Shin et al., 2022).
Multi-Task Fine-tuning for Cross-Task General-
ization Fine-tuning PLMs on a large collection of
downstream tasks enables generalization to unseen
tasks under zero-shot (Wei et al., 2022; Sanh et al.,
2022; Ouyang et al., 2022; Chung et al., 2022) and
few-shot (Min et al., 2022b; Chen et al., 2022b;
Mishra et al., 2022; Garg et al., 2022) scenarios.
However, the performance of multi-task fine-tuning
is largely restricted by the diversity of the annotated
training tasks (Gu et al., 2022b), which requires
massive human efforts to scale up. In addition, di-
rect training on downstream tasks easily brings un-
desired bias. In this work, we propose to meta-train
the model with the intrinsic tasks automatically col-
lected from the large-scale general corpus, which
is easier to scale up and introduces little bias.
Pre-training Data Programming The conven-
tional pre-training paradigm trains the model on
plain-text corpora with the language modeling ob-
jective (Radford et al., 2018, 2019; Brown et al.,
2020). Recently works have found that carefully
designed pre-training instances can further boost
specific abilities like prompt adaption (Gu et al.,48562022a), reasoning (Razeghi et al., 2022), or sen-
tence representation (Levine et al., 2021). Our
work studies constructing pre-training instances to
improve the PLM’s ICL ability while still maintain-
ing its generalization on various NLP tasks.
6 Conclusion
This paper presents PICL, a framework that ex-
ploits the in-context learning ability of PLMs by
pre-training models on concatenations of text para-
graphs sharing the same “intrinsic tasks” gathered
from the large-scale general corpus. In PICL, mod-
els learn to perform various intrinsic tasks condi-
tioning on their context while preserving their gen-
eralization due to the little bias of the pre-training
data. Extensive experiments show that PICL im-
proves the ICL performance on various datasets
against several baselines, enabling a 770 M model
to outperform a larger model with about 4x param-
eters while maintaining good generalization across
a wide range of tasks. For future work, we would
like to consider adding human instructions to our
pre-training framework to enhance more abilities
of PLMs like zero-shot instruction following.
Limitations
One limitation of our paper is that the exact distri-
bution of the intrinsic tasks in the original corpus
and the constructed data is still unknown. Know-
ing the distribution can offer a better interpretation
of the effectiveness of PICL, even of the strong
performance of large language models. Besides,
although we can find many constructed instances
that share obvious intrinsic tasks (see Appendix F),
there still exist some instances where the intrinsic
tasks are hard to identify. How to better evaluate
the contribution of these instances to the ICL abil-
ity or designing better filtering approaches to select
more informative data for ICL is worth studying.
Our task-semantics encoder inevitably contains
some bias because it is trained on downstream
datasets, although we have tried to ensure a large
number and diversity of the dataset collection.
However, the final language model is pre-trained on
the general corpus, and we add the full document
loss, which eliminates the bias to some extent.
Regarding computing power, we acknowledge
that our framework takes relatively large training
resources in the retrieval and pre-training process.
Therefore, we did not conduct experiments based
on extra-large language models.Acknowledgements
This work was supported by the NSFC projects
(Key project with No. 61936010 ). This work was
also supported by the Guoqiang Institute of Ts-
inghua University, with Grant No. 2020GQG0005.
References48574858485948604861A Details of the Pre-training Corpus
This section presents details of the data processing
of the pre-training corpus and its statistics.
Data Processing Our pre-training corpus is a
merge of OWT (Gokaslan et al., 2019),
WC (Foundation, 2022), and BC-(Zhu et al., 2015), downloaded from the Hug-
gingFace datasets repository. We first split each
document in the corpus into paragraphs with “ \n”.
To avoid training with too short paragraphs, we
concatenate a paragraph with previous paragraphs
if the token number after concatenation is lower
than 128. We also exclude paragraphs longer than
500 tokens because they are not likely to fit into an
instance with more than 1 paragraph. The filtering
process in Section 2.3 drops about 24% instances.
The licenses of all corpora allow for scientific re-
search.
Statistics We plot the distribution of the mean
paragraph length per instance in Figure 7(a) and the
distribution of the paragraph number per instance in
Figure 7(b). The average paragraph length is 150.0,
and the average paragraph number in an instance
is 11.7. We can see that the model sees various
demonstration numbers in PICL pre-training.
B Details of the Evaluation Data
B.1 Few-shot Text Classification
The details of each text classification dataset and
the corresponding prompt in evaluation are listed
in Table 6. All datasets are downloaded from the
HuggingFace datasets repository. We simplify
the evaluation prompts as much as possible to re-
duce the effect of prompt engineering. Following
previous works (Brown et al., 2020; Sanh et al.,
2022), the model is evaluated by the ranking score
strategy, where we compare the perplexity of each
classification label under the model and choose the
label with the lowest perplexity. The licenses of all
datasets allow for scientific research.
B.2 Instruction Following
The original test split of the benchmark S -
N I (Wang et al., 2022) con-
tains 119 tasks. We exclude tasks that appear in
the training tasks of the task-semantics encoder or
whose input length is too long to fit in the context
of our model. Our final evaluation includes 105
tasks. A full list of the tasks is shown in Table 8.
We use the same template to combine few-shot ex-
amples with task instructions as Wang et al. (2022).
The license of this benchmark is Apache License
2.0.
C Details of the Downstream Training
Data
The downstream datasets we use to train the task-
semantics encoder are a merge of the training data
used in (Sanh et al., 2022) and the HR →LR set-
ting in (Min et al., 2022b). All datasets are down-
loaded from the HuggingFace datasets repository
and all prompts come from the PromptSource li-
brary (Bach et al., 2022). We exclude datasets
from the sentiment classification task, the topic
classification task, and the natural language infer-
ence task because they are included in our text clas-
sification evaluation. We finally get a collection of
37 datasets, as listed in Table 5. The licenses of all
datasets allow for scientific research.
D More Experimental Details
All model checkpoints we used come from the Hug-
gingFace models repository. The searching inter-
val of each hyper-parameter is listed in Table 4.4862E More Results
E.1 Results on Larger Base Model
We test PICL based on the GPT2-xLarge (Radford
et al., 2019) with 1.5B parameters. From the results
in Figure 7, we can see that PICL is also applicable
to larger models, outperforming the baselines based
on the same-sized model on most datasets.
E.2 Instruction Following
We present the performance comparison between
PICL and MetaICL per evaluation task in Figure
8. PICL outperforms MetaICL on 77 / 105 tasks,
indicating that PICL ensures the better generaliza-
tion of the trained model. The name of each task
is also listed in Figure 8. We can see that the top
three tasks where MetaICL performs the best are:
which are all “Yes/No” classification tasks. The
top three tasks where PICL performs the best are:
which are text generation, text generation, and
“Correct/Wrong” classification tasks respectively.
The four tasks where PICL and MetaICL have the
same scores are:
which belong to commonsense reasoning and word
analogy tasks.
F Case Studies
In Table 9 and 10, we present several cases of the re-
trieved paragraphs and the corresponding intrinsic
tasks. We can see that there exists a large range of
intrinsic tasks in the constructed data and many of
them do not appear in the training data of the task-
semantics encoder, which shows the generalization
of the encoder.486348644865486648674868ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
The section named "Limitations"
/squareA2. Did you discuss any potential risks of your work?
The section named "Limitations"
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 4
/squareB1. Did you cite the creators of artifacts you used?
Section 3, Appendix B
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Appendix
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Appendix
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
The document to use our code and pre-trained model is in the supplementary materials.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
We reported the number of examples, details of train/dev splits, and paragraph lengths of the
pre-training corpus.
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 34869/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3, Section 4, Appendix
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.4870