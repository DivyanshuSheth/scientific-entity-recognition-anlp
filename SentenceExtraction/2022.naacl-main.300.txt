
Siyu Ren Kenny Q. Zhu
Shanghai Jiao Tong University
Shanghai, China
roy0702@sjtu.edu.cn, kzhu@cs.sjtu.edu.cn
Abstract
Current text-image approaches (e.g., CLIP)
typically adopt dual-encoder architecture us-
ing pre-trained vision-language representation.
However, these models still pose non-trivial
memory requirements and substantial incre-
mental indexing time, which makes them less
practical on mobile devices. In this paper,
we present an effective two-stage framework
to compress large pre-trained dual-encoder for
lightweight text-image retrieval. The result-
ing model is smaller (39% of the original),
faster (1.6x/2.9x for processing image/text re-
spectively), yet performs on par with or bet-
ter than the original full model on Flickr30K
and MSCOCO benchmarks. We also open-
source an accompanying realistic mobile im-
age search application.
1 Introduction
Text-image retrieval is the task aiming at retriev-
ing a list of relevant images from a large set of
images given a textual query speciﬁed by the
user. Recently, large-scale vision-language pre-
training (VLP) has spawned models (Tan and
Bansal, 2019; Li et al., 2020; Radford et al., 2021)
that established state-of-the-art results in various
vision-language tasks (Antol et al., 2015; Suhr
et al., 2019), including text-image retrieval. Ex-
isting VLP models for text-image retrieval can be
divided into two categories: cross-encoder architec-
ture and dual-encoder architecture. Cross-encoder
models show better retrieval accuracy by allowing
ﬁne-grained cross-modal attention among image
and text. However, they are prohibitively slow to
apply to the entire image pool because each im-
age has to go through the deep Transformer again
whenever a new text query comes in. Moreover,
most cross-encoder models rely on external object
detection models (Ren et al., 2015) to extract visualfeatures, which further increase memory consump-
tion. On the other hand, dual-encoder models are
more scalable in that they allow pre-computing im-
age representations as reusable vectors independent
of the text queries. These image vectors can be in-
dexed and efﬁciently retrieved at runtime using Ap-
proximate Nearest Neighbor (ANN) search (John-
son et al., 2017). As long as the image pool remains
unchanged, the image encoder is not required.
However, a more practical scenario calls for dy-
namic indexing of new images into the pool (e.g.,
private photo collections on mobile devices), which
requires both the image encoder and the text en-
coder to be resident in memory. This makes the
above approach less practical on mobile devices
with limited memory and processing power. Un-
fortunately, little attention has been paid to fulﬁll
this need. In this paper, we show that a large dual-
encoder model can be compressed into a much
smaller and faster counterpart while retaining its
retrieval accuracy using a novel two-stage com-
pression framework. In the ﬁrst stage, we make
use of abundant non-paired texts/images to sep-
arately compress text or image encoder with an
effective intra-modal contrastive knowledge distil-
lation scheme. In the second stage, we sequentially
ﬁne-tune the distilled image or text encoder on
paired text-image data with comprehensive learn-
ing objectives. Using CLIP (Radford et al., 2021)
as the target model, our compressed models de-
liver comparable performance on MSCOCO and
Flickr30K while being just 39% of the original
size and 1.6x/2.9x times faster for processing im-
age/text. Detailed ablation study shows the effec-
tiveness of each component in the compression
framework and their synergistic effects.
Our contributions are three-folds: 1) an effective
compression framework tailored for lightweight
text-image retrieval; 2) a leaner and faster model
with competitive accuracy; 3) open-sourced mod-
els and text-to-image search mobile applications4085
on both iOS and Android at https://github.
com/DRSY/MoTIS .
2 Related Work
Cross-encoder. Cross-encoder architecture (Tan
and Bansal, 2019; Chen et al., 2019; Li et al., 2020)
adopts a single Transformer network (Vaswani
et al., 2017) which is able to process inputs from
different modalities, e.g., images and texts. Beneﬁt-
ting from the self-attention mechanism, the hidden
states of images and texts interact with each other
at the patch/token-level, therefore yielding state-of-
the-art retrieval accuracy. Though effective, these
models suffer from huge memory consumption and
inference latency, making them inpractical in time-
sensitive real-world scenarios.
Dual-encoder. In contrast to cross-encoder,
dual-encoder architecture (Radford et al., 2021;
Jia et al., 2021) trains two seperate encoders for
vision and language modalities. The exact choices
of encoder architecture may be different. For ex-
ample, CLIP utilizes Transformers for both visual
and text encoders, while ALIGN (Jia et al., 2021)
uses pre-trained BERT as text encoder and Efﬁ-
cientNet as visual encoder. In dual encoder, in-
teractions between different modalities take place
only at the ﬁnal encoder layer, resulting in slightly
worse performance compared to cross-encoders.
Nevertheless, this late-interaction scheme of dual-
encoder allows for efﬁcient similarity computation,
thus rendering it suitable for prividing real-time
searching.3 Approach
3.1 Background on Dual-Encoder
Dual-encoder architecture employs two separate
neural networks to encode inputs from different
modalities and map them to a shared space.
We denote the image encoder as fand the
text encoder as fin the context of text-image
retrieval. To train fandf, it is common
to adopt an objective that pushes the embed-
dings of matched text-image pairs closer while
pushing those of non-matched text-image pairs
apart. Speciﬁcally, Contrastive Language- Image
Pretraining (CLIP) (Radford et al., 2021) optimizes
an InfoNCE (van den Oord et al., 2018) loss:
L=−1
N/summationdisplayloge
/summationtexte(1)
Here,f(x)andf(y)are the L2-normalized em-
beddings of text in the i-th pair and image in the
j-th pair.Nis the mini-batch size and τis the
temperature to scale the logits. The ﬁnal objective
is the sum of Land its symmetric version L.
3.2 Two-Stage Model Compression
Despite good retrieval accuracy, models like CLIP
still pose non-trivial memory footprint and infer-
ence time, which is undesirable for low-resource
devices such as smart phones.
To tackle this issue, we propose a two-stage com-
pression framework to make large dual-encoder
model smaller and faster while retaining its accu-
racy. A schematic overview is illustrated in Figure40861. The ﬁrst stage is task-agnostic , where we lever-
age massively available non-paired texts/images to
separately compress the text/image encoder using
an intra-modal contrastive knowledge distillation
scheme. The second stage is task-speciﬁc , where
we sequentially ﬁne-tune the distilled image and
text encoder using a combination of multiple tech-
niques. We denote the image and text encoder of
the large dual-encoder as fandfand those of
the compressed model as fandf.
3.2.1 Stage-1
The extremely large scale of text-image pairs (e.g.,
400 million used to train CLIP) makes it possible
to make up for the noise in data and train over-
parametrized large dual-encoder (i.e., fandf)
from scratch to learn aligned visual and language
representations. However, it is difﬁcult to train
small model (i.e., fandf) with lower capacity
using the same inter-modal learning scheme.
To circumvent this issue, we propose to exploit
massively available non-paired data from the web
and optimize an intra-modal contrastive objective
that aligns the output embeddings of fand pre-
trainedf, which can be seen as a form of knowl-
edge distillation (Hinton et al., 2015). Here we
take visual modality as an example. Given a collec-
tion of images{y}, we feed them to both f
andfto produce two sets of image embeddings
{f(y)}and{f(y)}. Then we optimize
the following contrastive objective for updating f:
L=−1
N/summationdisplayloge
/summationtexte(2)
The same formulation is symmetrically applied to
language modality to obtain Lfor updating f:
L=−1
N/summationdisplayloge
/summationtexte(3)
Essentially, f/fis trained to recover the repre-
sentation power of f/fin a decoupled manner.
3.2.2 Stage-2
After training fandfusing general-domain
data, it is necessary to adapt the learned represen-
tations to downstream tasks using in-domain data.
First, we ﬁne-tune fandfon paired text-image
dataD={(x,y)}using standard InfoNCE
loss (Section 3.1). In the experiments, we found
that jointly ﬁne-tuning image and text encoder re-
sults in retrieval performance even worse than noﬁne-tuning at all. Therefore, we choose to sequen-
tially ﬁne-tune f/fby ﬁxing the other one. The
resulting ﬁne-tuned encoders are denoted as f
andfand are henceforth kept ﬁxed. Next, for
trainingfandf, we propose several techniques
essential to successful compression:
Knowledge Distillation (KD). In addition to the
standard InfoNCE loss, we design two kinds of
knowledge distillation objectives to learn from f
andf. One is the Kullback-Leibler divergence
between image-text matching distribution predicted
byfandfand the one predicted by fandf.
This resembles previous response-based knowledge
distillation (Hinton et al., 2015). The other is the
same contrastive objective deﬁned in Section 3.2.1.
It indirectly encourages the alingment between vi-
sual and language representations.
Sequential Finetuning (SF). Similar to how we
getfandf, we also ﬁne-tune fandfin
a sequential manner. Concretely, we ﬁrst let the
compressed model share the same text encoder with
the target dual-encoder and only ﬁne-tune its image
encoder. After that, we then ﬁx the image encoder
and ﬁne-tune its text encoder in the same way.
Hard Negative Mining (HN). Prior works on con-
trastive representation learning (Chen et al., 2020;
Gao et al., 2021) typically exploit in-batch nega-
tive samples. Though efﬁcient, image-text pairs
in a batch are randomly sampled and are likely to
be trivially unrelated. Models trained in such a
way may fail in cases where candidates are similar.
To achieve more accurate retrieval, we mine hard
negatives from the entire corpus. In our sequential
ﬁne-tuning setting, we ﬁrst use fto compute em-
beddings of all texts in the corpus and index them
with Faiss (Johnson et al., 2017). During training
f, for each image ywe usef(y)as query to
the index and obtain its top-k texts as negative sam-
ples. Afterward, we use the trained fto compute
embeddings of all images in the corpus and build
the index. During training f, for each text xwe
usef(x)as query to the index and get its top-k
images as negative samples.
The complete training objective of stage-2 is
deﬁned asL=L+L+L+L.
4 Experiment
4.1 Setup
Dataset. We use Conceptual Caption (Sharma
et al., 2018) for stage-1 compression. It consists of
3M noisy image alt-text pairs. However, we do not4087Image TextMSCOCO (1K) MSCOCO (5K) Flickr 30K
R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10
ff 46.9 77.3 87.3 28.0 52.9 64.5 55.2 80.3 87.8
ff 61.0 87.9 94.7 40.9 67.6 77.9 58.0 82.3 89.1
ff 41.4 76.7 88.1 21.3 47.2 61.0 30.2 59.1 71.2
ff 62.0 88.0 94.4 42.0 69.2 79.0 55.0 81.3 88.4
ff 62.7 88.2 94.5 42.6 69.6 79.4 57.0 82.1 88.8
ImageMSCOCO (5K) ∆
R@1 R@5 R@10 R@1
f 36.7 64.6 75.3 -
w/o stage-1 32.6 59.6 70.7 -4.1
stage-1 22.6 46.7 58.5 -14.1
stage-1 31.7 58.5 69.6 -5.0
w/o SF 30.9 57.6 70.8 -5.8
w/o KD 35.8 63.1 74.2 -0.9
w/o HN 34.4 62.0 73.7 -2.3
w/o KD+HN 32.6 60.3 71.9 -4.1
Image TextDisk Space QPSQPS
MB # #
ff 578 1.00x 1.00x
ff 255 1.51x 1.98x
ff 230 1.51 x2.77x
use the image-text alignment information but only
treat it as a reservoir of general-domain images
and texts. In stage-2, we use MSCOCO (Lin et al.,
2014) and Flickr30K (Plummer et al., 2015) as the
benchmarks. For MSCOCO, there are 113,287 im-
ages for training, 5,000 images for validation, and
both 5K and 1K for testing. For Flickr30K, there
are 28,783 images for training, 1,000 images for
validation, and 1k for testing.
Evaluation Metrics. Following previous work,
we use recall R@K (K=1,5,10) as the main met-
ric of task performance. We also report the disk
space (MB) and how many image/text queries can
be encoded per second (QPSfor image and QPS
for text) to evaluate model’s memory footprints and
inference speed.
Target Model. We use the open-sourced ViT-B/32
CLIP as the target dual-encoder model to compress.
The image encoder fis a 12-layer Vision Trans-former (Dosovitskiy et al., 2020) with 768 hidden
dimension and 12 attention heads. The text encoder
fis a 12-layer Transformer with 512 hidden di-
mention and 8 attention heads. Note that this is
the largest publically available version according
to OpenAI’s ofﬁcial repository.
Compression Conﬁguration. For image encoder
f, we use a ViT-S/16 with 384 hidden dimen-
sion. We initialize it with weights pretrained on
ImageNet-21K (Ridnik et al., 2021)for faster con-
vergence and better performance. For text en-
coderf, we experiment with both 6-layer and
4-layer Transformer (marked as fandf), of
which the weights are initialized from correspond-
ing layers in f. We also compare with a baseline
compression method that directly ﬁne-tunes pre-
trained ViT-S/16 and 4-layer TinyBERT (Jiao et al.,
2019)f using InfoNCE loss throughout
both stages.
Implementation Detail. In stage-1, we train
1 epoch using AdamW (Loshchilov and Hutter,
2017) with a batch size of 84 for both images and
texts, learning rate of 3e-4, and weight decay of 0.1.
In stage-2, we use the same optimization setting
except that we train with batch size 96 for 5 epochs.
We employ a cosine learning rate scheduler with
10,000 warm-up steps for both stages. All reported
results are calculated on the test set using check-
points with the highest validation performance.
4.2 Results
Main Results. Table 1 summarizes the main re-
sults. As can be observed, the pre-trained CLIP
model can already deliver moderately good re-
trieval performance. The performance is further
improved after ﬁne-tuning. Fine-tuning pre-trained
ViT-S/16 and TinyBERT underperforms the zero-
shot CLIP, showing that training with inter-modal
InfoNCE is not effective without extremely large-
scale paired data. On most evaluation metrics, mod-
els compressed by our proposed two-stage pipeline4088perform on par with or better than the ﬁne-tuned
target model. We also found that the capacity of
text encoder has limited affect on the performance.
Ablation Study. We perform extensive abla-
tions to study the importance of each proposed
technique. Due to the computational budget, we
only conduct ablation on the image encoder and ﬁx
the text encoder as f. We evaluate w/o stage-1 ,
stage-1 (mean-square-error between fand
f), and stage-1 (identical to the loss in
Section 3.1) for stage-1 ablation. We also study
the effectiveness of KD/SF/HN by removing them
separately or together. We made several observa-
tions based on Table 2: 1) SF makes ﬁne-tuning
stable and is essential for convergence. 2) both KD
and HN improve retrieval accuracy and are comple-
mentary to each other. 3) intra-modal contrastive
distillation helps when image-text pairs are noisy
and outperforms inter-modal infoNCE loss.
Efﬁciency. In Table 3, we compare the disk
space and QSP used by models on a RTX 2080Ti
of 12GB memroy. The compressed image encoder
ftakes 85MB disk space (39% of f) meanwhile
being 1.51x times faster. Our compressed text en-
coder can achieve up to x2.77 inference speed-up
and 40% size reduction (from 243MB to 146MB).
We further benchmark models’ memory and run-
time performance on a real iPhone X with 1,000
images in the gallery for testing. It takes 870MB
and 295MB for loading CLIP and our compressed
model into main memory respectively. After in-
dexing, the response time for a single text query
is 0.4s for CLIP while it is only 0.1s for our com-
pressed model. Although the results are hardward-
dependent, our compressed model still shows an
evident improvement in efﬁciency.
5 Conclusion
In this paper, we present a two-stage framework
for lightweight text-image retrieval. Experiments
on two benchmarks show the effectiveness of each
component in the framework and the best perfor-
mance is achieved when combining them together.
It holds the merit of reducing model size and accel-
erating inference time, making memory/response-
sensitive applications more practical.
Acknowledgement
This research is partially supported by NSFC Grant
No. 91646205, and SJTU-CMBCC Joint Research
Scheme.References40894090