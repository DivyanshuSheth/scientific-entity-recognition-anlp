
Dongwon Kelvin RyuEhsan ShareghiMeng Fang
Yunqiu XuShirui PanGholamreza HaffariDepartment of Data Science & AI, Monash UniversityEindhoven University of TechnologyUniversity of Technology SydneyLanguage Technology Lab, University of Cambridge
firstname.lastname@monash.edu m.fang@tue.nl
yunqiu.xu@student.uts.edu.au
Abstract
Text-based games (TGs) are exciting testbeds
for developing deep reinforcement learning
techniques due to their partially observed en-
vironments and large action spaces. In these
games, the agent learns to explore the envi-
ronment via natural language interactions with
the game simulator. A fundamental challenge
in TGs is the efficient exploration of the large
action space when the agent has not yet ac-
quired enough knowledge about the environ-
ment. We propose C E, an exploration
technique that injects external commonsense
knowledge, via a pretrained language model
(LM), into the agent during training when the
agent is the most uncertain about its next ac-
tion. Our method exhibits improvement on the
collected game scores during the training in
four out of nine games from Jericho. Addition-
ally, the produced trajectory of actions exhibit
lower perplexity, when tested with a pretrained
LM, indicating better closeness to human lan-
guage.
1 Introduction
Text-based games (TGs) are environments where
agents learn to comprehend situations in language
and produce decisions in language (Hausknecht
et al., 2020; Côté et al., 2018; Narasimhan et al.,
2015). Deep Reinforcement Learning lends itself
as a natural paradigm to solve TGs due to its ability
to learn from unsupervised game playing experi-
ence. However, existing RL agents are far away
from solving TGs due to their combinatorially large
action spaces that hinders efficient exploration (Yao
et al., 2020; Ammanabrolu and Hausknecht, 2020).
Ammanabrolu and Riedl (2019); Ammanabrolu
and Hausknecht (2020) proposed incorporating a
belief knowledge graph ( BKG ) built from the tex-
tual observations to help the agent reason moreeffectively about observed objects during the game-
play. Most of the recent works neglected linguis-
tic aspects of TGs and focused on the construc-
tion and utilisation of BKG (Adhikari et al., 2020;
Dambekodi et al., 2020; Xu et al., 2020; Am-
manabrolu et al., 2020; Xu et al., 2021). Some
exceptions involve developing pre-trained language
models (LMs) to propose action candidates for a
given observation (Yao et al., 2020), and investigat-
ing the relationship between semantic coherence
and state representations (Yao et al., 2021).
In parallel, it has been argued that recent pre-
trained LMs capture commonsense factual knowl-
edge about the world (Petroni et al., 2019; Kassner
et al., 2021; Meng et al., 2021). More direct at-
tempt in this direction was the commonsense trans-
former ( COMT) which is a LM fine-tuned explic-
itly on commonsense knowledge graph ( CSKG ),
to explicitly generate commonsense inferences
(Bosselut et al., 2019; Hwang et al., 2021). Prior
works with commonsense focused on complet-
ingBKG using pre-defined CSKG (Murugesan
et al., 2020) or dynamic COMT-generated com-
monsense inferences (Dambekodi et al., 2020).
Nonetheless, there is no work on explicitly using
commonsense as an inductive bias in the context
of exploration for TGs.
To bridge the gap, we propose commonsense ex-
ploration (C E) which constructs a CSKG
dynamically, using COMT, based on the state of
textual observation per step. Then, the natural lan-
guage actions are scored with COMTand agent,
to re-rank the policy distributions. We refer to this
as applying commonsense conditioning . However,
doing this throughout the whole training is expen-
sive and may not be beneficial as gameplay is not
led by commonsense. To rectify this, we propose
anentropy scheduler , driven by the entropy of the
policy distribution, to regulate applying common-
sense conditioning.
We demonstrate that our method encourages515
the agent to achieve higher game score during
the training in four out of nine games in Jeri-
cho (Hausknecht et al., 2020). Furthermore, we
show our method leads to producing more human-
like natural language action. This is measured us-
ing the perplexity of the generated actions accord-
ing to GPT-2 (Radford et al., 2019). We believe
that natural language coherency/fluency is a crucial
aspect of interactive intelligent agents (e.g. robots
and dialogue systems) and hope our promising find-
ings facilitate further developments of methods in
this direction.
2 Approach
Notations. Text-based games are modelled as a
partially observable Markov decision processes
(POMDPs) of a tuple of ⟨S,A,P,O,Ω,R, γ⟩,
whereS,A,Ωdenote sets of states, actions, and ob-
servations, respectively. Also, Randγdenote the
reward function and the discount factor, while P
andOdenote the transition probabilities and set of
conditional observations probabilities, respectively.
The agent requires to map an observation to a
state ( Ω→ S ) and produce a policy π. By se-
lecting an action afrom the policy π, the agent
changes current state s, receives a reward sig-
nalr, receives an observation through transition
P(s|s, a), and also receives a conditional ob-
servation O(|s). The agent learns the policy
π(a|o)that maximizes the expectation of the cu-mulative reward function EPγr(s, a)
.
2.1 CSKG Construction
Let a CSKG be a graph K= (V,E), where Vis
a set of nodes or vertices and Eis a set of edges.
The root node of CSKG requires to carry adequate
information about the gameplay, so we amend the
input to be the same format as how COMTis
trained on, v=“I ”+a+“. ”+oand replace
all the “I” to “PersonX”. To build CSKG we use
COMTat every step of gameplay as a frozen
commonsense generator to produce the tail node
vgiven the head node vand edge eat time
stept, formally denoted as Pr(v|v,v,e).
Figure 1(Right) provides a visualisation of this.
COMTtakes vas a head node and eas an
edge and produces vwith the corresponding node-
to-node score ϕ. Multiple tail nodes and
node-to-node scores can be generated through the
same input and based on the edge, the tail nodes
vary dramatically. This process can be applied
recursively to the tail nodes, expanding CSKG, i.e.
generate tail nodes given vhead node with e.
See Appendix A for more details.
2.2 Commonsense Conditioning
To blend commonsense into the agent’s decision,
the log-likelihood score is employed to contem-
plate each component independently. We, then,
compute the total score as a weighted sum to pro-
mote the natural language action.516Agent-to-Action Score. The score function for the
gameplay is obtained from the agent,
ϕ=1
|a|Xlogπ(a|a,o),
where ϕis the agent-to-action score for kaction,
computed as the sum of log-likelihood of the natu-
ral language action. Intuitively, the agent-to-action
score signifies how much the action directs to the
reward signals. This is learned during the online
training of the agent.
Node-to-Action Score. Inspired by Bosselut et al.
(2021); Yasunaga et al. (2021), the commonsense
level of actions for each generated node is mea-
sured using COMT,
ϕ=1
|a|Xlog Pr(a|a,v,e),
ϕ= max(ϕ, ϕ,···),
where ϕis the score per vaedge, e∈ E,
while the node-to-action score is denoted by ϕ
which is the maximum ϕovervaedges. The
node-to-action score intersects commonsense with
action, implying how plausible the action is given
the commonsense prediction.
Node-to-Node Score. Additionally, we adopted
the score between nodes in CSKG from Bosselut
et al. (2021),
ϕ=1
|v|Xlog Pr(v|v,v,e),
ϕ= max(ϕ, ϕ,···, ϕ,···),
where ϕis the score per head node and vv
edges, e∈ E, while the node-to-node score
isϕ,max ofϕover head nodes and vv
edges.The node-to-node score is designed to pro-
mote commonsense triples that are more sensible
commonsense-wise.
Total Score. The total score assigned for each
action is computed as:
ϕ= max(γϕ+γϕ+γϕ),(1)
where ϕis the total score per action since max is
over nodes. The γcoefficients are hyperparameters
and balance the weights between different compo-
nents of the scoring function. Finally, the new con-
ditioned policy is obtained as softmax (ϕ). We
refer to this whole process as commonsense con-
ditioning. A visualisation of the overall model is
provided in the Figure 1(Left).
Intuitively, when the agent is not confident in
current time-step, the policy distribution is arbi-
trary, resulting in homogeneous ϕ. This would
be specifically the case during the initial stage of
the training, but can also occur at any stage of the
game where the agent cannot predict reward signal
in a small number of steps. Under these circum-
stances, ϕwould be more dictated by ϕandϕ.
Conversely, when the agent is confident, the ϕfor
different actions will diverge and ϕwill be directed
by both commonsense and the agent.
2.3 Entropy Scheduler
Since our technique uses a large LM for natural
language generation, the main drawback with our
approach is computational costs. In addition to this,
where the agent is confident about acquiring the
game score for a given action, commonsense could
act as an undesired noise. To reflect on these, we
propose the entropy scheduler to apply common-
sense conditioning based on the confidence, the
relative entropy of policy distribution. We collect
the last 1000 number of the entropy of the template
policy and apply commonsense conditioning if the
current entropy is higher than the median. Figure 2
visualizes how the entropy scheduler works during
training. This suggests that our entropy scheduler
with a median threshold can apply commonsense
conditioning to those actions with zero or negative517
reward signals.
3 Experiments
We use KG-A2C as our goal-driven baseline agent
and compare it with KG-A2C with commonsense
in a game suite of Jericho. A set of nine games are
selected from Jericho carefully based on genre, in-
cluding three daily puzzle games ( library ,lu-
dicorp ,reverb ) and the rest six fantasy adven-
ture games ( balances ,enchanter ,spirit ,
zork1 ,zork3 ,ztuu ). Both game setting
and optimal configuration for KG-A2C in Am-
manabrolu and Hausknecht (2020) were used in
our experiments. We reduced training steps to
25,000since our objective is to compare the qual-
ity of exploration during the training. Only hyper-
parameters in C E have been optimized
for fair comparison while all the parameters in
COMTwere fixed during the training, resulting in
the equal trainable parameters regardless of C-E. Details of the hyper-parameters and the
experimental setup can be found in Appendix B.
3.1 Main Results
Similar to Ammanabrolu and Hausknecht (2020),
we employed the optimal hyper-parameters fine-
tuned on zork1 for nine games in Jericho. Table
1 shows the mean score across the entire training
and the perplexity of the action given a root node.
The score is to compare whether the agent with
commonsense achieves higher game score during
the training . Doing so implies how fast the agent
learns with fewer steps, and therefore, more effi-
cient exploration. Perplexity from LM is used as
a metric for the smoothness of natural language
action. We used GPT-2 from Huggingface (Wolf
et al., 2020).
Score Table 1 shows that with C E,
the agent tends to acquire the game score more
frequent in four gaming environments ( spirit ,
zork1 ,zork3 ,ztuu ). All four have at least
15% increases in game score during training.
However, three environments ( balances ,en-
chanter ,ludicorp ) appear to gain no benefits
from using C E. On the other hand, the
remaining two games ( library ,reverb ) take
commonsense negatively, suggesting that the com-
monsense from COMTacts as a noise with re-
spect to pursuing rewards. Per genre, interestingly,
those daily puzzle games are either not influenced
or negatively influenced from commonsense induc-
tive bias while four out of six fantasy adventure
games benefited from it. We speculate this might
be due to the fine-tuning which was also done on a
single game, zork1 .
Coherency Table 1 shows that commonsense
prior reduces perplexity of the natural language
actions in all nine games. This is because, unlike
the game score that is not directly related to com-
monsense, the semantic properties of the actions
are directly related to commonsense. For envi-
ronments like balances andreverb , despite
the agent taking no benefits from commonsense,518
perplexity drops significantly ( e.g.,∼15%). This
large reduction in perplexity also appears for fan-
tasy games, in which zork3 had∼20% down and
spirit took as little as ∼3% reduction. This
suggests that the game takes advantages on the se-
mantic coherency regardless of whether it helps to
achieve high score of the game or the genre of the
game.
Qualitative Samples Table 2 provides qualita-
tive samples to show how natural language ac-
tions are re-ordered after commonsense condi-
tioning. For instance, in the first example of
zork1 ,C E suppresses open brown
and pushes put glass on table to the high-
est probability. In zork3 ,C E promotes
turn on lamp over others since the observa-
tion informs user that the surrounding is dark.
3.2 Ablation Results
We performed two ablation studies on zork1
to obtain the optimal hyper-parameters. The
first ablation study is for the absence of features,
in which we removed CSKG construction and
entropy scheduler completely. Thereafter, the
changes in score gamma factors have been in-
vestigated. The γcoefficients are changed from
(γ= 1, γ= 0.7, γ= 0.8)to(0.4,0.2,1)for
(v<a)model and (1,1,0.3)for(v>a)model.
Feature Figure 3 (Left) shows that the absence
ofCSKG construction or entropy scheduler causes
catastrophic forgetting. KG-A2C is prone to this
regardless of commonsense because it does not
use any memory component. However, injecting
commonsense stochastically enhances the likeli-
hood since the agent follows commonsense when
it should not, i.e. a particular action is required to
obtain game score. This overlaps with our motiva-
tion of entropy scheduler, that the game score is not
directly related to commonsense, so appropriateskipping is necessary .
Dynamic CSKG contributes to a variety of com-
monsense, amplifying its commonsense reasoning,
and a lack of this will provoke the agent acting
more narrow with limited commonsense. Our plot
shows that removing CSKG also contributes to
the cause of catastrophic forgetting. This suggests
that lack of diversity in commonsense may act as a
noise to the exploration, and may push the agent to
produce more skewed trajectories that cause failure.
Therefore, the absence of any component leads to
performance decay. Therefore, both are vital com-
ponents in C E.
Score Gamma Factor The contribution of the
commonsense and the agent score is investigated
on Figure 3 (Right). By increasing agent’s gamma
factor, the model acts more alike to the baseline
than the optimal hyper-parameters since it trusts its
own policy more. Conversely, adding more weights
on commonsense leads to catastrophic forgetting.
This is caused by the fact that the agent puts too
much trust on commonsense, diverging from its
own policy excessively. From these, we can con-
clude that the appropriate balancing is required to
make exploration efficient and feasible.
4 Conclusion
We investigated the effect of commonsense in text-
based RL agent during the training. Our results
show that despite the hyper-parameters tuning on a
single game, the proposed approach improves on
other gaming environments in Jericho, total four
out of nine. Furthermore, injecting commonsense
also positively influences the semantics of natural
language actions, resulting in lower perplexity. Our
future work will extend its application to different
text-based environments and investigate how this
linguistic properties from LM helps the agent.519References520A CSKG Construction
There are three different strategies for building the
root node from the textual observation and the natu-
ral language action. The most generic one is, given
a=“move rug" ando=“With a great effort,
the rug is moved to one side of the room, revealing
the dusty cover of a closed trap door.”, the root node
isv=“PersonX ” +a+“. ”+o=“PersonX
move rug. With a great effort, the rug is moved to
one side of the room, revealing the dusty cover of
a closed trap door.”. The example of CSKG with
vis in Figure A.1.
However, if the previous action awas not
admissible, we set the room description of the
textual observation as the root node. Finally, if
the action is admissible, but the observation is
too short (less than 20 tokens), the root node in-
cludes the previous room description of the textual
observation at the beginning of the page, v=
o+“ PersonX ” +a+“. ”+o.
These are motivated from 1) if the previous ac-
tion is not admissible, the environment is not af-
fected by it, so we simply use the previous room
description that captures a lot of information about
what the agent can do, 2) if the observation is too
short that it does not carry enough information
about the situation, we concatenate the previous
room description to subjoin the information about
surroundings, and 3) otherwise, the generic strat-
egy to build the root node, the previous action and
the consequence of it as textual observation.
B Experiment Setup
Action Sampling We set n to be dy-
namic, only selecting those based on the probability
threshold and validity. The threshold is calculated
as 0.75 of its uniform distribution. For instance,
zork1 contains 237 number of TEMPLATE , so the
threshold is 0.75×= 0.00316 . We only se-
lect the maximum of 7 TEMPLATE that exceeds
the threshold. This avoids a large shift in policy
distribution while attaining better computational ef-
ficiency. Additionally, we include valid templates
to enforce the agent to act more towards on chang-
ing the world tree. We sampled objects like KG-
A2C since KG-A2C already restricts objects and
the actions are usually determined by the template.
Therefore, |ϕ|=n , reducing the compu-
tations but still covering useful action sets.
Commonsense Transformer Our COMTis521
BART fine-tuned on A -2020 dataset, which
is crowdsourced with natural language sentence
nodes and 23 commonsense edges (Hwang et al.,
2021). We assumed that the general COMTis
still good enough to cover TGs. Since the gam-
ing environment runs by the player character, we
only focus on the social-interaction commonsense.
“xNeed" and “xIntent" are chosen for CSKG con-
struction, E, since they deal with what is needed
or intended for the event to occur, while “xWant"
and “xEffect" for scoring the natural language ac-
tions,E, since they deal with what the player
would do following the event. We further set
n= 1 andn= 2 from the observation that
they are good enough for zero-shot commonsense
question answering (Bosselut et al., 2021; Moghim-
ifar et al., 2020). During the online training of the
agent, we freeze the parameters for COMT.
C Computational Expense
The number of node-to-node scores is directly re-
lated to the size of CSKG,
|ϕ|=X(n× |E|),
where nis the number of hops, nis the num-
ber of triple generation and Eis the edge space
for CSKG.
On the other hand, the number of node-to-action
scores is equal to the number of the total score ϕ,
|ϕ|=|ϕ|=|ϕ| × |E| × |ϕ|,
where Eis the edge space for node-to-action
score.We assume |ϕ| ≈7since we select maximum
of 7 templates with highest probability and valid
templates. Therefore, in our setting, we can calcu-
late the number of the natural language generations
per step per environment as,
|ϕ|+|ϕ|=|ϕ|+|ϕ| × |E| × |ϕ|
=|ϕ| ·(1 +|E| × |ϕ|)
≈X(2×2)·(1 + 2 ×7)
= 75
Finally, we can estimate the average number of
natural language generation per step by multiplying
the number of environments per step n= 32 and
fraction from entropy scheduler p≈0.5,
(|ϕ|+|ϕ|)×n×p≈75×32×0.5
= 1200
Throughout the training, we require to perform
1200 natural language generations using a large
sizeCOMTper step, so this increases the training
time from ×3upto×10.522