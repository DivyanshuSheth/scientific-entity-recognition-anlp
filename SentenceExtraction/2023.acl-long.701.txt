
Yuxin Heand Jingyue Huand Buzhou TangDepartment of Computer Science, Harbin Institute of Technology, Shenzhen, ChinaPeng Cheng Laboratory, Shenzhen, China
21S051047@stu.hit.edu.cn
tangbuzhou@gmail.com
Abstract
Event co-occurrences have been proved effec-
tive for event extraction (EE) in previous stud-
ies, but have not been considered for event
argument extraction (EAE) recently. In this
paper, we try to fill this gap between EE re-
search and EAE research, by highlighting the
question that “Can EAE models learn better
when being aware of event co-occurrences?” .
To answer this question, we reformulate EAE
as a problem of table generation and extend a
SOTA prompt-based EAE model into a non-
autoregressive generation framework, called
TabEAE, which is able to extract the argu-
ments of multiple events in parallel. Under
this framework, we experiment with 3 differ-
ent training-inference schemes on 4 datasets
(ACE05, RAMS, WikiEvents and MLEE) and
discover that via training the model to ex-
tract all events in parallel, it can better dis-
tinguish the semantic boundary of each event
and its ability to extract single event gets
substantially improved. Experimental results
show that our method achieves new state-of-
the-art performance on the 4 datasets. Our
code is avilable at https://github.com/
Stardust-hyx/TabEAE .
1 Introduction
Event argument extraction (EAE) is an essential
subtask of event extraction (EE). Given an input
text and trigger(s) of target event(s), the EAE task
aims to extract all argument(s) of each target event.
Recently, substantial progress has been reported on
EAE, thanks to the success of pre-trained language
models (PLMs).
Previous studies on EE commonly take event co-
occurrences into account. However, recent works
on EAE (Ebner et al., 2020; Zhang et al., 2020;
Xu et al., 2022; Du and Cardie, 2020; Wei et al.,
2021; Liu et al., 2021; Li et al., 2021; Du et al.,Figure 1: An illustration of EE and EAE. The triggers
are in red and the arguments are underlined. EE models
aim at extracting all events concurrently, whereas main-
stream EAE models are trained to extract the arguments
for one event trigger at a time.
2021; Lu et al., 2021; Ma et al., 2022) only con-
sider one event at a time and ignore event co-
occurrences (as illustrated in Figure 1). In fact,
event co-occurrences always exisit in text and they
are useful in revealing event correlation and con-
trasting the semantic structures of different events.
For the instance in Figure 1, there exist two events
in the same context. The two events are triggered
by “leaving”, “become” respectively, and share the
same subject “Davies”. It is clear that there ex-
ists a strong causal correlation between the two
events. However, mainstream works on EAE split
the instance into two samples, which conceals this
correlation.
In this paper, we try to resolve this divergence be-
tween EE research and EAE research, by highlight-
ing the question that “Can EAE models learn better12542when being aware of event co-occurrences?” . To
address this question, we reformulate EAE as a
problem of table generation and extend the SOTA
prompt-based EAE model, PAIE (Ma et al., 2022),
into a non-autoregressive generation framework to
extract the arguments of multiple events concur-
rently. Our framework, called TabEAE, inherits
the encoding, prompt construction and span se-
lection modules from PAIE, but employs a novel
non-autoregressive decoder for table generation.
Under this framework, we explore three kinds
of training-inference schemes: (1) Single-Single ,
training model to extract single event at a time
and infer in the same way; (2) Multi-Multi , train-
ing model to extract all events in parallel and in-
fer in the same way; (3) Multi-Single , training
model to extract all events in parallel and let the
model extract single event at a time during infer-
ence. According to our experiments, the Multi-
Single scheme works the best on 3 benchmarks
(ACE, RAMS and WikiEvents) and the Multi-Multi
scheme works the best on the MLEE benchmark,
where the phenomenon of nested events extensively
exists. Besides, in-depth analysis reveals that via
training TabEAE to extract all events in parallel, it
can better capture the semantic boundary of each
event and its ability to extract single event at a time
gets substantially improved.
To sum up, our contributions include:
•We observe the divergence between EE research
and EAE research in terms of the phenomenon of
event co-occurrence. To resolve this divergence,
we extend the SOTA prompt-based EAE model
PAIE into a text-to-table framework, TabEAE,
which is able to extract the arguments of multiple
events concurrently.
•Under the TabEAE framework, we explore three
training-inference schemes, i.e. Single-Single,
Multi-Multi, Multi-Single, and verify the signifi-
cance of event co-occurrence for EAE.
•The proposed method outperforms SOTA EAE
methods by 1.1, 0.4, 0.7 and 2.7 in Arg-C F1 re-
spectively on the 4 benchmarks ACE05, RAMS,
WikiEvents and MLEE.
2 Related Work
2.1 Event Argument Extraction
As a crucial subtask of EE, EAE has long been
studied. In the early stages, EAE is only treatedas a component of EE systems (Chen et al., 2015;
Nguyen et al., 2016; Yang et al., 2018; Zheng et al.,
2019; Lin et al., 2020), where the phenomenon of
event co-occurrence is always taken into account.
Recently, more and more works study EAE as
a stand-alone problem. We summarize these re-
cent works on EAE into 4 categories: (1) span-
based methods that identify candidate spans and
predict the roles of them (Ebner et al., 2020; Zhang
et al., 2020; Xu et al., 2022); (2) QA-based methods
that query arguments using questions constructed
with predefined templates (Du and Cardie, 2020;
Wei et al., 2021; Liu et al., 2021); (3) sequence-to-
sequence methods that leveraging generative PLMs,
e.g. BART (Lewis et al., 2020) and T5 (Raffel et al.,
2020), to sequentially generate all arguments of the
target event (Li et al., 2021; Du et al., 2021; Lu
et al., 2021); (4) a prompt-based method by Ma
et al. (2022) that leverages slotted prompts to ex-
tract arguments in a generative slot-filling manner.
Among them, the prompt-based method, PAIE
(Ma et al., 2022), demonstrates SOTA performance.
However, all of them only consider one event at
a time, diverging from EE research. In this work,
we adapt PAIE into a non-autoregressive table gen-
eration framework, which is able to extract the
arguments of multiple events in parallel.
2.2 Text-to-Table
Although table-to-text (Bao et al., 2018; Chen et al.,
2020) is a well-studied problem in the area of
controllable natural language generation, Text-to-
Table, the inverse problem of table-to-text, is just
newly introduced by Wu et al. (2022). In Wu et al.
(2022), text-to-table is solved with a sequence-to-
sequence model enhanced with table constraint
and table relation embeddings. In contrast, our
table generation framework constructs the slotted
table input based on given trigger(s) and predefined
prompt(s), and generate in a non-autoregressive
manner.
3 Methodology
In this section, we will first give an formal def-
inition of EAE and then introduce TabEAE, our
solution to the task in detail.
3.1 Task Definition
An instance of EAE has the general form of
(x,{t},{e},{R},{A}), where
xis the text (a sentence or a document), Nis the12543
number of target events, tis the trigger of i-th
event, eis the type of i-th event, Ris the set of
argument roles associated with the event type e,
Ais the set of arguments of the i-th event and each
a∈Ais a textual span within xthat represents
the role r∈R. Different from the formulation by
previous research on EAE that only considers one
event for an input instance, this formulation takes
all events co-occurring in the same context into con-
sideration, providing a more comprehensive view
of the problem.
3.2 TabEAE
Our solution to EAE is a non-autoregressive table
generation framework, namely TabEAE, which is
derived from the SOTA prompt-based EAE model
PAIE. Figure 2 gives an overview of the framework.
A detailed description of each component comes
as follows.
3.2.1 Trigger-aware Context Encoding
Given an input text x=x, x, ..., xwith a set
of event triggers, we first mark each trigger with
a pair of markers (<T-i>,</T-i>), where icounts
the order of occurrence. Note that, there may be
multiple events sharing the same trigger, in which
case the shared trigger will only be marked once.
After that, we tokenize the marked text into
˜x= [<s>, x, x, ...,<T-1>, x,</T-1>,(1)
...,<T-i>, x,</T-i>, ..., x,</s>](2)
where xis the text fragment of the i-th trigger.By feeding ˜xinto a transformer-based encoder,
we can get the encoding of the text:
E=Encoder (˜x) (3)
We follow PAIE (Ma et al., 2022), to further
decodes Ewith a decoder to obtain the event-
oriented context representation:
H=Decoder (E) (4)
3.2.2 Slotted Table Construction
The decoder input is constructed as a slotted table,
where the column header is the concatenation of
event-schema prompt(s) proposed by PAIE. Consid-
ering the example in Figure 2, there are a Life-die
event with trigger “kills” and a Life-injure event
with trigger “injured”. Then the column header
is“Victim (and Victim ) died at Place (and Place )
killed by Killer (and Killer ).Victim (and Victim )
injured by Injurer (and Injurer ). ”, where the first
sentence is the prompt for Life-die event, the sec-
ond sentence is the prompt for Life-injure event,
and each underlined part is named after a argument
role, acting as the head of a column. There are
multiple columns sharing the same argument role
for the extraction of multiple arguments playing
the same role in an event.
We initialize the representation of column header
by feeding each prompt into the encoder in parallel
and concatenating the encoding outputs:
E=Encoder (PR) (5)
E= [E:...:E:...:E](6)12544where PRis the j-th prompt, Mis the number of
event type(s).
Thei-th row of the table starts with the i-th trig-
ger, followed by a sequence of argument slots S.
The initial representation of the i-th trigger, E, is
copied from the encoding of the marked text. And
the initial representation of argument slots, E, is
the average of the encoding of corresponding argu-
ment roles (in the column header) and the encoding
of corresponding trigger markers. We denote all
the argument slots as S={S}.
The initial representations of table components
are row-wise concatenated to obtain the initial rep-
resentation of the table:
E= [E:E:E:...:E:E](7)
3.2.3 Non-autoregressive Table Decoding
The non-autoregressive decoder iteratively updates
the representation of input table via structure-aware
self-attention inner the table as well as cross-
attention between the table and the encoder output.
Structure-aware Self-attention We devise a
structure-aware self-attention mask, M, so that
each element of the table can only attend to the
region related to it. Our design is as follows:
•All tokens within the column header attend to
each other.
•All tokens within the column header attend to the
event trigger(s).
•Each role along with corresponding argument
slot(s) attend to each other.
•Each event trigger along with corresponding ar-
gument slot(s) attend to each other.
Note that this attention mask is only used for
the decoding of slotted table. When computing H
(Equation 4), we employ normal self-attention.
The cross-attention mechanism is the same as
the one in Transformer (Vaswani et al., 2017) and
it is only employed to decode the table. When
computing H(Equation 4), it is skipped.
3.2.4 Span Selection
With the output of table decoding, H, we can
obtain the final representation of the argument slots,
H⊂H. We transform each representation
vector h∈Hinto a span selector {Φ,Φ}
(Du and Cardie, 2020; Ma et al., 2022):
Φ=h⊙w(8)
Φ=h⊙w(9)where wandware learnable weights, and ⊙
represents element-wise multiplication.
The span selector {Φ,Φ}is responsible
for selecting a span (ˆstart,ˆend)from the text to
fill in the argument slot s:
logit=HΦ∈R(10)
logit=HΦ∈R(11)
score(l, m) =logit[l] +logit[m](12)
(ˆstart,ˆend) = arg maxscore(l, m)
where lormrepresents the index of arbitrary token
within the text.
Note that, there can be more than one argument
playing the same role in an event, requiring fur-
ther consideration for the assignment of golden
argument spans during training. Hence, we follow
(Carion et al., 2020; Yang et al., 2021; Ma et al.,
2022) to fine tune our model with the Bipartite
Matching Loss. The loss for an training instance is
defined as
P=Softmax (logit) (13)
P=Softmax (logit) (14)
L=−/summationdisplay/summationdisplay(logP[start]
+ log P[end]) (15)
where δ(·)represents the optimal assignment cal-
culated with Hungarian algorithm (Kuhn, 1955)
according to the assignment cost devised by (Ma
et al., 2022), and (start, end)is the golden span
optimally assigned to the k-th argument slot. For
an argument slot relating to no argument, it is as-
signed with the empty span (0,0).
3.3 Three Training-Inference Schemes
Under the TabEAE framework, there exist three
possible training-inference schemes: (1) Single-
Single , train TabEAE to extract single event at a
time and infer in the same way; (2) Multi-Multi ,
train TabEAE to extract all events in parallel and in-
fer in the same way; (3) Multi-Single , train TabEAE
to extract all events in parallel and let it extract sin-
gle event at a time during inference. For the Single
mode, only one trigger is marked in the input text;
for the Multi mode, all the triggers are marked
in the text. Note that, when trained to extract all
events in parallel, TabEAE also learn to extract sin-
gle event, since a great portion of training instances
has only one event.12545
4 Experiments
4.1 Implementation Details
We implement TabEAE with Pytorch and run the
experiments with a Nvidia Tesla A100 GPU. We
instantiate the encoder with the first 17 layers of
RoBERTa-large (Liu et al., 2019).The weight
of the self-attention layers and feedforward layers
of the decoder is initialized with the weight of the
remaining 7 layers of RoBERTa-large. The setting
of 17-layer encoder + 7-layer decoder is found to be
optimal by our experiment (See Appendix C). Note
that the cross-attention part of the decoder is newly
initialized in random and we set its learning rate to
be 1.5 times the learning rate of other parameters.
We leverage the AdamW optimizer (Loshchilov
and Hutter, 2017) equipped with a linear learning
rate scheduler to tune our model. See Appendix B
for details of hyperparameter tuning.
4.2 Experiment Setups
Datasets We experiment with 4 datasets, includ-
ing ACE05 (Doddington et al., 2004), RAMS
(Ebner et al., 2020), WikiEvents (Li et al., 2021)
and MLEE (Pyysalo et al., 2012). ACE05 is
a sentence-level dataset, while the others are in
document-level. The corpora of ACE05, RAMS
and WikiEvents mainly consist of news, while the
corpus of MLEE lies in the biomedical domain.
Besides, the phenomenon of nested event is com-monly observed in MLEE, but rare in the other 3
datasets. See Appendix A for a detailed description
of the datasets.
Evaluation Metrics Following previous works
(Li et al., 2021; Ma et al., 2022), we measure the
performance with two metrics: (1) strict argument
identification F1 (Arg-I), where a predicted argu-
ment of an event is correct if its boundary matches
any golden arguments of the event; (2) strict argu-
ment classification F1 (Arg-C), where a predicted
argument of an event is correct only if its boundary
and role type are both correct. All the reported
results are averaged over 5 runs with different ran-
dom seeds.
4.3 Compared Methods
We compare TabEAE with several SOTA methods:
•EEQA (Du and Cardie, 2020), a QA-based
EAE model that treats EAE as a machine read-
ing comprehension problem;
•BART-Gen (Li et al., 2021), a seq-to-seq EAE
model that generates predicted arguments con-
ditioned on event template and context;
•TSAR (Xu et al., 2022), a two-stream AMR-
enhanced span-based EAE model;
•PAIE (Ma et al., 2022), a prompt-based EAE
model that leverages slotted prompts to obtain
argument span selectors;
•DEGREE (Hsu et al., 2022), a data-efficient
model that formulates EAE as a conditional
generation problem.12546
4.4 Main Results
The overall performances of compared baselines
and TabEAE are illustrated in Table 1. We find that
TabEAE (Single-Single) is competitive to previous
SOTA models (TSAR, DEGREE and PAIE) on
the four benchmarks. This is expected since these
models follow the same training-inference scheme
and leverage PLMs of the same scale.
In the mean time, TabEAE (Multi-Single) out-
performs the SOTA model by 0.6 Arg-I F1 and 1.1
Arg-C F1 on ACE05, by 0.2 Arg-I F1 and 0.4 Arg-
C F1 on RAMS, by 0.3 Arg-I F1 and 0.7 Arg-C F1
WikiEvents.
As for the MLEE dataset, TabEAE (Multi-Multi)
performs better than TabEAE (Multi-Single) and
yields 2.5 Arg-I F1 gain, 2.7 Arg-C F1 gain com-
pared to SOTA models.
We analyze the reason behind the results in §5.1.5 Analysis
5.1 The Effect of Training-inference Scheme
To analyze the influence of the training-inference
scheme, we measure the performances of EAE
models with different training-inference schemes
on handling instances with different numbers of
events. The results are shown in Table 2. We can
see that PAIE (Single-Single) and TabEAE (Single-
single) have similar capacity in extracting stand-
alone events and co-occurring events.
When trained to extract all the events in parallel,
the Arg-C F1 of TabEAE on instances with single
event increases by 2.17, 0.05, 2.03 and 1.87 on
the 4 datasets respectively. However, by letting
TabEAE extract all the events in parallel during
inference, the Arg-C F1 on instances with multiple
events drops by 0.38, 0.79, 0.14 on ACE, RAMS
and WikiEvents respectively, while increasing by
3.28 on MLEE. We believe this phenomenon is the
result of two factors:
1.The distribution of the number of events per
instance. As plotted in Figure 3, there are
much more instances with multiple events on
WikiEvents and MLEE than on ACE05 and
RAMS. Hence, the model is better trained
to extract multiple events concurrently on
WikiEvents and MLEE.
2.Difficulty. Generally, it is more difficult for
a model to extract all the events in one pass.
But it is not the case for the MLEE dataset,
since there are around 32.9% of the arguments
acting as triggers of other events in MLEE,
and when all triggers are provided (as in the
Multi-Multi scheme), it become easier for the
model to extract all the arguments.
When training TabEAE to extract all events in
parallel and letting it extract one event at a time
during inference, the Arg-C F1 of TabEAE on in-
stances with multiple events increases by 2.3, 0.88,12547
0.73 on ACE, RAMS and WikiEvents respectively.
This is reasonable, since there is a large portion of
instances having only one event, which means the
model is also well-trained to extract one event at a
time under the Multi-Single scheme.
5.2 Capturing the Event Semantic Boundary
We hypothesize that the performance gains yielded
by the Multi-Multi and Multi-Single schemes
rooted in the stronger ability of TabEAE to capture
the event semantic boundary. To verify this, we
further measure the model’s ability to capture the
event semantic boundary from two points of view:
(1) Inter-event Semantic; (2) Inner-event Semantic.
From the view of inter-event semantic , we com-
pare the performance of TabEAE with different
training-inference schemes in terms of their ability
to extract the arguments of overlapping events (i.e.,
events with shared arguments). As illustrated in
Table 3, when trained to extract all events concur-
rently, the model’s performance gains of extract-
ing the arguments of overlapping events are much
higher than that of extracting the arguments of non-
overlapping events. Specifically, the differences of
performance gains are 0.5 Arg-C F1 on ACE05, 0.4
Arg-C F1 on RMAS, 1.8 Arg-C F1 on WikiEvents
and 0.9 Arg-C F1 on MLEE. This suggests that
TabEAE can better distinguish the semantic bound-
ary between overlapping events.
From the view of inner-event semantic , we
compare the performance of TabEAE with differ-
ent training-inference schemes in terms of their
ability to extract arguments of different distances
to the triggers. We define the distance here as the
head word index of an argument minus the head
word index of its corresponding trigger. The ex-
periments are conducted on the document-level
datasets WikiEvents and MLEE, where the distance
distribution of event arguments is more disperse.
The results are plotted in Figure 4. We can observe
that, when equipped with the Multi-Multi/Multi-Single schemes the model’s performance gains of
extracting remote arguments are higher than the
performance gains of extracting nearby arguments.
This means TabEAE gets better at extracting argu-
ments around the event boundary.
5.3 Ablation Study
To verify the effectiveness of different components
of TabEAE, we conduct ablation study on the 4
datasets. The results are illustrated in Table 4.
After removing the structure-aware attention
mask , the Arg-C F1 scores drop by 1.9, 1.5, 1.1,
1.5 on ACE05, RAMS, WikiEvents and MLEE re-
spectively. This demonstrates the benefit of letting12548
each table token only paying attention to the table
region related to it.
After replacing the pre-computed encodings of
the input table with RoBERTa token embeddings,
the Arg-C F1 scores drop by 4.2, 3.4, 4.6, 3.9 on the
4 datasets. This proves the necessity of initializing
the embeddings of input table with the encodings
computed by the encoder.
When constructing the table column header with
the concatenation of argument roles instead ofprompts , the Arg-C F1 scores drop by 2.5, 1.8,
1.7 and 2.9 on the 4 datasets respectively. This
coincides with the finding by (Ma et al., 2022) that
hand-crafted prompts can be of great help to the
task of EAE.
When replacing the encoder/decoder of TabEAE
with BART encoder/decoder, the model perfor-
mance degrades by 2.3, 1.7, 1.1, 1.8 on the 4
datasets respectively. The reason behind this degra-
dation should be the uni-directional self-attention
employed by BART decoder is not suitable for the
decoding of table.
5.4 Case Study
Figure 5 illustrates 2 test cases from ACE05 and
MLEE respectively. In the first test case, there ex-
ist 2 events triggered by “leaving” and “become”,
with a shared argument “Davies”. PAIE incorrectly
predicts “London School of Economics” as an ar-
gument of the event triggered by “leaving”, which
is essentially an argument of the event triggered
by “become”. In contrast, TabEAE is able to avoid
this mistake, demonstrating a stronger capacity to
capture the event semantic boundary.
In the second test case, there exist 3 events trig-
gered by “regulator”, “regulates” and “angiogene-
sis” respectively. Among them, the event triggered
by “angiogenesis” has no argument. For the event
triggered by “regulates”, PAIE fails to extract the
remote argument “Vascular endothelial growth fac-
tor”, while TabEAE correctly extracts it by being
aware of the co-occurring event that shares this
argument.
6 Conclusion
In this paper, we point out that recent studies on
EAE ignore event co-occurrences, resulting in a
divergence from main-stream EE research. To rem-
edy this, we highlight the question that “Can EAE
models learn better when being aware of event
co-occurrences” and explore it with a novel text-to-
table framework, TabEAE , that can extract multiple
event in parallel. By experimenting with 3 training-
inference schemes on 4 datasets, we find that when
trained to extract all event concurrently, TabEAE
can better capture the event semantic boundary and
its ability to extract single event gets greatly im-
proved. Our work demonstrates the significance of
event co-occurrence for EAE and establishes a new
foundation for future EAE research.125497 Limitations
In this section, we summarize the limitations of our
work as follows:
•There is still a lot more to explore in terms of
event co-occurrence for EAE (e.g., iterative ex-
traction, course learning, etc.). We are unable to
cover all in this work and will explore further in
the future.
•As demonstrated by our ablation study, the high
performance of our model greatly relies on the
manual prompts. This limits the application
of our model to the scenes where high-quality
prompts are unavailable and difficult to construct.
To address this, we should look into the area of
automatic prompt construction.
•Our work ignores the phenomenon of entity co-
reference commonly existing in narrative doc-
uments. This limits the model’s ability to fig-
ure out the underlying relation between entities,
which is crucial for the task of EAE. And we
will take entity co-references into account in our
future works.
Acknowledgments
We thank the reviewers for their insightful com-
ments and valuable suggestions. This study is par-
tially supported by National Key R&D Program
of China (2021ZD0113402), National Natural Sci-
ence Foundations of China (62276082, U1813215
and 61876052), National Natural Science Founda-
tion of Guangdong, China (2019A1515011158),
Major Key Project of PCL (PCL2021A06), Strate-
gic Emerging Industry Development Special Fund
of Shenzhen (20200821174109001) and Pilot
Project in 5G + Health Application of Ministry of
Industry and Information Technology & National
Health Commission (5G + Luohu Hospital Group:
an Attempt to New Health Management Styles of
Residents).
References1255012551
A Profile of Datasets
ACE05 (Doddington et al., 2004)is an anno-
tated information extraction corpus of newswire,
broadcast news and telephone conversations. Weutilize its English event annotation for sentence-
level EAE. We preprocess the data in the same way
as Wadden et al. (2019) do.
RAMS (Ebner et al., 2020)is a document-
level EAE dataset, which contains 9,124 annotated
events from English online news. Since it is anno-
tated event-wise (each event occupies one instance),
we have to aggregate events occurring in the same
context into one instance with multiple events. We
follow the original train/dev/test data split.
WikiEvents (Li et al., 2021)is a document-level
EAE dataset, consisting of events recorded in En-
glish Wikipedia along with the linking news articles
that mention these events. The dataset is also anno-
tated with the co-reference links of arguments, but
we only use the exact argument annotations in our
experiments.
MLEE (Pyysalo et al., 2012)is a document-
level event extraction dataset with manually anno-
tated abstracts of bio-medical publications written
in English. We follow the preprocessing procedure
of (Trieu et al., 2020). Since there is only train/test
data split for the preprocessed dataset, we employ
the training set as the development set.
Statistics Detailed statistics of the datasets are
listed in Table 5.
B hyperparameter Settings
Most of the hyperparameters follow the same con-
figuration of (Ma et al., 2022). We only tune a few
hyperparameters manually for each dataset by try-
ing different values of each hyperparameter within
an interval and choosing the value that results in
the highest Arg-C F1 on the development set. The
trial-intervals and the final hyperparameter config-
uration are shown in Table 6.
C Number of Encoder/Decoder Layers
We have employed the bottom layers of RoBERTa-
large as our encoder and the top layers of RoBERTa-
large as our decoder. To find the optimal layer allo-
cation, we have tried different settings and recorded
the corresponding model performance. This exper-
iment is conduct on ACE and MLEE. The results
are plotted in Figure 6. We can observe that the
overall performance on the two datasets reaches12552Dataset ACE05 RAMS WikiEvents MLEE
# Event types 33 139 50 23
# Args per event 1.19 2.33 1.40 1.29
# Events per text 1.35 1.25 1.78 3.32
# Events
Train 4202 7329 3241 4442
Dev 450 924 345 -
Test 403 871 365 2200
hyperparameters Trial-Interval ACE05 RAMS WikiEvents MLEE
Training Steps - 10000 10000 10000 10000
Warmup Ratio - 0.1 0.1 0.1 0.1
Learning Rate - 2e-5 2e-5 2e-5 2e-5
Max Gradient Norm - 5 5 5 5
Batch Size [2, 16] 8 4 4 4
Context Window Size - 250 250 250 250
Max Span Length - 10 10 10 10
Max Encoder Seq Length - 200 500 500 500
Max Decoder Seq Length [200, 400] 250 200 360 360the peak when there are 17 encoder layers and 7
decoder layers in the model. This observation coin-
cides with recent findings on the areas of machine
translation and spell checking that “deep encoder
+ shallow decoder” is superior to the conventional
architecture with balanced encoder-decoder depth
(Kasai et al., 2020; Sun et al., 2021).
D Prompt Construction
The prompts for ACE05, RAMS and WikiEvents
are directly from (Li et al., 2021; Ma et al., 2022),
which are manually constructed from the pre-
defined ontology associated with each dataset. For
MLEE, we manually construct the prompts in a
similar manner, as shown in Table 7.1255312554ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7.
/squareA2. Did you discuss any potential risks of your work?
No potential risk is forseen.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract, Section 1.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 1, 3, 4.1, 4.2, Software Supplement.
/squareB1. Did you cite the creators of artifacts you used?
Section 1, 3, 4.1, 4.2.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
README (Software Supplement).
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
README (Software Supplement).
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
The datasets that we used are from ofﬁcial and trusted sources.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 4.2, Appendix A.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A.
C/squareDid you run computational experiments?
Section 4, 5.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
The backbone of our model is RoBERTa, which is commonly used and quite familiar to NLP
researchers.12555/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4.1, 4.2, Appendix B.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.2.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4.1, Software Supplement.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.12556