
Mohsen Tabasi,Kiamehr RezaeeandMohammad Taher PilehvarDepartment of CE, Iran University of Science and Technology, Tehran, IranCardiff NLP, School of Computer Science and Informatics, Cardiff University, UKTehran Institute for Advanced Studies, Khatam University, Tehran, Iran
s_tabasi@comp.iust.ac.ir ,rezaee.k@cardiff.ac.uk
mp792@cam.ac.uk
Abstract
As a recent development in few-shot learn-
ing, prompt-based techniques have demon-
strated promising potential in a variety of nat-
ural language processing tasks. However, de-
spite proving competitive on most tasks in
the GLUE and SuperGLUE benchmarks, ex-
isting prompt-based techniques fail on the se-
mantic distinction task of the Word-in-Context
(WiC) dataset. Speciﬁcally, none of the ex-
isting few-shot approaches (including the in-
context learning of GPT-3) can attain a perfor-
mance that is meaningfully different from the
random baseline. Trying to ﬁll this gap, we
propose a new prompting technique, based on
similarity metrics, which boosts few-shot per-
formance to the level of fully supervised meth-
ods. Our simple adaptation shows that the fail-
ure of existing prompt-based techniques in se-
mantic distinction is due to their improper con-
ﬁguration, rather than lack of relevant knowl-
edge in the representations. We also show
that this approach can be effectively extended
to other downstream tasks for which a single
prompt is sufﬁcient.
1 Introduction
Recently, there has been a resurgence of interest in
few-shot learning, especially after the introduction
of GPT-3 (Brown et al., 2020). The current dom-
inant few-shot approach is the so-called prompt-
based learning which involves a simple reformu-
lation of the target task as a cloze-style (Taylor,
1953) ﬁll-in-the-blank objective. The core idea is
to extract knowledge by asking the right question
from the pre-trained language model (PLM) using
a task-speciﬁc prompting template which directs
the PLM to generate a textual output correspond-
ing to a target class. This paradigm has proven its
effectiveness in the few-shot setting, even for rela-
tively smaller models, such as BERT (Devlin et al.,2019) and RoBERTA (Liu et al., 2019), when com-
bined with ensembling and ﬁne-tuning (Schick and
Schütze, 2021a). From the practical point of view,
prompt-based learning is particularly well-suited
for massive models, such as GPT-3, since it does
not involve parameter tuning.
Prompt-based techniques have shown impres-
sive performance in the few-shot setting, especially
when compared to standard ﬁne-tuning on datasets
of hundreds of data points (Le Scao and Rush,
2021). However, surprisingly, the Word-in-Context
task (Pilehvar and Camacho-Collados, 2019) –one
of the tasks in the SuperGLUE benchmark (Wang
et al., 2019)– is one exception on which these meth-
ods fail to stay on par with their ﬁne-tuned coun-
terparts.While a simple ﬁne-tuned BERT-base
model achieves around 69% accuracy on this task
(Wang et al., 2019), GPT-3, with more than 100
times the number of parameters, performs no better
than a random baseline by employing a prompt-
based approach (Brown et al., 2020). The same pat-
tern of failure is also observed in the more recent
prompt based attempts (Liu et al., 2021; Schick and
Schütze, 2021a).
The natural question that arises here is if the fail-
ure of few-shot techniques on WiC is due to lack
of relevant encoded knowledge in PLMs or the in-
efﬁciency of the employed prompt-based methods.
Two issues could be responsible for the latter case:
(1) improper prompt, or (2) inefﬁcient utilization
of PLM’s response. To address the ﬁrst issue, there
have been proposals to automatically ﬁnd a suit-
able prompt template using a search in the discrete
token space (Shin et al., 2020) or in the continuous
embedding space (Liu et al., 2021). However, none
of these have shown success on the WiC task.
In this work we investigate the latter issue by325
introducing a new conﬁguration for prompting.
Given the comparison-based nature of WiC, we
hypothesize that conventional prompting methods
fall short since they only utilize a single prompt
response. Hence, instead of relying on a single
response, we make use of the similarity of PLM’s
response to the combination of a pair of prompts.
The experimental results on the WiC dataset shows
that, with only 16 instances per class, our proposed
prompt-based technique can achieve comparable
results to the ﬁne-tuned models (with access to
full training data of 2700+ instances per class).
Moreover, we show that with few adjustments, this
simple approach can be effectively used for other
downstream tasks.
2 Methodology
Fine-tuning on a speciﬁc task can potentially up-
date PLMs on what the task is and how to solve it.
Assuming that PLMs know how to solve some tasks
(to some extent), prompt-based learning focuses on
the former, i.e., teaching the model what the task
is, without needing to resort to large amounts of
data or additional parameters. The common ap-
proach in prompt-based learning is to reformulate
the task as a cloze-style question. For instance,
to ask about the sentiment of a movie review, one
can augment the review with a cloze question like
“this movie was ——.”. Existing methods often
pick a set of one or few word predictions as a rep-
resentative for each class, utilizing the languagemodel’s response in a sub-optimal manner. We pro-
pose a similarity-based method that not only better
exploits the response, but also allows using multi-
ple prompts which paves the way for comparison-
based tasks, such as WiC. In what follows in this
section, we describe our similarity-based prompt-
ing approach which we will refer to as SP(Similar-
ity Prompting).
As shown in Figure 1, SP consists of three main
steps: (1) prompt generation, (2) feature extrac-
tion, and (3) prediction. Given a task-speciﬁc input
consisting of one or more text sequences, we ﬁrst
use a template function to generate a prompt—a
sequence of tokens containing one [ ] to-
ken—per input sequence. For instance, in senti-
ment analysis, for the movie review “Just give it a
chance.”, a valid template function would generate
as output prompt: “Just give it a chance. this movie
was ——.”. The next step is feature extraction
from a PLM. This is done by giving the generated
prompts to the PLM as input and obtaining its con-
textualized embedding at the index.
The third step is where SP differs from existing
prompt-based approaches. Here, we ﬁrst obtain
class-speciﬁc centroids by taking the average of
the embeddings of our few training exam-
ples. To classify a new sample at inference time, a
simple approach would be to employ a nearest cen-
troid classiﬁer. However, this assumes the variance
of different classes to be equal in the embedding
space. To alleviate the problem, we perform a class
centroid-based dimension reduction (i.e. by taking326the similarity to each centroid as a feature), and
train a simple linear classiﬁer. This linear model is
then used at inference time to evaluate SP on test
set.
2.1 Similarity Prompting for WiC
The surprising failure of existing prompt-based
techniques on the Word-in-Context task (Pilehvar
and Camacho-Collados, 2019, WiC), motivated us
to focus on ﬁlling this gap. Given an ambiguous
target word in two different contexts, the task in
WiC is deﬁned as a simple binary classiﬁcation
problem to identify if the triggered meaning of the
target word differs in the two contexts or not.
Previous work has fallen short of designing a sin-
gle prompt template which make the PLM answer
about the target word having the same meaning
or not (e.g., with "yes" or "no"). Therefore, we
ask PLM about the triggered meaning of the tar-
get word, separately for each context, and leave
the comparison to similarity measures. Having an
input sentence and the target word index, we in-
sert “or ——” after the target word, where “——”
indicates the token. In the ﬁrst step of SP,
we apply this template function to both input sen-
tences which generates a pair of prompts. Next
the prompts are separately fed to PLM, resulting
in a pair of mask embeddings as PLM’s response.
Finally, our classiﬁcation step reduces to that of
directly comparing our pair of embedding vectors
using a similarity function, to produce a single
similarity score for each instance. We then train
the same linear model as before on the similarity
scores of the training set examples to ﬁnd the best
discriminating threshold.
Similarity Measures. We opted for two similar-
ity metrics: cosine similarity and Spearman’s rank
correlation. The latter is a rank-based comparison
measure which is insensitive to the absolute values
of individual dimensions (rather checks for their
relative rankings).
3 Experiments
3.1 Comparison Systems
We compare our results on WiC with three other
methods, all of which use 32 examples for their
training. PET (Schick and Schütze, 2021b)
prefers ALBERT-xxlarge-v2 (Lan et al., 2019) over
RoBERTa (with an average gain of 8 points on a
subset of SuperGLUE tasks) and ﬁne-tunes it withmanually engineered cloze-style prompts. P-tuning
(Liu et al., 2021) uses the same PLM as PET, but
optimizes a continuous prompt instead of tuning
PLM parameters. GPT3 (Brown et al., 2020) is
different in that it employs the so-called in-context
learning which involves no parameter tuning.
3.2 Tasks
In addition to WiC, we also carried out experiments
on two more tasks. The goal of this additional ex-
periment is twofold: ﬁrst, to show the applicability
of SP to other settings, including tasks with single
input sequence; and second, to evaluate if SP is
effective when using prompt templates from other
techniques, including those optimized for speciﬁc
tasks. For this experiment, we compare against
AutoPrompt (Shin et al., 2020). The approach
makes use of full training set to optimize discrete
prompts for each speciﬁc target task. Following
AutoPrompt, we report results for the following
two task:
SST. Stanford Sentiment Treebank (Socher et al.,
2013) contains ﬁne-grained sentiment labeled parse
trees of sentences from movie reviews. Systems
are evaluated either on a ﬁve-way ﬁne-grained
or binary classiﬁcation task. We follow the lat-
ter (SST-2) in our experiments. For this task we
used the automatically-generated template of Auto-
Prompt, along with the following manual template:
T(sent ) =sent + “ this movie was ——.”, where
sent is the input sentence and “+” is concatenation
operator. This is the same manual prompt used in
AutoPrompt.
SICK. Sentences Involving Compositional
Knowledge (Marelli et al., 2014) is a collection
of sentence pairs annotated with their entailment
relationship as well as a quantiﬁed measurement
of their semantic similarity. In our experiments,
we only use the former annotations (SICK-E) to
compare our results with AutoPrompt, which only
reports results for its optimized prompt. Thus
we deﬁne our own manual template function as:
T(pre; hyp ) =pre+ “? Answer: ——, ” + hyp,
where preis the premise and hypis the hypothesis
of an input example.
3.3 Setup
To train our models, we only used 16 examples per
class. As for PLM, we opted for RoBERTA-large
to be able to benchmark our results against Auto-
Prompt’s (Shin et al., 2020). Our experiments are327
repeated 5 times using different randomly sampled
training examples. For each experiment, we report
the average performance along with the standard
deviation.
3.4 Results
Given that our experiments are mainly focused on
the WiC dataset, we ﬁrst report our results on this
benchmark, and then provide additional results for
the other two tasks.
3.4.1 WiC
Table 1 summarizes the results on WiC with
RoBERTa-Large as SP’s PLM. The performance
of SP in the few-shot setting is in the same ball-
park as supervised ﬁne-tuning (with nearly 170
times the data, i.e., 2,714 instances per class). This
observation suggests that PLMs already encode a
certain amount of task-related knowledge and the
supervised ﬁne-tuning mainly updates their task
description (i.e., what the task is, not how to solve
it). Therefore, using limited examples in the few-
shot setting they are able to reach their maximum
ﬁne-tuning potential on WiC. We report SP’s per-
formance on WiC for other PLMs in the Appendix
which shows our method/observation does not de-
pend on a speciﬁc PLM. We also include some
detailed examples of how SP works for WiC in the
Appendix.
3.4.2 SICK and SST-2
The results on SST-2 and SICK-E are shown in
Table 2. We compare SP with AutoPrompt which
searches for the best template for each task. For
SST-2, we observe that SP can exploit a manual
prompt template signiﬁcantly better than Auto-
Prompt, while being competitive using the best tem-
plate optimized by AutoPrompt (auto-generated).
This suggests that it is possible to gain signiﬁcant
improvement by simply exploiting a non-optimized
manual prompt template.
To compare our results with AutoPrompt on the
SICK-E task, we report accuracy score of SP for the
standard test set (with neutral majority) and its bal-
anced variant. SP retains an acceptable level of per-
formance, particularly with the manual prompt, but
lags behind with the auto-generated prompt. We
note that the goal of this experiment was to show-
case that our simple adaptation is also applicable to
scenarios other than the setting of WiC. In fact, one
could argue that the auto-generated prompt of Auto-
Prompt is sub-optimal for our model, which results
in dropped performance on the SICK-E dataset.
3.5 Similarity Measures Comparison
Notably, the Spearman correlation score, which
is less commonly used for comparing embeddings,
outperforms the cosine similarity on WiC by a large
margin while maintaining the same level of per-
formance on other tasks. This superiority can be
explained by the assumption that cosine similarity
is more susceptible to variations in the dominant
dimensions. To evaluate this hypothesis, we per-
formed an experiment in which the most dominant
dimension was set to zero for all the embeddings
(the dominant dimension is identical across all vec-
tors). The results approve the assumption: pruned
cosine similarity gains around 10% absolute perfor-
mance boost on WiC, ﬁlling the gap to Spearman
correlation. However, the gain in the other two
tasks is negligible.
The difference in the gain across tasks can be ex-
plained by the difference in their underlying nature.328
In WiC, the embeddings can potentially re-
fer to any word, varying from sample to sample.
However, in SST and SICK the template
embedding is more restricted, often representing
a closely related word to one of the class centroid
embeddings (e.g., in SST the embedding
almost always represents a positive or negative ad-
jective). This results in a higher spread on the
most dominant dimension in the case of WiC. It is
known that the most dominant dimensions in PLMs
often encode irrelevant information, such as word
frequency (Gao et al., 2019), therefore hampering
performance for sensitive metrics such as cosine
similarity. To verify our hypothesis, we ran an ex-
periment using 1200 sample embeddings
for each of our three tasks. Figure 2 illustrates
the distribution of values for the most dominant
dimension. The ratio of variance is 6.5 times for
WiC compared to SST and 27.3 times compared
to SICK. This further supports the sensitivity of
cosine similarity for WiC to the noisy variations
along the most dominant dimension compared to
the other two tasks.
4 Conclusion
We proposed an adaptation of prompt-based learn-
ing which addresses the common failure of existing
techniques on the WiC dataset. In this work we
showed that similarity based approach to prompt-
based learning is capable of achieving compara-
ble results to purely ﬁne-tuning based methods on
Word-in-Context task, in which previous few-shot
attempts have failed. We also showed that Spear-
man’s ranking correlation is a more robust choice
of similarity measure compared to cosine similarityin this setting. We hope that our positive results in-
spire other prompting strategies to better exploit the
encoded knowledge in PLMs. As future work, one
interesting direction could be to perform further
analysis on the behaviour of Spearman’s correla-
tion compared to cosine similarity anywhere it is
applicable as a similarity measure.
References329330A Experiments with other PLMs
This appendix contains more details on WiC exper-
iments. Table 3 shows full test set results of SP for
different PLMs and similarity measures to compare
the performance of SP in different scenarios. Since
our cloze-style prompt template is not applicable to
GPT2, we use a different template for it: sentence
+targetword + " means ——". The results in
Table 3 generally conﬁrm the effectiveness of SP
with different PLMs. Notably, this observation is in
line with our previous experiments that in general
Spearman has superior performance over Cosine
similarity.
Base model Cosine Spearman
RoBERTa-Large 63.6 70.2
BERT-Large-Cased 69.4 69.0
RoBERTa-Base 63.8 68.7
BERT-Base-Cased 64.8 67.1
GPT2-Large 56.4 63.3
GPT2-Base 62.3 62.6
B Qualitative Analysis
We include some examples of how SP works on
WiC in Table 4 for qualitative analysis. The exam-
ples are those from WiC dev set which had negative
labels. We did not include the positive examples,
since the observation that the same words with the
same senses are treated similarly, might not provide
a useful insight. The table presents our generated
prompts, top-5 most probable words predicted by
RoBERTa-Large for each prompt and the ﬁnal pre-
diction of SP. The top three examples are correctly
predicted as negative with high conﬁdence (high
similarity score), while the bottom three are pre-
dicted positive again with high conﬁdence. The
most probable predicted words for the top three
examples indicate that the PLM has spotted the
correct senses in both contexts. For the bottom
three where the model fails, we can observe that
the target words have very similar or close senses,
making them really hard to distinguish.331Prompt1 (Top-5 words) Prompt2 (Top-5 words) Prediction Ground
Truth
The drawing or —— of water from
the well.He did complicated pen-and-ink
drawings or —— like medieval
miniatures.Not
matchedNot
matched
(use, extraction, taking, pumping,
consumption)(paintings, sculptures, something,
more, looked)
The body or —— of the car was
badly rusted.Administrative body or ——. Not
matchedNot
matched
(trunk, roof, chassis, frame, grill) (agency, institution, government,
commission, equivalent)
The main body of the sound or ——
ran parallel to the coast.He strained to hear the faint sounds
or ——.Not
matchedNot
matched
(river, bay, sea, ocean, channel) (voices, footsteps, whispers, con-
versations, cries)
He could not conceal his hostility
or ——.He could no longer contain his hos-
tility or ——.Matched Not
matched
(anger ,disgust , irritation, contempt,
frustration )(anger , rage, frustration , aggres-
sion, disgust )
There was a blockage or —— in the
sewer, so we called out the plumber.We had to call a plumber to clear
out the blockage or —— in the
drainpipe.Matched Not
matched
(something, leak,obstruction , de-
fect, overﬂow)(debris, obstruction , water, leak,
crack)
The senator received severe criti-
cism or —— from his opponent.The politician received a lot of pub-
lic criticism or —— for his contro-
versial stance on the issue.Matched Not
matched
(threats, ridicule ,mockery , attacks,
threat)(backlash, ridicule ,mockery , con-
demnation, criticism)332