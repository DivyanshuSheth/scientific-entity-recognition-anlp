
Qinhong Zhou, Zonghan Yang, Peng Li, Yang LiuDept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, ChinaInstitute for AI Industry Research (AIR), Tsinghua University, Beijing, ChinaBeijing National Research Center for Information Science and TechnologyShanghai Artificial Intelligence Laboratory, Shanghai, China
Abstract
Conventional knowledge distillation (KD)
methods require access to the internal infor-
mation of teachers, e.g., logits. However, such
information may not always be accessible for
large pre-trained language models (PLMs). In
this work, we focus on decision-based KD for
PLMs, where only teacher decisions (i.e., top-1
labels) are accessible. Considering the infor-
mation gap between logits and decisions, we
propose a novel method to estimate logits from
the decision distributions. Specifically, deci-
sion distributions can be both derived as a func-
tion of logits theoretically and estimated with
test-time data augmentation empirically. By
combining the theoretical and empirical estima-
tions of the decision distributions together, the
estimation of logits can be successfully reduced
to a simple root-finding problem. Extensive ex-
periments show that our method significantly
outperforms strong baselines on both natural
language understanding and machine reading
comprehension datasets.
1 Introduction
Various natural language processing (NLP) tasks
have witnessed promising performance from large
pre-trained language models (PLMs) (Devlin et al.,
2019; Liu et al., 2019; Raffel et al., 2020; Brown
et al., 2020). However, PLMs are usually computa-
tionally expensive and memory intensive, hinder-
ing their deployment on resource-limited devices.
Knowledge distillation (KD) (Hinton et al., 2015)
is a popular technique to transfer knowledge from
large PLMs to lightweight models. Previous KD
works utilize various types of internal information
from the teacher model, such as output logits (Sanh
et al., 2019; Tang et al., 2019; Liu et al., 2020),
hidden states (Sun et al., 2019b; Jiao et al., 2020),and attention maps (Li et al., 2020). In real-world
applications, however, these types of information
are sometimes not accessible due to commercial
and privacy issues (Brown et al., 2020; Ouyang
et al., 2022). Specifically, large-scale PLMs usu-
ally only provide decisions (i.e., top-1 labels) to
users. Motivated by this scenario, we investigate
the task of decision-based KD (Wang, 2021) for
PLMs, in which only decisions of teacher predic-
tions are available.
The information gap between teacher decisions
and its internal states is the major challenge for
the task. A straightforward approach for decision-
based KD is to treat teacher decisions as ground
truth labels and use these labels to train a student
model (Zhang et al., 2022; Sanyal et al., 2022).
However, previous work reveals that logits contain
rich knowledge (Hinton et al., 2015), relying only
on decisions obviously suffers from information
loss. To alleviate the problem, Wang (2021) pro-
poses the DB3KD method to generate pseudo soft
labels according to the sample’s robustness. How-
ever, DB3KD requires that the input of a model
can be modified continuously (e.g., image), which
hinders its application on PLMs as their inputs are
discrete tokens. Therefore, how to fill the informa-
tion gap under the discrete input setting remains a
challenging problem.
Fortunately, the development of test-time data
augmentation for discrete input (Liu, 2019;
Shleifer, 2019; Xu et al., 2022) brings hope for
resolving the challenge. The basic idea is to mod-
ify selected tokens in a piece of text under certain
constraints to generate augmented samples and es-
timate or improve the desired properties of a model
based on its behaviors on these samples. Test-time
data augmentation has been shown to be effec-
tive for uncertainty estimation (Ayhan and Berens,
2018; Smith and Gal, 2018; Wang et al., 2019),
adversary robustness (Xu et al., 2022), and so on.
Is it possible to narrow down the information gap13234with test-time data argumentation in decision-based
KD for PLMs?
In this work, we propose a novel decision-based
KD method for PLMs. As illustrated in Figure 1,
our method is capable of estimating the teacher
logits for classes even without observed decisions,
narrowing down the information gap between deci-
sion and logits. Specially, we estimate the logits by
combining test-time data argumentation and non-
centred orthant probability estimation. On the one
hand, we can obtain an empirical estimation of the
decision distribution around a sample by test-time
data argumentation. On the other hand, we can also
derive a theoretical formula for the decision distri-
bution as a non-centred orthant probability, which
is a function of logits. As a result, the problem of
logits estimation can be reduced to finding the root
of the equation that the function takes the value of
the empirical estimation. Extensive experiments
on various natural language understanding and ma-
chine reading comprehension datasets demonstrate
the effectiveness of our proposed method, which
outperforms strong baselines significantly. More-
over, quantitative analysis reveals that our method
obtains better estimation of logits, narrowing down
the information gap.
2 Related Work
Decision-based Knowledge Distillation. To ad-
vance conventional knowledge distillation (KD)
to more challenging black-box model scenarios,
Wang (2021) first propose the problem of decision-
based KD, where only teacher decisions (i.e., top-1
labels) are accessible to students. They address
the problem by estimating the soft label (analogy
to output probabilities) of a sample based on its
distance to the decision boundary, which involves
continuous modification to the original input fed to
the black-box model. Instead, Zhang et al. (2022)
and Sanyal et al. (2022) synthesize pseudo data
in continuous space and leverage decisions of the
teacher model on these data directly. In this work,
we focus on decision-based KD for PLMs. Unfortu-
nately, the original inputs of the PLMs are discrete
tokens which can not be continuously modified.
Therefore, these methods are not applicable to our
scenario.
Decision-based KD is also related to black-box
KD (Orekondy et al., 2019; Wang et al., 2020) and
distillation-based black-box attacks (Zhou et al.,
2020; Wang et al., 2021; Truong et al., 2021;Kariyappa et al., 2021; Yu and Sun, 2022). Both
of them involve distilling a student model from
black-box models. However, these works generally
assume that the score-based outputs are accessible.
Decision-based KD focuses on a more challenging
scenario where only top-1 labels are accessible.
Test-Time Data Augmentation. Test-time data
augmentation is a common technique in computer
vision (Krizhevsky et al., 2009; Simonyan and Zis-
serman, 2015; He et al., 2016; Wang et al., 2019;
Lyzhov et al., 2020; Shanmugam et al., 2021) and
is also feasible for natural language processing
(NLP) (Liu, 2019; Shleifer, 2019; Xu et al., 2022).
Although differing in final purpose, test-time and
training-time data augmentation share a large por-
tion of common techniques in NLP. Due to the
discrete nature of language, one line of work con-
ducts augmentation by modifying tokens based on
rules (¸ Sahin and Steedman, 2018; Wei and Zou,
2019; Chen et al., 2020a) or models (Sennrich
et al., 2016; Yang et al., 2020; Quteineh et al.,
2020; Anaby-Tavor et al., 2020), and another line
of work operates in embedding or representation
space (Chen et al., 2020b; Cheng et al., 2020; Chen
et al., 2021; Wei et al., 2022). In this work, as we
do not have access to the teacher model, we fol-
low the first line of work to conduct test-time data
augmentation.
3 Background
Knowledge Distillation (KD) is a technique that
aims to transfer knowledge from the teacher model
to the student model by aligning certain statistics,
usually the logits, of the student to those of the
teacher. Given input x, we denote the pre-softmax
logits vector of the teacher and student as zandv,
respectively. The process of KD involves minimiz-
ing the Kullback-Leibler (KL) divergence between
the probabilities induced from zandvas follows:
L= KL (softmax( v/τ)||softmax( z/τ)),
(1)
where τis the temperature hyper-parameter. The
student model is trained by minimizing the loss
function
L=L+λL, (2)
where Lis the cross entropy loss over the
ground-truth label, and λis the scaling factor used
for balancing the importance of the two losses.13235
4 Methodology
4.1 Overview
In the decision-based scenario, the zitem in Eq. 1
is not accessible. Instead, the PLM API returns the
model decision d= arg maxz, where zis the j-
th logit in z, andLdenotes the dimension of output
label space. Obviously, donly carries the informa-
tion of “the j-th logit is the largest”. In constrast, z
contains richer information. For example, compar-
ingz= [0.6,0.3]andz= [0.9,0.1], although
they correpond to the same decision, i.e., d= 1,
they also imply that the second sample seems more
likely to be of the first class. Therefore, there is a
big information gap between logits and decsions,
and our proposed method aims at narrowing down
the gap by finding a better estimation of the logits.
Figure 1 shows the framework of our proposed
method. The key idea is combining the empirical
and theoretical estimations of the conditional de-
cision distribution P(Y|x), where Yis a random
variable denoting decision, to form an equation
whose solution is the logits. Specially, first we
leverage test-time data augmentation to generate
Naugmented samples for the given sample xand
collect the teacher decisions for them. Then, we
build an empirical estimation ˜P(Y|x)forP(Y|x)
based on the decisions. Next, we derive a theo-
retical estimation Q(Y|x;ˆz)parameterized by the
true logits ˆzforP(Y|x)and form the following
equation
˜P(Y|x)−Q(Y|x;ˆz) = 0 . (3)
Finally, by solving for ˆz, we get the estimatedlogits and the student model can be trained by con-
ventional KD (Eq. 2).
Compared with the existing decision-based KD
method (Wang, 2021), our method leverages data
augmentation instead of binary search and opti-
mization on the input samples. Therefore, it is
applicable to discrete inputs which can hardly be
searched or optimized.
4.2 Empirical Estimation of the Conditional
Decision Distribution
In this secion, we will introduce how to get ˜P(Y|x)
in Eq. 3. Given a sample xand a teacher model
Mparameterized by θ, we first generate Naug-
mented samples X={˜x= F( x, i)}with a
test-time data augmentation function F(·,·). Then
the teacher decisions D={d=M(˜x)}are
collected. Finally, P(Y|x)is approximated as
˜P(Y|x)≈1
N/summationdisplay1, (4)
where 1∈ {0,1}is aL-dimensional one-hot
vector whose d-th element is 1, and Lis the num-
ber of categories.
F(·,·)plays a crucial role in the process and
an ideal F(·,·)should satisfy three requirements.
First, it should conserve true labels (Wang et al.,
2019). Second, it should have a low computational
cost, since it will be computed repetitively. Third,
the degree of noise introduced by F(·,·)should be
quantizable and controllable, crucial for the follow-
ing steps. Thus, following Wei and Zou (2019),
we define F(·,·)as an operation randomly sam-13236Algorithm 1: Teacher Logits Estimation
Data: Input text x.
Result: Teacher logits estimation ˆz
Require: Teacher model M, data augmenta-
tion transformation function F(·,·), aug-
mented data number N, maximal iteration
number m, error bound ϵ, hyper-parameter
σ, label number L.
// Empirical estimation of the
conditional decision distribution{˜x}← {F(x, n)}{d}← {M(˜x)}˜P(Y|x)←/summationtext 1
// Solving equation to obtain ˆzk←0ˆz←0//ˆz= [ˆz],1≤i≤LInitialize R∈Rwhose diago-
nal elements are σand the others are 2σrepeatp←0//p= [p],1≤i≤L fori= 1, i≤Ldo µ←[ˆz−ˆz, . . . , ˆz−ˆz,ˆz−
ˆz, . . . , ˆz−ˆz] B←CholeskyDecompose( R) p←RecursiveIntegration( µ,B) i←i+ 1 end for k←k+ 1 ˆz←˜P(Y|x)−p+ˆzuntil|p−˜P(Y|x)| ≤ϵork=mreturn ˆz
pled from synonym replacement, random insertion,
random swap, and random deletion operations.
4.3 Theoretical Estimation of the Conditional
Decision Distribution
In this section, we will introduce how to get the the-
oretical estimation Q(Y|x;ˆz)parameterized by ˆz
in Eq. 3. The outline of the derivation is that we as-
sume the logits are sampled from an L-dimensional
distribution P(Z|x), where Z= [Z]is an L-
dimensional random variable denoting logits. Then
Q(Y=i|x;ˆz)is equal to the probability that the
i-th dimension of Ztakes the largest value, which
can be calculated mathematically from P(Z|x).
Following the above outline, we have
Q(Y=i|x;ˆz) =P/parenleftbigg
Z= maxZ/vextendsingle/vextendsingle/vextendsinglex/parenrightbigg
.(5)
To derive the above probability, we reformulate itin terms of orthant probability. First, we introduce
anL−1dimensional auxiliary random variable
U= [U], which is defined as
U=/braceleftigg
Z−Z (j < i )
Z−Z(j≥i),1≤j≤L−1.
(6)
Note that the i-th dimension of Zis eliminated due
toZ−Z= 0. Then Eq. 5 can be rewritten as a
non-centred orthant probability distribution
Q(Y=i|x;ˆz) =P(U≥0,1≤j≤L−1).
(7)
To simplify the calculation of the probability
in Eq. 7, we assume Zfollows a multivariate
Gaussian distribution with mean ˆzand covariance
matrix Σ, i.e., Z∼ N (ˆz,Σ). Then we have
U∼ N(µ,R), where
µ=/braceleftigg
ˆz−ˆz (j < i )
ˆz−ˆz(j≥i),1≤j≤L−1.(8)
And Eq. 7 can be calculated by the following mul-
tiple integrations
/integraldisplay···/integraldisplayϕ(U;µ,R)dU. . . dU,
(9)
where ϕ(U;µ,R)is the probability density
function.
We leverage the recursive algorithm proposed
by Miwa et al. (2003) to solve the above integra-
tions. Taking L= 4as an example, the major steps
of the algorithm are as follows. First, we decom-
pose the covariance matrix RasR=BBvia
Cholesky decomposition, where Bis a lower trian-
gular matrix. Then we have U=BM +µ, where
M∼ N(0,I)andIis an identity matrix
of dimension L−1. Next, Q(Y=i|x;ˆz)can be
further decomposed as
Q(Y=i|x;ˆz) =P(U≥0,1≤j≤3)
=P(bM+µ≥0,
bM+bM+µ≥0,
bM+bM+bM+µ≥0),
(10)
where bdenotes the elements in the Bmatrix, M
denotes the j-th elements of the random variable
M, andµdenotes j-th the elements of µ. Finally,
the required probability is given when b>0
Q(Y=i|x;ˆz) =/integraldisplayf(t)ϕ(t)dt, (11)13237where ϕ(t)is the standard normal probability den-
sity function, and fis defined as
f(s) =/integraldisplayf(s, t)ϕ(t)dt, (12)
f(s, s) =/integraldisplayϕ(t)dt. (13)
Algorithm 1 summarizes the entire procedure of
our proposed framework, where line 12 refers to
the integration steps in Eq. 11 to 13. We provide
the proofs of the integration steps in Appendix A.4.
In practice, we assume Σis a diagonal matrix and
Σ=σto simplify the calculation, where σis a
hyper-parameter of our algorithm.
5 Experiments
5.1 Experimental Settings
Datasets and Evaluation Metrics. We evalu-
ate our method on machine reading comprehen-
sion (MRC) and natural language understanding
(NLU) datasets. For MRC, two widely used
multiple-choice datasets RACE (Lai et al., 2017)
and DREAM (Sun et al., 2019a) are used. For
NLU, we select sentiment analysis dataset SST-
2 (Socher et al., 2013), linguistic acceptability
dataset CoLA (Warstadt et al., 2019), paraphras-
ing dataset MRPC (Dolan and Brockett, 2005) and
QQP (Chen et al., 2017), and natural language infer-
ence (NLI) datasets RTE (Bentivogli et al., 2009),
MNLI (Williams et al., 2018), and QNLI (Ra-
jpurkar et al., 2016) as representative datasets. Fol-
lowing previous works (Lai et al., 2017; Sun et al.,
2019a; Wang et al., 2018), we report Matthews
correlation coefficient for CoLA, F1 and accuracy
for MRPC and QQP, and accuracy for all the other
datasets. For each experiment, the model is evalu-
ated on the validation set once an epoch, and the
checkpoint achieving the best validation results
is evaluated on the test set. The results averaged
over five random seeds are reported for the MRC
datasets. Due to the submission quota, we only
report results for one trial for the NLU tasks.
Baselines. We compare our method with the fol-
lowing four baselines:
•Hard : We regard the teacher decisions as the
ground truth labels and train the student model
solely with the cross entropy loss.
•Noisy Logits (Wang, 2021): The student
model is trained via the KD objective (Eq. 2)with the teacher logits replaced with randomly
sampled soft labels.
•Smooth : We apply label smoothing (Szegedy
et al., 2016) with a smoothing factor 0.1on
the teacher decision of the original sample,
and use the smoothed decision as teacher pre-
diction probability. This a straightforward ap-
proach to generate soft labels from teacher
decisions.
To better investigate the upper bound of our method,
we also leverage the following three baselines from
Wang (2021):
•Student CE (Wang, 2021): The student model
is trained using only the cross entropy loss
calculated from the ground-truth labels.
•Standard KD : The student model is optimized
with the standard KD objective (Eq. 2). Note
that the teacher model is used as a white-box
model in this baseline.
•Surrogate : Following Wang (2021), we train
the student model via KD with a surrogate
teacher, simulating training a lightweight,
white-box teacher model for knowledge distil-
lation.
Implementation Details. We implement the
teacher model as the finetuned 12-layer BERT
model (BERT ) or 24-layer BERT model
(BERT ) for each task. The student model is
a 4-layer or 6-layer BERT-style model. Following
previous KD works (Sun et al., 2019b; Li et al.,
2021b), we initialize the student model from the
raw12-layer BERT model. We adopt EDA (Wei
and Zou, 2019) as the tool for test-time data aug-
mentation.
5.2 Results on MRC Datasets
Experimental results on the MRC datasets RACE
and DREAM are shown in Table 1 and Table 2,
respectively. In each table, we report three sets of
results with different teacher and student architec-
tures. For example, the string “12L →4L” in the
tables means we leverage the finetuned 12-layer
BERT model (BERT ) as the teacher, and the
4-layer BERT-style model as the student. Note
thatTeacher andStandard KD serve as the upper
bounds of our method. Therefore, we do not di-
rectly compare our method with them. From these
results, we can observe that:13238
(1) Our proposed method outperforms baselines
consistently and significantly. The performance
gap between our method and the second best base-
lines except Teacher andStandard KD are from
0.96 to 1.39 on the RACE datasets and from 0.51
to 0.72 on the DREAM dataset, indicating that nar-
rowing down the information gap between logits
and decisions are effective for decision-based KD.
(2) Surprisingly, our proposed method achieves
comparable results with Standard KD under a few
settings. Standard KD treats the teacher as a white-
box model and is an intuitive upper bound of our
method. However, the smallest performance gap
between our method and Standard KD is 0.06
(12L→4L on RACE-High) on the RACE datasets
and is 0.25 (12L →4L) on the DREAM dataset.
Moreover, among all the twelve pairs of results,
there is a third of them with a gap of less than 0.50.
These results further justify the effectiveness of our
proposed method. And we argue that this is mainly
due to our better estimation of the logits.
(3) None of the baselines besides Standard KD
can consistently achieve better results than trainingthe student model without KD ( Student CE ), indi-
cating that decision-based KD is a challenging task
and the lost information from decisions compared
with logits is essential. Hard performs slightly bet-
ter than Student CE on the RACE datasets but sig-
nificantly worse than Student CE on the DREAM
dataset. We conjecture that this is because the
teacher models have significantly better results on
the RACE datasets than on the DREAM dataset,
i.e.,Hard can only work well with strong teacher
models, whose decisions may be less noisy and
the information gap between decisions and logits
is smaller. Our method can be viewed as a special
form of logits smoothing. However, both Noisy
Logits andSmooth only achieves comparable or
worse results than Student CE , indicating straight-
forward logits smoothing is not effective.
(4) Our method benefits from both better teacher
models and larger student models. When the
teacher grows larger (12L →4L v.s. 24L →4L),
our method achieves a 1.10 performance gain on
the DREAM dataset. Meanwhile, when the stu-
dent models grow from 4 to 6 layers (12L →4L v.s.
12L→6L), the performance gains on all datasets
are remarkably larger. The same trend is also ob-
served for other baselines, suggesting that improv-
ing the capacity of the student model is a simple
yet effective way to improve the performance of
decision-based KD.
5.3 Results on NLU Datasets
Table 3 shows the results on the NLU datasets.
First, our proposed method achieves the best results
among all decision-based baselines, justifying that
our method is generalizable to a large range of NLU13239
tasks. Although our method does not outperform
Surrogate on the RTE and MNLI-mm datasets, the
gap is only 0.2. Second, the performance gap be-
tween ours and Standard KD is also small, provid-
ing extra evidence that our method estimates the
teacher logits well. Third, all the baselines exclud-
ingTeacher andStandard KD have comparable
performance, suggesting that decision-based KD
is also challenging for NLU tasks. Above all, in
conjunction with the results on the MRC datasets,
we can conclude that our method is effective for
diverse tasks and model architectures.
5.4 Analysis on Logits Estimation
We have conjectured that the good performance of
our method comes from better logits estimation.
To justify this assumption, we conduct a quantita-
tive analysis in this section. We compute the mean
squared errors (MSEs) between the soft labels gen-
erated from each method after softmax and teacher
predictions on the training set of RACE-High. As
shown in Figure 2, the soft labels generated by our
method are the closest to the teacher predictions
among all methods. However, it should be noted
that the probabilities (or logits) of the teacher are
not perfect, as it does not achieve perfect final per-
formance on the dataset. Therefore, the MSEs have
a positive correlation with the final performance
but are not oracle indicators.
5.5 Ablation Study
This section consists of a series of experiments
aimed at validating the contributions of different
components in our method. First, we compare our
method with its two variants in Figure 3: (1) w/o
Empirical Estimations. In this variant, we replace
the˜P(Y|x)in Eq. 4 with the teacher decision on
original data to skip the empirical estimation step.
(2)w/o Theoretical Estimation . We replace the
Q(Y|x;ˆz)term in Eq. 3 with softmax( z)to skip
the theoretical estimation step. For each dataset,
we also count the percentage of empirical estima-
tions ˜P(Y|x)being one-hot vectors, which means
that teacher decisions are consistent on augmented
inputs. According to the results, we find that the
performance drops for both variants on all datasets,
indicating the necessity of empirical estimation and
theoretical estimation. Interestingly, we also find
a positive correlation between the percentage of
one-hot ˜P(Y|x)and the performance degradation
from w/o Theoretical Estimation variant. This phe-
nomenon highlights the capability of the theoretical
estimation step to estimate teacher logits and nar-
row the information gap between decisions and
logits even without observed decisions.
Second, we further analyze the effect of empiri-
cal estimation by changing the sampling times N
in Eq. 4. As shown in Figure 4, when Nincreases,
the performance of our method first increases and
then stabilizes. Considering a larger Nleads to13240
more queries to the teacher model, Nshould be as
low as possible without compromising the model
performance. Therefore, N= 10 is the optimal
choice according to the results.
Finally, we investigate the contribution of Eq. 3,
which combines the empirical estimation and the
theoretical estimation together. In our framework,
the root ˆzof the equation is found by fixed-point
iteration, and its precision is controlled by the er-
ror bound ϵ. Results in Figure 5 show a negative
correlation between ϵand KD performance. As
ϵincreases from 10to10, the performance
of our method slightly drops. When ϵincreases
to1, which means the logits estimation becomes
extremely inaccurate, the performance drops dra-
matically and is close to the performance of Student
CEmethod.
5.6 Computational Cost Analysis
The additional computational cost of our method
compared to Standard KD consists of two parts.
The first part is test-time data augmentation, which
necessitates multiple queries to the teacher model
for each training sample. In this paper, we set
the default number of augmented samples Nper
training case to 10. The second part is solving
Eq. 3 using the empirical estimation of decision
distribution ˜P(Y|x), which is made negligible by
pre-building a lookup table from ˜P(Y|x)to logits
estimation ˆzbefore KD. In total, the additional
cost of our method mainly comes from 10 queries
made to the teacher model per training sample. By
contrast, the existing soft label generation method
DB3KD (Wang, 2021) requires 1,000 to 20,000
queries to the teacher model per training sample.
6 Conclusion
We introduce a novel decision-based KD method,
which bridges the information gap between teacher
decisions and logits by estimating teacher logits.
In contrast to existing solutions for decision-based
KD, our method is applicable to NLP tasks with
discrete inputs. Extensive experiments over vari-
ous tasks and model architectures demonstrate the
effectiveness of our proposed method.
One future direction for the decision-based KD
is the exploration of other NLP tasks, such as neu-
ral machine translation, text generation, and ques-
tion answering. The other direction is KD from
non-NN models to NN models, which benefits the
training of NN models with additional information
from a wider range of models. Unlike conventional
KD, decision-based KD does not require internal
information from NN models and is promising for
solving this problem.13241Limitations
This study has two main limitations. The first limi-
tation is its reliance on the assumption that teacher
logits on augmented data follow a Gaussian distri-
bution. This assumption is used in the derivation of
teacher logits in Section 4.3. However, in practice,
teacher logits may not strictly follow a Gaussian
distribution. It is challenging to estimate teacher
logits under more realistic assumptions, which re-
quires thorough investigations on the distribution
of teacher logits and more complex computations
for logits estimation.
The second limitation is that our method still
requires access to the training dataset of the down-
stream tasks. In this paper, we focus on KD when
teacher PLMs only return decisions. However, our
method is not capable of KD without publicly avail-
able training data, which is a more challenging sce-
nario for decision-based KD. We believe training a
data generation model (Wang, 2021; Zhang et al.,
2022; Sanyal et al., 2022) might be useful for such
cases.
Ethics Statement
In ethical considerations, our method risks being
used as a means of model stealing. Therefore, de-
fensive techniques against the proposed method
are required. However, it also has significant posi-
tive implications. On the one hand, it can serve as
a powerful tool for research on model extraction
attacks, thereby promoting the advancement of re-
lated studies. On the other hand, it has practical
applications in real-world scenarios. For instance,
a company may prefer to use a smaller model due
to cost considerations, and our method allows for
the easy distillation of smaller models without re-
quiring white-box access to larger models. Addi-
tionally, our method can be used to distill non-NN
models into NN models, reducing the number of
model types that need to be maintained and simpli-
fying operation and maintenance.
Acknowledgement
This work is supported by the National Key R&D
Program of China (2022ZD0160502) and the Na-
tional Natural Science Foundation of China (No.
61925601, 62276152, 62236011). We thank all
anonymous reviewers for their valuable comments
and suggestions on this work. We also thank Shuo
Wang and Xiaoyue Mi for their suggestions on the
writing.References1324213243
A Appendix
A.1 Experiment Details
Dataset Details In this paper, we use seven dif-
ferent datasets, and all of them are in the English
language. We downloaded these datasets from the
Datasets (Lhoest et al., 2021) library of version
2.4.0, and our use is consistent with their intended
use. The other details of the datasets we used are
summarized in Table 4.13244
Model Details We used BERT-like models (De-
vlin et al., 2019) in our experiments, includ-
ing BERT (110M parameters), BERT
(340M parameters), 4-layer BERT-like models
(53M parameters), and 6-layer BERT-like mod-
els (67M parameters). For BERT and
BERT , the raw model checkpoints are ob-
tained from Huggingface Transformers (Wolf et al.,
2020) platform. Following Li et al. (2021b), we
initialize the 4-layer and 6-layer BERT-like models
from the first 4 and 6 layers of the raw BERT
model, respectively.
Other Details We finetune the BERT and
BERT models for 4 epochs. Following Li
et al. (2021a), we train small 4-layer or 6-layer mod-
els for 10 epochs. We use a learning rate of 5×10
for MRC tasks and 2×10for NLU tasks. σin Al-
gorithm 1 is tuned from {0.5,1,2,4},λin Eq. 2 is
tuned from {0.2,0.5,0.7}, and τin Eq. 1 is tuned
from{5,10,20}expect for our method, which per-
forms better in the range of {1,2,4}. The αparam-
eters of all operations in EDA are sampled from a
half-normal distribution, and we adjust the scale
of the distribution to align its expectation with the
default α= 0.1in EDA. For MRC tasks, follow-
ing Sun et al. (2019b), we concatenate the input
passage and the question with a [SEP] token and
append each answer at the end of the question. The
random seeds we used for experiments on MRC
datasets and ablation studies on NLU datasets are
from 1 to 5. The random seed for teacher training
and other NLU experiments is 1. The training of a
4-layer student model on one RTX 3090 Ti GPU
costs approximately 6.5 hours for our method.Methods RACE-High
Student CE 39.79
Noisy Logits 30.19
Surrogate 37.86
Smooth 41.59
Hard 42.92
Ours 44.13
A.2 Experimental Results on Generative
Language Models
Theoretically, our method can be applied to gen-
erative LMs. In this paper, we evaluate the effec-
tiveness of our method on the RACE dataset. We
finetune a 12-layer GPT-2 (Radford et al., 2019)
teacher to predict the class label (A/B/C/D) given
the context, question, and options as prompt. Then
we distill the teacher model to a 4-layer GPT-2
student. For the student model, the output vocab-
ulary at the answer position is restricted to class
label tokens. Table 5 shows the performance of
our method and baseline methods on the dev set
of RACE-High. Our method significantly outper-
forms decision-based baseline methods and the stu-
dent CE method.
Different from classification tasks, our method
will require much more queries to the teacher
model on generation tasks because of the following
reasons:
1.The label space dimension Lin generative
tasks is equal to the vocabulary size, which is
quite large. The computational cost of build-
ing the look-up table will increase.132452.For each position iin a sequence, our method
estimates the logits in the i-th position given
all the tokens before the i-th position. There-
fore, to estimate the logits in the entire se-
quence, we need to sample teacher decisions
in each position. As a result, the compu-
tational cost will multiply by the sequence
length.
Therefore, how to improve the efficiency of ap-
plying our method to generation tasks is an inter-
esting future research direction.
A.3 Detailed Analysis of the Data
Augmentation Techniques
In this paper, we use the EDA augmentation
tool (Wei and Zou, 2019) for each sample, includ-
ing its αparameter and the following four default
augmentation techniques. Given a input sentence,
aαparameter, and the sentence length l, the
four techniques can be describe as following:
1.Synonym Replacement : First, select αl
words that are not stop words randomly. Sec-
ond, replace each word with a random Word-
Net (Miller, 1995) synonym of itself.
2.Random Insertion : First, select a word in
the sentence that is not a stop word randomly.
Second, find a random synonym of the se-
lected word. Third, insert the synonym into a
random position in the sentence. Finally, do
the above steps αltimes.
3.Random Swap : First, choose two words in
the sentence randomly. Second, swap the po-
sitions of the chosen words. Finally, do the
above steps αltimes.
4.Random Deletion : Remove each word in the
sentence with probability αrandomly.
In Table 6, we provide further ablation studies
on each technique of EDA. We also include the
performance of Surrogate method which is the best
decision-based baseline. According to the results,
our method is robust to different data augmentation
techniques.
A.4 Proofs
In this section, we provide the detailed proofs of
Eq. 11 to 13.Methods RACE-High
Ours 51.75
w/o synonym replacement 51.69
w/o random insertion 51.44
w/o random swap 51.75
w/o random deletion 51.60
Surrogate (best baseline) 50.33
In Section 4.3, we assume Σis a diagonal matrix
andΣ=σ. Therefore, the diagonal elements of
Bare positive, and Eq. 10 can be rewritten as:
Q(Y=i|x; ˆz) =P(M≥−µ
b,
M≥−µ−bM
b,
M≥−µ−bM−bM
b).
(14)
Then Q(Y=i|x; ˆz)can be calculated recur-
sively. Eq. 11 is an integration on M≥,
while Eq. 12 and Eq. 13 integrate on M≥andM≥, respec-
tively.13246ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitation Section
/squareA2. Did you discuss any potential risks of your work?
Ethics Section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract Section and Introduction Section
/squareA4. Have you used AI writing assistants when working on this paper?
Grammarly, to check grammar of the whole paper
B/squareDid you use or create scientiﬁc artifacts?
Section 5
/squareB1. Did you cite the creators of artifacts you used?
Section 5
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
In Appendix
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
In Appendix
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
In Appendix
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
In Appendix
C/squareDid you run computational experiments?
In Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
In Appendix13247/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
The experimental setup and hyperparameter search methods are in Section 5 and Appendix. We do
not include the best-found hyperparameter values
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
In section 5 and appendix
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
In section 5 and appendix
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.13248