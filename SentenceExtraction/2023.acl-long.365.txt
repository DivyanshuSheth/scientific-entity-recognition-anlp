
Shicheng Xu, Liang Pang, Huawei Shen, Xueqi ChengData Intelligence System Research Center,
Institute of Computing Technology, CASUniversity of Chinese Academy of Sciences
xschit@163.com {pangliang,shenhuawei,cxq}@ict.ac.cn
Abstract
Dense retrieval has shown promise in the first-
stage retrieval process when trained on in-
domain labeled datasets. However, previous
studies have found that dense retrieval is hard
to generalize to unseen domains due to its
weak modeling of domain-invariant and inter-
pretable feature (i.e., matching signal between
two texts, which is the essence of informa-
tion retrieval). In this paper, we propose a
novel method to improve the generalization of
dense retrieval via capturing matching signal
called BERM. Fully fine-grained expression
and query-oriented saliency are two properties
of the matching signal. Thus, in BERM, a sin-
gle passage is segmented into multiple units
and two unit-level requirements are proposed
for representation as the constraint in training
to obtain the effective matching signal. One is
semantic unit balance and the other is essential
matching unit extractability . Unit-level view
and balanced semantics make representation
express the text in a fine-grained manner. Es-
sential matching unit extractability makes pas-
sage representation sensitive to the given query
to extract the pure matching information from
the passage containing complex context. Ex-
periments on BEIR show that our method can
be effectively combined with different dense re-
trieval training methods (vanilla, hard negatives
mining and knowledge distillation) to improve
its generalization ability without any additional
inference overhead and target domain data.
1 Introduction
Dense retrieval encodes the texts to dense em-
beddings and efficiently gets the target texts via
approximate nearest neighbor search (Johnson
et al., 2021). Compared with the traditional
word-to-word exact matching methods such as
BM25 (Robertson et al., 1995), dense retrieval can
capture the relevance at the semantic level of twoFigure 1: Idea of our method. R1: semantic unit balance.
R2: essential matching unit extractability.
texts. Because of the excellent performance in effi-
ciency and effectiveness, dense retrieval has been
widely used in first-stage retrieval that efficiently
recalls candidate documents from the large cor-
pus (Karpukhin et al., 2020; Xiong et al., 2021a).
However, recent studies show that the excellent
performance of dense retrieval relies on the train-
ing on large in-domain datasets. When the trained
dense retrieval models are applied to the domains
that are inconsistent with the training datasets (i.e.,
zero-shot setting), the performance of the models
drops seriously (Ren et al., 2022; Thakur et al.,
2021). The poor generalization limits the appli-
cation scenarios of dense retrieval because it is
common that not enough training samples can be
obtained in some domains such as medicine, biol-
ogy and law that have restrictions on data privacy
or require professional knowledge to annotate.
In this work, we point out that according to out-
of-domain generalization learning theory (Ye et al.,
2021), making the model capture domain-invariant
feature (i.e., essence of tasks) is effective in improv-
ing generalization ability. As for dense retrieval,
matching signal between query and passage is the
important domain-invariant feature and reflects the
essence of information retrieval (IR). For exam-
ple, MoDIR (Xin et al., 2022) shows that repre-
sentation from the interaction-based cross-encoder6620(more fine-grained description for matching) is
much more domain-invariant than it from dense
retrieval. Match-Prompt (Xu et al., 2022a), NIR-
Prompt (Xu et al., 2022b) and MatchPyramid (Pang
et al., 2016) point out the positive significance of
matching signals for various IR tasks. The chal-
lenge of making dense retrieval model learn to cap-
ture matching signal is that in many IR tasks such
as open-domain question answering (Chen et al.,
2017) and document retrieval (Mitra et al., 2017),
the content that matches the query is usually only
a unit of the text. The description of matching
signal needs to distinguish the matching and not
matching information in the text and estimate the
overall relevance. This requires the retrieval model
to be able to evenly express each unit in the text
and dynamically extract matching units through the
interaction of the two text representations. How-
ever, the requirement on efficiency in first-stage
retrieval makes dense retrieval only estimate rele-
vance via vector similarity such as dot product and
cosine. Previous training methods based on this ar-
chitecture lack the above capability because of the
coarse-grained training objective and interaction.
In this paper, we propose a novel method called
BERM to capture the matching signal between
query and passage, which is the domain-invariant
feature, to improve the generalization ability of
dense retrieval during the training on the single
source domain without using the target domain data
and other additional modules. First, we introduce
a novel concept in dense retrieval, the matching
representation. Matching representation is deter-
mined by the text representations (output of text
encoder) of query and passage, which can reflect
the matching information of query and passage.
We propose that in the training of dense retrieval
models, in addition to using contrastive loss (Xiong
et al., 2021b) to optimize the text representation,
the information of the matching representation can
be used as a constraint to assist the optimization.
Based on this, we divide the single passage into
multiple units (each sentence is a unit) and propose
two requirements on the generalizable dense re-
trieval models as the constraint in training (shown
in Figure 1). One is semantic unit balance of text
representation (R1). The other is essential match-
ing unit extractability of matching representation
(R2). These two requirements can be integrated
into different dense retrieval training methods and
address the challenge mentioned above. R1meansthe semantics of units in a passage are implicitly
aggregated to its text representation and the text
representation should evenly and comprehensively
express the semantics of each unit. R2means that
the combination of text representations of query
and passage (i.e., matching representation) should
extract the information of the matching (i.e, the text
chunk in the passage that matches the query and we
call it essential matching unit ) while reducing the
overfitting of domain biases. This reflects the abil-
ity of the dense retrieval model to determine and
score the information that really matches the query
in a passage containing complex context, which
is the essence of the dense retrieval and domain-
invariant. R1andR2achieve that on the premise
that the text representation expresses each unit in a
balanced manner, to make essential matching units
for different queries be extracted, the semantics
of units tend to be orthogonal to each other. In
this way, in dot product between representations
of query and passage, the semantics of essential
matching unit are preserved, while the other units
are masked, which is suitable for matching.
Experiments on the standard zero-shot retrieval
benchmark (BEIR) show that our method can be
effectively combined with different dense retrieval
training methods (vanilla, hard negatives mining,
and knowledge distillation) to improve the gener-
alization ability without any additional modules,
inference overhead, and target domain data. Even
in domain adaptation, our method is also effective
and performs better than baselines. Code is re-
leased at https://github.com/xsc1234/BERM .
2 Related Work
Dense retrieval estimates the relevance via repre-
sentations of two texts. DPR (Karpukhin et al.,
2020) combines dense retrieval with pre-trained
models for open-domain question answering (Chen
et al., 2017). Besides, some methods focus on ob-
taining more valuable negatives (Qu et al., 2021;
Xiong et al., 2021a; Zhan et al., 2021). Some meth-
ods use a more powerful reranker for knowledge
distillation (Hofst√§tter et al., 2021; Lin et al., 2021).
Recently, the generalization of dense retrieval has
received attention. (Ren et al., 2022) performs the
examination of the generalization of dense retrieval.
BEIR (Thakur et al., 2021) is proposed as the
benchmark to evaluate the zero-shot ability of infor-
mation retrieval models. MoDIR (Xin et al., 2022)
uses the data from source and target domains for ad-6621versarial training to perform unsupervised domain
adaptation. GenQ (Ma et al., 2021) and GPL (Wang
et al., 2022) generate queries and pseudo labels
for domain adaptation. Contriever (Izacard et al.,
2021) uses contrastive pre-training on large cor-
pus (Wikipedia and CC-Net (Wenzek et al., 2020)).
COCO-DR (Yu et al., 2022) performs unsupervised
pre-training on target domain and introduces distri-
butional robust optimization. GTR (Ni et al., 2021)
scales up the model size to improve the general-
ization. (Huebscher et al., 2022; Formal et al.,
2022) introduce sparse retrieval to achieve better
generalization.
Improvement of generalization of dense retrieval
in previous studies comes from the adaptation
of the target domain, knowledge from large pre-
training corpus, and assistance of sparse retrieval
but not dense retrieval itself. They need to obtain
the target domain data in the training or increase the
complexity of the system. In this paper, we intro-
duce a novel method to improve the generalization
of dense retrieval without target domain data and
additional modules via learning the generalizable
representation for matching.
One thing must be emphasized that the meth-
ods of multi-view dense retrieval (Zhang et al.,
2022; Hong et al., 2022) also divide a passage
into multiple units, but our method is essentially
a completely different method. Multi-view dense
retrieval uses multiple representations to fully ex-
press a passage from multiple views, which focuses
on in-domain retrieval. Our method uses multiple
units to make the model learn to extract essential
matching unit from the passage containing complex
context, which is domain-invariant for generaliza-
tion. In our method, multiple units are only used
as the constraint for optimization in training and
only a single representation is used in inference.
Learning-based sparse retrieval such as COIL (Gao
et al., 2021) and SPLADE (Formal et al., 2021) also
aim to express fine-grained token-level semantics
but they need multiple vectors to represent tokens
in passage (COIL) or sparse-vector of vocabulary
size (SPLADE) and calculates the score by token-
to-token matching, which is not suitable for dense
retrieval that uses single dense vector to perform
representation and dot product.
3 Motivation
Dense retrieval is hard to generalize to unseen
domains due to its weak modeling of domain-
invariant feature (i.e., matching signal between
two texts, which is the essence of information re-
trieval). Fully fine-grained expression ( P1) and
query-oriented saliency ( P2) are two properties of
the matching signal. These two require the passage
representation to be able to evenly express each
unit in the text, and dynamically extract match-
ing units according to the interaction with differ-
ent queries. For example, BM25 uses one-hot to
evenly express each word of the text, only scores
matching words, and ignores not matching words
through word-to-word exact matching of the two
texts. Cross-encoder uses word embedding to repre-
sent the semantics of each token and uses attention
to describe the token-to-token semantic matching
between texts in a fine-grained manner.
In this paper, based on the above two properties,
for the training of dense retrieval, we segment a
single passage into multiple units and propose two
requirements as the constraint in training so that
dense retrieval can capture the stronger matching
signal and produces a suitable representation for
matching. One is semantic unit balance of text rep-
resentation (R1), and the other is essential match-
ing unit extractability of matching representation
(R2). Under R1, text representation evenly aggre-
gates semantics of the units in the passage to com-
prehensively express the passage in a fine-grained
manner. Besides, R1is the premise of R2. It is
because that matching representation is composed
of text representations from passage and query. Un-
balanced semantic expression of different units in
text representation will affect the identification of
essential matching unit in matching representation
because it leads to different preferences for differ-
ent units. Under R2, essential matching unit for
the query can be extracted from the passage and
reflected in matching representation. Unlike using
one-hot or word embedding to explicitly express
the semantics of each unit and extract matching6622
information through token-to-token interaction, as
shown in Figure 2, R1makes the model implic-
itly aggregate the semantics of each unit into the
text representation to satisfy P1, and R2makes the
semantics of units tend to be orthogonal to each
other (shown in Table 6). In dot product between
representations of query and passage, semantics
of essential matching unit are preserved, while the
other units are masked, which can satisfy P2. Our
method unlocks the ability of dense retrieval to cap-
ture matching signal without additional interaction.
4 Our Method
This section introduces the implementation of our
method (Figure 3). Our method optimizes the rela-
tionship between the representations and the units
in the passage. Therefore, before training, we per-
form unit segmentation and annotate the essential
matching unit for the datasets. Then, we design loss
functions according to the requirements of R1and
R2and combine these functions with task loss of
dense retrieval (contrastive loss) for joint training.
4.1 Unit Segmentation and Annotation
Given each positive query-passage pair (q, p)in
training data, we segment positive passage into mul-
tiple units Uas shown in Figure 3 (a) (We use the
sentence as the segmentation granularity to ensure
that each unit has complete semantic information.):
p‚àí‚Üí U={u, u, ..., u}. (1)ForUandq, BM25 is used to compute the word-to-
word matching score Sbetween qandu‚ààU:
S={bm25(q, u), ..., bm 25(q, u)}.
For the datasets for question-answering, a trained
reader model is additionally introduced to compute
the semantic matching score S between qand
u‚ààU. Specifically, reader model computes the
probability distribution A={a, a, .., a}of the
starting positions of the answer in p.aindicates
the probability that the i-thtoken in pis the
starting of the answer to q. For each u‚ààU, the
semantic matching score from the reader model is:
r=max(A[s:d]), (2)
where [s:d]are the indexes of tokens in u.
The hybrid matching score hbetween uandqis:
h=bm25(q, u) +Œ¥r,
where Œ¥is a hyperparameter. We set Œ¥as0.1to
give BM25 a higher weight than the reader. It
is because the word-to-word exact matching of
BM25 is more domain-invariant and conducive
to generalization than the semantic matching of
reader (Thakur et al., 2021). Then we get matching
score list H={h, h, ..., h}forU. The essen-
tial matching unit is the unit corresponding to the
maximum value in H. For the pair (q, p),yin
label list Y={y, ..., y}for essential matching
unit is that if iis the index corresponding to the
maximum value in H,y= 1, otherwise, y= 0.66234.2 Training for Generalization
Based on the analysis of properties of matching
signal in Section 3, we propose two requirements
as the constraints in the training of dense retrieval
to get a generalizable representation for matching
(shown in Figure 3 (b)). These two requirements
enable dense retrieval to extract essential matching
information under the premise of balanced expres-
sion of each unit, so as to learn domain-invariant
feature (i.e., matching signal) for generalization.
Implementation of R1. The first requirement is
semantic unit balance of text representation , which
means that the text representation of the passage
encoder can comprehensively express the seman-
tics of each unit in a balanced manner. Given the
passage p, the text encoder g(¬∑;Œ∏), output hid-
den states Z=g(p;Œ∏). Text representation t
ofpis the embedding of [CLS] token of Z. The
embeddings Eof units in pcan be obtained
fromZas the segmentation in Equ.(1):
E={e,e, ...,e}, (3)
where eis the embedding of the corresponding
unit(u)and it is the average pooling of the embed-
dings of tokens ( Z[s:d]) in the unit, where
[s:d]are the indexes of tokens in u. Under
the constraint of R1, the relationship between t
andEis described by the loss function as:
L =D[b||sim(t,E)], (4)
where D[¬∑||¬∑]is KL-divergence loss, b=
[, ...,]is a uniform distribution with equal val-
ues and sim(t,E) ={dot(t,e)|e‚ààE}is
a distribution to represent the semantic similarity
between tande‚ààE,dot(¬∑,¬∑)is dot product.
Implementation of R2. The second requirement
isessential matching unit extractability of matching
representation , which means that under the premise
ofR1, matching representation can saliently rep-
resent the unit where the essential matching block
is located in. The motivation for this design is dis-
cussed in Section 1 and 3. Given the positive query-
passage pair (q, p), text encoder g(¬∑;Œ∏), and the
text representations for q(t) and p(t). Match-
ing representation m‚ààR(vis the dimension of
representation) for qandpcan be obtained by
the combination of t‚ààRandt‚ààRas:
m=GELU (t‚äôt),
where ‚äôis the element-wise multiplication opera-
tor, and GELU (¬∑)is activation function (Hendrycksand Gimpel, 2016) to introduce stochastic regular-
ization. Under the premise of R1,tcan express
the semantics of the units in pin a balanced
manner. In addition, the semantic representation of
essential matching unit is more similar to tthan
other units because it really matches the query q.
Based on this, the model can be trained to achieve
the goal that element-wise multiplication between
tandtcan amplify similar patterns (i.e, seman-
tic representation of essential matching unit) and
mask the signals of other context units. This de-
sign can be supported by convolutional neural net-
works (LeCun et al., 1998) whose convolution op-
eration can amplify similar patterns in tensors (Gir-
shick et al., 2014). For the p, different qam-
plifies different matching units, which makes m
reflect the semantics of the corresponding essential
matching unit. Besides, mis obtained by element-
wise multiplication between tandt, which is
an important part of estimating the relevance of
two texts because dot(t,t) =sum(t‚äôt).
Thus, the optimization of mcan enable the model
to obtain the ability to extract essential matching
unit according to different queries when estimat-
ing relevance. In training, our method utilizes the
cross-entropy loss function to optimize the seman-
tic distance between mand each unit to identify
the corresponding essential matching units. Given
query-passage pair ( q,p), the embeddings Eof
the units in pas described in Equ. (3), and the
label Yfor essential matching unit of ( q,p) as
described in Sec. 4.1. Loss function for R2is:
L =‚àí/summationdisplayylog(dot(m,e)),(5)
where e‚ààE,y‚ààY.mis only used as the con-
straint in training but has important implications for
inference. It is because that mis the combination
of text representations ( tandt). The optimiza-
tion for mis training the text encoder to output the
text representation that is suitable for matching to
improve the generalization ability.
Effect of R1 and R2. Table 6 indicates that
compared with previous dense retrieval methods,
our method makes the semantics of units in text
representation tend to be orthogonal to each other.
In dot product between two texts, semantics of
essential matching unit are preserved, while the
other units are masked to capture matching signal.
Total Loss. In addition to L andL ,
contrastive loss is used to train the dense retrieval6624
model (Karpukhin et al., 2020) as:
L=‚àíexp(dot(t,t))
exp(dot(t,t)) + exp( dot(t,t)).
So the total loss for training in our method is:
L=L+Œ±L +Œ≤L ,
where Œ±andŒ≤are the hyperparameters.
5 Experiments
This section introduces the experimental setups and
analyzes the results.
5.1 Experimental Setups
Datasets. We use MS-MARCO (Nguyen et al.,
2016) as the training data (source domain) and
choose the 14 publicly available datasets from
BEIR, a heterogeneous benchmark to evaluate
the generalization ability of retrieval models. In
addition, we also introduce OAG-QA (Tam et al.,
2022) to evaluate the topic generalization ability.
Details of datasets are in Appendix A.
Baselines. Our method (BERM) aims to im-
prove the generalization of dense retrieval without
any additional modules and target domain data, and
it can be combined with different dense retrieval
training methods. We select three mainstream
dense retrieval training methods including vanilla,
hard negatives mining, and knowledge distillation
as the baselines. We follow DPR (Karpukhin et al.,2020) to perform vanilla, follow ANCE (Xiong
et al., 2021a) to perform hard negatives mining and
use a trained cross-encoder as the teacher model to
perform knowledge distillation. We compare the
change in generalization after combining BERM
with these three methods to show the effectiveness
of our method. Besides, as previous methods need
to obtain target domain data for domain adapta-
tion such as MoDIR (Xin et al., 2022), GenQ (Ma
et al., 2021), GPL (Wang et al., 2022) and COCO-
DR (Yu et al., 2022), we also compare our method
with these methods in domain adaptation setting.
Details of baselines are in Appendix B.
Implementation Details. To maintain a fair
comparison, we follow (Xiong et al., 2021a) to
keep all common hyperparameters (learning rate
and batch size, etc.) the same as the three dense
retrieval training methods in the baselines. The
model is initialized by Roberta125M. For the
hyperarameters in BERM, Œ¥is 0.1, Œ±is 0.1 and Œ≤is
1.0. In domain adaptation, we combine BERM with
continuous contrastive pretraining (Yu et al., 2022)
to perform unsupervised pre-training on BEIR and
use BERM to fine-tune the model on MS-MARCO.
We train the model with Pytorch (Paszke et al.,
2019) and Hugging Face (Wolf et al., 2020) on 2
Tesla V100 32GB GPUs for about 72 hours.
5.2 Retrieval Performance
Main Results. Table 1 shows the main results on
BEIR of different dense retrieval training methods.
The results indicate that our method (BERM) can6625
be combined with three mainstream dense retrieval
training methods (vanilla, knowledge distillation,
and hard negatives) to improve the generalization
ability without any additional modules and target
domain data. For a fair comparison, we combine
BERM with the baselines and ensure that their com-
mon hyperparameters are consistent. We compute
the Jaccard similarity (Ioffe, 2010) between each
dataset and MS-MARCO, which can reflect the do-
main shift between the source and target domain.
Table 1 shows that our method is more effective for
the datasets with lower Jaccard similarity between
MS-MARCO (i.e., domain shift is more signifi-
cant). This result reflects the ability of our method
to capture domain-invariant feature. DPR+BERM
and KD+BERM are better than KD, which shows
that BERM more effectively enables dense retrieval
to learn to capture matching signal than knowledge
distillation from cross-encoder.
Topic Generalization. Table 2 shows the gen-
eralization performance of DPR and DPR+BERM
on different topics of QAG-QA. Topic generaliza-
tion is important for out-of-domain generalization,
which reflects the availability of dense retrieval
model for topics with different word distributions.
The results show that BERM can significantly im-
prove cross-topic generalization of dense retrieval.
Domain Adaptation. Table 3 shows that BERMachieves the best performance in domain adaptation
compared with previous baselines. Specifically,
BERM achieves the best average out-of-domain
adaptation and in-domain performance. Besides,
it gets the best dense retrieval results on seven
datasets of BEIR, which is the most of all methods.
Our method not only learns the word distribution
of the target domain, but also learns the representa-
tion suitable for matching for the documents in the
target corpus during domain adaptation.
5.3 Ablation Study
Influence of Loss Functions. Table 4 shows
the ablation study on the loss functions constrained
byR1andR2via average performance on BEIR.
The results indicate that without L ,L
can not improve the generalization, which supports
our intuition in Section 3 that only based on the
balanced semantic expression of each unit in the
text representation, the matching representation is
meaningful for extracting the essential semantic
unit. This experiment shows that the generalization
can be improved significantly when the model is
constrained by both R1andR2.
Influence of Hyperparameters. Figure 4 shows
the average nDCG@10 performance on BEIR with
different Œ±andŒ≤that are used to tune the weights
of different loss functions in training. When Œ±is
0.1andŒ≤is1.0, our method can achieve the best
performance. When Œ±andŒ≤are too big, they will
interfere with the optimization of the contrastive
loss leading to performance degradation.6626
5.4 Model Analysis
Domain-invariant Representation. Figure 5
shows that our method is effective in capturing the
domain-invariant feature of the representation. We
utilize T-SNE to visualize the representations of
source and target (SciFact) domains encoded by
DPR and DPR+BERM respectively. The results
indicate that representations of the two domains en-
coded by DPR are more separable. After combin-
ing our method, the two domains become more dif-
ficult to separate, which indicates that our method
is more invariant to represent the texts in different
domains. More datasets are in Appendix C.
Evaluation of R1 and R2. Table 5 shows the
effectiveness of R1andR2. We randomly sam-
ple 100,000 query-passage pairs from the test set.
For each passage p, we compute semantic similar-
ity between text representation and each unit via
sim(t,E) ={dot(t,e)|e‚ààE}. We com-
pute the variance of sim(t,E)and get the av-
erage of variance on the sampled set, which can
reflect the balance of text representation on ex-
pressing the semantics of units. Table 5 shows
that BERM has a smaller variance (semantic unit
balance of text representation) and is more accu-
rate in identifying the essential matching unit (
essential matching unit extractability of matching
representation) than baselines, which indicates the
effectiveness of R1andR2.
Relationship Between Units. Table 6 shows
that our method makes units in a passage more
dispersed (tend to be orthogonal), which is more
conducive to determining the unit that matches the6627
query and masking the signals of other units. Our
method makes the representation of the passage
more suitable for matching, which is the domain-
invariant feature for generalization.
6 Conclusion
In this paper, we propose an effective method called
BERM to improve the generalization ability of
dense retrieval without target domain data and ad-
ditional modules. The basic idea of BERM is learn-
ing the domain-invariant feature, that is, matching
signal. To achieve it, we introduce a novel concept
of dense retrieval to represent the matching infor-
mation between two texts, the matching represen-
tation. Further, we propose two requirements for
matching and text representations as the constraint
in the training of dense retrieval to enhance the abil-
ity to extract essential matching information from
the passage according to different queries under the
premise of balanced expression of the text. The two
requirements unlock the ability of dense retrieval
to capture matching signal without additional in-
teraction. Experimental results show that BERM
is a flexible method that can be combined with
different dense retrieval training methods without
inference overhead to improve the out-of-domain
generalization ability. In domain adaptation setting,
our method is also effective and performs better
than baselines.
Limitations
In this paper, we propose a novel concept of dense
retrieval, the matching representation. Based on
this, we introduce a novel generalizable dense
retrieval training method via training the bal-
anced and extractable representation for match-
ing (BERM). Despite the strong performance of
our method in improving the generalization ability
of dense retrieval models, more theoretical proof
needs to be researched to gain the deeper under-
standing of generalization improvement. Espe-
cially for matching representation, more theoreticalanalysis and implementation will be discussed in
future work. We believe that the deeper study of
matching representation will promote the develop-
ment of dense retrieval, because it not only alle-
viates the problem that query and passage cannot
interact in depth during training, but also describes
the essence of retrieval task.
Ethics Statement
Our work innovatively proposes the concept of
matching representation in dense retrieval and de-
signs a generalization improvement strategy that
can be flexibly combined with different dense re-
trieval training methods. Our work has important
implications for improving the performance of neu-
ral information retrieval models. We declare that
our work complies with the ACL Ethics Policy.
Acknowledgements
This work was supported by the National Key
R&D Program of China (2022YFB3103700,
2022YFB3103704), the National Natural Science
Foundation of China (NSFC) under Grants No.
62276248, and the Youth Innovation Promotion
Association CAS under Grants No. 2023111.
References662866296630A Datasets
In our experiment, source domain datsaet used as
training data is MS-MARCO and target domain
datasets used as testing data are collected from
BEIR (Thakur et al., 2021), which is a a hetero-
geneous benchmark to evaluate the generalization
ability of retrieval models. Detials of the datasets
are shown in Table 7. In addition, we also intro-
duce OAG-QA (Tam et al., 2022), which is a fine-
grained question-answering retrieval dataset con-
sisting of different topics. We select datasets of
different topics from 20 disciplines as the testing
data to evaluate the generalization ability to differ-
ent topics with different word distribution. Details
of OAG-QA are shown in Table 8.
B Baselines
We introduce the baselines in the main experiment
and the domain adaptation experiment respectively.
B.1 Baselines for Main Experiment
In the main experiment, our method is com-
bined with different mainstream dense retrieval
training methods to improve its generalization.
We consider three training methods including
vanilla (DPR (Karpukhin et al., 2020)), knowl-
edge distillation (KD) and hard negatives mining
(ANCE (Xiong et al., 2021a)).
‚Ä¢DPR trains the dense retrieval model via
in-batch negative sampling. Different from
(Karpukhin et al., 2020), we train DPR on
MS-MARCO to achieve a fair comparison.
‚Ä¢KDtrains the dense retrieval model under the
guidance of the soft labels provided by the
teacher model. In the experiment, we use a
cross-encoder model trained on MS-MARCO
as the teacher model.
‚Ä¢ANCE trains the dense retrieval model with
hard negatives updated in parallel as described
in (Xiong et al., 2021a).
B.2 Baselines for Domain Adaptation
‚Ä¢MoDIR uses the data from source and target
domains for adversarial training to perform
unsupervised domain adaptation.
‚Ä¢Contriever performs unsupervised pre-
training on Wikipedia and CC-Net (Wenzek
et al., 2020).6631
‚Ä¢GenQ uses T5 (Raffel et al., 2020) generates
5 queries for each passage in target domain
and fine-tunes TAS-B (Hofst√§tter et al., 2021)
on this data.
‚Ä¢GPL improves the domain adaptation perfor-
mance based on GenQ. In addition to gener-
ated queries, GPL uses cross-encoder to pro-
vide the pseudo-label. GPL fine-tunes multi-
ple backbones on the generated queries and
pseudo-labels and we report the best perfor-
mance that is fine-tuned on TAS-B.
‚Ä¢COCO-DR performs unsupervised pre-training on target domain and introduces dis-
tributional robust optimization.
C Domain-invariant Representation
Visualized results of T-SNE of representations of
source and target (SCIDOCS, TREC-COVID, NF-
Corpus and DBpedia) domains encoded by DPR
and DPR+BERM respectively are shown in Fig-
ure 6.66326633ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
After the Section 6 Conclusion
/squareA2. Did you discuss any potential risks of your work?
Ethics Statement
/squareA3. Do the abstract and introduction summarize the paper‚Äôs main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiÔ¨Åc artifacts?
Section 5
/squareB1. Did you cite the creators of artifacts you used?
Section 5.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 5.1
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 5.1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps
taken to protect / anonymize it?
Ethics Statement
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Appendix (only domain)
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiÔ¨Åcant, while on small test sets they may not be.
Appendix
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 5.16634/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 5
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 5
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants‚Äô demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.6635