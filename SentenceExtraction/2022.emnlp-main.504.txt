
Wenbiao Li, Ziyang Wang, Yunfang WuMOE Key Laboratory of Computational Linguistics, Peking UniversitySchool of Software and Microelectronics, Peking University, Beijing, ChinaSchool of Computer Science, Peking University, Beijing, China
{liwb, wzy232303}@stu.pku.edu.cn ,wuyf@pku.edu.cn
Abstract
For readability assessment, traditional meth-
ods mainly employ machine learning classifiers
with hundreds of linguistic features. Although
the deep learning model has become the promi-
nent approach for almost all NLP tasks, it is less
explored for readability assessment. In this pa-
per, we propose a BERT-based model with fea-
ture projection and length-balanced loss (BERT-
FP-LBL) for readability assessment. Specially,
we present a new difficulty knowledge guided
semi-supervised method to extract topic fea-
tures to complement the traditional linguistic
features. From the linguistic features, we em-
ploy projection filtering to extract orthogonal
features to supplement BERT representations.
Furthermore, we design a new length-balanced
loss to handle the greatly varying length distri-
bution of data. Our model achieves state-of-the-
art performances on two English benchmark
datasets and one dataset of Chinese textbooks,
and also achieves the near-perfect accuracy of
99% on one English dataset. Moreover, our pro-
posed model obtains comparable results with
human experts in consistency test.
1 Introduction
Readability assessment is to automatically deter-
mine the difficulty level of a given text, aim-
ing to recommend suitable reading materials to
readers. There are wide applications of readabil-
ity assessment, such as automating readers’ advi-
sory (Pera and Ng, 2014), clinical informed consent
forms (Perni et al., 2019) and internet-based patient
education materials (Sare et al., 2020).
Comparing with other natural language pro-
cessing (NLP) tasks, readability assessment is
less explored. In the early days, researchers
exploit linguistic features to develop various
readability formulas, such as Flesch (Flesch,
1948), Dale-Chall (Dale and Chall, 1948) andSMOG (Mc Laughlin, 1969). Later, the main-
stream research (Deutsch et al., 2020; Hansen et al.,
2021; Lee et al., 2021) is to employ machine learn-
ing models to classify a text, by designing a large
number of linguistic features. There are also works
that treat it as a regression task (Sheehan et al.,
2010) or a ranking task (Lee and Vajjala, 2022).
Recently, unlike other NLP tasks, the introduc-
tion of deep neural networks for readability assess-
ment does not achieve overwhelming advantages
over traditional machine learning methods. Em-
ploying neural network models for readability as-
sessment, there are several challenges:
(1) The scale of the dataset for readability assess-
ment is small, which restricts the performance of
deep neural network models.
(2) The deep neural network model is mainly
based on characters or words and the extracted fea-
tures are often at a shallow level. As a result, words
with similar functions or meanings, such as “man”
and “gentleman”, are mapped into close vectors al-
though their reading difficulties are different (Jiang
et al., 2018).
(3) The linguistic features designed by re-
searchers and continuous features extracted by neu-
ral network models are from two different semantic
spaces. If two kinds of features are simply concate-
nated, it will bring redundant information or even
harmful effects to model performance.
(4) Unlike other NLP data whose length fol-
lows a normal distribution, a notable problem with
the data for readability assessment is that the text
length varies greatly. The texts with low difficulty
are usually shorter, while texts with high difficulty
are usually longer. For example, as shown in Ta-
ble 1, in ChineseLR the average length of Level
1 is only 266 characters, while the average length
of Level 5 is 3,299 characters. As a result, when
experimented with deep learning networks, shorter
texts tend to converge much faster than those longer
ones thus harm the overall performance.7446In order to solve the above problems, we propose
aBERT -based model with Feature Projection and
Length- Balanced Loss (BERT-FP-LBL ). With the
pre-trained BERT as the backbone, we employ fea-
ture projection to integrate linguistic features into
the neural model, and design a new length-balanced
loss function to guide the training. Concretely:
•We leverage BERT and a mixed-pooling
mechanism to obtain text representations,
which take advantage of the powerful repre-
sentative ability of pre-trained model, and thus
overcome the data-sparsity problem.
•Beyond traditional features, we extract a set of
topic features enriched with difficulty knowl-
edge, which are high-level global features.
Specifically, based on a graded lexicon, we
exploit a clustering algorithm to group related
words belonging to the same difficulty level,
which then serve as anchor words to guide the
training of a semi-supervised topic model.
•Rather than simple concatenation, we project
linguistic features to the neural network fea-
tures to obtain orthogonal features, which sup-
plement the neural network representations.
•We introduce a new length-balanced loss func-
tion to revise the standard cross entropy loss,
which balances the varying length distribution
of data for readability assessment.
We conduct experiments on three English bench-
mark datasets, including WeeBit (Vajjala and Meur-
ers, 2012), OneStopEnglish (Vajjala and Lu ˇci´c,
2018) and Cambridge (Xia et al., 2019), and one
Chinese dataset collected from school textbooks.
Experimental results show that our proposed model
outperforms the baseline model by a wide margin,
and achieves new state-of-the-art results on WeeBit
and Cambridge.
We also conduct test to measure the correlation
coefficient between the BERT-FP-LBL model’s in-
ference results and three human experts, and the
results demonstrate that our model achieves com-
parable results with human experts.
2 Related Work
Traditional Methods. Early research efforts fo-
cused on various linguistic features as defined
by linguists. Researchers use these features to
create various formulas for readability, includingFlesch (Flesch, 1948), Dale-Chall (Dale and Chall,
1948) and SMOG (Mc Laughlin, 1969). Although
the readability formula has the advantages of sim-
plicity and objectivity, there are also some prob-
lems, such as the introduction of fewer variables
during the development, and insufficient considera-
tion of the variables at the discourse level.
Machine Learning Classification Methods.
(Schwarm and Ostendorf, 2005) develop a method
of reading level assessment that uses support vec-
tor machines (SVMs) to combine features from
statistical language models (LMs), parse trees, and
other traditional features used in reading level as-
sessment. Subsequently, (Petersen and Ostendorf,
2009) present expanded results for the SVM de-
tectors. (Qiu et al., 2017) designe 100 factors to
systematically evaluate the impact of four levels
of linguistic features (shallow, POS, syntactic, dis-
course) on predicting text difficulty for L1 Chinese
learners and further selected 22 significant features
with regression. (Lu et al., 2019) design exper-
iments to analyze the influence of 88 linguistic
features on sentence complexity and results sug-
gest that the linguistic features can significantly im-
prove the predictive performance with the highest
of 70.78% distance-1 adjacent accuracy. (Deutsch
et al., 2020; Lee et al., 2021) evaluate the joint
application of handcrafted linguistic features and
deep neural network models. The handcrafted lin-
guistic features are fused with the features of neural
networks and fed into a machine learning model
for classification.
Neural Network Models. (Jiang et al., 2018)
provide the knowledge-enriched word embedding
(KEWE) for readability assessment, which encodes
the knowledge on reading difficulty into the rep-
resentation of words. (Azpiazu and Pera, 2019)
present a multi-attentive recurrent neural network
architecture for automatic multilingual readability
assessment. This architecture considers raw words
as its main input, but internally captures text struc-
ture and informs its word attention process using
other syntax and morphology-related datapoints,
known to be of great importance to readability.
(Meng et al., 2020) propose a new and compre-
hensive framework which uses a hierarchical self-
attention model to analyze document readability.
(Qiu et al., 2021) form a correlation graph among
features, which represent pairwise correlations be-
tween features as triplets with linguistic features as
nodes and their correlations as edges.74473 Methodology
The overall structure of our model is illustrated in
Figure 1. We integrate difficulty knowledge to ex-
tract topic features using the Anchored Correlation
Explanation (CorEx) (Gallagher et al., 2017), and
fuse linguistic features with neural network rep-
resentations through projection filtering. Further,
we propose a new length-balanced loss function to
deal with the unbalanced length distribution of the
readability assessment data.
3.1 Traditional Features
Many previous studies have proved that shallow
and linguistic features are helpful for readability
assessment. For Chinese traditional features, we
develop a Chinese toolkit zhfeat to extract charac-
ter, word, sentence and paragraph features. Please
refer to Appendix A for detailed descriptions. For
English traditional features, we extract discourse,
syntactic, lexical and shallow features, by directly
implementing the lingfeat (Lee et al., 2021) toolkit.
We denote the traditional features as f.
3.2 Topic Features with Difficulty Knowledge
Background. Besides the above lexical and syn-
tactic features, topic features provide high-level
semantic information for assessing difficulty level.
(Lee et al., 2021) also leverage topic features, but
they train the topic model in a purely unsupervised
way without considering difficulty knowledge. In-
spired by the work of Anchored Correlation Expla-
nation (CorEx) (Gallagher et al., 2017), which al-
lows integrating domain knowledge through anchor
words, we introduce word difficulty knowledge to
guide the training of topic model, thus obtaining
difficulty-aware topic features.
First, we introduce the concept of information
bottleneck (Tishby et al., 2000), which aims to
achieve a trade-off between compressing feature
Xinto representation Yand preserving as much
information as possible with respect to the label Z.
Formally, the information bottleneck is expressed
as:
maxξI(Z:Y)−I(X:Y) (1)
I(X:X) =H(X) +H(X)
−H(X1, X2)(2)
where I(X:X)is the mutual information of
random variables XandX,H(X)represents theentropy of the random variable X, andξrepresents
the Lagrange multiplier.
In CorEx, if we want to learn representations
that are more relevant to specific keywords, we can
anchor a word type Xto topic Y, and control the
strength of anchoring by constraining optimization
ξ≥1. The optimization objective is:
max/summationdisplay/parenleftig/summationdisplayξI(X:Y)−I(X:Y)/parenrightig
(3)
where urepresents the number of topics, vis the
number of words corresponding to the topic, and
ξrepresents the anchoring strength of the word i
to the topic j.
Extracting difficulty-aware Topic Features. We
utilize a lexicon containing words of varying dif-
ficulty levels to extract anchor words. Let Ω =
{L, L, ..., L}be a graded lexicon, where Lis
the set of words with difficulty level i.Cis the
corpus for pre-training the topic model. First, we
select out some high frequent words of each level
in the corpus C:
W=L∩C (4)
where∩represents the intersection operation.
For each level of words, we conduct KMeans
clustering algorithm to do classification, and then
remove isolated word categories (a single word is
categorized as a class):
W= KMeans( W) (5)
The clustering result of words with the
difficulty level iis denoted as W=
{{w, w, ...},{w, w, ...}, ...}. Thus,
the final anchor words are:
W={W, W, ..., W} (6)
These anchor words of different difficulty levels
serve as domain knowledge to guide the training of
topic models:
ATM = CorEx( C, anchors =W)) (7)
where ATM represents the anchored topic model.
Then, we implement the ATM to obtain a set of
topic distribution features involving difficulty in-
formation, which are denoted as f.7448
Combining traditional and topic features, we
obtain the overall linguistic features:
f=f⊕f (8)
where ⊕represents the splicing operation.
3.3 Feature Fusion with Projection Filtering
BERT Representation. We leverage the pre-
trained BERT model (Devlin et al., 2018) to obtain
sentence representation.
The length distribution of data for readability
evaluation varies greatly, and texts with higher dif-
ficulty are very long, which might exceed the input
limit of the model. Therefore, for an input text S,
we segment it as S= (s, s, ..., s). For each
segment, we exploit BERT to extract its semantic
representation: H= (h, h, ..., h).
Further, we adopt Mixed Pooling (Yu et al.,
2014) to extract representations of the entire text:
f=λMaxPooling( H)+
(1−λ)MeanPooling( H)(9)
where λis a parameter to balance the ratio between
max pooling and mean pooling.
Projection Filtering. To obtain better perfor-
mance, we try to combine BERT representations
with linguistic features. As for the method of direct
splicing, since two kinds of features come fromdifferent semantic spaces, not only will it intro-
duce some repetitive information, but also it may
bring contradictions between some features that
will harm the performance. When performing fea-
ture fusion, our goal is to obtain additional orthogo-
nal features to complement each other. Inspired by
the work (Qin et al., 2020), which uses two iden-
tical encoders with different optimization objec-
tives to extract common and differentiated features.
Unlike this work, our artificial features and neural
features are extracted in different ways, and our pur-
pose is to perform feature complementation. Since
the pre-trained model captures more semantic-level
features through the contextual co-occurrence rela-
tionship between large-scale corpora. This is not
enough for readability tasks, and the discrimination
of difficulty requires some supplementary features
(difficulty, syntax, etc.). So we consider the fea-
tures extracted by BERT as primary features and
linguistic features as secondary ones, and then the
secondary features are projected into the primary
features to obtain additional orthogonal features.
Concretely, based on the linguistic features f
and BERT representation f, we perform dimen-
sional transformation and project them into the
same vector space:
f=tanh(tanh(fW+b))W+b)(10)7449f=tanh(tanh(fW+b))W+b)(11)
where W,WandWare the trainable parame-
ters, and b,bandbare the scalar biases.
Next, we project the secondary features into pri-
mary ones to obtain additional orthogonal features
f:
f=f−f·f
|f|f (12)
The orthogonal features are further added to the
BERT representation to constitute the final text
representation:
f=f⊕f (13)
Finally, we compute the probability that a text
belongs to the i−thcategory by:
p= Softmax( fW+b) (14)
where Wis the trainable parameters, and bare
scalar biases.
3.4 Length Balanced Loss Function
The text length is an important aspect for deter-
mining the reading difficulty level. As shown in
Table 1, a text with high difficulty level generally
contains more tokens than that of low level. For
example, on Cambridge dataset, the average length
of Level 1 is 141 tokens, while the average length
of Level 5 is 751 tokens. When experimented with
deep learning methods, texts with short length tend
to converge much faster than the texts with long
length that influences the final performance.
To address this issue, we revise the loss to handle
varying length. Specially, we measure the length
distribution by weighting different length attributes,
including the average, median, minimum and max-
imum length:
θ=/summationdisplayπ, i= 1,2, ..., N (15)
where θrepresents the length value of the text
category i,π,π,πandπrepresent the
average, median, minimum and maximum length
of the i−thtext, respectively. Nis the total number
of categories.We normalize the length value to obtain the
length coefficient for each category:
κ=θ/summationtextθ(16)
Accordingly, the final loss function for a single
sample is defined as:
L=−/summationdisplayκylog(p) (17)
where yis the true label of text, ρis the adjustment
factor of length distribution. When ρ= 0, it is
reduced to the traditional cross entropy loss.
4 Experimental Setup
4.1 Datasets
To demonstrate the effectiveness of our proposed
method, we conduct experiments on three English
datasets and one Chinese dataset. We split the train,
valid and test data according to the ratio of 8:1:1.
The statistic distribution of datasets can be found
in Table 1.
WeeBit (Vajjala and Meurers, 2012) is often
considered as the benchmark data for English read-
ability assessment. It was originally created as an
extension of the well-known Weekly Reader cor-
pus. We downsample to 625 passages per class.
OneStopEnglish (Vajjala and Lu ˇci´c, 2018) is an
aligned channel corpus developed for readability
assessment and simplification research. Each text
is paraphrased into three versions.
Cambridge (Xia et al., 2019) is a dataset con-
sisting of reading passages from the five main suite
Cambridge English Exams (KET, PET, FCE, CAE,
CPE). We downsample to 60 passages per class.
ChineseLR. ChineseLR is a Chinese dataset that
we collected from textbooks of middle and primary
school of more than ten publishers. To suit our
task, we delete poetry and traditional Chinese texts.
Following the standards specified in the Chinese
Curriculum Standards for Compulsory Education ,
we category all texts to five difficulty levels.
4.2 Baseline Models
SVM. We employ support vector machines as the
traditional machine learning classifier. The input
to the model is the linguistic feature f. We adopt
MinMaxScaler (ranging from -1 to 1) for linguistic7450
features and use the RBF kernel function. We use
the libsvmframework for experiments.
BERT. We utilize fin Equation 9 followed by a
linear layer classifier as our BERT baseline model.
4.3 Training and Evaluation Details
For the selection of the difficulty level lexicon Ω,
on the English dataset, we use the lexicon released
by Maddela and Xu (2018), where we only use the
first 4 levels. On the Chinese dataset, we use the
Compulsory Education Vocabulary (Su, 2019). The
word embedding features of English and Chinese
word clustering algorithms are respectively used
(Pennington et al., 2014) and (Song et al., 2018).
We use the Wikipedia corpusfor pre-training
the semi-supervised topic models. Please refer to
Appendix B for some other details.
We do experiments using the Pytorch (Paszke
et al., 2019) framework. For training, we use the
AdamW optimizer, the weight decay is 0.02 and
the warm-up ratio is 0.1. The mixing pooling ratio
λis set to 0.5. Other specific parameter settings are
shown in Table 2.
For evaluation, we calculate the accuracy,
weighted F1 score, precision, recall and quadratic
weighted kappa (QWK). We repeated each experi-
ment three times and reported the average score.5 Results and Analysis
5.1 Overall Results
The experimental results of all models are summa-
rized in Table 3. First of all, it should be noted
that there are only a few studies on readability as-
sessment, and there is no unified standard for data
division and experimental parameter configuration.
This has led to large differences in the results of
different research works.
Our BERT-FP-LBL model achieves consistent
improvements over the baselines on all four
datasets, which validates the effectiveness of our
proposed method. In terms of F1 metrics, our
method improves WeeBit and ChineseLR by 1.66
and 3.7 compared to the baseline BERT model.
Overall, our model achieves state-of-the-art per-
formance on WeeBit and Cambridge. On On-
eStopEnglish, our model also achieves competi-
tive results compared to previous work (Lee et al.,
2021), also achieving near-perfect classification
accuracy of 99%.
Comparing the experimental results of SVM and
the base BERT, it can be observed that on Cam-
bridge and ChineseLR, SVM outperforms BERT.
We believe this benefits from the linguistic features
of our design.
5.2 Ablation Study
To illustrate the contribution of each module in
our model, we conduct ablation experiments on
WeeBit and ChineseLR, and the results are reported
in Table 4.
When A Wis removed, the CorEx changes from
semi-supervised to unsupervised. The F1 scores
of WeeBit and ChineseLR drop by 0.31 and 0.52,
respectively, and when TFDK is removed, the cor-
responding F1 scores drop by 0.61 and 1.27, re-
spectively. This indicates that our topic features in-
corporating difficulty knowledge indeed contribute
to readability assessment.
Furthermore, when FPis removed, as described
in Section 3.3, the simple splice operation brings7451
some duplication or even negative information to
the model. The F1 scores of WeeBit and Chine-
seLR both drop by 0.48.
Finally, when LBL is removed, the F1 scores
of WeeBit and ChineseLR drop by 0.97 and 1.81,
respectively. We believe that the difference in the
length distribution of the dataset affects the conver-
gence speed of different categories, which in turn
will have an impact on the results. Besides, the drop
in F1 metric is much more severe on ChineseLR
than on WeeBit, and this result can be attributed to
the more severe length imbalance on ChineseLR as
shown in Table 1.
5.3 Analysis on the Length-balanced Loss
To explore the effect of length-balanced loss, we
set different ρto conduct experiments. The larger
theρis, the difference between the loss of differentcategories is bigger. The loss difference leads to
different convergence rates. When ρis 0, the loss
function is the standard cross entropy loss, and
there is no difference in the loss contributed by
different categories. The specific results are shown
in Figure 2.
For BERT, the optimal value of ρis relatively
large, which means the model needs a relatively
big difference in the loss to solve the problem of
unbalanced text length. This indicates that there
are indeed differences in the convergence speed
between different classes, and this difference can
be reduced by correcting the loss contributed by
different classes. After adding orthogonal features,
the optimal value of ρis relatively small. We think
that whether the text is short or long, the number of
parameters of its corresponding orthogonal features
is fixed and does not require the length-balanced
loss to adjust. So, when BERT features are com-
bined with orthogonal features, the optimal value
ofρwill be lower than that in BERT alone.
In addition, the optimal value of ρon WeeBit is
0.8, while the optimal value of ρon ChineseLR is
0.4. This is perhaps because the WeeBit dataset has
a small span of length distribution (maximum 512
truncation), and we need to relatively amplify the
differences between different categories. However,
the length distribution of the ChineseLR dataset has
a large span (500 ×8), and we need to relatively nar-
row the differences between different categories.
Of course, the optimal value of ρis related to the7452
specific data distribution, which is a parameter that
needs to grid search. Generally speaking, when
the length difference between different categories
is small, we set ρrelatively large, and when the
length difference between different categories is
large, we set ρrelatively small.
5.4 Analysis on the Difficulty-aware Topic
Features
To further explore the impact of topic features with
domain knowledge, we visualize the traditional
features f, difficulty-aware topic features fand
combining features f. Specifically, On WeeBit
and ChineseLR, we randomly selected 50 samples
from each level of 1, 3 and 5 for visualization, as
shown in Figure 3 and 4.
For texts of completely different difficulty, their
traditional features are near in the latent space. This
shows that traditional features pay more attention to
semantic information rather than reading difficulty.
By adding difficulty-aware topic features, texts of
different difficulty are better differentiated. Further,
the combination of two kinds of features achieves
a better ability to distinguish reading difficulty.
5.5 Consistency Test with Human Experts
To judge the difficulty level of a text is also a hard
task for humans, and so we conduct experiments
to investigate how consistent the model’s inference
results are with human experts. We collected 200
texts from extracurricular reading materials, and
hired three elementary school teachers to do double-
blind labeling. Each text is required to annotate
with an unique label 1/2/3, corresponding to the
first/second/third level.Our model (denoted as M4) is regarded as a sin-
gle expert that is equal to the other three human ex-
perts (E1/E2/E3). We calculate the Spearman cor-
relation coefficient of annotation results between
each pair, and report the results in Table 5.
On the whole, there is a significant correlation
at the 0.01 level between human experts (E1, E2 or
E3) and our model. On the one hand, there is still a
certain gap between the model and human experts
(E1 and E2). On the other hand, the inference re-
sults of our model are comparable with the human
expert E3. Especially, when E1 is adopted as the
reference standard, the consistency of our model
prediction is slightly higher than that of E3 (0.836
vs. 0.829). When E2 is regarded as the reference
standard, the consistency of our model prediction
is slightly lower than that of E3.
Although there is no unified standard for the
definition of "text difficulty", which relies heavily
on the subject experiences of experts, our model
achieves competitive results with human experts.
6 Conclusions
In this paper, we propose a unified neural net-
work model BERT-FP-LBL for readability as-
sessment. We extract difficulty-aware topic fea-7453
tures through the Anchored Correlation Explana-
tion method, and fuse linguistic features with BERT
representations via projection filtering. We propose
a length-balanced loss to cope with the imbalance
length distribution. We conduct extensive experi-
ments and detailed analyses on both English and
Chinese datasets. The results show that our method
achieves state-of-the-art results on three datasets
and near-perfect accuracy of 99% on one English
dataset.
Limitations
From the perspective of experimental setup, there
is no uniform standard for data division and experi-
mental parameter configuration due to less research
on readability assessment. This leads to large differ-
ences in the results of different studies (Qiu et al.,
2021; Martinc et al., 2021; Lee et al., 2021), and
the results of the corresponding experiments are
not comparable. Therefore, objectively speaking,
our comparison object is only the baseline model,
which lacks a fair comparison with previous work.
From the perspective of readability assessment
task, since different datasets have different diffi-
culty scales and different length distributions. In
order to ensure the performance on the dataset as
much as possible, our length-balanced loss param-
eters are mainly calculated according to the length
distribution of the corresponding dataset, and itis impossible to transfer across datasets directly,
which is also a major difficulty in this field. In
cross-dataset and cross-language scenarios, there is
a lack of a unified approach. Without new ways to
deal with the difficulty scales of different datasets,
or without large public datasets, developing a gen-
eral readability assessment model will always be
challenging.
Acknowledgement
This work is supported by the National Natural
Science Foundation of China (62076008), the Key
Project of Natural Science Foundation of China
(61936012) and the National Hi-Tech RD Program
of China (No.2020AAA0106600).
References745474557456A Chinese Traditional Features
B Semi-supervised Topic Model Related
Parameters
C SVM model Related Hyperparameters
The search range of parameter cis [2, 4, 8, 16,
32, 64, 128, 256, 512, 1024, 2048, 4096, 8192,
16384, 32768], and the search range of parameter g
is [0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128,
0.256, 0.512, 1.024, 2.048, 4.096].7457