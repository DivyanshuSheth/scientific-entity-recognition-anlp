
Keshav Santhanam
Stanford UniversityOmar Khattab
Stanford UniversityJon Saad-Falcon
Georgia Institute of Technology
Christopher Potts
Stanford UniversityMatei Zaharia
Stanford University
Abstract
Neural information retrieval (IR) has greatly
advanced search and other knowledge-
intensive language tasks. While many neural
IR methods encode queries and documents
into single-vector representations, late
interaction models produce multi-vector repre-
sentations at the granularity of each token and
decompose relevance modeling into scalable
token-level computations. This decomposition
has been shown to make late interaction more
effective, but it inﬂates the space footprint of
these models by an order of magnitude. In this
work, we introduce ColBERTv2, a retriever
that couples an aggressive residual compres-
sion mechanism with a denoised supervision
strategy to simultaneously improve the quality
and space footprint of late interaction. We
evaluate ColBERTv2 across a wide range
of benchmarks, establishing state-of-the-art
quality within and outside the training domain
while reducing the space footprint of late
interaction models by 6–10 ×.
1 Introduction
Neural information retrieval (IR) has quickly domi-
nated the search landscape over the past 2–3 years,
dramatically advancing not only passage and doc-
ument search (Nogueira and Cho, 2019) but also
many knowledge-intensive NLP tasks like open-
domain question answering (Guu et al., 2020),
multi-hop claim veriﬁcation (Khattab et al., 2021a),
and open-ended generation (Paranjape et al., 2022).
Many neural IR methods follow a single-vector
similarity paradigm: a pretrained language model
is used to encode each query and each document
into a single high-dimensional vector, and rele-
vance is modeled as a simple dot product between
both vectors. An alternative is late interaction , in-
troduced in ColBERT (Khattab and Zaharia, 2020),
where queries and documents are encoded at a ﬁner-
granularity into multi-vector representations, andrelevance is estimated using rich yet scalable in-
teractions between these two sets of vectors. Col-
BERT produces an embedding for every token in
the query (and document) and models relevance
as the sum of maximum similarities between each
query vector and all vectors in the document.
By decomposing relevance modeling into token-
level computations, late interaction aims to reduce
the burden on the encoder: whereas single-vector
models must capture complex query–document re-
lationships within one dot product, late interaction
encodes meaning at the level of tokens and del-
egates query–document matching to the interac-
tion mechanism. This added expressivity comes
at a cost: existing late interaction systems impose
an order-of-magnitude larger space footprint than
single-vector models, as they must store billions
of small vectors for Web-scale collections. Con-
sidering this challenge, it might seem more fruit-
ful to focus instead on addressing the fragility of
single-vector models (Menon et al., 2022) by in-
troducing new supervision paradigms for negative
mining (Xiong et al., 2020), pretraining (Gao and
Callan, 2021), and distillation (Qu et al., 2021).
Indeed, recent single-vector models with highly-
tuned supervision strategies (Ren et al., 2021b; For-
mal et al., 2021a) sometimes perform on-par or
even better than “vanilla” late interaction models,
and it is not necessarily clear whether late inter-
action architectures—with their ﬁxed token-level
inductive biases—admit similarly large gains from
improved supervision.
In this work, we show that late interaction re-
trievers naturally produce lightweight token rep-
resentations that are amenable to efﬁcient storage
off-the-shelf and that they can beneﬁt drastically
from denoised supervision. We couple those in
ColBERTv2 ,a new late-interaction retriever that
employs a simple combination of distillation from3715a cross-encoder and hard-negative mining (§3.2)
to boost quality beyond any existing method, and
then uses a residual compression mechanism (§3.3)
to reduce the space footprint of late interaction by
6–10×while preserving quality. As a result, Col-
BERTv2 establishes state-of-the-art retrieval qual-
ity both within andoutside its training domain with
a competitive space footprint with typical single-
vector models.
When trained on MS MARCO Passage Rank-
ing, ColBERTv2 achieves the highest MRR@10 of
any standalone retriever. In addition to in-domain
quality, we seek a retriever that generalizes “zero-
shot” to domain-speciﬁc corpora and long-tail top-
ics, ones that are often under-represented in large
public training sets. To this end, we evaluate Col-
BERTv2 on a wide array of out-of-domain bench-
marks. These include three Wikipedia Open-QA
retrieval tests and 13 diverse retrieval and semantic-
similarity tasks from BEIR (Thakur et al., 2021). In
addition, we introduce a new benchmark, dubbed
LoTTE , for Long-TailTopic-stratiﬁed Evaluation
for IR that features 12 domain-speciﬁc search
tests, spanning StackExchange communities and
using queries from GooAQ (Khashabi et al., 2021).
LoTTE focuses on relatively long-tail topics in
its passages, unlike the Open-QA tests and many
of the BEIR tasks, and evaluates models on their
capacity to answer natural search queries with a
practical intent, unlike many of BEIR’s semantic-
similarity tasks. On 22 of 28 out-of-domain tests,
ColBERTv2 achieves the highest quality, outper-
forming the next best retriever by up to 8% relative
gain, while using its compressed representations.
This work makes the following contributions:
1.We propose ColBERTv2, a retriever that com-
bines denoised supervision and residual com-
pression, leveraging the token-level decom-
position of late interaction to achieve high
robustness with a reduced space footprint.
2.We introduce LoTTE, a new resource for out-
of-domain evaluation of retrievers. LoTTE fo-
cuses on natural information-seeking queries
over long-tail topics, an important yet under-
studied application space.
3.We evaluate ColBERTv2 across a wide range
of settings, establishing state-of-the-art qual-
ity within and outside the training domain.2 Background & Related Work
2.1 Token-Decomposed Scoring in Neural IR
Many neural IR approaches encode passages as
a single high-dimensional vector, trading off the
higher quality of cross-encoders for improved ef-
ﬁciency and scalability (Karpukhin et al., 2020;
Xiong et al., 2020; Qu et al., 2021). Col-
BERT’s (Khattab and Zaharia, 2020) late inter-
action paradigm addresses this tradeoff by com-
puting multi-vector embeddings and using a scal-
able “MaxSim” operator for retrieval. Several
other systems leverage multi-vector representa-
tions, including Poly-encoders (Humeau et al.,
2020), PreTTR (MacAvaney et al., 2020), and
MORES (Gao et al., 2020), but these target
attention-based re-ranking as opposed to Col-
BERT’s scalable MaxSim end-to-end retrieval.
ME-BERT (Luan et al., 2021) generates token-
level document embeddings similar to ColBERT,
but retains a single embedding vector for queries.
COIL (Gao et al., 2021) also generates token-level
document embeddings, but the token interactions
are restricted to lexical matching between query
and document terms. uniCOIL (Lin and Ma, 2021)
limits the token embedding vectors of COIL to a
single dimension, reducing them to scalar weights
that extend models like DeepCT (Dai and Callan,
2020) and DeepImpact (Mallia et al., 2021). To
produce scalar weights, SPLADE (Formal et al.,
2021b) and SPLADEv2 (Formal et al., 2021a) pro-
duce a sparse vocabulary-level vector that retains
the term-level decomposition of late interaction
while simplifying the storage into one dimension
per token. The SPLADE family also piggybacks on
the language modeling capacity acquired by BERT
during pretraining. SPLADEv2 has been shown
to be highly effective, within and across domains,
and it is a central point of comparison in the exper-
iments we report on in this paper.
2.2 Vector Compression for Neural IR
There has been a surge of recent interest in com-
pressing representations for IR. Izacard et al. (2020)
explore dimension reduction, product quantization
(PQ), and passage ﬁltering for single-vector retriev-
ers. BPR (Yamada et al., 2021a) learns to directly
hash embeddings to binary codes using a differen-
tiable tanh function. JPQ (Zhan et al., 2021a) and
its extension, RepCONC (Zhan et al., 2022), use
PQ to compress embeddings, and jointly train the
query encoder along with the centroids produced3716by PQ via a ranking-oriented loss.
SDR (Cohen et al., 2021) uses an autoencoder to
reduce the dimensionality of the contextual embed-
dings used for attention-based re-ranking and then
applies a quantization scheme for further compres-
sion. DensePhrases (Lee et al., 2021a) is a system
for Open-QA that relies on a multi-vector encod-
ing of passages, though its search is conducted
at the level of individual vectors and not aggre-
gated with late interaction. Very recently, Lee et al.
(2021b) propose a quantization-aware ﬁnetuning
method based on PQ to reduce the space footprint
of DensePhrases. While DensePhrases is effective
at Open-QA, its retrieval quality—as measured by
top-20 retrieval accuracy on NaturalQuestions and
TriviaQA—is competitive with DPR (Karpukhin
et al., 2020) and considerably less effective than
ColBERT (Khattab et al., 2021b).
In this work, we focus on late-interaction re-
trieval and investigate compression using a residual
compression approach that can be applied off-the-
shelf to late interaction models, without special
training. We show in Appendix A that ColBERT’s
representations naturally lend themselves to resid-
ual compression. Techniques in the family of resid-
ual compression are well-studied (Barnes et al.,
1996) and have previously been applied across sev-
eral domains, including approximate nearest neigh-
bor search (Wei et al., 2014; Ai et al., 2017), neural
network parameter and activation quantization (Li
et al., 2021b,a), and distributed deep learning (Chen
et al., 2018; Liu et al., 2020). To the best of our
knowledge, ColBERTv2 is the ﬁrst approach to use
residual compression for scalable neural IR.
2.3 Improving the Quality of Single-Vector
Representations
Instead of compressing multi-vector representa-
tions as we do, much recent work has focused
on improving the quality of single-vector mod-
els, which are often very sensitive to the speciﬁcs
of supervision. This line of work can be decom-
posed into three directions: (1) distillation of more
expressive architectures (Hofstätter et al., 2020;
Lin et al., 2020) including explicit denoising (Qu
et al., 2021; Ren et al., 2021b), (2) hard negative
sampling (Xiong et al., 2020; Zhan et al., 2020a,
2021b), and (3) improved pretraining (Gao and
Callan, 2021; O ˘guz et al., 2021). We adopt similar
techniques to (1) and (2) for ColBERTv2’s multi-
vector representations (see §3.2).
2.4 Out-of-Domain Evaluation in IR
Recent progress in retrieval has mostly focused on
large-data evaluation, where many tens of thou-
sands of annotated training queries are associated
with the test domain, as in MS MARCO or Natu-
ral Questions (Kwiatkowski et al., 2019). In these
benchmarks, queries tend to reﬂect high-popularity
topics like movies and athletes in Wikipedia. In
practice, user-facing IR and QA applications often
pertain to domain-speciﬁc corpora, for which little
to no training data is available and whose topics
are under-represented in large public collections.
This out-of-domain regime has received recent
attention with the BEIR (Thakur et al., 2021) bench-
mark. BEIR combines several existing datasets
into a heterogeneous suite for “zero-shot IR” tasks,
spanning bio-medical, ﬁnancial, and scientiﬁc do-
mains. While the BEIR datasets provide a use-
ful testbed, many capture broad semantic related-
ness tasks—like citations, counter arguments, or
duplicate questions–instead of natural search tasks,
or else they focus on high-popularity entities like
those in Wikipedia. In §4, we introduce LoTTE, a
new dataset for out-of-domain retrieval, exhibiting
natural search queries over long-tail topics.
3 ColBERTv2
We now introduce ColBERTv2, which improves
the quality of multi-vector retrieval models (§3.2)
while reducing their space footprint (§3.3).
3.1 Modeling
ColBERTv2 adopts the late interaction architecture
of ColBERT, depicted in Figure 1. Queries and pas-
sages are independently encoded with BERT (De-
vlin et al., 2019), and the output embeddings encod-
ing each token are projected to a lower dimension.
During ofﬂine indexing, every passage din the
corpus is encoded into a set of vectors, and these3717vectors are stored. At search time, the query qis
encoded into a multi-vector representation, and its
similarity to a passage dis computed as the summa-
tion of query-side “MaxSim” operations, namely,
the largest cosine similarity between each query to-
ken embedding and all passage token embeddings:
S=/summationdisplaymaxQ·D (1)
where Qis an matrix encoding the query with N
vectors and Dencodes the passage with Mvectors.
The intuition of this architecture is to align each
query token with the most contextually relevant
passage token, quantify these matches, and com-
bine the partial scores across the query. We refer
to Khattab and Zaharia (2020) for a more detailed
treatment of late interaction.
3.2 Supervision
Training a neural retriever typically requires posi-
tiveandnegative passages for each query in the
training set. Khattab and Zaharia (2020) train
ColBERT using the ofﬁcial /angbracketleftq, d, d/angbracketrighttriples
of MS MARCO. For each query, a positive dis
human-annotated, and each negative dis sampled
from unannotated BM25-retrieved passages.
Subsequent work has identiﬁed several weak-
nesses in this standard supervision approach
(see §2.3). Our goal is to adopt a simple, uniform
supervision scheme that selects challenging neg-
atives and avoids rewarding false positives or pe-
nalizing false negatives. To this end, we start with
a ColBERT model trained with triples as in Khat-
tab et al. (2021b), using this to index the training
passages with ColBERTv2 compression.
For each training query, we retrieve the top- k
passages. We feed each of those query–passage
pairs into a cross-encoder reranker. We use a
22M-parameter MiniLM (Wang et al., 2020) cross-
encoder trained with distillation by Thakur et al.
(2021).This small model has been shown to ex-
hibit very strong performance while being rela-
tively efﬁcient for inference, making it suitable
for distillation.
We then collect w-way tuples consisting of a
query, a highly-ranked passage (or labeled posi-
tive), and one or more lower-ranked passages. In
this work, we use w= 64 passages per example.
Like RocketQAv2 (Ren et al., 2021b), we use aKL-Divergence loss to distill the cross-encoder’s
scores into the ColBERT architecture. We use KL-
Divergence as ColBERT produces scores (i.e., the
sum of cosine similarities) with a restricted scale,
which may not align directly with the output scores
of the cross-encoder. We also employ in-batch
negatives per GPU, where a cross-entropy loss is
applied to the positive score of each query against
all passages corresponding to other queries in the
same batch. We repeat this procedure once to re-
fresh the index and thus the sampled negatives.
Denoised training with hard negatives has been
positioned in recent work as ways to bridge the
gap between single-vector and interaction-based
models, including late interaction architectures like
ColBERT. Our results in §5 reveal that such super-
vision can improve multi-vector models dramati-
cally, resulting in state-of-the-art retrieval quality.
3.3 Representation
We hypothesize that the ColBERT vectors cluster
into regions that capture highly-speciﬁc token se-
mantics. We test this hypothesis in Appendix A,
where evidence suggests that vectors correspond-
ing to each sense of a word cluster closely, with
only minor variation due to context. We exploit
this regularity with a residual representation that
dramatically reduces the space footprint of late in-
teraction models, completely off-the-shelf without
architectural or training changes. Given a set of
centroids C, ColBERTv2 encodes each vector vas
the index of its closest centroid Cand a quantized
vector ˜rthat approximates the residual r=v−C.
At search time, we use the centroid index tand
residual ˜rrecover an approximate ˜v=C+ ˜r.
To encode ˜r, we quantize every dimension of r
into one or two bits. In principle, our b-bit encod-
ing of n-dimensional vectors needs ⌈log|C|⌉+bn
bits per vector. In practice, with n= 128 , we use
four bytes to capture up to 2centroids and 16 or
32 bytes (for b= 1orb= 2) to encode the resid-
ual. This total of 20 or 36 bytes per vector contrasts
with ColBERT’s use of 256-byte vector encodings
at 16-bit precision. While many alternatives can be
explored for compression, we ﬁnd that this simple
encoding largely preserves model quality, while
considerably lowering storage costs against typi-
cal 32- or 16-bit precision used by existing late
interaction systems.
This centroid-based encoding can be considered
a natural extension of product quantization to multi-3718vector representations. Product quantization (Gray,
1984; Jegou et al., 2010) compresses a single vector
by splitting it into small sub-vectors and encoding
each of them using an ID within a codebook. In
our approach, each representation is already a ma-
trix that is naturally divided into a number of small
vectors (one per token). We encode each vector
using its nearest centroid plus a residual. Refer
to Appendix B for tests of the impact of compres-
sion on retrieval quality and a comparison with a
baseline compression method for ColBERT akin to
BPR (Yamada et al., 2021b).
3.4 Indexing
Given a corpus of passages, the indexing stage
precomputes all passage embeddings and orga-
nizes their representations to support fast nearest-
neighbor search. ColBERTv2 divides indexing into
three stages, described below.
Centroid Selection. In the ﬁrst stage, Col-
BERTv2 selects a set of cluster centroids C. These
are embeddings that ColBERTv2 uses to sup-
port residual encoding (§3.3) and also for nearest-
neighbor search (§3.5). Standardly, we ﬁnd that
setting|C|proportionally to the square root of
n in the corpus works well empirically.
Khattab and Zaharia (2020) only clustered the vec-
tors after computing the representations of all pas-
sages, but doing so requires storing them uncom-
pressed. To reduce memory consumption, we apply
k-means clustering to the embeddings produced by
invoking our BERT encoder over only a sample of
all passages, proportional to the square root of the
collection size, an approach we found to perform
well in practice.
Passage Encoding. Having selected the cen-
troids, we encode every passage in the corpus. This
entails invoking the BERT encoder and compress-
ing the output embeddings as described in §3.3,
assigning each embedding to the nearest centroid
and computing a quantized residual. Once a chunk
of passages is encoded, the compressed representa-
tions are saved to disk.
Index Inversion. To support fast nearest-
neighbor search, we group the embedding IDs that
correspond to each centroid together, and save this
inverted list to disk. At search time, this allows us
to quickly ﬁnd token-level embeddings similar to
those in a query.3.5 Retrieval
Given a query representation Q, retrieval starts with
candidate generation. For every vector Qin the
query, the nearest n≥1centroids are found.
Using the inverted list, ColBERTv2 identiﬁes the
passage embeddings close to these centroids, de-
compresses them, and computes their cosine simi-
larity with every query vector. The scores are then
grouped by passage ID for each query vector, and
scores corresponding to the same passage are max -
reduced. This allows ColBERTv2 to conduct an
approximate “MaxSim” operation per query vector.
This computes a lower-bound on the true MaxSim
(§3.1) using the embeddings identiﬁed via the in-
verted list, which resembles the approximation ex-
plored for scoring by Macdonald and Tonellotto
(2021) but is applied for candidate generation.
These lower bounds are summed across the
query tokens, and the top-scoring n can-
didate passages based on these approximate scores
are selected for ranking, which loads the complete
set of embeddings of each passage, and conducts
the same scoring function using all embeddings
per document following Equation 1. The result
passages are then sorted by score and returned.
4 LoTTE: Long-Tail, Cross-Domain
Retrieval Evaluation
We introduce LoTTE (pronounced latte), a new
dataset for Long-TailTopic-stratiﬁed Evaluation
for IR. To complement the out-of-domain tests of
BEIR (Thakur et al., 2021), as motivated in §2.4,
LoTTE focuses on natural user queries that pertain
tolong-tail topics , ones that might not be covered
by an entity-centric knowledge base like Wikipedia.
LoTTE consists of 12 test sets, each with 500–2000
queries and 100k–2M passages.
The test sets are explicitly divided by topic, and
each test set is accompanied by a validation set of
related but disjoint queries andpassages. We elect
to make the passage texts disjoint to encourage
more realistic out-of-domain transfer tests, allow-
ing for minimal development on related but distinct
topics. The test (and dev) sets include a “pooled”
setting. In the pooled setting, the passages and
queries are aggregated across all test (or dev) topics
to evaluate out-of-domain retrieval across a larger
and more diverse corpus.
Table 1 outlines the composition of LoTTE. We
derive the topics and passage corpora from the
answer posts across various StackExchange fo-3719
rums. StackExchange is a set of question-and-
answer communities that target individual topics
(e.g., “physics” or “bicycling”). We gather forums
from ﬁve overarching domains: writing, recreation,
science, technology, and lifestyle. To evaluate re-
trievers, we collect Search andForum queries, each
of which is associated with one or more target an-
swer posts in its corpus. Example queries, and
short snippets from posts that answer them in the
corpora, are shown in Table 2.
Search Queries. We collect search queries from
GooAQ (Khashabi et al., 2021), a recent dataset
of Google search-autocomplete queries and their
answer boxes, which we ﬁlter for queries whose
answers link to a speciﬁc StackExchange post. As
Khashabi et al. (2021) hypothesize, Google Search
likely maps these natural queries to their answers
by relying on a wide variety of signals for rele-
vance, including expert annotations, user clicks,
and hyperlinks as well as specialized QA compo-
nents for various question types with access to the
post title and question body . Using those annota-
tions as ground truth, we evaluate the models on
their capacity for retrieval using only free text of
the answer posts (i.e., no hyperlinks or user clicks,
question title or body, etc.), posing a signiﬁcant
challenge for IR and NLP systems trained only on
public datasets.
Forum Queries. We collect the forum queries
by extracting post titles from the StackExchange
communities to use as queries and collect their
corresponding answer posts as targets. We select
questions in order of their popularity and sample
questions according to the proportional contribu-
tion of individual communities within each topic.
These queries tend to have a wider variety than
the “search” queries, while the search queries may
exhibit more natural patterns. Table 3 compares a
random samples of search and forum queries. It
can be seen that search queries tend to be brief,
knowledge-based questions with direct answers,
whereas forum queries tend to reﬂect more open-
ended questions. Both query sets target topics that
exceed the scope of a general-purpose knowledge
repository such as Wikipedia.
For search as well as forum queries, the result-
ing evaluation set consists of a query and a target
set of StackExchange answer posts (in particular,
the answer posts from the target StackExchange
page). Similar to evaluation in the Open-QA lit-
erature (Karpukhin et al., 2020; Khattab et al.,3720
2021b), we evaluate retrieval quality by comput-
ing the success@5 (S@5) metric. Speciﬁcally, we
award a point to the system for each query where
it ﬁnds an accepted or upvoted (score ≥1) answer
from the target page in the top-5 hits.
Appendix D reports on the breakdown of con-
stituent communities per topic, the construction
procedure of LoTTE as well as licensing considera-
tions, and relevant statistics. Figures 5 and 6 quan-
titatively compare the search and forum queries.
5 Evaluation
We now evaluate ColBERTv2 on passage retrieval
tasks, testing its quality within the training domain
(§5.1) as well as outside the training domain in
zero-shot settings (§5.2). Unless otherwise stated,
we compress ColBERTv2 embeddings to b= 2
bits per dimension in our evaluation.
5.1 In-Domain Retrieval Quality
Similar to related work, we train for IR tasks on MS
MARCO Passage Ranking (Nguyen et al., 2016).
Within the training domain, our development-set re-
sults are shown in Table 4, comparing ColBERTv2
with vanilla ColBERT as well as state-of-the-art
single-vector systems.
While ColBERT outperforms single-vector sys-
tems like RepBERT, ANCE, and even TAS-B, im-
provements in supervision such as distillation from
cross-encoders enable systems like SPLADEv2,
PAIR, and RocketQAv2 to achieve higher qual-
ity than vanilla ColBERT. These supervision gains
challenge the value of ﬁne-grained late interaction,
and it is not inherently clear whether the stronger
inductive biases of ColBERT-like models permit it
to accept similar gains under distillation, especially
when using compressed representations. Despite
this, we ﬁnd that with denoised supervision and
residual compression, ColBERTv2 achieves the
highest quality across all systems. As we discuss
in §5.3, it exhibits space footprint competitive with
these single-vector models and much lower than
vanilla ColBERT.
Besides the ofﬁcial dev set, we evaluated Col-
BERTv2, SPLADEv2, and RocketQAv2 on the
“Local Eval” test set described by Khattab and Za-
haria (2020) for MS MARCO, which consists of
5000 queries disjoint with the training and the of-
ﬁcial dev sets. These queries are obtained from
labeled 50k queries that are provided in the ofﬁcial
MS MARCO Passage Ranking task as additional
validation data.On this test set, ColBERTv2 ob-
tains 40.8% MRR@10, considerably outperform-
ing the baselines, including RocketQAv2 which
makes use of document titles in addition to the
passage text unlike the other systems.3721
5.2 Out-of-Domain Retrieval Quality
Next, we evaluate ColBERTv2 outside the train-
ing domain using BEIR (Thakur et al., 2021),
Wikipedia Open QA retrieval as in Khattab et al.
(2021b), and LoTTE. We compare against a wide
range of recent and state-of-the-art retrieval sys-
tems from the literature.
BEIR. We start with BEIR, reporting the quality
of models that do not incorporate distillation from
cross-encoders, namely, ColBERT (Khattab and
Zaharia, 2020), DPR-MARCO (Xin et al., 2021),
ANCE (Xiong et al., 2020), and MoDIR (Xin et al.,
2021), as well as models that do utilize distil-
lation, namely, TAS-B (Hofstätter et al., 2021),
SPLADEv2 (Formal et al., 2021a), and also Rock-
etQAv2, which we test ourselves using the ofﬁcial
checkpoint trained on MS MARCO. We divide
the table into “search” (i.e., natural queries and
questions) and “semantic relatednes” (e.g., citation-
relatedness and claim veriﬁcation) tasks to reﬂect
the nature of queries in each dataset.
Table 5a reports results with the ofﬁcial
nDCG@10 metric. Among the models with-out distillation, we see that the vanilla ColBERT
model outperforms the single-vector systems DPR,
ANCE, and MoDIR across all but three tasks. Col-
BERT often outpaces all three systems by large
margins and, in fact, outperforms the TAS-B model,
which utilizes distillation, on most datasets. Shift-
ing our attention to models with distillation, we see
a similar pattern: while distillation-based models
are generally stronger than their vanilla counter-
parts, the models that decompose scoring into term-
level interactions, ColBERTv2 and SPLADEv2,
are almost always the strongest.
Looking more closely into the comparison be-
tween SPLADEv2 and ColBERTv2, we see that
ColBERTv2 has an advantage on six benchmarks
and ties SPLADEv2 on two, with the largest im-
provements attained on NQ, TREC-COVID, and
FiQA-2018, all of which feature natural search
queries. On the other hand, SPLADEv2 has the
lead on ﬁve benchmarks, displaying the largest
gains on Climate-FEVER (C-FEVER) and Hot-
PotQA. In C-FEVER, the input queries are sen-
tences making climate-related claims and, as a re-
sult, do not reﬂect the typical characteristics of
search queries. In HotPotQA, queries are written
by crowdworkers who have access to the target pas-3722sages. This is known to lead to artiﬁcial lexical
bias (Lee et al., 2019), where crowdworkers copy
terms from the passages into their questions as in
the Open-SQuAD benchmark.
Wikipedia Open QA. As a further test of out-
of-domain generalization, we evaluate the MS
MARCO-trained ColBERTv2, SPLADEv2, and
vanilla ColBERT on retrieval for open-domain
question answering, similar to the out-of-domain
setting of Khattab et al. (2021b). We report
Success@5 (sometimes referred to as Recall@5),
which is the percentage of questions whose short
answer string overlaps with one or more of the
top-5 passages. For the queries, we use the de-
velopment set questions of the open-domain ver-
sions (Lee et al., 2019; Karpukhin et al., 2020) of
Natural Questions (NQ; Kwiatkowski et al. 2019),
TriviaQA (TQ; Joshi et al. 2017), and SQuAD (Ra-
jpurkar et al., 2016) datasets in Table 5b. As a
baseline, we include the BM25 (Robertson et al.,
1995) results using the Anserini (Yang et al., 2018a)
toolkit. We observe that ColBERTv2 outperforms
BM25, vanilla ColBERT, and SPLADEv2 across
the three query sets, with improvements of up to
4.6 points over SPLADEv2.
LoTTE. Next, we analyze performance on the
LoTTE test benchmark, which focuses on natural
queries over long-tail topics and exhibits a different
annotation pattern to the datasets in the previous
OOD evaluations. In particular, LoTTE uses auto-
matic Google rankings (for the “search” queries)
and organic StackExchange question–answer pairs
(for “forum” queries), complimenting the pooling-
based annotation of datasets like TREC-COVID (in
BEIR) and the answer overlap metrics of Open-QA
retrieval. We report Success@5 for each corpus on
both search queries and forum queries.
Overall, we see that ANCE and vanilla Col-
BERT outperform BM25 on all topics, and that
the three methods using distillation are generally
the strongest. Similar to the Wikipedia-OpenQA
results, we ﬁnd that ColBERTv2 outperforms the
baselines across all topics for both query types, im-
proving upon SPLADEv2 and RocketQAv2 by up
to 3.7 and 8.1 points, respectively. Considering
the baselines, we observe that while RocketQAv2
tends to have a slight advantage over SPLADEv2
on the “search” queries, SPLADEv2 is consider-
ably more effective on the “forum” tests. We hy-
pothesize that the search queries, obtained from
Google (through GooAQ) are more similar to MSMARCO than the forum queries and, as a result,
the latter stresses generalization more heavily, re-
warding term-decomposed models like SPLADEv2
and ColBERTv2.
5.3 Efﬁciency
ColBERTv2’s residual compression approach sig-
niﬁcantly reduces index sizes compared to vanilla
ColBERT. Whereas ColBERT requires 154 GiB
to store the index for MS MARCO, ColBERTv2
only requires 16 GiB or 25 GiB when compressing
embeddings to 1 or 2 bit(s) per dimension, respec-
tively, resulting in compression ratios of 6–10 ×.
This storage ﬁgure includes 4.5 GiB for storing the
inverted list.
This matches the storage for a typical single-
vector model on MS MARCO, with 4-byte lossless
ﬂoating-point storage for one 768-dimensional vec-
tor for each of the 9M passages amounting to a little
over 25 GiBs. In practice, the storage for a single-
vector model could be even larger when using a
nearest-neighbor index like HNSW for fast search.
Conversely, single-vector representations could be
themselves compressed very aggressively (Zhan
et al., 2021a, 2022), though often exacerbating the
loss in quality relative to late interaction methods
like ColBERTv2.
We discuss the impact of our compression
method on search quality in Appendix B and
present query latency results on the order of 50–
250 milliseconds per query in Appendix C.
6 Conclusion
We introduced ColBERTv2, a retriever that ad-
vances the quality and space efﬁciency of multi-
vector representations. We hypothesized that clus-
ter centroids capture context-aware semantics of
the token-level representations and proposed a
residual representation that leverages these patterns
to dramatically reduce the footprint of multi-vector
systems off-the-shelf . We then explored improved
supervision for multi-vector retrieval and found
that their quality improves considerably upon distil-
lation from a cross-encoder system. The proposed
ColBERTv2 considerably outperforms existing re-
trievers in within-domain and out-of-domain evalu-
ations, which we conducted extensively across 28
datasets, establishing state-of-the-art quality while
exhibiting competitive space footprint.3723Acknowledgements
This research was supported in part by afﬁliate
members and other supporters of the Stanford
DAWN project—Ant Financial, Facebook, Google,
and VMware—as well as Cisco, SAP, Virtusa, and
the NSF under CAREER grant CNS-1651570. Any
opinions, ﬁndings, and conclusions or recommen-
dations expressed in this material are those of the
authors and do not necessarily reﬂect the views of
the National Science Foundation.
Broader Impact & Ethical Considerations
This work is primarily an effort toward retrieval
models that generalize better while performing
reasonably efﬁciently in terms of space consump-
tion. Strong out-of-the-box generalization to small
domain-speciﬁc applications can serve many users
in practice, particularly where training data is not
available. Moreover, retrieval holds signiﬁcant
promise for many downstream NLP tasks, as it
can help make language models smaller and thus
more efﬁcient (i.e., by decoupling knowledge from
computation), more transparent (i.e., by allowing
users to check the sources the model relied on when
making a claim or prediction), and easier to update
(i.e., by allowing developers to replace or add doc-
uments to the corpus without retraining the model)
(Guu et al., 2020; Borgeaud et al., 2021; Khattab
et al., 2021a). Nonetheless, such work poses risks
in terms of misuse, particularly toward misinforma-
tion, as retrieval can surface results that are relevant
yet inaccurate, depending on the contents of a cor-
pus. Moreover, generalization from training on
a large-scale dataset can propagate the biases of
that dataset well beyond its typical reach to new
domains and applications.
While our contributions have made ColBERT’s
late interaction more efﬁcient at storage costs, large-
scale distillation with hard negatives increases sys-
tem complexity and accordingly increases train-
ing cost, when compared with the straightforward
training paradigm of the original ColBERT model.
While ColBERTv2 is efﬁcient in terms of latency
and storage at inference time, we suspect that un-
der extreme resource constraints, simpler model de-
signs like SPLADEv2 or RocketQAv2 could lend
themselves to easier-to-optimize environments. We
leave low-level systems optimizations of all sys-
tems to future work. Another worthwhile di-
mension for future exploration of tradeoffs is re-
ranking architectures over various systems withcross-encoders, which are known to be expensive
yet precise due to their highly expressive capacity.
Research Limitations
While we evaluate ColBERTv2 on a wide range of
tests, all of our benchmarks are in English and, in
line with related work, our out-of-domain tests eval-
uate models that are trained on MS MARCO. We
expect our approach to work effectively for other
languages and when all models are trained using
other, smaller training set (e.g., NaturalQuestions),
but we leave such tests to future work.
We have observed consistent gains for Col-
BERTv2 against existing state-of-the-art systems
across many diverse settings. Despite this, almost
all IR datasets contain false negatives (i.e., rele-
vant but unlabeled passages) and thus some cau-
tion is needed in interpreting any individual result.
Nonetheless, we intentionally sought out bench-
marks with dissimilar annotation biases: for in-
stance, TREC-COVID (in BEIR) annotates the
pool of documents retrieved by the systems submit-
ted at the time of the competition, LoTTE uses au-
tomatic Google rankings (for “search” queries) and
StackExchange question–answer pairs (for “forum”
queries), and the Open-QA tests rely on passage-
answer overlap for factoid questions. ColBERTv2
performed well in all of these settings. We discuss
other issues pertinent to LoTTE in Appendix §D.
We have compared with a wide range of strong
baselines—including sparse retrieval and single-
vector models—and found reliable patterns across
tests. However, we caution that empirical trends
can change as innovations are introduced to each of
these families of models and that it can be difﬁcult
to ensure exact apple-to-apple comparisons across
families of models, since each of them calls for
different sophisticated tuning strategies. We thus
primarily used results and models from the rich
recent literature on these problems, with models
like RocketQAv2 and SPLADEv2.
On the representational side, we focus on reduc-
ing the storage cost using residual compression,
achieving strong gains in reducing footprint while
largely preserving quality. Nonetheless, we have
not exhausted the space of more sophisticated opti-
mizations possible, and we would expect more so-
phisticated forms of residual compression and com-
posing our approach with dropping tokens (Zhou
and Devlin, 2021) to open up possibilities for fur-
ther reductions in space footprint.3724References3725372637273728
A Analysis of ColBERT’s Semantic
Space
ColBERT (Khattab and Zaharia, 2020) decomposes
representations and similarity computation at the
token level. Because of this compositional archi-
tecture, we hypothesize that ColBERT exhibits a
“lightweight” semantic space: without any special
re-training, vectors corresponding to each sense of
a word would cluster very closely, with only minor
variation due to context.
If this hypothesis is true, we would expect the
embeddings corresponding to each token in the
vocabulary to localize in only a small number of
regions in the embedding space, corresponding
to the contextual “senses” of the token. To val-
idate this hypothesis, we analyze the ColBERT
embeddings corresponding to the tokens in the
MS MARCO Passage Ranking (Nguyen et al.,
2016) collection: we perform k-means clustering
on the nearly 600M embeddings—corresponding
to 27,000 unique tokens—into k= 2clusters.
As a baseline, we repeat this clustering with ran-
dom embeddings but keep the true distribution of
tokens. Figure 2 presents empirical cumulative dis-
tribution function (eCDF) plots representing the
number of distinct non-stopword tokens appear-
ing in each cluster (2a) and the number of distinct
clusters in which each token appears (2b).Most
tokens appear in a very small fraction of the num-
ber of centroids: in particular, we see that roughly
90% of clusters have ≤16 distinct tokens withthe ColBERT embeddings, whereas less than 50%
of clusters have≤16 distinct tokens with the ran-
dom embeddings. This suggests that the centroids
effectively map the ColBERT semantic space.
Table 6 presents examples to highlight the se-
mantic space captured by the centroids. The most
frequently appearing tokens in cluster #917 relate
to photography; these include, for example, ‘pho-
tos’ and ‘photographs’. If we then examine the
additional clusters in which these tokens appear,
we ﬁnd that there is substantial semantic overlap
between these new clusters (e.g., Photos-Photo,
Photo-Image-Picture) and cluster #917. We ob-
serve a similar effect with tokens appearing in clus-
ter #216932, comprising tornado-related terms.
This analysis indicates that cluster centroids can
summarize the ColBERT representations with high
precision. In §3.3, we propose a residual compres-
sion mechanism that uses these centroids along
with minor reﬁnements at the dimension level to
efﬁciently encode late-interaction vectors.
B Impact of Compression
Our residual compression approach (§3.3) pre-
serves approximately the same quality as the un-
compressed embeddings. In particular, when ap-
plied to a vanilla ColBERT model on MS MARCO
whose MRR@10 is 36.2% and Recall@50 is
82.1%, the quality of the model with 2-bit compres-
sion is 36.2% MRR@10 and 82.3% Recall@50.
With 1-bit compression, the model achieves 35.5%
MRR@10 and 81.6% Recall@50.
We also tested the residual compression ap-
proach on late-interaction retrievers that conduct
downstream tasks, namely, ColBERT-QA (Khat-
tab et al., 2021b) for the NaturalQuestions open-
domain QA task, and Baleen (Khattab et al., 2021a)
for multi-hop reasoning on HoVer for claim veriﬁ-
cation. On the NQ dev set, ColBERT-QA’s suc-
cess@5 (success@20) dropped only marginally
from 75.3% (84.3%) to 74.3% (84.2%) and
its downstream Open-QA answer exact match
dropped from 47.9% to 47.7%, when using 2-bit
compression for retrieval and using the same check-
points of ColBERT-QA otherwise.37293730Similarly, on the HoVer (Jiang et al., 2020) dev
set, Baleen’s retrieval R@100 dropped from 92.2%
to only 90.6% but its sentence-level exact match
remained roughly the same, going from 39.2% to
39.4%. We hypothesize that the supervision meth-
ods applied in ColBERTv2 (§3.2) can also be ap-
plied to lift quality in downstream tasks by improv-
ing the recall of retrieval for these tasks. We leave
such exploration for future work.
C Retrieval Latency
Figure 3 evaluates the latency of ColBERTv2
across three collections of varying sizes, namely,
MS MARCO, LoTTE Pooled (dev), and LoTTE
Lifestyle (dev), which contain approximately 9M
passages, 2.4M answer posts, and 270k answer
posts, respectively. We average latency across three
runs of the MS MARCO dev set and the LoTTE
“search” queries. Search is executed using a Titan
V GPU on a server with two Intel Xeon Gold 6132
CPUs, each with 28 hardware execution contexts.
The ﬁgure varies three settings of ColBERTv2.
In particular, we evaluate indexing with 1-bit and
2-bit encoding (§3.4) and searching by probing the
nearest 1, 2, or 4 centroids to each query vector
(§3.5). When probing probe centroids per vector,
we score either probe×2orprobe×2candi-
dates per query.
To begin with, we notice that the quality reported
on the x-axis varies only within a relatively narrow
range. For instance, the axis ranges from 38.50
through 39.75 for MS MARCO, and all but two of
the cheapest settings score above 39.00. Similarly,
they-axis varies between approximately 50 mil-
liseconds per query up to 250 milliseconds (mostly
under 150 milliseconds) using our relatively simple
Python-based implementation.
Digging deeper, we see that the best quality
in these metrics can be achieved or approached
closely with around 100 milliseconds of latency
across all three datasets, despite their various sizes
and characteristics, and that 2-bit indexing reliably
outperforms 1-bit indexing but the loss from more
aggressive compression is small.
D LoTTE
Domain coverage Table 9 presents the full dis-
tribution of communities in the LoTTE dev dataset.
The topics covered by LoTTE cover a wide range
of linguistic phenomena given the diversity in top-
ics and communities represented. However, since
all posts are submitted by anonymous users we do
not have demographic information regarding the
identify of the contributors. All posts are written
in English.
Passages As mentioned in §4, we construct
LoTTE collections by selecting passages from the
StackExchange archive with positive scores. We
remove HTML tags from passages and ﬁlter out
empty passages. For each passage we record its
corresponding query and save the query-to-passage
mapping to keep track of the posted answers corre-
sponding to each query.
Search queries We construct the list of LoTTE
search queries by drawing from GooAQ queries
that appear in the StackExchange post archive. We
ﬁrst shufﬂe the list of GooAQ queries so that in
cases where multiple queries exist for the same
answer passage we randomly select the query to
include in LoTTE rather than always selecting the
ﬁrst appearing query. We verify that every query
has at least one corresponding answer passage.
Forum queries For each LoTTE topic and its
constituent communities we ﬁrst compute the frac-
tion of the total queries attributed to each individ-
ual community. We then use this distribution to
construct a truncated query set by selecting the3731
highest ranked queries from each community as
determined by 1) the query scores and 2) the query
view counts. We only use queries which have an
accepted answer. We ensure that each community
contributes at least 50 queries to the truncated set
whenever possible. We set the overall size of the
truncated set to be 2000 queries, though note that
the total can exceed this due to rounding and/or the
minimum per-community query count. We remove
all quotation marks and HTML tags.
Statistics Figure 4 plots the number of words
per passage in each LoTTE dev corpus. Figures 5
and 6 plot the number of words and number of
corresponding answer passages respectively per
query, split across search and forum queries.
Dev Results Table 7 presents out-of-domain eval-
uation results on the LoTTE dev queries. Continu-
ing the trend we observed in 5, ColBERTv2 consis-
tently outperforms all other models we tested.Licensing and Anonymity The original Stack-
Exchange post archive is licensed under a Cre-
ative Commons BY-SA 4.0 license (sta). Personal
data is removed from the archive before being up-
loaded, though all posts are public; when we re-
lease LoTTE publicly we will include URLs to the
original posts for proper attribution as required by
the license. The GooAQ dataset is licensed under
an Apache license, version 2.0 (Khashabi et al.,
2021). We will also release LoTTE with a CC BY-
SA 4.0 license. The search queries can be used for
non-commercial research purposes only as per the
GooAQ license.
E Datasets in BEIR
Table 8 lists the BEIR datasets we used in our evalu-
ation, including their respective license information
as well as the numbers of documents as well as the
number of test set queries. We refer to Thakur et al.
(2021) for a more detailed description of each of
the datasets.
Our Touché evaluation uses an updated version
of the data in BEIR, which we use for evaluating the
models we run (i.e., ColBERTv2 and RocketQAv2)
as well as SPLADEv2.
We also tested on the Open-QA benchmarks NQ,
TQ, and SQuAD, each of which has approximately
9k dev-set questions and muli-hop HoVer, whose
development set has 4k claims. In the compression
evaluation §B, we used models trained in-domain
on NQ and HoVer, whose training sets contain 79k
and 18k queries, respectively.
F Implementation & Hyperparameters
We implement ColBERTv2 using Python 3.7,
PyTorch 1.9, and HuggingFace Transformers
4.10 (Wolf et al., 2020), extending the original im-
plementation of ColBERT by Khattab and Zaharia
(2020). We use FAISS 1.7 (Johnson et al., 2019) for3732k-means clustering,though unlike ColBERT we
do not use it for nearest-neighbor search. Instead,
we implement our candidate generation mechanism
(§3.5) using PyTorch primitives in Python.
We conducted our experiments on an internal
cluster, typically using up to four 12GB Titan V
GPUs for each of the inference tasks (e.g., index-
ing, computing distillation scores, and retrieval)
and four 80GB A100 GPUs for training, though
GPUs with smaller RAM can be used via gradient
accumulation. Using this infrastructure, computing
the distillation scores takes under a day, training a
64-way model on MS MARCO for 400,000 steps
takes around ﬁve days, and indexing takes approx-
imately two hours. We very roughly estimate an
upper bound total of 20 GPU-months for all experi-
mentation, development, and evaluation performed
for this work over a period of several months.
Like ColBERT, our encoder is a
bert-base-uncased model that is shared
between the query and passage encoders and which
has 110M parameters. We retain the default vector
dimension suggested by Khattab and Zaharia
(2020) and used in subsequent work, namely,
d=128. For the experiments reported in this paper,
we train on MS MARCO training set. We use
simple defaults with limited manual exploration on
the ofﬁcial development set for the learning rate
(10), batch size (32 examples), and warm up
(for 20,000 steps) with linear decay.
Hyperparameters corresponding to retrieval are
explored in §C. We default to probe = 2, but
useprobe = 4 on the largest datasets, namely,
MS MARCO and Wikipedia. By default we set
candidates =probe∗2, but for Wikipedia
we set candidates =probe∗2and for MS
MARCO we set candidates =probe∗2. We
leave extensive tuning of hyperparameters to future
work.
We train on MS MARCO using 64-way tuples
for distillation, sampling them from the top-500
retrieved passages per query. The training set of
MS MARCO contains approximately 800k queries,
though only about 500k have associated labels. We
apply distillation using all 800k queries, where
each training example contains exactly one “posi-
tive”, deﬁned as a passage labeled as positive or the
top-ranked passage by the cross-encoder teacher,
irrespective of its label.
We train for 400k steps, initializing from a pre-ﬁnetuned checkpoint using 32-way training exam-
ples and 150k steps. To generate the top- kpas-
sages per training query, we apply two rounds, fol-
lowing Khattab et al. (2021b). We start from a
model trained with hard triples (akin to Khattab
et al. (2021b)), train with distillation, and then use
the distilled model to retrieve for the second round
of training. Preliminary experiments indicate that
quality has low sensitivity to this initialization and
two-round training, suggesting that both of them
could be avoided to reduce the cost of training.
Unless otherwise stated, the results shown rep-
resent a single run. The latency results in §3 are
averages of three runs. To evaluate for Open-QA re-
trieval, we use evaluation scripts from Khattab et al.
(2021b), which checks if the short answer string
appears in the (titled) Wikipedia passage. This
adapts the DPR (Karpukhin et al., 2020) evaluation
code.We use the preprocessed Wikipedia Dec
2018 dump released by Karpukhin et al. (2020).
For out-of-domain evaluation, we elected to fol-
low Thakur et al. (2021) and set the maximum
document length of ColBERT, RocketQAv2, and
ColBERTv2 to 300 tokens on BEIR and LoTTE.
Formal et al. (2021a) selected maximum sequence
length 256 for SPLADEv2 both on MS MARCO
and on BEIR for both queries and documents, and
we retained this default when testing their system
on LoTTE. Unless otherwise stated, we keep the
default query maximum sequence length for Col-
BERTv2 and RocketQAv2, which is 32 tokens. For
the ArguAna test in BEIR, as the queries are them-
selves long documents, we set the maximum query
length used by ColBERTv2 and RocketQAv2 to
300. For Climate-FEVER, as the queries are rela-
tively long sentence claims, we set the maximum
query length used by ColBERTv2 to 64.
We use the open source BEIR implementation
and SPLADEv2 evaluationcode as the basis for
our evaluations of SPLADEv2 and ANCE as well
as for BM25 on LoTTE. We use the Anserini (Yang
et al., 2018a) toolkit for BM25 on the Wikipedia
Open-QA retrieval tests as in Khattab et al. (2021b).
We use the implementation developed by the Rock-
etQAv2 authors for evaluating RocketQAv2.37333734