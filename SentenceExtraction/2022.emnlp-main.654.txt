
David Wan andMohit Bansal
University of North Carolina at Chapel Hill
{davidwan,mbansal}@cs.unc.edu
Abstract
Current metrics for evaluating factuality for
abstractive document summarization have
achieved high correlations with human judg-
ment, but they do not account for the vision
modality and thus are not adequate for vision-
and-language summarization. We propose
CLIPBERTS , a simple weighted com-
bination of CLIPScore (Hessel et al., 2021)
and BERTScore (Zhang* et al., 2020) to lever-
age the robustness and strong factuality detec-
tion performance between image-summary and
document-summary, respectively. Next, due
to the lack of meta-evaluation benchmarks to
evaluate the quality of multimodal factuality
metrics, we collect human judgments of fac-
tuality with respect to documents and images.
We show that this simple combination of two
metrics in the zero-shot setting achieves higher
correlations than existing factuality metrics for
document summarization, outperforms an ex-
isting multimodal summarization metric, and
performs competitively with strong multimodal
factuality metrics specifically fine-tuned for
the task. Our thorough analysis demonstrates
the robustness and high correlation of CLIP-
BERTS and its components on four fac-
tuality metric-evaluation benchmarks. Finally,
we demonstrate two practical downstream ap-
plications of our CLIPBERTS metric:
for selecting important images to focus on dur-
ing training, and as a reward for reinforcement
learning to improve factuality of multimodal
summary generation w.r.t automatic and human
evaluation.
1 Introduction
Multimodal abstractive summarization is the task
of generating an abridged text that contains the
most important information of the source inputs
from various modalities. This challenging task
builds upon the success of document summariza-
tion, where the input is only text documents. Fordocument summarization, there has been tremen-
dous progress in improving the quality of the sum-
maries with the help of large pre-trained models
(Lewis et al., 2020; Zhang et al., 2020; Raffel et al.,
2020). However, one crucial problem for such
models is hallucination, where the model gener-
ates contents that are not present or entailed by the
document (Maynez et al., 2020; Falke et al., 2019).
While there have been significant advancements
in developing metrics that correlate highly with the
human judgment of factuality (Kryscinski et al.,
2020; Durmus et al., 2020; Goyal and Durrett,
2021; Scialom et al., 2021), these metrics only
measure factuality between the document and the
summary. The lack of judgment between other
modalities, such as vision, and the summary makes
such metrics not suitable for multimodal settings.
We demonstrate this with the example in Figure 1.
The given summary captures less relevant informa-
tion (cutting the nail) from the document, but it is
still considered factual to the document. However,
the image shows the main point of the document
(finding the place where the nail separates from
the quick), making the summary not factual with
respect to the image. Current factuality metrics do
not account for the image and thus cannot correctly
assess factuality for multimodal summaries.
In this work, we introduce a metric that judges
factuality of the summary with respect to each in-
put modality. Focusing on the vision-and-language
summarization, we propose CLIPBERTS , a
simple and robust automatic factuality evaluation
metric for multimodal summaries that combines
two successful metrics: CLIPScore (Hessel et al.,
2021), which shows strong performance in detect-
ing hallucinations between image and text, and
BERTScore (Zhang* et al., 2020), which correlates
well with the human judgment of factuality for doc-
ument summarization (Pagnoni et al., 2021).
Next, due to the lack of corpora containing
ground-truth human factuality judgments to eval-9632uate multimodal factuality metrics via correlation
with human evaluation, we propose a Multimodal
Factuality Meta-Evaluation ( MFME) bench-
mark by collecting human annotation for four sum-
marization systems and the reference summary on
WikiHow,a large collection of how-to articles
containing rich images relevant to the document.
We find that our simple CLIPBERTS met-
ric, which combines two off-the-shelf metrics in
the zero-shot setting, achieves higher correlation
with human judgment over existing text-based fac-
tuality metrics, outperforms current multimodal
summarization metric MMAE (Zhu et al., 2018),
and performs competitively with a metric trained
with a triplet loss specifically for this task.
Next, we perform a detailed analysis of CLIP-
BERTS by evaluating the correlation of the
metric and each of its modules on four additional
factuality metric-evaluation benchmarks. We first
propose the WikiHow Factuality (WikiHowFact)
task, derived from the Visual Goal-Step Inference
task (Yang et al., 2021). This ranking experiment
measures how well the metric can select the cor-
rect summary from four choices given the correct
document and image. Since hallucinations are
also present for image-captioning task (Xiao and
Wang, 2021), we also evaluate the correlations on
two captioning tasks focusing on hallucinations,
FOIL (Shekhar et al., 2017) and BISON (Hu et al.,
2019), and one document summarization factuality
benchmark FRANK (Pagnoni et al., 2021). Across
all these tasks, we demonstrate the robustness of
CLIPBERTS and its components, as they
achieve the highest correlations compared to strong
baselines in each of its respective settings.
Lastly, we present two practical applications
for improving the factuality of downstream
multimodal summarization models using CLIP-
BERTS : (1) Selecting the most important
images as visual guidance (Zhu et al., 2020), and
(2) Using the metric as a reward for self-critical
sequence training (Rennie et al., 2017). Our re-
sults indicate that both applications improve the
factuality of the generated summaries across two
multimodal summarization datasets, as measured
by three factuality metrics and human evaluation.
To summarize, our contributions are:
1.We propose a simple and robust factuality met-
ric for multimodal summarization based on a
combination of CLIPScore and BERTScore.
2.We create MFME, a meta-evaluation for fac-
tuality of multimodal summarization, and the
WikiHowFact task to evaluate the quality of mul-
timodal factuality metrics.
3.We present a detailed study of our metric and
its components on various factuality metric-
evaluation benchmarks and present strong em-
pirical evidence of its robustness.
4.We demonstrate two useful downstream applica-
tions of our metric to improve the factuality of
multimodal abstractive summarization models.
2 CLIPBERTS
CLIPBERTS consists of two parts that
tackle the image-summary and document-summary
factuality judgments, respectively. We show an
illustration of the computation in Figure 1.
Image-Summary. We use a variant of CLIP-
Score (Hessel et al., 2021) for evaluating the factu-
ality between images and the summary. This metric
is based on CLIP (Radford et al., 2021), a cross-
modal model trained on 400M image and caption
pairs with InfoNCE (Oord et al., 2018) contrastive
loss. Hessel et al. (2021) finds that using this off-
the-shelf model as a metric achieves the highest
human correlation on the FOIL (Shekhar et al.,
2017) benchmark that explores how well the metric
can detect hallucinations present in the captions.9633Thus, it serves as a fitting candidate for factuality
evaluation between the image and the summary.
We use CLIP-S, which calculates the cosine sim-
ilarity between the image embedding vand the text
embedding of the summary sentence t. To adapt
to multimodal summarization, where we have mul-
tiple images and multi-sentence summaries,we
take the average of the scores of all image and
sentence pairs. Formally, given a list of image em-
beddings Vand summary sentence embeddings T
from CLIP’s image and text encoder, respectively:
CLIP-S (V, T) =1
|V||T|/summationdisplay/summationdisplaycossim (v, t)
Document-Summary. To better detect halluci-
nations present in the summary with respect to
the document, we use the precision variant of
BERTScore (Zhang* et al., 2020), which achieves
high correlations with human judgments of factu-
ality for document summarization (Pagnoni et al.,
2021). See Section 4.4 for a detailed discussion and
comparison against other text-based factuality met-
rics. Formally, given the contextual embeddings of
each token in the document Dand summary S, it
calculates the pairwise cosine similarity between
each document and summary token embeddings:
BERT-S =1
|S|/summationdisplaymaxcossim (d, s)
Full Metric. The final score is a combination of
the factuality score for image-summary with CLIP-
S and that for document-summary with BERT-S:
CLIPBERTS =αCLIP-S +(1−α)BERT-S ,
where αis a tunable parameter. Please see Sec-
tion 3.4 for other ways to learn this combination.
3 Metric Meta-Evaluations
Next, after defining the multimodal factuality met-
ricCLIPBERTS , we want to evaluate the
quality of this new metric by checking whether it
correlates with human judgments, similar to what
has been done for textual factuality metrics (Wang
et al., 2020; Kryscinski et al., 2020; Durmus et al.,
2020; Scialom et al., 2021). As there is no human
annotations of factuality for multimodal summa-
rization, we first propose a Multimodal FactualityMeta-Evaluation ( MFME) benchmark derived
from WikiHow to test the correlations of CLIP-
BERTS with human judgments of factuality.
3.1 MFME
Dataset. We construct an English multimodal
WikiHow summarization dataset (Koupaee and
Wang, 2018) for the human evaluation, as this
datasets has been extensively studied for document
summarization (Koupaee and Wang, 2018; Ladhak
et al., 2020), and the images associated with the
how-to-articles are relevant to the text. We use
a recent WikiHow collection effort by Yang et al.
(2021) containing images.We generate the step-
level multimodal WikiHow dataset by breaking
each article into steps and following the construc-
tion described in Koupaee and Wang (2018): We
consider the first sentence of a step as the summary
and the rest of the paragraph as the document, and
add the corresponding image. We randomly select
6,000 articles as the validation and test set each,
and break each example into steps.Statistics of
the dataset can be found in Table 16 of the Ap-
pendix. For annotation, we randomly sample 50
articles from the test set, and evaluate the generated
summaries for all the corresponding steps. Similar
to Kryscinski et al. (2020), we split the 50 articles
into 10 articles as the validation and 40 as the test
set, resulting in 52 and 193 step-level examples for
the validation and test set, respectively.
Summarization Systems. Following Pagnoni
et al. (2021), we include model summaries from
summarization models with varying factuality ca-
pabilities. We train four abstractive summarization
systems on the multimodal WikiHow dataset, in-
cluding two document summarization models, T5
(Raffel et al., 2020) and PEGASUS (Zhang et al.,
2020), and two multimodal summarization models,
CLIP-BART (see section 5), and MOF (Zhu et al.,
2018). Details of the models are provided in Ap-
pendix A.2. We additionally include the reference
summaries, resulting in a total of 260 and 965 ex-
amples for the validation and test set, respectively.9634Annotations. We conduct the annotations on
Amazon Mechanical Turk(AMT) platform. For
each HIT, we provide the document and the image
and ask the workers to read the five summaries.
The workers then need to choose whether each
summary is faithful to the document and the im-
age separately. An example of the annotation page
can be seen in Appendix A.3. For high-quality
annotations, we first conduct a qualification test,
where we compare the annotations from the work-
ers against annotations by the authors. Only the
workers who have the same annotations on the se-
lected example can perform the actual annotation
task. We further select workers from the United
States, who have more than 10,000 HITs approved
and an approval rate greater than 98%. We pay 0.18
USD per task to ensure a >$12hourly rate. Each
task consists of three unique workers, and we take
the majority class for the document and image fac-
tuality judgments, similar to Pagnoni et al. (2021).
We consider the summary to be faithful only if it
is considered faithful to both document and image.
We also experiment beyond binary judgment by
taking the average over the two factuality judgment
to indicate a summary may be partially faithful to
one of the source, which is shown in Appendix B.
Inter-Annotator Agreement. We report Fleiss
Kappa κ(Fleiss, 1971) and percentage pof anno-
tators agreement on the majority class, similar to
Durmus et al. (2020). We obtain κ= 0.50, with
p= 88.5%, indicating a moderate agreement (Lan-
dis and Koch, 1977).
3.2 Experimental Setup
CLIPBERTS .For CLIP-S, we use the
RN50x64 visual backbone instead of the ViT-B/32
version used in the original metric, as the larger
backbone shows a higher correlation on fac-
tuality benchmarks. For BERT-S, we choose
RoBERTa-large-mnli to compute the contextual-
ized embeddings instead of RoBERTa-large for the
same reason. We refer readers to Section 4 for more
details. We use the validation set of MFMEto
tuneα, where we find that α= 0.25achieves the
best correlations on the combined judgment. We
use this parameter for all experiments (See Sec-
tion 3.4 for other ways to learn this combination).
Baseline Metrics. Having separate judgments for
document-summary, image-summary, and multi-
modal settings allows us to evaluate the metrics’
performance with different modality combinations.
For document-summary, we compare against exist-
ing factuality metrics, including FactCC (Kryscin-
ski et al., 2020), DAE (Goyal and Durrett, 2021),
QuestEval (Scialom et al., 2021), and the original
BERTScore (Zhang* et al., 2020). We also measure
the performance of the text-matching component
of CLIPScore, which we refer to as CLIPText-S.
For image-summary evaluation, we compare our
CLIP-S against Triplet Network, as described in
Yang et al. (2021). We train this metric on the
multimodal WikiHow dataset, allowing compar-
isons of correlations between CLIP-S in the zero-
shot setting and a fine-tuned metric for this task.
For multimodal factuality metrics, we experiment
with several weighted combinations of document-
summary and image-summary metrics by tuning
the weights on the validation set, including com-
binations of DAE with CLIP-S, Triplet Network
with BERT-S, and RefCLIP-S. We also compare to
MMAE (Zhu et al., 2018) developed for evaluating
the summary quality of multimodal summarization.
As the metric is originally designed for a different
dataset, we similarly use the multimodal WikiHow
to train its image-summary component IM. We
refer the readers to Appendix A.1 for details of the
metrics.9635
3.3 Meta-Evaluation Results
Table 1 shows the Pearson correlation of the au-
tomatic metrics. We first note that the combined
judgments require taking both modalities into con-
sideration. Metrics that only consider the docu-
ment correlate less with the combined judgment
than with the document-only judgment, indicating
the importance of capturing the vision modality for
evaluating factuality for multimodal summariza-
tion. Multimodal factuality metrics, on the other
hand, show positive transfers, as they correlate
higher on all three settings than its components.
Next, for the document-summary factuality judg-
ments, BERT-S achieves the highest correlation,
outperforming DAE by 8 points and the original
BERTScore by 4 points. Compared to MMAE,
which is developed for evaluating the quality of
multimodal summarization, CLIPBERTS
significantly outperforms on all three categories,
showing the importance of targeting the factual-
ity aspect. While Triplet-Net achieves better cor-
relations on image, CLIPBERTS actually
outperforms the fine-tuned variants for the docu-
ment case and provides the same correlations on
the combined case. We thus stress the simplicity
ofCLIPBERTS of only requiring the use
of two off-the-shelf metrics in the zero-shot set-
ting without the need for extra training to compare
competitively with fine-tuned method.
3.4 Comparison of Combination Strategies
While CLIPBERTS usesαto decide the
weights for CLIP-S and BERT-S, we also explore
using logistic regression (logis) and multi-layer
perceptron (MLP) to output a final score given
the two modules, following Zhu et al. (2018).
Similar to the αparameter, we tune the two meth-
ods by fitting the metric on the combined human
judgment scores and selecting the parameters that
would achieve the highest Pearson correlation on
the development set of MFMEmeta-evaluation
dataset.The result is presented in Table 2. While
logistic regression performs the worst, using MLP
for combining the two modules provides similar
correlations as CLIPBERTS that uses the α
parameter. Specifically, MLP provides a point in-
crease in correlations with respect to the document
but provides the same correlations on the combined
judgment. The weight combination strategies can
be chosen based on preference, but we advocate for
the simplicity with the αparameter.
4 Additional Factuality
Metric-Evaluation Benchmarks
We evaluate CLIPBERTS and its compo-
nents on additional factuality metric-evaluation
benchmarks, focusing on how robust the metrics
performs across a variety of tasks and domains.
4.1 WikiHow Factuality
We propose the WikiHow Factuality (WikiHow-
Fact) task that evaluates how well the metric can
choose the correct summaries over incorrect ones.
We derive this task from WikiHow VGSI (Yang
et al., 2021) to evaluate the text-image match-
ing performance as a ranking experiment, which
has been explored for factuality metric evaluation
(Falke et al., 2019). An example can be seen in Fig-
ure 2. Each example uses the correct document and
image as the prompt and includes four choices con-
sisting of the correct summary and three negative9636
summaries generated by random ,category , and
similarity sampling strategies described in Yang
et al. (2021). We note that this setup is actually a
more challenging task than the original VGSI task.
See Appendix C.1 for more details. Similar to the
meta-evaluation in Section 3.2, we consider the
document, image and combined settings depending
on the choice of the prompt, and evaluate using
the the same sets of metrics. We further compare
CLIP-S to that using the smaller ViT-B/32 visual
backbone. We compute the ranking accuracy of
assigning a higher score to the correct summary.
See Appendix C.1 for more details.
Results. We present the WikiHowFact result in
Table 3. First, for the image-summary setting,
we observe the power of larger visual backbone
at improving factuality, as CLIP-S achieves a 3,
5, and 6.3 point increase compared to CLIP-S
ViT-B/32 for the random, category, and similar-
ity split, respectively. For document-summary set-
ting, CLIPText-S and BERT-S achieve high accu-
racy across the sampling strategies. Interestingly,
CLIPText-S performs better than BERT-S, but this
does not apply to the multimodal case: CLIP-
BERTS actually outperforms RefCLIP-S,
showing the better positive transfer between CLIP-
S and BERT-S. Similar to the meta-evaluation re-
sults, the Triplet Network outperforms CLIP-S for
the image-summary setting, but the difference on
random and category splits is only around 1 point.
CLIPBERTS still outperforms Triplet Net-
work + BERT-S on the random and category splits,
indicating the strong performance of combining the
two metrics for evaluating factuality.
4.2 Hallucination in Image Captioning
The FOIL (Shekhar et al., 2017) dataset mea-
sures how well the metric can differentiate correct
MSCOCO captions from hallucinated ones gener-
ated by adversarially swapping out noun phrases.
We follow Hessel et al. (2021) and evaluate met-
rics on the paired setting. We compute the ranking
accuracy by giving only the image (no-ref), and
with 1 or 4 additional reference captions. We com-
pare CLIPBERTS and its components with
CLIPScore variants using the ViT-B/32 backbone.
We refer the readers to Appendix C.2 for more
details and results on all visual backbones. We
present the results in Table 4. BERT-S achieves
the highest accuracy in terms of the text-matching
performance. Especially when more text (4 refer-
ences) is provided, it outperforms CLIPText-S by
1.6 points. For image-text matching, we observe
similar improvement with larger visual backbones.
CLIPBERTS showcases its strength at posi-
tive transfer of its two components: we observe im-
provement over RefCLIP-S RN50x64 of 0.7 points
for 1-ref and 1.2 points for 4-ref.
4.3 Fine-grained Visual Grounding
BISON (Hu et al., 2019) measures the ability of
the metric to select the correct MSCOCO image
from a semantically similar image, requiring more
fine-grained visual grounding to achieve high ac-
curacy. We compare the image-summary metrics,
and refer the readers to Appendix C.3 for results
on all CLIP-S variants. Table 5 shows that CLIP-
S actually outperforms the fully fine-tuned SOTA
metric, SCAN t2i (Lee et al., 2018), indicating its
robustness and strong text-image detection perfor-
mance in the zero-shot setting. Triplet Network on9637
the other hand is not robust to this task, achieving
much lower accuracy than all other metrics.
4.4 Document Summarization Factuality
Evaluation
We compare how BERT-S and CLIPText-S corre-
late on FRANK, a factuality benchmark evalua-
tion for document abstractive summarization con-
taining 2,250 annotations for generated summaries
on XSum (Narayan et al., 2018) and CNN/DM
(Hermann et al., 2015). We report Pearson and
Spearman correlations, using the official evaluation
script.The result is shown in Table 13 in the
Appendix. CLIPText-S does not perform well for
detecting faithful summaries for summarization, as
Pearson and Spearman coefficients are around 0.10
for all datasets and 0.05 for XSum Spearman. In
contrast, BERT-S that uses RoBERTa (Liu et al.,
2019) model finetuned on the MNLI (Williams
et al., 2018) correlates higher than the original
BERTScore on Pearson and Spearman across both
datasets. It is thus useful to treat factuality as an
entailment problem and use the appropriate model.
5 Downstream Applications
Finally, we present two useful downstream applica-
tions for improving factuality of multimodal sum-
marization models: first by using the metric as a
reference image selection to guide the model in
attending important images, and second by using it
as a reward for self-critical sequence training. For
both applications, we train strong baseline mod-
els by adapting CLIP-BART (Sung et al., 2022)
for multimodal summarization. Specifically, we
extract visual features with CLIP and use a projec-
tion layer to transform the dimension of the visual
representation to the correct dimension of BART
(Lewis et al., 2020). Then, the projected features
are concatenated with the text features from the
original encoder as the joint input representation
for BART. See Appendix D for more details.5.1 Multimodal Visual Guidance
One of the well-known tasks is multimodal summa-
rization with multimodal output (Zhu et al., 2020,
MSMO), which incorporates the associated images
with the CNN/DM articles. The authors shows that
previous models suffer from modality bias, as the
cross-entropy loss is only based on the text modal-
ity. To help the model also attend to the vison
modality, the authors propose to create visual ref-
erences by ranking and selecting the most relevant
input images. While the authors show improved
performance by ranking the images by the ROUGE
score between the corresponding caption and the
reference summary, such reference does not explic-
itly guide the model to generate summaries faithful
with respect to the images. We thus propose to use
CLIPBERTS to select reference images. To
incorporate the visual guidance into the training,
we add a guidance loss by minimizing the cross-
entropy loss, where we consider the selected im-
ages by the reference as correct, and the remaining
images as incorrect. We use each image’s hidden
representation from the encoder to produce a binary
prediction using a linear layer.
We compare against the model using ROUGE
as the visual guidance. Following Zhu et al.
(2018), we report the performance of the models on
ROUGE, and image precision (IP) of the model’s
recommended images and human-annotated rel-
evant images. We additionally evaluate factual-
ity using BertScore, FactCC, DAE, and QuestE-
val. The result is shown in Table 6. We observe
a correlation between the guidance metric and the
metric score, as the model with ROUGE guidance
achieves higher ROUGE scores, and the model
with CLIPBERTS guidance improves all
factuality metrics and IP. Though the gain is rela-
tively small, the improvement on factuality metrics
is greater than the negligible drop in ROUGE.
5.2 Self-Critical Sequence Training with
CLIPBERTS Reward
A more generalized application to improve factual-
ity is to use CLIPBERTS as a reward for the
self-critical sequence training (Rennie et al., 2017,
SCST), which optimizes the model using the RE-
INFORCE algorithm (Williams, 1992). Formally,
given document d, images v, and the summary y,
the self critical loss is defined as:9638
where r(·)is a reward function, yis the sampled
summary, and ˆyis the summary obtained by greedy
decoding. We follow previous works (Pasunuru and
Bansal, 2018; Li et al., 2019; Parnell et al., 2021)
and train on the combined loss of cross-entropy
Land the self-critical loss: L=αL+ (1−
α)L, where we set α= 0.998.
We use CLIPBERTS and ROUGE-2 as
the rewards, so as to improve factually while main-
taining informativeness. Following Pasunuru and
Bansal (2018), we alternate the rewards for each
step during the training. We upweight CLIP-
BERTS by 2x (tuned on the validation set).
We experiment on MSMO, and the multimodal sen-
tence summarization (Li et al., 2018, MMSS) task,
which combines the Gigaword corpus (Graff et al.,
2003; Rush et al., 2015) with crawled images.
As the base models, we use the CLIP-BART +
CLIPBERTS model trained in Section 5.1
for MSMO, and we similarly train a CLIP-BART
model for the MMSS. We then use the fine-tunedmodels and train with SCST. Details of the training
details can be found in Appendix D. We report the
same metrics for MMSS except for IP, since the
task does not contain gold image labels.
The result is shown in Table 7. We see consis-
tent improvement over all metrics with SCST for
MMSS. Specifically, we observe a 5-point improve-
ment for FactCC and DAE, and a 4-point increase
for QuestEval while maintaing similar ROUGE
score. We observe a similar trend for training with
SCST on MSMO dataset, where SCST training im-
proves FactCC, DAE and QuestEval by 8 points, 5
points, and 1.5 points, respectively.
To evaluate the factuality of the summaries gen-
erated by models trained with SCST against that by
the base model, we conduct a human evaluation on
a randomly sampled 100 articles from the MMSS
test set. We perform the same AMT experiment
as described in Section 3.1. We ensure the same
>$12hourly rate and pay 0.1 USD per HIT. For
each summary, we aggregate the 3 annotator scores
for the document, image, and combined judgments.
The final factuality score is the average across the
100 examples. The result is shown in Table 8. The
model with SCST training achieves a statistically
significantly better factuality score with respect to
the document ( p= 0.002), image ( p= 0.041), and
especially to the combined case ( p <0.001) using
bootstrap test (Efron and Tibshirani, 1993). This
aligns with the factuality improvement we observe
with the automatic factuality scores in Table 7.96396 Related Work
Multimodal Summarization. The task of multi-
modal summarization takes additional inputs from
multiple modalities apart from the input text docu-
ment, including images (Li et al., 2018; Zhu et al.,
2020; Li et al., 2020a) and videos (Li et al., 2020c;
Palaskar et al., 2019). To incorporate multiple
modalities, many works have developed models
with multimodal attention (Zhu et al., 2020). When
multiple images are present, the rich information
present in the images may distract and thus hurt
the model’s performance. To this end, approaches
such as selective gating (Li et al., 2020b), visual
guidance (Zhu et al., 2020), and knowledge distilla-
tion Zhang et al. (2022) have been proposed. While
these methods have demonstrated improvement in
ROUGE, to the best of our knowledge, factuality
for such tasks has not been studied. We aim to
provide an evaluation benchmark for evaluating
factuality, and demonstrate methods to improve
factuality for the multimodal summarization task.
Faithfulness and Factuality Metrics. Many
metrics have been proposed to evaluate the fac-
tuality of generated summaries. The metrics
can be roughly categorized into entailment-based
and question generation and question answer-
ing (QGQA) metrics. Entailment-based metrics
(Kryscinski et al., 2020; Goyal and Durrett, 2021)
train metrics to predict entailment between the doc-
ument and summary units, such as sentences or
dependency arcs. QGQA approaches (Durmus
et al., 2020; Wang et al., 2020; Scialom et al.,
2021; Fabbri et al., 2022) generates questions from
one source using a question generation model and
then in turn uses a question answering model to
answer the generated questions given the other
source. Additionally, counterfactual estimation
(Xie et al., 2021) and embedding-based metrics
(Zhang* et al., 2020) have been explored. While
significant progress has been made, the proposed
metrics rely only on the document to detect hal-
lucinations and ignore the other modalities. We
thus propose CLIPBERTS that addresses
the missing modalities while maintaining similar
or higher correlations with human judgment of
factuality for the document and mulitmodal sum-
marization task. Meta-evaluations have also been
proposed to evaluate such metrics for text summa-
rization that differ in sizes and datasets (Fabbri
et al., 2021; Maynez et al., 2020; Wang et al., 2020;Kryscinski et al., 2020; Pagnoni et al., 2021). Our
MFMEis a similar effort but is the first meta-
evaluation proposed for the multi-modal summa-
rization task.
7 Conclusion
In this work, we present CLIPBERTS , an
automatic metric for evaluating factuality for mul-
timodal abstractive summarization. Through meta-
evaluation with MFMEand additional factuality
benchmarks, we show CLIPBERTS and its
modules correlate well with the human judgment
of factuality with respect to the document, image
and combined. CLIPBERTS is robust across
the different image and text domains and achieves
competitive correlation in the zero-shot setting with
more complex metrics. We hope this work provides
a meta-evaluation for evaluating future multimodal
factuality metrics with MFME, a strong base-
line metric CLIPBERTS to compare against,
and two methods to improve the factuality of mul-
timodal abstractive summarization models.
8 Limitations
We limit our work to the task that only contains the
vision modality through images and the text modal-
ity. However, we note that multimodal summariza-
tion also contains video and audio, which we leave
for future works. Furthermore, similar to all pre-
training models, CLIPScore and BERTScore are
also known for reflecting biases of the pre-training
data (Hessel et al., 2021; Agarwal et al., 2021),
leading to some incorrect predictions. Our work
is also focused for datasets in English. Ladhak
et al. (2020) proposed a multi-lingual WikiHow
by aligning the steps from various languages with
the image, and thus our work could be extended to
include other languages by including the images to
that dataset.
Acknowledgment
We thank the reviewers for their helpful comments.
We also thank Shiyue Zhang for useful discussions
and comments on the paper. This work was sup-
ported by NSF-CAREER Award 1846185, ARO
Award W911NF2110220, and NSF-AI Engage In-
stitute DRL-211263. The views, opinions, and/or
findings contained in this article are those of the
authors and not of the funding agency.9640References96419642
A Meta-Evaluation Details
A.1 Metrics Details
We describe the metrics we use for computing cor-
relations. We use the official scripts from the re-
spective repository.
FactCC. FactCC (Kryscinski et al., 2020) is an
entailment-based metric that uses BERT to output
a binary prediction of factuality given the concate-
nation of the document and a summary sentence
as input. The final score is the average factuality
score of all summary sentences.9643
DAE. DAE (Goyal and Durrett, 2021) is an
entailment-based metric that evaluates factuality on
dependency arc level instead of on sentence level.
We report the sentence-level error. The sentence is
considered to contain an error if any of its arcs are
predicted to be non-factual, and the final score is
the average of all sentence predictions. 0 indicates
a sentence contains no error, and 1 indicates the
sentence contains an error.
QuestEval. A QGQA metric, Scialom et al.
(2021) generates questions using a question gener-
ation model from both the document and the sum-
mary. Then, a question-answering model answers
the question generated using the document with
the summary, and answers the question generated
using the summary with the document. The final
score is the harmonic mean of the accuracy of the
predicted answers to the true answers from the
question generation model.
CLIPText-S. CLIPScore provides a variant of
the metric that takes in references for the image
captioning, and calculates the cosine similarity be-
tween the text embeddings Tand that of the ref-
erences R. The final score is calculated by taking
the average over the maximum reference cosine
similarity:
CLIPText-S (T, R) =1
|T|/summationdisplaymaxcossim (v, r)
BERTScore/BERT-S. The original BERTScore
uses roberta-large by default. For BERT-S, we
useroberta-large-mnli up to the 11th layer af-
ter tuning on FRANK’s validation set.
Triplet Network. This network maps image and
summary embeddings into the same space and min-
imize the distance between the positive pair and
maximize that between the pair of image and nega-
tive summaries with the Triplet loss (Vassileios Bal-
ntas and Mikolajczyk, 2016). Specifically, a triplet
Network takes in a triplet (V, S, S), the rep-
resentation of an image V, and that of a positivesummary and negative image. We then map the
representation to the same space and normalize the
embeddings. We then use the triplet loss with a
margin of 0.2. To generate the negative summaries,
we use the similarity split of VGSI and take the
summaries for the three negative choices. We use
the CLIP RN50x64 visual backbone to generate the
visual representation and use BERT to generate the
summary representation. We modify the example
training code provided by Transformers, and train
for 10 epochs with a learning rate of 5e-5. We use
the other default settings.
MMAE. MMAE (Zhu et al., 2018) is initially
developed for evaluating the summary quality on
MSMO, which we adapt to our task. The metric
consists of three submodules: image precision (IP),
IM, and ROUGE-L. For MFME, since we
only have a single image, IP is just 1. IM
is trained on the multimodal WikiHow dataset,
where the negative image-summary pair is from
the same batch. We use the same hyper-parameters
of the original MMAE metric. To combine the
three scores, we use the MLP variant tuned on the
validation set of MFME.
Combined Metrics. We tune the combined met-
rics on the validation set of MFME. We use
α= 0.45for CLIP-S + DAE, α= 0.10for Triplet
Net + BERT-S, and 0.25for CLIPBERTS .
A.2 Model Details
We train the models on the proposed multimodal
WikiHow dataset. The hyper-parameters are shown
in Table 9. The pre-trained models and the training
scripts for the transformer-based models are taken
from HuggingFace’s transformers library (Wolf
et al., 2020). We set the maximum input length
to 256 and output length to 32 for all models.
T5. T5 (Raffel et al., 2020) is an encoder-decoder
model pre-trained on a collection of tasks in a text-
to-text format. We use the t5-small model and
fine-tune as a document summarization tasks, ig-
noring the images. The total number of parameters9644is around 60 million. We use mixed precision, and
training was performed on 2 NVIDIA RTX A6000
GPUs for approximately 6 hours.
PEGASUS. PEGASUS (Zhang et al., 2020) is
another encoder-decoder model specifically de-
signed for the abstractive summarization task by
imitating the summarization setup during pre-
training. We use PEGASUS-large checkpoint and
fine-tune without the images. The total number of
parameters is around 571 million. Training was
performed on a single NVIDIA RTX A6000 GPU
for approximately 28 hours.
CLIP-BART. The architecture of CLIP-BART
is described in Section 5. The total number of
parameters is around 140 million. We fine-tune
the model starting from the BART-base checkpoint,
and use the CLIP RN50x64 visual encoder to extract
image features. We use mixed precision, and the
training was performed on a single NVIDIA RTX
A6000 GPU for approximately 6 hours.
MOF. MOF is based on Zhu et al. (2018), a mul-
timodal summariaziton model with multimodal at-
tention (Li et al., 2018). The model consists of a
single-layer unidirectional LSTM (Hochreiter and
Schmidhuber, 1997) with the embedding dimen-
sion of 256 and hidden dimension of 512 for the
text encoder and text decoder. The multimodal at-
tention is computed by concatenating the textual
attention layer and visual attention layer over the
visual features, extracted from the global fc7lay-
ers from VGG19 (Simonyan and Zisserman, 2015).
The total number of parameters is around 83M.
Training was performed on a single NVIDIA RTX
A6000 GPU for approximately 40 hours.
A.3 Annotation Details
Figure 3 shows a screenshot of the annotation task
on AMT.
B Meta-Evaluation with Continuous
Labels
We also experiment with combining the two judg-
ments in a continuous way, by taking the average of
the two judgments so that a score of 0.5 indicates
that the summary is faithful to only one modal-
ity. The combined judgment is shown in Table 10.
While the correlations are higher overall for all
metrics, the trend is similar to the Table 1, where
CLIPBERTS can match the correlations of
the fine-tuned metric, Triplet Net + BERT-S, indi-
cating the effectiveness and simplicity of our met-
ric.
C Additional Factuality Benchmark
Evaluations Details
C.1 WikiHowFact Details
The three negative images are selected with three
different sampling strategies, following Yang et al.
(2021): Random selects the three images arbitrarily,
Category randomly selects three negative images9645
from the same WikiHow category, and Similar-
ityselects top-3 most similar images from differ-
ent articles using similarity computed using FAISS
(Johnson et al., 2019). Random consists of 153,961
examples, similarity consists of 153,770 examples,
andcategory contains 153,961 examples.
The three sampling strategies provide settings
with increasing difficulty in terms of the negative
summaries; random is the easiest setting and simi-
larity is the hardest. Depending on which modality
we provide as the prompt, we can further break
down the task and evaluate the metric’s perfor-
mance with different combinations of modalities.
We use the same sets of metrics described in Sec-
tion 3.1 for each modality combination. FactCC
and DAE produce binary labels and thus are at a dis-
advantage for the ranking experiment, and we thus
use the probability for the factual label for FactCC
and the token error for DAE. To explore how larger
visual backbone can improve image-summary fac-
tuality detection, we compare against the original
CLIP-S that uses the ViT-B/32 backbone.
Comparison with VGSI. As described in Sec-
tion 4.1, the difference between VGSI and Wik-
iHowFact is what information is provided as the
prompt and the choices. For VGSI, we use the step
sentence, or the summary, as the prompt and ask
the models to select the correct image. Since the
document is not provided, we use CLIP-S to cal-
culate the score for each summary and image pair.
We show the result in Table 11. We see the same
surprising result that CLIP-S with the ViT-B/32
backbone achieves better ranking accuracy than the
Triplet Net model trained on the training split. In-
creasing the capacity of the CLIP-S with RN50x64 ,
the ranking accuracy improves by 4 points for ran-
dom, and 8 points for category and similarity, ap-
proaching the human performance, especially for
the random case. Additionally, when comparing
the performance of the same model for WikiHow-
Fact and VGSI, the ranking accuracies for VGSI
are much higher, indicating that WikiHowFact is
more difficult.
C.2 FOIL
We explore the ability of CLIP-S at differentiating
hallucinating captions. The FOIL (Shekhar et al.,
2017) dataset measures how well the metric can
differentiate hallucinated captions from the correct
ones. The task uses MSCOCO reference captions
and adversarially swaps out noun phrases to create
hallucinating summaries to create 64K test pairs.
One benefit of captioning tasks is that the cap-
tioning data contain references that can be treated
as the document in our setting, and thus enable eval-9646
uation of different modality combinations similar
to the multimodal summarization setting. We con-
sider three settings, where we show no reference
(evaluating only on the image-text setting), as well
as providing 1 or 4 additional reference captions
(excluding the true caption being evaluated). We
concatenate the references and consider them as
documents. We compare CLIPBERTS and
its components primarily against the CLIPScore
variants, including CLIPText-S and RefCLIP-S.
For the image-text hallucination detection, we
focus on how the different CLIP backbones af-
fects factuality detection between image and the
summary. This includes ViT-B/32 ,ViT-B/16 ,
ViT-L/14 ,RN50 ,RN101 ,RN50x4 ,RN50x16 , and
RN50x64 .
We present the results in Table 14. We observe
a clear trend that larger visual backbones improve
accuracy when considering only the visual perfor-
mance for the no-ref case. Interesingly, ViT-based
models outperform the RN-based ones for this task.
C.3 BISON
BISON (Hu et al., 2019) measures the ability of the
metric to select the correct image from two seman-
tically similar images, and thus assesses whether
the metric is able to detect fine-grained informa-
tion present in the text and image. We compare
the image-summary metrics, including all CLIP-S
variants, similar to FOIL (Appendix C.2)
Table 12 shows the result. We observe a similar
improvement in accuracy with larger visual back-
bones as observed with the FOIL dataset. While
we similarly observe improvement as the model
size grows, CLIP-S RN50x64 is the only backbone
that outperforms the fully fine-tuned SOTA metric,
SCAN t2i (Lee et al., 2018).
C.4 FRANK
We show the result in Table 13. As described in
Section 4.4, we compare existing factuality met-
rics with CLIPText-S and BERT-S. We also in-
clude QuestEval, which does not correlate better
than BERTScore variants. CLIPText-S does not
perform well for detecting faithful summaries for
summarization, as Pearson and Spearman coeffi-
cients are around 0.10 for all datasets and 0.05 for
XSum Spearman. In contrast, BERT-S that uses
RoBERTa (Liu et al., 2019) model finetuned on
the MNLI (Williams et al., 2018) correlates higher
than the original BERTScore on Pearson and Spear-
man across both datasets. It is thus useful to treat
factuality as an entailment problem and use the
appropriate model.
D Downstream Applications Details
For both experiments, we use the CLIP RN50x64
visual encoder to extract the visual features and we
limit the maximum number of images to 10. We
train the models with transformers library. For all
models, We train the models with mixed precision
and AdamW (Loshchilov and Hutter, 2019). Other
hyper-parameters are found in Table 15.
All CLIP-BART models are trained with 4
NVIDIA RTX A6000 GPUs. The training took
approximately an hour for MMSS, and approxi-
mately 19 hours for both MSMO baseline models.
For SCST training, we train the models on a
single NVIDIA RTX A6000 GPU. Training took
approximately 7 hours for MMSS and approxi-
mately 70 hours for MSMO. We perform a hyper-
parameter search manually by evaluating the mod-
els on the validation set of the corresponding
datasets and select the best-performing parame-
ter according to BERTScore and ROUGE-2 (since
these are the scores we optimize for). We first9647
determining the αfrom {0.90, 0.95,0.99, 0.995,
0.998, 0.999, 1.0}, where we find 0.998 to per-
form the best. We then tune the weight of CLIP-
BERTS from {1,2,5} and find that 2 per-
forms the best for both datasets.9648