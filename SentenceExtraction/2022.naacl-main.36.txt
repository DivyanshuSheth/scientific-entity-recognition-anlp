
Jessy LinGeza KovacsAditya Shastry
Joern WuebkerJohn DeNeroUniversity of California, BerkeleyGoogleLilt
jessy_lin@berkeley.edu, joern@lilt.com, john@lilt.com
Abstract
We introduce translation error correction
(TEC), the task of automatically correcting
human-generated translations. Imperfections
in machine translations (MT) have long moti-
vated systems for improving translations post-
hoc with automatic post-editing. In contrast,
little attention has been devoted to the problem
of automatically correcting human translations,
despite the intuition that humans make distinct
errors that machines would be well-suited to
assist with, from typos to inconsistencies in
translation conventions. To investigate this, we
build and release the A corpus with three
TEC datasets. We show that human errors
in TEC exhibit a more diverse range of errors
and far fewer translation fluency errors than the
MT errors in automatic post-editing datasets,
suggesting the need for dedicated TEC mod-
els that are specialized to correct human errors.
We show that pre-training instead on synthetic
errors based on human errors improves TEC
F-score by as much as 5.1 points. We con-
ducted a human-in-the-loop user study with
nine professional translation editors and found
that the assistance of our TEC system led them
to produce significantly higher quality revised
translations.
1 Introduction
Despite recent progress in machine translation
(MT), a tremendous amount of translated content
in the world is still written by humans (DePalma,
2021). Humans are often assumed to produce
trusted, high-quality translations. In reality, they
do make errors, including spelling, grammar, and
translation errors (Hansen, 2009). This paper in-
troduces the task of translation error correction
(TEC). Given a source sentence sand a human-
generated translation t, the goal of TEC is to pro-
duce an improved translation tby correcting all
errors in t.“Translation correction” has long been studied
in the MT community through the task of auto-
matic post-editing (APE), which aims to correct
errors in machine-generated translations (Simard
et al., 2007). TEC is structurally identical to APE.
However, it requires modeling a different data dis-
tribution: errors made by humans, which differ
from those made by MT systems (Freitag et al.,
2021). We characterize the error distribution in
TEC by building, analyzing, and releasing the
A corpus, a collection of three TEC datasets
from varying domains, with a total of 35,261 En-
glish–German translations produced and corrected
by professional translators in the natural course of
their work. While APE is dominated by the fluency
errors that are characteristic of MT systems (74%
of sentences), our TEC corpus exhibits a broader
distribution of errors that human translators are
prone to make.
Using this error analysis, we propose an ap-
proach for TEC that pre-trains on synthetic cor-
ruptions more similar to errors made by humans,
outperforming models that were developed for the
related tasks of MT, grammatical error correction,
and APE on all A datasets.
The task of TEC is often currently performed
by humans, e.g. translators hired to review and
edit translations (“reviewers”). Can a TEC system
help reviewers edit faster, or produce higher qual-
ity final translations than they would have without
assistance? We ran a human-in-the-loop user study
with nine professional translators using our best-
performing TEC model. We found that the reviews
produced when assisted with a TEC system were
rated as higher quality than those produced without,
and produced with less manual effort. Qualitatively,
users commented that trust and consistency of the
suggestions were critical. They speculated future
automated assistance could be helpful for onboard-
ing to new content, spotting technical errors, and
improving their own awareness of errors to catch.494Error Type Example Text
Monolingual: typos
Monolingual: grammar
Monolingual: fluency
Bilingual
Preferential
Looking forward, a natural question arises of
whether the research community should focus on
learning to revise model outputs (APE) or human
outputs (TEC). With recent improvements in MT,
it has been increasingly difficult for APE models to
improve model output that is already high quality
(Chollampatt et al., 2020). On the other hand, we
should expect that humans will continue to make er-
rors. TEC models will continue to provide benefit
as a way of assisting humans, whether for profes-
sional translation work or everyday language learn-
ers. TEC is synergistic with continuing advance-
ments in MT: improved MT will lead to improved
error correction for human-generated translations.
While APE pits models against models, TEC is
an opportunity to combine the best of humans and
models because humans and models make different
errors.
In sum, this paper revisits the notion of “trans-
lation correction” conceived narrowly as the MT-
centered task of APE, with an empirical investi-
gation of translation error correction (TEC), the
task of learning to correct human translations. Our
contributions are:
1.We release A, the first corpus for TEC
containing three datasets of human transla-
tions and revisions generated naturally from a
commercial translation workflow.
2.We analyze the kinds of errors humans make
inA, finding that while APE is dominated
by correcting translation fluency , TEC focuses
on correcting a broader range of errors thatappear in translation.
3.We propose a pre-training approach for TEC
that outperforms approaches developed for
similar tasks such as APE. Together, our re-
sults suggest the need for distinct approaches
to correct human translation errors.
4.We perform a human-in-the-loop user study,
finding that professional translators produce
higher quality translations when assisted by a
TEC model.
2 The ♠A Corpus for TEC
Given a source language sentence sand a human-
generated translation t, the goal of TEC is to pro-
duce a corrected target language sentence t.
We introduce the A corpus, a set of three
TEC datasets: A,E , and D O- (DO), each consisting of English–German
sentence triples ( s,t,t) from varying domains.
A is areal-world benchmark , containing nat-
uralistic data from a task humans perform, rather
than manually annotated data. All translations were
created by professional translators working with
Lilt, a localization services provider. All translators
have at least 5 years of professional translation ex-
perience and experience working with the customer
and domain. Each document was translated from
scratch (i.e. not post-edited) by a human translator
using an interactive neural MT system. Each trans-
lated document was then reviewed by a reviewer,
who Lilt selects as one of the more senior transla-
tors. As a result, the examples in our corpus exhibit495
real errors that translators make, and the corrected
translations are publication quality.
Secondly, A isdiverse , with the three
datasets from varying domains exhibiting differ-
ent error distributions and difficulty for initial work
on the TEC task. Information for each dataset is
shown in Table 2. The A dataset consists of
marketing content with product names and descrip-
tions for an activewear company. E con-
sists of industrial product names for a manufactur-
ing company. DOconsists of software engineering
tutorials. The various content types pose different
challenges for translators and thus for TEC systems,
which we discuss in Section 2.1 and Section 2.2.
Duplicate sentences with the same source swere
removed. A portion of sentences were rewritten
by the reviewer rather than being edited (a relative
edit distance of more than 25% and a minimum of
two edited words). We replace tin these sentences
withtin the corpus, so that training and evaluation
focuses on local edits rather than re-translations.
The train, dev, and test splits were constructed by
splitting along document boundaries.
2.1 How do TEC and APE errors differ?
To understand how the human errors in the TEC
task differ from model errors in APE, we compare
the types of errors in A with 100 randomly sam-
pled errors in the WMT 2021 APE shared task dev
set, which we then annotate with error types. We
define an error taxonomy that classifies each edit
as one of three types: (1) Monolingual edits are
identifiable from only the target-side text. We di-
vide these further into subcategories that highlight
different capabilities needed to correct edits: typos
(including spelling, punctuation, spacing, ortho-
graphic issues), grammar , and fluency (awkward
phrasing, word choice, or non-native-sounding dis-
fluencies); (2) Bilingual edits concern mismatches
between the source and target text, e.g. over- or
under-translation, mis-translations; (3) Preferen-tial edits correct text that is inconsistent with the
preferences of the customer, as described in ex-
tralinguistic project requirements (e.g. terminol-
ogy or stylistic preferences). Examples of each
error type are shown in Table 1. Our error taxon-
omy closely mirrors those of previous analyses of
human translation errors (Specia and Shah, 2014;
Yuan and Sharoff, 2020; Gupta et al., 2021), and
we confirm their findings that human translation
errors differ from MT errors. However, while pre-
vious work focuses on error detection and quality
estimation, TEC is concerned with error correc-
tion. Our error types are intended to isolate the
capabilities that models need to learn to correct
edits (e.g., target-side language models can learn
to correct monolingual errors, but cannot do well
on bilingual edits).
We annotate and release error labels for all test
sentences in A to enable its use as a diagnostic
set for per-type evaluation of models. On the larger
E andDOdatasets, we randomly sample
50 errors to annotate for this analysis. Error types
were annotated by a professional German transla-
tor. Each segment can have multiple error types. In
Table 3, we report the percentage of sentences with
at least one error of each type. 74% of sentences in
APE exhibit a fluency error, in contrast to up to 22%
of sentences in A, while other types like mono-
lingual grammar, bilingual, and preferential errors
are notably underrepresented in APE. We also note
that all sentences in the APE shared task are edited,
while a key feature of TEC is identifying when
a sentence does notneed to be edited. The error
distributions suggest that different modeling tech-
niques may shine in each: while APE challenges
models to correct disfluent translations characteris-
tic of MT systems, our task is designed to focus on
identifying and correcting the typos, mismatches,
and grammatical errors more commonly exhibited
by humans. Guided by this observation, we de-
scribe an approach in Section 3 that pre-trains on
synthetic edits that are more representative of this496
error distribution.
2.2 How difficult is it to learn to edit?
To quantify how difficult it may be to learn the
correct edits in A, we report statistics on edit
overlap : what proportion of edits that we expect
models to perform (e.g. adding a comma) appear
exactly in the training set? We use the errant toolkit
to identify discrete edits (Bryant et al., 2017). Each
edit is represented as a tuple (original span, re-
placement span) , e.g. (“auf”, “an”) to replace
“auf” with “an.” Edit statistics are reported in Ta-
ble 4: in A andDO,20% of the total number
of edits in dev and test appear in the training set,
while E has60% of dev and test edits
appear in the training set.
While the edit overlap rate provides a relative
sense of scale for precision and recall numbers,
it does not provide an upper bound on recall. It
is possible to learn edits that do not exactly ap-
pear in the training set. For example, capitaliz-
ing product names (“winterized” →“WINTER-
IZED”) is a learnable pattern that would appear as
many distinct edits. Additionally, some errors can
be corrected without fine-tuning because they are
generic typo, grammatical, fluency, or bilingual er-
rors. Conversely, it is also possible that it is wrong
to make an edit that appears in the training set,
depending on the surrounding sentential context.
3 Approaches to TEC
We propose a TEC model and compare it to sev-
eral models designed for related tasks to deter-
mine whether they are also effective for TEC. An
overview of the differences between the models is
shown in Figure 1. All models use the Transformer
neural architecture (Vaswani et al., 2017) that gen-
erates the target sequence tfrom left to right. All
are pre-trained on 36M sentences from the WMT18
translation taskand fine-tuned on A, unless in-
dicated otherwise. All pre-training and fine-tuning
data is pre-processed by normalizing punctuation
with the Moses toolkit (Koehn et al., 2007).
All models have 6 encoder and decoder layers,
model dimension of 256, feed-forward dimension
of 512, and 8 attention heads. We use a joint En-
glish–German vocabulary with 33k byte pair en-
coding subwords (Sennrich et al., 2016). During
pre-training, we set the dropout to 0.1 and use the
Adam optimizer (Kingma and Ba, 2015) with a
learning rate of 0.0002 . During fine-tuning, we
decrease the learning rate to 0.0001 and reset the
Adam momentum parameters. We select the best
fine-tuning checkpoint with edit-level Fscore on
our dev set. We use greedy inference.
3.1 Dual-Source Encoder-Decoder Model
We first describe the dual-source encoder-decoder
we use for the APE and TEC models. Formally, the
original Transformer architecture (Vaswani et al.,
2017) takes a sequence of Jsource tokens s
and predicts a sequence of Itarget tokens t.
We adapt the architecture to additionally encode the
original translation t, a sequence of Itokens, t.
We independently project tinto the embedding
space, add an offset vector o, and then concatenate
the embedding with the embedding of the source s
to form the encoder input. To allow the dual-source
model to copy tokens from the original translation
t, we implement the copy-mechanism proposed by
Zhao et al. (2019), which augments the model with
an additional encoder-decoder attention layer. An
expanded description of the model can be found in
Appendix Section A.497
3.2 Synthetic Data Generation
For the TEC and GEC model, we generate syn-
thetic triples ( s,t,t) for pre-training. We gener-
ate a synthetic tby corrupting the German side of
the translation data into t . For each sen-
tence, we sample the probability of corruption
p∼ N (µ= 0.01, σ= 0.04)clipped at 0. On
each character and word in that sentence, with prob-
ability p, we randomly select one of the following
perturbations to apply at that position: insertion,
deletion, transposition, repetition.
3.3 TEC Models
The five approaches we compare are:
TEC (this work) We implement the dual-source
encoder-decoder model that encodes two inputs ( s,
t) and outputs t, as described in Section 3.1, and
then pre-train on synthetic data generated with the
procedure in Section 3.2. We then fine-tune on
A.
MT We train an English-German neural machine
translation model (with the standard architecture
described previously) and fine-tune it on ( s,t)
A pairs, ignoring the original translation t.
GEC We evaluate a encoder-decoder (monolin-
gual) GEC model that takes an incorrect German
sentence tas input and outputs a corrected t. We
use the same copy mechanism to attend to tas our
TEC model. To pre-train, we perturb tusing the
procedure described in Section 3.2, throwing away
the source side to obtain (t, t) = ( t , t)
pairs. We then fine-tune on the A corpus, ig-
noring s.APE We implement a dual-source encoder-
decoder model that is identical to our TEC model.
Following common practice in APE (Junczys-
Dowmunt and Grundkiewicz, 2016; Negri et al.,
2018), we pre-train on synthetic “post-editing”
triples ( s,t,t) where t=tis generated by
translating swith an MT system. We split the train-
ing dataset into two parts, train an MT model on
each half, and use each model to translate the other
half of the dataset not seen during training. We
then fine-tune on A.
BERT-APE We also evaluate whether a state-of-
the-art APE model can be directly applied to our
task. We evaluate the BERT-based encoder-decoder
of Correia and Martins (2019), on which the WMT
2019 shared task winner was based (Lopes et al.,
2019). Following Correia and Martins (2019), we
fine-tuned on 23K English–German SMT triplets
from the WMT18 shared task. We reproduce their
results on the APE shared task test sets, and con-
tinue fine-tuning this model on A. Following
their paper, the inputs are pre-processed by tokeniz-
ing and joining the two inputs with a separator to
form ( s[SEP] t,t) pairs.
4 Results & Discussion
The primary metric for TEC is MaxMatch scores
(M) (Dahlmeier and Ng, 2012) computed with
theerrant toolkit (Bryant et al., 2017). Mis a
standard metric for GEC that aligns tandtto
extract discrete “edits.” We choose to follow the
GEC evaluation practice of up-weighting precision
by comparing F, since the original translation is498
mostly correct: it is better to suggest few correct
edits than potentially introduce new errors.
Table 5 shows that TEC achieves the best overall
Fscore on all datasets, from +0.1(onDO) up
to+5.1(onA ) above the next-best model.
Fine-tuning on actual human corrections provides
substantial gains; results without fine-tuning can
be found in Appendix Section B.
Both the MT model (which ignores t) and the
GEC model (which ignores s) underperform TEC.
The MT model’s high edit recall can be attributed to
the fact that it proposes many edits, greatly trading
off precision. Without conditioning on t, direct
MT translations of the source diverge from the
reference. The GEC model obtains high precision
but underperforms on recall. Conditioning on snot
only makes it possible to propose bilingual edits,
but also provides additional information to correct
monolingual edits, as we show in Section 4.1.
Can APE models be directly adapted for TEC?
Since our task is structurally identical to APE, a
natural question is whether models that are trained
on the APE objective can be directly adapted for
TEC. The APE and TEC models differ only in pre-
training, but the performance difference between
them is substantial, indicating that the more GEC-
like data synthesis procedure is a better fit for TEC
than APE-style data synthesis via MT. Even more,
the BERT-APE model, which is first fine-tuned to
achieve state-of-the-art on APE before fine-tuning
onA, achieves a particularly low Fscore
because it makes too many edits (low precision).
Although future work may find insights in APE,
our results emphasize that models that excel at
correcting machine errors cannot be assumed to
work well on human translations.
4.1 Fluency & Per-category Error Analysis
We perform a more in-depth comparison using al-
ternative metrics on A , which includes an-notated error labels as a diagnostic tool. First,
to understand how much models are editing, we
look at n-gram overlap with the GLEU metric
(Napoles et al., 2015), a variant of BLEU used
in GEC evaluation to measure the fluency of holis-
tic rewrites (Sakaguchi et al., 2016). Next, we
compare sentence-level accuracy , which measures
exact match with t. We compute overall sentence-
level accuracy, which includes unedited sentences
(which some models may incorrectly edit). We
also report accuracy per error type over (edited)
sentences annotated with that error type. These
metrics need to be interpreted carefully: a no-edit
baseline achieves a GLEU score of 87.85(since
original translations are mostly close in edit dis-
tance to the final) and sentence-level accuracy of
70.62% (the % of unedited sentences), outperform-
ing models like MT and BERT-APE that make too
many incorrect edits.
Our TEC model achieves the best score overall
on both alternative metrics over all sentences, but
various models outperform on specific error types.
The full results are shown in Table 6. Examples
of system outputs for different error types can be
found in Appendix Section C.
Notably, APE models lose the most accuracy
relative to our model on monolingual typo edits.
This may be because neural MT decoders much less
frequently introduce target-side errors that would
be similar to typos (compared to the frequency of
fluency errors). Still, the observation that different
models do well at different errors suggests that
future work can improve on TEC by leveraging
the strengths of different models, e.g. using MT
models to propose alternative translations.
5 User Study: Assisting Professional
Translators with TEC
Our automatic evaluation shows how our TEC
model can outperform other baseline systems, but499
we are ultimately interested in whether any TEC
system is indeed useful in practice. Presently,
TEC is done manually by humans. To investi-
gate whether TEC systems can already be useful
to humans—improving the quality, speed, or ease
of human review—we performed a human-in-the-
loop user study with our TEC model.
5.1 Methodology
We recruited 9 professional translators to serve as
reviewers. None of them had prior experience with
A content. They were allowed to read and
reference the sentences in the A training set
to familiarize themselves with the content and pre-
ferred terminology. Then, they were each assigned
to review 74 sentences from the test set of A .
Of the 74 sentences, our TEC system predicted a
suggested edit for 57 sentences, and for the remain-
ing 17 sentences our TEC system did not predict
any edits.
We opt for a within-subjects design to control
for speed and experience differences between re-
viewers. For each reviewer, the 74 sentences were
randomized such that half were in the “assisted
condition” showing the TEC suggestion if avail-
able for the sentence, and the other half were in the
“unassisted condition” where no TEC suggestion
was shown. The reviewing interface is shown in
Figure 2. If the sentence has suggestions available,
the reviewer is asked to first accept or reject each
of the suggestions. Then, they are asked to make
edits to the text until they are satisfied with the
translation. They then click a button to confirm
their translation and move to the next sentence.
During the review process, we track:
1.Whether the TEC suggestion, if shown, was
accepted or declined
2. Total time spent reviewing each sentence
3.Number of edit operations (insertions and
deletions) the user made
4.Levenshtein edit distance from the original
text to the final text
Finally, to evaluate whether TEC has an effect
on quality, we asked a 10th translator to compare
the quality of the reviewed sentences by ranking
the 9 reviewed translations, with ties allowed. This
translator was the translator who had reviewed the
reference translations in the corpus, as the explicit
goal is to ensure consistency with conventions in
the training documents.
5.2 Results
In the user study, 79% of TEC suggestions were
accepted. For the purpose of analyzing effects of
the TEC suggestions on time spent and translation
quality, we will focus on only the 255 sentences
(across 9 reviewers) where a TEC suggestion ex-
ists. Results are shown in Table 7. For all statis-
tical significance tests, we use the Mann-Whitney
(MW) U-test for testing statistical significance as
all quantities are neither normally distributed nor
log-normal.500
5.2.1 Effects of Suggestions on Time Spent
During the Review Process
We first analyze how TEC suggestions influence
the time spent reviewing a sentence. We compare
the time durations normalized by the length of the
sentence that needed to be reviewed (the length-
normalized review time ), as longer sentences re-
quire more time to be read and reviewed.
There is no significant difference in length-
normalized review time when the suggestion is
hidden vs. shown (MW U= 31654 , p=
0.460). When suggestions are shown, the length-
normalized review time is significantly less on sen-
tences where reviewers accepted the suggestion,
compared to sentences where they declined (MW
U= 3555 , p < 0.0005 ).
A potential explanation for these results is that
when reviewers are shown incorrect TEC sugges-
tions, they are distracted and slowed down, provid-
ing some evidence that precision should indeed be
emphasized in automatic evaluations of TEC.
5.2.2 Effects of Suggestions on Edits Made
During the Review Process
We also analyze the effects of suggestions on the
editing effort, as measured by the number of char-
acters the reviewer had to insert and delete, as well
as how different the final reviewed sentences were
from the original.
When suggestions are shown vs. hidden, there
is a significant reduction in the number of inser-
tions+deletions (MW U= 41348 .5, p < 0.0001 ).
There is also a significant reduction when a shown
suggestion is accepted vs. declined (MW U=
4007.5, p < 0.005). There is no significant dif-
ference in the Levenshtein distance from the orig-
inal translation to the final translation, between
when a suggestion is shown vs. hidden (MWU= 33750 .0, p= 0.611), or between when a
shown suggestion is accepted vs. declined (MW
U= 5485 .0, p= 0.783).
Thus, the TEC system suggestions help to sig-
nificantly reduce the amount of manual typing that
the user must perform.
5.2.3 Effects of suggestions on reviewed
sentence quality
To assess the effects of TEC assistance on qual-
ity, we used the quality rankings produced by the
independent reviewer. Quality rankings were not
normally distributed, so we use the Mann-Whitney
U-test for testing statistical significance. A box
plot of the quality rankings is shown in Figure 3.
The median quality ranking when the suggestion
is shown is 1, vs. 2 when the suggestion is hidden.
The quality ranking is significantly lower (meaning
quality is higher) when the suggestion is shown, vs.
hidden (MW U= 28738 .0, p < 0.01).
This suggests that showing TEC suggestions
may be helping reviewers correct errors they may
not have otherwise noticed, or help nudge them
towards desired corrections.
5.3 Qualitative Findings
We also conducted a post-study survey for review-
ers to report qualitative feedback. To understand
common themes in the responses, we present all
themes that at least two reviewers mention in their
commentary.
5.3.1 The Role of Reliability and Trust
Five reviewers commented that reliability is criti-
cal: it was difficult to trust the system when they
noticed some suggestions were incorrect, or the sys-
tem did not reliably make an edit when applicable
(e.g. always hyphenating when appropriate):501
These comments are in concordance with our
quantitative findings. Perhaps unlike other assis-
tive applications, it is not enough to only have high
precision: if reviewers cannot trust that the system
has caught most or all errors, they will not save
time as they still have to read the entire sentence
carefully. Conversely, a high-recall, low-precision
system is not only distracting, but also leads re-
viewers to be suspicious of whether suggestions
are correct in general. In general, future TEC sys-
tems must manage this balance of precision and
recall for user trust.
5.3.2 Use Cases for TEC
Many reviewers highlighted scenarios where trust-
worthy TEC systems could be particularly useful.
Two reviewers said TEC is helpful for correc-
tions and typos , similar to the use cases for GEC
in the wild (Omelianchuk et al., 2021):
On the other hand, three reviewers mentioned that
they hoped such a system would make more sub-
stantial corrections in order to save a non-negligible
amount of time, although of course these edits may
come at the expense of precision:
Three reviewers commented that a TEC system
could be a memory aid or substitute for research-
ing client-specific requirements , which is often an
intensive part of the production translation process.
One reviewer pointed out it could be particularly
useful as an instructive tool for translators who are
new to a client:
Finally, three reviewers commented that it could
be useful as an attention-directing tool by making
them aware of what errors they might look out for,
especially in repetitive content where it may be
easy to miss details:
6 Conclusion & Future Work
We introduced the task of translation error correc-
tion (TEC) and released the A corpus to study
automatic correction of human translations, consist-
ing of three TEC datasets across varying domains.
In our analysis of TEC data, we showed how the
errors that humans make differ from those made by
MT systems, suggesting that this task warrants dif-
ferent approaches from those previously studied in
the task of automatic post-editing. We confirm this
empirically by proposing a synthetic data genera-
tion procedure that more closely matches the distri-
bution of human translation errors and showing that
our TEC model, pre-trained on this data, consis-
tently outperforms models developed for APE, as
well as those for MT and GEC. Finally, we showed
how our TEC system is helpful to real humans, as-
sisting professional reviewers and leading them to
produce higher quality reviewed translations.
Future work may improve on our TEC system
by investigating how to leverage the strengths of
recent MT systems (e.g. for initializing systems or
proposing edits) or developing more sophisticated
synthetic data generation techniques (e.g. using
the source sentence or linguistic knowledge). Be-
yond our benchmark, it would be interesting to
apply TEC systems to other settings in which hu-
man translation errors appear, e.g., to correct trans-
lations written by language learners, denoise MT
training sets, or clean up MT evaluation sets.
From the perspective of human-AI interaction,
TEC presents a real-world use case and testbed
to study how to assist experts with modern NLP
systems, hinting at the opportunity to combine the
best of humans and machines.
Acknowledgments
We thank Sai Gouravajhala, Yunsu Kim, Eric Wal-
lace, and the other members of the Lilt research
team and Berkeley NLP group for helpful discus-
sion and feedback. We thank Morgan Raymond
and Spence Green for their support in releasing
the dataset. Finally, we are grateful to the profes-
sional translators who annotated the dataset and
participated in the user study.502References503504ATransformer Architecture Background
and Model Description
A.1 Transformer Architecture
The neural models implemented in this work are
based on the self-attentional Transformer archi-
tecture (Vaswani et al., 2017). Formally, given
a sequence of source tokens (encoded as one-hot
vectors) s= (s, . . . , s),s∈V, the goal
is to predict a sequence of target tokens t=
(t, . . . , t),t∈V, that is a translation of the
source sequence, where Vis the vocabulary. The
model has two main components, the encoder and
thedecoder . The encoder transforms the source
sequence sinto a sequence of hidden states by
first mapping each individual token into a continu-
ous embedding space, adding a positional embed-
ding and then processing it through a sequence of
self-attention and feed-forward layers:
x=Es+p (1)
h=encoder (x), (2)
where x∈V, j∈(1, . . . , J ),Eis the embedding
matrix for vocabulary Eandpis the sequence
of positional embeddings described in Sec. 3.5 of
(Vaswani et al., 2017). At a given time step i, the
decoder defines a probability distribution Pover
all vocabulary items in V:
y=Et+p (3)
h=decoder (y,h) (4)
P(t) =softmax (hE) (5)
where we assume a single shared vocabulary Vand
embedding matrix E. At training time we optimize
the cross-entropy loss
L(P) =−/summationdisplaylog(P(t)). (6)
A.2 Dual-Source Encoder-Decoder Model
Given an additional input sequence t=
(t, . . . , t). the dual-source model used for the
APE and TEC models is implemented by indepen-
dently projecting tinto the embedding space,
adding an offset vector oand concatenating the em-
bedding sequences. Equations 2 and 4 are rewritten
as
y=Et+p+o (7)
h=encoder ([x;y]) (8)
h=decoder (y,h),(9)where ois a single learned vector that is broadcast
to all positions i∈(1, . . . , I )and[·;·]denotes the
concatenate operation.
A.3 Copy-Attention Mechanism
The new output probability distribution for the next
target token P(t)is a weighted sum of the proba-
bility of generating and the probability of copying
token t:
ˆP(t) = (1 −α)P(t) +αP(t),
(10)
where the copy probabilities are calculated from the
attention matrix of an additional encoder-decoder
attention layer that is added on top of the final
decoder layer, A:
P(t) =softmax (A) (11)
The copy probability weight αis determined
with the attention context vector c, computed as a
weighted sum of the attention values (i.e. linearly
transformed encoder states) where the weights are
defined by A:
α=sigmoid (Wc). (12)
This copy-attention layer applies a source-side
mask so that it only attends to the positions (J+
1, . . . , J +I)that correspond to the second in-
put sequence t, and its implementation follows
Zenkel et al. (2019). In particular, it uses a single
attention head, no skip connection, and contains a
separate output layer that predicts the target word
based on its context vector with probability distri-
bution P(·). At training time both output layers
are optimized jointly by defining the overall loss L
as the weighted sum of both cross-entropy losses:
L=L(ˆP) +λL(P) (13)
λis set to 0.05in all experiments. We further ap-
ply source-word dropout (Junczys-Dowmunt et al.,
2018), setting the full embedding vector for words
intto1/pwith probability p= 0.05.
B Results without finetuning
See Table 8.
C Examples of System Output
See Table 9, Table 10, Table 11.
D Full User Study Results
See Table 12.505
Type: Monolingual: technical
s:
t:
Reference t:
Model Predicted t
MT
APE
BERT-APE
GEC
TEC (ours)
Type: Monolingual: technical
s:
t:
Reference t:
Model Predicted t
MT
APE
BERT-APE
GEC
TEC (ours)506Type: Bilingual
s:
t:
Reference t:
Model
MT
APE
BERT-APE
GEC
TEC (ours)507