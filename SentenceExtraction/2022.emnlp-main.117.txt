
Dustin WrightJiaxin PeiDavid JurgensIsabelle AugensteinDept. of Computer Science, University of Copenhagen, DenmarkSchool of Information, University of Michigan, Ann Arbor, MI, USA
{dw,augenstein}@di.ku.dk
{pedropei,jurgens}@umich.edu
Abstract
Whether the media faithfully communicate sci-
entific information has long been a core issue to
the science community. Automatically identify-
ing paraphrased scientific findings could enable
large-scale tracking and analysis of information
changes in the science communication process,
but this requires systems to understand the sim-
ilarity between scientific information across
multiple domains. To this end, we present
theS P I - C E D (S ), the
first paraphrase dataset of scientific findings
annotated for degree of information change.
S contains 6,000 scientific finding pairs
extracted from news stories, social media dis-
cussions, and full texts of original papers. We
demonstrate that S poses a challenging
task and that models trained on S im-
prove downstream performance on evidence re-
trieval for fact checking of real-world scientific
claims. Finally, we show that models trained
onS can reveal large-scale trends in
the degrees to which people and organizations
faithfully communicate new scientific findings.
Data, code, and pre-trained models are avail-
able at .
1 Introduction
Science communication disseminates scholarly in-
formation to audiences outside the research com-
munity, such as the public and policymakers
(National Academies of Sciences, Engineering,
and Medicine, 2017). This process usually in-
volves translating highly technical language to non-
technical, less-formal language that is engaging
and easily understandable for lay people (Salita,
2015). The public relies on the media to learn
about new scientific findings, and media portray-
als of science affect people’s trust in science while
at the same time influencing their future actionsFigure 1: We are interested in measuring the informa-
tion similarity of statements about scientific findings
between different sources, including scientific papers,
news, and tweets, shown here with real examples. The
finding in this figure comes from Fang et al. (2016) and
the news quote is from this Reuters story.
(Gustafson and Rice, 2019; Fischhoff, 2012; Kuru
et al., 2021). However, not all scientific communi-
cation accurately conveys the original information,
as shown in Figure 1. Identifying cases where sci-
entific information has changed is a critical but
challenging task due to the complex translating
and paraphrasing done by effective communicators.
Our work introduces a new task of measuring sci-
entific information change, and through developing
new data and models aims to address the gap in
studying faithful scientific communication.
Though efforts exist to track and flag when pop-
ular media misrepresent science,the sheer volume
of new studies, reporting, and online engagement
make purely manual efforts both intractable and
unattractive. Existing studies in NLP to help au-
tomate the study of science communication have
examined exaggeration (Wright and Augenstein,
2021b), certainty (Pei and Jurgens, 2021), and fact
checking (Boissonnet et al., 2022; Wright et al.,
2022), among others. However, these studies skip
over the key first step needed to compare scientific
texts for information change: automatically identi-1783fying content from both sources which describe the
same scientific finding. In other words, to answer
relevant questions about and analyze changes in
scientific information at scale, one must first be
able to point to which original information is being
communicated in a new way.
To enable automated analysis of science com-
munication, this work offers the following con-
tributions (marked by C). First, we present the
S P I
C E D dataset ( S ), a manually
annotated dataset of paired scientific findings from
news articles, tweets, and scientific papers ( C1,
§3). S has the following merits: (1) exist-
ing datasets focus purely on semantic similarity,
while S focuses on differences in the infor-
mation communicated in scientific findings; (2) sci-
entific text datasets tend to focus solely on titles or
paper abstracts, while S includes sentences
extracted from the full-text of papers and news arti-
cles; (3) S is largely multi-domain, covering
the 4 broad scientific fields that get the most media
attention (namely: medicine, biology, computer
science, and psychology) and includes data from
the whole science communication pipeline, from
research articles to science news and social media
discussions.
In addition to extensively benchmarking the per-
formance of current models on S (C2, §4),
we demonstrate that the dataset enables multiple
downstream applications. In particular, we demon-
strate how models trained on S improve zero-
shot performance on the task of sentence-level
evidence retrieval for verifying real-world claims
about scientific topics ( C3, §5), and perform an
applied analysis on unlabelled tweets and news ar-
ticles where we show (1) media tend to exaggerate
findings in the limitations sections of papers; (2)
press releases and SciTech tend to have less infor-
mational change than general news outlets; and (3)
organizations’ Twitter accounts tend to discuss sci-
ence more faithfully than verified users on Twitter
and users with more followers ( C4, §6).
2 Related Work
The analysis of scientific communication directly
relates to fact checking, scientific language anal-
ysis, and semantic textual similarity. We briefly
highlight our connections to these.
Fact Checking Automatic fact checking is con-
cerned with verifying whether or not a given claimis true, and has been studied extensively in mul-
tiple domains (Thorne et al., 2018; Augenstein
et al., 2019) including science (Wadden et al., 2020;
Boissonnet et al., 2022; Wright et al., 2022). Fact
checking focuses on a specific type of information
change, namely veracity. Additionally, the task gen-
erally assumes access to pre-existing knowledge re-
sources, such as Wikipedia or PubMed, from which
evidence can be retrieved that either supports or re-
futes a given claim. Our task is concerned with a
more general type of information change beyond
categorical falsehood and is a required task to com-
plete prior to performing any kind of fact check.
Scientific Language Analysis Automating tasks
beneficial for understanding changes in scientific
information between the published literature and
media is a growing area of research (Wright and
Augenstein, 2021b; Pei and Jurgens, 2021; Bois-
sonnet et al., 2022; Dai et al., 2020; August et al.,
2020b; Tan and Lee, 2014; Vadapalli et al., 2018;
August et al., 2020a; Ginev and Miller, 2020). The
three tasks most related to our work are under-
standing writing strategies for science communi-
cation (August et al., 2020b), detecting changes
in certainty (Pei and Jurgens, 2021), and detecting
changes in causal claim strength i.e. exaggera-
tion (Wright and Augenstein, 2021b). However,
studying these requires access to paired scientific
findings. To be able to do so at scale will require
the ability to pair such findings automatically.
Semantic Similarity The topic of semantic simi-
larity is well-studied in NLP. Several datasets ex-
ist with explicit similarity labels, many of which
come from SemEval STS shared tasks (e.g., Cer
et al., 2017) and paraphrasing datasets (Ganitke-
vitch et al., 2013). It is possible to build unla-
belled datasets of semantic similarity automatically,
which is the main method that has been used for
scientific texts (Cohan et al., 2020; Lo et al., 2020).
However, such datasets fail to capture more subtle
aspects of similarity, particularly when the focus
is solely on the scientific findings conveyed by a
sentence (see Appendix A). And as we will show,
approaches based on these datasets are insufficient
for the task we are concerned with in this work,
motivating the need for a new resource.
3 S
We introduce S , a new large-scale dataset of
scientific findings paired with how they are commu-1784nicated in news and social media. Communicating
scientific findings is known to have a broad impact
on public attitudes (Weigold, 2001) and to influ-
ence behavior, e.g., the way vaccines are framed
in the media has an effect on vaccine uptake (Kuru
et al., 2021). Building upon prior work in NLP
(Wright and Augenstein, 2021a; Pei and Jurgens,
2021; Sumner et al., 2014; Bratton et al., 2019),
we define a scientific finding as a statement that
describes a particular research output of a sci-
entific study, which could be a result, conclusion,
product, etc. This general definition holds across
fields; for example, many findings from medicine
and psychology report on effects on some depen-
dent variable via manipulation of an independent
variable, while in computer science many findings
are related to new systems, algorithms, or methods.
Following, we describe how the pairs of scientific
findings were selected and annotated.
3.1 Data Collection
An initial dataset of unlabelled pairs of scientific
communications was collected through Altmet-
ric ( ) a platform track-
ing mentions of scientific articles online. This ini-
tial pool contains 17,668 scientific papers, 41,388
paired news articles, and 733,755 tweets—note that
a single paper may be communicated about mul-
tiple times. The scientific findings were extracted
in different ways for each source. Similar to Prab-
hakaran et al. (2016), we fine-tune a RoBERTa (Liu
et al., 2019) model to classify sentences into meth-
ods, background, objective, results and conclusions
using 200K paper abstracts from PubMed that had
been self-labeled with these categories (Canese and
Weis, 2013). This sentence classifier attained 0.92
F1 score on a held-out 10% sample (details in Ap-
pendix I) and then the classifier was applied to
each sentence of the news stories and paper full-
texts. Given the domain difference between scien-
tific abstracts and news, we additionally manually
annotated a sample of 100 extracted conclusions;
we find that the precision of the classifier is 0.88,
suggesting that it is able to accurately identify sci-
entific findings in news as well. We extract each
sentence classified as “result” or “conclusion” and
create pairs with each finding sentence from news
articles written about it. This yields 45.7M poten-
tial pairs of ⟨news, paper ⟩findings. For tweets, we
take full tweets as is, yielding 35.6M potential pairs
of⟨tweet, paper ⟩findings.3.2 Data sampling
Pairing every finding from a news story with ev-
ery finding from its matched paper results in an
untenable amount of data to annotate. Addition-
ally, it has been shown that proper data selec-
tion can reduce the need to annotate every pos-
sible sample (MacKay, 1992; Holub et al., 2008;
Houlsby et al., 2011). Therefore, to obtain a sam-
ple of paired findings covering a range of similari-
ties, we first filter our pool of unlabelled matched
findings based on the semantics using Sentence-
BERT (SBERT, Reimers and Gurevych (2019)), a
Siamese BERT network trained for semantic text
similarity, trained on over 1B sentence pairs (see
Appendix G for further details). We use this model
to score pairs of findings from news articles and pa-
pers based on their embeddings’ cosine similarity
and conduct a pilot study to determine which data
to annotate.
For the pilot, we sample 400 pairs evenly for
every 0.05increment bucket in the range [0,1]of
similarity scores (20 per bucket). Each sample is
annotated by two of the authors of this study with
a binary label of “matching” vs “not matching”,
yielding a Krippendorff’s alpha of 0.73.From this
sample, we observed that there were no matches
below 0.3 and only 2 ambiguous matches below
0.4. At the same time, the vast majority of samples
from the entire dataset have a similarity score of
less than 0.4. Additionally, above 0.9 we saw that
each pair was essentially equivalent. Given the dis-
tribution of matched findings across the similarity
scale, in order to balance the number of annotations
we can acquire, the yield of positive samples, and
the sample difficulty, we sampled data as follows
based on their cosine similarity:
• Below 0.4= automatically unmatched.
•Above 0.9with a Jaccard index above 0.5=
automatically matched.
•Sample an equal number of pairs from each
0.05increment bin between 0.4and0.9for
human expert annotation.
We sample 600 ⟨news, paper ⟩finding pairs from
the four fields which receive the most media atten-
tion (medicine, biology, computer science, and psy-
chology) using this method. This yields 2,400 pairs
to be annotated. For extensive details on the pilot
annotation and visualizations, see Appendix B.1785
We follow a similar procedure to sample pairs
from papers and Twitter for annotation. However,
rather than use the SBERT similarity scores, we
instead first obtain annotations for news pairs using
the scheme to be described later in §3.3 in order
to train an initial model on our task (CiteBERT,
Wright and Augenstein 2021a). We then use the
trained model to obtain scores in the range [0,1]
for each pair and sample an equal number of pairs
from bins in 0.05 increments, for a total of 1,200
pairs (300 from each field of interest).
3.3 Finding Matching Annotation
We perform our final annotation based on the sam-
pling scheme above using the Prolific platform
( ) as it allows prescreen-
ing annotators by educational background. We
require each annotator to have at least a bache-
lor’s degree in a relevant field to work on the task.
Annotators are asked to label “whether the two sen-
tences are discussing the same scientific finding”
for 50 finding pairs with a 5-point Likert schema
where each value indicates that “The information
in the findings is...” (1): Completely different (2):
Mostly different (3): Somewhat similar (4): Mostly
the same, or (5): Completely the same. See Ap-
pendix C for details of how this rating scale was de-
cided. We call this the I M
S (IMS ) of a pair of findings. Annotation
was performed using P (Pei et al., 2022).
Full annotation instructions and details are listed in
Appendix D. Notably, annotators were instructed
to mark how similar the information in the findings
was, as opposed to how similar the sentences are.
Further, they were instructed to ignore extraneous
information like “The scientists show...” and “our
experiments demonstrate...”.
Post processing To improve the reliability of the
annotations, we use MACE (Hovy et al., 2013) toestimate the competence score of each annotator
and removed the labels from the annotators with
the lowest competence scores. We further man-
ually examine pairs with the most diverse labels
(standard deviation of ratings >1.2) and manually
replace the outliers with our expert annotations.
The overall Krippendoff’s αis 0.52, 0.57, 0.53,
and 0.52 for CS, Medicine, Biology, and Psychol-
ogy respectively, indicating that the final labels are
reliable. While many annotators considered the
task challenging, our quality control strategies al-
low us to collect reliable annotations.For all the
annotated pairs, we average the ratings as the final
similarity score. In addition to the 3,600 manually
annotated pairs, we include an extra 2,400 auto-
matically annotated pairs as determined in §3.2
(unmatched pairs get an IMS of 1, matched pairs
get an IMS of 5), for a total of 6,000 pairs. Given
that there can be multiple pairs from a single news-
paper pair, to avoid overlaps between training and
test sets, we split the dataset 80%/10%/10% based
on the paper DOI and balance across subjects. Fur-
ther dataset details in Appendix E
Selected Examples To highlight the difficulty of
S , we show a pair of samples from our final
dataset in Table 1. The IMS is compared to the
cosine similarity between embeddings produced
by SBERT. For the first case, SBERT presumably
picks up on similarities in the discussed topics,
such as erythritol and its relationship to adiposity,
but the paper finding is concerned with the con-
sistency of results and its biological implications
while the news finding explicitly mentions a re-
lationship between erythritol and adiposity. The
second case expresses the opposite effect; the news
finding contains a lot of extraneous information for1786
context, but one of the core findings it expresses
is the same as the paper finding, giving it a high
rating in S .
Comparison with existing datasets To further
characterize the difficulty of S compared to
existing datasets, we show the average normalized
edit distance between matching pairs in S ,
STSB (Cer et al., 2017), and SNLI (Bowman et al.,
2015) (see Appendix F for the calculation). STSB
is a semantic text similarity dataset consisting of
pairs of sentences scored with their semantic simi-
larity, sourced from multiple SemEval shared tasks.
SNLI is a natural language inference corpus, and
consists of pairs of sentences labeled for if they
entail each other, contradict each other, or are
neutral. We calculated the mean normalized edit
distance across all pairs of matching sentences in
each dataset’s training data; For S and STSB,
pairs are considered matching if their IMS or sim-
ilarity score is greater than 3, respectively. For
SNLI, pairs are considered matching if the label is
“entailment”.
We find that there is a much greater lexical differ-
ence between the matching pairs in S (0.726)
than existing general domain paired text datasets
(0.401 for STSB and 0.631 for SNLI). This gap
between STSB and S also emphasizes the
difference between traditional semantic textual sim-
ilarity tasks and the information change task we
describe here. Within S , Twitter pairs had
a higher distance (0.749) than news pairs (0.712),
suggesting stronger domain differences. For qual-
itative examples showing the difference between
S and STSB, see Appendix A.
Relationship of S to Fact Checking The
task introduced by S captures information
change more broadly than veracity as in automatic
fact checking, as the task is concerned with the
degree to which two sentences describe the same
scientific information—indeed, two similar sen-tences may describe the same information equally
poorly. Our task is similar to the sentence selec-
tion stage in the fact checking pipeline, and we
later demonstrate that models trained on S
data are useful for this task for science in section 5.
However, our task and annotation are agnostic to
whether a pair of sentences entail one another. This
is especially useful if one wants to compare how
a particular finding is presented across different
media. Fact-checking datasets are also explicitly
constructed to contain claims which are about a sin-
gle piece of information— S is not restricted
in this way, focusing on a more general type of in-
formation change beyond categorical falsehood. Fi-
nally, we note two more unique features of S :
1)S contains naturally occurring sentences,
while fact checking datasets like FEVER and Sci-
Fact often contain manually written claims. 2)
The combination of domains in S is unique;
sentences are paired between (news, science) and
(tweets, science), and these pairings don’t exist
currently.
4 Scientific Information Change Models
We now use S to evaluate models for esti-
mating the IMS of finding pairs in two settings:
zero-shot transfer and supervised fine-tuning.
4.1 Experimental setup
We use the following four models to estimate
zero-shot transfer performance. Paraphrase :
RoBERTa (Liu et al., 2019) pre-trained for para-
phrase detection on an adversarial paraphrasing
task (Nighojkar and Licato, 2021). We convert
the output probability of a pair being a paraphrase
to the range [1,5] for comparison with our labels.
Natural Language Inference (NLI) : RoBERTa
pre-trained on a wide range of NLI datasets (Nie
et al., 2020). The final score is the model’s mea-
sured probability of entailment mapped to the range
[1,5]. MiniLM : SBERT with MiniLM as the base
network (Wang et al., 2020a); we obtain sentence
embeddings for pairs of findings and measure the
cosine similarity between these two embeddings,
clip the lowest score to 0, and convert this score to
the range [1,5]. Note that this model was trained on
over 1B sentence pairs, including from scientific
text, using a contrastive learning approach where
the embeddings of sentences known to be similar
are trained to be closer than the embeddings of
negatively sampled sentences. SBERT models rep-1787
resent a very strong baseline on this task, and have
been used in the context of other matching tasks for
fact checking including detecting previously fact-
checked claims (Shaar et al., 2020). MPNet : The
same setting and training data as MiniLM but with
MPNet as the base network (Song et al., 2020).
We fine-tune the following six models on
S to estimate IMS as a comparison with zero-
shot transfer.
•MiniLM-FT : The same MiniLM model from
the zero-shot transfer setup but further fine-
tuned on S . The training objective is to
minimize the distance between the IMS and
the cosine similarity of the output embeddings
of the pair of findings.
•MPNet-FT : The same setup as MiniLM-FT
but using MPNet as the base network.
•RoBERTa : The RoBERTa (Liu et al., 2019)
base model; We perform a regression task
where the model is trained to minimize the
mean-squared error between the prediction
and IMS.
•SciBERT : A transformer model trained us-
ing masked language modeling on a large cor-
pus of scientific text (Beltagy et al., 2019).
The fine-tuning setup is the same as for the
RoBERTa model.•CiteBERT : A SciBERT model further fine-
tuned on the task of citation detection, and
was shown to have improved performance on
downstream tasks using scientific text (Wright
and Augenstein, 2021a). The training setup is
the same as for the RoBERTa model.
Please see Appendix G for further details on
the models and pretraining methods. For the fine-
tuned models, we train on the entire training set of
S , including both news findings and tweets.
For the test set we only use manually annotated
pairs. Performance is measured in terms of mean-
squared error (MSE) and Pearson correlation ( r)
(definitions of all metrics in Appendix F). All re-
sults are reported as the average and standard devi-
ation for each model across 5 random seeds.
4.2 Results
Paraphrase detection and natural language infer-
ence models perform very poorly for zero-shot
transfer on this task (Figure 2, grey bars), with
NLI having slightly better transfer, supporting our
hypothesis that transferring from existing tasks to
this domain is challenging. Fine-tuned models with
Masked Language Model (MLM) pretraining can
learn the task decently well (Figure 2, red bars),
but surprisingly RoBERTa performs just as well as1788SciBERT and CiteBERT which were specifically
pretrained on scientific texts. We posit that this
could be due to the fact that RoBERTa was pre-
trained on a wider range of texts that are reflective
of the domains in S , including news texts,
while SciBERT and CiteBERT were trained solely
on scientific papers.
SBERT models trained on large amounts of pre-
training sentences perform well in the zero-shot
transfer setup, with the MiniLM based model out-
performing MPNet. The best setup was using
SBERT fine-tuned on S (Figure 2, blue bars),
which yields up to 3.9 points gained overall in Pear-
son correlation and a reduction of 0.3 in terms of
MSE (MPNet to MPNet-FT). We also note that
there is a large gap between performance on this
data and general semantic similarity datasets such
as STSB, which see correlation scores in the 90s.
As such, there is potentially much room to grow in
terms of raw performance on this dataset.
Models performed worse for pairs with tweets
versus those from news (Appendix Table 7). This
performance difference is in line with our expec-
tations, as there is a large domain shift between
tweets and scientific texts and our base models
were not exposed to tweets during pre-training. All
models, including the zero-shot transfer SBERT
models, perform much worse on that split of the
data. Additionally, we only see minor gains in
performance in terms of MSE for MiniLM when
fine-tuned on tweets. We see larger gains for MP-
Net. Interestingly, the best performance (Pearson r)
for Tweets is RoBERTa, though the overall MSE is
still best for MPNet-FT. We show extended bench-
marking in Appendix J and the top-5 errors for
RoBERTa and MPNet-FT in Appendix K.
5 Application: Zero-Shot Evidence
Retrieval for Scientific Fact Checking
Accurately measuring the similarity of scientific
findings written in different domains enables a wide
range of downstream analyses and tasks. As a first
task, we consider evidence retrieval for scientific
fact checking of real-world scientific claims. In
general, automatic fact checking consists of retriev-
ing relevant evidence for a given claim and predict-
ing if that evidence supports or refutes the claim.
We test the ability of models trained on S
to perform the evidence retrieval task in a zero-
shot setting. In this, we use the models as is, with
no further fine-tuning on any evidence retrieval
data. We consider two fact checking datasets:
CoVERT (Mohr et al., 2022) is a dataset of sci-
entific claims sourced from Twitter, mostly in the
domain of biomedicine. We use the 300 claims and
the 717 unique evidence sentences in the corpus
in our experiment. COVID-Fact (Saakyan et al.,
2021) is a semi-automatically curated dataset of
claims related to COVID-19 sourced from Red-
dit. The corpus contains 4,086 claims with 3,219
unique evidence sentences.
Setup We compare different models’ ability to
rank the evidence sentences such that the ground
truth evidence for a given claim is ranked high-
est. We use four models in a zero-shot setting for
comparison (MiniLM, MiniLM-FT, MPNet, and
MPNet-FT; ’-FT’ indicates fine-tuning on S ),
and show results with the unsupervised BM25
(Robertson et al., 1994), a widely used bag-of-
words retrieval model. We report retrieval results
in terms of mean average precision (MAP) and
mean reciprocal rank (MRR), and average the re-
sults for models fine-tuned on S across 5
random seeds.
Results We find that fine-tuning on S pro-
vides consistent gains in retrieval performance on
both datasets for both SBERT models (Table 3).
This performance increase is encouraging, as there
are two notable differences between S and
the two datasets in our experiment. The first is that
the tasks are different: S provides a general
scientific information similarity task which proves
to be useful for evidence sentence ranking. The sec-
ond is that the domains are different: S con-
tains⟨news, paper ⟩and⟨tweet, paper ⟩pairs, while
CoVERT and COVID-Fact have claims from Twit-
ter and Reddit, respectively, paired with evidence
in news. Our results show that training on S
improves the IR performance of the SBERT mod-1789
els, despite the domain and topic differences from
our setting.
6 Application: Modeling Information
Change in Science Communication
Whether the media faithfully communicate scien-
tific information has long been a core question to
the science community (National Academies of
Sciences, Engineering, and Medicine, 2017). Our
dataset and models allow us to conduct a large-
scale analysis to study information change in sci-
ence communication. Here, we focus on three re-
search questions:
•RQ1: Do findings reported by different types
of outlets express different degrees of infor-
mation change from their respective papers?
•RQ2: Do different types of social media users
systematically vary in information change
when discussing scientific findings?
•RQ3: Which parts of a paper are more likely
to be miscommunicated by the media?
RQ1-2 focus on the holistic information change
captured in IMS , while RQ3 focuses on what types
of information might be changing.
6.1 RQ1: Comparing Media Outlets
Different types of media target different audi-
ences and tend to report the same issue differently
(Richardson, 1990; Mencher and Shilton, 1997).
While good science journalism requires outlets to
prioritize quality, in real practices, journalists may
adopt different writing strategies for different types
of audiences (Roland, 2009). Thus, we investigate
if findings reported by different types of outlets ex-
press different levels of information change, focus-
ing on three types of outlets: General News (e.g.,
NYTimes), Press Releases (e.g., Science Daily),
and Science & Technology (e.g., Popular Mechan-
ics). We use our best-performing MPNet-FT modelto estimate the IMS of over 1B pairs and keep those
with IMS >3, which finally leads to 1.1M paired
findings from 26,784 news stories and 12,147 pa-
pers. We then build a linear mixed effect regres-
sion model (Gałecki and Burzykowski, 2013) to
predict IMS for matching pairs from news stories
and research articles. We include a fixed effect for
the type of news outlet, using General News as
the reference category. To account for reporting
differences across fields and variations specific to
highly-publicized papers, we also include a fixed
effect for the scientific subject and a random effect
for each paper with 30+ pairs (all other papers are
pooled in a single random effect).
Results. Compared with General News, Science
& Technology news outlets and Press Releases re-
port findings that more closely match those from
the original paper (Figure 3 shows the regression
coefficients). This difference likely is due to some
form of audience design where the journalist is
writing for a more science-savvy readership in the
latter two, whereas General News journalists must
more heavily paraphrase the results for lay people.
6.2 RQ2: Comparing Social Media Accounts
Social media play an important role in dissemi-
nating scientific findings (Zakhlebin and Horvát,
2020), so what factors affect the presentation of
scientific information on social media becomes an
important question. Here, we focus on the types
of Twitter users who tweet about scientific find-
ings. Based on 182K matched tweets and paper
findings, we again build a linear mixed effect re-
gression model to predict IMS. We include fixed
effects of (1) if the account is run by an organiza-
tion, as inferred using M3 (Wang et al., 2019), (2) if
the account is verified (3) the number of followers
and following, both log-transformed, and (4) the
account age in years. We use the same field fixed
effects and paper random effects as in RQ1.
Results The type of user strongly influences how
faithful the tweets are to the original findings (Fig-
ure 4). Accounts from organizations tend to be
more faithful to the original paper findings, which
could be due to intentional actions of image man-
agement to build trust (Saffer et al., 2013). Sur-
prisingly, verified accounts were far more likely to
change information away from its original mean-
ing; similarly, accounts with more followers had
the same trend. Given their prominent roles in Twit-
ter communication (Bakshy et al., 2011; Hentschel1790
et al., 2014), multiple mechanisms may explain
this gap such as adding more commentary or try-
ing to translate original scientific findings to lay
language to make the findings easier to understand.
Appendix L shows the details of regression results.
6.3 RQ3: What Information Changes
Most studies on scientific misinformation focus
on paper titles and abstracts (e.g., Sumner et al.,
2014), which cannot fully reflect the information
presented in the full papers. Analyzing the informa-
tion change of findings paired from all sections of
papers could help to better understand the mecha-
nisms behind scientific misinformation and develop
strategies to reduce them. We use the same 1.1M
finding pair dataset as RQ1 and analyze what in-
formation might have changed using two models
trained for changes in scientific communication:
identifying exaggerations (Wright and Augenstein,
2021b) and certainty (Pei and Jurgens, 2021). See
Appendix H for more details on the exaggeration
detection task.
Results Journalists tend to downplay the cer-
tainty and strength of findings from abstracts (Fig-
ure 5), mirroring the results of Pei and Jurgens
(2021). However, this pattern does not persist for
findings in other parts of papers, especially the
limitations. Existing studies suggest that journal-
ists might fail to report the limitations of scientific
findings (Fischhoff, 2012), and our results here sug-
gest that findings presented in limitations are more
likely to be exaggerated and overstated. However,
it is also possible that scientists may adopt different
discourse strategies for different parts of a paper
(Clark, 2013). Nonetheless, our result obviates the
necessity of analyzing the full text of a paper when
studying science communication.
7 Conclusion
Faithful communication of scientific results is crit-
ical for disseminating new information and estab-
lishing public trust in science. Given the challenge
of—and occasional failures in—communicating
science, new resources and models are needed to
evaluate how science is reported. Here, we in-
troduce S , a new science communication
paraphrases dataset labeled with information sim-
ilarity. Extensive experiments demonstrate that
models can predict the degree to which two re-
ports of a scientific finding have the same informa-
tion but that this is a challenging task even for cur-
rent SOTA pre-trained language models. In down-
stream applications, we show S improves
model performance for evidence retrieval for sci-
entific fact checking; and, using the trained model
to perform a large-scale analysis of information
change in science communication, we show sys-
tematic behaviors in how different people and news
outlets faithfully convey scientific results. Data,
code, and pretrained models are available at .
AcknowledgementsThis project has received funding from
the European Union’s Horizon 2020 research
and innovation programme under the Marie
Skłodowska-Curie grant agreement No 801199 and
a Rackham Graduate Student Research Grant at the
University of Michigan.
Limitations
We note three limitations of our study. Our data
and analysis in social media is limited to only one
platform, Twitter, and includes only tweets directly
linked to the original paper, as indicated through
Altmetric. While Twitter is among the largest so-
cial media platforms and is the most common in1791the Altmetric data, our data potentially omits other
kinds of scientific communication about papers that
do not directly link to a paper or tweets that link
to a paper that cannot be easily identified to a DOI
(e.g., linking to a PDF hosted on a personal web-
site). Other types of tweets may be omitted from
our dataset such as those written in a thread, or in a
tweetorial, about a paper (Gero et al., 2021), which
may include additional tweets that describe a pa-
per’s findings. While our models would likely still
be able to effectively analyze such tweets, these
additional forms of scientific communication could
add new variety. We leave identifying and collect-
ing such tweets to future work.
Second, our study focuses on only four large sci-
entific fields. While these fields do cover a broad
selection of papers, we were unable to annotate ad-
ditional fields due to annotation budget and limita-
tions from the Prolific platform. On Prolific, not all
potential domains had sufficient numbers of quali-
fied annotators (we required at least a Bachelor’s
degree in the domain) and the number of unique
surveys to run scaled linearly with the number of
domains, creating a significant human overhead.
However, we will open source our annotation inter-
face and pipeline and we encourage further efforts
to build a larger dataset across more scientific do-
mains.
Finally, while our models achieve moderately
high performance at inferring the information
matching (Figure 2), performance is not perfect,
which potentially limits our ability in downstream
models and tasks. While we show the data is still
useful in training for related tasks (§5) and a trained
model can be used to identify systematic behav-
ior by types of users and outlets (§6), more accu-
rate models would likely be needed to identify any
trends for finer-grained settings, such as looking at
the behavior of a specific outlet. For this reason,w e
have kept our analyses at a higher level (e.g., outlet
categories).
Ethics and Impacts
Miscommunication of scientific information can
have negative impacts on many aspects of our so-
ciety. Our study contributes to a large research
program on the science of science communications
(National Academies of Sciences, Engineering, and
Medicine, 2017). Our dataset and model could be
used to keep track of information change in sci-
ence communication, enable large-scale analysisto understand the current science communication
ecosystem, and finally help to facilitate better and
more effective science communications.
Crowdsourcing ethics Annotating paired find-
ings requires deep attention and may lead to annota-
tor burnout. We carefully designed our annotation
pipeline to provide a good annotation experience
for the annotators. We designed a user-friendly
Web-based annotation interface that allows annota-
tors to do annotations using keyboard shortcuts. All
the annotators are encouraged to leave comments
and answer several questions about their annota-
tion experience. More than 95% of the annotators
are satisfied with their annotation experience and
many people suggest that our study helps them to
better understand the science communication pro-
cessand our annotation interface makes their task
easier.
References1792179317941795
A Information Change vs. Semantic
Similarity
We wish to highlight key differences between infor-
mation change and semantic similarity, particularly
with an eye to what makes the task introduced in
S difficult compared to semantic similarity
scoring. To illustrate this, we present a sample of
pairs in STSB that have the highest similarity score
of ‘5’ vs. samples in S which have an IMS
of 5 in Table 4 and Table 5.
In this, for a pair to be perfectly similar from
a semantics perspective, the entire sentence must
contain exactly equivalent meaning. This is not
the case with our task. For the information change
task, pairs are highly similar even if some aspects
of the semantics of the sentence are changed e.g. in
the first sample, there is a difference between the
two sentences semantically: the second in the pair
discusses “being intrigued” by the finding, which
is shared between the pair. This also makes the task
extremely difficult – a model must learn to compare
only the salient scientific facts between the pair of
sentences, as opposed to the entire meaning of each
sentence.
B Pilot Annotation Details
For the pilot, we use 20 pairs from 20 differ-
ent cosine similarity score bins in increments of
0.05starting from 0. In other words, we have
20 bins with ranges of scores as: 0.0−0.05,
0.05−0.1...0.9−0.95,0.95−1.0. This results
in 400 samples to annotate. The score distribution
from 7,392,690 pairs from 3,525 source papers
which we use for sampling is given in Figure 6.
Each sample is annotated by two of the authors
of the study with a binary label of “matching” vs
“not matching”, yielding a Krippendorff’s alpha of
0.73.
The number of positive samples per bin from the
pilot study is given in Figure 7. We see here that
bins with a cosine similarity below 0.65 tend to
have very few positive samples, and only above 0.8
do we start to see many positive samples in the bins.
Almost all samples above 0.9are matching, and
the only unmatched pairs appear to be instances of
SBERT failing, since the matched pairs are almost
exactly copied text. Additionally, this histogram
indicates that the base rate of positive matching
findings is low as the overall distribution of samples
in the high cosine similarity region, where most of
the matches exist, is small. At the same time, we
note that some of the matches we find in the lower
cosine similarity regions constitute quite interesting
samples; for example, the following which has a
cosine similarity of 0.41.
Paper finding: For cases comparing a
drone and a vehicle carrying a single
package over similar distances, for ex-
ample, a customer picking up a package
from a retail store, the drone is clearly a
lower-impact solution. (Stolaroff et al.,
2018)1796
News finding: But if you forgot that
essential ingredient for tonight’s dinner,
our findings suggest it’s much better to
have the grocery store send it to you by
drone rather than to take your car to the
store and back.
Both sentences are talking about the same find-
ing, that drone delivery is more efficient over short
distances than using a car, but in entirely different
ways. From this, it is clear that simply using se-
mantic text similarity is insufficient for solving this
task, and we should include some of these lower
similarity samples in our annotation. We, therefore,
propose the following sampling scheme in order
to balance the number of annotations we can ac-
quire, the yield of positive samples, and the sample
difficulty:
•Label all samples with a cosine similarity be-
low0.4as unmatched.
•Label all samples above 0.9with a Jaccardindex above 0.5as matching.
•Sample an equal number of pairs from each
0.05increment bin between 0.4and0.9for
human expert annotation.
C Experimented annotation
We experimented with two annotation schemas: a
binary schema where the annotators are asked to
label “whether the two sentences are discussing
the same scientific finding” with Yes or No, and a
Likert schema where the annotators are asked to
label if “The information in the findings is...”
• 1: Completely different
• 2: Mostly different
• 3: Somewhat similar
• 4: Mostly the same
• 5: Completely the same
We ran several pilots using the two annotation
schemas and the Likert a schema led to higher inter-
annotator agreement (0.45 Krippendorff’s alpha)
compared with the binary schema (0.21 Krippen-
dorff’s alpha). Therefore we adopt the 5-point Lik-
ert schema for the annotation.1797D Full Annotation Instructions
Annotation was performed using Prolific work-
ers who labeled using P (Pei et al.,
2022). The annotation interface setup is avail-
able at which
includes all the following instructions as well.
Task description: The task is to label to what de-
gree two sentences have the same information. The
information in the sentences is scientific findings.
Here, a scientific finding is a statement that de-
scribes a research output of a scientific study, such
as a result, conclusion, product, etc. You should
rate how similar the findings are; you can ignore
extra information like “The researchers showed...”,
“In vivo experiments demonstrated...” etc. For ex-
ample, in the sentence “After controlling for weight
and age, researchers found that overconsumption
of sugar is linked with an increase in diabetes,” the
information in the finding is “overconsumption of
sugar is linked with an increase in diabetes”. Some
sentences may have no findings or multiple find-
ings, so use your best judgment about what are the
core findings being said.
You will rate this on a 5-point scale, where each
level means the following:
1.The information in the findings is completely
different
•Sentences in this category have findings
which say completely different informa-
tion
•The sentences may be on totally different
topics
–Overconsumption of sugar causes di-
abetes
–Regular exercise improves heart
health
•There may be some overlap in key words
used between the two sentences, but the
actual information is completely differ-
ent
–Chocolate contains a lot of sugar,
and therefore can have an effect on
weight.
–Overconsumption of sugar leads to
diabetes.
2.The information in the findings is mostly dif-
ferent•The findings may talk about the same
topic, but the actual information is
mostly different; for example, these sen-
tences convey mostly different informa-
tion even though they talk about the same
topic:
–Overconsumption of sugar causes di-
abetes
–Sugar is good for your health
•There could be a link between the two
findings, but the information conveyed is
still different
–Overconsumption of sugar increases
blood glucose levels
–High blood glucose over time in-
creases the risk of developing dia-
betes
3.The information in the findings is somewhat
similar
•The findings are discussing relevant re-
search outputs but there are some differ-
ences in the information conveyed. Here
the difference is that (i) talks about the re-
lationship between overconsumption of
sugar and diabetes and (ii) describes how
genetics plays a role in overconsumption
of sugar
–Overconsumption of sugar causes di-
abetes
–Overconsumption of sugar might be
genetically determined
4.The information in the findings is mostly the
same
•In this case there may be some changes in
e.g. the level of generality. Additionally,
one sentence may go into more detail
than the other and add additional context,
but the information is largely the same
•Here the two findings have the same in-
formation but at different levels of gener-
ality:
–A link between sugar and diabetes
was found
–Overconsumption of sugar is associ-
ated with the onset of diabetes
•Here both sentences have the same core
finding, but one sentence goes into more
detail1798
–Overconsumption of sugar causes di-
abetes
–Experiments demonstrated that over-
consumption of sugar led to an in-
crease in blood glucose levels, which
over a long enough time period was
linked to an increased prevalence of
diabetes in the cohort.
• One finding could support the other
–Overconsumption of sugar causes di-
abetes
–Overconsumption of sugar can have
negative effects on health
5.The information in the findings is completely
the same
•In this case there is complete overlap in
the information in the findings conveyed
by the two sentences
–Overconsumption of sugar leads to
diabetes.
–The researchers found that overcon-
sumption of sugar leads to diabetes
•Note that there can be changes in e.g. the
level of certainty or the strength of the
information.
–Overconsumption of sugar leads to
diabetes.
–It is likely that there is a link between
overconsumption of sugar and the on-
set of diabetes.
E Final dataset details
Figure 9 shows the IMS distribution in S .
Figure 10 shows the IMS distribution for annotated
pairs in S . Figure 11 shows the IMS distri-
bution for each split.
We measure various aspects of lexical richness
between the different domains of the data in Ta-
ble 6.1799
F Metrics
Average Normalized Edit Distance We calcu-
late the normalized edit distance as follows:
d=1
|D|/summationdisplayd(s, s)
max (|s|,|s|)
where |D|is the size of the dataset, (s, s)is a
sentence pair, and dis the edit distance.
Jaccard Index The Jaccard index is calculated
based on the overlap of the members of two sets
(e.g. the words in two sentences XandY):
J=|X∩Y|
|X∪Y|
Cosine Similarity The cosine similarity between
two vectors aandbis calculated as:
S(a,b)=a·b
∥a∥∥b∥
Which is their dot product divided by the product
of their lengths.
Mean Squared Error The mean squared error
between two lists of numbers of length nis calcu-
lated as:
MSE (Y,ˆY) =1
n/summationdisplay(Y−ˆY)
Mean Average Precision The mean average pre-
cision in ranking takes the average Precision@k
(P@k) for every relevant sample in a ranked list.
First, P@k is calculated as follows:
P@k(ˆY) =1
k/summationdisplay1(ˆY= 1)where 1is the indicator function. The average
precision is then taken over all relevant items in the
list, where there are rrelevant items:
AP(ˆY) =1
r/summationdisplayP@k(ˆY[:k])where ˆY= 1
The mean average precision for a set of nranked
listsDis then the mean of the average precision of
each of these lists:
MAP =1
n/summationdisplayAP(D)
Mean Reciprocal Rank The mean reciprocal
rank (MRR) calculates the mean rank for each rele-
vant item in a list i.e. its position in that list. It is
calculated as follows for Dlists or relevant items
inˆYranked lists:
MRR (D) =1
|D|/summationdisplay1
|D|/summationdisplay1
rank(ˆY)
where rank(ˆY)is the rank of item iin list ˆY.
G Full Model Details
All baseline experiments were run on a shared clus-
ter. Requested jobs consisted of 16GB of RAM and
4 Intel Xeon Silver 4110 CPUs. We used a single
NVIDIA Titan RTX GPU for experiments. Train-
ing takes approximately 3 minutes for all MLM-
based models and 2 minutes for SBERT models.
RoBERTa RoBERTa is a large pretrained trans-
former language model, trained using the masked
language modeling (MLM) objective on a large
corpus of English text. We use the base model of
RoBERTa for our experiments. Huggingface model
name: roberta-base – 124,647,170 parameters
MiniLM We use a popular pretrained SBERT
model based on MiniLM (Wang et al., 2020b),
which is trained by distilling multiple language
models into one compressed model. SBERT uses
siamese BERT encoders to obtain sentence em-
beddings for pairs of sentences and is trained to
decrease the distance between these two embed-
dings. The pretraining for the sentence similarity
task consists of a wide range of datasets covering
multiple domains and >1 billion sentence pairs,
including science (Cohan et al., 2020; Lo et al.,
2020). As much of the data is collected automati-
cally, it uses a contrastive learning objective where
known relevant pairs are treated as positive values1800
and other samples in a batch are treated as negative
values. The model is then trained to minimize the
cross-entropy between the dot-product of embed-
dings and the label acquired from positive/negative
samples. Huggingface model name (sentence trans-
formers): all-MiniLM-L6-v2 – 22,713,216 param-
eters
MPNet This is the same setup as in MiniLM
but with using MPNet as the base network (Song
et al., 2020). MPNet is trained using a permuted
language modeling (PLM) objective with position
information as input to achieve the best of both
worlds between MLM and PLM. The base network
is used in the SBERT setup where it is further fine-
tuned on the same dataset and same task as with
MiniLM
Huggingface model name (sentence transform-
ers): all-mpnet-base-v2 – 109,486,464 parameters
Paraphrase Detection This is a paraphrase de-
tection model based on RoBERTa used in (Nigho-
jkar and Licato, 2021). The model is trained on the
adversarial paraphrase dataset introduced in that
paper.
Huggingface model name (sentence transform-
ers): coderpotter/adversarial-paraphrasing-detector
– 124,647,170 parameters
NLI This is a RoBERTa model trained on a wide
array of NLI datasets, including SNLI (Bowman
et al., 2015), MNLI (Williams et al., 2018), FEVER
(a fact-checking dataset) (Thorne et al., 2018) and
ANLI (Nie et al., 2020).
Huggingface model name (sen-
tence transformers): ynie/roberta-large-
snli_mnli_fever_anli_R1_R2_R3-nli –
124,647,170 parametersSciBERT SciBERT is the original BERT model
trained using MLM on a large set of scientific pa-
pers from Semantic Scholar (Lo et al., 2020).
Huggingface model name (sentence trans-
formers): allenai/scibert_scivocab_uncased –
109,920,514 parameters
CiteBERT CiteBERT is SciBERT further fine-
tuned on the CiteWorth dataset for the task of cita-
tion detection, which predicts if a given sentence
requires a citation or not (Wright and Augenstein,
2021a).
Huggingface model name (sentence transform-
ers): copenlu/citebert – 109,920,514 parameters
We use sane defaults when fine-tuning each of
our models. In this, for the MLM based models
we use [lr: 2e-5, n_epochs: 3, warmup_steps: 200,
weight_decay: 0.01, batch_size: 8]. For SBERT
models we use the same setting except we train for
5 epochs.
H Exaggeration Detection
The problem of scientific exaggeration detection
was studied in (Wright and Augenstein, 2021b).
The basic task is: given a pair of scientific find-
ings (e.g. a reference finding from a paper and
its counterpart in a news article), determine if one
finding is exaggerating the other finding. More for-
mally, the task focuses on differences in the causal
claim strength of the two findings, where the claim
strength can take on one of four values:
• 0: No statement of relationship
•1: Correlational statement (e.g. “X is associ-
ated with Y”)
•Conditional causal statement (e.g. “X might
cause Y under circumstance Z”)
• Causal statement (e.g. “X causes Y”)1801Wright and Augenstein (2021b) curate data and
build models for performing the exaggeration detec-
tion task in two different settings: as predicting the
individual claim strengths and comparing, and as
an inference task where a model is fed both findings
and asked to predict if the reference finding is being
exaggerated, downplayed, or faithfully represented
by its counterpart. We use the best-performing
model from their paper, which is a multi-task few-
shot learning model based on pattern exploiting
training (PET) called MT-PET. In particular, we
use the model for strength classification which has
seen 4,500 individual findings labeled for claim
strength and 200 pairs labeled for exaggeration.
I Scientific Text Parser
We fine-tuned a RoBERTa model over 200K self-
labeled abstracts from PubMed. The model is
trained to predict five labels including: BACK-
GROUND, CONCLUSIONS, METHODS, OB-
JECTIVE and RESULTS. We did a 8:1:1 split for
the data and fine-tune the RoBERTa model for 1
epoch. 0.92 F1 is attained on the test set.
J Extended Benchmarking
Tables with extended benchmarking results can be
found in Table 7 to Table 11.
K Error Examples
Examples of errors which our best models made on
⟨tweet, paper ⟩pairs can be found in Table 12 and
Table 13.
L Regression details
Table 15 shows the regression table for RQ1. Ta-
ble 16 shows the regression table for RQ2.18021803180418051806Model: MixedLM Dependent Variable: paper_sentence_score
No. Observations: 1111150 Method: REML
No. Groups: 6705 Scale: 0.1084
Min. group size: 31 Log-Likelihood: -349944.7797
Max. group size: 67063 Converged: Yes
Mean group size: 165.7
βCoef. Std.Err. z P >|z|[0.025 0.975]
Intercept 3.299 0.007 489.729 0.000 3.286 3.312
Outlet Type: Press Release 0.037 0.001 31.187 0.000 0.035 0.039
Outlet Type: Science & Technology 0.034 0.001 30.581 0.000 0.032 0.036
Field: Biology -0.018 0.020 -0.904 0.366 -0.056 0.021
Field: Psychology 0.040 0.018 2.168 0.030 0.004 0.076
Field: Medicine 0.206 0.017 11.813 0.000 0.171 0.240
Field: Computer_science 0.050 0.024 2.132 0.033 0.004 0.096
Group Var 0.009 0.001
Model: MixedLM Dependent Variable: paper_sentence_score
No. Observations: 182735 Method: REML
No. Groups: 1360 Scale: 0.1525
Min. group size: 31 Log-Likelihood: -89654.8514
Max. group size: 89523 Converged: Yes
Mean group size: 134.4
βCoef. Std.Err. z P >|z|[0.025 0.975]
Intercept 3.777 0.013 292.571 0.000 3.752 3.803
Is Verified User? -0.047 0.004 -11.044 0.000 -0.056 -0.039
Is Organizational Account? 0.042 0.002 -19.026 0.000 -0.046 -0.037
User Metric: log(Followers) -0.003 0.001 -5.059 0.000 -0.004 -0.002
User Metric: log(Following) 0.000 0.001 0.369 0.712 -0.001 0.002
User Metric: Account Age (in years) 0.004 0.000 10.824 0.000 0.003 0.005
Field: Biology -0.025 0.030 -0.850 0.395 -0.083 0.033
Field: Psychology 0.308 0.028 11.052 0.000 0.254 0.363
Field: Medicine 0.206 0.026 7.826 0.000 0.155 0.258
Field: Computer_science -0.352 0.035 -10.158 0.000 -0.420 -0.284
Group Var 0.059 0.0061807