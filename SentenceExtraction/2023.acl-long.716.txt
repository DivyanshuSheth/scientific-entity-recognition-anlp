
Liang MaShuyang CaoRobert L. Logan IVDi Lu
Shihao RanKe ZhangJoel TetreaultAlejandro JaimesDataminr Inc.University of Michigan, Ann Arbor
{lma,rlogan,dlu,sran,kzhang,jtetreault,
ajaimes}@dataminr.com caoshuy@umich.edu
Abstract
The proliferation of automatic faithfulness met-
rics for summarization has produced a need
for benchmarks to evaluate them. While exist-
ing benchmarks measure the correlation with
human judgements of faithfulness on model-
generated summaries, they are insufficient for
diagnosing whether metrics are: 1) consistent ,
i.e., indicate lower faithfulness as errors are
introduced into a summary, 2) effective on
human-written texts, and 3) sensitive to dif-
ferent error types (as summaries can contain
multiple errors). To address these needs, we
present a benchmark of unfaithful minimal
pairs (BUMP), a dataset of 889 human-written ,
minimally different summary pairs, where a sin-
gle error is introduced to a summary from the
CNN/DailyMail dataset to produce an unfaith-
ful summary. We find BUMP complements
existing benchmarks in a number of ways: 1)
the summaries in BUMP are harder to discrimi-
nate and less probable under SOTA summariza-
tion models, 2) unlike non-pair-based datasets,
BUMP can be used to measure the consistency
of metrics, and reveals that the most discrimi-
native metrics tend not to be the most consis-
tent, and 3) unlike datasets containing gener-
ated summaries with multiple errors, BUMP
enables the measurement of metrics’ perfor-
mance on individual error types.
1 Introduction
Although modern abstractive summarization sys-
tems have improved in their ability to produce flu-
ent text (Lewis et al., 2020), their ability to gen-
erate text that is factually grounded in the source
article remains an issue (Kryscinski et al., 2020).
This phenomenon has inspired the NLP community
to develop faithfulness evaluation metrics (Fabbri
et al., 2022; Laban et al., 2022; Honovich et al.,
2021; Scialom et al., 2021) that automatically mea-
sure the extent to which abstractive summarization
systems produce summaries that contain informa-
tion that cannot be verified by the source article.As the number of these automatic faithfulness
metrics has increased, there has arisen a corre-
sponding need for benchmarks that evaluate their
relative strengths. To satisfy this need, researchers
have developed datasets such as FRANK (Pagnoni
et al., 2021) and TRUE (Honovich et al., 2022)
that are comprised of model-generated summaries
along with human-annotated faithfulness levels. Al-
though these datasets are useful for evaluating the
degree to which faithfulness metrics correlate with
human judgements and can discriminate unfaith-
ful summaries, a number of factors limit the con-
clusions that can be drawn from them. For one,
because model summaries can vary in terms of
length, content, and number of errors, these bench-
marks are ill-suited for drawing conclusions about
theconsistency (Gabriel et al., 2021) of metrics,
i.e., whether their scores indicate lower faithful-
ness as summaries become increasingly unfaithful,
as well as their sensitivity to specific types of errors
since summaries can contain multiple errors. Fur-
thermore, as the summaries are machine-generated,
these benchmarks cannot evaluate whether metrics
can detect human-written unfaithful summaries.
To enable research on these topics, we present
BUMP—a benchmark of unfaithful minimal pairs—
a dataset of 889 minimally different summary pairs
where all unfaithful summaries are generated by
human annotators . As illustrated in Figure 1, given
an article and its reference summary, we ask a hu-
man annotator to edit the reference summary in
a minimal way such that the edited summary ex-
hibits one unfaithful error. We design two tasks
for performance comparisons: 1) taxonomy-based
edits, where a specific unfaithfulness error type is
required according to our proposed taxonomy, and
2) freestyle edits, where no error type constraints
are imposed. The motivation behind the first task
setting is to ensure that different error types are
adequately represented in our dataset, while the
second task setting is important for understanding12788
the completeness of our error type taxonomy as
well as whether annotation difficulty is affected
by instructing annotators to focus on specific error
types.
We use BUMP to study the ability and perfor-
mance consistency of faithfulness evaluation met-
rics in differentiating unfaithful summaries from
faithful ones. Similar to how minimal pairs are
used to diagnose linguistic knowledge of language
models (Marvin and Linzen, 2018; Warstadt et al.,
2020), the minimal summary pairs in BUMP allow
targeted tests of a metric’s consistency on different
types of errors (Table 1). This setup minimizes
the effect of confounding factors that affect simi-
lar analyses (e.g., Pagnoni et al. (2021) and Tang
et al. (2022)) such as text length, stylistic variation,
and multiple errors occurring in the same summary.
We evaluate standard and state-of-the-art faithful-
ness metrics on BUMP using meta-evaluation pro-
tocols that target two phenomena: 1) consistency ,
i.e. the fraction of unfaithful summaries that re-
ceive a lower score than their corresponding faith-
ful summaries, and 2) discriminability , i.e., the
metric’s ability to classify unfaithful vs. faithful
summaries as measured by ROC AUC.
Our results (Section 5) yield a number of use-
ful findings: 1) BUMP differs substantially from
existing benchmarks : the summaries in BUMP are
harder to discriminate (ROC AUC scores between
50–70 %vs. 70–84 %) and are less probable under
SOTA summarization models. 2) Discriminability
!= consistency : interestingly, the most consistentmetrics ( BARTS ,CC) tend to have poor
discriminability. 3) Some error types are harder
than others : e.g., metrics seem to uniformly strug-
gle with summaries containing Intrinsic Error s.
In sum, our contributions are three-fold: 1) We
build a benchmark of human-generated unfaith-
ful minimal pairs (BUMP) for evaluating faith-
fulness metrics. 2) We show human-generated
unfaithful errors are substantially different from
and more challenging than model-generated ones.
3) We demonstrate how BUMP provides insights
on both the consistency and discriminative abil-
ity of faithfulness metrics on different error types
than prior evaluation benchmarks that complement
insights from existing benchmarks. The BUMP
dataset is available at: https://github.com/
dataminr-ai/BUMP .
2 Related Work
Standard evaluation metrics for text generation
tasks, e.g., BLEU andROUGE , do not correlate
well with human judgements of factual alignment
in summarization settings (Kryscinski et al., 2019;
Maynez et al., 2020). This has motivated the de-
velopment of automated faithfulness metrics that
quantify factual alignment through methods that ei-
ther: use NLI to measure the entailment degree be-
tween the source article and summary (Kryscinski
et al., 2020; Goyal and Durrett, 2020; Laban et al.,
2022), compare summary probabilities when rele-
vant information is removed from the source (Xie
et al., 2021), or use question answering models to12789measure if questions derived from the source can
be answered by the summary and vice versa (Wang
et al., 2020; Durmus et al., 2020; Scialom et al.,
2021).
Existing faithfulness metric evaluations use
one of two classes of benchmarks: 1) machine-
generated summaries paired with human-annotated
faithfulness levels (Laban et al., 2022; Pagnoni
et al., 2021; Tang et al., 2022), and 2) summary
pairs pertaining to the same article where one sum-
mary is faithful and the other is unfaithful (Falke
et al., 2019; Gabriel et al., 2021). While both
classes can evaluate a metric’s ability to discrimi-
nate unfaithful summaries, the latter additionally
allows for consistency tests, i.e., whether metrics
assign higher values to more faithful summaries.
The BUMP dataset belongs to the second class
of benchmarks; however, it has a number of unique
properties. First, unlike both Falke et al. (2019)
and Gabriel et al. (2021), the unfaithful summaries
in BUMP are human-written. In addition, the un-
faithful summaries in BUMP are minimally dif-
ferent , in the sense that only a single error differ-
entiates the faithful and unfaithful summary. As
shown in Section 5, this produces summary pairs
that are substantially more challenging for metrics
to differentiate. Inspired by the use of minimal
pairs to diagnose linguistic knowledge of language
models (Marvin and Linzen, 2018; Warstadt et al.,
2020), the benefit of this approach is that it allows
targeted tests of a metric’s consistency on differ-
ent types of errors (Section 3.2) while minimiz-
ing the effect of confounding factors. Therefore,
unlike other benchmarks with error type annota-
tions (Pagnoni et al., 2021; Tang et al., 2022), re-
sults on BUMP are not complicated by issues such
as multiple errors appearing in the same summary.
3 Benchmark of Unfaithful Minimal
Pairs (BUMP)
Two annotation tasks are designed for BUMP,
where Task 1 is taxonomy-based (a specific er-
ror type is required for the edited summary), and
Task 2 allows freestyle edits (i.e., no error type
constraints are imposed). In this section, we first
describe how data sources are selected to build
BUMP (3.1), and then describe the details of the
two annotation tasks (3.2 and 3.3).3.1 Dataset
For Task 1, we randomly select 100 article-
summary pairs from the test set of the
CNN/DailyMail dataset (Hermann et al., 2015).
For Task 2, we select an additional 100random
article-summary pairs. Both tasks are performed
via Amazon Mechanical Turk.
3.2 Task 1: Taxonomy-based Unfaithful
Summaries
To obtain fine-grained performance evaluations of
faithfulness metrics, it is critical to evaluate their
sensitivity regarding various error types. Further-
more, benchmarks should contain sufficiently many
instances associated with each error type to enable
statistically significant comparisons to be made. To
this end, we first define a taxonomy of unfaithful
error types, and then ask annotators to introduce er-
rors of a specific type in order to ensure each error
type is adequately represented in the final dataset.
We note that existing taxonomies of error types
may contain overlapped error types, e.g., grammat-
ical vs. entity errors in FRANK (Pagnoni et al.,
2021) or lack fine granularity, e.g., Tang et al.
(2022). By considering the strengths and short-
comings of existing taxonomies, we define our own
taxonomy in Table 1. Our taxonomy is first adapted
from the one in FRANK (Pagnoni et al., 2021) by
including semantic frame errors ( Predicate Error ,
Entity Error , and Circumstance Error ) and Coref-
erence Error , and removing error types that might
overlap with others. To further categorize each
semantic frame error, we adopt the notions of In-
trinsic andExtrinsic errors (Maynez et al., 2020;
Goyal and Durrett, 2020; Tang et al., 2022). Note
that we do not simply categorize errors into the
Intrinsic andExtrinsic ones, as we believe seman-
tic frame errors can better instruct annotators to
create summaries with diverse unfaithful errors. In
our taxonomy, the Intrinsic /Extrinsic distinction
only applies to the Predicate, Entity, and Circum-
stance Error , since for a Coreference Error , it is
generally ambiguous whether an erroneous pro-
noun/reference that does not exist in the source
article should be regarded as intrinsic or extrinsic.
In total, this results in seven different error types.12790For each of the seven error types in this taxon-
omy, given an article-summary pair, we ask the
annotator to introduce an error of the required type
through a minimal edit to the reference summary.
All<article, summary, error type> Human Intelli-
gence Tasks (HITs) in Amazon Mechanical Turk
are shuffled and there is no annotation repetition,
i.e., one assignment per HIT. This increases the
chance that edits of the same reference summary
will be made by different annotators. Additional
details regarding qualification tests and annotation
instructions are presented in Appendix A.
After the data collection, we manually check the
validity of each edit. For cases where the edits
do not match the required error types, we relabel
them with the corrected error types based on our
taxonomy. The dataset statistics after correction
are shown in Table 2. For this task, one common
mistake is that annotators consider the quantity of a
noun object as a circumstance and make edits to the
quantity (the first example in Table 3), hence mis-
takenly treat Entity Error s asCircumstance Error s,
which causes the total number of Circumstance
Error s to be only 160 (much smaller than that of
Entity Error s; see Table 2). Another frequent mis-
take is that the edited word actually exists in the
original article for the required extrinsic error (the
second example in Table 3), which results in a
smaller number of Extrinsic Error s than intrinsic
ones across all semantic frame errors, especially
forPredicate Error s. Furthermore, Table 2 shows
all edited summaries can be categorized by our tax-
onomy (no summaries are relabeled as “Other”),
and the incorrect response rate is 16%, suggesting
that, in general, annotators correctly respond with
the required error types.
3.3 Task 2: Freestyle Unfaithful Summaries
In addition to the taxonomy-based Task 1, we also
conduct a separate task, Task 2, where annotators
can edit reference summaries in any way they want,
i.e., freestyle editing, as long as only one error is
introduced to the reference summary via minimal
edits. The goal of Task 2 is to understand how
human-generated unfaithful summaries may vary,
and how the performance of faithfulness evaluation
metrics changes accordingly, when there are no
error type constraints. In particular, only annotators
who did not participate in the qualification test of
Task 1 are considered to participate in this task; in
this way, we ensure the edited summaries in Task 2
are not constrained to any known error types.
To post-process all data collected in Task 2, we
manually assign an error type to each data point,
based on our error type taxonomy in Task 1. With-
out informing annotators of any specific error types,
we observe the rate that the “Other” label occurs
is only 2.5%for Task 2 in Table 2. This confirms
that the vast majority of errors produced by humans
adhere to our proposed taxonomy. For more details
on Task 2, please see Appendix B.
Remark. For both tasks, we ask annotators to
introduce only oneerror (by editing the reference
summary in a minimal way). We acknowledge that
some reference summaries may be unfaithful in
the first place; nevertheless, for both tasks, edited
summaries are based on reference summaries, by
which we ensure the edited summaries are always
more unfaithful than reference summaries.
4 Meta-Evaluation of Faithfulness
Evaluation Metrics
In this section, we first describe the faithfulness
evaluation metrics benchmarked on BUMP (4.1).12791
Then meta-evaluation protocols are discussed (4.2).
4.1 Faithfulness Metrics
To cover diverse types of faithfulness metrics, in
this section, we select metrics that are generally
used for measuring generation quality (i.e., n-gram-
based metrics), recent metrics that are proposed
specifically for faithfulness evaluations, as well as
some pre-trained model based metrics, which are
detailed as follows. We investigate their abilities
to distinguish faithful summaries from their mini-
mally edited counterparts.n-Gram-Based Metrics: We evaluate the fol-
lowing 2 n-gram-based metrics: BLEU (Papineni
et al., 2002) and ROUGE (ROUGE-2 Precision
specifically) (Lin, 2004).Faithfulness Evaluation Metrics: We evalu-
ate the following 7 faithfulness evaluation metrics:
Q E (Scialom et al., 2021), Q(Honovich
et al., 2021), QAFE (Fabbri et al., 2022),
FCC(Kryscinski et al., 2020), DAE (Goyal
and Durrett, 2020), S C(Laban et al., 2022)
andCC(Xie et al., 2021) in this paper. To ob-
tain a score for FCC, we take the classifier
probability of the summary being faithful.Other Metrics: We evaluate the following 3 pre-
trained model based metrics: BLEURT (Sellam
et al., 2020) with the BLEURT -20 checkpoint (Pu
et al., 2021), BERTS (Zhang et al., 2020)
(specifically the BERTS -precision variant)
using the DeBERTa-xl-MNLI model (He et al.,
2021), and BARTS (Yuan et al., 2021) with
a BART (Lewis et al., 2020) model fine-tuned on
the CNN/DailyMail dataset.
Note that for reference-based metrics, faithful-
ness scores are computed by treating the input arti-
cle as the reference, and the reference/edited sum-mary as the system output. We also normalize the
direction of the metric score so that a higher score
always corresponds to better faithfulness from the
metric’s view, e.g., FCCpredicts the probabil-
ity that a summary is unfaithful, and so to obtain a
faithfulness score, we take its complement.
4.2 Meta-Evaluation
Each faithfulness metric takes an article-summary
pair and outputs a numerical faithfulness score. In
our analysis, we measure faithfulness scores for
both the reference summary as well as the human-
annotated erroneous summary. We quantify the
difference between faithfulness metrics on BUMP
using two measurement protocols: consistency and
ROC AUC . Originally introduced in Gabriel et al.
(2021), consistency measures the success rate of a
metric assigning a lower faithfulness score to the
erroneous unfaithful summary. In contrast, ROC
AUC instead measures the overall capability of a
metric to discriminate faithful from unfaithful con-
tent for an input summary, and has previously been
used by Honovich et al. (2022) for meta-evaluation.
Although other metrics such as balanced accuracy
have also been used to evaluate disciminability (La-
ban et al., 2022), we opt to use ROC AUC as it
does not require determining a decision threshold.
5 Results
We report and analyze the performance of faithful-
ness metrics in this section using meta-evaluation
protocol consistency and ROC AUC.
Consistency. The consistency studies of the two
tasksfor all the metrics are reported in Table 4. In
terms of the difficulty per error type, 1) for Task 1,12792
Extrinsic Entity Error s are generally the easiest,
while all but BARTS struggle with Intrinsic
Predicate Error s; 2) for Task 2, Intrinsic Entity
Error s are the hardest. This implies that when an-
notators are not presented with any error types, the
introduced error styles may differ from those in
Task 1 (see Section 6), potentially causing inconsis-
tencies for metrics in these two tasks. Nevertheless,
we observe that for both tasks, Intrinsic Error s are
more challenging than extrinsic ones across all but
FCCin Task 2. This is likely because Intrin-
sic Error s can be derived from the original article,
while Extrinsic Error s contain words that do not ap-
pear in the original article, making Intrinsic Error s
more subtle to be identified than extrinsic ones.
For the overall performance (all error types are
considered), BARTS has the highest consis-
tency in both tasks, though BARTS has not
been proposed specifically for faithfulness evalua-
tions. Other metrics that rank top 4 in both tasks in-
clude QAFE andCC. By comparison,
QandFCChave the worst consistency, even
worse than n-gram-based metrics ROUGE and
BLEU ; nevertheless, they exhibit different rank-
ings in terms of ROC AUC (see the next section).
ROC AUC. ROC AUC scores are presented in
Table 5. We observe that the overall ranking of
faithfulness metrics according to ROC AUC sub-stantially differs from the ranking according to con-
sistency. In particular, the rank of BARTS
drops from the top one to the fifth, while Qim-
proves significantly from second to last to second
overall. QAFE consistently exhibits high
performance and even ranks first under ROC AUC,
while n-gram based metrics, e.g., ROUGE -2 and
BLEU consistently show the worst performance,
as expected. In general, metrics that are specifically
proposed for faithfulness evaluations rank higher
than generic NLG evaluation metrics.
We additionally observe that the relative rank-
ings of ROC AUC scores across error types and
task settings are largely consistent with the relative
rankings of consistency scores. Specifically, we
again observe that on a per metric basis: 1) ROC
AUC scores are generally lower for Task 2 than
Task 1 (particularly for Entity Error s), and 2) met-
rics generally show worse performance on Intrinsic
Error s than extrinsic ones.
For our two meta-evaluation protocols, consis-
tency is suitable for the pairwise ranking of two
summaries for a given input article, while ROC
AUC is more adequate in evaluating the absolute ca-
pacity of unfaithful summary detection. If a metric
has high consistency but low ROC AUC, it implies
that the scores for predicted faithful and unfaithful
summaries overlap frequently. Such overlap makes12793
it challenging to establish a clear decision boundary
for classifications. Hence, to improve the classifi-
cation capability of metrics with high consistency,
more calibration is needed to increase the score gap
between faithful and unfaithful summaries.
6 Analysis of BUMP
In this section, we conduct more analysis of BUMP
by studying how BUMP differs from other bench-
marks, followed by a qualitative analysis of the
detection difficulty between Tasks 1 and 2.
Comparison with Model-Generated Unfaithful
Summaries. We compare the generation proba-
bilities of our edited summaries to those of sum-
maries generated from beam search by a BART-
based summarizer (trained using the training data
of CNN/DailyMail) for the same set of documents
in our dataset. We report the difference of these
generation probabilities normalized by the text
length in Figure 2, where we find our edited sum-
maries are much different from model generations
in terms of the model generation probabilities. This
highlights that existing metrics may not work well
on summaries of various styles and experiments
are needed to verify their effectiveness in human-
generated unfaithful summaries.
Furthermore, we compare our ROC AUC scores
with those in existing datasets as shown in TRUE
(Honovich et al., 2022). In BUMP, faithful and
unfaithful samples under each error type are bal-
anced for both Tasks 1 and 2. Therefore, for a
fair comparison, we pick QAGS-C (Wang et al.,
2020) (also a balanced dataset on CNN/DailyMail)
in TRUE. In Table 5, it shows that the ROC AUC
scores evaluated on BUMP are generally much
smaller (50–70 %with many values close to ran-
dom baseline), whereas most ROC AUC scores are
70–84 %in QAGS-C (see Appendix C). This again12794
indicates that the human-generated errors in BUMP
are more difficult for metrics to detect than model-
generated errors in existing datasets, reinforcing
the value of BUMP as a challenging benchmark
for evaluating faithfulness metrics. In addition, we
also compare the ROC AUC rankings of different
faithfulness metrics under QAGS-C and BUMP.
Specifically, we summarize the performance rank-
ings under QAGS-C from Appendix C as well as
those from Table 5 under Intrinsic/Extrinsic Error
types in Tasks 1 and 2, and report them in Figure 3,
where only faithfulness metrics used in both (Hon-
ovich et al., 2022) and Table 5 are presented. In
Figure 3, we observe that for some faithfulness
metrics, such as QandBARTS , their ROC
AUC rankings are quite stable across all datasets.
However, for other faithfulness metrics, the per-
formance ranking under QAGS-C is very differ-
ent from the ranking derived from BUMP, e.g.,
Q E mostly exhibits high ROC AUC rank-
ing in BUMP; by contrast, it experiences the worst
performance in QAGS-C. Thus, we believe BUMP
complements existing benchmarks and allows a
more comprehensive analysis of faithfulness met-
rics in future studies.
Qualitative Analysis. We provide a qualitative
analysis of examples that demonstrate the increased
difficulty of Task 2. The examples are provided in
Table 6. Each row contains edited summaries from
Tasks 1 and 2 for the same original article and its
reference summary. In addition, to compare edited
summaries under the same error type, we pick ex-
amples where the corrected error type from Task 1
is the same as the exhibited error type from Task 2.
As shown in Table 6, in the first example, for the
Extrinsic Entity Error type, the annotator in Task 1
modifies the entity elevator shaft to another entity
staircase . Whereas the annotator in Task 2 modi-
fies the word communal toprivate (i.e., also an Ex-
trinsic Entity Error ) which requires commonsense
knowledge to infer that private is contradictory to
the fact that the elevator is used by several busi-
nesses in the building . In the second example, for
theExtrinsic Entity Error type, the annotator in
Task 1 modifies the entity name from Claire Nu-
gent to a random name Tim Horton , whereas the
annotator in Task 2 changes record player toblack
and white TV to fit the 60stheme, which again,
requires additional knowledge. In the last example,12795the annotator in Task 2 modifies the temporal state
of the action sign from signed tois considering
signing which is more challenging than changing
the action urged to its antonym discouraged as the
annotator in Task 1 does.
For the first two examples of Task 2 in Ta-
ble 6, only 4 metrics ( QAFE,Q E-,BLEURT , and BERTS for the first
example; QAFE,S C,BLEURT ,
andROUGE-2 for the second example) succeed
in giving a higher score to the reference summary.
In comparison, 9 and 11 metrics succeed in giving
a higher score to the reference summary in their
Task 1 counterparts, respectively. For the last exam-
ple, 8 metrics succeed in Task 2 and all 12 metrics
succeed in Task 1. Thus, Table 6 shows that some
unfaithful summaries in Task 2 are more challeng-
ing for faithfulness metrics to detect, which further
exemplifies the challenges of Task 2 in BUMP.
7 Conclusion
In this paper, we presented a benchmark of unfaith-
ful minimal pairs (BUMP) to evaluate faithfulness
metrics. Unlike prior work where all unfaithful
summaries are model generated, each unfaithful
summary in BUMP is generated by minimal hu-
man edits to introduce one unfaithful error given
a reference summary. Through our experiments,
we found that BUMP complements existing bench-
marks in a number of ways. First, we found that the
summaries in BUMP are harder to discriminate and
less probable under SOTA summarization models.
Second, we used BUMP to measure the consistency
of metrics, which cannot be readily measured using
other benchmarks. This analysis revealed a discrep-
ancy between the discriminability and consistency
of existing metrics, highlighting an important area
for future faithfulness metric research to address.
Finally, we used BUMP to study faithfulness met-
rics’ performance on individual error types—where
our minimal-pair-inspired setup helped control for
conclusions being conflated across multiple error
types—which revealed that sensitivity to intrinsic
errors is another important area for future research
to focus on.
Acknowledgements
We would like to thank our colleague Aoife Cahill
at Dataminr for her valuable comments, sugges-
tions, and support for this paper. We also thank
the anonymous reviewers for their feedback andcomments.
Limitations
Although BUMP is, to our knowledge, the first
dataset on which to study the consistency of faith-
fulness metrics on human-written errors across dif-
ferent error types, there are some limitations regard-
ing the conclusions that can be drawn from it. For
one, because BUMP is comprised of minimal edits
to reference summaries from CNN/DailyMail, it is
not suitable for analyzing the consistency of faith-
fulness metrics when errors are added to reference
summaries already containing many errors. In ad-
dition, due to a combination of resource constraints
and human preferences for writing specific types
of errors, the sample sizes for some error types in
Task 2 (e.g., Coreference Error andIntrinsic Predi-
cate Error ) may not be sufficiently large to enable
statistically significant comparisons between differ-
ent metrics for specific error types.
Ethics Statement
The collection of BUMP involves human annota-
tions. The human annotators are provided with
clear task instructions and informed of the condi-
tions where they would be qualified and disqual-
ified. We compensate annotators with $3.00per
assignment in the qualification task and $0.50per
assignment in the full task for both Tasks 1 and 2.
The final paid rate is $15per hour which is over the
US national minimum wageof$7.25. We are also
aware that our shared datasets could be potentially
misused as training samples, albeit a small number,
to develop models to generate unfaithful content.
References1279612797
A Details of Task 1: Taxonomy-based
Unfaithful Summaries
A.1 Qualification Task
The instructions and the task interface for the qual-
ification task of Task 1 are shown in Figures A1 to
A4.
In this qualification task, all US-based annota-
tors are able to participate. Specifically, we ask
annotators to read a news article and seven pairs of
summaries. For each pair of summaries, the first
summary is the correct reference summary, and the
second summary is the unfaithfully edited summary
that contains one of the seven error types in our tax-
onomy. We then ask the annotators to select one
answer from the seven error types to indicate which
type of error is introduced in the edited unfaithful
summary. Only the annotators who answered 6 out
of these 7 questions correctly passed the qualifi-
cation task. We launched 3 batches in total with
9 assignments for each batch, and 9 annotators
passed the qualification task.
A.2 Full Task
The instructions and the task interface for the full
task of Task 1 are shown in Figures A5 to A7.
In the full task for Task 1, different from the qual-
ification task, we ask the annotators to read a news
article from CNN/DailyMail (Hermann et al., 2015)
and one reference summary for the article. We then
ask the annotators to edit the reference summary
to introduce the error type specified through a min-
imal edit. If they cannot introduce the error type
based on the reference summary, they can write
“N/A” to indicate that it is impossible to introduce
the specified error type based on the provided ref-
erence summary. There are 18 samples in Task 1
dataset that are annotated as “N/A” by the annota-
tors, all of which are reviewed by the authors of
this paper and re-annotated with the correct edits
(we note that the required error types can be pro-
vided for all these cases) as a post-processing step
to ensure the completeness of the dataset.
In addition, for Task 1, to help reduce the confu-
sion from annotators regarding Circumstance Er-
rors and Entity Error s, we explicitly specify that
theCircumstance Error s should only be erroneous
edits concerning the time, duration, or location of12798an event, and changing the quantity of a noun is
not considered as a Circumstance Error .
B Details of Task 2: Freestyle Unfaithful
Summaries
B.1 Qualification Task
The instructions and the task interface for the qual-
ification task of Task 2 are shown in Figures A8 to
A9.
In this qualification task, all US-based annotators
who did not participate in the qualification task of
Task 1 are qualified to participate. Specifically, we
show the annotators four pairs of news article and
its summary from CNN/DailyMail, and ask them
to answer if the summaries are faithful based on the
original news articles. Among the four pairs, three
of them are unfaithful and one is faithful. Only the
annotators who answered correctly to all of these 4
pairs passed the qualification task. We launched 3
batches in total with 9 assignments for each batch,
and 8 annotators passed the qualification task.
B.2 Full Task
The instructions and the task interface for the full
task of Task 2 are shown in Figures A10 to A11.
In the full task of Task 2, unlike Task 1, we do
not list any potential error types so as to achieve
freestyle editing. The edited summary is valid as
long as only one error is introduced based on the
reference summary via a minimal edit. Further-
more, we also do the following to ensure the quality
of edited summaries:
•For minimal edits, we explicitly ask annota-
torsnotto write from scratch, but to introduce
only one error on top of the given reference
summary.
•In the pilot study, we notice that some edited
summaries are simply removing/adding sen-
tences or phrases (such data points are re-
moved in the final released data); we, there-
fore, add additional instructions that require
the edited and the reference summaries to con-
tain a similar amount of information about the
given news article (i.e., similar coverage).
•The edited summaries should be grammati-
cally correct.
•The edited summaries should be plausible and
adhere to common sense.•Some examples of edited summaries are given
in the task instructions.
C ROC AUC Results from Other
Benchmarks
To compare BUMP with other benchmarks, we
also report the ROC AUC scores from TRUE (Hon-
ovich et al., 2022). Specifically, in BUMP, faithful
and unfaithful samples under each error type are
balanced for both Tasks 1 and 2. Therefore, for
a fair comparison, 1) in TRUE, we pick QAGS-
C (Wang et al., 2020), which is also a balanced
dataset on CNN/DailyMail. The ROC AUC scores
of QAGS-C are reported in Table A1; 2) for faith-
fulness metrics in Table A1, we use the same im-
plementation and model checkpoints in this paper
as those in TRUE (Honovich et al., 2022). Then
according to Table A1, the metric performance
ranking in terms of ROC AUC for QAGS-C is
Q> S C > BARTS > FCC >
BLEURT > BERTS > Q E, which
is very different from the ranking derived from
our BUMP dataset, e.g., S Cexhibits worse
ROC AUC than Q E for most error types
in both Tasks 1 and 2 (see Table 5).
In addition to the balanced dataset QAGS-C in
TRUE, we also report the ROC AUC scores of im-
balanced FRANK (Pagnoni et al., 2021) and Sum-
mEval (Fabbri et al., 2021) datasets (two datasets
containing CNN/DailyMail) from TRUE in Ta-
ble A1. Although the FRANK and SummEval
datasets are imbalanced, we have similar observa-
tions as those from the QAGS-C dataset: 1) their
ROC AUC scores (mostly 70–90 %) are much larger
than the ROC AUC scores (50–70 %) derived from
our BUMP dataset; 2) in terms of the ROC AUC
ranking, the top two remain QandS Cfor
both FRANK and SummEval, and S Cal-
ways ranks higher than Q E. By contrast,
in Table 5, we show that S Cmostly exhibits
worse ROC AUC than Q E.127991280012801128021280312804128051280612807128081280912810ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
In the Limitations section
/squareA2. Did you discuss any potential risks of your work?
In the section of Ethics Statement
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3
/squareB1. Did you cite the creators of artifacts you used?
Section 3.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
CNN/DailyMail is distributed under the MIT license. We plan on releasing our dataset under the
MIT license as well pending legal approval. The LICENSE will be provided alongside the dataset as
a text ﬁle on GitHub when the paper is published.
UPDATE AFTER REVIEW: We recently learned that the data we were using is distributed under an
Apache 2.0 license instead of MIT.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
CNN/DailyMail is distributed under the MIT license. Our BUMP dataset is derived from CNN/DailyMail.
We plan on releasing BUMP under the MIT license as well pending legal approval. The LICENSE
will be provided alongside the dataset as a text ﬁle on GitHub when the paper is published. Therefore,
the data collection and distribution of BUMP is consistent with the license in CNN/DailyMail.
UPDATE AFTER REVIEW: We recently learned that the data we were using is distributed under an
Apache 2.0 license instead of MIT.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We manually checked all collected human annotations; see Section 3.2 and 3.3
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 3, and Appendix A and B
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Table 2 and its descriptions in Section 3.2 and 3.312811C/squareDid you run computational experiments?
Section 4 and 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Not applicable. Left blank.
/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Not applicable. Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4.1
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Section 3, Appendix A and B
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section of Ethics Statement, and Appendix A.1 and B.1
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
The BUMP dataset is derived from CNN/DM. Attribution is provided in Section 3.1. Since CNN/DM
is distributed under the MIT license (which allows modiﬁcations), we did not discuss with annotators
on how their modiﬁed summaries will be used.
UPDATE AFTER REVIEW: We recently learned that the data we were using is distributed under an
Apache 2.0 license instead of MIT.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
We used a review process internal to our organization with HCI research scientists.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Appendix A.1 and B.112812