
Yau-Shian Wang Ashley Wu Graham Neubig
Carnegie Mellon University
king6101@gmail.com wangchew@andrew.cmu.edu
gneubig@cs.cmu.edu
Abstract
Universal cross-lingual sentence embeddings
map semantically similar cross-lingual sen-
tences into a shared embedding space. Align-
ing cross-lingual sentence embeddings usually
requires supervised cross-lingual parallel sen-
tences. In this work, we propose mSimCSE,
which extends SimCSE (Gao et al., 2021) to
multilingual settings and reveal that contrastive
learning on English data can surprisingly learn
high-quality universal cross-lingual sentence
embeddings without any parallel data. In unsu-
pervised and weakly supervised settings, mSim-
CSE significantly improves previous sentence
embedding methods on cross-lingual retrieval
and multilingual STS tasks. The performance
of unsupervised mSimCSE is comparable to
fully supervised methods in retrieving low-
resource languages and multilingual STS. The
performance can be further enhanced when
cross-lingual NLI data is available.
1 Introduction
Universal cross-lingual sentence embeddings map
the sentences from multiple languages into a
shared embedding space, where semantically sim-
ilar sentences across languages are close to each
other. These embeddings have a wide spectrum
of applications such as multi-lingual document re-
trieval (Artetxe and Schwenk, 2019a; Lin et al.,
2020), multi-lingual question answering (Asai
et al., 2021a,b; Kumar et al., 2022), unsupervised
machine translation (Tran et al., 2020), and zero-
shot transfer learning (Phang et al., 2020).
As shown in Figure 1 (a), without finetuning
on downstream tasks, the embedding space of pre-
trained multilingual language models such as m-
BERT (Devlin et al., 2019) or XLM-R (Conneau
et al., 2020) separate the embeddings of each lan-
guage into different clusters. To align cross-lingual
Figure 1: We visualize the sentence embeddings on
XNLI corpus, where blue dots and green dots denote the
sentences from English and Swahili respectively. Here,
red dots, black dots, and purple dots denote the parallel
sentences from different languages. In (a), the sentence
embeddings from different languages are clearly sepa-
rated into two clusters. In (b), after English NLI training,
the embedding space becomes indistinguishable for dif-
ferent languages, and the parallel sentences are aligned
to each other.
sentence embeddings, previous work (Artetxe and
Schwenk, 2019a; Chidambaram et al., 2019; Feng
et al., 2020) finetunes multilingual language mod-
els with billions of parallel data. However, it is
non-trivial to obtain numerous parallel data for all
languages. One potential direction to alleviate the
need for parallel data is to enhance cross-lingual
transfer of sentence embeddings.
Pre-trained multilingual language models (Pires
et al., 2019; Phang et al., 2020) have shown im-
pressive performance on cross-lingual zero-shot
transfer (Pires et al., 2019) that a model finetuned
on a source language can generalize to target lan-
guages. This implies the representations finetuned
on downstream tasks are universal across various
languages. In this work, we explore various cross-
lingual transfer settings on sentence retrieval tasks,
especially in the setting of using English data only.
We propose multilingual-SimCSE (mSimCSE)
which extends SimCSE (Gao et al., 2021), a fa-
mous sentence embedding method on English, to
multilingual for cross-lingual transfer. SimCSE is9122a contrastive learning (Chopra et al., 2005; Hadsell
et al., 2006; Chen et al., 2020a) method that pulls
closer semantically similar sentences (i.e. positive
sentence pairs) in embeddings space. As done in
SimCSE, we obtain positive training pairs by either
natural language inference (NLI) (Conneau et al.,
2017; Reimers and Gurevych, 2019) supervision or
unsupervised data augmentation using dropout. We
also investigate model performance when a small
amount of parallel data or cross-lingual NLI data
are available.
In our experiments, as shown in Figure 1 (b),
we are surprised to find that contrastive learning
on pure English data seems to be able to align
cross-language representations. Sentences that are
semantically similar across languages are clearly
closer together. Compared with previous unsu-
pervised or weakly supervised methods, our unsu-
pervised method significantly improves the perfor-
mance on cross-lingual STS and sentence retrieval
tasks. In retrieving low-resource languages and
STS tasks, our method is even on par with fully
supervised methods trained on billions of parallel
data. Our results show that using contrastive learn-
ing to learn sentence relationships is more efficient
than using massively parallel data for learning uni-
versal sentence embeddings. To the best of our
knowledge, we are the first to demonstrate that us-
ing only English data can effectively learn universal
sentence embeddings.
2 Related Work
2.1 Cross-lingual Zero-shot Transfer
Learning
Due to the similarities between different languages,
such as words, grammar, and semantics, multilin-
gual models (Devlin et al., 2019; Conneau and
Lample, 2019; Conneau et al., 2020; Wei et al.,
2021; Chi et al., 2022) have been shown to gen-
eralize to unseen languages in a wide range of
tasks. In machine translation, multilingual mod-
els exhibit the ability to perform zero-shot trans-
fer in that they can translate on unseen language
pairs (Zoph et al., 2016; Johnson et al., 2017). With
the help of self-supervised learning, model can bet-
ter acquire language-invariant representation, thus
improving zero-shot machine translation (Siddhant
et al., 2020; Liu et al., 2020).
To learn language-invariant representations
cross-lingual tasks, previous work apply adversar-
ial networks (Keung et al., 2019; Chen et al., 2019)or align representations via parallel corpus (Cao
et al., 2020). Pires et al. (2019) revealed that
mBERT is good at zero-shot cross-lingual trans-
fer that fintuning on a specific monolingual task
can generalize to other languages. More recently,
X-Mixup (Yang et al., 2022) performs manifold
mixup of source and target languages to learn gen-
eral representations. Our method is the most rel-
evant to the work which enhances cross-lingual
transfer by using English downstream tasks to fine-
tune multilingual language models (Phang et al.,
2020) in that both methods leverage English NLI
supervision. Our method differs from theirs in that
we apply contrastive learning for representation
learning and we only focus on sentence embed-
dings.
2.2 English Sentence Embeddings
Sentence embeddings aim to map sentences into a
shared embedding space in which sentences with
similar meanings can be close to each other. Previ-
ous work learns sentence embeddings by predicting
the surrounding sentences of an input sentence, in
either generative (Kiros et al., 2015) or discrimi-
native (Logeswaran and Lee, 2018) manners. Re-
cently, with the success of contrastive learning on
learning visual representations (Chen et al., 2020b),
more and more work explores contrastive learning
on sentence embeddings. The training signal of
contrastive learning can be obtained by data aug-
mentation (Fang et al., 2020; Yan et al., 2021; Meng
et al., 2021; Carlsson et al., 2021) or self-guided ar-
chitecture (Kim et al., 2021; Carlsson et al., 2021).
Among these, SimCSE (Gao et al., 2021) is the
most famous one, which adopts either NLI supervi-
sion to define relevant sentences or unsupervised
dropout as data augmentation, and improves state-
of-the-art results. We choose to extend SimCSE to
multilingual due to its impressive performance and
simplicity.
2.3 Cross-lingual Sentence Embeddings
Universal cross-lingual sentence embeddings align
semantically similar cross-lingual sentences into
a shared embeddings space. To learn cross-
lingual embeddings, previous work uses large
amounts of parallel data to train neural networks
to bring the embeddings of parallel sentences
closer. LASER (Artetxe and Schwenk, 2019b)
trains Bi-LSTM with parallel sentences of 93 lan-
guages to encourage consistency between the cross-
lingual sentence embeddings. MUSE (Yang et al.,9123
2020) learns universal sentence embedding for 16
languages via translation based bridge tasks in-
cluding multifeature question-answer prediction,
translation ranking, and NLI. LaBSE (Feng et al.,
2020) leverages dual-encoder framework to fine-
tune mBERT on 6 billion parallel sentence pairs
over 109 languages. Reimers and Gurevych (2020)
extend the monolingual sentence embedding to
multilingual by teacher-student training, in which
a target language student model mimics source lan-
guage embeddings from the teacher model. Sen-
tence piece encoder (SP) (Wieting et al., 2021) sim-
ply learns sentence piece embeddings using parallel
data and outperforms BERT-base models.
It is also possible to learn cross-lingual sentence
embeddings in weakly supervised or even unsu-
pervised manners. CRISS (Tran et al., 2020) uses
the representations of mBART encoder as initial
sentence embeddings, which are used to mine par-
allel texts from monolingual corpora. They itera-
tively extract bitexts and use the bitexts to update
the model in a self-training loop. Notice that our
method can be easily combined with CRISS by us-
ing mSimCSE as the initial model. Our method is
closely related to DuEAM (Goswami et al., 2021),
a dual-encoder model that leverages word mover
distance and cross-lingual parallel NLI sentences
to construct positive training pairs. The main differ-
ences are that we use contrastive learning, which
is more effective than dual encoder, and that our
method is more simple that only requires Englishdata.
3 Method
3.1 SimCSE
SimCSE (Gao et al., 2021) apply batch contrastive
learning (Chen et al., 2020b) to learn sentence em-
beddings on English data. Batch contrastive learn-
ing puts positive training pairs and negative training
pairs into a same batch, increasing the difficulty of
a contrastive task.
Specifically, a positive training pair means the
sentences in the pair are semantically similar and
their embeddings should be pulled closer, while
a negative training pair means the sentences are
semantically different. Given a batch of positive
training pairs B={(x, x)}, SimCSE calcu-
lates the batch constrastive loss for i-th pair as:
L=−logee. (1)
Here, Ndenotes the batch size, (x, x)denotes
two semantically similar sentences, and h=
E(x)is the sentence embedding from encoder
E. The key to using this loss is how to define the
semantically similar pairs, which we elaborate on
in the following section.
3.2 Multilingual SimCSE
In this section, we elaborate on how we extend
SimCSE to multilingual and illustrate our proposed9124mSimCSE in Figure 2. We explore four different
multilingual training strategies, including the un-
supervised strategy, the English NLI supervised
strategy, the parallel NLI supervised strategy, and
the fully supervised strategy. The difference be-
tween different strategies is how to define a positive
training pair. Here, both unsupervised and English
NLI supervised strategies can be recognized as an
“unsupervised” setting for multilingual training be-
cause both of them only use English data and do
not use any parallel data.
Unsupervised mSimCSE In unsupervised Sim-
CSE, xandxare the same sentence. As x
andxare encoded by the same encoder but with
different dropout, the dropout can be viewed as a
light-weight data augmentation method. We use
the wikipedia data from the original SimCSE repos-
itory to train our model.
English NLI supervised mSimCSE In the En-
glish NLI supervised strategy, we use English natu-
ral language inference (NLI) (Conneau et al., 2017;
Reimers and Gurevych, 2019) datasets to construct
positive and hard negative training pairs. Specifi-
cally, if two sentences are labeled as “entailment”
relationship, they are viewed as a positive pair (x,
x). For each x, we also include a hard negative
example xin the same training batch, where x
andxare labeled as “contradiction” relationship.
Cross-lingual NLI supervised mSimCSE The
English NLI supervision can be easily extended
to the multilingual strategy by constructing a pos-
itive training pair from different languages. We
use XNLI (Conneau et al., 2018), which translates
English NLI to multiple languages. Similar to the
English NLI strategy, the cross-lingual sentence
pairs with “entailment” and “contradiction” rela-
tionship are viewed as positive and negative pairs
respectively, but as shown in Figure 2, the language
ofx,xandxare randomly sampled. They can
come from either different languages or the same
language.
Supervised mSimCSE In supervised mSimCSE,
we simply define a positive training pair as the par-
allel sentences from different languages. This strat-
egy is the same as previous supervised methods,
but we use relatively few parallel sentences. Note
that different strategies can be easily combined by
mixing training pairs from different strategies.4 Experiments
4.1 Experimental Setup
Training Details We adapt SimCSE codebase
to a multi-lingual setting. We keep all other hy-
perparameters same as the original SimCSE, and
fix learning rate to be 1e−5, training epoch to
be1, and batch size to be 128for all experiments.
We use our method to finetune XLM-Roberta-large
(XLM-R) (Conneau et al., 2020). We examine the
performance of different hyperparameters in Ap-
pendix A
Training Data for Different mSimCSE Strate-
gies In unsupervised mSimCSE and English NLI
supervised mSimCSE, we use the pre-processed
English Wikipedia and English NLI training tuples
downloaded from the SimCSE codebase respec-
tively. In all the tables in this paper, the subscripts
ofmSimCSE denote the languages that we use
to train our model. In cross-lingual NLI supervi-
sion,mSimCSE denotes we use English and
translated French NLI data to train our model and
mSimCSEmeans that we use all the languages
in XNLI (Conneau et al., 2018) dataset.
In supervised finetuning, mSimCSEdenotes
that we use the translation pairs of English and
Swahili. For each language, we randomly select
100k parallel sentences from ParaCrawl project
via the OPUS corpus collection. In supervised
finetuning, “ mSimCSE+NLI” denotes that we
mix English NLI sentence pairs with English-
Swahili translation pairs. Note that because parallel
sentences don’t have hard negative sentences, to
mix them with NLI data, we also remove hard neg-
ative sentences of NLI in “ mSimCSE+NLI”.
4.2 Baselines
First, we compare our method to unsupervised pre-
trained language models including XLM-R and M-
BERT without finetuning to show that unsupervised
contrastive learning can learn more generalized
cross-lingual sentence embeddings. In some tasks,
we also compare our method with more competi-
tive language models in Xtreme (Hu et al., 2020)
benchmark, such as XLM-E (Chi et al., 2022),
HICTL (Wei et al., 2021) and INFOXLM (Chi
et al., 2021). CRISS (Tran et al., 2020) is an unsu-
pervised sentence retrieval method, which mines9125
parallel sentences from multiple monolingual cor-
pora using self-training.
Our main baselines are other methods which
also leverage NLI supervision. Phang et al. (2020)
finetune multilingual language models on vari-
ous English tasks, including English NLI task.
DuEAM (Goswami et al., 2021) also leverages
multilingual NLI supervision to learn universal sen-
tence embeddings.
We also compare our method with fully super-
vised methods that leverages parallel sentences, in-
cluding LASER (Artetxe and Schwenk, 2019b),
LaBSE (Feng et al., 2020), and SP (Wieting
et al., 2021). Note that among all the methods,
mSimCSEis the only method that only uses
English data.
4.3 Sentence Retrieval
Following the previous work of cross-lingual sen-
tence embedding learning (Goswami et al., 2021),
we evaluate our model on multi-lingual sentence
retrieval, including Tatoeba (Artetxe and Schwenk,
2019b) and BUCC (Zweigenbaum et al., 2018).
Tatoeba requires models to match parallel sen-
tences from source and target language sentence
pools. BUCC is a bitext mining task, in which a
model needs to rank all the possible sentence pairs,
and predicts sentence pairs whose scores are above
an optimized threshold.
In Table 1, we follow the setting in Xtremebenchmark to evaluate model performance on sen-
tence retrieval task. In unsupervised setting, mSim-
CSE trained on English Wikipedia improves the
performance by a large margin. This implies that
contrastive training can effectively pull closer cross-
lingual semantically similar sentences.
With English NLI supervision, it significantly
improves the performance against unsupervised
methods and DuEAM. It even beats fully-suprvised
methods that leverages parallel sentences on BUCC
task. This implies that with an objective that
learns more difficult semantic relationship between
sentences, model can learn better universal cross-
lingual sentence embeddings.
By comparing cross-lingual NLI supervised
mSimCSE with Enlgish NLI supervised mSimCSE,
we observe that the model performance can be fur-
ther improved using translated NLI pairs from other
languages. In general, including more languages
can improve the performance. Comparing with
DuEAM which also leverages parallel NLI super-
vision, contrastive learning can learn universal sen-
tence embedding more effectively. In the BUCC
dataset, the performance of mSimCSEis bet-
ter than fully supervised methods, such as LaBSE,
which is trained on 6 billion parallel data. Note
thatmSimCSEuses far less parallel data than
LaBSE, which demonstrates the effectiveness of
our method.
In the fully supervised setting, comparing9126
mSimCSE with and without NLI supervi-
sion, we find that if the translation pairs are rare,
adding English NLI supervision can significantly
improve the performance. Also, compared with
the mSimCSE that only uses English NLI supervi-
sion, adding a few extra parallel data can slightly
improve the performance.
Following DuEAM, in Table 2, we select some
high-resource languages including Hindi (hin),
French (fra), German (deu), Afrikaans (afr) and
Swahili (swh), and low-resource languages includ-
ing Telugu (tel), Tagalog (tgl), Irish (gle), Georgian
(kat), and Amharic (amh) from Tatoeba dataset to
further analyze the model performance. In high-
resource languages, fully-supervised achieves bet-
ter performance because large amounts of parallel
sentence pairs are available in these languages. On
the other hand, in low-resource languages, due to
the lack of training pairs, supervised method can
not generalize well on these languages while mSim-
CSE can generalize better.
Finally, we evaluate whether including cross-
lingual NLI supervision in a target language can
improve the performance. In Table 2, com-
pared with using only English NLI supervision,
mSimCSE in the cross-lingual NLI set-
ting which includes Swahili in training signifi-
cantly improves the performance in Swahili. Its
performance gain is greater than fully supervised
“mSimCSE +NLI”, which leverages parallel
sentences.
4.4 Cross-lingual STS
Cross-lingual STS (Cer et al., 2017) evaluates
whether a model predicted semantic similarity be-
tween two sentences are correlated to human judge-
ment. The two sentences can come from either
the same language or different languages. Given a
sentence pair, we compute the cosine similarity of
sentence embeddings as a model prediction.
The results of multi-lingual STS benchmark are
shown in Table 3. For unsupervised XLM-R and
mBERT without finetuning, we try several pooling
methods and find that averaging over the first and
the last layers yields the best results on STS. The
poor results of pre-trained language models mean
that the sentence embeddings of pre-trained lan-
guage models do not capture the semantics in the
cosine similarity space well. With unsupervised
mSimCSE pre-training, it enhances the semantics
of monolingual STS tasks, i.e. “ar-ar” and “es-es”.9127For the tasks that requires cross-lingual alignment,
including “ar-en”, “en-es”, and “tr-en”, the gap
between unsupervised baselines and English NLI
supervised baselines is still large.
Comparing with the methods that utilize either
parallel NLI supervision or supervised parallel data,
the English NLI training achieves the best results
on ar-ar, es-es, and es-en pairs. This implies that
mSimCSE can effectively transfer semantic knowl-
edge learned from English NLI supervision to other
languages. We find that using parallel data can only
improve the results of bilingual pairs, and reduces
the performance of monolingual pairs.
4.5 Unsupervised Classification
We conduct unsupervised classification to evaluate
whether the model can cluster semantically similar
documents together on the languages other than En-
glish. We use Tnews dataset in CLUE benchmark
to evaluate the performance of our model. Tnews
is a Chinese news classification dataset, which con-
tains 15 news categories. We first conduct k-means
clustering on sentence embedding, in which clus-
ter number kis set to be the same as the number
of news categories. Then, we evaluate the mean
accuracy of each cluster that measures what per-
centages of documents in each cluster are from the
same human-labeled category.
Compared with unsupervised pre-trained lan-
guage models, mSimCSE significantly improves
the purity scores. This is expected because with-
out fine-tuning, the embeddings from pre-trained
language models cannot capture relative distances
between instances. Similar to the observation in
the previous section, English NLI supervision can
greatly enhance the performance, closing the gap
between the fully supervised fine-tuned BERT.
4.6 Zero-shot Cross-lingual Transfer of
Sentence Classification
To evaluate the cross-lingual zero-shot transfer of
pre-trained sentence embedding, we evaluate our
model on PAXS-X (Yang et al., 2019) sentence
classification task. PAXS-X requires a model to de-
termine whether two sentences are paraphrases. In
Table 5, compared with XLM-R and XLM-E (Chi
et al., 2022), mSimCSE without using NLI data im-
proves the performance, which demonstrates that
mSimCSE is an effective approach for zero-shot
cross-lingual transfer. In this task, using English
NLI supervision does not improve performance.
5 Analysis
5.1 The Effect of Parallel Sentences Number
As parallel data is easy to obtain for most lan-
guages (Artetxe et al., 2020), we investigate the
effect of the number of parallel sentences. We mix
parallel English-French sentences with the English
NLI data and gradually increase the number of par-
allel sentences. Here, 0 parallel sentence means
we only use English NLI data without using hard
negative examples, so the results are different from
the results in Table 1.
The BUCC dataset has only four high-resource
languages, of which French is one of them. With
more parallel data, the consistent improvement on
BUCC dataset implies that using more English-9128French translation pairs can improve the perfor-
mance on the English-French mining task, thus
improving the results of BUCC. On the other hand,
the Tatoeba dataset includes much more languages,
which evaluates a model’s generalization. We ob-
serve that using more parallel data does not influ-
ence the performance on Tatoeba, which implies
that using translation data on a single language pair
does not generalize well to other languages. The
results suggest that using large amounts of parallel
data may not be the most efficient way to learn
universal sentence embeddings while learning sen-
tence relationships is a more promising direction.
5.2 Can Contrastive Learning Removes
Language Identity?
Masked language modeling requires a model to
capture the language identity to predict correct to-
kens for a specific language. On the other hand,
English contrastive loss only learns the relationship
between sentences, which does not seem to require
language identity. We speculate that the contrastive
loss can thus remove the language identity in sen-
tence embeddings and enhance the general shared
cross-lingual semantics.
To verify this, in Table 7, we train two language
classifiers on en,de,fr,hi and en,tr,ar,bg respectively.
The language classifier needs to predict the correct
language of the input sentence embeddings. We use
the sentences from XNLI as our training and testing
data. With contrastive learning, the accuracy of
language classifier decreases, which implies the
embeddings are more language-invariant, which
more or less verifies our assumption. However, the
accuracy is still very high because the language
classifier can still predict the language of a text by
language-specific features such as grammar and
characters.
6 Discussion
Our experimental results demonstrate that in both
unsupervised and English NLI supervised settings,
using English data alone can surprisingly align
cross-lingual sentence embeddings. By comparing
unsupervised results with NLI supervised results,we observe that learning more meaningful sentence
relationships can further enhance the alignment. In
our analysis, we find that infinitely increasing paral-
lel training data is not the most efficient manner to
learn universal sentence embeddings; instead, our
results suggest that designing a more challenging
contrastive task or more effective sentence embed-
ding learning method on English data may be a
more efficient direction. Also, contrastive learn-
ing may be a promising direction for improving
the zero-shot transfer of pre-trained multilingual
language models.
We attribute the alignment to language-invariant
contrastive training. Because multilingual lan-
guage models have shown good performance on
zero-shot transfer, we speculate that multilingual
language models encode texts into two disen-
tangled embeddings, a language-specific embed-
ding and a general language agnostic embed-
ding. Because English contrastive task doesn’t re-
quire mlms to capture language identity, it only
pulls closer language-agnostic sentence embed-
dings while weakening language-specific embed-
ding. This property can be verified in Figure 1 and
Table 7. However, it still requires more investiga-
tion to fully understand why contrastive learning
on English data can achieve cross-lingual transfer.
7 Conclusion
In this work, we demonstrate that using only En-
glish data can effectively learn universal sentence
embeddings. We propose four different strategies
to extend SimCSE to multilingual, including un-
supervised, English NLI supervised, cross-lingual
NLI supervised, and supervised strategies. We sur-
prisingly find that the English NLI supervised strat-
egy can achieve performance on par with previous
supervised methods that leverage large amounts of
parallel data. Our work provides a new perspec-
tive on learning universal sentence embeddings
and cross-lingual transfer that using contrastive
learning to learn sentence semantic relationships
on monolingual corpora may be a promising direc-
tion.
8 Limitations
In the previous sections, we attribute why models
trained on English data can learn cross-lingual sen-
tence embeddings to language-invariant contrastive
task. We speculate that multilingual language mod-
els have already implicitly learned such universal9129representations, but they also learn some language-
specific representations. Contrastive learning en-
hances the language-invariant representations, di-
minishing the language-specific representations
without distorting the semantics embedded in the
representations. However, this speculation still re-
quires more evidence to support it. Also, it is im-
portant to understand to which the zero-shot trans-
fer happens, such as which languages are easier
to transfer, and what is the properties of these lan-
guages. For the properties of these languages, by
observing the experimental results in Table 2, we
have two speculations, one is their similarity be-
tween English, and another one is the number of the
monolingual pre-training data for these languages,
but again these speculations also require more anal-
ysis to verify. By understanding the reason for this
phenomenon, it is possible to achieve better zero-
shot transfer and learn more “universal” sentence
embeddings.
References913091319132
A Hyperparameters
In Table 8, we show how different hyperparame-
ters influence the model performance. We choose
mSimCSEtrained on English NLI data as the
model under examination. We find that the perfor-
mance of different hyperparameters is very close,
which implies that our method is stable and not sen-
sitive to hyperparameters. Increasing the number of
training epochs to 2 can improve the performance
on BUCC.9133