
Xuezhi Wang
Google Research
xuezhiw@google.comHaohan Wang
Carnegie Mellon University
haohanw@cs.cmu.eduDiyi Yang
Georgia Institute of Technology
dyang888@gatech.edu
Abstract
As NLP models achieved state-of-the-art per-
formances over benchmarks and gained wide
applications, it has been increasingly impor-
tant to ensure the safe deployment of these
models in the real world, e.g., making sure
the models are robust against unseen or chal-
lenging scenarios. Despite robustness being
an increasingly studied topic, it has been sepa-
rately explored in applications like vision and
NLP, with various deﬁnitions, evaluation and
mitigation strategies in multiple lines of re-
search. In this paper, we aim to provide a uni-
fying survey of how to deﬁne, measure and
improve robustness in NLP. We ﬁrst connect
multiple deﬁnitions of robustness, then unify
various lines of work on identifying robust-
ness failures and evaluating models’ robust-
ness. Correspondingly, we present mitigation
strategies that are data-driven, model-driven,
and inductive-prior-based, with a more system-
atic view of how to effectively improve robust-
ness in NLP models. Finally, we conclude
by outlining open challenges and future direc-
tions to motivate further research in this area.
1 Introduction
NLP models, especially with the recent advances
of large pre-trained language models have achieved
great progress and gained wide applications in the
real world. Despite the performance gains, NLP
models are still fragile and brittle to out-of-domain
data (Hendrycks et al., 2020a; Wang et al., 2019d),
adversarial attacks (McCoy et al., 2019; Jia and
Liang, 2017; Jin et al., 2020), or small perturba-
tion to the input (Ebrahimi et al., 2018; Belinkov
and Bisk, 2018). Those failures could hinder the
safe deployment of these models in the real world,
and impact NLP models’ trustworthiness to users.
As a result, an increasing line of work has been
conducted to understand robustness issues in the
language technologies communities. Still, diverse
sets of research across multiple dimensions andnumerous levels of depth exist and are scattered
across various communities; for instance, using
a variety of deﬁnitions on a wide range of very
different NLP tasks. In this work, we provide a uni-
fying overview of what is robustness in NLP, how
to identify robustness failures and evaluate model’s
robustness, and systematic ways to improve robust-
ness, as well as a conceptual schema categorizing
ongoing research directions. We identify gaps be-
tween the to-date robustness work, the technical
opportunities, and discuss possible paths forward.
2 Deﬁnitions of Robustness in NLP
Robustness, despite its speciﬁc deﬁnitions in var-
ious lines of research, can typically be uniﬁed as
follows: denote the input as x, and its associated
gold label for the main task as y, assume a model
fis trained on (x, y)∼D and its prediction over
xasf(x); now given test data (x, y)∼D/negationslash=D,
we can measure a model’s robustness by its perfor-
mance onD, e.g., using the model’s robust accu-
racy (Tsipras et al., 2019; Yang et al., 2020), de-
ﬁned as E[f(x) =y]. Existing literature
on robustness in NLP can be roughly categorized
by howDis constructed: by synthetically per-
turbing the input (Section 2.1), or Dis naturally
occurring with a distribution shift (Section 2.2).
The above deﬁnition works for a range of NLP
tasks like text classiﬁcation and sequence labeling
where yis deﬁned over a ﬁxed set of discrete la-
bels. For tasks like text generation, robustness is
less well deﬁned and can manifest as positional
bias (Jung et al., 2019; Kryscinski et al., 2019), or
hallucination (Maynez et al., 2020; Parikh et al.,
2020; Zhou et al., 2021). One major challenge here
is a lack of robust metrics in evaluating the quality
of the generated text (Sellam et al., 2020; Zhang
et al., 2020b), i.e., we need a reliable metric to
determine the relationship between f(x)andy
when both are open-ended texts.45692.1 Robustness against Adversarial Attacks
In one line of research, Dis constructed by per-
turbations around input xto form x(xtypically
being deﬁned within some proximity of x). This
topic has been widely explored in computer vision
under the concept of adversarial robustness, which
measures models’ performances against carefully
crafted noises generated deliberately to deceive the
model to predict wrongly, pioneered by (Szegedy
et al., 2013; Goodfellow et al., 2015), and later
extended to NLP, such as (Ebrahimi et al., 2018;
Alzantot et al., 2018; Li et al., 2019; Feng et al.,
2018; Kuleshov et al., 2018; Jia et al., 2019; Zang
et al., 2020; Pruthi et al., 2019; Wang et al., 2019e;
Garg and Ramakrishnan, 2020; Tan et al., 2020a,b;
Schwinn et al., 2021; Li et al., 2021; Boucher et al.,
2022) and multilingual adversaries (Yang et al.,
2019; Tan and Joty, 2021). The generation of ad-
versarial examples primarily builds upon the ob-
servation that we can generate samples that are
meaningful to humans (e.g., by perturbing the sam-
ples with changes that are imperceptible to humans)
while altering the prediction of the models for this
sample. In this regard, human’s remarkable ability
in understanding a large set of synonyms (Li et al.,
2020) or interesting characteristics in ignoring the
exact order of letters (Wang et al., 2020b) are often
opportunities to create adversarial examples. A re-
lated line of work such as data-poisoning (Wallace
et al., 2021) and weight-poisoning (Kurita et al.,
2020) exposes NLP models’ vulnerability against
attacks during the training process. One can refer
to more comprehensive reviews and broader dis-
cussions on this topic in Zhang et al. (2020c) and
Morris et al. (2020b).
Assumptions around Label-preserving and
Semantic-preserving Most existing work in vi-
sion makes a relatively simpliﬁed assumption that
the gold label of xremains unchanged under a
bounded perturbation over x, i.e., y=y, and a
model’s robust behaviour should be f(x) =y
(Szegedy et al., 2013; Goodfellow et al., 2015).
A similar line of work in NLP follows the same
label-preserving assumption with small text pertur-
bations like token and character swapping (Alzan-
tot et al., 2018; Jin et al., 2020; Ren et al., 2019;
Ebrahimi et al., 2018), paraphrasing (Iyyer et al.,
2018; Gan and Ng, 2019), semantically equiva-
lent adversarial rules (Ribeiro et al., 2018), and
adding distractors (Jia and Liang, 2017). How-
ever, this label-preserving assumption might notalways hold, e.g., Wang et al. (2021b) studied sev-
eral existing text perturbation techniques and found
that a signiﬁcant portion of perturbed examples are
notlabel-preserving (despite their label-preserving
assumptions), or the resulting labels have a high
disagreement among human raters (i.e., can even
fool humans). Morris et al. (2020a) also call for
more attention to the validity of perturbed examples
for a more accurate robustness evaluation.
Another line of work aims to perturb the input x
toxin small but meaningful ways that explicitly
change the gold label, i.e., y/negationslash=y, under which
case the robust behaviour of a model should be
f(x) =yandf(x)/negationslash=y(Gardner et al., 2020;
Kaushik et al., 2019; Schlegel et al., 2021). We
believe these two lines of work are complementary
to each other, and both should be explored in fu-
ture research to measure models’ robustness more
comprehensively.
One alternative notion is whether the perturba-
tion from xtoxis “semantic-preseving” (Alzan-
tot et al., 2018; Jin et al., 2020; Ren et al., 2019)
or “semantic-modifying” (Shi and Huang, 2020;
Jia and Liang, 2017). Note this is slightly dif-
ferent from the above label-preserving assump-
tions, as it is deﬁned over the perturbations on
(x, x)rather than making an assumption on (y, y),
e.g., semantic-modifying perturbations can be ei-
ther label-preserving (Jia and Liang, 2017; Shi and
Huang, 2020) or label-changing (Gardner et al.,
2020; Kaushik et al., 2019).
2.2 Robustness under Distribution Shift
Another line of research focuses on (x, y)drawn
from a different distribution that is naturally-
occurring (Hendrycks et al., 2021), where robust-
ness can be deﬁned around model’s performance
under distribution shift. Different from work on
domain adaptation (Patel et al., 2015; Wilson and
Cook, 2020) and transfer learning (Pan and Yang,
2010), existing deﬁnitions of robustness are closer
to the concept of domain generalization (Muan-
det et al., 2013; Gulrajani and Lopez-Paz, 2021), or
out-of-distribution generalization to unforeseen dis-
tribution shifts (Hendrycks et al., 2020a), where the
test data (either labeled or unlabeled) is assumed
not available during training, i.e., generalization
without adaptation. In the context of NLP, robust-
ness to natural distribution shifts can also mean
models’ performance should not degrade due to the
differences in grammar errors, dialects, speakers,4570languages (Craig and Washington, 2002; Blodgett
et al., 2016; Demszky et al., 2021), or newly col-
lected datasets for the same task but in different
domains (Miller et al., 2020). Another closely con-
nected line of research is fairness, which has been
studied in various NLP applications, see (Sun et al.,
2019) for a more in-depth survey in this area. For
example, gendered stereotypes or biases have been
observed in NLP tasks including co-reference reso-
lution (Zhao et al., 2018a; Rudinger et al., 2017),
occupation classiﬁcation (De-Arteaga et al., 2019),
and neural machine translation (Prates et al., 2019;
Font and Costa-jussà, 2019).
2.3 Connections and A Common Theme
The above two categories of robustness can be uni-
ﬁed under the same framework, i.e., whether D
represents a synthetic distribution shift (via adver-
sarial attacks) or a natural distribution shift. Exist-
ing work has shown a model’s performance might
degrade substantially in both cases, but the trans-
ferability of the two categories is relatively under-
explored. In the vision domain, Taori et al. (2020)
investigate models’ robustness to natural distribu-
tion shift, and show that robustness to synthetic
distribution shift might offer little to no robustness
improvement under natural distribution shift. Some
studies show NLP models might not generalize to
unseen adversarial patterns (Huang et al., 2020;
Jha et al., 2020; Joshi and He, 2021), but more
work is needed to systematically bridge the gap be-
tween NLP models’ robustness under natural and
synthetic distribution shifts.
To better understand whymodels exhibit a lack
of robustness, some existing work attributed this to
the fact that models sometimes utilize spurious cor-
relations between input features and labels, rather
than the genuine ones, where spurious features are
commonly deﬁned as features that do not causally
affect a task’s label (Srivastava et al., 2020; Wang
and Culotta, 2020b): they correlate with task labels
but fail to transfer to more challenging test con-
ditions or out-of-distribution data (Geirhos et al.,
2020). Some other work deﬁned it as “prediction
rules that work for the majority examples but do
not hold in general” (Tu et al., 2020). Such spuri-
ous correlations are sometimes referred as dataset
bias (Clark et al., 2019; He et al., 2019), annota-
tion artifacts (Gururangan et al., 2018), or group
shift (Oren et al., 2019) in the literature. Further,
evidence showed that controlling model’s learningin spurious features will improve model’s perfor-
mances in distribution shifts (Wang et al., 2019a,b);
also, discussions on the connections between adver-
sarial robustness and learning of spurious features
have been raised (Ilyas et al., 2019; Wang et al.,
2020a). Theoretical discussions connecting these
ﬁelds have also been offered by crediting a reason
of model’s lack of robustness in either distribution
shift or adversarial attack to model’s learning of
spurious features (Wang et al., 2021c).
Further, in certain applications, model “robust-
ness” can also be connected with models’ insta-
bility (Milani Fard et al., 2016), or models hav-
ing poorly-calibrated uncertainty estimation (Guo
et al., 2017), where Bayesian methods (Graves,
2011; Blundell et al., 2015), dropout-based (Gal
and Ghahramani, 2016; Kingma et al., 2015) and
ensemble-based approaches (Lakshminarayanan
et al., 2017) have been proposed to improve mod-
els’ uncertainty estimation. Recently, Ovadia et al.
(2019) have shown models’ uncertainty estimation
can degrade signiﬁcantly under distributional shift,
and call for more work to ensure a model “knows
when it doesn’t know” by giving lower uncertainty
estimates over out-of-distribution data. This is an-
other example where models can be less robust
under distributional shifts, and again emphasizes
the need of building more uniﬁed benchmarks to
measure a model’s performance (e.g., robust accu-
racy, calibration, stability) under distribution shifts,
in addition to in-distribution accuracy.
3 Robustness in Vision vs. in NLP
Despite the widely study of robustness in vision,
the study of robustness in NLP cannot always di-
rectly borrow the ideas. We categorize the main
differences with the three following points:
Continuous vs. Discrete in Search Space The
most obvious characteristic is probably the discrete
nature of the space of text. This particularly posed
a challenge towards the adversarial attack and de-
fense regime when the study in vision is transferred
to NLP (Lei et al., 2019; Zhang et al., 2020c), in the
sense that simple gradient-based adversarial attacks
will not directly translate to meaningful attacks in
the discrete text space, and multiple novel attack
methods are proposed to ﬁll the gap, as we will
discuss in later sections.
Perceptible to Human vs. Not On a related
topic, one of the most impressive property of ad-4571versarial attack in vision is that small perturbation
of the image data imperceptible to human are sufﬁ-
cient to deceive the model (Szegedy et al., 2013),
while this can hardly be true for NLP attacks. In-
stead of being imperceptible, the adversarial at-
tacks in NLP typically are bounded by the fact that
the meaning of the sentences are not altered (de-
spite being perceptible). On the other hand, there
are ways to generate samples where the changes,
although being perceptible, are often ignored by
human brain due to some psychological prior on
how a human processes the text (Anastasopoulos
et al., 2019; Wang et al., 2020b).
Support vs. Density Difference of the Data Dis-
tributions Another difference is more likely seen
in the discussion of the domain adaptation of vision
and NLP study. In vision study, although the im-
ages from training distribution and test distribution
can be sufﬁciently different, the train and test distri-
butions mostly share the same support (the pixels
are always sampled from a 0-255 integer space),
although the density of these distributions can be
very different (e.g., photos vs. sketches). On the
other hand, domain adaptation of NLP sometimes
studies the regime where the supports of the data
differ, e.g., the vocabularies can be signiﬁcantly
different in cross-lingual studies (Abad et al., 2020;
Zhang et al., 2020a).
A Common Theme Despite the disparities be-
tween vision and NLP, the common theme of push-
ing the model to generalize from DtoDpreserves.
The practical difference between DandDis more
than often deﬁned by the human’s understanding
of the data, and can differ in vision and NLP as
humans perceive and process images and texts in
subtly different ways, which creates both opportu-
nities for learning and barriers for direct transfer.
Certain lines of research try to bridge the learning
in the vision domain to the embedding space in the
NLP domain, while other lines of research create
more interpretable attacks in the discrete text space
(see Table 1 for these two lines of work). How
those two lines of research transfer to each other,
or complement each other, is not fully explored and
calls for additional research.
4 Identify Robustness Failures
As robustness gained increasing attention in NLP
literature, various lines of work have proposed
ways to identify robustness failures in NLP models.Existing works can be roughly categorized by how
the failures are identiﬁed, among which a large
portion of work relies on human priors and error
analyses over existing NLP models (Section 4.1),
and other lines of work adopt model-based ap-
proaches (Section 4.2). The identiﬁed robustness
failure patterns are usually organized into challeng-
ing/adversarial benchmark datasets to more accu-
rately measure an NLP model’s robustness. In Ta-
ble 1, we organize commonly used perturbation
types for identifying models’ robustness failures,
and in Table 2 we summarize common robustness
benchmarks for each NLP task.
4.1 Human Prior and Error Analyses Driven
An increasing body of work has been conducted on
understanding and measuring robustness in NLP
models (Tu et al., 2020; Sagawa et al., 2020b;
Geirhos et al., 2020) across various NLP tasks,
largely relying on human priors and error analyses.
Natural Language Inference Naik et al. (2018)
sampled misclassiﬁed examples and analyzed their
potential sources of errors, which are then grouped
into a typology of common reasons for error. Such
error types then served as the bases to construct
thestress test set, to further evaluate whether NLI
models have the ability to make real inferential
decisions, or simply rely on sophisticated pattern
matching. Gururangan et al. (2018) found that
current NLI models are likely to identify the la-
bel by relying only on the hypothesis, and Poliak
et al. (2018) provided similar augments that us-
ing a hypothesis-only model can outperform a set
of strong baselines. Kaushik et al. (2019) asked
humans to generate counterfactual NLI examples,
to better understand what features are causal and
encourage models to learn those features.
Question Answering Jia and Liang (2017) pro-
posed to generate adversarial QA examples by con-
catenating an adversarial distracting sentence at the
end of a paragraph. Miller et al. (2020) built four
new test sets for the Stanford Question Answer-
ing Dataset (SQuAD) and found most question-
answering systems fail to generalize to this new
data, calling for new evaluation metrics towards
natural distribution shifts.
Machine Translation Belinkov and Bisk (2018)
found that character-based neural machine trans-
lation (NMT) models are brittle under noisy data,
where noises (e.g., typos, misspellings, etc) are4572
synthetically generated using possible lexical re-
placements. Data augmentation with artiﬁcially-
introduced grammatical errors (Anastasopoulos
et al., 2019) or with random synthetic noises (Vaib-
hav et al., 2019; Karpukhin et al., 2019) can make
the system more robust to such spurious patterns.
On the other hand, Wang et al. (2020b) showed
another approach by limiting the input space of
the characters so that the models will be likely to
perceive data typos and misspellings.
Syntactic and Semantic Parsing Robust pars-
ing has been studied in several existing works (Lee
et al., 1995; Aït-Mokhtar et al., 2002). More recent
work showed that neural semantic parsers are still
not robust against lexical and stylistic variations,
or meaning-preserving perturbations (Marzinotto
et al., 2019; Huang et al., 2021), and proposed ways
to improve their robustness through data augmenta-
tion (Huang et al., 2021) and adversarial learning
(Marzinotto et al., 2019).
Text Generation Existing work found that text
generation models also suffer from robustness is-
sues, e.g., text summarization models suffer from
positional bias (Jung et al., 2019), layout bias(Kryscinski et al., 2019), and a lack of faithfulness
and factuality (Kryscinski et al., 2019; Maynez
et al., 2020; Chen et al., 2021b); data-to-text mod-
els sometimes hallucinate texts that are not sup-
ported by the data (Parikh et al., 2020; Wang et al.,
2020d). In addition, Sellam et al. (2020); Zhang
et al. (2020b) pointed out the deﬁciency of exist-
ing automatic evaluation metrics and proposed new
metrics to better align the generation quality with
human judgements.
Connection with Dataset Biases The robust-
ness failures can sometimes be attributed to dataset
biases, i.e., biases introduced during dataset col-
lection (Fouhey et al., 2018) or human annotation
artifacts (Gururangan et al., 2018; Geva et al., 2019;
Rudinger et al., 2017), which could affect how well
a model trained from this dataset generalizes, and
how accurately we estimate a model’s performance.
For example, Lewis et al. (2021) show there is a
signiﬁcant test-train data overlap in a set of open-
domain question-answering benchmarks, and many
QA models perform substantially worse on ques-
tions that cannot be memorized from training data.
In natural language inference, McCoy et al. (2019)4573show that commonly used crowdsourced datasets
for training NLI models might make certain syn-
tactic heuristics more easily adopted by statistical
learners. Further, Bras et al. (2020) propose to use
a lightweight adversarial ﬁltering approach to ﬁlter
dataset biases, which is approximated using each
instance’s predictability score.
4.2 Model-based Identiﬁcation
In addition to the human-prior and error-analysis
driven approaches which are usually speciﬁc to
each task, other lines of work identify robustness
failures that are task-agnostic like white-box text at-
tack methods (Ebrahimi et al., 2018; Alzantot et al.,
2018; Jin et al., 2020), and even input-agnostic like
universal adversarial triggers (Wallace et al., 2019a)
and natural attack triggers (Song et al., 2021).
Another line of work proposes to learn an addi-
tional model to capture biases, e.g., in visual ques-
tion answering, Clark et al. (2019) train a naive
model to predict prototypical answers based on the
question only irrespective of the context; He et al.
(2019); Utama et al. (2020a) propose to learn a
biased model that only uses dataset-bias related
features. This framework has also been used to
capture unknown biases assuming that the lower
capacity model learns to capture relatively shallow
correlations during training (Clark et al., 2020). In
addition, Wang and Culotta (2020a) identify model
shortcuts by training classiﬁers to better distinguish
“spurious” correlations from “genuine” ones based
on human annotated examples.
Model-in-the-loop vs. Human-in-the-loop
Some work adopts human-in-the-loop to gener-
ate challenging examples, e.g., Counterfacutal-NLI
(Kaushik et al., 2019) and Natural-Perturbed-QA
(Khashabi et al., 2020). Other work applies model-
in-the-loop to increase the likelihood that the per-
turbed examples are challenging for state-of-the-art
models, but it might also introduce biases towards
the particular model used. For example, SWAG
(Zellers et al., 2018) was introduced that fooled
most models at the time of publishing but was soon
“solved” after BERT (Devlin et al., 2019) was in-
troduced. As a result, Yuan et al. (2021) present
a study over the transferability of adversarial ex-
amples, and Contrast Sets (Gardner et al., 2020)
intentionally avoid using model-in-the-loop. Fur-
ther, more recent work adopts adversarial human-
and-model-in-the-loop to create more difﬁcult ex-
amples for benchmarking, e.g., Adv-QA (Bartoloet al., 2020), Adv-Quizbowl (Wallace et al., 2019b),
ANLI (Nie et al., 2020), and Dynabench (Kiela
et al., 2021).
5 Improve Model Robustness
Correspondingly, there are multiple lines of direc-
tions that try to improve robustness in NLP mod-
els. Depending on where and how the intervention
is applied, those approaches can be categorized
into the following categories: data-driven (Sec-
tion 5.1), model-based and training-scheme-based
(Section 5.2), inductive-prior-based (Section 5.3)
and ﬁnally causal intervention (Section 5.4).
5.1 Data-driven Approaches
Data augmentation recently gained a lot of interest,
in improving performance in low-resourced lan-
guage settings, few-shot learning, mitigating biases,
and improving robustness in NLP models (Feng
et al., 2021; Dhole et al., 2021). Techniques like
Mixup (Zhang et al., 2018), MixText (Chen et al.,
2020), CutOut (DeVries and Taylor, 2017), Aug-
Mix (Hendrycks et al., 2020b), HiddenCut (Chen
et al., 2021a), have been shown to substantially
improve the robustness and the generalization of
models. Such mitigation strategies are operated at
the data level, and often hard to be interpreted in
terms of how and why mitigation works.
Other lines of work deal with spans or regions
associated within data points to prevent models
from heavily relying on spurious patterns. To make
NLP models more robust on sentiment analysis and
NLI tasks, Kaushik et al. (2019) proposed curating
counterfactually augmented data via a human-in-
the-loop process, and showed that models trained
on the combination of this augmented data and
original data are less sensitive to spurious patterns.
Differently, Wang et al. (2021d) performed strate-
gic data augmentation to perturb the set of “short-
cuts” that are automatically identiﬁed, and found
that mitigating these leads to more robust models in
multiple NLP tasks. This line of mitigation strate-
gies closely relates to how spurious correlations
can be measured and identiﬁed, as many of the
challenging or adversarial examples (Table 1) can
sometimes be used to augment the original model
to improve its robustness, either in the discrete in-
put space as additional training examples (Liu et al.,
2019; Kaushik et al., 2019; Anastasopoulos et al.,
2019; Vaibhav et al., 2019; Khashabi et al., 2020),
or in the embedding space (Zhu et al., 2020; Zhao4574et al., 2018b; Miyato et al., 2017; Liu et al., 2020).
5.2 Model and Training-based Approaches
Pre-training Recent work has demonstrated pre-
training as an effective way to improve NLP
models’ out-of-distribution robustness (Hendrycks
et al., 2020a; Tu et al., 2020), potentially due to
its self-supervised objective and the use of large
amounts of diverse pre-training data that encour-
ages generalization from a small number of exam-
ples that counter the spurious correlations. Tu et al.
(2020) showed a few other factors can also con-
tribute to robust accuracy, including larger model
size, more ﬁne-tuning data, and longer ﬁne-tuning.
A similar observation is made by Taori et al. (2020)
in the vision domain, where the authors found train-
ing with larger and more diverse datasets offer bet-
ter robustness consistently in multiple cases, com-
pared to various robustness interventions proposed
in the existing literature.
Training with a Better Use of Minority Exam-
ples Further, there are several works that propose
to robustify the models via a better use of minority
examples, e.g., examples that are under-represented
in the training distribution, or examples that are
harder to learn. For example, Yaghoobzadeh et al.
(2021) proposed to ﬁrst ﬁne-tune the model on the
full data, and then on minority examples only.
In general, the training strategy with an empha-
sis on a subset of samples that are particularly hard
for the model to learn is sometimes also referred
to as group DRO (Sagawa et al., 2020a), as an ex-
tension of vanilla distributional robust optimization
(DRO) (Ben-Tal et al., 2013; Duchi et al., 2021).
Extensions of DRO are mostly discussing the strate-
gies on how to identify the samples considered as
minority: Nam et al. (2020) trained two models
in parallel, where the “debiased” model focuses
on examples not learned by the “biased” model;
Lahoti et al. (2020) used an adversary model to
identify samples that are challenging to the main
model; Liu et al. (2021) proposed to train the model
a second time via up-weighting examples that have
high training losses during the ﬁrst time.
When to Use Data-driven or Model-based Ap-
proaches? In many cases both the data and the
model can contribute to a model’s lack of ro-
bustness, hence data-driven and model-based ap-
proaches could be combined to further improve a
model’s robustness. One interesting phenomenonobserved by (Liu et al., 2019) is to attribute mod-
els’ robustness failures to blind spots in the training
data, or the intrinsic learning ability of the model.
The authors found that both patterns are possible:
in some cases models can be inoculated via be-
ing exposed to a small amount of challenging data,
similar to the data augmentation approaches men-
tioned in Section 5.1; on the other hand, some
challenging patterns remain difﬁcult which con-
nects to the larger question around generalizability
tounseen adversarial and counterfactual patterns
(Huang et al., 2020; Jha et al., 2020; Joshi and
He, 2021), which is relatively under-explored but
deserves much attention.
5.3 Inductive-prior-based Approaches
Another thread is to introduce inductive bias (i.e., to
regularize the hypothesis space) to force the model
to discard some spurious features. This is closely
connected to the human-prior-based identiﬁcation
approaches in Section 4.1 as those human-priors
can often be used to re-formulate the training ob-
jective with additional regularizers. To achieve
this goal, one usually needs to ﬁrst construct a
side component to inform the main model about
the misaligned features, and then to regularize the
main model according to the side component. The
construction of this side component usually relies
on prior knowledge of what the misaligned fea-
tures are. Then, methods can be built accordingly
to counter the features such as label-associated
keywords (He et al., 2019), label-associated text
fragments (Mahabadi et al., 2020), and general
easy-to-learn patterns of data (Nam et al., 2020).
Similarly, Clark et al. (2019, 2020); Utama et al.
(2020a,b) propose to ensemble with a model ex-
plicitly capturing bias, where the main model is
trained together with this “bias-only” model such
that the main model is discouraged from using bi-
ases. More recent work (Xiong et al., 2021) shows
the ensemble-based approaches can be further im-
proved via better calibrating the bias-only model.
Furthermore, additional regularizers have been in-
troduced for robust ﬁne-tuning over pre-trained
models, e.g., mutual-information-based regulariz-
ers (Wang et al., 2021a) and smoothness-inducing
adversarial regularization (Jiang et al., 2020).
In a broader scope, given that one of the main
challenges of domain adaptation is to counter the
model’s tendency in learning domain-speciﬁc spu-
rious features (Ganin et al., 2016), some methods4575contributing to domain adaption may have also pro-
gressed along the line of our interest, e.g., domain
adversarial neural network (Ganin et al., 2016).
This line of work also inspires a family of methods
forcing the model to learn auxiliary-annotation-
invariant representations with a side component
(Ghifary et al., 2016; Wang et al., 2017; Rozantsev
et al., 2018; Motiian et al., 2017; Li et al., 2018;
Wang et al., 2019c; Vernikos et al., 2020).
Despite the diverse concrete ideas introduced,
the above is mainly training for small empirical
loss across different domains or distributions in
addition to forcing the model to be invariant to
domain-speciﬁc spurious features. As an exten-
sion along this direction, invariant risk minimiza-
tion (IRM) (Arjovsky et al., 2019) introduces the
idea of invariant predictors across multiple environ-
ments, which was later followed and discussed by
a variety of extensions (Choe et al., 2020; Ahmed
et al., 2020; Rosenfeld et al., 2021). More recently,
Dranker et al. (2021) applied IRM in natural lan-
guage inference and found that a more naturalistic
characterization of the problem setup is needed.
5.4 Causal Intervention
Casual analyses have also been utilized to exam-
ine robustness. Srivastava et al. (2020) leverage
humans’ common sense knowledge of causality to
augment training examples with a potential unmea-
sured variable, and propose a DRO-based approach
to encourage the model to be robust to distribution
shifts over the unmeasured variables. Balashankar
et al. (2021) study the effect of secondary attributes,
or confounders, and propose context-aware coun-
terfactuals that take into account the impact of sec-
ondary attributes to improve models’ robustness.
Veitch et al. (2021) propose to learn approximately
counterfactual invariant predictors dependent on
causal structures of the data, and show it can help
mitigate spurious correlations in text classiﬁcation.
5.5 Connections between Mitigations
Connecting these methods conceptually, we con-
jecture three different mainstream approaches: one
is to leverage the large amount of data by taking ad-
vantages of pre-trained models, another is to learn
invariant representations or predictors across do-
mains or environments, while most of the rest build
upon the prior on what the spurious patterns are and
encourage the models to not rely on those patterns.
Then the solutions are invented through countering
model’s learning of these patterns by either dataaugmentation, reweighting (the minorities), ensem-
ble, inductive-prior design, and causal interven-
tion. Interestingly, statistical work has shown that
many of these mitigation methods are optimizing
the same robust machine learning generalization
error bound (Wang et al., 2021c).
6 Open Questions
In addition to the challenges mentioned above, we
list below a few open questions that call for addi-
tional research going forward.
Identifying Unknown Robustness Failures Ex-
isting identiﬁcation around robustness failures rely
heavily on human priors and error analyses, which
usually pre-deﬁne a small or limited set of patterns
that the model could be vulnerable to. This re-
quires extensive amount of expertise and efforts,
and might still suffer from human or subjective bi-
ases in the end. How to proactively discover and
identify models’ unrobust regions automatically
and comprehensively remains challenging.
Interpreting and Mitigating Spurious Correla-
tions Interpretability matters for large NLP mod-
els, especially key to the robustness and spurious
patterns. How can we develop ways to attribute or
interpret these vulnerable portions of NLP models
and communicate these robustness failures with
designers, practitioners, and users? In addition,
recent work (Wallace et al., 2019c; Wang et al.,
2021d; Zhang et al., 2021) show interpretability
methods can be utilized to better understand how
a model makes its decision, which in turn can be
used to uncover models’ bias, diagnose errors, and
discover spurious correlations.
Furthermore, the mitigation of spurious corre-
lations often suffers from the trade-off between
removing shortcuts and sacriﬁcing model perfor-
mance (Yang et al., 2020; Zhang et al., 2019a). Ad-
ditionally, most existing mitigation strategies work
in a pipeline fashion where deﬁning and detect-
ing spurious correlations are prerequisites, which
might lead to error cascades in this process. How
to design end-to-end frameworks for automatic mit-
igation deserves much attention.
Uniﬁed Framework to Evaluate Robustness
With a variety of potential spurious patterns in NLP
models, it becomes increasingly challenging for
developers and practitioners to quickly evaluate
the robustness and quality of their models. This
calls for more uniﬁed benchmarking efforts such as4576CheckList (Ribeiro et al., 2020), Reliability Test-
ing (Tan et al., 2021), Robustness Gym (Goel et al.,
2021) and Dynabench (Kiela et al., 2021), to facili-
tate fast and easy evaluation of robustness.
User Centered Measures and Mitigation In-
stead of passively detecting spurious correlations
from a post-processing perspective, how to ap-
proach robustness from a user centric perspective
needs further investigation. Based on the dual-
process models of information processing, humans
use two different processing styles (Evans, 2010).
One is a quick and automatic style that relies on
well-learned information and heuristic cues. The
other is a qualitatively different style that is slower,
more deliberative, and requires more reﬂective rea-
soning. Would these well-learned information and
heuristic rules be leveraged to help design better
human priors to measure and mitigate spurious cor-
relations? If users or stakeholders are involved in
this process, collecting a set of test cases where a
system might perform well for the wrong reasons
could help design sanity tests.
Connections between Human-like Linguis-
tic Generalization and NLP Generalization
Linzen (2020) argue NLP models should behave
more like humans to achieve better generalization
consistently. It is interesting to note that how
humans process information in NLP tasks exactly
is still under exploration, and to what extent
models should leverage human-knowledge is still
a debatable topic.Nonetheless, if we can better
understand and utilize the robustness properties
in human perception, we can potentially advance
models’ robustness in a more meaningful way.
7 Conclusion
In this paper, we provided a unifying overview
over robustness deﬁnitions, evaluations and mit-
igation strategies in the NLP domain. We also
highlighted open challenges in this area to motivate
future research, encouraging people to think deeply
about more comprehensive benchmarks, transfer-
ability and validity of adversarial examples, uni-
ﬁed framework to evaluate and improve robustness,
user-centered measures and mitigation, and ﬁnally
how to potentially achieve human-like linguistic
generalization more meaningfully.Acknowledgements
The authors would like to thank reviewers for their
helpful insights and feedback. This work is funded
in part by a grant from Google.
References4577457845794580458145824583458445854586