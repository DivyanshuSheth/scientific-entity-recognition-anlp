
Mohaddeseh Bastan
Stony Brook University
mbastan@cs.stonybrook.eduMihai Surdeanu
University of Arizona
msurdeanu@email.arizona.edu
Niranjan Balasubramanian
Stony Brook University
niranjan@cs.stonybrook.edu
Abstract
Text generation often involves producing texts
that also satisfy a given set of semantic con-
straints. While most approaches for conditional
text generation have primarily focused on lex-
ical constraints, they often struggle to effec-
tively incorporate syntactic constraints, which
provide a richer language for approximating
semantic constraints. We address this gap by
introducing N S D ,
a new decoding algorithm that incorporates syn-
tactic constraints to further improve the quality
of the generated text. We build N S - D on the NeuroLogic Decod-
ing (Lu et al., 2021b) algorithm, which en-
ables language generation models to produce
fluent text while satisfying complex lexical con-
straints. Our algorithm is powerful and scal-
able. It tracks lexico-syntactic constraints (e.g.,
we need to observe dogas subject and ballas
object) during decoding by parsing the partial
generations at each step. To this end, we adapt
a dependency parser to generate parses for in-
complete sentences. Our approach is evaluated
on three different language generation tasks,
and the results show improved performance in
both lexical and syntactic metrics compared to
previous methods. The results suggest this is a
promising solution for integrating fine-grained
controllable text generation into the conven-
tional beam search decoding.
1 Introduction
Controllable text generation uses decoding algo-
rithms that support lexico-semantic constraints to
control what is included in the output. Check-
ing for the presence or absence of certain words
has been shown to improve applications such as
recipe generation (Lu et al., 2021b), sentiment anal-
ysis (Howard et al., 2022), and predicting implicit
consequences of beliefs (Alexeeva et al., 2022).Figure 1: An example that compares the output pro-
duced by Neurologic Decoding with lexical constraints
alone vs. the output generated by N S
D with lexico-syntactic constraints.
Most of the current work in constrained genera-
tion focuses on handling lexical constraints (Miao
et al., 2019; Lu et al., 2021b; Guo and Roth, 2021;
Turcan et al., 2022). However, lexical constraints
alone cannot directly support more complex se-
mantic constraints such as the presence/absence
of relations or events between multiple concepts.
Consider the simple example in Figure 1, in which
target sentence requires that different people have
specific roles in an event. Lexical constraints can
only check presence or absence of words (ala bag-
of-words) and thus clearly cannot address such
role requirements. This work targets methods
that can handle these requirements. Such com-
plex syntactico-semantic constraints exist in many
other tasks, e.g., biomedical mechanism genera-
tion, where certain signaling pathways must be
present in the output (Bastan et al., 2022), goal-
oriented generation, where the output must include
the correct syntactic representation of the semantic
goal (Baumler and Ray, 2022), and machine trans-
lation, where syntactic differences induce semantic
failures (Wang et al., 2018).
We introduce a new decoding algorithm called
N S D , which supports
structural lexico-syntactic constraints during infer-9496ence. Specifically, it supports unary constraints
that verify syntactic roles (e.g., the word John ap-
pears as a subject), binary constraints that verify
a single dependency (e.g., the word John is the
subject of introduced ), and triplet constraints that
verify a two-way relation (e.g., John is the subject
ofintroduced whose object is Kayla ).
To efficiently track whether these structural con-
straints are satisfied, we extend the NeuroLogic
Decoding (Lu et al., 2021b) algorithm, which as-
signs constraint scores to indicate the degree to
which the constraints are satisfied. We address two
specific challenges here. First, unlike their lexi-
cal counterparts, syntactic constraints do not lend
themselves to hard definitions of reversible and
irreversible constraints. Therefore, we use a soft
interpretation that allows for satisfiability at later
stages. Second, enforcing structural constraints re-
quires tracking syntactic relations at every step of
the decoding process, which in turn requires pars-
ing the partially generated texts. Since parsers are
typically trained to handle complete sentences, we
adapt a dependency parser to handle incomplete
sentences.
We demonstrate the utility of N S - D on three different tasks and
on multiple language generation models. Evalu-
ations show that automatically-derived structural
constraints enforced by N S D- improve generation performance on three
distinct tasks: (i) a constrained text generation
challenge, COMMONGEN (Lin et al., 2019); (ii)
a mechanism summarization task, SuMe (Bastan
et al., 2022), (iii) a machine translation task from
German to English (Barrault et al., 2019).
2 Related Work
Constrained generation methods aim to produce
outputs that satisfy specific constraints. There is a
vast body of work spanning methods that operate
at inference-time (Post and Vilar, 2018; Miao et al.,
2019; Lu et al., 2021b; Kumar et al., 2022; Yang
and Klein, 2021; Turcan et al., 2022), or training-
time (Krishna et al., 2020; Lample et al., 2018;
Kobayashi et al., 2022; Kumar et al., 2021).
The inference-time methods, which include the
work proposed here, can be divided into two broad
categories based on the types of constraints consid-
ered. The first category focuses on simple lexical
constraints, which specify the presence or absence
of certain words or phrases in the generated text.The second focuses on satisfying more sophisti-
cated non-lexical constraints and enforcing rules
on the overall organization of the generated text.
These two categories are discussed in detail below.
2.1 Lexical Constraints
Anderson et al. (2017) introduced constrained
beam search by using a finite-state machine to track
constraints. Their method forces the inclusion of
selected tag words in the output and fixed, pre-
trained word embeddings to facilitate vocabulary
expansion to previously unseen tag words.
Hokamp and Liu (2017) developed grid beam
search as an extension of traditional beam search
that incorporates pre-specified lexical constraints
and groups together hypotheses based on the num-
ber of constraints satisfied. This method is biased
towards fulfilling constraints greedily, which can
lead to sub-optimal solutions in many cases.
Post and Vilar (2018) improved the previous
method with a dynamic beam allocation strategy,
which can only consider a small number of hy-
potheses at each step. Later, Miao et al. (2019) de-
veloped the Constrained Generative Model which
allows multiple keywords in the generated sen-
tences. It involves inserting all positive constraint
keywords in random order and then randomly sam-
pling edits to improve the fluency of the sentence
by replacing, inserting, or deleting words.
Sha (2020) presented an unsupervised optimiza-
tion method for addressing lexically-constrained
generation problems. Using gradients, they deter-
mine the appropriate location in a sequence for
inserting, replacing, or deleting words.
Lu et al. (2021b) proposed NeuroLogic Decod-
ing which uses logical constraints to include or
exclude lexical constraints, i.e., words or simple
phrases. The method proposed here is similar to
this approach, but without limiting the constraints
to lexical ones. Instead, our method also employs
structural constraints that contain syntactic rela-
tions between words. This approach enables more
flexible and effective constrained generation, al-
lowing for the consideration of both lexical and
structural constraints simultaneously.
2.2 Non-Lexical Constraints
Kumar et al. (2021) introduced a decoding algo-
rithm for controlled text generation which allows
for the simultaneous consideration of multiple dif-
ferentiable sentence-level constraints to control the
desired attributes of the generated text. However,9497the inclusion of complex structure based attributes
can slow down the decoding process. Wang et al.
(2021) proposed a method that can be incorporated
to leverage multiple rules simultaneously. These
rules go beyond simple lexical constraints, but they
still only consider semantics (content control) and
do not take into account syntactic structures. Nye
et al. (2021) introduced a method that incorporates
logical constraints to improve the coherence and
consistency of the generated text. This approach
examines the generated sentences for semantic con-
sistency and operates only at the sentence level.
It only considers semantic consistency with previ-
ously known conditions rather than syntactic con-
straints.
The incorporation of structure into machine
translation has been the subject of several stud-
ies. Chen et al. (2017) proposed a syntax-aware
encoder-decoder approach. These studies have
aimed to train models that incorporate structure into
the generated output. Fei et al. (2020) introduced
a structure-aware transformer model for machine
translation. Bastan and Khadivi (2022) presented a
reordering layer to the BiLSTM architecture with
the aim of incorporating structural information dur-
ing the training process of MT in low resource
settings. Yang et al. (2022) proposed a tree-based
machine translation approach that utilizes lexical
dependency parsing.
N S D is different
from previous research in that it aims to incorporate
syntax in the form of structural constraints into the
beam search decoding algorithm for constrained
generation. Our approach allows for the generation
of output that satisfies both lexical and syntactic
constraints without modifying the decoding algo-
rithm or operating only at the semantic level. All
is done at the inference stage without necessitating
any additional model training.
3 N S D
Our goal is to support structural constraints that re-
quire certain relationships to hold between the lexi-
cal items in the generated sentence. In this work,
we target structural constraints based on syntactic
dependencies due to their ubiquity. Our method can
potentially accommodate other constraints such as
semantic roles or other domain-specific relations.
We expand on the NeuroLogic Decoding (Lu
et al., 2021b) framework, which only handles lexi-
cal constraints. The fundamental idea is to assigna score to each hypothesis in the beam based on
how many lexical constraints have been satisfied.
The algorithm uses pruning, grouping, and selec-
tion strategies to efficiently search for the highest
scoring hypotheses that satisfy as many of the con-
straints as possible. Our work modifies this frame-
work to support structural syntactic constraints and
their logical combinations.
3.1 Structural Constraints
Formally, the input to N S D- consists of a conjunctive normal form
(CNF): {C∧C∧ ··· ∧ C}where each clause
C= (D∨...∨D)is a disjunction of some m
literals. Each literal Dis a structural constraint
expressing a logical condition for whether a spe-
cific syntactic structure should be present or absent
in the generated text. To assess if a hypothesis in
the beam (i.e., a partial generation) satisfies such
a constraint, we check if the dependency tree of
the hypothesis contains the syntactic structure. We
support three types of structural constraints:
•Unary Constraint: A unary constraint asserts a
dependency role for a single word. For example
the unary constraint D= (ball, obj )specifies
that the word ballshould appear in the generated
text as an object of some verb.
•Binary Constraint: A binary constraint asserts
a dependency relation between two words. The
binary constraint D= (team, subj, run ), for
example, asserts that the word team should ap-
pear as the subject of the word run.
•Triplet Constraint: A triplet constraint asserts a
syntactic condition over three words specifying
two dependency relations. For example, the con-
straint D= (team, run, field )specifies two
connected dependency relations. The word run
should appear as the verb that connects the sub-
jectteam and the object field. The triplet con-
straints allow for more fine-grained control for
approximating semantic relations often expressed
via predicate-argument structures.
3.2 Decoding
The goal of decoding is to find sequences that are
highly probable under the decoding model and ide-
ally satisfy the constraints. In practice, the problem
is framed as an optimization problem that balances
both aspects by using a scoring function that penal-
izes for the number of clauses that are not satisfied.9498For a CNF with kclauses this can be stated as the
following maximization objective for decoding:
ˆy= arg maxP(y|x)−λ/summationdisplay(1−C)
where we overload Cto denote a function that
returns 1if the underlying clause is true, i.e., if at
least one of the literals in its disjunction is satisfied,
and zero otherwise.
This objective is then used with a beam search
where at each step we score each hypothesis in
the beam based on the language model probability
and the penalty for structural constraint satisfac-
tion. The overall process can be summarized in the
following steps:
1.Use the decoder to generate the top nprobable
words to extend each hypothesis in the beam.
2.For each extended hypothesis l, produce a parse
treePusing a dependency parser.
3.Use pruning to remove hypotheses that have
irreversibly violated any of the clauses; then group
hypotheses based on shared irreversibly satisfied
clauses. Within each group, use a selection strategy
to maximize the chances of finding hypotheses that
meet the constraints.
4.Compute the penalty for each clause Cbased on
whether any of its individual structural constraint
Dis satisfied in P.
5.Use the λ-weighted combination of the total
penalties for each hypothesis and its model proba-
bility as the final score for each group.
We refer the reader to Section 2.3 in (Lu et al.,
2021b) for details on the pruning, grouping, and
selection strategies. Here we detail the key changes
in how we determine reversible and irreversible
satisfaction.
3.3 Constraint States for Pruning
Some hypotheses can be effectively pruned from
the beam if we know that they have violated a con-
straint in an irreversible manner. The NeuroLogic
Decoding framework tracks the following states for
each clause:
Satisfied-Reversible: A constraint that is satisfied
but can be unsatisfied later on.
Satisfied-Irreversible: A constraint that is satis-
fied and can not later be changed to the unsatisfied.
Unsatisfied-Reversible: A constraint that is yet
unsatisfied but can be reversed later on.Unsatisfied-Irreversible: A constraint that is un-
satisfied and can not be satisfied later.
Assigning these states is more complicated for
N S D because the
words mentioned in a structural constraint can ap-
pear in a hypothesis but violate the expected struc-
tural relationship. In such cases, we need to deter-
mine if this violation is an irreversible one. Note
that unlike lexical constraints, here we cannot make
hard guarantees for the irreversible determinations
because the syntactic structure of a sentence may
change as more tokens are generated. The heuris-
tic we apply in this paper is two fold: (a) All ap-
pearance of binary and triplet constraints are irre-
versible because, in our experience, larger syntactic
structures are less likely to change during decod-
ing, and (b) All appearance of unary constraints are
reversible. More details below.
For binary and triplet constraints, if a constraint
is satisfied, it will be considered irreversible. For
instance, if a word boyis seen as the subject of
verb plays , we assume it can not be reversed later.
We also assume that if a structure is seen with dif-
ferent syntactic condition other than the given con-
straint, this is an Unsatisfied-Irreversible condition.
For instance, consider the triplet constraint (Kathy,
plays, guitar) . If the noun John appears as the sub-
ject of the verb plays rather than Kathy , and the
word ballappears as its object rather than guitar ,
we deem the constraint as Unsatisfied-Irreversible.
For unary constraints, we are more cautious with
irreversibility. That is, even if the word mentioned
in a constraint appears in a different syntactic role
(thus violating the constraint), we allow it to remain
reversible. For example, for the constraint (John,
subj) , if the word John is generated as the subject
violating its required syntactic role as an object,
we keep the constraint reversible, so that the word
John can reappear later in the decoding process.
3.4 Parsing Incomplete Sentences
Handling syntactic constraints requires parsing in-
complete sentences that are generated at each step
of the decoding process. Since dependency parsers
are typically trained on complete sentences, we in-
troduce a simple adaptation process to extend their
capabilities to handle incomplete sentences.
We use a two-stage process, where we first train
the parser on the standard dependency training us-9499ing Penn Treebank WSJ dataset (Marcus et al.,
1993). Then, we adapt it to handle incomplete
sentences by continuing the training on a modified
dataset containing incomplete sentences. We con-
struct this new dataset as follows. First, for each
sentence in the original WSJ dataset, we extract
multiple sentence fragments containing words at
positions [0, k], for every k∈[1, n], where nis
the sentence length. Second, we extract the con-
stituency edges for the words included in these
fragments. Third, We convert each constituency
tree to the corresponding fragment’s dependency
parse tree. In this case we make sure we don’t have
any missing dependency edges. As we show in the
next section, this simple two-stage training process
substantially improves the parser’s performance on
incomplete sentences.
4 Evaluation
We demonstrate the utility of N S - D on a variety of benchmark tasks
related to the controllable text generation: COM-
MONGEN in Section 4.1, SuMe in Section 4.2,
and Machine Translation in Section 4.3.
To prepare the training data for partial sentences,
we convert constituency parse trees of partial sen-
tences to dependency parse trees using Stanford
CoreNLP library (Manning et al., 2014). To train
the dependency parser for partial sentences, we use
Stanza (Qi et al., 2020) model. We adapt it to parse
incomplete sentences using the three-stage training
described in Section 3.4.
Table 1 shows the improvements of the adapted
parser over the default Stanza parser model on the
test partition of the WSJ dataset, modified to con-
tain both complete and incomplete sentences. For
all tasks, when checking for syntactic constraints
for subjects we use nsubj dependencies and for
objects relations use objandobldependencies.
4.1 Constrained Commonsense Generation
COMMONGEN (Lin et al., 2019) is a constrained
text generation challenge designed as an assess-
ment of generative commonsense reasoning. The
task is to create a coherent sentence describing a
scenario using a set of common concepts. A plausi-
ble sentence describing a real-life scenario requires
a grammatically correct construction while adher-
ing to and reasoning over commonsense relation-
ships between the concepts.Default Adapted
UAS 78.75 95.09
LAS 74.90 93.86
CLAS 72.76 93.00
MLAS 71.41 92.57
4.1.1 Problem Formulation
Given a set of input concepts the goal is to use them
and construct a sentence ythat is grammatically
correct, makes sense, and describes a typical sce-
nario, in which all concepts are used. The input
concepts are a set of nouns {c, ...c}and verbs
{v,···, v}. An example instance is the set of
nouns {girl, hand} and a verb {wash} and a valid
output sentence is: The girl washed her hand .
Given the noun and verb sets we define differ-
ent constraints. The unary constraints assert that
the nouns should appear in a subject or an object
role (e.g., girl← − − *), and the verb must be the
main verb of the sentence (i.e., its head is root ).
The binary constraints assert that the nouns should
pair with some verb in a subject or object depen-
dency relation (e.g., girl← − − wash). The triplet
constraints assert that noun pair should appear in
the subject and object dependency relation with a
verb (e.g. girl← − − wash− − → hand).
Formally, we can state the constraints as follows:
Unary :
([c← − − ∗ ]∨[c← − − ∗ ])
∧ ··· ∧ ([c← − − ∗ ]∨[c← − − ∗ ])
∧([v← − − root]∨ ··· ∨ [v← − − root])
Binary : For each vin the verbs we add:
([cv]∨[c← − −v])
∧ ··· ∧ ([c← − − v]∨[c← − −v])
Triplet : For each verb vwe add the following
constraints:
([c← − − v− − →c]∨[c← − − v− − →c]∨ ···∨
[c← − − v− − →c]∨[c← − − v− − →c])95004.1.2 Evaluation Setup
We treat this problem as a conditional text genera-
tion, where we train a model on the training data
and use the automatically derived constraints at test
time to perform N S D .
The COMMONGEN dataset consists of 32,651 in-
stances in train, 993 in validation set, and 1,497
in test set. We benchmark state-of-the-art text to
text generation models T5 (Raffel et al., 2020) and
BART (Lewis et al., 2020) and evaluate the perfor-
mance of different types of constraints based on
these models. We use a finetuned models from the
Huggingface library (Wolf et al., 2019).
For evaluation, we follow the evaluation metrics
introduced in (Lin et al., 2019) and structural cov-
erage which measure the percentage of structural
constraints that are satisfied. We extract the gold
structural constraints by using Stanza (Qi et al.,
2020) parser on the gold outputs. We evaluate the
performance of Vanilla Decoding (beam-based de-
coder), NeuroLogic Decoding, and N S - D .
4.1.3 Results
Table 2 shows the results for vanilla decoding, Neu-
roLogic Decoding and N S D- when using the three types of constraints.
For both models, the lexical constraints in Neu-
roLogic Decoding improve over vanilla decoding.
Unary constraints provide small gains over vanilla
decoding but binary and triplet constraints show
significant improvements over both vanilla and the
lexical constraints. The lexical coverage numbers
show that vanilla decoding satisfies most of the
lexical constraints already, which means that the
underlying models already produces sentences that
contain the necessary words but with low structural
coverage between the concepts. In general, the im-
provements in structural coverage corresponds to
gains in the target quality measures, showing the
benefit of structural constraints for this task.
Table 3 presents examples of the outputs gener-
ated by the COMMONGEN task, comparing the
results obtained from the NeuroLogic Decoding
andN S D . These ex-
amples highlight three primary advantages of N-S D : (a) it generates full
sentences whereas NeuroLogic Decoding may fail
sometimes; (b) it can adhere to the intended syn-
tactic structure, and (c) it can produce semantically
and syntactically correct sentences that surpass the
quality of those from NeuroLogic Decoding.4.2 Biomedical Mechanism Generation
Summarizing biomedical mechanisms requires gen-
erating the relation underlying two biomedical en-
tities and a succinct and informative sentence that
effectively explains this relationship (Bastan et al.,
2022). This task presents significant challenges,
as it requires extracting and integrating informa-
tion from different sentences in order to generate a
mechanism sentence that explains whythe relation
exists or how the relation comes about.
4.2.1 Problem Formulation
Given a set of sentences from the abstracts of the
biomedical literature X={x, x, ..., x}and
two main entities etheregulator andethereg-
ulated entity , the goal is to output the correct re-
lation between two entities ( positive ornegative
regulation) and generate a sentence Ysummariz-
ing the mechanism underlying this relation.
For the lexical constraints in NeuroLogic Decod-
ing we define a multi-word constraint that encloses
the entity in the correct tag for it. Note that the task
provides the regulator and regulated entity informa-
tion as part of the input. We use this to create the
two multi-word lexical constraints.
[< re > e< er > ],[< el > e< le > ]
For the structural constraints, our goal is to
nudge the model towards sentences that express
the correct relation between the input entities. We
add unary and binary constraints that target some
lexico-syntactic connection between the entities as
a proxy for their semantic connection as follow.
Unary : These specify that the regulator entity ap-
pears as the subject of some verb and the regulated
entity as the object of some verb.
([e← − − ∗ ])∧([e← − − ∗ ])
Binary : As a binary constraint, we require that the
regulator and the regulated entities appear as the
subject andobject of the same verb:
([e∗e])
Note that we don’t specify which verb heads these
relations; it just needs to be the same verb.
4.2.2 Evaluation Setup
The SuMe (Bastan et al., 2022) dataset consists
of 20,765 instances in the training set, 1,000 in-
stances in the development set, and 1,000 instances9501ModelStructural
CoverageLexical
CoverageROUGE-L METEOR CIDEr SPICEVanilla Decoding 41.1 89.7 41.8 21.0 12.1 43.1
Neurologic Decoding 59.3 97.7 42.7 22.2 12.6 44.3
N Unary 59.5 98.1 42.8 22.1 13.2 44.9
S Binary 52.2 98.0 43.5 23.4 13.3 45.6
D Triplet 61.7 98.3 44.1 23.4 13.9 45.9Vanilla Decoding 41.5 95.5 41.3 21.1 12.8 43.5
Neurologic Decoding 58.2 98.1 42.6 22.4 13.0 44.4
N Unary 62.1 98.2 42.8 22.9 14.4 44.7
S Binary 65.6 98.5 43.1 23.7 15.1 45.6
D Triplet 72.2 99.1 44.5 24.2 15.5 46.1
in the test set. We only use the test set to evaluate
the performance of different constrained genera-
tion methods. We use a fine-tune model on top of
a pre-trained SciFive (Phan et al., 2021) for this
task and compare the three decoding strategies:
vanilla beam decoding, NeuroLogic Decoding, and
N S D . For evaluation,
we follow the metrics introduced in (Bastan et al.,
2022). We also report the structural coverage for
all decoding algorithms.
4.2.3 Results
Table 4 shows the comparison between different
types of decoding for SuMe task. While lexical
constraints in NeuroLogic Decoding provide small
gains in the generation quality measures, using
structural constraints with N S
D yields larger gains. Even the unary
constraint provides improvements over the lexicalconstraints, and binary improves even further.
Lastly, we find that the structural constraints re-
sult in improvements in the relation generation per-
formance as well. More analysis on the outputs of
this task available in Appendix B.95024.3 Machine Translation
Lexical constraints have been previously used in
machine translation (MT) for controlling specific
terminology (Lu et al., 2021a), inferring the correct
word inflection given lemmatized constraints (Jon
et al., 2021), reducing gender bias problems (Lu
et al., 2021b), and improving the positive lexical
constraints satisfaction by vectorized data augmen-
tation (Hu et al., 2019).
In this work, we show how automatically de-
rived structural constraints when used with N-S D can help improve
MT performance. We evaluate our system on DE-
EN translation, which is known to suffer from non-
verbal agreement, idioms, terminology and name-
entity mismatch, and intricate German verbal gram-
mar (Barrault et al., 2019; Macketanz et al., 2018;
Avramidis et al., 2019).
4.3.1 Problem Formulation
The MT models take as input a source language
sentence X={x, ..., x}and output a target lan-
guage sentence Y={y, ..., y}. We formulate
the constrained decoding version of this by auto-
matically deriving a set of constraints Cfrom X
and use them during decoding.
To this end, we first parse the source language
sentence Xto identify the main verb ( root )x,
its subject x, and its object x. We then use a
word-to-word translation of these to obtain a set
of candidate translations for the verb ( Y), the sub-
ject (Y), and the object ( Y). . Lastly, we add
constraints that capture the subject-verb-object re-
lationship between these candidate translation sets
to varying degrees.
Unary : This requires the subject translations to
appear as the subject of some verb and similar spec-
ifications for the object and the main verb transla-
tions.
([y← − − ∗ ]∨ ··· ∨ [y← − − ∗ ])∧
([y← − − ∗ ]∨ ··· ∨ [y← − − ∗ ])∧
([y← − − root]∨ ··· ∨ [y← − − root])
Binary : This requires dependency relations be-
tween the subject and verb translations and the ob-
ject and verb translations. For each verb translation
y, we add the following:
([y← − − y]∨ ··· ∨ [y← − − y])
∧([y← − −y]∨ ··· ∨ [y← − −y])Triplet : For each translated verb vwe add the fol-
lowing constraints to establish that a pair of subject
and object translations share the corresponding de-
pendency relations with a specific verb translation.
For each verb translation ywe add the following:
([y← − − y− − →y]∨[y← − − y− − →y]
∨ ··· ∨ [y← − − y− − →y])
4.3.2 Evaluation Setup
We use the German to English test portion of
the WMT19 dataset (Barrault et al., 2019). This
dataset contains 2000 sentences with a total of
36,141/39,561 words, and 8,763/6,764 unique
word types in German and English. We use the
same model as described in (Lu et al., 2021a) and
use thee newstest19 set to evaluate.
For word to word translation process explained
in previous section, we take advantage of free, open
source Google Translate API.
4.3.3 Results
Table 5 shows that the automatically derived struc-
tural constraints produce improvements in trans-
lation quality as measured by BLEU. Using the
word-to-word translations directly as lexical con-
straints for NeuroLogic Decoding leads to a drop
in translation quality, which suggests that the can-
didate word translations produced through word-
to-word translation (which ignores context) are im-
perfect. Note that this penalizes N S - D as well, since it uses the same
lexical items in its structural constraints. Further,
the structural coverage of NeuroLogic Decoding is
the same as vanilla decoding, which suggests that
the model likely struggles to generate structural re-
lations between the translated words on its own. On
the other hand, enforcing structural constraints be-
tween the word-to-word translations provides con-
sistent gains across all three types of constraints,9503with improvements of 1.2 BLEU and 3.3 struc-
tural coverage over vanilla decoding. These results
demonstrate the effectiveness of N S - D considering that it is negatively
impacted by the imperfect word-to-word transla-
tions that are used in its constraints. Overall, these
findings highlight the potential of N S - D as a means to enhance the per-
formance of existing MT models.
5 Conclusion
We introduced a novel constrained generation de-
coding algorithm, called N S
D , which infuses structural constraints
driven by syntactic dependencies in the generated
output. We evaluated our method on three genera-
tion tasks: constrained commonsense generation,
summarizing biomedical mechanisms, and MT. We
showed that our method generates better texts ac-
cording to the various task-specific metrics when
compared against vanilla decoding and constrained
decoding that relies solely on lexical constraints.
Limitations
While we evaluate our method on three distinct
generation tasks, we acknowledge that we rely on
a single language (English) and a single type of
structural constraints (on top of syntactic depen-
dencies). Further work is required to verify if the
proposed approach holds on other languages (e.g.,
it is unclear how much our method is impacted
by low-resource languages where syntactic parsers
may be of lower quality) and other types of struc-
tural constraints (e.g., semantic roles). This work
focuses on relatively smaller langauge models and
does not address the impact and modes of usage
of structural constraints on larger language models
such as GPT-3.
Ethics Statement
This work did not rely on any user studies or anno-
tations.
Constrained generation may potentially be used
for nefarious purposes such as infusing biases or
misinformation in the generated texts. The authors
do not condone any such usages of the proposed
approach.
Acknowledgements
This material is based on research that is supported
in part by the National Science Foundation underthe award IIS #2007290, and in part by an award
from the Stony Brook Trustees Faculty Awards
Program.
References95049505
A Hyper-parameters
In Table 6 we use the following hyper parameters
forN S D . The same
name convention used as in (Lu et al., 2021b).
TaskCOMMON
GENSuMe WMT19
beam size 50 50 100
prune factor 100 50 200
length penalty 0.2 0.1 0.6
sat. tolerance 2 2 3
batch size 32 4 32
B SuMe Full Evaluation
The complete results of using different decoding
algorithms on the SuMe task is shown in Table 7
The mechanism sentences have to express the
correct relation and explain how or why the relation9506ModelStructural
CoverageROUGE-L BLEURT RG (F1)
Vanilla Decoding 38.2 43.3 47.8 79
Neurologic Decoding 41.5 43.7 48.1 80
N Unary 54.1 43.8 48.9 80
S Binary 55.0 44.1 49.1 81
D Triplet - - - -
is true. This often means that the model has to get
multiple semantic connections correct in the gen-
erated sentence some cases are shown in Table 8.
The lexico-syntactic connections enforced by the
structural constraints help the model with the main
semantic connection and also results in longer sen-
tences that have a better chance of describing the
underlying mechanisms.
Table 8 shows some examples of the generated
output via N S D vs
NeuroLogic Decoding. These examples show N-S D can solve some of
the issues mentioned in (Bastan et al., 2022) like
missing entities, wrong relation, and generated
mechanism.
CMachine Translation Output Examples
Table 9 shows the effectiveness of using N-S D over unconstrained
translation systems.95079508ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Not applicable. Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.9509/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.9510