
Minyi ZhaoLu ZhangYi Xu
Jiandong DingJihong GuanShuigeng ZhouShanghai Key Lab of Intelligent Information Processing, and School of
Computer Science, Fudan University, ChinaAlibaba GroupTongji University, China{zhaomy20, l_zhang19, yxu17, jdding, sgzhou}@fudan.edu.cnjhguan@tongji.edu.cn
Abstract
Recent works have empirically shown the ef-
fectiveness of data augmentation (DA) for NLP
tasks, especially for those suffering from data
scarcity. Intuitively, given the size of generated
data, their diversity andquality are crucial to
the performance of targeted tasks. However, to
the best of our knowledge, most existing meth-
ods consider only either the diversity or the
quality of augmented data, thus cannot fully tap
the potential of DA for NLP. In this paper, we
present an easy and plug-in data augmentation
framework EPiDA to support effective text clas-
sification. EPiDA employs two mechanisms:
relative entropy maximization (REM) and con-
ditional entropy minimization (CEM) to control
data generation, where REM is designed to en-
hance the diversity of augmented data while
CEM is exploited to ensure their semantic con-
sistency. EPiDA can support efficient and con-
tinuous data generation for effective classifier
training. Extensive experiments show that EP-
iDA outperforms existing SOTA methods in
most cases, though not using any agent network
or pre-trained generation network, and it works
well with various DA algorithms and classifi-
cation models. Code is available at https:
//github.com/zhaominyiz/EPiDA .
1 Introduction
Data augmentation (DA) is widely-used in clas-
sification tasks (Shorten and Khoshgoftaar, 2019;
Feng et al., 2021; Zhang et al., 2021). In computer
vision (CV), (Krizhevsky et al., 2012; Chatfield
et al., 2014; Szegedy et al., 2015) adopt strate-
gies like flipping, cropping, tilting to perform DA.
In natural language processing (NLP), (Xie et al.,
2017; Coulombe, 2018; Niu and Bansal, 2018;
Wei and Zou, 2019) find that native augmentation
skills such as spelling errors, synonym replacement,
deleting and swapping, can bring considerable per-
formance improvement. All these methods usevarious transformations for data augmentation, but
they do not achieve equal success in different NLP
tasks (Yang et al., 2020). Sometimes, they fail
to guarantee semantic consistency, and may even
bring semantic errors that are harmful to classifica-
tion. The reason lies in that data augmentation for
NLP is in discrete space, so it can easily incur large
deviation of semantics ( e.g.in sentiment classifica-
tion task, deleting emotional words from a sentence
will make its meaning completely different).
Generally, given the size of generated data, their
diversity and quality are crucial to the perfor-
mance of targeted tasks (Ash et al., 2019). Re-
cent works have begun to emphasize the diversity
or quality of augmented data. For example, in
CV , AA (Cubuk et al., 2019), Fast-AA (Lim et al.,
2019) and LTA (Luo et al., 2020) employ agent
networks to learn how to enhance diversity. In
NLP, language models are widely used to control
generation quality, including Back-translation (Sen-
nrich et al., 2016; Yu et al., 2018), Seq2seq models
(Kobayashi, 2018; Kumar et al., 2019; Yang et al.,
2020), GPT-2 (Radford et al., 2019; Anaby-Tavor
et al., 2020; Quteineh et al., 2020; Liu et al., 2020)
and T5 (Dong et al., 2021). In addition, some
works (Morris et al., 2020) in NLP utilize adver-
sarial augmentation to enrich the diversity of the
samples. However, to the best of our knowledge,
most existing works consider only either the quality
or the diversity of augmented data, so cannot fully
exploit the potential of data augmentation for NLP
tasks. Besides, recent existing DA methods for
NLP tasks usually resort to pre-trained language
models, are extremely inefficient due to huge model
complexity and tedious finetuning, which limits the
scope of their applications.
In this paper, we propose a new data augmenta-
tion framework for text classification. This frame-
work is called EPiDA (the abbreviation of Easy
Plug-inDataAugmentation), which employs two
mechanisms to control the diversity and quality4742
of augmented data: relative entropy maximiza-
tion(REM) and conditional entropy minimization
(CEM), where the former is for boosting diversity
while the latter for ensuring quality. Fig. 1 shows
the pipeline of EPiDA. EPiDA consists of a DA
algorithm, a classifier, and a Sample Evaluation
And Selection (SEAS) module. SEAS works with
the DA algorithm and the classifier, and evaluates
the candidate samples with the feedback of the
classifier. With REM and CEM, SEAS can select
samples of high diversity and quality to train the
classifier continuously until the model converges.
The main contributions of this paper are as fol-
lows:
1.We propose an easy plug-in data augmenta-
tion framework EPiDA for text classification.
EPiDA can work with various existing DA
algorithms and classification models, it is gen-
eral, efficient, and easy-to-deploy.
2.We design two mechanisms relative entropy
maximization (REM) and conditional entropy
minimization (CEM) to boost the diversity and
quality of augmented data simultaneously in
an explicit and controllable way.
3.We conduct extensive experiments to evaluate
EPiDA. Experimental results show that EP-
iDA outperforms existing DA methods, and
works well with different DA algorithms and
classification models.
The rest of this paper is organized as follows: Sec. 2
reviews related work and highlights the differences
between our work and major existing methods.
Sec. 3 introduce our method in details. Sec. 4
presents the results of performance evaluation, and
Sec. 5 concludes the paper.2 Related Work
In this section, we first review the related work of
DA for NLP, then expound the differences between
our method and the major existing ones. Accord-
ing to the methodology of data generation, exist-
ing methods can be categorized into three types:
rule-based, interpolation-based, and model-based,
respectively.
2.1 Rule-Based Methods
These works use easy and predetermined transfor-
mations without model components. (Kolomiyets
et al., 2011; Zhang et al., 2015; Wang and Yang,
2015) use synonyms to replace words. EDA (Wei
and Zou, 2019) and AEDA (Karimi et al., 2021)
introduce random insertions, swaps, and deletions.
Xie et al. (2017) employed spelling errors to aug-
ment sentences. ¸ Sahin and Steedman (2018) con-
ducted sentence rotating via dependency tree mor-
phing. Wei et al. (2021) proposed a multi-task view
of DA. SUB(Shi et al., 2021) generates new exam-
ples by substituting substructures via constituency
parse trees. Although these methods are easy to
implement, they do not consider controlling data
quality and diversity.
2.2 Interpolation-Based Methods
MIXUP (Zhang et al., 2017) pioneers this type of
works by interpolating the input and labels of two
or more real examples. Recently, many MIXUP
strategies (Verma et al., 2019; Yun et al., 2019)
were proposed in CV . Due to the discrete nature of
inputs of NLP tasks, such methods can be applied
to NLP tasks only via padding and mixing embed-
dings or higher hidden layers (Chen et al., 2020; Si
et al., 2021).
2.3 Model-Based Methods
Seq2seq and language models have been used to
generate high quality samples. Among these ap-
proaches, Back-translation (Sennrich et al., 2016;
Yu et al., 2018) translates sentences into another
language and then translates it back to the origi-
nal language. RNNs and transformers are used to
reconstruct sub-parts of real data with contextual
information (Kobayashi, 2018; Gao et al., 2019;
Yang et al., 2020). Recently, we have witnessed the
great success of large-scale pre-trained language
models (PLMs) such as BERT (Devlin et al., 2019),
XLNet (Yang et al., 2019), GPT-2 (Radford et al.,
2019) in NLP tasks. These state-of-the-art PLMs4743
are also widely used to augment samples (Ng et al.,
2020; Nie et al., 2020; Anaby-Tavor et al., 2020;
Quteineh et al., 2020; Liu et al., 2020; Dong et al.,
2021). For example, DataBoost (Liu et al., 2020)
develops a reinforcement learning strategy to guide
the conditional generation without changing the ar-
chitecture of GPT-2. Besides, adversarial augmen-
tation ( i.e.,attack, GANs) are also used to enrich
the diversity of the generated samples (Morris et al.,
2020; Simoncini and Spanakis, 2021). Although
model-based methods can control generation qual-
ity well via PLMs, they are computationally ineffi-
cient, which limits their applications.
2.4 Differences between EPiDA and Existing
Methods
To expound the differences between EPiDA and
typical existing methods, in Tab. 1 we present
a qualitative comparison from four dimensions:
whether controlling the diversity and quality of the
augmented data, whether using pre-trained model
(language model or agent network), and whether
using the feedback from the classifier.
As shown in Tab. 1, among the existing methods,
most control only either diversity ( e.g. AA and
EDA) or quality ( e.g. DataBoost) of augmented
data, thus cannot completely leverage the poten-
tial of data augmentation. And most use language
model or agent network, which is beneficial to
data quality but also inefficient. Only the recent
LearnDA (Zuo et al., 2021) and VDA (Zhou et al.,
2021) consider both diversity and quality, and only
AA uses feedback of the classifier. Our EPiDA
addresses both diversity and quality of augmented
data via the feedback of the classifier in an explicit
and controllable way, without the help of any addi-
tional model components, which makes it not only
more effective but also more efficient.Note that in addition to the differences listed in
Tab. 1, our method EPiDA differs from LearnDA
in at least three other aspects: 1) LearnDA em-
ploys perplexity score (PPL) and cosine similar-
ity to measure diversity and quality respectively,
while EPiDA adopts two mechanisms relative en-
tropy maximization (REM) and conditional entropy
minimization (CEM) to control diversity and qual-
ity, which is theoretically more rational and solid.
2) LearnDA is for event causality identification,
while EPiDA is mainly for text classification. 3)
LearnDA needs knowledge guidance, while EP-
iDA does not. These make it difficult to evaluate
LearnDA in our experimental settings. Thus, we
do not conduct performance comparison between
EPiDA and LearnDA. Nevertheless, in our abla-
tion study, we replace REM and CEM with PPL
and cosine similarity in EPiDA, and our experimen-
tal results show that EPiDA with REM and CEM
performs better than that with PPL and cosine sim-
ilarity. Besides, comparing with VDA that requires
PLM to provide substitution probability, EPiDA is
free of PLMs, and is more effective, efficient and
practical.
3 Method
As shown in Fig. 1, EPiDA consists of three com-
ponents: a DA algorithm T, a classifier or classi-
fication model C, and a Sample Evaluation and
Selection (SEAS) module that is the core compo-
nent of EPiDA. Generally, the DA algorithm and
the classifier can be any of existing DA algorithms
and classifiers. With the feedback of the classifier,
SEAS evaluates candidate samples generated by
the DA algorithm in terms of diversity andqual-
ityvia the Relative Entropy Maximization (REM)
mechanism and the Conditional Entropy Minimiza-
tion(CEM) mechanism, and outputs the qualified
samples to further train the classifier. So EPiDA
can serve as a plug-in component to boost existing
DA algorithms for training better target models.
3.1 The Rationale to Control DA
Consider a classification task with a dataset Xofn
samples: X={(x, y),(x, y),...,(x, y)}. Here,
xis a sample, yis its label. The loss function is
L(ω) =1
n/summationdisplayl(ωϕ(x);y). (1)
where ϕ:R→Ris a finite-dimensional feature
map, ω∈Rmeans learnable parameters, and l4744can be a common loss function like cross-entropy.
Now we employ a DA algorithm Tto conduct
augmentation for each sample in X. Lettbe the
j-th sample generated by Twithxas input, and
msamples are generated from x, the loss function
for the generated samples can be written as
L(ω) =1
n/summationdisplay1
m/summationdisplayl(ωϕ(t);y). (2)
Here, we assume 1) tandxhave the same label
y, so we can use yto optimize the new loss func-
tion; 2) Data augmentation does not significantly
change the feature map ϕ, that is, augmentation can
maintain semantic consistency of the sample space.
Now we combine the augmented samples into the
original samples, thus the total loss function of
EPiDA can be written as follows:
L(ω) =L(ω) +L(ω). (3)
Recall that we use the feedback of the classifier
Cto select samples. Specifically, we use the orig-
inal training samples Xto pre-train the classifier
C, and for each generated sample t, the feedback
signal about tfrom the classifier is used for evalu-
ating t. When the generation process is over, all
generated samples {t}are used to train Cagain.
First, we consider how to generate samples
of high diversity. Intuitively, generated samples
should be different from the original samples. Re-
calling that the classifier Cis pretrained by X,
so for generated sample t, its loss l(ωϕ(t);y)
should be large. In this sense, given the classifier
C(ωis fixed), we select samples that meet the
following objective function:
maxL(ω, ϕ(t)),(4)
which means that we are to generate “hard” sam-
ples for the classifier to cope with.
Second, we consider how to control the quality
of augmented data. Recall that we assume for each
augmented sample t, its label ykeeps unchanged,
so we can use the original label to evaluate the loss
function. However, due to the discrete nature of
language, it is nontrivial for augmented samples to
meet this assumption. Taking the sentiment analy-
sis task for example, suppose we use EDA (Wei and
Zou, 2019) to augment x=“you’ll probably love
it”, EDA may delete the word “love”. Obviously,the resulting sentence breaks the semantic consis-
tency. To guarantee semantic consistency, we limit
the semantic deviation of ϕ(t)from ϕ(x). Let
Mandρbe a metric function to measure semantic
difference between samples and a threshold respec-
tively, we impose the following constraint on ϕ(t):
|M(ωϕ(t), ωϕ(x))| ≤ρ. (5)
Thus, we can enhance data diversity by optimiz-
ing Eq. (4), and improve data quality using Eq. (5).
The problem turns to solve Eq. (4) and Eq. (5).
3.2 Relative Entropy Maximization
We rewrite the objective function in Eq. (4) via:
where p,H,Dindicate probability distribution,
Shannon entropy , and relative entropy respectively,
and actually H(p(y)) = 0 since p(y)is a one-
hot vector. According to Eq. (6), we try to aug-
ment samples with large relative entropy under the
given labels. Thus, we call this method relative
entropy maximization (REM) mechanism. As rela-
tive entropy measures the difference between the
two distributions p(ωϕ(t))andp(y), the larger
the difference is, the more diverse the augmented
sample is. Therefore, we define the diversity score
sof augmented sample tas follows:
s=D(p(ωϕ(t)), p(y)). (7)
3.3 Conditional Entropy Minimization
We use conditional entropy as the metric function
in Eq. (5) to constrain the semantic deviation of
ϕ(t)from ϕ(x),i.e.,M(·,·) := H(·|·). Then,
Eq. (5) can be rewritten to
H(p(ωϕ(t))|p(ωϕ(x)))≤ρ. (8)
where H(·,·)isconditional entropy . Furthermore,
to meet Eq. (8), we select samples {t}by solving
the following optimization problem:
minH(p(ωϕ(t))|p(ωϕ(x))).(9)
We call this conditional entropy minimization
(CEM) mechanism. The smallest value of4745H(p(ωϕ(t))|p(ωϕ(x)))is 0, indicating that
given p(ωϕ(x)),p(ωϕ(t))is exactly pre-
dictable. Eq. (9) can also be expanded to the dif-
ference between Shannon entropy Hand mutual
information I,i.e.,H(X|Y)=H(X)-I(X, Y ). In
other words, CEM minimizes the entropy of the
selected sample tand maximizes the mutual in-
formation between tand the original sample x,
which means that CEM tries to augment samples
of high prediction probability and high similarity
with the original sample. As in REM, we define
thequality score sof augmented sample tas
s=−H(p(ωϕ(t))|p(ωϕ(x))).(10)
Algorithm 1: EPiDA Data Augmentation.
Input: Classification model ω, input sample
xand its label y, DA algorithm T,
augmentation number mand
amplification factor K.
Output: maugmented samples.
ˆt←T(x);
s, s, s=R,R,R;
forj= 1,2, . . . , K ∗mdo
Calculate svia Eq. (7) ;
Calculate svia Eq. (10) ;
end
Take Min_Max_Norm for sands;
s=s+s;
id= argtopm( −s);
Return ˆt[id];
3.4 Algorithm and Implementation
The procedure of EPiDA is presented in Alg. 1.
For each input sample x, EPiDA outputs maug-
mented samples. First, we employ Tto generate
K∗mcandidate augmented samples for x, where
Kis a hyperparameter to amplify the number of
candidate samples, which is called amplification
factor . Then, for each augmented sample, we use
REM and CEM to evaluate its diversity score ( s)
and quality score ( s), respectively. Next, we
adopt Min_Max_Norm to make sandsfall
in [0,1]. After that, we add them together as the
overall score of the sample, and sort all the aug-
mented samples in descending order according to
their scores. Finally, we take the top msamples
from all the K∗mcandidate samples as the output,
and utilize them to train the classifier.
By nature, the goals of REM and CEM are con-
flicting, i.e., a sample of high diversity is more
probably of low quality, and vice versa. We give an
example in Tab. 2 to demonstrate this point. REM
encourages to change salient words, which is prone
to break the semantic consistency (see the 3rd row,
“excited” is changed to “mad”, leading to large di-
versity score but small quality score). However,
CEM tends to make the augmented samples keep
semantic consistency, i.e., has large quality score
but small diversity score (see the 4th row, “comes”
is deleted). By jointly considering REM and CEM,
satisfactory samples with balanced diversity and
quality can be found (see the 5th row).
Besides, the calculation of sandsrequires
the feedback of the classifier, so we first pre-train
the classifier using the original samples, then with
EPiDA we can generate samples of high diversity
and quality for the classifier continuously.
4 Performance Evaluation
In this section, we conduct extensive experiments
to evaluate EPiDA, including performance compar-
ison with SOTA methods, performance evaluation
when working with different DA algorithms and
classification models, ablation study, and qualita-
tive visualization of samples augmented by EPiDA.
4.1 Datasets and Settings
Datasets for five different tasks are used in our
experiments: Question Classification (Li and
Roth, 2002) ( TREC ,N=5,452), News Classifica-
tion (Zhang et al., 2015) ( AGNews ,N=120,000),
Tweets Sentiment Analysis (Rosenthal et al., 2017)4746
(Sentiment ,N=20,631), Tweets Irony Classifica-
tion (Van Hee et al., 2018) ( Irony ,N=3,817) and
Tweets Offense Detection (Founta et al., 2018)
(Offense ,N=99,603), where Nis the number of
training samples. To fully demonstrate the perfor-
mance of data augmentation, we use only part of
each dataset. In the following experiments, the
percentage (%) that follows the task name means
the ratio of training data used from each dataset,
e.g.Irony 1% means that 1% of the dataset is used.
Macro-F1 (F1 for binary tasks) is used as perfor-
mance metric, and all the experiments are repeated
five times. The amplification factor Kis set to 3.
4.2 Comparing with SOTA Methods
Here we carry out performance comparison with
major SOTA methods to show the superiority of
EPiDA on three datasets: Sentiment ,Irony and
Offense . For a fair comparison, we strictly follow
the experimental setting of DataBoost (Liu et al.,
2020): we do only one round of augmentation to
ensure that the number of samples of our method
is consistent with that of the other methods, and
use BERT as the classifier. We use the widely used
EDA as the DA algorithm of EPiDA. We do not
use DataBoost because it is not yet open-sourced.
The experimental results are presented in Tab. 3.From Tab. 3, we can see that 1) with the help
of EPiDA, the performance of EDA is greatly im-
proved. In particular, comparing with the original
EDA, EPiDA gets performance improvement of
14.1%, 8.39%, 22.83%, 33.40%, 6.75% and 9.22%
in six task settings, respectively. 2) Our method
outperforms DataBoost in four settings. In particu-
lar, EPiDA +EDA gets performance improvement
of 8.12%, 2.65%, 10.15% and 6.99% in various
settings of the Sentiment andIrony tasks. 3) The
variants of EPiDA that utilize only REM or CEM
to enhance diversity or quality are inferior to us-
ing both, which demonstrates the effectiveness of
joint enhancement. 4) DataBoost performs better
in the Offense task, the reason lies in that Data-
Boost can create novel sentences from Offense (a
relatively huge corpus) via GPT-2, while EDA only
conducts word-level augmentation, which limits
EPiDA’s performance. 5) We also present PPL as
an auxiliary metric to measure the generation per-
plexity. Our method outperforms the others due
to the high quality of data generation. We also
provide experimental comparisons with other DA
approaches (SUBand VDA) and generation speed
results in the supplementary file. In conclusion,
EPiDA is a powerful and efficient technique.4747
4.3 Performance with Different DA
Algorithms and Classifiers
EPiDA is a plug-in component that can work with
different DA algorithms and classifiers. Here, to
check how EPiDA performs with different DA algo-
rithms and classifiers, we consider three frequently-
used DA algorithms: rule-based EDA (Wei and
Zou, 2019), model-based CWE (Kobayashi, 2018)
and Attack-based TextAttack (Morris et al., 2020),
and three different classifiers: CNN (Kim, 2014),
BERT (Devlin et al., 2019) and XLNet (Yang et al.,
2019). And to show that EPiDA can cope with
different NLP classification tasks, we present the
results on five different tasks: TREC ,AGNews ,
Sentiment ,Irony andOffense . In order to fully
evaluate the performance of DA algorithms, we use
only a small part of the training data. The exper-
imental results are presented in Tab. 4. Here, we
remove the restriction of only one time augmen-
tation so that EPiDA can continuously generate
qualified samples. We call this online augmenta-
tion, to differentiate it from one-time augmentation.
As shown in Tab. 4, EPiDA is applicable to various
NLP classification tasks. Although these tasks have
different forms of data (questions or tweets) and
different degrees of classification difficulty, EPiDA
boosts performance on these tasks in almost all
cases. More details on how EPiDA controls the
generation quality are discussed in ablation study.
Besides, we can also see that EPiDA works well
with the three DA algorithms EDA, CWE and Tex-
tAttack. All achieve improved performance in most
cases. For the three different classification models:
CNN, BERT and XLNet, with the help of EPiDA,
they all but XLNet on Sentiment get classification
performance improvement, which shows that EP-
iDA is insensitive to classification models.
4.4 Ablation Study
Here we conduct ablation study to check the ef-
fectiveness of different EPiDA configurations. We
take CNN as the classifier, EDA as the DA algo-
rithm and report the Macro-F1 score over five re-
peated experiments on TREC 1% and Irony 1%.
Tab. 5 shows the experimental results.
Effect of REM and CEM . The 4th and 5th rows
show the results with only REM and CEM, respec-
tively. Both of them perform better than the base-
line (1st row), but not as good as the combined case4748(the 8th row). On TREC (relatively simple task),
REM outperforms CEM (0.729 vs. 0.723), while
onIrony (relatively hard task), CEM outperforms
REM (0.559 vs. 0.557). Using only REM can lim-
itedly boost performance since REM promotes the
generation of high diversity samples, which may
have wrong labels. And using only CEM is also
not enough to fully tap the performance as CEM
tends to generate redundant samples.
We also compare our ‘REM + CEM’ with ‘PPL
+ cosine similarity’ used in LDA (Zuo et al., 2021).
Our method achieves the performance of 0.740 and
0.576 on TREC 1% and Irony 1%, while the latter
achieves 0.730 and 0.562. This shows that our
‘REM + CEM’ is more effective.
Effect of online augmentation . Comparing the
results of the 2nd and the 3rd rows, the 6th and
the 7th rows, the 8th and the 9th rows, we can
see that generally online augmentation can boost
performance, as online augmentation can generate
sufficient qualified samples to train the model.
Effect of pre-training . As REM and CEM use
the feedback of the classifier, a pre-trained classi-
fication model should be beneficial to REM and
CEM. By comparing the results of the 6th and the
8th rows, the 7th and the 9th rows, it is obvious
that pre-training can improve performance.
Effect of normalization . In Alg. 1, we normal-
izesands. Here, we check the effect of
normalization. With the same experimental set-
tings, the performance results on TREC 1% and
Irony 1% without normalization are 0.732 and
0.568, lower than the normalized results 0.740 and
0.576. This shows that normalization is effective.
How to combine REM and CEM ? How to com-
bine REM and CEM is actually how to combine the
values of sands. We consider three simple
schemes: addition ( s=s+s), multipli-
cation ( s=s∗s) and weighted addition
(s=αs+(1−α)s,αis a hyperparameter
to tradeoff REM and CEM). Note that for multi-
plication, there is possibly an extreme situation:
after normalization, sorsmay be very small
and even approaches 0, then the multiplication re-
sult is very small or even zero, which means that
REM and CEM do not take effect in sample genera-
tion. In our experiments, the multiplication scheme
achieves performance of 0.725 and 0.572 on TREC
1% and Irony 1%, lower than the addition scheme
0.740 and 0.576. As for weighted addition, we find
that setting α= 0.5can achieve satisfactory re-
sults (see the supplementary file). This is actually
equal to the addition scheme. Therefore, in our
experiments, we use only the addition scheme.
Quality and diversity metrics . Here, we pro-
vide another two metrics to verify EPiDA from the
perspective of quality and diversity. For quality, we
use the augmentation error rate. As for diversity,
we calculate the average distance of samples before
and after augmentation (ignoring wrong samples).
From the perspective of quality and diversity, a
good DA should has a small error rate but a large
distance. Experimental results are given in Tab. 6.
We can see that EPiDA gets better trade-off be-
tween error rate and distance.
Effect of the amplification factor K. The am-
plification factor Kdetermines the size Km of
candidate samples from which msamples are cho-
sen. On the one hand, with a large K, we have
more choices, which seems beneficial to diversity.
On the other hand, more candidate samples make
the selected samples more homogenous, not good
for diversity. By grid search, we set Kto 3 in our
experiments, the experimental results are shown in
the supplementary file.47494.5 Visualization Effect of EPiDA
Above we give comprehensive quantitative perfor-
mance evaluation of EPiDA, here to intuitively il-
lustrate the effectiveness of EPiDA, we visualize
some augmented samples of EPiDA, and compare
them with that of EDA. Specifically, we utilize
BERT as the classifier and visualize its hidden state
on the sentiment analysis task via t-SNE (Van der
Maaten and Hinton, 2008). Fig. 2 shows the re-
sults. In terms of data quality, we find that two
negative samples generated by EDA are located in
Neural and Positive classes, while samples gener-
ated by EPiDA are generally properly located. And
in the point of view of diversity, samples gener-
ated by EPiDA extend the distributed areas of the
original data, while samples generated by EDA are
mainly located in the areas of the original samples.
This shows that samples generated by EPiDA are
more diverse than those generated by EDA.
5 Conclusion
In this paper, we present an easy plug-in data aug-
mentation technique EPiDA to control augmented
data diversity and quality via two mechanisms: rel-
ative entropy maximization andconditional entropy
minimization . Through extensive experiments, we
show that EPiDA outperforms existing methods,
and can work well with different DA algorithms
and classification models. EPiDA is general, ef-
fective, efficient, and easy-to-deploy. In the future,
more verification of our method is expected to be
conducted on other classification tasks.
Acknowledgement
This work was partially supported by Na-
tional Key R&D Program of China under grant
No. 2021YFC3340302, and Alibaba Innova-
tive Research (AIR) programme under contract
No. SCCW802020046613.
References475047514752