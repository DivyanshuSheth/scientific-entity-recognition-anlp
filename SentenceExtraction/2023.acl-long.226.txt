
Hang Yan, Yu Sun, Xiaonan Li, Yunhua Zhou, Xuanjing Huang, Xipeng Qiu
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
{hyan19,lixn20,zhouyh20,xjhuang,xpqiu}@fudan.edu.cn
yusun21@m.fudan.edu.cn
Abstract
Information Extraction (IE) spans several tasks
with different output structures, such as named
entity recognition, relation extraction and event
extraction. Previously, those tasks were solved
with different models because of diverse task
output structures. Through re-examining IE
tasks, we find that all of them can be inter-
preted as extracting spans and span relations.
They can further be decomposed into token-
pair classification tasks by using the start and
end token of a span to pinpoint the span, and
using the start-to-start and end-to-end token
pairs of two spans to determine the relation.
Based on the reformulation, we propose a
Unified Token-pair Classification architecture
forInformation Extraction ( UTC-IE ), where
we introduce Plusformer on top of the token-
pair feature matrix. Specifically, it models
axis-aware interaction with plus-shaped self-
attention and local interaction with Convolu-
tional Neural Network over token pairs. Ex-
periments show that our approach outperforms
task-specific and unified models on all tasks in
10 datasets, and achieves better or comparable
results on 2 joint IE datasets. Moreover, UTC-
IE speeds up over state-of-the-art models on
IE tasks significantly in most datasets, which
verifies the effectiveness of our architecture.
1 Introduction
Information Extraction (IE) aims to identify and
classify structured information from unstructured
texts (Andersen et al., 1992; Grishman, 2019). IE
consists of a wide range of tasks, such as named
entity recognition (NER), joint entity relation ex-
traction (RE)and event extraction (EE) .
In the last decade, many paradigms have been
proposed to solve IE tasks, such as sequence label-ing (McCallum and Li, 2003; Huang et al., 2015;
Zheng et al., 2017; Yu et al., 2020a), span-based
classification (Jiang et al., 2020; Yu et al., 2020b;
Wang et al., 2021; Ye et al., 2022), MRC-based
methods (Levy et al., 2017; Li et al., 2020; Liu
et al., 2020) and generation-based methods (Zeng
et al., 2018; Yan et al., 2021a; Hsu et al., 2022). The
above work mainly concentrates on solving individ-
ual tasks, but it is desired to unify all IE tasks with-
out designing dedicated modules, as tackling all
IE tasks with one model can facilitate knowledge
sharing between different tasks. Therefore, various
attempts have been made to unify all IE tasks with
one model structure. Wadden et al. (2019); Lin et al.
(2020); Nguyen et al. (2021) encode all IE tasks’
target structure as graphs and design graph-based
methods to predict them; Paolini et al. (2021); Lu
et al. (2022) solve general IE tasks in a genera-
tive way with text-to-text or text-to-structure frame-
works. However, graph-based models tend to be
complex to design, while generative models are
time-consuming to decode.
In our work, we creatively propose a simple yet
effective paradigm for unified IE. Inspired by Jiang
et al. (2020), we re-examine IE tasks and consider
that all of them are fundamentally span extraction
(entity extraction in NER and RE, trigger extraction
and argument span detection in EE) or relational
extraction(relation extraction in RE and argument
role classification in EE). Based on this perspective,
we further simplify and unify all IE tasks into token-
pair classification tasks . Figure 1 shows how each
task can be converted. Specifically, a span is de-
composed into start-to-end and end-to-start token
pairs. As depicted, the entity “School of Computer
Science” in Figure 1(a) is decomposed into indices
of (School, Science) and (Science, School). As for4096
detecting the relation between two spans, we con-
vert it into start-to-start and end-to-end token pairs
from head mention to tail mention. For example,
in Figure 1(b), the relation “Author” between “J.K.
Rowling” and “Harry Potter novels” is decomposed
into indices of (J.K., Harry) and (Rowling, novels).
Based on the above decomposition, we propose
aUnified Token-pair Classification architecture for
Information Extraction ( UTC-IE ). Specifically, we
first apply Biaffine model on top of the pre-trained
language model (PLM) to get representations of
token pairs. Then we design a novel Transformer
to obtain interactions between them. As the plus-
shaped dotted lines depicted in Figure 1, token
pairs in horizontal and vertical directions cover vi-
tal information for the central token pair. For span
extraction, token pairs in the plus-shaped orienta-
tion are either clashing or nested with the central
token pair, for example, eis contained by ein
Figure 1(a); for relational extraction, the central
token pair’s two constituent mentions locate in the
plus-shaped orientation, such as in Figure 1(b), r
is determined by eande. Therefore, we make
one token pair only attend horizontally and verti-
cally in the token pair feature matrix. Additionally,
position embeddings are incorporated to keep the
token pairs position-aware. Moreover, neighboringtoken pairs are highly likely to be informative to
determine the types of the central token pair, so
we apply Convolutional Neural Network (CNN) to
model the local interaction after the plus-shaped
attention. Since the attention map for one token
pair is intuitively similar to the plus operator, we
name this whole novel module as Plusformer .
We conduct numerous experiments in two set-
tings. When training separately on each task
(named as single IE task ), our model outperforms
previous task-specific and unified models on 10
datasets of all IE tasks. When training a single
model simultaneously on all IE tasks in one dataset
(named as joint IE task ), UTC-IE achieves better
or comparable results than 2 joint IE baselines. To
thoroughly analyze why UTC-IE is useful under
the token-pair paradigm, we execute several ab-
lation studies. We observe that CNN module in
Plusformer plays a significant role in IE tasks by
the abundant local dependency between token pairs
after the reformulation. Besides, owing to the good
parallelism of self-attention and CNN, UTC-IE is
one to two orders of magnitude faster than prior
unified IE models and some task-specific work. To
summarize, our key contributions are as follows
1.We introduce UTC-IE, which decomposes all
IE tasks into token-pair classification tasks .4097In this way, we can unify all single IE tasks
under the same task formulation, and use one
model to fit all IE tasks without designing task-
specific modules. Besides, this unified decom-
position is much faster than recently proposed
generation-based unified frameworks.
2.After the reformulation of different IE tasks,
we propose the Plusformer to model interac-
tion between different token pairs. The plus-
shaped self-attention and CNN in Plusformer
are well-motivated and effective in the refor-
mulated IE scenario. Experiments in 12 IE
datasets all achieve state-of-the-art (SOTA)
performance which justifies the superiority of
Plusformer in IE tasks.
3.The reformulation enables us to use one model
to fit all IE tasks concurrently. Therefore, we
can train one model on three IE tasks, and
results on two joint IE datasets show that the
proposed unification can effectively benefit
each IE task through multi-task learning.
4.Extensive ablation experiments reveal that
components in Plusformer are necessary and
beneficial. Among them, CNN module in
Plusformer can be essential to the overall per-
formance. Analysis shows that this perfor-
mance gain is well-explained because when
reformulating IE tasks into token-pair classifi-
cations, the adjacent token pairs can be infor-
mative and CNN can take good advantage of
the local dependency between them.
2 Task Decomposition and Decoding
We first introduce how we decompose IE tasks to
conduct training, then present the decoding proce-
dure for decomposition. More discussions about
the decomposition are presented in Appendix A.
2.1 Task Decomposition
Formally, given an input sentence of Ltokens x=
[x, x, ..., x], the potential token pairs can form a
score matrix Y∈R, where Sis span
classes, Ris relational classes. We stipulate
•When a span (s, e)is of type t, thenY=
Y= 1, where s, e∈[1, L]andt∈
[1,|S|]are the start, end token indices and
span type;•When the span (s, e)forms the relation r∈
[|S|+1,|S|+|R|]with another span (s, e),
thenY=Y= 1.
NER aims to extract all entities {(s, e, t)},
where t∈ SandSis pre-defined entity types.
Therefore, in NER, S=SandR=ϕ.
RE aims to extract all relations
{((s, e, t), r,(s, e, t))}, where the su-
perscript handtdenotes the head and tail entities,
t, t∈ S, r∈ RandS,Rare pre-defined
entity types, and relation types. Therefore, in RE,
S=SandR=R.
EE aims to extract all events
{{(s, e, t),(s, e, rol), . . . , (s, e, rol)}},
where (s, e)means the trigger span, t∈ S
is the event type, Sis pre-defined event types;
s, e∈[1, L]are the start and end token
indices of an argument span, kis the number of
arguments of the trigger. To extract argument
spans, we identify argument span into Swhich
binarily denotes "has argument / no argument",
thus|S|= 1;rol∈ Ris the role type of
the argument and Ris pre-defined role types.
Following the formulation in RE, we can view role
types from the trigger to the arguments as relations.
Therefore, in EE, S=S∪ SandR=R.
Joint IE aims to jointly extract entities, relations,
and events in the text. Extracting entities and rela-
tions are generally the same as those in NER and
RE. When extracting events, there is no need to
extract argument spans purposely because all argu-
ment candidates are entities. Therefore, in joint IE,
S=S∪ SandR=R∪ R.
2.2 Decoding
The decoding essentially extracts spans and re-
lations from the score matrix Y. IfY=
Y= 1andt∈[1,|S|], then the span (s, e)
is of type t. And for two spans (s, e)and
(s, e), ifY=Y= 1 andr∈
[|S|+1,|S|+|R|], then the span (s, e)forms re-
lation rwith the span (s, e). The above decoding
is for the ideal situation, where no span clash exists.
However, for model’s predictions, we need to first
resolve the conflicts. The decoding with model’s
predictions will be presented in Appendix B.
3 Method
Figure 2 shows an overview of the architecture.
Firstly, we present Biaffine (Dozat and Manning,
2017) model based on PLMs. Then, we propose a4098
novel Transformer-like structure named Plusformer
to model interactions between token pairs. Finally,
we describe loss functions.
3.1 Biaffine Model
Given an input sentence, we first apply a PLM as
our sentence encoder to obtain the contextualized
representation as follows
where H∈R,dis the PLM ’s hidden size.
Next, we use the Biaffine mechanism to get fea-
tures for each token pair as follows
where MLP,MLPare multi-layer percep-
tron layers, H,H∈R,W∈R,
W∈R,v∈R,⊕refers to concatenation;
S∈Rprovides features for all possible to-
ken pairs, and cis the feature dimension size.
3.2 Plusformer
As illustrated in Section 1, when modeling the inter-
action between token pairs, the plus-shaped and lo-
cal interaction should be beneficial. Therefore, we
introduce the axis-aware plus-shaped self-attention
and position embeddings to conduct plus-shaped in-
teraction, we name this self-attention PlusAttention.
Then, we leverage CNN to model local dependen-
cies. We name this whole structure Plusformer .
PlusAttention. We first apply the self-attention
mechanism (Vaswani et al., 2017) horizontally andvertically as follows
where W,W,W,W,W,W∈R,
Z,Z∈R. After the self-attention, we use
the following method to merge Z,Z
S= MLP( Z⊕Z), (4)
where S∈R. We make the plus-shaped
self-attention axis-awareness by using two groups
of attention parameters and using concatenation
instead of an addition to merge Z,Z.
Position Embeddings. Although the model
should be able to distinguish between horizontal
and vertical directions through axis-aware plus-
shaped attention, it still lacks the sense of distances
between token pairs and the area the token pair
locates. Hence, we use two kinds of position em-
beddings to enable the model with these abilities.
•Rotary Position Embedding (RoPE) (Su
et al., 2021) can encode the relative distance
between two token pairs. It is utilized in both
horizontal and vertical self-attention.
•Triangle position embedding is incorporated
to mark the position of token pairs in the fea-
ture map, indicating whether the cell is in the
upper or lower triangles. It adds to Sin Eq.(3)
before Attention.4099CNN Layer. After the PlusAttention, we apply
CNN with kernel size 3×3on the Sto help the
model exploit the local dependency between neigh-
boring token pairs. The formulation is as follows
S= Conv( σ(Conv( S))) (5)
where S∈R, andσis the activation func-
tion; and the bias term of CNN is not used to avoid
result inconsistencies for a sample when it is in
batches of different lengths.
The Plusformer layer will be repeatedly used to
interact fully between token pairs. Layer normaliza-
tion (Ba et al., 2016) is ignored in the formulation
for brevity.
3.3 Loss Function
Finally, we get final scores as follows
ˆY,ˆY= Sigmoid( ˆY),ˆY,
ˆY= MLP( S+S),(6)
where ˆY∈R,ˆY∈Rare
scores for span extraction and relational extraction,
respectively; and ˆY∈R. The +1
in(|R|+1) is because we use the adaptive thresh-
olding loss (ATL) from Zhou et al. (2021) to avoid
a global threshold in relational extraction.
For the span extraction, we use the binary cross-
entropy (BCE) loss as follows
L=−/summationdisplay/summationdisplay[YlogˆY
+ (1−Y)log(1 −ˆY)](7)
For the relational extraction, we utilize the ATL
as follows
where PandNdenote the positive and negative
classes, ˆYis the score for the threshold
class TH. Only token pairs with scores higher
than their corresponding adaptive thresholds are
considered when decoding. We do not use ATL for
span extraction because we need to sort span scores
when decoding spans. The total loss L=L+L
is used for optimization.4 Experiments
4.1 Experimental Settings
We conduct experiments on 10 datasets across three
IE tasks, including NER, RE, and EE, and on 2 joint
IE datasets. We evaluate NER task with CoNLL03
(Sang and Meulder, 2003) and OntoNotes (Pradhan
et al., 2013) on flat NER, and with ACE04 (Dod-
dington et al., 2004), ACE05-Ent (Walker et al.,
2006) and GENIA (Kim et al., 2003) on nested
NER. As for relation extraction, we use ACE05-
R (Walker et al., 2006) and SciERC (Luan et al.,
2018). Since Wang et al. (2021) and Ye et al.
(2022) consider symmetric relations, which shall
massively influence the performance, we name this
scenario Symmetric RE with datasets ACE05-R
and SciERC. For event extraction, we follow
Lin et al. (2020) to perform experiments on three
datasets, ACE05-E, ACE05-E+ (Doddington et al.,
2004) and ERE-EN (Song et al., 2015). And for
joint IE, we test on ACE05-E+ and ERE-EN. Statis-
tics of all these datasets and detailed experimental
settings are described in Appendix C.
4.2 Results on Single IE tasks
In this section, we report the UTC-IE performance
in each single IE task. Results are shown in Table
1. The complete results for UTC-IE is shown in
Appendix D. The table shows that UTC-IE exceeds
previous SOTA models on all IE tasks. Particularly,
UTC-IE averagely improves the entity F1 of NER,
the entity F1 and relation F1 of RE, the entity F1
and the relation F1 of symmetric RE for +0.18,
+0.71, +1.07, +0.21, +0.94, respectively. And for
the EE tasks, UTC-IE increases the trigger F1, argu-
ment F1 for +0.35 and +1.67. We highlight that our
model is helpful for relational extraction, such as
relation extraction and argument extraction, which
proves the effectiveness of interaction between to-
ken pairs. Although the performance increment of
span extraction is not as significant as that of rela-
tional extraction, UTC-IE consistently improves on
various span extraction tasks.
Besides, we also test UTC-IE without Plus-
former. Surprisingly, this simple model surpasses
previous SOTA models on four results marked with, which proves the effectiveness of the task de-
composition. The comparison between models
with and without Plusformer clearly shows that
Plusformer is effective in all tested datasets, and
the performance improvement ranges from +0.40
(on OntoNotes) to +3.00 (on SciERC). Notably,4100
the average performance gain of adding Plusformer
on symmetric RE (+2.06) is more remarkable than
that on RE (+1.03). We presume this is because the
interaction between token pairs are more beneficial
for symmetric relations.
4.3 Results on Joint IE task
Multi-task learning has proven to be useful in the IE
area (Lin et al., 2020; Nguyen et al., 2021). Since
UTC-IE unifies all IE tasks into a token-pair clas-
sification scenario, it is natural to test whether one
UTC-IE model can benefit from jointly learning
all IE tasks. In Table 2, the performance of UTC-
IE is from the entity F1 of NER, relation F1
of RE, trigger F1 of EE and argument F1 of EE,
respectively. Based on the comparison between
UTC-IE and UTC-IE, it is obvious thatjointly learning these three tasks consistently im-
proves performance in the 2 joint IE datasets.
Moreover, UTC-IE outperforms previous
SOTA joint IE models in Table 2, the average per-
formance enhancement is +0.69 in ACE05-E+ and
+0.75 in ERE-EN. Specifically, UTC-IE in-
creases the average performance of relational ex-
traction by +1.30. Thusly, through unifying dif-
ferent IE tasks through our task decomposition,
Plusformer can enjoy the benefit of multi-tasking
learning, and achieve better performance than pre-
vious SOTA models.
4.4 Speed Comparison
To get a sense of the speed superiority of UTC-IE,
we compare the inference speed of UTC-IE with
previous unified models on ACE05 series datasets4101
and with task-specific SOTA models on every IE
tasks. The former comparison is presented in Table
3 and the latter locates in the Appendix E. Com-
pared with the generative UIE (Lu et al., 2022),
UTC-IE improves F1 from 1.73 (on ACE05-R) to
2.89 (on ACE05-E+), and obtains one order mag-
nitude of speed boost. Compared with OneIE (Lin
et al., 2020), UTC-IE fundamentally enhances the
performance for relational extractions (e.g., Rel.
and Arg.) with an average of 4.47 F1 increment in
joint IE. At the same time, UTC-IE is one order of
magnitude faster than OneIE. In a nutshell, com-
pared with previous SOTA models (whether task-
specific, unified or joint), UTC-IE achieves sub-
stantial performance gain across several datasets
with a significant speed boost.
4.5 Ablation Study
To analyze the effectiveness of each component
in Plusformer, we ablate each of them and list the
outcomes in Table 4, and results on more datasets
are presented in Appendix F. Besides, we study
how many Plusformer layers are suitable in Ap-
pendix F.5. Based on the ablation, CNN is the most
useful component among all IE tasks. The reason
behind this improvement is that once token pairs
are organized in the square feature map, the spatial
correlations between neighboring token pairs be-
come allusive, and CNN excels at exploiting these
local interactions. More comprehensive analysis
of CNN in Plusformer locates in Appendix F.1. To
deepen our understanding of UTC-IE, we try an-
other variant of Plusformer where the PlusAttention
is discarded, and we name this variant CNN-IE .
The bottom line of Table 4 shows that the CNN-IE
model can surpass or approach previous SOTA per-
formance in almost all datasets, which proves the
universality of our proposed task formulation.
However, CNN is not a panacea for UTC-IE.
From Table 4, removing position embeddings or
axis-awarenessfrom UTC-IE will lead to an av-
erage of 0.39 or 0.44 performance degradation, re-
spectively. Moreover, based on the performance
of CNN-IE and UTC-IE, the average performance
shrinks from 74.53 to 74.17 if the PlusAttention
is deprived of Plusformer, which means the plus-
shaped self-attention is a desideratum. In addition,
we present some intuitive examples and deeper
analysis for position embeddings and axis-aware in
Appendix F.3 and F.4.
5 Related Work
Information extraction tasks, which consist of
named entity recognition, relation extraction, and
event extraction, have long been a fundamental and
well-researched task in the natural language pro-
cessing (NLP) field. Previous researches mainly
only focus on one or two tasks. Recently, build-
ing joint neural models of unified IE tasks has at-
tracted increasing attention. Some of them incorpo-
rate graphs into IE structure. Wadden et al. (2019)
propose a unified framework called DYGIE++ to4102
extract entities, relations and events by leverag-
ing span representations via span graph updates.
Lin et al. (2020) and Nguyen et al. (2021) extend
DYGIE++ by incorporating global features to ex-
tract cross-task and cross-instance interactions with
multi-task learning. In addition to the graph-based
models mentioned above, other studies focus on
tackling general IE by generative models. Paolini
et al. (2021) construct a framework called TANL,
which enhances the generation model using aug-
mented language methods. Moreover, Lu et al.
(2022) regard IE task as a text-to-structure genera-
tion task, and leveraging prompt mechanism.
We unify all IE tasks as several token-pair clas-
sification tasks, which are fundamentally similar to
the span-based methods on the IE task, for the start
and end tokens can locate a span. Numerous NER
studies emerge on span-based models, which are
compatible with both flat and nested entities and
perform well (Eberts and Ulges, 2020; Yu et al.,
2020b; Li et al., 2021; Zhu and Li, 2022). In ad-
dition to entities, the span-based method is also
used in RE. Some models (Wang et al., 2021; Ye
et al., 2022) only leverage span representations to
locate entities and simply calculate the interaction
between entity pair, while others (Wang et al., 2020;
Zhong and Chen, 2021) encode span pair informa-
tion explicitly to extract relations. With regard to
event extraction, as far as we know, there is little
work on injecting span information into EE explic-
itly. Wadden et al. (2019) leverage span representa-
tions on general IE, but their model is complicated
and only considers span at the embedding layer
without further interaction. Conceptually, Jiang
et al. (2020)’s work is similar to ours, but they
need a two-stage model to determine the span type
and span relations, respectively. Detailed analysis
are depicted in Appendix G. Although many span-
based IE models exist, they are task-specific and
lack interaction between token pairs. DecomposingIE tasks as token-pair classification and conduct-
ing interaction between token pairs can uniformly
model span-related knowledge and advance SOTA
performance.
The most novel component of Plusformer is the
plus-shaped attention mechanism, which can make
token pairs interact with each other in an efficient
way. A similar structure called Axial Transform-
ers (Ho et al., 2019) is proposed in the Computer
Vision (CV) field, which is designed to deal with
data organized as high-dimension tensors. Tan et al.
(2022) incorporate axial attention into relation clas-
sification to improve the performance on two-hop
relation. However, CNN was not used in these
works, while CNN has been proven to be vital to
the IE tasks. Another similar structure named Twin
Transformer (Guo et al., 2021) used in CV , where
they encode pixels of image from row and column
sequentially, and leverage CNN on top of them.
But the position embeddings, which are important
for IE tasks, are not used in the Twin Transformer.
Besides, we want to point out that the usage of
plus-shaped attention and CNN originates from the
reformulation of IE tasks, any other modules which
can directly enable interaction between constituent
spans of a relation and between adjacent token pairs
should be beneficial.
6 Conclusion
In this paper, we decompose NER, RE and EE tasks
into token-pair classifications. Through the decom-
position, we unify all IE tasks under the same for-
mulation. After scrutinizing the token-pair feature
matrix, we find the adjacent and plus-shaped inter-
actions between token pairs should be informative.
Therefore, we propose Plusformer, which uses an
axis-aware plus-shaped self-attention followed by
CNN layers to help token pairs interact with each
other. Experiments on 10 single IE datasets and
2 joint IE datasets all outperform or approach the4103SOTA performance. Besides, owing to the par-
allelism of self-attention and CNN, our model’s
inference speed is substantially faster than previ-
ous SOTA models in RE and EE. Lastly, most of
the previous IE models limit the interaction in the
1-D sequential dimension, while the reformulation
of IE tasks opens a new angle to broaden the com-
munication to the 2-D feature matrix.
Limitations
While we unify diverse IE tasks into token-pair
classification tasks and propose a simple but use-
ful architecture to help token pairs interact with
each other in an effective way, there are still sev-
eral limitations that are worth discussing. Firstly,
all modules in our UTC-IE are based on pre-trained
language models, and experiments proves that dif-
ferent PLMs may influence the performance on
the same dataset. Hence, our model relies on the
capability of the PLM, which need a lot of GPU
resources to complete the experiments. Addition-
ally, although incorporating PlusAttention instead
of self-attention can effectively reduce the mem-
ory and computational complexity from O(L)to
O(2L), it still require a little large computation.
Future work can leverage the backbone of our uni-
fication and model, and focus on the acceleration
on each module.
Acknowledgements
We would like to thank the anonymous reviewers
for their insightful comments. We also thank the
developers of fastNLPand fitlog. Thank Yuntao
Chen for helping us preparing the code for publish-
ing. This work was supported by the National Nat-
ural Science Foundation of China (No. 62236004
and No. 62022027) and CCF-Baidu Open Fund.
References410441054106
A Discussion on Task decomposition
In this section, we will discuss two issues of the
decomposition. The first is the inconsistency stipu-
lation about the relation decomposition, the second
is the false positive issue when decoding relations.
A.1 The inconsistency
As our stipulation in Section 2, if a span (s, e)
has an expected span type t, both the Yand
Yare1. If two spans (s, e)and(s, e)
have relation r, this means the relation should also
exist between spans (s, e)and(e, s)(the end-
to-start version of the span (s, e)), then based
on our stipulation on the relation, the Y
andYshould also equal 1, but we only
define the Y= 1 andY= 1, this
causes an inconsistency between the stipulations.
We ignore YandYto make the
decoding less cluttered.
A.2 False Positive Relation
A potential risk of the decomposition and decoding
is that it may cause false positive relations. Given
four spans p= (s, e), p= (s, e), p=
(s, e), p= (s, s), ifphas relation rwith
pandp, and no relation exist between pand
p, then Y=Y= 1,Y=
Y= 1. However, if s=s,e=e.
Namely, pshares start token with pandpshares
end token with p. Then, based on Y=
Y= 1, we get Y=Y= 1„
the decoding process will mistakenly think phas
relation rwithp. However, this situation should
be rare, and none is found in the tested datasets.
B Decoding with model’s predictions
In this section, we will detail the decoding process
for models’ predictions. The process described in
Section 2.2 is not directly applicable to models’ pre-
dictions since spans may conflict with each other.4107With prediction score matrix ˆYfrom Eq.(6), we
follow previous work (Yu et al., 2020b) to first fil-
ter out spans whose scores are less than 0.5; for
the remaining spans, we sort the spans based on
their scores, then choose spans in descending or-
der and make sure the span has no boundary clash
with chosen spans. For relational extraction, we
first decode all spans, then we get a binary ma-
trix¯Y=ˆY>ˆY, then we
pair spans to check whether they form relations.
Take two spans (s, e)and(s, e)for instance, if
¯Y=¯Y= 1, we claim the first
span has relation rwith the second span. For the
RE task, we pair all entity spans to check if they
form relations; for the EE task, we pair the trigger
spans and argument spans to check if they form
a role relationship; and for the joint IE task, we
pair entity spans to check if they form relations, we
pair the trigger spans and entity spans (because all
argument spans are entity spans) to check if they
form a role relationship.
C Experimental Settings
In this section, we describe all experimental set-
tings in detail, such as the statistics of datasets,
baseline models, and more implementation details.
C.1 Datasets
We conduct experiments on 10 single IE datasets
and 2 joint IE datasets, and we detail the statistics
of all datasets in Table 5.
Named entity recognition. We perform ex-
periments on both flat and nested NER bench-
marks. In flat NER, we adopt CoNLL03 (Sang and
Meulder, 2003) and OntoNotes(Pradhan et al.,
2013) datasets. In nested NER, we experiment
on ACE04(Doddington et al., 2004), ACE05
(Walker et al., 2006) and GENIA (Kim et al., 2003).
To distinguish ACE05 dataset used in other tasks,
we name ACE05 in named entity recognition as
ACE05-Ent. Specifically, we use the same prepro-
cessing and splitting procedure on nested datasets
as Yan et al. (2022), for they fix some annota-
tion problems to unify different versions of these
datasets and make a strictly fair comparison.
Relation extraction. We conduct experiments
on two relation extraction datasets, ACE05 (Walker
et al., 2006) and SciERC(Luan et al., 2018). TheACE05 dataset, named as ACE05-R in our paper, is
collected from various domains, such as newswire
and online forums. The SciERC dataset provides
entity, coreference and relation annotations from
AI conference/workshop proceedings. In our exper-
iments, we only use entity and relation annotations.
We follow the data preprocessing in Luan et al.
(2019) to split ACE05-R and SciERC into train,
dev and test sets.
In typical RE, it is crucial to distinguish which
entity comes first (head entity) and which comes
next (tail entity). As for symmetric relational in-
stance, the relation exists from both head-to-tail
and tail-to-head directions. There are one such rela-
tion type in ACE05-R and two in SciERC. Some pa-
pers (Wang et al., 2021; Ye et al., 2022) regard each
symmetric relational example as two directed rela-
tions, while others regard them as one relation. We
find that this setting will hugely influence the per-
formance. Therefore, we name the setting of hav-
ing two directed relations as Symmetric Relation
Extraction and name the corresponding datasets
ACE05-Rand SciERC.
Event extraction. We evaluate UTC-IE on two
widely used event extraction datasets, ACE2005
(Doddington et al., 2004) and ERE (Song et al.,
2015). Following the prior preprocessing step
(Wadden et al., 2019; Lin et al., 2020; Lu et al.,
2021) on them, we obtain three datasets, ACE05-E,
ACE05-E+ and ERE-EN. ACE05-E+ additionally
takes relation arguments, pronouns and multi-token
event triggers into consideration compared with
ACE05-E. We use the same train/dev/test split as
Lu et al. (2021) for all datasets to ensure a fair com-
parison. Furthermore, we still use ACE05-E+ and
ERE-EN on joint IE, for they have annotations on
all IE tasks.
C.2 Baselines
TANL (Paolini et al., 2021) and UIE (Lu et al.,
2022) are both unified information extraction mod-
els in the generative way, with different input and
output formats. TANL uses T5-base as the back-
bone model, while UIE uses T5-large. We com-
pare our model with them in every IE task. For
TANL, we report single-task results for our model
is trained under each task. For UIE, we report
results with pre-training, which have better perfor-
mance. In addition to these two baselines, each task
also compares with a series of recently proposed
task-specific methods as follows.4108
CNN-IE is the baseline model we design to
prove the necessity of PlusAttention. The only
difference between CNN-IE and UTC-IE is the for-
mer ignores the PlusAttention in Figure 2. We tune
the number of CNN layers in CNN-IE from 2 to 6,
and the best results are reported.
Named entity recognition. We compare our
model’s performance on NER with several recently
proposed NER methods.
•BART-NER (Yan et al., 2021a) formulates
unified NER model as entity span sequence
generation task. They use BART-large as the
pre-trained model.
•WNER (Li et al., 2022) formulates unified
NER model as word-to-word classification
task. The model employs BioBERT on GE-
NIA and BERT-large on other datasets.
•BS(Zhu and Li, 2022): authors use span-
based NER model as baseline and propose
boundary smoothing as a regularization tech-
nique to improve model performance. It lever-
ages RoBERTa-base as the base encoder.
•CNN-NER (Yan et al., 2022) utilizes CNN
to model local spatial correlations between
spans and surpass recently proposed meth-
ods on nested NER. We report results using
RoBERTa-base model.
Relation extraction. For relation extraction, we
compare our model with several SOTA models.•UniRE (Wang et al., 2021) jointly extracts
entities and relations using a table containing
all word pairs.
•PURE (Zhong and Chen, 2021) adopts a
pipeline approach to solve NER and RE inde-
pendently, using distinct contextual represen-
tations for entities and relations.
•PFN (Yan et al., 2021b) claims that some in-
formation should be shared between named
entity recognition and relation extraction,
while other information should be indepen-
dent. They propose PFN to model two-way
interaction (partition and filter) between two
tasks.
•PL-Marker (Ye et al., 2022): authors con-
sider interactions between spans and propose
PL-Marker by strategically packing the mark-
ers in the encoder.
Previous models mentioned above use different RE
datasets. Specifically, UniRE and PL-Marker re-
gard symmetric relations as two directed relations,
while other work does not. Besides, these two mod-
els utilize cross-sentence context.
Event extraction. Generative methods are pop-
ular in recently proposed event extraction papers.
•TEXT2EVENT (Lu et al., 2021) is a
sequence-to-structure model which outputs
a tree-like event structure with a given input
sentence. The model uses T5-large as the base
model.4109
•DEGREE (Hsu et al., 2022) leverages man-
ually designed prompts to generate event
records in natural language. We report the
end-to-end performance of DEGREE instead
of the pipeline way. The model leverages
BART-large as encoder-decoder.
Joint IE. There are only two previous models
that consider the joint IE in ACE05-E and ERE-EN
datasets.
•OneIE (Lin et al., 2020) proposes an end-to-
end IE model, which employs global features
and type dependency constraint at decoding
step.
•FourIE (Nguyen et al., 2021) further im-
proves the model by incorporating interaction
dependency on representation level and label
level.For a fair comparison, we list the pre-trained
model used for all baselines and our model on
every IE dataset in Table 6. When choosing our
pre-trained language model in different IE tasks’
datasets, we pick the same pre-trained model as the
most recently published papers, such as BioBERT
for GENIA and RoBERTa-base for other NER
datasets. For RE and joint IE tasks, we choose
the same pre-trained model as previous work. For
tasks where previous work applied a generative pre-
trained model, we choose pre-trained model that
has a similar size. For example, in event extraction,
we use DeBERTa-large, whose number of parame-
ters is 390M, which is closest to BART-large and
T5-large used by previous EE papers.
C.3 Evaluation Metrics
We report micro-F1 on all tasks:
•Entity: an entity is correct if its entity type4110
and span offsets match a golden entity. We
use “Ent.” to represent entity F1 through all
tables.
•Relation: a relation is correct if its type and
its head and tail entities are correct, and the
offsets and type of entities should also match
the golden instance. We use “Rel.” to repre-
sent relation F1 through all tables.
•Event trigger: a trigger is correct if its span
offset and event type is correct. We use “Trig.”
to represent trigger F1 through all tables.
•Event argument: an argument is correct if its
span offset, event type and role type all match
the ground truth. We use “Arg.” to represent
argument F1 through all tables.
C.4 Hyper-Parameters
The detailed hyper-parameters used in each dataset
are listed in Table 7. We use AdamW opti-
mizer (Loshchilov and Hutter, 2019) with weightdecay 1e-2 for all datasets. Experiments are con-
ducted five times with five different random seeds.
We report the performance on test sets based on the
model which achieves the best dev results in each
dataset. For NER, the best results are calculated by
the entity F1; for RE, the best results are calculated
by the sum of entity F1 and relation F1; for EE, the
best results are calculated by the best argument F1;
for joint IE, the best results are calculated by the
sum of relation F1 and trigger F1.
D Complete Results
We present the complete results of UTC-IE and that
without Plusformer in Table 8.
E Speed Comparison
We test the speed of other models through their
released code. For models, such as OneIE (Lin
et al., 2020), DEGREE (Hsu et al., 2022) and PL-4111
Marker(Ye et al., 2022), they also released a
trained model along with their code, and we used
their released model to test the inference speed. For
UIE (Lu et al., 2022) and BS (Zhu and Li, 2022),
we trained a model with their code. The speed test
is conducted in one RTX 3090 GPU and the batch
size is set as 32 for all models (if the model goes
out of memory, we choose the largest batch size
that can accommodate the GPU); the test corpus is
the test set of each dataset. The speed is measured
by the number of sentences in the test set divided
by the number of seconds that elapsed. And each
inference is repeated three times, the average speed
is reported.
The speed comparison can be roughly catego-
rized into two kinds. The first kind is the compar-
ison with previous universal IE models, namely
OneIE (Lin et al., 2020) and UIE (Lu et al., 2022),
and results are depicted in Table 3. Compared with
UIE in five chosen datasets, UTC-IE is x19.7 faster
and improves performance by 1.86 averagely. Be-
sides, for the joint IE task, UTC-IE is 18.4 times
faster than OneIE and improves performance by
2.72 on average. The second kind is the compari-
son between UTC-IE and SOTA models targetedfor each IE task, and results are presented in Table
9. Compared with previous SOTA models, the aver-
age performance increments for entity F1, relation
F1 and argument F1 are 0.31, 0.94 and 2.15. In the
meantime, UTC-IE speeds up for x1.0, x5.5 and
x101.9 averagely.
In short, using UTC-IE for IE tasks can not
only substantially enhance the performance in most
cases, but also significantly speed up the inference
speed in almost all datasets.
F Ablation Study
For ablation, we will choose two datasets for each
IE task to study the effect of each component in
Plusformer. We separately list the performance for
span extraction (including entity extraction in NER
and RE, trigger extraction in EE) in Table 10 and
relational extraction (including relation extraction
in RE and argument extraction in EE) in Table 11.
Besides, we also study how the performance varies
with the change of the number of Plusformer layers
in Figure 10.
F.1 CNN
Based on our ablations in Table 10 and Table 11,
the CNN module in Plusformer contributes most to
the performance enhancement. To reveal why CNN
is so effective in both span extraction and relational4112
extraction, we first present an intuitive example in
Figure 3 to show how CNN helps to extract entities
and relations in the RE task. Like in Figure 3(a), for
NER, the entity ecan interact with entity eand
relation rthrough CNN. Besides, for RE, CNN
can contribute in two ways. On the one hand, CNN
helps the relational token pair to directly gather
information from its constituent entities, like the r
in Figure 3(b). On the other hand, the start-to-start
and end-to-end relational token pairs, like two r
cells, can directly interact with each other through
CNN.
To quantitatively present the effectiveness of
CNN in UTC-IE, we propose further ablations to
show how the distance between the relational token
pair and its constituent spans affects the relational
F1, and how the distance between start-to-start
and end-to-end token pairs affects the relational
F1. Furthermore, we conduct experiments on UTC-
IE with different kernel sizes and choose the most
proper size.
F.1.1 Distance Between the Relational Token
Pair and Its Constituent spans VS.
Relational F1
In this section, we will show how the relational F1
(relation F1 in RE and argument F1 in EE) will
change when the distance between the relational
token pair and its constituent spans varies. For two
spans (s, e)and(s, e)(we ignore their diago-
nally symmetric counterparts, since they will not
affect the calculation here), the span relation from
(s, e)to(s, e)is represented by two token pairs
(s, s)and(e, e). The distance between the two
token pairs and its constituent spans is calculated
as follows
d= max( |s−e|,|s−e|) + 1, (9)where the distance dis named as “ Span-Rel-Span
Distance ”, it represents the longest distance be-
tween the relational token pairs to their constituent
spans. The relation between dand the relational
F1 is shown in Figure 4. Without CNN, the per-
formance for extracting relations between nearby
constituent spans will drop sightly, while less af-
fected for further ones, which proves that CNN is
effective for exploiting local dependency to predict
relations.
F.1.2 Distance Between Start-to-Start and
End-to-End Token Pairs VS. Relational
F1
As shown in Figure 3(b), if the distance between the
start-to-start and end-to-end relational token pairs
is small, the CNN should be helpful. To verify this
assumption, we first define the “ Inner Relational
Distance ” as follows, for two spans (s, e)and
(s, e), the relational token pairs are (s, s)and
(e, e), then the distance between two relational
token pairs is calculated as follows
d= max( e−s, e−s) + 1, (10)
where dreveals the distance between start-to-start
and end-to-end token pairs, and it is actually de-
cided by the max constituent span length. And its
relation with the relational F1 is shown in Figure 5.
It is clear that, most of the start-to-start token pairs
are near to their end-to-end token pairs, and CNN
takes advantage of this adjacency to make better
predictions.
F.1.3 CNN Kernel Size VS. F1
We study the relation between the kernel size of
CNN and F1 performance in Figure 6. We observe
that CNN with kernel size 3 obtains the best perfor-
mance on almost all datasets and tasks. Specifically,4113
reducing CNN kernel size to 1 significantly harms
the performance on all datasets, for CNN will lose
the capability of interacting with neighboring token
pairs. In contrast, F1 also slightly decreases with
larger CNN kernel size. We presume that CNN
with a larger kernel size may introduce more noise
and harm performance. Therefore, we choose ker-
nel size 3 for all datasets.
F.2 Is CNN All We Need?
Since CNN is so effective in the Plusformer, it is
natural to ask whether it is enough only to use CNN.
Therefore, we conduct experiments on models with-
out the plus-shaped self-attention and named this
model CNN-IE. We conduct experiments for CNN-
IE in six datasets, and results are listed in Table
10 and Table 11. With only the CNN module, the
model can achieve SOTA or near SOTA perfor-
mance in all six datasets, which depicts the effec-
tiveness of the proposed token-pair decomposition
and CNN module. However, it still lags behind the
UTC-IE model, which reveals the necessity of the
PlusAttention.It is worth noting that CNN-IE is different from
CNN-NER (Yan et al., 2022). As for model
structures, CNN-IE is one of our baseline models
and reserves the general framework of Plusformer,
namely self-attention with CNN layers. However,
CNN-NER only uses residual CNN layers. As we
can see in Table 1, CNN-IE has different results
on ACE05-Ent than CNN-NER does. Besides, as
for tasks, CNN-NER only formulates the nested
NER task and can not transfer to other IE tasks
directly. However, our CNN-IE can easily apply
to NER, RE and EE. In each IE task on CNN-IE,
CNN modules have their specific functions to cap-
ture different neighboring token pairs, as depicted
in Figure 3.
F.3 Position Embeddings
The RoPE embedding aims to help token pairs be
aware of the spatial relationships between each
other, and the triangle position embedding tries to
enable spans to be informed of their areas in the fea-
ture map. From Table 10 and Table 11, the position
embeddings enhance the span extraction and rela-4114
tional extraction for 0.39, 0.64 averagely. Besides,
in Figure 7, we show that the position embeddings
can help the model exploit the distance bias to im-
prove the performance of relational extraction.
F.4 Axis-aware Plus-shaped Self-Attention
Lastly, we study the effect of PlusAttention. We
present an example to delineate why the axis-aware
is valuable for span extraction and relational extrac-
tion in Figure 8. From Figure 8, axis-aware should
be worthwhile no matter what the task is, span ex-
traction or relational extraction. As expected, from
Table 10 and Table 11, if we discard the axis-aware
in Plusformer, the average performance of span ex-
traction and relational extraction diminish 0.28 and
0.86, respectively, which reveals the necessity of
axis-aware in the PlusAttention module.
Besides, we show two case studies of the plus-
shaped attention in Figure 9. The sentences
are from the test dataset of ACE2005-Ent and
ACE2005-R. Both cases put larger attention scores
on informative token pairs.F.5 Number of Plusformer Vs. F1
We study the relation between the number of Plus-
former layers and F1 performance in Figure 10. For
the NER datasets, we use two layers of Plusformer,
and for the RE and EE we use three.
G Comparison with GLAD
Jiang et al. (2020) claims that many NLP tasks
can be regarded as the span prediction and predic-
tion of relations between pairs of spans (named as
span extraction and relational extraction in our pa-
per), which is conceptually similar to our insights.
However, our work is fundamentally distinct from
theirs on both formulation and model architecture.
Jiang et al. (2020) classify various NLP tasks into
two separate tasks and design different modules for
them. To contrast, we unify all traditional IE tasks
into a single formulation, namely token-pair classi-
fication. Therefore, we only need one model for all
tasks. Besides, Jiang et al. (2020) simply use the
concatenation of the start and end token represen-
tations to represent a span, and for relations, they
concatenate the head and tail span representations.
Therefore, in their work, the interaction between4115
spans are weak. In our work, we obtain the feature
matrix of all token pairs and add well-designed
Plusformer module on top of all token pairs, where
token pairs can interact with others thoroughly.
In order to prove the superiority of our reformu-
lation and UTC-IE model, we make a fair com-
parison on several tasks from the GLAD bench-
mark (Jiang et al., 2020). We choose 3 additional
IE tasks, including Open Information Extraction
(OIE), Semantic Role Labeling (SRL) and Aspect
Based Sentiment Analysis (ABSA), and NER and
RE. We use WLP (Hashimoto et al., 2017) on NER
and RE, OIE2016 (Stanovsky and Dagan, 2016) on
OIE, OntoNotes (Pradhan et al., 2013) on SRL and
SemEval14 (Pontiki et al., 2014) on ABSA. The
detailed experimental settings are the same as those
in GLAD, to ensure a fair comparison. Results are
present in Table 12.
The table shows that UTC-IE outperforms
GLAD on all chosen tasks exceedingly, with +2.76
improvement on average. Moreover, we observe
that UTC-IE without Plusformer also surpasses
GLAD benchmarks on all tasks with +0.84 im-
provement on average, which proves the superiority
of our unified reformulation.41164117411841194120ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section "Limitations" (7th section)
/squareA2. Did you discuss any potential risks of your work?
Section "Limitations" (7th section)
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
"Abstract" and section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix C4121/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix C and section 4
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Appendix C and Section 4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix C
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.4122