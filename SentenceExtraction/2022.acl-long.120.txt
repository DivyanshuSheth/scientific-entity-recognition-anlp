
Pratyusha Sharma Antonio Torralba Jacob Andreas
Massachusetts Institute of Technology
{pratyuss,torralba,jda}@mit.edu
Abstract
We present a framework for learning hierarchi-
cal policies from demonstrations, using sparse
natural language annotations to guide the
discovery of reusable skills for autonomous
decision-making. We formulate a generative
model of action sequences in which goals
generate sequences of high-level subtask de-
scriptions, and these descriptions generate se-
quences of low-level actions. We describe
how to train this model using primarily unan-
notated demonstrations by parsing demonstra-
tions into sequences of named high-level sub-
tasks, using only a small number of seed anno-
tations to ground language in action. In trained
models, natural language commands index a
combinatorial library of skills; agents can use
these skills to plan by generating high-level
instruction sequences tailored to novel goals.
We evaluate this approach in the ALFRED
household simulation environment, providing
natural language annotations for only 10% of
demonstrations. It achieves task completion
rates comparable to state-of-the-art models
(outperforming several recent methods with
access to ground-truth plans during training
and evaluation) while providing structured and
human-readable high-level plans.
1 Introduction
Building autonomous agents that integrate high-
level reasoning with low-level perception and con-
trol is a long-standing challenge in artiﬁcial intelli-
gence (Fikes et al., 1972; Newell, 1973; Sacerdoti,
1973; Brockett, 1993). Fig. 1 shows an example: to
accomplish a task such as cooking an egg , an agent
must ﬁrst ﬁnd the egg , then grasp it , then locate a
stove or microwave , at each step reasoning about
both these subtasks and complex, unstructured sen-
sor data. Hierarchical planning models (e.g. Sut-
ton et al., 1999)—which ﬁrst reason about abstractFigure 1: Hierarchical imitation learning using weak
natural language supervision. During training, a small
number of seed annotations are used to automatically
segment and label unannotated training demonstrations
with natural language descriptions of their high-level
structure. When deployed on new tasks, learned poli-
cies ﬁrst generate sequences of natural language sub-
task descriptions, then modularly translate each de-
scription to a sequence of low-level actions.
states and actions, then ground these in concrete
control decisions—play a key role in most existing
agent architectures. But training effective hierarchi-
cal models for general environments and goals re-
mains difﬁcult. Standard techniques either require
detailed formal task speciﬁcations, limiting their
applicability in complex and hard-to-formalize en-
vironments, or are restricted to extremely simple
high-level actions, limiting their expressive power
(Bacon et al., 2017; Sutton et al., 1999; Dietterich,
1999; Kaelbling and Lozano-Pérez, 2011).
Several recent papers have proposed to overcome
these limitations using richer forms of supervision—
especially language—as a scaffold for hierarchi-
cal policy learning. In latent language policies
(LLPs; Andreas et al., 2018), controllers ﬁrst map1713from high-level goals to sequences of natural lan-
guage instructions, then use instruction following
models to translate those instructions into actions.
But applications of language-based supervision for
long-horizon policy learning have remained quite
limited in scope. Current LLP training approaches
treat language as a latent variable only during pre-
diction, and require fully supervised (and often
impractically large) datasets that align goal spec-
iﬁcations with instructions and instructions with
low-level actions. As a result, all existing work
on language-based policy learning has focused on
very short time horizons (Andreas et al., 2018),
restricted language (Hu et al., 2019; Jacob et al.,
2021) or synthetic training data (Shu et al., 2018;
Jiang et al., 2019).
In this paper, we show that it is possible to train
language-based hierarchical policies that outper-
form state-of-the-art baselines using only minimal
natural language supervision. We introduce a pro-
cedure for weakly andpartially supervised training
of LLPs using ungrounded text corpora, unlabeled
demonstrations, and a small set of annotations link-
ing the two. To do so, we model training demon-
strations as generated by latent high-level plans: we
describe a deep, structured latent variable model
in which goals generate subtask descriptions and
subtask descriptions generate actions. We show
how to learn in this model by performing inference
in the inﬁnite, combinatorial space of latent plans
while using a comparatively small set of annotated
demonstrations to seed the learning process.
Using an extremely reduced version of the AL-
FRED household robotics dataset (Shridhar et al.,
2020)—with 10% of labeled training instructions,
no alignments during training, and no instructions
at all during evaluation—our approach performs
comparably a state-of-the-art model that makes
much stronger dataset-speciﬁc assumptions (Blukis
et al., 2021), while outperforming several models
(Zhang and Chai, 2021; Suglia et al., 2021; Kim
et al., 2021) that use more information during both
training and evaluation . Our method correctly seg-
ments and labels subtasks in unlabeled demonstra-
tions, including subtasks that involve novel compo-
sitions of actions and objects. Additional experi-
ments show that pretraining on large (ungrounded)
text corpora (Raffel et al., 2020) contributes to this
success, demonstrating one mechanism by which
background knowledge encoded in language can
beneﬁt tasks that do not involve language as aninput or an output.
Indeed, our results show that relatively little in-
formation about language grounding is needed for
effective learning of language-based policies—a
rich model of natural language text, a large number
of demonstrations, and a small number of annota-
tions sufﬁce for learning compositional libraries of
skills and effective policies for deploying them.
2 Preliminaries
We consider learning problems in which agents
must perform multi-step tasks (like cooking an egg ;
Fig. 1) in interactive environments. We formalize
these problems as undiscounted, episodic, partially
observed Markov decision processes (POMDPs)
deﬁned by a tuple (S;A;T;
;O), whereSis a set
ofstates ,Ais a set of actions ,T:SA!S is
an (unknown) state transition function ,
is a set
ofobservations , andO:S! 
is an (unknown)
observation function .We assume that observa-
tions include a distinguished goal speciﬁcation g
that remains constant throughout an episode; given
a datasetDof consisting of goalsganddemon-
strations d(i.e.D=f(d;g);(d;g):::g;d=
[(o;a);(o;a);:::];o2
;a2 A ), we
aim to learn a goal-conditional policy (aj
a;o;g) =(aja;:::;a;o;:::;o;g)
that generalizes demonstrated behaviors to novel
goals and states.
For tasks like the ones depicted in Fig. 1, this
learning problem requires agents to accomplish
multiple subgoals (like ﬁnding an egg oroper-
ating an appliance ) in a feasible sequence. As
in past work, we address this challenge by fo-
cusing on hierarchical policy representations that
plan over temporal abstractions of low-level ac-
tion sequences. We consider a generic class of
hierarchical policies that ﬁrst predict a sequence
ofsubtask speciﬁcations from a distribution
(j;g)(the controller ), then from each
generate a sequence of actions a:::afrom a
distribution (aja;o;)(theexecutor ).
At each timestep, may either generate an action
fromA; or a special termination signal ; af-
ter is selected, control is returned to and
a newis generated. This process is visualized1714
in Fig. 2(a). Trajectories generated by hierarchi-
cal policies themselves have hierarchical structure:
each subtask speciﬁcation generates a segment
of a trajectory (delimited by a action) that
accomplishes a speciﬁc subgoal.
Training a hierarchical policy requires ﬁrst deﬁn-
ing a space of subtask speciﬁcations , then param-
eterizing controller and executor policies that can
generate these speciﬁcations appropriately. Most
past research has either pre-deﬁned an inventory of
target skills and independently supervised and
(Sutton et al., 1999; Kulkarni et al., 2016; Dayan
and Hinton, 1992); or performed unsupervised dis-
covery of a ﬁnite skill inventory using clustering
techniques (Dietterich, 1999; Fox et al., 2017).
Both methods have limitations, and recent work
has explored methods for using richer supervision
to guide discovery of skills that are more robust
than human-speciﬁed ones and more generalizable
than automatically discovered ones. One frequently
proposed source of supervision is language: in la-
tent language policies ,is trained to generate
goal-relevant instructions in natural language, is
trained to follow instructions, and the space of ab-
stract actions available for planning is in principle
as structured and expressive as language itself. But
current approaches to LLP training remain imprac-
tical, requiring large datasets of independent, ﬁne-
grained supervision for and. Below, we de-
scribe how to overcome this limitation, and instead
learn from large collections of unlabeled demon-
strations augmented with only a small amount of
natural language supervision.3 Approach
Overview We train hierarchical policies on unan-
notated action sequences by inferring latent natural
language descriptions of the subtasks they accom-
plish (Fig. 2(b)). We present a learning algorithm
that jointly partitions these action sequences into
smaller segments exhibiting reusable, task-general
skills, labels each segment with a description, trains
to generate subtask descriptions from goals, and
to generate actions from subtask descriptions.
Formally, we assume access to two kinds of
training data: a large collection of unannotated
demonstrationsD=f(d;g);(d;g);:::g
and a smaller collection of annotated demon-
strationsD=f(d;g;);(d;g;);:::g
where each consists of a sequence of natural
language instructions [;;:::]corresponding
to the subtask sequence that should be generated
by. We assume that even annotated trajectories
leave much of the structure depicted in Fig. 2(a)
unspeciﬁed, containing no explicit segmentations
or markers. (The number of instructions jj
will in general be smaller than the number of ac-
tionsjdj.) Trainingrequires inferring the cor-
respondence between actions and annotations on
Dwhile inferring annotations themselves on D.
Training objective To begin, it will be conve-
nient to have an explicit expression for the probabil-
ity of a demonstration given a policy (;). To
do so, we ﬁrst observe that the hierarchical genera-
tion procedure depicted in Fig. 2(a) produces a la-
tentalignment between each action and the subtask1715that generated it. We denote these alignments ,
writing=jto indicate that awas generated
from. Becauseexecutes subtasks in sequence,
alignments are monotonic , satisfying=or
=+ 1. Let seg()denote the segmenta-
tion associated with , the sequence of sequences
of action indices [[i:= 1];[i:= 2];:::]
aligned to the same instruction (see Fig. 2(a)).
Then, for a ﬁxed policy and POMDP, we may write
the joint probability of a demonstration, goal, an-
notation, and alignment as:
p(d;g;;)/Y
(j;g)
Y(aja;o;)
(ja;o)
: (1)
Heres(in a slight abuse of notation) denotes all
segments preceding s, and sis the index of the
ith action in s. The constant of proportionality in
Eq. (1) depends only on terms involving T(sj
s;a),O(ojs)andp(g), all independent of or
; Eq. (1) thus describes the component of the
data likelihood under the agent’s control (Ziebart
et al., 2013).
With this deﬁnition, and given DandDas de-
ﬁned above, we may train a latent language policy
using partial natural language annotations via or-
dinary maximum likelihood estimation, imputing
the missing segmentations and labels in the train-
ing set jointly with the parameters of and
(which we denote ) in the combined annotated
and unannotated likelihoods:
arg maxL(^;^;^) +L(^;^) (2)
where
L(^;^;^) =Xlogp(d;g;^;^) (3)
L(^;^) =Xlogp(d;g;;^)(4)
and where we have suppressed the dependence
ofp(d;g;;)on^for clarity. This objective
involves continuous parameters ^, discrete align-
ments ^, and discrete labelings ^. We optimize it
via block coordinate ascent on each of these compo-
nents in turn: alternating between re- segmentingdemonstrations, re- labeling those without ground-
truth labels, and updating parameters . The full
learning algorithm, which we refer to as (SL)
(semi-supervised skill learning with latent lan-
guage ), is shown in Algorithm 1, with each step
of the optimization procedure described in more
detail below.
Segmentation: arg maxL(^;^;^)+L(^;^)
The segmentation step associates each low-level ac-
tion with a high-level subtask by ﬁnding the highest
scoring alignment sequence for each demonstra-
tion inDandD. While the number of possible
alignments for a single demonstration is exponen-
tial in demonstration length, the assumption that
depends only on the current subtask implies the
following recurrence relation:
maxp(d;g;;)
= max
maxp(d;g;;)
p(d;g;;=m)
(5)
This means that the highest-scoring segmentation
can be computed by an algorithm that recursively
identiﬁes the highest-scoring alignment to each pre-
ﬁx of the instruction sequence at each action (Al-
gorithm 2), a process requiring O(jdjjj)space
andO(jdjjj)time. The structure of this dy-
namic program is identical to the forward algorithm
for hidden semi-Markov models (HSMMs), which
are widely used in NLP for tasks like language
generation and word alignment (Wiseman et al.,
2018). Indeed, Algorithm 2 can be derived imme-
diately from Eq. (1) by interpreting p(d;g;;)
as the output distribution for an HSMM in which
emissions are actions, hidden states are alignments,
the emission distribution is and the transition
distribution is the deterministic distribution with
p(+ 1j) = 1 .
This segmentation procedure does not produce
meaningful subtask boundaries until an initial ex-
ecutor policy has been trained. Thus, during the
ﬁrst iteration of training, we estimate a segmenta-
tion by by ﬁtting a 3-state hidden Markov model to
training action sequences using the Baum–Welch
algorithm (Baum et al., 1970), and mark state tran-
sitions as segment boundaries. Details about the
initialization step may be found in Appendix B.1716Labeling: arg maxL(^;^;^)
Inference of latent, language-based plan descrip-
tions in unannotated demonstrations involves an
intractable search over string-valued . To ap-
proximate this search tractably, we used a learned,
amortized inference procedure (Wainwright and
Jordan, 2008; Hoffman et al., 2013; Kingma and
Welling, 2014) to impute descriptions given ﬁxed
segmentations. During each parameter update step
(described below), we train an inference model
q(ja;a;g)to approximate the posterior
distribution over descriptions for a given segment
given a goal, the segment’s actions, and the actions
from the subsequent segment.Then, during the
labeling step, we label complete demonstrations by
choosing the highest-scoring instruction for each
trajectory independently:
arg maxlogp(d;g;;)
h
arg maxq(ja;a;g)s2seg()i
(6)
Labeling is performed only for demonstrations in
D, leaving the labels for Dﬁxed during training.
Param update:
This is the simplest of the three update steps: given
ﬁxed instructions and alignments, and ,pa-
rameterized as neural networks, this objective is
differentiable end-to-end. In each iteration, we
train these to convergence (optimization details are
described in Section 4 and Appendix C). During
the parameter update step, we also ﬁt parameters
of the proposal model to maximize the likeli-
hoodPPlogq(^ja;o)with respect to
the current segmentations ^sand labels ^.
As goals, subtask indicators, and actions may
all be encoded as natural language strings, and
may be implemented as conditional language
models. As described below, we initialize both
policies with models pretrained on a large text
corpora.
4 Experimental Setup
Our experiments aim to answer two questions.
First, does the latent-language policy representa-
tion described in Section 3 improve downstream
performance on complex tasks? Second, how many
natural language annotations are needed to train1717an effective latent language policy given an initial
dataset of unannotated demonstrations?
Environment We investigate these questions in
the ALFRED environment of Shridhar et al. (2020).
ALFRED consists of a set of interactive simulated
households containing a total of 120 rooms, accom-
panied by a dataset of 8,055 expert task demonstra-
tions for an embodied agent annotated with 25,743
English-language instructions. Observations oare
bitmap images from a forward-facing camera, and
actionsaare drawn from a set of 12 low-level nav-
igation and manipulation primitives. Manipulation
actions (7 of the 12) additionally require predicting
a mask over the visual input to select an object for
interaction. See Shridhar et al. (2020) for details.
While the ALFRED environment is typically
used to evaluate instruction following models,
which map from detailed, step-by-step natural lan-
guage descriptions to action sequences (Shridhar
et al., 2020; Singh et al., 2020; Corona et al., 2021),
our experiments focus on an goal-only evaluation in
which agents are given goals (but not ﬁne-grained
instructions) at test time. Several previous studies
have also considered goal-only evaluation for AL-
FRED, but most use extremely ﬁne-grained super-
vision at training time , including full supervision
of symbolic plan representations and their align-
ments to demonstrations (Min et al., 2021; Zhang
and Chai, 2021), or derived sub-task segmentations
using ALFRED-speciﬁc rules (Blukis et al., 2021).
In contrast, our approach supports learning from
partial, language-based annotations without seg-
mentations or alignments, and this data condition
is the main focus of our evaluation.
Modeling details andare implemented
as sequence-to-sequence transformer networks
(Vaswani et al., 2017). , which maps from
text-based goal speciﬁcations to text-based instruc-
tion sequences, is initialized with a pre-trained
T5-small language model (Raffel et al., 2020). ,
which maps from (textual) instructions and (image-
based) observations to (textual) actions and (image-
based) object selection masks is also initialized
with T5-small ; to incorporate visual input, this
model ﬁrst embeds observations using a pretrained
ResNet18 model (He et al., 2016) and transforms
these linearly to the same dimensionality as the
word embedding layer. Details about the architec-
ture ofandmay be found in Appendix C.Model variants for exploration In ALFRED,
navigation in the goal-only condition requires ex-
ploration of the environment, but no exploration is
demonstrated in training data, and techniques other
than imitation learning are required for this speciﬁc
skill. To reﬂect this, we replace all annotations con-
taining detailed navigation instructions go to the
glass on the table to your left with generic ones ﬁnd
a glass . Examples and details of how navigation in-
structions are modiﬁed can be found in Appendix E
and Fig. 7. The ordinary (SL)model described
above is trained on these abstracted instructions.
A key advantage of (SL)is modularity: individ-
ual skills may be independently supervised or re-
implemented. To further improve (SL)’s naviga-
tion capabilities, we introduce two model variants
in which sub-task speciﬁcations beginning Find. . .
are executed by a either a planner with ground-truth
environment information or a specialized naviga-
tion module from the HLSM model (Blukis et al.,
2021) rather than . Outside of navigation, these
models preserve the architecture and training pro-
cedure of (SL), and are labeled (SL)+planner
and(SL)+HLSM in experiments below.
Baselines and comparisons We compare the
performance of (SL)to several baselines:
seq2seq : A standard (non-hierarchical) goal-
conditioned policy, trained on the (g;d)pairs in
D[Dto maximizePlog(ajo;g), with
parameterized similar to .
seq2seq2seq : A supervised hierarchical policy
with the same architectures for andas in
(SL), but withtrained to generate subtask se-
quences by maximizingPlog(jg)and
trained to maximizePlog(ajo;;g)
using onlyD. Becausemaps from complete
task sequences to complete low-level action se-
quences, training of this model involves no explicit
alignment or segmentation steps.
no-pretrain ,no-latent : Ablations of the
full (SL)model in which andare, respec-
tively, randomly initialized or updated only on
L(^;^)during the parameter update phase.
We additionally contextualize our approach by
comparing it to several state-of-the-art models
for the instruction following task in ALFRED: S+
(Shridhar et al., 2020), MOCA (Singh et al., 2020),
Modular (Corona et al., 2021), HiTUT (Zhang and
Chai, 2021), ABP(Kim et al., 2021), ET(Pashe-
vich et al., 2021), EmBERT (Suglia et al., 2021),
andFILM (Min et al., 2021). Like seq2seq , these1718are neural sequence-to-sequence models trained
to map instructions to actions; they incorporate
several standard modeling improvements from the
instruction following literature, including progress
monitoring (Ma et al., 2019) and pretrained object
recognizers (Singh et al., 2020). Many of these
models are trained with stronger supervision than
(SL), including instructions and alignments dur-
ing training, and ground truth instructions during
evaluation ; see Table 3 for details.
Evaluation Following Shridhar et al. (2020), Ta-
ble 1(a) computes the online ,subtask-level accu-
racy of each policy, and Table 1(b) computes the
end-to-end success rate of each policy. See the AL-
FRED paper for details of these evaluations. For
data-efﬁciency experiments involving a large num-
ber of policy variants (Table 2, Fig. 4), we instead
use an ofﬂine evaluation in which we measure the
fraction of subtasks in which a policy’s predicted
actions (ignoring object selection masks) exactly
match the ground truth action sequence.
5 Results
Table 1 compares (SL)with ﬂat and hierarchical
imitation learning baselines. The table includes
two versions of the model: a 100% model trained
with full instruction supervision ( jDj= 0,jDj=
21000 ) and a 10% model trained with only a small
fraction of labeled demonstrations ( jDj= 19000 ,
jDj= 2000 ).seq2seq andseq2seq2seq models
arealways trained with 100% of natural language
annotations. Results are shown in Table 1. We ﬁnd:
(SL)improves on ﬂat policies: In both the
10% and 100% conditions, it improves over the
subtask completion rate of the seq2seq (goals-to-
actions) model by 25%. When either planner- or
mapping-based navigation is used in conjunction
with (SL), it achieves end-to-end performance
comparable to the HLSM method, which relies
on similar supervision. Strikingly, it outperforms
several recent methods with access to even more
detailed information at training or evaluation time.
Language-based policies can be trained with
sparse natural language annotations: Perfor-
mance of (SL)trained with 10% and 100% natural
language annotations is similar (and in both cases
superior to seq2seq andseq2seq2seq trained on
100% of data). Appendix Fig. 4 shows more de-
tailed supervision curves. Ablation experiments in
Table 2 show that inference of latent training plans
is important for this result: with no inference of
latent instructions (i.e. training only on annotated
demonstrations), performance drops from 56% to
52%. Fig. 3 shows an example of the structure
inferred for an unannotated trajectory: the model
inserts reasonable segment boundaries and accu-
rately labels each step.
Language model pretraining improves auto-
mated decision-making. Ablation experiments in
Table 2 provide details. Language model pretrain-
ing ofand(onungrounded text) is crucial
for good performance in the low-data regime: with
10% of annotations, models trained from scratch
complete 49% of tasks (vs 56% for pretrained mod-
els). We attribute this result in part to the fact that
pretrained language models encode information
about the common-sense structure of plans, e.g. the
fact that slicing a tomato ﬁrst requires ﬁnding a
knife . Such models are well-positioned to adapt
to “planning” problems that require modeling re-
lations between natural language strings. These1719
experiments point to a potentially broad role for
pretrained language models in tasks that do not
involve language as an input or an output.
One especially interesting consequence of the
use of language-based skills is our model’s ability
to produce high-level plans for out-of-distribution
goals , featuring objects or actions that are not part
of the ALFRED dataset at all. Examples are pro-
vided in Fig. 5 and discussed in Appendix A. While
additional modeling work is needed to generate
low-level actions for these high-level plans, they
point to generalization as a key differentiator be-
tween latent language policies and ordinary hierar-
chical ones.
6 Related Work
Our approach draws on a large body of research
at the intersection of natural language processing,
representation learning, and autonomous control.
Language-based supervision and representa-
tion The use of natural language annotations to
scaffold learning, especially in computer vision
and program synthesis applications, has been the
subject of a number of previous studies (Brana-
van et al., 2009; Frome et al., 2013; Andreas et al.,
2018; Wong et al., 2021). Here, we use language to
support policy learning, speciﬁcally by using natu-
ral language instructions to discover compositional
subtask abstractions that can support autonomous
control. Our approach is closely related to previous
work on learning skill libraries from policy sketches
(Andreas et al., 2017; Shiarlis et al., 2018); instead
of the ﬁxed skill inventory used by policy sketches,
(SL)learns an open-ended, compositional library
of behaviors indexed by natural language strings.
Hierarchical policies Hierarchical policy learn-
ing and temporal abstraction have been major areas
of focus since the earliest research on reinforce-
ment learning and imitation learning (McGovern
and Barto, 2001; Konidaris et al., 2012; Daniel
et al., 2012). Past work typically relies on direct
supervision or manual speciﬁcation of the space
of high-level skills (Sutton et al., 1999; Kulkarni
et al., 2016) or fully unsupervised skill discov-
ery (Dietterich, 1999; Bacon et al., 2017). Our
approach uses policy architectures from this lit-
erature, but aims to provide a mechanism for su-
pervision that allows ﬁne-grained control over the
space of learned skills (as in fully supervised ap-
proaches) while requiring only small amounts of
easy-to-gather human supervision.
Language and interaction Outside of
language-based supervision, problems at the1720
intersection of language and control include
instruction following (Chen and Mooney, 2011;
Branavan et al., 2009; Tellex et al., 2011; Anderson
et al., 2018; Misra et al., 2017), embodied question
answering (Das et al., 2018; Gordon et al., 2018)
and dialog tasks (Tellex et al., 2020). As in our
work, representations of language learned from
large text corpora facilitate grounded language
learning (Shridhar et al., 2021), and interaction
with the environment can in turn improve the
accuracy of language generation (Zellers et al.,
2021); future work might extend our framework
for semi-supervised inference of plan descriptions
to these settings as well.
7 Conclusion
We have presented (SL), a framework for learning
hierarchical policies from demonstrations sparsely
annotated with natural language descriptions. Us-
ing these annotations, (SL)infers the latent struc-
ture of unannotated demonstrations, automaticallysegmenting them into subtasks and labeling each
subtask with a compositional description. Learn-
ing yields a hierarchical policy in which natural
language serves as an abstract representation of
subgoals and plans: a controller sub-policy maps
from goals to natural language plan speciﬁcations,
and a modular executor that maps each compo-
nent of the plan to a sequence of low-level actions.
In simulated household environments, this model
can complete abstract goals (like slice a tomato )
with accuracy comparable to state-of-the-art mod-
els trained and evaluated with ﬁne-grained plans
(ﬁnd a knife ,carry the knife to the tomato , . . . ).
While our evaluation has focused on household
robotics tasks, the hierarchical structure inferred by
(SL)is present in a variety of learning problems,
including image understanding, program synthesis,
and language generation. In all those domains, gen-
eralized versions of (SL)might offer a framework
for building high-quality models using only a small
amount of rich natural language supervision.1721Acknowledgements
We would like to thank Valts Blukis and Shikhar
Murty for helpful discussions. Also thanks to Joe
O’ Connor, Gabe Grand and the anonymous re-
viewers for their feedback on an early draft of the
paper.
References17221723A Out-of-distribution Generalization
One of the advantages of language-based skill
representations over categorical representations
is open-endedness: (SL)does not require pre-
speciﬁcation of a ﬁxed inventory of goals or ac-
tions. As a simple demonstration of this potential
for extensibility, we design goal prompts consisting
of novel object names, verbs and skill combina-
tions not seen at training time, and test the model’s
ability to generalize to out-of-distribution samples
across the three categories. Some roll-outs can be
seen in Fig. 5. We observe the following:
Novel sub-task combinations We qualitatively
evaluate the ability of the model to generalize sys-
tematically to novel subtask combinations and sub-
task ordering not encountered at training time. Ex-
amples are shown in Fig. 5. For example, we
present the model with the goal slice a heated ap-
ple; in the training corpus, objects are only heated
after being sliced. It can be seen in Fig. 5 that
the model able correctly orders the two subtasks.
The model additionally generalizes to new combi-
nations of tasks such as clean and cool an apple .
Novel objects and verbs The trained model also
exhibits some success at generalizing novel object
categories such as carrot andmask . In the carrot
example, an incorrect Find the lettuce example is
generated at the ﬁrst step, but subsequent subtasks
refer to a carrot (and apply the correct actions to
it). The model also generalizes to new but related
verbs such as scrub but fails at ones like squash
that are unrelated to training goals.
Limitations One shortcoming of this approach
is that affordances and constraints are incompletely
modeled. Given a (physically unrealizable) goal
clean the bowl and then slice it , the model cannot
detect the impossible goal and instead generates
a plan involving slicing the bowl. Another short-
coming of the model is the ability to generalize to
goals that may involve considerably larger number
of subgoals than goals seen at training time. For
plans that involve very long sequences of skills
(slice then clean then heat. . . ) the generated plan
skips some subtasks Fig. 5.
B Initialization: Segmentation Step
The training data contains no actions, so
cannot be initialized by training on D. Using
a randomly initialized during the segmentationstep results in extremely low-quality segmentations.
Instead, we obtain an initial set of segmentations
viaunsupervised learning on low-level action se-
quences.
In particular, we obtain initial segmentations us-
ing the Baum–Welch algorithm for unsupervised
estimation of hidden Markov models (Baum et al.,
1970). We replace string-valued latent variables
produced by with a discrete set of hidden states
(in our experiments, we found that three hidden
states sufﬁced). Transition and emission distribu-
tions, along with maximum a posteriori sequence
labels, are obtained by running the expectation–
maximization algorithm on state sequences. We
then insert segment boundaries (and an implicit action) at every transition between two dis-
tinct hidden states. Evaluated against ground-truth
segmentations from the ALFRED training set, this
produces an action-level accuracy of 87.9%. The
detailed algorithm can be found in Baum et al.
(1970).
C Model Architecture: Details
The controller policy is a ﬁne-tuned T5-small
model. The executor policy decodes the low-
level sequence of actions conditioned on the ﬁrst-
person visual observations of the agent. We use
the same architecture across the remaining base-
lines too. Fig. 6 depicts the architecture of the
image-conditioned T5 model. In addition to task
speciﬁcations, we convert low-level actions to tem-
plated commands: for example, put(cup,table)
becomes put the cup on the table . These are parsed
to select actions to send to the ALFRED simula-
tor. During training, both models are optimized
using the AdamW algorithm (Loshchilov and Hut-
ter, 2019) with a learning rate of 1e-4, weight decay
of 0.01, and=1e-8. We use a MaskRCNN model
to generate action masks, selecting the predicted
mask labeled with the class of the object name
generated by the action decoder. The same model
architecture is used across all baselines.
D Role of trajectory length
We conduct an additional set of ablation exper-
iments aimed at clarifying what aspects of the
demonstrated trajectories (SL)is better able to
model than baselines. We begin by observing
that most actions in our data are associated with
navigation, with sequences of object manipulation
actions (like those depicted in Fig. 3) constitut-1724
ing only about 20% of each trajectory. We con-
struct an alternative version of the dataset in which
all navigation subtasks are replaced with a single
TeleportTo action. This modiﬁcation reduces av-
erage trajectory length from 50 actions to 9. In
this case (SL)andseq2seq2seq perform com-
parably well (55.6% success rate and 56.7% suc-
cess rate respectively), and only slightly better than
seq2seq (53.6% success rate). Thus, while (SL)
(and all baselines) perform quite poorly at naviga-
tion skills, identifying these skills and modeling
their conditional independence from other trajec-
tory components seems to be crucial for effective
learning of other skills in the long-horizon setting.
Hierarchical policies are still useful for modeling
these shorter plans, but by a smaller margin than
for long demonstrations.
E Navigation Instructions
The original ALFRED dataset contains detailed
instructions for navigation collected post-hoc after
the demonstrations are generated. For example, the
sub-task speciﬁcation associated with ﬁnding an
apple might be given as Go straight and turn to the
right of the fridge and take a few steps ahead and
look down . Such instructions cannot be used for
high-level planning, as they can only be generated
with advance knowledge of the environment layout;
successful behavior in novel environments requires
exploration or explicit access to the environment’s
map.
To address the mismatch between the agent’s
knowledge and the information needed to generate
detailed navigation instructions, we navigation in-
structions in the ALFRED dataset with templated
instructions of the form Go to the [object] (for ap-
pliances and containers) and Find the [object] (for
movable objects). Because the ALFRED dataset
provides PDDL plans for each demonstration, we
can obtain the name of the target [object] directly
from these plans. Examples are shown in Fig. 7.17251726