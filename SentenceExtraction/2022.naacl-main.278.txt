
Howard Chen Jacqueline He Karthik Narasimhan Danqi Chen
Department of Computer Science, Princeton University
{howardchen, karthikn, danqic}@cs.princeton.edu
jyh@princeton.edu
Abstract
A growing line of work has investigated the de-
velopment of neural NLP models that can pro-
duce rationales —subsets of input that can ex-
plain their model predictions. In this paper, we
ask whether such rationale models can provide
robustness to adversarial attacks in addition to
their interpretable nature. Since these models
need to ﬁrst generate rationales (“rationalizer”)
before making predictions (“predictor”), they
have the potential to ignore noise or adversari-
ally added text by simply masking it out of the
generated rationale. To this end, we systemat-
ically generate various types of ‘AddText’ at-
tacks for both token and sentence-level ratio-
nalization tasks and perform an extensive em-
pirical evaluation of state-of-the-art rationale
models across ﬁve different tasks. Our ex-
periments reveal that rationale models show
promise in improving robustness but struggle
in certain scenarios—e.g., when the rational-
izer is sensitive to position bias or lexical
choices of the attack text. Further, leverag-
ing human rationales as supervision does not
always translate to better performance. Our
study is a ﬁrst step towards exploring the inter-
play between interpretability and robustness in
the rationalize-then-predict framework.
1 Introduction
Rationale models aim to introduce a degree of inter-
pretability into neural networks by implicitly bak-
ing in explanations for their decisions (Lei et al.,
2016; Bastings et al., 2019; Jain et al., 2020). These
models are carried out in a two-stage ‘rationalize-
then-predict’ framework, where the model ﬁrst se-
lects a subset of the input as a rationale and then
makes its ﬁnal prediction for the task solely us-
ing the rationale. A human can then inspect the
selected rationale to verify the model’s reasoningFigure 1: Top: input text is processed by a rationale
model (rationalizer and predictor) and a full-context
model (making predictions based on the whole input)
separately in a beer review sentiment classiﬁcation
dataset. Both models make correct predictions. Bot-
tom: when an attack sentence “ The tea looks horrible. ”
is inserted, the full-context model fails. The rational-
izer successfully excludes the negative-sentiment word
“horrible” from the selected rationales (yellow high-
lights). The predictor is hence not distracted by the
attack sentence.
over the most relevant parts of the input for the
prediction at hand.
While previous work has mostly focused on the
plausibility of extracted rationales and whether they
represent faithful explanations (DeYoung et al.,
2020), we ask the question of how rationale models
behave under adversarial attacks (i.e., do they still
provide plausible rationales?) and whether they can
help improve robustness (i.e., do they provide bet-
ter task performance?). Our motivation is that the
two-stage decision-making could help models ig-
nore noisy or adversarially added text within the in-
put. For example, Figure 1 shows a state-of-the-art
rationale model (Paranjape et al., 2020) smoothly
handles input with adversarially added text by se-
lectively masking it out during the rationalization
step. Factorizing the rationale prediction from the
task itself effectively ‘shields’ the predictor from3792having to deal with adversarial inputs.
To answer these questions, we ﬁrst generate ad-
versarial tests for a variety of popular NLP tasks
(§4). We focus speciﬁcally on model-independent,
‘AddText’ attacks (Jia and Liang, 2017), which aug-
ment input instances with noisy or adversarial text
attest time , and study how the attacks affect ratio-
nale models both in their prediction of rationales
and ﬁnal answers. For diversity, we consider in-
serting the attack sentence at different positions
of context, as well as three types of attacks: ran-
dom sequences of words, arbitrary sentences from
Wikipedia, and adversarially-crafted sentences.
We then perform an extensive empirical eval-
uation of multiple state-of-the-art rationale mod-
els (Paranjape et al., 2020; Guerreiro and Martins,
2021), across ﬁve different tasks that span review
classiﬁcation, fact veriﬁcation, and question an-
swering (§5). In addition to the attack’s impact on
task performance, we also assess rationale predic-
tion by deﬁning metrics on gold rationale coverage
and attack capture rate. We then investigate the
effect of incorporating human rationales as super-
vision, the importance of attack positions, and the
lexical choices of attack text. Finally, we investi-
gate an idea of improving the rationalizer by adding
augmented pseudo-rationales during training (§7).
Our key ﬁndings are the following:
1.Rationale models show promise in providing
robustness. Under our strongest type of attack,
rationale models in many cases achieve less
than 10% drop in task performance while full-
context models suffer more (11%–27%).
2.However, robustness of rationale models can
vary considerably with the choice of lexical
inputs for the attack and is quite sensitive to
the attack position.
3.Training models with explicit rationale super-
vision does not guarantee better robustness to
attacks. In fact, their accuracy drops under
attack are higher by 4-10 points compared to
rationale models without supervision.
4.Performance under attacks is signiﬁcantly
improved if the rationalizer can effectively
mask out the attack text. Hence, our simple
augmented-rationale training strategy can ef-
fectively improve robustness (up to 4.9%).
Overall, our results indicate that while there is
promise in leveraging rationale models to improve
robustness, current models may not be sufﬁciently
equipped to do so. Furthermore, adversarial testsmay provide an alternative form to evaluate ratio-
nale models in addition to prevalent plausability
metrics that measure agreement with human ratio-
nales. We hope our ﬁndings can inform the devel-
opment of better methods for rationale predictions
and instigate more research into the interplay be-
tween interpretability and robustness.
2 Related Work
Rationalization There has been a surge of work
on explaining predictions of neural NLP systems,
from post-hoc explanation methods (Ribeiro et al.,
2016; Alvarez-Melis and Jaakkola, 2017), to anal-
ysis of attention mechanisms (Jain and Wallace,
2019; Serrano and Smith, 2019). We focus on ex-
tractive rationalization (Lei et al., 2016), which
generates a subset of inputs or highlights as “ra-
tionales” such that the model can condition pre-
dictions on them. Recent development has been
focusing on improving joint training of rationalizer
and predictor components (Bastings et al., 2019; Yu
et al., 2019; Jain et al., 2020; Paranjape et al., 2020;
Guerreiro and Martins, 2021; Sha et al., 2021), or
extensions to text matching (Swanson et al., 2020)
and sequence generation (Vafa et al., 2021). These
rationale models are mainly compared based on
predictive performance, as well as agreement with
human annotations (DeYoung et al., 2020). In this
work, we question how rationale models behave
under adversarial attacks and whether they can pro-
vide robustness beneﬁts through rationalization.
Adversarial examples in NLP Adversarial ex-
amples have been designed to reveal the brittle-
ness of state-of-the-art NLP models. A ﬂood of
research has been proposed to generate different ad-
versarial attacks (Jia and Liang, 2017; Iyyer et al.,
2018; Belinkov and Bisk, 2018; Ebrahimi et al.,
2018, inter alia ), which can be broadly catego-
rized by types of input perturbations (sentence-,
word- or character-level attacks), and access of
model information (black-box or white-box). In
this work, we focus on model-independent , label-
preserving attacks, in which we insert a random
or an adversarially-crafted sentence into input ex-
amples (Jia and Liang, 2017). We hypothesize that
a good extractive rationale model is expected to
learn to ignore these distractor sentences and hence
achieve better performance under attacks.
Interpretability and robustness A key motiva-
tion of our work is to bridge the connection be-3793tween interpretability and robustness, which we
believe is an important and under-explored area.
Alvarez-Melis and Jaakkola (2018) argue that ro-
bustness of explanations is a key desideratum for
interpretability. Slack et al. (2020) explore unreli-
ability of attribution methods against input pertur-
bations. Camburu et al. (2020) introduce an adver-
sarial framework to sanity check models against
their generated inconsistent free-text explanations.
Zhou et al. (2020) propose to evaluate attribution
methods through dataset modiﬁcation. Noack et al.
(2021) show that image recognition models can
achieve better adversarial robustness when they
are trained to have interpretable gradients. To the
best of our knowledge, we are the ﬁrst to quantify
the performance of rationale models under textual
adversarial attacks and understand whether ratio-
nalization can inherently provide robustness.
3 Background
Extractive rationale modelsoutput predictions
through a two-stage process: the ﬁrst stage (“ratio-
nalizer”) selects a subset of the input as a rationale ,
while the second stage (“predictor”) produces the
prediction using only the rationale as input. Ra-
tionales can be any subset of the input, and we
characterize them roughly into either token-level or
sentence-level rationales, which we will both inves-
tigate in this work. The task of predicting rationales
is often framed as a binary classiﬁcation problem
over each atomic unit depending on the type of
rationales. The rationalizer and the predictor are
often trained jointly using task supervision, with
gradients back-propagated through both stages. We
can also provide explicit rationale supervision, if
human annotations are available.
3.1 Formulation
Formally, let us assume a supervised classiﬁca-
tion datasetD={(x,y)}, where each input
x=x,x,...,xis a concatenation of Tsen-
tences and each sentence x= (x,x,...x)
containsntokens, and yrefers to the task label.
A rationale model consists of two main compo-
nents: 1) a rationalizer module z=R(x;θ), which
generates a discrete mask z∈{0,1}such that
z⊙xselects a subset from the input ( L=T
for sentence-level rationalization or L=/summationtextnfor token-level rationales), and 2) a predictor mod-
uleˆy=C(x,z;φ)that makes a prediction ˆyus-
ing the generated rationale z. The entire model
M(x) =C(R(x))is trained end-to-end using the
standard cross-entropy loss. We describe detailed
training objectives in §5.
3.2 Evaluation
Rationale models are traditionally evaluated along
two dimensions: a) their downstream task perfor-
mance, and b) the quality of generated rationales.
To evaluate rationale quality, prior work has used
metrics like token-level F1 or Intersection Over
Union (IOU) scores between the predicted ratio-
nale and a human rationale (DeYoung et al., 2020):
IOU=|z∩z|
|z∪z|,
wherezis the human-annotated gold rationales.
A good rationale model should not sacriﬁce task
performance while generating rationales that con-
cur with human rationales. However, metrics like
F1 score may not be the most appropriate way to
capture this as it only captures plausibility instead
offaithfulness (Jacovi and Goldberg, 2020).
4 Robustness Tests for Rationale Models
4.1 AddText Attacks
Our goal is to construct attacks that can test the
capability of extractive rationale models to ignore
spurious parts of the input. Broadly, we used two
guiding criteria for selecting the type of attacks: 1)
they should be additive since an extractive rationale
model can only “ignore” the irrelevant context. For
other attacks such as counterfactually edited data
(CAD) (Kaushik et al., 2020), even if the rational-
izer could identify the edited context, the predictor
is not necessarily strong enough to reason about
the counterfactual text; 2) they should be model-
independent since our goal is to compare the per-
formance across different types of rationale and
baseline models. Choosing strong gradient-based
attacks (Ebrahimi et al., 2018; Wallace et al., 2019)
would probably break all models, but that is beyond
the scope of our hypothesis. An attack is suitable as
long as it reduces performance of standard classiﬁ-
cation models by a non-trivial amount (our attacks
reduce performance by 10%–36% absolute).
Keeping these requirements in mind, we focus
on label-preserving text addition attacks Jia and
Liang (2017), which can test whether rationale3794models are invariant to the addition of extraneous
information and remain consistent with their pre-
dictions. Attacks are only added at test time and
are not available during model training.
Attack construction Formally, an AddText at-
tackA(x)modiﬁes the input xby adding an attack
sentencex, without changing the ground truth
labely. In other words, we create new perturbed
test instances (A(x),y)for the model to be eval-
uated on. While some prior work has considered
the addition of a few tokens to the input (Wallace
et al., 2019), we add complete sentences to each
input, similar to the attacks in Jia and Liang (2017).
This prevents unnatural modiﬁcations to the exist-
ing sentences in the original input xand also allows
us to test both token-level and sentence-level ratio-
nale models (§5.1). We experiment with adding the
attack sentence xeither at the beginning or the
end of the input x.
Types of attacks We explore three different
types of attacks: (1) AddText-Rand : we simply
add a random sequence of tokens uniformly sam-
pled from the task vocabulary. This is a weak at-
tack that is easy for humans to spot and ignore
since it does not guarantee grammaticality or ﬂu-
ency. (2) AddText-Wiki : we add an arbitrarily
sampled sentence from English Wikipedia into the
task input (e.g., “Sonic the Hedgehog, designed
for . . . ”). This attack is more grammatical than
AddText-Rand, but still adds text that is likely ir-
relevant in the context of the input x. (3)AddText-
Adv: we add an adversarially constructed sentence
that has signiﬁcant lexical overlap with tokens in
the inputxwhile ensuring the output label is un-
changed. This type of attack is inspired by prior
attacks such as AddOneSent (Jia and Liang, 2017)
and is the strongest attack we consider since it is
more grammatical, ﬂuent, and contextually rele-
vant to the task. The construction of this attack is
also speciﬁc to each task we consider, hence we
provide examples listed in Table 1 and more details
in §5.3.
4.2 Robustness Evaluation
We measure the robustness of rationale models un-
der our attacks along two dimensions: task perfor-
mance , and generated rationales . The change in
task performance is simply computed as the differ-ence between the average scores of the model on
the original vs perturbed test sets:
∆ =1
|D|/summationdisplayf(M(x),y)−f(M(A(x)),y),
wherefdenotes a scoring function (F1 scores in
extractive question answering and I(y= ˆy)in text
classiﬁcation). To measure the effect of the attacks
on rationale generation, we use two metrics:
Gold rationale F1 (GR) This is deﬁned as the F1
score between the predicted rationale and a human-
annotated rationale, either computed at the token
or sentence level. The token-level GR score is
equivalent to F1 scores reported in previous work
(Lei et al., 2016; DeYoung et al., 2020). A good
rationalizer should generate plausible rationales
and be not affected by the addition of attack text.
Attack capture rate (AR) We deﬁne AR as the
recall of the inserted attack text in the rationale
generated by the model:
AR =1
|D|/summationdisplay|x∩(z⊙A(x))|
|x|,
wherexis the attack sentence added to each
instance (i.e., A(x)is the result of inserting x
intox),z⊙A(x)is the predicted rationale. The
metric above applies on both token or sentence
level (|x|= 1for sentence-level rationalization
and number of tokens in the attack sentence for
token-level rationalization). This metric allows us
to measure how often a rationale model can ignore
the added attack text—a maximally robust rationale
model should have an AR of 0.
5 Models and Tasks
We investigate two different state-of-the-art selec-
tive rationalization approaches: 1) sampling-based
stochastic binary mask generation (Bastings et al.,
2019; Paranjape et al., 2020), and 2) determinis-
tic sparse attention through constrained inference
(Guerreiro and Martins, 2021). We adapt these
models, using two separate BERT encoders for the
rationalizer and the predictor, and consider train-
ing scenarios with and without explicit rationale
supervision. We also consider a full-context model
as baseline. We provide a brief overview of each
model here and leave details including loss func-
tions and training to §A.1.3795
5.1 Models without Rationale Supervision
Variational information bottleneck (VIB)
This model (Paranjape et al., 2020) imposes a
discrete bottleneck objective (Alemi et al., 2017)
to select a mask z∈ {0,1}from the input x.
The rationalizer samples zusing Gumbel-Softmax
and the predictor uses only z⊙xfor the ﬁnal
prediction. During inference, we select the top- k
scored rationales, where kis determined by the
sparsityπ.
Sparse structured text rationalization (SPEC-
TRA) This model (Guerreiro and Martins, 2021)
extracts a deterministic structured mask zby solv-
ing a constrained inference problem by applying
factors to the global scoring function while op-
timizing the end task performance. The entire
computation is deterministic and allows for back-
propagation through the LP-SparseMAP solver
(Niculae and Martins, 2020). We use the BUDGET
factor to control the sparsity π.
Full-context model (FC) As a baseline, we also
consider a full-context model, which makes predic-
tions directly conditioned on the entire input. The
model is a standard BERT model which adds task-
speciﬁc classiﬁers on top of the encoder (Devlin
et al., 2019). The model is trained with a cross-
entropy loss using task supervision.
5.2 Models with Rationale Supervision
VIB with human rationales (VIB-sup) When
human-annotated rationales zare available, they
can be used to guide the prediction of the sampled
maskszby adding a cross entropy loss between
them (more details in §A.1). VIB-sup leverages
this supervision signal to guide rationale prediction.
Full-context model with human rationales (FC-
sup) We also extend the FC model to lever-
age human-annotated rationales supervision dur-
ing training by adding a linear layer on top of the
sentence/token representations. Essentially, it is
multi-task learning of rationale prediction and the
original task, shared with the same BERT encoder.
The supervision is added by calculating the cross
entropy loss between the human-annotated ratio-
nales and the predicted rationales (§A.1).
5.3 Tasks
We evaluate the models on ﬁve datasets that cover
both sentence-level (FEVER, MultiRC, SQuAD)
and token-level (Beer, Hotel) rationalization (ex-
amples in Table 1). We summarize the dataset
characteristics in Table 2.
FEVER FEVER is a sentence-level binary classi-
ﬁcation fact veriﬁcation dataset from the ERASER
benchmark (DeYoung et al., 2020). The input
contains a claim specifying a fact to verify and
a passage of multiple sentences supporting or re-
futing the claim. For the AddText-Adv attacks, we
add modiﬁed query text to the claims by replacing
nouns and adjectives in the sentence with antonyms3796from WordNet (Fellbaum, 1998).
MultiRC MultiRC (Khashabi et al., 2018) is
a sentence-level multi-choice question answering
task (reformulated as ‘yes/no’ questions). For the
AddText-Adv attacks, we transform the question
and the answer separately using the same procedure
we used for FEVER.
SQuAD SQuAD (Rajpurkar et al., 2016) is a pop-
ular question answering dataset. We use the Ad-
dOneSent attacks proposed in Adversarial SQuAD
(Jia and Liang, 2017), except that they always in-
sert the sentence at the end of the paragraph and
we consider inserting at the beginning, the end, and
a random position. Since SQuAD does not contain
human rationales, we use the sentence that con-
tains the correct answer span as the ground truth
rationale sentence. We report F1 score for SQuAD.
Beer BeerAdvocate is a multi-aspect sentiment
analysis dataset (McAuley et al., 2012), modeled
as a token-level rationalization task. We use the
appearance aspect in out experiments. We convert
the scores into the binary labels following Chang
et al. (2020). This task does not have a query as
in the previous tasks, we insert a sentence with
the template “ {SUBJECT} is{ADJ} ” into a neg-
ative review where the adjective is positive (e.g.,
“The tea looks fabulous.”) and vice versa. We con-
sider one object “car” and eight adjectives (e.g.,
“clean/ﬁlthy”, “new/old”).
Hotel TripAdvisor Hotel Review is also a multi-
aspect sentiment analysis dataset (Wang et al.,
2010). We use the cleanliness aspect in our ex-
periments. We generate AddText-Adv attacks in
the same way as we did for the Beer dataset. We
consider three objects ranging from more relevant
words such as “tea” to less related word “car-
pet” and six adjectives (e.g., “pretty/disgusting”,
“good/bad”, “beautiful/ugly”).
6 Results
For all attacked test sets, we report the average
scores with attack sentence inserted at the begin-
ning and the end of the inputs. Our ﬁndings shed
light on the relationship between GR, AR, and drop
in performance, which eventually lead to a promis-
ing direction to improve performance of rationale
models under attacks (§7).6.1 Task Performance
Figure 2 summarizes the average scores on all
datasets for each model under the three attacks
we consider. We ﬁrst observe that all models (in-
cluding the full-context models FC and FC-sup)
are mildly affected by AddText-Rand and AddText-
Wiki, with score drops of around 1-2%. However,
the AddText-Adv attack leads to more signiﬁcant
drops in performance for all models, as high as
46% for SPECTRA on the Hotel dataset. We break
out the AddText-Adv results in a more ﬁne-grained
manner in Table 3. Our main observation is that
the rationale models (VIB, SPECTRA, VIB-sup)
are generally more robust than their non-rationale
counterparts (FC, FC-sup) on four out of the ﬁve
tasks, and in some cases dramatically better. For in-
stance, on the Beer dataset, SPECTRA only suffers
a 5.7% drop (95.4→89.7) compared to FC’s huge
34.3% drop (93.8→59.5) under attack. The only
exception is the Hotel dataset, where both the VIB
and SPECTRA models perform worse under attack
compared to FC. We analyze this phenomena and
provide a potential reason below.
6.2 Robustness Evaluation: GR vs AR
In Table 4, we report the Gold Rationale F1 (GR)
and Attack Capture Rate (AR) for all models.
When attacks are added, GR consistently decreases
for all tasks. However, AR ranges widely across
datasets. VIB and SPECTRA have lower AR and
higher GR compared to FC-sup across all tasks,
which is correlated with their superior robustness
to AddText-Adv attacks.
Next, we investigate the poor performance of
VIB and SPECTRA on the Hotel dataset by ana-
lyzing the choice of words in the attack. Using
the template “My car is {ADJ} .”, we measure the
percentage of times the rationalizer module selects
the adjective as part of its rationale. When the ad-
jectives are “dirty” and “clean”, the VIB model
selects them a massive 98.5%of the time. For “old”
and “new”, VIB still selects them 50% of the time.
On the other hand, the VIB model trained on Beer
reviews with attack template “The tea is {ADJ} .”
only selects the adjectives 20.5%of the time (when
the adjectives are “horrible” and “fabulous”). This
shows that the bad performance of the rationale
models on the Hotel dataset is due to their inability
to ignore task-related adjectives in the attack text,
hinting that the lexical choices made in construct-
ing the attack can largely impact robustness.3797
We examine where the rationale model gains
robustness by inspecting the generated rationales.
Table 5 shows the accuracy breakdown under attack
for VIB and VIB-sup models. Intuitively, both
models perform best when the gold rationale is
selected and the attack is avoided, peaking at 91.1
for VIB and 92.4for VIB-sup. Models perform
much worse when the gold rationale is omitted and
the attack is included ( 73.6for VIB and 74.1for
VIB-sup), highlighting the importance of choosing
good and skipping the bad as rationales.
6.3 Impact of Gold Rationale Supervision
Perhaps surprisingly, adding explicit rationale su-
pervision does not help improve robustness (Ta-ble 3). Across FEVER, MultiRC and SQuAD, VIB-
sup consistently has a higher ∆between its scores
on the original and perturbed instances. We observe
that models trained with human rationales gener-
ally have higher GR , but they also capture a much
higher AR across the board (Table 4). On MultiRC,
for instance, the VIB-sup model outperforms VIB
in task performance because of its higher GR ( 36.1
versus 15.8). However, when under attack, VIB-
sup’s high 58.7AR, hindering the performance
compared to VIB, which has a smaller 35.8AR.
This highlights a potential shortcoming of prior
work in only considering metrics like IOU (similar
in spirit to GR) to assess rationale models. The
ﬁnding also points to the risk of straightforwardly3798
incorporating supervised rationale as it could result
in the existing model overﬁtting to them.
6.4 Sensitivity of Attack Positions
We further analyze the effect of attack text on ra-
tionale models by varying the attack position. Fig-
ure 3 displays the performance of VIB, VIB-sup
and FC on FEVER and SQuAD when the attack
sentence is inserted into the ﬁrst, last or any ran-
dom position in between. We observe performance
drops on both datasets when inserting the attack
sentence at the beginning of the context text as
opposed to the end. For example, when the at-
tack sentence is inserted at the beginning, the VIB
model drops from 77.1F1 to 40.9F1, but it only
drops from 77.1F1 to 72.1F1 for a last position
attack on SQuAD. This hints that rationale mod-
els may implicitly be picking up positional biases
from the dataset, similar to their full-context coun-
terparts (Ko et al., 2020). We provide ﬁne-grained
plots for AR versus attack positions in §A.4.
7 Augmented Rationale Training
From our previous analysis on the trade-off be-
tween GR and AR (§6.2), it is clear that avoiding
attack sentences in rationales is a viable way to
make such models more robust. Note that this is
not obvious by construction since the addition of
attacks affects other parameters such as position
of the original text and discourse structure, which
may throw off the ‘predictor’ component of the
model. As a more explicit way of encouraging ‘ra-
tionalizers’ to ignore spurious text, we propose a
simple method called augmented rationale train-
ing(ART). Speciﬁcally, we sample two sentences
at random from the Wikitext-103 dataset (Merity
et al., 2017) and insert them into the input passage
at random position, setting their pseudo rationale
labelsz= 1and the labels for all other sen-
tences asz= 0. We limit the addition to only
inserting two sentences to avoid exceeding the ra-
tionalizer maximal token limit. We then add an
auxiliary negative binary cross entropy loss to train
the model to notpredict the pseudo rationale. This
encourages the model to ignore spurious text that
is unrelated to the task. Note that this procedure
is both model-agnostic and does not require prior
knowledge of the type of AddText attack.
Table 6 shows that ART improves robustness
across the board for all models (FC-sup, VIB and
VIB-sup) in both FEVER and MultiRC, dropping
∆scores by as much as 5.9% (VIB-sup on FEVER).
We further analyzed these results to break down
performance in terms of attack and gold sentence
capture rate. Table 7 shows that ART greatly im-
proves the percentage of sentences under the “Gold
Attack ” category ( 31.8%→65.4%for VIB
and11.3%→63.5%for VIB-sup). This corrobo-
rates our expectations for ART and shows its effec-3799tiveness at keeping GR high while lowering AR.
Interestingly, the random Wikipedia sentences
we added in ART are not topically or contextually
related to the original instance text at all, yet they
seem to help the trained model ignore adversari-
ally constructed text that is tailored for speciﬁc test
instances. This points to the promise of ART in
future work, where perhaps more complex gener-
ation schemes or use of attack information could
provide even better robustness.
8 Discussion
In this work, we investigated whether neural ratio-
nale models are robust to adversarial attacks. We
constructed a variety of AddText attacks across ﬁve
different tasks and evaluated several state-of-the-
art rationale models. Our ﬁndings raise two key
messages for future research in both interpretability
and robustness of NLP models:
Interpretability We identify an opportunity to
use adversarial attacks as a means to evaluate ra-
tionale models (especially extractive ones). In con-
trast to existing metrics like IOU used in prior
work (DeYoung et al., 2020; Paranjape et al., 2020),
robustness more accurately tests how crucial the
predicted rationale is to the model’s decision mak-
ing. Further, our analysis reveals that even state-
of-the-art rationale models may not be consistent
in focusing on the most relevant parts of the input,
despite performing well on tasks they are trained
on. This points to the need for better model ar-
chitectures and training algorithms to better align
rationale models with human judgements.
Robustness For adversarial attack research, we
show that extractive rationale models are promising
for improving robustness, while being sensitive to
factors like the attack position or word choices in
the attack text. Research that proposes new attacks
can use rationale models as baselines to assess theireffectiveness. Finally, the effectiveness of ART
points to the potential for data augmentation in
improving robustness of NLP systems, even against
other types of attacks beyond AddText.
We hope our results can inspire more research at
the intersection of interpretability and robustness.
Acknowledgement
We thank the members of the Princeton NLP group
and the anonymous reviewers for their valuable
comments and feedback. HC is supported by the
Princeton Upton Fellowship. This research is also
supported by a Salesforce AI Research Grant.
References380038013802A Appendix
A.1 Model Details
VIB details The sentence or token level logits
s∈R(A.2 describes how the logits are obtained)
parameterize a relaxed Bernoulli distribution p(z|
x) = RelaxedBernoulli( s)(also known as the
Gumbel distribution (Jang et al., 2017)), where
z∈{0,1}is the binary mask for sentence t. The
relaxed Bernoulli distribution also allows for sam-
pling a soft mask z=σ()∈(0,1), where
gis the sampled Gumbel noise. The soft masks
z= (z,z,...,z)are sampled independently
to mask the input sentences such that the latent
z=m⊙xfor training. The following objective
is optimized:
/lscript(x,y) = E/bracketleftbig
−logp(y|z⊙x;φ)/bracketrightbig
+βKL/bracketleftbig
p(z|x;θ)||p(z)/bracketrightbig
,
whereφdenotes the parameters of the predictor C,
θdenotes the parameters of the rationalizer R,p(z)
is a predeﬁned prior distribution parameterized by
a sparsity ratio π, andβ∈Rcontrols the strength
of the regularization.
During inference, we take the rationale as z=
1[s∈top-k(s)], wheres∈Ris the vector of
token or sentence-level logits, and kis determined
by the sparsity π.
VIB-sup details With human raitonale supervi-
sionz, the objective below is optimized:
/lscript (x,y) = E/bracketleftbig
−logp(y|z⊙x;φ)/bracketrightbig
+βKL/bracketleftbig
p(z|x;θ)||p(z)/bracketrightbig
+γ/summationdisplay−zlogp(z|x;θ),
whereβ,γ∈Rare hyperparameters. During in-
ference, the rationale module generates the mask
zthe same way as the VIB model by picking the
top-kscored positions as the ﬁnal hard mask. The
third loss term will encourage the model to predict
human annotated rationales, which is the ability we
expect a robust model should exhibit.
SPECTRA details SPECTRA optimizes the fol-
lowing objective:
/lscript (x,y) =−logp(y|z⊙x;φ),
z= argmax(score(z;s;θ)−1
2/vextenddouble/vextenddoublez/vextenddouble/vextenddouble),wheres∈Ris the logit vector of tokens or sen-
tences, and a global score(·)function that incorpo-
rates all constraints in the predeﬁned factor graph.
The factors can specify different logical constraints
on the discrete mask z, e.g a BUDGET factor that
enforces the size of the rationale as/summationtextz≤B.
The entire computation is deterministic and allows
for back-propagation through the LP-SparseMAP
solver (Niculae and Martins, 2020). We use the
BUDGET factor in the global scoring function. To
control the sparsity at π(e.g.,π= 0.4for40%
sparsity), we can choose B=L×π.
FC-sup details The FC model can be extended
to leverage human annotated rationales supervision
during training (FC-sup). We add a linear layer on
top of the sentence/token representation and obtain
the logitss∈R. The logits are passed through
the sigmoid function into mask probabilities to op-
timize the following objective:
/lscript (x,y) =−logp(y|x;φ)
+γ/summationdisplay−zlogp(z|x;φ,ξ),
wherezis the human rationale, ξaccounts for
the parameters of the extra linear layer, and the
hyperparameter γis selected based on the original
performance by tuning on the development set.
A.2 Implementation Details
We use two BERT-base-uncased (Wolf et al.,
2020) as the rationalizer and the predictor compo-
nents for all the models and one BERT-base for
the Full Context (FC) baseline. The rationales for
FEVER, MultiRC, SQuAD are extracted at sen-
tence level, and Beer and Hotel are at token-level.
BERT (x) =/parenleftbig
h,h,h,...,h,h,
h,h,...,h,...,h,h,...,h,h/parenrightbig
,
where the input text is formatted as query with
sentence index 0andcontext with sentence index
1toT. For sentiment tasks, the 0-th sentence and
the ﬁrst [SEP] token are omitted. For sentence-
level representations, we concatenate the start and
end vectors of each sentence. For instance, the
t-th sentence representation is h= [h;h].
For token-level representations, we use the hidden
vectors directly. The representations are passed to a
linear layer{w,b}to obtain logit for each sentence
s=wh+b.3803Training Both the rationalizer and the predic-
tor in the rationale models are initialized with pre-
trained BERT (Devlin et al., 2019). We predeter-
mine rationale sparsity before ﬁne-tuning based on
the average rationale length in the development set
following previous work (Paranjape et al., 2020;
Guerreiro and Martins, 2021). We set π= 0.4
for FEVER, π= 0.2for MultiRC, π= 0.7for
SQuAD,π= 0.1for Beer, and π= 0.15for Hotel.
The hyperparameter k(for top-kratioanle extrac-
tion) is selected based on the percentage πof the
human annotated rationales in the development set
(following Paranjape et al. (2020)). During evalu-
ation, for each passage k=π×#sentences . We
select the model parameters based on the highest
ﬁne-tuned task performance on the development
set. The models with rationale supervision will se-
lect the same amount of text as their no-supervision
counterparts. The epoch/learning rate/batch size
for the different datasets are described in Table A.2.
Dataset Epoch Learing Rate Batch Size
FEVER 10 5e-5 32
MultiRC 10 5e-5 32
SQuAD 3 1e-5 32
Beer 20 5e-5 64
Hotel 20 5e-5 64
A.3 Qualitative Examples
We provide qualitative examples of the rationale
model predictions for each dataset in Table 8.A.4 Attack Position and Lexical Variation
Figure 4 shows a more ﬁne-grained trend reﬂecting
the sensitivity of AR against inserted attack posi-
tion. As the attack position move from the begin-
ning of the passage towards the end, AR decreases
across all models. With ART training (R6 in §6),
the AR also becomes less sensitive to positions. We
also experimented with various adjectives related
to appearance as the attack and observe the same
trend. For example, when inserting “The carpet
looks really ugly/beautiful.” to the Beer dataset,
VIB performance drops 93.8 →83.1 while FC
drops 93.8 →61.6.38043805