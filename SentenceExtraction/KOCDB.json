[{"text": "Kernelized Offline Contextual Dueling Bandits Viraj Mehta 1 Ojash Neopane 2 Vikramjeet Das 2 Sen Lin Jeff Schneider 1 2 Willie Neiswanger 3 Abstract answering setting"}, {"text": "Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible"}, {"text": "A notable recent example arises in reinforcement learning from human feedback on large language models"}, {"text": "For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive"}, {"text": "In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting"}, {"text": "We give an upper-confidence-bound style algorithm for this setting and prove a regret bound"}, {"text": "We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts"}, {"text": "Recently, these techniques have seen large amounts of attention when popularized through reinforcement learning from human feedback (RLHF)"}, {"text": "RLHF is one of the major techniques for aligning large language models (LLMs) for use as a chat assistant or for other specialized applications after pretraining on a sequence modeling objective"}, {"text": "In these applications, human raters are provided with a prompt and several possible responses taken from the LLM"}, {"text": "They are asked to rank the human responses based on their preferences given the prompt"}, {"text": "This process requires a relatively large number of samples from human raters (tens of thousands) in order for the alignment process to succeed"}, {"text": "This can incur large costs for the data collection"}, {"text": "For more specialized problems than a general chatbot assistant, the feedback required may be impractical or expensive relative to the desired application"}, {"text": "In many applications, the feedback is collected online as the policy is learned"}, {"text": "Under these circumstances, the contexts are typically assumed to be drawn from an (unknown) stationary probabity distribution and the agent\u2019s object is to quickly find a near optimal decision rule"}, {"text": "Currently, in the RLHF setting, the prompts presented to the model in order to sample responses and then to the raters are typically sampled uniformly from a response set intended to be representative of the test-time distribution"}, {"text": "1"}, {"text": "Introduction In many decision making problems in information retrieval, question answering, clinical trials, advertising, and other fields, feedback about the performance of a particular choice is only available in the form of a preference between several options"}, {"text": "Such feedback often takes the form of a user clicking on a link in the wild but can also be collected from labelers in an attempt to understand their underlying preferences and train a system to optimize them"}, {"text": "This is an especially useful method for collecting human feedback because humans are unreliable in giving scalarized feedback compared to the accuracy of their preferences (Ouyang et al., 2022)"}, {"text": "Often, these settings come with some context that can provide information about the associated distribution over preferences"}, {"text": "This could be the search term, some information about a trial participant, or the prompt in a question In such cases, the aforementioned online setting does not allow us to fully take advantage of the problems structure \u2013 we can control which prompts and responses are presented to the human raters for feedback and we are not interested in the performance of the actions chosen during labeling, just the performance of the policy at test time afterwards"}, {"text": "Instead of the standard contextual bandit problem it is then more appropriate to consider the so-called offline contextual bandit (Char et al., 2019) where we are additionally allowed to select the contexts for which we receive feedback"}, {"text": "By leveraging this control over this less restrictive data-generating process, we show how to select contexts and actions in order output policies with stronger optimality guarantees without the need to collect more data"}, {"text": "*Equal contribution 1Robotics Institute, Carnegie Mellon University, USA 2Machine Learning Department, Carnegie Mellon University, USA 3Department of Computer Science, Stanford University, USA"}, {"text": "Correspondence to: Viraj Mehta <virajm@cs.cmu.edu>"}, {"text": "Here, we tackle the special case of pairwise feedback where a pair of actions are compared given a particular context"}, {"text": "Following Xu et al"}, {"text": "(2020), we first reduce the problem The Many Facets of Preference Learning Workshop at the International Conference on Machine Learning (ICML), Honolulu, Hawaii, USA, 2023"}, {"text": "Copyright 2023 by the author(s)"}, {"text": ""}][{"text": "Kernelized Offline Contextual Dueling Bandits of finding the optimal action given pairwise feedback to finding the action that optimizes the Borda function given a particular context"}, {"text": "The Borda function is the probability that for a particular context, a selected action is preferred over another action selected uniformly at random"}, {"text": "We select contexts which maximize the uncertainty over the Borda \u2018value function\u2019 and then select one action optimistically and the other uniformly"}, {"text": "and overall quality"}, {"text": "The results are spectacular, with the 1.3B parameter InstructGPT matching the 175B GPT-3 in performance on a variety of tasks"}, {"text": "Because the focus was ensuring representation both in the inputs to models used in real life and in the human feedback received, the team used 40 labelers and worked with a dataset of more than 100,000 examples"}, {"text": "A related paper from Zhu et al"}, {"text": "(2023) also explores a simplified version of this problem and showed that under the strong assumption of a linear model given a known feature mapping, the policy obtained by optimizing the pessimistic MLE given a fixed dataset is provably optimal for learning in the k-wise comparison context"}, {"text": "Given these strong assumptions, the authors point out that a G-optimal experimental design for online data collection as in Soare et al"}, {"text": "(2014) would be maximally informative"}, {"text": "However, these assumptions are unrealistic and do not represent the methods used in practice as reward model training is usually conducted over all layers of a deep model"}, {"text": "In this work, we show that our method provably achieves (cid:16) L1\u221a suboptimality at most O everyT where in the context space after T iterations with probability 1 \u2212 \u03b4 under the assumption that the Borda function is bounded by B in RKHS norm"}, {"text": "We also demonstrate on a distribution of synthetic problems that it performs well when implemented and outperforms a baseline with uniformly selected contexts and an optimistic policy as well as entirely uniform sampling"}, {"text": "(cid:113) (cid:17)(cid:17) (cid:16) log 1 \u03b4 B + \u03a6T 2"}, {"text": "Related Work Dueling Bandits At the same time, the bandit literature has also explored the effectiveness of comparative feedback (\u201cdueling bandit\u201d) while considering the cost of acquiring such information"}, {"text": "This was first studied by Yue et al"}, {"text": "(2012) in settings where comparative information is relatively easy to extract but absolute rewards (i.e., direct queries) are illdefined and have no absolute scale"}, {"text": "Later, Bengs et al"}, {"text": "(2021) surveyed methods used in the online learning setting, where the trade off with cost of information is most acute, including those used in the online contextual dueling bandit setting by Dud\u00b4\u0131k et al"}, {"text": "(2015)"}, {"text": "These constraints motivate a kernelized approach that can incorporate the nonlinearities in the models used in practice"}, {"text": "Learning from Comparative Feedback There is a rich literature on reinforcement learning from comparative human feedback, including work by F\u00a8urnkranz et al"}, {"text": "(2012), Akour (2014) and Christiano et al"}, {"text": "(2017)"}, {"text": "Many of these works grappled with the heightened need for sample efficiency given the cost of acquiring human feedback"}, {"text": "In particular, Christiano et al"}, {"text": "(2017) made it feasible to use human feedback for deep reinforcement learning by training a reward model that is then used as the target for reinforcement learning"}, {"text": "In their Atari test case, where naive deep RL would have required thousands of hours of gameplay, they were able to achieve superior performance with only 5,500 or several hours of human queries"}, {"text": "More recently, methods of using comparative human feedback have gained prominence as a means of improving the performance of language models"}, {"text": "These methods have been shown to be effective at improving stylistic continuation (Ziegler et al., 2019), text summarization (Stiennon et al., 2020), translation (Kreutzer et al., 2018), semantic parsing (Lawrence & Riezler, 2018), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019)"}, {"text": "However, while effective, incorporating human feedback brought substantial costs"}, {"text": "For example, Stiennon et al"}, {"text": "(2020) achieved significant improvements to baseline, but needed summaries on 123,169 posts from the TL;DR dataset generated by a small team of labelers (more than 21 persons) from Upwork, Scale, and Lionbridge to train"}, {"text": "Offline Contextual Bandit Optimization When there are distinct phases of learning and then deployment, an agent can make maximal use of every example during learning to acquire information that can be exploited once deployed"}, {"text": "Char et al"}, {"text": "(2019) introduce this idea for black-box function approximation by considering a setting where at test time the goal is to perform well on average across a context distribution while during learning the goal is to choose contexts and actions that are most useful for that goal"}, {"text": "Given a reward function for each task, the authors proposed a multi-task version of Thompson sampling during the offline training phase, which allows provable regret bounds in that problem setting"}, {"text": "We extend this setting from cardinal to ordinal rewards as is appropriate for comparative feedback"}, {"text": "This heavy-resource requirement is again reflected even in later, state-of-the-art work"}, {"text": "Ouyang et al"}, {"text": "(2022) focused on using RLHF to improve alignment of the GPT-3 model (at 175B parameters) with human values on a variety of directions, including toxicity, hallucinations, moral opinion, In Li et al"}, {"text": "(2023), the agent queries the states (or contexts, in a bandit setting) where the value function is most uncertain and acts optimistically"}, {"text": "Combined with least-squares value iteration, this method leads to provable polynomial"}][{"text": "Kernelized Offline Contextual Dueling Bandits to the contextual setting as follows, given as fr : X \u00d7 A \u2192 [0, 1] where fr(x, a) = Ea\u2032\u223cU (A) [P (a \u227b a\u2032 | x)], where U (A) is the uniform measure over the action space"}, {"text": "It is clear from the definition that fr and r will have the same maximizers"}, {"text": "sample convergence in the worst-case error of the value function estimate in reinforcement learning in general, and as a corollary the setting from Char et al"}, {"text": "(2019) as a special case"}, {"text": "This sets the foundation that we will adapt to the comparative feedback setting"}, {"text": "We conclude this section by discussing the structural assumptions we place on the reward function as well as the contextual Borda function"}, {"text": "Our first assumption restricts the reward and contextual Borda functions to be \u2018smooth\u2019 in an underlying Reproducing Kernel Hilbert Space (RKHS)"}, {"text": "3"}, {"text": "Problem Setting In this paper, we consider a dueling variant of the so-called offline contextual bandit problem introduced in Char et al"}, {"text": "(2019)"}, {"text": "An instance of this problem is defined by a tuple (X , A, f ) where X denotes the context space, A denotes the action space and f : X \u00d7 A \u00d7 A \u2192 [0, 1] is a preference function so that f (x, a, a\u2032) denotes the probability that the action a is preferred to the action a\u2032 when the underlying context is x"}, {"text": "We will design algorithms that operate under the following interaction protocol, which occurs for T time steps"}, {"text": "During each time step t \u2208 [T ], the agent selects a context xt \u2208 X and a pair of actions at, a\u2032 t \u2208 A and observes a binary random variable Rt \u223c Bern(f (xt, at, a\u2032 t)) which equals one if at is preferred to a\u2032 t) and zero otherwise"}, {"text": "Assumption 3.1"}, {"text": "Let \u03ba denote a Positive Semi-Definite kernel and let H\u03ba denote its associated RKHS"}, {"text": "We assume that \u2225r\u2225\u03ba , \u2225fr\u2225\u03ba \u2264 B, where B is a known constant"}, {"text": "Note that this assumption is different than the standard assumption which only requires that r has bounded RKHS norm"}, {"text": "This is due to the generality of our setting which allows for multiple different link functions"}, {"text": "While this assumption is not ideal, it is difficult to bound the norm of fr given a bound on the norm of r"}, {"text": "We investigate this issue more in Section B where we empirically find that the norm of the Borda function is almost always smaller than the norm of the reward function"}, {"text": "t (denoted at \u227b a\u2032 We assume that the preference function takes the following form Our second assumption relates the reward function to the contextual Borda function"}, {"text": "Assumption 3.2"}, {"text": "Let f \u2217 r (x) = maxa fr(x, a) and r\u2217(x) = maxa r(x, a)"}, {"text": "There exists a constant L1 such that for every x \u2208 X , a \u2208 A we have 1 r (x) \u2212 L1 fr(x, a)"}, {"text": "f (x, a, a\u2032) = \u03c3 (r(x, a) \u2212 r(x, a\u2032)) , (1) where \u03c3 : R \u2192 [0, 1] is the link function and r : X \u00d7 A \u2192 R is the reward function"}, {"text": "Common link functions include the logistic function, which leads to the BradleyTerry-Luce (BTL) model (Bradley & Terry, 1952) as well as the Gaussian CDF (Thurstone, 1927)"}, {"text": "We also place some additional assumptions on the reward function which we discuss at the end of this section"}, {"text": "(r\u2217(x) \u2212 r(x, a)) \u2264 f \u2217 This assumption implies that differences in r will cause a similar magnitude of difference in fr In fact, when \u03c3 is Lipschitz continuous, it is sufficient for the Lipschitz constant of \u03c3 to be at least 1/L1 for this condition to hold"}, {"text": "Our objective within this protocol is to design algorithms that are able to quickly identify policies with small suboptimality"}, {"text": "We define the suboptimality of a policy \u03c0 as 4"}, {"text": "Method and Analysis (cid:18) (cid:19) SubOpt(\u03c0) = sup x\u2208X sup a\u2208A r(x, a) \u2212 r(x, \u03c0(x)) "}, {"text": "(2) At a high level, our approach reduces the dueling feedback problem to contextual optimization over a single action via the contextual Borda function introduced in Section 3"}, {"text": "Once reduced appropriately, we apply techniques adapted from recent work on active exploration in reinforcement learning to construct a sampling rule and policy selection rule which allows us to output a policy with provably small suboptimality"}, {"text": "Broadly, our sampling rule samples contexts at which there is maximum uncertainty over the Borda \u2018value function\u2019 and then compares the optimistic action with an action sampled uniformly from the action set"}, {"text": "We remark that this notion of suboptimality is much stronger than usual notions that look at the expected suboptimality of the final policy when the contexts are sampled from some known distribution"}, {"text": "In contrast, the form of suboptimality we consider here looks at the worst-case context for each policy"}, {"text": "Before discussing the assumptions we place on the reward function, we first introduce a closely related function, called the contextual borda function fr, which generalizes the borda function introduced in by Xu et al"}, {"text": "(2020)"}, {"text": "The Borda function as introduced in Xu et al"}, {"text": "(2020) for dueling-choice optimization is defined as the probability that a particular action a will be preferred over a random action a\u2032 uniformly sampled from the action space"}, {"text": "We generalize this definition 4.1"}, {"text": "Estimating The Contextual Borda Function By design, we can easily estimate the contextual Borda ? \u227b a\u2032 function from data of the form {xt, at t}, where the "}][{"text": "Kernelized Offline Contextual Dueling Bandits Algorithm 1 Borda-AE contexts xt and first actions at are arbitrary and the second actions a\u2032 t are uniformly selected"}, {"text": "In this work, we model the contextual Borda function using standard kernelized ridge regression (KRR) (Rasmussen et al., 2006)"}, {"text": "The key feature of KRR is that besides an estimate of the contextual Borda function after t observations \u00b5t(x, a), we can also estimate the uncertainty over the prediction \u03c3t(x, a) using standard results; under the assumptions in Section 3 and given an value for \u03b2 appropriate chosen for our confidence level, we can bound |fr(x, a) \u2212 \u00b5t(x, a)| \u2264 \u03b2\u03c3t(x, a) with the desired confidence"}, {"text": "1: Input: kernel function \u03ba(\u00b7, \u00b7), exploration parameters \u03b2(r) t , number of inital data n0 ? \u227b a\u2032 i}n0 2: Let Dn0 = {si, ai 3: for t = n0 + 1, "}, {"text": ""}, {"text": ""}, {"text": ", T do 4: 5: 6: i=1 for si, ai, a\u2032 i uniform"}, {"text": "Compute \u00b5t(\u00b7, \u00b7), \u03c3t(\u00b7, \u00b7) using KRR"}, {"text": "Choose xt according to (3)"}, {"text": "Choose at according to (4), a\u2032 Let Dt = Dt\u22121 \u222a {(xt, at t \u223c U (A)"}, {"text": "? \u227b a\u2032 t)}"}, {"text": "7: 8: end for 9: Output a final policy \u02c6\u03c0T according to (5)"}, {"text": "4.2"}, {"text": "Selecting Contexts and Actions Our sampling rule builds on top of the one established in Li et al"}, {"text": "(2023) \u2014put simply the rule is to sample the state with the maximum uncertainty over the value function and then act optimistically"}, {"text": "We will now present our algorithm which highlights how to extend these idea to the dueling setting via the contextual Borda function fr"}, {"text": "For now, we assume that there is a known bonus term \u03b2(r) for all t"}, {"text": "We can then define upper and lower confidence bounds f t r(x, a) = \u00b5t(x, a) \u2212 \u03b2(r) where rA = [r(x)]x\u2208A , \u03f5A \u223c N (0, \u03b72I) and I(X; Y ) = H(X) \u2212 H(X|Y ) is the mutual information"}, {"text": "With this definition, we are now ready to state our result"}, {"text": "Theorem 4.1"}, {"text": "Suppose we run Algorithm 1 with (cid:115) (cid:19) (cid:18) 2 \u03b4 \u03b2(r) t = 2B + 2\u03a6t + 1 + log , (7) t r(x, a) = \u00b5t(x, a) + \u03b2(r) then, with probability at least 1 \u2212 \u03b4, we have that t \u03c3t(x, a) and f t (cid:32) (cid:32) (cid:33)(cid:33) (cid:114) t \u03c3t(x, a)"}, {"text": "Our rule is to sample a context L1\u221a T 1 \u03b4 B + \u03a6T SubOpt(\u02c6\u03c0T ) \u2264 O log "}, {"text": "(8) (cid:18) (cid:19) f t r(x, a) f t r(x, a) \u2212 max a\u2208A xt \u2208 arg maxx\u2208X max a\u2208A "}, {"text": "(3) Proof Sketch"}, {"text": "At a high-level the proof of this result is as follows"}, {"text": "First, we use standard results on KRR to show that our choice of \u03b2(r) guarantees that our confidence bands contain f r(x, a) with high probability simultaneously for all t and x, a \u2208 X \u00d7 A"}, {"text": "Next, we use assumption 3.2 to show that the suboptimality of the pessimistic policy induced by our estimated contextual borda function is small whenever we are able to estimate the contextual borda function well"}, {"text": "Finally, we conclude the proof by showing that our sampling rule indeed allows us to estimate thet contextual borda function well"}, {"text": "Here, we are choosing a context that maximizes the difference between the optimistic \u2018value function\u2019 and the pessimistic \u2018value function\u2019 (both of which require a maximization over actions to compute)"}, {"text": "We then optimistically choose an action by at \u2208 arg maxa\u2208A f t r(xt, a)"}, {"text": "(4) After repeating this process T times, we output a pessimistic policy against the tightest lower bound we can find, which is the maximizer of all our lower bounds through the optimization process"}, {"text": "Put formally we return \u02c6\u03c0T : X \u2192 A such that Concrete Performance Bounds"}, {"text": "To more concretely understand the performance of our algorithm, we instantiate our results for three commonly studied kernels: the linear, squared-exponential"}, {"text": "For both of these settings, the scaling of the information gain is well known (see for example (Srinivas et al., 2010))"}, {"text": "In the linear setting, we have that \u03a6T = d log T leading to a bound of O "}, {"text": "For squared exponential kernels we have \u03a6T = O (cid:0)log(T )d+1(cid:1) leading to a suboptimality bound of O f t r(x, a)"}, {"text": "\u02c6\u03c0T (x) \u2208 arg maxa\u2208A max t\u2264T (5) From these pieces we construct the full algorithm, BordaAE, which we present in Algorithm 1"}, {"text": "(cid:113) (cid:16) L1\u221a (cid:17)(cid:17) (cid:16) 4.3"}, {"text": "Bounding the regret of Borda-AE log 1 \u03b4 dB log T T Before proceeding with our algorithm\u2019s formal guarantees, we first introduce the maximal-information gain which plays an important role in our results"}, {"text": "The maximum information gain over t rounds, denoted \u03a6t, is defined as B log(T )d+1(cid:113) (cid:16) (cid:16) L1\u221a (cid:17)(cid:17) "}, {"text": "log 1 \u03b4 T When compared to existing results for dueling bandits (Xu et al., 2020) as well as standard bandits (Chowdhury & Gopalan, 2017), we see that our suboptimality bounds \u03a6t = max A\u2282X \u00d7A:|A|=t I(rA + \u03f5A; rA), (6) "}][{"text": "Kernelized Offline Contextual Dueling Bandits match, thus showing that our algorithm is able to achieve the same performance under a stronger performance metric"}, {"text": "5"}, {"text": "Experiments In order to assess the validity of our theory we have begun an experimental campaign starting with synthetic experiments that allow us to come as close as possible to the theoretical setting and empirically confirm our results"}, {"text": "To do so, we implemented the regression using the BernoulliGP model provided by GPyTorch (Gardner et al., 2018)"}, {"text": "We use a Mat\u00b4ern kernal with automatic relevance detection with hyperparameters fit via maximum a posteriori optimized by the Adam algorithm (Kingma & Ba, 2014)"}, {"text": "We tested on distributions of synthetic reward functions generated by sampling a random linear combination of Random Fourier Features (Rahimi & Recht, 2007) derived from a squared exponential kernel"}, {"text": "For each sampled reward function r, we used the Bradley-Terry model where p(a \u227b a\u2032 | x) = 1+exp(r(x,a\u2032)\u2212r(x,a)) to generate comparison data"}, {"text": "For each trial we uniformly sampled n0 = 25 datapoints and then selected data to observe until T = 500 total datapoints had been collected according to one of three methods: 1 \u2022 Borda-AE: our method, as described in Section 4"}, {"text": "\u2022 Borda-Uniform: uniform sampling of context and actions"}, {"text": "\u2022 Borda-UCB: uniform sampling of contexts with UCB Figure 1"}, {"text": "Performance of all methods across 10 random functions r with 1D Context and 1D action"}, {"text": "The top plot shows the median regret across contexts and the bottom shows the maximum"}, {"text": "Error regions are standard errors"}, {"text": "actions as in Borda-AE"}, {"text": "This last method reduces to the method presented in Xu et al"}, {"text": "(2020) when na\u00a8\u0131vely generalized to the contextual setting"}, {"text": "All methods have the same test-time behavior of executing the action found by optimizing the pessimistic Borda function estimate for the test context"}, {"text": "By optimizing the ground-truth reward function we were able to approximate the optimal policy and therefore estimate the regret of our policy against it"}, {"text": "We give an example of the progression of our method for 1D context and 1D actions in Figure 2 as well as a comparison against Borda-Uniform and BordaUCB in Figure 1"}, {"text": "One can see that Borda-AE performs best both on median regret and on the maximum regret, which was the metric of interest in our theoretical analysis"}, {"text": "able to in theory and in controlled practical settings achieve promising performance, it has a number of issues that prevent its wider application"}, {"text": "First, we reflect on the fact that the Borda function is a blunt tool: in order to make sampling tractable we must sample a\u2032 uniformly during exploration"}, {"text": "Since we have the possibility of choosing both a and a\u2032 this seems somewhat wasteful of the a\u2032 samples"}, {"text": "In the context of RLHF with language models this is especially grim as uniformly chosen sequences are likely to be gibberish and the obvious preference for plausible sequences vs uniform ones will mean that the data selected via this strategy would be less likely to be useful"}, {"text": "It is clear the method is quickly able to concentrate samples in regions that could plausibly be the optimum and it is similarly clear that the peaks in the acquisition function over contexts are sensible given the mean and uncertainty estimates of fr"}, {"text": "Many of these applications also benefit from the parallel nature of modern hardware and only make sense when presented with large batches of data"}, {"text": "The sequential nature of Borda-AE makes it unsuitable for these applications as well"}, {"text": "With all this in mind, however, we still believe that there is tremendous potential in finding methods for solving these problems, especially with the worst-case style guarantees such as the ones provided here"}, {"text": "We hope to continue the progress as we work towards solving these issues"}, {"text": "6"}, {"text": "Conclusion and Future Work In this work we introduced the offline contextual dueling bandit setting and presented a first efficient algorithm for solving it in the kernelized setting"}, {"text": "Though our method is "}][{"text": "Kernelized Offline Contextual Dueling Bandits Time = 50 Time = 150 Time = 600 Figure 2"}, {"text": "Progress of Borda-AE across 50, 150, and 600 datapoints"}, {"text": "From the top the charts show the ground truth function, the mean of the posterior estimate of fr, the uncertainty function, the estimate of the value function as well as the acquisition function given in (3), and the regret"}, {"text": ""}][{"text": "Kernelized Offline Contextual Dueling Bandits References Kingma, D"}, {"text": "P"}, {"text": "and Ba, J"}, {"text": "Adam: A method for stochastic optimization"}, {"text": "arXiv preprint arXiv:1412.6980, 2014"}, {"text": "Akour, R"}, {"text": "Robust preference learning-based reinforcement learning"}, {"text": "2014"}, {"text": "Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S"}, {"text": "Can neural machine translation be improved with user feedback? 2018"}, {"text": "Bengs, V., Busa-Fekete, R., Mesaoudi-Paul, A"}, {"text": "E., and H\u00a8ullermeier, E"}, {"text": "Preference-based online learning with dueling bandits: A survey, 2021"}, {"text": "Lawrence, C"}, {"text": "and Riezler, S"}, {"text": "Improving a neural semantic parser by counterfactual learning from human bandit feedback"}, {"text": "2018"}, {"text": "Bradley, R"}, {"text": "A"}, {"text": "and Terry, M"}, {"text": "E"}, {"text": "Rank analysis of incomplete block designs: I"}, {"text": "the method of paired comparisons"}, {"text": "Biometrika, 39(3/4):324\u2013345, 1952"}, {"text": "Li, X., Mehta, V., Kirschner, J., Char, I., Neiswanger, W., Schneider, J., Krause, A., and Bogunovic, I"}, {"text": "Near-optimal policy identification in active reinforcement learning"}, {"text": "In The Eleventh International Conference on Learning Representations, 2023"}, {"text": "URL https://openreview"}, {"text": "net/forum?id=3OR2tbtnYC-"}, {"text": "Char, I., Chung, Y., Neiswanger, W., Kandasamy, K., Nelson, A"}, {"text": "O., Boyer, M., Kolemen, E., and Schneider, J"}, {"text": "Offline contextual bayesian optimization"}, {"text": "In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch\u00b4e-Buc, F., Fox, E., and Garnett, R"}, {"text": "(eds.), Advances in Neural Information Processing Systems, volume 32"}, {"text": "Curran Associates, Inc., 2019"}, {"text": "URL https://proceedings.neurips"}, {"text": "cc/paper_files/paper/2019/file/ 7876acb66640bad41f1e1371ef30c180-Paper"}, {"text": "pdf"}, {"text": "Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al"}, {"text": "Training language models to follow instructions with human feedback"}, {"text": "Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022"}, {"text": "Cho, W"}, {"text": "S., Zhang, P., Zhang, Y., Li, X., Galley, M., Brockett, C., Wang, M., and Gao, J"}, {"text": "Towards coherent and cohesive long-form text generation"}, {"text": "2018"}, {"text": "Perez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K"}, {"text": "Finding generalizable evidence by learning to convince q&a models"}, {"text": "2019"}, {"text": "Chowdhury, S"}, {"text": "R"}, {"text": "and Gopalan, A"}, {"text": "On kernelized multiarmed bandits"}, {"text": "In International Conference on Machine Learning, pp"}, {"text": "844\u2013853"}, {"text": "PMLR, 2017"}, {"text": "Rahimi, A"}, {"text": "and Recht, B"}, {"text": "Random features for large-scale kernel machines"}, {"text": "In Platt, J., Koller, D., Singer, Y., and Roweis, S"}, {"text": "(eds.), Advances in Neural Information Processing Systems, volume 20"}, {"text": "Curran Associates, Inc., 2007"}, {"text": "URL https://proceedings.neurips"}, {"text": "cc/paper_files/paper/2007/file/ 013a006f03dbc5392effeb8f18fda755-Paper"}, {"text": "pdf"}, {"text": "Christiano, P., Leike, J., Brown, T"}, {"text": "B., Martic, M., Legg, S., and Amodei, D"}, {"text": "Deep reinformcement In Advances learning from human preferences"}, {"text": "in Neural 30 Systems URL https://papers"}, {"text": "(NIPS 2017), 2017"}, {"text": "nips.cc/paper_files/paper/2017/hash/ d5e2c0adad503c91f91df240d0cd4e49-Abstract"}, {"text": "html"}, {"text": "Information Processing Rasmussen, C"}, {"text": "E., Williams, C"}, {"text": "K., et al"}, {"text": "Gaussian processes for machine learning, volume 1"}, {"text": "Springer, 2006"}, {"text": "Dud\u00b4\u0131k, M., Hofmann, K., Schapire, R"}, {"text": "E., Slivkins, A., and Soare, M., Lazaric, A., and Munos, R"}, {"text": "Best-arm identification in linear bandits"}, {"text": "Advances in Neural Information Processing Systems, 27, 2014"}, {"text": "Zoghi, M"}, {"text": "Contextual dueling bandits, 2015"}, {"text": "F\u00a8urnkranz, J., H\u00a8ullermeier, E., Cheng, W., and Park, S.H"}, {"text": "Preference-based reinforcement learning: a formal framework and a policy iteration algorithm"}, {"text": "Mach Learn, 2012"}, {"text": "Srinivas, N., Krause, A., Kakade, S"}, {"text": "M., and Seeger, M"}, {"text": "Gaussian process optimization in the bandit setting: No regret and experimental design"}, {"text": "International Conference on Machine Learning, 2010"}, {"text": "Gardner, J"}, {"text": "R., Pleiss, G., Bindel, D., Weinberger, K"}, {"text": "Q., and Wilson, A"}, {"text": "G"}, {"text": "Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration"}, {"text": "In Advances in Neural Information Processing Systems, 2018"}, {"text": "Stiennon, N., Ouyang, L., Wu, J., Ziegler, D"}, {"text": "M., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P"}, {"text": "Learning to summarize from human feedback"}, {"text": "2020"}, {"text": "Kandasamy, K., Dasarathy, G., Oliva, J., Schneider, J., and Poczos, B"}, {"text": "Multi-fidelity gaussian process bandit optimisation"}, {"text": "Journal of Artificial Intelligence Research, 66: 151\u2013196, 2019"}, {"text": "Thurstone, L"}, {"text": "L"}, {"text": "The method of paired comparisons for social values"}, {"text": "The Journal of Abnormal and Social Psychology, 21(4):384, 1927"}, {"text": ""}][{"text": "Kernelized Offline Contextual Dueling Bandits Xu, Y., Joshi, A., Singh, A., and Dubrawski, A"}, {"text": "Zeroth order non-convex optimization with dueling-choice bandits"}, {"text": "In Conference on Uncertainty in Artificial Intelligence, pp"}, {"text": "899\u2013908"}, {"text": "PMLR, 2020"}, {"text": "Yue, Y., Broder, J., Kleinberg, R., and Joachims, T"}, {"text": "The k-armed dueling bandits problem"}, {"text": "2012"}, {"text": "Zhu, B., Jiao, J., and Jordan, M"}, {"text": "I"}, {"text": "Principled reinforcement learning with human feedback from pairwise or k-wise comparisons, 2023"}, {"text": "Ziegler, D"}, {"text": "M., Stiennon, N., Wu, J., Brown, T"}, {"text": "B., Radford, A., Amodei, D., Christiano, P., and Irving, G"}, {"text": "Fine-tuning language models from human preferences"}, {"text": "arXiv preprint arXiv:1909.08593, 2019"}, {"text": ""}][{"text": "Kernelized Offline Contextual Dueling Bandits A"}, {"text": "Proof of Theorem 4.1 In this section we will prove our main Theorem, 4.1"}, {"text": "The overall strategy of the proof is to use our Lipschitz assumption on the link function (more precisely, the relative Lipschitzness of the reward r and the Borda function fr) in order to go to the Borda function, which we can directly model from data"}, {"text": "Then, we use our selection criteria as well as confidence bounds taken from Chowdhury & Gopalan (2017) and convergence rates taken from Kandasamy et al"}, {"text": "(2019) in order to complete the argument"}, {"text": "We give these cited results as lemmas in what follows"}, {"text": "In order to attain a particular policy performance with probability 1 \u2212 \u03b4, we must bound the error of the estimates given by our KRR process for a particular confidence level"}, {"text": "In order to do so, we adapt the result from Chowdhury & Gopalan (2017), Theorem 2"}, {"text": "t = 2||fr||\u03ba + (cid:112)2(\u03a6t\u22121(X ) + 1 + log(2/\u03b4))"}, {"text": "Then with probability 1 \u2212 \u03b4/2 we have for all time t Lemma A.1"}, {"text": "Let \u03b2(r) and any point x \u2208 X , |\u00b5t\u22121(x) \u2212 fr(x)| \u2264 \u03b2(r) t \u03c3t\u22121(x)"}, {"text": "This lemma jointly bounds the modeling error over the Borda function for all time t though it introduces a dependence on the RKHS norm of fr"}, {"text": "This dependence is inherited from prior work, but we empirically study the relationship between the RKHS norm of a particular reward function and that of the associated Borda function in Section B"}, {"text": "We also adapt a result from Lemma 8 of Kandasamy et al"}, {"text": "(2019) in order to understand the convergence of our uncertainty function \u03c3t"}, {"text": "Lemma A.2"}, {"text": "Suppose we have n queries (qt)n t=1 taken from X \u00d7 A"}, {"text": "Then the posterior \u03c3t satisfies 2 log(1 + \u03b7\u22122) (cid:88) \u03c32 t\u22121(qt) \u2264 \u03a6n(X \u00d7 A)"}, {"text": "qt Lemma A.2 gives us a handle on how quickly we can expect the uncertainty function to shrink as additional datapoints are observed"}, {"text": "Now that we have lemmas A.1 and A.2 in place, we can proceed to the proof of the main result"}, {"text": "Proof"}, {"text": "In this proof, we condition on the event in Lemma A.1 holding true"}, {"text": "Given that occurence, we can say the following "}][{"text": "Kernelized Offline Contextual Dueling Bandits for every x \u2208 X "}, {"text": "(cid:18) (cid:19) Assumption 3.2 \u2264 max a\u2208A r(x, a) \u2212 r(x, \u02c6\u03c0T (s)) max a\u2208A L1 fr(x, a) \u2212 fr(x, \u02c6\u03c0T (x)) (9) (cid:18) (cid:19) Lemma A.1 \u2264 f t r(x, \u02c6\u03c0T (x)) L1 max a\u2208A fr(x, a) \u2212 max t\u2208[T ] (10) (cid:18) (cid:19) Def"}, {"text": "of \u02c6\u03c0T= L1 f t r(x, a) max a\u2208A fr(x, a) \u2212 max a\u2208A max t\u2208[T ] (11) (cid:18) (cid:19) f t r(x, a) = L1 min t\u2208[T ] max a\u2208A fr(x, a) \u2212 max a\u2208A (12) (cid:18) (cid:19) Lemma A.1 \u2264 f t r(x, a) f t r(x, a) \u2212 max a\u2208A L1 min t\u2208[T ] max a\u2208A (13) (cid:19) (cid:18) Def"}, {"text": "of xt r(xt, a) \u2212 max f t a\u2208A r(xt, a) f t \u2264 L1 min t\u2208[T ] max a\u2208A (14) Def"}, {"text": "of at (cid:17) r(xt, at) (cid:16) r(xt, at) \u2212 f t f t \u2264 L1 min t\u2208[T ] (15) T (cid:88) (cid:17) r(xt, at) (cid:16) L1 T r(xt, at) \u2212 f t f t \u2264 (16) t=1 T (cid:88) L1 T 2\u03b2(r) t \u03c3t(xt, at) = (17) t=1 (cid:118) (cid:117) (cid:117) (cid:116) (cid:33)2 (cid:32) T 2L1\u03b2(r) T T \u03b2(r) t is increasing (cid:88) \u03c3t(xt, at) \u2264 (18) t=1 (cid:118) (cid:117) (cid:117) (cid:116)T T (cid:88) 2L1\u03b2(r) T T Cauchy-Schwarz \u2264 \u03c32 t (xt, at) (19) t=1 2L1\u03b2(r) T\u221a T 2L1\u221a (2B + (cid:112)2(\u03a6t\u22121 + 1 + log(2/\u03b4))) T (cid:33)(cid:33) (cid:32) Lemma A.2 \u2264 (cid:112) C1\u03a6T (20) def of \u03b2(r) T= (cid:112) C1\u03a6T (21) (cid:32) (cid:114) L1\u221a T 1 \u03b4 B + \u03a6T log "}, {"text": "= O (22) B"}, {"text": "RKHS norms of r and fr In order to understand the dependence of our estimation bound on the RKHS norm ||fr||\u03ba, we ran numerical experiments on sampled reward functions"}, {"text": "For a variety of context and action dimensions, we sampled 1000 reward functions as in Section 5 and numerically approximated their RKHS norms"}, {"text": "We also made a Monte-Carlo estimate of the Borda function fr for each of the reward functions sampled and numerically approximated its RKHS norm"}, {"text": "To do this, we uniformly sample 1,000 points xi from the input space, compute the regularized kernel matrix K for this set xi, solve the KRR problem K\u03b1 = f (x) for \u03b1"}, {"text": "Then we compute the quadratic form \u221a \u03b1T K\u03b1 as an estimate of the RKHS norm"}, {"text": "In Table 1, we present the results of comparing the RKHS norms of 1000 reward functions and their associated Borda functions sampled as in Section 5"}, {"text": "A \u2018win\u2019 was counted when the Borda function had smaller RKHS norm and a \u2018loss\u2019 otherwise"}, {"text": "The win margin is the average difference in RKHS norms of the reward and Borda functions, with a positive value when the Borda function was of smaller norm"}, {"text": "It is clear here that in general (though not always) the RKHS norm of "}][{"text": "Kernelized Offline Contextual Dueling Bandits Context Dimension Action Dimension Win Rate Win Margin 0 1 1 3 3 10 1 1 3 1 3 10 0.16 0.89 1 1 1 1 -6.3 5.1 21.4 21.5 38.7 19.6 Table 1"}, {"text": "Comparison of RKHS norms of reward functions and associated Borda functions the Borda function fr for a particular reward function r is smaller than the RKHS norm of the reward function r itself"}, {"text": "This relationship seems to grow stronger as the input dimensionality of the reward function grows larger"}, {"text": ""}]