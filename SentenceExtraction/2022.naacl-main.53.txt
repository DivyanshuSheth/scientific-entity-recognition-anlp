
Ankita Pasad, Felix Wu, Suwon Shon, Karen Livescu, Kyu J. HanASAPPToyota Technological Institute at Chicago
Abstract
Spoken language understanding (SLU) tasks
involve mapping from speech signals to se-
mantic labels. Given the complexity of such
tasks, good performance is expected to re-
quire large labeled datasets, which are difﬁ-
cult to collect for each new task and domain.
However, recent advances in self-supervised
speech representations have made it feasible
to consider learning SLU models with lim-
ited labeled data. In this work, we focus on
low-resource spoken named entity recognition
(NER) and address the question: Beyond self-
supervised pre-training, how can we use exter-
nal speech and/or text data that are not anno-
tated for the task? We consider self-training,
knowledge distillation, and transfer learning
for end-to-end (E2E) and pipeline (speech
recognition followed by text NER) approaches.
We ﬁnd that several of these approaches im-
prove performance in resource-constrained set-
tings beyond the beneﬁts from pre-trained rep-
resentations. Compared to prior work, we ﬁnd
relative improvements in F1 of up to 16%.
While the best baseline model is a pipeline
approach, the best performance using external
data is ultimately achieved by an E2E model.
We provide detailed comparisons and analyses,
developing insights on, for example, the ef-
fects of leveraging external data on (i) different
categories of NER errors and (ii) the switch in
performance trends between pipeline and E2E
models.
1 Introduction
Named entity recognition (NER) is a popular task
in natural language processing. It involves detect-
ing the named entities and their categories from a
text sequence. NER can be used to extract infor-
mation from unstructured data, which can also be
used as features for other tasks like question an-
swering (Chen et al., 2017) and slot ﬁlling for task-
oriented dialogues (Louvan and Magnini, 2018).Figure 1: Improvements in spoken NER with 100 hours
of external data of different types. “Pipeline” refers to
approaches consisting of speech recognition followed
by a text NER model; “E2E” refers to approaches that
directly map from speech to NER-tagged text. The
“Baseline” and “Text NER” numbers are from previ-
ously established baselines (Shon et al., 2022).
Thanks to pre-trained text representations such
as BERT (Devlin et al., 2019), text-based NER has
recently improved greatly (Wang et al., 2021b; Li
et al., 2020b). Spoken NER, on the other hand, is
not as well-studied. It has the added challenges of
continuous-valued and longer input sequences and,
at the same time, provides opportunities to take
advantage of acoustic cues in the input. A recent
study (Shon et al., 2022) shows that there is still
10-20% absolute degradation in the F1 scores of
spoken NER models compared to text-based NER
using gold transcripts (see Figure 1) despite us-
ing large pre-trained speech representation models.
Closing this gap remains a critical challenge.
In this work, we study the potential beneﬁts of us-
ing a variety of external data types: (a) plain speech
audio, (b) plain text, (c) speech with transcripts,
and (d) text-based NER data. We benchmark our
ﬁndings against recently published baselines for
NER on the V oxPopuli dataset of European Par-
liament speech recordings (Shon et al., 2022) and
also introduce baselines of our own. We observe
improvement from leveraging every type of exter-
nal data. Our analysis also quantiﬁes the pros and
cons of the pipeline (speech recognition followed724by text NER) and end-to-end (E2E) approaches.
The key improvements are summarized in Figure 1.
Speciﬁc contributions include:
(i) Unlike previous work, we devote equal effort to
improving both pipeline and E2E approaches.
(ii) We present experiments using various external
data types and modeling approaches.
(iii) Overall, we obtain F1 improvements of up to
16% for the E2E model and 6% for the pipeline
model over previously published baselines, setting
a new state of the art for NER on this dataset.
(iv) We benchmark the advantage of self-
supervised representations (SSR) over a baseline
that uses standard spectrogram features. SSR
gives relative improvements of 36%/31% for
pipeline/E2E models, respectively. To our knowl-
edge, prior work has not directly measured this
improvement over competitive baselines tuned for
the task.
(v) We establish that E2E models outperform
pipeline approaches on this task, given access to
external data, while the baseline models without
the external data have the opposite relationship.
(vi) We provide a detailed analysis of model behav-
ior, including differences in error types between
pipeline and E2E approaches and the reasoning for
the superiority of E2E over pipeline models when
using external data but not in the baseline setting.
2 Related work
2.1 Spoken named entity recognition
Relatively little work has been conducted on spo-
ken NER (Kim and Woodland, 2000; Sudoh et al.,
2006; Parada et al., 2011; Ghannay et al., 2018;
Caubrière et al., 2020; Yadav et al., 2020; Shon
et al., 2022) as compared to the extensively studied
task of NER on text (Nadeau and Sekine, 2007;
Ratinov and Roth, 2009; Yadav and Bethard, 2018;
Li et al., 2020a). While spoken NER is commonly
done through a pipeline approach (Sudoh et al.,
2006; Raymond, 2013; Jannet et al., 2015), there
is rising interest in E2E approaches in the speech
community (Ghannay et al., 2018; Caubrière et al.,
2020; Yadav et al., 2020; Shon et al., 2022). These
two approaches are depicted in Fig. 2.
An early E2E spoken NER model was intro-
duced by Ghannay et al. (2018). The approach
is based on the DeepSpeech2 (Amodei et al.,
2016) architecture, with the addition of special
characters for NER labels around the named en-
tities in the transcription, and is trained withcharacter-level connectionist temporal classiﬁca-
tion (CTC) (Graves et al., 2006). Yadav et al.
(2020) introduced an English speech NER dataset
and proposed an E2E approach similar to Ghannay
et al. (2018). They show that LM fusion improves
the performance of the E2E approach. Caubrière
et al. (2020) provided a detailed comparison be-
tween E2E and pipeline models; however, they fo-
cused on small RNN/CNN models and did not use
state-of-the-art SSR models. All these approaches
use at least 100 hours of annotated data.
These previous efforts have shown that E2E mod-
els can outperform pipeline approaches in a fully
supervised setting. Borgholt et al. (2021) also made
the same observation on a simpliﬁed NER task.
However, these studies do not account for improve-
ments in NLP from self-supervised text represen-
tations for their pipeline counterparts. Shon et al.
(2022) introduced and worked with a low-resource
NER corpus and showed that E2E models still do
not rival pipeline approaches when state-of-the-art
pre-trained models are used.
When using pre-trained representations, E2E
models are at a disadvantage since the pipeline
model also has access to a text model trained on
>50GB of text, in addition to the same speech rep-
resentation model as E2E. This inspires us to study
the beneﬁts of using additional unlabeled data.
We choose to work with the NER-annotated V ox-
Populi corpus (Wang et al., 2021a; Shon et al.,
2022). V oxPopuli consists of naturally spoken
speech, unlike Bastianelli et al. (2020), and is an-
notated manually, unlike Yadav et al. (2020) and
Borgholt et al. (2021) who obtain ground-truth
labels using text model predictions. The SLUE
benchmark (Shon et al., 2022) is aimed at low-
resource SLU and includes annotations for only 15
hours of data; this matches with the goals of our
work, making it an ideal choice to benchmark our
ﬁndings.
2.2 Self-supervised pre-training
There is a long history of using unsupervised
pre-training in NLP to improve performance over
limited-data supervised training on a broad range
of tasks. SSRs have started to make an impact on
speech tasks as well, with the ﬁrst improvements
seen in large-scale ASR with wav2vec (Schneider
et al., 2019). More recently, improvements have
been seen on more tasks with wav2vec 2.0 (Baevski
et al., 2020) and other models (Yang et al., 2021).725
However, it is not yet clear how universal these
pre-trained representations are for speech tasks,
particularly for semantic understanding tasks like
NER. Some pre-trained models achieve impressive
performance across a variety of tasks, including
some understanding tasks (Yang et al., 2021) and
analyses suggest that they contain at least some
word meaning information (Pasad et al., 2021).
However, these pre-trained models have not yet
been tested on a broad range of challenging under-
standing tasks. We believe our work is the ﬁrst to
quantify the improvements from SSR, speciﬁcally
on spoken NER.
2.3 Leveraging external data
Self-training (Scudder, 1965; Yarowsky, 1995;
Riloff, 1996) is a popular approach to improve
supervised models when some additional unanno-
tated data is available. Self-training has been ob-
served to improve ASR (Parthasarathi and Strom,
2019; Xu et al., 2021) and is also complementary
to pre-training (Xu et al., 2021). To the best of our
knowledge, this is the ﬁrst work to introduce it to
spoken NER while also studying its effects on both
E2E and pipeline approaches.
Knowledge distillation is widely used in model
compression research. In this approach, some in-
termediate output from a teacher model is used to
train a smaller student model (Hinton et al., 2014).
In the context of our work, the teacher and student
networks are two different approaches to solving
NER tasks, and the latter is trained on the ﬁnal
output tags of the former.
Transfer learning has been widely employed for
SLU tasks (Lugosch et al., 2019; Jia et al., 2020),
including E2E spoken NER (Ghannay et al., 2018;
Caubrière et al., 2020). Automatic speech recog-nition (ASR) is a typically chosen task for pre-
training a model before ﬁne-tuning it for SLU. This
choice is facilitated by the wider availability of tran-
scribed speech than SLU annotations. Speciﬁcally
for NER, ASR pre-training is expected to help since
the accuracy of decoded texts can directly affect
the ﬁnal NER predictions.
3 Methods
Spoken NER involves detecting the entity phrases
in a spoken utterance along with their tags. The
annotations include the text transcripts for the au-
dio and the entity phrases with their corresponding
tags. Spoken NER, like any other SLU task, is typi-
cally tackled using one of two types of approaches:
(i) Pipeline and (ii) End-to-end (E2E). As shown
in Fig. 2, a pipeline approach decodes speech to
text using ASR and then passes the decoded text
through a text NER module, whereas an E2E sys-
tem directly maps the input speech to the output
task labels. Each approach has its own set of ad-
vantages and shortcomings. Pipeline systems can
enjoy the individual advances from both the speech
and the text research communities, whereas com-
bining two modules increases inference time, and
propagation of ASR errors can have unexpected
detrimental effects on the text NER module perfor-
mance. On the other hand, E2E models directly
optimize a task-speciﬁc objective and tend to have
faster inference. However, such models typically
require a large amount of task-speciﬁc labeled data
to perform well. This can be seen from previous
papers on E2E NER (Yadav et al., 2020; Ghannay
et al., 2018), where using at least 100 hours of
labeled data is typical.7263.1 Baseline models
The baselines we use for E2E and pipeline models
are taken from Shon et al. (2022). Similarly to
previous work (Shon et al., 2022; Ghannay et al.,
2018; Yadav et al., 2020), we formulate E2E NER
as character-level prediction with tag-speciﬁc spe-
cial characters delimiting entity phrases. For ex-
ample, the phrases “irish” and “eu” are tagged as
NORP($) and GPE(%) respectively in “ the $
irish ] system works within a legal and regulatory
policy directive framework dictated by the % eu ] ”.
The E2E NER and ASR modules are initialized
with the wav2vec2.0 base (Baevski et al., 2020) pre-
trained speech representation, while the text NER
module is pre-trained with DeBERTa base (He
et al., 2021). These pre-trained models are then
ﬁne-tuned for ASR/NER after adding a linear layer
on top of the ﬁnal hidden-state output.
Since text transcripts are typically a part of
the NER annotations, we can also train an NER
model that uses the ground-truth text as input. This
text NER model serves roughly as a topline and
is further used in experiments with external data.
The E2E NER and ASR models are trained with
a character-level CTC objective. The text NER
model is trained for token-level classiﬁcation with
cross-entropy loss.
It is expected that using self-supervised represen-
tations gives a signiﬁcant boost in limited labeled
data settings. In order to quantify the beneﬁts of
the pre-trained representations in our setting, we
also report the performance of E2E and pipeline
baselines that are trained from scratch, not utilizing
any pre-trained models.
3.2 Evaluation metrics
Similarly to previous work (Ghannay et al., 2018;
Yadav et al., 2020), we evaluate performance using
micro-averaged F1 scores on an unordered list of
tuples of named entity phrase and tag pairs pre-
dicted for each sentence. An entity prediction is
considered correct if both the entity text and the
entity tag are correct.
Spoken NER introduces an added variability to
the possible model errors due to speech-to-text con-
version. We report word error rate (WER) to evalu-
ate this aspect. WER is the word-level Levenshtein
distance between the ground-truth text and the de-
coded text generated by the model. Additionally, toget an idea of the errors made by the model speciﬁ-
cally on named entities, we also evaluate NE ACC ,
the proportion of entity phrases correctly decoded
in the speech-to-text conversion. An entity phrase
is considered accurate only if all the words in the
phrase are correctly decoded in the right order.
3.3 Utilizing external data
Next, we describe our approaches that use data
external to the task-speciﬁc labeled data to improve
both the pipeline and the E2E models for spoken
NER. We consider four types of external data: (i)
unlabeled speech ( Un-Sp ), (ii) unlabeled text ( Un-
Txt), (iii) transcribed speech ( Sp-Txt ), and (iv) text-
based NER data.
External
data typeMethodTarget
model
Un-Sp SelfTrain-ASR ASR
Un-Txt SelfTrain-txtNER text NER
Sp-Txt Pre-ASR ASR
The majority of techniques we consider involve
labeling the external data with a labeling model
(typically one of the baseline models) to produce
pseudo-labels . The target model is then trained
on these generated pseudo-labels along with the
original labeled NER data. Tables 1 and 2 present
a detailed list of all methods we consider for im-
proving pipeline and E2E models respectively. The
methods we include use the ﬁrst three kinds of
external data listed above. The fourth kind, exter-
nal text-based NER data, is used in experiments
attempting to improve the text NER model; since
it does not succeed (Sec. 5.2.1), this data source
is not explored further for the pipeline and E2E
models.
When the labeling model is the same as the target
model, this is a well-established process called self-
training (Scudder, 1965; Yarowsky, 1995; Riloff,
1996; Xu et al., 2020, 2021). In our setting, a
word-level language model (LM) is used for de-
coding both the ASR and E2E NER models. Shon
et al. (2022) observed that a LM consistently im-
proves performance of all of the baseline models.727External data type Method Labeling model Target model LM for decoding
Un-SpSelfTrain-E2E E2E-NER E2E-NER pLabel 4-gram
Distill-PipelinePipeline-NER
(after SelfTrain-ASR)E2E-NER pLabel 4-gram
Un-Txt Distill-txtNER-lm text NER n/a pLabel 4-gram
Sp-TxtDistill-txtNER text NER E2E-NER pLabel 4-gram
Pre-ASR n/a n/a ftune 4-gram
So we may expect self-training from pseudo-labels
to improve the target models by distilling the LM
information into all model layers.
When the two models are different, we refer to
it as knowledge distillation (Hinton et al., 2014),
where the information is being distilled from the
labeling model to the target model. This ap-
proach enables the target model to learn from the
better-performing labeling model via pseudo-labels.
Among the baseline models, the pipeline performs
better than E2E approaches, presumably since the
former uses strong pre-trained text representations.
So, for instance, distilling from the pipeline (la-
beling model) into the E2E model (target model)
is expected to boost the performance of the E2E
model.
The LMs used for decoding in different ap-
proaches are mentioned in Tab. 2. All the ASR
experiments use language models trained on the
TED-LIUM 3 LM corpus (Hernandez et al., 2018)
as in Shon et al. (2022). The language model used
in baseline E2E NER experiments is trained on
the 15hr ﬁne-tune set ( ftune 4-gram ). The gener-
ated pseudo-labels also provide additional anno-
tated data for LM training, which can be used in
E2E models. These are referred to as plabel 4-
gram ) (for "pseudo-label 4-gram").
Unlabeled speech: The unlabeled speech is
used to improve the ASR module of the pipeline
approach via self-training ( SelfTrain-ASR ).
For improving the E2E model, the improved
pipeline can be used as the labeling model, fol-
lowed by training the E2E model on the generated
pseudo-labels ( Distill-Pipeline ). Alternatively, the
unlabeled audio can be directly used to improve
the E2E model via self-training ( SelfTrain-E2E ).
Unlabeled text: The text NER module in the
pipeline approach is improved by self-training us-
ing the unlabeled text data ( SelfTrain-txtNER ). The
E2E model uses the pseudo labels generated fromthe text NER baseline module on the unlabeled
text to update the LM used for decoding ( Distill-
txtNER-lm ).
Transcribed speech: The pipeline approach
is improved by using the additional transcribed
speech data to improve the ASR module ( Pre-ASR ).
The E2E model uses this updated ASR as an ini-
tialization in a typical transfer learning setup. Al-
ternatively, for paired speech text data, the pseudo-
labels generated from the text NER model can be
paired with audio and used for training the E2E
model, thus distilling information from a stronger
text NER model into it ( Distill-txtNER ).
Text NER data: In addition to improving the
pipeline and E2E models using the approaches
mentioned above, we also look for any possible
improvements in the text NER model by leverag-
ing a larger external annotated text NER corpus.
The DeBERTa base model is ﬁrst ﬁne-tuned on the
larger external corpus, and then further ﬁne-tuned
on the in-domain labeled data. The ﬁrst ﬁne-tuning
step is expected to help avoid shortcomings in per-
formance due to the limited size of the in-domain
labeled data.
This approach is limited by the availability
of external datasets with the same annotation
scheme as the in-domain corpus. We use the
OntoNotes5.0 (Pradhan et al., 2013) corpus, whose
labeling scheme inspired that of V oxPopuli (Shon
et al., 2022). See Tab. 3 for more information on
OntoNotes5.0.
4 Experimental setup
4.1 Dataset
V oxPopuli (Wang et al., 2021a) is a large multilin-
gual speech corpus consisting of European Parlia-
ment event recordings with audio, transcripts, and
timestamps from the ofﬁcial Parliament website.
The English subset of the corpus has 540 hours of728Data split # uttDuration
(hours)# entity
phrases
ﬁne-tune 5k 15 5820
dev 1.7k 5 1862
test 1.8k 5 2006
ext-100h 350k 101N/Aext-500h 177k 508
ext-NER
(ontonotes-
train)66.6k N/A 81.8k
spoken data with text transcripts. Shon et al. (2022)
recently published NE annotations for a 15-hour
subset of the train set and the complete standard
dev set. Test set annotations are not public, but
we obtain test set results by submitting model out-
puts following the SLUE site instructions.For our
experiments with external in-domain data, we use
uniformly sampled 100-hour and 500-hour subsets
of the remainder of the V oxPopuli train set. The
statistics for these splits are reported in Tab. 3. For
more information on NE tags and label distribution,
we direct the reader to the dataset and annotation
papers (Pradhan et al., 2013; Shon et al., 2022).
4.2 Baseline models
We closely follow the setup for E2E and pipeline
baselines in Shon et al. (2022).We use wav2vec
2.0 base (Baevski et al., 2020) and DeBERTa-
base (He et al., 2021) as the unsupervised pre-
trained models, which have 95M and 139M pa-
rameters respectively. The publicly available
wav2vec2.0 base model is pre-trained on 960 hours
of the LibriSpeech audiobooks corpus (Panayotov
et al., 2015).
For baselines that do not use pre-trained rep-
resentations, we utilize the DeepSpeech2 (DS2)
toolkit(Amodei et al., 2016). DS2 ﬁrst converts
audio ﬁles into spectrogram features. The model
processes the spectrogram features through two
2-D convolutional layers followed by ﬁve bidirec-
tional 2048-dim LSTM layers and a softmax layer.The softmax layer outputs the probabilities for a
sequence of characters. The model has 26M pa-
rameters and is trained with SpecAugment data
augmentation (Park et al., 2019) and a character-
level CTC objective. Following Shon et al. (2022),
we train on the ﬁner label set (18 entity tags) and
evaluate on the combined version (7 entity tags).
4.3 Utilizing external data
We use fairseq library (Ott et al., 2019) to ﬁne-
tune wav2vec 2.0 models for the E2E NER and
ASR tasks. We ﬁne-tune the model for 80k (160k)
updates on 100 (500) hours of pseudo-labeled data.
It takes 20 (40) hours (wall clock time) to ﬁne-
tune on 100 (500) hours of data using 8 TITAN
RTX GPUs. We use HuggingFace’s transformers
toolkit (Wolf et al., 2020) for training the text NER
model on pseudo-labels. Detailed conﬁg ﬁles will
be provided in the public codebase.
5 Results
5.1 Baseline models
NER
systemPretrained modelF1Speech Text
Pipeline  DeBERTa-B 52.4
E2E   51.8
Pipeline W2V2-B DeBERTa-B 72.0
E2E W2V2-B  68.1
Text NER  DeBERTa-B 86.0
Results from all the baseline models are reported
in Tab. 4. The models here are trained on the
15hr ﬁne-tune set. We see that self-supervised
pre-training gives a signiﬁcant performance boost
over no pre-training. The text NER model (which
uses ground-truth transcripts) is far better than
the pipeline method, which is better than the E2E
model.
5.2 Leveraging external data
We report F1 scores on the dev set using different
pipeline and E2E approaches in Tables 5 and 6 re-729Ext. data Method 100h 500h
Un-Sp SelfTrain-ASR 73.8 74.4
Un-Txt SelfTrain-txtNER 72.3 70.8
Sp-Txt Pre-ASR 75.6 77.7
Ext. data Method 100h 500h
Un-SpSelfTrain-E2E 70.6 72.1
Distill-Pipeline 76.5 77.5
Un-Txt Distill-txtNER-lm 71.0 71.7
Sp-TxtDistill-txtNER 79.2 82.2
Pre-ASR 70.7 73.2
spectively. Fig. 1 presents key results when using
each external data type for both E2E and pipeline
models. The key ﬁndings are:
(i) Using external data reduces the gap between
spoken NER baselines and text NER.
(ii) With access to either unlabeled speech or tran-
scribed speech, E2E models outperform pipeline
models, whereas, for the baselines, the opposite
holds.
(iii) Using unlabeled text gives the smallest boost
among the three types of external data, and the
pipeline approach performs better in that setting.
A summary of test set results is presented in
Appendix A.1. The results follow the same trend
as on the dev set.
5.2.1 External text NER data
We try to improve the text NER model by using the
OntoNotes5.0 NER corpus (Pradhan et al., 2013).
Fine-tuning DeBERTa-base on OntoNotes5.0 pro-
duces an F1 of 60% on the V oxPopuli dev set. Fine-
tuning it further on V oxPopuli gives F1 86% on the
dev set. Since we do not see any boost over the ex-
isting vanilla approach (86%, see Tab. 4), we retain
the original text NER model using only in-domain
data and do not perform further experiments using
the OntoNotes-ﬁnetuned model.6 Discussion and analysis
The baseline results are not surprising: The limited
labeled data is not enough for the baseline E2E
approach, but the pipeline model can leverage a
strong text representation model, which gives it
an edge. Improvements to these models can be
attributed to either (i) a better speech-to-text con-
version or (ii) a better semantic understanding of
the input content. Next, we use this distinction to
understand the observed improvements from using
external data.
6.1 Improved E2E results
When using external data with the E2E model,
the best performing methods use either (a) exter-
nal unlabeled speech ( Distill-Pipeline ) or (b) tran-
scribed speech ( Distill-txtNER ). The labeling mod-
els have a stronger semantic component than the
E2E baseline in both of these scenarios because
of their strong text NER module. The same can-
not be said for the other competing approaches
for these external data categories, SelfTrain-NER
andPre-ASR , which provide much lower improve-
ments. SelfTrain-NER distills information from the
LM into the model layers, but the n-gram LM is
much less powerful than the transformer-based text
NER module used in Distill-Pipeline . The Pre-ASR
approach has no means to improve the semantic
component in the updated model.
In the presence of unlabeled text data, the modi-
ﬁcation comes from a better 4-gram LM trained on
pseudo-labels. Note that the baseline E2E model
parameters do not change, unlike when using the
other two types of external data. This can explain
why this approach only has a small improvement
over the baseline.
6.2 Improved pipeline results
The baseline pipeline model already takes advan-
tage of the text NER module, which leaves little
room for improvement in the semantic understand-
ing component. Speciﬁcally, using unlabeled text
data to improve the text NER module ( SelfTrain-
txtNER ) gives a small boost of 0.4%. For compar-
ison, note that the improvement from using unla-
beled speech is 2.5% over baseline. So, the hope
with pipeline models is for the external data to
improve the speech-to-text conversion, which can
then help reduce error propagation between the
independent pipeline modules.7306.3 Amount of external data
Almost all experiments produce a larger improve-
ment when using 500 hours of external data than
100 hours. Only SelfTrain-txtNER has a reverse
trend (see Tab. 5). The higher amount of external
data naturally increases the fraction of noisy data
that the target model is trained on, and that may
lead to a poorer model. We hypothesize that meth-
ods for balancing between the effects of manually
annotated and pseudo-labeled examples could help
tackle this issue (Park et al., 2020). However, we
leave an in-depth investigation of this phenomenon
to future work.
6.4 Error analysis
For analysis, we choose the best-performing mod-
els within each category.
Fig. 3 presents the NE accuracy and word error
rates (WER). We strip off the tag-speciﬁc special
character tokens when evaluating WER for the E2ENER models. Note that we report 100−WER so
that higher is better in both plots. We observe that
the ASR used in pipeline models typically performs
better than the speech-to-text conversion of E2E
models, even when the former has a poorer F1
(Fig. 1). This may lead us to hypothesize that the
E2E model recognizes NE words better while doing
worse for other words. However, this hypothesis is
not supported by the NE-ACC results (Fig. 3).
Next, we look at the breakdown of F1 into pre-
cision and recall (Fig. 4). We see that pipeline
models have worse precision, thus suggesting that
these suffer from a higher false-positive rate than
the E2E models. This explains why NE-ACC is not
predictive of F1; the former can inform us about
errors due to false negatives, but not false positives.
6.4.1 Error categories
For a more detailed understanding of model behav-
ior, we categorize the NER errors into an exhaus-
tive list of types (details in Appendix A.2). We
focus on four major categories showing noteworthy
differences between pipeline and E2E approaches.
We provide this analysis for the baselines, Distill-
Pipeline , and SelfTrain-ASR models using external
unlabeled speech data. The trends and observa-
tions presented here are consistent with the other
two external data types.
The major error categories, along with examples,
are presented in Fig. 5. We observe that:
(i) False detections are 1.5 times more common in
pipeline models than in E2E models, as expected
based on the lower precision for the former. This
happens even when the falsely detected text is not
a speech-to-text conversion error.
(ii) Over-detections are 3.5 to 4 times more com-
mon in the pipeline models even when the entity
phrase is decoded correctly.
(iii) Missed detections for the E2E Distill-Pipeline
model are drastically reduced compared to the E2E
baseline. Missed detections refer to cases where the
entity phrases are correctly transcribed but are not
labeled as named entities. The improvement here
therefore suggests that Distill-Pipeline improves
the understanding capability of the E2E model, in
addition to its speech-to-text capability. Also, note
that the pipeline model does not enjoy the same
beneﬁt from unlabeled speech since this only in-
volves self-training (instead of knowledge distilla-
tion from a much richer model for E2E).
Overall, the pipeline models suffer dispropor-
tionately from false positives. This seems to stem731
from the text NER model, which has even higher
over-detection and false detection rates than the
pipeline baseline models (Fig. 5). The reasons
behind this difference between E2E and pipeline
models need further investigation.
7 Conclusion
We have explored various ways to use different
external data types that improve both pipeline
and E2E methods for spoken NER. The best-
performing model when using external data is an
E2E approach. This is one of the few results in the
literature thus far showing better performance for
E2E over pipeline methods that use state-of-the-art
modules for spoken language understanding. We
develop some insights into this difference; we no-
tice that pipeline models are adversely affected by
false positives and that leveraging external data im-
proves the semantic understanding capability of the
E2E models.
We hope that our work provides guiding prin-
ciples for researchers working on SLU tasks in
similar low-resource domains when some form of
external data is abundant. This work also leaves
some interesting research questions for future work.
For example, we see minor improvements between
100h and 500h of external data (see Tab. 5 and 6),which suggests the question: What is the smallest
amount of external data needed to obtain signiﬁ-
cant improvements in NER performance? Addi-
tionally, one preliminary experiment with external,
out-of-domain text NER data (OntoNotes 5.0) fails
to improve the text NER performance, suggest-
ing the challenges of dealing with out-of-domain
datasets. The scenario where we have access to
out-of-domain external data is common but chal-
lenging, and warrants further study. From the mod-
eling perspective, better ﬁne-tuning strategies for
wav2vec2.0 in low supervision settings have been
proposed for ASR (Pasad et al., 2021); it would
be interesting to explore how these ﬁndings may
transfer to an SLU task.
Acknowledgements
We thank Yoav Artzi for his feedback and inputs
to strengthen our experimental setup and analysis.
We also thank the anonymous reviewers for the
detailed comments and suggestions.
References732733734A Appendix
A.1 Results on the test set
We obtain test set results for our best-performing
models, by submitting model outputs following the
SLUE instructions.. These results are presented
in Fig. 6. We observe similar trends as on the dev
set (see Fig. 1).
We can see from the precision and recall scores
in Fig. 7, that our analytical conclusions about the
pipeline model performing poorly due to false pos-
itives are consistent across these two splits.
A.2 Error categories
Fig. 8 illustrates via a ﬂowchart our process of as-
signing the tuples in ground-truth and predicted
outputs into different error categories. Tab. 7
presents examples for the four categories discussed
in Sec 6.4. These are examples from the dev set,
using the Distill-Pipeline E2E model trained on
100 hours of data.735Error categoryOutputs from E2E model
GT Predicted
Correct ASR,
over detectionand this means that you look
and tell us honestly what does it
mean if you start @ three years ]
later
[(‘WHEN’, ‘three years’)]this means that you look and
tell us honestly what does it mean if
you start @ three years later ]
[(‘WHEN’, ‘three years later’)]
Correct ASR,
missed detectionthe situation in the % drc ] is indeed
terrible and it has been this way for
quite a while and i am deeply
concerned about the handling of the
current issue with regard to the % kasai ]
province
[(‘PLACE’, ‘drc’), (‘PLACE’, ‘kasai’)]the situation in the drc is indeed
terrible and it has been this way for
for quite a while and i am deeply
concerned about the handling of
the current issue with regard to
the a province
[]
Correct ASR,
false detectionand yet @ one month ] after we adopted
our compromise the council did not
put it on the agenda did not even present
it i used this time to talk to the
member states and the presidencies
[(‘WHEN’, ‘one month’)]still @ one month ] after we voted
a compromise the ‘ council ] did
not put it on the agenda did
not even present i use this time
to talk with the member states and
the presidency
[(‘WHEN’, ‘one month’),
(‘ORG’, ‘council’)]
Incorrect ASR,
false detectionit has nothing to do with religion
but it has all to do with patriarchy
[]it has nothing to do with religion
but it has all to do with % turkey ]
[(‘PLACE’, ‘turkey’)]736737