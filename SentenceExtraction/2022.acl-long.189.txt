
Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, and Anna Rumshisky
Department of Computer Science
University of Massachusetts Lowell
{skul,okovalev,nshivagu,arum}@cs.uml.edu
Abstract
Solving crossword puzzles requires diverse
reasoning capabilities, access to a vast amount
of knowledge about language and the world,
and the ability to satisfy the constraints im-
posed by the structure of the puzzle. In this
work, we introduce solving crossword puz-
zles as a new natural language understand-
ing task. We release a corpus of crossword
puzzles collected from the New York Times
daily crossword spanning 25 years and com-
prised of a total of around nine thousand puz-
zles. These puzzles include a diverse set of
clues: historic, factual, word meaning, syn-
onyms/antonyms, ﬁll-in-the-blank, abbrevia-
tions, preﬁxes/sufﬁxes, wordplay, and cross-
lingual, as well as clues that depend on the
answers to other clues. We separately release
the clue-answer pairs from these puzzles as
an open-domain question answering dataset
containing over half a million unique clue-
answer pairs. For the question answering task,
our baselines include several sequence-to-
sequence and retrieval-based generative mod-
els. We also introduce a non-parametric con-
straint satisfaction baseline for solving the en-
tire crossword puzzle. Finally, we propose an
evaluation framework which consists of sev-
eral complementary performance metrics.
1 Introduction
Recent breakthroughs in NLP established high stan-
dards for the performance of machine learning
methods across a variety of tasks. However, even
state-of-the-art models demonstrate fragility (Wal-
lace et al., 2019) and exhibit sensitivity to shallow
data patterns (McCoy et al., 2019; Zellers et al.,
2019; Jin et al., 2020; Si et al., 2019; Sugawara
et al., 2020; Yogatama et al., 2019; Niven and Kao,
2019). This has led to a growing demand for suc-
cessively more challenging tasks.
One of the important tasks in natural language
understanding is question answering (QA), with
many recent datasets created to address differentdifferent aspects of this task (Yang et al., 2018;
Rajpurkar et al., 2016; Kwiatkowski et al., 2019a;
Zellers et al., 2019; Dua et al., 2019; Rogers et al.,
2021). There are two main forms of question an-
swering (QA): extractive QA and open-domain QA.
In extractive QA, a passage that answers the ques-
tion is provided as input to the system along with
the question. In open-domain QA, only the ques-
tion is provided as input, and the answer must be
generated either through memorized knowledge or
via some form of explicit information retrieval over
a large text collection which may contain answers.
The task of answering clues in a crossword is a
form of open-domain question answering. Once a
human or an open-domain QA system generates a
few possible answer candidates for each clue, one
of these candidates may form the correct answer to
a word slot in the crossword grid, if the candidate
meets the constraints of the crossword grid.
Solving a crossword puzzle is therefore a chal-
lenging task which requires (1) ﬁnding answers to
a variety of clues that require extensive language
and world knowledge, and (2) the ability to pro-
duce answer strings that meet the constraints of the
crossword grid, including length of word slots and
character overlap with other answers in the puzzle.
Our contributions in this work are as follows:
•We introduce a new natural language under-
standing task of solving crossword puzzles,
along with a dataset of New York Times cross-
words from Dec. 1, 1993 to Dec. 31, 2018.
•We propose an evaluation framework which
consists of several complementary perfor-
mance metrics.
•We release the collection of clue-answer pairs
as a new open-domain QA dataset.
•We provide baselines for the proposed cross-
word task and the new QA task, including
several sequence-to-sequence and retrieval-
augmented generative Transformer models,
with a constraint satisfaction crossword solver.2648
.
2 Related Work
Our work is in line with open-domain QA bench-
marks. Examples of such tasks include datasets
where each question can be answered using in-
formation contained in a relevant Wikipedia arti-
cle (Yang et al., 2015; Kwiatkowski et al., 2019a;
Yang et al., 2018). Several QA tasks have been
designed to require multi-hop reasoning over struc-
tured knowledge bases (Berant et al., 2013; Bordes
et al., 2015). The main limitation of such datasets is
that their question types are mostly factual. Cross-
word clues differ from these efforts in that they
combine a variety of different reasoning types.
Another line of research that is relevant to our
work explores the problem of solving Sudoku puz-
zles since it is also a constraint satisfaction problem.
Most sudoku puzzles can be efﬁciently solved by al-
gorithms that take advantage of the ﬁxed input size
and do not rely on machine learning methods (Si-
monis, 2005). The machine learning attempts for
solving Sudoku puzzles have been inspired by con-
volutional (Mehta, 2021) and recurrent relational
networks (Palm et al., 2017). Unlike Sudoku, how-
ever, where the grids have the same structure, shape
and constraints, crossword puzzles have arbitrary
shape and internal structure and rely on answers to
natural language questions that require reasoning
over different kinds of world knowledge.Several previous studies have treated crossword
puzzle solving as a constraint satisfaction problem
(CSP) (Littman et al., 2002; Ernandes et al., 2005;
Ginsberg, 2011). Littman et al. (2002)’s Proverb
system incorporates a variety of information re-
trieval modules to generate candidate answers. The
Database module searches a large database of his-
torical clue-answer pairs to retrieve the answer can-
didates. They ﬁnd very poor crossword-solving per-
formance in ablation experiments where they limit
their answer candidate generator modules to not use
historical clue-answer databases. WebCrow (Ernan-
des et al., 2005) builds upon Proverb and makes
improvements to the database retriever module aug-
mented with a new web module which searches the
web for snippets that may contain answers. It al-
lows partial matching to retrieve clues-answer pairs
in the historical database that do not perfectly over-
lap with the query clue. Dr. Fill system proposed
by Ginsberg (2011) treats each crossword puzzle as
a singly-weighted CSP. Similarly to prior work, Dr.
Fill relies on a large set of historical clue-answer
pairs (up to 5M) collected over multiple years from
the past puzzles by applying direct lookup and a
variety of heuristics. One common design aspect
of all these solvers is to generate answer candi-
dates independently from the crossword structure
and later use a separate puzzle solver to ﬁll in the
actual grid. In our work, we partition the task of2649crossword solving similarly.
Barlacchi et al. (2014) and Severyn et al. (2015)
observe that the most important source of candi-
date answers for a given clue is a large database
of historical clue-answer pairs and introduce meth-
ods to better search these databases. Barlacchi
et al. (2014) apply a BM25 retrieval model to gen-
erate clue lists similar to the query clue from his-
torical clue-answer database, where the generated
clues get further reﬁned through application of re-
ranking models. Severyn et al. (2015) introduce a
distributional neural network to compute similari-
ties between clues trained over a large scale dataset
of clues that they introduce.
In contrast to the previous work, our goal in
this work is to motivate solver systems to gener-
ate answers organically, just like a human might,
rather than obtain answers via the lookup in his-
torical clue-answer databases. The answers could
be generated either from memory of having read
something relevant, using world knowledge and
language understanding, or by searching encyclo-
pedic sources such as Wikipedia or a dictionary
with relevant queries.
3 Task and Dataset
For the purposes of our task, crosswords are deﬁned
as word puzzles with a given rectangular grid of
white- and black-shaded squares. The goal is to
ﬁll the white squares with letters, forming words
or phrases by solving textual clues which lead to
the answers. The answer words and phrases are
placed in the grid from left to right ("Across") and
from top to bottom ("Down"). The shaded squares
are used to separate the words or phrases. Usually,
the white spaces and punctuation are removed from
the answer phrases. A sample crossword puzzle
is given in Figure 1. Note that the answers can
include named entities and abbreviations, and at
times require the exact grammatical form, such as
the correct verb tense or the plural noun.
Solving a crossword puzzle is a complex task
that requires generating the right answer candi-
dates and selecting those that satisfy the puzzle
constraints. Similar to prior work, we divide the
task of solving a crossword puzzle into two sub-
tasks, to be evaluated separately. The ﬁrst subtask
can be viewed as a question answering task, where
a system is trained to generate a set of candidate an-
swers for a given clue without taking into account
any interdependencies between answers. The sec-ond subtask involves solving the entire crossword
puzzle, i.e., ﬁlling out the crossword grid with a
subset of candidate answers generated in the previ-
ous step.
The two tasks could be solved separately or in
an end-to-end fashion. In contrast to prior work
(Ernandes et al., 2005; Ginsberg, 2011), our clue-
answer data is linked directly with our puzzle-
solving data, so no data leakage is possible between
the QA training data and the crossword-solving test
data. In the present work, we propose a separate
solver for each task. We provide details on the chal-
lenges of implementing an end-to-end solver in the
discussion section.
3.1 NYT Crossword Collection
Our dataset is sourced from the New York Times,
which has been featuring a daily crossword puzzle
since 1942. We worked with daily puzzles in the
date range from December 1, 1993 through Decem-
ber 31, 2018 inclusive. All the crossword puzzles
in our corpus are available to play through the New
York Times games website. We release two sepa-
rate speciﬁcations of the dataset corresponding to
the subtasks described above: the NYT Crossword
Puzzle dataset and the NYT Clue-Answer dataset.
There are a few details that are speciﬁc to the
NYT daily crossword. First, the clue and the an-
swer must agree in tense, part of speech, and even
language, so that the clue and answer could easily
be substituted for each other in a sentence. Second,
abbreviated clues indicate abbreviated answers.
Further, clues that end in a question mark indicate
a play on words in the clue or the answer. There are
also a lot of short words that appear in crosswords
much more often than in real life. These 3- and
4-letter words, referred to as crosswordese, can be
very helpful in solving the puzzles. Finally, every
Sunday through Thursday NYT crossword puzzle
has a theme, something that unites the puzzle’s
longest answers. Theme answers are always found
in symmetrical places in the grid.
Crossword Puzzle Dataset. The dataset consists
of 9152 puzzles, split into the training, validation,
and test subsets in the 80/10/10 ratio which give us
7293/922/941 puzzles in each set. We removed the
total of 50/61 special puzzles from the validation2650and test splits, respectively, because they used non-
standard rules for ﬁlling in the answers, such as
L-shaped word slots or allowing cells to be ﬁlled
with multiple characters (called rebus entries).
Most NYT crossword grids have a square shape
of1515cells, with the exception of Sunday-
released crosswords being 2121cells. Other
shapes combined account for less than 3%of the
data. The vast majority of both clues and answers
are short, with over 76% of clues consisting of a
single word. For traditional sequence-to-sequence
modeling such conciseness imposes an additional
challenge, as there is very little context provided
to the model. In most puzzles, over 80% of the
grid cells are ﬁlled and every character is an inter-
section of two answers. Such high answer inter-
dependency suggests a high cost of answer mispre-
diction, as errors affect a larger number of intersect-
ing words. More detailed statistics on the dataset
are given in Table 1.
Clue-Answer Dataset. We generate an open-
domain question answering dataset consisting
solely of clue-answer pairs from the respective
splits of the Crossword Puzzle dataset described
above (including the special puzzles). Within each
of the splits, we only keep unique clue-answer pairs
and remove all duplicates. However, certain clues
may still be shared between the puzzles contained
in different splits. We therefore remove from the
training data the clue-answer pairs which are found
in the test or validation data. This ensures that the
model can not trivially recall the answers to the
overlapping clues while predicting for the test and
validation splits.
This produces the total of 578k clue-answer
pairs, with 433k/72k/72k examples in the
train/validation/test splits, respectively. Since cer-
tain answers consist of phrases and multiple words
that are merged into a single string (such as "VERY-
FAST"), we further postprocess the answers by
splitting the strings into individual words using a
dictionary. Out of all the possible word splits of a
given string we pick the one that has the smallest
number of words. If there are multiple solutions,
we select the split with the highest average word
frequency. Examples of a variety of clues found in
this dataset are given in the following section.
3.2 Clue types
To provide more insight into the diversity of the
clue types and the complexity of the task, we cate-gorize all the clues into multiple classes, which we
describe below.
Factual. Clues that encode encyclopedic knowl-
edge and typically can be answered using resources
such as Wikipedia (e.g. Clue: South Carolina State
tree, Answer: PALMETTO ). This type of clue is
the closest to the questions found in open-domain
QA datasets. Note that the facts required to solve
some of the clues implicitly depend on the date
when a given crossword was released. For instance,
the clue " President of Brazil " has a time-dependent
answer.
Historical. Clues that require the knowledge of
historical facts and temporal relations between
events. (e.g. Clue: Automobile pioneer, Answer:
BENZ ).
Word meaning. Clues that exploit general vo-
cabulary knowledge and can typically be resolved
using a dictionary. (e.g. Clue: Opposing sides,
Answer: FOES ).
Synonyms/Antonyms. Clues that focus on para-
phrasing and synonymy relations (e.g. Clue: Prog-
nosticators, Answer: SEERS ). In most cases, such
clues can be solved with a thesaurus.
Fill in the blank. Clues formulated as a cloze
task (e.g. Clue: Magna Cum __, Answer: LAUDE ).
Fill-in-the-blank clues are expected to be easy to
solve for the models trained with the masked lan-
guage modeling objective (Devlin et al., 2019).
Abbreviations. Clues answered with acronyms
(e.g. Clue: (Abbr.) Old Communist state, Answer:
USSR ). Abbreviation clues are marked with " Abbr. "
label.
Preﬁx/Sufﬁx. Clues that suggest the answer is a
sufﬁx or preﬁx. (e.g. Clue: Sufﬁx with mountain,
Answer: EER )
Wordplay. Clues that rely on wordplay, ana-
grams, or puns / pronunciation similarities (e.g.
Clue: Consider an imaginary animal, Answer:
BEAR IN MIND ). In a lot of cases, wordplay clues
involve jokes and exploit different possible mean-
ings and contexts for the same word.
Cross-lingual. Clues that either explicitly use
words from other languages, or imply a speciﬁc
language-dependent form of the answer. (e.g. Clue:
Sunrise dirección, Answer: ESTE ).2651
Clues dependent on other clues. Clues the an-
swer to which can be provided only after a differ-
ent clue has been solved (e.g. Clue: Last words of
45 Across ). Although rare, this category of clues
suggests that the entire puzzle has to be solved in
certain order.
To understand the distribution of these classes,
we randomly selected 1000 examples from the test
split of the data and manually annotated them. Fig-
ure 2 illustrates the class distribution of the an-
notated examples, showing that the Factual class
covers a little over a third of all examples. The
synonyms/antonyms, word meaning and wordplay
classes taken together comprise 50% of the data.
The remaining 20% are taken by ﬁll-in-the-blank
and historical clues, as well as the low-frequency
classes (comprising less than or around 1%), which
include abbreviation, dependent, preﬁx/sufﬁx and
cross-lingual clues. We illustrate each one of these
classes in the Figure 1.
3.3 Evaluation metrics
In this section, we describe the performance met-
rics we introduce for the two subtasks.
Clue-Answer Task. For the clue-answer task,
we use the following metrics:
•Exact Match (EM) . Model output matches
the ground-truth answer exactly.
•Contains (In) . Model output contains the
ground-truth answer as a contiguous substring
Since the ground-truth answers do not contain dia-
critics, accents, punctuation and whitespace char-
acters, we also consider normalized versions of the
above metrics, in which these are stripped from the
model output prior to computing the metric. We
will refer to them as EM andIn,
We report these metrics for top- kpredictions,
where kvaries from 1 to 20.
Crossword Puzzle Task. To evaluate the perfor-
mance of the crossword puzzle solver, we propose
to compute the following two metrics:
•Character Accuracy (Acc). Percentage
of characters in the predicted crossword solu-
tion that match the ground-truth solution.
•Word Accuracy (Acc). Percentage of
words in the predicted crossword solution that
match the ground-truth solution.
Since the clue-answering system might not be
able to generate the right answers for some of the2652clues, it may only be possible to produce a partial
solution to a puzzle. The crossword puzzle solver
will fail to produce a solution when the answer
candidate list for a clue does not contain the cor-
rect answer. To prevent this from happening, the
character cells which belong to that clue’s answer
must be removed from the puzzle grid, unless the
characters are shared by other clues. We propose
two additional metrics to track what percentage of
the puzzle needs to be redacted to produce a partial
solution:
•Word Removal (Rem). % of words that
need to be removed from the puzzle to pro-
duce a partial solution.
•Character Removal (Rem). % of char-
acters that need to be removed from the puzzle
grid to produce a partial solution.
The motivation for introducing the removal met-
rics is to indicate the amount of constraint relax-
ation. For instance, a completely relaxed puzzle
grid, where many character cells have been re-
moved, such that the grid has no word intersection
constraints left, could be considered "solved" by
selecting any candidates from the answer candidate
lists at random. However, this solution will mostly
be incorrect when compared to the gold puzzle solu-
tion. As the word and character removal percentage
increases, the potential for correctly solving the re-
maining puzzle is expected to decrease, since the
under-constrained answer cells in the grid can be
incorrectly ﬁlled by other candidates (which may
not be the right answers). The removal metrics are
thus complementary to word and character level
accuracy.
4 Baselines
Our baseline approach is a two-step solution that
treats each subtask separately. We ﬁrst develop
a set of baseline systems that solve the question
answering problem, ignoring the grid-imposed an-
swer interdependencies. We use seq-to-seq and
retrieval-augmented Transformer baselines for this
subtask. We feed generated answer candidates to
a crossword solver in order to complete the puzzle
and evaluate the produced puzzle solutions.
4.1 Clue-Answer Task Baselines
Sequence-to-sequence baselines. We ﬁne-tune
two sequence-to-sequence models on the clue-
answer training data. We select two widely known
models, BART (Lewis et al., 2019) and T5 (Raffelet al., 2019), which achieved state-of-the-art results
on a set of generative tasks, including speciﬁcally
abstractive QA involving commonsense and multi-
hop reasoning (Fan et al., 2019; Khashabi et al.,
2018; Zhang et al., 2018).
We train both models for 8 epochs with the learn-
ing rate of 510, and a batch size of 60.
Retrieval-augmented generation. T5 and
BART store world knowledge implicitly in their
parameters and are known to hallucinate facts
(Maynez et al., 2020). Recently, a new method
called retrieval-augmented generation (RAG)
(Lewis et al., 2020) has been introduced for open-
domain question answering. This method involves
a Transformer encoder to encode the question and
a decoder to generate the answer (Vaswani et al.,
2017), but the encoded query is supplemented
with relevant excerpts retrieved from an external
textual corpus via Maximum Inner Product Search
(MIPS); the entire neural network is trained
end-to-end. Due to a built-in retrieval mechanism
for performing a soft search over a large collection
of external documents, such systems are capable of
producing stronger results on knowledge-intensive
open-domain question answering tasks than the
vanilla sequence-to-sequence generative models
and are more factually accurate (Shuster et al.,
2021). Motivated by this, we train RAG models
to extract knowledge from two separate external
sources of knowledge:
(a)RAG-wiki uses a full Wikipedia dump from
December 2018. Following existing work
Lewis et al. (2020); Karpukhin et al. (2020);
Lee et al. (2019), each Wikipedia article is
split into disjoint 100-word chunks, resulting
in a total of 21M passages.
(b)RAG-dict uses several English dictionaries
and thesauri sources, including Wiktionary,
Merriam-Webster, and Google’s English dic-
tionary by Oxford Languages.
For both of these models, we use the retriever em-
beddings pretrained on the Natural Questions cor-
pus Kwiatkowski et al. (2019b) in order to prime
the MIPS retrieval to return meaningful entries
(Lewis et al., 2020). We train with a batch size2653
of 8, label smoothing set to 0.1, dropout probability
of 0.1, weight decay rate of 0.001, and a learning
rate of 310for 8 epochs.
4.2 Crossword Puzzle Task
A crossword puzzle can be cast as an instance of
a satisﬁability problem, and its solution represents
a particular character assignment so that all the
constraints of the puzzle are met. Under such for-
mulation, three main conditions have to be satisﬁed:
(1) the answer candidates for every clue must come
from a set of words that answer the question, (2)
they must have the exact length speciﬁed by the
corresponding grid entry, and (3) for every pair of
words that intersect in the puzzle grid, acceptable
word assignments must have the same character at
the intersection offset.
This class of problems can be modelled through
Satisﬁability Modulo Theories (SMT). SMT is a
generalization of Boolean Satisﬁability problem
(SAT) in which some of the binary variables are
replaced by ﬁrst-order logic predicates over a set of
non-binary variables. In the case of crosswords, a
variable represents one character in the crossword
grid which can be assigned a single letter of the En-
glish alphabet and 0 through 9 digit values. This is
further subject to the constraints mentioned above
which can be formulated with the equality operator
and Boolean logical operators: AND andOR. For
example, a word slot of length 3 where the candi-
date answers are "ESC", "DEL" or "CMD" can be
formalised as:
To solve the entire crossword puzzle, we use the
formulation that treats this as an SMT problem. Wemodify an open source implementationof this for-
mulation based on Z3 SMT solver (de Moura and
Bjørner, 2008). The answer length and intersection
constraints are imposed on the variable assignment,
as speciﬁed by the input crossword grid.
We take the top- kpredictions from our baseline
models and for each prediction, select all possible
substrings of required length as answer candidates.
For simplicity, we exclude from our consideration
all the crosswords with a single cell containing
more than one English letter in it.
Our current baseline constraint satisfaction
solver is limited in that it simply returns "not-
satisﬁed" ( nosat ) for a puzzle where no valid
solution exists, that is, when allthe hard constraints
of the puzzle are not met by the inputs. Since the
candidate lists for certain clues might not meet all
the constraints, this results in a nosat solution for
almost all crossword puzzles, and we are not able
to extract partial solutions. To bypass this issue
and produce partial solutions, we pre-ﬁlter each
clue with an oracle that only allows those clues
into the SMT solver for which the actual answer is
available as one of the candidates.
5 Results
5.1 Clue-Answer Task
In Table 2 we report the Top-1, Top-10 and Top-20
match accuracies for the four evaluation metrics
deﬁned in Section 3.3.
Our results (Table 2) suggest a high difﬁculty
of the clue-answer dataset, with the best achieved
accuracy metric staying under 30% for the top-1
model prediction. Even top-20 predictions have an
almost 40% chance of not containing the ground-
truth answer anywhere within the generated strings.
Generative Transformer models such as T5-base
and BART-large perform poorly on the clue-answer
task, however, the model accuracy across most2654
metrics almost doubles when switching from T5-
base (with 220M parameters) to BART-large (with
400M parameter).
Our strongest baseline, RAG-wiki and RAG-dict,
achieve 50.6 and 50.0 exact-match accuracies on
the clue-answer dataset, respectively. The In
score, which looks at whether any substrings in
the generated answer match the ground truth – and
which can be seen an upper bound on the model’s
ability to solve the puzzle – is slightly higher, at
56.7 for RAG-wiki and 56.2 for RAG-dict.
Not surprisingly, these results show that the ad-
ditional step of retrieving Wikipedia or dictionary
entries increases the accuracy considerably com-
pared to the ﬁne-tuned sequence-to-sequence mod-
els such as BART which store this information in
its parameters. The normalized metrics which re-
move diacritics, punctuation and whitespace bring
the accuracy up by 2-6%, depending on the model.
We examined the top-20 exact-match predictions
generated by RAG-wiki and RAG-dict and ﬁnd
that both models are in agreement in terms of an-
swer matches for around 85% of the test set. In
other words, both models either correctly predict
the ground truth answer or both fail to do so.
5.2 Crossword Puzzle Task
The baseline performance on the entire crossword
puzzle dataset shows there is signiﬁcant room for
improvement of the existing architectures (see Ta-
ble 3). Our best model, RAG-wiki, correctly ﬁlls
in the answers for only 26% (on average) of the to-
tal number of puzzle clues, despite having a much
higher performance on the clue-answer task, i.e.
measured independently from the crossword grid
(Table 2). This is explained by the fact that the
clues with no ground-truth answer present among
the candidates have to be removed from the puzzles
in order for the solver to converge, which in turn
relaxes the interdependency constraints too much,
so that a ﬁlled answer may be selected from the set
of candidates almost at random. Despite that, thebaseline solver is able to solve over a quarter of
each the puzzle on average.
6 Qualitative analysis
Evaluation on the annotated subset of the data re-
veals that some clue types present signiﬁcantly
higher levels of difﬁculty than others (see Table 4).
In particular, all of our baseline systems struggle
with the clues requiring reasoning in the context of
historical knowledge. As expected, all of the mod-
els demonstrate much stronger performance on the
factual and word-meaning clue types, since the rele-
vant answer candidates are likely to be found in the
Wikipedia data used for pre-training. We observe
the biggest differences between BART and RAG
performance for the “abbreviation” and the “preﬁx-
sufﬁx” categories. The document retrieval step in
RAG allows for more efﬁcient matching of sup-
porting documents, leading to generation of more
relevant answer candidates. For instance, the clue
“Warehouse abbr. ” results in “pkg” and“bldg” can-
didates among RAG predictions, whereas BART
generates abstract and largely irrelevant strings.
Our manual inspection of model predictions
suggest that both BART and RAG correctly in-
fer the grammatical form of the answer from the
formulation of the clue. For example, the clue
“Stitched” produces the candidate answers “Sewn”
and“Made” , and the clue “Word repeated after
“Que”” triggers mostly Spanish and French genera-
tions (e.g. “Avec” or“Sera” ).
As previously stated RAG-wiki and RAG-dict
largely agree with each other with respect to the
ground truth answers. We qualitatively assessed
instances where either RAG-wiki or RAG-dict pre-
dict the answer correctly in Appendix A.
7 Discussion and Future Work
The presented task is challenging to approach in
an end-to-end model fashion. There are several
reasons for this, which we discuss below.
Character-level outputs. Commonly used
Transformer decoders do not produce character-
level outputs and produce BPE and wordpieces
instead, which creates a problem for a potential
end-to-end neural crossword solver. One possible
solution can be the modiﬁcation of the loss term,
designed with character-based output logits instead
of BPE since the crossword grid constraints are
at a single cell- (i.e. character-) level. There is2655
some work done in the character-level output
transformer encoders such as Ma et al. (2020).
However, to our best knowledge there is no
major generative Transformer architecture which
supports character-level outputs yet, we intend
to explore this avenue further in future work to
develop an end-to-end neural crossword solver.
SMT solver constraints. As mentioned earlier,
our current baseline solver does not allow partial
solutions, and we rely on pre-ﬁltering using the or-
acle from the ground-truth answers. Although this
strategy is ﬂawed for the obvious use of the oracle,
the alternatives are currently either computation-
ally intractable or too lossy. One such strategy is
to remove kclues at a time, starting with k= 1
and progressively increasing the number of clues
removed until the remaining relaxed puzzle can be
solved – which has the complexity of O( 2), where
nis the total number of clues in the puzzle. Another
approach we tried was to relax certain constraints
of the puzzle grid, maximally satisfying as many
constraints as possible, which is formally known
as the maximal satisfaction problem (MAX-SAT).
This is a NP-hard problem for which it is hard to
ﬁnd approximate solutions (Papadimitriou, 1994).
Our initial foray into such approximate solvers
(Previti and Marques-Silva, 2013; Lifﬁton and Ma-
lik, 2013) produced severely under-constrained
puzzles with garbage character entries. Further
work needs to be done to extend this solver to han-
dle partial solutions elegantly without the need for
an oracle, this could be addressed with probabilis-
tic and weighted constraint satisfaction solvers, in
line with the work by Littman et al. (2002); Keim
et al. (1999) and Ginsberg (2011), but without the
dependency on the past crossword clues.
8 Conclusion
We present a new challenging task of solving cross-
word puzzles and present the New York Times
Crosswords Dataset, which can be approached at
a QA-like level of individual clue-answer pairs, or
at the level of an entire puzzle, with imposed an-swer interdependency constraints. This new bench-
mark contains a broad range of clue types that re-
quire diverse reasoning components. We carry out
a set of baseline experiments that indicate the over-
all difﬁculty of this task for the current systems,
including retrieval-augmented SOTA models for
open-domain question answering. We also discuss
the technical challenges in building a crossword
solver and obtaining partial solutions as well as in
the design of end-to-end systems for this task. We
hope that the NYT Crosswords task would deﬁne a
new high bar for the AI systems.
9 Ethical Considerations
The New York Times daily crossword puzzles are
a copyright of the New York Times. We have ob-
tained preliminary approval from the New York
Times to release this data under a non-commercial
and research use license, and are in the process of
ﬁnalizing the exact licensing terms and distribution
channels with the NYT legal department.
10 Acknowledgments
We would like to thank the anonymous review-
ers for their careful and insightful review of our
manuscript and their feedback. We would like to
thank Parth Parikh for the permission to modify
and reuse parts of their crossword solver. We are
grateful to New York Times staff for their support
of this project. This project is funded in part by
an NSF CAREER award to Anna Rumshisky (IIS-
1652742).
References265626572658
A Qualitative Analysis of RAG-wiki and
RAG-dict Predictions
We examined top-20 exact-match predictions gen-
erated by RAG-wiki and RAG-dict. With some
exceptions, both models predict similar results (in
terms of answer matches) for around 85% of the
test set.
Table 5 shows examples where RAG-dict failed
to generate the correct predictions but RAG-wiki
succeeded, and vice-versa. Most of the instances
where RAG-dict predicted correctly and RAG-wiki
did not are the ones where answer is closely related
to the meaning of the clue. The instances where
only RAG-wiki predicted correctly are where an-
swer is not a direct meaning of the clue, and some
more information is required predict.
CategoryRAG-dict predicts correctly
RAG-wiki failsRAG-wiki predicts correctly
RAG-dict fails
Clue Answer Clue Answer
FactualAsian nursemaid
Pill alternative, for shortamah
iudQuisling’s city
Avatar of Vishnuoslo
rama
Word MeaningPause indicator
Moves along quicklycomma
scootsSites for grand entrances
Point of no return?archways
ace
Word PlayKind of contribution
Without iceira
neatI’m impressed!
Airport no noooh
knife
Synonyms AntonymsStitched
Promptlysewn
on timeguess idea
Fill in the Blanks__rug
canola __area
oil__-Israeli relations arab2659