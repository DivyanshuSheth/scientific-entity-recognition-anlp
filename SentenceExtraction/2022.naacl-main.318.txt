
Zhengxuan Wu, Atticus Geiger, Joshua Rozner, Elisa Kreiss, Hanson Lu
Thomas Icard, Christopher Potts, Noah D. Goodman
Stanford University
{wuzhengx, atticusg}@stanford.edu
Abstract
Distillation efforts have led to language mod-
els that are more compact and efficient without
serious drops in performance. The standard
approach to distillation trains a student model
against two objectives: a task-specific objec-
tive (e.g., language modeling) and an imitation
objective that encourages the hidden states of
the student model to be similar to those of the
larger teacher model. In this paper, we show
that it is beneficial to augment distillation with
a third objective that encourages the student
to imitate the causal dynamics of the teacher
through a distillation interchange intervention
training objective (DIITO ).DIITO pushes the
student model to become a causal abstrac-
tion of the teacher model – a faithful model
with simpler causal structure. DIITO is fully
differentiable, easily implemented, and com-
bines flexibly with other objectives. Compared
against standard distillation with the same set-
ting, DIITO results in lower perplexity on the
WikiText-103M corpus (masked language mod-
eling) and marked improvements on the GLUE
benchmark (natural language understanding),
SQuAD (question answering), and CoNLL-
2003 (named entity recognition).
1 Introduction
Large pretrained language models have improved
performance across a wide range of NLP tasks, but
can be costly due to their large size. Distillation
seeks to reduce these costs while maintaining per-
formance by training a simpler student model from
a larger teacher model (Hinton et al., 2015; Sun
et al., 2019; Sanh et al., 2019; Jiao et al., 2019).
Hinton et al. (2015) propose model distillation
with an objective that encourages the student to
produce output logits similar to those of the teacher
while also supervising with a task-specific objec-
tive (e.g., sequence classification). Sanh et al.
(2019), Sun et al. (2019), and Jiao et al. (2019)adapt this method, strengthening it with additional
supervision to align internal representations be-
tween the two models. However, these approaches
may push the student model to match all aspects
of internal states of the teacher model irrespective
of their causal role in the network’s computation.
This motivates us to develop a method that focuses
on aligning the causal role of representations in the
student and teacher models.
We propose augmenting standard distillation
with a new objective that pushes the student to
become a causal abstraction (Beckers and Halpern,
2019; Beckers et al., 2020; Geiger et al., 2021a)
of the teacher model: the simpler student will
faithfully model the causal effect of teacher rep-
resentations on output. To achieve this, we employ
theinterchange intervention training (IIT) method
of Geiger et al. (2021b). The distillation inter-
change intervention training objective (DIITO )
aligns a high-level student model with a low-level
teacher model and performs interchange interven-
tions (swapping of aligned internal states); during
training the high-level model is pushed to conform
to the causal dynamics of the low-level model.
Figure 1 shows a schematic example of this pro-
cess. Here, hidden layer 2 of the student model
(bottom) is aligned with layers 3 and 4 of the
teacher model. The figure depicts a single inter-
change intervention replacing aligned states in the
left-hand models with those from the right-hand
models. This results in a new network evolution
that is shaped both by the original input and the
interchanged hidden states. It can be interpreted as
a certain kind of counterfactual as shown in Fig-
ure 1: what would the output be for the sentence
“I ate some ⟨MASK⟩.” if the activation values for the
second token at the middle two layers were set
to the values they have for the input “The water
⟨MASK⟩solid.”? DIITO then pushes the student
model to output the same logits as the teacher, i.e.,
matching the teacher’s output distribution under4288
the counterfactual setup.
To assess the contribution of distillation with
DIITO , we begin with BERT (Devlin et al.,
2019) and distill it under various alignments be-
tween student and teacher while pretraining on the
WikiText-103M corpus (Merity et al., 2016) achiev-
ing−2.24 perplexity on the MLM task compared to
standard DistilBERT trained on the same data. We
then fine-tune the best performing distilled mod-
els and find consistent performance improvements
compared to standard DistilBERT trained with the
same setting on the GLUE benchmark (+1.77%),
CoNLL-2003 name-entity recognition (+0.38% on
F1 score), and SQuAD v1.1 (+2.46% on EM score).
2 Related Work
Distillation was first introduced in the context of
computer vision (Hinton et al., 2015) and has since
been widely explored for language models (Sun
et al., 2019; Sanh et al., 2019; Jiao et al., 2019).
For example, Sanh et al. (2019) propose to extract
information not only from output probabilities of
the last layer in the teacher model, but also from in-termediate layers in the fine-tuning stage. Recently,
Rotman et al. (2021) adapt causal analysis methods
to estimate the effects of inputs on predictions to
compress models for better domain adaptation. In
contrast, we focus on imbuing the student with the
causal structure of the teacher.
Interventions on neural networks were originally
used as a structural analysis method aimed at il-
luminating neural representations and their role
in network behavior (Feder et al., 2021; Pryzant
et al., 2021; Vig et al., 2020; Elazar et al., 2021;
Giulianelli et al., 2020; Geiger et al., 2020, 2021a).
Geiger et al. (2021b) extend these methods to net-
work optimization. We contribute to this existing
research by adapting intervention-based optimiza-
tion to the task of language model distillation.
3 Causal Distillation
Here, we define our distillation training procedure.
See Algorithm 1 for a summary.
GV. The GV operator is an
activation-value retriever for a neural model. Given
a neural model Mcontaining a set of neurons N4289Algorithm 1 Causal Distillation via Interchange
Intervention Training
Require: Student model S, teacher model T,
student output neurons N, alignment Π, shuf-
fled training dataset D.
1:S.train()
2:T.eval()
3:D= random.shuffle( D)
4:N=Π(N)
5:while not converged do
6:for{x,y},{x,y}initer(D,D)do
7:N=sample_student_neurons()
8:N=Π(N)
9: with no_grad:
10: T=SV(
11: T,N,GV(T,x,N))
12: o=GV(T,x,N)
13:S=SV(
14: S,N,GV(S,x,N))
15:o=GV(S,x,N)
16:L=get_loss (o, o)
17: Calculate L,L,L
18:L=L +L+L+L
19:L.backward()
20: Step optimizer
21:end while
(an internal representations) and an appropriate in-
putx,GV(M,x,N)is the set of values
thatNtakes on when processing x. In the case
thatNrepresents the neurons corresponding to the
final output, GV(M,x,N)is the output of
model Mwhen processing x(i.e., output from a
standard forward call of a neural model).
SV. The SV operator is a function
generator that defines a new neural model with a
computation graph that specifies an intervention
on the original model M(Pearl, 2009; Spirtes
et al., 2001). SV(M,N,v)is the new neu-
ral model where the neurons Nare set to constant
values v. Because we overwrite neurons with v
in-place, gradients can back-propagate through v.
Interchange Intervention . An interchange in-
tervention combines GV andSV op-
erations. First, we randomly sample a pair of exam-
ples from a training dataset (x,y),(x,y)∈ D.
Next, where Nis the set of neurons that we are
targeting for intervention, we define Mto abbre-
viate the new neural model as follows:
SV/parenleftbig
M,N,GV(M,x,N)/parenrightbig
(1)This is the version of Mobtained from setting the
values of Nto be those we get from processing
input x. The interchange intervention targeting
Nwithxas the source input and xas the base
input is then defined as follows:
II(M,N,x,x)=
GV(M,x,N)(2)
where Nare the output neurons. In other words,
II(M,N,x,x)is the output state we get
fromMfor input xbut with the neurons Nset to
the values obtained when processing input x.
DIITO .DIITO employs Tas the teacher
model, Sas the student model, Das the training
inputs to both models, and Πas an alignment that
maps sets of student neurons to sets of teacher neu-
rons. For each set of student neurons Nin the
domain of Π, we define DIITO loss as:
L=
/summationdisplayCE/parenleftig
II(S,N,x,x),
II(T,Π(N),x,x)/parenrightig
(3)
where CEis the smoothed cross-entropy loss mea-
suring the divergences of predictions, under inter-
change, between the teacher and the student model.
Distillation Objectives . We adopt the standard
distillation objectives from DistilBERT (Sanh et al.,
2019) (defined formally in Appendix A.1): L
for the task-specific loss for the student model, L
for the loss measuring the divergence between the
student and teacher outputs on masked tokens, and
Lfor the loss measuring the divergence between
the student and teacher contextualized representa-
tions on masked tokens in the last layer. Our final
training objective for the student is a linear com-
bination of the four training objectives reviewed
above: L,L,L, andL. In a further
experiment, we introduce a fifth objective L
which is identical to L, except the teacher and
student are undergoing interchange interventions
(see Appendix A.2 for details).
4 Experimental Set-up
We adapt the open-source Hugging Face implemen-
tation for model distillation (Wolf et al., 2020).
We distill our models on the MLM pretraining
task (Devlin et al., 2019). We use large gradient4290
accumulations over batches as in Sanh et al. (2019)
for better performance. Specifically, we distill all
models for three epochs for an effective batch size
of 240. In contrast to the setting of 4K per batch in
BERT (Devlin et al., 2019) and DistilBERT (Sanh
et al., 2019), we found that small effective batch
size works better for smaller dataset. We weight
all objectives equally for all experiments. With our
new objectives, the distillation takes approximately
9 hours on 4 NVIDIA A100 GPUs.
Student and Teacher Models . Our two students
have the standard BERT architecture, with 12 heads
with a hidden dimension of 768. The larger student
has 6 layers, the smaller 3 layers. Our pretrained
teacher has the same architecture, except with 12
layers. Following practices introduced by Sanh
et al. (2019), we initialize our student model with
weights from skipped layers (one out of four layers)
in the teacher model. We use WikiText for distilla-
tion to simulate a practical situation with a limited
computation budget. We leave the exploration of
our method on larger datasets for future research.
Alignment . Our teacher and student BERT mod-
els create columns of neural representations above
each token with each row created by the feed-
forward layer of a Transformer block, as in Fig-
ure 1. We define LandLto be the number of
layers in the student and teacher, respectively. In
addition, we define SandTto be the representa-
tions in the ith row and jth column in the student
and teacher, respectively. An alignment Πis a par-
tial function from student representations to sets ofteacher representations. We test three alignments:
FULL Πis defined on all student representations:
Π(S) ={T: 0≤k < L/L}
MIDDLE Πis defined for the row L/sslash2:
Π(S) ={T}
LATE Πis defined on the student representations
in the first and second rows:
Π(S) ={T}andΠ(S) ={T}
For each training iteration, we randomly
select one aligned student layer to perform
the interchange intervention, and we randomly
select 30% of token embeddings for align-
ment for each sequence. We experiment
with three conditions with the FULL alignment:
consecutive tokens ( DIITO), random to-
kens ( DIITO+Random ) and masked tokens
(DIITO+Masked ). We also include Lto
theFULL alignment ( DIITO+L).
5 Results
Language Modeling . We first evaluate our models
using perplexity on the held-out evaluation data
from WikiText. As shown in Table 1, DIITO
brings performance gains for all alignments. Our
best result is from the FULL alignment with the
L(DIITO+L), which has −2.24 per-
plexity compared to standard DistilBERT trained
with the same amount of data.
GLUE . The GLUE benchmark (Wang et al.,
2018) covers different natural language understand-
ing tasks. We report averaged GLUE scores on the4291
development sets by fine-tuning our distilled mod-
els in Table 1. Individual task performance scores
for each GLUE task are included in Table 2 in the
Appendix. The results suggest that distilled mod-
els with DIITO lead to consistent improvements
over standard DistilBERT trained under the same
setting, with our best result ( DIITO+L)
being +1.77% higher.
Named Entity Recognition . We also evalu-
ate our models on the CoNLL-2003 Named Entity
Recognition task (Tjong Kim Sang and De Meul-
der, 2003). We report accuracy and Macro-F1
scores on the development sets. We fine-tune our
models for three epochs. Our best performing
model ( DIITO ) numerically surpasses not
only standard DistilBERT (+0.38% on F1 score)
trained under the same setting, but also its teacher,
BERT (+0.05% on F1 score). Though these
improvements are small, in this case distillation
produces a smaller model with better performance .
Question Answering . Finally, we evaluate on a
question answering task, SQuAD v1.1 (Rajpurkar
et al., 2016). We report Exact Match and Macro-
F1 on the development sets as our evaluation met-
rics. We fine-tune our models for two epochs.
DIITO again yields marked improvements (Ta-
ble 1). Our best result is from the vanilla FULL
alignment ( DIITO), with +2.46% on standard
DistilBERT trained under the same setting.
Low-Resource Model Distillation We experi-
ment with an extreme case in a low-resource setting
where we only distill with 15% of WikiText, keep-
ing other experimental details constant. Our results
suggest that DIITO training is also beneficial in
extremely low-resource settings (Figure 2).
Layer-wise Ablation We further study the ef-
fect of DIITO training with respect to the size of
the student model through a layer-wise ablation
experiment. As shown in Figure 3, we compare
GLUE performance for models trained with stan-
dard distillation pipeline and with DIITO training
(DIITO). Specifically, we compute the aver-
aged GLUE scores following the same procedure
described in Section A.4. Our results suggest that
DIITO training brings consistent improvements
over GLUE tasks with smaller models booking the
greatest gains.
6 Conclusion
In this paper, we explored distilling a teacher by
training a student to capture the causal dynamics
of its computations. Across a wide range of NLP
tasks, we find that DIITO leads to improvements,
with the largest gains coming from the models
that use the richest alignment between student and
teacher. Our results also demonstrate that DIITO
performs on-par (maintaining 97% of performance
on GLUE tasks) with standard DistilBERT (Sanh
et al., 2019) while consuming 97% less training
data. These findings suggest that DIITO is a
promising tool for effective model distillation.
References42924293A Appendix
A.1 Standard Distillation Objectives
In our setting, our teacher model Tis a BERT
model, and our student model Sis a shallower
BERT model with fewer layers.
Assume that we randomly draw a training exam-
ple(x,y)∈ D, where xis the input to our mod-
els and yis the corresponding ground truth (the
token prediction at each masked position). We de-
note the model predictions (output logits) as T(x)
andS(x). Additionally, we denote the contextu-
alized representation for tokens for xat the last
layer as BERT(x)and BERT(x).
We adopt the three standard distillation objec-
tives of Sanh et al. (2019):
L The masked language modeling loss of the
student model calculated over all examples
using the cross-entropy loss as follows:
/summationdisplayCE(S(x),y) (4)
LFollowing Hinton et al. (2015), the smoothed
cross-entropy loss measuring the divergence
between the student and teacher outputs as
follows:
/summationdisplayCE(S(x),T(x)) (5)
LThe cosine embedding loss defined in terms
of the final hidden states of the teacher and
the student as follows:
/summationdisplayC(BERT(x),BERT(x))(6)
As a result, comparing to standard DistilBERT,
DIITO essentially adds a new type of objective
by pushing the student model to become a causal
abstraction of the teacher model.
A.2 Causal Distillation Objectives
In addition to our causal loss L , we also pro-
pose a new loss L which is identical to L
with interchange interventions. In this section, we
provide a formal definition for L .
We denote our teacher and student models as
TandSrespectively. Using the notational con-
ventions from Section 3, we use NandNto
represent the neurons corresponding to the finaloutput for each model. Likewise, we use Nand
Nto represent the neurons representing contex-
tualized representation for each token after the final
BERT layer.
Assuming we randomly sample a pair of exam-
ples from a training dataset (x,y),(x,y)∈ D,
we can then rewrite our causal loss Lby rear-
ranging Eqn. 2 and Eqn. 3 as follows:
/summationdisplayCE/parenleftig
GV(M,x,N),
GV(M,x,N)/parenrightig(7)
where MandMare derived as in Eqn. 1 for
each model respectively. Crucially, Eqn. 7 can
be regarded as the causal form of the standard
smoothed cross-entropy loss with interchange in-
tervention. Likewise, we can further define the
L as:
/summationdisplayC/parenleftig
GV(M,x,N),
GV(M,x,N)/parenrightig(8)
with adjusted interchange alignments for Nand
N.
A.3 Evaluation Set-up
GLUE We fine-tune for 25 epochs for the smaller
datasets (RTE and CoLA) and 3 epochs for the oth-
ers. Following Devlin et al. (2019) and Sanh et al.
(2019), we use Matthew’s Correlation for CoLA, F1
for MRPC and QQP, Spearman correlation for STS-
B, and accuracy for all the other tasks in GLUE.
A.4 Reproducibility
To foster reproduciblity and provide a fair compar-
ison between methods, we distill BERT for each
condition with three distinct random seeds. We
then fine-tune each model with five distinct random
seeds. Consequently, we report results aggregated
from three distinct runs for the language modeling
task, and 15 distinct runs for others.
Named Entity Recognition We follow the ex-
perimental set-up in the Hugging Face (Wolf et al.,
2020) repository for evaluation for the CoNLL-
2003 Named Entity Recognition task (Tjong
Kim Sang and De Meulder, 2003). For fine-tuning,
we set the learning rate to 5ewith an effective
batch size of 32 for three epochs.4294
Question Answering We use the experimental
set-up of Sanh et al. (2019) for evaluation on
SQuAD v1.1 (Rajpurkar et al., 2016). For fine-
tuning, we set the learning rate to 3ewith an
effective batch size of 48 for two epochs. We set
the stride to 128.4295