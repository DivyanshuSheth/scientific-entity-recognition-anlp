
Raghav Gupta, Harrison Lee, Jeffrey Zhao, Abhinav Rastogi, Yuan Cao, Yonghui Wu
Google Research
{raghavgupta, harrisonlee}@google.com
Abstract
Building universal dialogue systems that op-
erate across multiple domains/APIs and gen-
eralize to new ones with minimal overhead is
a critical challenge. Recent works have lever-
aged natural language descriptions of schema
elements to enable such systems; however, de-
scriptions only indirectly convey schema se-
mantics. In this work, we propose Show, Don’t
Tell, which prompts seq2seq models with a la-
beled example dialogue to show the semantics
of schema elements rather than tellthe model
through descriptions. While requiring simi-
lar effort from service developers as generat-
ing descriptions, we show that using short ex-
amples as schema representations with large
language models results in state-of-the-art per-
formance on two popular dialogue state track-
ing benchmarks designed to measure zero-
shot generalization - the Schema-Guided Di-
alogue dataset and the MultiWOZ leave-one-
out benchmark.
1 Introduction
Task-oriented dialogue (TOD) systems need to sup-
port an ever-increasing variety of services. Since
many service developers lack the resources to col-
lect data and train models, zero and few-shot trans-
fer to unseen services is critical to the democratiza-
tion of dialogue agents.
Recent approaches to generalizable TOD sys-
tems primarily rely on combining two techniques:
large language models like BERT (Devlin et al.,
2019) and T5 (Raffel et al., 2020), and schema-
guided modeling - i.e. using natural language de-
scriptions of schema elements (intents and slots) as
model inputs to enable transfer to unseen services
(Rastogi et al., 2020a,b). Models combining the
two currently hold state-of-the-art (SotA) results
on dialogue state tracking (DST) (Heck et al., 2020;
Lee et al., 2021a; Zhao et al., 2022).However, description-based schema representa-
tions have some drawbacks. Writing precise natural
language descriptions requires manual effort and
can be difﬁcult to write succinctly. Also, descrip-
tions only provide indirect supervision about how
to interact with a service compared to an exam-
ple. Furthermore, Lee et al. (2021b) showed that
schema-guided DST models are not robust to vari-
ations in schema descriptions, causing signiﬁcant
quality drops.
We propose using a single dialogue example
with state annotations as an alternative to the
description-based schema representation, similar
to one-shot priming (Brown et al., 2020) - an ap-
proach we call Show, Don’t Tell (SDT) . Through
demonstration, we show models the schema seman-
tics rather than tellthem through natural language
descriptions, as seen in Figure 1. SDT achieves
SotA accuracy and generalization to new APIs
across both the Schema-Guided Dataset (SGD)
(Rastogi et al., 2020b) and MultiWOZ Leave-One-
Out (Budzianowski et al., 2018; Lin et al., 2021b)
benchmarks, while being more data-efﬁcient and
robust to schema variations.
2 Show, Don’t Tell
Following SoTA models, we pose DST as a seq2seq
task (Wu et al., 2019; Zhao et al., 2021a) and ﬁne-
tune T5 on DST datasets. The model input consists
of aprompt to convey API semantics and context to
represent the current dialogue instance. The target
contains ground truth belief states corresponding
to the context. We compare against two baselines:
•T5-ind (Lee et al., 2021a): Model input com-
prises a single slot description for the prompt,
concatenated with the dialogue history as the
context. The target is the value of the single slot
in the dialogue state. Model inference is invoked
once per slot - i.e. values for different slots are
independently decoded.4541
•T5-seq (Zhao et al., 2022): Model input com-
prises the descriptions of all slots as the prompt,
concatenated with the dialogue history as the
context. The target is the sequence of slot-value
pairs in the dialogue state - i.e. the dialogue state
is decoded sequentially in a single pass.
We modify the prompt formats above to utilize
demonstrations instead of descriptions as described
below and illustrated in Figure 1.
•SDT-ind : Aprompt Pcomprises a single ex-
ample utterance and the ground truth slot-value
pair formatted as
P= [ex];u; [slot];sv
where uis a user utterance where slot iis
active/not null and svis the slot-value pair.
[ex],[slot]are special delimiter tokens, and ;
denotes concatenation.
•SDT-seq : Aprompt Pcomprises a single la-
beled dialogue formatted as:
P= [ex];u;...;u; [slot];sv;...;sv
where uis an utterance, and other symbols are
explained in the SDT-ind section above. In sim-
ple terms, the prompt is constructed by concate-
nating all utterances in an example dialogue fol-
lowed by all slot-value pairs in the dialogue state.
In both the T5-* and SDT-* approaches, the con-
text is the serialized dialogue history for the current
dialogue instance. The ﬁnal model input is formed
by concatenating the prompt and the context strings,and the target string is the same as T5-*, containing
only a single slot value for *-ind models and the
entire turn’s belief state for *-seq models.
For both T5-* and SDT-*, we enumerate the
categorical slot values in multiple-choice format
in the prompt and task models with decoding the
multiple choice letter corresponding to the correct
categorical value.
More details on prompt design and its impact on
performance are provided in Appendix A.
Creating prompt examples: It is imperative
that SDT prompts contain enough information to
infer the semantics for all slots in a schema. For
SDT-ind, we create individual utterances that show-
case a single slot. For SDT-seq, we create example
dialogues where all slots in the schema are used.
Multi-domain examples: It is not feasible to
construct multi-domain demonstrations for every
combination of domains. Thus, we stick to single-
domain SDT prompts and create separate train-
ing instances for each domain present in a multi-
domain dialogue turn; for inference, we run infer-
ence once for each domain and combine the results.
3 Experimental Setup
Datasets: We conduct experiments on two DST
benchmarks: Schema-guided Dialogue (SGD)
(Rastogi et al., 2020b) and MultiWOZ 2.1
(Budzianowski et al., 2018; Eric et al., 2020). For
MultiWOZ, we evaluate on the leave-one-out setup
(Wu et al., 2019; Lin et al., 2021a), where models
are trained on all domains but one and evaluated on
the holdout domain. Additionally, we apply the rec-4542ommended TRADE pre-processing scriptfor fair
comparison with other work. For both datasets, we
created concise example dialogues modeled after
dialogues observed in the datasets.
Implementation: We train SDT models by ﬁne-
tuning pretrained T5 1.1 checkpoints. For SDT-seq,
we select one example dialogue for each service to
create a prompt and use that prompt across all dia-
logue instances of that service, across training and
evaluation. We do the same for SDT-ind but create
one prompt per slot instead of per service. Unless
otherwise noted, all T5-based models are based
on T5-XXL (11B parameters). Appendices B and
C contain more details on training and baselines
respectively.
4 Results
4.1 SGD Results
Table 1 contains results on the SGD test set. SDT-
seq achieves the highest JGA by +1.1%, outper-
forming the description-based T5-* models, partic-
ularly on unseen services. SDT-ind is comparable
to its counterpart T5-ind and better than T5-seq.
Since SDT results vary with the choice of exam-
ple dialogue provided in the prompt, we created
5 different versions of prompts for each service
using different examples. We report the average
JGA across the 5 versions and the 95% conﬁdence
intervals using the Student’s-t distribution.
We hypothesize that the main advantage of SDT
is that the schema semantics are conveyed via
demonstration, which is more similar in form to
the end task of state tracking and more informa-
tive than descriptions. On the other hand, natural
language descriptions can be viewed as an interme-
diary that models must interpret in order to achieve
the end goal of slot value prediction.
We see that SDT-seq outperforms SDT-ind and
posit that this is because the full dialogue prompts
in SDT-seq demonstrate more complex linguistic
patterns (e.g. coreference resolution, long term
dependencies) than the single utterance prompts
of SDT-ind. On the other hand, we believe T5-
seq does not outperform T5-ind because no addi-
tional information is conveyed to the model through
concatenating independent descriptions. All-else-
equal, decoding all slots in one pass is more chal-
lenging than decoding each slot independently.
We also experimented with using up to 5 ex-
ample dialogues in each prompt of SDT-seq, but
accuracy did not increase.
4.2 MultiWOZ Results
Table 2 summarizes results for the MultiWOZ 2.1
leave-one-out setup. SDT-seq outperforms T5-seq
by +1.5% overall and in 3 of the 5 domains, achiev-
ing state-of-the-art performance.
4.3 Impact of Model Size
T5’s XXL size (11B parameters) may be unsuitable
in resource-constrained settings. To understand
how the the impact of model size, we measure
SDT’s performance on SGD across multiple T5
sizes in Table 3. For base and large sizes, both SDT
variations offer higher JGA than their description-
based counterparts, possibly due to smaller T5 mod-
els being less capable of inferring unseen slots with
just a description, whereas SDT models provide
more direct supervision in contrast. Additionally,
SDT-ind outperforms SDT-seq for both the smaller
sizes, potentially due to SDT-seq’s prediction task
being more complex than that of SDT-ind.
4.4 Data Efﬁciency
To examine the data efﬁciency of SDT models, we
also experiment with training SDT-seq with 0.16%
(10-shot), 1%, and 10% of the SGD training data4543
and evaluating on the entire test set. For 10-shot,
we randomly sample 10 training dialogues from ev-
ery service; for 1% and 10%, we sample uniformly
across the entire dataset. SDT-seq demonstrates
far higher data efﬁciency than T5-seq (Table 4),
indicating that SDT is more suitable for bootstrap-
ping dialogue systems with a limited budget for
collecting training data.
4.5 Robustness
Large LMs are often sensitive to the choice of
prompt (Zhao et al., 2021b; Reynolds and Mc-
Donell, 2021). To this end, we evaluate SDT-seq
on the SGD-X (Lee et al., 2021b) benchmark, com-
prising 5 variants with paraphrased slot names and
descriptions for every schema (Appendix Figure 4).
Note that SDT-seq only makes use of slot names,
so variations in description have no effect on it.
Table 5 shows SDT-seq achieves the highest aver-
age JGA ( JGA) and lowest schema sensitivity
(SS, lower value indicates higher robustness),
making it the most robust of the compared mod-
els. While the JGA decline indicates that SDT-seq
is somewhat sensitive to how slot names are writ-
ten, when compared to a variant of T5-seq (Zhao
et al., 2022) that only uses slot names, it is still
more robust based on the schema sensitivity, and
the relative drop in JGA is nearly equal.
5 Discussion
5.1 Writing descriptions vs. demonstrations
The information provided to SDT is not identical to
what is provided to typical schema-guided models,
as SDT exchanges natural language descriptions for
a demonstration of identifying slots in a dialogue.
However, we argue that from the developer stand-
point, creating a single example is similar in effort
to writing descriptions, so we consider the methods
comparable. Creating the SDT-seq prompts for all
45 services in SGD took an experienced annotator
∼2 hours, compared to ∼1.5 hours for generating
all slot descriptions. SDT-ind prompts are even
simpler to write because they relax the requirement
for creating a coherent dialogue involving all slots.
Descriptions can sometimes be easier to gener-
ate than a succinct dialogue that covers all slots.
However, given the performance gain, example-
based prompts may be a better choice for many
settings, especially for smaller model sizes and low
resource settings where the gain over description-
based prompts is more pronounced.
5.2 Descriptions plus demonstrations
We tried combining both descriptions and a demon-
stration in a single prompt to try to further improve
performance. However, results showed that this did
not improve upon using demonstrations alone (see
Appendix Table A1 for details).
We hypothesize that demonstrations, along with
slot names, already convey slot semantics sufﬁ-
ciently, rendering descriptions extraneous. How-
ever, given that using slot names alone underper-
forms using descriptions (Zhao et al., 2022), the
improvement SDT exhibits over using descriptions
does not result purely from the use of slot names.
5.3 Prompting vs. traditional ﬁnetuning
To understand the impact of using a single demon-
stration as a prompt vs. traditional ﬁnetuning, we
ﬁnetune T5-seq an additional time on the same set
of dialogues used in SDT-seq prompts; therefore
it has access to both slot descriptions as well as a
single demonstration for each service. In this case,
T5-seq is provided strictly more information than4544
SDT-seq. T5-seq with ﬁnetuning obtains a JGA of
87.7% on SGD, on par with T5-ind but still lower
than SDT-seq, suggesting that, when scarce, dia-
logue examples are better used as prompts (Le Scao
and Rush, 2021).
Interestingly, ﬁnetuning on up to 5 dialogue ex-
amples per service did not improve performance
after the ﬁrst example (Appendix Figure 3).
5.4 Error analysis
Figure 2 compares some common error patterns
made by T5-seq vs. SDT-seq. The patterns sug-
gest that SDT’s demonstrations are helpful when
multiple slots in the same domain are similar to
each other (#1 in Figure 2) and when slots dissimi-
lar from those seen in training are introduced (#2).
However, SDT can sometimes be limited by its
prompt. For instance, in #3 it has only seen the
"music" value for the event_type slot in the prompt,
potentially resulting in under-predicting the cate-
gorical values not featured in the example dialogue
(e.g. "theater").
6 Related Work
Prior approaches focused on framing DST as ques-
tion answering (Ruan et al., 2020; Ma et al., 2019;
Zhang et al., 2021). Many MultiWOZ cross-
domain models leverage slot names/descriptions
(Wu et al., 2019; Lee et al., 2019; Lin et al., 2021a).
Pretrained generative LLMs (Raffel et al., 2020;
Brown et al., 2020) have enabled framing NLP
tasks as seq2seq problems. Some DST papers
(Zhao et al., 2021a; Feng et al., 2021) look at set-
tings with no train-test discrepancy. Many studies
explore the efﬁcacy of task-speciﬁc prompts (Jiang
et al., 2020; Liu et al., 2021). Madotto et al. (2020)
and prime LMs with examples for dialogue tasks,but without ﬁnetuning. Wei et al. (2021) ﬁnetunes
language models to teach them to use prompts to
generalize across NLP tasks.
7 Conclusion
We study the use of demonstrations as LM prompts
to convey the semantics of APIs in lieu of natu-
ral language descriptions for TOD. While taking
similar effort to construct, demonstrations outper-
form description-based prompts in our experiments
across DST datasets (SGD and MultiWOZ), model
sizes, and training data sizes, while being more
robust to changes in schemata. This work provides
developers of TOD systems with more options for
API representations to enable transfer to unseen ser-
vices. In future work, we would like to explore this
representation for other TOD tasks (e.g. dialogue
management and response generation).
8 Ethical Considerations
We proposed a more efﬁcient way of building TOD
systems by leveraging demonstrations in place of
descriptions, leading to increased accuracy with
minimal/no data preparation overhead. We con-
duct our experiments on publicly-available TOD
datasets in English, covering domains which are
popular for building conversational agents. We
hope our work leads to building more accurate
TOD systems with similar or less overhead and
encourages further research in the area.
References45454546
A Prompt Design
We experimented with various formats for the SDT
prompt before arriving at the ﬁnal format. Below,
we list alternative designs that we tried and their
impact on JGA, as evaluated on the SGD test set.
A.1 Categorical value strings vs. multiple
choice answers
We found that JGA dropped -2% when we
tasked the model with decoding categorical val-
ues instead of multiple choice answers - e.g.
payment_method=debit card instead of
payment_method=b (where bis linked to the
value debit card in the prompt as described
in Section 2). When tasking the model to decode
categorical values, it would often decode related
yet invalid values, which we counted as false in
our evaluation. For example, instead of debit
card , the model might decode bank balance .
A.2 Slot IDs vs. slot names
When we delexicalized slot names with slot IDs,
JGA dropped -5%. One downside of this approach
is that the model lost access to valuable semantic
information conveyed by the slot name. Another
downside is that the model could not distinguish
two slots that had the same value in the prompt.
For example, if the prompt was "I would like a pet-
friendly hotel room with wiﬁ" and the correspond-
ing slots were 1=True (has_wiﬁ) and 2=True
(pets_allowed), it is ambiguous which ID refers to
which slot.
The potential upside of using slot IDs was to
remove dependence on the choice of slot name, but
this did not succeed for the reasons above.
A.3 Decoding active slots vs. all slots
We experimented with training the model to only
decode active slots rather than all slots with none
values when they were inactive. JGA dropped -
0.4%, which we hypothesized might be a result of
greater dissimilarity between the slot-value string
in the prompt (which contained all slots by con-
struction) and the target, which only contained a
subset of slots.4547A.4 In-line annotations vs. dialogue+slots
concatenated
We hypothesized that bringing the slot annotation
in the prompt closer to where it was mentioned
in the dialogue might help the model better under-
stand the slot’s semantic meaning. We changed the
format as follows:
•Original: [ex] [user] I would
like a pet-friendly hotel room
with wifi [system] I found ...
[slot] has_wifi=True
•In-line: [ex] [user] I would like
a pet-friendly hotel room with
wifi [has_wifi=True] [system]
I found ...
However, this decreased JGA by more than -
20%. We hypothesized that this was likely due to
a mismatch between the prompt’s annotations and
the target string format, which we did not change.
B SDT Model Details
We used the publicly available T5 checkpoints.
For all experiments, we used a sequence length of
2048, 10% dropout and a batch size of 16. We
used a constant learning rate of 1e−3or1e−
4. All models were trained for 50k steps or until
convergence, and each experiment was conducted
on either 64 or 128 TPU v3 chips (Jouppi et al.,
2017).
C Baseline Models
For SGD, we compare against SGP-DST (Ruan
et al., 2020), MRC+WD-DST (Ma et al., 2019),
T5-seq (Zhao et al., 2022) and T5-ind (Lee et al.,
2021a).
For MultiWOZ, we compare against TRADE
(Wu et al., 2019), SUMBT (Lee et al., 2019), Trans-
ferQA (Lin et al., 2021a), and T5-seq. Transfer QA
is based on T5-large.45484549