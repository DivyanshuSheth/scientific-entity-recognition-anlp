
Or HonovichRoee AharoniJonathan HerzigHagai Taitelbaum
Vered CohenDoron KuklianskyThomas ScialomIdan Szpektor
Avinatan HassidimYossi MatiasTel Aviv UniversityGoogle ResearchMeta AI
Tel Aviv, Israel Paris, France
or.honovich@gmail.com
{roeeaharoni,szpektor}@google.com
Abstract
Grounded text generation systems often gen-
erate text that contains factual inconsistencies,
hindering their real-world applicability. Auto-
matic factual consistency evaluation may help
alleviate this limitation by accelerating evalu-
ation cycles, ﬁltering inconsistent outputs and
augmenting training data. While attracting in-
creasing attention, such evaluation metrics are
usually developed and evaluated in silo for a
single task or dataset, slowing their adoption.
Moreover, previous meta-evaluation protocols
focused on system-level correlations with hu-
man annotations, which leave the example-
level accuracy of such metrics unclear. In
this work, we introduce TRUE: a comprehen-
sive survey and assessment of factual consis-
tency metrics on a standardized collection of
existing texts from diverse tasks, manually
annotated for factual consistency. Our stan-
dardization enables an example-level meta-
evaluation protocol that is more actionable
and interpretable than previously reported cor-
relations, yielding clearer quality measures.
Across diverse state-of-the-art metrics and 11
datasets we ﬁnd that large-scale NLI and
question generation-and-answering-based ap-
proaches achieve strong and complementary
results. We recommend those methods as a
starting point for model and metric developers,
and hope TRUE will foster progress towards
even better evaluation methods.
1 Introduction
A core issue in deploying text generation mod-
els for real-world applications is that they often
generate factually inconsistent text with respect to
the input they are conditioned on, or even com-
pletely “hallucinate” (Lee et al., 2018; Rohrbach
et al., 2018; Maynez et al., 2020; Zhao et al., 2020)
as exempliﬁed in Table 1.
Table 1: Factual inconsistencies (in red) from various
tasks which are part of the TRUE study. The corre-
sponding parts in the input/grounding are in blue.
To tackle such inconsistencies, one would like
to detect them automatically by predicting whether
a generated text is factually consistent with respect
to a grounding text (frequently referred to as the
“input”, or the “knowledge”). Such automatic meth-
ods attract increasing attention (Zhou et al., 2021;
Deng et al., 2021) as they enable both better evalua-
tion and better generation models by automatically
ﬁltering training data (Gehrmann et al., 2021) or by
augmenting training data for controlled generation
(Rashkin et al., 2021b).
While automatically evaluating factual consis-
tency is an active line of work, there is no sin-
gle agreed-upon meta-evaluation protocol for mea-
suring the quality of such methods, and labeling
schemes vary in their granularity. Works are usu-3905ally done in silo, introducing new datasets and
methods that target a speciﬁc task or domain, such
as summarization (Falke et al., 2019; Kryscinski
et al., 2020; Wang et al., 2020; Scialom et al., 2021;
Deutsch et al., 2021; Xie et al., 2021) or dialogue
(Dziri et al., 2021; Honovich et al., 2021; Nie et al.,
2021; Qin et al., 2021). Comparing the robustness
of such methods across tasks and datasets is there-
fore difﬁcult, impeding progress on this subject.
In this work, we present TRUE: a compre-
hensive survey and assessment of factual consis-
tency evaluation methods, covering various met-
rics, tasks and datasets. We consolidate 11 exist-
ing datasets annotated for factual consistency into
a uniﬁed format, including pairs of a target text
and a grounding source text, with a binary annota-
tion of whether the target text is factually consis-
tent w.r.t its source. TRUEcovers summarization,
knowledge-grounded dialogue, paraphrasing and
fact veriﬁcation.The proposed standardization
enables us to properly compare consistency eval-
uation methods in a robust manner across these
various tasks and domains.
Previous works on automatic factual consis-
tency evaluation have mainly focused on measuring
system-level correlations of the proposed metrics
with human judgements (Pagnoni et al., 2021). Yet,
these correlations are not useful for estimating the
performance of a measured metric when making
example-level, binary decisions, decoupled from
speciﬁc system implementations (see recent discus-
sion by Deutsch et al. (2022) on the limitations of
reporting correlations). Instead, we aim to measure
how well a method detects inconsistent texts ( re-
call) and how often it falsely disregards consistent
texts ( precision ), which can be easily computed
using the aforementioned binary labeling scheme.
Therefore, as a meta-evaluation protocol we report
the Area Under the ROC Curve (ROC AUC) with
respect to inconsistent example detection for each
evaluation metric and dataset.
Our thorough survey and assessment of 12 met-
rics draws a clearer picture on the state of evalu-
ating factual consistency. We show that Natural
Language Inference (NLI) approaches, as well as
Question Generation and Answering (QG-QA) ap-proaches achieve signiﬁcantly betterresults on a
wide variety of tasks and datasets. We also show
that NLI and QG-QA are complementary: combin-
ing the two yields even better results and hints at
room for further improvement. Finally, we perform
both quantitative and qualitative analysis of our re-
sults, ﬁnding that all approaches struggle with long
inputs, labeling issues and personal statements –
paving interesting avenues for future work.
To summarize, our contributions are as follows:
(1) We argue that work on factual consistency eval-
uation should be uniﬁed and generalized across
tasks, and standardize 11 published datasets into a
single labeling scheme to corroborate this. (2) We
propose a meta-evaluation protocol that allows
more actionable and interpretable quality measures
than previously reported correlations. (3) We sur-
vey and evaluate 12 diverse metrics in this uni-
ﬁed perspective, showing that large-scale NLI and
QG-QA-based approaches achieve strong and com-
plementary results across tasks. (4) We analyze
our results both qualitatively and quantitatively,
pointing at challenges like long inputs and personal
statements to be addressed in future work.
2 Standardizing Factual Consistency
In this section we elaborate on our re-evaluation
setup. We ﬁrst formally deﬁne what factual con-
sistency refers to in this work. We then detail the
datasets we consider and how we standardize them.
Finally, we discuss the meta-evaluation protocol
we propose for measuring the performance of eval-
uation methods on the standardized datasets.
2.1 Deﬁnitions and Terminology
We deﬁne a text to be factually consistent w.r.t
its grounding text if all the factual information it
conveys is consistent with the factual information
conveyed by the grounding text.While some pre-
vious works distinguished between inconsistent er-
roneous text to inconsistent correct text (Maynez
et al., 2020), we take a strict approach, requiring the
text to be faithful to its grounding text, regardless
of the “correctness” w.r.t the “real world”. In other
words, we consider only the information present
in the input text, not external knowledge, to assess
faithfulness. This enables a more well-deﬁned task,
since determining the truthfulness of a fact w.r.t a3906
general “real world” is subjective and depends on
the knowledge, values and beliefs of the subject
(Heidegger, 2001). This deﬁnition follows similar
strictness in Textual Entailment, Question Answer-
ing, Summarization and other tasks where compre-
hension is based on a given grounding text, irre-
spective of contradiction with other world knowl-
edge. This is also in line with recent work on
evaluating attribution in text generation (Rashkin
et al., 2021a), where humans are required to judge
whether a generated text is attributable to a ground-
ing text. We use the terms consistent ,grounded ,
faithful andfactual interchangeably.
2.2 Standardization Process
We include 11 datasets that contain human anno-
tations w.r.t factual consistency in diverse tasks
(Table 2). Other than the importance of cover-
ing a wide variety of error types, this also allevi-
ates issues of rating quality which may vary across
datasets (Denton et al., 2021).
To allow a uniﬁed evaluation framework we con-
vert all annotations to binary labels that correspond
to whether the entire target text is factually con-
sistent w.r.t the given grounding text or not. We
note that a ﬁne-grained annotation scheme, i.e., a
typology of errors, was proposed for factual con-
sistency (Pagnoni et al., 2021). While useful, most
existing datasets do not include such labels. More-
over, while Machine Translation (MT) evaluation
also showed value in ﬁne-grained annotations (Fre-
itag et al., 2021), it was proposed after years of
improving MT to the level where coarse-grained
annotation is insufﬁcient. We argue that current
grounded generation models are still at early stages
w.r.t factual consistency, making binary labeling
more beneﬁcial now as it enables easier standard-
ization across tasks and domains, with the goal
of bringing researchers to collaborate on a shared
methodology. Binary annotation also correspondsto practical applications where ﬁltering out unfaith-
ful predictions is desired, and is in-line with the
recommendations for human evaluation of attribu-
tion in text generation by Rashkin et al. (2021a).
We next detail the 11 datasets included in TRUE.
2.2.1 Abstractive Summarization
FRANK Pagnoni et al. (2021) proposed a ty-
pology of factual errors, grounded in frame se-
mantics (Fillmore, 1976; Palmer et al., 2005)
and linguistic discourse theory (Brown and Yule,
1983). Based on this typology, they collected an-
notations for model-generated summaries on the
CNN/DailyMail (CNN/DM; Hermann et al., 2015)
and XSum (Narayan et al., 2018) datasets, resulting
in 2250 annotated system outputs. Each summary
sentence was annotated by three annotators. We
take the majority vote for each sentence to get a
sentence-level label and consider a summary as
consistent if all sentences are consistent.
SummEval SummEval (Fabbri et al., 2020) is
a comprehensive study of evaluation metrics for
text summarization. The authors collected human
judgments for 16 model outputs on 100 articles
taken from the CNN/DM dataset, using both ex-
tractive and abstractive models. Annotators were
asked to rate summaries on a Likert scale from 1
to 5, over 4 dimensions: consistency ,coherence ,
ﬂuency andrelevance . Each summary was scored
by 5 crowd-workers and 3 expert annotators. We
label summaries as consistent only if all the expert
annotators gave a consistency score of 5.
MNBM Maynez et al. (2020) annotated system
outputs for the XSum dataset (Narayan et al., 2018).
They sampled 500 articles and annotated sum-
maries generated by four different systems, as well
as the gold summaries. Annotators were asked to
assess whether the summary includes hallucina-
tions. Judgments from three different annotators
were collected for each document-summary pair.
To convert to a binary-label format, we use the bi-
nary consistency decision of whether a summary
contains no hallucinations, and assign a label by
taking the majority vote of the three annotators.
QAGS Wang et al. (2020) collected judgments
of factual consistency on generated summaries for
CNN/DM and XSum. Annotators were presented
with the summaries one sentence at a time, along
with the article, and determined whether each sen-
tence is factually consistent w.r.t the article. Each3907sentence was annotated by 3 annotators, using the
majority vote as the ﬁnal score. To convert to
binary-label format, we consider a summary con-
sistent only if all its sentences are consistent.
2.2.2 Dialogue Generation
BEGIN (Dziri et al., 2021) is a dataset for eval-
uating groundedness in knowledge-grounded dia-
logue systems, in which system outputs should be
consistent with a grounding knowledge provided to
the dialogue agent. BEGIN frames the task as tex-
tual entailment (Dagan et al., 2006; Bowman et al.,
2015), adopting the entailment andcontradiction
labels, and splitting the neutral label into three sub-
categories: hallucination ,off-topic responses and
generic responses. Dialogue responses were gen-
erated by ﬁne-tuning two systems on the Wizard
of Wikipedia ( WW) dataset (Dinan et al., 2019),
in which responses should be grounded in a span
of text from Wikipedia. The generated responses
were split into sentences, and each sentence was
annotated separately. To convert to a binary-label
format, we treat entailed sentences as consistent
and all others as inconsistent.
QHonovich et al. (2021) annotated 1,088 gen-
erated dialogue responses for binary factual consis-
tency w.r.t the knowledge paragraph provided to the
dialogue model, for two dialogue models trained
onWW. Responses were annotated using binary
labels by 3 of the paper authors, one annotator per
response. We use Q’s labels without changes.
DialFact Gupta et al. (2021) introduced the task
of fact-veriﬁcation in dialogue and constructed a
dataset of conversational claims paired with pieces
of evidence from Wikipedia. They deﬁne three
tasks: (1) detecting whether a response contains
veriﬁable content (2) retrieving relevant evidence
and (3) predicting whether a response is supported
by the evidence, refuted by the evidence or if there
isnot enough information to determine. We use
the veriﬁable (i.e., factual, rather than personal)
responses annotated for the third task, treating sup-
ported annotations as consistent and the rest as
inconsistent. In cases where several evidence were
marked as required for veriﬁcation, we concatenate
all evidence sentences to be the grounding text.
2.2.3 Fact Veriﬁcation
FEVER Thorne et al. (2018) introduced FEVER
(Fact Extraction and VERiﬁcation), a dataset for
fact veriﬁcation against textual sources. FEVERwas constructed by extracting information from
Wikipedia, generating claims using annotators,
then labeling whether each claim is supported or
refuted by Wikipedia. Claims can also be labeled
with NotEnoughInfo , meaning that there is not
enough information in Wikipedia to either verify
or refute the claim. Given a claim, the task deﬁned
by FEVER is to ﬁrst extract evidence, then to de-
termine whether it supports or refutes the claim.
In a slightly different framing, the latter stage in
FEVER is to determine whether the claim is fac-
tually consistent or not w.r.t the evidence, which
is aligned with what we aim to measure in TRUE.
We use the development set of the NLI version of
FEVER (Nie et al., 2019, 2020), treating supported
claims as consistent and the rest as inconsistent.
VitaminC Schuster et al. (2021) derived a large-
scale fact veriﬁcation dataset from factual revisions
to Wikipedia pages. Each example includes an
evidence text from Wikipedia and a fact, with an
annotation of whether the fact is supported, refuted
or neutral w.r.t the evidence. The authors collected
factual revisions to Wikipedia articles (pairs of “be-
fore” and “after” sentences), and asked annotators
to write two facts for each pair: one that is sup-
ported by the ﬁrst sentence and refuted by the sec-
ond, and vice versa. When no explicit contradiction
was present, the annotators wrote facts that are neu-
tralw.r.t the evidence. Additional examples were
created by revising examples from FEVER. We
treat examples that include supported facts as con-
sistent, and refuted orneutral facts as inconsistent.
2.2.4 Paraphrase Detection
PA WS Zhang et al. (2019) constructed a dataset
for paraphrase identiﬁcation with 108,463 para-
phrase and non-paraphrase pairs with high lexical
overlap, generated by controlled word swapping
and back-translation, followed by judgments from
human raters. Source sentences were drawn from
Wikipedia and the Quora Question Pairs (QQP)
corpus. We only use the examples with Wikipedia
source sentences and view the binary paraphrase
labels as consistency labels. We note that the deﬁ-
nition of paraphrase is not equivalent to the deﬁni-
tion of factual consistency, as a subset of a source
text is not a paraphrase but may still be factually
consistent with the source. However, PAWS was
constructed such that non-paraphrases usually have
contradicting meanings and is therefore relevant.39082.3 Meta-Evaluation
Previous work on evaluating factual consistency fo-
cused on measuring correlation with human judge-
ments (Pagnoni et al., 2021) to compare different
metrics. However, such system-level numbers are
not very informative when one is interested in eval-
uating the absolute performance of inconsistency
detection methods that perform a binary decision
w.r.t each input. Deutsch et al. (2022) also recently
discuss various issues in measuring system-level
correlations to assess the validity of automatic eval-
uation metrics for summarization.
To conduct a more ﬁne-grained evaluation at the
single example level, we report the Receiver Oper-
ating Characteristic Area Under the Curve (ROC
AUC) w.r.t binary detection of inconsistent exam-
ples.The ROC curve is created by plotting the
true positive rate (TPR, a.k.a. the recall) against
thefalse positive rate (FPR, a.k.a. the fallout) at
different possible thresholds for each tested metric.
Measuring ROC AUC evaluates the different met-
rics without setting a speciﬁc decision threshold.
For datasets with existing development/test split,
we also tune a threshold for the binary consis-
tency/inconsistency decision on the development
set and report the test set accuracy using this
threshold. We tune the thresholds by optimiz-
ing the geometric mean of the TPR and 1-FPR:/radicalbig
TPR∗(1−FPR ).
3 Evaluation Metrics
We compare various standard as well as state-of-
the-art approaches to measure factual consistency.
This comparison should draw a clear picture of
current research on this subject and raise directions
for future work. For example, we expect that robust
metrics should perform well across various tasks
and datasets. We next describe the different metrics
we assess as part of this study. We note that for all
reference-based metrics, we use the grounding text
as the reference. For metrics where the scores are
not in the [0,1] range, we normalize the scores to
be in that range.
3.1 N-Gram Based Metrics
Standard N-Gram matching metrics such as
BLEU (Papineni et al., 2002), ROUGE (Lin, 2004)
and token-level F1 were shown to have weak cor-
relation with factual consistency (Maynez et al.,2020; Honovich et al., 2021). We add them as base-
lines to this study mainly to corroborate this claim
on a wide set of datasets and tasks.
3.2 Model-Based Metrics
BERTScore (Zhang et al., 2020) aggregates sim-
ilarity scores between the BERT contextual embed-
ding of tokens in candidate and reference sentences.
We report results for the BERTScore-precision vari-
ant as it showed better results in preliminary experi-
ments. We use BERTScore version 0.3.11 with the
DeBERTa-xl-MNLI model (He et al., 2021; Nangia
et al., 2017), which is the recommended model as
of the time of writing this paper.
BLEURT (Sellam et al., 2020a,b) is a learned
metric based on BERT (Devlin et al., 2019) for
evaluating text generation. BLEURT includes ad-
ditional pretraining on synthetic data followed by
ﬁne-tuning on human judgements to train a model
that scores system outputs. We use the recom-
mended BLEURT-20 checkpoint (Pu et al., 2021).
FactCC (Kryscinski et al., 2020) is a BERT-
based metric for verifying the factual consistency
of summaries. It is trained on synthetically gen-
erated data obtained by applying rule-based trans-
formations to generate consistent and inconsistent
summaries.
BARTScore (Yuan et al., 2021) evaluates text us-
ing probabilities from force-decoding with a BART
model (Lewis et al., 2020). We use the version ﬁne-
tuned on the ParaBank2 dataset (Hu et al., 2019).
CTC (Deng et al., 2021) measures the average
token-level alignment of the generated text w.r.t
the grounding text using a BERT sequence tag-
ging model. The model is trained to detect hallu-
cinated tokens generated by a BART model in a
self-supervised manner.
3.3 Natural Language Inference Metrics
ANLI The task of Textual Entailment (Dagan
et al., 2006) or Natural Language Inference (NLI;
Bowman et al., 2015) is to determine, given two3909sentences, a hypothesis and a premise , whether the
hypothesis in entailed by the premise , contradicts it,
or is neutral w.r.t to it. The resemblanceof NLI
to factual consistency evaluation has led to utiliz-
ing NLI models for measuring factual consistency
(Thorne et al., 2018; Welleck et al., 2019; Maynez
et al., 2020; Dziri et al., 2021). We trained an NLI
model by ﬁne-tuning T5-11B (Raffel et al., 2020)
on the Adversarial NLI (ANLI; Nie et al., 2020)
dataset. As suggested by Maynez et al. (2020),
we compute the entailment probability with the
grounding text as the premise and the generated
text as the hypothesis and use it as the example-
level factual consistency score.
S C(Summary Consistency; Laban et al.,
2021) is focused on evaluating factual consistency
in summarization. They use NLI for detecting in-
consistencies by splitting the document and sum-
mary into sentences and computing the entailment
probabilities on all document/summary sentence
pairs, where the premise is a document sentence
and the hypothesis is a summary sentence. They
aggregate the NLI scores for all pairs by either tak-
ing the maximum score per summary sentence and
averaging (SC) or by training a convolutional
neural network to aggregate the scores (SC).
We use the publicly available implementation.
3.4 QG-QA Based Metrics
Durmus et al. (2020) and Wang et al. (2020) pro-
posed to use Question Generation (QG) and Ques-
tion Answering (QA) models to automatically eval-
uate factual consistency in abstractive summariza-
tion, showing promising results. Honovich et al.
(2021) employed a similar approach for evaluating
knowledge-grounded dialogue generation.
The steps of the QG-QA approach are as follows:
(1) Questions are automatically generated for spans
in the generated text, such that the answer to a ques-
tion is its respective input span. (2) The generated
questions are answered using a QA model on the
grounding text, resulting in an answer span or a
“no-answer” output. (3) For each question, the two
answer spans from the grounding and the generated
text are compared to get a score. (4) The scores forall questions are aggregated into a ﬁnal score.
Q(Honovich et al., 2021) is a QG-QA method
that employs an NLI model to compare the two
answers for each question, where the grounding
text answer is the premise and the generated text
answer is the hypothesis. We report results for a
re-implementation of Qusing T5-11B as the back-
bone for the QG, QA and NLI models. While Hon-
ovich et al. (2021) validate each generated question
by answering it using a QA model and compar-
ing to the original extracted answer candidate us-
ing exact match, we relax this and instead use F1
token-overlap with a predeﬁned threshold.
QuestEval (Scialom et al., 2021) is a QG-QA
method that measures both factual consistency and
relevance (by reversing the roles of the generated
and grounding texts). The authors trained a model
that weights each generated question according to
the relevance of its answer to appear in the gen-
erated text. Their results showed high correlation
with human judgments in comparison to prior work
on the SummEval benchmark (Fabbri et al., 2021a).
We use the publicly available version.
4 Results
We report the ROC AUCof various metrics on
the standardized datasets in Table 3. The ROC
curves can be found in Figure 2 in the appendix.
SCwas trained on VitaminC which includes ex-
amples from FEVER, so we exclude those datasets
from the average AUC calculation for a more fair
comparison. As all metrics operate in a “zero-shot”
manner on all datasets (except for SCon Vita-
minC and FEVER) and no threshold tuning is re-
quired, we report results on the development sets.
The results show that the NLI-based models
(ANLI, SC) outperformed the other approaches
on 6 datasets, with average AUC of 81.5 and 81.4
for ANLI and SC, respectively. Qoutperformed
the other approaches on 4 datasets, with an average
AUC of 80.7. The next best method, BARTScore,3910
had lower average AUC of 72.2. All other ap-
proaches scored 72 or lower on average across all
datasets (excluding FEVER and VitaminC). As ex-
pected, the simple token-matching based metrics
did not perform well, and for completeness, we
report their performance in Table 9 in the appendix.
We keep the F1 score in Table 3 for convenient
comparison to the other metrics.
One outlier is BEGIN, which is the only
dataset where simple metrics like F1 token overlap
achieved scores higher than 80. We measured the
average overlap between the grounding and target
texts per dataset, and found that BEGIN exhibits a
high difference between grounded and ungrounded
texts in comparison to other datasets (Table 8 in
appendix A), which explains this.
We follow Laban et al. (2021) and perform
signiﬁcance testing through bootstrap resampling
(Efron, 1982), comparing the best method to the
second-best method on each dataset. We perform
interval comparison at p= 0.05andp= 0.01
and ﬁnd signiﬁcantly best results on 6 datasets, 3
achieved by Qand 3 by the ANLI-based model.
Given that no single method outperformed the
rest on all datasets, we hypothesize that the NLI and
QG-QA based metrics are complementary. We test
this by averaging the Q, ANLI and SCscores
per example(Ensemble in Table 3). Indeed, av-
eraging the three methods yields better results on
most datasets and on average, with an increase of
4.5 in ROC AUC from the best single-metric result.
Our results show that a single metric can do
well across all tasks and datasets, with all 3 best
metrics scoring higher than 80 on average on the
11 datasets. This corroborates our hypothesis thatevaluating factual consistency can be uniﬁed, and
we hope such uniﬁed perspective will be adopted in
future work to accelerate progress on the subject.
5 Analysis
Input Length. As QA and NLI models may
struggle with long inputs (Ko ˇciský et al., 2018;
Pang et al., 2021; Yin et al., 2021; Shaham et al.,
2022), metrics based on them may fail when han-
dling long text. To study the effect of input length
on the metrics performance, we unify all datasets
and split examples into 6 bins according to the
grounding length.We focus on the grounding as
the target texts are usually short (see Table 7 in Ap-
pendix A). We measure AUC of the best 3 metrics
according to their overall score for each length bin,
sampling 1,000 examples per bin.
The results are shown in Figure 1. We ﬁnd that
there is a consistent degradation for texts longer
than 200 tokens for all metrics, including SC
which is designed to better handle long text. We
ﬁnd it surprising that the ANLI-based model and
Qstill do relatively well on the longest bin (with
AUC > 0.825) as they perform end-to-end QA and
NLI on text with more than 500 tokens.
Model Size. Model-based metrics are expected
to beneﬁt from increasing the model size. To quan-
tify this we study the effect of using smaller models
for the ANLI, BLEURT and BERTScore metrics.
We compare the average ROC AUC of larger and3911
smaller model variants for each metric. The abla-
tion results are in Table 4. We ﬁnd an advantage of
4.7, 3.7 and 1.3 average ROC AUC for the larger
ANLI, BLEURT and BERTScore variants respec-
tively, showing that larger models indeed allow for
better factual consistency evaluation metrics, and
hinting at potential improvements from using even
larger models.
Qualitative Analysis. We conduct manual error
analysis to point at weaknesses of the different
metrics and present challenges posed by the task.
We analyze 80 examples that were misclassiﬁed by
all three best metrics, as well as 100 examples that
were correctly classiﬁed by one or two of the three.
Out of the analyzed examples, many seem to
have a wrong label. This is especially true for
cases in which all best metrics failed, with annota-
tion errors in 35/80 cases. For the cases where one
or two metrics failed, we found annotation errors in
27/100 cases. To verify that the high annotation er-
ror rate is indeed a result of inspecting the “hardest”
examples and not a general issue in the datasets
we used, we uniformly sample 100 additional ex-
amples, ﬁnding that only 10 had annotation errors.
We therefore stress that the high misannotation rate
indeed characterizes “hard” examples only, and is
not a general property of the datasets we used. This
is inline with the ﬁndings of Freitag et al. (2021),
who showed that in some cases, metrics may be“better” than non-expert annotators. These ﬁndings
demonstrate the potential of automatic methods in
“cleaning” training data by ﬁltering factually incon-
sistent examples.
Despite showing impressive results, the best-
performing metrics fail to detect subtle inconsisten-
cies, as presented in Table 5. This was the case for
21/180 analyzed examples. Metrics that aggregate
scores across parts of a target text, such as Qor
SC, might assign a high score for texts in which
all but a small part is consistent. End-to-end NLI
should predict “contradiction” even when only a
small part of the text contradicts the grounding, but
it may fail to do so. Applying a strict approach
in the aggregation step, like taking the minimum
instead of the average, could potentially remedy
this – with the price of having more false-negatives.
Other errors are caused by domain-speciﬁc chal-
lenges, such as handling personal statements in
dialogues. As shown in Table 5, such statements
may be falsely classiﬁed as ungrounded. This was
the case for 10/62 analyzed dialogue responses. A
possible way to alleviate this would be to automati-
cally exclude non-factual parts from the evaluation.
Ensemble Analysis. As shown in §4, a simple
averaging ensemble using the three best metrics
achieves strong results, outperforming individual
metrics on most datasets. To understand this fur-
ther, we analyze cases in which at least one of
the best three metrics failed, while the ensemble
succeeded. Overall, there were 25,761 such cases,
where in 85.2% of these cases, two out of the three
metrics succeeded, and only one failed. In 14.6% of
these cases, one metric succeeded while the other
two failed, and only in 0.2% of the cases, the en-
semble succeeded while all metrics failed. These
cases are a result of the different threshold used for
the ensemble model vs. the thresholds for the indi-
vidual metrics. We sample 100 of these examples
and manually analyze them. Out of the sampled
examples, 47% were misclassiﬁed by one metric
only, where this metric assigned a borderline score
- i.e., close to the decision threshold. 36% of these
examples were misclassiﬁed by one metric only,
and also with a non-borderline score - i.e., the met-
ric was far from a correct prediction. Other cases
include two, or even three, erroneous metrics.
6 Related Work
Adding to the related work mentioned through-
out the paper, works on uniﬁed evaluation of text3912
generation across tasks include GEM (Gehrmann
et al., 2021), where the focus is on evaluating sys-
tem outputs and not the factual consistency evalua-
tion methods as in TRUE. BEAMetrics (Scialom
and Hill, 2021) proposes meta-evaluation proto-
cols across tasks, but does not focus on factual
consistency. When discussing consistency (“cor-
rectness”) they measure correlations, which are not
sufﬁcient as mentioned in Section 2.3. Chen et al.
(2021) present an adversarial meta-evaluation for
factual consistency evaluators, focused on summa-
rization. Other works on meta-evaluation of factual
consistency across datasets include GO-FIGURE
(Gabriel et al., 2021) FRANK (Pagnoni et al., 2021)
SummaC (Laban et al., 2021) and QAFactEval
(Fabbri et al., 2021b), however they all focus solely
on summarization. Yeh et al. (2021) conduct a thor-
ough assessment of dialog metrics, however not
speciﬁcally around factual consistency. To the best
of our knowledge, our work is the ﬁrst to general-
ize the discussion on evaluating factual consistency
across tasks and datasets, and the ﬁrst to show that
large-scale QG-QA and NLI are strong and highly
complementary – setting better baselines and meta-
evaluation methodology for future work.
7 Discussion and Future Work
We discuss the main takeaways of the TRUE study,
pointing at actionable insights for future work.
First, as QG-QA and NLI-based methods show bet-
ter performance than other approaches, especially
when combined together, we recommend model de-
velopers to use those methods for evaluation when
factual consistency is a priority. As for metric de-
velopers, we recommend using those methods and
the datasets in TRUE when evaluating new metrics.We also suggest reporting ROC AUC rather than
correlations, as it is more interpretable and action-
able. Our proposed binary annotation scheme al-
lows to easily test new metrics across tasks and
datasets, which would be useful for future work.
Finally, we encourage data curators to use the
binary annotation scheme, which is inline with the
recommendations of Rashkin et al. (2021a). Hav-
ing said that, we do not rule out more detailed label-
ing schemes – but rather ask to provide a protocol
for converting such labels into the more general
binary format. Future work may also address the
challenges of long inputs and personal statements
in dialogue, which we point out in our analysis.
8 Conclusions
We presented TRUE, a survey and assessment of
automatic factual consistency evaluation methods.
We standardized various datasets from diverse tasks
into a uniﬁed labeling scheme to perform a thor-
ough comparison of automatic evaluation methods,
showing that large scale NLI and QG-QA based
approaches perform well across multiple tasks and
datasets. We further show that these methods are
highly complementary – hinting at additional head-
room for improvement while pointing on current
limitations. We hope our results and methodology
will encourage a more uniﬁed perspective in future
work to foster progress towards more factually-
consistent NLP applications.
Acknowledgements
We thank Dipanjan Das, Sebastian Gehrmann and
Joshua Maynez for their valuable comments and
suggestions for this work.3913References3914391539163917A Additional Data Statistics
Tables 6 and 7 presents statistics regarding the
length of the grounding text and the generated text
for TRUE’s datasets, respectively.
B Implementation Details
We train all models using the t5x library.
QG-QA For our reimplementation of Q(Hon-
ovich et al., 2021) we use T5-11B as the pretrained
model for QG, QA and NLI, while Honovich et al.
(2021) used T5-Base, ALBERT (Lan et al., 2019),
and RoBERTa (Liu et al., 2019) for the QG, QA
and NLI models, respectively. We use a maximum
length of 2048 tokens for the input. We set the F1
token overlap threshold to 0.54 by tuning it on a
held-out dataset. We use beam search with a beam
size of 4 to generate multiple questions, and use the
ﬁrst question that passes the validation threshold.
NLI We ﬁne-tune a T5-11B model on ANLI (Nie
et al., 2020) for 25Ksteps with a learning rate of
10and a batch size of 32. During inference we
use a maximum input length of 2048 tokens.
C ROC Curves
Figure 2 presents the ROC curves for the dif-
ferent datasets studied in TRUE, using the best-
performing metrics.391839193920