
Srijan BansalSuraj TripathiSumit Agarwal
Teruko Mitamura Eric Nyberg
{srijanb, surajt, sumita, teruko, en09} @andrew.cmu.edu
Language Technologies Institute, Carnegie Mellon University
Abstract
Code-switched (CS) data is ubiquitous in to-
day’s globalized world, but the dearth of anno-
tated datasets in code-switching poses a signifi-
cant challenge for learning diverse tasks across
different language pairs. Parameter-efficient
prompt-tuning approaches conditioned on
frozen language models have shown promise
for transfer learning in limited-resource setups.
In this paper, we propose a novel instance-
based prompt composition technique, PRO-
CS, for CS tasks that combine language and
task knowledge. We compare our approach
with prompt-tuning and fine-tuning for code-
switched tasks on 10 datasets across 4 language
pairs. Our model outperforms the prompt-
tuning approach by significant margins across
all datasets and outperforms or remains at par
with fine-tuning by using just 0.18% of total
parameters. We also achieve competitive re-
sults when compared with the fine-tuned model
in the low-resource cross-lingual and cross-
task setting, indicating the effectiveness of our
approach to incorporate new code-switched
tasks. Our code and models will be available at
https://github.com/srijan-bansal/PRO-CS
1 Introduction
Code-Switching (CS) is the phenomenon of shift-
ing from one language to another in the same con-
text, usually used in an informal way of speaking
or writing (Sitaram et al., 2019; Jose et al., 2020).
Within the scope of this paper, we have focused on
the most commonly used form of code-switching
which happens at the intra-sentential level. With
the advent of social media platforms and the global
rise in multilingual speakers, people generally con-
verse in more than one language and often switch
from one language to another (Parshad et al., 2016).
Code-switching can go beyond the mere insertion
of borrowed words, fillers, and phrases, and in-
clude morphological and grammatical mixing. It isFigure 1: Different settings of PRO-CS, (a) prompt-
composition for En-Hi POS code-switched task with
source languages (En, Hi) and task (POS) prompts (b)
prompt-composition for cross-lingual transfer to En-Bn
POS from related source language Hi-POS prompt, (c)
prompt-composition for cross-task transfer from En and
Hi SA to En-Hi HS.
frequently used to convey stronger emotions or to
express one’s ideas precisely (Sitaram et al., 2019).
Code-switching has gained attention in the NLP
community due to a plethora of unstructured text
data available that require an understanding of in-
termixing of two languages in the same sentence.
Understanding the meaning of such text relies on10243subtasks such as part-of-speech (POS) tagging, sen-
timent analysis (SA), and hate speech (HS), and
the characteristic of CS languages. Recent work on
CS tasks focuses on the identification of sentiments
in online texts (Ravi and Ravi, 2016; Singh and
Lefever, 2020), filtering of hate posts written in
CS language (Ombui et al., 2019), part-of-speech
tagging in CS text (Chandu et al., 2018; Chopra
et al., 2021). Aguilar et al. (2020); Khanuja et al.
(2020) have created benchmarks in a variety of
code-switched language pairs and tasks.
One of the main challenges in the code-switched
setting is the lack of high-quality labeled datasets
across different language pairs and tasks. Re-
cent works (Aguilar and Solorio, 2020; Sengupta
et al., 2022; Khanuja et al., 2020) do not discuss
much about the transferability of models to un-
seen combinations, which is essential for this low-
resource domain. Parameter efficient approaches
such as adapter-based models (Houlsby et al.,
2019), prompt-tuning (Lester et al., 2021; Li and
Liang, 2021) show promise for cross-lingual and
cross-task transfer in multilingual few-shot and
zero-shot settings (Pfeiffer et al., 2020a; Ansell
et al., 2021a).
We take a step forward towards solving code-
switching tasks through parameter efficient prompt-
tuning with multilingual pre-trained language mod-
els (Devlin et al., 2019). Specifically, we wish
to explore the effectiveness of the prompt-tuning
approach for code switching by training prompts
with multilingual LMs. Since code-switching com-
prises of information from two different languages
along with the task information, we also investigate
whether prompt-composition of task and language
specific prompts can be applied for downstream CS
tasks and how efficiently this method transfers to
other code-switched language pairs or tasks. Es-
sentially in this paper, we want to address three
research questions.
•Are prompt-tuning methods that have been
effective in monolingual and multilingual set-
tings also effective in code-switched settings?
•Can we employ prompt-composition using
task and language prompts for downstream
code-switching task?
•How effective is prompt-composition in low-
resource cross-lingual and cross-task settings?
Although current parameter-efficient techniquesshow promise for NLP, we find a big gap between
the prompt-tuning and fine-tuning paradigms for
code-switched NLP tasks. We feel this dispar-
ity is due to language and task complexities, as
it involves switching between two languages. In
this paper, we propose a novel PRO-CS prompt-
composition technique that leverages language
and task specific knowledge captured through lan-
guage and task-based prompts. Task knowledge
in prompts is learned through monolingual task
corpora and language prompts are learned through
masked token prediction via discriminative training
on unlabeled language data.
All these prompts (referred as source prompts)
serve as a pool of stored knowledge that can be
reused for different language pairs and tasks. Un-
like prompt-tuning, which learns prompts from
scratch, we use an attention module to learn
instance-based prompts by weighing the contri-
butions of frozen source prompts and trainable
prompts (referred as target prompts). We also use
prompts trained on language identification, a char-
acteristic task for code-switched language model-
ing to capture mixing between the two languages to
initialize target language prompt. The frozen mul-
tilingual LM is then conditioned on the resulting
prompt prepended with input for downstream tasks
(see Section 3 for more details).
We hypothesize that both monolingual tasks and
language understanding expose different facets of
knowledge required for the code-switch setting.
Our results support this claim, showing that our pro-
posed approach outperforms prompt-tuning across
all datasets. We test the generalizability of our ap-
proach by transferring across tasks and languages
in low-resource setting (see Figure 1). For cross-
lingual transfer, we substitute task prompt of a lan-
guage (Bn-POS) with related language (Hi-POS).
Similarly, for cross-task transfer, we replace task
prompts (X-HS) with related tasks (X-SA).
In summary, the main contribution of our work
are as follows (i) To the best of our knowledge, this
is the first work investigating prompt-composition
for code switching tasks. (ii) We leverage language
and task-related prompts for PRO-CS and achieve
consistent improvement over prompt-tuning. Fur-
ther, we outperform or remain at par with the
fine-tuning performance in most datasets with
just 0.18% of the total parameters. (iii) We
also show the effectiveness of PRO-CS for cross-
lingual/cross-task transfer settings.102442 Background & Related Work
Fine-tuning pre-trained language models (Devlin
et al., 2019; Yang et al., 2019) is arguably the most
prevalent paradigm in NLP research for transfer
learning (Ruder et al., 2019). In multilingual set-
ting, fine-tuning language models trained on data
from multiple languages has shown to be effective
for downstream tasks and cross-lingual transfer
(Devlin et al., 2019; Conneau et al., 2020).
Recent work in code-switched NLP (Khanuja
et al., 2020; Winata et al., 2021) fine-tune large
multilingual models for different downstream tasks.
As the size of the pre-trained multilingual models
(Devlin et al., 2019; Conneau et al., 2020) such
as multilingual BERT (178M), XLM-R (470M),
increases, it becomes computationally expensive to
fine-tune them for ever evolving new tasks and new
code-switching languages. In low-resource scenar-
ios (such as code switching), fine-tuning these large
pre-trained models (Chopra et al., 2021; Nayak
and Joshi, 2022) is often susceptible to overfitting.
Khanuja et al. (2020) attempts to mitigate this by
generating synthetic CS data, which in most cases
is not able to generate diverse examples to mimic
real-world scenarios. Santy et al. (2021) shows that
training multilingual LMs on real code-switched
data is more helpful than training on synthetic data.
Code-switching datasets have very few anno-
tated examples that present an ideal low-resource
setting for parameter-efficient transfer learning. Re-
cent study (Winata et al., 2021) shows promise
in exploring parameter-efficient training (PET)
for code switching using meta-embeddings that
achieve similar performance to fine-tuned multilin-
gual models with fewer parameters.
He et al. (2021) presents a unified view of differ-
ent parameter-efficient training (PET) approaches
like Adapter (Houlsby et al., 2019), Prompt-tuning
(Lester et al., 2021), BitFit (Ben-Zaken et al., 2022),
LoRA (Hu et al., 2021), and sparse fine-tuning
(Guo et al., 2021) that have shown promise as an
alternative to fine-tuning. These techniques are
closely connected and share design elements that
are essential for their effectiveness. We choose
prompt-tuning in this study as it is being thoroughly
explored in the NLP community and can be used
as plug-ins for different tasks and language pairs
with a frozen LM.
Recently, many studies have focused on PET
approaches for multilingual settings. Hyper-X (Us-
tun et al., 2022) uses a hypernetwork to generateweights for adapter modules conditioned on both
tasks and language embeddings. MAD-G (Ansell
et al., 2021b) learns a single model to generate a
language adapter for an arbitrary target language us-
ing the generation of contextual parameters. Zhao
and Schütze (2021) show that discrete and soft
prompting outperform fine-tuning in cross-lingual
transfer. Polyglot prompt (Fu et al., 2022) proposes
a two-tower encoder-based approach for language-
independent prompt generation. Although PET
techniques have been shown to be effective in
monolingual and multilingual settings, this has not
been explored in the code-switching domain.
Compositions of parameter-efficient methods for
tasks and languages for better downstream trans-
fer have been explored in the multilingual domain.
MAD-X learns modular task and language-based
adapters for cross-lingual transfer (Pfeiffer et al.,
2020b). Ansell et al. (2021a) composes language
and task information by selecting a subset of pa-
rameters that change the most during fine-tuning.
It uses this composition to show zero-shot cross-
lingual transfer. Asai et al. (2022) explores prompt-
composition where the target prompt is learned by
attending over multiple source task prompts. Moti-
vated by this idea of prompt-composition and mod-
ular learning of task and language-based parame-
ters for multilingual tasks, we explore the direction
of prompt-composition for code-switching.
3 Method
Prompts can learn different facets of knowledge
from both tasks and languages. In this paper, we
hypothesize that monolingual task-based and lan-
guage prompts can be used to compose instance-
level code-switched task prompts. Such composi-
tions are also compatible with heterogeneous su-
pervision, that is, for different task and language
combinations, and can be useful for initializing
models in a low-resource setting.
3.1 Problem Setup and Motivation
We denote code-switched task as {t;cs(l, l)}
where tis the type of task, and csis the code
switched language comprising of two languages,
landl. We use prompt-tuning (Liu et al., 2021)
with frozen multilingual LM as a backbone model
to first train source task prompts P,P, ...,Pspanning source monolingual tasks t, ..., tin
language l(section 3.2) and source language
prompts P,P, ...,Pspanning across source10245
languages l, ..., l(section 3.3). For the target
code-switched task {t;cs(l, l)}, we learn both
task and language-based target prompts ( P,P)
(section 3.4) along with separate attention mod-
ule (G,G) (section 3.4.1) which attends over
task prompts ( P,P,P) and language prompts
(P,P,P) respectively.
Given an input instance (x,y), we com-
pose an instance-based prompt P(section 3.4.2)
by attending over the relevant source and target
prompts. This Pis then prepended with the cor-
responding instance and passed to the frozen lan-
guage model to learn all the trainable parameters.
A detailed description of the architecture is shown
in Figure 2. We now discuss each component of
the model diagram in the following subsections.
3.2 Source Task Prompt Training
To learn source task prompts, we choose mono-
lingual task corpora of tasks same as that of
code-switching target tasks (like POS, SA). These
prompts are trained using soft prompt-tuning (Liu
et al., 2021) for each monolingual source task by
keeping multilingual LM frozen.
Specifically, for source tasks t, a prompt P∈
Ris learned, where dis the hidden embedding
size of the language model, and kis the number
of prompt tokens. Prompts are initialized with ran-
dom words from the vocabulary which has been
shown to be more effective than initializing with
random vectors (Li and Liang, 2021). Given an
input (x,y)for task tin language l, the input
representation X∈Ris generated by passing
x(sequence of ntokens) through the embedding
layer of the LM. The prompt is then prepended tothis input [P;X]and the model (parameterized by
θ) is trained to predict the correct label y. Depend-
ing on the type of task, we apply a classification
head on top of the [CLS] embedding (for classifi-
cation), or use the last hidden state (for sequence
tagging). During training, only prompt tokens are
tuned to enable the prompt to capture the maximum
amount of information related to the task at hand.
The prompt parameters are trained exactly as fine-
tuning, that is, by minimizing the classification loss
Lfor the prediction of the output label (y):
minL([P;X],y, θ)
3.3 Source Language Prompt Training
We train source language prompts P∈R
for each source language lto capture language-
specific knowledge. Inspired by discriminative pre-
training of language models (Clark et al., 2020), we
use a distilled mBERT as a generator to generate
ˆXfrom masked input Xusing masked lan-
guage modeling. Xis obtained by randomly
masking tokens from the input X. The mBERT
discriminator learns to predict whether the tokens
generated by the generator ( ˆX) match the input
(X) or are fake, as shown in Figure 3. The gener-
ator is usually kept smaller than the discriminator
to enable the discriminator to be better at capturing
mistakes made by the generator. This discrimina-
tive pre-training has been shown to be better than
masked-language modeling (Clark et al., 2020),
and hence we apply the language prompt Pin the
discriminator concatenated with the input embed-
ding of ˆX. We train this model end-to-end with
frozen discriminator model while keeping gener-10246
ator and prompt tokens trainable. We minimize
the weighted sum (denoted by λ) of MLM loss
(L ) of the generator ( θ) and the binary clas-
sification loss ( L) of the discriminator ( θ). We
setλ= 50 for our experiments.
minL (X, θ) +λL([P;ˆX], θ)
3.4 PRO-CS : Code-Switched Task Prompt
Training
Given code-switched task {t;cs(l, l)}, we define
two target prompts PandPto capture task and
language specific information of code-switched
task, respectively. Attention modules GandG
are trained to learn the contribution of task and lan-
guage source and target prompts. Learned attention
weights are then used to compose an instance based
{P}from target task prompt ( P), source task
prompts {P,P}, target language prompt {P}
and source language prompts {P,P}.
3.4.1 Training attention modules
Given input embedding X∈R, and the pool
of source and target prompts, PRO-CS’s attention
modules learn attention weights over these prompts
per instance to compose instance-based prompt
P∈Rwhere kis the number of prompt
tokens. We keep separate attention modules for
task and language, GandGrespectively, to en-
able the model to learn task-based information and
language-based information separately.
Following (Asai et al., 2022), we model atten-
tion modules as bottleneck subnetworks (shown in
Figure 2-D). Input Xis first max-pooled across
sequence length such that ˆX∈R. This is then
passed to sub-network that projects the pooled rep-
resentation of the input to the prompt subspace.
We use W∈Rwhere ( r < d ) and
W∈Rto project the input embedding ˆXtoH∈R. We use SILU as the nonlinear activation
unit. This His then used to calculate the attention
scores on the source prompts {P,P,P,P}
along with the target prompts {P,P}.
H =WˆX
H=W(NonLinear (H))
H=LayerNorm (H)
Asai et al. (2022) computes logits by taking the
dot product of Hwith the max-pooled represen-
tation of P. However, max-pooling leads to infor-
mation loss, resulting in similar attention scores.
We address this by summing k-dimensional vector
obtained by product ( PH) where P∈Rand
H∈R. We use similar architectures for the task
and language attention modules GandGto com-
pute HandHrespectively. P∈ {P,P,P}
is used for GwhileGuses P∈ {P,P,P}.
a=/summationdisplay(PH) b=/summationdisplay(PH)
We apply softmax over logits to compute at-
tention weights, αfor task prompts and βfor
language prompts. We make use of the softmax
temperature introduced by Radford et al. (2021)
and scale the logits by 1/d×exp(K)to prevent
the attention modules from being overconfident,
where K is a hyperparameter and is set to 1 for our
experiments.
α=e
/summationtexteβ=e
/summationtexte
3.4.2 Instance-based prompt-composition
The learned attention weights over source task
prompts, source language prompts, target task, and
language prompt are used to compose the instance-
based prompt ( P). We explicitly add the target
prompts to the weighted sum to ensure the influ-
ence of target prompts in the composition. Thus
target prompts will always be updated even if their
corresponding attention weights are very small.
P= (P+αP+αP+αP)+
(P+βP+βP+βP)
Similarly to the original task-based prompt
training, this instance-based prompt Pis then
used along with Xto learn the CS classifica-
tion/sequence tagging task by optimizing the loss
Lof predicting the label y.
minL([P;X],y,p)102473.4.3 Target prompt initialization
Language identification (LID) is the task of iden-
tifying words from languages landlin code-
switched data and capturing the notion of shift
from one language to another. Various code-
switching models have modeled downstream CS
tasks with language identification as an auxiliary
task (Chandu et al., 2018), which helps the model
by learning code-switching points. Inspired by
that, we initialize our target language prompt P
by a prompt trained on the LID task for the code-
switched language pair to add inductive bias for
the downstream task. The target task prompt Pis
initialized with words from the vocabulary, similar
to source task prompt training and is kept trainable.
The remaining source prompts (both language and
task) are kept frozen.
4 Datasets
We evaluate the performance of our model on a
diverse set of code-switching datasets that span a
variety of tasks and language pairs.
4.1 Code-switched datasets
We make use of datasets used in popular code-
switching benchmarks, GLUECoS (Khanuja et al.,
2020) and LinCE (Aguilar et al., 2020). In this
study, we focus on classification and sequence tag-
ging datasets due to their availability across differ-
ent language pairs. For classification, we use sen-
timent analysis (SA), hate speech (HS), and intent
classification (IN) datasets, and use part-of-speech
tagging (POS), named entity recognition (NER)
datasets for sequence tagging. These datasets span
across multiple domains and the cardinality of their
label set varies across language pairs.
We run experiments on 10 datasets across 4 lan-
guage pairs. Details about the dataset and their
sources are mentioned in Table 1. We evaluate
our model in full data setting on tasks in En-Hi
and En-Es as these two language pairs are most
ubiquitous in the real world. We simulate a low
resource setting for code-switched tasks which do
not have relevant source monolingual task data.
We evaluate cross-lingual and cross-task transfer
in low-resource setting to test transferability of our
proposed prompt-composition.
4.2 Monolingual datasets
Source task prompts : Monolingual datasets used
for training source task prompts are discussed
in appendix (see Table 4). Similar to our target
task setting, source tasks consist of classification
and sequence tagging corpus across different
languages. For languages whose monolingual
source task does not exist, we choose relevant task
in another language to show cross-lingual transfer.
Source language prompts : We used raw
Wikipedia data for source language prompt train-
ing using discriminative language modeling. We
took the latest Wikipedia dump and used WikiEx-
tractorto extract 100k examples per language for
training.
5 Experiment
We describe the baselines used in this paper and
the experimental setup of our PRO-CS approach
in the following subsections. We also discuss the
transfer learning settings on which we evaluate our
model.
5.1 Baselines
We evaluate our proposed technique, PRO-CS
against fine-tuned multilingual BERT (178M train-
able parameters), denoted as FT. Furthermore, we
adopt the popular prompt-tuning technique (Lester
et al., 2021) denoted as PT to compare with the
proposed prompt-composition approach. Prompt-
tuning has been shown to generate competitive
results on the popular SuperGLUE (Wang et al.,
2019) benchmark, but has not been explored in the
code switch setting.10248
5.2 Experimental setting
For code-switching tasks, the relevant source task
and language prompts are chosen for our proposed
prompt composition method (PRO-CS). We run ex-
periments on bert-base-multilingual-cased
as our base model. Due to the unavailability of test
set in most datasets, we use the development set
of each corpus to report our model’s performance.
We create a small subset from the training set
(known as validation set) for hyperparameter
tuning and checkpoint selection. After extensive
tuning, we keep learning rate of 1e-3, batch size
of 32 and maximum number of tokens as 512 for
all the experiments. Since, the label distribution in
datasets is very unbalanced (specially for sequence
tagging), we report the macro F1 score (mean of
the F1 score for each label). All models in target
code-switch tasks are trained for 20 epochs, and
the best checkpoint based on validation metric
is used for evaluation. Both the source task and
language prompts are trained for 40 epochs on
monolingual datasets. All prompts consist of
k= 100 tokens with d= 768 . We run our
experiments on a single 3090 GPU.
Transfer Learning : We evaluate cross-lingual
and cross-task transferability of different ap-
proaches on CS tasks which do not have relevant
source monolingual task data available. In such
cases, we transfer knowledge from high-resource
code-switch tasks by using models trained on them
to initialise models for low-resource task. This
includes backbone model for finetuning, prompts
for prompt-tuning and target prompts and atten-
tion modules for PRO-CS. We further replace the
source task prompt with related prompt from our
pool of monolingual source prompts for PRO-CS.
For cross-lingual transfer, we substitute the non-English source task prompt with prompts trained on
same task for a high-resource language. Similarly,
for cross-task transfer, we use source task prompt
of the most relevant task of the same language pair.
(Refer Figure 1)
We evaluate both the cases of transfer learning
in low resource setting. The low-resource setting
aptly mimics the real-world scenario for CS do-
main, since downstream tasks often have much
lower-quality data. We train models on small sub-
sets of 512 training instances (low-resource) that
are sampled such that the label distribution is bal-
anced. We evaluated these models on the entire
development set.
6 Result
Table 2 compares the performance of our prompt-
composition approach (PRO-CS) with fine-tuning
(FT) and prompt-tuning (PT) on code-switching
tasks. We see that our model outperforms prompt-
tuning by a significant margin across all datasets,
showing that language and task based composition
is more effective in code switching than prompt-
tuning. On average, we achieve an improvement of
3 and 3.5 points in Macro-F1 scores on sequence
tagging and classification tasks compared to the
prompt-tuning approach in the full data setting.
We achieve competitive results compared to fine-
tuning setting and even outperform it for En-Es
SA, En-Es POS and En-Hi SA with just 0.18% of
mBERT model parameters (178M). There is a per-
formance gap between the fine-tuning and PRO-CS
approach on the NER dataset. We believe it is be-
cause the NER corpus is highly imbalanced across
label classes. For example En-Hi NER corpus con-
sists of 7 labels where 4 labels combined have less
than 5% of total examples. Due to this skewed
distribution of labels and limited model capacity
for PRO-CS or PT, there is a drop in performance
compared to FT. We expect this drop to reduce10249
as we increase model capacity through number of
prompt tokens. Also, for En-Es NER, we observed
a lot less code switching in the dataset, which also
contributed to the performance drop.
6.1 Cross-lingual and cross-task transfer
We also compare the models in low-resource cross
lingual and cross-task transfer settings. For cross-
lingual transfer, we make use of En-Hi models to
transfer to En-Bn (Bengali) and En-Ta (Tamil) due
to the similarity between Indian languages. From
Table 2, we observe that our approach performs
better than prompt-tuning for both the languages.
Furthermore, our model either exceeds or remains
at par with the fine-tuned mBERT model. In the
cross-task setup, we make use of the En-Hi senti-
ment analysis (SA) models for English-Hindi hate
speech (HS) and intent classification (IN) task. As
shown in Table 2, we outperform the prompt-tuning
approach and achieve commensurate performance
to the fine-tuning approach. These results clearly
show our method’s generalizability to other low
resource CS language pairs.
7 Analysis
In this section, we perform a detailed analysis of
our model’s performance with increasing number
of examples for cross-lingual and cross-task trans-
fer. Further, we conduct an ablation study to show
the effectiveness of each component in our model.
7.1 PRO-CS performance with increasing
number of examples
We show PRO-CS performance comparison with
prompt-tuning (PT) and fine-tuning (FT) with an
increase in the number of training samples in bothcross-lingual and cross-task settings. We experi-
mented with total training instances ranging from
16 to 512. As shown in Figure 4, for cross-lingual
transfer, PRO-CS is significantly more data effi-
cient compared to both the fine-tuning and prompt-
tuning approaches. Further, our proposed approach,
PRO-CS, even outperforms the fine-tuning ap-
proach when the training data size is very small
in both cross-lingual and cross-task settings. With
increasing number of examples, the performance
of PRO-CS matches that of FT with just 0.18% of
model parameters.
7.2 Ablation study
We evaluate the contribution of each module
of our proposed PRO-CS model by conducting a
thorough ablation study. We ablate PRO-CS with
different configurations (a) target language prompt
not getting initialized from LID prompt; (b) non-
trainable target language prompts; (c) shared atten-
tion subnetworks Gto generate attention weights
over language and task prompts; (d) only using
language prompts; and (e) only using task prompts.
We conduct these ablation studies on two language
pairs (En-Hi, En-Es) for the POS task. Table 3
shows that each of our components contribute to10250the performance of the model. Randomly initial-
izing target language prompt shows a drop of 4
points, indicating the importance of LID prompt
based initialization which gives the model an idea
about code-switching. Additionally, having a dif-
ferent trainable language prompt for the target is
also equally essential. We also observed that it is
important to have separate attention subnetworks
for language and task prompts to capture the con-
tribution of language and task more precisely. The
significance of task and language prompts is fur-
ther supported by the amount of performance drop
when we remove either of them.
8 Limitation
Datasets in code-switching are usually very unbal-
anced making the task harder. Our improvements
in low-resource setting show that with a limited
number of good examples, prompt-composition
methods can achieve good results. Our models
suffer when there is a skewed distribution of data.
We also restrict our models to only classification
tasks as we find that they are more ubiquitous in
the CS domain. With the recent rise in translation
between English and CS data, it will be interesting
to apply our techniques for generation tasks like
translation, open domain question answering, dia-
logue, etc. We have also made an assumption of
existing monolingual corpora for training language-
based prompts, which may not be true for various
low-resource languages where code switching is
prominent.
9 Conclusion
In this paper, we perform a detailed analysis of
prompt-tuning techniques to show that they are
effective in code-switching settings as well. How-
ever we find a significant gap between finetuning
and widely used prompt-tuning. We address this
gap by proposing a novel technique of prompt-
composition PRO-CS for code-switching tasks.
Our approach outperforms prompt-tuning tech-
nique across all 6 datasets in full-data setting with
an average improvement of 3 and 3.5 points on
sequence tagging and classification tasks respec-
tively. This shows that composing prompts from
source task and language prompt is more effec-
tive than training target-task only prompts. It also
achieves competitive results to fine-tuning, even
in low-resource cross-lingual and cross-task set-
ting for both classification and sequence taggingtasks. For future work, we want to investigate these
compositions for encoder-decoder T5 models for
generation-based tasks. It would also be interesting
to see if multi-task training helps in the downstream
code-switch prompt-composition.
10 Ethical Considerations
There are various forms of code-switching that
exist between different Asian and European lan-
guages, for example, Turkish-German (Çetino ˘glu,
2017), Modern Arabic-Egyptian Arabic Aguilar
et al. (2020). In this work, we evaluate our models
on code-switching language pairs that are widely
studied and used in the real world like En-Es, En-
Hi. Our work does not undermine the existence of
other code-switching language pairs. Instead, by
showing effective transfer to other CS tasks, we
aim to create language technologies to support the
rise of CS scenarios on social media platforms and
enable multilingual speakers to express their ideas
with precision. We also believe that our work will
present new directions for future research in the CS
setting.
11 Acknowledgement
This work is partially funded by the Air Force
Research Laboratory under agreement number
FA8750-19-2-0200. The U.S. Government is au-
thorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copy-
right notation thereon. The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of the Air Force Research Laboratory
or the U.S. Government. We are very grateful to
Sanket Vaibhav Mehta for his insightful discussion
and helpful feedback on our work.
References1025110252102531025412 Appendix
12.1 Datasets
Table 4 mentions the sources of different source
task datasets that we have used in our experi-
ments. For Spanish, we translated the English-
sentiment-imdb dataset, since we were not able
to find any publicly available Spanish sentiment
analysis dataset.
For code-switching datasets, we pick different
types of pair of languages for different tasks. We
choose the En-Hi sentiment analysis dataset from
(Patwa et al., 2020), and the En-Es dataset from
(Aguilar et al., 2020). en-ta was collected from
(Chakravarthi et al., 2020) respectively. En-Hi POS
and En-Es POS were collected from (Jamatia et al.,
2016) nad (AlGhamdi et al., 2016) respectively.
The NER for En-Hi and En-Es were taken from
(Aguilar et al., 2020). We also use En-Bn POS
dataset from ICON 2016 workshop. We use the
intent classification corpus for En-Hi from (Baner-
jee et al., 2018). The En-Hi hate speech dataset
was taken from (Bohra et al., 2018).10255