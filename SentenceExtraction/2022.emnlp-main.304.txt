
Jiao SunAnjali Narayan-ChenShereen OrabyAlessandra Cervone
Tagyoung ChungJing HuangYang LiuNanyun PengUniversity of Southern CaliforniaAmazon Alexa AIUniversity of California, Los Angeles
jiaosun@usc.edu
{naraanja,orabys,cervon,tagyoung,jhuangz,yangliud}@amazon.com
violetpeng@cs.ucla.edu
Abstract
The tasks of humor understanding and gen-
eration are challenging and subjective even
for humans, requiring commonsense and real-
world knowledge to master. Puns, in particu-
lar, add the challenge of fusing that knowledge
with the ability to interpret lexical-semantic
ambiguity. In this paper, we present the Ex-
PUNations (ExPUN) dataset, in which we aug-
ment an existing dataset of puns with detailed
crowdsourced annotations of keywords denot-
ing the most distinctive words that make the
text funny, pun explanations describing why
the text is funny, and fine-grained funniness rat-
ings. This is the first humor dataset with such
extensive and fine-grained annotations specif-
ically for puns. Based on these annotations,
we propose two tasks: explanation generation
to aid with pun classification and keyword-
conditioned pun generation, to challenge the
current state-of-the-art natural language under-
standing and generation models’ ability to un-
derstand and generate humor. We showcase
that the annotated keywords we collect are help-
ful for generating better novel humorous texts
in human evaluation, and that our natural lan-
guage explanations can be leveraged to improve
both the accuracy and robustness of humor clas-
sifiers.
1 Introduction
Humor serves multiple purposes and provides nu-
merous benefits, such as relieving anxiety, avoiding
painful feelings and facilitating learning (Buxman,
2008). As a specific example of humor, the creative
uses of puns, wordplay and ambiguity are impor-
tant ways to come up with jokes (Chiaro, 2006).
Pun understanding and generation are particularly
challenging tasks because they require extensive
commonsense and world knowledge to compose
and understand, even for humans. Despite growing
Table 1: Two examples of annotated Keywords (KWD)
and Natural Language Explanations (NLEx) for puns
in our dataset. The highlighted texts are annotated
keywords that contribute to making the text funny.
interest in the area, there are limited amounts of
data available in the domain of humor understand-
ing and generation.
Existing humor datasets are usually only anno-
tated with binary labels indicating whether each
sentence is a joke, pun, or punchline (Hasan et al.,
2019; Weller and Seppi, 2019; Castro et al., 2018;
Mittal et al., 2021). This is insufficient to bench-
mark models’ ability to understand and generate
novel humorous text, since hardly anything mean-
ingful can be learned from such a sparse supervi-
sion signal and coarse-grained annotation.
To facilitate research on humor understanding
and generation, we present the ExPUNations (Ex-
PUN) dataset, in which we augment an existing
dataset of puns from SemEval 2017 Task 7 (Miller
et al., 2017) with detailed crowdsourced annota-
tions of fine-grained funniness ratings on a Likert
scale of one to five, along with keywords denot-
ing the most distinctive words that make the text
funny and natural language explanations describ-
ing why the text is funny (Table 1). In addition, we
collect annotations indicating whether a person un-
derstands the sentence, thinks it is a pun, and finds4590
the joke offensive or inappropriate. Since these
tasks are all highly subjective, we collect multi-
ple annotations per sample, and present a detailed
agreement analysis. We believe our annotations
can be used in many other applications beyond
pun understanding and generation, such as toxicity
detection.
The contributions of our work are threefold:
•We contribute extensive high-quality annota-
tions for an existing humor dataset along mul-
tiple dimensions.
•Based on the annotations, we propose two
tasks, explanation generation for pun classi-
fication and keyword-conditioned pun gener-
ation, to advance research on humor under-
standing and generation.
•We benchmark state-of-the-art NLP models
on explanation generation for pun classifica-
tion and keyword-conditioned pun generation.
Our experiments demonstrate the benefits of
utilizing natural language keywords and expla-
nations for humor understanding and genera-
tion while highlighting several potential areas
of improvement for the existing models.
2 ExPUN Dataset
In this section, we describe our data annotation
procedure, including details of the annotation fields
and our assessment of the annotation quality.2.1 Data Preparation
The original SemEval 2017 Task 7 dataset (Miller
et al., 2017)contains puns that are either homo-
graphic (exploiting polysemy) or heterographic (ex-
ploiting phonological similarity to another word).
The dataset also contains examples of non-pun text.
We sample 1,999 text samples from SemEval 2017
Task 7 as the basis for our humor annotation.
2.2 Dataset Annotation
The annotated fields ( AF) come in the order of:
AF[understandability ]: whether the annotator
understands the text or not, regardless of
whether they perceive it as funny.
AF[offensiveness ]: whether the annotator finds
the text offensive or inappropriate.
AF[joke]: whether the annotator thinks the text
is intended to be a joke.
AF[funniness ]: rate the funniness on a Likert
scale of 1-5, where 1 means very not funny
and 5 means very funny.
AF[explanation ]: explain in concise natural lan-
guage about why this joke is funny. More
specifically, if external or commonsense
knowledge is required to understand the joke
and/or its humor, the annotator should in-
clude the relevant knowledge in the explana-
tion. If the joke is a pun or play on words,
they must provide an explanation of how the
play on words works.4591AF[joke keywords ]: pick out (as few as possi-
ble) keyword phrases from the joke that are
related to the punchline/the reason the joke is
funny. We emphasize that phrases should be
sparse and mainly limited to content words,
can be multiple words long, and the keywords
should be copied verbatim from the joke.
If an annotator rates the instance as not under-
standable, they will skip the rest of the annotation
for that instance ( AF-AF). In addition, if an an-
notator rates an example as not a joke, they can
skip the rest of the annotation ( AF-AF). Table 2
shows two examples in our dataset. The first ex-
ample has two annotators who think the text is a
joke, and therefore it has two explanations. In the
second instance, all annotators unanimously agree
it is a joke. Here, we sample two explanations
from the original five. For both instances, we use
underline to highlight the external commonsense
knowledge in the explanation. If the joke is a play
on words, the explanation also shows how the play
on words works (e.g., the second joke). We show
the full annotation guidelines, including calibrating
examples, in Appendix A.
We crowdsourced 5 annotations per sample us-
ing a professional team of 10 dedicated full-time
annotators within our organization. Before starting
the task, we held a kick-off meeting with the team
to explain the annotation guidelines in detail. We
then conducted 3 pilot rounds for calibration and
iteratively met with annotators, including more de-
tails and examples to address annotator questions.
Finally, we conducted 7 rounds of annotation, each
with between 100-300 puns per round grouped into
minibatches of 50 examples. Each sample in a
minibatch was annotated by consistent subteams
of 5 annotators. After receiving a completed batch
of annotations, we manually examined their qual-
ity and provided feedback on any quality issues,
redoing batches as necessary.
2.3 Dataset Statistics and Quality Control
We report overall dataset statistics in Table 3. For
AF−AF, we count the number of samples la-
beled positive by majority vote. For AF, we com-
pute the average of all funniness scores, excluding
blank annotations, and find that while annotators
recognized most samples as jokes, they did not find
them to be particularly funny. For AFandAF,
we compute lexical statistics of our explanations
and keyword annotations and provide deeper anal-
ysis of these key annotation fields in Section 2.4.
We report inter-annotator agreement for all an-
notation fields in Table 4.For fields AF-AF,
we compute agreement using (1) the average of
Cohen’s kappa scores of each annotator against the
majority vote, and (2) the average Spearman cor-
relation between each pair of annotators. We find
that annotators show moderate agreement when de-
ciding if the given text is a joke ( AF), but lower
agreement on the task of understanding the text
(AF) as well as the much more subjective task of
rating how funny a joke is ( AF). We also find
weak average Spearman correlation between each
pair of annotations for the subjective categories of
offensiveness ( AF),whether the text is a joke
(AF) and joke funniness ( AF).
For the free text fields in AFandAF, we com-
pute averaged BLEU-4 (Papineni et al., 2002) and
METEOR (Banerjee and Lavie, 2005) scores in a
pairwise fashion. We treat each annotator’s expla-
nation (for AF) or list of keyword phrases joined
into a string (for AF) as candidate text, with the
remaining annotators’ annotations as a set of refer-
ences. We find high similarity between joke key-
word annotations, suggesting that annotators iden-
tify similar spans of keyword phrases, and a lower
degree of similarity between pun explanations.
2.4 Dataset Analysis
Explanations. As seen in Figures 1a and 1b, on
average, samples are annotated with multiple expla-
nations, and the explanations are lengthy, spanning
multiple sentences, and lexically diverse (14,7484592
token vocabulary size, with 210,580 tokens over-
all). Figure 3 in Appendix B shows the distribu-
tion of the top 50 most frequent content-words
in our explanations. The frequent use of usually
andoften indicate the explanation of commonsense
knowledge, e.g., thunder and lightning are usu-
ally present in a weather storm or“pain” means
physical discomfort often felt by a hospital patient.
The most frequent words, means andword , indicate
that annotators frequently provide word sense infor-
mation as part of their explanations, while sounds
frequently appears in explanations of heterographic
puns. Each of these most frequent words comprise
less than 2.8% of all tokens in the explanations,
illustrating the rich diversity of our corpus.
Keywords. As seen in Figures 1c and 1d, on aver-
age, keyword phrases in ExPUN, which are derived
from the original puns, are short and sparse (5,497
token vocabulary size, with 27,820 tokens overall).
This follows from our guidelines to annotate key-
words concisely, focusing mainly on content words
that are essential to understanding the joke. Table 5
shows two examples of pun keyword annotations
in our dataset that showcase different annotation
styles among annotators. For instance, one anno-
tator may tend to select wordy keyword phrases
that introduce unnecessary tokens, while another
may omit salient keywords that other annotators
mention. Aggregating these annotations among
annotators to construct a single ground truth set
of keyword phrases is therefore challenging be-
cause of differing annotation styles. The problem
of merging keywords is further complicated be-
cause the keywords from different annotators are
often not aligned well, as different annotators may
annotate varying numbers of keyword phrases and
different spans. Taking these considerations into
account, we propose a keyword aggregation algo-
rithm to address these issues and construct a single
set of aggregated keywords per sample.
Keywords Aggregation. Algorithm 1 in Ap-
pendix C describes our keyword aggregation
method. The algorithm aims to generate a com-
prehensive list of concise keywords for each sam-
ple. First, we compute a reliability score for each
annotation, defined as the average of (# keyword
phrases −# average tokens in each keyword phrase).
The higher the score, the more comprehensive and
concise the keywords from an annotator should be.
We choose the annotator with the highest score to
be the anchor . We note, however, that keyword
annotations are not always error-free; e.g., in the
first example of Table 5, whas an incorrect word
(fancy chairs instead of royal chairs ). Therefore,
for each keyword phrase, we compute the fuzzy
matching score between the anchor’s annotation
with the rest of annotators’ annotations. For each
annotator, we keep the keyword phrase that has
the highest fuzzy matching score with the anchor
annotator’s, with a minimum threshold score of
60.This process produces a filtered keyword list
where each of the remaining keyword phrases look
similar to the anchor’s. Then, we compute the av-
erage fuzzy matching score between the anchor’s
keyword phrase and each element in the filtered
keyword list. We then choose the annotator with4593
the second-highest reliability score to be the an-
chor, and repeat the above process. Finally, by
choosing the resulting keyword phrases that attain
the maximum average fuzzy matching score be-
tween the first and second anchors, we get the final
aggregated keywords for this instance.
3 Experiments
With the collected annotations, we propose two
new tasks, pun explanation and keyword condi-
tioned pun generation, to showcase novel tasks that
our dataset uniquely enables and push the frontiers
of NLU and NLG for humor. Note that the rich
annotations in ExPUN can also enable many other
interesting tasks such us pun keywords extraction,
fine-grained funniness prediction, and others. How-
ever, we prioritize NLG tasks as they are relatively
under-explored compared to NLU tasks. In this sec-
tion, we benchmark current state-of-the-art models’
performance on the proposed tasks.
3.1 Pun Explanation
The task of pun explanation takes a pun sentence as
input and outputs a natural language explanation of
why the pun is funny. This requires extensive under-
standing of background and commonsense knowl-
edge. We hypothesize that existing NLP models
would struggle to generate high-quality explana-
tions for puns. On the other hand, high-quality
explanations can improve humor understanding,
and thus help tasks such as humor classification.
Formally, given text T, our target is to generate
an explanation Eof why Tis funny. Additionally,
we use the explanations to support the task of punclassification, where, given T(and optionally an
explanation E), we output whether Tis a joke.
Data Preparation. For each data sample, we use
the longest human-written explanation from Ex-
PUN ( AF), substituting in the pun text if no ex-
planations exist.For pun classification, we assign
output labels using the majority vote of AF(is
a joke). For both tasks, we split our dataset into
1,699/100/200 for train/dev/test. Dev and test con-
tain an equal distribution jokes to non-jokes, while
training contains 1,299 jokes and 400 non-jokes.
Evaluation Metrics. We do not report lexical
overlap metrics as our primary evaluation metric
for generated explanations because these are not
suited for measuring plausibility (Camburu et al.,
2018; Kayser et al., 2021; Clinciu et al., 2021) or
faithfulness of explanations (Jacovi and Goldberg,
2020). Rather, we follow prior work and use the
“simulatability score ” metric from Wiegreffe et al.
(2021) to measure explanation quality from the
lens of usability of the explanation. It reflects the
utility of explanations by measuring the improve-
ment in task performance when explanations are
provided as additional input vs. when they are not:
acc(IE →O)−acc(I→O), where Idenotes the
input text, Eis the explanation and Ois the clas-
sification of whether Iis a joke. We evaluate how
useful explanations can be by measuring the perfor-
mance increase of acc(IE →O)as we increase the
ratio of samples with explanations in the training
data, and report acc(I→O)as a constant baseline
that uses no explanations.
Models. We use the following model varia-
tions:
Noexplanations. As a baseline, we finetune BERT-
base (Devlin et al., 2019), RoBERTa-base (Liu
et al., 2019) and DeBERTa-base (He et al., 2021)
to classify whether the given text is a joke without
any explanations in the input.
Gold explanations. To find the upper bound of how
useful explanations can be, we augment the input
to the above baseline models with gold human-
annotated explanations in both training and testing.
The majority of non-punny examples (identified
as unfunny by majority vote and thus labeled as
unfunny) contain at least one explanation from an
annotator who marked it as funny. In these cases,4594
we use any provided explanations as E, both in
training and in testing with gold explanations. Oth-
erwise, to construct training examples that have
no annotated explanations, or where explanations
are held out, we try two variants: (1) representing
the missing explanation as an empty string ( “w/
gold expl. ” ), or (2) randomly sampling a negative
explanation from another annotated example to use
as input ( “w/ gold + sampled neg. ” ).
Generated explanations. Following previous work
on explanation generation (Wiegreffe et al., 2021),
we first finetune a T5 (Raffel et al., 2020) model to
generate pun explanations given pun sentences as
input. For text that contains no annotated explana-
tions, we use the pun sentence itself as the output
explanation. We then use gold human-annotated
explanations to train and T5-generated explana-
tions to test the explanation-augmented classifica-
tion models.
ELV (Zhou et al., 2020a). ELV is a probabilistic
framework for text classification where natural lan-
guage Explanations are treated as Latent Variables.
Two modules, an explanation generation module
and an explanation-augmented prediction module
are jointly trained using a variational EM frame-
work. As another baseline, we train an ELV model
for pun classification using the ExPUN dataset.
Results. We show our results on the pun classi-
fication task in Figure 2. Baseline performance
of the no explanations models are shown using
constant dotted lines. Figure 2a shows the upper
bound of performance improvement when mod-
els are provided with gold explanations , indicating
that human-written explanations are useful for this
task, and that including more gold explanations in
training data generally helps. In particular, adding
randomly-sampled negative explanations (“ w/ gold+ sampled neg. ”) further improves the classification
accuracy, showing the utility of our collected expla-
nations in improving model performance. However,
Figure 2b shows that using generated explanations
at test time does not help to improve classifica-
tion accuracy. Using the more carefully-designed
ELV framework to jointly train the generation and
classification modules shows improvement in clas-
sification accuracy (Figure 2c); however, qualita-
tive analysis of the ELV explanations showed that
many generated outputs are not fluent natural lan-
guage, suggesting that performance improvements
may stem more from modeling improvements as
opposed to explanations. Given the huge improve-
ments we see when incorporating gold explanations
during test, we note explanations are clearly highly
valuable if the quality of generated explanations
can be improved.
Table 6 shows examples of T5-generated expla-
nations for given puns. Qualitative analysis shows
that generated explanations often identify the rele-
vant pun word, and can include somewhat accurate
word sense information for one sense of the pun.
However, the model usually fails to explain the al-
ternate word sense and its relation, which is crucial
to understanding the wordplay. The model espe-
cially fails to explain phonological similarity in
heterographic puns; e.g., in the first three examples,
explanations fail to mention alternate words carry ,
whet andhumor . For both pun types, our model
can devolve into repetitively copying words from
the input. Our results exhibit the challenge of gen-
erating good pun explanations and that high-quality
explanations are useful for understanding humor.
3.2 Keyword-Conditioned Pun Generation
The task of keyword-conditioned pun generation
takes human-annotated pun keywords as input and4595
produces novel puns as output. This benchmarks
models’ capability to draw connections among
words to generate novel fluent, sensible, and hu-
morous texts. This is a challenging task with many
downstream applications, such as context-situated
humor generation, a task that involves generating
humorous text in a given situation or context. In
this case, input keywords can come from conversa-
tional context (e.g., chatbot dialogues) or narrative
context (e.g., creative short stories).
More formally, we take as input keywords K,
the pun word pand alternate pun word a,and
produce novel and fluent puns that incorporate the
keywords.Optionally, we also include pun word
sense annotations SandSfrom the original
SemEval 2017 Task 7 annotations.
Data Preparation. For this task, we limit our
data to samples that contain both (1) annotated
human keywords Kfrom ExPUN ( AF), and (2)
pun word sense annotations SandSfrom Se-
mEval 2017 Task 7. There are 1,482 such sam-
ples that have both annotations, from which we
reserve 100 as test data and use the rest for model
training. To construct input human-annotated key-
words for this task, we aggregate keywords for
each sample using the method described in Sec-tion 2.4. Additionally, we evaluate the effect of
finetuning on automatically-extracted keywords in-
stead of human-annotated keywords by automati-
cally extracting keywords for each sample by run-
ning the RAKE (Rose et al., 2010) algorithm on
the pun text.
Evaluation Metrics. We use both automatic met-
rics and human evaluation to evaluate the quality of
generated puns. For automatic evaluation, we cal-
culate word incorporation rate for both pun words
and keywords, which measure the model’s ability
to incorporate all input keywords. Additionally, we
run human evaluation using Amazon Mechanical
Turk, in which we asked Turkers to label whether
or not a given generated pun was successful.
Models. We use the following models:
AmbiPun (Mittal et al., 2022). We use the current
state-of-the-art homographic pun generation model,
AmbiPun, with no further finetuning. We follow
the AmbiPun prompt format: “generate sentence:
K,p,a”.
Finetuned T5(T5FT). We finetune T5-base on
ExPUN using input prompt “generate a pun that
situated in K, using the word p,pmeans S,
ameans S.” The output is the pun itself.
Finetuned T5with pretrain ing(T5PT+FT). To in-
crease the model’s ability to incorporate keywords,
we pretrain T5 on non-pun text. For a given pun
word, we first extract 200 sentences that contain
the pun word from BookCorpus (Zhu et al., 2015),
then use RAKE to automatically extract keywords
for each sentence. We construct examples where
inputs are automatically extracted keywords, and
outputs are sentences from BookCorpus including
pun words. We pretrain a T5 model with this data
before finetuning it on ExPUN.
Results. Table 7 shows results of our pun gen-
eration models. While the AmbiPun baseline
achieves superior word incorporation performance,
our T5PT+FTmodel finetuned using ExPUN key-
words generates successful puns at a higher rate,
showing the value of training on our dataset. Fur-
thermore, while pun word incorporation is im-4596
proved by pretraining on outside sources using
RAKE keywords, using automatically-extracted
keywords when training on in-domain pun text
does not translate to more successful puns. In-
stead, models finetuned with the more carefully-
selected, human-annotated ExPUN keywords gen-
erate puns relatively more successfully than their
RAKE-trained counterparts.
Table 8 shows examples of generated puns from
our ExPUN-T5PT+FTmodel. The model is able to
generate both homographic and heterographic puns
somewhat coherently using one of the pun word
senses. However, while some puns are successful,
Rows 3 and 6 show some ways our model can strug-
gle to generate the respective pun types: it does not
always incorporate the alternate word sense in a
clever or meaningful way, and can stitch copied
input keywords together into incoherent sentences.
Our results show pun generation is a very challeng-
ing task, and that careful selection of pun keywords
and a deeper understanding of humor in wordplay
is essential for generating puns successfully.
4 Related Work
In this work, we contribute annotations for a humor
dataset as well as two humor-related generation
tasks. The work is broadly related to pun gener-
ation, pun detection, explanation generation, and
humor generation. We briefly summarize works in
these directions.
Pun generation. Many of the previous works
on pun generation have focused on phonological
or syntactic patterns rather than semantic patterns
(Miller and Gurevych, 2015; Hong and Ong, 2009;
Petrovi ´c and Matthews, 2013; Valitutti et al., 2013),
thus lacking flexibility. He et al. (2019) make use
of local-global surprisal principle to generate homo-
phonic puns and Yu et al. (2020) uses constrained
lexical rewriting for the same task. Hashimoto et al.
(2018) use a retrieve and edit approach to generate
homographic puns and Yu et al. (2018); Luo et al.
(2019) propose complex neural model architectures
such as constrained language model and GAN. Mit-
tal et al. (2022) generate homographic puns given a
polyseme and try to incorporate the multiple senses
of the polyseme. Tian et al. (2022) proposed a
unified framework to generate both homographic
and homophonic puns leveraging humor principles.
Our keyword-conditioned pun generation task en-
courages models to focus more on the linguistic
structures via pun keywords as we observe that
human-extracted keywords usually reflect the am-
biguity and distinctiveness principles as discussed
in Kao et al. (2016). The keyword-conditioned pun
generation setup can also facilitate more engaging
pun generation scenarios such as context-situated
pun generation (Sun et al., 2022).
Humor generation. With the recent advent of
diverse datasets (Hasan et al., 2019; Mittal et al.,
2021; Yang et al., 2021), it has become easier to
detect and generate humor. While large pre-trained
models have become fairly successful at detection,
humor generation still remains an unsolved prob-
lem. Therefore, humor generation is usually stud-
ied in a specific settings. Petrovi ´c and Matthews4597(2013) generates jokes of the type ’I like my X like
I like my Y , Z’. Garimella et al. (2020) develops a
model to fill blanks in a Mad Libs format to gener-
ate humorous sentences and Yang et al. (2020) edit
headlines to make them funny. More research is
required to generate humorous sentences that are
not constrained by their semantic structure.
Natural language explanation generation. Col-
lecting and utilizing natural language explanations
to help various NLP tasks is an emerging topic.
The earliest work by Ling et al. (2017) collected
natural language justifications, called rationales, to
help solve math problems. However, their setup
is limited to solving math problems given how
their rationales and models were structured. Jansen
et al. (2018) composed a dataset of explanation
graphs for elementary science questions to support
multi-hop inference. Like Ling et al. (2017), they
emphasized the explanations structures. Several
works have introduced large-scale datasets of natu-
ral language explanations for the natural language
inference (NLI) (Camburu et al., 2018; Kumar and
Talukdar, 2020), commonsense reasoning (Rajani
et al., 2019), and hate speech detection (Mathew
et al., 2021) tasks. However, there are no existing
datasets or models that focus on explaining humor,
which is a challenging task that involves common-
sense and world knowledge.
Pun detection. Being able to detect puns can be
an essential step to generating them. SemEval 2017
Task 7 (Miller et al., 2017) introduced the chal-
lenge of pun detection, location detection and sense
interpretation for homographic and heterographic
puns. They also released a dataset which has be-
come the backbone of our and several other related
works. Diao et al. (2019) make use of gated atten-
tion networks to detection heterographic puns. Zou
and Lu (2019) introduce a tagging scheme to jointly
detect and locate puns, and apply this approach to
both heterographic and homographic puns. Zhou
et al. (2020b) jointly model contextual and phono-
logical features into a self-attentive embedding in
their approach for pun detection and location tasks.
5 Conclusion
In this paper, we contribute a dataset of extensive,
high-quality annotations of humor explanation, key-
words, and fine-grained funniness ratings. This is
the first humor dataset with such extensive and fine-
grained annotations. Based on the annotations, wepropose two tasks: pun explanation and keyword-
conditioned pun generation, to challenge state-of-
the-art natural language understanding and genera-
tion models’ ability to understand and generate hu-
morous text. We benchmark several strong models’
performances on the two proposed tasks to validate
the practical usage of the proposed annotations, and
show that our human-annotated explanations and
keywords are beneficial in understanding and gen-
erating humor. Future directions include a deeper
analysis of how to characterize pun explanation
more objectively within our annotation scheme, as
well as further exploration of better models for both
the pun explanation and pun generation tasks.
Acknowledgements
The authors would like to thank Scott Benson and
the rest of the Alexa Data Services Rapid Machine
Learning Prototyping (RAMP) team for all of their
help with preparing and performing the annotation
task. We also thank anonymous reviewers for their
constructive feedback and suggestions that helped
improve the paper.
Limitations
This work focuses on understanding and generation
of puns, a single and very specific form of humor-
ous language. We hope that our annotation schema
and methods can be used in the future to extend to
other forms of humor, e.g., joke generation. Ad-
ditionally, we acknowledge that humor is a highly
subjective area, i.e., what might be perceived as
humorous may differ greatly from one person to
another depending on their unique backgrounds
and experiences. We hope this work can be used
as an initial framework to begin characterizing hu-
mor through human-written explanations, such that
it can be used more broadly to give insight into
what contributes to humorous content for different
individuals and groups.
Finally, since we use pretrained language models
for our generation tasks, we note that this makes
our models susceptible to generating biased or sen-
sitive content. While we do not explicitly address
concerns around bias/sensitive content within our
framework to date, we aim to incorporate these con-
siderations into pun generation as we develop new
models, including methods to filter our inputs and
generated data for toxicity and biased references
that may be deemed offensive.4598Ethics
We hereby acknowledge that all of the co-authors
of this work are aware of the provided ACL Code
of Ethics and honor the code of conduct.
The text in the dataset (puns and non-pun text)
is from the SemEval 2017 Task 7 dataset (Miller
et al., 2017) including jokes, aphorisms, and other
short texts sourced from professional humorists and
online collections. No user data from commercial
voice assistant systems is used. We collect the
human annotation of pun keywords, explanations,
and other meta-fields via full-time employees (with
all employee-entitled fringe benefits) who are hired
by the co-authors’ organization for the purposes
of data annotation and are not co-authors of the
paper. We ensure that all the personal information
of the workers involved (e.g., usernames, emails,
urls, demographic information, etc.) is discarded
in our dataset. Overall, we ensure our pay per task
is above the the annotator’s local minimum wage
(approximately $15 USD / Hour).
References459946004601A ExPUN Dataset Annotation Details
A.1 Annotation Guidelines
Below, we include the annotation guidelines we
used to collect the ExPUN dataset. All pun texts
in the provided examples are from the original Se-
mEval 2017 Task 7 dataset (Miller et al., 2017).
Guidelines You will be provided a CSV file of
short texts, one short text to be annotated per
row. Each row contains the text content as well
as columns for each of the requested annotations.
For each row, read the text carefully, and provide
the following annotations:
1.Mark whether you understood the text with
0/1 (0 didn’t understand, 1 understood the
text).
•If you don’t understand the meaning of
the text (regardless of whether or not it
should be perceived as funny), rate the
sample as 0 (didn’t understand).
•For this assessment, you can use a quick
Google search to look up any vocabu-
lary/terms you don’t immediately under-
stand. However, if the amount of re-
search it would take to understand the
text goes beyond a quick (<1 min) search,
rate the sample as 0 (didn’t understand).
•Example text that was marked “don’t un-
derstand” (0): A doctor’s mistakes go
six feet under; a dentist’s cover an acre.
•If you rate this sample as 0 (didn’t under-
stand), skip the rest of the annotation for
this sample.
2.Mark whether you find the text offensive or
inappropriate with 0/1 (0 not offensive, 1 of-
fensive), meaning the text is racist or is biased
against marginalized groups, or is generally
offensive. If you rate this sample as 1 (is of-
fensive), you may optionally skip the rest of
the annotation for this sample.
3.Mark whether you think the text is intended
to be a joke with 0/1 (0 not a joke, 1 is a joke).
•Text should be labeled as 1 (is a joke)
even if it intends to be humorous, but
falls flat or is a lame/bad joke.•Example text labeled 0 (not a joke): All
that glistens is not gold.
•Example text labeled 1 (is a joke): These
are my parents, said Einstein relatively.
Why is this a joke? Though subtle, the
text is a pun on the word “relatively” that
associates Einstein with his relatives (par-
ents) and his theory of relativity.
•If you rate this sample as 0 (not a joke),
skip the rest of the annotation for this
sample.
4.Rate funniness on a Likert scale of 1-5 (1 very
not funny, 5 very funny).
•Score of 1: A very not funny joke con-
sists of a joke that is not funny at all, or
tries to be funny but does not achieve the
intended effect.
Example of Funniness 1 (not funny):
These are my parents, said Einstein rela-
tively.
•Score of 3: An average joke consists of
a joke that that is average and may elicit
some chuckles (or groans) from you or
others.
Example of Funniness 3 (average funni-
ness): When they told him that his drum
couldn’t be fixed, it didn’t resonate very
well.
•Score of 5: A very funny joke consists
of a good joke that you find humorous
and potentially would want to share/tell
to others.
Example of Funniness 5 (very funny):
Yesterday I accidentally swallowed some
food coloring. The doctor says I’m OK,
but I feel like I’ve dyed a little inside.
5.Explain in concise natural language about why
this joke is funny. If external or commonsense
knowledge is required to understand the joke
and/or its humor, please include the relevant
knowledge in your explanation. If the joke is
a pun or play on words, you must provide an
explanation of how the play on words works.
•Example joke: What do you use to cut a
Roman Emperor’s hair? Caesars.
Bad explanation: The joke is a play on
words about Caesar and scissors.
Good explanation: The joke is a play4602on words: Caesar was a Roman Em-
peror, and “Caesars” sounds like “scis-
sors”, which is something you use to cut
hair.
•Example joke: There was a kidnapping
at school yesterday. Don’t worry, though
– he woke up!
Bad explanation: The joke is a play on
words about kidnapping →kid napping.
Good explanation: The joke is a play on
words. The word “kidnapping” implies
that a kid was taken hostage at school,
but “he woke up” suggests that it was
actually just a kid taking a nap instead.
6.Pick out (as few as possible) keyword phrases
from the joke that are related to the punch-
line/the reason of the joke being funny (writ-
ten as a pipe-separated (|) list of phrases with
spaces).
• Phrases can be multiple words long.
•The keyword phrases should be copied
verbatim from the joke (no need to re-
word them).
•Keep keyword phrases sparse and mainly
limited to content words. The keyword
phrases should not span the entire joke.
As a general guideline, the words in key-
word phrases should make up <50% of
the words in the full joke (though this
may be difficult to achieve for shorter
jokes).
•Example joke: I used to hate maths but
then I realised decimals have a point.
Bad keywords (too dense): I used to hate
maths but | decimals have a point
Good keywords: maths | decimals | point
We note that this is a highly subjective task, since
different people perceive humor differently! We
encourage you to do your best to determine how to
annotate each item as consistently as possible.
Example annotations (funniness ratings are sub-
jective, and may differ from yours!):
•Text: Yesterday I accidentally swallowed
some food coloring. The doctor says I’m OK,
but I feel like I’ve dyed a little inside.
Understand: 1
Offensive: 0
Is a Joke: 1Funniness: 5
Explanation: The joke is a pun. The main
character feels they’ve “died a little inside”
meaning they’ve been changed for the worse
by swallowing food coloring. At the same
time, food coloring contains dye, so the main
character has been “dyed” on the inside by
swallowing some.
Keywords: swallowed | food coloring | dyed a
little inside
•Text: Waiter, there’s a fly in my soup! “I know.
It gives you a nice buzz doesn’t it?”
Understand: 1
Offensive: 0
Is a Joke: 1
Funniness: 2
Explanation: This is both a pun and a ref-
erence to a common joke format. “Waiter,
there’s a fly in my soup!” is an old joke setup
with varying punchlines. Flies make a noise
commonly described as a “buzz”. “Buzz”
can be used as a noun referring to a pleasant
heightened sensation, commonly from drink-
ing alcohol.
Keywords: fly | soup | buzz
•Text: The evil onion had many lairs.
Understand: 1
Offensive: 0
Is a Joke: 1
Funniness: 3
Explanation: This is a pun. An evil lair is a
hideout for a villain in a comic book or show.
Onions are layered vegetables. The joke is
that the onion had many lairs because it was
evil.
Keywords: evil onion | many lairs
•Text: Hope for the best, but prepare for the
worst.
Understand: 1
Offensive: 0
Is a Joke: 0
(No need to fill in any more information in
subsequent columns, as this text is not a joke.)
Additional calibrating examples The following
examples were rated with an average Funniness
rating >= 2 in previous pilot rounds and can be used
to calibrate your rubric for assigning Funniness
scores.4603•Text: Drinking too much of a certain potent
potable may require a leave of absinthe.
Funniness ratings: [3, 4, 2, 1, 2]
Average rating: 2.4
•Text: Animals that tunnel in the soil have to
have an escape root.
Funniness ratings: [3, 3, 1, 2, 2]
Average rating: 2.2
•Text: My friend’s bakery burned down last
night. Now his business is toast.
Funniness ratings: [4, 3, 2, 1, 2]
Average rating: 2.4
•Text: What is the best store to be in during an
earthquake? A stationery store.
Funniness ratings: [1, 3, 2, 2, 3]
Average rating: 2.2
A.2 Feedback from Pilot Rounds
We did a few pilot rounds to help annotators cal-
ibrate on funniness, since not only is funniness
highly subjective, but also since many puns aren’t
“traditionally funny”, but instead more humorous
due to being “clever” or “creative”. Feedback we
received from annotators was mostly around in-
cluding more detailed definitions and examples for
highly-subjective criteria such as “funniness” and
“offensiveness”. We added questions on whether
annotators “understood the text” to help distin-
guish between puns that were not understood vs.
puns that were understood but then marked as “not
funny”, and added clarifying examples of “joke
keywords” to discourage excessive copying of the
input text in the annotations.
A.3 Inter-Annotator Agreement for
Offensiveness ( AF)
We note relative low inter-annotator agreement for
AF, as identifying offensive/inappropriate con-
tent is a highly subjective and complex task, as it
generally covers social stereotypes, biases, aggres-
sive expressions, micro-aggressions, etc. Kappa
looks at agreement of raw scores, while Spearman
computes correlation of ranks. Combining these
differences with the subjectivity of the task could
explain the disparity between Kappa and Spearman
scores for AF.
B Analysis of Annotated Explanations
B.1 Frequent Explanation Keywords
Figure 3 shows the distribution of the top 50 most
frequent words in our annotated explanations (after
removing stop words and punctuation, as well as
the task-specific words “pun” and “joke”).
B.2 Explanation Sentence Templates
To further explore what kinds of explanations an-
notators have provided within ExPUN, we use a
simple templatization scheme to uncover common
patterns. Given an input pun and explanation pair
from the dataset, we templatize an explanation by
replacing any content words (non-stop words) from
the pun that show up in the explanation. For ex-
ample, for the pun “I wrote a novel about a fellow
who had a small garden. It didn’t have much of a
plot.”, the explanation sentence “This is a play on
the word plot.” would become the template “this is
a play on the word [X].” We then count the number
of unique templates across the dataset.
Table 9 shows counts frequency counts for dif-
ferent selected templates found in ExPUN. The top
half of the table shows instances of highly-frequent
explanations, such as “[X] sounds like [X]” indi-
cating a heterographic pun. The bottom half of
the table shows examples of unique templates that
show up only once but exemplify rich explanations
that include context-specific words that are useful
for pun understanding. We note that the more fre-
quent templates help to characterize common ways
to explain the humor within puns, while the unique
templates serve as highly-informative descriptions4604
that can aid in the pun classification task (e.g., a de-
tailed, contextualized definition of a word/phrase).
C Keyword Aggregation Algorithm
We propose the keyword aggregation algorithm in
Algorithm 1 to merge keywords annotation among
different workers.
D Classifier Implementation Details
We finetune pretrained language models for classi-
fying whether given text samples are jokes, and we
use HuggingFace (Wolf et al., 2020) throughout our
implementation for accessing model checkpoints
and modeling. For hyper-parameter search, we
tried the combinations of learning rate { 1e,3e,
1e,3e} * training epoch {3, 10, 20}. The
final hyperparameters for bert-base, roberta-base
and deberta-base are: learning rate 1e, training
epoch 20 and training batch size 32. For roberta-
large-mnli and bart-large-mnli models, we reduce
the training epochs to 3 and training batch size to 8.
We choose the checkpoint with the best accuracy
on the dev set for inference.
For ELV model, we use the released code and
inherited most of their default hyperparameters for
ELV-sa .We change the training batch size per
GPU to 4 to accelerate the training.Algorithm 1 Keyword Aggregation Algorithm
E T5 Implementation Details
We finetune multiple T5 models (Raffel et al., 2020)
in our work, and we use T5-base from SimpleT5
throughout our implementation. We use 512 and
256 for the maximum source length and the maxi-
mum target length respectively. As the optimizer,
we use AdamW (Loshchilov and Hutter, 2019) with
a learning rate of 0.0001. For the pretraining stage,
we finetune T5 for 3 epochs on retrieved BookCor-
pus data. During the finetuning stage, we train each
model on a Tesla V100 with a batch size of 8 for 30
epochs. During inference, we use beam search as
the decoding method with a beam size of 2. We ter-
minate decoding when the EOS token is generated
or the maximum target length is reached.4605