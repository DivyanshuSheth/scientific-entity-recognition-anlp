
Zhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang,
Guangyong Chen,Xin Jiang,Qun LiuGuangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,
Shenzhen Institute of Advanced Technology, Chinese Academy of ScienceUniversity of Chinese Academy of SciencesHuawei Noah’s Ark LabHuawei Cloud ComputingZhejiang Lab
zw.wan1@siat.ac.cn, gychen@zhejianglab.com
{yinyichun, zhangwei379, shijiaxin3, shang.lifeng, jiang.xin, qun.liu}@huawei.com
Abstract
Recently, domain-specific PLMs have been pro-
posed to boost the task performance of specific
domains (e.g., biomedical and computer sci-
ence) by continuing to pre-train general PLMs
with domain-specific corpora. However, this
Domain-Adaptive Pre-Training (DAPT; Gu-
rurangan et al. (2020)) tends to forget the
previous general knowledge acquired by gen-
eral PLMs, which leads to a catastrophic for-
getting phenomenon and sub-optimal perfor-
mance. To alleviate this problem, we pro-
pose a new framework of General Memory-
Augmented Pre-trained Language Model ( G-
MAP ), which augments the domain-specific
PLM by a memory representation built from
the frozen general PLM without losing any gen-
eral knowledge. Specifically, we propose a
new memory-augmented layer, and based on
it, different augmented strategies are explored
to build the memory representation and then
adaptively fuse it into the domain-specific PLM.
We demonstrate the effectiveness of G-MAP
on various domains (biomedical and computer
science publications, news, and reviews) and
different kinds (text classification, QA, NER)
of tasks, and the extensive results show that the
proposed G-MAPcan achieve SOTA results
on all tasks.
1 Introduction
Pre-trained Language models (PLMs), such as
BERT (Devlin et al., 2019) and RoBERTa (Liu
et al., 2019b), have achieved promising perfor-
mance on NLP tasks. Typically, these general
models are firstly pre-trained on large unlabeled
corpus and then directly fine-tuned on downstream
tasks. However, there is an inherent gap in text
distribution between unlabeled pre-training corpusFigure 1: Masked LM (MLM) loss of RoBERTa on
50K randomly sampled documents from each domain
before and after DAPT. Figure A and B denote the infer-
ence loss of general RoBERTa-base and domain-specific
PLMs on the samples of biomedical (BM) and computer
science (CS). Figure C means the loss of these models
on the samples from the pre-training (PT) corpus of
RoBERTa. We report the results of (Gururangan et al.,
2020) and lower MLM loss is better.
and labeled task corpus, which leads to the dis-
tribution shift problem (Gururangan et al., 2020)
and makes PLMs perform poorly on some domain
tasks (Beltagy et al., 2019; Lee et al., 2020b). To
address this shift problem, the domain-adaptive pre-
training (DAPT) is proposed (Huang et al., 2019;
Beltagy et al., 2019; Gururangan et al., 2020; Lee
et al., 2020b) to further pretrain general PLMs on
large-scale domain corpora, achieving better per-
formance than general PLMs.
Although DAPT can effectively learn the do-
main distribution of the target task, its continual
pretraining process updates the parameters of gen-
eral PLMs, which inevitably leads to partial general
knowledge being forgotten. This catastrophic for-
getting (Goodfellow et al., 2014; Li and Hoiem,
2016; Thompson et al., 2019) phenomenon is veri-
fied in Figure 1, where we observe that the domain-
specific PLMs show better results than general
PLMs on domain corpus, but perform worse on
the general corpus. We argue that this forgotten
knowledge is beneficial for domain-specific PLMs
and should be used to improve their generalization
ability on domain tasks.
To alleviate the catastrophic forgetting , we pro-
pose a simple yet effective memory-augmented6585framework named General Memory- Augmented
Pre-trained model ( G-MAP ). In addition to the
backbone domain-specific PLM, G-MAP intro-
duces a new memory-augmented layer. It explicitly
incorporates the representation built from a frozen
general PLM as the memory to make the back-
bone model access the complete general knowl-
edge. Then, a new proposed memory-attention
within the memory-augmented layer enables the
domain-specific PLM adaptively combine the mem-
ory representation and the domain-specific repre-
sentation. Using the memory built from the frozen
general PLM has two advantages: (1) frozen PLM
never suffers from forgetfulness since the param-
eters remain unchanged (Levine et al., 2022); (2)
it doesn’t require additional training for the gen-
eral PLM during fine-tuning. However, building
and fusing memory into a backbone model is es-
sentially a many-to-many scenario, where we need
to choose which layer output of the general PLM
as the memory representation, and which layer in
the domain-specific PLM should be fused. Thus,
we propose several memory-augmented strategies
for better building and then combining the memory
representation into domain-specific PLM.
We evaluate our G-MAP on text classification,
Question Answering (QA) and Name Entity Recog-
nition (NER) tasks covering four domains: biomed-
ical science, computer science, news, and reviews.
Experimental results demonstrate that G-MAP out-
performs existing baselines on all tasks. We com-
pare different memory-augmented strategies, and
the results show that the proposed chunk-based
gated memory transfer strategy achieves the best
results. In addition, for the memory representation
building, we empirically find that the freezing way
is better than the unfreezing one, which also has
better training efficiency. Furthermore, we apply
the proposed framework to a small-scale domain
pre-training setting and find that G-MAP is also
practical in achieving lower MLM loss. Our contri-
butions are summarized below:
•We empirically find that forgotten general knowl-
edge due to catastrophic forgetting can benefit
the domain-specific downstream tasks since it
can improve PLMs’ generalization ability.
•We propose a novel G-MAP framework, which
introduces several memory-augmented strategies
to construct the memory representation from the
frozen general PLM effectively and then fuse it
into the domain-specific PLM by a new memory-
augmented layer.
•We conduct extensive experiments on various
domain-specific tasks, including text classifica-
tion, QA, and NER, the results demonstrating
that our G-MAP outperforms existing baselines.
2 The Method of G-MAP
In this section, we first overview the G-MAP frame-
work. Then we detail a new memory-augmented
layer that fuses general knowledge into domain-
specific PLMs. Finally, we propose different
memory-augmented strategies, including single-
layer memory transfer, multiple-layer memory
transfer, gated memory transfer, and chunk-based
gated memory transfer.
2.1 Overview
Our G-MAP framework aims to tackle the catas-
trophic forgetting of domain-specific PLMs by us-
ing the memory cache built from the frozen gen-
eral PLMs, which is illustrated in Figure 2. Given
an sequence x= [x, x, . . . , x, . . . , x]withx
denoting the t-th token, general PLMs output the
contextual representations of the input tokens as
the memory cache, which is fed into the domain-
specific PLMs to build final representation for do-
main tasks:
where θandθare the parameters of general
and domain-specific PLMs, respectively. We only
update the θand the θis frozen when fine-
tuning. The general PLM could be a BERT or6586RoBERTa, which contains llayers of Transformer
(Vaswani et al., 2017) encoder blocks and outputs a
set of hidden states denoted as a memory cache
M=/braceleftbig
M, M, . . . , M/bracerightbig
. In the G-MAP
framework, the domain-specific PLM utilizes a new
memory-augmented layer to adaptively incorporate
the memory representation built from the memory
cache Mand enhance its generalization ability.
Note that memory-augmented layer is built by only
the parameters of the original Transformer layer
without adding new ones. Moreover, we explore
different memory-augmented strategies and further
propose the chunk-based memory transfer strat-
egy, which fully uses the memories from different
chunks of the general PLM.
2.2 Memory-Augmented Layer
Memory-augmented layer differs from the tradi-
tional Transformer layer only in the multi-head self-
attention module. The new memory-augmented
attention module is proposed to fuse the memory
representation into the domain-specific PLM, de-
noted as memory-attention. Its main idea is to
linearly transform the memory representation into
new pairs of (keys, values) and concatenate them
into the back of pairs produced by the domain-
specific PLM. Then the multi-head self-attention
is performed to adaptively fuse this new concate-
nated representation. The whole process reuses
the parameters of the Transformer layer of domain-
specific PLM and does not introduce any new pa-
rameters.
Specifically, if i-th Transformer layer is a
memory-augmented one, it obtains the domain-
specific representation Hfrom the previous
layer and the memory representation Mas the
input and fuses them by the following way:
where Mis the memory representation directly
extracted from the memory cache Mor effectively
constructed by some adaptive aggregation strate-
gies, which has the same shape as the intermediate
hidden state Hof the domain-specific PLM, k
means the number of heads and Wis a train-
able parameter matrix. Then, Mis linearly trans-
formed into new pairs of (keys, values), which were
appended to the last of domain-specific ones:
where W,WandWare trainable parame-
ters to generate queries, keys, values respectively,
andjrefers to j-th attention head. Then the self-
attention is performed on the queries and merged
pairs of (keys, values) as follows:
where dis the head dimension acting as a scal-
ing factor. Firstly, a unified attention matrix is
computed by the standard scaled dot-product of
each query against the keys of general memory
and the domain-specific keys. Then, a softmax
operation gets the normalized scores that weigh
and sum these concatenated values. Without ad-
ditional parameter updates for the general PLM,
domain-specific PLM can dynamically capture use-
ful general knowledge and ignore noisy informa-
tion through the memory-augmented layer.
2.3 Memory-Augmented Strategies
The remaining problem is how to build the mem-
ory representation Mfrom the memory cache
Mand which layer of the domain-specific PLM
should be the memory-augmented layer to fuse
M. Essentially, it is a many-to-many layer as-
signment problem between the general PLM and
the domain-specific PLM. To study the effect of
layer assignment, we propose and compare differ-
ent strategies, as shown in Figure 3.
Single-Layer Memory Transfer We first consider
a single-layer memory transfer approach, where
the last hidden state of the memory cache Mis
extracted as Mand then it is fused into one layer
of domain-specific PLM with memory-attention.
We choose the layer near the top of the domain-
specific PLM model as the memory-augmented
layer which performs best in the experiment. This
strategy does not require additional parameters.
Multiple-Layer Memory Transfer The single-
layer memory transfer may ignore the knowledge6587
learned from shallow layers of the general PLM. To
perform layer-wise interaction between the general
PLM and the domain-specific PLM, we propose
a multiple-layer transfer strategy. This strategy
leverages all hidden states from the memory cache
Mas the memory representations and then fuses
them into the corresponding layers of the domain-
specific PLM, which also does not introduce any
new parameters.
Gated Memory Transfer Multiple-layer memory
transfer uses the hidden states output by all layers
of the frozen general PLM as the memory repre-
sentations, which inevitably introduces homoge-
neous and noisy information. To avoid the problem,
we further propose the strategy of gated memory
transfer, which firstly exploits the token-level gate
mechanism to adaptively weigh and sum represen-
tations of different layers into a memory represen-
tation, and then it will be fused into one layer of
domain-specific PLM. We also choose the layer
near the top of the domain-specific PLM as the
memory-augmented one which achieves optimal
performance in the experiment. The gate fusion
mechanism is formulated as below:
where mis the token representation, tdenotes the
token index, lis the layer index, nis the length
of tokens and grefers to a linear layer. We utilize
a softmax function to calculate the importance of
tokens in different layers. Therefore, the output
token representation mis obtained by weighing
thet-th token representations from different layers
with their corresponding importance α. Finally,
the built memory representation Mis fused to the
memory-augmented layer.Chunk-based Gated Memory Transfer The pre-
vious work (Liu et al., 2019a; Phang et al., 2021)
have observed that the representations from up-
per layers and lower layers of pre-trained lan-
guage models are significantly different. Motivated
by this observation, based on the gated memory
transfer, we further propose a chunk-based vari-
ant, which separates the layers of general PLM
into a high-level chunk and a low-level chunk, and
then apply the gate fusion strategy to get upper
and lower-layer memory representations Mand
M, respectively. Finally, we fuse them into two
memory-augmented layers in the domain-specific
PLM, and the details of different layer selections
for this strategy are presented in Section 4.3.
3 Experiments
In this section, we first introduce the evaluation
tasks and metrics. Then we illustrate the baseline
methods, and implementation settings. Finally, we
conduct the experimental analysis of G-MAP.
3.1 Datasets and Metrics
Datasets We evaluate our model on three
tasks: text classification, QA, and NER. For
text classification, we conduct experiments on
eight tasks that cover four domains, including
C P (Kringelum et al., 2016) and RCT
(Dernoncourt and Lee, 2017) in the biomedical
domain, ACL-ARC (Jurgens et al., 2018) and S-
ERC (Luan et al., 2018) in the computer science
domain, H P (Kiesel et al., 2019) and
AGN (Zhang et al., 2015) in the news domain,
H (McAuley et al., 2015) and IMDB
(Maas et al., 2011) in the reviews domain. In addi-
tion, we use micro-F1 as the metric for ChemProt
and RCT, and use macro-F1 for the other datasets
following (Gururangan et al., 2020). For NER, we
use two datasets, including NCBI-Disease (Dogan
et al., 2014) in the biomedical domain, CoNNL-
2003 (Sang and Meulder, 2003) in the news do-6588
main. We use the F1 score as the evaluation metric.
For QA, we utilize two datasets including Med-
ication (Pampari et al., 2018) in the biomedical
domain, NewsQA (Trischler et al., 2017) in the
news domain. We use the Exact-Match (EM) and
the F1 score as the evaluation metrics. The detailed
description and statistics of each task are shown in
Appendix A.
3.2 Baselines
In our experiment, all the baselines are built on
the RoBERTa-base. The details baselines of text
classification tasks are described as follows:
•Fine-Tuning : directly fine-tuning the general
PLM for downstream tasks.
•DAPT (Gururangan et al., 2020): pre-training
the general PLM with large-scale domain unla-
beled corpora to get the domain-specific PLM
then fine-tuning it.
•Logits Fusion : a straightforward method
combines the frozen general PLM and the
domain-specific PLM by adding their logits.
This method is optimized end-to-end and does
not include any memory-augmented strategies
in the model.
•Ensemble LMs : an ensemble method that
adds the predicted probabilities of the fine-tuned general and the domain-specific PLMs
for final prediction.
•TAPT (Gururangan et al., 2020): task-
adaptive pretraining continues to pre-train the
PLM on the training dataset, and then we fine-
tune it for the downstream tasks.
For the NER and QA tasks, in addition to these
above baselines, we also introduce KALA (Kang
et al., 2022). Since KALA is only verified in QA
and NER, we use it as a baseline for these two
tasks.
•KALA : constructing an entity memory and
knowledge graph on a task-specific domain
and then augmenting PLM by this additional
knowledge.
3.3 Implementation
We implement the G-MAP framework based on
RoBERTa-base. For the domain-specific PLM, we
use the released pre-trained weights DAPT. More
details on fine-tuning of the downstream tasks are
shown in Appendix B.
3.4 Results and Analysis
Our experiment results on the domain-specific clas-
sification tasks are shown in Table 1 and 2, the6589
results of QA and NER tasks are shown on Table 3.
Performance on Domain Classification Tasks
From Table 1, we can observe that G-MAP with the
proposed chuck-based gated memory transfer can
achieve better results than all the baselines, which
proves that incorporating memory from the general
frozen PLM is beneficial for the domain-specific
PLM. Specifically, the strategy of chunk-based gate
memory transfer outperforms other strategies, we
conjecture that it adaptively selects the token-level
information across different layers and adequately
utilizes the general knowledge from both the high-
level and low-level chunks. However, we also ob-
serve that multiple-layer memory transfer has little
improvement compared with the baselines, because
it incorporates excessive redundant and noisy infor-
mation from the general PLM without the proposed
gated fusion. Besides, single-layer memory trans-
fer is a simple yet effective strategy achieving better
than the baselines and the non-gated fusion strategy
of multiple-layer memory transfer.
Since the chunk-based gate memory transfer
strategy achieves the best performance compared
with the baselines, we use it as the default memory-
augmented strategy within G-MAP in the following
experiments.
Comparison with Further TAPT Further task-
adaptive pre-training (TAPT) has been proven to
improve the domain-adaptive pre-training (DAPT)
(Gururangan et al., 2020). To demonstrate the ef-
fectiveness of G-MAP on TAPT, we build a G-
MAP framework that replaces the domain-specific
pre-trained PLM with the task-adaptive pre-trained
PLM. From Table 2, we find that our G-MAP also
outperforms DAPT+TAPT on all datasets, indi-
cating that the proposed framework is general for
different backbone models, including the domain-
adaptive and the task-adaptive PLMs.Effectiveness for QA and NER We also evaluate
G-MAP on the tasks of QA and NER, and the ex-
periment results are shown in Table 3. From the
results, we see that our method achieves better re-
sults than the baselines on all datasets, especially
KALA (Kang et al., 2022), which spends a con-
siderable effort to construct entity memory and
knowledge graph from the contexts. These results
further demonstrate the effectiveness of G-MAP.
4 Further Discussion
In the following sections, we conduct some de-
tailed analysis of G-MAP to demonstrate the effec-
tiveness of the frozen general PLM and memory-
attention. Moreover, we apply the proposed frame-
work in the pre-training stage and study the effect
of layer selection on performance.
4.1 Effectiveness of Frozen Memory
We compare the frozen and unfrozen ways when
building the general memory representation, and
the results are shown in Table 4. From the table,
we observe that the frozen method is better than
the unfrozen model on all datasets, and both are
better than the baseline DAPT. We argue that using
the frozen memory has two advantages: (1) more
efficient in model training without updating the
parameters of the general PLM; (2) keeps general
knowledge from PLM unchanged when fine-tuning,
so it does not lead to a forgetting problem.
4.2 Effectiveness of Memory-Attention
To study the effectiveness of our proposed
memory-attention module, we compare it with
other attention-based variants within a memory-
augmented layer. Specifically, cross-attention is an
attention module widely-used in the multi-modal
learning (Li et al., 2021; Zeng et al., 2021), we
apply it to adaptively fuse the memory represen-6590
tationMand the output representation from the
self-attention module. We also include the gate-
attention (Wu et al., 2022) as the fusion baseline,
which utilizes a gate mechanism to weigh and sum
the local and external memory for long-sequence
modeling. As shown in Table 5, our memory-
attention module outperforms other variants with-
out additional trainable parameters.
4.3 Layer Selection for Memory-Attention
Besides the strategy of multiple-layer memory
transfer, other strategies need to do the layer se-
lection. For the strategies of single and gated-
memory transfer, we fuse the memory represen-
tationMinto different layers {3,6,9,12}in the
domain-specific PLM and find that the layer 9 as
the memory-augmented layer can achieve the best
performance in both strategies. We present more
detailed results in Appendix C. For the chunk-
based gate memory transfer strategy, we experi-
ment with transferring the memory representation
of the high-level chunk into layers 7 to 12, and
the other one of the low-level chunk into layers
1 to 6, which keeps the same layer interval be-
tween the two memory-augmented layers in the
domain-specific PLM. The experimental results are
shown in Figure 4. The results show that there
is an increasing tendency when placing memory-
augmented layers to the top of the domain-specific
PLM. Finally, we choose layers 6 and 12 as the
memory-augmented ones for the strategy of chunk-
based gated memory transfer.
4.4 Apply G-MAP in the Pre-training Stage
In the previous experiment, we incorporated the
domain-specific PLM with the G-MAP framework
in the fine-tuning stage. In this section, we fur-
ther study whether G-MAP is beneficial for the
pre-training stage. To this end, we randomly sam-
ple 50k documents from general, biomedical and
computer science domains (Lo et al., 2020), re-
spectively. In addition, we randomly split 70 %of
the data from each domain as the pre-training sam-
ples and the rest data as the test samples. More
details about pre-processing the domain samples
are shown in Appendix B. Then, we pre-train the
models on the pre-training samples and calculate
the masked LM loss on the test samples. From
the experiment results shown in Figure 5, com-
pared with baseline DAPT, we observe that utiliz-
ing G-MAP can reduce masked LM loss on the
biomedical, cs, and general domains. These results
demonstrate that the proposed G-MAP also miti-
gates catastrophic forgetting during the adaptive6591pre-training.
5 Related work
Domain Adaptation for PLMs Recently, the
domain shift problem of PLMs has attracted in-
creasing research (Beltagy et al., 2019; Huang
et al., 2019; Lee et al., 2020b; Gururangan et al.,
2020) since the domain discrepancies between
the pre-training corpora and the downstream tasks
can lead to a significant performance drop. To
bridge the domain gap, SciBERT (Beltagy et al.,
2019) and BioBERT (Lee et al., 2020b) further
pre-train BERT with 1.14M scientific papers from
Semantic Scholar corpus and biomedical docu-
ments, respectively, which can improve the per-
formance of domain-specific NLU tasks compared
with general BERT. Also, Gururangan et al. (2020)
proposed domain-adaptive pre-training (DAPT)
and task-adaptive pre-training (TAPT). Concretely,
DAPT continues to pre-train the PLM with domain-
specific corpora, while TAPT directly pre-trains the
PLM on the task dataset. Moreover, BT-TAPT (Lee
et al., 2021) inherits the crucial step of TAPT and
leverages the back-translated strategy to augment
the task data to improve the performance of PLM.
TAPTER (Nishida et al., 2021) equips TAPT with
domain-specific word embedding regularization to
improve fine-tuning performance. However, the
above approaches suffer from catastrophic forget-
ting of general domain knowledge after adaptive
pre-training, which leads to sub-optimal perfor-
mance on downstream tasks.
Catastrophic Forgetting Catastrophic forgetting
is a common phenomenon for continual learning,
and it occurs when a training model forgets pre-
viously learned knowledge and over-fits to new
tasks (Mccloskey and Cohen, 1989). Typically,
regularization-based methods (Goodfellow et al.,
2014; Kirkpatrick et al., 2016; Serrà et al.; Deng
et al., 2021) exploit regularization to constrain the
parameter update to alleviate the forgetting prob-
lem, and the memory-based methods (Guo et al.,
2020; Saha et al., 2021) mitigate forgetting by stor-
ing important samples from past tasks in the exter-
nal memory and rehearsing them via some gradi-
ent transformation strategies. In addition, plenty
of works have been proposed to address catas-
trophic forgetting for NLP tasks. Dakwale and
Monz. (2017) subtly minimized KL-divergence of
prediction losses as a regularization term betweenfine-tuning and general domain models. Lee et al.
(2020a) introduced a new regularization technique
to mix the PLM parameters with vanilla parameters
instead of stochastical dropout. Chen et al. (2020)
adopted multi-task learning to jointly learn pre-
training and downstream tasks with less forgetting
during fine-tuning. Xie et al. (2021) preserved the
model neurons of general and language-specific
parts during fine-tuning. However, our method
is orthogonal to the above approaches since we
aim to effectively incorporate the domain-specific
PLM with the memory representation built from
the frozen general PLM to solve the forgetting is-
sue without adding additional regularization terms
in the model or using external memory to preserve
samples from the past tasks.
Knowledge-Enhanced PLMs Knowledge-
enhanced methods have shown effectiveness for
PLMs via introducing internal or external knowl-
edge. To improve the performance of fine-tuning
tasks, REINA (Wang et al., 2022) retrieves the
labeled training instances most similar to the
input data and concatenates them before feeding
them into PLMs. Besides, RETRO (Borgeaud
et al., 2021) enhances the auto-regressive language
model via leveraging a pre-trained frozen BERT
model to retrieve related texts and then use a
chunked cross-attention module to incorporate
them. Memorizing transformer (Wu et al., 2022)
leverages a learned gate to combine the attention
results of the local context and the external context
retrieved from previously seen sub-sequences.
KALA (Kang et al., 2022) is the approach most
relevant to our work. It incorporates intermediate
hidden representations with domain-specific
entities and their relational facts during task-
specific fine-tuning for domain tasks. However,
our method doesn’t need to retrieve similar texts
or construct additional knowledge graphs. We
propose several memory-augmented strategies to
build the memory representation and then transfer
it into the domain-specific PLM to mitigate the
forgetting of general knowledge.
6 Conclusion
In this work, we propose G-MAP, a novel
framework that utilizes the memory-augmented
layer to fuse the memory representation built
from the frozen general PLM to mitigate catas-
trophic forgetting of general knowledge caused
by domain-adaptive pre-training. We explore dif-6592ferent memory-augmented strategies to construct
the memory representation and empirically find
that chunk-based gate memory transfer achieves
the most optimal performance. We validate G-
MAP on various domains of classification, QA,
and NER tasks. The results show that our method
consistently outperforms existing baselines on all
datasets, implying that explicitly leveraging for-
gotten general knowledge is beneficial for domain-
specific downstream tasks.
7 Limitations
Our G-MAP framework has been validated on
domain-specific tasks and a small-scale domain
pre-training experiment in Section 4.4. Due to the
lack of large GPU resource, we have not validated
our G-MAP framework in large-scale pre-training,
a more challenging setting that we leave as future
work. We also consider automatic layer selection
to be an under-studied problem and believe that
AutoML techniques (Pham et al., 2018; Tan and
Le, 2019), such as evolutionary search (Deb et al.,
2002; Chen et al., 2021), will be promising meth-
ods. Finally, the proposed framework is built on the
encoder-only model, RoBERTa-base. In the future,
we will apply our framework on the other types of
architectures, such as decoder-only GPT (Radford
et al., 2018) and encoder-decoder BART (Lewis
et al., 2020).
Acknowledge
This work is supported by National Key R &D
Program of China(2022YFE0200700), National
Natural Science Foundation of China (Project No.
62006219) and Natural Science Foundation of
Guangdong Province (2022A1515011579).
References65936594A Dataset Descriptions and Statistics
This section describes the details and statistics of
three tasks: domain classification, domain extrac-
tive question answering (QA), and named entity
recognition (NER).
For the text classification tasks , we lever-
age the following datasets covering four domains,
including biomedical science, computer science,
news, and reviews. In the biomedical domain,
C P(Kringelum et al., 2016) is the relation
classification dataset based on chemical-protein in-
teraction. RCT (Dernoncourt and Lee, 2017) is
the role classification task constructed from the ab-
stract of the biomedical articles. In the computer
science domain, ACL-ARC (Jurgens et al., 2018)
is the task of annotated citations for articles’ func-
tions. SERC (Luan et al., 2018) is constructed
from scientific abstracts annotated with relation. In
the news domain, H P (Kiesel et al.,
2019) is the news text classification for determining
partisan leanings. AGN (Zhang et al., 2015) is
the topic classification for news. In the reviews do-
main, A (McAuley et al., 2015) is a binary
classification task consisting of feedback on prod-
ucts. IMDB (Maas et al., 2011) consists of movies
reviews, which is a binary sentiment classification
dataset.
For the NER tasks , we use two datasets involv-
ing the news and biomedical domain. Concretely,
CoNLL-2003 (Sang and Meulder, 2003) consists
of news stories from the Reuters Corpus. NCBI-
Disease (Dogan et al., 2014) is annotated with dis-
ease mentions.
For the QA tasks , we utilize two domain-
specific datasets. Specifically, NewsQA (Trischler
et al., 2017) is a machine comprehension dataset
consisting of news articles. Medication (Pampari
et al., 2018) is constructed by electronic medical
records from clinical text.
The detailed statistics of text classification tasks
and NER tasks are shown in Table 7, QA in Table 8.
B Implementation Details
We use the huggingface (Wolf et al., 2020) li-
brary to implement our G-MAP framework, which
contains various transformer-based pre-trained lan-
guage models (PLMs) and their saved checkpoints.
We implement the DAPT, TAPT, and DAPT+TAPT
models of biomedical, cs, news, and reviews do-6595mainsfrom the library released by (Gururangan
et al., 2020). All the experiments are implemented
on Nvidia V100 GPUs with 32GB memory. We se-
lect the best checkpoint on the validation set during
training to infer the test set.
Configurations for Classification Tasks In this
section, we explain the setting of fine-tuning for
domain-specific classification tasks. We fine-tune
the domain-specific PLM with our G-MAP frame-
work for 5 to 15 epochs, respectively, with the
same learning rate of 4e-5 and the dropout rate of
0.5. The default classification layer of the model
is 1 except for the IMDB dataset with 2, and the
default maximum sequence length is 256 except
for IMDB with 512. We leverage the Adam opti-
mizer to schedule the learning rate, with the Adam
epsilon of 1e-8, the Adam beta-1 of 0.9, and the
Adam beta-2 of 0.999. We apply the grid-search
method to find the optimal batch size and numbers
of GPUs for all the classification datasets. The
detailed settings are shown in Table 7.
Configurations for QA and NER For the extrac-
tive QA tasks, we fine-tune the domain-specific
PLM with our G-MAP framework for 3 epochs,
which can converge to optimal performance. Be-
sides, we train the model with a maximum se-
quence length of 384 and the learning rate of 3e-5,
the weight decay rate with 1e-2 and the warm-up
rate of 6e-2. For the experiments on NER tasks, we
fine-tune NCBI-Disease for 20 epochs and CoNNL-
2003 for 15 epochs, with the same maximum se-
quence length of 128, the learning rate of 5e-5, and
the weight decay rate and warm-up rate are set to 0.
Different from domain classification tasks, we uti-
lize AdamW as the learning rate optimizer instead
of Adam. In addition, we also adopt the grid-search
method to find the optimal batch size and number
of GPUs for all tasks. The detailed settings are
shown in Table 10.
Configurations for small-scale Pre-training This
part describes the experimental settings of adaptive
pre-training with G-MAP framework. Our sim-
ple pre-training experiment needs some external
domain-relative corpora of two domains: Biomedi-
cal and Computer Science. Following by (Gururan-
gan et al., 2020), we adopt C(Neumann
et al.) as a sentence splitting tool to obtain ab-
stract and body paragraphs from S2ORC (Lo et al.,
2020). After pre-processing for the corpora, we ran-
domly sample 50K data for each domain and split
70%of them as pre-training sets and 30 %as test
sets. For the general corpus similar to RBERT’s
pre-training corpus, we also randomly sample 50K
data from BCand split them by using
the method mentioned above. The detailed hyper-
parameter settings for this cross-domain adaptive
pre-training are shown in Table 6.
C Layer Selection Experiment
For single-layer memory transfer and gated mem-
ory transfer strategies, we experiment with adding
the memory-attention to layers 3,6,9 and 12 in
a 12-layer RoBERTa-base model, with the result
shown in Figure 6. We empirically find that adding
memory-attention to the 9-th layer of the domain-
specific model as the memory-augmented layer will
obtain the best results for the two strategies. How-
ever, adding memory-attention to either too upper
or too lower obtained fewer gains. Therefore, we
adopt memory-attention on the 9-th layer as the
default choice for the two strategies in the main
experiment shown in Table 1.65966597