
Zhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia, Jiaqi Guo,
Yan Gao,Shi Han,Jian-Guang Lou,Dongmei ZhangShanghai Jiao Tong University,Microsoft Research AsiaCarnegie Mellon University,Xi’an Jiaotong University
blankcheng@sjtu.edu.cn, zhiruow@cs.cmu.edu
jasperguo2013@stu.xjtu.edu.cn
{hadong,jia.ran,yan.gao,shihan,jlou,dongmeiz }@microsoft.com
Abstract
Tables are often created with hierarchies, but
existing works on table reasoning mainly fo-
cus on flat tables and neglect hierarchical ta-
bles. Hierarchical tables challenge table rea-
soning by complex hierarchical indexing, as
well as implicit relationships of calculation and
semantics. We present a new dataset, HiTab,
to study question answering (QA) and natu-
ral language generation (NLG) over hierarchi-
cal tables. HiTab is a cross-domain dataset
constructed from a wealth of statistical reports
and Wikipedia pages, and has unique charac-
teristics: (1) nearly all tables are hierarchical,
and (2) questions are not proposed by anno-
tators from scratch, but are revised from real
and meaningful sentences authored by analysts.
(3) To reveal complex numerical reasoning in
analysis, we provide fine-grained annotations
of quantity and entity alignment. Experimen-
tal results show that HiTab presents a strong
challenge for existing baselines and a valu-
able benchmark for future research. Target-
ing hierarchical structure, we devise an effec-
tive hierarchy-aware logical form for symbolic
reasoning over tables. Furthermore, we lever-
age entity and quantity alignment to explore
partially supervised training in QA and con-
ditional generation in NLG, which largely re-
duces spurious predictions in QA and mean-
ingless descriptions in NLG. The dataset and
code are available at .
1 Introduction
In recent years, there are a flurry of works on rea-
soning over semi-structured tables, e.g., answering
questions over tables (Yu et al. , 2018; Pasupat and
Liang, 2015) and generating fluent and faithful text
from tables (Lebret et al. , 2016; Parikh et al. , 2020).Figure 1: A hierarchical table and accompanied descrip-
tions in a National Science Foundation report.
But they mainly focus on simple flat tables and ne-
glect complex tables, e.g., hierarchical tables. A
table is regarded as hierarchical if its header ex-
hibits a multi-level structure (Lim and Ng, 1999;
Chen and Cafarella, 2014; Wang et al. , 2020). Hi-
erarchical tables are widely used, especially in data
products, statistical reports, and research papers in
government, finance, and science-related domains.
Hierarchical tables challenge QA and NLG due
to:(1) Hierarchical indexing. Hierarchical head-
ers, such as D2:G3 and A4:A25 in Figure 1, are
informative and intuitive for readers, but make cell
selection much more compositional than flat tables,
requiring multi-level and bi-dimensional indexing.
For example, to select the cell E5 (“66.6”), one
needs to specify two top header cells, “Master’s”
and “Percent”, and two left header cells, “All full-
time” and “Self-support”. (2) Implicit calcula-
tion relationships among quantities. In hierarchi-
cal tables, it is common to insert aggregated rows
and columns without explicit indications, e.g., total
(columns B,D,F and rows 4,6,7,20) and proportion
(columns C,E,G), which challenge precise numeri-1094cal inference. (3) Implicit semantic relationships
among entities. There are various cross-row, cross-
column, and cross-level entity relationships, but
lack explicit indications, e.g., “source” and “mecha-
nism” in A2 describe A6:A19 and A20:A25 respec-
tively, and D2 (“Master’s”) and F2 (“Doctoral”)
can be jointly described by a virtual entity, “De-
gree”. How to identify semantic relationships and
link entities correctly is also a challenge.
In this paper, we aim to build a dataset for hier-
archical table QA and NLG. But without sufficient
data analysts, it’s hard to ensure questions and de-
scriptions are meaningful and diverse (Gururangan
et al. , 2018; Poliak et al. , 2018). Fortunately, large
amounts of statistical reports are public from a vari-
ety of organizations (StatCan; NSF; Census; CDC;
BLS; IMF), containing rich hierarchical tables and
textual descriptions. Take Statistics Canada (Stat-
Can) for example, it consists of 6,039reports in
27domains authored by over 1,000 professionals.
Importantly, since both tables and sentences are
authored by domain experts, sentences are natural
and reflective of real understandings of tables.
To this end, we propose a new dataset, HiTab,
for QA and NLG on hierarchical tables. (1)All sen-
tence descriptions of hierarchical tables are care-
fully extracted and revised by human annotators.
(2)It shows that annotations of fine-grained and
lexical-level entity linking significantly help table
QA (Lei et al. , 2020; Shi et al. , 2020), motivat-
ing us to align entities in text with table cells.
In addition to entity, we believe aligning quanti-
ties (Ibrahim et al. , 2019), especially composite
quantities (computed by multiple cells), is also im-
portant for table reasoning, so we annotate under-
lying numerical relationships between quantities in
text and table cells, as Table 1 shows. (3)Since real
sentences in statistical reports are natural, diverse,
and reflective of real understandings of tables, we
devise a process to construct QA pairs based on
existing sentence descriptions instead of asking an-
notators to propose questions from scratch.
HiTab presents a strong challenge to state-of-the-
art baselines. For the QA task, MAPO (Liang et
al., 2018) only achieves 29.2%accuracy due to the
ineffectiveness of the logical form customized for
flat tables. To leverage the hierarchy for table rea-
soning, we devise a hierarchy-aware logical form
for table QA, which shows high effectiveness. We
propose partially supervised training given annota-
tions of linked mentions and formulas, which helpsmodels to largely reduce spurious predictions and
achieve 45.1%accuracy. For the NLG task, models
also have difficulties in understanding deep hierar-
chies and generate complex analytical texts. We
explore controlled generation (Parikh et al. , 2020),
showing that conditioning on both aligned cells and
calculation types helps models to generate mean-
ingful texts.
2 Dataset Construction and Analysis
We design an annotation process with six steps. To
well-handle the annotation complexity, we recruit
18 students or graduates (13 females and 5 males)
in computer science, finance, and English majors
from top universities, and provide them with com-
prehensive online training, documents, and QAs.
The annotation totally costs 2,400 working hours.
We will discuss the ethical considerations in Sec-
tion 8.
2.1 Hierarchical Table Collection
We select two representative organizations, Statis-
tics Canada (StatCan) and National Science Foun-
dation (NSF), that are rich of statistical reports.
Different from Census; CDC; BLS; IMF that only
provide PDF reports where table hierarchies are
hard to extract precisely (Schreiber et al. , 2017),
StaCan and NSF also provide reports in HTML,
from which cell information such as text and for-
mats can be extracted precisely using HTML tags.
First, we crawl English HTML statistical reports
published in recent five years from StatCan ( 1,083
reports in 27well-categorized domains) and NSF
(208reports from 11organizations in science foun-
dation domain). We merge StatCan and NSF and
get the combination of various domains. In addi-
tion, ToTTo contains a small proportion ( 5.03%)
of hierarchical tables, so we include them to cover
more domains from Wikipedia. To keep the balance
between statistical reports and Wikipedia pages, we
include random 1,851tables ( 50% of our dataset)
from ToTTo. Next, we transform HTML tables
to spreadsheet tables using a preprocessing script.
Since spreadsheet formula is easy to write, execute,
and check, the spreadsheet is naturally a great anno-
tation tool to align quantities and answer questions.
To enable correct formula execution, we normalize
quantities in data cells by excluding surrounding su-
perscripts, internal commas, etc. Extremely small
or large tables are filtered out (Appendix A.1 gives
more details).1095
2.2 Sentence Extraction and Revision
In this step, annotators manually go through sta-
tistical reports and extract sentence descriptions
for each table. Sentences consisting of multiple
semantic-independent sub-sentences will be care-
fully split into multiple ones. Annotators are in-
structed to eliminate redundancy and ambiguity in
sentences through revisions including decontextu-
alization and phrase deletion (Parikh et al. , 2020).
Fortunately, most sentences in statistical reports
are clean and fully supported by table data, so few
revisions are needed to get high-quality text.
2.3 Entity and Quantity Alignment
In this phase, annotators are instructed to align men-
tions in text with corresponding cells in tables. It
has two parts, entity alignment and quantity align-
ment, as shown in Table 1. For entity alignment, we
record the mappings from entity mentions in text to
corresponding cells. Single-cell quantity mentions
can be linked similar with entity mentions, but com-
posite quantity mentions are calculated from two or
more cells through operators like max/sum/div/diff
(Table 2). The spreadsheet formula is powerful
and easy-to-use for tabular data calculation, so we
use the formula to record the calculations process
of composite quantities in text, e.g., ‘10 points
higher’ ( =G23-G24 ). Although quantities are oftenrounded in descriptions, we neglect rounding and
refer to precise quantities in table cells.
2.4 Converting Sentences to QA Pairs
Existing QA datasets instruct annotators to propose
questions from scratch, but it’s hard to guarantee
the meaningfulness and diversity of proposed ques-
tions. In HiTab, we simply revise declarative sen-
tences into QA pairs. For each sentence, annotators
need to identify a target key part to question about
(according to the underlying logic), then convert
it to the QA form. All questions are answered by
formulas that reflect the numerical inference pro-
cess. For example, the ‘XLOOKUP’ operator is
frequently used to retrieve the header cells of su-
perlatives, as shown in Table 1. To keep sentences
as natural as they are, we do not encourage unnec-
essary sentence modification during the conversion.
If an annotator finds multiple ways to question re-
garding a sentence, he/she only needs to choose
one way that best reflects the overall meaning.
2.5 Regular Inspections and the Final Review
We ask the two most experienced annotators to per-
form regular inspections and the final review. (1) In
the labeling process, they regularly sample annota-
tions (about 10%) from all annotators to give timely
feedback on labeling issues. (2) Finally, they re-
view all annotations and fix labeling errors. Also, to
assist the final review, we write a script to automati-
cally identify spelling issues and formula issues. To
double-check the labeling quality before the final
review, we study the agreement of annotators by
collecting and comparing annotations on randomly
sampled 50tables from two annotators. It shows
0.89and0.82for quantity and entity alignment
in Fleiss Kappa respectively, which are regarded
as “almost perfect agreement” (Landis and Koch,
1977), and 64.5in BLEU-4 after sentence revision,
which also indicates high agreement. We further
show annotation artifacts are substantially avoided1096
in our dataset in Appendix A.2.
2.6 Hierarchy Extraction
We follow existing work (Lim and Ng, 1999; Chen
and Cafarella, 2014; Wang et al. , 2020) and use the
tree structure to model hierarchical headers. Since
cell formats such as merging, indentation, and font
bold, are commonly used to present hierarchies, we
adapt heuristics in (Wang et al. , 2020) to extract top
and left hierarchical trees, which has high accuracy.
We go through 100randomly sampled tables in
HiTab, 94% of them are precisely extracted. Figure
8 in Appendix shows an illustration.
2.7 Dataset Statistics and Comparison
Table 3 shows a comprehensive comparison of re-
lated datasets. HiTab is not among the largest ones,
but(1)it is the first dataset to study QA and NLG
over hierarchical tables (accounting for 98.1% ta-
bles in HiTab) in-depth; (2)it is annotated with
fine-grained entity and quantity alignment; (3)com-
pared with TAT-QA, FinQA, and NumericNLG
that are single-domain, HiTab has a wide cover-
age of different domains from statistical reports
and Wikipedia, even wider than ToTTo or WTQ
that only involves Wikipedia tables; (4)the number
of real descriptions per table ( 5.0) in statistical re-
ports (HiTab) is much richer than 1.4in Wikipedia
(ToTTo) and 3.8in scientific papers, contributing
more analytical aspects per table.
Figure 2 analyzes this dataset by domains andoperations: domains are diverse, covering 28
domains from statistical reports (fully listed in
Appendix A.3) and other open domains from
Wikipedia; a large proportion of questions involves
complex cell selection and numerical operations.
3 Hierarchical Table QA
Table QA is essential for table understanding, doc-
ument retrieval, ad-hoc search, etc. Hierarchical
tables are quite common in these scenarios like
in webpages and reports, while current Table QA
tasks and methods focus on simple flat tables.
Problem Statement Hierarchical Table QA is
defined as follows: given a hierarchical table tand
a question xin natural language, output answer y.
The question-answer pair should be fully supported
by the table. Our dataset D={(x, t, y)}, i∈
[1, N]is a set of Nquestion-table-answer triples.
Table QA is usually formulated as a semantic
parsing problem (Pasupat and Liang, 2015; Liang
et al. , 2017), where a parser converts the question
into logical form, and an executor executes it to pro-
duce the answer. However, existing logical forms
for Table QA (Pasupat and Liang, 2015; Liang et
al., 2017; Yin et al. , 2020) are customized for flat or
database tables. The three challenges mentioned in
Section 1 (hierarchical indexing, implicit indexing
relationships, and implicit semantic relationships)
make QA more difficult on hierarchical tables.
3.1 Hierarchy-aware Logical Forms
To this end, we propose a hierarchy-aware logical
form that exploits table hierarchies to mitigate these
challenges. Specifically, we define region as the
operating object, and propose two functions for
hierarchical region selection.
Definitions Given tree hierarchies of tables ex-
tracted in Section 2.6, we define header as a header
cell (e.g., A7(“Federal”) in Figure 1), and level as a
level in the left/top tree (e.g., A5,A6,A20 are on the
same level). Existing logical forms on tables treat1097rows as operating objects and columns as attributes,
and thus can not perform arithmetic operations on
cells in the same row. However, a row in hierar-
chical tables is not necessarily a subject or record,
thus operations can be applied on cells in the same
row. Motivated by this, we define region as our
operating object, which is a data region in table
indexed by both left and top headers (e.g., B6:C19
is a rectangular region indexed by A6,B2). The
logical form execution process is divided into two
phases: region selection and region operation.
Region Selection We design two functions
(filter tree h )and(filter level l )to do region
selection, where his a header, lis a level. Func-
tions can be applied sequentially: the subsequent
function applies on the return region of the previ-
ous function. (filter tree h )selects a sub-tree
region according to a header cell h: ifhis a leaf
header (e.g., A8), the selected region should be the
row/column indexed by h(row 8); if his a non-leaf
header (e.g., A7), the selected region should be the
rows/columns indexed by both hand its children
headers (row 7-16). (filter level l )selects a sub-
tree from the input tree according to a level land
return the sub-region indexed by headers on level l.
These two functions mitigate aforementioned three
challenges: (1) hierarchical indexing is achieved
by applying these two functions sequentially; (2)
withfilter level , data with different calculation
types (e.g., rows 4-5) will not be co-selected, thus
not incorrectly operated together; (3) level-wise se-
mantics can be captured by aggregating header cell
semantics (e.g., embeddings) on this level. Some
logical form execution examples are shown in Ap-
pendix C.2.
Region Operation Operators are applied on the
selected region to produce the answer. We define
19operators, mostly following MAPO (Liang et
al., 2018), and further include some operators (e.g.,
difference rate ) for hierarchical tables. Complete
logical form functions are shown in Appendix C.1.
3.2 Experimental Setup
3.2.1 Baselines
We present baselines in two branches. One is logi-
cal form-based semantic parsing, and the other is
end-to-end table parsing without logical forms.
Neural Symbolic Machine (Liang et al. , 2017) is
a powerful semantic parsing framework consisting
of a programmer to generate programs from NL
and save intermediate results, and a computer toexecute programs. We replace the LSTM encoder
with BERT (Devlin et al. , 2018), and implement
a lisp interpreter for our logical forms as executor.
Table is linearized by placing headers in level order,
which is shown in detail in Appendix C.4.
TaPas (Herzig et al. , 2020) is a state-of-the-art end-
to-end table parsing model without generating logi-
cal forms. Its power to select cells and reason over
tables is gained from its pretraining on millions of
tables. To fit TaPas input, we convert hierarchical
tables into flat ones following WTQ (Pasupat and
Liang, 2015). Specifically, we unmerge the cells
spanning many rows/columns on left/top headers
and duplicate the contents into unmerged cells. The
first top header row is specified as column names.
3.2.2 Weak Supervision
In weak supervision, the model is trained with QA
pairs, without golden logical forms. For NSM, we
compare three widely-studied learning paradigms:
MML (Dempster et al. , 1977) maximizes the
marginal likelihood of observed programs. RE-
INFORCE (Williams, 1992) maximizes the re-
ward of on-policy samples. MAPO (Liang et al. ,
2018) learns from programs both inside and out-
side buffer, and samples efficiently by systematic
exploration.
Since these methods require consistent programs
for learning or warm start, we randomly search
15,000programs per sample before training. The
pruning rules are shown in Appendix C.3. Finally,
6.12consistent programs are found per sample.
For TaPas, we use the pre-trained version and fol-
low its weak supervised training process on WTQ.
3.2.3 Partial Supervision
Given labeled entity links, quantity links, and cal-
culations (from the formula), we further explore to
guide training in a partially supervised way. These
three annotations indicate selected headers, region,
and operators in QA. For NSM, we exploit them to
prune spurious programs, i.e., incorrect programs
that accidentally produce correct answers, in two
ways. (1) When searching consistent programs,
besides producing correct answers, programs are
required to satisfy at least two constraints. In this
way, the average consistent programs reduces from
6.12to2.13per sample. (2) When training, satis-
fying each condition will add 0.2to the original1098
binary 0/1 reward. Sampled programs with reward
r≥1.4are added to the program buffer.
For TaPas, we additionally provide answer coor-
dinates and calculation types in training following
its WikiSQL setting.
3.2.4 Evaluation Metrics
We use Execution Accuracy (EA) as our metric
following (Pasupat and Liang, 2015), measuring
the percentage of samples with correct answers.
We also report Spurious Program Rate to study the
percentage that incorrect logical forms produce cor-
rect answer. Since we do not have golden logical
forms, we manually annotate logical forms for 150
random samples in dev set for evaluation.
3.2.5 Implementations
We split 3,597tables into train ( 70%), dev ( 15%)
and test ( 15%) with no overlap. We download
pre-trained models from huggingface. For NSM,
we utilize ‘bert-base-uncased’, and fine-tune 20K
steps on HiTab. Beam size is 5for both training
and inference. To test MAPO original logical form,
we convert flatten tables as we do for TaPas. For
TaPas, we adopt the PyTorch (Paszke et al. , 2019)
version in huggingface. We utilize ‘tapas-base’,
and fine-tune 40epochs on HiTab. All experiments
are conducted on a server with four V100 GPUs.
3.3 Results
Table 4 summarizes our evaluation results.
Weak Supervision First, MAPO with our
hierarchy-aware logical form outperforms that us-
ing its original logical form by a large margin
11.5%, indicating the necessity of designing a log-
ical form leveraging hierarchies. Second, MAPO
achieves the best EA(40.7%) with the lowest spuri-
ous rate ( 19%). But >50% questions are answered
incorrectly, proving QA on HiTab is challenging.Third, though TaPas benefits from pretraining on
tables, it performs worse than the best logical form-
based method without table pretraining.
Partial Supervision From Table 4, we can con-
clude the effectiveness of partial supervision in two
aspects. First, it improves EA. The model learns
how to deal with more cases given high-quality pro-
grams. Second, it largely lowers %Spurious . The
model learns to generate correct programs instead
of some tricks. MML, whose performance highly
depends on the quality of searched programs, bene-
fits the most ( 36.7%to45.1%), indicating partial
supervision improves the quality of consistent pro-
grams by pruning spurious ones. However, TaPas
does not gain much improvements from partial su-
pervision, which we will discuss in the next para-
graph.
Error Analysis For TaPas, 98.7%of success
cases are cell selections, which means TaPas ben-
efits little from partial supervision. This may be
caused by: (1) TaPas does not support some com-
mon operators on hierarchical table like difference ;
(2) the coarse-to-fine cell selection strategy first
selects columns then cells, but cells in different
columns may also aggregate in hierarchical tables.
For MAPO under partial supervision, we analyze
100error cases. Error cases fall into four categories:
(1) entity missing ( 23%): the header to filter is not
mentioned in question, where a common case is
omitted Total ; model failure, including (2) failing
to select correct regions ( 38%) and (3) failing to
generate correct operations ( 20%); (4) out of cov-
erage ( 19%): question types unsolvable with the
logical form, which is explained in Appendix C.1.
Spurious programs occur mostly in two patterns.
In cell selection, there may exist multiple data cells
with correct answers (e.g., G9,G16 in Figure 1),
while only one is golden. In superlatives, the model
can produce the target answer by operating on dif-
ferent regions (e.g., in both region B21:B25 and
B23:B25, B23 is the largest).
Level-wise Analysis In Figure 3, we present
level-wise accuracy of HiTab QA with MAPO and
our hierarchy-aware logical form. Level here stands
for sum of left and top header levels. As shown, the
QA accuracy degrades when table level increases
as table structure becomes more complex, except
for level = 2,i.e.,tables with no hierarchies. The
reason level = 2performs relatively worse might
be that only 1.9%tables without hierarchies are
seen in HiTab. We also present an annotated table1099
example from our dataset to illustrate in detail the
challenges mentioned in Section 1 that hierarchical
tables bring in Appendix C.5.
4 Hierarchical Table-to-Text
4.1 Problem Statement
Some works formulate table-to-text as a summa-
rization problem (Lebret et al. , 2016; Wiseman
et al. , 2017). However, since a full table often
contains quite rich information, there lack explicit
signals on what to generate, which renders the task
unconstrained and the evaluation difficult. On the
other hand, some recent works propose controlled
generation to enable more specific and logical gen-
eration: (1) LogicNLG generates a sentence con-
ditioned on a logical form guiding symbolic oper-
ations over given cells, but writing correct logical
forms as conditions is challenging for common
users who are more experienced to write natural
language directly, thus restricting the application to
real scenario; (2) ToTTo generates a sentence given
a table with a set of highlighted cells. In ToTTo’s
formulation, the condition of cell selection is much
easier to specify than the logical form, but it ne-
glects symbolic operations which are critical for
generating some analytical sentences involving nu-
merical reasoning in HiTab.
We place HiTab as a middle-ground of ToTTo
and LogicNLG to make the task more controllable
than ToTTo and closer to real application than Log-
icNLG. In our setting, given a table, the model
generates a sentence conditioned on a group of se-
lected cells (similar to ToTTo) and operators (much
easier to be specified than logical forms). Although
we use two strong conditions to guide symbolic
operations over cells, there still leaves a consider-
able amount of content planning to be done by the
model, such as retrieving contextual cells in a hier-
archical table given selected cells, identifying howoperators are applied on given cells, and composing
sentences in a faithful and logical manner.
We now define our task as: given a hierarchical
table T, highlighted cells C, and specified opera-
torsO, the goal is to generate a faithful description
S. The dataset H= (T, S), i∈[1, N]is a set
ofNtable-description instances. Description S
is a sentence about a table Tand involves a series
of operations O= [O, O, ..., O]on certain
table cells C= [c, c, ..., c].
4.2 Controlled Generation
4.2.1 With Highlighted Cells
An entity or quantity in text can be supported by
table cells if it is directly stated in cell contents, or
can be logically inferred by them. Different from
only taking data cells as highlighted cells (Parikh et
al., 2020), we also take header cells as highlighted
cells, and it is usually the case for superlative ARG-
type operations on a specific header level in hier-
archical tables, e.g., “Teaching assistantships” is
retrieved by ARGMAX in Figure 1. In our dataset,
highlighted cells are extracted from annotations of
the entity and quantity alignment.
4.2.2 With Operators
Highlighted cells can tell the target for text genera-
tion, but is not sufficient, especially for analytical
descriptions involving cell operations in HiTab. So
we propose to use operators as extra control. It
contributes to text clarity and meaningfulness in
two ways. (1) It clarifies the numerical reasoning
intent on cells. For example, given the same set of
data cells, applying SUM, A VERAGE, or COUNT
conveys different meanings thus should yield dif-
ferent texts. (2) Operation results on highlighted
cells can be used as additional input sources. Exist-
ing seq2seq models are not powerful enough to do
arithmetic operations (Thawani et al. , 2021), e.g.,
adding up a group of numbers, and it greatly limits
their ability to generate correct numbers in sen-
tences. Explicitly pre-computing the calculation
results is a promising alternative way to mitigate
this gap in seq2seq models. Operators are extracted
from annotations of formulas shown in Table 2.
4.2.3 Sub Table Selection and Serialization
Sub Table Selection Under controls of se-
lected cells and operators, we devise a heuristic to
retrieve all contextual cells as a sub table. (1) We
start with highlighted cells extracted from our en-
tity and quantity alignment, then use the extracted1100table hierarchy to group the selected cells into the
top header, the left header, and the data region. (2)
Based on the extracted table hierarchy, we use the
source set of top and left header cells to include
their indexed data cells, and we also use the source
set of data cells to include corresponding header
cells. (3) We also include their parent header cells
in table hierarchy to construct a full set of headers.
In the end, we take the union of of them as the
result of sub table selection.
Serialization On each sub table, we do a row-
turn traversal on linked cells and concatenate their
cell strings using [SEP] tokens. Operator tokens
and calculation results are also concatenated with
the input sequence. We also experimented with
other serialization methods, such as header-data
pairing or template-based method, yet none re-
ported superiority over the simple concatenation.
Appendix B.1 gives an illustration.
4.3 Experiments
We conduct experiments by fine-tuning four state-
of-the-art text generation methods on HiTab.
Pointer Generator (See et al. , 2017) A LSTM-
based seq2seq model with copy mechanism. While
originally designed for text summarization, it is
also used in data-to-text (Gehrmann et al. , 2018).
BERT-to-BERT (Rothe et al. , 2020) A trans-
former encoder-decoder model (Vaswani et al. ,
2017) initialized with BERT (Devlin et al. , 2018).
BART (Lewis et al. , 2019) A pre-trained denois-
ing autoencoder with standard Transformer-based
architecture and shows effectiveness in NLG.
T5(Raffel et al. , 2019) A transformer-based pre-
trained model. It converts all textual language prob-
lems into text-to-text and proves to be effective.
4.3.1 Evaluation Metrics
We use two automatic metrics, BLEU and PAR-
ENT. BLEU (Papineni et al. , 2002) is broadly used
to evaluate text generation. PARENT (Dhingra et
al., 2019) is proposed specifically for data-to-text
evaluation that additionally aligns n-grams from the
reference and generated texts to the source table.
4.3.2 Experiment Setup
Samples are split into train ( 70%), dev ( 15%), and
test (15%) sets just the same as the QA task. The
maximum length of input/output sequence is set to
512/64. Implementation details of all baselines are
given in Appendix B.2.
4.3.3 Experiment Result and Analysis
As shown in Table 5, first, from an overall point of
view, both metrics are not scored high. This well
proves the difficulty of HiTab. It could be caused
by the hierarchical structure, as well as statements
with logical and numerical complexity. Second , by
comparing two controlled scenarios (cell highlights
& both cell highlights and operators), we see that
adding operators to conditions greatly help models
to generate descriptions with higher scores, show-
ing the effectiveness of our augmented conditional
generation setting. Third , results on two controlled
scenarios across baselines are quite consistent. Re-
placing the traditional LSTM with transformers
shows large increasing. Leveraging seq2seq-like
pretraining yields a rise of +6.5BLEU and +11.3
PARENT. Lastly, between pretrained transformers,
T5 reports higher scores over BART, probably for
T5 is more extensively tuned during pre-training.
Further, to study the generation difficulty con-
cerning table hierarchy , we respectively evaluate
samples at different hierarchical depths, i.e., table’s
maximum depths in top and left header trees. In
groups of 2, 3, 4+ depth, BLEU scores 31.7,26.5,
and21.3; PARENT scores 40.9,36.5, and 31.6.
The reason could be that, as the table header hi-
erarchy grows deeper, the data indexing becomes
increasingly compositional, rendering it harder to
baseline models to configure entity relationships
and compose logical sentences.
5 Related Work
Table-to-Text Existing datasets are restricted in
flat tables or specific subjects (Liang et al. , 2009;
Chen and Mooney, 2008; Wiseman et al. , 2017;
Novikova et al. , 2016; Banik et al. , 2013; Lebret et
al., 2016; Moosavi et al. , 2021). The most related
table-to-text dataset to HiTab is ToTTo (Parikh et
al., 2020), in which complex tables are also in-
cluded. There are two main differences between
HiTab and ToTTo: (1) in ToTTo, hierarchical tables
only account for a small proportion ( 5%), and there
are no indication and usage of table hierarchies. (2)
in addition to cell highlights, Hitab conditions on1101
operators that reflect symbolic operations on cells.
Table QA mainly focuses on DB tables (Wang
et al. , 2015; Yu et al. , 2018; Zhong et al. , 2017)
and flat web tables (Pasupat and Liang, 2015; Sun
et al. , 2016). Recently, there are some datasets
on domain-specific table QA (Chen et al. , 2021;
Zhu et al. , 2021) and jointly QA over tables and
texts (Chen et al. , 2020b; Zhu et al. , 2021), but hier-
archical tables still have not been studied in depth.
CFGNN (Zhang, 2020) and GraSSLM (Zhang et
al., 2020) uses gragh neural networks to encode
tables for QA, but all tables are database tables and
relational web tables without hierarchies, respec-
tively. Wang et al. (2021) include some hierarchi-
cal tables but only focuses on table search.
6 Discussion
HiTab also presents cross-domain and complicated-
calculation challenges. (1) To explore cross-
domain generalizability, we randomly split
train/dev/test by domains for three times and
present the average results of our best methods
in Table 6. We found decreases in all metrics in
QA and NLG. (2) Figure 4 shows a case that chal-
lenges existing methods: performing complicated
calculations requires to jointly consider quantity
relationships, header semantics, and hierarchies.
7 Conclusion
We present a new dataset, HiTab, that simultane-
ously supports QA and NLG on hierarchical tables,
where tables are collected from statistical reports
and Wikipedia in various domains. Importantly,we provide fine-grained annotations on entity and
quantity alignment. In experiments, we introduce
strong baselines and conduct detailed analysis on
QA and NLG tasks on HiTab. Results suggest
that HiTab can serve as a challenging and valuable
benchmark for future research on complex tables.
8 Ethical Considerations
This work presents HiTab, a free and open English
dataset for the research community to study table
question-answering and table-to-text over hierar-
chical tables. Our dataset contains well-processed
tables, annotations (QA pairs, target text, and bidi-
rectionally mappings between entities and quan-
tities in text and the corresponding cells in table),
recognized table hierarchies, and source code. Data
in HiTab are collected from two public organiza-
tions, StatCan and NSF. Both of them allow sharing
and redistribution of their public reports, so there
is no privacy issue. We collect tables and accompa-
nied descriptive sentences from StatCan and NSF.
We also include hierarchical tables in Wikipedia
from ToTTo, which is a public dataset under MIT
license, so there is no risk to use it. And in the label-
ing process, annotators need to check if there exist
any names or uniquely identifies individual people
or offensive content. They did not find any such
sensitive information in our dataset. We recruit
18students or graduates in computer science, fi-
nance, and English majors from top universities( 13
females and 5males). Each student is paid $7.8
per hour (above the average local payment of simi-
lar jobs), totally spending 2,400hours. We finally
get3,597tables and 10,672well-annotated sen-
tences. And the data got approval from an ethics
review board by an anonymous IT company. The
details for our data collection and characteristics
are introduced in Section 2.
References1102110311041105A More Details on Dataset
A.1 Dataset Preprocessing
We filter tables using these constraints: (1) num-
ber of rows and columns are more than 2and less
than64; (2) cell strings have no more than one
non-ASCII character and 20tokens; (3) hierarchies
are successfully parsed via the method in 2.6. (4)
hierarchies have no more than four levels on one
side. Finally, 85% tables meet all constraints.
A.2 Annotation Artifacts
Annotation artifacts are common in large scale NLP
datasets, which may raise unwanted statistical cor-
relations making the task easier (Gururangan et
al., 2018). In HiTab, the annotation artifacts may
come from homogeneous patterns of questions. To
address this issue, we ask annotators to revise ques-
tions from the high-quality descriptions from sta-
tistical reports from 28domains to guarantee the
diversity and naturalness, and encourage them to
choose the best way to raise question reflecting
the overall meaning of the description. To further
check whether and where artifacts may exist in our
dataset, we conduct two experiments on QA and
count the ratio of answer occurring in the question:
•Use table as only input without question, to
see if there is a potential pattern between ta-
ble and answer. We train BERT+MAPO for
10,000steps and TaPas for 10epochs. Both
methods can’t converge under this setting,
with4.0%and2.6%accuracy on the test set.
The poor performance indicates model can’t
learn the answers by exploring and leveraging
artifacts between the table and answer, and
thus should learn to jointly inference the ques-
tion and table.
•Shuffle the rows and columns of table ran-
domly. Experiments show similar perfor-
mance ( ±1%) between our original tables and
shuffled tables. The result shows that the cor-
relation between answer and table cell posi-
tion is very little, thus model can’t choose
some specific positions, e.g., cell at the first
row and first column, as a shortcut prediction.
•The ratio that answer occurs in the question is
only5.3%. Model that only learns to retrieve
the question can’t achieve high performance.A.3 Domain Distribution
The full 29domains of sample distribution in HiTab
are shown in Figure 5.
A.4 Annotation Interface
The annotation interface looks like Figure 6. Since
spreadsheet formula is easy to write, execute, and
check, the spreadsheet is naturally a great annota-
tion tool. Annotators can use the Excel formula
conveniently for cell linking and calculation in en-
tity alignment and answering questions.
B Hierarchical Table-to-Text
B.1 Illustration on Controlled Generation in
Hierarchical Table-to-Text.
Please find the illustration shown in Figure 7.
B.2 Baseline Implementation Details
We perform optimized tuning for baselines using
the following settings.
Pointer Generator (See et al. , 2017) A LSTM-
based seq2seq model with copy mechanism. The
model uses two-layer bi-directional LSTMs for the
encoder with 300-dim word embeddings and 300
hidden units. We perform fine-tuning using batch
size2, learning rate 0.05, and beam size 5.
BERT-to-BERT (Rothe et al. , 2020) A trans-
former encoder-decoder model (Vaswani et al. ,
2017) where the encoder and decoder are both
initialized with BERT (Devlin et al. , 2018) by
loading the checkpoint named ‘bert-base-uncased’
provided by the huggingface/transformers repos-
itory. We perform fine-tuning using batch-size 2
and learning rate 3e.
BART (Lewis et al. , 2019) BART is a pre-
trained denoising autoencoder for seq2seq lan-1106guage modeling. It uses standard Transformer-
based architecture and shows effectiveness in NLG.
We align model configuration with the BASE ver-
sion of BART, and use the model ‘facebook/bart-
base’ in huggingface/transformers. During fine-
tuning, we use a batch size of 8and a learning rate
of2e.
T5(Raffel et al. , 2019) T5 is also a transformer-
based pre-training LM. It trains extensively on text-
to-text tasks and scores high on generation tasks.
We use the pre-trained model ‘t5-base’ in hugging-
face/transformers. For fine-tuning, we set batch
size to 8and learning rate to 2e.
We use a beam size of 5to search decoded out-
puts (sequence lengths range from 8to60tokens)
C Hierarchical Table QA
C.1 Logical Form Function List
We list our logical form functions in Table 7.
Union selection is required for comparative and
arithmetic operations. It is achieved by allowing
variable number of headers in filter tree, where
“variable” is one or two in practice.
In our implementation, a function by default
takes the selected region of last function as in-
put region to prune search space. We use gram-
mars to filter left headers before top headers, and
a(filter level)is necessary after filtering one di-
rection of tree even when only the leaf level is
available. And we deactivate order relation func-
tions (e.g., eqfunction) and the order argument k11071108
inargmax/argmin because there are few questions
in these types and activating them will largely in-
crease number of spurious programs when search-
ing.
The logical form coverage after deactivation
is78.3%in300iterations of random exploration.
Some typical question types that can not be covered
are: (1) scale conversion, e.g., 0.984to98.4%, (2)
operating data indexed by different levels of head-
ers, e.g., proportion of total, (3) complex composite
operations, e.g., Figure 4.
C.2 Examples of Logical Form Execution
Take the table in Figure 8 as input table, we demon-
strate three types of questions with complete logical
forms in Table 8.
C.3 Pruning Rules in Searching
We use trigger words and POS tags for some func-
tions in random exploration, which is inspired by
(Zhang et al. , 2017; Liang et al. , 2018). Functions
are allowed to be selected only when triggers ap-
pear in the question. Triggers are listed in Table 9.
C.4 Table Linearization
We linearize the question and table according to
Figure 8.
The input is concatenation of question and ta-
ble. Table is linearized by putting headers in level
order. Each level is led by a [LEVEL] token to
gather current level embedding. The first [LEVEL]
token stands for level zero of left. Each header isFunction Trigger Words
argmax JJR, JJS, RBR, RBS, top,
argmin first, bottom, and last.
max JJS, RBS
min
average average, mean
sum all, combine, total, sum
count how, many, total, number
difference difference, more, than,
difference rate change,compare, JJR
difference raterev RBR.
proportion times, percent,
proportion rev percentage, fraction
linearized as name|type.name is the tokenized
header string. typeis the entity type parsed by Stan-
ford CoreNLP, which includes “string”, “number”,
“datetime” in our case. Headers with the same name
will gather token embeddings by mean pooling.
C.5 Illustration on Challenges in Hierarchical
Table
We present an annotated example in Figure 9 to
show the challenges of hierarchical table intro-
duced in Section 1.
To precisely answer the question in the figure,
the model/method first needs to hierarchically in-
dex the grey region with “field in science” and
“doctoral”, which requires understanding of tex-
tual and spatial semantics of the hierarchical table
since the textual headers are spatially (seen as a
tree) related with the region. Second, from the
phrase “most enrolled”, it should further indexes
“All” (column G) rather than “Percent” (column H)
and infers argmax operation, , which calls for the
ability to distinguish between different calculation
relationships.11091110