
Bang Yang, Fenglin Liu, Xian Wu, Yaowei Wang, Xu Sun, and Yuexian ZouADSPLAB, School of ECE, Peking UniversityPeng Cheng LaboratoryUniversity of OxfordTencent Jarvis LabSchool of Computer Science, Peking University
{yangbang, zouyx}@pku.edu.cn; fenglin.liu@eng.ox.ac.uk
Abstract
Supervised visual captioning models typically
require a large scale of images or videos paired
with descriptions in a specific language (i.e.,
the vision-caption pairs) for training. However,
collecting and labeling large-scale datasets is
time-consuming and expensive for many sce-
narios and languages. Therefore, sufficient la-
beled pairs are usually not available. To deal
with the label shortage problem, we present a
simple yet effective zero-shot approach Mul-
tiCapCLIP that can generate visual captions
for different scenarios and languages without
any labeled vision-caption pairs of downstream
datasets. In the training stage, MultiCapCLIP
only requires text data for input. Then it con-
ducts two main steps: 1) retrieving concept
prompts that preserve the corresponding do-
main knowledge of new scenarios; 2) auto-
encoding the prompts to learn writing styles
to output captions in a desired language. In
the testing stage, MultiCapCLIP instead takes
visual data as input directly to retrieve the con-
cept prompts to generate the final visual de-
scriptions. The extensive experiments on image
and video captioning across four benchmarks
and four languages (i.e., English, Chinese, Ger-
man, and French) confirm the effectiveness
of our approach. Compared with state-of-the-
art zero-shot and weakly-supervised methods,
our method achieves 4.8% and 21.5% abso-
lute improvements in terms of BLEU@4 and
CIDEr metrics. Our code is available at https:
//github.com/yangbang18/MultiCapCLIP .
1 Introduction
Visual captioning targets to first 1) understand the
information of visual inputs, which are typically
videos or images, and then 2) produces a corre-
sponding textual sentence describing the visual ob-
jects/attributes/relationships. Visual captioning has
drawn remarkable attention from natural languageprocessing and computer vision fields due to its
wide applications, e.g., cross-modal retrieval (Luo
et al., 2022; Cheng et al., 2023b) and helping the
visually impaired (Çaylı et al., 2021). Currently,
visual captioning models based on the encoder-
decoder framework (Huang et al., 2020; Liu et al.,
2020; Yang et al., 2021; Zhang et al., 2021; Hu
et al., 2022; Lin et al., 2022) have achieved tremen-
dous progress in advancing the state-of-the-art.
These models are usually trained with full supervi-
sion and rely on large-scale humanly-annotated
training data (i.e., vision-caption pairs), which
needs expensive labeling work. In particular, when
it comes to Non-English caption systems, it is
challenging to collect and label sufficient vision-
caption pairs in a timely manner, which prevents
such encoder-decoder models from rapid deploy-
ment in different scenarios and languages.
To deal with the shortage of labeled pairs, we
propose the MultiCapCLIP - a prompt-based natu-
ral language auto-encoder. As shown in Figure 1,
MultiCapCLIP only requires textual input for train-
ing, and it can conduct zero-shot multilingual vi-
sual captioning, including image and video cap-
tioning. Therefore, MultiCapCLIP can deal with
the situation where the labeled vision-caption pairs
are missing. MultiCapCLIP is particularly suitable
for new scenarios and languages, improving the
practical value of visual captioning.
To implement MultiCapCLIP, we first adopt a
pre-trained vision-language model, i.e., CLIP (Rad-
ford et al., 2021), as our backbone. CLIP has shown
success in correlating the visual and textual modal-
ities into the same latent space (vision-language
embedding space) (Tewel et al., 2022b; Su et al.,
2022; Zeng et al., 2023). We observe two critical
issues for zero-shot visual captioning: the under-
standing of domain visual knowledge (e.g., objects,
attributes, and relationships) and the generation of
descriptive sentences in a specific writing style and
language. Therefore, we propose a prompt-based11908auto-encoder, which introduces the visual concept
prompts Pto preserve the corresponding domain
knowledge and writing styles of zero-shot visual
captioning. During training, given the text-only
data, we train the model by reconstructing the cap-
tionSin the S→ P → Sauto-encoding pipeline.
Since the auto-encoding process reconstructs the
same input sentence, the model training needs only
unlabeled text data. In the reconstruction process,
the model is able to preserve the necessary domain
knowledge and the writing styles of visual caption-
ing (Wang et al., 2016; Tschannen et al., 2018).
During inference, we can directly take the vision
input Vas queries to retrieve the domain knowl-
edge preserved in the visual concept prompts and
finally rely on the learned writing styles in a spe-
cific language in the text decoder to generate visual
descriptions in the V→ P → Spipeline.
Meanwhile, to further bridge the modality gap
between the visual and textual data (Liang et al.,
2022), we introduce an augmentation method, in-
cluding input augmentation and feature augmenta-
tion, which can boost the robustness of the model
and in turn improve the performance of zero-
shot visual captioning. The experiments on four
benchmark datasets, i.e., MS-COCO (Chen et al.,
2015), MSR-VTT (Xu et al., 2016), V ATEX (Wang
et al., 2019), and Multi30K (Elliott et al., 2016),
show that our approach can accurately and data-
efficiently generate visual captions in English, Chi-
nese, German, and French.
Overall, our main contributions are as follows:
•We propose a simple yet effective approach
MultiCapCLIP that requires no downstream
labeled data to make the first attempt for zero-
shot multilingual visual captioning.
•MultiCapCLIP first introduces visual concept
prompts to preserve the domain knowledge
and then auto-encodes them to learn the writ-
ing styles of captioning. After text-only train-
ing, our approach can shift from text-to-text
generation to vision-to-text generation.
•The out-of-domain and in-domain experi-
ments on image and video captioning across
different languages show that our approach
trained on text-only data significantly outper-
forms previous zero-shot/weakly-supervised
methods trained on unpaired or partial labeled
visual and textual data, setting new state-of-
the-art zero-shot performance.2 Approach
In this section, we first give a brief review of CLIP,
whose vision-language embedding space lays a
foundation for our approach. Next, we introduce
the framework of the proposed MultiCapCLIP, fol-
lowed by two key components: concept prompts
and textual augmentations.
2.1 A Brief Review of CLIP
CLIP uses two independent encoders to process
image and text input separately and then bridges
the gap between modalities with contrastive learn-
ing. The image encoder ϕ(·)can be a convo-
lutional neural network like ResNet (He et al.,
2016) or a vision Transformer like ViT (Dosovit-
skiy et al., 2021), and it extracts a feature vec-
tor for each input image. The text encoder ϕ(·)
is based on Transformer (Vaswani et al., 2017),
and it outputs a vector representation of the input
text. By training two encoders on 400M image-
text data with noisy correspondences under the In-
foNCE objective (Oord et al., 2018), CLIP learns
a powerful vision-language embedding space that
measures image-text similarity well and enables
open-vocabulary classification. In this paper, we
re-purpose CLIP for zero-shot multilingual visual
captioning and always keep ϕ(·)andϕ(·)frozen.
2.2 Overview of MultiCapCLIP
As shown in Figure 1, MultiCapCLIP consists of
visual and textual encoders from CLIP and a train-
able Multilingual Language Model (MLM). Multi-
CapCLIP supports English text, images or videos
as inputs and can produce output in desired lan-
guage. Specifically, we implement MLM with a
stack of Transformer decoder blocks, each of which
comprises a masked self-attention layer, a cross-
attention layer, and a feed-forward layer. Moreover,
we add explicit signals in the embedding layer to
indicate which language to be generated.
Let denote the text input as S, the vision input as
V, and concept prompts as P. Unlike typical visual
captioning models that are trained on a vision-text
dataset, MultiCapCLIP relies on a text dataset and
follows the S→P→Sauto-encoding pipeline
during training. Based on the semantic alignment
characteristic of CLIP’s feature space, MultiCap-
CLIP uses the V→P→Spipeline for visual
captioning during inference. We extend MultiCap-
CLIP to support multilingual text generation by us-11909
ing parallel corpora with (S, T)pairs, where Tde-
notes the target text in a desired language. In such
a case, MultiCapCLIP follows the S/V→P→T
translation pipeline.
In the following, we will detail how to extract
and leverage Pin Section 2.3. Then in Section
2.4, we will introduce an augmentation method to
improve the training of MultiCapCLIP.
2.3 Decoding with Concept Prompts
A set of visual concepts is a good embodiment of
domain visual knowledge because a visual concept
(e.g., “a young girl”) manifest as the explicit clue
in the vision input. Given a pure text dataset, we
use the spaCy toolkitto extract noun phrases and
reserve the most frequent 1,000 noun phrases as
visual concepts, which are first embedded into a
prompt template “{concept}”and then fed into the
CLIP’s text encoder ϕ(·)to extract l2-normalized
concept features C={c, . . . , c}.
During training, given the text input S, we first
encode it into a global feature f:
f= Norm( ϕ(S)), (1)
where Norm( ·)denotes L2 normalization. Next,
we calculate the dot product of fandCto measure
cosine similarities, based on which we obtain soft
concept prompts P, a subset of Cthat includes K
concept features most semantically similar to f.
Assuming that the dimension of vectors outputted
by CLIP is d,Pis in the shape of K∗d. To prompt
MLM, we prefix embeddings of the target text SwithPto obtain the final input embeddings E:
E= Concat( ω(P), e(S)) (2)
where ω(·)is implemented as a fully connected
layer followed by a layer normalization (LN) (Ba
et al., 2016), and e(·)denotes the summation of
position, language, and token embeddings for each
s∈S, followed by LN. The prompt sequence gen-
erated by ω(P)and token sequence generated by
e(S)are concatenated together and sent to the text
decoder of MLM to regenerate the input sequence
S. Considering that fmay contain information
supplementary to P, we do not discard f. We
first feed the projected feature f=ω(f), where
ω(·)has the same structure as ω(·)but shares no
parameters, into the text decoder of MLM. Then
we calculate the cross attention between fandE.
We train the model with a cross-entropy loss:
L=−/summationdisplaylogp(s=s|S, P, f),(3)
where p(·)is MLM’s predicted distribution over a
vocabulary and θdenotes all trainable parameters.
During inference, we process the vision input
Vin a similar manner, except that we use CLIP’s
image encoder ϕ(·)to obtain V’s vector repre-
sentation fand obtain relevant concept prompts
Pbased on (averaged) image-concept similarities.
Given the previously generated text S, the pre-
diction of the next token is based on the following
probability distribution:
p(s|S, P, f). (4)
2.4 Training with Augmentations
Our MultiCapCLIP’s ability of shifting text-to-text
generation to vision-to-text generation is built on11910
the assumption that the paired vision-text data is
well aligned in the vision-language embedding
space of CLIP. However, Liang et al. (2022) demon-
strated that there exists modality gap in CLIP-like
models and such gap has a significant impact on
model generalization ability. To this end, inspired
bydenoising auto-encoders (Vincent et al., 2008),
we propose to train MultiCapCLIP with augmented
text features f. Here we consider both the input
augmentation (IA) and the feature augmentation
(FA). Specifically, IA replaces the source text S
with a semantically similar one Sto obtain f:
f= Norm( ϕ(S)), (5)
where S∼XandX={S, S, . . . , S}de-
notes the candidate set of S. For simplicity, we
useϕ(·)to measure similarities among text in the
dataset and select the most similar N−1text to con-
structXfor each S. Since we sample text from
Xwith uniform probability, there will be 1/N
probability that the input text keeps unchanged. As
for FA, we follow Li et al. (2021) to add Gaus-
sian noise n∼ N (0, ϵ)into text features. Hence,
Eq. (5) can be further extended to:
f= Norm(Norm( ϕ(S)) +n). (6)
During training, we replace fin Eq. (1) with f
in Eq. (6) to encourage the model to learn more
robust latent representations.
3 Main Experiments
In this section, we first introduce the datasets, met-
rics, settings of the experiments; Then, we provide
the out-of-domain and in-domain results of our ap-
proach for zero-shot visual captioning.
3.1 Experimental Setups
Datasets. As shown in Table 1, we use three
benchmark datasets under CC BY 4.0 licence in
this section: MS-COCO (Chen et al., 2015), MSR-
VTT (Xu et al., 2016), and V ATEX (Wang et al.,
2019). We apply the Karpathy and Fei-Fei’s (2015)
split to MS-COCO and follow the official split of
MSR-VTT for English captioning. Besides, V A-
TEX is a multilingual video captioning dataset that
contains parallel English-Chinese captions. We
use it for Chinese captioning. In Section 4, we
will further use the Multi30K dataset (Elliott et al.,
2016) for German and French caption generation.
Metrics. Following the common practice in the
literature, we report BLEU@4 (Papineni et al.,
2002), METEOR (Banerjee and Lavie, 2005),
ROUGE-L (Lin, 2004) and CIDEr (Vedantam et al.,
2015) for video captioning, and additionally mea-
sure SPICE (Anderson et al., 2016) for image cap-
tioning. All metrics are computed by Microsoft
COCO Evaluation Server(Chen et al., 2015).
Settings As shown in Table 2, we conduct the
out-of-domain and in-domain experiments. 1) Out-
of-Domain Experiments are performed by training
the model on the text-only data of A dataset, and
then evaluating on the B dataset. 2) In-Domain Ex-
periments are conducted by training the model on
the text-only data of A dataset, and then evaluating
on the A dataset.11911
Baselines Since previous works can not gener-
ate zero-shot multilingual visual captions directly,
we implement a zero-shot CLIP-based model: ZS-
CapCLIP , which is trained on text-only data with
the same architecture as our MultiCapCLIP but
without our proposed concept prompts and text aug-
mentations. To observe the gap between zero-shot
and fully-supervised methods, We also implement
CapCLIP trained on vision-caption pairs.
Implementations. Following the previous works
in zero-shot captioning (Tewel et al., 2022b; Su
et al., 2022; Zeng et al., 2023), we adopt the
CLIP (ViT-B/16 variant) (Radford et al., 2021) as
our image encoder and text encoder, and adopt a
randomly initialized Transformer-BASE (Vaswani
et al., 2017) as our language decoder. We adopt the
same vocabulary as BERT / multilingual BERT
(Devlin et al., 2019) for English / non-English
captioning. We use the Jieba toolkitto segment
Chinese sentences. We select the hyperparame-
terKfrom values {4,8,16,32},Nfrom values
{5,10,20}andϵfrom values {0.01,0.1,1.0}ac-
cording to the CIDEr performance on the valida-
tion split, and set K= 16 ,N= 5,ϵ= 0.01
for all datasets and settings except that ϵ= 0.1
for in-domain experiments on MS-COCO. Dur-
ing training, we apply label smoothing (Szegedy
et al., 2016) of 0.1, use batches of 32 samples and
AdamW (Loshchilov and Hutter, 2019) with L2
weight decay of 0.01 to train models for 10 epochs.
We set the learning rate fixed to 1e-4 with 10%
warm-up iterations when training on text-only data.
During inference, we use beam search with a beam
size of 3 to generate captions.3.2 Out-of-Domain Results
In this section, we evaluate the zero-shot multilin-
gual captioning performance of our approach under
out-of-domain settings. We can notice from Table 3
that our zero-shot model MultiCapCLIP achieves
competitive performance on three datasets across
English and Chinese. Although SGM (Honda et al.,
2021) and RM (Guo et al., 2020) perform bet-
ter than our model on CIDEr and SPICE metrics
on MS-COCO, they require the large-scale image
datasets for training and use a larger training cor-
pus (2.3M sentences) than ours (130K sentences).
While the previous methods do not target non-
English caption generation, our MultiCapCLIP
gains obvious relative improvements against the
CapCLIP on V ATEX Chinese captioning. The out-
of-domain results show that our approach is able
to generate multilingual visual captions without
any labeled vision-caption data, which could have
the potential to promote the application of visual
captioning for low-resource language applications.
3.3 In-Domain Results
For comparisons, we further consider state-of-the-
art fully-supervised and large-scale pre-trained
models and models under the unpaired setting,
i.e., both vision and text data of the target dataset
are utilized for training independently, leaving
their pairing annotations unused. As shown in Ta-
ble 4, our approach significantly outperforms previ-
ous unpaired/zero-shot competitors by up to 4.8%
BLEU@4, 3.9% ROUGE-L, and 21.5% CIDEr
scores in MS-COCO English captioning. When it
comes to MSR-VTT English captioning and V A-
TEX Chinese captioning, our MultiCapCLIP sur-
passes ZS-CapCLIP by a large margin under the
CIDEr metric, e.g., an absolute improvement of11912
22.5% on MSR-VTT. These results prove the effec-
tiveness of MultiCapCLIP in zero-shot multilingual
visual captioning. Nevertheless, there still exists
performance gaps between MultiCapCLIP trained
on text-only data and existing state-of-the-art fully-
supervised models trained on full vision-text data.
4 Analysis
In this section, we conduct several analyses to bet-
ter understand our approach.
4.1 Semi-Supervised Visual Captioning
To further prove the effectiveness of our approach,
we fine-tune MultiCapCLIP with partial labeled
vision-caption data of downstream datasets. To
this end, in Figure 2, we evaluate the performanceof MultiCapCLIP with respect to the increasing
amount of labeled data. Specifically, we randomly
sample a small portion of training images/videos
and use the resulting vision-caption pairs for fine-
tuning. We repeat this process by three times and
report the average performance. For a fair compari-
son, we also train CapCLIP (Section 3.1) with the
same amount of pairs. As we can see in Figure 2,
for both in-domain or a out-of-domain corpus, Mul-
tiCapCLIP consistently outperforms CapCLIP with
different ratios of training data. It is worth not-
ing that the fewer the labeled vision-caption pairs,
the larger the margins. In detail, under the ex-
tremely low label setting, e.g., 0.1% of paired data
on MSR-VTT (only 6 videos), our approach under
the in-domain setting significantly surpasses the11913
CapCLIP by 23.1% absolute BLEU@4 score. It
further proves the effectiveness of our approach,
which can relax the reliance on the vision-caption
annotations. We can make use of available unpaired
text-only data as a solid basis for multilingual vi-
sual captioning tasks.
4.2 Quantitative Analysis
In this section, we analyze the contributions of each
component in our approach.
Ablation Study We conduct the ablation study
on the out-of-domain and in-domain settings using
MS-COCO dataset (Chen et al., 2015). As shown
in Table 5, each component in our proposed ap-
proach can boost the performance over all metrics,
verifying our arguments and the effectiveness of
our approach. In particular, setting (a) shows that
the introduced prompts can improve the base model
with absolute gains up to 5.9% and 13.3% CIDEr
scores under out-of-domain and in-domain settings,
respectively. Settings (b,c) show that either input
augmentation (IA) or feature augmentation (FA)
respectively boost performance, indicating the im-
portance of bridging the modality gap between the
visual and textual data and in turn, boosting the
robustness of the model and improving the perfor-
mance of zero-shot visual captioning. Moreover,
by comparing the results of (b) and (c), we ob-
serve that FA brings more improvements under the
in-domain setting whereas IA is better under the
out-of-domain setting. This indicates that structure
noises are more suitable to bridge the modality gap
between vision and text data from the same domain.
From another perspective, we need a more com-plex feature adaptation method for out-of-domain
transfer. Since the IA and FA can improve the per-
formance from different perspectives, as shown in
setting (d), combining them can lead to the most
prominent improvement across all metrics. More-
over, compared with (d), our full model in the set-
ting (g) can still gain improvements under most
metrics, especially the CIDEr metric, showing that
concept prompts benefit visual captioning by gen-
erating more accurate details.
Effect of K As shown in Table 5 (e-h), when
we set the number of prompts Kto 16, the model
substantially performs the best. For other Kvalues,
when K < 16, the performance is improved with
an increasing Kdue to more adequate guidance sig-
nals to the model. However, when K > 16, we can
observe saturated or impaired captioning perfor-
mance, possibly because retrieving more prompts
do not include additional useful clues and introduce
irrelevant noises to the model.
Concept Type Other than prompting the model
with noun phrases (Section 2.3), we also consider
the effect of verbs. As shown in Table 5, setting
(g) surpasses settings (i) and (j) at most cases, i.e.,
using verb-based prompts degrades performance.
We speculate the reason is that the vision-language
model we used (i.e., CLIP) can recognize salient
objects more accurately than human actions.
4.3 Robustness Analysis: Extensions to More
Non-English Languages
We adopt the Multi30K dataset (Elliott et al., 2016)
to further evaluate in-domain performance on Ger-
man and French image captioning. As shown in11914
Table 6, our full model again outperforms the base
model by a large margin, proving the effectiveness
of concept prompts and text augmentations.
4.4 Qualitative Analysis
In this section, we give some visualization results
and examples to better understand our approach.
Visualization To verify the effect of our method
on representation learning, we use t-SNE (van der
Maaten and Hinton, 2008) to visualize the features
produced by ZS-CapCLIP and our MultiCapCLIP
in Figure 4, which shows that our approach can
bridge the modality gap between visual and textual
inputs during training and obtain a blended distribu-
tion, leading to a more robust shift from text-to-text
generation to vision-to-text generation.
Examples In Figure 3, we compare our model
trained with out-of-domain corpora with CapCLIP
trained on full in-domain supervision. As we can
see, our model can generate accurate keywords,
e.g., “sand” in (a), “tire” in (c), and “helmet” in
(d), which can be attributed to the useful clues of
concept prompts. However, there exist noises in
the retrieved concepts in some cases, e.g., “a punch-
ing bag” in (b), misleading the model to produce
wrong details. Besides, in (e), we can observe how
the training corpus affect the writing style of the
model: the corpus of a video caption dataset (V A-
TEX) makes the model focus on the temporal evolu-
tion of events, resulting in a speculative description
“the catcher catches the ball”. Overall, our approach
can be a solid basis for zero-shot multilingual vi-
sual captioning. It requires no vision-caption pairs
but generates plausible visual descriptions.
5 Related Works
The related works are introduced from zero-shot
learning and visual captioning.11915Zero-shot Learning Adapting models to novel
tasks with limited labeled data is an important re-
search topic toward general intelligence (Griffiths
et al., 2019). Contrastive pre-training is an effective
technique to achieve this goal and has revolution-
ized multimodal research (Hou et al., 2021; Gan
et al., 2022; Jin et al., 2022; Cheng et al., 2023a).
Specifically for the vision-language field, models
such as CLIP (Radford et al., 2021) and ALIGN
(Jia et al., 2021) learn a shared multimodal embed-
ding space from large-scale noisy image-text pairs,
leading to an impressive zero-shot performance
on tasks like image classification and vision-text
retrieval (Zhang et al., 2022; Luo et al., 2022). Nev-
ertheless, employing CLIP-like models in low-data
vision-grounded text generation (i.e., visual cap-
tioning) remains challenging.
Visual Captioning As a key vision-language
task, visual captioning has achieved tremendous
progress under the encoder-decoder framework (Xu
et al., 2015) and the “pre-training and fine-tuning”
paradigm. Yet, typical visual captioning methods
require curated datasets of numerous images or
videos paired with descriptions in a specific lan-
guage, which are costly to collect. To this end,
some weakly-supervised approaches are proposed
(Feng et al., 2019; Guo et al., 2020; Honda et al.,
2021; Ben et al., 2022). These methods require
disjoint vision and text data for training and rely
on a pre-trained object detector like Faster R-CNN
(Ren et al., 2015) to construct weak supervision sig-
nals. However, the detectors they use are limited
to a pre-defined set of categories. Recently, several
works integrate CLIP with large language models
(LLMs) like GPT (Radford et al., 2019; Brown
et al., 2020) for zero-shot visual captioning (Tewel
et al., 2022b; Su et al., 2022; Liu et al., 2022; Zeng
et al., 2023). Although effective, these methods
suffer from over-parameterization of large LLMs.
We instead train a lightweight decoder from scratch.
Besides, some concurrent works address zero-shot
visual captioning by training CLIP with text-only
data (Nukrai et al., 2022; Gu et al., 2022; Li et al.,
2023; Yang et al., 2023). What differentiates our
work from them is that we consider visual concept
prompts that perverse domain visual knowledge.
6 Conclusions
We have presented a data-efficient method dubbed
MultiCapCLIP to re-purpose CLIP-like vision-
language pre-trained models for zero-shot multilin-gual visual captioning. Our approach reduces the
reliance on labeled vision-caption pairs of down-
stream datasets by auto-encoding concept prompts
on text-only data. Extensive experiments on four
datasets and four languages confirm the effective-
ness of our approach, which can be a solid basis
for visual captioning in low-data regimes and low-
resource languages.
Limitations
Although the proposed MultiCapCLIP can generate
multilingual zero-shot visual captions without any
labeled vision-caption training pairs. We still need
the independent set of text for training/translating,
which may still be difficult to collect for some low-
resource languages. This might be alleviated in
the future with techniques such as knowledge dis-
tillation from publicly-available pre-trained mod-
els, e.g., BERT (Devlin et al., 2019). Besides, our
approach uses CLIP to measure text-text similari-
ties for retrieving concept prompts and conducting
input augmentation during training. Considering
that CLIP is optimized by image-text global con-
trast (Radford et al., 2021) and intra-modal retrieval
of such a model is not as well as its cross-modal
retrieval (Jia et al., 2021), an improvement direc-
tion of our approach is using a vision-language
pre-trained model that measures intra-modal and
inter-modal semantic similarities well (Yang et al.,
2022b).
Ethics Statement
We conduct the experiments on public datasets,
which are exclusively about natural images, videos,
and captions. These datasets have been carefully
pre-processed for the academic study purpose, and
therefore do not contain any information that names
or uniquely identifies individual people or offensive
content. It is noteworthy that our approach inher-
its the drawback of the pre-trained backbone, i.e.,
CLIP, which has demonstrated that improper class
design used for prompting may raise unwanted
biases (Radford et al., 2021). Therefore, careful
examination is needed before employing our ap-
proach in real-world scenarios to avoid prejudices.
Acknowledgements
This paper was partially supported by NSFC (No:
62176008) and Shenzhen Science & Technology
Research Program ( ).11916References11917119181191911920ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Please see Section Limitations
/squareA2. Did you discuss any potential risks of your work?
Please see Section Ethics Statement
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Please see Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Please see Section 3.1
/squareB1. Did you cite the creators of artifacts you used?
Please see Section 3.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Please see Section 3.1
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Please see Section 3.1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We carry out a detailed anonymization process. We manually examine the data of widely adopted
benchmark datasets. If there exists information that names individual people, we replace it with
expressions like "he", "she", and "a person".
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Please see Table 1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Please see Table 1
C/squareDid you run computational experiments?
Please see Sections 3 and 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Please see Section 3.111921/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Please see Section 3.1
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Please see Section 4.1
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Please see Sections 2.3 and 3.1
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.11922