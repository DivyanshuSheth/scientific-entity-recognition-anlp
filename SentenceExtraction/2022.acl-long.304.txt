
Mingzhe Li, Xiexiong Lin, Xiuying Chen, Jinxiong Chang, Qishen Zhang,
Feng Wang,Taifeng Wang,Zhongyi Liu,Wei Chu,Dongyan Zhao,Rui YanWangxuan Institute of Computer Technology, Peking University, Beijing, ChinaCenter for Data Science, AAIS, Peking University, Beijing, ChinaAnt GroupComputational Bioscience Reseach Center, KAUSTGaoling School of Artificial Intelligence, Renmin University of China
li_mingzhe@pku.edu.cn ,xiexiong.lxx@antfin.com
Abstract
Contrastive learning has achieved impressive
success in generation tasks to militate the “ex-
posure bias” problem and discriminatively ex-
ploit the different quality of references. Exist-
ing works mostly focus on contrastive learn-
ing on the instance-level without discriminat-
ing the contribution of each word, while key-
words are the gist of the text and dominant the
constrained mapping relationships. Hence, in
this work, we propose a hierarchical contrastive
learning mechanism, which can unify hybrid
granularities semantic meaning in the input text.
Concretely, we first propose a keyword graph
via contrastive correlations of positive-negative
pairs to iteratively polish the keyword repre-
sentations. Then, we construct intra-contrasts
within instance-level and keyword-level, where
we assume words are sampled nodes from a
sentence distribution. Finally, to bridge the
gap between independent contrast levels and
tackle the common contrast vanishing problem,
we propose an inter-contrast mechanism that
measures the discrepancy between contrastive
keyword nodes respectively to the instance dis-
tribution. Experiments demonstrate that our
model outperforms competitive baselines on
paraphrasing, dialogue generation, and story-
telling tasks.
1 Introduction
Generation tasks such as storytelling, paraphras-
ing, and dialogue generation aim at learning a cer-
tain correlation between text pairs that maps an
arbitrary-length input to another arbitrary-length
output. Traditional methods are mostly trained with
“teacher forcing” and lead to an “exposure bias”
problem (Schmidt, 2019). Incorporating the gen-
eration method with contrastive learning achieved
impressive performance on tackling such issues,
which takes an extra consideration of synthetic neg-
ative samples contrastively (Lee et al., 2021).Figure 1: The semantic meaning of the sentence “what
are the best books on cosmology?” would be greatly
changed if the keyword “cosmology” is changed to “as-
trophysic”.
Existing contrastive mechanisms are mainly fo-
cused on the instance level (Lee et al., 2021; Cai
et al., 2020). However, word-level information is
also of great importance. Take the case shown in
the upper part of Figure 1 for example, the keyword
covers the gist of the input text and determines the
embedding space of the text. The text representa-
tion will be significantly affected if adding a slight
perturbation on the keyword, i.e.,changing “cos-
mology” to “astrophysics”. In addition, as shown
on the bottom part, under some circumstances, it
is too easy for the model to do the classification
since the semantic gap between contrastive pairs is
huge. Thus, the model fails to distinguish the actual
discrepancy, which causes a “contrast vanishing”4432problem at both instance-level and keyword-level.
Based on the above motivation, in this paper, we
propose a hierarchical contrastive learning method
built on top of the classic CV AE structure. We
choose CV AE due to its ability in modeling global
properties such as syntactic, semantic, and dis-
course coherence (Li et al., 2015; Yu et al., 2020).
We first learn different granularity representations
through two independent contrast, i.e.,instance-
level and keyword-level. Specifically, we use the
universal and classic TextRank (Mihalcea and Ta-
rau, 2004) method to extract keywords from each
text, which contain the most important informa-
tion and need to be highlighted. On the instance-
level, we treat the keyword in the input text as
an additional condition for a better prior semantic
distribution. Then, we utilize Kullback–Leibler di-
vergence (Kullback and Leibler, 1951) to reduce
the distance between prior distribution and posi-
tive posterior distribution, and increase the distance
with the negative posterior distribution. While on
the keyword-level, we propose a keyword graph via
contrastive correlations of positive-negative pairs
to learn informative and accurate keyword repre-
sentations. By treating the keyword in the output
text as an anchor, the imposter keyword is produced
by neighboring nodes of the anchor keyword and
forms the keyword-level contrast, where the simi-
larity between the imposter keyword and the anchor
keyword is poorer than the positive keyword.
To unify individual intra-contrasts and tackle the
“contrast vanishing” problem in independent con-
trastive granularities, we leverage an inter-contrast,
the Mahalanobis contrast, to investigate the con-
trastive enhancement based on the Mahalanobis
distance (De Maesschalck et al., 2000), a measure
of the distance between a point and a distribution,
between the instance distribution and the keyword
representation. Concretely, we ensure the distance
from the anchor instance distribution to the ground-
truth keyword vector is closer than to the imposter
keyword vector. The Mahalanobis contrast plays
an intermediate role that joins the different granu-
larities contrast via incorporating the distribution
of instance with the representation of its crucial
part, and makes up a more comprehensive keyword-
driven hierarchical contrastive mechanism, so as to
ameliorate the generated results.
We empirically show that our model outperforms
CV AE and other baselines significantly on three
generation tasks: paraphrasing, dialogue genera-tion, and storytelling.
Our contributions can be summarized as follows:
•To our best knowledge, we are the first to
propose an inter-level contrastive learning method,
which unifies instance-level and keyword-level con-
trasts in the CV AE framework.
•We propose three contrastive learning measure-
ments: KL divergence for semantic distribution, co-
sine distance for points, and Mahalanobis distance
for points with distribution.
•We introduce a global keyword graph to obtain
polished keyword representations and construct im-
poster keywords for contrastive learning.
2 Related Work
2.1 Contrastive Learning
Contrastive learning is used to learn representa-
tions by teaching the model which data points are
similar or not. Due to the excellent performance
on self-supervised and semi-supervised learning, it
has been widely used in natural language process-
ing (NLP). Firstly, Mikolov et al. (2013) proposed
to predict neighboring words from context with
noise-contrastive estimation. Then, based on word
representations, contrastive learning for sentence
has been utilized to learn semantic representations.
Lee et al. (2021) generated positive and negative ex-
amples by adding perturbations to the hidden states.
Cai et al. (2020) augmented contrastive dialogue
learning with group-wise dual sampling. More-
over, contrastive learning has also been utilized
in caption generation (Mao et al., 2016), summa-
rization (Liu and Liu, 2021) and machine transla-
tion (Yang et al., 2019). Our work differs from pre-
vious works in focusing on hierarchical contrastive
learning on hybrid granularities.
2.2 Mahalanobis Distance
The Mahalanobis distance is a measure of the dis-
tance between a point and a distribution (De Maess-
chalck et al., 2000). The distance is zero if the point
is on the distribution. Recently, Mahalanobis dis-
tance is popularly applied to the NLP tasks (Tran
et al., 2019). Podolskiy et al. (2021) showed that
while Transformer is capable of constructing homo-
geneous representations of in-domain utterances,
the Mahalanobis distance captures geometrical dis-
parity from out of domain utterances. Further, Ren
et al. (2021) considered that the raw density from
deep generative models may fail at out-of-domain
detection and proposed to fix this using a likeli-4433hood ratio between two generative models as a
confidence score.
2.3 Conditional Variational Auto-Encoder
Variational autoencoder (V AE) was proposed
by Kingma and Welling (2013), and has been
widely used in various tasks such as headline gen-
eration (Li et al., 2021), dialogue generation (Ser-
ban et al., 2017) and story generation (Yu et al.,
2020). Based on V AE, a more advanced model,
Conditional V AE (CV AE), was proposed to gener-
ate diverse images conditioned on certain attributes,
which was also applied to generate diverse outputs
in NLP tasks (Zhao et al., 2017; Qiu et al., 2019).
Existing works concentrate on generating diverse
outputs, and we take one step further to utilize prior
and posterior latent distribution to compare positive
and negative samples, which helps to learn more
accurate semantic information.
3 Method
3.1 Background
VAE: Variational auto-encoder (V AE) is a typi-
cal encoder-decoder structural model with certain
types of latent variables. Given an input x, V AE
models the latent variable zthrough the prior dis-
tribution p(z), and the observed data xis re-
constructed by the generative distribution p(x|z)
which is the likelihood function that generates x
conditioned on z. Since zis unknown, it should
be estimated according to the given data xas
p(z|x). While the posterior density p(z|x) =
p(x|z)p(z)/p(x)is intractable, V AE introduces
a recognition posterior distribution q(z|x)approx-
imates to the true posterior p(z|x). Thus, V AE
is trained by optimizing the lower bound on the
marginal likelihood of data xas:
logp(x)≥E[logp(x|z)]
−D(q(z|x)||p(z)),(1)
where Dis the Kullback–Leibler divergence.
CV AE: The conditional variational auto-encoder
(CV AE) is the supervised version of V AE with
an additional output variable. Giving a dataset
{x, y}consisting of Nsamples, CV AE is
trained to maximize the conditional log-likelihood,
and the variational lower bound of the model is
written as follows:
logp(y|x)≥E[logp(y|x, z)]
−D(q(z|x, y)||p(z|x)).(2)Assuming the type of latent variable obeys Gaus-
sian distribution, the first right-hand side term can
be approximated by drawing samples {z}from
the recognition posterior distribution q(z|x, y),
where z∼N(µ, σI), and then objective of the
CV AE with Gaussian distribution can be written as:
L(x, y;θ, ϕ) =−1
NXlogp(y|x, z)
+D(q(z|x, y)||p(z|x)),(3)
where z=g(x, y, ϵ),ϵ∼ N (0, I). The dis-
tribution q(z|x, y)is reparameterized with a dif-
ferentiable function g, which enables the model
trainable via stochastic gradient descent.
Inspired by Wu et al. (2019), we add keyword u
as an additional condition to the prior distribution
to control the generation process, which turns the
p(z|x)in Equaton 3 into p(z|x, u).
3.2 Hierarchical Contrastive Learning
In this section, we introduce our hierarchical con-
trastive learning method, which is comprised of
three parts: instance-level contrast based on KL di-
vergence (sec.3.2.1), keyword-level contrast based
on keyword graph (sec.3.2.2), and inter-contrast:
Mahalanobis contrast (sec.3.2.3).
3.2.1 Instance-level Contrastive Learning
To tackle the “exposure bias” problem and discrim-
inatively exploit the different quality of references,
instance-level contrastive learning is introduced to
learn discrepancies of targets. Specifically, in ad-
dition to the observed input data xand positive
output y, a negative output yis added to con-
struct a contrastive pair {(x, y),(x, y)}. In this
case, the prior distribution p(z|x)is learned from
a prior network, which is denoted as f(x). The ap-
proximate posteriors q(z|x, y)andq(z|x, y)
are learned from a posterior network and repre-
sented as f(x, y)andf(x, y), respectively.
The objective here is to make the distance between
a prior distribution and positive posterior distribu-
tion closer than with the negative posterior distri-
bution. Thus, the instance-level contrastive loss
function can be written as:
where the y∈Ycan be positive sample yor
negative sample y, and the τis a temperature4434
parameter to control push and pull force. The func-
tionh(·)denotes the distance between elements,
which is set as Kullback–Leibler divergence (Kull-
back and Leibler, 1951) in instance-level contrast,
D(f(x, y)||f(x)), to measure the difference
between two distributions.
3.2.2 Keyword-level Contrastive Learning
Since the instance-level contrast focuses on learn-
ing high-level information and fails to discriminate
the contribution of each word, we incorporate it
with a keyword-level contrast to pay more attention
to the specific keyword.
Keyword Graph: Given an input-output text pair
(x, y), keywords k, kcan be extracted from xand
y, respectively. For an input text xwith keyword
k, input texts that contain the same keyword are
gathered into a cluster C={x}, k∈x,
where nis the number of texts in C. Each text
x∈Chas a positive-negative output text pair
{(y, y)}containing a positive output keyword
kand a negative one k, respectively. Thus,
spreading to the entire cluster C, for the output
texty, there exists positive relations rbetween
its keyword kand each of the surrounded posi-
tive keywords {k}. Likewise, negative rela-
tionsrcorrelates the output keyword kand the
surrounded negative ones {k}.
Based on these keywords as nodes and their
relations as edges (Chen et al., 2021), the key-word graph Gis constructed. Each node repre-
sentation his initialized as the average BERT
embedding (Devlin et al., 2018) of texts in the
cluster Cwith the same corresponding keyword
k. Then, the relation edge rthat connects node
iand node jis learned via a feedforward layer
r=FFN([h;h]).
Then, the representations of nodes and relation
edges are iteratively updated with their connected
nodes via the graph attention (GAT) layer and the
feed-forward (FFN) layer. In the t-th iteration, we
first update each edge representation by paying
attention to the connected nodes, denoted as:
β=softmax ((rW)(hW)
√
d), (4)
p=βh+βh, (5)
r=FFN(r+p), (6)
where hcan be horh.
Then, based on the obtained edge representation
r, we update the node representations consid-
ering both the related nodes and relation edges by
the graph attention layer, GAT(h, h, r), which
is designed as:
e=, (7)
α=, (8)
u=Pα(hW+r), (9)4435where W, W, WandWare all learnable param-
eters, and the αis the attention weight between h
andh. Besides, to avoid gradient vanishing after
several iterations, a residual connection is added to
the output uand the updated node representations
his obtained. In this way, the new representa-
tion of each keyword node consists of the relation
dependency information from neighbor nodes N.
We take the node representations from the last iter-
ation as the final keyword representations, denoted
asufor brevity.
Keyword-level Contrast: The keyword-level
contrastive learning arises from input keywords
against positive output keywords and negative im-
postor keywords. The input keyword uis ex-
tracted from the input text as an anchor, and the
output keyword uis extracted from ground-truth
output text. While the impostor keyword is calcu-
lated from the negative neighbours of the output
keyword u, written as u=PWu, where
uis the representation of keyword node which
is obtained by the keyword graph learning proce-
dure described above. In this way, with the help
of neighbour nodes in the graph, we can obtain a
more indistinguishable and difficult negative sam-
ple. The loss of keyword level contrastive learning
thus can be written as:
L =−E[loge
Pe],(10)
where u∈Udenotes the positive output keyword
uor imposter keyword u. In keyword-level
contrast, h(·)utilizes cosine similarity to calculate
the distance between points.
3.2.3 Mahalanobis Contrastive Learning
Note that there exists a space gap between the
instance-level contrast and the keyword-level con-
trast, which disturbs the completeness of this hi-
erarchical contrastive architecture. Besides, the
contrastive values vanish when the distance met-
ric is hard to measure the actual discrepancy be-
tween positive and negative merely in instance
distributions or in keyword representations. To
mitigate such problems, we design a Mahalanobis
contrastive mechanism to correlate the instance dis-
tribution and keyword representation, where the
objective is to minimize the margin between the
output keyword uand the posterior semantic dis-
tribution q(z|x, y)≜f(x, y)and maximize the
margin between the imposter keyword uandthe posterior distribution f(x, y):
L=−E[log(1−e
Pe)],
(11)
where u∈Ucan be the positive output keyword
uor negative imposter keyword u. In Ma-
halanobis contrast, h(·)utilizes Mahalanobis dis-
tance (De Maesschalck et al., 2000) to measure
the similarity from keyword point to the instance
distribution. In the univariate Gaussian case, z∼
p(z|x, y) =N(µ, σ), then the h(f(x, y), u)≜
D(p(z|x, y)||u) = (u−µ)σI(u−µ).
Finally, we equip the CV AE model with the
proposed hierarchical contrastive learning frame-
work to unify hybrid granularities by adding L,
L andLto the reconstructed loss of
Equation 3.
4 Experiment
4.1 Tasks and Datasets
We conduct experiments on three public datasets
QQP, Douban, RocStories for paraphrasing, dia-
logue generation, and storytelling task, respectively.
The details of the datasets are as follows:
Dialogue (Douban) Douban (Cai et al., 2020)
consists of Chinese daily conversations between
pairs of speakers, collected from a popular social
network website, Douban group. The dataset
contains 218,039/10,000/10,000 context-response
pairs for training/validation/test, with an average
of 3.94 turns per context and 38.32 characters per
utterance. We concatenate historical dialogues and
turn it into a single-turn dialogue training corpus.
Paraphrasing (QQP) QQP (Iyer et al., 2017;
Wang et al., 2019) is a dataset published by the
community question-answering website Quora on
whether a pair of questions is semantically con-
sistent. To adapt it to the contrastive learning
task, we only keep question pairs that have pos-
itive and negative rewriting for the same input.
Thus, there remain 44,949 samples in the dataset,
which are split into training/validation/test sets of
40,441/2,254/2,254 samples.
Storytelling (RocStories) RocStories consists of
98,163 high-quality hand-crafted stories, which
capture causal and temporal commonsense rela-
tions of daily events (Mostafazadeh et al., 2016).4436
Each story paragraph contains 5 sentences with an
average of 43 words. Following the previous work
Yu et al. (2021), we split the dataset into 8:1:1 for
training, validation, and test.
For the above three datasets, in order to con-
struct different levels of contrastive learning, we
performed the same preprocessing of extracting
keywords. We utilize the TextRank model (Mi-
halcea and Tarau, 2004) to extract keywords from
each input and output sample, respectively. Be-
sides, the vocabulary size of both datasets is the
same as BERT (Devlin et al., 2018) setting.
4.2 Implementation Details
Our experiments are implemented in Tensor-
flow (Abadi et al., 2016) on an NVIDIA Tesla P100
GPU. For our model and all baselines, we follow
the same setting as described below. We pad or
cut the input to 100, 20, 100 words for dialogue
generation, paraphrasing, and storytelling, respec-
tively. The truncation length is decided based on
the observation that there is no significant improve-
ment when increasing input length. The minimum
decoding step is 5, and the maximum step is 20 forall tasks. Experiments were performed with a batch
size of 256, and we use Adam optimizer (Kingma
and Ba, 2015) as our optimizing algorithm. Dur-
ing the test stage, the beam-search size is set to 4
for all methods and the checkpoint with the small-
est validation loss is chosen. Note that for better
performance, our model is built based on BERT,
and the decoding process is the same as Trans-
former (Vaswani et al., 2017). Finally, due to the
limitation of time and memory, small settings are
used in the pre-training baselines.
4.3 Compared Baselines
We compare our method against several traditional
generation models, pretrained-based generation
models, and contrastive learning models.
Traditional generation models: (1)CV AE (Zhao
et al., 2017) generates sentences based on latent
variables, sampling from potential semantic dis-
tribution. (2) Seq2Seq (Sutskever et al., 2014)
is a sequence-to-sequence framework combined
with attention mechanism and pointer network. (3)
Transformer (Vaswani et al., 2017) is an abstrac-
tive method based solely on attention mechanisms.4437
Pretrained-based generation models: (4)
Seq2Seq-DU (Feng et al., 2021) is concerned
with dialogue state tracking in a task-oriented
dialogue system. (5) DialoGPT (Zhang et al.,
2020) proposes a large, tunable neural conversa-
tional response generation model trained on more
conversation-like exchanges. (6) BERT-GEN (De-
vlin et al., 2018) augments Seq2Seq with BERT as
the encoder. (7) T5(Raffel et al., 2020) introduces
a unified framework that converts all text-based
language problems into a text-to-text format.
Contrastive learning methods: (8)Group-
wise (Cai et al., 2020) augments contrastive dia-
logue learning with group-wise dual sampling. (9)
T5-CLAPS (Lee et al., 2021) generates negative
and positive samples for contrastive learning by
adding small and large perturbations, respectively.
4.4 Evaluation Metrics
To evaluate the performance of our model against
baselines, we adopt the following metrics widely
used in existing studies.
BLEU We utilize BLEU score (Papineni et al.,
2002) to measure word overlap between the gen-
erated text and the ground-truth. Specifically, fol-
lowing the conventional setting of (Gu et al., 2019),
we adopt BLEU-1 ∼4 scores under the smoothing
techniques (smoothing 7).
Embedding To evaluate our model more compre-
hensively, we also capture the semantic matching
degrees between the bag-of-words (BOW) embed-
dings of generated text and reference (Gu et al.,
2019). Particularly we adopt three metrics: 1) Ex-
trema , cosine similarity between the largest ex-
treme values among the word embeddings in the
two texts; 2) Average , cosine similarity between
the averaged word embeddings of generated text
and reference; 3) Greedy , greedily matching words
in the two texts based on cosine similarities.
4.5 Experimental Results
4.5.1 Overall Performance
Automatic Evaluation The experimental results
ars summarized in Table 1. The upper part lists the
effects of traditional generation methods such as
Seq2Seq andTransformer , and the lower part
shows the latest pretrained-based methods includ-
ingDialoGPT andT5. Overall, pretrained-based
methods generally outperform traditional methods,
and this also proves the effectiveness of the pre-
trained language model on the generation tasks.
Secondly, we can find that the performance is sig-
nificantly improved after adding contrast learning.
Finally, our method outperforms T5-CLAPS by
2.7%, 3.6% on QQP, by 20.3%, 24.9% on Douban,
and by 3.9%, 6.3% on RocStories in terms of
BLEU-1, BLEU-2, respectively, which proves the
superiority of our model.
Human Evaluation We also assessed system per-
formance by eliciting human judgments on 100
randomly selected test instances on QQP dataset.
Three annotators are asked to rate paraphrasing
questions generated by T5-CLAPS ,DialoGPT ,
Seq2Seq-DU , and our model according to Flu-
ency (Flu), Meaningfulness (Mean), and Differen-
tial (Diff). The rating score ranges from 1 to 3, with
3 being the best. Table 3 lists the average scores of
each model, showing that our model outperforms
other baselines among all metrics, which indicates
that our model generates paraphrasing sentences
more readable successfully. The kappa statistics
are 0.53, 0.61, and 0.56 for fluency, meaningful-
ness, and differential, respectively, which indicates
the moderate agreement between annotators.
4.5.2 Ablation Study
We conduct ablation tests to assess the importance
of the keyword graph architecture ( w/o graph ),
keyword ( w/o keyword ), as well as the Maha-4438
lanobis contrast ( w/o MA contrast ), and the
results are shown in Table 2. Concretely, after re-
moving the keywords (w/o keyword), using only
instance-level contrastive, the effect of our model is
greatly reduced by about 10.4%, which illustrates
the desirability of considering the contributions of
words in a sentence. On this basis, adding keyword
contrastive learning with removing the keyword
graph, the effect of the model has been improved
but is still lower than our model by 2.1%. This
shows that keywords are indeed conducive to cap-
turing important information, and it also illustrates
the significance of a keyword graph. Finally, the ex-
periment of removing the Mahalanobis contrastive
loss indicates that only with granularity indepen-
dent contrast is not sufficient, and the Mahalanobis
contrast plays a critical intermediate role.
4.5.3 Visualization of Different Levels of
Contrastive Learning
To study the hierarchical contrastive learning, we
visualize the vectors of keyword, input text, posi-
tive and negative output text on randomly sampled
cases from QQP dataset, as shown in Figure 3.
For visualization purposes, we reduce the dimen-
sion of the latent vector with t-SNE (Maaten and
Hinton, 2008). It can be observed that the input
sentence representation is located close to the key-
word, which shows that the keyword, as the most
important information in the sentence, determines
the semantic distribution. Moreover, in contrastive
learning, it can be seen that after training, the po-
sition of the input sentence is close to the positive
samples and far away from the negative samples.
This suggests that contrastive learning can correct
the semantic distribution.
4.5.4 Analysis of Different Keywords
We finally investigate the influence of sampling dif-
ferent keywords. As shown in Table 4, for an input
question, we provide keywords extracted by Tex-
tRank and randomly-selected keywords as the con-
dition to control the semantic distribution and ex-
amine the quality of the generated text. As the most
important information unit, different keywords lead
to different semantic distributions and will result
in different generated texts. The more properly
the keywords are selected, the more accurately the
sentences will be generated. When utilizing the
keywords extracted by TextRank as a condition, the
information “belly fat” is focused during the gener-
ation of paraphrasing questions, and the generated
sentences are more accurate. On the contrary, after
adding the random-selected keyword “disposable”,
the generated question emphasizes “one-off exer-
cise”, which brings incorrect information.
We also compare our model with several base-
lines in Table 4. Most baselines can generate fluent
questions in this case. However, they focus on
“lose weight”, and miss the significant information
“belly fat”. Based on the above analysis, we can
observe that keywords can emphasize and protect
the highlight information in sentences, and affect
the semantic distribution of as a condition.
5 Conclusion
In this paper, we propose a hierarchical con-
trastive learning mechanism, which consists of
intra-contrasts within instance-level and keyword-
level and inter-contrast with Mahalanobis con-
trast. The experimental results yield significant
out-performance over baselines when applied in4439the CV AE framework. In the future, we aim to
extend the contrastive learning mechanism to dif-
ferent basic models, and will explore contrastive
learning methods based on external knowledge.
6 Acknowledgments
We would like to thank the anonymous reviewers
for their constructive comments. This work was
supported by National Key Research and Develop-
ment Program of China (No. 2020AAA0106600),
National Natural Science Foundation of China
(NSFC Grant No. 62122089, No. 61832017 & No.
61876196), and Beijing Outstanding Young Scien-
tist Program No. BJJWZYJH012019100020098.
This work was also supported by Alibaba Group
through Alibaba Research Intern Program.
7 Ethics Impact
In this paper, we propose an inter-level contrastive
learning method, which unifies instance-level and
keyword-level contrasts in the CV AE framework.
The positive impact lies in that it can help improve
the capability of generation models on paraphras-
ing, dialogue generation, and storytelling tasks.
The negative impact may be that the generation
process of the system is not fully controllable, so
it is possible to generate inaccurate or unreason-
able content in some extreme cases. Hence, extra
processing steps might be needed if this method
were to be used in scenarios where high accuracy
is required.
References44404441