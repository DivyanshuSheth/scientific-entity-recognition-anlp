
Longyue Wang, Siyou Liu, Mingzhou Xu, Linfeng Song, Shuming Shi, Zhaopeng Tu
Tencent AI Lab
{vinnylywang,lifengjin,shumingshi,zptu}@tencent.com
guofeng-ai@googlegroups.com
Abstract
Zero pronouns (ZPs) are frequently omitted
in pro-drop languages (e.g. Chinese, Hungar-
ian, and Hindi), but should be recalled in non-
pro-drop languages (e.g. English). This phe-
nomenon has been studied extensively in ma-
chine translation (MT), as it poses a significant
challenge for MT systems due to the difficulty
in determining the correct antecedent for the
pronoun. This survey paper highlights the ma-
jor works that have been undertaken in zero
pronoun translation (ZPT) after the neural revo-
lution so that researchers can recognize the cur-
rent state and future directions of this field. We
provide an organization of the literature based
on evolution, dataset, method, and evaluation.
In addition, we compare and analyze compet-
ing models and evaluation metrics on different
benchmarks. We uncover a number of insight-
ful findings such as: 1) ZPT is in line with the
development trend of large language model;
2) data limitation causes learning bias in lan-
guages and domains; 3) performance improve-
ments are often reported on single benchmarks,
but advanced methods are still far from real-
world use; 4) general-purpose metrics are not
reliable on nuances and complexities of ZPT,
emphasizing the necessity of targeted metrics;
5) apart from commonly-cited errors, ZPs will
cause risks of gender bias.
1 Introduction
Pronouns play an important role in natural lan-
guage, as they enable speakers to refer to people,
objects, or events without repeating the nouns that
represent them. Zero pronoun (ZP)is a complex
phenomenon that appears frequently in pronoun-
dropping (pro-drop) languages such as Chinese,
Hungarian, and Hindi. Specifically, pronouns are
often omitted when they can be pragmaticallyor grammatically inferable from intra- and inter-
sentential contexts (Li and Thomson, 1979). Since
recovery of such ZPs generally fails, this poses
difficulties for several generation tasks, including
dialogue modelling (Su et al., 2019), question an-
swering (Tan et al., 2021), and machine transla-
tion (Wang, 2019).
When translating texts from pro-drop to non-pro-
drop languages (e.g. Chinese ⇒English), this phe-
nomenon leads to serious problems for translation
models in terms of: 1) completeness , since transla-
tion of such invisible pronouns cannot be normally
reproduced; 2) correctness , because understanding
the semantics of a source sentence needs to identi-
fying and resolving the pronominal reference.
Figure 1 shows ZP examples in three typological
patterns determined by language family (detailed
in Appendix §A.1). Taking a full-drop language for
instance, the first-person subject and third-person
object pronouns are omitted in Hindi input while
these pronouns are all compulsory in English trans-
lation. This is not a problem for human beings
since we can easily recall these missing pronoun
from the context. However, even a real-life MT
system still fails to accurately translate ZPs.
In response to this problem, zero pronoun trans-
lation (ZPT) has been studied extensively in the
MT community on three significant challenges:
•Dataset : there is limited availability of ZP-
annotated parallel data, making it difficult to de-
velop systems that can handle ZP complexities.
•Approach : due to the ability to capture seman-
tic information with distributed representations,
ideally, the representations of NMT should em-
bed ZP information by learning the alignments
between bilingual pronouns from the training
corpus. In practice, however, NMT models only
manage to successfully translate some simple
ZPs, but still fail when translating complex ones
(e.g. subject vs. object ZPs).
•Evaluation : general evaluation metrics for MT3325
are not sensitive enough to capture translation
errors caused by ZPs.
We believe that it is the right time to take stock of
what has been achieved in ZPT, so that researchers
can get a bigger picture of where this line of re-
search stands. In this paper, we present a survey
of the major works on datasets, approaches and
evaluation metrics that have been undertaken in
ZPT. We first introduce the background of linguis-
tic phenomenon and literature selection in Sec-
tion 2. Section 3 discusses the evolution of ZP-
related tasks. Section 4 summarizes the annotated
datasets, which are significant to pushing the stud-
ies move forward. Furthermore, we investigated
advanced approaches for improving ZPT models
in Section 5. In addition to this, Section 6 covers
the evaluation methods that have been introduced
to account for improvements in this field. We con-
clude by presenting avenues for future research in
Section 7.
2 Background
2.1 Linguistic Phenomenon
Definition of Zero Pronoun Cohesion is a sig-
nificant property of discourse, and it occurs when-
ever “the interpretation of some element in the dis-
course is dependent on that of another” (Halliday
and Hasan, 1976). As one of cohesive devices,
anaphora is the use of an expression whose inter-pretation depends specifically upon antecedent ex-
pression while zero anaphora is a more complex
scenario in pro-drop languages. A ZP is a gap
in a sentence, which refers to an entity that sup-
plies the necessary information for interpreting the
gap (Zhao and Ng, 2007). ZPs can be categorized
into anaphoric and non-anaphoric ZP according
to whether it refers to an antecedent or not. In
pro-drop languages such as Chinese and Japanese,
ZPs occur much more frequently compared to non-
pro-drop languages such as English. The ZP phe-
nomenon can be considered one of the most diffi-
cult problems in natural language processing (Peral
and Ferrández, 2003).
Extent of Zero Pronoun To investigate the ex-
tent of pronoun-dropping, we quantitatively ana-
lyzed ZPs in two corpora and details are shown in
Appendix §A.2. We found that the frequencies and
types of ZPs vary in different genres: (1) 26% of
Chinese pronouns were dropped in the dialogue
domain, while 7% were dropped in the newswire
domain; (2) the most frequent ZP in newswire text
is the third person singular 它(“it”) (Baran et al.,
2012), while that in SMS dialogues is the first per-
son我(“I”) and 我们(“we”) (Rao et al., 2015).
This may lead to differences in model behavior
and quality across domains. This high proportion
within informal genres such as dialogues and con-
versation shows the importance of addressing the
challenge of translation of ZPs.33262.2 Literature Selection
We used the following methodology to provide a
comprehensive and unbiased overview of the cur-
rent state of the art, while minimizing the risk of
omitting key references:
•Search Strategy : We conducted a systematic
search in major databases (e.g. Google Scholar)
to identify the relevant articles and resources.
Our search terms included combinations of key-
words, such as "zero pronouns," "zero pronoun
translation," and "coreference resolution."
•Selection Criteria : To maintain the focus and
quality of our review, we established the follow-
ing criteria. (1) Inclusion, where articles are pub-
lished in journals, conferences and workshop pro-
ceedings. (2) Exclusion, where articles that are
not available in English or do not provide suffi-
cient details to assess the validity of their results.
•Screening and Selection : First, we screened the ti-
tles and abstracts based on our Selection Criteria.
Then, we assessed the full texts of the remain-
ing articles for eligibility. We also checked the
reference lists of relevant articles to identify any
additional sources that may have been missed
during the initial search.
•Data Extraction and Synthesis : We extracted key
information from the selected articles, such as
dataset characteristics, and main findings. This
data was synthesized and organized to provide
a comprehensive analysis of the current state of
the art in ZPT.
3 Evolution of Zero Pronoun Modelling
Considering the evolution of ZP modelling, we
cannot avoid discussing other related tasks. Thus,
we first review three typical ZP tasks and conclude
their essential relations and future trends.
3.1 Overview
ZP resolution is the earliest task to handle the un-
derstanding problem of ZP (Zhao and Ng, 2007).
ZP recovery and translation aim to directly generate
ZPs in monolingual and crosslingual scenarios, re-
spectively (Yang and Xue, 2010; Chung and Gildea,
2010). This is illustrated in Figure 2.
Zero Pronoun Resolution The task contains
three steps: ZP detection, anaphoricity determi-
nation and reference linking. Earlier works inves-
tigated rich features using traditional ML models
(Zhao and Ng, 2007; Kong and Zhou, 2010; Chenand Ng, 2013, 2015). Recent studies exploited neu-
ral models to achieve the better performance (Chen
and Ng, 2016; Yin et al., 2018; Song et al., 2020).
The CoNLL2011 and CoNLL2012are commonly-
used benchmarks on modeling unrestricted coref-
erence. The corpus contains 144K coreference in-
stances, but dropped subjects only occupy 15%.
Zero Pronoun Recovery Given a source sen-
tence, this aims to insert omitted pronouns in proper
positions without changing the original mean-
ing (Yang and Xue, 2010; Yang et al., 2015, 2019a).
It is different from ZP resolution, which identifies
the antecedent of a referential pronoun (Mitkov,
2014). Previous studies regarded ZP recovery
as a classification or sequence labelling problem,
which only achieve 40 ∼60% F1 scores on closed
datasets (Zhang et al., 2019; Song et al., 2020),
indicating the difficulty of generating ZPs. It is
worth noting that ZP recovery models can work for
ZPT task in a pipeline manner: input sentences are
labeled with ZPs using an external recovery system
and then fed into a standard MT model (Chung and
Gildea, 2010; Wang et al., 2016a).
Zero Pronoun Translation When pronouns are
omitted in a source sentence, ZPT aims to gen-
erate ZPs in its target translation. Early stud-
ies have investigate a number of works for SMT
models (Chung and Gildea, 2010; Le Nagard and
Koehn, 2010; Taira et al., 2012; Xiang et al., 2013;
Wang et al., 2016a). Recent years have seen a surge
of interest in NMT (Yu et al., 2020; Wang et al.,
2018a), since the problem still exists in advanced
NMT systems. ZPT is also related to pronoun trans-
lation, which aims to correctly translate explicit
pronoun in terms of feminine and masculine. The
DiscoMTis a commonly-cited benchmark on pro-
noun translation, however, there was no standard
ZPT benchmarks up until now.
3.2 Discussions and Findings
By comparing different ZP-aware tasks, we found
three future trends:
1.From Intermediate to End . In real-life systems,
ZP resolution and recovery are intermediate tasks
while ZPT can be directly reflected in system out-
put. ZP resolution and recovery will be replaced
by ZPT although they currently work with some
MT systems in a pipeline way.3327
2.From Separate To Unified . With the develop-
ment of large language models (LLMs), it is un-
necessary to keep a specific model for each task.
For example, Song et al. (2020) leveraged a uni-
fied BERT-based architecture to model ZP reso-
lution and recovery. Furthermore, we observed
that ChatGPTalready possesses the capability
for ZP resolution and recovery.
4 Datasets
4.1 Overview
Modeling ZPs has so far not been extensively ex-
plored in prior research, largely due to the lack of
publicly available data sets. Existing works mostly
focused on human-annotated, small-scale and
single-domain corpora such as OntoNotes (Pradhan
et al., 2012; Aloraini and Poesio, 2020) and Tree-
banks (Yang and Xue, 2010; Chung and Gildea,
2010). We summarize representative corpora as:
•OntoNotes.This is annotated with structural
information (e.g. syntax and predicate argument
structure) and shallow semantics (e.g. word sense
linked to an ontology and coreference). It com-
prises various genres of text (news, conversa-
tional telephone speech, weblogs, usenet news-
groups, broadcast, talk shows) in English, Chi-
nese, and Arabic languages. ZP sentences are
extracted for ZP resolution task (Chen and Ng,
2013, 2016).
•TVSub.This extracts Chinese–English subtitles
from television episodes. Its source-side sen-
tences are automatically annotated with ZPs by aheuristic algorithm (Wang et al., 2016a), which
was generally used to study dialogue translation
and zero anaphora phenomenon (Wang et al.,
2018a; Tan et al., 2021).
•CTB.This is a part-of-speech tagged and fully
bracketed Chinese language corpus. The text
are extracted from various domains including
newswire, government documents, magazine ar-
ticles, various broadcast news and broadcast con-
versation programs, web newsgroups and we-
blogs. Instances with empty category are ex-
tracted for ZP recovery task (Yang and Xue,
2010; Chung and Gildea, 2010).
•BaiduKnows. The source-side sentences are col-
lected from the Baidu Knows website,which
were annotated with ZP labels with boundary
tags. It is widely-used the task of ZP recov-
ery (Zhang et al., 2019; Song et al., 2020).
4.2 Discussions and Findings
Table 1 lists statistics of existing ZP datasets and
we found the limitations and trends:
1.Language Bias . Most works used Chinese and
Japanese datasets as testbed for training ZP mod-
els (Song et al., 2020; Ri et al., 2021). However,
there were limited data available for other pro-
drop languages (e.g. Portuguese and Spanish), re-
sulting that linguists mainly used them for corpus
analysis (Pereira, 2009; Russo et al., 2012). How-
ever, ZP phenomenon may vary across languages
in terms of word form, occurrence frequency and
category distribution, leading to learning bias on
linguistic knowledge. Thus, it is necessary to es-
tablish ZP datasets for various languages (Prasad,3328
2000; Bacolini, 2017).
2.Domain Bias . Most corpora were established
in one single domain (e.g. news), which may
not contain rich ZP phenomena. Because the
frequencies and types of ZPs vary in different
genres (Yang et al., 2015). Future works need
more multi-domain datasets to better model be-
havior and quality for real-life use.
3.Become An Independent Research Problem .
Early works extracted ZP information from
closed annotations (e.g. OntoNotes and Tree-
banks) (Yang and Xue, 2010; Chung and Gildea,
2010), which were considered as a sub-problem
of coreference or syntactic parsing. With fur-
ther investigation on the problem, MT commu-
nity payed more attention to it by manually
or automatically constructing ZP recovery and
translation datasets (e.g. BaiduKnows and TV-
sub) (Wang et al., 2018a; Zhang et al., 2019).
4.Coping with Data Scarcity . The scarcity of
ZPT data remains a core issue (currently only
2.2M∼0.1K sentences) due to two challenges:
(1) it requires experts for both source ZP anno-
tation and target translation (Wang et al., 2016c,
2018a); (2) annotating the training data manu-
ally spends much time and money. Nonetheless,
it is still necessary to establish testing datasets
for validating/analyzing the model performance.
Besides, pre-trained modes are already equipped
with some capabilities on discourse (Chen et al.,
2019; Koto et al., 2021). This highlights the im-
portance of formulating the downstream task ina manner that can effectively leverage the capa-
bilities of the pre-trained models.
5 Approaches
5.1 Overview
Early researchers have investigated several ap-
proaches for conventional statistical machine trans-
lation (SMT) (Le Nagard and Koehn, 2010; Xiang
et al., 2013; Wang et al., 2016a). Modeling ZPs
for advanced NMT models, however, has received
more attention, resulting in better performance in
this field (Wang et al., 2018a; Tan et al., 2021;
Hwang et al., 2021). Generally prior works fall
into three categories: (1) Pipeline , where input
sentences are labeled with ZPs using an external
ZP recovery system and then fed into a standard
MT model (Chung and Gildea, 2010; Wang et al.,
2016a); (2) Implicit , where ZP phenomenon is
implicitly resolved by modelling document-level
contexts (Yu et al., 2020; Ri et al., 2021); (3) End-
to-End , where ZP prediction and translation are
jointly learned in an end-to-end manner (Wang
et al., 2019; Tan et al., 2021).
Pipeline The pipeline method of ZPT borrows
from that in pronoun translation (Le Nagard and
Koehn, 2010; Pradhan et al., 2012) due to the
strong relevance between the two tasks. Chung and
Gildea (2010) systematically examine the effects
of empty category (EC)on SMT with pattern-,3329CRF- and parsing-based methods. The results show
that this can really improve the translation quality,
even though the automatic prediction of EC is not
highly accurate. Besides, Wang et al. (2016a,b,
2017b) proposed to integrate neural-based ZP re-
covery with SMT systems, showing better perfor-
mance on both ZP recovery and overall translation.
When entering the era of NMT, ZP recovery is also
employed as an external system. Assuming that
no-pro-drop languages can benefit pro-drop ones,
Ohtani et al. (2019) tagged the coreference infor-
mation in the source language, and then encoded it
using a graph-based encoder integrated with NMT
model. Tan et al. (2019) recovered ZP in the source
sentence via a BiLSTM–CRF model (Lample et al.,
2016). Different from the conventional ZP recov-
ery methods, the label is the corresponding transla-
tion of ZP around with special tokens. They then
trained a NMT model on this modified data, let-
ting the model learn the copy behaviors. Tan et al.
(2021) used ZP detector to predict the ZP position
and inserted a special token. Second, they used a
attention-based ZP recovery model to recover the
ZP word on the corresponding ZP position.
End-to-End Due the lack of training data on
ZPT, a couple of studies pay attention to data
augmentation. Sugiyama and Yoshinaga (2019)
employed the back-translation on a context-aware
NMT model to augment the training data. With the
help of context, the pronoun in no-pronoun-drop
language can be translated correctly into pronoun-
drop language. They also build a contrastive dataset
to filter the pseudo data. Besides, Kimura et al.
(2019) investigated the selective standards in detail
to filter the pseudo data. Ri et al. (2021) deleted
the personal pronoun in the sentence to augment
the training data. And they trained a classifier to
keep the sentences that pronouns can be recovered
without any context.
About model architecture, Wang et al. (2018a)
first proposed a reconstruction-based approach to
reconstruct the ZP-annotated source sentence from
the hidden states of either encoder or decoder, or
both. The central idea behind is to guide the cor-
responding hidden states to embed the recalled
source-side ZP information and subsequently to
help the NMT model generate the missing pro-
nouns with these enhanced hidden representations.
Although this model achieved significant improve-
ments, there nonetheless exist two drawbacks: 1)
there is no interaction between the two separatereconstructors, which misses the opportunity to
exploit useful relations between encoder and de-
coder representations; and 2) testing phase needs
an external ZP prediction model and it only has
an accuracy of 66% in F1-score, which propagates
numerous errors to the translation model. Thus,
Wang et al. (2018b) further proposed to improve
the reconstruction-based model by using shared
reconstructor and joint learning. Furthermore, rely-
ing on external ZP models in decoding makes these
approaches unwieldy in practice, due to introduc-
ing more computation cost and complexity.
About learning objective, contrastive learning is
often used to let the output more close to golden
data while far away from negative samples. Yang
et al. (2019b) proposed a contrastive learning to
reduce the word omitted error. To construct the
negative samples, they randomly dropped the word
by considering its frequency or part-of-speech tag.
Hwang et al. (2021) further considered the coref-
erence information to construct the negative sam-
ple. According to the coreference information, they
took place the antecedent in context with empty,
mask or random token to get the negative samples.
Besides, Jwalapuram et al. (2020) served the pro-
noun mistranslated output as the negative samples
while golden sentences as positive sample. To get
the negative samples, they aligned the word be-
tween model outputs and golden references to get
the sentences with mistranslated pronoun.
Implicit Some works consider not just the ZPT
issue but rather focus on the overall discourse prob-
lem. The document-level NMT models (Wang
et al., 2017a; Werlen et al., 2018; Ma et al., 2020;
Lopes et al., 2020) are expected to have strong
capabilities in discourse modelling such as transla-
tion consistency and ZPT. Another method is the
round-trip translation, which is commonly-used in
automatic post-editing (APE) (Freitag et al., 2019),
quality estimation (QE) (Moon et al., 2020) to cor-
rect of detect the translation errors. V oita et al.
(2019) served this idea on context-aware NMT to
correct the discourse error in the output. They
employed the round-trip translation on monolin-
gual data to get the parallel corpus in the target
language. They then used the corpus to train a
model to repair discourse phenomenon in MT out-
put. Wang et al. (2019) proposed a fully unified
ZPT model, which absolutely released the reliance
on external ZP models at decoding time. Besides,
they exploited to jointly learn inter-sentential con-3330
text (Sordoni et al., 2015) to further improve ZP
prediction and translation.
5.2 Discussions and Findings
Table 1 shows that only the TVsub is suitable for
both training and testing in ZPT task, while others
like LATL is too small and only suitable for testing.
To facilitate fair and comprehensive comparisons
of different models across different benchmarkss,
we expanded the BaiduKnows by adding human
translations and included in-house dataset. As
shown in Table 2, we re-implemented three repre-
sentative ZPT methods and conducted experiments
on three benchmarks, which are diverse in terms
of domain, size, annotation type, and task. As the
training data in three benchmarks decrease, the dif-
ficulty of modelling ZPT gradually increases.
1.Existing Methods Can Help ZPT But Not
Enough . Three ZPT models can improve ZP
translation in most cases, although there are still
considerable differences among different domain
of benchmarks (BLEU and APT ↑). Introducing
ZPT methods has little impact on BLEU score
(-0.4∼+0.6 point on average), however, they
can improve APT over baseline by +1.1 ∼+30.1.
When integrating golden ZP labels into baseline
models (ORACLE), their BLEU and APT scores
largely increased by +3.4 and +63.4 points, re-
spectively. The performance gap between Oracle
and others shows that there is still a large space
for further improvement for ZPT.2.Pipeline Methods Are Easier to Integrate with
NMT . This is currently a simple way to enhance
ZPT ability in real-life systems. As shown in Ta-
ble 3, we analyzed the outputs of pipeline method
and identify challenges from three perspectives:
(1)out-of-domain , where it lacks in-domain data
for training robust ZP recovery models. The dis-
tribution of ZP types is quite different between
ZP recovery training data (out-of-domain) and
ZPT testset (in-domain). This leads to that the ZP
recovery model often predicts wrong ZP forms
(possessive adjective vs. subject). (2) error prop-
agation , where the external ZP recovery model
may provide incorrect ZP words to the followed
NMT model. As seen, Z+performs worse
than a plain NMT model Ndue to wrong pro-
nouns predicted by the Zmodel (你们vs.我).
(3)multiple ZPs , where there is a 10% percent-
age of sentences that contain more than two ZPs,
resulting in more challenges to accurately and si-
multaneously predict them. As seen, two ZPs are
incorrectly predicted into “ 我” instead of “ 他”.
3.Data-Level Methods Do Not Change Model
Architecture . This is more friendly to NMT.
Some researchers targeted making better usage
of the limited training data (Tan et al., 2019;
Ohtani et al., 2019; Tan et al., 2021). They
trained an external model on the ZP data to re-
cover the ZP information in the input sequence
of the MT model (Tan et al., 2019; Ohtani et al.,
2019; Tan et al., 2021) or correct the errors in the
translation outputs (V oita et al., 2019). Others
aimed to up-sample the training data for the ZPT
task (Sugiyama and Yoshinaga, 2019; Kimura
et al., 2019; Ri et al., 2021). They preferred to3331
improve the ZPT performance via a data aug-
mentation without modifying the MT architec-
ture (Wang et al., 2016a; Sugiyama and Yoshi-
naga, 2019). Kimura et al. (2019); Ri et al. (2021)
verified that the performance can be further im-
proved by denoising the pseudo data.
4.Multitask and Multi-Lingual Learning . ZPT
is a hard task to be done alone, researchers are
investigating how to leverage other related NLP
tasks to improve ZPT by training models to per-
form multiple tasks simultaneously (Wang et al.,
2018a). Since ZPT is a cross-lingual problem,
researchers are exploring techniques for train-
ing models that can work across multiple lan-
guages, rather than being limited to a single lan-
guage (Aloraini and Poesio, 2020).
6 Evaluation Methods
6.1 Overview
There are three kinds of automatic metrics to eval-
uate performances of related models:
•Accuracy of ZP Recovery : this aims to measure
model performance on detecting and predicting
ZPs of sentences in one pro-drop language. For
instance, the micro F1-score is used to evaluating
Chinese ZPR systems Song et al. (2020).
•General Translation Quality : there are a num-
ber of automatic evaluation metrics for measur-
ing general performance of MT systems (Snover
et al., 2006). BLEU (Papineni et al., 2002) is
the most widely-used one, which measures the
precision of n-grams of the MT output com-
pared to the reference, weighted by a brevity
penalty to punish overly short translations. ME-
TEOR (Banerjee and Lavie, 2005) incorporates
semantic information by calculating either exact
match, stem match, or synonymy match. Fur-
thermore, COMET (Rei et al., 2020) is a neural
framework for training multilingual MT evalua-
tion models which obtains new SOTA levels of
correlation with human judgements.
•Pronoun-Aware Translation Quality : Previous
works usually evaluate ZPT using the BLEU met-
ric (Wang et al., 2016a, 2018a; Yu et al., 2020;
Ri et al., 2021), however, general-purpose met-
rics cannot characterize the performance of ZP
translation. As shown in Table 3, the missed or
incorrect pronouns may not affect BLEU scores
but severely harm true performances. To fix this
gap, some works proposed pronoun-targeted eval-
uation metrics (Werlen and Popescu-Belis, 2017;
Läubli et al., 2018).
6.2 Discussions and Findings
As shown in Table 4, we compare different eval-
uation metrics on ZPT systems. About general-
purpose metrics, we employed BLEU, TER, ME-
TEOR and COMET. About ZP-targeted metrics,
we implemented and adapted APT (Werlen and
Popescu-Belis, 2017) to evaluate ZPs, and ex-
perimented on three Chinese-English benchmarks
(same as Section 5.2). For human evaluation, we
randomly select a hundred groups of samples from
each dataset, each group contains an oracle source
sentence and the hypotheses from six examined
MT systems. We asked expert raters to score all of
these samples in 1 to 5 scores to reflect the cohe-
sion quality of translations (detailed in Appendix3332§A.4). The professional annotators are bilingual
professionals with expertise in both Chinese and
English. They have a deep understanding of the
ZP problem and have been specifically trained to
identify and annotate ZPs accurately. Our main
findings are:
1.General-Purpose Evaluation Are Not Applica-
ble to ZPT . As seen, APT reaches around 0.67
Pearson scores with human judges, while general-
purpose metrics reach 0.47 ∼23. The APT shows
a high correlation with human judges on three
benchmarks, indicating that (1) general-purpose
metrics are not specifically designed to measure
performance on ZPT; (2) researchers need to de-
velop more targeted evaluation metrics that are
better suited to this task.
2.Human Evaluations Are Required as A Com-
plement . Even we use targeted evaluation, some
nuances and complexities remain unrecognized
by automatic methods. Thus, we call upon the re-
search community to employ human evaluation
according to WMT (Kocmi et al., 2022) espe-
cially in chat and literary shared tasks (Farinha
et al., 2022; Wang et al., 2023c).
3.The Risk of Gender Bias . The gender bias refers
to the tendency of MT systems to produce out-
put that reflects societal stereotypes or biases
related to gender (Vanmassenhove et al., 2019).
We found gender errors in ZPT outputs, when
models make errors in identifying the antecedent
of a ZP. This can be caused by the biases present
in the training data, as well as the limitations in
the models and the evaluation metrics. There-
fore, researchers need to pay more attention to
mitigate these biases, such as using diverse data
sets and debiasing techniques, to improve the
accuracy and fairness of ZPT methods.
7 Conclusion and Future Work
ZPT is a challenging and interesting task, which
needs abilities of models on discourse-aware under-
standing and generation. Figure 3 best illustrates
the increase in scientific publications related to ZP
over the past few years. This paper is a literature
review of existing research on zero pronoun trans-
lation, providing insights into the challenges and
opportunities of this area and proposing potential
directions for future research.
As we look to the future, we intend to delve
deeper into the challenges of ZPT. Our plan is to
leverage large language models, which have shown
great potential in dealing with complex tasks, to
tackle this particular challenge (Lu et al., 2023;
Wang et al., 2023b; Lyu et al., 2023). Moreover, we
plan to evaluate our approach on more discourse-
aware tasks. Specifically, we aim to utilize the
GuoFeng Benchmark (Wang et al., 2022, 2023a),
which presents a comprehensive testing ground for
evaluating the performance of models on a variety
of discourse-level translation tasks. By doing so,
we hope to gain more insights into the strengths
and weaknesses of our approach, and continually
refine it to achieve better performance.
Acknowledgement
The authors express their sincere gratitude to all
reviewers whose keen interest and insightful feed-
back have significantly improved the quality of this
paper. Their affirmation and encouragement have
further solidified our commitment to the path of
computational linguistics. This work is part of the
GuoFeng AI (guofeng-ai@googlegroups.com) and
TranSmart (Huang et al., 2021) projects.
Limitations
We list the main limitations of this work as follows:
1.Zero Pronoun in Different Languages : The zero
pronoun phenomenon may vary across languages
in terms of word form, occurrence frequency
and category distribution etc. Due to page lim-
itation, some examples are mainly discussed in
Chinese and/or English. However, most results
and findings can be applied to other pro-drop
languages, which is further supported by other
works (Ri et al., 2021; Aloraini and Poesio, 2020;
Vincent et al., 2022). In Appendix §A.1, we add
details on the phenomenon in various pro-drop3333languages such as Arabic, Swahili, Portuguese,
Hindi, and Japanese.
2.More Details on Datasets and Methods : We have
no space to give more details on datasets and
models. We will use a Github repository to re-
lease all mentioned datasets, code, and models,
which can improve the reproducibility of this
research direction.
Ethics Statement
We take ethical considerations very seriously, and
strictly adhere to the ACL Ethics Policy. In this
paper, we present a survey of the major works on
datasets, approaches and evaluation metrics that
have been undertaken in ZPT. Resources and meth-
ods used in this paper are publicly available and
have been widely adopted by researches of ma-
chine translation. We ensure that the findings and
conclusions of this paper are reported accurately
and objectively.
References333433353336A Appendix
A.1 Zero Pronoun in Different Languages
The pronoun-dropping conditions vary from lan-
guage to language, and can be quite intricate. Pre-
vious works define these typological patterns as
pro-drop that can be subcategorized into three cate-
gories (as shown in Figure 1):
•Topic Pro-drop Language allows referential pro-
nouns to be omitted, or be phonologically null.
Such dropped pronouns can be inferred from pre-
vious discourse, from the context of the conver-
sation, or generally shared knowledge.
•Partial Pro-drop Language allows for the dele-
tion of the subject pronoun. Such missing pro-
noun is not inferred strictly from pragmatics, but
partially indicated by the morphology of the verb.
•Full Pro-drop Language has rich subject agree-
ment morphology where subjects are freely
dropped under the appropriate discourse condi-
tions.
A.2 Analysis of Zero Pronoun
As shown in Table 5, 26% of Chinese pronouns
were dropped in the dialogue domain, while 7%
were dropped in the newswire domain. ZPs in
formal text genres (e.g. newswire) are not as
common as those in informal genres (e.g. dia-
logue), and the most frequently dropped pronouns
in Chinese newswire is the third person singular
它(“it”) (Baran et al., 2012), which may not be
crucial to translation performance.
A.3 The Linguistic Concept
Zero anaphora is the use of an expression whose
interpretation depends specifically upon antecedent
expression. The anaphoric (referring) term is called
an anaphor. Sometimes anaphor may rely on
the postcedent expression, and this phenomenon
is called cataphora. Zero Anaphora (pronoun-
dropping) is a more complex case of anaphora. In
pro-drop languages such as Chinese and Japanese,pronouns can be omitted to make the sentence com-
pact yet comprehensible when the identity of the
pronouns can be inferred from the context. These
omissions may not be problems for our humans
since we can easily recall the missing pronouns
from the context.
A.4 Human Evaluation Guideline
We carefully design an evaluation protocol accord-
ing to error types made by various NMT systems,
which can be grouped into five categories: 1) The
translation can not preserve the original seman-
tics due to misunderstanding the anaphora of ZPs.
Furthermore, the structure of translation is inappro-
priately or grammatically incorrect due to incorrect
ZPs or lack of ZPs; 2) The sentence structure is
correct, but translation can not preserve the original
semantics due to misunderstanding the anaphora
of ZPs; 3) The translation can preserve the original
semantics, but the structure of translation is inap-
propriately generated or grammatically incorrect
due to the lack of ZPs; 4) where a source ZP is in-
correctly translated or not translated, but the transla-
tion can reflect the meaning of the source; 5) where
translation preserves the meaning of the source and
all ZPs are translated. Finally, we average the score
of each target sentence that contains ZPs to be the
final score of our human evaluation. For human
evaluation, we randomly select a hundred groups
of samples from each domain, each group contains
an oracle source sentence and the hypotheses from
six examined MT systems. Following this protocol,
we asked expert raters to score all of these sam-
ples in 1 to 5 scores to reflect the quality of ZP
translations. For the inter-agreement, we simply
define that a large than 3 is a good translation and
a bad translation is less than 3. The annotators
reached an agreement of annotations on 91% (2750
out of 3000) samples. In general, the process of
manual labeling took five professional annotators
one month in total, which cost US $5,000.3337ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section Limitations.
/squareA2. Did you discuss any potential risks of your work?
Section Ethics Statement.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section Abstract and 1.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Not applicable. Left blank.
C/squareDid you run computational experiments?
Section 5.2 and Section 6.2.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
This is a survey and all details are same as related citations.3338/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
This is a survey and all details are same as related citations.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5.2
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 6.2.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix A.4.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Appendix A.4.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Appendix A.4.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.3339