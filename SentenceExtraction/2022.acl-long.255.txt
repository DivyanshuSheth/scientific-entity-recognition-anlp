
Yanzhe Zhang
Georgia Institute of Technology
z_yanzhe@gatech.eduXuezhi Wang
Google
xuezhiw@google.comDiyi Yang
Georgia Institute of Technology
dyang888@gatech.edu
Abstract
Continual learning is essential for real-world
deployment when there is a need to quickly
adapt the model to new tasks without forget-
ting knowledge of old tasks. Existing work
on continual sequence generation either al-
ways reuses existing parameters to learn new
tasks, which is vulnerable to catastrophic for-
getting on dissimilar tasks, or blindly adds
new parameters for every new task, which
could prevent knowledge sharing between sim-
ilar tasks. To get the best of both worlds,
in this work, we propose continual sequence
generation with adaptive compositional mod-
ules to adaptively add modules in transformer
architectures and compose both old and new
modules for new tasks. We also incorporate
pseudo experience replay to facilitate knowl-
edge transfer in those shared modules. Exper-
iment results on various sequences of genera-
tion tasks show that our framework can adap-
tively add modules or reuse modules based on
task similarity, outperforming state-of-the-art
baselines in terms of both performance and pa-
rameter efﬁciency. We make our code pub-
lic at https://github.com/GT-SALT/
Adaptive-Compositional-Modules .
1 Introduction
Current state-of-the-art language generation mod-
els can achieve great performance on a wide range
of sequence generation tasks (Radford et al., 2019;
Lewis et al., 2020) with a static data distribution.
However, real-world scenarios are often changing
which requires the model to learn with dynamic
data distributions. In such cases of data distribu-
tions shift, current generation models often suf-
fer from catastrophic forgetting (Sun et al., 2019):
models completely and abruptly forget previously
learned information upon learning new information.
Continual learning (CL) (Ring, 1998; Thrun, 1998)
has been introduced to improve model’s ability to
learn tasks in a stream by mitigating forgettingFigure 1: Comparison between previous methods (a
and b) and our proposed method (c), from a multi-layer
transformer model perspective. The blue blocks re-
fer to learnable modules and the yellow blocks refer
to frozen pretrained modules . a: retrain the whole
model every time when new tasks arrive. b: insert task-
speciﬁc modules for each task, while keeping the pre-
trained model frozen. c: detect reusable old modules
and add new modules adaptively.
and facilitating knowledge transfer (Lopez-Paz and
Ranzato, 2017), however, continual sequence gen-
eration is relatively under-investigated.
Comparing to continual learning on text classiﬁ-
cation and question answering (Wang et al., 2020;
Holla et al., 2020; Huang et al., 2021), continual
sequence generation is more challenging, since the
output is no longer discrete labels but sequential
text data in different styles/domains. Based on how
to retain old knowledge while learning new tasks,
current continual sequence generation methods can
be categorized into two types. The ﬁrst one con-
tinually learns new tasks on old parameters (Fig
1 a), with approaches like experience replay (Sun
et al., 2019; Chuang et al., 2020) and regulariza-
tion (Mi et al., 2020) to maintain old knowledge.
However, since all tasks share the same parameters,
some degree of interference between tasks is un-
avoidable. Another line of work continually inserts
new task-speciﬁc modules (adapters proposed by
Houlsby et al., 2019) into every transformer layer
for every new task while freezing pretrained mod-3653els and modules used by old tasks (Fig 1 b, Madotto
et al., 2021), which might prevent knowledge trans-
fer between tasks and introduce possible parameter
redundancy. In this work, we aim to get the best of
both worlds: how to encourage the models to reuse
modules from previous tasks as much as possible
and to only add new modules if needed?
To this end, we propose continual sequence gen-
eration with adaptive compositional modules , as
shown in Fig 1 c. Speciﬁcally, we introduce a
two-stage process for every new coming task: a
decision stage and a training stage. During deci-
sion stage, we decide which modules to reuse and
whether we need to add a new module. During
training stage, the model architecture is determined
and ﬁxed. We augment new task’s training process
with pseudo experience replay (Sun et al., 2019) to
further mitigate forgetting and facilitate knowledge
transfer in those shared layers. Our model archi-
tecture is adaptive , as it can automatically add new
modules for dissimilar tasks and reuse modules
for similar tasks, thus making it robust to different
scenarios of continual learning. Furthermore, it is
compositional because for every new task, our new
architecture is composed of reused modules from
old tasks and newly added modules, which allows
knowledge reuse and transfer.
To evaluate the above adaptive compositional
framework, we experiment with four representative
sequence generation tasks following prior work
(Sun et al., 2019; Chuang et al., 2020): natural
language generation, SQL query generation, sum-
marization and task-oriented dialogue arriving in
a stream. Different from prior work that only tests
their methods on very short task sequences or long
task sequences with similar tasks only, we validate
our approach on longer sequences containing di-
verse tasks with different levels of similarity. We
believe this is a suitable scenario to validate both
the model’s ability to mitigate forgetting and its
ability to facilitate knowledge transfer. In summary,
this work makes two key contributions: (1) We
propose continual sequence generation with adap-
tive compositional modules, to maximize knowl-
edge transfer via module-reusing while adaptively
adding new modules to mitigate task-interference
and catastrophic forgetting. (2) Experiments with
longer and more task sequences show that our ap-
proach outperformed baselines with higher param-
eter efﬁciency.2 Related Work
Continual Learning Without allocating new pa-
rameters for new tasks, prior work mainly lever-
ages experience replay (Wang et al., 2019; Sun
et al., 2019) and regularization to mitigate catas-
trophic forgetting. In experience replay, models
are retrained on old examples from previous tasks
while learning new tasks. Those old examples are
usually stored in a ﬁxed size (Mi et al., 2020) or
expanding (Huang et al., 2021) memory buffer. Be-
sides replaying old examples, regularization on the
hidden states (Wang et al., 2019; Han et al., 2020;
Huang et al., 2021) or parameters (Mi et al., 2020)
could be further added to prevent severe distortion.
Another line of work is to create new parameters
for new tasks while freezing parameters used by
old tasks. In computer vision, progressive neu-
ral network (Rusu et al., 2016) continually adds
new branches of parameters for new image classi-
ﬁcation tasks with lateral connections to facilitate
forward knowledge transfer. Dynamically expand-
able network (Yoon et al., 2017) expands neural
networks at neuron level by using regularization to
restrict the number of added neurons. While allo-
cating a big network in advance, PackNet (Mallya
and Lazebnik, 2018) continually assigns a param-
eter subset to each task by network pruning.Li
et al. (2019) employ neural architecture search (Liu
et al., 2018) to optimize on new task’s structure
before learning new tasks. In language domain,
prior work often utilizes adapter (Houlsby et al.,
2019; Madotto et al., 2021; Ermis et al., 2022),
which could be considered as task-speciﬁc MLPs
inserted into frozen transformer layers. However,
since all adapter modules are designed for only one
speciﬁc task, no knowledge transfer is directly al-
lowed in this case. Extra modules like attention
module (Pfeiffer et al., 2021), capsule network (Ke
et al., 2021), and hypernetworks (Jin et al., 2021)
are demonstrated beneﬁcial for knowledge transfer,
but they need to introduce extra parameters and fail
to consider any reusable or compositional modules.
Avoiding privacy concerns, this work also fol-
lows a line of work that doesn’t store real examples
for experience replay, such as generating examples
by GAN (Atkinson et al., 2018), synthesizing ex-
amples (Xu et al., 2022) by model-inversion (Smith
et al., 2021b), and using unlabeled data in the learn-
ing environment (Smith et al., 2021a). In language
domain, LAMOL (Sun et al., 2019) trains the lan-
guage model to solve current tasks and generate3654current training examples simultaneously, then this
model can generate “pseudo” old examples for re-
play before any new tasks. We adopt this pseudo
experience replay along to alleviate the forgetting
in the shared modules of our approach.
Continual Learning for Sequence Generation
Building on an auto-regressive language model,
LAMOL (Sun et al., 2019) makes initial explo-
ration on continual sequence generation. On the
basis of LAMOL, knowledge distillation (Chuang
et al., 2020; Sun et al., 2020) is shown to be ef-
fective via improving knowledge transfer while
changing tasks. ARPER (Mi et al., 2020) combines
regularization on parameters (Kirkpatrick et al.,
2017) with prioritized exemplar replay. Keeping
the pretrained model frozen, Madotto et al. (2021)
added task-speciﬁc modules for each task together
with a perplexity-based classiﬁer, without taking
into account the potential for knowledge transfer
between different tasks. Instead of blindly adding
new modules for new tasks, our approach can de-
tect reusable modules and strategically add new
adapter modules in those layers in which reusing
old modules would lead to severe forgetting. With-
out introducing extra knowledge transfer modules,
our approach enables knowledge transfer via mod-
ule sharing.
Task-speciﬁc Modules Traditional ﬁnetuning
approaches (Peters et al., 2018; Devlin et al., 2019;
Radford et al., 2019) usually modify all the param-
eters in large pretrained modules while learning
downstream tasks. Recently, a line of work has
been proposed to improve the parameter-efﬁciency
of ﬁnetuning by inserting task-speciﬁc modules
into freezing pretrained models. Adapter (Houlsby
et al., 2019) inserts MLP layers into each trans-
former layer. PreﬁxTuning (Li and Liang, 2021)
prepends key-value pairs to each transformer layer
as activations. Prior work also shows that these
task-speciﬁc modules might beneﬁt from a more
adaptive usage. For example, AdapterDrop (Rücklé
et al., 2021) shows that removing adapters from
lower transformer layers can almost maintain the
original performance while reducing computational
overhead. Guo et al. (2021) leveraged latent vari-
ables to decide whether to skip adapter modules
in certain transformer layers to speed up decoding.
However, our approach goes beyond the notion of
“task-speciﬁc”, recomposes reusable modules from
different tasks, and learns compositional architec-tures for new coming tasks.
3 Background
Continual Generation Formulation Assuming
multiple sequence generation tasks fT:::Tgar-
rive in a stream, each task Thas a set of train-
ing examplesfP;P:::;Pg, wherePdenotes a
(input;output )pair in Task i. While learning on
taskT(i >2), we have no access to examples
from previous tasks. The ﬁnal goal is to optimize
the model’s average performance on alltasks after
training on the whole sequence.
Finetuning In order to integrate different se-
quence generation tasks into a single framework,
we use ﬁnetuning as a general strategy. On the
basis of an autoregressive language model, the
core idea is to feed the model input and train the
model to generate the corresponding output sub-
sequently. To distinguish between tasks, we add
an extraquestion following every input to de-
scribe the purpose of each task. For example, the
question for natural language generation tasks is
What is the natural language form? Formally, for
each (input;question;output )triple, the model
is optimized to generate the corresponding output
giveninput andquestion :
L (x) =X logP(xjx)
wherex=fx;:::;xgdenotes the concatenation
ofinput ,question andoutput , andfx;:::;xg
refers toinput andquestion .
Adapter The module used in our framework
refers to adapter (Houlsby et al., 2019), which is a
task-speciﬁc module inserted into each frozen pre-
trained transformer layers (Vaswani et al., 2017).
In addition to residual connection (He et al., 2016)
and layer normalization (Ba et al., 2016), one trans-
former layer contains two primary sub-layers: an at-
tention layer and a feed forward layer. One adapter
module consists of two multi-layer perceptrons
(MLP ), one (MLP) following the multi-head
attention layer and one ( MLP) following the
feed forward layer.
4 Two-Stage Methods
Motivated by prior continual sequence generation
work (Madotto et al., 2021) that uses Adapter
(Houlsby et al., 2019) to insert new adapter module3655into every transformer layer for each new coming
task, we propose to strategically decide whether
we can reuse some adapter modules from old tasks
before training on each new coming task, in a two-
stage manner: decision stage and training stage,
where the former determines the architecture for
new tasks and the later trains the model.
4.1 Decision Stage
The decision stage aims to answer two questions:
do we need to add a new module in this layer? If
not, which old modules should we reuse? Inspired
by interpolation-based data augmentation (Chen
et al., 2020, 2021) and neural architecture search
(Liu et al., 2018), we utilize Hidden State Mixing
for module selection. Assume that there are several
modules as potential candidates to be selected, after
calculating their output separately, we calculate
their weighted average as the overall output, which
is then passed to the next part of the model (See
the left part in Figure 2). After training the entire
model end-to-end, we assume that the module with
the largest learned weight is the most useful one,
and thus will be selected for the reuse.
Formally, assume that we already have inserted
kmodules into the lth transformer layer, each
consisting of two MLPs: (MLP;MLP)...
(MLP;MLP). At the beginning of
decision stage, we add one more module
(MLP;MLP). Given these learnable
weight coefﬁcients [;:::;], multi-head
attention layer output o, the feed forward layer
outputo, we mix the hidden states as follow:
h=XMLP(o)
h=XMLP(o)
where both handhare then fed into
their following Add & Norm layers. To ensureP= 1, we use softmax function to pro-
duce;:::;fromc;:::;c:
=e
Pe;i= 1:::k + 1
Using this mixing approach in every transformer
layer, we optimize our model using L (see Sec
4.2) for the new task and ﬁnd the most suitable mod-
ules for each layer. Note that (i) In this process, thepretrained model and all old modules are frozen,
and only mixing coefﬁcients and newly added mod-
ules will be learned. (ii) Calculating the weighted
average is a convenient approximation of using one
adapter at a time, which is the real setting during
training stage and inference. (iii) Comparing to
other baselines in Figure 1, introduced decision
stage to decide the architecture does introduce ex-
tra computation, while computation of different
MLPs at one position is parallelizable to speed up.
To avoid the learned weight coefﬁcient
;:::;to be too close to a uniform dis-
tribution in certain layers, we further add an addi-
tional regularization term to L, which is the
sum of entropy of every discrete probability distri-
bution [;:::;]:
L =XX log()
whereis a coefﬁcient tuned as a hyper-parameter.
In this stage, a trivial solution could be allocating
a new module in every layer regardless of whether
old modules are reusable. To avoid this trivial so-
lution and reuse shareable modules as much as
possible, we design a prior using the initialization
of the coefﬁcient weights . For every l,c:::c
is initialized to c(c >0), whilecis initial-
ized to c. After softmax , the weight of each old
module isetimes the weight of the new module,
increasing the tendency to reuse old modules.
4.2 Training Stage
We further incorporate pseudo experience replay
(Sun et al., 2019) to mitigate forgetting and facil-
itate knowledge transfer in those shared modules.
The main idea is to teach a generative model to
solve current task and to generate current task’s
examples simultaneously . Then before training
on each new task, we can generate a set of pseudo
old examples and replay them during training.
Thus, in addition to the ﬁnetuning loss to solve
each task, we introduce an extra loss Lfor
the model to generate current task’s examples.
Formally, given the whole sequence of x=
finput;question;output g, we ﬁrst add a special
token [GEN] at the beginning of xto form a new
sequencex, and then optimize the model as fol-
lows:
L(x) =X logP(xjx)3656
Note that we use different special tokens for dif-
ferent tasks, thus we can generate examples for
speciﬁed tasks afterwards. Combining with the
ﬁnetune loss, the overall training loss is:
L =L +L
whereis the weight for the Lloss.
Once our model has the ability to generate
“pseudo“ examples from old tasks, another question
isWhen to generate “pseudo“ examples? Since
those “pseudo“ examples are for shared modules
between old tasks and the current task, we only
generate them while some old modules are reused
for the current task. In that case, we train our model
usingL on the current dataset together with
the generated examples. Otherwise, there is no
need for pseudo experience replay and we just train
our model using L on the current dataset.
5 Experiments
5.1 Datasets
Following Sun et al. (2019) and Chuang et al.
(2020), we evaluate our approach on four represen-
tative sequence generation tasks: natural language
generation, SQL query generation, summarization
and task-oriented dialogue modeling. Speciﬁcally,we test our proposed approach under two common
scenarios: (1) CL on similar tasks : in this case,
the new coming tasks often share the same task
pattern with learned tasks, but are from different
domains. We use E2ENLG (Novikova et al., 2017)
and four different domains (restaurant, hotel, tv,
laptop) from RNNLG (Wen et al., 2015) to form
ﬁvesimilar tasks. Then we use four different or-
ders of these tasks as our testing task sequences.
(2)CL on dissimilar tasks : in this case, the distribu-
tion shift between new tasks and old tasks could be
relatively large, so the major challenge is to retain
old knowledge as much as possible while learning
new tasks. In this case, we further incorporate Wik-
iSQL (SQL query generation, Zhong et al., 2017),
CNN/DailyMail (news article summarization See
et al., 2017), MultiWOZ (semantic state sequence
generation (Budzianowski et al., 2018)) into our
task sequences. We randomly pick four different
orders as our testing task sequences. In total, we
use eight different task sequences (Table 1) to eval-
uate our models. The statistics/metrics for each
dataset and the ﬁnetuing results are in Appendix A.3657Order Task Sequence
1 e2e resthoteltvlaptop
2 laptop tvhotelreste2e
3 rest tve2elaptophotel
4 hotel e2erestlaptoptv
5 woz cnne2eresthotel
6 e2e wikihotelwozrest
7 hotel e2ewozwikicnn
8 cnn hotelwikie2ewoz
5.2 Baselines
We compare our proposed model with the follow-
ing baselines: (i) Finetune (Yogatama et al., 2019):
We ﬁnetuned GPT-2 model on several tasks sequen-
tially. (ii) EWC (Kirkpatrick et al., 2017) added
regularization on parameters according to their im-
portance to old tasks. (iii) LAMOL (Sun et al.,
2019) ﬁnetuned the whole GPT-2 model contin-
ually with the help of pseudo experience replay.
(iv)Adapter+CL (Madotto et al., 2021) inserted
adapter (Houlsby et al., 2019) modules into every
GPT-2’s layer for each task. (v) Adapter+Drop
(Rücklé et al., 2021): We removed all those adapter
modules from the ﬁrst three layers in GPT-2 based
on Adapter+CL. (vi) Adapter+LAMOL : We only
inserted adapter modules into every transformer
layer for the ﬁrst task, then used those adapter
modules to learn the whole the task sequence with
pseudo experience replay. Note that ARPER (Mi
et al., 2020) also tackles the problem of continual
sequence generation, but it needs an extra memory
buffer to store examples from old tasks, which is
not comparable with ours.
Implementation Details We use GPT-2 (Rad-
ford et al., 2019) in HugginceFace Transformers
(Wolf et al., 2020) as our backbone and adapter im-
plementation by AdapterHub (Pfeiffer et al., 2020).
More details can be found in Appendix A.
6 Results and Analysis
To evaluate the overall performance on all tasks,
we use the mean of all tasks’ performance score fol-
lowing Sun et al. (2019); Mi et al. (2020); Madotto
et al. (2021). For each scenario ( similar tasks and
dissimilar tasks), we report the average of mean
scores on all sequences as an overall metric. Be-yond these, we also provide (i) evaluation results
using geometric mean and (ii) ﬁnal performance
of each task in Appendix A. Table 2 summarizes
the ﬁnal performance on all eight task sequences.
We observed that ﬁnetuning sequentially suffered
from very severe forgetting, no matter on similar
ordissimilar tasks, highlighting the importance of
continual learning work. Though EWC can signiﬁ-
cantly increase the performance of ﬁnetuning, its
performance is still far behind LAMOL, highlight-
ing the importance of experience replay.
For sequences containing similar tasks, the
performance of Adapter+CL is inferior to
Adapter+LAMOL even with more learnable pa-
rameters. This indicates that sharing parameters
and experience replay can further facilitate knowl-
edge transfer when tasks are similar. On the
premise of pseudo experience replay, our method
performs better than Adapter+LAMOL, demon-
strating the effectiveness of our adaptive composi-
tional architecture. Our approach also achieves a
much higher parameter efﬁciency than Adapter+CL
and Adapter+Drop. For sequences containing
dissimilar tasks where the transferable knowl-
edge is limited and parameter sharing might cause
degradation, Adapter+CL and Adapter+Drop seem
more robust compared to Adapter+LAMOL and
LAMOL, since they avoid catastrophic forgetting
by parameter isolation. Using a similar number
of parameters to Adapter+Drop, our method out-
performs Adapter+CL consistently on all task se-
quences, conﬁrming that our method can prevent
interference between dissimilar tasks while reduc-
ing parameter redundancy.
6.1 Ablation Studies
We randomly selected task sequence #1 from simi-
lartasks and sequence #8 from sequences of dis-
similar tasks for our ablation studies.
Importance of Each Component To examine
the importance of each component in our method,
we experiment with different settings: not using
entropy loss (w/o Entropy Loss), initializing all
weight coefﬁcients with zero (w/o Weight Ini), and
not replaying pseudo data (w/o Pseudo ER). As
shown in Table 3, we found that (i) After remov-
ing entropy loss, the performance on sequence #1
is almost maintained by using more parameters.
Meanwhile, the performance on sequence #8 drops
signiﬁcantly while using the same number of pa-
rameters. This observation suggests that the en-3658
tropy loss is beneﬁcial to achieve a better trade-off
between adding parameters and maintaining good
performance. (ii) When we initialize all weight co-
efﬁcients with zero, there is no explicit tendency to
reuse old examples. In this case, many redundant
modules are created thus preventing knowledge
transfer, which leads to performance drop on both
sequences. The drop on sequence #1 is more se-
vere due to there is more transferable knowledge
between similar tasks. We therefore conclude that
weight initialization is important to enable knowl-
edge transfer between similar tasks. (iii) Removing
pseudo experience replay leads to the most severe
performance drop on both sequences. Though our
approach strategically detect which modules can be
reused, directly training them on new tasks without
protecting old knowledge will lead to catastrophic
forgetting.
Impact of Task Sequence Length Prior work
in continual learning (Madotto et al., 2021; Huang
et al., 2021) suggests that the differences in se-
quence length could inﬂuence the performance of
continual learning. To this end, we further inves-
tigated the impact of sequence length in Table 4,
where we reported the average performance at ev-
ery step and calculated Backward Transfer follow-
ing Lopez-Paz and Ranzato (2017):
BWT=1
k 1E(R R)
whereRis the performance score on the jth task
after training on the ith task.3659
We found that, on sequence #1,
Adapter+LAMOL and our method consis-
tently outperform Adapter+CL in all stages, which
could be explained by better knowledge transfer
between multiple tasks. Beyond that, our method
outperforms Adapter+LAMOL in most cases,
demonstrating the beneﬁts of adaptively adding
modules. On sequence #8, Adapter+LAMOL
struggles when the length of task sequence
becomes longer. As more and more tasks arrive,
the impact of task dissimilarity and distribution
shift gets larger that pseudo experience replay
cannot cope with. In that case, there is limited
backward transfer but severe forgetting. In contrast,
Adapter+CL and our method demonstrate their
robustness after learning more tasks in a stream.
Our method also outperforms Adapter throughout
the learning process, demonstrating we can enable
knowledge transfer even the similarity between
tasks is limited.
Case Study We selected e2e in sequence #1 and
wiki in sequence #8 as two representative tasks to
illustrate the ﬁnal output generated by different ap-
proaches in Table 5. After training on the whole
sequence, Adapter+LAMOL cannot correctly con-vey the information provided in the input, suffering
from generating grammar mistakes and missing
key points. This could be attributed to the inter-
ference from learning new coming tasks. While
Adapter+CL successfully mitigate this problem
by parameter isolation, our approach works sim-
ilarly using less parameters and generates better
sequences without redundant information.
6.2 The Growth of Compositional Modules
To illustrate the process of adding/reusing modules,
we depict the model architecture at each stage in
Fig 3 using sequence #4, which is the most chal-
lenging sequence containing similar tasks accord-
ing to Table 2. Since the similarity between the
second task (e2e) and the ﬁrst task (hotel) is low
(see Figure 4 in Appendix A), our framework au-
tomatically learns to add extra adapter modules in
layerf6;8;9;10;11gbefore training on the second
task. When the third task (rest) arrives, given its
high similarity to the ﬁrst task, our method cor-
rectly decides to reuse all modules used in the ﬁrst
task. Interestingly, the architecture for the fourth
task is composed of shared modules with the ﬁrst
3 tasks in layerf1;2;3;4;5;7;12g, shared module
with the second task in layer 6, shared the mod-3660E2E NLG (#1): name[Strada], eatType[coffee shop], area[city centre]
Reference There is a coffee shop in the city centre called the Strada.
Adapter+CL Strada serves coffee, is a nice coffee shop, in city centre.
Adapter+LAMOL Strada is a coffee shop serving city centre food
Ours Strada is a coffee shop located in the city centre.
WikiSQL (#8): which team has pick 13 in round 2 ?
Reference select team from table where round = 2 and pick = 13
Adapter+CL select team from table where pick = 13 and round = round 2
Adapter+LAMOL select team from table where round = 2 (missing: and pick = 13)
Ours select team from table where pick = 13 and round = 2
ule with the ﬁrst and the third task in layer 8, and
also added new modules for the fourth task in layer
f9;10;11g. For the ﬁfth task, our method reuses
all modules used by the fourth tasks due to their
high similarity. This demonstrates that our method
is adaptive to different incoming tasks and is able to
compose modules from different old tasks for new
tasks. We also provide a comparison in Appendix B
to demonstrate the effect of reusing modules from
different transformer layers.
7 Conclusion
This work examined continual sequence generation
with adaptive compositional modules, where we
proposed hidden state mixing to adaptively com-
pose old and new modules for new tasks and uti-
lized pseudo experience replay to facilitate knowl-
edge transfer. Experiments conducted on various
sequence generation tasks demonstrated that our
method achieves better performances with higher
parameter efﬁciency over previous state-of-the-art
baselines, both on similar task sequences and dis-
similar task sequences. Our work is also subject
to a few limitations such as the introduced extra
training time. In the future, we plan to investigate
how to further speed up the decision stage more
efﬁciently and generalize the current framework to
more diverse NLP tasks such as text classiﬁcation
and machine translation.
Acknowledgment
We would like to thank the anonymous reviewers
for their helpful comments, and the members of
Georgia Tech SALT group for their feedback. This
work is funded in part by Salesforce and Cisco.References366136623663
A Supplementary Details and Results
Data and Metric Table 6 summaries the datasets
and metrics we used, all datasets are using the
public version from prior work Sun et al. (2019);
Chuang et al. (2020). Note that some big datasets
(WikiSQL, CNN/DailyMail, E2E NLG, RNNLG
(laptop)) are reduced to a smaller size by random
sampling due to data imbalance.
Dataset Metric # Train # Test
E2E NLG
ROUGE6000 2000
RNNLG(rest.) 6228 1039
RNNLG(hotel) 6446 1075
RNNLG(tv) 8442 1407
RNNLG(laptop) 7944 2649
WikiSQL lfEM 6525 15878
CNN/DailyMail ROUGE 6604 2250
MultiWOZ dsEM 2536 1646
Task Sequences In the scenario of CL on dissim-
ilar tasks , each task sequence also contains two
or three similar natural language generation tasks,
so the model cannot cheat by always adding new
modules without detecting reusable ones.
Implementation Details We use GPT-2 (Rad-
ford et al., 2019) in HugginceFace Transformers
(Wolf et al., 2020) as our backbone. We use thearchitecture from Houlsby et al. (2019) in Adapter-
Hub (Pfeiffer et al., 2020) with its default setting,
in which the reduce factor for bottle-neck archi-
tecture is 16. All experiments are conducted on
NVIDIA RTX 2080 Ti with 11GB memory with
a maximum batch size of 4. Training on one task
sequence takes 5to9hours.
We use AdamW (Loshchilov and Hutter, 2019)
as our optimizer. We select learning rate from
f1e 4;1:75e 4;3e 4gand set the learning
ratelr= 1:75e 4for all tasks except WikiSQL,
andlr= 3e 4for WikiSQL. For decision stage,
we train 6epochs to make decisions. For train-
ing stage, we select the best epoch number from
f9;12;15g, and use 9forsimilar scenario and 12
fordissimilar scenario. Weight initialization pa-
rametercis selected fromf0:03;0:05;0:07gfor
similar scenario andf0:12;0:15;0:17gfordissim-
ilarscenario. Loss coefﬁcient is selected from
f0:01;0:05g,is set to 0:25. Following Sun et al.
(2019), we use top- ksampling where k= 20 and
set the pseudo-data sample rate to 0:2. In our pre-
liminary experiments, increasing the replay fre-
quency can further alleviate forgetting. Thus, for
those approaches using pseudo experience replay
in this work, we set half of the training batches as
pseudo-examples whenever learning a new task.
Note that the original design of Adapter+CL
(Madotto et al., 2021) uses perplexity to distin-
guish which task each testing example belongs to.
In this work, we ignore that part and assume that
the task-id of each testing example is given dur-
ing inference for all baselines and our approach to
ensure fair comparison.
Finetuning Results We provide the results of
ﬁnetuning GPT-2 (Radford et al., 2019) and ﬁne-
tuning adapter (Houlsby et al., 2019) on all eight
datasets in Table 7. Since Chuang et al. (2020)
shows that the generation loss Lcould slightly
increase the performance of ﬁnetuning on certain
tasks, we also include the ﬁnetuning results after
addingLloss.
Our results conﬁrm that ﬁnetuning adapter can
almost maintain the performance of ﬁnetuning the
whole model. We also demonstrated that the perfor-
mance of ﬁnetuning adapter could be improved by
simply integrating Lloss. This suggests that the
performance of Adapter+CL could be naively im-
proved by adding Lto training loss. In that case,
the average of mean score for Adapter+CL could
be improved to 64.3 on similar task sequences and366459.6 ondissimilar task sequences, which are still
signiﬁcantly worse than our approach.
Method e2e rest hotel tv laptop
GPT-2y 48.8 64.0 65.4 70.8 73.0
GPT-2y48.8 64.2 65.5 71.0 72.8
Adapter 49.8 64.0 64.9 70.6 71.7
Adapter 49.9 64.3 65.1 70.6 71.8
Method woz cnn wiki
GPT-2y 84.8 25.5 63.1
GPT-2y82.2 25.9 63.7
Adapter 82.8 26.0 63.1
Adapter 83.5 26.0 63.8
Results using Geometric Mean While the mean
of all tasks’ performance score is always used (Sun
et al., 2019; Mi et al., 2020; Madotto et al., 2021)
to represent the overall performance on several
tasks, it could be largely inﬂuenced by the absolute
change of one single number. In this work, we
also leverage geometric mean as an supplementary
metric to measure the overall performance on dif-
ferent tasks, which provides another perspective to
consider relative change during comparison.
Table 8 summarizes the ﬁnal performance using
geometric mean. We observed the same trend as
in Table 2, which demonstrates that our approach
does improve the performance of baselines compre-
hensively on all tasks, not just in favor of absolute
value increments on some tasks.
Ablation Study Table 9 summarizes the full de-
tails of ablation study conducted on sequence #1
and #8.
Detailed Final Performance Table 10 provide
the ﬁnal performance of each task on every se-
quence for our approach and Adapter+LAMOL.
For Adapter+CL, the ﬁnal results are in Table 7.
Task similarity Figure 4 illustrates task similar-
ity between ﬁve natural language generation tasks,
which is calculated by the cosine similarity be-
tween each task’s word frequency distribution.
B Module Comparison
In order to demonstrate the compositional nature of
our method, that is, each module contains different
knowledge required for solving each task, we also
study the performance difference to quantify the
effect of reusing different modules.
Method After training on task A, we specify a
layerk;k= 1;2:::12to add a new module for task
B. Then we train the model on task B together with
pseudo experience replay. After training on task
B, we replace the new module with the old module
from task A in layer k, and compare the perfor-
mance difference on solving task B between the
modiﬁed architecture and the original architecture.
On one hand, if the new added module contains
speciﬁc knowledge of task B, then replacing it will
result in the absence of corresponding feature in
the generate output . On the other hand, if the old
module contains speciﬁc knowledge of task A, then
using it will result in some features of task A being
generated in the output .
Results Here we use laptop for task A and e2e for
task B. We quantify the task knowledge contained
in generated output by calculating the cosine simi-
larity of word frequency distribution between spe-
ciﬁc task’s data and generated output . In Table 11,
we see that replacing the new module in layer 11
results in the most severe information loss of task
B in the modiﬁed architecture, suggesting that the
module in layer 11contains the most important in-
formation of word frequency for task B. In the same
way, we conclude that module in layer 3contains
the least important information of word frequency
for task B. This is consistent with previous ﬁndings
(Jawahar et al., 2019) that bag-of-word information3665
is mainly captured by higher transformer layers,
while lower transformer layers capture surface and
syntactic information.
Similarly, by analyzing the cosine similarity of
word frequency distribution to task A, we ﬁnd that
the old module in layer 9contains the most impor-
tant information of word frequency for task A and
the old module in layer 5contains the least. While
taking a closer look, we also ﬁnd that modules
in different layers contain information of different
high-frequency words in task A. For example, mod-
ule in layer 9;10contains the most information of
the word “computing”, and “laptop”, respectively,
and module in layer 11contains more informa-
tion of the word “business” than any other mod-
ules. This further demonstrates that different task-
speciﬁc knowledge is contained in different mod-
ules from different layers, which results in different
potential for reuse. By selectively reusing old mod-
ules to enable knowledge transfer and adding nec-
essary modules to mitigate knowledge interference,
our method derives a compositional architecture
for every new task, as depicted in Figure 3.3666
LayerTask A Task B
O M O M
1 59.6 72.5 95.1 92.5
2 60.2 72.3 95.0 93.3
3 60.1 71.3 95.1 93.6
4 60.0 70.2 95.1 93.4
5 60.0 68.9 95.2 91.3
6 59.8 72.6 95.1 88.3
7 60.0 71.2 95.0 86.2
8 59.9 72.6 95.0 81.9
9 59.6 76.7 95.0 83.8
10 59.9 74.1 95.2 81.2
11 59.9 74.5 95.0 80.3
12 59.7 75.5 94.9 82.03667