
Nayeon Kim, Jun-Hyung Park, Joon-Young Choi,
Eojin Jeon, Youjin Kang, SangKeun LeeDepartment of Computer Science and EngineeringDepartment of Artificial Intelligence
Korea University, Seoul, Republic of Korea
{lilian1208, irish07, johnjames, skdlcm456, yjkang10, yalphy}@korea.ac.kr
Abstract
We introduce Basic, Tiniest Subword ( BTS )
units for the Korean language, which are in-
spired by the invention principle of Hangeul ,
the Korean writing system. Instead of rely-
ing on 51 Korean consonant and vowel let-
ters, we form the letters from BTS units by
adding strokes or combining them. To exam-
ine the impact of BTS units on Korean lan-
guage processing, we develop a novel BTS -
based word embedding framework that is read-
ily applicable to various models. Our experi-
ments reveal that BTS units significantly im-
prove the performance of Korean word em-
bedding on all intrinsic and extrinsic tasks
in our evaluation. In particular, BTS -based
word embedding outperforms the state-of-the-
art Korean word embedding by 11.8% in word
analogy. We further investigate the unique ad-
vantages provided by BTS units through in-
depth analysis. Our code is available at https:
//github.com/irishev/BTS .
1 Introduction
Distributed word representations, known as word
embeddings, play a fundamental role in natural lan-
guage processing (NLP). With their capability to
formulate semantic and syntactic relationships be-
tween words, word embeddings serve as a useful
feature for various NLP applications, such as ma-
chine translation (Qi et al., 2018), sentence classifi-
cation (Kim, 2014), and named entity recognition
(Lample et al., 2016). Initial approaches to word
embeddings, such as Word2vec (Mikolov et al.,
2013a,b) and GloVe (Pennington et al., 2014), rep-
resent each word in a vocabulary using distinct
vectors. To improve the quality of word representa-
tions, many approaches have considered the inter-
nal structure of words with subword information.
(Kim et al., 2016; Wieting et al., 2016; Bojanowski
et al., 2017; Sasaki et al., 2019; Zhao et al., 2020).For non-English languages, prior studies (Park
et al., 2018; Chen et al., 2020) have demonstrated
that word embeddings should be designed to con-
sider their linguistic structures. In particular, Ko-
rean has a unique and complex linguistic structure
as a morphologically rich and agglutinative lan-
guage (Song, 2006). In the realm of Korean word
embedding, decomposing a word into jamo letters
is believed to be the best feasible option for captur-
ing syntactic and semantic information of Korean
words, which has been empirically supported by
Park et al. (2018).
However, previous approaches have overlooked
the academic background of the Korean alpha-
bet, Hangeul . According to Hunminjeongeum,
Hangeul involves five basic consonants ( ㄱ,ㄴ,ㅁ,
ㅅ,ㅇ) and three basic vowels ( ·,ㅡ,ㅣ). Other
letters are made from the basic letters by adding
strokes or combining together (National Hangeul
Museum, 2018). This unique invention principle
ofHangeul indeed helps to understand the Korean
linguistic structure. Although this principle is well-
recognized by the Korean linguistic community, its
application to Korean NLP is yet to be explored.
In this paper, we firstly bring attention to the
overlooked background of Hangeul for Korean
NLP. We propose novel decomposition units to an-
alyze the Korean linguistic structure called Basic,
Tiniest Subword ( BTS ) units, based on the inven-
tion principle of Hangeul .BTS units involve the
basic consonants and vowels of Hangeul and rep-
resent all jamo letters by themselves or through
expansion. We implement this concept by devel-
oping a simple-yet-effective BTS -based word em-
bedding framework. To verify the effectiveness of
BTS units, we evaluate our BTS -based embed-
dings on the intrinsic tasks of word analogy and7007similarity, and the extrinsic task of sentiment anal-
ysis. The results demonstrate that our embeddings
consistently outperform the state-of-the-art Korean
word embeddings on all evaluation tasks. Through
our in-depth analysis, we verify that BTS units im-
prove both syntactic and semantic information in
Korean words, leading to consistent performance
improvements. We summarize our contributions as
follows:
•We introduce novel basic units for Korean
called BTS units, based on the invention prin-
ciple of Hangeul . To the best of our knowl-
edge, our work is the first to investigate and
examine the deep insights of BTS units in
Korean NLP.
•We develop a simple-yet-effective word em-
bedding framework based on BTS units that
is readily applicable to various NLP models.
•We demonstrate the effectiveness of BTS
units through extensive experiments and ver-
ify that our BTS -based embeddings outper-
form the state-of-the-art Korean word embed-
dings on all intrinsic and extrinsic tasks in our
evaluation.
2 Related Work
2.1 Language-Specific Word Embeddings
Word embeddings have long been studied in the
NLP field, providing useful features for various
tasks. Mikolov et al. (2013a,b) and Pennington et al.
(2014) have proposed to assign continuous repre-
sentations to each word by training word vectors
based on the co-occurrence of words. However,
such word-level approaches are unsuitable for mor-
phologically rich languages that tend to have struc-
turally and semantically similar yet non-identical
words. To handle this problem, many works have
focused on assigning vectors to character-level sub-
words (Kim et al., 2016; Wieting et al., 2016). Fast-
Text (Bojanowski et al., 2017) has proposed to train
vectors corresponding to subword n-grams by us-
ing the summation of the subword vectors as the
word vector.It has recently been demonstrated that linguis-
tic structures of languages must be considered in
word embeddings (Park et al., 2018; Chen et al.,
2020). Since the aforementioned methods were
specifically designed for English and other Latin
script-based languages, there have been studies on
language-specific word embeddings that take the
linguistic structure of non-English languages into
account. For example, previous works on Chinese
and Japanese embeddings have introduced diverse
sub-elements of characters (Sun et al., 2014; Yu
et al., 2017; Cao et al., 2018; Karpinska et al.,
2018).
2.2 Word Embedding for Korean NLP
There have been studies on subword information
in Korean, which is an agglutinative language. Var-
ious methods have been proposed to obtain sub-
words in morpheme units and inter-character struc-
ture units. Stratos (2017) has demonstrated the util-
ity of decomposing Korean into consonants and
vowels by applying the results to sentence parsing.
Choi et al. (2017) have suggested learning word
representations using character-level embeddings.
However, as each Korean character is a combi-
nation of consonants and vowels, it can be decom-
posed into smaller units. Park et al. (2018) have sug-
gested effective subword-level Korean word repre-
sentations based on n-gram extraction from Korean
words decomposed into consonants and vowels. To
better capture the distinct characteristics of the Ko-
rean language, we propose to decompose Korean
words into the basic units of Hangeul .
3 Method - BTS
In this section, we introduce BTS units for Korean
and present the implementation details.
3.1 Linguistic Structure of Korean Words
Korean words are formed by an explicit hierarchi-
cal structure, as shown in Figure 1(a). First, a Ko-
rean word can be decomposed into one or more
characters. Each Korean character corresponds to
a syllable, whereas English syllables are formed
by a sequential combination of characters. Korean
characters in turn can be decomposed into three
jamo letters, i.e., consonants and vowels in Ko-
rean. Specifically, each Korean character strictly
consists of the initial sound ‘chosung’, the middle
sound ‘joongsung’, and the final sound ‘jongsung’.
Chosung and jongsung are consonants, while joong-7008
sung is a vowel. Chosung and joongsung are neces-
sary for forming a character, whereas jongsung can
be empty. A character is written with chosung at
the top, joongsung at the right or below of chosung,
and jongsung (if not empty) at the bottom. See
Appendix A.2 for additional details.
3.2BTS -Level Decomposition
We decompose Korean words into BTS units,
based on the invention principle of Hangeul .BTS
units comprise eight basic units ( ㄱ,ㄴ,ㅁ,ㅅ,
ㅇ,·,ㅡ,ㅣ), each of which is an n-stroke letter
defined as an atomic unit in Hunminjeongeum .
BTS -level consonants
Modern Hangeul consonants consist of five basic
consonants (ㄱ,ㄴ,ㅁ,ㅅ,ㅇ) and nine extended
consonants (ㅋ,ㄷ,ㅌ,ㄹ,ㅂ,ㅍ,ㅈ,ㅊ,ㅎ). The
nine extended consonants are formed by adding
stroke(s), red lines in Figure 1(b), to the five basic
consonants.
Specifically, adding strokes to the basic conso-
nants gives first-level extended consonants ( ㅋ,ㄷ,
ㅂ,ㅈ) and second-level extended consonants ( ㅌ,
ㅍ,ㅊ,ㅎ), which express stronger sounds of basic
consonants (National Hangeul Museum, 2018). We
decompose consonants into the basic consonants
and a symbol for adding a stroke ‘-’, e.g., ‘ ㅋ’ is
decomposed into ‘ㄱ-’.A total of 30 consonant letters are used for
chosung and jongsung, which are written using
14 consonants as in their original form or by com-
bining them. It is noteworthy that the consonant
‘ㄹ’ is an extension of ‘ ㄴ’, but is not a stronger
sound (National Hangeul Museum, 2018). For this
reason, we have an exception in this work - we
deliberately take ‘ㄹ’ as a basic consonant.
BTS -level vowels
Modern Hangeul vowels are formed by the com-
bination of three basic vowels ( ·,ㅡ,ㅣ). By
combining the basic vowels, first-level combined
vowels (ㅗ,ㅏ,ㅜ,ㅓ) and second-level combined
vowels (ㅛ,ㅑ,ㅠ,ㅕ) are created. For example,
‘ㅑ’ can be decomposed into ‘ ㅏ’ and ‘ ·’, where
‘ㅏ’ consists of ‘ㅣ’ and ‘ ·’. As a result, ‘ㅑ’ can
be broken down into ‘ ㅣ· ·’ (National Hangeul
Museum, 2018). Note that the basic vowel ‘ ·’ is
distinct from the aforementioned stroke letter ‘-’.
A total of 21 vowel letters are used as joongsung,
and written using 10 vowels (excluding ‘ ·’ ) as in
their original form or by combining them.
Throughout this paper, we refer to BTS -level de-
composition as the decomposition of Korean words
into basic consonants, strokes, and basic vowels.7009Decomposition Level Subword Sequence
word 친구
character 친,구
jamo (Park et al., 2018) ㅊ,ㅣ,ㄴ,ㄱ,ㅜ
stroke (ours) ㅅ, -, -,ㅣ,ㄴ,ㄱ,ㅜ
cji(ours) ㅊ,ㅣ,ㄴ,ㄱ,ㅡ,·
BTS (ours) ㅅ, -, -,ㅣ,ㄴ,ㄱ,ㅡ,·
We consider both consonant-only BTS decom-
position (denoted as stroke -level) and vowel-only
BTS decomposition (denoted as cji-level, short for
cheonjiin). Examples of the proposed decomposi-
tion methods are presented in Table 1. Appendix B
shows the details of the decomposition of 51 jamo
letters into BTS units.
3.3BTS -Level N-grams Extraction
We can extract n-grams from BTS -level decom-
posed Korean words at five levels: BTS -level,
stroke -level, cji-level, jamo -level, and character-
level. Subword n-grams from these levels are to
be integrated for word representations. Unlike the
jamo -level decomposition presented by Park et al.
(2018), where each character is ensured to consist
of three jamo letters, there is no such fixed length
for characters in BTS -level decomposition. For ex-
ample, characters ‘ 가’ and ‘카’ are decomposed
into tri-gram ‘ㄱㅣ·’ and four-gram ‘ㄱ-ㅣ·’, re-
spectively. For clear distinction, we insert the char-
acter separator ‘ / ’ at the end of each character.
We then add ‘<’ and ‘>’ to indicate the start and
the end of each word. Consider the following BTS -
level decomposed sequence of ‘ 친구’ as an
example:
{<,ㅅ, - , - ,ㅣ,ㄴ, /,ㄱ,ㅡ,·, /, >}.
BTS -level n-grams. We extract BTS -level sub-
word n-grams from the decomposed sequence.
BTS tri-grams for ‘ 친구’ are extracted as fol-
lows:
{<,ㅅ, - }, {ㅅ, - , - }, {- , - ,ㅣ},
{- ,ㅣ,ㄴ}, {ㅣ,ㄴ, /}, {ㄴ, /,ㄱ},
{/,ㄱ,ㅡ}, {ㄱ,ㅡ,·}, {ㅡ,·, /}, {·, /, >}.
Jamo -level n-grams. To treat all jamo letters in-
dependently, we can extract jamo n-grams by re-
composing the decomposed sequence.For example, jamo -level tri-grams for ‘ 친구’
are re-composed as follows:
{<,ㅅ, - , - ,ㅣ}, {ㅅ, - , - ,ㅣ,ㄴ},
{ㅣ,ㄴ, /}, {ㄴ, /,ㄱ}, {/,ㄱ,ㅡ,·},
{ㄱ,ㅡ,·, /}, {ㅡ,·, /, >}.
Character-level n-grams. We extract character-
level n-grams from the decomposed sequence, by
splitting the sequence by the separator ‘ / ’. For ex-
ample, the character-level uni-grams and bi-grams
of ‘친구’ are extracted from the decomposed
sequence as follows:
{ㅅ, - , - ,ㅣ,ㄴ, /}, {ㄱ,ㅡ,·, /},
{ㅅ, - , - ,ㅣ,ㄴ, /,ㄱ,ㅡ,·/}.
3.4BTS -Based Subword Representations
We begin this section with a brief explanation
of the word-level Skip-gram model (Mikolov
et al., 2013a). The goal of the model is to
learn the word vectors corresponding to each
word. Suppose we are given a sequence of words
{w, w, w, ..., w}from the training corpus,
where Tis the length of the sequence. The model is
trained to predict words {w}within
a context, based on the target word w. Here, cde-
notes the size of the context window. Formally, the
objective of the model is to maximize the following
log-likelihood expression:
1
T/summationdisplay/summationdisplaylogp(w|w). (1)
While p(w|w)can be defined as a softmax func-
tion, the computation of precise softmax outputs
is infeasible. Therefore, we adopt negative sam-
pling (Mikolov et al., 2013b), approximating the
log probability to the following binary logistic loss
function:
log/parenleftbig
1 +e/parenrightbig
+/summationdisplaylog/parenleftbig
1 +e/parenrightbig
,
(2)
where Nis the set of all negative examples, and
sis a scoring function that assigns a real number to
each word pair. In the Skip-gram model, each word
wis represented by a unique vector. Thus, the value
s(w, w)of function sis computed as the dot
product between the input vector corresponding to
wand the output vector corresponding to w.
In the subword information Skip-gram model
(Bojanowski et al., 2017), we compute the vector7010representation of a word as an average of the vec-
tor representations of its n-grams. Therefore, the
scoring function sis defined as:
s(w, w) =1
|G|/summationdisplayzv, (3)
where Gdenotes the set of n-grams extracted from
the word w.zis the vector representation cor-
responding to each n-gram g∈G.vis the
output vector corresponding to the word w.
We extract character-level, jamo -level, and one
ofstroke -,cji-, orBTS -level n-grams from a Ko-
rean word as described in Section 3.3. The word
representation is then computed as the average of
vectors corresponding to each n-gram. As different
levels of n-grams are utilized based on the model
setting, the scoring function sis defined as follows:
s(w, w) =1
|U|/summationdisplayzv. (4)
Here, Udenotes the union set of n-grams ex-
tracted from diverse levels of subwords, based on
the model settings. If we train a SISG model with
character- and BTS -level n-grams, an embedding
of a word is calculated by averaging the vectors
of the whole word, character-level n-grams, and
BTS -level n-grams.
4 Experiments
In this section, we demonstrate the effectiveness
ofBTS units through the evaluation of intrinsic
and extrinsic tasks. See Appendix C for details on
datasets in these experiments.
4.1 Corpus
We collect a corpus of Korean documents from
three sources to enable the model to learn rich rep-
resentations in diverse domains. The sources are:
1) 2020 newspaper corpus provided by the MODU
corpus, 2) Korean Wikipedia, and 3) 21st century
Sejong corpus (Kim et al., 2007). The training cor-
pus is constructed by randomly mixing sentences
from all three sources, and the resulting corpus con-
tains 19M sentences with 1.13M unique vocabulary
words.4.2 Experimental Settings
Models. We experiment with our proposed
stroke -level, cji-level, and BTS -level Subword In-
formation Skip-Gram models (i.e., SISG( stroke ),
SISG( cji), SISG( BTS ), respectively) and compare
their performances to those of the baseline models.
Additionally, we experiment with models trained
on multiple levels of subword n-grams. Specifically,
these models are trained on integration of one of
jamo - or character-level subword n-grams, and one
ofstroke -,cji-, orBTS -level n-grams. For instance,
SISG(jm+ BTS ) is a model trained on integrated
subword n-grams of jamo - andBTS -level.
Baseline. We use the word-level Skip-Gram
model (SG) (Mikolov et al., 2013a,b), character-
andjamo -level Subword Information Skip-Gram
models (SISG(ch) and SISG(jm), respectively) (Bo-
janowski et al., 2017; Park et al., 2018) as our base-
lines. We include models trained on both character-
andjamo -level subword n-grams to baseline mod-
els, i.e., SISG(ch4+jm) and SISG(ch6+jm) from
Park et al. (2018). Due to the difference in the
training corpus, we re-implement and evaluate the
baselines based on the code released by Park et al.
(2018). Note that we have fixed some bugs (e.g.,
composing Korean characters and extracting n-
grams) in the released code.
N-gram range. For our baseline models, we fol-
low character- and jamo -level n-grams settings of
Park et al. (2018). For our proposed models, we
search the best-performing n-gram range settings
from 2 or 3 through 20 for each task. We provide
specific results for each hyperparameter setting in
Appendix D.
Hyperparameter settings. We train all of our
models and baselines on 300-dimensional embed-
dings, 5 epochs, a window size of 5, a learning
rate of 0.025, and 5 negative samples. We filter
out words that appear less than 10 times in the cor-
pus and apply subsampling (Mikolov et al., 2013b)
with a threshold of t= 10. Additionally, a hash
function from Bojanowski et al. (2017) is used to
map all n-grams to integers ranging from 1 to 10.
4.3 Evaluation Tasks
We evaluate SISG( BTS ), its variants, and baselines
on two intrinsic tasks of word analogy and word
similarity, and one extrinsic task of sentiment anal-
ysis. We report the average scores of five runs with
different random seeds in these experiments.7011
Word Analogy
We evaluate the semantic and syntactic features of
word vectors on the Korean word analogy dataset
(Park et al., 2018), in which questions are in the
form of Ais toBasCis toD, where Dis the
target word to predict. Consistent with Park et al.
(2018), we employ the 3COSADD method and
measure the distance between prediction and target
vectors. We report the results for each category, the
average of semantic and syntactic categories, and
the overall average. We empirically observe that
overfitting occurs in syntactic feature evaluation
results. These results exhibit different trends com-
pared to the other experimental results. Therefore,
we report word analogy results for the model that
performs best on semantic tasks, i.e., the model
with the lowest average distance on 5 semantic
tasks.
Word Similarity
We evaluate the trained word vectors on how well
they formulate the relationship between words on
the Korean version of the WS-353 dataset (Finkel-
stein et al., 2002). Each word similarity question
comprises a pair of words and human-annotated
similarity labels ranging from 0 to 10 (e.g., 호랑
이,고양이= 7.17). We report Spearman’s
correlation coefficient between human-annotated
labels and the cosine similarities of word represen-
tations. A higher correlation coefficient indicates
higher consistency between word similarity and
human judgments.
Sentiment Analysis
To highlight the efficacy of our trained word vec-
tors on extrinsic tasks, we evaluate them on Naver
Sentiment Movie Corpus (NSMC). We report
the accuracy, precision, recall, and F1 score values
of trained models. We train a single-layer LSTM
model with 300 hidden dimensions and 0.5 dropout
rates topped with a sigmoid activation function on
the final LSTM state. We train 10 epochs of the
dataset and use 0.5 dropout probability, 32 batch
size, a cross-entropy error, an Adam optimizer with
β= 0.9, β= 0.999, and 0.001 learning rate.
The embedding layer contains representations for
words in the train set and is frozen during training.7012
4.4 Results
Word analogy
The results for word analogy are shown in Table 2.
The upper half of Table 2 shows that SISG( BTS )
outperforms all baselines in whole categories. This
demonstrates the significance of BTS units in
capturing information from Korean words. While
SISG( stroke ) and SISG( cji) models also outper-
form all baselines, SISG( BTS ) yields the great-
est performance improvements in most categories.
This reveals that both BTS -level consonants and
BTS -level vowels advance performance and com-
plement each other.
Further experiments are conducted to examine
the power of BTS units when combined with other-
level n-grams. As shown in the lower half of Ta-
ble 2, we observe that the performance of all our
combined models is worse than or comparable to
that of SISG( BTS ). This indicates that BTS units
effectively capture the syntactic and semantic infor-
mation for word analogy, and cover the information
captured by character- and jamo -level n-grams.
For the baselines, we observe that our exper-
imental results are not necessarily consistent
with those of prior work (Park et al., 2018). For
example, contrast to the prior work, we observe
that SISG(ch) outperforms SG on semantic
categories. We suspect that this is due to the dif-
ferent training corpus and bugs in the original code.
Word similarity
The results for word similarity are shown in Ta-
ble 3. Consistent with the word analogy results,
SISG( stroke ), SISG( cji), and SISG( BTS ) show
performance improvements over the baselines.
Specifically, SISG( cji) and SISG( BTS ) outperform
the previous state-of-the-art Korean embedding
(SISG(ch4+jm)) by 3%. In addition, we observe
that SISG(ch6+ cji) achieves the best result, which
shows that adding a wide range of character n-
grams provides an advantage in capturing the infor-
mation about word similarity.
Sentiment analysis
The results of sentiment analysis are reported in
Table 4. SISG( BTS ) consistently outperforms the
baseline models, and adding character-level sub-
words improves the accuracy and F1 scores. These
results demonstrate the effectiveness of BTS -level
decomposition for the extrinsic task in Korean.
5 Analysis
5.1 N-gram Range Analysis
We analyze how the performance of SISG( BTS )
depends on the subword n-gram range on word
analogy task.
First, the overall average performance of
SISG(BTS ) on word analogy is maximized when
we use n-gram ranges of 2-11 and 2-15. These
are longer n-gram ranges compared with 3-6 of
SISG(jm), which are explainable in that decompos-
ing into BTS units generates longer sequences than
those of SISG(jm).7013
In addition, for SISG( BTS ), word vectors includ-
ingBTS bi-grams generally perform better than
those excluding BTS bi-grams. According to Fig-
ure 2,BTS bi-grams improve performance on all
evaluation criteria. As every extended consonant
and vowel letter can be decomposed into at least
twoBTS units, we speculate that including BTS
bi-grams provides better information for each jamo
letter in word analogy.
Another observation is that the semantic and
syntactic analogy performances of models show an
opposite trend. As we include longer BTS n-grams
in word vectors, the semantic analogy performance
tends to increase, while the syntactic analogy per-
formance tends to decrease. This is clear from Fig-
ure 2, which shows that the best performing n-gram
ranges for semantic and syntactic analogy are 2-15
and 2-5, respectively.
5.2 Nearest Neighbor Words Analysis
While we have verified that SISG( BTS ) effectively
enriches word vectors with syntactic and semantic
information, we further observe interesting results
that show unique advantages provided by BTS
units. We conduct qualitative analysis on the top-
3 nearest neighbors for Korean words containing
common typos. We report the results in Table 5
with human judgment on whether query words and
nearest neighbor words are semantically equivalent.
As seen in a few examples in Table 5, BTS
units show a strong capability to handle typos andout-of-vocabulary (OOV) words. We observe that
SISG(BTS ) shows better capability in identifying
the meanings of words containing typos, compared
to the baselines.
First, given ‘ 케잌’, of which the intended mean-
ing is ‘cake’, SISG( BTS ) correctly finds the stan-
dard word ‘ 케이크’ with other words, such as
‘케익’ semantically equivalent but containing dif-
ferent typos and ‘ 케익을 ’ formed by appending
objective case ‘ 을’ to ‘케익’. In contrast, SG and
SISG(ch) models find words related to cake such
as ‘버터크림 ’ or ‘프랄린 ’, but fail
to find the semantically equivalent words.
In addition, Koreans occasionally make inten-
tional typos to emphasize the meaning, as can be
seen in ‘찐한’ coined by replacing ‘ ㅈ’ with ‘ㅉ’.
SISG(BTS ) finds the standard word ‘ 진한 ’ in
this case.
We observe cases where SISG( BTS ) tends to
over-prioritize overlapping letters like other SISG
models do. For example, all nearest neighbor words
of ‘웬지’ in SISG( BTS ) include the character ‘ 웬’,
while the target word ‘ 왠지 ’ does not
contain it. Mitigating such a problem would be the
future work for word embeddings with subword
information.
Finally, in the case of OOV words, given an
OOV word ‘ 뒤치닥거리’, SISG( BTS ) effectively
finds the standard word ‘ 뒤치다꺼리 ’ with
intended meanings, while other models fail to do
so. This result suggests a promising direction for
further exploration of BTS . We leave this for future
works.70146 Conclusion
Until now, among the Korean NLP community, var-
ious efforts have been made to reflect the unique
writing system of Korean to NLP systems, but they
have overlooked how Hangeul was created. In this
paper, we have developed an unprecedented ap-
proach to Korean NLP inspired by the invention
principle of Hangeul , leading to novel BTS units
for analyzing the Korean linguistic structure. In-
stead of relying on the conventional approach of
using Korean consonant and vowel letters, we have
formed letters from BTS units by adding strokes
or combining them together. We have shown that
ourBTS units effectively improve the quality of
Korean word embeddings, which is a fundamental
and versatile component of NLP models. We have
demonstrated the efficacy of BTS units for both
intrinsic and extrinsic tasks by outperforming ex-
isting Korean word embeddings methods on whole
tasks in our evaluation. We hope that our BTS will
light the way up like dynamitefor future research
on Korean NLP.
Limitations
While we have shown that our BTS units facili-
tate performance improvements for Korean NLP
tasks, there are some limitations that present av-
enues for future research. First, the efficacy of BTS
units on Transformer-based Korean language mod-
els remains unexplored. We believe that the two-
dimensional linguistic structure of Korean neces-
sitates the investigation or even re-designing of a
vanilla Transformer architecture, which takes an
input sequence in a one-dimensional and mono-
layer form, as well as tokenization units. Future
work on both topics would be of ultimate signifi-
cance for Korean NLP, triggering new directions
in various areas including Korean natural language
understanding (Park et al., 2021), sub-character
and morpheme-aware tokenization (Mielke et al.,
2021), and multilingual NLP.
Additionally, we expect that our BTS units help
improve the quality of representations for address-
ing typos and OOV words, as observed in Table 5,
although we have not fully examined the efficacy
ofBTS units for typos and OOV words. Future
work in this direction is expected to be another
promising avenue for Korean NLP.Acknowledgement
We thank the anonymous reviewers for their helpful
comments. This work was supported by the Basic
Research Program through the National Research
Foundation of Korea (NRF) grant funded by the
Korea government (MSIT) (2020R1A4A1018309),
National Research Foundation of Korea (NRF)
grant funded by the Korea government (MSIT)
(2021R1A2C3010430) and Institute of Informa-
tion communications Technology Planning Evalua-
tion (IITP) grant funded by the Korea government
(MSIT) (No. 2019-0-00079, Artificial Intelligence
Graduate School Program (Korea University)).
References70157016A Invention principle of Hangeul
We explain the invention principle of Hangeul , the
Korean alphabet, based on the literature distributed
by National Hangeul Museum (2018).
A.1 Historical Backgrounds
Until the Middle Ages, there was no official writing
system for Korean, so it was written in Classical
Chinese. To remedy the lack of a writing system
and designated alphabets for writing Korean, King
Sejong the Great, the 4th monarch of the Joseon
dynasty (1397-1450), invented Hangeul . The inven-
tion was finished in 1443, and Hunminjeongeum ,
the book with detailed explanations of Hangeul ,
was published in 1446. Unlike other writing sys-
tems whose invention principle and period are un-
known, those of Hangeul are revealed by Hunmin-
jeongeum . In addition, Hunminjeongeum refers to
the original name of Hangeul .
A.2 Korean Writing System
In Korean, consonants and vowels that make up
a syllable are combined and written as syllable
blocks, whereas in English, letters make up words
through the sequential combination of characters.
In this study, we consider syllable blocks as being
equivalent to characters.
Each Korean character is made up of at least one
consonant and vowel. When we construct a sylla-
ble block, it generally follows the order of CV C
(Lee, 2006), where C, Cand V are short for con-
sonant and vowel, respectively. In the CV Cstruc-
ture, each component refers to special terms: 1) C
to the initial sound, ‘chosung’, 2) V to the middle
sound, ‘joongsung’, and 3) Cto the final sound,
‘jongsung’. Joongsung is placed at the right of or
below chosung, and jongsung (if used) is placed
below chosung and joongsung. Note that jongsung
is not mandatory to form a Korean character. See
Figure A.3.
A.3 Consonants and Vowels of Hangeul
In Korean, we refer to consonants as ‘ jaeum ’,
and vowels as ‘ moeum ’. Consonants and vowels,
grouped together, are called jamo s. We describe
how Hangeul jamo s are formed based on the inven-
tion principle of Hangeul .
Consonant extension via stroke addition. Con-
sonants in modern Korean consist of five basic con-
sonants (ㄱ,ㄴ,ㅁ,ㅅ,ㅇ) and nine extended con-
sonants (ㅋ,ㄷ,ㅌ,ㄹ,ㅂ,ㅍ,ㅈ,ㅊ,ㅎ). The ba-
sic consonants were formed by imitating the shape
of vocal organs (See Figure A.5(a)), and the ex-
tended consonants were formed by adding strokes
to the basic consonants. For example, ‘ ㅋ’ is a
single stroke extension of ‘ ㄱ’ that expresses the
aspirated sound of ‘ ㄱ’. Figure 1 (b) depicts how
other consonants were extended from basic conso-
nants. In addition, double consonants are formed by
combining the same consonants (e.g., ㅈ+ㅈ→
ㅉ), and consonant clusters are created by com-
bining different consonants (e.g., ㄴ+ㅎ→ㄶ).
Double consonants and consonant clusters are used
for constructing characters.
Vowel creation via combination. Hangeul vow-
els are formed through the combination of three
basic vowels ( ·,ㅡ,ㅣ). Each of the basic vowels
represents the shape of the round, heaven ‘. [ 2]’, flat
earth ‘ㅡ[W]’, and standing human ‘ ㅣ[i]’, and
are pronounced as ‘cheon( 天 )’, ‘ji(地)’,
and ‘in(人 )’, respectively in Sino-Korean pro-
nunciation. See Figure A.5.
More vowels are created by combining the basic
vowels. ‘ ·’ is placed either above or below the
letter ‘ㅡ’ and on the either left or right side of
‘ㅣ’, resulting in four vowels ‘ ㅗ,ㅏ,ㅜ,ㅓ’. The
further addition of ‘ ·’ creates four more vowels
‘ㅛ,ㅑ,ㅠ,ㅕ’ (National Hangeul Museum, 2018).
For example, ‘ㅑ’ is created by combining ‘ ㅏ’ and
‘·’, while ‘ㅏ’ consists of ‘ㅣ’ and ‘ ·’. Therefore,
‘ㅑ’ can be broken down into ‘ ㅣ· ·’.
Additionally, compound vowels can be formed7017
by combining multiple vowels. For example, a com-
pound vowel ‘ㅘ[wa]’ is formed by combining ‘ ㅗ’
and ‘ㅏ’ where ‘ㅗ’ is further decomposed into ‘ ·’
and ‘ㅡ’, and ‘ㅏ’ is further broken down into ‘ ㅣ’
and ‘·’. For this reason, we can decompose ‘ ㅘ’
into ‘·ㅡㅣ·’.
Consonant and vowel letters. There are 51 jamo
letters used in Korean characters, including 30 con-
sonant letters and 21 vowel letters. 30 consonantletters consist of 5 basic consonants, 9 extended
consonants (via stroke addition), 5 double conso-
nants, and 11 consonant clusters. Double conso-
nants and consonant clusters result from further
combinations of basic/extended consonants. All
of the basic/extended consonants and double con-
sonants can be used for chosung, resulting in 19
possible letters. For jongsung, a total of 27 con-
sonants can be used, including all basic/extended
consonants, two double consonants (‘ ㄲ’ and ‘ㅆ’),
and all consonant clusters. 21 vowel letters consist
of 10 vowels (created via basic vowel combina-
tion) and 11 compound vowels, all of which can be
used as joongsung. Specific letters are presented in
Figure A.4.7018B Full Comparison of jamo -level and
BTS -level Decomposition
We provide a full comparison of how each Korean
jamo letter is decomposed by jamo - andBTS -level
subword.7019C Dataset Details
C.1 Corpus
We provide detailed information on three sources
comprising the training corpus.
2020 Newspaper Corpus. The 2020 newspaper
corpus contains approximately 0.7M articles ex-
tracted from 35 media sources, covering eight do-
mains (political, economic, social, life, IT/science,
entertainment, sports culture, and beauty/health).
The corpus contains approximately 133M words
and 9M sentences.
Korean Wikipedia. The Korean Wikipedia cor-
pus covers 0.4M documents, containing 69M words
and 7M sentences.
21st Century Sejong Corpus. The 21st century
Sejong corpus was developed from 1998 to 2007
under the national research project "21st Century
Sejong Project". It is composed of written modern
Korean corpus, spoken-transcript modern Korean
corpus, and parallel corpus (e.g., Korean-English,
Korean-Japanese) collected from newspapers, mag-
azines, books, and monologue/dialogue scripts. We
use a filtered version of the corpus that contains
31M words and 3M sentences.
C.2 Word Analogy
The Korean word analogy evaluation dataset (Park
et al., 2018) consists of 10 categories, five for se-
mantic feature evaluation and five for syntactic fea-
ture evaluation. Each category contains 1K items,
and each item consists of four words that evaluate
the semantic and syntactic features of learned word
representations. We provide details regarding the
word analogy dataset released by Park et al. (2018)
by presenting an example for each category.
Semantic :
•Capital-Country (Capt): Pairs of a country
name and its capital city
•Male-Female (Gend): Pairs of corresponding
male-female words
•Name-Nationality (Name): Pairs of a celebrity
name and nationality•Country-Language (Lang): Pairs of a country
name and official language :
•Miscellaneous (Misc): Various semantic fea-
tures not included in the above four categories
Syntactic :
•Case: Pairs of a noun and the noun with case
marker attached
•Tense: Pairs of the present tense and past tense
of a verb
•V oice: Pairs of active voice and passive voice
of a verb
•Verb ending form (Verb): Pairs of a verb and
the verb with the ending form attached
•Honorific (Honr): Pairs of morphological vari-
ation for verbs in Korean.
C.3 Word Similarity
The Korean version of the WS-353 dataset (Finkel-
stein et al., 2002) was released by Park et al. (2018).
It consists of 353 questions annotated by 14 Ko-
rean native speakers, asking about the similarity
between pairs of words.
C.4 Sentiment Analysis
Naver Sentiment Movie Corpus (NSMC) dataset
consists of 150K training and 50K test samples,
scraped from the Naver Movies website. Each sam-
ple contains a movie review sequence in Korean
and a binary score (0 for negative, 1 for positive).
We sample 100K train, 25K validation, and 25K
test samples from the dataset, balancing the ratio
of labels among each set. Since the original source
of the dataset is inevitably noisy, we preprocess the
dataset so that only Korean, English, and numbers
remain in the dataset.7020D Word Analogy Results by N-gram
Range
We report the experimental results of word analogy
by n-gram range.
D.1 Single Decomposition Models7021D.2 Jamo -Integrated Models7022D.3 Character(1-4) - Integrated Models7023D.4 Character(1-6) - Integrated Models
For SISG(ch6+stroke), the best performing model
reported in Table 2 was experimented on n-gram
range of 2-10, so the results may vary with Figure
D.10.7024