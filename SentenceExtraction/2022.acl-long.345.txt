
Chun Sik Chan, Huanqi Kong, Guanqing Liang
Wisers AI Lab, Wisers Information Limited
{tonychan, katekong, quincyliang}@wisers.com
Abstract
Interpretation methods to reveal the internal
reasoning processes behind machine learning
models have attracted increasing attention in
recent years. To quantify the extent to which
the identified interpretations truly reflect the
intrinsic decision-making mechanisms, various
faithfulness evaluation metrics have been pro-
posed. However, we find that different faithful-
ness metrics show conflicting preferences when
comparing different interpretations. Motivated
by this observation, we aim to conduct a com-
prehensive and comparative study of the widely
adopted faithfulness metrics. In particular, we
introduce two assessment dimensions, namely
diagnosticity andtime complexity . Diagnostic-
ity refers to the degree to which the faithfulness
metric favours relatively faithful interpretations
over randomly generated ones, and time com-
plexity is measured by the average number of
model forward passes. According to the ex-
perimental results, we find that sufficiency and
comprehensiveness metrics have higher diag-
nosticity and lower time complexity than the
other faithfulness metrics.
1 Introduction
NLP has made tremendous progress in recent years.
However, the increasing complexity of the models
makes their behaviour difficult to interpret. To
disclose the rationale behind the models, various
interpretation methods have been proposed.
Interpretation methods can be broadly classified
into two categories: model-based methods and post-
hoc methods. Model-based approaches refer to
designing simple and white-box machine learning
models whose internal decision logic can be easily
interpreted, such as linear regression models, deci-
sion trees, etc. A post-hoc method is applied after
model training and aims to disclose the relation-
ship between feature values and predictions. As
pretrained language models (Devlin et al., 2019a;
Liu et al., 2019; Brown et al., 2020) become morepopular, deep learning models are becoming more
and more complex. Therefore, post-hoc methods
are the only option for model interpretations. Post-
hoc interpretation methods can be divided into
two categories: gradient-based (Simonyan et al.,
2014; Sundararajan et al., 2017; Shrikumar et al.,
2019) and perturbation-based (Robnik-Šikonja and
Kononenko, 2008; Zeiler and Fergus, 2013; Ribeiro
et al., 2016). Gradient-based methods assume the
model is differentiable and attempt to interpret the
model outputs through the gradient information.
Perturbation-based methods interpret model out-
puts by perturbing the input data.
To verify whether, and to what extent, the inter-
pretations reflect the intrinsic reasoning process,
various faithfulness metrics have been proposed.
Most faithfulness metrics use a removal-based cri-
terion, i.e., removing or retaining only the impor-
tant tokens identified by the interpretation and ob-
serving the changes in model outputs (Serrano and
Smith, 2019; Chrysostomou and Aletras, 2021; Ar-
ras et al., 2017; DeYoung et al., 2020).
However, we observe that the existing faithful-
ness metrics are not always consistent with each
other and even lead to contradictory conclusions.
As shown in the example from our experiments
(Table 1), the conclusions that are drawn by two
different faithfulness metrics, Sufficiency (SUFF)
and Decision Flip - Fraction of Tokens (DFFOT),
conflict with each other. More specifically, DFFOT
concludes that the interpretation by LIME method
is the best among the four interpretations, while
SUFF ranks it as the worst. In this case, which
faithfulness metric(s) should we adopt to compare
interpretations?
Motivated by the above observation, we aim to
conduct a comprehensive and comparative study of
faithfulness metrics. We argue that a good faithful-
ness metric should be able to effectively and effi-
ciently distinguish between faithful and unfaithful
interpretations. To quantitatively assess this capa-5029
bility, we introduce two dimensions, diagnosticity
andtime complexity .
Diagnosticity refers to the extent to which a faith-
fulness metric prefers faithful rather than unfaithful
interpretations. However, due to the opaque nature
of deep learning models, it is not easy to obtain
the ground truth for faithful interpretation (Jacovi
and Goldberg, 2020). To concretize this issue, we
use random interpretations, i.e., randomly assign-
ing importance scores to tokens regardless of the
internal processes of the model, as the relatively
unfaithful interpretations. In contrast, we treat in-
terpretations generated by interpretation methods
as relatively faithful interpretations. In this way, we
constructed the hypothesis that a faithfulness met-
ric is diagnostic only if it can clearly distinguish
between random interpretations and interpretations
generated from interpretation methods. In addi-
tion, we introduce time complexity to estimate the
computational speed of each metric, by using the
average number of model forward passes.
In this paper, we evaluate six commonly adopted
faithfulness metrics. We find that the sufficiency
and comprehensiveness metrics outperform the
other faithfulness metrics, which are more diagnos-
tic and less complex. Secondly, the two correlation-
based metrics, namely Correlation between Impor-
tance and Output Probability andMonotonicity ,
have a promising diagnosticity but fail in terms of
the high time complexity. Last but not least, deci-
sion flip metrics, such as Fraction of Tokens and
Most Informative Token, perform the worst in the
assessments.
The main contributions of this paper are as follows:
•We conduct a comparative study of six widely
used faithfulness metrics and identify the in-
consistencies issues.
•We propose a quantitative approach to assess
faithfulness metrics through two perspectives,namely diagnosticity and time complexity.
2 Terminology and Notations
We first introduce the prerequisite terminology and
notations for our discussions.
Terminology A “classification instance” is the
input and output values of a classification model,
which we apply interpretation methods on. An
“interpretation” of a classification instance is a se-
quence of scores where each score quantifies the
importance of the input token at the corresponding
position. An “interpretation pair” is a pair of inter-
pretations of the same classification instance. An
“interpretation method” is a function that generates
an interpretation from a classification instance with
the associated classification model.
Notations Letxbe the input tokens. Denote the
number of tokens of xasl. Denote the predicted
class of xasc(x), and the predicted probability
corresponding to class jasp(x).
Assume an interpretation is given. Denote the
k-th important token as x. Denote the input se-
quence containing only the top k(or top q%) impor-
tant tokens as x(orx). Denote the modified
input sequence from which a token sub-sequence
xare removed as x\x.
Let(x, y)be a classification instance associated
with classification model m, andgbe an interpreta-
tion method. Denote the interpretation of zgener-
ated by gasg(x, y, m ). Letube an interpretation,
(u, v)be an interpretation pair, and Fbe a faithful-
ness metric. Denote the importance score that u
assigns to the i-th input token as [u]. Denote the
statement “ uis more faithful than v” as “ u≻v”,
and the statement “ Fconsiders uas more faithful
thanv” as “u≻v”.50303 Faithfulness Metrics
An interpretation is called faithful if the identified
important tokens truly contribute to the decision
making process of the model. Mainstream faith-
fulness metrics are removal-based metrics, which
measure the changes in model outputs after remov-
ing important tokens.
We compare the most widely adopted faithful-
ness metrics, introduced as follows.
Decision Flip - Most Informative Token
(DFMIT) Introduced by Chrysostomou and Ale-
tras (2021), this metric focuses on only the most
important token. It assumes that the interpretation
is faithful only if the prediction label is changed
after removing the most important token, i.e.
DFMIT =(
1ifc(x)̸=c(x\x))
0ifc(x) =c(x\x))
A score of 1 implies that the interpretation is faith-
ful.
Decision Flip - Fraction of Tokens (DFFOT)
This metric measures faithfulness as the minimum
fraction of important tokens needed to be erased in
order to change the model decision (Serrano and
Smith, 2019), i.e.
DFFOT =(
mins.t.c(x)̸=c(x\x)
1 ifc(x) =c(x\x)for any k
If the predicted class change never occurs even if
all tokens are deleted, then the score will be 1. A
lower value of DFFOT means the interpretation is
more faithful.
Comprehensiveness (COMP) As proposed by
DeYoung et al. (2020), comprehensiveness as-
sumes that an interpretation is faithful if the im-
portant tokens are broadly representative of the
entire input sequence. It measures the faithfulness
score by the change in the output probability of the
original predicted class after the important tokens
are removed, i.e.
COMP =1
|B|X(p(x)−p(x\x))
We use q∈B={1,5,10,20,50}as in the origi-
nal paper. A higher comprehensiveness score im-
plies a more faithful interpretation.Sufficiency (SUFF) Also proposed by DeYoung
et al. (2020), this metric measures whether the im-
portant tokens contain sufficient information to re-
tain the prediction. It keeps only the important
tokens and calculates the change in output proba-
bility compared to the original specific predicted
class, i.e.
SUFF =1
|B|X(p(x)−p(x))
We use q∈B={1,5,10,20,50}as in the origi-
nal paper. The lower the value of SUFF means that
the interpretation is more faithful.
Correlation between Importance and Output
Probability (CORR) This metric assumes that
the interpretation is faithful if the importance of the
token and the corresponding predicted probability
when the most important token is continuously re-
moved is positively correlated (Arya et al., 2019),
i.e.
CORR =−ρ(u,p)
where udenotes the token importance in de-
scending order and p= [p(x\x), p(x\
x), ..., p(x\x)].ρ(·)denotes the Pearsons
correlation. The higher the correlation the more
faithful the interpretation is.
Monotonicity (MONO) This metric assumes
that an interpretation is faithful if the probability of
the predicted class monotonically increases when
incrementally adding more important tokens (Arya
et al., 2019). Starting from an empty vector, the
features are gradually added in ascending order of
importance, and the corresponding classification
probabilities are noted. Monotonicity is calculated
as the correlation between the feature importance
and the probability after adding the feature, i.e.
MONO =ρ(u,p)
where udenotes the token importance in de-
scending order and p= [p(x), p(x\
x), p(x\x), ..., p(x\x)].ρ(·)de-
notes the Pearsons correlation. The higher the
monotonicity the more faithful the interpretation
is.
4 Evaluation of Faithfulness Metrics
In this section, we propose an evaluation paradigm
for faithfulness metrics by addressing two aspects:
(1) diagnosticity and (2) time complexity. They5031are the two complementary and important factors
in selecting a faithfulness metric for assessing the
faithfulness of interpretations.
4.1 Diagnosticity of Faithfulness Metric
As we have observed in Table 1, faithfulness met-
rics might disagree with each other on faithfulness
assessment. This naturally raises a question: Which
faithfulness metric(s) should we trust?
To the best of our knowledge, there is no pre-
ceding work in quantifying the effectiveness of
faithfulness metrics. As a first attempt, we intro-
duce diagnositicity , which is intended to measure
“the degree to which a faithfulness metric favours
faithful interpretations over unfaithful interpreta-
tions”. Intuitively, the higher the diagnosticity the
more effective the faithfulness metric is.
4.1.1 Definition of Diagnosticity
Definition 4.1 (Diagnosticity ).We define the diag-
nosticity of a faithfulness metric as the probability
that given an interpretation pair (u, v)such that u
is more faithful than v, the faithfulness metric also
considers uas more faithful than v, i.e.
D(F) =P(u≻v|u≻v)
As we will see later in this section, a set of in-
terpretation pairs (u, v)such that u≻vis required
for estimating diagnosticity. Constructing such a
dataset leads us to a paradox: we cannot be guar-
anteed that some generated interpretation is more
faithful than the others when the measurement of
faithfulness is still under debate. It is more realistic
to assume that we can generate an interpretation
pair(u, v)such that uisvery likely to be more
faithful than v. Thus, we relax the condition in
Definition 4.1 to a probabilistic one as follows.
Definition 4.2 (ε-diagnosticity ).Let(u, v)be
any interpretation pair, and 0≤ε≤1. The ε-
diagnosticity of a faithfulness metric Fis defined
as
D(F) =P(u≻v|P(u≻v)>1−ε)
In the above definition, εrepresents the uncer-
tainty in comparing the faithfulness of uandv. In
the next Theorem, we show that ε-diagnosticity ef-
fectively approximates diagnosticity as long as εis
small enough.
Theorem 4.1 (Error Bound of ε-diagnosticity ).
We can approximate diagnosticity with ε-
diagnosticity with error less than ε, i.e.
|D(F)−D(F)|< εThe proof is provided in Appendix A.
4.1.2 Estimation of Diagnosticity
In the following, we show how we estimate ε-
diagnosticity with a set of interpretation pairs (u, v)
where the uisvery likely to be more faithful than
v, namely an ε-faithfulness golden set where εis
small.
Definition 4.3 (ε-faithfulness golden set ).Let0≤
ε≤1. A set Zof interpretation pairs is called a ε-
faithfulness golden set, if it satisfies the following
conditions.
1.All interpretation pairs in Zare independent
and identically distributed (i.i.d.).
2. P(u≻v)>1−εfor any interpretation pair
(u, v)∈Z.
Lemma 4.2. Let 1(·)be the indicator function
which takes a value 1 when the input statement is
true and a value 0 when it is false. Then 1(u≻
v)|(P(u≻v)>1−ε)is a random variable and
its expected value is equal to ε-diagnosticity, i.e.
D(F) =E[ 1(u≻v)|P(u≻v)>1−ε]
The proof is provided in Appendix B.
As a result, given an ε-faithfulness golden set
Z, we can estimate the ε-diagnosticity of a faith-
fulness metric Fby estimating the expected value
in Lemma 4.2. Then by the law of large numbers,
we can simply estimate the expected value by com-
puting the average value of 1(u≻v)onZ, i.e.
D(F)≈1
|Z|X1(u≻v) (1)
When |Z|is large enough, we will have
|P1(u≻v)−D(F)|< εaccord-
ing to Theorem 4.1.
4.1.3 Generation of an ε-faithfulness golden
set
According to Theorem 4.1 and Lemma 4.2, we
can estimate the diagnosticity of any faithfulness
metric using Equation 1 as long as we have an
ε-faithfulness golden set where εis small enough.
We called the uandvin Definition 4.3 a rel-
atively faithful interpretation and a relatively
unfaithful interpretation respectively. Next, we
discuss the processes to generate them respectively.5032Generating Relatively Unfaithful Interpreta-
tions By definition, a faithful interpretation is
an interpretation that truly reflects the underlying
decision making process of the classification model.
Therefore, an unfaithful interpretation is one that
is completely irrelevant to the underlying decision
making process of the classification model. We pro-
pose to generate relatively unfaithful interpretations
by assigning a random importance score to each to-
ken in the input sequence, i.e. [v]∼Uniform (0,1)
for any token 1≤i≤l, where Uniform denotes
the uniform distribution.
Generating Relatively Faithful Interpretations
We propose to generate relatively faithful interpre-
tations with the interpretation methods that infer in-
terpretations from the underlying mechanism of the
classification model. There are two mainstream cat-
egories of interpretations methods that satisfy this
requirement (Alvarez-Melis and Jaakkola, 2018):
•Perturbation-based : Relying on querying
the model around the classification instance
to infer the importance of input features.
•Gradient-based : Using information from gra-
dients to infer the importance of input fea-
tures.
We select the representative methods from both
categories and introduce them in the following.
•Perturbation-based - LIME (Ribeiro et al.,
2016): For each classification instance, a lin-
ear model on the input space is trained to ap-
proximate the local decision boundary, so that
the learned coefficients can be used to quan-
tify the importance of the corresponding input
features on the model prediction.
•Perturbation-based - Word Omission (WO)
(Robnik-Šikonja and Kononenko, 2008): For
eachi-th input token, WO quantifies the im-
portance of the input token by the change
in output probability after removing it from
the original input sequence, i.e. p(x)−
p(x).
•Gradient-based - Saliency Map (SA) (Si-
monyan et al., 2014): For each i-th input to-
ken, SA computes the gradients of the orig-
inal model output with respect to the em-
bedding associated with the input token, i.e.|, and quantifies the importanceAlgorithm 1 Anε-faithfulness golden set genera-
tion mechanism.
Input: X: A set of i.i.d. classification instances
associated with classification model m;
G: The set of interpretation methods for generat-
ing relatively faithful interpretations, i.e. {LIME,
WO, SA, SA, IG, IG};
K: Sample size;
Output: Anε-faithfulness golden set Z;
Z← {} ;
For 1 to K;
(x, y)←RandomSampler (X);
g←RandomSampler (G);
u←g(x, y, m )
v←r∈Rwhere [r]∼Uniform (0,1);
Z←Z∪ {(u, v)};
return Z;
of the input token by taking either the mean or
thel2norm of the gradients in the embedding
dimension. We denote the former approach as
SAand the later approach as SA
•Gradient-based - Integrated Gradients (IG)
(Simonyan et al., 2014): As shown by Si-
monyan et al. (2014), Integrated Gradients
provides more robust interpretations than
Saliency Map in general. For each i-th in-
put token, it approximates the integral of the
gradients of the original model output with
respect to the embedding corresponding to the
input token along a straight line from a refer-
ence point xto the original input sequence,
i.e.Rdz, and quantifies the im-
portance of the input token by taking either
the mean or the l2norm of the integral in the
embedding dimension. We denote the former
approach as IGand the later approach as
IG.
The interpretations generated using the above
interpretation methods are highly likely to be more
faithful than the randomly generated interpretations
because the generation processes of the former ones
actually involve inferences from model behaviours,
while the random generation process is independent
of any model behaviour. Therefore, in principle,
the set of generated interpretation pairs will have a
small value of εin Definition 4.3.
In Algorithm 1, we propose a mechanism to
generate an ε-faithfulness golden set from a set of
i.i.d. classification instances based on the above5033
processes. Note that the generated interpretation
pairs will satisfy the first condition in Definition 4.3
because they are generated from i.i.d. samples, and
will satisfy the second condition in Definition 4.3
with a presumably small εas we have discussed.
4.2 Time Complexity of Faithfulness Metric
Two of the main applications of faithfulness metrics
are (1) evaluating interpretation methods based on
their average faithfulness scores on a dataset; and
(2) gauging the quality of individual interpretations
by spotting out “unfaithful” interpretations.
Time complexity is an important aspect in eval-
uating faithfulness metrics because a fast faithful-
ness metric will shorten the feedback loop in devel-
oping faithful interpretation methods, and would
allow runtime faithfulness checking of individual
interpretations in a production environment.
Measurement of time complexity From the def-
initions of the faithfulness metrics in Section 3, we
observe that their computations are dominated by
model forward passes, which are denoted as c(·)
orp(·). Thus, we measure the time complexities
of the faithfulness metrics in number of model for-
ward passes .
5 Experimental Setup
Datasets We conduct experiments on three text
classification datasets used in (Wiegreffe and Pin-
ter, 2019): (i) Stanford Sentiment Treebank ( SST)
(Socher et al., 2013); (ii) IMDB Large Movie Re-
views ( IMDB ) (Maas et al., 2011); (iii) AG News
Corpus ( AG) (Zhang et al., 2015). We summarize
the dataset statistics in Table 2.
Text classification models We adopt two most
common model architectures for text classification:
(i)BERT (Devlin et al., 2019b); (ii) CNN (Kim,
2014). The former one encodes contextualized rep-
resentations of tokens and has higher accuracy in
general, but at a cost of consuming more memory
and computational resources. The latter one uses
pretrained word embeddings as token representa-
tions and is lighter and faster. Their performances
on test data sets are shown in Table 2. The imple-
mentation details of both models can be found in
Appendix C.1.
ε-faithfulness golden set For each dataset and
text classification model, we transform the test set
into a set of classification instances and feed it into
Algorithm 1 to generate an ε-faithfulness golden
set with a size of 8,000 ( Kin Algorithm 1). The
implementation details of interpretation methods
can be found in Appendix C.2.
6 Results and Discussion
Diagnosticity We estimate the diagnositicities of
the faithfulness metrics in Section 3 on all datasets
for both CNN and BERT models. The results are
shown in Table 3.
COMP and SUFF have the highest and the sec-
ond highest average diagnosticites for both mod-
els. Hence, they are the most effective faithfulness
metrics. We also observe that COMP has higher
diagnosticities than SUFF on all datasets for BERT
model. This can be explained by the contextual-
ization property of Transformer encoders (Vaswani
et al., 2017): the hidden state of each token depends
on all other tokens in the input sequence. Remov-
ing a portion of the important tokens will alter the
whole context, and is likely to cause a dramatic5034change in model output.
DFMIT and DFFOT have the lowest and the
second lowest average diagnosticities. Removing
the most important token is usually not creating
enough perturbation to flip the original model de-
cision. In fact, the probability of decision flipping
by removing the most important token is ≤14%
for recent state-of-the-art interpretation methods
(Chrysostomou and Aletras, 2021). As a result, up
to 86% of interpretations are considered as indif-
ferent by DFMIT. For DFFOT, the probability of
decision flipping by removing the important tokens
in order does not only depend on the quality of
interpretation but also depends on any model bias
towards certain classes. For instance, decision flip-
ping will be less likely to occur if the predicted
class on the original input is the same as the one
on the empty input sequence. Therefore, we found
that decision flipping metrics (DFMIT, DFFOT) are
less effective than the metrics that operate on output
probabilities (SUFF, COMP, CORR, MONO).
Time complexity We compare the time complex-
ities of the faithfulness metrics in Section 3 mea-
sured in number of model forward passes. We first
analyze their time complexities based on their def-
initions in Table 4 and then measure their actual
time complexities on all datasets in Table 5. Note
that the time complexity here is equal to the number
of model forward passes.
DFMIT is the fastest faithfulness metric, which
requires only one model forward pass. DFFOT
has a non-deterministic time complexity, which
depends on how fast the decision flipping occurs,
and it is the second slowest faithfulness metrics
on all datasets. SUFF and COMP are the second
fastest faithfulness metric on average, which re-
quire at most 5 model forward passes. CORR and
MONO are the slowest faithfulness metrics, which
have time complexity equal to the number of input
tokens.
Which faithfulness metric(s) should we adopt?
In Figure 1, we evaluate the faithfulness metrics by
both their diagnosticities and time complexities.
Figure 1 suggests that we should always adopt
COMP and SUFF. Because (i) they have higher
diagnosticities and lower time complexities than
DFFOT, ; (ii) they have a similar level of diag-
nosticity and much lower time complexities than
CORR and MONO; (iii) DFMIT has diagnosticity
less than 0.1, which is below an acceptable level.
We would prefer COMP and SUFF over DFMIT
even though it has the lowest time complexity.
Note that our evaluation framework can be used
to compare any faithfulness metrics. In general, we
prefer faithfulness metrics that have higher diagnos-
ticities and lower time complexities, i.e. closer to
the top-right corner in Figure 1. But what if one has
a higher diagnosticity and the other one has a lower
time complexity? In this case, we should consider
diagnosticity first: a faithfulness metric should not
be used if it cannot effectively assess faithfulness,5035i.e. diagnosticity below a certain threshold. In
scenarios where we are subject to constraints of
hardware or timeliness, we might need to select a
faster metric with a lower but acceptable level of
diagnosticity.
7 Related Work
Interpretation methods Interpretation meth-
ods can be roughly classified into two cate-
gories: model-based methods and post-hoc meth-
ods. Model-based methods refer to the construction
of simple machine learning models whose internal
decision logic can be easily interpreted, such as lin-
ear regression models, decision trees, etc. Post-hoc
methods interpret the internal reasoning process
behind the model after training. Generally, post-
hoc methods can be divided into gradient-based
and perturbation-based. A gradient-based inter-
pretation method assumes deep learning model is
differentiable and discloses the decision making
mechanism of the model according to the gradient
information (Simonyan et al., 2014; Sundararajan
et al., 2017; Shrikumar et al., 2019). A perturbation-
based interpretation method interprets the model
by perturbing the input of data samples and measur-
ing how the predictions change (Robnik-Šikonja
and Kononenko, 2008; Zeiler and Fergus, 2013;
Ribeiro et al., 2016).
Interpretation method evaluation To assess the
quality of different interpretation methods, vari-
ous evaluation metrics have been proposed. Exist-
ing evaluation methods on interpretations can be
broadly classified into two categories, plausibility
and faithfulness. Plausibility measures if the in-
terpretation agrees with human judgments on how
a model makes a decision (Ribeiro et al., 2016;
Doshi-Velez and Kim, 2017; Lundberg and Lee,
2017; DeYoung et al., 2020). However, even if
the interpretation conforms to human criteria, it
is not certain that it truly reflects the underlying
decision mechanism behind the model. To this end,
faithfulness measures the extent to which the inner
decision-making mechanism actually relies on the
identified important features (Arras et al., 2017;
Serrano and Smith, 2019; Jain and Wallace, 2019;
Wiegreffe and Pinter, 2019; DeYoung et al., 2020;
Chrysostomou and Aletras, 2021).
In general, existing faithfulness metrics are de-
veloped through a removal-based criterion, which
measures the changes in model output when per-
turbing or removing tokens identified as importantby the interpretation. Serrano and Smith (2019)
proposed a decision flipping metric that evaluates
the proportion of tokens that need to be erased in or-
der to change the model decision. Also using deci-
sion flip as an indicator, Chrysostomou and Aletras
(2021) introduces a metric that counts the average
flips that occur when removing the most important
token marked by the interpretation method. In ad-
dition to decision flips, changes in model output
probabilities by removing or retaining important
tokens is also widely used to measure faithfulness
(Arras et al., 2017; Arya et al., 2019; DeYoung
et al., 2020).
Some recent work also focuses on the study of
faithfulness metrics. Jacovi and Goldberg (2020)
argued that the definition of faithfulness remains
inconsistent and informal, and provided concrete
guidelines on how evaluations of interpretation
methods should and should not be conducted. More
recently, Yin et al. (2021) discussed the limitations
of removal-based faithfulness metrics and proposed
two other quantitative criteria, namely sensitivity
and stability. Different from the aforementioned
previous work that does not focus on assessing
faithfulness metrics, we mainly focus on the mea-
surement of faithfulness and conduct a comprehen-
sive study of existing faithfulness metrics.
8 Conclusion
In this paper, we propose a framework to quantita-
tively evaluate six widely adopted faithfulness met-
rics in terms of diagnosticity and time complexity.
In particular, diagnosticity measures whether the
faithfulness metric correctly favours relatively faith-
ful interpretations over random ones; time com-
plexity is concerned with computational efficiency,
estimated by the average number of model forward
passes. The experimental results show that suffi-
ciency and comprehensiveness metrics outperform
the other faithfulness metrics with higher diagnos-
ticity and lower time complexity. For this reason,
we suggest using these two metrics for faithfulness
evaluation. We hope our work will bring more
awareness to the standardization of faithfulness
measurement. For future work, we would like to ex-
plore evaluating faithfulness metrics using a white-
box model such as linear regression, from which
we can derive an intrinsically faithful interpretation
as the “ground truth”.5036References5037A Proof of Theorem 4.1
Proof.
B Proof of Lemma 4.2
Proof. From Definition 4.2, we have 1(u≻
v)|(P(u≻v)>1−ε)∼Bernoulli (p), where p=
D(F). Then based on the property of Bernoulli dis-
tribution, we know that the expected value of the
random variable is equal to p.
C Implementation Details
C.1 Text classification models
The text classification models are all implemented
in PyTorch. For BERT, we use the “bert-base-
uncased” from Huggingface transformersas the
pretrained model . We use the same set of hyper-
parameters regardless of dataset for fine-tuning:
dropout rate 0.2, AdamW (Loshchilov and Hutter,
2019) with an initial learning rate 2e-5, batch size
32 with no warmup steps. We set the maximum
number of finetuning epochs to be 10 and perform
early stopping when the performance on the test
set does not improve for 3 consecutive epochs
For CNN classifier, we use a one-layer CNN
encoder with a linear classifier. The embedding
is initialized with the 300-dimensional pretrained
GloVe word embedding (Pennington et al., 2014).
The CNN layer has 256 kernels and the size of
the kernels is 3. We use max-pooling and AdamW
with an initial learning rate 1e-3, batch size 32,
with no warmup steps. The maximum number of
epochs is 40 with early stopping after 3 consecutive
non-improving epochs.
C.2 Interpretation methods
For LIME, Saliency Map, Integrated Gradients and
DeepLift, we apply the implementation in Captum. For Word Omission, we use our own implemen-
tation.5038