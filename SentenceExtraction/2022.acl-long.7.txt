
Bin Liang, Qinglin Zhu, Xiang Li, Min Yang, Lin Gui, Yulan He, Ruifeng XuSchool of Computer Science and Technology,
Harbin Institute of Technology, Shenzhen, ChinaJoint Lab of HITSZ and China Merchants Securities, Shenzhen, ChinaSIAT, Chinese Academy of Sciences, Shenzhen, ChinaDepartment of Computer Science, University of Warwick, U.KThe Alan Turing Institute, UK,Peng Cheng Laboratory, Shenzhen, China
{bin.liang, zhuqinglin, xiangli}@stu.hit.edu.cn
min.yang@siat.ac.cn, {lin.gui, Yulan.He}@warwick.ac.uk
xuruifeng@hit.edu.cn
Abstract
Zero-shot stance detection (ZSSD) aims to de-
tect the stance for an unseen target during the
inference stage. In this paper, we propose a
joint contrastive learning (JointCL) framework,
which consists of stance contrastive learning
and target-aware prototypical graph contrastive
learning. Specifically, a stance contrastive
learning strategy is employed to better gener-
alize stance features for unseen targets. Fur-
ther, we build a prototypical graph for each
instance to learn the target-based representa-
tion, in which the prototypes are deployed as
a bridge to share the graph structures between
the known targets and the unseen ones. Then
a novel target-aware prototypical graph con-
trastive learning strategy is devised to general-
ize the reasoning ability of target-based stance
representations to the unseen targets. Extensive
experiments on three benchmark datasets show
that the proposed approach achieves state-of-
the-art performance in the ZSSD task.
1 Introduction
Stance detection aims to automatically identify
one’s opinionated standpoint/attitude (e.g. Pro,
Con, orNeutral , etc.) expressed in text towards
a specific proposition, topic, or target (Somasun-
daran and Wiebe, 2010; Augenstein et al., 2016;
Mohammad et al., 2016; Sobhani et al., 2017). For
example, a text “ Everyone is able to believe in
whatever they want. ” expresses a stance of “ Pro”
towards the target “ Atheism ”.
Existing methods achieved promising perfor-
mance in in-target stance detection when trained
and tested on the datasets towards the same set oftargets (Mohtarami et al., 2018; Graells-Garrido
et al., 2020), and in cross-target stance detection
that identifies the stance of a destination target us-
ing models trained on a related source target in a
one-to-one way (Xu et al., 2018; Zhang et al., 2020;
Liang et al., 2021a). In practice, however, it is in-
feasible to enumerate all possible (in-target) or re-
lated (cross-target) targets beforehand for training
stance detection models. Hence, zero-shot stance
detection (ZSSD) (Allaway and McKeown, 2020),
which aims to detect the stance for unseen targets
during the inference stage is a promising scenario
forward.
To deal with ZSSD, intuitively, we can either
reason the target-based stance features from the
learned stance information based on the context
(i.e., from the context-aware perspective), or iden-
tify stance information that is potentially relevant
with unseen targets from the learned target-related
stance expressions (i.e., from the target-aware
perspective). Existing research attempted to ex-
plore attention mechanism (Allaway and McKe-
own, 2020), adversarial learning (Allaway et al.,
2021), or graph architecture based on external com-
monsense knowledge (Liu et al., 2021) to learn the
stance representations from the context regarding
the known targets, aiming to generalize the learned
stance features to the unseen targets for ZSSD. But
they tend to ignore that the stance information of an
unseen target can be represented in the light of the
known targets from the target-aware perspective.
In this paper, to generalize the stance features to
the unseen targets, we propose a joint contrastive
learning ( JointCL ) framework to leverage the
stance features of known targets from both the
context-aware and the target-aware perspectives.
On the one hand, from the context-aware perspec-
tive, we explore a Stance Contrastive Learning81strategy, which effectively improves the quality of
stance features by leveraging the similarity of train-
ing instances in a stance class while pushing away
instances from other stance classes. This essen-
tially allows the exploitation of target-based con-
textual stance features to better generalize to the
unseen targets. On the other hand, from the target-
aware perspective, we propose a feasible solution
to capture the relationships between the known tar-
gets and the unseen ones. Specifically, inspired
by (Li et al., 2021), we explore a clustering method
to generate prototypes from all training instances.
We then build prototypical graphs linking the pro-
totypes with the target-based representations, in
which each prototype is regarded as a bridge that
allows the sharing of the graph structures between
known targets and unseen ones. Based on the pro-
totypical graphs, we devise a novel Target-Aware
Prototypical Graph Contrastive Learning strat-
egy to learn the correlation and difference among
the target-based representations. Specifically, a
novel edge-oriented graph contrastive loss is de-
ployed to make the graph structures similar for
similar target-based representations, and different
for dissimilar ones. This essentially generalizes the
graph structures learned from the known targets to
the unseen ones, so as to better derive target-aware
stance information for the unseen targets by the
graph representations.
The main contributions of our work are summa-
rized as follows:
•The ZSSD task is approached from a new
perspective for detecting stance of an un-
seen target via reasoning the target-based
stance features from the learned stance infor-
mation based on the context or devising the
target-aware stance information that is poten-
tially relevant with the unseen target from the
learned ones.
•We propose a novel joint contrastive learn-
ing (JointCL ) framework, which consists of
stance contrastive learning and target-aware
prototypical graph contrastive learning, to gen-
eralize the target-based stance features to the
unseen targets.
•Extensive experiments on three benchmark
datasets show that the proposed JointCL
framework outperforms state-of-the-art base-
lines in the ZSSD task. Further, the proposed
JointCL framework can be easily extendedto the few-shot and cross-target stance detec-
tion and achieves outstanding performance.
2 Related Work
2.1 Zero-Shot Stance Detection
Zero-shot stance detection (ZSSD) aims to detect
stance for destination unseen targets by learning
stance features from known targets (Allaway and
McKeown, 2020). To deal with zero-shot stance
detection, Allaway and McKeown (2020) created
a new dataset consisting of a large range of topics
covering broad themes, called Varied Stance Topics
(V AST). Based on it, they proposed a topic-grouped
attention model to implicitly capture relationships
between targets by using generalized topic repre-
sentations. Allaway et al. (2021) adopted a target-
specific stance detection dataset (Mohammad et al.,
2016) and deployed adversarial learning to extract
target-invariant transformation features in ZSSD.
More recently, to exploit both the structural-level
and semantic-level information of the relational
knowledge, Liu et al. (2021) proposed a common-
sense knowledge enhanced graph model based on
BERT (Devlin et al., 2019) to tackle ZSSD.
2.2 Contrastive Learning
Contrastive learning in the latent space has recently
shown great promise, which aims to make the rep-
resentation of a given anchor data to be similar
to its positive pairs and dissimilar to its negative
pairs (Hadsell et al., 2006; Wu et al., 2018; Tian
et al., 2020; Chen et al., 2020a; Khosla et al., 2020;
Chen et al., 2020b; Zhang et al., 2021; Wang et al.,
2021; Gunel et al., 2021). Various contrastive learn-
ing approaches have been developed to deal with
natural language processing tasks (Kachuee et al.,
2021; Qin et al., 2021; Yang et al., 2021; Liu and
Liu, 2021; Liang et al., 2021b), including unsu-
pervised text representation learning (Giorgi et al.,
2021), text classification (Qiu et al., 2021), and text
clustering (Zhang et al., 2021).
More recently, Li et al. (2021) presented proto-
typical contrastive learning and a ProtoNCE loss
to encourage representations to be closer to their
assigned prototypes. However, this method only
models the relationship between an anchor instance
and its nearest prototype. On the other hand, You
et al. (2020) proposed a graph contrastive learn-
ing framework based on graph data augmentation,
which improves the graph representations for better
generalizability and robustness. However, their ap-82
proach ignores the relationships of edges regarding
the graph structures. In our ( JointCL ) frame-
work, we devise a novel edge-oriented graph con-
trastive loss to learn the contrastive information
of the relationships between prototypes and the
targets, thus generalizing the graph structures to
the unseen targets for learning target-aware stance
information.
3 Methodology
In this section, we describe the proposed Joint Con-
trastive Learning ( JointCL ) framework for zero-
shot stance detection in detail. As demonstrated in
Figure 1, the architecture of the JointCL frame-
work contains four main components: 1) stance
contrastive learning , which performs contrastive
learning based on the supervised signal of stance
labels for better generalization of stance features;
2)prototypes generation , which derives the proto-
types of the training data by a clustering method; 3)
target-aware prototypical graph contrastive learn-
ing, which performs the edge-oriented graph con-
trastive learning strategy based on the target-aware
prototypical graphs for sharing the graph structures
between known targets and unseen ones; 4) classi-
fier, which detects the stances of targets based on
the hidden vectors and graph representations.
3.1 Task Description
Formally, let D={(r, t, y)}be the train-
ing set for the source targets, where tandyarethe training target and the stance label towards the
context rrespectively. Nis the number of the
training instances. Further, let D={(r, t)}
be the testing set for the targets which are unseen
in the training set. Here, tis the testing target in
the context r. The goal of ZSSD is to predict a
stance label (e.g. “ Pro”, “Con”, or “ Neutral ”) of
each testing instance by training a model on the
training set.
3.2 Encoder Module
Given a sequence of words r={w}and the
corresponding target t, where nis the length of
the sentence r, we adopt a pre-trained BERT (De-
vlin et al., 2019) as the Encoder Module and feed
“[CLS]r[SEP ]t[SEP ]” as input into the encoder
module to obtain a d-dimensional hidden repre-
sentation h∈Rof each input instance:
h= BERT([ CLS]r[SEP ]t[SEP ])(1)
Here, we use the vector of the [CLS]token to
represent the input instance. For the training set D,
the hidden representations of the training instances
can be represented as H={h}.
3.3 Stance Contrastive Learning
As previously discussed in Gunel et al. (2021),
good generalization requires capturing the simi-
larity between examples in one class and contrast-
ing them with examples in other classes. To im-
prove the generalization ability of stance learn-
ing, we define a stance contrastive loss on the
hidden vectors of instances with the supervised
stance label information. Given the hidden vectors
{h}in a mini-batch B(here, Nis the size of
mini-batch), and an anchor of hidden vector h,
h,h∈ B with the same stance label is consid-
ered as a positive pair, i.e. y=y, where y
andyare the stance labels of handh, respec-
tively, while the samples {h∈ B, k̸=i}are
treated as negative representations with respect
to the anchor . Then the contrastive loss is com-
puted across all positive pairs, both (h,h)and
(h,h)in a mini-batch:
L =−1
NXℓ(h) (2)
ℓ(h) = logP 1exp(f(h,h)/τ)
Pexp(f(h,h)/τ)
(3)83where 1∈ {0,1}is an indicator function eval-
uating to 1 iff i=j.f(u,v) =sim(u,v) =
uv/∥u∥∥v∥denotes the cosine similarity be-
tween vectors uandv.
3.4 Prototypes Generation
In the Prototypical Networks for few-shot learn-
ing, Snell et al. (2017) derived the prototype of
each class by computing the mean vector of the
embedded support points belonging to the class.
However, in the ZSSD data, the distribution of tar-
gets is usually imbalanced. Therefore, inspired
by (Li et al., 2021), we perform k-means cluster-
ing on the hidden vectors of the training instances
H={h}to generate kclusters as the proto-
typesC={c}with respect to the target-based
representations of training set. Here, a prototype is
defined as a representative embedding for a group
of semantically similar instances (Li et al., 2021).
Clustering is performed at each training epoch to
update the prototypes.
3.5 Prototypical Graph
Once the prototypes are generated, a prototypi-
cal graph is constructed to capture the relation-
ships between the prototypes and the known tar-
gets. This enables the learning of the represen-
tation of a target-based instance by modeling the
different weights of edges between its correspond-
ing target and various prototypes, so as to gen-
eralize the learned graph information to the un-
seen targets. Here, the prototypes and the target-
based representations are updated in an alternative
manner. For a hidden vector hof a training in-
stance i, we first treat the prototypes Cand the hid-
den vector has nodes of the prototypical graph:
X= [c,c,···,c,h], and then construct the
adjacency matrix G ∈Rof the fully-
connected graph, G=G= 1.
Next, we feed the nodes Xand the correspond-
ing adjacency matrix Ginto a graph attention net-
work (GAT) (Velickovic et al., 2018) to derive the
attention scores αand the graph representation z
for the target-based instance i:
α=a(GAT( X;G)) (4)
z=f(GAT( X;G)) (5)
where GAT(·)represents GAT operation. a(·)de-
notes retrieving the attention score matrix from the
GAT operation, f(·)denotes retrieving the graph
representation for h.3.6 Target-Aware Prototypical Graph
Contrastive Learning
From the target-aware perspective, we further ex-
plore a Target-Aware Prototypical Graph Con-
trastive Learning strategy, aiming at generalizing
the graph structures learned from the known tar-
gets to the unseen ones. Specifically, for the atten-
tion matrices {α}in each mini-batch B, we
devise a novel edge-oriented prototypical graph
contrastive loss, making the graph structure of sim-
ilar target-based representations to be similar. This
essentially allows the model to learn the represen-
tations of (unseen) targets through the prototypes,
thus generalizing the target-aware stance informa-
tion to the unseen targets.
For an anchor instance iwith edge weights (i.e.,
the attention score matrix) α, we construct a posi-
tivepair(α,α)by retrieving the attention score
matrix of instance jwhich is either about the same
target or has been assigned to the same prototype,
and expresses the same stance as i. We also con-
struct negative pairs, (α,α),α∈ B, k̸=i.
Then, the edge-oriented graph contrastive loss is
defined as:
L =−1
NXℓ(α) (6)
ℓ(α) = logPΦ(i, j)exp( f(α,α)/τ)
Pexp(f(α,α)/τ)
(7)
Φ(i, j) =(
1ify=yandp=p
0otherwise(8)
where p=prepresents the instances iandj
correspond to the same target or belong to the same
prototype, and express the same stance.
The calculation of the stance and edge-oriented
prototypical graph contrastive losses for each mini-
batchBis illustrated in Algorithm 1.
3.7 Stance Detection
For each instance i, we first concatenate the hidden
vector hand the graph representation zto get the
output representation vtowards the instance i:
v=h⊕z (9)
Then the output representation vis fed into a clas-
sifier with a softmax function to produce the pre-84Algorithm 1: Calculation of the stance
and edge-oriented prototypical graph con-
trastive losses for each mini-batch B.
dicted stance distribution ˆy∈R:
ˆy= softmax( Wv+b) (10)
where dis the dimensionality of stance labels.
W∈Randb∈Rare trainable parame-
ters. We adopt a cross-entropy loss between pre-
dicted distribution ˆyand ground-truth distribution
yof instance ito train the classifier:
L=−XXylogˆy (11)
3.8 Learning Objective
The learning objective of our proposed model is
to train the model by jointly minimizing the three
losses generated by stance detection, stance con-
trastive learning, and target-aware prototypical
graph contrastive learning. The overall loss Lis
formulated by summing up three losses:
where γ,γandγare tuned hyper-parameters.
Θdenotes all trainable parameters of the model, λ
represents the coefficient of L-regularization.
4 Experimental Setup
4.1 Datasets
We conduct experiments on three datasets to
evaluate the proposed JointCL framework. 1)
V (Allaway and McKeown, 2020), which con-
tains a large variety of targets. Each instance con-
sists of a sentence r, a target t, and a stance la-
bely(“Pro”, “Con”, or “ Neutral ”) towards t. To
show the generalizability of coping with few-shot
stance detection, following (Allaway and McKe-
own, 2020), we also conduct experiments on few-
shot condition. The statistics of V dataset are
shown in Table 1. 2) S16, which contains 6
pre-defined targets, including Donald Trump (DT),
Hillary Clinton (HC), Feminist Movement (FM),
Legalization of Abortion (LA), Atheism (A), and
Climate Change (CC). Each instance can be clas-
sified as Favor ,Against orNeutral . 3)W-,
which contains 5 pre-defined company pairs (tar-
get), including CVS_AET (CA), CI_ESRX (CE),
ANTM_CI (AC), and AET_HUM (AH). Each in-
stance refers to a stance label from Support (cor-
responding to Favor ),Refute (corresponding to
Against ),Comment (corresponding to Neutral ), or
Unrelated . The statistics of W-andS16
datasets are shown in Table 2. Following (Allaway
et al., 2021) and (Conforti et al., 2020), for S16
andW-datasets, we use the leave-one-target-
out evaluation setup.
4.2 Implementation Detail
Training Settings The pre-trained uncased
BERT-base (Devlin et al., 2019) is used as the85embedding module in which each word token is
mapped to a 768-dimensional embedding. The
learning rate is set to 3e-5. Following (Xu et al.,
2018), the coefficient λis set to 1e-5. Adam is
utilized as the optimizer. The mini-batch size is set
to 16, considering the trade-off between computa-
tional resource and evaluation performance. For
contrastive losses, both the temperature parameters
τandτare set to 0.07. For clustering, the num-
ber of clusters are set to k= 100 for the V
dataset and k= 10 for the W-andS16
datasets respectively. Corresponding to the num-
ber of k, we set γ= 0.8,γ= 1, and γ= 0.1
forV dataset and γ= 0.5forW-and
S16datasets, respectively. They are the optimal
hyper-parameters in the pilot studies. We apply
early stopping in training process and the patience
is 5. We report averaged scores of 10 runs to obtain
statistically stable results.
Evaluation Metric For the Vdataset, follow-
ing (Allaway and McKeown, 2020), we calculate
Macro-averaged F1 of each label to measure the
testing performance of the models. For the S16
dataset, following (Allaway et al., 2021), we re-
portF, the average of F1 on Favor andAgainst .
For the W-dataset, following (Conforti et al.,
2020), we report the Macro F1 score of each target.
4.3 Comparison Models
We compare the proposed JointCL with a se-
ries of strong stance detection baselines, includ-
ing neural network-based method: BiCond (Au-
genstein et al., 2016), attention-based models:
CrossNet (Xu et al., 2018) and SiamNet (San-
tosh et al., 2019), knowledge-based method:
SEKT (Zhang et al., 2020), graph network method:
TPDG (Liang et al., 2021a), adversarial learn-
ing method: TOAD (Allaway et al., 2021), and
BERT-based methods: BERT (Devlin et al.,
2019), TGA Net (Allaway and McKeown, 2020),
BERT-GCN (Liu et al., 2021), and CKE-Net (Liu
et al., 2021).
In addition, we provide variants of our proposed
JointCL in the ablation study:
(1) “w/oL ” denotes without stance con-
trastive learning.
(2) “w/oL ” denotes without prototypical
graph contrastive learning.
(3) “w/ograph ” denotes that this model per-
forms the target-aware contrastive learning on the
hidden representations of the instances with thesupervised information from target labels. That is,
the contrastive loss functions of Eq. 6 and Eq. 7 are
replaced by:
L =−1
NXℓ(h) (13)
ℓ(h) = logP 1exp(f(h,h)/τ)
Pexp(f(h,h)/τ)
(14)
(4) “w/ocluster ” denotes without using clus-
tering to generate prototypes. That is, this model
simply takes the mean of target-based hidden rep-
resentations as a prototype.
(5) “w/oedge ” denotes without considering edge
information, i.e., it performs the prototypical graph
contrastive learning on the graph representations of
the instance nodes. The contrastive loss functions
of Eq. 6 and Eq. 7 are replaced by:
L =−1
NXℓ(z) (15)
ℓ(z) = logP 1exp(f(z,z)/τ)
Pexp(f(z,z)/τ)
(16)
5 Experimental Results
5.1 Main Results
The main comparison results of ZSSD on three
benchmark datasets are reported in Table 3. It
can be observed from the experimental results, our
proposed JointCL framework performs consis-
tently better than the non-BERT and the BERT-
based comparison models on both the V and
W-datasets, and achieves overall better per-
formance than the comparison baselines on the
S16dataset. This verifies the effectiveness of
ourJointCL framework in the ZSSD task. Fur-
thermore, the significance tests of JointCL over
the baseline models show that our JointCL signif-
icantly outperforms the baseline models (the results
ofp−value on most of the evaluation metrics are
less than 0.05). More concretely, in comparison
with the adversarial learning-based model ( TOAD ),
ourJointCL achieves significant improvement
across all datasets. This indicates that exploring
graph contrastive learning to model the relation-
ships among targets can better generalize the target-
based stance features to the unseen targets. In addi-
tion, the comparison results between our JointCL86
and the previous BERT-based models demonstrate
that the stance representations learned from known
targets can be better generalized to the unseen tar-
gets with our proposed novel contrastive learning
strategy.
5.2 Ablation Study
To analyze the impact of different components in
our proposed JointCL on the performance, we
conduct an ablation study and report the results
in Table 4. We can observe that the removal of
stance contrastive learning (“ w/oL ”) sharply
reduces the performance in all evaluation metrics
and across all datasets. This indicates that perform-
ing contrastive learning based on stance informa-
tion can improve the quality of stance represen-
tations for better generalizing the learned stance
features to the unseen targets, and thus improve
the performance of ZSSD. The removal of edge-
oriented prototypical graph contrastive learning
(“w/oL ”) leads to considerable performance
degradation. This implies that performing target-
based contrastive learning for prototypical graph
can generalize the graph relations between known
targets and prototypes to the unseen targets, which
enables the model to derive better representation
for the examples of unseen targets, and thus leadsto improved ZSSD performance.
In addition, from the results of “ w/ograph ” we
can see that purely performing the target-based
contrastive learning on the hidden representations
slashes the learning ability of stance contrastive
learning, and thus leads to poorer performance.
This verifies the effectiveness of exploring proto-
typical graph contrastive learning in our JointCL .
We also observe that the performance of “ w/o
cluster ” drops consistently across datasets, which
indicates that exploring clustering method can ef-
fectively relieve the problem of the imbalanced
distribution of targets in the dataset. The removal
of edge-oriented graph contrastive strategy (“ w/o
L”) leads to noticeable performance degrada-
tion. This implies that, to represent the (unseen)
targets with prototypes, we should pay more atten-
tion to the relationships between targets and pro-
totypes, rather than simply drawing closer similar
target-based representations in the graph.
5.3 Impact of the Values of k
To analyze the impact of using different values of k
ink-means clustering on the performance, we con-
duct experiments on the three datasets, and show
the results in Figure 2. Here, for V, we show
the results of all labels. For the S16andW-87, we show the average performance of all targets.
We observe that for Vthat contains a large num-
ber of targets (more than 5,000 in the training set),
the performance increases with the increasing value
ofkand peaks at k= 100 . Further increasing the
values of kresults in worse performance. Similarly,
forS16andW-, better performance is ob-
tained in the region of k∈[10,20]and peaks when
k= 10 . This implies that we can set an appropriate
region for the value of kaccording to the number
of targets in the dataset.
5.4 Generalizability Analysis
Analysis of Few-Shot Condition To evaluate
the generalizability of our JointCL framework in
few-shot stance detection, following (Allaway and
McKeown, 2020; Liu et al., 2021), we also evaluate
JointCL in the few-shot condition on the V
dataset. From the experimental results shown in Ta-
ble 5, we can see that JointCL performs overall
better than all the comparison methods under the
few-shot condition. This verifies the effectiveness
and generalizability of JointCL in dealing with
both zero-shot and few-shot stance detection.
Analysis of Cross-Target Scenario We further
conduct comparison experiments in the cross-target
scenario on the S16dataset. Cross-target stance
detection trains on a source target and tests on an
unseen but related one, which is a task related to
ZSSD. We report the results in Table 6. It can be
observed that JointCL achieves consistently bet-
ter performance on all cross-target scenarios, which
verifies that our JointCL can generalize the learn-
ing ability to deal with cross-target scenarios. In ad-
dition, when compared with the results of Table 3,
we see that the results of cross-target stance detec-
tion are generally better than ZSSD. This shows
that recognizing the relationships among targets
in advance can potentially improve the stance de-
tection performance for the unseen targets, which
illustrates the challenge of the ZSSD task from
another angle.
5.5 Visualization
To qualitatively demonstrate how the proposed
JointCL captures good generalization of stance
features for unseen targets in ZSSD, we randomly
select 200 test instances for each label from V
dataset and show the t-SNE (van der Maaten and
Hinton, 2008) visualization of intermediate em-
beddings learned by BERT-GCN and our proposed
JointCL onVin Figure 3. It can be seen that
the distributions of representations derived from
BERT-GCN largely overlap especially for the Pro
andCon stances. But there are clear separations
between different stances (including the Proand
Con stances) produced by our proposed JointCL .
This verifies that the novel joint contrastive learning
strategy in JointCL can better separate represen-
tations from different stances, so as to improve the
performance of ZSSD.886 Conclusion
In this paper, we propose a novel joint contrastive
learning ( JointCL ) framework to deal with the
zero-shot stance detection (ZSSD) task. On the
one hand, we deploy a stance contrastive learning
strategy to improve the quality of stance representa-
tions, so as to capture good generalization of stance
features for the unseen targets. This is based on our
observation that for some cases we can determine
the stance towards a specific target from its associ-
ated context. On the other hand, we devise a target-
aware prototypical graph contrastive learning strat-
egy to generalize the learned graph information to
the unseen targets by leveraging the prototypes as
a bridge to model the relationships between known
and unseen targets. This is for other cases when it is
difficult to infer the stance for an unseen target from
the context, but instead, could be relatively easier
by exploiting the target-aware stance information
from the learned associated targets. Experimental
results on three benchmark datasets show that our
JointCL achieves state-of-the-art performance in
ZSSD. Further, the generalizability analysis shows
that our JointCL can also perform outstandingly
on few-shot and cross-target stance detection.
Acknowledgments
This work was partially supported by the National
Natural Science Foundation of China (61876053,
62006062, 62176076, 62006060), UK Engineer-
ing and Physical Sciences Research Council (grant
no. EP/V048597/1, EP/T017112/1), Natural Sci-
ence Foundation of Guangdong Province of China
(No. 2019A1515011705), Shenzhen Foundational
Research Funding (JCYJ20200109113441941,
JCYJ20210324115614039), Shenzhen Science
and Technology Innovation Program (Grant No.
KQTD20190929172835662), Joint Lab of Lab of
HITSZ and China Merchants Securities. Yulan He
is supported by a Turing AI Fellowship funded by
the UK Research and Innovation (UKRI) (grant no.
EP/V020579/1).
References899091