
Xueliang Zhao, Tingchen Fu, Chongyang Tao, Wei Wu, Dongyan Zhao, Rui YanWangxuan Institute of Computer Technology, Peking UniversityCenter for Data Science, AAIS, Peking UniversityGaoling School of Artificial Intelligence, Renmin University of ChinaMicrosoftMeituan, Beijing, China
{xl.zhao,zhaody}@pku.edu.cn ruiyan@ruc.edu.cn
{lucas.futingchen,chongyangtao,wuwei19850318}@gmail.com
Abstract
Grounding dialogue generation by extra knowl-
edge has shown great potentials towards build-
ing a system capable of replying with knowl-
edgeable and engaging responses. Existing
studies focus on how to synthesize a response
with proper knowledge, yet neglect that the
same knowledge could be expressed differently
by speakers even under the same context. In
this work, we mainly consider two aspects of
knowledge expression, namely the structure of
the response and style of the content in each
part. We therefore introduce two sequential
latent variables to represent the structure and
the content style respectively. We propose a
segmentation-based generation model and op-
timize the model by a variational approach to
discover the underlying pattern of knowledge
expression in a response. Evaluation results on
two benchmarks indicate that our model can
learn the structure style defined by a few exam-
ples and generate responses in desired content
style.
1 Introduction
Building an open domain dialogue system has at-
tracted increasing attention from the community
of AI and NLP. Despite the impressive progress,
existing models are notorious for replying with
generic and bland responses. To bridge the gap,
researchers resort to ground dialogue generation by
extra knowledge such as unstructured documents
(Zhou et al., 2018c; Dinan et al., 2019). By this
means, the documents serve as content sources and
make a dialogue system knowledgeable regarding
various concepts in a discussion.
However, existing studies focus on how to syn-
thesize a response with proper knowledge (Dinan
et al., 2019; Kim et al., 2020; Zhao et al., 2020b),
but pay little attention to the fact that the same
knowledge could be expressed differently even un-
der the same context. These models usually em-Table 1: A case from CMU _DoG. Given the same
knowledge and context, the last two turns in left and
right conversations exhibit positive and negative senti-
ments, respectively. Each utterance can be decomposed
into knowledge-related and knowledge-irrelevant
segments.
ploy a regular decoder to generate the response
in an auto-regressive manner given the contextual
representations of knowledge and dialogue context,
which makes the generation process less explain-
able and controllable.
In general, the expression style of response is
composed of two aspects: the structure of the re-
sponse and the style of the content in each part. As
the example shown in Table 1, knowledge-related
phrases and clauses tend to be long, like “And I’d
give credit to three different voice actors for anna.”,
or short, like “74 in Metacritics”. Besides, they
may appear at the beginning of the sentence, or at
the end. For the sake of description, we decompose
a response into a sequence of non-overlapping seg-
ments, each is either related to certain background
knowledge and diverse in content style, or almost
irrelevant to the knowledge but simply playing the
role of stitching the context and carrying on the
conversation. We therefore define the structure
style as the distribution and number of two kinds
of segments. The structure style itself is far from
dominant in the sentence expression, since different
speakers could convey converse attitude even if the2258context and the knowledge are exactly the same. So
it is necessary to introduce the content style as the
expression fashion within each knowledge-related
segment. We further introduce two latent variables
to facilitate end-to-end training, one for predicting
the start and end positions of a segment, the other
for deciding the category of each segment. Since
the human annotations for sentence segmentation
are absent and enumerating over all possibilities
to maximize the likelihood of the response is time-
consuming, we propose a variational framework
for segmentation-based generation and induce an
evidence lower bound of the likelihood.
Formally, our model follows an encoder-decoder
architecture. The encoder is to obtain the contex-
tual representation of conversational context and
knowledge in a regular way. The decoder con-
sists of three types of modules: (1) context module,
for response only based on context without knowl-
edge; (2) plain-knowledge module, for response
referring knowledge but without particular style;
and (3) stylized-knowledge module, for response
referring knowledge and with a specific style. The
context module is the only module not relying on
knowledge, but simply paying attention to contex-
tual information. Compared with plain-knowledge
module, stylized-knowledge module has unique
adapters, which is their primary discrepancy. When
decoding, the decoder first predicts the segmenta-
tion of the response and then makes a choice in
three kinds of modules to generate a single segment.
Both the segmentation and the module selection
are instructed under sequential latent variables.
We train our model on the Reddit Corpus pub-
lished by Li et al. (2020) and evaluate our model on
two benchmarks of knowledge-grounded conversa-
tion: Wizard of Wikipedia (Wizard) (Dinan et al.,
2019) and CMU Document Grounded Conversa-
tion (CMU _DoG) (Zhou et al., 2018c). Evaluation
results indicate that our model can significantly
outperform state-of-the-art methods in the zero-
resource setting (i.e., only trained on the Reddit
Corpus). In addition, the performance of our model
improves significantly on Wizard and CMU _DoG
with the presence of only 10% training data and
the segment distributions after fine-tuning are con-
sistent with our prior knowledge about the two
datasets, indicating that our model can learn the
structure style with little cost. Finally, our model
outperforms previous state-of-the-art models on the
accuracy of performing sentiment classification us-ing generated responses, which indicates that the
model can be controlled to express knowledge with
the desired content style.
Contributions in this work are three-fold:
(1) exploration of the knowledge expression in
knowledge-grounded conversation; (2) proposal of
a variational segmentation-based generation model
to discover the underlying expression style in a
response; (3) empirical verification of the effective-
ness of the proposed model on two benchmarks of
knowledge-grounded conversation.
2 Related Work
On the vanilla encoder-decoder architecture (Shang
et al., 2015; Vinyals and Le, 2015), various exten-
sions have been made to model the structure of di-
alogue contexts (Serban et al., 2016, 2017; Zhang
et al., 2019a); to improve diversity of responses (Li
et al., 2015; Xing et al., 2017; Zhao et al., 2017;
Tao et al., 2018); to control attributes of responses
(Xu et al., 2019; Zhou et al., 2018a; Wang et al.,
2018; See et al., 2019); and to bias responses to
some specific personas (Li et al., 2016; Zhang et al.,
2018). Recently, grounding dialogue generation by
extra knowledge has seemed promising to bridge
the gap between conversation with existing sys-
tems and conversation with humans, and the knowl-
edge could be obtained from knowledge graphs
(Zhou et al., 2018b; Moon et al., 2019; Tuan et al.,
2019), retrieved from unstructured documents (Di-
nan et al., 2019; Lian et al., 2019; Zhao et al., 2019,
2020a; Kim et al., 2020; Li et al., 2020; Fu et al.,
2022) or visual background (Mostafazadeh et al.,
2017; Shuster et al., 2018; Huber et al., 2018). In
this work, we study document-grounded dialogue
generation. Rather than selecting knowledge rele-
vant to dialogue context and directly exploiting pre-
trained language models to generate the response,
we focus on expressing knowledge in this task.
The idea of sequence modeling via segmenta-
tion (Wang et al., 2017; Kim et al., 2019) has at-
tracted widespread attention in several natural lan-
guage processing tasks. In text segmentation task,
Wang et al. (2017) propose a probabilistic model
for sequence modeling via their segmentation and
a “Sleep-WAke Network” (SWAN) method. In
machine translation, Huang et al. (2017) propose
a neural phrase-based machine translation system
that models phrase structures in the target language
using SWAN. In data-to-text generation, Wiseman
et al. (2018) develop a neural template-like genera-2259tion model with a HSMM decoder, which is learned
tractably by backpropagating through a dynamic
program; to tackle the problem of weak Markov
assumption for the segment transition probability,
Shen et al. (2020) propose to explicitly segment
target text into fragments and align them with their
data correspondences, and jointly learn the segmen-
tation and correspondence via dynamic program-
ming.
3 Approach
We start by providing the problem formalization
and overview of the proposed model in Sec.3.1.
Then in Sec.3.2 we describe the design for each
components. Lastly, we elaborate how to opti-
mize the components with variational inference
and weak supervision in Sec.3.3.
3.1 Overview
Suppose that we have a dataset D =
{(U, K, R)}, where ∀i∈ { 1, . . . , N },
Kserves as background knowledge of the dia-
logue (U, R)withKbeing the j-th sentence,
Uis the context of the dialogue with Uthe
j-th utterance, and Ris the response. To bias
the expression to a specific structure style, we
further assume that there are a few examples
D={(U, K, R)}provided by users
depicting the required style for knowledge expres-
sion. Note that we have N≫M, since corpus in
a specific expression style is rare and difficult to
acquire. The goal is to learn a generation model
p(R|U, K)(θdenotes the parameters of the
model) from D, to generate a response Rfollowing
p(R|U, K)given a new dialogue context Uand
the associated knowledge K. Different from
previous KGC generation model, we allow users to
either (1) bias the structure style of P(R|U, K)to
Dwith little cost; or (2) switch the content style
of knowledge expression in R.
Figure 1 gives an overview of the proposed
model, which is based on the encoder-decoder
architecture. The encoder generates the contex-
tual representations of the dialogue and knowledge,
while the decoder generates the segments one af-
ter another. hencodes the dialogue context up
to timestep t−1withNdenoting the number of
decoder layers. Given R= (r,···, r,···, r)
withrreferring the t-th token of Rwhose length
is supposed to be l, the variable Z={z}is
utilized to control the choice of module of eachsegment ( Module Indicator ), and its historical in-
formation is encoded by {c}.M={m}
is a sequence of binary variables and used to de-
termine the boundary of each segment ( Boundary
Indicator ). Specifically, m= 1 indicating that
the current segment is already completed and a
new segment should be created at the next timestep.
Otherwise, m= 0 and the current segment re-
mains unfinished. The generative process is disas-
sembled into two steps: (1) determine the type of
a new segment based on previously generated text
and previous segment types; (2) generate within the
current segment until the binary variable m= 1.
3.2 Model Architecture
Context and Knowledge Encoding. We exploit
the pre-trained BART (Lewis et al., 2020) as
the backbone of our architecture, which is pre-
trained using a variety of denoising objectives
and achieves state-of-the-art results on a range
of text generation tasks. Given the dialogue con-
textU= (U,···, U), we simply concatenate
them as (u,···, u). Similarly, we concatenate
the associated knowledge K= (K,···, K)as
(k,···, k).landlare the length of dialogue
context and background knowledge respectively.
The input of the encoder is then defined as:
I= [BOS] k. . . k[EOS] u. . . u[EOS] .(1)
The input Iis truncated or padded to the maxi-
mum capacity and then passes through the stacked
self-attention layers and results in a knowledge-
aware context representation C, and a context-
aware knowledge representation K. Specifically,
the context-aware knowledge representation is de-
fined as K= [h,···,h]where his the
last layer of BART encoder at time t. Similarly, the
knowledge-aware context representation is defined
asC= [h,···,h].
Prior of Module Indicator. We use the sequen-
tial discrete latent variable Z={z}to decide
which module to invoke at each timestep. The
transition of zoccurs only when a segment is com-
pleted, which is decided by the binary boundary
variable M. The prior quantifies the distribution
ofzbefore we observe the segment, and it is rea-
sonable to assume that the prior of zdepends on
previous module choices zand previously gen-
erated text. Then the transition of Zis defined as:2260
p(z|r, z, m) =m·˜p(z|c)
+(1−m)·δ(z=z),(2)
where δis a Kronecker delta function. cencodes
all previous latent states zand generated text r
as follows:
c=m·f(˜ z,c)+(1−m)·c.
(3)
˜ z= [e;h]withethe embedding
ofzandhthe representation of last gen-
erated token. Specifically, m= 0 means that
the next timestep tis still in the same segment
as the previous timestep t−1and thus the latent
variable zshould not be updated. Otherwise, it
means that current segment is completed and z
is updated with the transition function ˜p(z|c).
Because we only have N+ 2 options when
choosing a module, where Nis the number of
different user-defined styles in addition to 2de-
fault styles, so in this model, the latent variable z
ranges in natural integer to denote corresponding
style type. Specifically, z= 0 denotes choos-
ing the context expression module to generate a
knowledge-irrelevant segment; z= 1 tells the
model to choose the knowledge expression mod-
ule without specially customized style; we leave
thez≥2to be user-defined so as to select the
knowledge expression module combined with cus-
tomized style. The transition function ˜p(z|c)isthen implemented as a multinomial distribution pa-
rameterized by Softmax (f(c)).
Prior of Boundary Indicator. The boundary
indicator M={m}depicts the segmen-
tal structure of the response, with m= 1 in-
dicates that a new segment should start at time
t+ 1. Presumably, the prior of mcould be in-
ferred from randz. We model the distribution
p(m|r, z)by a Bernoulli distribution param-
eterized by σ(f([e;h])), where σde-
notes the sigmoid function.
Stylized Generation. As mentioned above, the
generation process involves scheduling different
modules according to z. Here we give a systematic
description of the generation process. The decoder
accepts the token generated last timestep ras
input, performs transformation in Ndecoder layers,
finally obtains a dense representation.
We use hto denote the hidden state after the
l-th layer at timestep t, which is a shorthand for
h for brevity. Specially, his the output of
the embedding layer. When z= 0, it implies
that knowledge encoding is unnecessary for current
segment so his defined as:
h=DecoderLayer (h,H,C),(4)
where H= [h,···,h]is a sequence of
decoder hidden states in previous timesteps, and2261Cis the context representation mentioned above.
DecoderLayer (·,·,·)is implemented as pre-trained
BART decoder layer where hfirst plays self-
attention on Hthen performs cross-attention
onC. The probability p(r|r, z= 0) is de-
fined as a multinomial distribution parameterized
bySoftmax (f(h)), where hencodes the
generated tokens up to timestep t−1. When z= 1,
the implementation of decoder layer is analogous
to the z= 0case except that we replace Cwith
K, since knowledge is needed:
h=DecoderLayer (h,H,K).(5)
To generate a segment with a particular cus-
tomized style when z≥2, we introduce some
adapters to bias the generation efficiently following
Houlsby et al. (2019). Specifically, the hidden state
his defined as:
h=DecoderLayer(h,H,K),(6)
where DecoderLayer(·,·,·)denotes the trans-
former decoder layer with adapters inserted. To
make the style fine-grained and adjustable, each
style has a unique set of adapters. Different styles
have no adapter in common. In addition, our model
has the ability to learn to express in any style, as
long as a discriminator for the desired style is pro-
vided.
3.3 Learning Details
We introduce auxiliary distributions q(M|R) =/producttextq(m|R)and q(Z|M, R ) =/producttextq(z|M, R ), which serve as an ap-
proximation to the intractable posterior of the
boundary indicator Mand the module indicator
Z. We then apply variational approximation which
gives the following evidence lower bound objective(ELBO) (Hoffman et al., 2013):where p(z)and p(m)stand for
p(z|r, z, m) and p(m|r, z)
respectively, and D(·∥·))refers to Kull-
back–Leibler divergence. Detailed derivations are
presented in the appendix.
Based on the intuition that the response provides
hints about the segmentation, we construct the pos-
terior distribution q(m|R)as a Bernoulli dis-
tribution parameterized by σ(f(ψ)).ψis
a feature extracted from a bi-directional LSTM
ψ(R). Since the module indicator is kept un-
changed within a segment, the posterior distribu-
tionq(z|M, R )is conditioned on the boundary
indicator mand defined as:
q(z|M, R ) =m·˜q(z|ψ)
+ (1−m)·δ(z=z),
(8)
where δ(·)denotes Dirac delta function and
the transition function ˜q(z|ψ)is implemented
as a multinomial distribution parameterized by
Softmax (f(ψ)). Once we have the poste-
rior distribution, we apply Gumbel-Softmax (Jang
et al., 2016) to take samples of mandz.
Weak Supervision on M and Z. We first use
StanfordNLP toolkit (Manning et al., 2014) to parse
every response in the training set as a sequence of
segments, and use ˜M={˜m}to denote the
results of segmentation labeling. The pseudo la-
bel of module choice ˜Z={z}is tagged in a
similar way to multiclass classification, determined
by (1) the similarity between each segment and
knowledge and (2) the classification confidence of
the style discriminator. More details about the con-
struction of ˜Zand˜Mare provided in the appendix.
With ˜Zand˜M, the loss function of weak super-
vision is defined as:
L=−/summationdisplaylogp( ˜m|r,˜z),
L=−/summationdisplay˜m·logp(˜z|r,˜z,˜m).
(9)
The learning algorithm is summarized and provided
in the appendix.
4 Experiments
4.1 Datasets
We test our model on benchmarks of knowledge-
grounded dialogue generation, including Wizard of2262
Wikipedia (Wizard) (Dinan et al., 2019) and CMU
Document Grounded Conversations (CMU _DoG)
(Zhou et al., 2018c). We choose the Reddit Corpus
published by (Li et al., 2020) as Dfor pre-training.
Since it is abundant in expression style as a corpus
from online forum, the two latent variables could
be well initialized. We use part of the training data
of Wizard and CMU _DoG as Drespectively,
for these two datasets are distinctive in expression
style and differ from each other. The dialogues
in CMU _DoG tend to be causal and short, with
most utterances irrelevant to knowledge while the
responses in Wizard are usually long and knowl-
edgeable, as some phrases are directly extracted
from wiki articles.
More details about the datasets are described in
the appendix.
4.2 Experimental Setup
In this paper, we mainly consider two experimental
setups, corresponding to the two aspects of knowl-
edge expression. To explore how our model can be
used to control the distribution of different kinds
of segments (knowledge-related and knowledge-
irrelevant), we first train the model on the Reddit
Corpus and then fine-tune it on a small amount
of examples in Wizard and CMU _DoG, respec-
tively.To verify whether our model can generate
the knowledge-related segments in the desired style,
we still train the model on the Reddit Corpus, and
use a style tag to control the generation process.
In this experimental setup, we are primarily con-
cerned with generating with two kinds of styles,
positive and negative, where z= 2·min(1 , z)
tells the model to generate a response in positive
sentiment and z= 3·min(1 , z)is for response
in negative sentiment.
Evaluation Metrics. Following Dinan et al.
(2019), we choose PPL and unigram F1 as themetrics to evaluate the appropriateness. We fur-
ther use Distinct- 1/2(D-1/2), which are calcu-
lated as ratios of distinct unigrams and bigrams in
responses respectively, to evaluate the distinctness.
We also employ classification accuracy as the evalu-
ation metrics for style control experiments.Due to
space limitation, we provide automatic evaluation
results on more metrics (i.e., BLEU- 1, METEOR,
and ROUGE-L) in the appendix.
To further verify whether our model could learn
structure style and content style, we randomly sam-
ple300examples from Test Seen of Wizard, and
the test set of CMU _DoG respectively, and recruit
6well-educated native speakers to do qualitative
analysis on the responses generated by our model
and all baselines. The annotators need to judge
the quality of the responses from four aspects (i.e.,
fluency ,context coherence ,knowledge relevance
andstyle consistency ), and assign a score from
{0,1,2}(representing “bad”, “fair” and “good” re-
spectively) to each response for each aspect. The
agreement among all annotators is measured via
Fleiss’ kappa (Fleiss, 1971). More details about the
setup of human evaluation as well as the results on
learning content style are provided in the appendix.
4.3 Baselines
For the exploration of structure style, we select the
following models as baselines: (1) BART (Lewis
et al., 2020): a model that achieves state-of-the-
art performance on various text generation tasks.
Note that our model degrades into BART once we
remove the module indicator Z and the boundary in-
dicator M; (2) Zero-resource Knowledge-grounded
Conversation (ZRKGC) (Li et al., 2020): a model
that is based on UniLM (Dong et al., 2019) and
optimized with Generalized EM method.
For the content style, we consider the follow-
ing models as baselines: (1) Emotional Chatting2263
Machine (ECM) (Zhou et al., 2018a): a model
which can generate appropriate responses not only
content-relevant but also emotional consistent; (2)
variant of DialoGPT (Zhang et al., 2019b): we add
a sentiment indicating token at the first of the se-
quence and explore whether such simple heuristics
works for controlling knowledge expression; (3)
CTRL (Keskar et al., 2019): a large-scale model
trained on conditional codes to govern the style and
content of generation.
Our model and all baselines are trained on the
identical Reddit Corpus to maintain fairness.
4.4 Results on Learning Structure Style
In this section, we demonstrate the effectiveness of
our segmentation-based generation framework in
both low-resource setting and zero-resource setting
and empirically verify that our model can learn
structure style with a few annotated examples.
In zero-resource setting, we trained our model on
the Reddit Corpus published by Li et al. (2020) and
tested on Wizard and CMU _DoG respectively. Au-
tomatic evaluation results are shown in Table 2. It
could be observed that: (1) our model significantly
outperforms ZRKGC and BART on most metrics
and achieves the new state-of-the-art performance
on Wizard. It is impressive that our model exceeds
BART in CMU _DoG especially since the proposed
model degrades into BART without two sequen-
tial latent variables Z and M. The result serves as
strong evidence for the effect of two latent vari-
ables, which enable the model to learn complex
expression style in Reddit Corpus to handle flexible
expression in CMU _DoG. By contrast, BART is
far from satisfying with only a regular decoder. (2)
our model exceeds ZRKGC significantly in terms
of Distinct metrics, for ZRKGC mainly focuses on
leveraging external knowledge sources for response
generation, but falls short on expression diversity.
In the low-resource setting, after training our model
on the Reddit Corpus, we then fine-tune it with
only10% training size of Wizard and CMU _DoGrespectively (i.e., Din Sec 3.1) to adjust p(z)
andp(m)to a new structure style. When provided
with only 10% training data, our model gets obvi-
ous improvement ( ∼1%increase in F1) in contrast
with BART ( ∼0.5%increase in F1) and ZRKGC
(∼0.2%increase in F1), proving that the proposed
model can learn more sophisticated structure style
through quick adjustment on a specific dataset with
little cost.
Human Evaluation. Table 3 shows human eval-
uation results on learning structure style. It could
be observed that: (1) our model is significantly
superior to others on style consistency , indicating
that the model can learn a consistent expression
style with very little data. (2) our model has better
performance on context coherence andknowledge
relevance , tallying with its impressive performance
in the low-resource scenario.
Fine-tuning with Limited Annotated Data. We
first train the model on the Reddit Corpus and then
fine-tune it with the amount of annotated data (e.g.,
Wizard and CMU _DoG) gradually increasing from
2%to10%. To have a more intuitive understand-
ing of the effects of latent variables Z and M, we
compare the proposed model with BART, which
generates the response with a single decoder. The
evaluation results are shown in Figure 2. It can be
concluded from the result that: (1) our model can
learn the expression style of a particular dataset
more efficiently. As the training data increases, our
model has a more significant improvement in terms
of the F1 metric; (2) our model performs better
in meager resources since there is a considerable
gap between our model and BART when the train-
ing data is close to 0%; (3) the expression style of
CMU _DoG can be learned with less data because
the model has a significant change in performance
after using 2%CMU_DoG training data.
Refashioning of Knowledge-related Segments.
To know how our model adjusts to different2264
datasets, we compare the knowledge-related seg-
ments before and after trained with annotated data
from two aspects: (1) the average proportion of
knowledge-related segments ( pklg) in a sentence;
(2) the average proportion of words belonging to
knowledge-related segments ( lklg). The motiva-
tion behind is that the frequency and length of
these two kinds of segments generally indicates
how well the latent variable is learned to capture
the knowledge expression structure. We identify
these two kinds of segmentation by comparing their
lexical similarities with the knowledge. Figure 3
reports the results. The results indicate that our
model could learn the underlying structure style
of both datasets, with the great difference of pklg
andlklg before and after fine-tuning as evidence.
After fine-tuning with Wizard data, pklg drops to
0.26while the lklg grows up a bit, indicating that
the knowledge-related segments generated by our
model are fewer and longer, which tallies with the
fact that the responses in Wizard are probably di-
rectly copied from background knowledge. How-ever, after CMU _DoG data is fed to the model,
bothpklg andlklg shrink drastically, which agrees
with the fact that crowd-sourcing workers converse
more liberally online and the responses are less
relevant to the background knowledge.
4.5 Results on Learning Content Style
We further investigate whether the proposed model
could express knowledge with the desired senti-
ment. Specifically, we introduce two sets of style
adapters to endow knowledge expression in two
different sentiments, namely positive and negative.
So in this scenario, it is required that responses
are not only coherent with context but also lim-
ited in positive or negative sentiment. To apply
ECM on knowledge-grounded conversation, we la-
bel the sentiment category for each response with
a classifier pre-trained on the SST (Socher et al.,
2013) training set. For DialoGPT, we similarly
annotate each response with a sentiment category
and append the sentiment token before the context
tokens. The evaluation results are shown in Table
4. We can conclude that: (1) The proposed model
outperforms all baseline models in terms of all met-
rics, which indicates that our model can control the
sentiment of knowledge expression and guarantee
high quality of the generated responses; (2) Simply
adding a sentiment indicating token at the begin-
ning of the sequence can not effectively control
the style of knowledge expression, as the perfor-
mance of DialoGPT on sentiment control is poor;
(3) Although ECM is designed for sentiment con-2265trol, it still fails to perform well in this task, proving
that sentiment control in the knowledge-grounded
conversation is rather difficult. Besides, ECM can
only control the sentiment of the whole response
but is helpless to manage every knowledge-related
segment at a fine-grained level.
5 Conclusions
We explore knowledge expression in knowledge-
grounded conversation and break down the expres-
sion style of a response into the structure of the
response (structure style) and the style of the con-
tent in each part (content style). We propose a
variational segmentation-based generation model
to discover the underlying expression style in re-
sponse. Specifically, we introduce two latent vari-
ables to model these two aspects of expression
style respectively and induce an evidence lower
bound of the likelihood. Evaluation results on two
benchmarks of the task indicate that our model can
learn the structure style with little cost and gener-
ate responses in desired content style without any
human-annotated data.
Ethical Considerations
It’s crucial for an open-domain dialogue system
to be able to automatically detect and discover the
underlying structural pattern of a sentence. With
the ability to handle a variety of expression styles,
whether positive or negative, serious or casual, our
work suggests that we are getting closer to the goal
of creating an artificial intelligent dialogue system
that can freely communicate with humans, which
is beyond the wildest dreams of most AI and NLP
researchers. However, a detailed survey should be
undertaken in advance to consider the immediate
audience’s and developers’ interests, as well as any
potential stakeholder groups.
Furthermore, knowledge-grounded dialogue sys-
tems have the potential to fabricate facts and dis-
tribute rumors and false information, particularly
when the source of external background knowl-
edge is unreliable. If the knowledge candidate set
is contaminated by fake news, the response gener-
ated by the dialogue system is likely to suffer from
the “hallucination” issue. Controlling the source
of knowledge sentences, such as paragraphs ex-
tracted from the wiki, authoritative news sites, or
authoritative product documents, is a necessary and
practical strategy.Acknowledgement
Thanks for the anonymous reviewers for their con-
structive comments. This work was supported
by the National Key Research and Development
Program of China (No. 2020AAA0106600), Na-
tional Natural Science Foundation of China (NSFC
Grant No. 62122089 and No. 61876196), and
Beijing Outstanding Young Scientist Program (No.
BJJWZYJH012019100020098). Rui Yan is also
supported by Beijing Academy of Artificial Intelli-
gence (BAAI).
References226622672268A Appendix
A.1 Derivation of ELBO
According to the mean-filed approxima-
tion, q(M, Z )≈ q(M)q(Z). There-
fore, Elogp(R|M, Z ) and
E/parenleftbig
logq(M, Z|R)−logp(M, Z )/parenrightbig
can be re-written as:
A.2 Details about the Construction of ˜Mand
˜Z
In this section, we provide more details about of
construction of ˜Mand˜Z. For every response in
the training set, we parse it as a syntax tree using
StanfordNLP toolkit (Manning et al., 2014). Thesyntax tree we obtain is in a hierarchical and nested
structure. The root node of the tree represents the
whole response sentence and the root node of every
subtree represents a corresponding phrase, a small
part of a sentence. For example, if a phrase could be
divided into three parts, then the node representing
the phrase has three child nodes and each represents
a part of the phrase. After we acquire the parsing
tree, segmentation is then carried out recursively.
To be concrete, we traverse the parsing tree by deep-
first search order. Every time we arrive at a node,
compute the similaritybetween the knowledge
and the phrase represented by the node. If the
similarity is above the threshold µ, we mark
the phrase as a segment and search in this branch
terminates. Else we continue to search the child
nodes of the current node to segment at a more
refined level. We use ˜M={˜m}to denote the
results of segmentation labeling.
The pseudo label of module choice ˜Z={z}
is tagged in a similar way to multiclass classifica-
tion. Specifically, for a segment (r,···, r)where
sandeare the start and end position of a segment
respectively. If the similarity between this segment
and the knowledge falls below a threshold µ, its
pseudo label (z,···, z)will be set to 0. Other-
wise we send the segment to a series of style dis-
criminators one after another until the classification
confidence given by a discriminator is above µ
and pseudo module choice label will be set to i+ 1.
If all discriminators fail to classify the segment at a
confidence greater than µ,(z,···, z)are all 1,
indicating knowledge should be expressed without
particular style.
A.3 Learning Algorithm
The learning algorithm is summarized in Algorithm
1.
A.4 Details of Datasets
Training Data. We choose the Reddit Corpus
published by (Li et al., 2020) as Dfor pre-training.
The data contains 842,521 context-knowledge-
response triples for training and 2,737 context-
knowledge-response triples for validation. On aver-
age, each dialogue contains 3.1utterances in both
sets, and the average length of the utterance is 16.0
in training and is 16.1in validation.
Evaluation Data. We test our model on bench-
marks of knowledge-grounded dialogue generation,2269Algorithm 1 Learning Algorithm
including Wizard of Wikipedia (Wizard) (Dinan
et al., 2019) and CMU Document Grounded Con-
versations (CMU _DoG) (Zhou et al., 2018c). Both
datasets are split into training sets, validation sets,
and test sets by the data owners. We follow Di-
nan et al. (2019) and conduct the pre-processing
with the code published on ParlAI. Topics in Wiz-
ard cover a wide range ( 1,365in total), and each
conversation happens between a wizard who has
access to the knowledge about a specific topic and
an apprentice who is just eager to learn from the
wizard about the topic. The test set is split into two
subsets. Test Seen only contains dialogues with
topics that have already appeared in the training
set, while topics in Test Unseen never appear in the
training set and the validation set. Different from
Wizard, CMU _DoG focuses on movie domain, and
besides wizard-apprentice conversations, the data
also contain conversations between two workers
who know the document and try to discuss the con-
tent in depth. In both datasets, only the turns where
knowledge is accessible are considered in response
generation. Table 5 reports the statistics of the
Wizard data and the CMU_DoG data
A.5 More Implementation Details
We employ a knowledge selection (KS) module
to select the top 7related sentences in knowledge.
The KS module is implemented based on Roberta-
base (125M) and trained on the Reddit Corpus.
Specifically, we treat the sentence which has thehighest F1 score with the response as the positive
sample, and the negative sample is randomly sam-
pled from all the other knowledge sentences. We
train the KS module via maximum likelihood es-
timation (MLE) with a batch size of 64and an
initial learning rate of 1e−5. The threshold µ,
µ,µandµin weak supervision are set
as0.9,0.5,0.8and0.8, respectively. The encoder-
decoder architecture is implemented on the basis
of Bart-base (139M) and trained on the Reddit Cor-
pus with a batch size of 64and an initial learning
rate of 5e−6. The parameters for prior and pos-
terior distributions of ZandM(i.e., θ,θ,ϕ
andϕ) are initialized randomly, and optimized
with a learning rate of 1e−4. The parameters for
adapters are initialized randomly and optimized
with a learning rate of 2e−3. We only train the
adapters for the first 1000 steps. We utilize gated
recurrent units (GRUs) as the basic units in f.
We set the hidden size and the number of layers of
RNN in our model (i.e., fandψ(·)) as128
and1respectively. The embedding size for Zis set
as128and the adapter size is set as 64. When fine-
tuning the model on the Wizard and CMU _DoG
datasets, the learning rate and the batch size are
set as 5e−5and32respectively. We employ
greedy search in response decoding. All models
are learned with Adam (Kingma and Ba, 2015)
optimizer with β= 0.9andβ= 0.999. We
increase the learning rate linearly for the first 2002270
steps and decrease it thereafter proportionally to
the inverse square root of the step number. Early
stopping on validation is adopted as a regulariza-
tion strategy. All models are trained on a 8×RTX
2080 Ti machine.
A.6 More Results about Automatic
Evaluation
Table 6 reports more results about the automatic
evaluation, from which we can see that our model
still outperforms the baselines.
A.7 Human Evaluation
We randomly sample 300examples from Test Seen
of Wizard, and the test set of CMU _DoG respec-
tively, and recruit 6well-educated native speakers
to do qualitative analysis on the responses gener-
ated by our model and all baselines. For each of
the300examples, an annotator is provided with
the context, the ground-truth knowledge, model
responses and the associated style types. For eval-
uation of structure style, we defined two kinds of
structure styles based on two datasets, namely the
Wizard-like style S and the CMU _DoG-like
style S . While for evaluation of content
style, we roughly divide content styles in two cat-
egories, SandSfor convenience. The re-
sponses provided by different models are randomly
shuffled to hide their sources. The annotators need
to judge the quality of the responses from four as-
pects: (1) fluency : whether the response is fluent
without any grammatical errors; (2) context coher-
ence: whether the response is coherent with the
context; (3) knowledge relevance : whether the re-sponse is relevant with the knowledge; and (4) style
consistency : whether the response exhibits the de-
sired style. Each annotator assigns a score from
{0,1,2}(representing “bad”, “fair” and “good” re-
spectively) to each response for each aspect. Each
response obtains four scores for aforementioned
four aspects, and the agreement among all annota-
tors is measured via Fleiss’ kappa (Fleiss, 1971).
Results on Learning content style. Table 7 re-
ports the human evaluation results on learning con-
tent style. The three models are trained on the
Reddit Corpus. We can conclude that: (1) by in-
troducing two latent variables and a number of
adapters for different styles, our model can gen-
erate responses in desired content style (i.e., S
andS) more accurately and achieve significant
improvement on style consistency , which is con-
sistent with the results in Table 4; (2) our model
also outperforms ECM and DialoGPT on fluency ,
context coherency andknowledge relevance thanks
to the capacity of large-scale pre-trained language
models and the introduction of external knowledge
respectively.
A.8 Comparison with More Baselines
We compare with models trained on full training
data, and Table 8 shows the evaluation results.
First, it is noted that our model outperforms Knowl-
edGPT in terms of F1 by using only 10% training
dataon CMU _DoG, which provides a strong sup-
port for the effectiveness of the proposed model.2271
Second, by adjusting the structure style on a small
amount of data, the gap between our model and
KnowledGPT is further narrowed, while the im-
provements on ZRKGC and BART are trivial.
A.9 Ablation over Weak Supervision
To have more insights into the impact of weak su-
pervision on the performance of our model, we
compare the proposed model with the following
variants: (1) -weak supervision on Z : the weak su-
pervision on module indicator Z is removed; (2) -
weak supervision on Z and M : the weak supervision
on module indicator and boundary indicator is re-
moved. Table 9 reports the evaluation results. We
can conclude that (1) the weak supervision objec-
tives significantly improve model performance; (2)
the weak supervision objectives play a more cru-
cial role on CMU _DoG, as removing them causes
a dramatic drop in performance. The reason is
that this dataset has more sophisticated expression
styles and it is difficult to learn these styles without
auxiliary supervision signals.
A.10 Ablation over Boundary Indicator
Since the module indicator is conditioned on the
boundary indicator, we are curious about what will
happen if the Mis removed. The ablation result
is shown on Figure 4. There is an evident drop
on Wizard Seen and Wizard Unseen, verifying the
effect of boundary indicator in assisting the mod-
ule indicator. The margin is tiny on CMU_DoG,
perhaps because its structure feature is easier to
capture, so the module indicator could works prop-
erly itself.
A.11 Case Study
This section mainly studies how different models
vary in knowledge expression for the same context
and background knowledge. Table 10 shows an
example from the test set of CMU _DoG. This ex-
ample contains the background knowledge which
gives a plot from the movie, and the dialogue con-
text which is generated by discussing the content in
the knowledge. We choose the following four mod-
els to generate the response in corresponding style
given the dialogue context and knowledge, and all
models are pre-trained with the Reddit Corpus: (1)
Wizard Model for S : the model fine-tuned
with10% training data in Wizard; (2) CMU _DoG
Model for S : the model fine-tuned with 10%
training data in CMU _DoG; (3) Positive Model for
S: the model forced to express knowledge with2272
positive sentiment; (4) Negative Model for S:
the model forced to express knowledge with neg-
ative sentiment. We can see that the knowlege ex-
pression style of the Wizard Model and CMU _DoG
Model are quite different. The central part of the
Wizard Model response is copied from the back-
ground knowledge, which is consistent with the
style of Wizard data. The response generated by
CMU _DoG Model is more casual in knowledge
expression, and the content is mainly related to the
conversation context. Besides, responses generated
by the Positive Model exhibit evident positive sen-
timent, while responses generated by the Negative
Model show relatively negative sentiment.2273