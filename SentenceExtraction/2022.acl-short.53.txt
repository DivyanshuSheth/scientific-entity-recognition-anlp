
Jannis VamvasandRico SennrichDepartment of Computational Linguistics, University of ZurichSchool of Informatics, University of Edinburgh
{vamvas,sennrich}@cl.uzh.ch
Abstract
Omission and addition of content is a typi-
cal issue in neural machine translation. We
propose a method for detecting such phenom-
ena with off-the-shelf translation models. Us-
ing contrastive conditioning, we compare the
likelihood of a full sequence under a transla-
tion model to the likelihood of its parts, given
the corresponding source or target sequence.
This allows to pinpoint superﬂuous words in
the translation and untranslated words in the
source even in the absence of a reference trans-
lation. The accuracy of our method is com-
parable to a supervised method that requires a
custom quality estimation model.
1 Introduction
Neural machine translation (NMT) is susceptible to
coverage errors such as the addition of superﬂuous
target words or the omission of important source
content. Previous approaches to detecting such
errors make use of reference translations (Yang
et al., 2018) or employ a separate quality estima-
tion (QE) model trained on synthetic data for a
language pair (Tuan et al., 2021; Zhou et al., 2021).
In this paper, we propose a reference-free al-
gorithm based on hypothetical reasoning. Our
premise is that a translation has optimal coverage if
it uses as little information as possible and as much
information as necessary to convey the source se-
quence. Therefore, an addition error means that the
source would be better conveyed by a translation
containing less information. Conversely, an omis-
sion error means that the translation would be more
adequate for a less informative source sequence.
Adapting our contrastive conditioning ap-
proach (Vamvas and Sennrich, 2021), we use prob-
ability scores of NMT models to approximate this
concept of coverage. We create parse trees for both
the source sequence and the translation, and treat
their constituents as units of information. Omis-
sion errors are detected by systematically deletingconstituents from the source and by estimating the
probability of the translation conditioned on such a
partial source sequence. If the probability score is
higher than when the translation is conditioned on
the full source, the deleted constituent might have
no counterpart in the translation (Figure 1). We
apply the same principle to the detection of addi-
tion errors by swapping the source and the target
sequence.
When comparing the detected errors to human
annotations of coverage errors on the segment
level (Freitag et al., 2021), our approach surpasses
a supervised QE baseline that was trained on a large
number of synthetic coverage errors. Human raters
ﬁnd that word-level precision is higher for omis-
sions than additions, with 39% of predicted error
spans being precise for English–German transla-
tions, and 20% for Chinese–English. False positive
predictions can occur especially in cases where the
translation has different syntax than the source. We
believe our algorithm could be a useful aid when-
ever humans remain in the loop, for example in a
post-editing workﬂow.
We release the code and data to reproduce our
ﬁndings, including a large-scale dataset of syn-
thetic coverage errors in English–German and Chi-
nese–English machine translations.
2 Related Work
Coverage errors in NMT Addition and omis-
sion of target words have been observed by human
evaluation studies in various languages, with omis-
sion as the more frequent error type (Castilho et al.,
2017; Zheng et al., 2018). They are included as
typical translation issues in the Multidimensional
Quality Metrics (MQM) framework (Lommel et al.,
2014). Addition is deﬁned as an accuracy issue
where the target text includes text not present in
the source, and omission is deﬁned as an accuracy490
issue where content is missing from the translation
but is present in the source.
Freitag et al. (2021) used MQM to manually
re-annotate English–German and Chinese–English
machine translations submitted to the WMT 2020
news translation task (Barrault et al., 2020). Their
ﬁndings conﬁrm that state-of-the-art NMT systems
still erroneously add and omit target words, and that
omission occurs more often than addition. Simi-
lar patterns can be found in English–French ma-
chine translations that have been annotated with
ﬁne-grained MQM labels for the document-level
QE shared task (Specia et al., 2018; Fonseca et al.,
2019; Specia et al., 2020).
Detecting and reducing coverage errors While
reference-based approaches include measuring the
n-gram overlap to the reference (Yang et al., 2018)
and analyzing word alignment to the source (Kong
et al., 2019), this work focuses on the reference-free
detection of coverage errors.
Previous work has employed custom QE models
trained on labeled parallel data. For example, Zhou
et al. (2021) insert synthetic hallucinations and
train a Transformer to predict the inserted spans.
Similarly, Tuan et al. (2021) train a QE model on
synthetically noisy translations. In this paper, we
propose a method that is based on off-the-shelf
NMT models only.
Other related work has focused on improving
coverage during decoding or training, for example
via attention (Tu et al., 2016; Wu et al., 2016; Li
et al., 2018; among others). More recently, Yang
et al. (2019) found that contrastive ﬁne-tuning onreferences with synthetic omissions reduces cover-
age errors produced by an NMT system.
3 Approach
Contrastive Conditioning Properties of a trans-
lation can be inferred by estimating its probability
conditioned on contrastive source sequences (Vam-
vas and Sennrich, 2021). For example, if a certain
translation is more probable under an NMT model
when conditioned on a counterfactual source se-
quence, the translation might be inadequate.
Application to Omission Errors Figure 1 illus-
trates how contrastive conditioning can be directly
applied to the detection of omission errors. We con-
struct partial source sequences by systematically
deleting constituents from the source. If the prob-
ability score of the translation (average token log-
probability) is higher when conditioned on such a
partial source, the deleted constituent is taken to be
missing from the translation.
To compute the probability score for a transla-
tionYgiven a source sequence X, we sum up the
log-probabilities for every target token and normal-
ize the sum by the number of target tokens:
score (YjX) =1
jYjlogp(yjX; y)
Application to Addition Errors We apply the
same method to addition detection, but swap the
source and target languages. Namely, we use an
NMT model for the reverse translation direction,
and we score the source sequence conditioned on
the full translation and a set of partial translations.491Potential Error Spans In its most basic form,
our algorithm does not require any linguistic re-
sources apart from tokenization. For a source sen-
tence of ntokens one could create npartial source
sequences with the ith token deleted. However,
such an approach would rely on a radical assump-
tion of compositionality, treating all tokens as inde-
pendent constituents.
We thus propose to extract potential error
spans from parse trees, speciﬁcally from depen-
dency trees predicted by Universal Dependency
parsers (de Marneffe et al., 2021), which are widely
available. This allows (a) to skip function words
and (b) to include a reasonable number of multi-
word spans in the set of potential error spans. For-
mally, we consider word spans that satisfy the fol-
lowing conditions:
1.A potential error span is a complete subtree
of the dependency tree.
2. It covers a contiguous subsequence.
3. It contains a part of speech of interest.
For every potential error span, we create a partial
sequence by deleting the span from the original
sequence. This is still a simpliﬁed notion of con-
stituency, since some partial sequences will be un-
grammatical. Our assumption is that NMT models
can produce reliable probability estimates despite
the ungrammatical input.
4 Experimental Setup
In this section we describe the data and tools that
we use to implement and evaluate our approach.
Scoring model We use mBART50 (Tang et al.,
2021), which is a sequence-to-sequence Trans-
former pre-trained on monolingual corpora in many
languages using the BART objective (Lewis et al.,
2020; Liu et al., 2020) that was ﬁne-tuned on
English-centric multilingual MT in 50 languages.
Sequence-level probability scores are computed by
averaging the log-probabilities of all target tokens.
We use the one-to-many mBART50 model if En-
glish is the source language, and the many-to-one
model if English is the target language.
Error spans We use Stanza (Qi et al., 2020) for
dependency parsing, a neural pipeline for various
languages trained on data from Universal Depen-
dencies (de Marneffe et al., 2021). We make use
of universal part-of-speech tags (UPOS) to deﬁneOriginal source
Partial sourceFull translation
Partial translationtranslateDelete random
constituentsCheck addition
propertytranslate
parts of speech that might constitute potential error
spans. Speciﬁcally, we treat common nouns, proper
nouns, main verbs, adjectives, numerals, adverbs,
and interjections as relevant parts of speech.
Gold Standard Data We use state-of-the-art En-
glish–German and Chinese–English machine trans-
lations for evaluation, which have been annotated
by Freitag et al. (2021) with translation errors.We
set aside translations by the system Online-B as
a development set, and use the other systems as
a test set, excluding translations by humans. The
development set was used to identify the typical
parts-of-speech of coverage error spans, listed in
the paragraph above.
Synthetic Data We also create synthetic cover-
age errors, which we use for training a supervised
baseline QE system. We propose a data creation
process that is inspired by previous work (Yang
et al., 2019; Zhou et al., 2021; Tuan et al., 2021)
but is deﬁned such that it works for both additions
and omissions, and produces ﬂuent translations.
Figure 2 illustrates the process. We start from
the original source sentences and create partial
sources by deleting randomly selected constituents.
Speciﬁcally, we delete each constituent with a prob-
ability of 15%. We then machine-translate both the
original and the partial sources, yielding fulland
partial machine translations . We retain only sam-
ples where the full machine translation is different
from the partial one, and can be constructed by
addition.
This allows us to treat the full translations as
overtranslations of the partial sources, and the
added words as addition errors. Conversely, the
partial translations are treated as undertranslations
of the original sources. Negative examples are cre-492Approach Detection of additions Detection of omissions
Precision Recall F1 Precision Recall F1
EN–DESupervised baseline 6.9 1.9 2.90.9 4.01.3 40.35.2 6.10.1 10.60.2
Our approach 4.0 15.0 6.3 22.3 18.8 20.4
ZH–ENSupervised baseline 4.3 0.6 4.70.7 4.50.6 49.60.6 9.41.0 15.91.4
Our approach 1.7 40.6 3.4 25.8 62.0 36.5
ated by pairing the original sources with the full
translations, and the partial sources with the partial
translations.
Our synthetic data are based on monolingual
news text released for WMT.To train the baseline
system, we use 80k unique source segments per
language pair. Statistics are reported in Table A3.
Supervised baseline system Following the ap-
proach outlined by Moura et al. (2020), we use
the OpenKiwi framework (Kepler et al., 2019) to
train a separate Predictor-Estimator model (Kim
et al., 2017) per language pair, based on XLM-
RoBERTa (Conneau et al., 2020). The supervised
task can be described as token-level binary classi-
ﬁcation. Every token is classiﬁed as either OKor
BAD, similar to the word-level labels used for the
QE shared tasks (Specia et al., 2020). A source
token is BAD if it is omitted in the translation, and
a token in the translation is BAD if it is part of an
addition error. For English and German, we use the
Moses tokenizer (Koehn et al., 2007) to separate
the text into labeled tokens; for Chinese we label
the text on the character level.
Where suitable, we use the default settings of
OpenKiwi. We ﬁne-tune the large version of XLM-
RoBERTa, which results in a model of similar pa-
rameter count as the mBART50 model we use for
contrastive conditioning. We train for 10 epochs
with a batch size of 32, with early stopping on the
validation set. For token classiﬁcation we train
two linear layers, separately for source and target
language (which corresponds to omissions and ad-
ditions, respectively). We use AdamW (Loshchilov
and Hutter, 2019) with a learning rate of 1e-5, freez-
ing the pretrained encoder for the ﬁrst 1000 steps.5 Evaluation
5.1 Segment-Level Comparison to Gold Data
The accuracy of our approach can be estimated
based on the human ratings by Freitag et al. (2021).
Evaluation Design We use the MQM error types
Accuracy/Addition andAccuracy/Omission , and ig-
nore other types such as Accuracy/Mistranslation .
We count a prediction as correct if any one of the
human raters has marked the same error type any-
where in the segment.We exclude segments from
the evaluation that might have been incompletely
annotated (because raters stopped after marking
ﬁve errors). For ease of implementation, we also ex-
clude segments that consist of multiple sentences.
Results The results of the gold-standard compar-
ison are shown in Table 1. Our approach clearly
surpasses the baseline in the detection of omis-
sion errors in both language pairs. However, both
approaches recognize addition errors with low ac-
curacy, and especially the supervised baseline has
low recall. Considering its high performance on
a synthetic test set (Table A1 in the Appendix), it
seems that the model does not generalize well to
real-world coverage errors, highlighting the chal-
lenges of training a supervised QE model on purely
synthetic data.
5.2 Human Evaluation of Precision
We perform an additional word-level human eval-
uation to analyze the predictions obtained via our
approach in more detail. Our human raters were
presented segments that had been marked as true
or false positives in the above evaluation, allowing
us to quantify word-level precision.493EN–DE ZH–EN
TargetAddition errors 2.3 1.2
Any errors 7.4 12.0
SourceOmission errors 36.3 13.8
Any errors 39.4 19.5
Evaluation Design We employed two linguistic
experts per language pair as raters.Each rater
was shown around 700 randomly sampled positive
predictions across both types of coverage errors.
Raters were shown the source sequence, the
machine translation, and the predicted error span.
They were asked whether the highlighted span was
indeed translated badly, and were asked to perform
a ﬁne-grained analysis based on a list of predeﬁned
answer options (Figures 3 and 4 in the Appendix).
A part of the samples were annotated by both
raters. The agreement was moderate for the
main question, with a Cohen’s kappa of 0.54 for
English–German and 0.45 for Chinese–English.
Agreement on the more subjective follow-up ques-
tion was lower (0.32 / 0.13).
Results The ﬁne-grained answers allow us to
quantify the word-level precision of the spans high-
lighted by our approach, both with respect to cover-
age errors in particular and to translation errors in
general (Table 2). Precision is higher than expected
when detecting omission errors in English–German
translations, but is still low for additions. The dis-
tribution of the detailed answers (Figures 3 and 4 in
the Appendix) suggests that syntactical differences
between the source and target language contribute
to the false positives regarding additions. Example
predictions are provided in Appendix F, which in-
clude cases where all three raters of Freitag et al.
(2021) had overlooked the coverage error.
Finally, Table 2 shows that many of the predicted
error spans are in fact translation errors, but not cov-
erage errors in a narrow sense. For example, more
than 10% of the spans marked in Chinese–English
translations were classiﬁed by our raters as a differ-
ent type of accuracy error, such as mistranslation.6 Limitations and Future Work
We hope that the automatic detection of cover-
age errors could be an aid to translators and post-
editors, given that manually detecting such errors is
tedious. Our results on omissions are encouraging,
and user studies are recommended in order to vali-
date the usefulness of the predictions to practition-
ers. Further work needs to be done to improve the
detection of additions, of which the real-world data
contain few examples. Higher accuracy would be
necessary for word-level QE to be helpful (Shenoy
et al., 2021), and so with regard to detecting addi-
tion errors, the practical utility of both the baseline
and of our approach remains limited.
Inference time should also be discussed. In Ap-
pendix C we perform a comparison, ﬁnding that on
a long sentence pair contrastive conditioning can
take up to ten times longer than a forward pass of
the baseline. However, this is still a fraction of the
time needed for generating a translation in the ﬁrst
place. In addition, restricting the potential error
spans that are considered could further improve
efﬁciency.
7 Conclusion
We have proposed a reference-free method to au-
tomatically detect coverage errors in translations.
Derived from contrastive conditioning, our method
relies on hypothetical reasoning over the likelihood
of partial sequences. Since any off-the-shelf NMT
model can be used to estimate conditional likeli-
hood, no access to the original translation system
or to a quality estimation model is needed. Evalu-
ation on real machine translations shows that our
approach outperforms a supervised baseline in the
detection of omissions. Future work could address
the low precision on addition errors, which are rel-
atively rare in the datasets we used for evaluation.
Acknowledgments
This work was funded by the Swiss Na-
tional Science Foundation (project MUTAMUR;
no. 176727). We would like to thank Xin Sennrich
for facilitating the recruitment of annotators, and
Chantal Amrhein as well as the anonymous review-
ers for helpful feedback.
References494495
A Annotator Guidelines
You will be shown a series of source sentences
and translations. One or several spans in the text
are highlighted and it is claimed that the spans
are translated badly. You are asked to determine
whether the claim is true. The highlighted spans
can be either in the source sequence or in the trans-
lation. If a span is in the source sentence, check
whether it has been correctly translated. If a span
is in the translation, check whether it correctly con-
veys the source. Sometimes, multiple spans are
highlighted. In that case, focus your answer on
the span that is most problematic for the transla-
tion. In a second step, you are asked to select an
explanation. On the one hand, if you agree that
the highlighted span is translated badly, please ex-
plain your reasoning by selecting your explanation.
On the other hand, if you disagree and think that
the span is well-translated, please select an expla-
nation why the span might have been marked as
badly translated in the ﬁrst place. Should multiple
explanations be equally plausible, select the ﬁrst
from the top.496Detection of additions Detection of omissions
Prec. Recall F1 MCC Prec. Recall F1 MCC
EN–DE
Supervised
Baseline 98.80.4 98.0.298.4.296.8.1 94.01.3 96.60.4 95.3.590.5.2
Ours 78.1 88.3 82.9 76.7 80.9 98.6 88.9 78.1
ZH–EN
Supervised
Baseline 87.21.5 75.7.681.0.372.6.6 67.31.3 68.01.2 67.7.953.8.3
Ours 26.1 88.9 40.4 23.3 28.3 92.0 43.3 40.3
Short sentence pair Long sentence pair
Additions Omissions Both Additions Omissions Both
Supervised baseline - - 25 ms - - 25 ms
Our approach 40 ms 45 ms 83 ms 165 ms 197 ms 365 ms
– excluding parser 18 ms 21 ms 38 ms 102 ms 144 ms 239 ms
B Evaluation on Synthetic Errors
We used a test split held back from the synthetic
data to perform an additional evaluation. On the
segment level, we report Precision, Recall and F1-
score. Like in Section 5.1, a prediction is treated
as correct on the segment level if for a predicted
coverage error there is indeed a coverage error of
that type anywhere in the segment.
On the word level, we follow previous work on
word-level QE (Specia et al., 2020) and report the
Matthews correlation coefﬁcient (MCC) across all
the tokens in the test set.
Results Results are shown in Table A1. The
supervised baseline has a high accuracy on En-
glish–German translations and a moderate accuracy
on Chinese–English translations. In comparison,
our approach performs clearly worse than the su-
pervised baseline on the synthetic errors.C Inference Time
Inference times are reported in Table A2. We mea-
sure the time needed to run the coverage error de-
tection methods on a short sentence pair and on a
long sentence pair for English–German. The short
sentence pair is taken from Figure 1 and the long
sentence pair has 40 tokens in the source sequence
and 47 tokens in the target sequence. We average
over 1000 repetitions on RTX 2080 Ti GPUs.
The higher inference times for our approach can
be explained by the number of translation probabili-
ties that need to be estimated. On average, we com-
pute 30 scores per sentence in the English–German
MQM dataset, and 44 per sentence in the Chi-
nese–English MQM dataset. Still, the time needed
for computing all these scores is only a fraction of
the time it takes to generate a translation (254 ms
for the short source sentence and 861 ms for the
long sentence, assuming a beam size of 5).
The required number of scores could be reduced
by considering fewer potential error spans. Further-
more, scoring could be parallelized across batches
of multiple translations. Finally, using a more ef-
ﬁcient parser, or no parser at all, could speed up
inference.497D Dataset Statistics
Dataset split Number of segments Number of tokens
Total W/ addition W/ omission Src. OK Src.BAD Tgt.OK Tgt.BAD
EN–DE Train 135269 18423 18423 2185918 58378 2197843 53911
EN–DE Dev 16984 2328 2328 273311 7398 275156 6781
EN–DE Test 16984 2328 2328 273277 7701 275036 7032
ZH–EN Train 110195 10697 10697 2576135 62311 1866567 37730
ZH–EN Dev 14149 1383 1383 326743 7562 236685 4244
ZH–EN Test 14026 1342 1342 322000 7566 234757 4882
Dataset split Number of segments
Total With an addition error With an omission error
EN–DE Dev 1418 77 187
EN–DE Test 8508 407 1057
– without excluded segments 4839 162 484
ZH–EN Dev 1999 69 516
ZH–EN Test 13995 329 3360
– without excluded segments 8851 149 1569
E Examples of Synthetic Coverage Errors
English–German Example
Addition error
Partial source: But they haven’t played.
Full machine translation: Aber sie haben nichtgegeneinTeamwieuns gespielt.
Omission error
Full source: But they haven’t playedagainstateamlikeus.
Partial machine translation: Aber sie haben nicht gespielt.
Chinese–English Example
Addition error
Partial source:医院和企业共同研发相关检测试剂盒，惠及更多患者。
Full translation: Hospitals and enterprises jointly develop related test kits to beneﬁt morecancer patients.
Omission error
Full source:医院和企业共同研发相关检测试剂盒，惠及更多肿瘤患者。
Partial translation: Hospitals and enterprises jointly develop related test kits to beneﬁt more patients.498F Examples of Coverage Errors Predicted by Contrastive Conditioning
English–German Examples
Predicted addition error
Source: He added: "It’s backﬁred on him now, though, that’s the sad thing."
Machine translation: Er fügtehinzu : "Es ist jetzt auf ihn abgefeuert, aber das ist das Traurige."
Original MQM rating (Freitag et al., 2021): No related accuracy error marked by the three raters.
Answer by our human rater: The highlighted target span is not translated badly. It might have been
highlighted because it is syntactically different from the source.
Meaning of highlighted span: hinzu = ‘additionally’
Predicted omission error
Source: UK’s medicaldrug supply still uncertain in no-deal Brexit
Machine translation: Die medizinische Versorgung Großbritanniens ist im No-Deal-Brexit noch ungewiss
Original MQM rating: No accuracy error marked by the three raters.
Answer by our human rater: The highlighted source span is indeed translated badly. It contains informa-
tion that is missing in the translation but can be inferred or is trivial.
Predicted omission error
Source: The automaker is expected to report its quarterly vehicle deliveries in the nextfewdays.
Machine translation: Der Autohersteller wird voraussichtlich in den nächsten Tagen seine vierteljährlichen
Fahrzeugauslieferungen melden.
Original MQM rating: No related accuracy error marked by the three raters.
Answer by our human rater: The highlighted source span is not translated badly. The words in the span
do not need to be translated.
Chinese–English Examples
Predicted addition error
Source:美方指责伊朗制造了该袭击，并对伊朗实施新制裁。
Machine translation: The US accused Iran of causing the attack and imposed new sanctionson Iran .
Original MQM rating (Freitag et al., 2021): No related accuracy error marked by the three raters.
Answer by our human rater: The highlighted target span is not translated badly. No phenomenon that
might have caused the prediction was identiﬁed.
Predicted omission error
Source:目前已收到来自俄罗斯农业企业的约50项申请。
Machine translation: About 50 applications have been received from Russian agricultural enterprises.
Original MQM rating: No accuracy error marked by the three raters.
Answer by our human rater: The highlighted source span is indeed translated badly. It contains informa-
tion that is missing in the translation.
Meaning of highlighted span: 目前= ‘at present’
Predicted omission error
Source:他说，该系统目前在世界上有很大需求，但俄罗斯军队也需要它，其中包括在北极地区。
Machine translation: He said that the system is currently in great demand in the world, but the Russian
army also needs it, including in the Arctic.
Original MQM rating: No accuracy error marked by the three raters.
Answer by our human rater: The highlighted source span is not translated badly. The words in the span
do not need to be translated.
Meaning of highlighted span: 其中= ‘among’499G Detailed Results of Human Evaluation500