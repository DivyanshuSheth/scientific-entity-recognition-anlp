
Yuchen Eleanor JiangTianyu LiuShuming Ma
Dongdong ZhangMrinmaya SachanRyan CotterellETH ZürichMicrosoft Research Asia
{yuchen.jiang,tianyu.liu,ryan.cotterell,mrinmaya.sachan}@inf.ethz.ch
{shuming.ma,dongdong.zhang}@microsoft.com
Abstract
Several recent papers claim to have achieved
human parity at sentence-level machine
translation (MT)—especially between high-
resource language pairs. In response, the MT
community has, in part, shifted its focus to
document-level translation. Translating docu-
ments requires a deeper understanding of the
structure and meaning of text, which is often
captured by various kinds of discourse phe-
nomena such as consistency, coherence, and
cohesion. However, this renders conventional
sentence-level MT evaluation benchmarks
inadequate for evaluating the performance
of context-aware MT systems. This paper
presents a new dataset with rich discourse
annotations, built upon the large-scale parallel
corpus introduced in Jiang et al. (2022a).
The new annotation introduces four extra
evaluation aspects, i.e., entity, terminology,
coreference, and quotation, covering 15,095
entity mentions in both languages. Using these
annotations, we systematically investigate
the similarities and differences between the
discourse structures of source and target
languages, and the challenges they pose to
MT. We discover that MT outputs differ
fundamentally from human translations in
terms of their latent discourse structures. This
gives us a new perspective on the challenges
and opportunities in document-level MT. We
make our resource publicly available to spur
future research in document-level MT and its
generalization to other language translation
tasks.https://github.com/EleanorJiang/
BlonDe/tree/main/BWB
1 Introduction
The ﬁeld of machine translation (MT) has made
tremendous strides in recent years thanks to
neural machine translation (NMT) models that
can utilize massive quantities of parallel training
data (Vaswani et al., 2017; Junczys-Dowmunt et al.,2018, inter alia ). Unfortunately, most parallel cor-
pora are aligned only at the sentence level, and
document-level translation has remained limited to
small-scale studies (Ansari et al., 2020; Lison et al.,
2018; Koehn, 2005; Tiedemann, 2012).
There are, however, inherent differences be-
tween sentence-level translation and document-
level translation, as documents consist of com-
plex discourse structures that go beyond the mere
concatenation of individual sentences. Three key
discourse features are particularly important in
document-level translation. First, the translations
ofnamed entities have to be consistent. For ex-
ample, in Fig. 1, the same terminologies are not
consistently referred to with the same translations
(i.e., Weibo vs.micro-blog ,Qiao Lian vs.Joe
vs.Joe love ), and as a result, the sentence-level
MT system fails to capture discourse dependencies
across sentences. Second, the coreference relation-
ship in the source language needs to be preserved.
In particular, the relations between entities and their
pronominal anaphora should be preserved, as well
as the transition of discourse centers. In Fig. 1
and Fig. 8, coreference chains are color-coded, vi-
sually demonstrating the preservation of anaphoric
referential relations and the transition chains of cen-
ters. Third, the conversational structure , such as
transitions between speakers, must be maintained.
Inferring latent discourse structures from doc-
uments is essential for translation coherence be-
cause of these discourse features. As a result, con-
ventional sentence-level MT pipelines that do not
consider context are unable to generate natural and
coherent translations (Lapshinova-Koltunski et al.,
2018; Werlen and Sadiht, 2021). While efforts
have been made to develop context-aware NMT
models over the past few years (Tiedemann and7853
Scherrer, 2017; Agrawal et al., 2018; V oita et al.,
2018; Bawden et al., 2018; Zhang et al., 2018), a
reliable evaluation method for document-level MT
has not yet been developed.
Traditional MT evaluation metrics, such as (Papineni et al., 2002), (Snover et al.,
2006), (Banerjee and Lavie, 2005), are
limited to evaluating translation quality at the
sentence level and do not consider contextual
information. Even with the use of more com-
plex neural evaluation methods, e.g., (Rei
et al., 2020), (Sellam et al., 2020) and
BERTS (Zhang et al., 2020), it is not possible
to accurately assess the coherence of translations
within the context of a document. (Jiang
et al., 2022a) has attempted to evaluate document-
level translation quality and obtained a higher cor-
relation with human evaluation. Yet, it is a coarse-
grained automatic evaluation metric based on sur-
face syntactic matching, thus being unable to evalu-
ate deeper discourse features, such as coreferences
and quotations. Vernikos et al. (2022) propose to
concatenate context to extend pretrained metrics
to the document level. However, its lack of inter-
pretability leaves it unclear as to why it assigns a
particular score to a given translation, which is a
major reason why some MT researchers are reluc-
tant to employ learned metrics in order to evaluate
their MT systems (Karpinska et al., 2022).
To address this issue, we have annotated a new
benchmark based on the large-scale bilingual paral-
lel corpus proposed in previous work (Jiang
et al., 2022a). This benchmark includes four evalu-
ation aspects annotated on both source and target
sides: entity, terminology, coreference, and quota-tion. It consists of 80 documents across multiple
ﬁction genres, with 15,095 mentions covered in
150,287 words in total.
Using these annotations, we systematically eval-
uate the latent discourse structures of MT models.
We show that, while context-aware MT systems per-
form better than phrase- and sentence-level ones
in terms of entity translation, they still lag behind
human translation in terms of proper noun and per-
sonal name translation. In addition, we demon-
strate that humans are far more adept at preserving
coreference chains than MT models.
The main contributions of our paper are:
•We propose a new benchmark that includes
four types of discourse annotations that are
closely related to document-level translation.
•We systematically investigate the similarities
and differences between the discourse struc-
tures of source and target languages, and the
challenges they pose to machine translation.
•We demonstrate that machine translations dif-
fer fundamentally from human translations in
terms of their latent discourse structures. We
believe these new insights can lead to poten-
tial improvements in future MT systems.
2 Corpus
We draw our source material for annotation from
the texts in the test set of (Jiang et al., 2022a),
which consists of 65,107 words, 2,633 sentences in
80 different documents drawn from 6 web novels
across different genres. The distribution of genres
in are shown in Fig. 2. The statistics of
are shown in Tab. 1. We refer the readers to App. D7854
for the dataset creation details.
3 Annotation
In this section, we describe the annotation scheme
and the process of annotating the corpus, and pro-
vide examples of how the annotations can be used
to study discourse phenomena in the two languages.
The annotation was conducted by eight profes-
sional translators. A total of 8,585 mentions on
the English side and 6,070 mentions on the Chi-
nese side have been annotated.
3.1 Named Entities
The mistranslation of named entities (NEs) can
signiﬁcantly harm the quality of translation, al-
though often under-reported by automatic evalu-
ation metrics (e.g., , ). Therefore,
we annotate named entities according to the ACE
2005 guidelines (Walker et al., 2005). These an-
notations identify six categories of entities: people
(PER), facilities ( FAC), geo-political entities ( GPE),
locations ( LOC), vehicles ( VEH), and organizations
(ORG). In addition, each unique entity is assigned
anentity_id for the purpose of coreference res-
olution. For example, the following text includes
annotations for two entities:
(1)···[the scion of [the Ri family]]
Nested entities are also annotated. The annotated
dataset contains a total of 5,984 English entities
and 4,853 Chinese entities. There are fewer entities
annotated in Chinese compared to English because
the subject is often dropped in Chinese. This phe-
nomenon also serves as a signiﬁcant contributor
to translation errors in context-agnostic MT sys-
tems when translating from Chinese to English, as
shown in Fig. 1 sentence (3).
3.2 Terminology
Terminology refers to specialized words or phrases
conventionally associated with a particular subject
matter or whose use is agreed upon by a community
of speakers. In the context of a novel, terminology
refers to the speciﬁc words and phrases that are
used to describe the concepts, characters, settings,
and events in the story. These words and phrases
may be speciﬁc to the genre or style of the novel, or
they may be unique to the world or setting created
by the author. For example, in a fantasy novel that
uses made-up magic terms, incorrect terminology
translation could lead to misunderstandings of the
powers and abilities of characters, the rules of the
magical world, or the signiﬁcance of certain events.
Similarly, incorrect translations of named entities
could lead to confusion or misunderstandings about
the identities and roles of characters in the story.
Inconsistent terminology translation can compro-
mise the integrity and cohesion of a work and make
it difﬁcult for readers to fully grasp the intended
meaning. Therefore, we have included a layer for
terminology identiﬁcation. This layer is a binary
classiﬁcation of whether a certain span counts as
terminology ( T) or not ( N). Tab. 2 shows some term
and non-term annotations in the dataset. There are
2,156 English terms and 2,290 Chinese terms that
have been annotated, accounting for approximately
52% of all entities.78553.3 Coreference
Coreference is the phenomenon in which a word
or phrase in a text refers to another word or
phrase that has been previously mentioned in
the text. Establishing coreference is important
for determining gender and number marking on
pronouns, determiners, and adjectives as well as for
making lexical decisions. Errors can compromise
coherence and accuracy. For example, consider the
following sentence in English:
In this example, both itself anditsrefer back to the
statue . However, in,itsis incorrectly translated
as seine, which is the masculine possessive
pronoun in German, as a result of the incorrect
coreference resolution.
We follow the OntoNotes Coreference
Annotation Guidelines for Chinese and En-
glish (Weischedel et al., 2012), and consider proper
noun phrases, common noun phrases, and personal
pronouns as coreference candidates. In particular,
we follow the three important principles of the
OntoNotes guidelines.
Maximal Spans. We include all modiﬁers in an
annotated span. I.e., we annotate [the surrounding
passersby, who were actually reporters in disguise]
rather than simply [passersby].
Rather Lack Than Abuse. When the annotators
were in doubt, they were told to not mark a difﬁcult
coreference decision.
Ellipsis. Omitted pronouns are marked with
Oand other pronouns are marked with P. For
instance, consider:
In this example, the ﬁrst her is omitted in
Chinese and is therefore marked as <O,1> , where
1is the entity id of Qiao Lian .
3.4 Quotations
The ﬁnal annotation layer is quotation. In this
stage of the process, we identify instances of di-
rect speech and attribute the speech to its speaker.
The inclusion of direct speech is common in liter-
ature, and its proper translation is essential. For
instance, the same person can be addressed by dif-
ferent names by different people. Furthermore, MT
systems that lack contextual awareness may have
difﬁculty correctly identifying the speaker in in-
stances of direct speech, leading to inconsistencies
in overall contextual translation. For example,
(2) [“Oh dear! Oh dear! I shall be late!”]
where 2is the entity id of the speaker and “Oh
dear!” is an exclamation. In this example, discrim-
inating exclamations from vocatives is vital for the
cohesiveness of the story. In addition, there are
cases where knowing the speaker is important for
coreference, e.g., John (Mary, respectively) said to
Mary (John, respectively), “Oh, this is your dog.
Her (His, respectively) dog barked.” In test
set, there are 840 (31.9%) sentences that contain
quotations, and there are 25 distinct speakers in
total.
Inter-Annotator Agreement. To ensure annota-
tion quality, we randomly select 10 documents
and have them independently annotated by another
expert, following previous work (Bamman et al.,
2019). We then calculate the mention overlap and
F1 scores for entity, terminology, coreference, and
quotation between the two annotations, using the
regular annotator’s result as the hypothesis and the
second annotator’s as the reference. We achieve
comparable inter-annotator agreements with pre-
vious work (Cohen et al., 2017; Bamman et al.,
2019). The results are reported in Tab. 3.
4 Bilingual Analysis
We conduct a thorough bilingual analysis of the
novel evaluation aspects using the new annotation.
The following two research questions are being
investigated:
•How different (or similar) are the discourse
structures in the source language (Chinese)
and the target language (English)?
•How do the differences in discourse structures
affect the translation quality of MT systems?7856
Entity types and terminology. Tab. 4 demon-
strates the distribution of entity annotations and
term annotations in both Chinese and English in the test set. First, the test set is dominated
by person and facility entities, with a much lower
proportion of geo-political entities. In addition, ter-
minology entities and person entities are repeated
more frequently, requiring better translation consis-
tency. An important sanity check from our analysis
is that the number and distribution of entities in Chi-
nese and English are similar. This suggests that at
the discourse level, the information being conveyed
in these two languages is largely the same.
Pronouns. Pronoun translation has been the fo-
cus of discourse-level MT evaluation (Hardmeier,
2012; Miculicich Werlen and Popescu-Belis, 2017).
We compare the numbers of different types of pro-
nouns in Chinese and English in Tab. 5. As can
be seen, Chinese has signiﬁcantly fewer pronouns
due to its pronoun-dropping property. This poses
additional challenges for NMT systems, as they
must be able to resolve anaphoric references. In
addition, Tab. 5 reveals a notable difference in the
frequency of neuter pronouns. English exhibits
a higher prevalence of neuter pronouns, possibly
due to the presence of a larger number of expletive
subjects in the language.
Coreference. We further conduct analyses to in-
vestigate the differences and similarities between
coreference behaviors in Chinese and English.
Fig. 3a examines the distribution in distances to
thenearest antecedent for both Chinese and En-
glish. The average distance between antecedents
and anaphora in English is shorter than in Chinese.
This may be connected to the more common use
of pronoun ellipsis in Chinese—when referring
to closer antecedents, pronouns are often omitted.
Fig. 3b illustrates the distance between the ﬁrst
and last mention of an entity within each coref-
erence chain. It can be observed that, although
English coreference chains tend to be longer in
general compared to Chinese coreference chains,
the distribution of these lengths is consistent. This
represents another language-independent discourse
feature in addition to entity distribution. The spread
distributions of the two languages are further de-
picted in Fig. 8 and Fig. 9. Finally, we present an
analysis of the size of coreference chains in Fig. 3c.
Our results indicate that the number of mentions in
English coreference chains tends to be larger than
in Chinese.
Challenges for MT. So far our analyses have re-
vealed that the Chinese source language and En-
glish target language exhibit several distinct lin-
guistic characteristics, leading to various discourse
phenomena, including the omission of pronouns
in Chinese and the use of expletive subjects in En-
glish, as well as shorter English coreference chains,
among others. These discourse phenomena present
challenges for MT. In order to systematically ana-
lyze translation errors caused by these phenomena,
we conducted a study in which four professional
translation experts compared reference translations
to those generated by a commercial MT system7857
Google Translate. The annotation guidelines are as
follows:
1.Identify translation errors and identify
whether the translation error is at the- level, e.g., the translation is inconsis-
tent with the context or does not comply with
the global criterion of coherence.
2.Categorize the errors in accor-
dance with the discourse phenomena and mark
the corresponding spans in the reference (En-
glish) that cause the MT output to be incorrect.
The results, summarized in Tab. 6, indicate that
named entity translation is the most signiﬁcant is-
sue, with terminology translation, coreference res-
olution failures, and pronoun omission also being
notable problems. This analysis emphasizes the
importance of accurately inferring latent discourse
structures in the translation of long texts.
5 Exploring Discourse Features of MT
Systems and Human Translation
The existence of a bilingual corpus with discourse
annotations allows us to test the performanceof both human translations and MT systems—
including the popularly used commercial systems
and those trained on the in-domain dataset .
The following 6 MT systems are adapted:
•: phrase-based baseline (Chiang, 2007).
• ,ooe,aiu: commercial systems.
• : the sentence-level transformer-based
(Vaswani et al., 2017) baseline model.
• : the document-level NMT model that
adopts two-stage training (Zhang et al., 2018).
•: the post-edited outputs produced by
professional translators. They were instructed
to correct only discourse-level errors with min-
imal modiﬁcation.
Overall Quality. The quality of the translations
produced by each system was ﬁrst evaluated using
a range of automatic translation metrics, includ-
ing , , , , and . The results of this evaluation are pre-
sented in Tab. 7. In addition, we conducted a hu-
man evaluation of the translations, the results of
which are shown in Fig. 4.The large gap between
the performances ofandindicates that the
genre of , i.e., literary translation, is challeng-
ing for, and NMT systems are far beneath hu-
man parity. performs signiﬁcantly better than , suggesting that contains rich discourse
phenomena that can only be translated accurately
when the context is taken into account. It is also
worth noting that even thoughis the post-edit of
the relatively poor-performing system , it still
achieves surprisingly better performance than
at the document level. This observation conﬁrms7858
our claim that discourse phenomena have a huge
impact on human judgment of translation quality.
Terminology Translation. We next turn to eval-
uating the performance of each system on termi-
nology translation, depicted in Fig. 5. The recall
rate of terminology is reported. As can be seen,
although the accuracy of each system in the transla-
tion of common phrases is not signiﬁcantly differ-
ent, the performance of thesystem is superior to
that of the MT systems in the translation of proper
names. Notably, despite having been trained on
in-domain data, the system is unable to ac-
curately translate terms on the test set due to the
varied terminologies used in different novels, re-
sulting in a performance that is even lower than
that of commercial MT systems.
Entity Translation. For the evaluation of entity
translations, we compared translations of PERand
translations of other entities. As illustrated in Fig. 6,
the translation of personal names and terms exhibits
a similar trend, with the MT systems performing
signiﬁcantly worse than. This outcome is ex-
pected, as both personal names and terms present
similar challenges for MT, including (1) the need
for consistent translation, (2) the potential for multi-
word personal names to be combined in speciﬁc
ways that may not translate well when each word
is translated individually, and (3) the possibility of
personal names not having a direct equivalent in
other languages. For example, in Fig. 9, the char-
acter name Ye Qing Luo (literal: night, clear, fall)
is mistranslated into The night fell . Additionally,
wordplay and puns based on character names may
be difﬁcult to convey in translation, as character
names may be chosen for their sound or meaning
in the original language, which may not translate
effectively into other languages.
Pronoun Translation. Our evaluation of pro-
noun translation focuses on the impact of omit-
ted pronouns on translation quality. As depicted7859B↑CEAF ↑ ↓ 43.3 49.7 4 63.5 55.1 3 74.0 67.3 2 88.1 76.7 1
in Fig. 7, an increase in the proportion of omit-
ted pronouns leads to a decrease in the translation
quality of the human post-editing system. This
ﬁnding suggests that pronoun omission is a ma-
jor issue in Chinese-to-English translation. How-
ever, the scores of the sentence-level
and document-level MT systems do not follow a
monotonic trend with the increase in the proportion
of omitted pronouns. Rather, the highest value is
reached when the proportion of omitted pronouns
is within the interval of [0.6,0.7]. This result indi-
cates that NMT models process pronoun omissions
in a manner that is fundamentally different from
that of humans. In contrast, the scores
exhibit a pattern that is consistent with the trend
observed in thesystem, indicating that while the
omission of pronouns may not signiﬁcantly affect
the quality of translation at the sentence level, it is
crucial for ensuring the overall quality of document-
level translation.
Translation Coherence. Finally, the corefer-
ence annotations allow us to measure the coher-
ence of translated texts. We ﬁrst ﬁne-tune a neu-
ral coreference model (Lee et al., 2017) trained
on OntoNotes (Hovy et al., 2006). This end-to-
end model jointly performs mention detection and
antecedent linking. We modify the model by re-
placing the span representations with SpanBERT
embeddings (Joshi et al., 2020). Additionally, dur-
ing inference, we inject mentions that match the
reference annotations into the antecedent candidate
sets after the mention detection stage. The model
then greedily selects the most likely antecedent
for each mention in a document, or a dummy an-
tecedent that begins a new coreference chain with
the mention. Coreference resolution performance
and the coherence ranking are reported in Tab. 8.
Coherence is compared at the document level ac-
cording to the centering theory (Grosz et al., 1995).We use the operationalization of Jiang et al. (2022b)
and comparescores of each document.
As shown in Tab. 8,surpassesin terms of
entity coherence. Additionally, human post-editing
was found to achieve entity coherence comparable
to that of. These results highlight the challenges
that MT currently faces in regard to maintaining
contextual coherence when translating longer texts.
6 Comparing to Related Work
Evaluation Test Suites for Context-Aware MT.
Several context-aware test suites have been pro-
posed in recent years (Hardmeier et al., 2015; Guil-
lou and Hardmeier, 2016; Burchardt et al., 2017;
Isabelle et al., 2017; Rios Gonzales et al., 2017;
Müller et al., 2018; Bawden et al., 2018; V oita et al.,
2019; Guillou and Hardmeier, 2018). Although
they facilitate the development of context-aware
machine translation, they are not without limita-
tions: First, the previous test suites are limited in
size, with the largest being Parcorfull (Lapshinova-
Koltunski et al., 2018) which has 82,379 words
from three different domains. Moreover, it is often
the case that test sets are annotated based on a small
portion of a parallel corpus that comprises docu-
ments. However, the parallel corpus from which
previous test sets are selected is generally quite
small, which makes it impossible to train large in-
domain NMT models. In contrast, our test set is
based on the ultra-large-scale parallel corpus ,
which is twice as large as the OpenSubtitles fr-en
corpus (Lison et al., 2018) upon which most pre-
vious challenge sets were based (see Tab. 9). This
enables us to differentiate between the source of
mistranslation being underﬁtting or the model not
having the capability to model discourse structure.
In addition, the scope of most test sets has been
restricted to a single discourse phenomenon (the
majority of which focus on pronoun translation),
which makes it impossible to objectively compare
a model’s translation capability for different dis-
course phenomena in the same language and do-
main. For example, it is hard to decipher whether
the model misinterpreted a pronoun’s gender as
a result of a coreference resolution error or as a
consequence of misjudging the gender of the entity.
In sharp contrast to this, we have four annotation
layers on the same parallel corpus.
Monolingual Corpora with Discourse Annota-
tions. Linguistically annotated corpora have con-
tributed signiﬁcantly to the advancement of key7860natural language technologies such as named en-
tity recognition (Tjong Kim Sang and De Meulder,
2003), coreference resolution (Lee et al., 2017),
and discourse parsing (Surdeanu et al., 2015). The
majority of evaluation has, however, only been
conducted on monolingual corpora such as the
BBN named entity and pronoun coreference cor-
pus (Weischedel and Brunstein, 2005), the Penn
Discourse Treebank (Miltsakaki et al., 2004; Web-
ber et al., 2019), and the OntoNotes corpus (Hovy
et al., 2006). And yet, languages differ consider-
ably regarding the discourse phenomena they ex-
hibit. In particular, different languages have differ-
ent linguistic features that inﬂuence the application
of cohesive devices, and there are language-speciﬁc
constraints governing the choice of referring ex-
pressions. For instance, Fig. 1 demonstrates a
prominent feature distinguishing Chinese from En-
glish. In Chinese, it is a common practice to omit
pronouns, and an anaphoric link can be inferred
from context without explicit assertion. English,
in contrast, does not generally allow the omission
of pronouns. Our dataset contains aligned parallel
discourse annotations in two languages, allowing
analysis of the transferability of current NER and
coreference resolution models across languages.
7 Conclusion
In this paper, we introduced the bilingual parallel
corpus -test, which includes annotations of var-
ious discourse phenomena. Our analysis of these
annotations revealed the signiﬁcant challenges
posed by discourse phenomena to MT. Therefore,
we advocate for greater attention on discourse co-
herence and consistency of the outputs of NMT
models. The main discourse challenges faced by
MT include entity consistency, entity recognition,
anaphoric information loss, and coreference. Ad-
ditionally, the corpus, with its rich discourse
annotations, serves as a valuable resource for a
variety of purposes, including studying the transfer-
ability of named entity recognition and coreference
resolution, as well as the development of multilin-
gual structured prediction models.
Limitations
There are several limitations to the current study.
Firstly, as for now, the corpus used in this study
only consists of a single language pair. Secondly,
the coherence of the MT systems was evaluated
using a ﬁne-tuned conference model, as no anno-tations were available for the MT outputs. How-
ever, as shown in Tab. 8, the ﬁne-tuned conference
model is not perfect and may affect the quality of
our coherence evaluation. Thirdly, this paper fo-
cuses on using discourse annotations to reveal and
analyze discourse phenomena and the challenges
they present to machine translation. Using the an-
notations to improve MT models is beyond the
scope of this study and is left for future work.
Ethical Considerations
The annotators were paid a fair wage, and the
annotation process did not solicit any sensitive
information from the annotators. In regard
to the copyright of our dataset, as stated in
the paper, the crawling script that we plan
to release will allow others to reproduce our
dataset faithfully and will not be in breach of
any copyright. In addition, the release of our
annotated test set will not violate the doctrine of
Fair Use (US/EU), as the purpose and character
of the use is transformative . Please refer to
https://www.nolo.com/legal-encyclopedia/
fair-use-the-four-factors.html for relevant
laws.
References7861786278637864A Document-Level Parallel Corpora
There are some document-level parallel corpora in the market: TED Talks of IWSLT dataset (Ansari et al.,
2020), News Commentary (Tiedemann, 2012), LDCand OpenSubtitle (Lison et al., 2018). The sizes and
average length of these corpora are summarized in Tab. 9. Below we review these document-level parallel
corpora in detail.
LDC. This corpus consists of formal articles from the news and law domains. The articles ave syntactic
structures such as conjoined phrases, which make machine translation challenging. However, the news
articles in this corpus are relatively outdated.
IWSLT. This corpus contains the TED Talks that covers the variety of topics. However, it is quite small
in scale, which makes training large transformer-based models impractical.
News Commentary. This corpus consists of political and economic commentary crawled from the web
site Project Syndicate.However, the scale of this corpus is also quite small. Moreover, there are no
parallel Chinese-to-English data available in this corpus.
Europarl. The corpus is extracted from the proceedings of the European Parliament. Only European
language pairs are available in this corpus.
OpenSubtitle. This corpus is a collection of translated movie subtitles (Lison and Tiedemann, 2016).
Besides being simple and short, the documents in this corpus are usually verbal and informal as well. .The corpus is the largest corpus in terms of size. Moreover, the sentences and documents in are substantially longer than previous corpora. It is also worth noting that differs from previous
corpora in terms of genre – in-depth human analysis shows that it is very challenging for current NMT
systems due to its rich discourse phenomena.
B Case Study
We provide two example chapters in with coreference annotation in Fig. 8 and Fig. 9. We observe
that the dataset poses challenges for NMT in the following ways.7865Entity Consistency. There are many named entities in the dataset that have a high repetition rate,
such as ﬁctional characters. Therefore, named entity consistency is a signiﬁcant challenge in machine
translation on this dataset. For example, the translations of Weibo andQiao Lian in Fig. 8 are not
consistent in context.
Entity Recognition and Retrieval. In addition to the ﬂuency of entity translation, the adequacy of
entity translation is another challenge in . In the case of ﬁctional characters with strange names, the
NMT model may not correctly recognize named entities, resulting in extremely poor translation quality,
as in Fig. 9. “Ye Qing Luo” could be literally translated as “night”, “clear”, “fall”; however, it is actually
the name of a ﬁctional character. Even though ﬁctional characters are difﬁcult to translate, they are
relatively rare throughout the text, so it would be beneﬁcial to abandon the assumption of inter-sentence
independence in consideration of global contextual information. One potential way to alleviate this
problem is to equip NMT models with an entity recognition module.
Anaphoric Information Loss. Chinese, being one of the pro-drop languages, omits many pronouns,
while the English language does not, as shown in Tab. 5. Translating from Chinese to English thus
requires context to infer the correct English pronouns to compensate for the anaphoric information loss of
sentence-level Chinese-to-English translations.
Morphological Information Loss. Tense information is also frequently absent in Chinese and can only
be inferred from context. In general, this problem, which we refer to as morphological information loss, is
often encountered when translating from a morphologically poorer language to a morphologically richer
one. In the case of Chinese-to-English translation, tense information is often lost, while in other language
pairs, such as English-to-French and English-to-German, gender information is often missed since as
French and German are morphologically richer than English.
Coreference. In addition, in Fig. 8, we observe that the focus entity of the document is shifting
throughout the text ( Qiao Lian− →Shen Liangchuan − →Wang Wenhao− →Shen Liangchuan − →
Song Cheng ), and this information is language-independent, i.e., consistent in source and target. This
information could be used to improve the coherence of translation.
C Experiment Setup
We adopt the parameters of Transformer Big (Vaswani et al., 2017) for both and. More
precisely, the layers in the big encoders and decoders are N= 12 , the number of heads per layer is
h= 16 , the dimensionality of input and output is d= 1024 , and the inner-layer of a feed-forward
networks has dimensionality d= 4096 . The dropout rate is ﬁxed as 0.3. We adopt Adam optimizer
withβ= 0.9,β= 0.98,/epsilon1= 10, and set learning rate 0.1of the same learning rate schedule as
Transformer. We set the batch size as 6,000 and the update frequency as 16 for updating parameters to
imitate 128 GPUs on a machine with 8 V100 GPU. The datasets are encoded by BPE with 60K merge
operations.
D The Corpus
In this section, we describe three stages of the dataset creation process: collecting bilingual parallel
documents, quality control and dataset split.
D.1 Bilingual Document Collection
385 Chinese web novels across multiple genres were selected, including action, fantasy, romance, comedy,
science ﬁction, martial arts, etc. The genre distribution is shown in Fig. 2. We then scrape their
corresponding English translations from the Internet.The English versions are translated by professional
translators who are native speakers of English, and then corrected and aligned by professional editors
at the chapter level. The text is converted to UTF-8 and certain data cleansing (e.g., deduplication) is7866performed in the process. Chapters that contain poetry or couplets in classical Chinese are excluded as
they are difﬁcult to translate directly into English. Further, we exclude chapters with less than 5 sentences
and chapters where the sequence ratio is greater than 3.0. The titles of each chapter are also removed,
since most of them are neither translated properly nor at the document level. The sentence alignment is
automatically performed by Bleualign(Sennrich and V olk, 2011). The ﬁnal corpus has 384 books with
9,581,816 sentence pairs (a total of 461.8 million words).
D.2 Quality Control
We hired four bilingual graduate students to perform the quality control of the aforementioned process.
These annotators were native Chinese speakers and proﬁcient in English. We randomly selected 163
chapters and asked the annotators to distinguish whether a document was well aligned at the sentence
level by counting the number of misalignment. It is identiﬁed as a misalignment if, for example, line 39 in
English corresponds to line 39 and line 40 in Chinese, but the tool made a mistake in combining the two
sentences. We observed an alignment accuracy rate of 93.1%.
D.3 Dataset Split
We construct the development set and the test set by randomly selecting 160 chapters from 6 novels, which
contain 3,018 chapters in total. To prevent any train-test leakage, these 6 novels are removed from the
training set. Tab. 1 provides the detailed statistics of the dataset split. In addition, we asked the same
annotators who performed the quality control to manually correct misalignments in the development and
test sets, and 7.3% of the lines were corrected in total.
E Human Evaluation
We conducted human evaluation on the test set following the protocol proposed by (Läubli et al.,
2018, 2020). As stated in §5, we evaluated two units of linguistic context ( and )
independently based on their respective and . We showed raters isolated sentences
in random order in the -level evaluation, whereas in the -level evaluation, we
presented entire documents and asked raters to evaluate a sequence of ﬁve sequential sentences at a time
in order. The evaluation was based solely on source texts, whereas neither source texts nor
references were included in the evaluation.
The evaluation was conducted by four professional Chinese to English translators, and the evaluation was conducted by four native English revisers. The four translators were different
from the professional translators who performed human translation. For human evaluation, we deliberately
invited another group of specialists to avoid making judgments biased towards human translation.
We adopted relative ranking because it has been shown to be more effective than direct assessment
when conducted by experts rather than crowd workers (Barrault et al., 2019). In particular, raters were
presented with the system outputs and were asked to evaluate the system outputs vis-à-vis one another,
e.g., to decide whether system A was better than system B (with ties allowed).
By randomizing the order of presentation of the system outputs, we were able to blind the origin of
the output sentences and documents. While in the -level evaluation, the system outputs were
presented in different orders for each sentence, the -level evaluation used the same ordering
of systems within a document to help raters better assess global coherence.
Additionally, we used spam items for quality control.(Kittur et al., 2008). At the -level, we
make one of the ﬁve options nonsensical in a small fraction of items by randomly shufﬂing the order of
the translated words, except for 10% at the beginning and end. At the -level, we randomly
shufﬂe all translated sentences except the ﬁrst and last sentence at the document level, rendering one of
the ﬁve options nonsensical. If a rater marks a spam item as better than or equal to an actual translation,
this is a strong indication that they did not read both options carefully.7867 1 2 1 /check /check 2 /check /check 3/check /check 4/check /check 1 2 5 /check /check 6 /check /check 7/check /check 8/check /check 1- 2 .171 .169 3- 4 .294 .346 5- 6 .323 .402 7- 8 .378 .342
Each raters evaluated 180 documents (including 18 spam items) and 180 sentences (including 18 spam
items). The 180 sentences were randomly sampled from 1or 2. We spited the test set into
two non-overlapping subsets, referred to as 1and 2. Note that 1and 2were chosen
from different books. Each rater evaluated both sentences and documents, but never the same text in both
conditions so as to avoid repetition priming (Gonzalez et al., 2011). Each document or sentence was
therefore evaluated by two raters, as shown in Tab. 10.
We report pairwise inter-rater agreement in Tab. 11. Cohen’s kappa coefﬁcients were used:
κ=P(A)−P(E)
1−P(E)(1)
whereP(A)is the proportion of times that two raters agree, and P(E)is the likelihood of agreement by
chance.786878697870ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Limitations
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
2 + Appendix
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
2 + Appendix
C/squareDid you run computational experiments?
5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Not applicable. we used existing models7871/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
appendix
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
in lab experts
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Sec. 3.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Sec. 3.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Sec. 3.7872