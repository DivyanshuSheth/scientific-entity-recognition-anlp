
Yafu Li, Leyang Cui, Yongjng Yin, Yue ZhangZhejiang UniversitySchool of Engineering, Westlake UniversityTencent AI labInstitute of Advanced Technology, Westlake Institute for Advanced Study
yafuly@gmail.com leyangcui@tencent.com
yinyongjing@westlake.edu.cn yue.zhang@wias.org.cn
Abstract
Despite low latency, non-autoregressive ma-
chine translation (NAT) suffers severe perfor-
mance deterioration due to the naive indepen-
dence assumption. This assumption is further
strengthened by cross-entropy loss, which en-
courages a strict match between the hypothe-
sis and the reference token by token. To alle-
viate this issue, we propose multi-granularity
optimization for NAT, which collects model
behaviors on translation segments of various
granularities and integrates feedback for back-
propagation. Experiments on four WMT bench-
marks show that the proposed method signifi-
cantly outperforms the baseline models trained
with cross-entropy loss, and achieves the best
performance on WMT’16 En ⇔Ro and highly
competitive results on WMT’14 En ⇔De for
fully non-autoregressive translation.
1 Introduction
Neural machine translation (NMT) systems have
shown superior performance on various bench-
mark datasets (Vaswani et al., 2017; Edunov et al.,
2018a). In the training stage, NMT systems min-
imize the token-level cross-entropy loss between
the reference sequence and the model hypothesis.
During inference, NMT models adopt autoregres-
sive decoding, where the decoder generates the
target sentence token by token ( O(N)). To reduce
the latency of NMT systems, Gu et al. (2018) pro-
pose non-autoregressive neural machine translation
(NAT), which improves the decoding speed by gen-
erating the entire target sequence in parallel ( O(1)).
Despite low latency, without modeling the tar-
get sequence history, NAT models tend to generate
translations of low quality (Gu et al., 2018; Sun
et al., 2019; Ghazvininejad et al., 2019). NAT ig-
nores inter-token dependency and naively factor-
izes the sequence-level probability as a product of
independent token probability. However, vanillaFigure 1: An illustration of modeling the multi-
granularity token-dependency beyond cross-entropy.
NAT adopts the same training optimization method
as autoregressive (AT) models, i.e., cross-entropy
loss (XE loss), which forces the model to learn
a strict position-to-position mapping, heavily pe-
nalizing hypotheses that suffer position shifts but
share large similarity with the references. Given a
reference “she left her keys yesterday .”, an inappro-
priate hypothesis “she left her her her .” can yield
a lower cross-entropy than one reasonable hypoth-
esis “yesterday she left her keys .”. Autoregressive
models suffer less from the issue by considering
previous generated tokens during inference, which
is however infeasible for parallel decoding under
the independence assumption. As a result, NAT
models trained using cross-entropy loss are weak
at handling multi-modality issues and prone to to-
ken repetition mistakes (Sun et al., 2019; Qian et al.,
2021; Ghazvininejad et al., 2020).
Intuitively, generating adequate and fluent trans-
lations involves resolving dependencies of various
ranges (Yule, 2006). For example, to generate a
translation “Each of the students has a book”, the
model needs to consider the local n-gram pattern
“a - book”, the subject-verb agreement across the
non-continuous span “each - has”, and the global
context. To capture the token dependency without5073the language model, feedback on model’s behav-
ior on text spans of multiple granularities can be
incorporated. To this end, we propose a multi-
granularity optimization method to provide NAT
models with rich feedback on various text spans
involving multi-level dependencies. As shown in
Figure 1, instead of exerting a strict token-level
supervision, we evaluate model behavior on vari-
ous granularities before integrating scores of each
granularity to optimize the model. In this way,
for each sample we highlight different parts of the
translation, e.g., “a book” or “each of the students
has”.
During training, instead of searching for a single
output for each source sequence, we explore the
search space by sampling a set of hypotheses. For
each hypothesis, we jointly mask part of the tokens
and those of the gold reference at the same posi-
tions. To directly evaluate each partially masked
hypothesis, we adopt metric-based optimization
(Ranzato et al., 2016; Shen et al., 2016) which re-
wards the model with a metric function measuring
hypothesis-reference text similarity. Since both
the hypothesis and the reference share the same
masked positions, the metric score of each sam-
ple is mainly determined by those exposed seg-
ments. Finally, we weigh each sample score by the
model confidence to integrate the metric feedback
on segments of various granularities. An illustra-
tive representation is shown in Figure 2, where a
set of masked hypothesis-reference pairs are sam-
pled and scored respectively before being merged
by segment probabilities. In this way, the model
is optimized based on its behavior on text spans
of multiple granularities for each training instance
within a single forward-backward pass.
We evaluate the proposed method across four
machine translation benchmarks: WMT14 En ⇔De
and WMT16 En ⇔Ro. Results show that the pro-
posed method outperforms baseline NAT models
trained with XE loss by a large margin, while
maintaining the same inference latency. The pro-
posed method achieves two best performances for
fully non-autoregressive models among four bench-
marks, and obtains highly competitive results com-
pared with the AT model. To the best of our knowl-
edge, we are the first to leverage multi-granularity
metric feedback for training NAT models. Our code
is released at https://github.com/yafuly/MGMO-
NAT.2 Method
We first briefly introduce some preliminaries in-
cluding non-autoregressive machine translation
(Section 2.1) and cross-entropy (Section 2.2), and
then we elaborate our proposed method where the
model learns segments of different granularities for
each instance (Section 2.3).
2.1 Non-autoregressive Machine Translation
(NAT)
The machine translation task can be formally de-
fined as a sequence-to-sequence generation prob-
lem, where the model generates the target language
sequence y={y, y, ..., y}given the source
language sequence x={x, x, ..., x}based on
the conditional probability p(y|x)(θdenotes the
model parameters). Autoregressive neural machine
translation factorizes the conditional probability
to:/producttextp(y|y, ..., y,x). In contrast, non-
autoregressive machine translation (Gu et al., 2018)
ignores the dependency between target tokens and
factorizes the probability as/producttextp(y|x), where
tokens at each time step are predicted indepen-
dently.
2.2 Cross Entropy (XE)
Similar to AT models, vanilla NAT models are typ-
ically trained using the cross entropy loss:
L=−logp(y|x) =−/summationdisplaylogp(y|x)(1)
In addition, a loss for length prediction during
inference is introduced:
L =−logp(T|x) (2)
Ghazvininejad et al. (2019) adopt masking
scheme in masked language models and train the
NAT models as a conditional masked language
model (CMLM):
L =−/summationdisplaylogp(y|Ω(y,Y(y)),x)
(3)
where Y(y)is a randomly selected subset of tar-
get tokens and Ωdenotes a function that masks a
selected set of tokens in Y(y). During decoding,
the CMLM models can generate target language
sequences via iteratively refining translations from
previous iterations.5074
2.3 Multi-granularity Optimization for NAT
We propose multi-granularity optimization which
integrates feedback on various types of granulari-
ties for each training instance. The overall method
illustration is presented in Figure 2.
Sequence Decomposition In order to obtain out-
put spans of multiple granularities, we sample K
output sequences from the model following a two-
step sampling process. In particular, we first sam-
ple a hypothesis length and then sample the output
token at each time step independently given the
sequence length. The probability of the k-th hy-
pothesis his calculated as:
p(h|x) =p(T|x)/productdisplayp(h|x) (4)
To highlight different segments of multiple gran-
ularities for each sample, we apply a masking strat-
egy that randomly masks a subset of the tokens
for both the hypothesis and the reference at the
same position. We denote the masked hypoth-
esis and reference as h={h, . . . , h}and
y={y, . . . , y}, respectively, and denote the
set of masked positions as M. Note that the refer-
ence length Tmay be different from the hypothesis
length T.For the first hypothesis output ( k= 1) in Figure
2, given the randomly generated masked position
setM={1,6,7}, the masked hypothesis and ref-
erence are h={⟨m⟩, h, h, ...,⟨m⟩,⟨m⟩, h}
andy={⟨m⟩, y, y, ...,⟨m⟩,⟨m⟩, y, y}, re-
spectively, where ⟨m⟩represents the masked to-
ken. To determine the number of masked tokens
|M|for each training instance, we first sample
a threshold τfrom a uniform distribution U(0, γ),
and computes |M|as follows:
|M|=max(⌊T−τ∗T⌋,0) (5)
where γis a scaling ratio that controls the likeli-
hood of being masked for each token. Note that
the value of |M|lies within the range [0, T−1],
meaning that at least one token is kept.
In this way, we decompose each training instance
intoKpairs of masked hypotheses and references
with different granularities exposed. For example,
in the last sample ( k=K) in Figure 2, only the
verb phrase “ pay these taxes ” and the period (“.”)
are learned by the model, whereas the sample ( k=
1) reveals more informative segments.
Metric-based Optimization (MO) To avoid the
strict mapping of XE loss (Ghazvininejad et al.,
2020; Du et al., 2021), we incorporate metric-based5075optimization which is widely studied in reinforce-
ment learning for NMT (Ranzato et al., 2016; Shen
et al., 2016; Kiegeland and Kreutzer, 2021; Wu
et al., 2018). Metric-based optimization allows
more flexibility of token positions by rewarding the
model with global scores instead of forcing it to fit
a strict position-to-position mapping under XE.
The objective of metric-based optimization is to
maximize the expected reward with respect to the
posterior distribution given the parameters θ. The
reward ℜ(θ)for a training instance can be formally
written as:
ℜ(θ) =/summationdisplayp(h|x)R(h,y) (6)
where H(x)denotes the set of all possible candi-
date hypotheses for the source sequence x.R(h,y)
denotes the metric function that measures the sim-
ilarity between the hypothesis and the reference
under a specific evaluation metric, e.g., GLEU (Wu
et al., 2016).
Multi-granularity Metric-based Optimization
(MgMO) Despite a lower gap between training
and evaluation under the metric-based optimiza-
tion, it suffers relatively coarse training signals as
rewards are obtained by measuring sequence-level
similarity. However, due to lack of explicit token
dependency, NAT requires more fine-grained feed-
back for capturing complex token dependencies
that can spread across various lengths, e.g., continu-
ous local dependencies and subject-verb agreement
across the non-continuous spans. We enrich the
metric feedback in Equation 6 by decomposing a
single sequence-level reward into multi-granularity
evaluation. For each training instance, the model
is optimized by integrated feedback on its perfor-
mance on various sequence segments of diverse
granularities.
As enumerating segments of all possible gran-
ularities is intractable, we consider a finite set of
sampled hypotheses to traverse as many granulari-
ties as possible. Combining with sequence decom-
position, we can rewrite the Equation 6 into a form
of loss function:
L(θ) =−/summationdisplayp(h|x)R(h,y) (7)
where K(x)is a sampled subset consisting of K
sampled hypotheses. Applying the log derivativetrick, the gradient can be derived:
∂L(θ)
∂θ=−/summationdisplay[R(h,y)∇logp(h|x)]
(8)
which does not require differentiation of the metric
function R. Since both the hypothesis and the refer-
ence are partially masked at the same positions, the
exposed segments exert much larger effects on the
metric function R(h,y), i.e., the dissimilarity
only origins from the unmasked tokens.
In order to make the model focus on the exposed
segments, we transform the sequence probability
in Equation 7 into the probability of the segment
co-occurrence. Since the output distributions of
tokens are independent with each other in NAT, the
segment co-occurrence probability is simply the
multiplication of the probability of each unmasked
token:
ˆp(h|x) =p(T|x)/productdisplayp(h|x)(9)
where (M)denotes the complementary set of
the masked position set M.
Within the sample space, the sequence probabil-
ity can be renormalized by model confidence (Och,
2003; Shen et al., 2016):
q(h|x;α) =ˆp(h|x)
/summationtextˆp(h|x)(10)
where αcontrols the distribution sharpness.
Taking the renormalized probability into Equa-
tion 7, the loss of multi-granularity metric optimiza-
tion is formally written as:
L (θ) =−/summationdisplayq(h|x)R(h,y)(11)
In this way, MgMO decomposes the sequence-
level feedback into pieces of multi-granularity met-
ric feedback before integrating them to optimize
the model, resulting in a set of more fine-grained
training signals that examines different parts of the
hypothesis.
Training Following previous work (Ranzato
et al., 2016; Shen et al., 2016; Shao et al., 2020;
Kong et al., 2019; Du et al., 2021), we adopt a
two-stage training strategy, where CMLM loss is
first applied for initialization and then replaced by
MgMO loss for finetuning. The length loss is main-
tained throughout the training process.5076
3 Experiments
3.1 Settings
Data We conduct experiments on both directions
of two standard machine translation datasets in-
cluding WMT14 En ⇔De and WMT16 En ⇔Ro.
Knowledge distillation is commonly used for train-
ing NAT models (Gu et al., 2018; Sun et al., 2019;
Ghazvininejad et al., 2019, 2020). We use the dis-
tilled dataset released by Huang et al. (2021) to
obtain comparable baseline models.
Initialization We mainly follow Huang et al.
(2021) for the model configuration. We use Trans-
former and adopt Transformer_Base configuration
for all experiments: both the encoder and decoder
consist of 6 layers with 8 attention heads, and the
hidden dimension and feedforward layer dimension
is 512 and 2,048, respectively. We train the model
using Adam (Kingma and Ba, 2015) optimizer. We
set the weight decay as 0.01 and label smoothing
as 0.1. The learning rate increases to 5·10in the
first 10K steps and then anneals exponentially. For
WMT16 En ⇔Ro, we use a dropout rate of 0.3 and
a batch size of 32K tokens, whereas for WMT14
En⇔De, we switch to 0.1 and 128K accordingly.
Code implementation is based on Fairseq (Ott et al.,
2019). We train all models for 300,000 steps and
select the checkpoint with the best validation per-
formance for MgMO finetuning.Finetuning We finetune all models for 100,000
steps and use a dropout rate of 0.1. The batch size
for WMT16 En ⇔Ro and WMT14 En ⇔De is 256
and 1,024, respectively. We use a fixed learning
rate of 2·10during finetuning. We use GLEU
(Wu et al., 2016) score as the metric function and
set the value of αin Q-distribution as 0.005. Based
on validation results, we use a maximum n-gram
size of 6 for GLEU score, set the scaling ratio γas
8 and set the sample space size Kas 40.
Evaluation We use BLEU (Papineni et al., 2002)
for all directions. Similar to autoregressive settings,
we use l= 5 length candidates during inference
(Ghazvininejad et al., 2020; Du et al., 2021). We
select the best checkpoint for evaluation based on
validation BLEU scores.
3.2 Main Results
We compare our method with the autoregressive
Transformer and other fully NAT baselines (i.e.,
one decoding pass). The results are shown in Ta-
ble 1. We can observe that applying metric-based
optimization (MO) on the CMLM baseline brings
an improvement of 4.1 BLEU scores on average.
By decomposing the metric feedback into multi-
granularity levels, MgMO further obtains a signifi-
cant improvement (with p <0.01(Koehn, 2004)),
expanding the advantage to 5.5 BLEU scores on
average. Compared with other representative NAT
baselines, MgMO achieves the best performance5077
on WMT16 En ⇔Ro and highly competitive perfor-
mance on WMT14 En ⇔De. Since no modification
is involved in model architecture and inference,
MgMO achieves the same inference latency with
the vanilla NAT model.
3.3 Training Strategies
In addition to the default setting, we consider three
alternative training strategies. Generally, these
strategies differ in how much information is fed
into the NAT decoder and how much information
left requires the decoder to fit: (1) none of the tar-
get tokens are observed and the complete set of
target tokens are considered for computing metric
feedback ( N&C ); (2) part of the target tokens are
observed and the complete set of target tokens are
considered for computing metric feedback ( P&C );
(3) part of the target tokens are observed and a
partial set of the target tokens are highlighted for
computing metric feedback ( P&P ); (4) none of the
target tokens are observed and a partial set of the
target tokens are highlighted for computing met-
ric feedback ( N&P , i.e., the default setting). We
present an example in Table 2 as an intuitive illus-
tration for different training strategies.
We conduct experiments on the WMT16
En⇔Ro dataset. The results are shown in Table 3.
We can observe that both partial observations (P&C
and P&P) and partial predictions (P&P and N&P)
obtain consistent improvement over the N&C strat-
egy, i.e., metric-based optimization without multi-
granularity (MO in Table 1). P&C achieves similar
performance to P&P, since it is easy for the model
to copy the partial observations as the correspond-
ing predictions, and thus focus on the unobserved
tokens. In other words, P&C, P&P and N&P all
incorporate training signals from multiple granular-
ities, either explicitly or implicitly, and therefore
obtain better performance. Due to a smaller dis-
crepancy between training and inference, N&P ob-
tains further performance improvement over P&C
and P&P.
3.4 Ratio of Exposed Segments
The scaling ratio γcontrols how likely one token
is masked, i.e., a larger γindicates a higher proba-
bility of being exposed for each token and a lower
probability otherwise. In general, a proper scaling
ratio yields more diverse granularities from which
the model learns rich token dependencies.
As can be seen from the Figure 3, increasing the
scaling ratio γuntil 8.0 steadily brings performance5078
improvement in terms of validation BLEU scores.
This can be because a larger γresults in longer and
more informative segments across different sam-
ples. Therefore, the model is encouraged to learn to
handle longer and more difficult token dependency,
which is common in long sequences and is a major
challenge for NAT (Gu et al., 2018; Qian et al.,
2021; Du et al., 2021). Analysis of performance
on different sequence lengths (Section 4.1) further
validate our assumption.
As the ratio increases further, model perfor-
mance begins to deteriorate since overly large ratios
nearly expose all tokens for each sample, resulting
in coarse granularities with limited diversity. The
extreme case becomes the N&C strategy ( γ→ ∞ ),
which reveals all target tokens in every sample. In
this case, the complete sequence becomes the only
granularity, giving a coarse and monotonous feed-
back that is hard to learn from.
To provide a statistical intuition of why the ra-
tio of 8 obtains better performance, we traverse
the training set (WMT’16 En ⇒Ro) under different
scaling ratios, and calculate the ratios of different
granularities, i.e., the segments of different lengths.
The results are shown in Figure 4. We can observe
that masking with a larger ratio spreads more pro-portions on larger granularities. While maintaining
a significant portion on large granularities, the ratio
of 8.0 yields a relatively smooth distribution over
various granularities, contributing to a progressive
learning curriculum.
3.5 Size of Search Space
Intuitively, a larger sample space, i.e., a larger K,
brings better granularity diversity since more sets
of segments are encountered. On the other hand,
enlarging the sample space increases computational
complexity. We explore the effect of sample space
sizeK, with results shown in Figure 5. We can
observe that increasing Kup to 40 brings a steady
performance improvement on the validation set. A
larger K(i.e., 60) does not lead to further improve-
ment, indicating that the model has encountered
sufficient types of granularities.
3.6 Alternative Optimization Targets
Beyond the standard GLEU metric, we also explore
the effects of the maximum n-gram size and other
alternative metrics for optimization.
Effects of N-gram Size We vary the maximum n-
gram size in GLEU score and the results are shown
in Table 4. We can observe that rewarding local
matches (1-gram and 2-gram) obtains comparable
performance to that of larger span matches (4-gram
and 6-gram). We hypothesize that multi-granularity
optimization compensates for capturing word order-
ing in some degree by simultaneously evaluating
various granularities of the target sequence, which
implicitly restricts the token locations.
Alternative Metric Functions We also explore
the model performance under different metric func-
tions, with results presented in Table 5. We can see
that different metric functions achieve comparable
performance as they all measure sequence-level
similarities. Specifically, the metrics (GLEU and
chrF (Popovi ´c, 2015)) considering both n-gram
precision and recall obtain better performance com-
pared with BLEU which only considers precision.
Since Rouge-2 (Lin, 2004) considers a maximum
n-gram size of 2, it achieves worse performance
than GLEU. TER (Snover et al., 2006) measures
the number of edit operations and obtains a slightly
worse performance, since it does not reward n-gram
match and suffers a discrepancy from the evalua-
tion metric, i.e., BLEU.5079
4 Analysis
In this section, we conduct quantitative and qualita-
tive analysis to dig some insights into how MgMO
benefits non-autoregressive translation.
4.1 Sequence Lengths
We analyze the effectiveness of our method by com-
paring performance on test sentences of different
lengths. We use compare-mt (Neubig et al., 2019)
to split the WMT’14 De ⇒En test sets into sev-
eral subsets based on target sequence lengths. The
results are shown in Figure 7. As the sequence
length grows, the baseline model trained using XE
loss suffers great performance deterioration. Under
MgMO, the model maintains relatively stable per-
formance on test sentences across different lengths,
proving that multi-granularity learning brings ben-
efits for capturing non-local dependencies that can
spread across long text spans.4.2 Prediction Confidence
NAT shows weakness in handling multi-modality
(Gu et al., 2018; Sun et al., 2019), which is reflected
by its low confidence on locating token translations
among neighboring positions (Ghazvininejad et al.,
2020; Du et al., 2021). Ideally, we expect each to-
ken to have a high probability mass at the position
it is predicted, but low at the neighboring positions.
Following previous work (Sun and Yang, 2020; Du
et al., 2021), we compute Normalized Corpus-level
multimodality (NCM) on the WMT14 En ⇔De test
set which measures average token-level prediction
confidence. The results are shown in Table 6, and
lower NCM scores indicate higher confidence. We
can see that applying MgMO largely increases the
model prediction confidence at each step. This can
be because MgMO better captures token interde-
pendency via optimizing model predictions based
on various contexts, i.e., different sets of exposed
segments. We show an example in Figure 6 to
provide an intuition of effects brought by higher
prediction confidence.
5 Related Work
Fully Non-Autoregressive Models To bridge the
performance gap between fully NAT and the autore-
gressive counterpart, lots of techniques have been
proposed to build dependencies among the target to-
kens such as curriculum learning (Guo et al., 2020;
Liu et al., 2020; Ding et al., 2021a), latent vari-
able modeling (Libovický and Helcl, 2018; Kaiser
et al., 2018; Ma et al., 2019; Saharia et al., 2020;
Bao et al., 2022, 2021), improving distillation train-
ing (Zhou et al., 2020; Ding et al., 2021c,b), and
adaptive token sampling (Qian et al., 2021). De-
spite their success, these methods are trained with
XE loss, which forces a strict mapping. Another
line of work shift to improvement of XE loss or
metric-based objectives. For example, Ghazvinine-5080jad et al. (2020) soften the penalty for word order
errors based on a monotonic alignment assumption,
and Du et al. (2021) computes XE loss based on the
best possible alignment between predictions and
target tokens. In contrast, we unitize hypothesis
sampling and metric-based optimization, allowing
the model to explore hypothesis of different lengths.
Sun et al. (2019) incorporate an approximation of
Conditional Random Fields (CRF) to model out-
put dependency, while the decoding is not paral-
lelized. Shao et al. (2019) devise customized re-
inforcement algorithms to optimize global metrics
for NAT. Shao et al. (2020) and Liu et al. (2022)
propose differentiable n-gram matching losses be-
tween the hypothesis and reference. In comparison,
we propose to integrate feedback from evaluating
model behavior on multi-level granularities within
a single forward-backward propagation.
Metric-based Optimization for NMT Metric-
based optimization has been utilized in NMT (Ran-
zato et al., 2016; Shen et al., 2016; Bahdanau et al.,
2017; Wu et al., 2018; Edunov et al., 2018b; Kong
et al., 2019; Kiegeland and Kreutzer, 2021) to alle-
viate the mismatch between the optimization during
training and evaluation during inference. For ex-
ample, Ranzato et al. (2016) train NMT using the
objective gradually shifting from token-level likeli-
hood to sentence-level BLEU score, and Shen et al.
(2016) adopt minimum risk training (MRT) to min-
imize the task specific expected loss (i.e., induced
by BLEU score). Kong et al. (2019) use translation
adequacy as the metric function. Different from
them, we propose to integrate metric feedback on
various granularities instead of a coarse sequence-
level reward.
6 Conclusion
We proposed multi-granularity optimization for
NAT, which considers metric feedback on hypoth-
esis segments of multiple granularities. Through
integrating multi-granularity feedback, the model
is optimized by focusing on different parts of the
sequence within a single forward-backward pass,
obtaining more detailed and informative training
signals. Empirical results demonstrated that our
method achieved highly competitive performance
compared with other representative baselines for
fully NAT. Analysis further showed that MgMO
maintained strong performance on long sequences
that vanilla NAT models suffer, and obtained high
prediction confidence. Beyond non-autoregressivetranslation, our proposed method can be used in
other text generation tasks.
Limitations
Despite competitive performance, the model still
suffers common issues in NMT. Firstly, although
the total GPU memory cost for our method is lower
than that of XE loss (as the batch size under MgMO
is much lower), MgMO requires a relatively large
minimum memory capacity since lots of samples
are considered for forward and backward propaga-
tion for each training instance. Secondly, like many
other NAT models, our model also suffers, though
smaller, performance deterioration without using
data distilled from autoregressive teacher models.
Acknowledgment
We thank all reviewers for their insightful com-
ments. This publication has emanated from re-
search conducted with the financial support of the
Pioneer and "Leading Goose" R&D Program of
Zhejiang under Grant Number 2022SDXHDX0003.
This work is also under a grant from Lan-bridge
Information Technology Co., Ltd.
References5081508250835084