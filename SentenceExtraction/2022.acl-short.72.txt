
Meng Zhang, Liangyou Li, Qun Liu
Huawei Noah’s Ark Lab
{zhangmeng92, liliangyou, qun.liu}@huawei.com
Abstract
Triangular machine translation is a special case
of low-resource machine translation where the
language pair of interest has limited parallel
data, but both languages have abundant par-
allel data with a pivot language. Naturally,
the key to triangular machine translation is the
successful exploitation of such auxiliary data.
In this work, we propose a transfer-learning-
based approach that utilizes all types of auxil-
iary data. As we train auxiliary source-pivot
and pivot-target translation models, we initial-
ize some parameters of the pivot side with a
pre-trained language model and freeze them
to encourage both translation models to work
in the same pivot language space, so that they
can be smoothly transferred to the source-target
translation model. Experiments show that our
approach can outperform previous ones.
1 Introduction
Machine translation (MT) has achieved promising
performance when large-scale parallel data is avail-
able. Unfortunately, the abundance of parallel data
is largely limited to English, which leads to con-
cerns on the unfair deployment of machine transla-
tion service across languages. In turn, researchers
are increasingly interested in non-English-centric
machine translation approaches (Fan et al., 2021).
Triangular MT (Kim et al., 2019; Ji et al., 2020)
has the potential to alleviate some data scarcity
conditions when the source and target languages
both have a good amount of parallel data with a
pivot language (usually English). Kim et al. (2019)
have shown that transfer learning is an effective
approach to triangular MT, surpassing generic ap-
proaches like multilingual MT.
However, previous works have not fully ex-
ploited all types of auxiliary data (Table 1). For
example, it is reasonable to assume that the source,
target, and pivot language all have much monolin-
gual data because of the notable size of parallel
data between source-pivot and pivot-target.
Table 1: Data usage of different approaches (Section
3.2). X,Y, and Zrepresent source, target, and pivot
language, respectively. Our triangular transfer uses all
types of data.
In this work, we propose a transfer-learning-
based approach that exploits all types of auxiliary
data. During the training of auxiliary models on
auxiliary data, we design parameter freezing mech-
anisms that encourage the models to compute the
representations in the same pivot language space,
so that combining parts of auxiliary models gives
a reasonable starting point for ﬁnetuning on the
source-target data. We verify the effectiveness of
our approach with a series of experiments.
2 Approach
We ﬁrst present a preliminary approach that is a
simple implementation of our basic idea, for ease
of understanding. We then present an enhanced ver-
sion that achieves better performance. For notation
purpose, we use X,Y, and Zto represent source,
target, and pivot language, respectively.
2.1 Simple Triangular Transfer
We show the illustration of the preliminary ap-
proach in Figure 1, called simple triangular trans-
fer. In Step (1), we prepare a pre-trained language
model (PLM) with the pivot language monolingual
data. We consider this PLM to deﬁne a representa-
tion space for the pivot language, and we would like
subsequent models to stick to this representation644
space. In order to achieve this, we freeze certain
parameters in Step (2) as we train source-pivot and
pivot-target translation models, which are partly
initialized by the PLM. For example, the pivot-
target translation model has the pivot language on
the source side, so the encoder is initialized by the
PLM, and some (or all) of its parameters are frozen.
This ensures that the encoder produces representa-
tions in the pivot language space, and the decoder
has to perform translation in this space. Likewise,
the encoder in the source-pivot translation model
needs to learn to produce representations in the
same space. Therefore, when the pivot-target de-
coder combines with the source-pivot encoder in
Step (3), they could cooperate more easily in the
space deﬁned in Step (1).
We experimented with RoBERTa (Liu et al.,
2019) and BART (Lewis et al., 2020) as the PLMs.
We found that simple triangular transfer attains
about 0.8 higher BLEU by using BART instead of
RoBERTa. In contrast, we found that dual trans-
fer (Zhang et al., 2021), one of our baselines, per-
forms similarly with BART and RoBERTa. When
used to initialize decoder parameters, RoBERTa
has to leave the cross attention parameters ran-
domly initialized, which may explain the superior-
ity of BART for our approach, while dual transfer
does not involve initializing decoder parameters.
Therefore, we choose BART as our default PLM.
2.2 Triangular Transfer
A limitation of simple triangular transfer is that it
does not utilize monolingual data of the source andtarget languages. A naive way is to prepare source
and target PLMs and use them to initialize source-
pivot encoder and pivot-target decoder, respectively.
However, this leads to marginal improvement for
the ﬁnal source-target translation performance (Sec-
tion 3.5). This is likely because the source, target,
and pivot PLMs are trained independently, so their
representation spaces are isolated.
Therefore, we intend to train source and target
PLMs in the pivot language space as well. To this
end, we design another initialization and freezing
step inspired by Zhang et al. (2021), as shown in
Figure 2. In this illustration, we use BART as the
PLM. Step (2) is the added step of preparing BART
models in the source and target languages. As the
BART body parameters are inherited from the pivot
language BART and frozen, the source and target
language BART embeddings are trained to lie in
the pivot language space. Then in Step (3), every
part of the translation models can be initialized
in the pivot language space. Again, we freeze pa-
rameters in the pivot language side to ensure the
representations do not drift too much.
2.3 Freezing Strategy
There are various choices when we freeze parame-
ters in the pivot language side of the source-pivot
and pivot-target translation models. Take the en-
coder of the pivot-target translation model as the
example. In one extreme, we can freeze the em-
beddings only; this is good for the optimization of
pivot-target translation, but may result in a space
that is far away from the pivot language space given
by the pivot PLM. In the other extreme, we can
freeze the entire encoder, which clearly hurts the
pivot-target translation performance. This is hence
a trade-off. We experiment with multiple freezing
strategies between the two extremes, i.e., freezing a
given number of layers. We always ensure that the
number of frozen layers is the same for the decoder
of the source-pivot translation model.
Besides layer-wise freezing, we also try
component-wise freezing inspired by Li et al.
(2021). In their study, they found that some com-
ponents like layer normalization and decoder cross
attention are necessary to ﬁnetune, while others
can be frozen. In particular, we experiment with
three strategies based on their ﬁndings of the most
effective ones in their task. These strategies apply
to Step (3) of triangular transfer.645
language code # sentence (pair)
En-De 3.1m
Fr-En 29.5m
Fr-De 247k
Zh-En 11.9m
Zh-De 189k
En 93.9m
De 100.0m
Fr 44.6m
Zh 20.0m
LNA-E,D All layer normalization, encoder self
attention, decoder cross attention can be ﬁnetuned.
Others are frozen.
LNA-D All encoder parameters, decoder layer
normalization and cross attention can be ﬁnetuned.
LNA-e,D Use LNA-D when training the source-
pivot model. When training the pivot-target model,
freeze encoder embeddings in addition to LNA-D.
3 Experiments
3.1 Setup
We conduct experiments on French (Fr) !German
(De) and Chinese (Zh) !German (De) translation,
with English (En) as the pivot language. Training
data statistics is shown in Table 2. The evaluationmetric is computed by SacreBLEU(Post, 2018).
All approaches use Transformer base (Vaswani
et al., 2017) as the translation model, but note
that pivot translation needs two translation mod-
els for decoding, equivalently doubling the number
of parameters. Further details can be found in the
appendix.
3.2 Baselines
We compare with several baselines as follows.
No transfer This baseline directly trains on the
source-target parallel data.
Pivot translation Two-pass decoding by source-
pivot and pivot-target translation.
Step-wise pre-training This is one of the ap-
proaches in (Kim et al., 2019). It is simple and
robust, and has been shown to outperform multilin-
gual MT. It trains a source-pivot translation model
and uses the encoder to initialize the encoder of
a pivot-target translation model. In order to make
this possible, these two encoders need to use a
shared source-pivot vocabulary. Then the pivot-
target translation model is trained while keeping its
encoder frozen. Finally the model is ﬁnetuned on
source-target parallel data.
Shared target dual transfer Dual transfer
(Zhang et al., 2021) is a general transfer learning ap-
proach to low-resource machine translation. When646approach BLEU
no transfer 13.49
pivot translation through no transfer 18.99
step-wise pre-training 18.49
shared target transfer 18.88
shared source transfer 18.89
triangular transfer 19.91
approach BLEU
no transfer 11.39
pivot translation through no transfer 12.91
triangular transfer 16.03
applied to triangular MT, it cannot utilize both
source-pivot and pivot-target parallel data. Shared
target dual transfer uses pivot-target auxiliary trans-
lation model and does not exploit source-pivot par-
allel data.
Shared source dual transfer The shared source
version uses source-pivot translation model for
transfer and does not exploit pivot-target parallel
data.
3.3 Main Results
We present the performance of our approach and
the baselines on Fr!De in Table 3. The no transfer
baseline performs poorly because it is trained on a
small amount of parallel data. The other baselines
perform much better. Among them, pivot transla-
tion attains the best performance in terms of BLEU,
at the cost of doubled latency. Our approach can
outperform all the baselines.
Taking pivot translation as the best baseline, we
further evaluate our approach on Zh !De. Results
in Table 4 show that the performance improvement
of our approach is larger for this translation direc-
tion.
3.4 The Effect of Freezing Strategies
From Table 5, we can observe the effect of dif-
ferent freezing strategies. For layer-wise freezing,
we see a roughly monotonic trend of the Fr-En
and En-De performance with respect to the num-
ber of frozen layers: The more frozen layers, thestrategy Fr-En En-De Fr-De
L=0 31.42 20.95 19.62
L=1 31.41 20.98 19.76
L=2 31.55 20.56 19.71
L=3 31.06 20.54 19.91
L=4 30.92 20.22 19.68
L=5 30.39 19.95 19.21
L=6 30.31 19.11 19.02
LNA-E,D 28.72 17.92 17.97
LNA-D 31.08 20.23 18.75
LNA-e,D 31.08 19.97 18.25
approach BLEU
pivot translation through no transfer 18.99
pivot translation through 2 19.06
shared target transfer 18.88
shared target transfer + naive mono. 18.93
shared source transfer 18.89
shared source transfer + naive mono. 18.97
simple triang. transfer 18.96
simple triang. transfer + naive mono. 19.00
triangular transfer 19.62
lower their BLEU scores. However, the best Fr-De
performance is achieved with L=3. This indi-
cates the trade-off between the auxiliary models’
performance and the pivot space anchoring. For
component-wise freezing, the Fr-En and En-De
performance follows a similar trend, but the Fr-De
performance that we ultimately care about is not as
good.
3.5 Using Monolingual Data
Table 6 shows the effect of different ways of us-
ing monolingual data. The naive way is to prepare
PLMs with monolingual data and initialize the en-
coder or decoder where needed. For pivot trans-
lation, this is known as 2 (Rothe et al.,
2020) for the source-pivot and pivot-target transla-
tion models. For dual transfer, parts of the auxiliary
models can be initialized by PLMs (e.g., for shared
target transfer, the pivot-target decoder is initial-647approach BLEU
no transfer 18.74
shared target transfer 20.53
shared source transfer 20.73
triangular transfer 20.84
ized). For Step (2) in simple triangular transfer,
we can also initialize the pivot-target decoder and
source-pivot encoder with PLMs. However, none
of the above methods shows clear improvement.
This is likely because these methods only help the
auxiliary translation models to train, which is not
necessary as they can be trained well with abun-
dant parallel data already. In contrast, our design
of Step (2) in triangular transfer additionally helps
the auxiliary translation models to stay in the pivot
language space.
3.6 Pivot-Based Back-Translation
Following Kim et al. (2019), we generate syn-
thetic parallel Fr-De data with pivot-based back-
translation (Bertoldi et al., 2008). Speciﬁcally, we
use a no transfer En !Fr model to translate the
English side of En-De data into French, and the au-
thentic Fr-De data are oversampled to make the ra-
tio of authentic and synthetic data to be 1:2. Results
in Table 7 show that triangular transfer and dual
transfer clearly outperform the no transfer baseline.
4 Conclusion
In this work, we propose a transfer-learning-based
approach that utilizes all types of auxiliary data,
including both source-pivot and pivot-target paral-
lel data, as well as involved monolingual data. We
investigate different freezing strategies for train-
ing the auxiliary models to improve source-target
translation, and achieve better performance than
previous approaches.
References648
A Data and Preprocessing
We gather data from WMT and ParaCrawl, shown
in Tables 8 and 9.
We use jiebafor Chinese word segmentation,
and Mosesscripts for punctuation normalization
and tokenization of other languages. The corpora
are deduplicated. Each language is encoded with
byte pair encoding (BPE) (Sennrich et al., 2016)
with 32k merge operations. The BPE codes and
vocabularies are learned on each language’s mono-
lingual data, and then used to segment parallel data.
Sentences with more than 128 subwords are re-
moved. Parallel sentences are cleaned with length
ratio 1.5 (length counted by subwords).
B Hyperparameters
Our implementation is based on fairseq (Ott
et al., 2019). We share decoder input and output
embeddings (Press and Wolf, 2017). The optimizeris Adam. Dropout and label smoothing are both
set to 0.1. The batch size is 6,144 per GPU and
we train on 8 GPUs. The peak learning rate is
510for the no transfer baseline and auxiliary
models, 110for the Fr!De model of step-
wise pre-training and dual transfer, and 710
for the last step of triangular transfer. The learning
rate warms up for 4,000 steps, and then follows
inverse square root decay. Early stopping happens
when the development BLEU does not improve for
10 epochs.
RoBERTa and BART models use exactly the
same architecture as Transformer base. The mask
ratio is 15%. The batch size is 256 sentences per
GPU, and each sentence contains up to 128 tokens.
The learning rate warms up for 10,000 steps to the
peak 510, and then follows polynomial decay.
They are trained for 125k steps.
We use beam size of 5 for decoding, includ-
ing for pivot translation and pivot-based back-
translation.649lang. source train dev test
En-De WMT 2019Europarl v9, News Commentary v14,newstest2011 newstest2012Document-split Rapid corpus
Fr-En WMT 2015Europarl v7, News Commentary v10,newstest2011 newstest2012UN corpus, 10French-English corpus
Fr-De WMT 2019 News Commentary v14, newstest2008-2010 newstest2011 newstest2012
Zh-En ParaCrawl ParaCrawl v9 newsdev2017 newstest2017
Zh-De WMT 2021 News Commentary v16 - dev - test 3k split 3k split
lang. source name
En WMT 2018 News Crawl 2014-2017
De WMT 2021 100m subset from WMT 2021
Fr WMT 2015Europarl v7, News Commentary v10,
News Crawl 2007-2014, News Discussions
Zh WMT 2021 News Crawl, Zh side of parallel data650