
Jinjie Ni and Rui Mao and Zonglin Yang and Han Lei and Erik Cambria
Nanyang Technological University, Singapore
{jinjie001, leih0003}@e.ntu.edu.sg,
{rui.mao, zonglin.yang, cambria}@ntu.edu.sg
Abstract
Recent studies have revealed some issues of
Multi-Head Attention (MHA), e.g., redundancy
and over-parameterization. Specifically, the
heads of MHA were originally designed to at-
tend to information from different representa-
tion subspaces, whereas prior studies found
that some attention heads likely learn simi-
lar features and can be pruned without harm-
ing performance. Inspired by the minimum-
redundancy feature selection, we assume that
focusing on the most representative and dis-
tinctive features with minimum resources can
mitigate the above issues and lead to more ef-
fective and efficient MHAs. In particular, we
propose Grouped Head Attention, trained with
a self-supervised group constraint that group at-
tention heads, where each group focuses on an
essential but distinctive feature subset. We ad-
ditionally propose a V oting-to-Stay procedure
to remove redundant heads, thus achieving a
transformer with lighter weights. Moreover,
our method achieves significant performance
gains on three well-established tasks while con-
siderably compressing parameters.
1 Introduction
Transformers have shown promising performance
across various tasks . However, they have some
issues, e.g., redundancy and over-parameterization,
which is mainly caused by Multi-Head Attention
(MHA) (Michel et al., 2019; V oita et al., 2019)
and Feed-Forward Network (FFN) (Sukhbaatar
et al., 2019; Wu et al., 2019, 2020) of trans-
former. We aim to mitigate the redundancy and
over-parameterization issues by optimizing the
MHA module. The multi-heads were originally
designed to attend to different representation sub-
spaces of input (Vaswani et al., 2017). However,
prior works (Michel et al., 2019; V oita et al., 2019)
have shown that the attention heads are highly re-
dundant and over-parameterized after training be-
cause some heads can be switched off with a negli-
gible performance drop.Such an issue is probably caused by their paral-
lel design: the heads naturally work in the same
way and likely attend to similar features (Cor-
donnier et al., 2020). The existing redundancy
optimization methods are mainly based on ho-
mogenization, diversification, and head signifi-
cance. However, they all have some limits. (1)
The homogenization-based methods mitigate re-
dundancy and over-parameterization by making
heads similar and removing unnecessary parame-
ters. Cordonnier et al. (2020) homogenized atten-
tion heads by sharing most weights between all
heads, which reduced the redundant parameters
but sacrificed the performance somewhat because
of the lack of diversity. (2)The diversification-
based methods diversify the heads to enrich fea-
tures and reduce the inter-head redundancy. Li et al.
(2018) found that diversifying attention heads by
adding a regularization could force MHA to reduce
inter-head information redundancy, yielding per-
formance gains in Machine Translation. However,
such strategy that retains all feature subsets is sub-
optimal, because it does not address the issue that
MHA is over-parameterized. (3)The significance-
based methods (Michel et al., 2019; V oita et al.,
2019; Li et al., 2021) learn significance scores for
the heads to prune unimportant ones. However,
the retained important heads still remain inter-head
redundancy without diversifying them.
Considering the issues of the above-mentioned
methods, we hypothesize that attending to the
most representative and distinctive feature sub-
sets with minimum resources leads to more ef-
fective and efficient MHAs, which is inspired by
the minimum-redundancy feature selection (Cor-
donnier et al., 2020). Accordingly, we propose
a divide-and-conquer strategy, including Group-
Constrained Training (GCT) and V oting-to-Stay
(V2S), to achieve the setting of our assumption and
mitigate the above-mentioned issues. We illustrate
them below.14526We first propose a strategy to group and distin-
guish attention heads, where a Grouped Head At-
tention (GHA) is obtained via the self-supervised
GCT. By encouraging homogenization within a
group and diversification between groups, the
MHA is forced to divide its heads to work in sev-
eral separate groups, where each group focuses on
an essential but unique feature subset, being in line
with the setting of our assumption. Note that the
redundancy exists when the resources deployed by
the model are more than enough to process cur-
rent information (Cordonnier et al., 2020). GHA
reduces the redundancy in two aspects:
•The intra-group homogenization reduces redun-
dancy by encouraging similar intra-group heads
and only retaining the most representative one
later to lower the resource deployment.
•The inter-group diversification reduces redun-
dancy by forcing heads to attend to more diversi-
fied features (with less overlap between heads) so
that the unique information to process increases
and matches the resources deployed.
Next, we show that GHA-PS (GHA with the Pil-
lar of Strength), a lighter-weight GHA, can be
achieved by excluding the redundant heads of GHA
via the V2S procedure. V2S culls the redundant
heads that share similar patterns with the most rep-
resentative head (PS head) of a group, which is se-
lected by voting on different training batches. Note
that upon the convergence of the GCT, the heads
are highly homogenized within a group, thus being
redundant because they process similar information.
As a result, once the redundant heads are culled,
the PS heads can still achieve the essential utility
of the original attention layer and yield comparable
performance to the unculled model. The Lottery
Ticket hypothesis (Frankle and Carbin, 2019) ar-
gues that subnetworks in an over-parameterized
neural network can converge faster and achieve
comparable or better performance than the original
network. Our GHA-PS achieving better results is
also in line with this hypothesis.
Such a divide-and-conquer combination resolves
the issues of previous redundancy optimization
methods: (1)Our model achieves better param-
eter efficiency, resolving the issue of previous
diversification-based methods; (2)The feature di-
versity is guaranteed and the inter-head redundancy
is reduced, resolving the problems of previous
homogenization- and significance-based methods.We evaluate our method on three benchmarking
tasks. We denote the corresponding transformer ar-
chitectures of GHA and GHA-PS as Grouped Head
Transformers (GHT) and Grouped Head Transform-
ers with the Pillars of Strength (GHT-PS), respec-
tively. GHT and GHT-PS achieve significant im-
provements over the strong baselines in Machine
Translation (MT) BLEU scores (+3.8% and +4.4%
averaged on 7 datasets), Language Modeling (LM)
perplexity (-2.8% and -2.9%), and Abstractive sum-
marization (AS) F1-Rouge (+6.7% and +7.0% on
average). GHT-PS exhibits higher efficiency in
model size, inference speed, and floating-point op-
erations (FLOPs). The light architecture of GHT-
PS reduces 63.6% parameters of the vanilla trans-
former and yields comparable performance. The
key contributions of our workare threefold:
•We find that, in a certain range, higher com-
pactness of attention heads (i.e., the intra-group
heads become closer to each other and the inter-
group ones become farther) improves MHA’s per-
formance, forcing MHA to focus on the most
representative and distinctive features. It pro-
vides guidance for future architectural designs of
MHA.
•We propose a divide-and-conquer strategy that
consists of GCT and V2S. It mitigates the redun-
dancy and over-parameterization issues of MHA.
Our method uses fewer parameters and achieves
better performance, outperforming the existing
MHA redundancy/parameter reduction methods.
•We verify our methods on three well-established
NLP tasks. The superior results on datasets
with multiple languages, domains, and data sizes
demonstrate the effectiveness of our method.
2 Related Work
Parameter efficiency. Different methods were
proposed to achieve lightweight transformers: (1)
replacing attention with lightweight modules, e.g.,
convolution modules, such as Dynamic Conv (Wu
et al., 2019) and Lite Transformer (Wu et al., 2020);
(2)removing or replacing the feed-forward lay-
ers, such as Sukhbaatar et al. (2019) and Wu et al.
(2020); (3)pruning the model, such as Michel et al.
(2019), V oita et al. (2019), and Li et al. (2021).14527Modified multi-head mechanism. Ahmed et al.
(2017) learned to weight the projected output of
different heads, performing weighted sum over
them. Li et al. (2019) aggregated the output of
different heads by dynamic routing; Cui et al.
(2019) used different attention mechanisms, e.g.,
global/local and forward/backward attention for
different heads; Shazeer et al. (2020) mixed differ-
ent heads before and after the softmax operation
in an attention function to achieve communication
between heads.
Head redundancy optimization. Michel et al.
(2019) and V oita et al. (2019) found that only a sub-
set of the attention heads have significant utilities
in transformer, where the important heads could be
identified by Expected Sensitivity and Layer-wise
Relevance Propagation (LRP) (Ding et al., 2017).
Upon this, Li et al. (2021) learned per-head im-
portance scores and pruned the heads. Cordonnier
et al. (2020) homogenized the attention heads by
sharing a part of the weights between heads, which
lowered the number of parameters but sacrificed
performance. Li et al. (2018) found that diversify-
ing attention heads by adding a regularization can
force MHA to reduce inter-head redundancy, yield-
ing performance gains for Machine Translation.
However, previous methods either traded perfor-
mance for efficiency or retained extra parameters.
3 Methodology
There are two core components in our method,
namely the Group-Constrained Training (GCT) and
the V oting-to-Stay (V2S) procedure. GHA (Fig-
ure 1) is developed with GCT that removes head
redundancy; GHA-PS is developed by removing
the redundant parameters of GHA in V2S. In this
section, we detail the process of developing the
GHA and finding its Pillars of Strength (PS).
3.1 Grouped Head Attention with Hidden
Units
First, we detail the core module of GHT, the GHA
with hidden units, which is built based on MHA via
the GCT. The GCT divides the attention heads of
MHA into several groups and makes heads within
a group become more similar, whereas heads be-
tween groups become more different. Thus, MHA
is forced to divide its heads to work in several
separate groups, where each group focuses on an
essential but unique feature subset to reduce head
redundancy. We will show the effectiveness in § 5.
Given a transformer model f(x;θ)withnat-
tention layers, the set of heads at attention layer
lis denoted as H={h, ...,h}, where kis
the number of heads. The outputs of the attention
heads are concatenated and projected with W,
where the i-th head output oin layer lresults
from the computation of the projection matrices
W,W, and Wof this head:
MHA(Q,K,V) =Concate (o, ...,o)W
(1)
o=softmax ((QW)(KW)
√d)(VW).
(2)
Three feature maps (FMs) of GHA are extracted for
the self-supervised GCT: (1)the result of VW,
denoted as ˆV={v, ...,v}(the value FM); (2)
the attention weights of the l-th layer, denoted as
A={a, ...,a}(the attention FM); (3)the out-
put of the l-th layer before the output projection
W, denoted as O={o, ...,o}(the head
output FM). Moreover, ˆV={ˆV, ...,ˆV},A=
{A, ...,A},O={O, ...,O}. Given the FMs,
a Hidden Unit Discovery System (HUDS) Ωas-
signs a hidden unit zfor each head to represent
its group property, where idenotes the i-th head
andjdenotes the j-th group hidden unit. z∈ˆZ,
where ˆZ={z, ...,z}represents the hidden unit
candidates, and the hidden units assigned to the
heads are denoted as Z={z, ...,z}.14528Zis discovered by the HUDS Ω:Z= Ω(E),
where Edenotes either one of the ˆV,A, orO.
Here Ω(·)is an unsupervised algorithm that divides
the heads into Cgroups given their FMs, such as
K-means:
Ω(E) = arg min/summationdisplay/summationdisplay||x−µ||,(3)
where ˆEis the set of feature maps of the i-th head
group in the l-th attention layer. Then, the feature
map groups of the l-th attention layer are denoted
asˆE={ˆE, ...,ˆE, ...,ˆE}.µis the mean of
the feature map vectors in ˆE. The hidden units
Z={Z, ...,Z}areC-class categorical variables
(Eq.4(A)) or continuous vectors (Eq.4(B)) to super-
vise the GCT. The objective of the self-supervised
GCT is termed as:
(4)
Either when Zare categorical variables (Eq.4(A))
or continuous vectors (Eq.4(B)), the objective is
composed of a homogenization term and a diver-
sification term.v,a, andodenote the fea-
ture maps of the i-th head belonging to the j-th
group. p(z|v,a,o)denotes the predicted
probability of the assigned group hidden variable
z, given v,a, and o.φ(x;y)denotes a
cosine similarity measurement between xandy
(following Li et al. (2018)). φ(v,a,o;z)
=τφ(v;z) +τφ(a;z) +τφ(o;z),
where τis a coefficient, determined by the specific
settings for each dataset & task. When Zare cat-
egorical variables, the grouping is a classification
task whose classification heads project the output
intoCclasses; when Zare continuous vectors, the
grouping process is a metric learning task whosesimilarity computations are conducted between Z
and the projected FM representations. In both con-
ditions, GHA is supervised by Zto make the heads
in the same group yield similar patterns, whereas
those in different groups repulse from each other.
The overall objective is given by L=L+L,
where Lis the task-specific objective.
3.2 The Pillars of Strength
Being consistent with Lottery Ticket hypothe-
sis (Frankle and Carbin, 2019), we establish the
GHT-PS from GHT as its subnetwork by removing
redundant heads from GHA to achieve higher pa-
rameter efficiency. We propose the V2S procedure
to find the Pillars of Strength (PS) heads that con-
stitute the core of the GHA and remove other heads.
We first describe the V2S roughly. In GHA, the
heads within each group exhibit similar patterns
upon the convergence of the Group-Constrained
Training (GCT). Then, we only keep the heads with
the most explicit group patterns (the PS heads), and
switch off the other ones within the same group via
V2S. The main idea of V2S is to vote on all heads of
the GHA, and only retain one head for each group
– the head receiving the most votes. Specifically, it
takes an entire epoch to collect the layer-wise votes
m∈ {0,1}from the whole training set (each
data batch bcreates one layer-wise vote mper
attention layer), where kdenotes the head number;
0indicates that the corresponding head should be
switched off and 1indicates that a head is retained.
We assume that there are Bmini-batches in the
training set. Then, each attention layer receives B
layer-wise votes within which each head-wise vote
is denoted by either 0or1. For each group, the head
receiving the most ‘ 1’s are assigned a ‘ 1’ in the fi-
nal head mask m∈ {0,1}for attention layer l,
indicating that this head will be retained. Follow-
ing Michel et al. (2019) and V oita et al. (2019),
we mask out the output of heads as the equivalent
operation of head removal. The V2S procedure
is outlined in Algorithm 1. We detail some of its
definitions below. (1)ρindicates the full conver-
gence of GHT, i.e., the hidden units found by Ω
have a center shift less than a threshold. (2)In Step
7-9, given feature maps ˆV,A, andOof the l-
th attention layer, the vote vectors m,m, and
m∈ {0,1}are determined by the group pattern
scores ηof each head, indicating the explicitness
of group patterns.14529Algorithm 1 The V oting-to-Stay (V2S) algorithmProcedure V oting-to-Stay( f,ˆV,A,O,Z)ifsatisfy ρ, andmis none then Start voting epoch; Freeze f. Γ←[ ] ▷CreatΓto store votes forbatch binBtraining batches do forlayer linLlayers do forEin{ˆV,A,O}do Based on η={η, ..., η}, create m,m,m. Addm,m,mtoΓ. forlinndo ▷V ote at each attn layer m←V OTE (Γ) m←[m, ...,m]▷Stack layer votes Unfreeze f; end voting epoch.f=f⊙m▷Mask GHT attn outputs with m
We set the corresponding digit in the vote vec-
tors as 1 for the head achieving the highest ηin its
group, indicating the most representative head of
the group. Here η=p(z|e)ifzis categori-
cal; otherwise η=−φ(e;z).edenotes the
i-th head feature map (either one of the v,a, or
o).(3)V OTE means counting the ‘ 1’s for each
head based on the 0-1votes in Γand only keeping
the heads with the highest counts. After V2S, a
finetuning is applied to adapt the pruned network.
GHT-PS compresses considerable parameters.
In the case of two head groups, GHT-PS reduces
75% parameters for an attention layer and 32.1%
for the entire model. We will show that V2S re-
moving non-PS heads does not sacrifice model per-
formance. Instead, it brings accuracy gains in some
cases and improves inference speed.
4 Experimental Setup
In this section, we detail the key architectural con-
figurations. Further training, model, dataset & eval-
uation setups are detailed in A.1, A.2, & A.3. We
follow the transformer of Vaswani et al. (2017) as
a backbone architecture for all datasets and tasks
in our experiments. Following Wu et al. (2019,
2020), for Machine Translation and Abstractive
Summarization, we adopt the same 8-head encoder-
decoder architecture with 6 layers for both encoderand decoder, where the model dimension d =
512and feed-forward dimension d= 2048 . For
LM, we adopt the 16-head decoder-only architec-
ture with 16 layers, where the model dimension
d = 1024 and feed-forward dimension d=
4096 . The layer normalization is applied before the
residual connection of each layer. The parameters
of decoder input and output projections are shared.
Our models are based on fairseq (Ott et al., 2019)
implementations.
We perform the GCT as a metric learning task
because it does not introduce additional projection
layers when the shapes of similarity inputs are iden-
tical (Eq.4(B)), which makes GHT weight-lighter.
In addition, it performs better in our experiments
compared to the classification-based grouping.
5 Results and Analysis
5.1 Machine Translation
Ours vs. vanilla transformer. We first report
results by comparing GHT and GHT-PS with the
vanilla transformer (Vaswani et al., 2017) which is
the backbone of our model. As shown in Table 1,
the models are compared at different parameter lev-
els. GHT does not have weight reduction, keep-
ing the same parameter size as the vanilla trans-
former (44M, the same setting as transformer base
(Vaswani et al., 2017)). In contrast, GHT-PS is
compressed to 30M parameters via V2S. For a fair
comparison, we first compare GHT-PS with two lite
architectures, Transformer-Lite1 and Transformer-
Lite2, whose parameter numbers are 30M as well.
Keeping other settings unchanged, the encoder and
decoder of Transformer-Lite1 are reduced to 4 lay-
ers, respectively. Transformer-Lite2 reduces the
model dimension d to 424, and dto 1696.
GHT and GHT-PS consistently and significantly
outperform their backbone models at the same pa-
rameter level across all datasets. On average, the
GHT surpasses 44M vanilla transformer by 3.8% in
BLEU (Papineni et al., 2002); GHT-PS surpasses
Lite1 and Lite2 by 4.9% and 4.4%, respectively. Al-
though GHT-PS reduces 32.1% parameters, it sig-
nificantly outperforms both 44M and 30M vanilla
transformers, which is comparable to GHT on all
datasets. It shows that V2S reduces the parameter
size of the original transformer without sacrificing
accuracy on MT. Efficiency is analyzed later.14530
Ours vs. efficient attention models. We com-
pare GHT with two state-of-the-art (SOTA) MHA
redundancy optimization baselines. Cordonnier
et al. (2020) and Li et al. (2018) are respectively
homogenization- and diversification-based meth-
ods. In addition, we compare GHT-PS with four
SOTA baselines that made major contributions to
attention parameter compression and redundancy
optimization. V oita et al. (2019) and Li et al.
(2021) are significance-based pruning methods.
Dynamic Conv (Wu et al., 2019) and Lite Trans-
former (Wu et al., 2020) modify the MHA arch to
reduce parameters.
Table 2 shows that GHT outperforms all its base-
lines on all datasets, exceeding the strongest base-
line by 2.9% in averaged BLEU scores. GHT-PS
outperforms all its baselines on 6 out of 7 datasets,
exceeding the strongest baseline by 4.4% on av-
erage. Model compression of the baselines may
sacrifice performance (especially on large datasets,
e.g., WMT en-de and en-fr), while GHT-PS is al-most not affected by the parameter reduction, even
surpassing GHT’s baselines with 44M parameters.
Ablation Study. We evaluate the impacts of the
features we choose for GHT and GHT-PS (Table 3).
We first ablate the diversification/homogenization
term of GCT (see Eq.4), which lowers the BLEU
scores. Next, we show the importance of GCT for
V2S. w/o GCT denotes that we directly perform
V2S at the very beginning without GCT. w/o GC14531
denotes that V2S is employed after normal train-
ing without Group Constrain (GC). Both ablation
models yield lower BLEU, because they do not
homogenize unnecessary heads and prepare them
for pruning. Next, we validate the power of Pillars
of Strength. w/o HUDS denotes we replace HUDS
with randomly switching off heads after GCT; w/o
PS stay denotes we keep random group members
instead of the Pillars of Strength after GCT. We ob-
serve lower BLEU in w/o HUDS andw/o PS stay .
Finally, we find that GC only needs to be added
before V2S. We denote the training stages before
and after V2S as stages 1 and 2. We compare the
proposed Stage 1-based GHT-PS with models that
perform GCT at Stage 2 ( w/ stage 2 GC ) and at
both stages ( w/ stage 1& 2 GC ). BLEU scores of
both ablation models decrease.
Effect of group compactness. We hypothesize
that more compact group patterns bring perfor-
mance gains to the GHT. Figure 2 shows the
correlation between the compactness of the final
group patterns and the final BLEU scores GHT
achieved on 5 IWSLT’14 development sets when
the GHT is fully converged in GCT. One data point
corresponds to an independent run. We choose
Silhouette Coefficient (SC) (Rousseeuw, 1987)
and Dunn’s Index (DI) (Bezdek and Pal, 1995) as
the measurements of group pattern compactness,
both of which increase as the intra-group samples
become more similar and the inter-group ones
become more separated. The SC and DI are
computed with the FMs of GHA (§ 3.1) and
controlled by tuning the αandβ(Eq.4).
Figure 2 shows that, within the normal range, the
BLEU scores rise with higher SC/DI scores, which
is in line with our assumption. The BLEUs start to
drop after the peak as the SC/DI scores increase,
because the very heavy group constraint prohibits
the model from learning useful task-specific knowl-
edge.
Effect of group number. Figure 3 shows the per-
formance trends of 16-head GHT and GHT-PS by
different numbers of group hidden units. For GHT,
different datasets have different optimal hidden unit
quantities, while a similar trend is observed. The
optimal group number is between 2 and 8, which is
in line with the claim that our group strategy is su-
perior to sole homogenization (1 group) or diversi-
fication (16 groups) strategies. For GHT-PS, when
the group number is larger than 1, it shows compa-
rable performance to GHT on most datasets. This
also verifies that non-PS heads can be switched off
without sacrificing performance.14532
Group pattern trends. Figure 4 shows the trends
of intra-group homogeneity (given by the 1stterm
of Eq.4(B)) and inter-group diversity (given by
the2nd term of Eq.4(B)) of GHT and vanilla
transformer in the training process on five IWSLT
datasets. By training, GHT yields higher intra-
group homogeneity and inter-group diversity abso-
lute values, leading to more compact groups, while
the vanilla transformer shows flattened trends. It
shows that GCT can effectively homogenize intra-
group heads and diversify inter-group heads.
Efficiency analysis. In Tables 1 and 2, the effi-
ciency metrics are controlled to be identical. Our
models with higher inference speed and lower
FLOPs show efficiency by culling redundant pa-
rameters. We also compare the efficiency metrics
by controlling BLEU scores. In Table 5, we select
models from the works in Table 1 and 2 that are
reported to achieve close BLEU scores on new-
stest2013 as the baselines. The GHT-PS-LITE is
a light version of GHT-PS that has a dof 1024.
Given BLEU ranges from 25.8 to 26.9, GHT-PS-
LITE is much more efficient than the baselines.
Noticeably, GHT-PS-LITE achieves 90.36% fewer
parameters, 62.05% faster inference speed, and
80.90% fewer FLOPs against Lite Conv which
yields the same BLEU as it.5.2 Abstractive Summarization
We evaluate the ability of our model to process
longer inputs via Abstractive Summarization on
the CNN-DailyMail dataset. We take vanilla
transformer as the backbone. Table 4 shows that
both GHT and GHT-PS achieve higher F1-Rouge
scores (Lin, 2004a) on this task. GHT-PS
achieves 4.1% higher Rouge-1, 18.6% higher
Rouge-2, and 4.4% higher Rouge-L against vanilla
transformer. It also achieves 0.4% higher Rouge-1,
31.1% higher Rouge-2 and 2.4% higher Rouge-L
against the best-performing baseline (Dynamic
Conv). Meanwhile, GHT-PS only takes 68.18%
parameters of the vanilla transformer and exhibits
higher inference speed and fewer FLOPs.
5.3 Language Modeling
We evaluate LM performance on WIKITTEXT-
103 dataset. The backbone is a decoder-only trans-
former with 16 layers and adaptive inputs (Baevski
and Auli, 2019). We compare with the backbone
model, as well as comparable SOTA LM mod-
els, including S4 (Gu et al., 2022), BERT-Large-
CAS (Wang et al., 2019), and GPT-2 Large (Rad-
ford et al., 2018).14533Table 6 shows that both GHT and GHT-PS
achieve lower perplexity (Vajapeyam, 2014) than
the baselines on both validation and test sets (2.9%
and 9.0% less perplexity against the backbone and
the best performing LM baseline, respectively).
Meanwhile, GHT-PS achieves 16.92% parameter
reduction, 2times faster inference speed, and 75%
FLOPs compared with the backbone.
6 Conclusion
In this paper, we assume that only focusing on the
most representative and distinctive features with
minimum resources may mitigate the redundancy
and over-parameterization issues of MHA. Accord-
ingly, we propose a divide-and-conquer strategy,
including GCT and V2S to mitigate the issues. The
improvements on three tasks and the extensive anal-
ysis verify our hypothesis and the effectiveness of
our redundancy optimization methods. Our study
may inspire future MHA design and training to
achieve higher accuracy and efficiency.
Limitations
In this work, we evaluate the proposed models for
NLP tasks only. However, tasks in other fields such
as Computer Vision may present a very different
input inductive bias, thus affecting the performance.
Moreover, our models are trained from scratch,
hence it is unknown whether the same divide-and-
conquer strategy works for pre-trained models. We
will study these limitations in the future to give a
more extensive exploration.
Ethics Statement
This article follows the ACL Code of Ethics. The
annotations are based on public datasets that do
not contain private data. The algorithm we devel-
oped is an architectural optimization technique for
improving model performance. To our best knowl-
edge, there are no foreseeable potential risks to
using this technique.
Acknowledgments
This research is supported by the Agency for Sci-
ence, Technology and Research (A*STAR) under
its AME Programmatic Funding Scheme (Project
#A18A2b0046).References145341453514536A Appendix
A.1 Trainig Settings
A.1.1 Machine Translation
We use Adam to optimize the MT models and set
theβ= 0.9, β= 0.98. We use the Inverse
Square Root Schedule (Vaswani et al., 2017) where
it first warms up for 4K steps until the learning
rate reaches 5×10, and then it exponentially
decays the learning rate. We apply early stop as
a termination condition. We apply a 0.3 dropout
rate for all Machine Translation models. A weight
decay of 10is used for all IWSLT 2014 models,
whereas for WMT models we use a weight decay
of 0. We apply a 0.1 label smoothing (Szegedy
et al., 2016; Pereyra et al., 2017) for the uniform
prior distribution over the vocabulary.
A.1.2 Language Modeling
Following Baevski and Auli (2019), we use Nes-
terov’s accelerated gradient method (Sutskever
et al., 2013) with a momentum of 0.99. We clip
the gradient norm if it exceeds 0.1 (Pascanu et al.,
2013). The learning rate is linearly warmed up
from 10to 1 for 16K steps and then annealed
using a cosine learning rate schedule (Loshchilov
and Hutter, 2017) with multiple cycles. Each cycle
doubles the number of updates than the previous
cycle and we shrink the maximum and minimum
learning rates by 0.75 compared to the previous
cycle. The initial minimum learning rate is 10
and the maximum is 1. We apply 0.2 adaptive soft-
max dropout rate, 0.1 attention dropout rate, and
0.1 activation dropout rate.
A.1.3 Abstractive Summarization
We use the same training setup with IWSLT 2014
models. We apply 0.1 clip norm and 0.2 attention
dropout. The model is warmed up for 10K updates.
A.2 Further Model Settings
Different α,β, and head feature maps ( ˆV,A, and
O) are preferred for different datasets to achieve
optimal performance. The Machine Translation
configurations are detailed in Table 7; Language
Modeling and Abstractive Summarization configu-
rations are detailed in Table 8.
Note that φ(v,a,o;z) =τφ(v;z)
+τφ(a;z) +τφ(o;z), we set one of the
{τ, τ, τ}to be 1, the other to be 0.A.3 Datasets and Evaluation
A.3.1 Efficiency Metrics settings
Inference speed. All inference speed results are
generated with beam size 5, batch size 256, max-
imum decoding length 10 on a single NVIDIA
Quadro RTX A6000.
FLOPs. We use the fvcoreto calculate the
FLOPs, with a fixed input length of 30.
A.3.2 Machine Translation
To fully evaluate the effectiveness of our meth-
ods, we evaluate seven MT datasets of IWSLT’14
and WMT 2014 benchmarks. We closely follow
the setup of Vaswani et al. (2017) for data prepa-
ration. WMT 2014 English-German dataset con-
sists of about 4.5M sentence pairs. It is encoded
with byte-pair encoding (Britz et al., 2017), hav-
ing a shared source-target vocabulary of about 40K
tokens. Following the standard setting (Vaswani
et al., 2017), we validate on newstest2013 and test
on newstest2014 for experiments on this dataset.
The WMT 2014 English-French dataset consists
of 36M sentence pairs and is encoded with a
joint source-target BPE of about 43K vocabular-
ies. Following the standard split, we validate on a
concatenation of newstest2012 and newstest2013
and test on newstest2014. For IWSLT’14 Ger-
man to English, IWSLT’14 English to German,
IWSLT’14 English to French, IWSLT’14 English
to Italian and IWSLT’14 Italian to English, we en-
code the sentence pairs with joint source-target
BPE. Following Edunov et al. (2018), the val-
idation set is randomly splited from the train-
ing set with a ratio of 1:23. The testset con-
sists TED.tst2010, TED.tst2011, TED.tst2012 and
TED.dev2010, TEDX.dev2012 for IWSLT’14 Ger-
man to English, IWSLT’14 English to German, and
IWSLT’14 English to French; the TEDX.dev2012
is replaced by TEDX.dev2014 for IWSLT’14 En-
glish to Italian and IWSLT’14 Italian to English.
For all Machine Translation datasets, we use
detokenized BLEU. WMT 2014 English-German
and WMT 2014 English-French are evaluated with
a beam size 4 and length penalty 0.6; IWSLT’14
datasets are evaluated with a beam size 5 and with-
out length penalty.14537ModelIWSLT ( α/β/FM ) WMT ( α/β/FM )
de-en it-en en-de en-it en-fr en-de en-fr
GHT 0.7/0.5/ ˆV 0.3/0.5/ ˆV 0.3/0.1/ A 0.3/0.3/ ˆV 0.7/0.7/ ˆV 0.5/0.5/ ˆV 0.3/0.3/ ˆV
GHT-PS 0.5/0.7/O 0.3/0.3/ A 0.3/0.7/ O 0.3/1/ ˆV 0.5/0.3/ A 0.5/0.5/ ˆV 0.3/0.3/ ˆV
Model α/β/FM
GHT 0.5/0.5/ ˆV
GHT-PS 0.5/0.5/ ˆV
A.3.3 Language Modeling
We evaluate LM on WIKITEXT-103 (Merity et al.,
2017) which has about 100M tokens and a 260K
BPE vocabulary. Following Baevski and Auli
(2019), we use perplexity as an evaluation met-
ric and a context window of 2047 at the inference
stage.
A.3.4 Abstractive Summarization
We also evaluate on CNN-DailyMail (Hermann
et al., 2015) for AS to test the ability of GHT in
hard tasks with long inputs. The dataset comprises
over 280K news articles paired with multi-sentence
summaries. Following Wu et al. (2019), articles
are truncated to 512 tokens and encoded with 50K
BPE. We use F1-Rouge (Lin, 2004b) to evaluate
the performance, including Rouge-1, Rouge-2 and
Rouge-L.14538ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7
/squareA2. Did you discuss any potential risks of your work?
The study is theoretical. It does not involve human in the experiment or involve ethic issues. It will
not result in any potential risks.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 4; Appendix A.3
/squareB1. Did you cite the creators of artifacts you used?
Section 4; Appendix A.3
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The tools are for public use.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No speciﬁc intended use is speciﬁed by the artifact creators.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
They does not contain any information that names or uniquely identiﬁes individual people or offensive
content.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 4; Appendix A.3
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A.3
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 5; Appendix A.114539/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4, 5; Appendix A.1, 2, 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix A.1, 2, 3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.14540