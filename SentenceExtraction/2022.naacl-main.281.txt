
Vyas Raina
ALTA Institute, Cambridge University
vr313@cam.ac.ukMark Gales
ALTA Institute, Cambridge University
mjfg@cam.ac.uk
Abstract
Deep learning based systems are susceptible
to adversarial attacks, where a small, imper-
ceptible change at the input alters the model
prediction. However, to date the majority of
the approaches to detect these attacks have
been designed for image processing systems.
Many popular image adversarial detection ap-
proaches are able to identify adversarial exam-
ples from embedding feature spaces, whilst in
the NLP domain existing state of the art de-
tection approaches solely focus on input text
features, without consideration of model em-
bedding spaces. This work examines what dif-
ferences result when porting these image de-
signed strategies to Natural Language Process-
ing (NLP) tasks - these detectors are found to
not port over well. This is expected as NLP
systems have a very different form of input:
discrete and sequential in nature, rather than
the continuous and fixed size inputs for images.
As an equivalent model-focused NLP detection
approach, this work proposes a simple sentence-
embedding "residue" based detector to identify
adversarial examples. On many tasks, it out-
performs ported image domain detectors and
recent state of the art NLP specific detectors.
1 Introduction
In the last decade deep learning based models have
demonstrated success in a wide range of applica-
tion areas, including Natural Language Processing
(NLP) (Vaswani et al., 2017) and object recogni-
tion (He et al., 2015). These systems may be de-
ployed in mission critical situations, where there is
the requirement for a high level of robustness. How-
ever, Szegedy et al. (2014) demonstrated that deep
models have an inherent weakness: small perturba-
tions in the input can yield significant, undesired,
changes in the output from the model. These input
perturbations were termed adversarial examples
and their generation adversarial attacks.Adversarial attacks have been developed for sys-
tems operating in various domains: image sys-
tems (Serban et al., 2020; Biggio and Roli, 2017;
Bhambri et al., 2019) and NLP systems (Lin et al.,
2014; Samanta and Mehta, 2017; Rosenberg et al.,
2017). The characteristics of the input can be
very different between these application domains.
Broadly, the nature of inputs can be described us-
ing two key attributes: static (fixed length) vs se-
quential and continuous vs discrete. Under this
categorisation, image inputs are continuous and
static, whilst NLP inputs are discrete and sequen-
tial. This work argues that due to the fundamental
differences in the input and resulting adversarial
perturbations in the different domains, adversarial
attack behaviour can vary significantly from one
domain to another. Hence, the extensive research
on exploring and understanding adversarial pertur-
bation behaviour in the continuous, static world of
image systems does not necessarily transfer well to
the NLP tasks.
For adversarial attack generation, a number of
specific NLP attacks have been proposed that are
designed for NLP task inputs (Lin et al., 2014;
Samanta and Mehta, 2017; Rosenberg et al., 2017;
Huang et al., 2018; Papernot et al., 2016; Grosse
et al., 2016; Sun et al., 2018; Cheng et al., 2018;
Blohm et al., 2018; Neekhara et al., 2018; Raina
et al., 2020; Jia and Liang, 2017; Minervini and
Riedel, 2018; Niu and Bansal, 2018; Ribeiro et al.,
2018; Iyyer et al., 2018; Zhao et al., 2017). How-
ever, there has been less research on developing
defence schemes. These defence strategies can be
split into two main groups: model modification ,
where the model or data is altered at training time
(e.g. adversarial training (Yoo and Qi, 2021)) and
detection , where external systems or algorithms are
applied to trained models to identify adversarial at-
tacks. As model modification approaches demand
re-training of models, detection approaches are usu-
ally considered easier for implementation on de-3836ployed systems and thus are often preferred. Hence,
this work investigates the portability of popular de-
tection approaches designed for image systems to
NLP systems. Furthermore, this work introduces a
specific NLP detection approach that exploits the
discrete nature of the inputs for NLP systems. This
approach out-performs standard schemes designed
for image adversarial attack detection, as well as
other NLP detection schemes.
The proposed NLP specific detection approach
will be referred to as residue detection , as it is
shown that adversarial attacks in the discrete, word
sequence space result in easily detectable residual
components in the sentence embedding space. This
residue can be easily detected using a simple lin-
ear classifier operating in the encoder embedding
space. In addition, this work shows that even when
an adversary has knowledge of the linear residual
detector, they can only construct attacks at a frac-
tion of the original strength. Hence this work ar-
gues that realistic (word level, semantically similar)
adversarial perturbations at the natural language in-
put of NLP systems leave behind easily detectable
residue in the sentence embedding. Interestingly,
the residue detection approach is shown to perform
poorly when used to detect attacks in the image do-
main, supporting the hypothesis that the nature of
the input has an important influence on the design
of effective defence strategies.
2 Related Work
Previous work in the image domain has analysed
the output of specific layers in an attempt to iden-
tify adversarial examples or adversarial subspaces.
First, Feinman et al. (2017) proposed that adver-
sarial subspaces have a lower probability density,
motivating the use of the Kernel Density (KD) met-
ric to detect the adversarial examples. Nevertheless,
Ma et al. (2018) found Local Intrinsic Dimension-
ality (LID) was a better metric in defining the sub-
space for more complex data. In contrast to the
local subspace focused approaches of KD and LID,
Carrara et al. (2019b) showed that trajectories of
hidden layer features can be used to train a LSTM
network to accurately discriminate between authen-
tic and adversarial examples. Out performing all
previous methods, Lee et al. (2018) introduced an
effective detection framework using Mahalanobis
Distance Analysis (MDA), where the distance is
calculated between a test sample and the closest
class-conditional Gaussian distribution in the spacedefined by the output of the final layer of the clas-
sifier (logit space). Li and Li (2016) also explored
using the output of convolutional layers for image
classification systems to identify statistics that dis-
tinguish adversarial samples from original samples.
They find that by performing a PCA decomposi-
tion the statistical variation in the least principal
directions is the most significant and can be used
to separate original and adversarial samples. How-
ever, they argue this is ineffective as an adversary
can easily suppress the tail distribution. Hence, Li
and Li (2016) extract statistics from the convolu-
tional layer output to train a cascade classifier to
separate the original and adversarial samples. Most
recently, Mao et al. (2019) avoid the use of artifi-
cially designed metrics and combine the adversarial
subspace identification stage and the detecting ad-
versaries stage into a single framework, where a
parametric model adaptively learns the deep fea-
tures for detecting adversaries.
In contrast to the embedding space detection
approaches, Cohen et al. (2019) shows that influ-
ence functions combined with Nearest Neighbour
distances perform comparably or better than the
above standard detection approaches. Other de-
tection approaches have explored the use of un-
certainty: Smith and Gal (2018) argues that ad-
versarial examples are out of distribution and do
not lie on the manifold of realdata. Hence, a dis-
criminative Bayesian model’s epistemic (model)
uncertainty should be high. Therefore, calcula-
tions of the model uncertainty are thought to be
useful in detecting adversarial examples, indepen-
dent of the domain. However, Bayesian approaches
aren’t always practical in implementation and thus
many different approaches to approximate this un-
certainty have been suggested in literature (Leibig
et al., 2017; Gal, 2016; Gal and Ghahramani, 2016).
There are a number of existing NLP specific
detection approaches. For character level attacks,
detection approaches have exploited the grammat-
ical (Sakaguchi et al., 2017) and spelling (Mays
et al., 1991; Islam and Inkpen, 2009) inconsisten-
cies to identify and detect the adversarial samples.
However, these character level attacks are unlikely
to be employed in practice due to the simplicity
with which they can be detected. Therefore, detec-
tion approaches for the more difficult semantically
similar attack samples are of greater interest, where
the meaning of the textual input is maintained with-
out compromising the spelling or grammatical in-3837tegrity. To tackle such word-level, semantically
similar examples, Zhou et al. (2019) designed a
discriminator to classify each token representation
as part of an adversarial perturbation or not, which
is then used to ‘correct’ the perturbation. Other
detection approaches (Raina et al., 2020; Han et al.,
2020; Minervini and Riedel, 2018) have shown
some success in using perplexity to identify adver-
sarial textual examples. Most recently, Mozes et al.
(2020) achieved state of the art performance with
the Frequency Guided Word Substitution (FGWS)
detector, where a change in model prediction after
substituting out low frequency words is revealing
of adversarial samples.
3 Adversarial Attacks
An adversarial attack is defined as an imperceptible
change to the input that causes an undesired change
in the output of a system. Often, an attack is found
for a specific data point, x. Consider a classifier F,
with parameters ˆθ, that predicts a class label for an
input data point, x, sampled from the input distri-
bution X. A successful adversarial attack is where
a perturbation δat the input causes the system to
miss-classify,
F(x+δ)̸=F(x). (1)
When defining adversarial attacks, it is impor-
tant consider the interpretation of an imperceptible
change. Adversarial perturbations are not consid-
ered effective if they are easy to detect. Hence, the
size of the perturbation must be constrained:
G(x,x+δ)≤ϵ, (2)
where the function G()describes the form of con-
straint and ϵis a selected threshold of imperceptibil-
ity. Typically, when considering continuous space
inputs (such as images), a popular form of the con-
straint of Equation 2, is to limit the perturbation in
thelnorm, with p∈[1,∞), e.g.||δ||≤ϵ.
For whitebox attacks in the image domain, the
dominant attack approach has proven to be Pro-
jected Gradient Descent (PGD) (Kurakin et al.,
2016). The PGD approach, iteratively updates the
adversarial perturbation, δ, initialised as δ=0.
Each iterative step moves the perturbation in the
direction that maximises the loss function, L, used
in the training of the model,
δ=clip(δ+α∇L(x+δ;ˆθ)),(3)where αis an arbitrary step-size parameter and the
clipping function, clip, ensures the impercepti-
bility constraint of Equation 2 is satisfied.
When considering the NLP domain, a sequen-
tial, discrete input of Lwords, can be explicitly
represented as,
x=w=w, w, . . . , w, w, (4)
where, the discrete word tokens, w, are often
mapped to a continuous, sequential word embed-
ding (Devlin et al., 2019) space,
h=h,h, . . . ,h,h. (5)
Attacks must take place in the discrete text space,
x+δ=w=w, w, . . . , w, w.(6)
This requires a change in the interpretation of the
perturbation δ. It is not simple to define an ap-
propriate function G()in Equation 2 for word se-
quences. Perturbations can be measured at a char-
acter or word level. Alternatively, the perturba-
tion could be measured in the vectorized embed-
ding space (Equation 5), using for example l-norm
based (Goodfellow et al., 2015) metrics or cosine
similarity (Carrara et al., 2019a), which have been
used in the image domain. However, constraints in
the embedding space do not necessarily achieve im-
perceptibility in the original word sequence space.
The simplest approach is to use a variant of an edit-
based measurement (Li et al., 2018), L(), which
counts the number of changes between the original
sequence, wand the adversarial sequence w,
where a change is a swap/addition/deletion, and
ensures it is smaller than a maximum number of
changes, N,
L(w, w)≤N. (7)
For the NLP adversarial attacks this work only
examines word-level attacks, as these are consid-
ered more difficult to detect than character-level
attacks. As an example, for an input sequence of L
words, a N-word substitution adversarial attack,
w, applied at word positions n, n, . . . , n
gives the adversarial output, w
w=w, . . . , w, w, w, . . . ,
w, w, w, . . . , w. (8)
The challenge is to select which words to replace,
and what to replace them with. A simple yet ef-
fective substitution attack approach that ensures a3838small change in the semantic content of a sentence
is to use saliency to rank the word positions, and to
use word synonyms for the substitutions (Ren et al.,
2019). This attack is termed Probability Weight
Word Saliency (PWWS). The highest ranking word
word can be swapped for a synonym from a pre-
selected list of given synonyms. The next most
highly ranked word is substituted in the same man-
ner and the process is repeated till the required N
words have been substituted.
The above approach is limited to attacking spe-
cific word sequences and so cannot easily be gener-
alised to universal attacks (Moosavi-Dezfooli et al.,
2016), where the same perturbation is used for
all inputs. For this situation, a simple solution
is concatenation (Wang and Bansal, 2018; Blohm
et al., 2018), where for example, the same N-length
sequence of words is appended to each input se-
quence of words, as described in Raina et al. (2020).
Here,
w=w, . . . , w, w, . . . , w. (9)
In both the substitution attack (Equation 8) and
the concatenation attack (Equation 9), the size of
the attack can be measured using the number of
edits,L(w, w) =N.
4 Adversarial Attack Detection
For a deployed system, the easiest approach to de-
fend against adversarial attacks is to use a detection
process to identify adversarial examples without
having to modify the existing system.
For the image domain Section 2 discusses many
of the standard detection approaches. In this work,
we select two distinct approaches that have been
generally successful: uncertainty (Smith and Gal,
2018), where adversarial samples are thought to
result in greater epistemic uncertainty and Maha-
lanobis Distance (Lee et al., 2018), where the Ma-
halanobis distance in the logit space is indicative of
how out of distribution a sample is (adversarial sam-
ples are considered more out of distribution). In the
NLP domain, when excluding trivial grammar and
spelling based detectors, perplexity based detectors
can be used (Raina et al., 2020). Many other NLP
specific detectors (Zhou et al., 2019; Han et al.,
2020; Minervini and Riedel, 2018) have been pro-
posed, but Mozes et al. (2020)’s FGWS detector
is considered the state of art and is thus selected
for comparison. Here low frequency words in an
input are substituted for higher frequency wordsand the change in model prediction is measured
- adversarial samples are found to generally have
a greater change. This work introduces a further
NLP specific detector: residue detection , described
in detail in Section 4.1.
When considering any chosen detection mea-
sureF, a threshold βcan be selected to decide
whether an input, w, is adversarial or not, where
F(w)> β, implies that wis an adversarial
sample. To assess the success of the adversarial
attack detection processes, precision-recall curves
are used. For the binary classification task of iden-
tifying an input as adversarially attacked or not, at
a given threshold β, the precision and recall val-
ues can be computed as prec=TP/TP+FPand
rec=TP/TP+FN, where TP, FP and FN are
the standard true-positive, false-positive and false-
negative definitions. A single point summary of
precision-recall curves is given with the Fscore.
4.1 Residue Detection
In this work we introduce a new NLP detection
approach, residue detection , that aims to exploit
the nature of the NLP input space, discrete and
sequential. Here we make two hypotheses:
1.Adversarial samples in an encoder embedding
space result in larger components ( residue )
in central PCA eigenvector components than
original examples.
2.The residue is only significant (detectable) for
systems operating on discrete data (e.g. NLP
systems).
The rationale behind these hypotheses is discussed
next.
Deep learning models typically consist of many
layers of non-linear activation functions. For exam-
ple, in the NLP domain systems are usually based
on layers of the Transformer architecture (Vaswani
et al., 2017). The complete end-to-end model F()
can be treated as a two stage process, with an ini-
tial set of layers forming the encoding stage,F()
and the remaining layers forming the output stage,
F(), i.e.F(x) =F(F(x)).
If the encoding stage of the end-to-end classifier
is sufficiently powerful, then the embedding space
F(x)will have compressed the useful informa-
tion into very few dimensions, allowing the output
stage to easily separate the data points into classes
(for classification) or map the data points to a con-
tinuous value (for regression). A simple Principal3839Component Analysis (PCA) decomposition of this
embedding space can be used to visualize the level
of compression of the useful information. The PCA
directions can be found using the eigenvectors of
the covariance matrix, C, of the data in the en-
coder embedding space. If {q}, where dis the
dimension of the encoder embedding space, repre-
sent the eigenvectors of Cordered in descending
order by the associated eigenvalue in magnitude,
then it is expected that almost all useful information
is contained within the first few principal directions,
{q}, where p≪d. Hence, the output stage,
F()will implicitly use only these useful compo-
nents. The impact of a successful adversarial per-
turbation, F(x+δ), is the significant change in
the components in the principal eigenvector direc-
tions{q}, to allow fooling of the output stage.
Due to the complex nature of the encoding stage
and the out of distribution nature of the adversarial
perturbations, there are likely to be residual compo-
nents in the non-principal {q}eigenvector
directions. These perturbations in the non-principal
directions are likely to be more significant for the
central eigenvectors, as the encoding stage is likely
to almost entirely compress out components in the
least principal eigenvector directions, {q},
where d≈d. Hence, {q}can be viewed
as a subspace containing adversarial attack residue
that can be used to identify adversarial examples.
The existence of adversarial attack residue in
the central PCA eigenvector directions, {q},
suggests that in the encoder embedding space,
F(x), adversarial and original examples are lin-
early separable. This motivates the use of a simple
linear classifier as an adversarial attack detector,
P(adv|x) =σ(WF(x) +b), (10)
where Wandbare the parameters of the linear
classifier to be learnt and σis the sigmoid function.
The above argument cannot predict how signifi-
cant the residue in the central eigenvector space is
likely to be. For the discrete space NLP attacks, the
input perturbations are semantically small, whilst
for continuous space image attacks the perturba-
tions are explicitly small using a standard l-norm.
Hence, it is hypothesised that NLP perturbations
cause larger errors to propagate through the system,
resulting in more significant residue in the encoder
embedding space than that for image attacks. Thus,
the residue technique is only likely to be a feasible
detection approach for discrete text attacks.The hypotheses made in this section are analysed
and empirically verified in Section 5.3.
5 Experiments
5.1 Experimental Setup
Table 1 describes four NLP classification datasets:
IMDB (Maas et al., 2011); Twitter (Saravia et al.,
2018); AG News (Zhang et al., 2015) and DB-
pedia (Zhang et al., 2015). Further, a regression
dataset, Linguaskill-Business (L-Bus) (Chambers
and Ingham, 2011) is included. The L-Bus data is
from a multi-level prompt-response free speaking
test i.e. candidates from a range of proficiency lev-
els provide open responses to prompted questions.
Based on this audio input a system must predict a
score of 0-6 corresponding to the 6 CEFR (Council
of Europe, 2001) grades. This audio data was tran-
scribed using an Automatic Speech Recognition
system with an average word error rate of 19.5%.
All NLP task models were based on the Trans-
former encoder architecture (Vaswani et al., 2017).
Table 2 indicates the specific architecture used for
each task and also summarises the classification
and regression performance for the different tasks.
For classification tasks, the performance is mea-
sured by top 1 accuracy, whilst for the regression
task (L-Bus), the performance is measured using
Pearson Correlation Coefficient (PCC).
Table 3 shows the impact of realistic adversar-
ial attacks on the tasks: substitution (sub) attack
(Equation 8), which replaces the Nmost salient
tokens with a synonym defined by WordNet, as3840dictated by the PWWS attack algorithm described
in Section 3; or a targeted universal concatenation
(con) attack (Equation 9), used for the regression
task on the L-Bus dataset, seeking to maximise the
average score output from the system by appending
the same Nwords to the end of each input. For
classification tasks, the impact of the adversarial at-
tack is measured using the fooling rate, the fraction
of originally correctly classified points, misclassi-
fied after the attack, whilst for the regression task,
the impact is measured as the average increase in
the output score.
5.2 Results
Section 4.1 predicts that adversarial attacks in the
discrete text space leave residue in a system’s en-
coder embedding space that can be detected using
a simple linear classifier. Hence, using the 12-
layer Transformer encoder’s output CLS token em-
bedding as the encoder embedding space for each
dataset’s trained system (Table 2), a simple linear
classifier, as given in Equation 10, was trained
to detect adversarial examples from the adversarial
attacks given for each dataset in Table 3. The train-
ing of the detection linear classifier was performed
on the training data (Table 1) augmented with an
equivalent adversarial example for each original
input sample in the dataset. Using the test data
samples augmented with adversarial examples (as
defined by Table 3), Table 4 compares the efficacy
of the linear residue detector to other popular de-
tection strategies(from Section 4) using the best
Fscore. It is evident from the high F-scores, that
for most NLP tasks the linear detection approach is
better than other state of the art NLP specific and
ported image detection approaches.
However, an adversary may have knowledge of
the detection approach and may attempt to design
an attack that directly avoids detection. Hence, for
each dataset, the attack approaches were repeated
with the added constraint that any attack words that
resulted in detection were rejected. The impact
of attacks that suppress detection have been pre-
sented in Table 5. Generally, it is shown across all
NLP tasks that an adversary that attempts to avoid
detection of its residue by a previously trained lin-
ear classifier, can only generate a significantly less
powerful adversarial attack.
5.3 Analysis
The aim of this section is verify that the success of
the residue detector can be explained by the two
main hypotheses made in Section 4.1. The claim
that residue is left by adversarial samples in the
central PCA eigenvector components is explored
first. For each NLP task a PCA projection matrix
is learnt in the encoder embedding space using the
original training data samples (Table 1). Using
the test data, the residue in the embedding space
can be visualized through a plot of the average
(across the data) component, ρ=/summationtextρin
each eigenvector direction, qof the original and
attacked data, where
ρ=/vextendsingle/vextendsingleF(x)q/vextendsingle/vextendsingle, (11)
withxbeing the jth data point. Figure 1 shows
an example plot for the Twitter dataset, where ρis
plotted against the eigenvalue rank, ifor the origi-
nal and attacked data examples. Residue plots for3841other datasets are included in Appendix A. Next,
it is necessary to verify that the residue detector
specifically uses the residue in the central eigen-
vector components to distinguish between original
and adversarial samples. To establish this, each en-
coder embedding, F(x)’s components not within
a target subspace of PCA eigenvector directions
{q}, are removed, i.e. we have a projected em-
bedding, x=F(x)−/summationtextqF(x)q,
where wis a window size to choose. Now, us-
ingF(x)and a residue detector trained us-
ing the modified embeddings, x, the classifier’s
(F(x)) accuracy and detector performance
(measured using F1 score) can be found. Figure 2
shows the performance of the classifier ( F(x))
and the detector for different start components p,
with the window size, w= 5. It is clear that the
principal components hold the most important in-
formation for classifier accuracy, but, as hypothe-
sised in Section 4.1, it is the more central eigen-
vector components that hold the most information
useful for the residue detector, i.e. the subspace de-
fined by {q}holds the most detectable residue
from adversarial examples.
The second hypothesis in Section 4.1 claims that
the existence of residue in the central eigenvector
components is due to the discrete nature of NLP
adversarial attacks. Hence, to analyze the impact
of the discrete aspect of the attack, an artificial
continuous space attack was constructed for the
Twitter NLP system, where the continuous input
embedding layer space (Equation 5) of the system
is the space in which the attack is performed. Us-
ing the Twitter emotion classifier, a PGD (Equation
3) attack was performed on the input embeddings
for each token, where the perturbation size was
limited to be ϵ= 0.1in the lnorm, achieving a
fooling rate of 0.73. Note that this form of attack
is artificial, as a real adversary can only modify the
discrete word sequence (Equation 4). To compare
the influence of discrete and continuous attacks
on the same system, the average (across dataset)
land lnorms of the perturbations in the input
layer embedding space were found. Further, a sin-
gle value summary, N, of the residue plot (e.g.
Figure 1), was calculated for each attack. Nis the
average difference in standard deviations between
theoriginal component mean, ρ andattack
mean, ρ ,
N=1
I/summationdisplay/vextendsingle/vextendsingle/vextendsingleρ−ρ/vextendsingle/vextendsingle/vextendsingle
/radicalig
Var[ρ]. (12)
Table 6 reports these metrics for the discrete and
artificial continuous NLP adversarial attacks on the
Twitter system. It is apparent that perturbation
sizes for the discrete attacks are significantly larger.
Moreover, Nis significantly smaller for the con-
tinuous space attack, indicating that the residue left
by continuous space adversarial attacks is smaller.
To explicitly observe the impact of the nature
of data on detectors, adversarial attacks are con-
sidered in four domains: the discrete, sequential3842NLP input space (NLP-disc); the artificial con-
tinuous, sequential embedding space of an NLP
model (NLP-cont); the continuous, static image in-
put space (Img-cont) and a forced discretised, static
image input space (Img-disc). For the NLP-disc
and NLP-cont the same attacks as in Table 6 are
used. For the continuous image domain (Img-cont),
a VGG-16 architecture image classifier trained on
CIFAR-100 (Krizhevsky et al., 2009) image data
(achieving a top-5 accuracy of 90.1%) and attacked
using a standard lPGD approach (Equation 3) is
used. For the discrete image domain (Img-disc),
the CIFAR-100 images, X∈Z were discre-
tised using function Q:Z→Z, where
Z={0,1,2, . . . , 255}. In this work 2-
bit quantization was used, i.e. q= 4. With this
quantization, a VGG-16 architecture was trained to
achieve 78.2% top-5 accuracy. To perform a dis-
crete space attack, a variant of the PWWS synonym
substitution attack (Section 3) was implemented,
where synonyms were interpreted as closest permit-
ted quantisation values and Npixel values were
substituted. For these different domains, Table 7
compares applicable detection approaches (certain
NLP detection approaches are not valid outside
the word sequence space) using the best Fscore,
where different attack perturbation sizes are consid-
ered ( Nsubstitutions for discrete attacks and for
continuous attacks |δ| ≤ϵfor perturbation δ).
In the discrete domains, the residue detection
approach is better than all the other approaches.
However, in the continuous data type domains, the
Mahalanobis Distance dominates as the detection
approach, with the residue detection approach per-
forming the worst. As predicted by the second
hypothesis of Section 4.1, the lack of success of
the residue detection approach is expected here -
the residue detection approach is only successful
for discrete space attacks.To verify that the residue detection approach is
agnostic to the type of attack, the residue detector
trained on substitution attack examples was evalu-
ated on concatenation attack examples. Using the
Twitter dataset, a N= 3concatenation attack was
applied, achieving a fooling rate of 0.59. In this
setting, the residue detector (trained on the N= 6
substitution adversarial examples) achieved a F
score of 0.81, which is comparable to the original
score of 0.84 (from Table 4). This shows that even
with different attack approaches similar forms of
residue are produced, meaning a residue detector
can be used even without knowledge of the type of
adversarial attack.
6 Conclusions
In recent years, deep learning systems have been
deployed for a large number of tasks, ranging from
the image to the natural language domain. How-
ever, small, imperceptible adversarial perturbations
at the input, have been found to easily fool these
systems, compromising their validity in high-stakes
applications. Defence strategies for deep learning
systems have been extensively researched, but this
research has been predominantly carried out for
systems operating in the image domain. As a result,
the adversarial detection strategies developed, are
inherently tuned to attacks on the continuous space
of images. This work shows that these detection
strategies do not necessarily transfer well to attacks
on natural language processing systems. Hence, an
adversarial attack detection approach is proposed
that specifically exploits the discrete nature of per-
turbations for attacks on discrete sequential inputs.
The proposed approach, termed residue detec-
tion, demonstrates that imperceptible attack pertur-
bations on natural language inputs tend to result
in large perturbations in word embedding spaces,
which result in distinctive residual components.
These residual components can be identified using
a simple linear classifier. This residue detection ap-
proach was found to out-perform both detection ap-
proaches ported from the image domain and other
state of the art NLP specific detectors.
The key finding in this work is that the nature
of the data (e.g. discrete or continuous) strongly
influences the success of detection systems and
hence it is important to consider the domain when
designing defence strategies.38437 Limitations, Risks and Ethics
A limitation of the residue approach proposed in
this work is that it requires training on adversarial
examples, which is not necessary for other NLP de-
tectors. This means there is a greater computational
cost associated with this detector. Moreover, asso-
ciated with this limitation is a small risk, where in
process of generating creative adversarial examples
to build a robust residue detector, the attack gen-
eration scheme may be so strong that it can more
easily evade detection from other existing detectors
already deployed in industry. There are no further
ethical concerns related to this detector.
8 Acknowledgements
This paper reports on research supported by
Cambridge Assessment, University of Cambridge.
Thanks to Cambridge English Language Assess-
ment for support and access to the Linguaskill-
Business data. The authors would also like to thank
members of the ALTA Speech Team.
References38443845Appendix A
Training Details
For each NLP dataset, pre-trained base (12-layer,
768-hidden dimension, 110M parameters) Trans-
former encoderswere fine-tuned during train-
ing. Table A.1 gives the training hyperparameters:
learning rate (lr), batch size (bs) and the number of
training epochs. In all training regimes an Adam
optimizer was used. With respect to hardware,
NVIDIA V olta GPU cores were used for training
all models.
Dataset Model lr bs epochs
IMDB BERT 1e-5 8 2
Twitter ELECTRA 1e-5 8 2
AG News BERT 1e-5 8 2
DBpedia ELECTRA 1e-5 8 2
L-Bus BERT 1e-6 16 5
Experiments
Figure A.1 presents the impact of adversarial at-
tacks of different perturbation sizes, Non each
NLP dataset. All classification datasets’ models
underwent saliency ranked, N-word substitution
attacks described in Equation 8, whilst the regres-
sion dataset, L-Bus, was subject to a N-word con-
catenation attack as in Equation 9. For the classi-
fication tasks the impact of the adversarial attacks
was measured using fooling rate, whilst for the L-
Bus dataset task, the average output score from the
system is given. Figure A.2 gives the encoder em-
bedding space PCA residue plots for all the datasets
not included in the main text.
Table A.2 compares the impact on error sizes
(using landlnorms) and the residue plot met-
ric,Nfor the original text space discrete attacks
and an artificial input embedding space continuous
attack. The purpose of this table is to present the
results for the datasets not included in the main text
in Table 6.3846
Dataset Attack N lerror lerror
IMDBDiscrete 0.181 73.54.02
Continuous 0.111 6.730.09
TwitterDiscrete 1.201 50.23.26
Continuous 0.676 5.350.08
AG NewsDiscrete 0.642 67.93.35
Continuous 0.393 5.410.09
DBpediaDiscrete 1.355 57.43.29
Continuous 0.991 6.570.09
L-BusDiscrete 0.201 94.65.91
Continuous 0.135 8.220.0738473848