
Maarten SapRonan Le BrasDaniel FriedYejin ChoiAllen Institute for AI, Seattle, WA, USALanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, USAPaul G. Allen School of Computer Science, University of Washington, Seattle, WA, USA
maartensap@cmu.edu
Abstract
Social intelligence and Theory of Mind
(TM), i.e., the ability to reason about the
different mental states, intents, and reactions
of all people involved, allow humans to effec-
tively navigate and understand everyday social
interactions. As NLP systems are used in in-
creasingly complex social situations, their abil-
ity to grasp social dynamics becomes crucial.
In this work, we examine the open question
of social intelligence and Theory of Mind in
modern NLP systems from an empirical and
theory-based perspective. We show that one
of today’s largest language models (GPT-3;
Brown et al., 2020) lacks this kind of social
intelligence out-of-the box, using two tasks:
S IQ(Sap et al., 2019b), which mea-
sures models’ ability to understand intents and
reactions of participants of social interactions,
and TM(Le et al., 2019), which measures
whether models can infer mental states and re-
alities of participants of situations.
Our results show that models struggle sub-
stantially at these Theory of Mind tasks, with
well-below-human accuracies of 55% and 60%
on S IQand TM, respectively. To
conclude, we draw on theories from pragmat-
ics to contextualize this shortcoming of large
language models, by examining the limita-
tions stemming from their data, neural archi-
tecture, and training paradigms. Challeng-
ing the prevalent narrative that only scale is
needed, we posit that person-centric NLP ap-
proaches might be more effective towards neu-
ral Theory of Mind.
1 Introduction
With the growing prevalence of AI and NLP sys-
tems in everyday social interactions, the need for
AI systems with social intelligence andTheory of
Mind (TM), i.e., the ability to infer and reason
about the intents, feelings, and mental states of oth-
ers, becomes increasingly evident (Pereira et al.,Figure 1: Theory of Mind is the ability for humans to
reason about the intents, reactions, and mental states of
others. We asses these abilities in LLMs through two
question-answering tasks that measure social common-
sense and emotional intelligence (S IQ; top)
and reasoning about people’s mental states and reali-
ties (TM; bottom); ﬁnding that GPT-3 ( ) struggles
on both tasks. We discuss why that may be, drawing
from theories of the pragmatics of language.
2016; Langley et al., 2022). For humans, Theory of
Mind is a crucial component that enables us to in-
teract and communicate effectively with each other
(Premack and Woodruff, 1978; Apperly, 2010). It
allows us, for example, to infer that someone likely
feels boastful instead of ashamed after winning a
wrestling match (Fig. 1; top). In addition, TM
also enables us to reason about people’s mental re-
alities, e.g., if someone was out of the room while
a pen was moved, she will likely search for the pen3762where she last saw it instead of where it was moved
to (Fig. 1; bottom).
While humans develop it naturally, TMand
social intelligence remain elusive goals for modern
AI systems (Choi, 2022), including large neural
language models (LLMs). With advances in scal-
ing the sizes of models and datasets, these LLMs
have proven very impressive at generating human-
like language for conversational, summarization,
or sentence continuation settings, often with zero
to few examples to learn from (Brown et al., 2020;
Clark et al., 2021; Chowdhery et al., 2022). How-
ever, increasing scrutiny has shed light on the short-
comings of these LLMs, showing that they often
fall prey to spurious correlational patterns instead
of displaying higher-order reasoning (Elkins and
Chun, 2020; Dale, 2021; Marcus, 2022).
In line with EMNLP 2022’s theme, we examine
the open research question of whether and how
much LLMs—which are the backbone of most
modern NLP systems—exhibit social intelligence
andTMabilities. Using some of the largest En-
glish models in existence ( GPT-3 ; Brown et al.,
2020), we demonstrate that out-of-the-box LLMs
struggle at two types of reasoning abilities that
requisites for Theory of Mind (shown in Fig. 1).
We argue that these reasoning abilities are neces-
sary but not sufﬁcient for Theory of Mind, and that
larger models will likely provide upper bounds on
what equivalent-but-smaller models are capable of.
We ﬁrst assess whether LLMs can reason about
social commonsense and emotional intelligence
with respect to social interactions (§3), using the
S IQbenchmark (Sap et al., 2019b) illus-
trated in Fig. 1 (top). Results show our best per-
forming few-shot GPT-3 setup achieving only 55%
accuracy, lagging >30% behind human perfor-
mance. Furthermore, social reasoning about the
protagonists of situations is easier for GPT-3 (5-
15% absolute difference) compared to reasoning
about other secondary participants.
Second, we measure LLMs’ ability to under-
stand other people’s mental states and realities in
short stories (§4). We use the TMQA bench-
mark (illustrated in Fig. 1; bottom; Le et al., 2019),
which was inspired by the classic Sally-Ann False
Belief Theory of Mind test (Baron-Cohen et al.,
1985). Here, our results show that GPT-3 models
peak at 60% accuracy on questions about partic-
ipants’ mental states, compared to 90–100% on
factual questions.Our novel insights show that reasoning about
social situations and false beliefs still presents a
signiﬁcant challenge for large language models, de-
spite their seemingly impressive performance on
tasks that could require social intelligence (e.g.,
story generation, dialogues). In §5, we ﬁrst ex-
amine these shortcomings; drawing on theories of
the pragmatics of language, we speculate that the
type of texts in LLMs’ training datasets could sub-
stantially limit learning social intelligence. Then,
we outline some possible future directions towards
socially aware LLMs, reﬂecting on the feasibil-
ity of interactional data selection, person-centric
inductive biases, and interaction-based language
learning. Our ﬁndings suggest that only increasing
the scale of LLMs is likely not the most effective
way to create socially aware AI systems, challeng-
ing a prevalent narrative in AI research (Narang
and Chowdhery, 2022).
2 Theory of Mind & Large LMs
Why do LLMs need Theory of Mind? Social
intelligence, Theory of Mind, and commonsense
reasoning have been a longstanding but elusive
goal of artiﬁcial intelligence for decades (Gun-
ning, 2018; Choi, 2022). These reasoning abil-
ities are becoming increasingly necessary as AI
assistants are used in situations that require social
intelligence and Theory of Mind in order to op-
erate effectively (Wang et al., 2007; Dhelim et al.,
2021; Langley et al., 2022). For example, new tech-
nologies are emerging where AI is used to interact
andadapt to users (Bickmore and Picard, 2005;
Jaques, 2019), e.g., voice assistants, and tutoring
systems; or where AI helps enhance communica-
tionbetween multiple users, e.g., email autocom-
plete (Chen et al., 2019), AI-assisted counseling
(Kearns et al., 2020; Allen, 2020; Sharma et al.,
2021), or facilitated discussion (Rosé et al., 2014).
As we move beyond just asking single-turn ques-
tions to social and interactive AI assistants, higher-
order reasoning becomes necessary (McDonald
and Pearson, 2019). For example, AI systems
should be capable of more nuanced understand-
ing, such as ensuring an alarm is on if someone
has a job interview the next morning (Dhelim et al.,
2021), knowing to call for help when an elderly
person falls (Pollack, 2005), inferring personality
and intentions in dialogues (Mairesse et al., 2007;
Wang et al., 2019), reasoning about public com-
mitments (Asher and Lascarides, 2013), predicting3763
emotional and affective states (Litman and Forbes-
Riley, 2004; Jaques et al., 2020), and incorporating
empathy, interlocutor perspective, and social intel-
ligence (Kearns et al., 2020; Sharma et al., 2021).
What is Theory of Mind? Theory of Mind
(TM) describes the ability that we, as humans,
have to ascribe and infer the mental states of others,
and to predict which likely actions they are going
to take (Apperly, 2010).This ability is closely re-
lated to (interpersonal) social intelligence (Ganaie
and Mudasir, 2015), which allows us to navigate
and understand social situations ranging from sim-
ple everyday interactions to complex negotiations
(Gardner et al., 1995).
Interestingly, the development of Theory of
Mind and language seem to happen around sim-
ilar ages in children (Sperber and Wilson, 1986;
Wellman, 1992; Miller, 2006; Tauzin and Gergely,
2018).Theories of the pragmatics of language
and communication can frame our understanding
of this link (Rubio-Fernandez, 2021), positing that
one needs to reason about an interlocutor’s mental
state ( TM) to effectively communicate and un-
derstand language (Grice, 1975; Fernández, 2013;
Goodman and Frank, 2016; Enrici et al., 2019).
3 S IQ: Do LLMs have Social
Intelligence and Social Commonsense?
A crucial component of Theory-of-Mind is the abil-
ity to reason about the intents and reactions of par-
ticipants of social interactions. To measure this, we
use the dev. set of the S IQQA benchmark
(Sap et al., 2019b), which was designed to probe so-
cial and emotional intelligence in various everyday
situations. This benchmark covers questions about
nine social reasoning dimensions, drawn from the
A knowledge graph (Sap et al., 2019a).
S IQinstances consist of a context, ques-
tion, and three answer choices, written in English.
Each question relates to a speciﬁc reasoning dimen-
sion from A : six dimensions focus on the
pre- and post-conditions of the agent or protago-
nist of the situation (e.g., needs, intents, reactions,
next actions), and three dimensions focus on the
post-conditions of other participants involved in
the situation (reaction, next action, effect). In to-
tal, there are 1954 three-way QA tuples; see Tab. 1
for examples, and Tab. 3 in Appendix A for per-
dimension counts.
3.1 Probing LLMs with S IQ
To probe our language models, we use a k-shot lan-
guage probing setup, following Brown et al. (2020).
We select the answer that has the highest likelihood
under the language model conditioned on the con-
text and question, as described in Appendix C.
To test the limits of what the models can do, we
select kexamples that have the same A rea-
soning dimension as the question at hand, varying k3764
from 0 to 35 in increments of 5. We use three GPT-
3model sizes: GPT-3-A(smallest), and GPT-
3-C and GPT-3-DV (two largest).
3.2 S IQResults
Shown in Fig. 2, GPT-3 models perform sub-
stantially worse than humans (>30% less) on S- IQ,and also worse than models ﬁnetuned
on the S IQtraining set (>20%; Lourie
et al., 2021).Although it is not surprising that
GPT-3-DV reaches higher accuracies than
GPT-3-Aand GPT-3-C , the gains are
small, which suggests that increasing model size
might not be enough to reach human-level accuracy.
These ﬁndings are in line with recent BIG-Bench
results on S IQwith the BIG-G (128B pa-
rameters; Srivastava et al., 2022) and PaLM (353B
parameters; Chowdhery et al., 2022) LLMs, whichlag behind humans with 45% and 73% accuracy,
respectively (see Fig. 7 in Appendix A.2).
Focusing on GPT-3-DV, while increasing
the number of examples kimproves performance,
the differences are marginal after k=10 examples
(only 1% increase from 10 to 35 examples). This
suggest that performance either plateaus or follows
a logarithmic relationship with increasing number
of conditioning examples.
Finally, we examine the differences in GPT-
3-DV with respect to which participant is
the focus. Shown in Fig. 3, we ﬁnd that GPT-3-
DV performs consistently better on agent-
centric questions, compared to other-oriented ques-
tions. Shown in the example predictions in Tab. 1,
GPT-3-DV often confuses which participant
is being asked about. In example (e), after Aubrey
babysat for Tracy, GPT-3-DV fails to pre-
dict that Tracy will likely want to “ let Aubrey know
they are appreciated ,” and instead mistakenly pre-
dicts that Tracy will want to “ save up for vacation ,”
which is what Aubrey would likely do. GPT-3-3765
DV displays a similar participant confusion
in example (f) in Tab. 1.
4 TM: Can LLMs Reason about
Mental States and Realities?
Another key component of Theory of Mind is the
ability to reason about mental states and realities of
others, recognizing that they may be different than
our own mental states. As a measure of this ability
in humans, psychologists developed the Sally Ann
false-belief test (Wimmer and Perner, 1983), in
which two people (Sally and Ann) are together
in a room with a ball, a basket, and a box, and
while Sally is away, Ann moves the ball from the
basket to the box. When asked where Sally will
look for her ball, Theory of Mind allows us to infer
that Sally will look in the basket (where she left
the ball), instead of in the box (where the ball is,
unbeknownst to Sally).
To measure the false-belief abilities of LLMs,
we use the TMQA dataset of English Sally-Ann-
like stories and questions (Le et al., 2019).TM
stories were created using a stochastic rule-based
algorithm that samples two participants, an object
of interest, and a set of locations or containers,
and weaves together a story that involves an object
being moved (see Tab. 2). All questions have two
possible answers: the original object location, and
the ﬁnal object location.
We investigate how LLMs answer the TM
story-question pairs, distinguishing between ques-
tions about factual object locations ( F) and
questions about where participants think objects
are located (i.e., their mental states; M). The
F questions either ask about the object’s origi-
nal (F-M) or ﬁnal ( F-R) location.
TheMquestions cover ﬁrst-order (e.g., “ where
will Abby look for the object? ”;M-1st) and
second-order beliefs (e.g., “ where does James think
that Abby will look for the object? ”;M-2nd).
We further distinguish the Mquestions between
true belief ( T) and false belief ( F), i.e., stories
where a participant was present or absent when an
object was moved, respectively.
Importantly, answering the Mquestions re-
quires Theory of Mind and reasoning about reali-
ties and mental states of participants—regardless
of the true- or false-belief setting—whereas F
questions do not require such TM. There are a
total of 1861 two-way QA pairs in our TMprobe
set, with 519 F and 1342 Mquestions (see
Tab. 4 in Appendix B for more detailed counts).
4.1 Probing LLMs with TM
We use the k-shot probing setup to test this TM
component in LLMs, with k∈ {2,4,8,16,24}.
We select kexamples of the same reasoning type
(i.e., F-M,M-1st, etc.), ensuring a 50-
50 split between true- and false-belief examples for
theMquestions. As before, we test GPT-3-
A, GPT-3-C , and GPT-3-DV.
4.2 TMResults
Shown in Fig. 4, our results indicate that GPT-3
models struggle substantially with the TMques-
tions related to mental states ( M), reaching
60% accuracy in the best setup. As expected, the
best performance is reached with GPT-3-DV
compared to smaller models which do not surpass3766
55% accuracy; however, as before, the gains from
scaling up GPT-3 are very small. Similarly, in-
creasing the number of few-shot examples beyond
k= 4does not substantially improve performance,
corroborating ﬁndings on S IQ.
Further examining GPT-3-DV with re-
spect to question types, we show that the model
struggles substantially more with questions about
mental states (55–60% for k > 0) compared to
factual questions (90–100% for k > 0; Fig. 5;
columns). Furthermore, the difference between per-
formance on M-TandM-Fquestions
shows an interesting pattern when conditioning on
an increasing number of examples k(Fig. 5; lines):
GPT-3-DV’sM-Taccuracy ﬁrst in-
creases, peaks at k= 4, then decreases. This peak
seems to be due to the model defaulting to the most
recent object location (i.e., the correct M-T
answer), as illustrated in example (e) in Tab. 2.
Apparent in Fig. 10 in Appendix B, this recency
bias is a phenomenon that has been previously doc-
umented in LLMs (O’Connor and Andreas, 2021).
In general, GPT-3-DV’s comparably poor
performance for M-TandM-Fques-
tions at k > 8suggests that it cannot properly
answer questions about participants’ mental states
and realities.5 Discussion: Towards NLP with Neural
Theory of Mind
Most humans develop social intelligence and The-
ory of Mind naturally. However, in this work, we
showed that these abilities do not emerge automati-
cally in large-pretrained language models. These
shortcomings contrast with the wealth of successes
of LLMs at a variety of tasks, including tasks that
potentially require social intelligence. For exam-
ple,GPT-3 has been shown to generate stories with
emotional arcs that are virtually indistinguishable
from human-written stories (Clark et al., 2021). Ad-
ditionally, recent work has used GPT-3 to generate
social commonsense knowledge related to protago-
nists of situations (West et al., 2022). While those
ﬁndings suggest some level of social and emotional
intelligence in LLMs, our explorations highlight
the limits of these abilities, and raise the open ques-
tion: how can we create NLP systems with true
social intelligence and Theory of Mind?
To begin answering this question, we ﬁrst dis-
cuss the current LLMs training paradigm (§5.1),
drawing from theories of pragmatics to examine
why these models are not learning social intelli-
gence efﬁciently. Then, we outline some possible
future directions to bias models towards Theory of
Mind (§5.2), through person-centric neural archi-3767tectures, data selection, and training objectives.
5.1 The Pragmatics of “Static” Text
To understand why LLMs are still struggling with
social intelligence, we examine LLMs’ training
paradigm through the lens of pragmatics . As dis-
cussed in §2, pragmatics provides a connection be-
tween language development and Theory of Mind
(Sperber and Wilson, 1986; Miller, 2006; Tauzin
and Gergely, 2018): learning to communicate effec-
tively with language requires reasoning about what
our interlocutor knows or does not know (Grice,
1975; Fernández, 2013; Goodman and Frank, 2016;
Enrici et al., 2019).
One major use of language by people is to com-
municate about relationships and personal experi-
ences (Clark and Schaefer, 1989; Dunbar, 1993).
This is fundamentally different from the training
data of LLMs, which consists of language found
in what we call static texts: documents that are
written for a general audience and are relatively
self-contained and topically focused (e.g., news ar-
ticles, books, Wikipedia articles; Gao et al., 2020;
Dodge et al., 2021). Such static text is typically
written such that readers only require the language
itself as input, which they then combine with their
world knowledge and commonsense to understand
its meaning (Graesser et al., 1994).
If AI systems are to learn social intelligence and
Theory of Mind, we posit that static text has certain
limitations, from a pragmatics lens, outlined below.
Reporting bias. Following Grice’s maxim of
quantity (Grice, 1975), static text often avoids re-
dundancy by omitting content that is known by
both the author and the reader (Clark and Brennan,
1991). Also known as reporting bias (Gordon and
Van Durme, 2013; Lucy and Gauthier, 2017), this
phenomenon likely limits LLMs’ ability to learn
social commonsense knowledge from static text.
Lack of communicative intent and alternatives.
A corollary to reporting bias, static text does not
provide any direct access to communicative intent
(why words were used) or to alternatives (which
words were not used, and why). This reasoning
about intents, alternatives, and their implications
is highly predictive of the pragmatic inferencespeople draw about their interlocutors (Goodman
and Frank, 2016) — for example, when someone
answers Where does Taylor live? with Somewhere
in the U.S. , it implies that they likely do not know
or do not want to share the exact location, since, if
they did, they would have been more speciﬁc. This
poses a likely limitation that LLMs only learn what
words are used, but not which words were not used,
and why.
Lack of communicative effects. Language is
primarily learned (Wells and Bridges, 1981;
Tomasello et al., 2005) and used (Clark, 1996) in
collaborative and interactive settings (Clark and
Schaefer, 1989), which allow interlocutors to give
immediate feedback to each other on whether their
language was understood (Clark and Krych, 2004)
or should be adjusted (Krauss and Weinheimer,
1966), and observe the perlocutionary effects that
their language has on their partners (Austin, 1975).
Since static text has no such feedback, LLMs learn
from all texts, as if they were all equally under-
standable by readers.
Centering theory. At any given time, most text
focuses on describing one protagonist and their re-
lation to their surroundings, according to Centering
Theory (Grosz et al., 1995). As such, main char-
acters and their mental states are more likely to be
described , whereas other participants might only
bementioned . Additionally, main characters or
protagonists are more likely to be referred to with
pronouns, whereas secondary characters with their
names.
Thus, a model trained purely on static text might
not learn to reason about social intelligence or men-
tal states and realities of different characters of
situations; they might not even inherently learn to
resolve coreference for multiple characters (Sak-
aguchi et al., 2020). In fact, challenges of corefer-
ence resolution could explain why GPT-3 models
struggle on S IQwhich contains questions
with pronouns, and centering theory and main char-
acter biases in static text could explain why models
ﬁnd non-protagonist questions more challenging.
On the other hand, TMdoes not contain any pro-
nouns, and thus requires social intelligence beyond
coreference resolution.
5.2 Future directions towards LLMs with
Theory of Mind
While there is no one best path towards LLMs with
social intelligence and Theory of Mind, it seems3768likely that progress will require challenging the
standard paradigm of training on static text with
the language modeling objective. Based on our
ﬁndings and the limitations we discussed, we re-
ﬂect on some possible directions forward.
Beyond static text as training data? Perhaps
the key is in the data: the knowledge contained in
static text might be too limited for models to learn
social intelligence, for reasons described in §5.1
Socially grounded text (containing elaborations
of communicative intents, character mental states,
speaker identities, etc.) could enable more efﬁcient
learning of Theory of Mind abilities (Bender and
Koller, 2020; Bisk et al., 2020; Hovy and Yang,
2021), similar to how visual groundings can help
with learning physical knowledge (Zhang et al.,
2022a). Examples of such datasets include “Social
Stories,” which are devised to help individuals with
autism improve their interpersonal skills (Gray,
1995), or the Story Commonsense (Rashkin et al.,
2018) and GLUCOSE (Mostafazadeh et al., 2020)
commonsense-annotated story datasets. Alterna-
tively, perhaps interactional texts, such as dialogues
and other datasets that were explicitly created to
require reasoning about mental states, could help
with neural Theory of Mind (Bara et al., 2021).
Nevertheless, the scale of training datasets seems
to be crucial for LLMs (Kaplan et al., 2020; Chowd-
hery et al., 2022), which poses a challenge: text
datasets rich in social intelligence and interactions
are not easily found naturally due to reporting bi-
ases, and they are costly to create (Rashkin et al.,
2018; Mostafazadeh et al., 2020). Promising re-
sults on commonsense reasoning suggest a possi-
ble hybrid approach: LLMs could be jointly or
sequentially trained on static text and common-
sense knowledge bases or socially grounded or
interactional text (Bosselut et al., 2019; Hwang
et al., 2021), ﬁrst trained on static text and then
enhanced for commonsense knowledge via rein-
forcement learning (Zhou et al., 2021).
Person-centric neural inductive biases? While
more socially grounded training data could help,
LLMs might also learn social intelligence better
if they are designed with person-centric inductive
biases and training objectives. Hinting at this, prior
work has shown that training entity-centric neural
architectures on text with entity coreference infor-
mation yields more entity-aware LLMs, both in
recurrent (Henaff et al., 2017; Ji et al., 2017; Yanget al., 2017; Liu et al., 2019) and Transformer-
based models (Févry et al., 2020; De Cao et al.,
2020; Rosset et al., 2020; Zhang et al., 2022c).
However, Theory of Mind and social intelligence
require much richer social grounding than corefer-
ence chains, which is challenging to obtain for su-
pervised settings, especially at the scale that LLMs
require. Thus, unsupervised approaches to adding
inductive biases to models could be a promising so-
lution. Future work could look to cognitive science
and neuroscience research for possible directions
(Langley et al., 2022), such as exploring LLMs’
equivalents of human concept cells (i.e., sets of
neurons that activate for important people or con-
cepts; Bowers, 2017; Calvo Tapia et al., 2020).
Alternatively, examining the internal or latent
representations of LLMs could point to future di-
rections towards inductive biases for neural Theory
of Mind. As an example, recent work has found ev-
idence of latent representations of grounded seman-
tics in models trained only on static text (Li et al.,
2021), which can be tied to real-world grounding
with a small amount of additional supervised train-
ing (Patel and Pavlick, 2022). Future work might
similarly analyze deep learning models for repre-
sentations of Theory of Mind, toward augmenting
the models with structure or objectives that surface
and strengthen these representations.
Interactive and experiential grounding? It is
possible, nevertheless, that socially grounded data
and person-centric inductive biases will not sufﬁce.
Some researchers have argued that language un-
derstanding could only emerge from interactions
and experiences (Bender and Koller, 2020; Bisk
et al., 2020). Likely, this applies to Theory of Mind
and social intelligence as well, due to lack of com-
municative intents and alternatives in static text.
Future work could explore approaches grounded
more explicitly in interaction, intents, and alterna-
tives, e.g., by explicitly predicting possible next
steps and learning why predictions were wrong. In
fact, promising research has shown that using an
interactive learning or multi-agent communication
paradigm can enable some Theory of Mind capa-
bilities of models (Hawkins et al., 2019; Lazaridou
et al., 2020; Zhu et al., 2021; Wang et al., 2022).
However, there are limits to the types of Theory
of Mind that can be learned from interactive simula-
tions, which are often task-speciﬁc (e.g., describing
objects in an image; Lazaridou et al., 2020; Steinert-
Threlkeld et al., 2022). Furthermore, models that3769were trained in interactive simulation settings of-
ten struggle to generalize beyond the simulation
environment (Ludwin-Peery et al., 2021; Mu and
Goodman, 2021). Based on promising results by
Lazaridou et al. (2020); Zhu et al. (2021), future
work might create generalizable LLMs with neural
Theory of Mind through hybrid approaches that
combine pretraining with interactive learning: up-
dating models trained on static text using super-
vision either from humans (Stiennon et al., 2020;
Ouyang et al., 2022; Scheurer et al., 2022) or from
proxies for human behavior or social environments
(Ammanabrolu et al., 2022a,b) based on broad cov-
erage LLMs (Perez et al., 2022).
Probing and evaluating TM While neural
Theory of Mind and social intelligence may re-
main an elusive goal for some time, developing
measures of those abilities in systems can be done
in tandem. We encourage further research in devel-
oping benchmarks that measure speciﬁc social abil-
ities in LLMs (e.g., Sap et al., 2019b; Zadeh et al.,
2019), especially those that minimize annotation
artifacts and spurious correlations (Schwartz et al.,
2017; Gururangan et al., 2018; Le et al., 2019).
Additionally, we encourage further investigations
into probing the latent knowledge within LLMs
(Tenney et al., 2019; Li et al., 2021) or examining
how LLMs handle entities and people (Onoe et al.,
2022; Schuster and Linzen, 2022), which could
shed light onto better data choices and inductive
biases towards neural Theory of Mind and social
intelligence.
6 Conclusion
We explore the open question of whether and how
much modern large-scale language models (LLMs)
can reason about social intelligence and Theory of
Mind. Our results show that out-of-the-box LLMs
struggle substantially with these abilities, which
we argue are necessary but not sufﬁcient aspects
of Theory of Mind. Speciﬁcally, GPT-3 ’s social
intelligence as measured by S IQlags be-
hind humans (>30%), and the model struggles to
answer TMquestions about mental states (55-
60%) compared to factual questions (90–100%).
In light of these shortcomings, we critically exam-
ine the large language model pretraining paradigm
from a pragmatics-based perspective, and discuss
possible directions towards enabling true social in-
telligence in NLP systems.We make our preprocessed datasets available at
http://maartensap.com/neuralToM .
7 Limitations
Our work focuses on investigating the Theory of
Mind abilities in large pretrained language models,
but we focus on accessing GPT-3 (Brown et al.,
2020) through an API, since we do not have ac-
cess to some of the larger models out there (PaLM;
Chowdhery et al., 2022) nor do we have the com-
putational resources to run an open-source version
ofGPT-3 (OPT; Zhang et al., 2022b). We hypoth-
esize that results would not be drastically differ-
ent with such models, based on the low accuracy
displayed on S IQin the recently released
BIG-Bench experiments (Srivastava et al., 2022).
Nevertheless, we hope developers of larger LLMs
will investigate these TMabilities to conﬁrm or
refute our ﬁndings.
We measure the ability to answer questions about
people’s mental states using TM, which is an au-
tomatically constructed corpus of stories involving
people, objects, and locations. The automatic na-
ture of the creation process could induce biases and
artifacts, such as objects being in locations that are
plausible but not typical (e.g., bananas in a closet),
which could inﬂuence model’s ability to answer
questions properly. Based on the near-perfect ac-
curacy on the factual questions, however, this may
not be a signiﬁcant issue. Future work should in-
vestigate more naturalistic settings to probe this
ability in LLMs.
A potential limitation of our work is that mod-
els could latch onto surface patterns and spurious
correlations in our two datasets. For example, the-
oretically, a model prompted with many TM
examples may be able to reverse-engineer the data
creation algorithm to ﬁnd the solution to each ques-
tion. However, this would be a bigger limitation if
our claims were that LLMs dohave social intelli-
gence and Theory of Mind; instead, given that our
results show low performance on these tasks even
though they are potentially easier due to correla-
tional patterns, this would indicate that LLMs have
potentially even less reasoning abilities.
Additionally, while we operationalize our mea-
sure of social intelligence and Theory of Mind
through two speciﬁc tasks, S IQandTM,
these abilities are much broader. As noted earlier,
we view these benchmarks as necessary but not suf-
ﬁcient conditions for LLMs to have TM; solving3770the benchmarks does not imply that LLMs have
TM, but LLMs with TMshould be able to solve
them. We hope that future research will further
investigate other aspects of Theory of Mind abil-
ities in large pretrained LMs, drawing on social
science research. For example, future work could
make use of the “unexpected content” task (Gopnik
and Astington, 1988) or the “George Washington
University Social Intelligence Test” (Hunt, 1928)
to measure the social intelligence of LLMs.
Finally, the focus on English language LLMs and
benchmarks for Theory of Mind is another limita-
tion of our work. Echoing recent cognitive science
work that argues the need for non-English cognitive
science investigations (Blasi et al., 2022). Speciﬁ-
cally, false-belief abilities are greatly inﬂuenced by
language structure and grammar (Boeg Thomsen
et al., 2021; Zhang and Zhou, 2022).
Broader Sociotechnical Implications
AI systems are part of a broader sociotechnical sys-
tem that also involves individual motivations and
societal norms (Johnson and Verdicchio, 2017). As
such, per a contextualist view of AI (instead of
utopian or dystopian; Barbour, 1992), we envision
AI systems with social intelligence and Theory of
Mind being used in ways that enhance human’s
lives, autonomy, and agency (Chan, 2022). In par-
allel, we strongly support the development and re-
search of policy and regulation, to prevent misuses
of AI with social intelligence (Wischmeyer and
Rademacher, 2020; Crawford, 2021; Reich et al.,
2021).
Acknowledgements
We would like to thank Jack Hessel, Rowan Zellers,
Jena D. Hwang, Prithviraj Ammanabrolu for their
feedback on preliminary versions of this work, and
Anna Jafarpour and Noah Goodman for fruitful
cognitive science discussions about the research.
We also thank the anonymous reviewers for their
thoughtful comments. This research was supported
by the Allen Institute for AI and the DARPA MCS
program through NIWC Paciﬁc (N66001-19-2-
4031).
References3771377237733774377537763777
A S IQDetails
A.1 Data Preprocessing
We downloaded the S IQtraining and dev.
datasets from the publicly available S IQ
website.This version of the S IQdataset
contains the original A dimensions that
workers were prompted with to create a question,
as well as the correspondence between questions
and which character they focus on (agent or other).
To ensure consistency, for each context, question,
and answer, we normalize the casing to start with a
capital letter if the text does not already.
A.2 Further S IQresults
In addition to results discussed in §3.2, we report
further S IQresults here.
S IQbroken down by reasoning dimen-
sion. We break down the best performing GPT-3-
DV (35-shot) setup by reasoning dimension.
Shown in Fig. 6, we ﬁnd that GPT-3-DV
struggles most with questions related to what peo-
ple needed to do before a situation could take place
(Need). Conversely, questions related to a situa-
tion’s agent’s intent (Intent) and the effect of the
situation on the agent (Effect) are seemingly easier
forGPT-3-DV. Future work should explore
LLMs’s reasoning abilities along each of these di-
mensions in further detail.
BIG-Bench and PaLM results on S IQ.
To further corroborate that LLMs struggle with
S IQ, we show the performance of the non-
publicly available BIG-G (Srivastava et al., 2022)
and PaLM (Chowdhery et al., 2022) LLMs, along
with the GPT-3 models, in Fig. 7. Both models are
proprietary LLMs developed and tested on the 200+
datasets in BIG-Bench by Google / DeepMind.
While they are not discussed in the main BIG-
Bench paper, the S IQresults for few-shot
settings up to k=3 for BIG-G and k=5 for PaLM
can be found on the BIG-Bench github website
(accessed on 2022-11-10). Plotted in Fig. 7, both
the BIG-G and PaLM LLMs lag behind humans
with 45% and 73% peak accuracy, respectively.
B TMDetails
B.1 Data Preprocessing
We generated TMstories using the github
repository provided by Le et al. (2019). The
code generated 5994 training and 5994 dev. stories.
From those, we removed the story-question pairs
which wrongly answered TM-requiring questions
from an omniscient perspective (i.e., answered
M-Fquestions from an omniscient perspec-
tive instead of the perspective of the character)
which we noticed upon manual data inspection.
After this ﬁltering, 5190 training and 5170 dev.
stories remained.
For the ﬁnal TMdev. set, we used strati-
ﬁed sampling to obtain similar numbers of story-
question pairs for all types ( F-R,F-
M,M-1st- F,M-1st- T,M-2nd-
Fand M-2nd- T). The exact counts are3778
shown in Tab. 4. We release our ﬁnal preprocessed
TMdev. dataset at http://maartensap.com/
neuralToM/ToMi-finalNeuralTOM.csv
B.2 Further TMresults
Shown in Fig. 8-10, we provide additional results
to supplement those in §4.2.
Performance by model size, number of exam-
ples, and Mversus F.In Fig. 8, we
show the different accuracies that GPT-3 models
of various sizes, prompted with various number of
examples, for TMMandF questions.
This plot shows the same accuracies as Fig. 4, with
the addition of the F accuracies. These results
show that in the few-shot prompting setup, GPT-
3-C andGPT-3-DV can achieve nearperfect performance on factual questions about ob-
ject locations ( F), but struggle substantially
more on questions related to mental states ( M).
Surprisingly, GPT-3-Astruggles with both fac-
tual and mental state questions, possibly due to its
smaller size.
Performance by question order. In Fig. 9, we
break the GPT-3-DV performance down by
TMorder (i.e., M-1st, M-2nd). Results
show that with a number of examples between37792 and 16, GPT-3-DV performs better on
M-1st questions (e.g., “Where will Sally look
for the ball?”) and struggles more with M-2nd
questions (e.g., “Where does Ann think that Sally
will look for the ball?”). This difference is some-
what diminished but still present for k=24 few-shot
examples. These results somewhat mirror how hu-
mans struggle with increasingly higher-order TM
questions (Valle et al., 2015).
Recency bias in predictions. We further ex-
amine the results from §4.2, looking at GPT-3-
DV’s rate of predicting the location where
the object was moved to (i.e., F-R). Shown
in Fig. 10, GPT-3-DV accurately learns to
almost always predict the last object location for
F-F-R questions, and almost never for
F-F-Mlocations.
Interestingly, the rates of selecting the last object
location for Mquestions follows a concave pat-
tern. This helps shed light onto the concave accu-
racy pattern seen in Fig. 5 for M-T(and con-
vex pattern for M-F). Likely, in the few-shot
setting with 2< k < 8,GPT-3-DV defaults
to the most recently mentioned object location due
to recency bias, which has been previously docu-
mented in LLMs (O’Connor and Andreas, 2021).
C GPT-3 Access and Probing Details
To probe our language models, we use a k-shot
language probing setup, following Brown et al.
(2020). Speciﬁcally, we concatenate the context ( c)
and question ( q) together with proper punctuation,
and assign the model prediction to the answer ( a,
i∈1,2,3) with the highest conditional likelihood
under the language model: arg maxp(a|
c, q,C)where Cdenotes the ktraining examples,
for which we provide the context, question, and cor-
rect answer concatenated. Note that we explored
various probing setups and formats, such as QA-
oriented formats and normalizing by marginal like-
lihood of each answer p(a)(as also explored in
Brown et al., 2020), but found very little difference
in performance.
We access GPT-3 through the OpenAI API.3780