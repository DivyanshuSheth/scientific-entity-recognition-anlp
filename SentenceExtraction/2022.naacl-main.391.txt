
Richard Yuanzhe PangAlicia ParrishNitish JoshiNikita Nangia Jason Phang
Angelica Chen Vishakh Padmakumar Johnny Ma Jana Thompson He He
Samuel R. Bowman
New York University
{yzpang,alicia.v.parrish}@nyu.edu
Abstract
To enable building and testing models on
long-document comprehension, we introduce
QuALITY , a multiple-choice QA dataset with
context passages in English that have an aver-
age length of about 5,000 tokens, much longer
than typical current models can process. Un-
like in prior work with passages, our ques-
tions are written and validated by contributors
who have read the entire passage, rather than
relying on summaries or excerpts. In addi-
tion, only half of the questions are answerable
by annotators working under tight time con-
straints, indicating that skimming and simple
search are not enough to consistently perform
well. Our baseline models perform poorly on
this task (55.4%) and signiﬁcantly lag behind
human performance (93.5%).
1 Introduction
Most of the best models for natural language un-
derstanding are restricted to processing only a few
hundred words of text at a time, preventing them
from solving tasks that require a holistic under-
standing of an entire passage. Moving past this
limitation would open up new applications in ar-
eas like news comprehension, summarization, or
applied question answering. We think that new
benchmark datasets will help us do this. Most ex-
isting datasets (Rajpurkar et al., 2018; Fan et al.,
2019; Lelkes et al., 2021) use shorter contexts that
humans can read within a few minutes. While there
are open-domain QA datasets that require longer
contexts (Joshi et al., 2017; Zhu et al., 2021), ﬁnd-
ing a short excerpt that answers the questions at
hand often sufﬁces; however, long-document QA
requires understanding a long context as a whole
to correctly answer questions.
NarrativeQA (Ko ˇciský et al., 2018) is the most
established existing long-text benchmark for lan-
guage understanding. It’s a free-text-response QAFigure 1: The crowdsourcing pipeline with an example.
One writer reads the passage and writes 10 questions.
Each question is validated by three or ﬁve annotators
who read the full article, plus ﬁve more who have only
45 seconds per question. Writers receive feedback from
both validations between writing batches. If a majority
of timed annotators get the question wrong, but the un-
timed annotators get it right, we classify the example
as and give the writer a bonus.
dataset built around movie scripts and books, with
an average of about 63k tokens of input per ques-
tion. The authors creatively use summaries of the
texts as the basis for their questions to make data
collection relatively efﬁcient. This protocol leads
to short answers (avg. 4.7 tokens), and few ques-
tions require complex explanation-based reason-
ing: >60% are what/who questions and <10% are
why/reason questions. Further, the sources are usu-
ally famous, such that they are analyzed and dis-
cussed widely in the training data used by large lan-
guage models. Additionally, the generation-based
format comes with the hurdle of determining how5336
to fairly assess accuracy, as metrics like BLEU,
ROUGE, or BERTScore may not accurately convey
the quality of generations (Wang et al., 2020; Dur-
mus et al., 2020). To ease the burden of evaluation,
we opt for a multiple-choice format to evaluate a
model’s long-document understanding ability.
We introduce our dataset QuALITY , Question
Answering with Long Input Text, Yes!This is
a multiple-choice QA dataset that uses English
source articles of 2k–8k tokens.We collect this
dataset using a creative crowdsourcing pipeline that
ensures the examples have unambiguous answers
but are still challenging. We instruct example writ-
ers to carefully read the full source article before
writing questions, and to then write questions that
are unambiguous and require consolidating infor-
mation from multiple parts of the text. Then, to
ensure our questions require readers to understand
the larger context from the passage, in addition
to running standard validation where annotators
read the text and answer the questions, we also run
speed validation (§2.3). In speed validation, anno-
tators only have access to the text for 45 seconds,so they can only skim or search for phrases to an-
swer the question. If a question is unanswerable in
this setting but unambiguous and answerable in the
standard untimed setting, we use it as a signal for
question difﬁculty. This crowdsourcing process is
slow and expensive ($9.10/question),but we suc-
cessfully collect a challenging, high-quality, long-
document multiple-choice QA dataset. QuALITY
has 6,737 questions in total, of which 3,360 ques-
tions are in the difﬁcult subset, QuALITY- .
Table 1 shows representative and ex-
amples from different types of source texts.
We test the Longformer (including the LED vari-
ant), RoBERTa, DeBERTaV3, and T5 models, us-
ing as much of the full source text as possible. In
particular, for models whose context lengths are
much shorter than the average article length, we
test two-step systems with an extraction step that
passes shorter contexts to the QA model. For text
extraction, we use ROUGE-1 recall, fastText, or
DPR based matching with the questions. The best
model performance is achieved by DeBERTaV3-
large with DPR-based extraction, with an accuracy
of 55.4%. The best model’s accuracy on QuALITY- is 46.7%. Model accuracy is far below hu-
man accuracy on QuALITY , where human accu-
racy is 93.5% on the full dataset and 89.1% on5337QuALITY- .
2 Data Collection
2.1 Overview
Sources In order to create a dataset that is both
broadly usable and meets the goal of contain-
ing long input texts, we use only sources that
are licensed under CC-BY (or more permissive
licenses) and contain articles of at least 2k to-
kens that are likely to allow for complex ques-
tions. We ultimately use Project Gutenberg ﬁc-
tion stories (mostly science ﬁction),Slate mag-
azine articles from the Open American National
Corpus (Fillmore et al., 1998; Ide and Suderman,
2004), and other nonﬁction articles taken from The
Long+Short,Freesouls,and the book Open Ac-
cess (Suber, 2012). Table 3 shows how many ar-
ticles and questions come from each. Most of the
Gutenberg texts are from the 1950s–1970s, while
the other texts are mostly from the 1990s and after.
Texts are provided with the original HTML tags
indicating paragraph breaks and basic formatting
(e.g., italics), and it is in this format, with images
removed, that we present the texts to our writers
and annotators. In our dataset release, we also
include a version of each ﬁle with this information
stripped away, as current models, including our
baselines, are not trained to consume these tags.
We set a maximum length for the texts at 6k
words using word-level tokenization without count-
ing HTML tags.For around 40% of the Gutenberg
articles, the full text data is much longer; in these
cases, we truncate the texts and manually check to
make sure the truncation happens at a reasonable
location (i.e., not in the middle of a paragraph).
Stages of Data Collection We collect data over
several rounds to provide writers with feedback
throughout the process. We iterate through the fol-
lowing pipeline each round: (i) we assign writers
a set of passages, and they write 10 questions for
each (§2.2), (ii) annotators complete speed valida-
tion (§2.3.1), (iii) annotators complete untimed val-
idation (§2.3.2), and (iv) we award writers bonuses
and send feedback based on the annotations.2.2 Question Writing
We hire 22 writers—most with degrees or profes-
sional experience in literature or teaching—from
the freelancing platform Upwork and design a
multi-part incentive structure to encourage difﬁ-
cult yet answerable questions. Details about hiring
and writer qualiﬁcations are in Appendix A.1.1.
The Writing Task We design a feedback and
incentive structure to encourage writers towards
questions that are answerable, unambiguous, and
difﬁcult. Writers construct examples over multi-
ple rounds, and they receive (i) detailed feedback
based on the two validation tasks and (ii) bonuses
based on how many of their questions met our crite-
ria for questions. Each writer constructs 10
questions with four answer options for a given pas-
sage, and they complete 6–30 such passages each
round. Each passage is assigned to two writers,
so there are 20 questions for each passage before
ﬁltering. Writers earn an average rate of $21.05/hr,
after bonuses. Details about this process and the
timeline are in Appendix A.1.2.
2.3 Data Validation
We use two validation tasks to evaluate if (i) the
questions are difﬁcult by testing if they are answer-
able under strict time constraints (speed validation)
and (ii) the questions have a single correct answer
(untimed validation). We recruit 45 annotators via
Amazon Mechanical Turk (MTurk); details on the
qualiﬁcation process are in Appendix A.2.1.
2.3.1 Speed Validation
We want to ensure that the questions require under-
standing of the full text to answer correctly. If a
person can quickly identify the answer to a ques-
tion, such as through skimming or ctrl-F-style in-
browser search, then the question does not require
broader understanding of the passage, and a model
is likely to be able to identify the correct answer
via extractive methods. More precisely, we aim to
collect questions for which annotators, in the aggre-
gate, are unable to select the correct answer under
strict time constraints, and we construct a speed
validation task to test this. Questions that pass this
bar make up the subset of QuALITY .
To our knowledge, this is a novel data collection
method—it is inspired by adversarial collection
methods (Nie et al., 2020; Bartolo et al., 2020) as a
way of collecting more challenging data. As model
performance on our dataset is very low, a true ad-5338versarial design would not be practical because
model behavior would not provide enough signal
to the crowdworkers, and we would risk limiting
the usefulness of QuALITY as a test set for a full
range of models (Bowman and Dahl, 2021). Thus,
we design this task in a way that writers can reason
about what would be difﬁcult for another human as
opposed to a model, and we award bonuses based
on that metric.
Procedure We collect ﬁve annotations per ques-
tion; within each task, questions appear one at a
time to ensure the time limit is consistent for each
question. The worker ﬁrst reads the question and
the four answer options without access to the pas-
sage. Then they press a button to reveal the passage,
and they have 40 seconds to skim or search for key-
words (e.g., with ctrl+F) to determine the correct
answer. After the timer runs out, the passage dis-
appears, and they have 5 more seconds to select an
answer. Appendix A shows the user interface.
Each task consists of 10 questions from differ-
ent passages, and the order of the answer options
is randomized. Within each task, there are nine
questions written by the Upwork writers and one
question written by the authors as a catch question.
We pay workers $2.25 per task and award a bonus
of $0.20 for each correct answer. On average, work-
ers earn a bonus of $1.03 per task, and we estimate
based on workers’ survey responses that each task
takes 11-12 minutes, for an effective rate of just
over $17/hr. We use the catch questions to track
annotator performance and ensure that all workers
are performing well above chance on these exam-
ples, indicating that they are consistently making
a faithful effort to ﬁnd the answer in the text (see
Appendix A.2.2 for additional details on the task,
catch questions, and annotator performance).
2.3.2 Untimed Validation
To ensure all questions in QuALITY are correct and
unambiguous, we conduct a validation task without
a time limit, but with strong incentives towards
accuracy. We collect three annotations for each
example in the training set, and ﬁve annotations for
each example in the dev and test sets.
Procedure Each task consists of one passage
with all 20 questions created by the writers. Each of
the 20 reading comprehension questions has three
evaluation questions immediately below it. We in-
struct workers to ﬁrst read the passage carefully and
then answer all the questions. Each task pays $6.50,
with a $0.50 bonus for each question in which both
the reading comprehension question and evaluation
question 1 (see below) agree with the majority vote
label.We estimate based on survey responses that
workers spend about 50–60 minutes on this task;
the average bonus rate is $8.13 per task, for an
average rate of $15.96/hr.
Evaluation Questions We ask the three evalua-
tion questions in Table 2 immediately following
each reading comprehension question to assess
question quality. Q1 is used to determine inclusion
into the ﬁnal dataset, as we exclude any questions
for which the majority of annotators marked that
the question was either ambiguous or unanswerable.
Q2 and Q3 are used for feedback to the writers.
We ﬁnd that responses to the evaluation ques-
tions slightly differ between the and
subsets. For Q1, individual raters are less likely
to rate a question as answerable and unam-
biguous (92.8%) compared to an question
(95.1%). For Q2, in the subset, 26.1% of the
time, the question is rated as needing at least a third
of the context or more (the 3rd and 4th options),
compared to 21.7% of the time in the subset.
In the subset, 81.3% of the questions are
rated as needing at least a long paragraph or two of
context, compared to 73.9% in the subset.5339
Annotator Performance We track annotator
performance throughout data collection and re-
move any workers whose accuracy falls below 75%
in any given round. Annotator agreement on the
reading comprehension questions for each passage
is high, with a median Krippendorff’s alpha of 0.71.
Agreement on Q1 is also high, with 92.6% indi-
vidual agreement with the majority vote, with the
two ‘No’ options collapsed for analysis (alpha val-
ues are less valuable on such a skewed question).
As Q2 and Q3 are more subjective, responses are
noisy, with median alpha values of 0.12 and 0.21,
respectively. Additional details and our protocol
for reannotating data are in Appendix A.2.3.
3 Dataset Information and Analysis
After aggregating the labels assigned via untimed
validations with the original writer’s label, we cal-
culate the gold label via majority vote of anno-
tators.We only keep questions for which (i) a
majority vote label (strictly larger than 50%) can be
assigned and (ii) the majority of annotators rate the
questions as answerable and unambiguous. 6,737
out of 7,620 (88.4%) questions meet these inclusion
criteria. The subset corresponds to ques-
tions that the majority of the annotators answer
incorrectly in the speed validation setting, and this
constitutes 49.9% of the ﬁnal dataset.
3.1 Human Accuracy
We estimate human accuracy on QuALITY on a
random sample of 20 passages (367 questions).
Each question is annotated by 3 new annotators
who had not previously annotated that passage, andwhose labels do not contribute to the assignment
of the gold label. We calculate the majority vote of
the annotators, which yields an accuracy of 93.5%
relative to the gold label. This breaks down to
89.1% on the subset and 97.0% on the
subset. Annotators marked 98.5% of questions as
answerable and unambiguous.
3.2 Size and Splits
We split the data into train/dev/test setssuch
that there is minimal overlap in question writers
among train/dev/test sets. This ensures that a model
will not be rewarded for overﬁtting to any idiosyn-
crasies of a single writer’s style. Table 3 shows the
number of articles in QuALITY and ques-
tions for each of the split. Gutenberg sources result
in the highest proportion of questions, and
misc. sources result in the lowest proportion.
3.3 Length
Figure 2 shows the article lengths. The two peaks
in the histogram correspond to articles from Slate
and Gutenberg. The average context length is 5,159
tokens, much longer than other existing challenging
QA datasets—CosmosQA (Huang et al., 2019) and
RACE (Lai et al., 2017) contain an average context
length of 70 and 322 tokens, respectively. We plot
question length and option length in Figure 2 as
well. The average question length is 12.5 tokens,
and the average option length is 11.2 tokens.
3.4 Lexical Overlap
Prior work has shown that a lot of questions in ex-
isting datasets such as SQuAD can be answered
by exploiting lexical overlap of the question with
the article (Weissenborn et al., 2017). To under-5340
stand how effective this heuristic is in QuALITY ,
we compute the lexical overlap between the options
and the article in QuALITY . The lexical overlap is
computed as the fraction of the tokens in the option
which are present in the article. Figure 7 in the Ap-
pendix plots the distribution of lexical overlap for
the correct options and the incorrect options—since
each question has three incorrect options, we use
the maximum lexical overlap among the three. Sim-
ply predicting the option with the highest lexical
overlap achieves only 26.6% accuracy, so correct
options do not have a higher lexical overlap than
the incorrect options, making it difﬁcult for models
to rely on this heuristic.
3.5 Question Types
We analyze the proportion of question types by
automatically categorizing each question based on
the ﬁrst question word it contains.Table 4 shows
that QuALITY contains many questions that re-
quire complex responses about “how” and “why”
an event happened in a greater proportion of casesthan similar datasets such as NarrativeQA. How-
ever, we do not observe that our measure of ques-
tion difﬁculty varies by question type.
3.6 Reasoning Strategies
As a qualitative analysis, we manually annotate
the reasoning strategy needed in each question and
present the results in Table 5. We take a random
subset of 500 questions from the full dataset and
manually annotate them. Each question is anno-
tated by two of the authors; any disagreements in
categorization are resolved via discussion. As we
do not read the full passages, it is not always pos-
sible to determine the reasoning strategy, but we
consider both the question and answer options in
categorizing each item.
We ﬁnd that many of the questions rely on (i)
reasoning about the best description, (ii) determin-
ing the correct explanation for why something hap-
pened, or (iii) the reader making an interpretation
or using symbolism. All three of these reasoning
types are likely to rely on broader context from
the passage, compared to questions about who did
something or where something happened. For ex-
ample, the question How do you think Meredith
feels about the rest of the crew? requires a descrip-
tion of the character’s feelings (description), and
it also requires the reader to interpret the charac-
ter’s feelings (symbolism/interpretation) and rea-
son about the relation between different characters
(relation). We also ﬁnd that, despite questions us-
ing “what” being the most frequent in the question-
types analysis, very few of the questions in QuAL-
ITY depend on reasoning about objects or entities.
Rather, most of these “what” questions ask for the
description of a person or situation, or they ask
for an interpretation from the reader. Further de-
tails about this analysis, the categories used, and
examples of each reasoning type are in Appendix
B.2.5341
4 Baseline Experiments
4.1 Models
Long-Context Models We experiment with the
Longformer model (Beltagy et al., 2020), which
uses a combination of sliding-window local atten-
tion and global attention to encode long inputs.
The Longformer encoder models support up to
4,096 tokens. We test Longformer because it is
likely to ﬁt most or all of the context needed to
answer the questions for the majority of examples
in QuALITY .We also experiment with Long-
former Encoder-Decoder (LED) which supports up
to 16,384 encoder input tokens.
Extractive Models As an alternative to feeding
the whole input context into a transformer model or
truncating, we also test retrieval methods to score
and extract relevant sentences from the passage and
feed only the selected sentences as inputs to a given
model. We can thus use a wider range of higher-
performing short-sequence transformer models, at
the cost of missing some input context.
Using the question as a retrieval query, we score
each sentence in the passage relative to the query.
We then select sentences in order of descending
relevance until we reach 300 words.We then sort
the selected sentences based on the original passageorder and use the concatenation as the ‘passage’ for
that example.
We consider three scoring methods. First, we use
ROUGE-1 recall relative to the query. Second, we
use cosine similarity based on bag-of-words of fast-
Text (Bojanowski et al., 2017) embeddings. Third,
we use DPR (Karpukhin et al., 2020), a model
trained for open-domain retrieval for QA. Because
DPR tackles span-based question-answering, the
reader model is unsuitable for our multiple-choice
dataset. However, we can use the retriever model
for extraction, using the separate question- and
context-encoders to encode our question and con-
text sentences to vector representations. We then
score similarity based on the negative Euclidean
(L) distance.
After extraction, we apply standard models for
multiple-choice question-answering: RoBERTa
(Liu et al., 2019) and DeBERTaV3 (He et al., 2021)
encoder models, and the T5 (Raffel et al., 2020)
encoder-decoder model. To establish an upper
bound of how well extractive models can do, we
also introduce an oracle baseline in which we apply
the same extraction strategy described above, but
we use the correct answer as the extraction query.
Question-Only Baselines To test for dataset ar-
tifacts, we consider a baseline where we only give
the models the questions and answer options, leav-
ing out the passage.
Supplementary Training Data To supplement
the training examples in QuALITY , we incorpo-
rate additional training examples from the RACE
task dataset (Lai et al., 2017). Like QuALITY ,
RACE is a passage-based, four-way multiple-
choice question-answering dataset. Although the
passages are much shorter (321.9 words on aver-
age), the training set is large ( ∼88k questions), so
we can expect reasonable knowledge transfer from
RACE to QuALITY . We use the full RACE dataset,
including both middle-school and high-school ques-
tions, for our intermediate training.
We consider three ﬁne-tuning formats: (1) ﬁne-
tuning on QuALITY data, (2) ﬁne-tuning on RACE
and zero-shot evaluating on QuALITY , and (3) ap-
plying intermediate training (Phang et al., 2018;
Pruksachatkun et al., 2020) by ﬁrst ﬁne-tuning on
RACE and then ﬁne-tuning on QuALITY .
4.2 Results and Analysis
Table 6 shows model performance on the test set.
The results on the development set and additional5342
results from training just on RACE are in Appendix
D.3. All results in Table 6 fall well below human
performance. There is a gap of 38.1 points between
our current best-performing model (DeBERTaV3-
large trained on RACE →QuALITY , using DPR-
based extraction) and human performance on the
full test set. On QuALITY- , this gap in-
creases to 42.4 points.
Comparing models using different training data,
we see that the RACE →QuALITY results outper-
form RACE results in most cases (Table 9). Fine-
tuning on QuALITY contributes to a small perfor-
mance gain. Both RACE and RACE →QuALITY
signiﬁcantly outperform the QuALITY only results,
likely because of the small size of the QuALITY
training set, though this suggests that knowledge
transfer from RACE is useful.
In terms of extraction strategies, DPR-based ex-
traction almost always produces the best result. In
terms of models, DeBERTaV3-large consistently
performs best. Compared to the RoBERTa and
DeBERTa models ﬁned-tuned on short contexts,
the Longformer and LED models appear to strug-
gle to learn the task from the long inputs, under-
performing even the RoBERTa-base extraction-
based models. We speculate that a combination
of more long-context training data and better long-
context models may improve performance beyond
the extraction-based models. As with other models,
intermediate training on RACE improves perfor-
mance on QuALITY .Question-Only Baselines The best-performing
question-only baseline is DeBERTaV3-large using
the RACE→QuALITY setting for training, achiev-
ing an accuracy of 43.3%. The corresponding per-
formance is only 12.1 percentage points lower than
the DeBERTaV3-large’s performance with text ex-
cerpts from DPR. This small margin of improve-
ment may indicate that current models are not ef-
fectively using the input contexts.
QuALITY- Model performance is always
lower on QuALITY- than on the full test-set,
even on the question-only baselines. This suggests
that speed-validation ﬁltering yields more challeng-
ing questions for human annotators andmodels.
Extraction by Oracle Answer We show in Ap-
pendix D.3, Table 11 the results of the oracle-
answer-based extraction on the development set.
Compared to Table 10, using the oracle answers
for extraction improves performance signiﬁcantly
(topping out at 78.3%), but is still below human
performance by 15 points. This demonstrates that
extracting relevant excerpts alone is insufﬁcient
to solve QuALITY questions, and that QuALITY
questions require reasoning over the full passage.
5 Related Work
Rogers et al. (2021) survey the QA dataset explo-
sion of recent years and the many formats and types
of QA datasets. TriviaQA (Joshi et al., 2017) and
SearchQA (Dunn et al., 2017) contain questions5343with more than one document as the context, but
since the supporting documents are collected after
writing the question-answer pairs, most questions
can be answered after retrieving a short context.
HotpotQA (Yang et al., 2018), QAngaroo (Welbl
et al., 2018), and ComplexWebQuestions (Talmor
and Berant, 2018) are constructed to have more
challenging questions which require multi-hop rea-
soning across multiple paragraphs. However, there
has been recent work (Jiang and Bansal, 2019; Min
et al., 2019) showing that these datasets contain
reasoning shortcuts and a large fraction of the ques-
tions can be answered with single-hop reasoning.
NarrativeQA (Ko ˇciský et al., 2018), the most
similar work to ours, uses entire Gutenberg books
and ﬁlm scripts as contexts, with an average length
of 60k tokens. The authors creatively make data-
collection tractable by using Wikipedia summaries
for the books as context when crowdsourcing ques-
tions. Unlike QuALITY , NarrativeQA is a free-
form generation-based task. While there are many
existing multiple-choice QA datasets (Richardson
et al., 2013; Hill et al., 2015; Lai et al., 2017; Baj-
gar et al., 2016; Huang et al., 2019), they use much
shorter contexts (<500 tokens) than our dataset.
A primary challenge of building a long-
document QA dataset like QuALITY or Nar-
rativeQA is building a tractable crowdsourcing
pipeline that enables collecting high-quality ex-
amples. Roit et al. (2020) collect a challenging
QA-SRL dataset by carefully hiring and training
crowdworkers, with a strict qualiﬁcation followed
by two hours of training with extensive feedback.
Nangia et al. (2021) compare crowdsourcing meth-
ods for collecting high-quality QA data and ﬁnd
that a long training process with iterative feedback
and qualiﬁcations is an effective strategy.
6 Conclusion
We introduce the long-document QA dataset
QuALITY . This dataset was crowdsourced and
validated by humans to ensure that the questions
are answerable, unambiguous, and challenging.
The QuALITY- subset, comprising half the
dataset, consists of questions that are unanswerable
by annotators working under tight time constraints,
helping ensure that skimming and simple search do
not yield high performance.
We ﬁnd that our baseline models signiﬁcantly
lag behind human performance on QuALITY , with
a 38.1 percentage point gap between human an-notators and the best performing model. The gap
is even wider on QuALITY- , at 42.3 points.
We hope that research that aims at this gap will
contribute to expanding the scope of texts on which
effective NLU systems can be applied.
Ethical Considerations
Both the authors of our source texts and the authors
of our questions are based primarily in the US, and
represent a relatively privileged, educated popula-
tion. A system that performs well on our dataset is,
thus, only demonstrating its effectiveness on main-
stream US English, and should not be presumed to
be effective on text in other languages or language
varieties.
Author Contributions
•Locating appropriate passage sources: AP, JM,
VP, AC, NN, NJ, JT
• Preprocessing passages: AC, NN, NJ, JP
•Data collection protocol design: NN, AP, RP,
SB, HH
• Data collection user interface: RP, JT
• Data collection backend infrastructure: AC
•Data collection management and postprocess-
ing: RP, AP, NJ
•Writing catch questions: VP, JM, JT, AP, NN,
NJ, RP
• Data analysis: AP, NJ, RP, VP
• Modeling: JP, AC
• Writing: AP, RP, NN, NJ, JP, HH, SB
• Project management: RP, AP
• Advising: SB
Acknowledgements
This project has beneﬁted from ﬁnancial support to
SB by Eric and Wendy Schmidt (made by recom-
mendation of the Schmidt Futures program), Sam-
sung Research (under the project Improving Deep
Learning using Latent Structure ), Samsung Ad-
vanced Institute of Technology (under the project
Next Generation Deep Learning: From Pattern
Recognition to AI ), and Apple. This material is
based upon work supported by the National Sci-
ence Foundation under Grant Nos. 1922658 and
2046556. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reﬂect
the views of the National Science Foundation.
We thank Jon Ander Campos, Alex Wang, Saku
Sugawara, and Omer Levy for valuable discussion.5344We thank the anonymous reviewers for useful feed-
back. Finally, we thank the writers who wrote our
source texts (credited in the data itself) and the writ-
ers who wrote our questions: Megan Barbee, Brid-
get Barrett, Kourtney Bradley, Kyle J. Brown, Ali-
cia Chatten, Christine D., Leah Dorschner-Karim,
Bobbie Dunn, Charisse Hake, Javier Hernandez,
Molly Montgomery, Carilee Moran, Tracy M. Sny-
der, Lorna Stevenson, Isaiah Swanson, Kyla Thiel,
Lisa V ., Ryan Warrick, Julia Williamson, and oth-
ers who chose to remain anonymous.
References534553465347A Details on Writing, Speed Validation,
and Untimed Validation
A.1 Writing
A.1.1 Writer Recruitment
We need writers who have good reading com-
prehension skills and/or writers who have expe-
rience constructing reading comprehension ques-
tions (e.g., literature teachers who have experience
writing tests for their students). We hire two groups
of writers on the freelancing platform Upwork (the
second group two months after the ﬁrst group, after
we decided to increase the ﬁnal size of the dataset).
For each group, we advertise a task titled Writ-
ing college-level reading comprehension questions .
The job post is visible to all U.S. Upwork free-
lancers, and we speciﬁcally send out job invitations
to promising freelancers who are writers or teach-
ers, or who have college-level degrees in English,
Literature, Creative Writing, Philosophy, Educa-
tion, or similar ﬁelds. In the original job ad, we
explain who we are and tell them how we will use
their data. Speciﬁcally, we include the following
phrase: “The data we collect through this project
will be made publicly available for AI research.
We will not distribute any identifying information
about you, the writers.”
For the ﬁrst group, we received 104 applications
in the span of two weeks. Of those, we selected
26 people to complete a qualiﬁcation task as a paid
interview. For the second group, we received 65 ap-
plications and interviewed 11. The interview task
consists of (i) reading through detailed instructions,
(ii) reading through a tutorial example passage with
10 example questions, each with an explanation of
what made it a good or a bad question, and (iii) writ-
ing 10 reading comprehension questions for a new
passage; regardless of whether we eventually hire
them, we pay workers $30 and estimate that this
task takes 2 hours to complete. Three authors (of
this paper) then assess each writer’s work using the
following criteria: (i) whether the writer-provided
correct answers are actually the correct answers,
(ii) whether the questions are answerable and un-
ambiguous, and (iii) whether more than just a few
sentences of context are needed to correctly answer
the questions. Based on these criteria, we select
the top performing 15 writers to continue on to the
main task in the ﬁrst group, and 7 in the secondgroup.
Of the 22 writers we hire after the interview, 15
have a college degree in English, literature, philos-
ophy, creative writing, or education; 4 of these 15
writers are Ph.D. students or graduates. 11 of the
22 writers have taught high-school or college-level
English or literature classes; among these 11 writ-
ers, 7 have 5+ years of teaching experience. 2 of
the 22 writers mention that they write novels.
A.1.2 Writing Task
Each writer constructs 10 questions for a given pas-
sage, completing 6-30 passages in a given round
and continuing for three complete rounds.Each
round is followed by feedback (detailed below) to
allow writers to improve for the next round. Writ-
ers earn $12.50 per passage and receive a bonus
of $1.20 for each question that meets the follow-
ing criteria: (i) the majority of validators agree
with the writer’s original label, (ii) the majority
of validators rated the question as answerable and
unambiguous, and (iii) the majority of validators
answered the question incorrectly in the speed val-
idation task (§2.3.1). On average, writers receive
bonuses on 4.2 questions per passage, resulting in
average earnings of $17.54 per passage. Based on
writer self-reports, the median time to complete
one writing task is about 50 minutes, for an effec-
tive rate of $21.05/hr. Upwork charges fees on the
workers’ end. We account for this by adding an
extra 20% to their pay, bringing our ﬁnal cost to
$2.10 per question.
Besides using the monetary bonus as an incen-
tive for writing answerable, unambiguous, and dif-
ﬁcult questions, we also instruct writers that their
questions should use the entire context. Through-
out the course of data collection, we provide writ-
ers with detailed feedback based on validations
(detailed in §2.3.2) and this feedback includes in-
formation about how much of the passage needed
to be read in order to answer the question. We
monitor the proportion of questions that require
more than a few paragraphs of context to answer
correctly; if this rate signiﬁcantly lags behind other
writers, we inform the writers that their work is
falling below expectations and ask them to be more
careful with this issue in the next round. We also5348
encourage writers to write difﬁcult distractors, and
the feedback we provide also contains what anno-
tators think is the most difﬁcult distractor for each
question (§2.3.2).
If writers have fewer than 40% of questions meet
the above three bonus criteria andfewer than 75%
of questions meet criteria (i) and (ii), we exclude
them from future writing rounds: One writer was
excluded after batch 1, and one writer was excluded
after batch 2, for this reason. We also exclude
two writers who missed deadlines by signiﬁcant
margins. Two other writers voluntarily left the
project before ﬁnishing all three batches.
A.1.3 The Writing UI
Figure 3 shows our writing UI. A writer creates
10 multiple-choice questions with four answer op-
tions each on each page. Before the interview task
and each batch of data collection, we explain our
bonus structure to the writers. In order to encourage
writers towards writing the types of questions that
require understanding of the general context from
the passage, we provide the following examples of
themes that questions can target in order to spur
writers’ creativity and provide suggestions if they
have trouble coming up with difﬁcult questions;
however, they do not have to follow our sugges-
tions.
• Characters’ feelings and motivations•Causes and consequences of described events
•Deﬁnitions, properties, and processes ex-
plained in a passage
• The summary and lesson of a passage
•What would have happened had a character
made a different choice
We also allow writers to skip a given passage in
case they ﬁnd that they would be unable to write
high-quality questions for that passage. Speciﬁ-
cally, we tell writers the following.
If a passage is too difﬁcult to write ques-
tions for, you can skip the article by
choosing another URL to work on. We
recommend that you do this if: (1) The
text is hard to read due to major format-
ting issues. (2) The text is very techni-
cal or relies on cultural knowledge that
you’re unfamiliar with. (3) You think the
passage is much too boring. We ideally
want you to write questions for passages
you ﬁnd interesting!
A.2 Validation
A.2.1 Annotator Recruitment
We recruit annotators via Amazon Mechanical Turk
(MTurk). We use a qualiﬁcation task to identify
annotators with good reading comprehension skills.5349
This task is open to all workers with more than
1000 tasks (HITs) accepted and a HIT accept rate
of at least 98%. We pay $5 for completing the
qualiﬁcation task, plus a $5 bonus for passing it.
The task consists of a ∼3000-word passage with 10
multiple choice questions written or reviewed by
the authors, each with a series of evaluation ques-
tions asking about the quality of that question. Of
the 10 questions, 2 are intentionally ambiguous
in order to test if workers can accurately identify
poorer quality questions.
In order to pass the qualiﬁcation, workers need
to (i) get at least 6/7 or 7/8 of the unambiguous
questions correct, (ii) correctly identify at least
one of the two ambiguous questions as ambigu-
ous/unanswerable, and (iii) correctly identify at
least half of the unambiguous questions as unam-biguous. A total of 148 crowdworkers completed
the task, and 45 of them passed (30.4%). All work-
ers who pass the qualiﬁcation are invited to com-
plete tasks as part of both the speed validation and
the untimed validation. We make it clear in this task
as well as the main speed validation and untimed
validation tasks who we are and which research
group we are afﬁliated with. In order to help work-
ers understand that we plan to use their data for
research purposes related to language technologies,
we also include the following in the FAQ section of
each hit: “With your help, we think we’ll be able
to build some pretty exciting technologies to help
computers better understand human language.”
A.2.2 Speed Validation
Catch Questions We expect accuracy in the
speeded task to be fairly low, so we construct catch
questions to ensure that workers are not randomly
guessing without attempting to ﬁnd the correct an-5350swer. These trials are written by the authors and
are designed to be answerable with only 45 sec-
onds of access to the passage. For example, a catch
question may ask who spoke a quote, like “Who
said ‘You’re a wizard Harry!’?”, where a single
ctrl+F search of the quote gives the annotator the
answer. Another catch question may have four op-
tions, three of which are clearly improbable. We
do not validate the catch questions for correctness
in the untimed validation, and so we do not include
them in the ﬁnal dataset, but we release them as a
supplemental ﬁle for reproducibility.
Payment For most of the tasks, we pay work-
ers $2.25 per HIT and award a bonus of $0.20 for
each correct answer. However, during the ﬁrst of
six rounds, we paid $2.00 per HIT with a $0.18
bonus for each correct answer. After asking work-
ers for feedback about the task via a survey, we
decided to increase the rate of pay because workers
reported spending slightly longer on the task than
we originally estimated.
Task Procedure Each MTurk task consists of 10
speed validation questions from different randomly
chosen articles. In each task, once the annotator
clicks into the page, they have unlimited time to
read the question and the answer options, but the
article is not shown (Figure 4). Then, the annotator
clicks the button that says “I ﬁnished reading the
instruction, the question, and the choices. Show
me the article (please click)!” As soon as the anno-
tator clicks the button, the countdown clock of 45
seconds starts, and the article appears (Figure 5).
The annotator can make the choice and submit at
any time.
When there are only 5 seconds left, the arti-
cle hides itself. The annotator has 5 seconds to
make the choice. If the time expires, the page auto-
submits, and we record that the annotator did not
make a choice, which we score as incorrect. The
exact instructions that the annotator sees are as
follows:
In this task, you will see a long text pas-
sage and a multiple choice question that
can be answered from that text. Read
the question and select the best answer
option. You only have 45 seconds to
choose an answer, so this is not enough
time to read the whole passage. We en-
courage you to skim and use keyword-
based searches (e.g., using ctrl+F) duringthis time. Even if you are unsure of the
answer, you should make an educated
guess.
After 45 seconds, your answer will be
locked in and submitted. If you have
not provided an answer at the end of 45
seconds, you will not be able to answer
this question and will be automatically
moved to the next question. We will not
reject your work for a couple of blank
answers, but excessive failures to answer
will result in a loss of the qualiﬁcation to
complete these HITs.
You will not be penalized for wrong an-
swers. We will give you a bonus of $0.20
for each correct answer. Thus, it is in
your best interest to attempt each ques-
tion, even if it is just a guess. A few of
the questions will be answerable in just
45 seconds, and we do expect you to get
these right reasonably often.
Annotator Performance Individual annotators
consistently scored well above the chance rate of
25% on the catch questions. In all cases where an
annotator’s accuracy fell below 50% in a round,
they were removed from future rounds. Two an-
notators fell below this threshold, though in that
round they had also performed below threshold in
the untimed validation. No annotators needed to be
removed solely based on performance in the speed
validation task. Average overall accuracy on the
catch questions was 83.8%, indicating that most
workers were able to develop a strategy for ﬁnding
a correct answer when it could be found.
Accuracy on the questions written by Upwork
writers was 48.2% overall, but annotators got better
at this task over time, likely by developing new
strategies to search for answers. Average accuracy
was 39.5% in the ﬁrst round, rising to 58.4% in
the ﬁnal round of data collection. When the major-
ity of annotators (at least 3/5) are able to answer
questions correctly in this setting, we exclude that
question from the subset.
A.2.3 Untimed Validation
Figure 6 shows the UI for untimed validation. As
two writers each write 10 questions for the same
article, there are 20 unique questions per article.
Each validation UI page contains all 20 sets of
questions, and each set of questions contains the5351
reading comprehension question and the three addi-
tional evaluation prompts. Therefore, in total, there
are 80 prompts on each page. The annotator has
to complete all 80 before they can submit the page
and complete the task. The exact instructions that
the annotators read for this task are as follows:
In this task, you will answer multiple
choice questions corresponding to a long
article. Each passage comes with 20 sets
of questions. Each set contains a com-
prehension question and three evaluation
questions. Please read each passage care-
fully before answering the questions for
that passage. Estimated time: 45-55 min-
utes.
If it is impossible to say which of the
other answer options is correct, then se-
lect the answer option that is closest to
correct.
You will receive a bonus of $0.50 for
each reading comprehension question
that you correctly answer. We con-
sider the answer to be correct if your
response of the reading comprehension
question and the ﬁrst evaluation question
both agree with the most common an-
swer from other workers and the original
writer. If there’s no agreement on an an-
swer, we count it as correct for everyone.This means that it’s possible to receive a
total bonus of $10.00 on this HIT.
We expect you to answer most of the
questions correctly; however, we under-
stand that some questions may be difﬁ-
cult, ambiguous, or mal-formed. If you
answer a large number of questions in-
correctly, we will disqualify you from
future work on this task.
For the second and third evaluation ques-
tions, we will check if your choices agree
with the majority of other workers who
work on the same task, but your bonus is
not dependent on these answers. The
results will also be used to determine
whether you retain the qualiﬁcation for
future batches of this task, but to a lower
threshold because these questions tend
to be more subjective.
Annotator Performance Individual annotator
agreement with the gold label is 91.2% for all data
collected in the main data collection (not including
the responses collected to measure human accu-
racy described in §3.1). Throughout the course of
the study, workers need to maintain at least 75%
accuracy each round to keep the qualiﬁcation and
continue to the next round. In a few cases, we
identify passages that are themselves ambiguous or5352
especially difﬁcult. In these cases, we do not use
those passages in computing by-round accuracy for
the annotators. We exclude a total of 11 workers
throughout the course of data collection for low
accuracy, most of them after the ﬁrst or second
round.
Data Reannotation During each untimed vali-
dation round, we keep track of the rate at which
each worker agrees with the original writers’ la-
bels for each question in order to quickly identify
cases where either (i) a worker has misunderstood
the passage, or (ii) a worker is putting insufﬁcient
effort towards the task. For any tasks where the
individual annotator disagrees with the writer’s la-
bels on at least 40% of questions, we automatically
re-post that passage for reannotation and replace
the data, with the assumption that the annotator
may have misunderstood something crucial in the
passage.After all the annotations are complete,
we calculate the gold label answer via majority vote
of annotators plus the original writer’s label, and
assess individual annotator accuracy. If any worker
is excluded in a round for low accuracy (i.e., below
75% accuracy), we discard all of their responses
from that round, reannotate and replace their data,
and re-calculate the gold label and accuracy scores.
B Data
Switchboard Data In order to increase the di-
versity of genres we use as context passages, we
attempted to include Switchboard conversations.
However, after presenting just 12 such conversa-
tions to our writers, we decided to discard all the
Switchboard questions because many writers in-
formed us that it was very difﬁcult to come up
with difﬁcult questions for the Switchboard con-
versations. The writers indicated they found the
Switchboard articles more difﬁcult because the con-
versations are relatively short and usually involve
very simple everyday topics, without the kinds of
plot twists that are more common in short stories
or complex details that are more common in long-
form articles.
Lexical Overlap We analyze the lexical overlap
between the answer options and the passage text
(detailed in §3.4. In Figure 7, we plot the lexical
overlap of both the correct answer option and the
incorrect answer option with the passage.
B.1 Question Types
As described in §3.5, we analyze the different ques-
tion types in QuALITY , split by and
subsets, and present these results along with exam-
ples in Table 7. Most of the questions in the ‘other’
category are ﬁnish-the-phrase style questions, and5353for the example in the table, the answer options are
different ways that sentence could be completed.
Note that in most of the yes/no questions, the an-
swer options include the necessary reasoning to
support the yes/no answer, meaning that these are
often complex, multi-part questions. Examples
shown in Table 7 are randomly selected from the
training set, with the caveat that we selected the
‘when’ question by hand, since about half of the
questions categorized as ‘when’ are referencing
some timepoint (e.g., ‘when X happened, what did
...’)
B.2 Reasoning Types
For the purposes of this analysis, we deﬁne rea-
soning type as the category that needs to be rea-
soned about in selecting the correct answer option
(e.g., ‘person’ is usually a ‘who’ question and cor-
responds to answer options that are characters or
people) or the type of strategy that must be used
in answering (e.g., ‘symbolism/interpretation’ re-
quires the reader to extrapolate from the context
or identify something not stated in a passage, like
its theme). We identify 15 categories of reason-
ing types to include in our analysis. These cate-
gories are initially inspired by those used in Nar-
rativeQA, but we adapt them to our dataset, as we
ﬁnd that many questions in QuALITY do not ﬁt
their categorization. These categories are not mutu-
ally exclusive, and nearly a third of the questions
are categorized as two or more types.
Reasoning Type Deﬁnitions The following in-
cludes deﬁnitions of all the categories used, along
with at least one hand-selected example to demon-
strate a question belonging to that category. All
in-text examples are selected out of the training set.
•Description : The question relies on the
reader reasoning about which description is
correct. Often these questions are about
describing a character’s feelings (‘How do
Lowry and the Exec feel about the Venu-
sians?’) or point of view (‘How is the book
"Living a Normal Sex Life" seen by these peo-
ple?’), describing a feature of the story (‘What
makes Grannie Annie’s writing remarkable?’),
or describing an individual (‘Which word least
describes Don?’)
•Why/reason : The question relies on the
reader reasoning about the cause or expla-
nation for something in the story. Most ofthese questions begin with ‘why’ and ask
about the cause of an event (‘Why does the
crew get off the ship with Moran?’), causes
of characters’ feelings (‘Why does Ben take
offence to Cobb’s comments about space-
men?’), or characters’ internal motivations
(‘Why does Joseph lie about the water sup-
ply?’), though other questions formulate this
differently while still asking for the underly-
ing reason (‘What makes Gubelin an outlier
in the present day?’ and ‘What is the purpose
of a comanalysis?’).
•Symbolism/interpretation : The question re-
lies on the reader making an interpretation
that goes beyond what is explicitly said in the
story, or it asks about symbolism or themes
from the story. Many questions explicitly ask
the reader to interpret what message the author
was trying to convey (‘What point is being
made by comparing Fight Club to the UFC?’)
or what tone the story takes (‘What is the tone
like throughout the story?’). Other questions
require the reader to predict what will happen
next (‘What will happen next to Jery?’) or
ask about the use of literary cues like irony
(‘What is ironic about Earth’s customer ser-
vice policy?’).
•How/method : The question relies on reason-
ing about how something happened or the
method that was used. Most of these questions
rely on the question word ‘how’ to ask about
a process (‘How did Meryl Streep prepare for
the role of Roberta?’), the manner in which
something happens (‘How did Templin ﬁnd
about about Pendleton’s death?’), or a method
by which something happens (‘How does the
shape of Starre’s ship beneﬁt them?’).
•Event : The question relies on reasoning about
an event, or asks for an event as the answer
option. The majority of these questions focus
on what someone did/plans to do (‘What did
Joe and Glmpauszn plan to do?’) or what hap-
pened to someone (‘What happened to Mor-
gan Brockman by the end of the passage?’).
•Person : The question relies on reasoning
about which person or people are involved.
Most ask about a speciﬁc person (‘Who is
Owen Fiss and what did he do?’), though
many questions of this type still require rea-5354
soning about the entire passage to answer
(‘Who seems to have the least to hide in the
text?’).
•Not/except : The question requires the reader
to select the answer option that least an-
swers the question, ﬂipping the typical way
a multiple-choice task is performed. All of
these questions use some word to indicate
this ﬂipping, such as ‘least’ (‘Which word
least describes McGill?’), ‘not’ (‘What word
doesn’t describe the natives from Tunpesh?’),
or ‘except’ (‘Dole makes all of the following
charges against the New York Times EXCEPT
for:’).
•Relation : The question relies on reasoning
about the relationship between two or more
characters, as in ‘Who is Sporr and what is
his authority in calling the narrator Yandro?’
or questions that ask about how one character
feels about another (‘How does Jakdane feel
about Trella?’).
•Entity : The question relies on reasoning
about a non-human entity or a group, as in
‘We can assume that Saladin’s army represents
which group?’.
•Finish the phrase : The form of the ques-tion requires either a ﬁll-in-the-blank style
response or is a partial phrase that must be
completed by selecting the correct answer op-
tion. Often, these questions do not include an
explicit question word. Some of them have a
blank written in (‘The ﬁlm reviewer is gener-
ally _____ the actors in "Princess Mononoke,"
and ______ the actors in "The Limey," respec-
tively:’) and others are just a partial sentence
(‘The less you share...’).
•Location : The question relies on reasoning
about a place, as in ‘What city is Temple-
Tracy in?’.
•Numeric : The question relies on ﬁnding or
computing the correct numeric option, as in
‘How many caves had Garmon and Rolf trav-
eled through before their crash?’.
•Object : The question relies on reasoning
about an object, as in ‘What does Captain
Hannah use as an organic processor?’.
•What if : The question requires the reader to
make an inference about what would have
been true if some fact from the story were
changed, and most of these questions explic-
itly set up the counterfactual scenario (‘What5355
would have happened if the Peace State had
not crash landed?’).
•Duration : The question relies on reasoning
about how long something happened for or
how much time passed between two events, as
in ‘How long did Maggie care for Ben before
he ﬁnally awoke after rescuing him?’.
Annotation Details Three authors of this paper
analyze a set of 500 randomly selected questions.
One author annotates all 500, and the other two an-
notators analyze 250 each, such that each example
is annotated by two unique individuals. Following
annotation, the authors discuss any disagreements
and adjust their original coding once consensus is
reached. Using this consensus approach allows for
clariﬁcation of the categories during and after an-
notation, which leads to an internally consistent
coding scheme.
Sample Annotations Table 8 shows a set of rep-
resentative example annotations from this analysis,
demonstrating several sentences that were catego-
rized as more than one reasoning type.C More Details on Analysis
Lexical Overlap In addition to comparing lexi-
cal overlap of the correct option and the maximum
lexical overlap of the incorrect option with the ar-
ticle (Section 3.4), we also plot a normalized dis-
tribution of lexical overlap for all the correct and
incorrect options in Figure 8. Despite a higher frac-
tion of the correct options having complete overlap5356
with the article, models would not be able to exploit
this heuristic, since other incorrect options for the
same question may have complete overlap. This is
demonstrated by the plot in Figure 7 and the fact
that a baseline which relies on the lexical overlap
heuristic only achieves 26.6% accuracy.
D More Details on Modeling
D.1 Extraction
For ROUGE-1 scoring, we use the
rouge-score Python package.
For fastText scoring, we use SpaCy with the
en_core_web_sm model for tokenization, and
use embeddings trained on Common Crawl,us-
ing the top 300k words in the vocabulary.
For DPR, we use the Transformers package
(Wolf et al., 2020), using the facebook/
dpr-ctx_encoder-multiset-base and
facebook/dpr-question_encoder-
multiset-base models for encoding the
context and query respectively.D.2 Training
The full sets of hyperparameters used for tuning
our baselines are shown in Table 12 and Table 13.
For RoBERTa, DeBERTaV3 and Longformer
models, we train on QuALITY for 20 epochs.
Where we do intermediate training on RACE, we
do so for 3 epochs. Warmup is set to 10% of the
full training steps.
D.3 Results
Table 10 shows the results on development set. Ta-
ble 11 shows the results using oracle-answer-based
extraction. Please refer to the discussion in Sec-
tion 4.2.53575358