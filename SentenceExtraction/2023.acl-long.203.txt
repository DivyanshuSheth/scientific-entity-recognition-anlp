
Tanmay ParekhI-Hung HsuKuan-Hao Huang
Kai-Wei ChangNanyun PengComputer Science Department, University of California, Los AngelesInformation Science Institute, University of Southern California
{tparekh, khhuang, kwchang, violetpeng}@cs.ucla.edu
{ihunghsu}@isi.edu
Abstract
Recent works in Event Argument Extraction
(EAE) have focused on improving model
generalizability to cater to new events and
domains. However, standard benchmarking
datasets like ACE and ERE cover less than
40event types and 25entity-centric argument
roles. Limited diversity and coverage hinder
these datasets from adequately evaluating the
generalizability of EAE models. In this paper,
we first contribute by creating a large and
diverse EAE ontology. This ontology is created
by transforming FrameNet, a comprehensive
semantic role labeling (SRL) dataset for EAE,
by exploiting the similarity between these
two tasks. Then, exhaustive human expert
annotations are collected to build the ontology,
concluding with 115events and 220argument
roles, with a significant portion of roles not be-
ing entities. We utilize this ontology to further
introduce GENEV A, a diverse generalizability
benchmarking dataset comprising four test
suites, aimed at evaluating models’ ability
to handle limited data and unseen event type
generalization. We benchmark six EAE models
from various families. The results show that
owing to non-entity argument roles, even the
best-performing model can only achieve 39%
F1 score, indicating how GENEV A provides
new challenges for generalization in EAE.
Overall, our large and diverse EAE ontology
can aid in creating more comprehensive future
resources, while GENEV A is a challenging
benchmarking dataset encouraging further
research for improving generalizability in EAE.
The code and data can be found at https:
//github.com/PlusLabNLP/GENEVA .
1 Introduction
Event Argument Extraction (EAE) aims at extract-
ing structured information of event-specific argu-
ments and their roles for events from a pre-defined
taxonomy. EAE is a classic topic (Sundheim, 1992)
and elemental for a wide range of applications like
building knowledge graphs (Zhang et al., 2020),Figure 1: Distribution of event types into various ab-
stract event typesfor GENEV A, ACE, ERE, RAMS,
and WikiEvents datasets. We observe that GENEV A is
relatively more diverse than the other datasets.
question answering (Berant et al., 2014), and others
(Hogenboom et al., 2016; Wen et al., 2021; Yang
et al., 2019b). Recent works have focused on build-
ing generalizable EAE models (Huang et al., 2018;
Lyu et al., 2021; Sainz et al., 2022) and they uti-
lize existing datasets like ACE (Doddington et al.,
2004) and ERE (Song et al., 2015) for benchmark-
ing. However, as shown in Figure 1, these datasets
have limited diversity as they focus only on two
abstract types,Action and Change. Furthermore,
they have restricted coverage as they only comprise
argument roles that are entities. The limited diver-
sity and coverage restrict the ability of these exist-
ing datasets to robustly evaluate the generalizability
of EAE models. Toward this end, we propose a new
generalizability benchmarking dataset in our work.
To build a strong comprehensive benchmarking
dataset, we first create a large and diverse ontology.
Creating such an ontology from scratch is time-
consuming and requires expert knowledge. To re-
duce human effort, we exploit the shared properties
between semantic role labeling (SRL) and EAE
(Aguilar et al., 2014) and leverage a diverse and
exhaustive SRL dataset, FrameNet (Baker et al.,
1998), to build the ontology. Through extensive
human expert annotations, we design mappings3664that transform the FrameNet schema to a large and
diverse EAE ontology, spanning 115event types
from five different abstract types. Our ontology
is also comprehensive, comprising 220argument
roles with a significant 37% of roles as non-entities.
Utilizing this ontology, we create GENEV A - a
Generalizability B ENchmarking Dataset for EVent
Argument Extraction. We exploit the human-
curated ontology mappings to transfer FrameNet
data for EAE to build GENEV A. We further per-
form several human validation assessments to en-
sure high annotation quality. GENEV A comprises
four test suites to assess the models’ ability to learn
from limited training data and generalize to unseen
event types. These test suites are distinctly differ-
ent based on the training and test data creation – (1)
low resource, (2) few-shot, (3) zero-shot, and (4)
cross-type transfer settings.
We use these test suites to benchmark various
classes of EAE models - traditional classification-
based models (Wadden et al., 2019; Lin et al.,
2020; Wang et al., 2022a), question-answering-
based models (Du and Cardie, 2020), and gener-
ative approaches (Paolini et al., 2021; Hsu et al.,
2022b). We also introduce new automated refine-
ments in the low resource state-of-the-art model
DEGREE (Hsu et al., 2022b) to generalize and
scale up its manual input prompts. Experiments
reveal that DEGREE performs the best and exhibits
the best generalizability. However, owing to non-
entity arguments in GENEV A, DEGREE achieves
an F1 score of only 39% on the zero-shot suite.
Under a similar setup on ACE, DEGREE achieves
53%, indicating how GENEV A poses additional
challenges for generalizability benchmarking.
To summarize, we make the following contri-
butions. We construct a diverse and comprehen-
sive EAE ontology introducing non-entity argu-
ment roles. This ontology can be utilized further to
develop more comprehensive datasets for EAE. In
addition, we propose a generalizability evaluation
dataset GENEV A and benchmark various recent
EAE models. Finally, we show how GENEV A
is a challenging dataset, thus, encouraging future
research for generalization in EAE.
2 Related Work
Event Extraction Datasets and Ontologies: The
earliest datasets in event extraction date back to
MUC (Sundheim, 1992; Grishman and Sundheim,
1996). Doddington et al. (2004) introduced thestandard dataset ACE while restricting the ontol-
ogy to focus on entity-centric arguments. The ACE
ontology was further simplified and extended to
ERE (Song et al., 2015) and various TAC KBP
Challenges (Ellis et al., 2014, 2015; Getman et al.,
2017). These datasets cover a small and restricted
set of event types and argument roles with limited
diversity. Later, MA VEN (Wang et al., 2020) in-
troduced a massive dataset spanning a wide range
of event types. However, its ontology is limited
to the task of Event Detectionand does not con-
tain argument roles. Recent works have introduced
document-level EAE datasets like RAMS (Ebner
et al., 2020), WikiEvents (Li et al., 2021), and Do-
cEE (Tong et al., 2022); but their ontologies are
also entity-centric, and their event coverage is lim-
ited to specific abstract event types (Figure 1). In
our work, we focus on building a diverse and com-
prehensive dataset for benchmarking generalizabil-
ity for sentence-level EAE.
Event Argument Extraction Models: Tradition-
ally, EAE has been formulated as a classification
problem (Nguyen et al., 2016). Previous
classification-based approaches have utilized
pipelined approaches (Yang et al., 2019a; Wadden
et al., 2019) as well as incorporating global
features for joint inference (Li et al., 2013; Yang
and Mitchell, 2016; Lin et al., 2020). However,
these approaches exhibit poor generalizability in
the low-data setting (Liu et al., 2020; Hsu et al.,
2022b). To improve generalizability, some works
have explored better usage of label semantics by
formulating EAE as a question-answering task (Liu
et al., 2020; Li et al., 2020; Du and Cardie, 2020).
Recent approaches have explored the use of natural
language generative models for structured predic-
tion to boost generalizability (Schick and Schütze,
2021a,b; Paolini et al., 2021; Li et al., 2021).
Another set of works transfers knowledge from
similar tasks like abstract meaning representation
and semantic role labeling (Huang et al., 2018; Lyu
et al., 2021; Zhang et al., 2021). D (Hsu
et al., 2022b) is a recently introduced state-of-the-
art generative model which has shown the best
performance in the limited data regime. In our
work, we benchmark the generalizability of various
classes of old and new models on our dataset.36653 Ontology Creation
Event annotations start with ontology creation,
which defines the scope of the events and their cor-
responding argument roles of interests. Towards
this end, we aim to construct a large ontology of
diverse event types with an exhaustive set of event
argument roles. However, it is a challenging and
tedious task that requires extensive expert supervi-
sion if building from scratch. To reduce human ef-
fort while maintaining high quality, we leverage the
shared properties of SRL and EAE and utilize a di-
verse and comprehensive SRL dataset — FrameNet
to design our ontology. We first re-iterate the EAE
terminologies we follow (§ 3.1) and then describe
how FrameNet aids our ontology design (§ 3.2).
Finally, we present our steps for creating the final
ontology in § 3.3 and ontology statistics in § 3.4.
3.1 Task Definition
We follow the definition of event as a class attribute
with values such as occurrence, state, or report-
ing(Pustejovsky et al., 2003; Han et al., 2021).
Event Triggers are word phrases that best express
the occurrence of an event in a sentence. Following
the early works of MUC (Sundheim, 1992; Grish-
man and Sundheim, 1996), event arguments are
defined as participants in the event which provide
specific and salient information about the event.
Event argument role is the semantic category of
the information the event argument provides. We
provide an illustration in Figure 2 describing an
event about “Destroying” , where the event trigger
isobliterated , and the event consists of argument
roles — Cause andPatient .
It is worth mentioning that these definitions are
disparate from the ones that previous works like
ACE, and its inheritors, ERE and RAMS, follow.
In ACE, the scope of events is restricted to the at-
tribute of occurrence only, and event arguments are
restricted to entities, wherein entities are defined
as objects in the world. For example, in Figure 2,
the subsequent explosions isn’t an entity and will
not be considered an argument as per ACE defini-
tions. Consequently, Cause won’t be part of their
ontology. This exclusion of non-entities leads to
incomplete information extraction of the event. In
our work, we follow MUC to consider a broader
range of events and event arguments.
3.2 FrameNet for EAE
To overcome the challenge of constructing an
event ontology from scratch, we aim to leverage
FrameNet, a semantic role labeling (SRL) dataset,
to help our ontology creation. The similarity be-
tween SRL and EAE (Aguilar et al., 2014) provides
us with the ground for leveraging FrameNet. SRL
assigns semantic roles to phrases in the sentence,
while EAE extracts event-specific arguments and
their roles from the sentence. Hence, selecting
event-related parts of a fine-grained annotated SRL
dataset can be considered as an exhaustively anno-
tated resource for EAE.
We choose FrameNet(Baker et al., 1998) as the
auxiliary SRL dataset since it is one of the most
comprehensive SRL resources. It comprises 1200+
semantic frames (Fillmore et al., 1976), where a
frame is a holistic background that unites similar
words. Each frame is composed of frame-specific
semantic roles ( frame elements ) and is evoked by
specific sets of words ( lexical units ).
To transfer FrameNet’s schema into an EAE
ontology, we map frames as events, lexical
units as event triggers, and frame elements as
argument roles. However, this basic mapping
is inaccurate and has shortcomings since not all
frames are events , and not all frame elements
are argument roles per the definitions in § 3.1.
We highlight these shortcomings in Figure 3,
which enlists some FrameNet frames and frame
elements for the Arrest frame. Based on EAE
definitions, only some frames like Arrest, Travel,
etc(highlighted in yellow) can be mapped as
events, and similarly, limited frame elements like
Authorities, Charges, etc (highlighted in green) are
mappable as argument roles.
3.3 Building the EAE Ontology
To overcome the shortcomings of the basic map-
ping, we follow a two-step approach (Figure 4).
First, we build an event ontology for accurately
mapping frames to events. Then, we augment this
ontology with argument roles by building an event
argument ontology. We describe these steps below.
Event Ontology: In order to build the event on-3666
tology, we utilize the event mapping designed
by MA VEN (Wang et al., 2020), which is an
event detection dataset. They first recursively filter
frames having a relation with the "Event" frame in
FrameNet. Then they manually filter and merge
frames based on the definitions, resulting in an
event ontology comprising 168 event types mapped
from 289 filtered frames.
Event Argument Ontology: In order to augment
argument roles to the event ontology, we perform
an extensive human expert annotation process. The
goal of this annotation process is to create an argu-
ment mapping from FrameNet to our ontology by
filtering and merging frame elements. We describe
this annotation process below.
Annotation Instructions: Annotators are provided
with a list of frame elements along with their de-
scriptions for each frame in the event ontology.
They are also provided with definitions for events
and argument roles as discussed in Section 3.1.
Based on these definitions, they are asked to anno-
tate each frame element as (a) not argument role,
(b) argument role, or (c) merge with existing argu-
ment role (and mention the argument role to merge
with). To ensure arguments are salient, annotators
are instructed to filter out frame elements that are
super generic (e.g. Time, Place, Purpose) unless
they are relevant to the event. Ambiguous cases are
flagged and commonly reviewed at a later stage.
Additionally, annotators are asked to classify
each argument role as an entity or not. This ad-
ditional annotation provides flexibility for quick
conversion of the ontology to ACE definitions. Fig-
ure 14 in the Appendix provides an illustration of
these instructions and the annotation process.
Annotation Results: We recruit two human experts
who are well-versed in the field of event extraction.
We conduct three rounds of annotations and dis-
cussions to improve consistency and ensure a high
inter-annotator agreement (IAA). The final IAA
measured as Cohen’s Kappa (McHugh, 2012) was
0.82for mapping frame elements and 0.94for en-
tity classification. A total of 3,729frame elements
from 289frames were examined as part of the an-
notation process. About 63% frame elements were
filtered out, 14% were merged, and the remaining
23% constitute as argument roles.
Event Ontology Calibration: The MA VEN event
ontology is created independent of the argument
roles. This leads to some inaccuracies in their on-
tology wherein two frames with disparate sets of
argument roles are mapped as a single event. For
example, Surrendering_possession andSurrender-
ingframes are merged together despite having dif-
ferent argument roles. Based on our human expert-
curated event argument ontology, we rectify these
inaccuracies (roughly 8%of the event ontology)
and create our final ontology.
3.4 Ontology Statistics
We present the statistics of our full ontology in
Table 1 and compare it with existing ACE (Dod-
dington et al., 2004) and RAMS (Ebner et al.,
2020) ontologies. But as we will specify in § 4.1,
we use a subset of this ontologyfor creating
GENEV A. Hence, we also include the statistics
of the GENEV A ontology in the last column in
Table 1. Overall, our curated full ontology is the
largest and most comprehensive as it comprises
179 event types and 362 argument roles. Defin-
ingabstract event types as the top nodes of the
ontology tree created by MA VEN (Wang et al.,
2020), we show that our ontology spans 5different
abstract types and is the most diverse. We orga-
nize our ontology into a hierarchy of these abstract3667
event types in Appendix A.3. Our ontology is also
dense with an average of 4.82argument roles per
event type. Finally, we note that a significant 35%
of the event argument roles in our ontology are
non-entities. This demonstrates how our ontology
covers a broader and more comprehensive range
of argument roles than other ontologies following
ACE definitions of entity-centric argument roles.
4 GENEV A Dataset
Previous EAE datasets for evaluating generalizabil-
ity like ACE and ERE have limited event diver-
sity and are restricted to entity-centric arguments.
To overcome these issues, we utilize our ontology
to construct a new generalizability benchmarking
dataset GENEV A comprising four specialized test
suites. We describe our data creation process in
§ 4.1, provide data statistics in § 4.2 and discuss
out test suites in § 4.3.
4.1 Creation of GENEV A
Since annotating EAE data for our large ontol-
ogy is an expensive process, we leverage the an-
notated dataset of FrameNet to create GENEV A
(Figure 4). We utilize the previously designed on-
tology mappings to repurpose the annotated sen-
tences from FrameNet for EAE by mapping frames
to corresponding events, lexical units to event trig-
gers, and frame elements to corresponding argu-
ments. Unmapped frames and frame elements (not
in the ontology) are filtered out from the dataset.
Since FrameNet doesn’t provide annotations for all
frames, some events from the full ontology are not
present in our dataset (e.g. Military_Operation ).
Additionally, to aid better evaluation, we remove
events that have less than 5event mentions (e.g.
Lighting ). Finally, GENEV A comprises 115 event
types and 220 argument roles. Some examples are
provided in Figure 10 (Appendix).
Human Validation: We ensure the high quality of
our dataset by conducting two human assessments:
(1)Ontology Quality Assessment : We present the
human annotators with three sentences - one pri-
mary and two candidates - and ask them if the event
in the primary sentence is similar to the events in
either of the candidates or distinct from both (Ex-
ample in Appendix F). One candidate sentence is
chosen from the frame merged with the primary
event, while the other candidate is chosen from
a similar unmerged sister frame. The annotators
chose the merged frame candidates 87% of the
times, demonstrating the high quality of the ontol-
ogy mappings. This validation was done by three
annotators over 61triplets with 0.7IAA measured
by Fleiss’ kappa (Fleiss, 1971).
(2)Annotation Comprehensiveness Assessment :
Human annotators are presented with annotated
samples from our dataset and they are asked to
report if there are any arguments in the sentence
that have not been annotated. The annotation is
considered comprehensive if all arguments are an-
notated correctly. The annotators reported that the
annotations were 89% comprehensive, ensuring
high dataset quality. Corrections majorly comprise
ambiguous cases and incorrect role labels. This
assessment was done by two experts over 100sam-
pled annotations with 0.93IAA (Cohen’s kappa).
4.2 Data Analysis
Overall, GENEV A is a dense, challenging, and
diverse EAE dataset with good coverage. These
characteristics make GENEV A better-suited than
existing datasets like ACE/ERE for evaluating the
generalizability of EAE models. The major statis-
tics for GENEV A are shown in Table 2 along with
its comparison with ACE and ERE. We provide
more discussions about the characteristics of our
dataset as follows.
Diverse: GENEV A has wide coverage with a
tripled number of event types and 10 times the
number of argument roles relative to ACE/ERE.3668
Figure 1 further depicts how ACE/ERE focus only
on specific abstractions Action and Change, while
GENEV A is the most diverse with events ranging
from 5abstract types.
Challenging: The average number of mentions
per event type and argument role (Table 2) is rela-
tively less for GENEV A. Consequently, EAE mod-
els need to train from fewer examples on average
which makes training more challenging.
Dense: We plot the distribution of arguments per
sentencefor ACE, ERE, and GENEV A in Figure 5.
We note that GENEV A has the highest density of
4argument mentions per sentence. Both ACE and
ERE have more than 70% sentences with up to 2
arguments. In contrast, GENEV A is denser with
almost 50% sentences having 3 or more arguments.
Coverage: Qualitatively, we show some cover-
age of diverse examples in Figure 9 (Appendix)
and provide coverage for all events categorized by
their abstraction in Figure 14 (Appendix). We ob-
serve frequent events like Statement, Arriving, Ac-
tion while Recovering, Emergency, Hindering are
less-frequent events. In terms of diversity of data
sources, our data comprises a mixture of news arti-
cles, Wall Street Journal articles, books, Wikipedia,
and other miscellaneous sources too.
4.3 Benchmarking Test Suites
With a focus on the generalizability evaluation of
EAE models, we fabricate four benchmarking test
suites clubbed into two higher-level settings:
Limited Training Data: This setting mimics the
realistic scenario when there are fewer annotations
available for the target events and evaluates the
models’ ability to learn from limited training data.
We present two test suites for this setting:
•Low resource (LR): Training data is created
byrandomly sampling nevent mentions.Werecord the model performance across a spec-
trum from extremely low resource ( n= 10 ) to
moderate resource ( n= 1200 ) settings.
•Few-shot (FS): Training data is curated by sam-
pling nevent mentions uniformly across all
events. This sampling strategy avoids biases to-
wards high data events and assesses the model’s
ability to perform well uniformly across events.
We study the model performance from one-shot
(n= 1) to five-shot ( n= 5).
Unseen Event Data: The second setting focuses
on the scenario when there is no annotation avail-
able for the target events. This helps test models’
ability to generalize to unseen events and argument
roles. We propose two test suites:
•Zero-shot (ZS): The training data comprises the
topmevents with most data, where mvaries
from 1 to 10.The remaining 105 events are
used for evaluation.
•Cross-type Transfer (CTT): We curate a train-
ing dataset comprising of events of a single ab-
straction category (e.g. Scenario), while the test
dataset comprises events of all other abstrac-
tion types. This test suite also assesses models’
transfer learning strength.
Data statistics for these suites are presented
in Appendix A.2. For each setup, we sample 5
different datasetsand report the average model
performance to account for the sampling variation.
5 Experimental Setup
We evaluate the generalizability of various EAE
models on GENEV A. We describe these models in
§ 5.1 and the evaluation metrics in § 5.2.
5.1 Benchmarked Models
Overall, we benchmark six EAE models from vari-
ous representative families are described below. Im-
plementation details are specified in Appendix G.
Classification-based models: These traditional
works predict arguments by learning to trace the
argument span using a classification objective. We
experiment with three models: (1) DyGIE++ (Wad-
den et al., 2019), a traditional model utilizing multi-
sentence BERT encodings and span graph propaga-
tion. (2) OneIE (Lin et al., 2020), a multi-tasking3669objective-based model exploiting global features
for optimization. (3) Query&Extract (Wang et al.,
2022a) utilizing the attention mechanism to extract
arguments from argument role queries.
Question-Answering models: Several works for-
mulate event extraction as a machine reading com-
prehension task. We consider one such model - (4)
BERT_QA (Du and Cardie, 2020), a BERT-based
model leveraging label semantics using a question-
answering objective. In order to scale BERT_QA
to the wide range of argument roles, we generate
question queries of the form “ What is {arg-name}? ”
for each argument role {arg-name} . (5) TE(Lyu
et al., 2021), a zero-shot transfer model that utilizes
an existing pre-trained textual entailment model to
automatically extract events. Similar to BERT_QA,
we design hypothesis questions as “ What is {arg-
name}? ” for each argument role {arg-name} .
Generation-based models: Inspired by great
strides in natural language generation, recent works
frame EAE as a generation task using a language-
modeling objective. We consider two such models:
(6)TANL (Paolini et al., 2021), a multi-task lan-
guage generation model which treats EAE as a
translation task. (7) DEGREE (Hsu et al., 2022b),
an encoder-decoder framework that extracts event
arguments using natural language input prompts.
Automating DEGREE : DEGREE requires human
effort for manually creating natural language
prompts and thus, can not be directly deployed
for the large set of event types in GENEV A. In our
work, we undertake efforts to scale up DEGREE by
proposing a set of automated refinements. The first
refinement automates the event type description as
“The event type is {event-type} " where {event-type}
is the input event type. The second refinement au-
tomates the event template generation by splitting
each argument into a separate self-referencing mini-
template “ The {arg-name} is some {arg-name} "
where {arg-name} is the argument role. The final
event-agnostic template is a simple concatenation
of these mini-templates. We provide an illustration
and ablation of these automated refinements for
DEGREE in Appendix B.
5.2 Evaluation Metrics
Following the traditional evaluation for EAE tasks,
we report the micro F1 scores for argument classi-
fication. To encourage better generalization across
a wide range of events, we also use macro F1 score
that reports the average of F1 scores for each event
type. For the limited data test suites, we record a
model performance curve, wherein we plot the F1
scores against the number of training instances.
6 Results
Following § 4.3, we organize the main experi-
mental results into limited training data and un-
seen event data settings. When trained on com-
plete training data, we observe that OneIE and
Query&Extract models achieve poor micro F1
scores of just 30.03and40.41while all other mod-
els achieve F1 scores above 55. This can be at-
tributed to the inability of their model designs to
effectively handle overlapping arguments.Due
to their inferior performance, we do not include
OneIE and Query&Extract in the benchmarking
results. We present the full results in Appendix H.
6.1 Limited Training Data
Limited training data setting comprises of the low
resource and the few-shot test suites. We present
the model benchmarking results in terms of macro
and micro F1 scores for the low resource test suite
in Figure 6 and for the few-shot test suite in Fig-
ure 7 respectively. We observe that DEGREE out-
performs all other models for both the test suites
and shows superior generalizability. In general, we
observe that generation-based models show better3670
generalization while on the other hand, traditional
classification-based approaches show poor general-
izability. This underlines the importance of using
label semantics for better generalizability. We also
detect a stark drop from micro to macro F1 scores
for TANL and DyGIE++ in the low resource test
suite. This indicates that these models are more
easily biased toward high data events and do not
generalize well uniformly across all events.
6.2 Unseen Event Data
This data setting includes the zero-shot and the
cross-type transfer test suites. We collate the re-
sults in terms of micro F1 scores for both the test
suites in Table 3. Models like DyGIE++ and TANL
cannot support unseen events or argument roles and
thus, we do not include these models in the experi-
ments for these test suites. TE cannot be trained on
additional EAE data, and hence we only report the
pure zero-shot performance of this model.
From Table 3, we observe that DEGREE
achieves the best scores across both test suites
outperforming BERT_QA by a significant margin
of almost 13-15% F1 points. Although TE is not
comparable as it’s a pure zero-shot model (without
training on any data), it’s performance is relatively
super low in both settings. Thus, DEGREE shows
superior transferability to unseen event types and
argument roles.
7 Analysis
In this section, we provide analyses highlight-
ing the various new challenges introduced by
GENEV A. We discuss the performance of large
language models, the introduction of non-entity ar-
gument roles, and model performance including
Time and Place argument roles.
7.1 Large Language Model Performance
Recently, there has been an advent of Generative
AI in the form of Large Language Models (LLMs)
like GPT-3 (Brown et al., 2020), GPT-4, PaLM
(Chowdhery et al., 2022), Code4Struct (Wang et al.,
2022b), and many more. We evaluate one of these
models GPT3.5-turbo on the task of EAE on the
zero-shot test suite of GENEV A. More specifi-
cally, we provide 5 in-context examples from top-
10 events and evaluate test examples from the re-
maining 105 events. Our GPT-prompt template
follows the DEGREE template wherein model re-3671
places placeholders with arguments if present, else
copies the original teample. An illustration is pro-
vided in Figure 8.
Despite the strong generation capability,
GPT3.5-turbo achieves a mere 22.73 F1 score
while DEGREE achieves 24.06 and 39.43 F1
scores in the ZS-1 and ZS-10 test suites respec-
tively. Although these scores aren’t directly
comparable, it shows how GENEV A is quite chal-
lenging for LLMs in the zero-shot/few-shot setting.
7.2 New Challenge of Non-entity Roles
In Table 4, we show the model performances of
BERT_QA and DEGREE on GENEV A and ACE
under similar benchmarking setups. We note how
both models exhibit relatively poor performance
on GENEV A (especially the zero-shot test suite).
To investigate this phenomenon, we break down
the model performance based on entity and non-
entity argument roles and show this analysis in
Table 5. This ablation reveals a stark drop of 10-
14% F1 points across all models when predicting
non-entity arguments relative to entity-based argu-
ments. This trend is observed consistently across
all different test suites as well. We can attribute
this difference in model performance to non-entity
arguments being more abstract and having longer
spans, in turn, being more challenging to predict
accurately. Thus, owing to a significant 37% non-
entity argument roles, GENEV A poses a new and
interesting challenge for generalization in EAE.
7.3 GENEV A with Time and Place
In the original GENEV A dataset, we filtered super
generic argument roles, but some of these roles like
Time and Place are key for several downstream
tasks. We include Time and Place arguments in
GENEV Aand provide results of the models on
the full dataset in Table 6. Compared to original
GENEV A results in the same setting, we observe
a slight dip in the model performance owing to
the addition of extra arguments. Overall, the trend
is similar where TANL performs the best and we
observe better generalization in terms of macro F1
performance.
7.4 Discussion
Overall, our generalizability benchmarking reveals
various insights. First, generation-based models
like DEGREE exhibit strong generalizability and
establish a benchmark on our dataset. Second,
macro score evaluation reveals how models like
TANL and DyGIE++ can be easily biased toward
high-data events. Finally, we show how GENEV A
poses a new challenge in the form of non-entity
arguments, encouraging further research for im-
proving generalization in EAE.
8 Conclusion and Future Work
In our work, we exploit the shared relations be-
tween SRL and EAE to create a new large and di-
verse event argument ontology spanning 115event
types and 220 argument roles. This vast ontol-
ogy can be used to create larger and more compre-
hensive resources for event extraction. We utilize
this ontology to build a new generalizability bench-
marking dataset GENEV A comprising four distinct
test suites and benchmark EAE models from var-
ious families. Our results inspire further research
of generative models for EAE to improve general-
ization. Finally, we show that GENEV A poses new
challenges and anticipate future generalizability
benchmarking efforts on our dataset.3672Acknowledgements
We would like to thank Hritik Bansal, Di Wu, Sidi
Lu, Derek Ma, Anh Mac, and Zhiyu Xie for their
valuable insights, experimental setups, paper re-
views, and constructive comments. We thank the
anonymous reviewers for their feedback. This work
was partially supported by NSF 2200274, AFOSR
MURI via Grant #FA9550- 22-1-0380, Defense Ad-
vanced Research Project Agency (DARPA) grant
#HR00112290103/HR0011260656, and a Cisco
Sponsored Research Award.
Limitations
We would like to highlight a few limitations of
our work. First, we would like to point out that
GENEV A is designed to evaluate the generalizabil-
ity of EAE models. Although the dataset contains
event type and event trigger annotations, it can
only be viewed as a partially-annotated dataset if
end-to-end event extraction is considered. Sec-
ond, GENEV A is derived from an existing dataset
FrameNet. Despite human validation efforts, there
is no guarantee that all possible events in the sen-
tence are exhaustively annotated.
Ethical Consideration
We would like to list a few ethical considerations
for our work. First, GENEV A is derived from
FrameNet which comprises of annotated sentences
from various news articles. Many of these news
articles cover various political issues which might
be biased and sensitive to specific demographic
groups. We encourage careful consideration for uti-
lizing this data for training models for real-world
applications.
References367336743675A Additional Analysis of GENEV A
A.1 Event Type Distribution for GENEV A
We show the distribution of event mentions per
event type for GENEV A in Figure 9. We observe
a highly skewed distribution with 44event types
having less than 25 event mentions. Furthermore,
93event types have less than 100 event mentions.
We believe that this resembles a more practical
scenario where there is a wide range of events with
limited event mentions while a few events have a
large number of mentions.
A.2 Data Statistics for different
benchmarking test suites
We present the data statistics for the various test
suites in Table 7. For the training set of the low
resource and few-shot test suites (indicated by ∗
in Table 7), we sample a smaller training set (as
discussed in Section 4.3). For the zero-shot setup,
the top 10 event types contribute to a large pool
of1,889sentences. For the test suites, a fixed
number of 450 and 115 sentences are sampled for
the training and the development set (indicated by
+in Table 7) from this larger pool of data.
A.3 Event Ontology Organization
The broad set of event types in GENEV A can be
organized into a hierarchical structure of abstractevent types. Adhering to the hierarchical tree struc-
ture introduced in MA VEN, we show the corre-
sponding organization for event types in GENEV A
in Figure 15. The organization mainly assumes
five abstract event categories - Action, Change,
Scenario, Sentiment, and Possession. The most
populous abstract type is Action with a total of 53
events, while Scenario abstraction has the lowest
number of 9 events.
We also study the distribution of event mentions
per event type in Figure 15 where the bar heights
are indicative of the number of event mentions for
the corresponding event type (heights in log-scale).
We observe that the most populous event is State-
ment which falls under the Action abstraction. On
the other hand, the least populous event is Recover-
ingwhich belongs to the Change abstraction.
GENEV A comprises of a diverse set of 115 event
types and it naturally shares some of these with
the ACE dataset. In Figure 15, we show the ex-
tent of the overlap of the mapped ACE events in
the GENEV A event schema (text labels colored in
red).We can observe that although there is some
overlap between the datasets, GENEV A brings in
a vast pool of new event types. Furthermore, most
of the overlap is for the Possession and Action
abstraction types.
A.4 Dataset Examples
We provide some examples of annotated sentences
from the GENEV A dataset in Figure 10. We indi-
cate the abstract event type in braces and cover an
example from each abstraction.
B Automated Refinements for DEGREE
B.1 DEGREE
DEGREE is an encoder-decoder based generative
model which utilizes natural language templates as
part of input prompts. The input prompt comprises
of three components - (1) Event Type Description
which provides a definition of the given event type,
(2)Query Trigger which indicates the trigger word
for the event mention, and (3) EAE Template which
is a natural sentence combining the different argu-
ment roles of the event. We illustrate DEGREE
along with an example of its input prompt design
in Figure 11.3676
Despite the superior performance of D
in the low-data setting, it can not be directly de-
ployed on GENEV A. This is because D re-
quires manual human effort for the creation of in-
put prompts for each event type and argument role
and can’t be scaled to the wide set of events in
GENEV A. Thus, there is a need to automate the
manual human effort to scale up D .B.2 Automated Refinements
D requires human effort for two input
prompt components - (1) Event Type Description
and (2) EAE Template. We describe the automated
refinements in DEGREE for these components be-
low.
Automating Event Type Description Event
type description is a natural language sentence de-
scribing the event type. In order to automate this
component, we propose a simple heuristic that cre-
ates a simple natural language sentence mentioning
the event type - “ The event type is {event-type} .",
as illustrated in Figure 12.
Automating EAE Template EAE template gen-
eration in D can be split into two subtasks,
which we discuss in detail below.
Argument Role Mapping: This subtask maps
each argument role to a natural language place-
holder phrase based on the characteristics of the
argument role. For example, the argument role
Employer is mapped to “ some organization ” in
Figure 11. For automating this mapping pro-
cess, we propose a simple refinement of self-
mapping, which maps each argument role to a self-
referencing placeholder phrase “ some {arg-name} ”,
where {arg-name} is the argument role itself. For
example, the argument role Employer would be
mapped to “ some employer ”. We illustrate an ex-
ample of this heuristic in Figure 12.
Template Generation: The second subtask re-
quires generating a natural sentence(s) using the
argument role-mapped placeholder phrases (as
shown in Figure 11). To automate this subtask,
we create an event-agnostic template composed of
argument role-specific sentences. For each argu-
ment role in the event, we generate a sentence of3677
the form “ The {arg-name} is {arg-map}. ” where
{arg-name} and{arg-map} is the argument role and
its mapped placeholder phrase respectively. For
example, the sentence for argument role Employer
with self-mapping would be " The employer is some
employer. ". The final event-agnostic template is
a simple concatenation of all the argument role
sentences. We provide an illustration of the event-
agnostic template in Figure 12.
B.3 Ablation Study
In our work, we introduce automated refinements
for scaling DEGREE for GENEV A. We provide an
ablation study for these automated refinements (Au-
tomated DEGREE) on the ACE dataset in Table 8.
We observe that the automated DEGREE almost
at-par with DEGREE with a minor difference of
only0.8%F1 points.
C Impact of Pre-training
In this section, we explore the impact of pre-
training models on the generalizability evaluation.
We consider DEGREE and BERT_QA, pre-train
them on the ACE dataset and show the model per-
formance on low resource test suite in Figure 13.
We observe that pre-training helps model per-
formance by 5-10% F1 points, and naturally in
the low-data regime. But the gains diminish
and are almost negligible as the number of train-
ing event mentions increases. In terms of zero-
shot performance of the pre-trained models, DE-
GREE achieves a micro F1 score of 12.83% and
BERT_QA achieves a score of 6.82% respectively.
Poor zero-shot performance and diminishing per-
formance gains indicate that GENEV A is distribu-
tionally distinct from ACE, which makes it chal-
lenging to achieve good model performance on
GENEV A merely via transfer learning.
D Case Study: Is ACE diverse enough?
We conduct a case study to analyze how the limited
diversity of ACE can affect the generalizability of
EAE models. We compare the performance of two
models with different initializations - (1) DEGREE
pre-trained on the ACE dataset and (2) DEGREE
with no pre-training - on the zero-shot with 10
event types benchmarking setup. We dissect the F1
scores into different abstract event types and show
the results in Table 9.
We observe that pre-training yields major im-
provements for the abstractions of Action, Posses-
sion, and Change - which are well-represented in
ACE. On the other hand, we observe lower perfor-
mance improvement for the abstractions of Sen-
timent and Scenario - which are not represented
in ACE. This trend clearly shows that the lack of
diversity in ACE restricts the models’ ability to
generalize well to out-of-domain event types. We
also highlight the significance of GENEV A as its
diverse evaluation setup helps analyze these trends.3678
E Human expert annotation for EAE
ontology creation
Figure 14 present the annotation instructions and
example input data for the human expert annotation
process used for event argument ontology creation.
F Human validation for GENEV A
We provide an example of the annotation setup
used for the Ontology Quality Assessment as part of
GENEV A validation process in Table 10. Similarly,
we provide the annotation setup and some examples
for the Annotation Comprehensiveness Assessment
in Table 11.
G Implementation Details
In this section, we provide details about the experi-
mental setups and training details for various EAE
models we mentioned in our work.
G.1 DEGREE
We closely follow the training setup by D
for training the DEGREE models. We run experi-
ments for DEGREE on a NVIDIA GeForce RTX
2080 Ti machine with support for 8 GPUs. We
present the complete range of hyperparameter de-
tails in Table 12. We deploy early stopping criteria
for stopping the model training.
G.2 BERT_QA
We mostly follow the original experimental setup
and hyperparameters as described in Du and Cardie
(2020). We use B-L instead of the orig-
inalB-B to ensure that the PLMs are of
comparable sizes for DEGREE and BERT_QA. We
run experiments for this model on a NVIDIA A100-
SXM4-40GB machine with support for 4 GPUs. Amore comprensive list of hyperparameters is pro-
vided in Table 13.
G.3 TANL
We report the hyperparameter settings for the
TANL experiments in Table 14. We make optimiza-
tion changes in the provided source code of TANL
to include multiple triggers in a single sentence.
Experiments for TANL were run on a NVIDIA
GeForce RTX 2080 Ti machine with support for 8
GPUs.
G.4 DyGIE++
We report the hyperparameter settings for the Dy-
GIE++ experiments in Table 15. Experiments for
DyGIE++ were run on a NVIDIA GeForce RTX
2080 Ti machine with support for 4 GPUs.
G.5 OneIE
We report the hyperparameter settings for the
OneIE experiments in Table 16. Experiments for
OneIE were run on a NVIDIA GeForce RTX 2080
Ti machine with support for 4 GPUs.
G.6 Query&Extract
We report the hyperparameter settings for the
Query&Extract experiments in Table 17. Experi-
ments for OneIE were run on an NVIDIA GeForce
RTX 2080 Ti machine with support for 4 GPUs.
G.7 TE
We use the original SRL engine and model pro-
vided in the repo for running the TE model. Since
there was no training, we do not change any hyper-
parameters.3679
PLM BART-Large
Training Batch Size 6
Eval Batch Size 12
Learning Rate 1×10
Weight Decay 1×10
# Warmup Epochs 5
Gradient Clipping 5
Max Training Epochs 50
# Accumulation Steps 1
Beam Size 1
Max Sequence Length 200
Max Output Length 150
H Complete Results
In this section, we present the exhaustive set of
results for each of the runs for the different bench-
marking suites. We show the results for the low
resource and few-shot setting are shown in Fig-
ures 16 and 17 respectively. Figure 18 displays
the results for the zero-shot and cross-type transfer
settings.PLM BERT-Large
Training Batch Size 12
Eval Batch Size 8
Learning Rate 1×10
# Training Epochs 50
# Evaluations per Epoch 5
Max Sequence Length 300
Max Answer Length 50
N-Best Size 20
PLM T5-Base
Training Batch Size 8
Eval Batch Size 12
Learning Rate 5×10
# Training Epochs 20
Evaluation per # Steps 100
Max Sequence Length 256
# Beams 8
PLM BERT-Large
Training Batch Size 6
Eval Batch Size 12
Learning Rate 2×10
# Training Epochs 200
Evaluation per # Epoch 1
Max Sequence Length 175
# Beams 83680PLM BERT-Large
Training Batch Size 6
Eval Batch Size 12
Learning Rate 1×10
# Training Epochs 150
Evaluation per # Epoch 1
Max Sequence Length 175
# Beams 8
PLM BERT-Large
Training Batch Size 16
Eval Batch Size 16
Learning Rate 5×10
Weight Decay 0.001
# Training Epochs 5
Evaluation per # Epoch 10
Entity Embedding Size 1003681368236833684ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
After Section 7 (Conclusion)
/squareA2. Did you discuss any potential risks of your work?
After Section 7 (Conclusion)
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3 and 4
/squareB1. Did you cite the creators of artifacts you used?
Section 3 and 4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 3
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 3
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
The data we use is all public news data and doesn’t contain private personal information.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Section 5 and 6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Partially in Appendix G3685/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix G
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3 and 4
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.3686