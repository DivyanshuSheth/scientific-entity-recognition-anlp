
Wenxuan Zhou, Fangyu Liu, Ivan Vuli ¬¥c, Nigel Collier, Muhao ChenLUKA Lab, University of Southern California, USALanguage Technology Lab, TAL, University of Cambridge, UK
{zhouwenx,muhaoche}@usc.edu {fl399,iv250,nhc30}@cam.ac.uk
Abstract
Knowledge bases (KBs) contain plenty of
structured world and commonsense knowl-
edge. As such, they often complement dis-
tributional text-based information and facili-
tate various downstream tasks. Since their
manual construction is resource- and time-
intensive, recent e orts have tried leveraging
large pretrained language models (PLMs) to
generate additional monolingual knowledge
facts for KBs. However, such methods have
not been attempted for building and enrich-
ing multilingual KBs. Besides wider appli-
cation, such multilingual KBs can provide
richer combined knowledge than monolingual
(e.g., English) KBs. Knowledge expressed
in dierent languages may be complementary
and unequally distributed: this implies that
the knowledge available in high-resource lan-
guages can be transferred to low-resource ones.
To achieve this, it is crucial to represent mul-
tilingual knowledge in a shared /uniÔ¨Åed space.
To this end, we propose a uniÔ¨Åed representa-
tion model, Prix-LM , for multilingual KB
construction and completion. We leverage two
types of knowledge, monolingual triples and
cross-lingual links , extracted from existing
multilingual KBs, and tune a multilingual lan-
guage encoder XLM-R via a causal language
modeling objective. Prix-LM integrates use-
ful multilingual and KB-based factual knowl-
edge into a single model. Experiments on stan-
dard entity-related tasks, such as link predic-
tion in multiple languages, cross-lingual en-
tity linking and bilingual lexicon induction,
demonstrate its e ectiveness, with gains re-
ported over strong task-specialised baselines.
1 Introduction
Multilingual knowledge bases (KBs), such as DB-
Pedia (Lehmann et al., 2015), Wikidata (Vrande Àáci¬¥c
and Kr√∂tzsch, 2014), and YAGO (Suchanek et al.,
2007), provide structured knowledge expressed inFigure 1: An illustration of the main idea support-
ingPrix-LM : it infuses complementary multilingual
knowledge from KGs into a multilingual causal LM;
e.g., Japanese KG stores more comprehensive genre
information of T heTale of Genjithan KGs in other
languages. Through cross-lingual links (translations),
such knowledge is then propagated across languages.
multiple languages. Those KBs are modeled as
knowledge graphs (KGs) that possess two types
of knowledge: monolingual triples which describe
relations of entities, and cross-lingual links which
match entities across languages. The knowledge
stored in such KGs facilitates various downstream
applications such as question answering (Dai et al.,
2016; Bauer et al., 2018; Wang et al., 2021b), rec-
ommendation (Zhang et al., 2016; Wang et al.,
2018, 2021c), and dialogue systems (Madotto et al.,
2018; Liu et al., 2019; Yang et al., 2020).
Manually constructing large-scale knowledge
bases has been labor-intensive and expensive (Paul-
heim, 2018), leading to a surge of interest in auto-
matic knowledge base construction (Ji et al., 2022).
Recent research (Bosselut et al., 2019; Yao et al.,
2019; Wang et al., 2020, inter alia ) proposes to
generate structured knowledge using pretrained lan-5412guage models (PLMs; Devlin et al. 2019), where
missing elements in KB facts (i.e., triples) can be
completed (i.e., Ô¨Ålled in) by the PLM.
While these methods arguably perform well for
English, such automatic KB construction has not
yet been tried for multilingual KBs ‚Äì improving
the knowledge in multilingual KBs would have a
positive impact on applications in other languages
beyond English. Moreover, KBs in multiple lan-
guages may possess complementary knowledge,
and knowledge bases in low-resource languages
often su er severely from missing entities and
facts. This issue could be mitigated by propagat-
ing knowledge from multiple well-populated high-
resource languages‚Äô KBs (e.g., English and French
KBs) to the KBs of low-resource languages, this
way ‚Äòcollectively‚Äô improving the content stored in
the full multilingual KB.
However, training LMs to capture structural
knowledge independently for each language will
fall short of utilizing complementary and trans-
ferable knowledge available in other languages.
Therefore, a uniÔ¨Åed representation model is re-
quired, which can capture, propagate and enrich
knowledge in multilingual KBs. In this work, we
thus propose to train a language model for con-
structing multilingual KBs. Starting from XLM-
R (Conneau et al., 2020) as our base model, we
then pretrain it on the multilingual DBpedia, which
stores both monolingual triples and cross-lingual
links (see Figure 1). We transform both types of
knowledge into sequences of tokens and pretrain
the language model with a causal LM objective
on such transformed sequences. The monolingual
triples infuse structured knowledge into the lan-
guage model, while the cross-lingual links help
align knowledge between di erent languages. This
way, the proposed model Prix-LM (Pre-trained
Knowledge- incorporated Cross -lingual Language
Model) is capable of mapping knowledge of di er-
ent languages into a uniÔ¨Åed /shared space.
We evaluate our model on four di erent tasks
essential for automatic KB construction, covering
both high-resource and low-resource languages:
link prediction, cross-lingual entity linking, bilin-
gual lexicon induction, and prompt-based LMknowledge probing. The main results across all
tasks indicate that Prix-LM brings consistent and
substantial gains over various state-of-the-art meth-
ods, demonstrating its e ectiveness.
2Prix-LM
We now describe Prix-LM , Ô¨Årst outlining the data
structure and pretraining task, and then describing
its pretraining procedure in full (¬ß2.1), and e cient
inference approaches with Prix-LM (¬ß2.2).
Pretraining Task. We rely on multilingual DB-
pedia, but note that Prix-LM is also applicable to
other KBs. DBpedia contains two types of struc-
tured knowledge: monolingual knowledge triples,
and cross-lingual links between entities. The mono-
lingual triples represent (relational) facts expressed
in a structured manner. Each triple is denoted as
fe;r;eg: the elements of a triple are identiÔ¨Åed
as the subject entity e, relation (or predicate) r,
and object entity e, respectively (see also Figure 1
for examples). For instance, the fact ‚Äú The capi-
tal of England is London ‚Äù can be represented as
fEngland;capital;Londong. The cross-lingual
links, denoted asfe;eg, represent the correspon-
dence of ‚Äòmeaning-identical‚Äô entities eandein
two di erent languages: e.g., the English entity
London is mapped to L ondres in Spanish.
We treat both types of knowledge using the same
input formatfs;p;og, where s=e;p=r;o=
efor monolingual knowledge triples, and s=
e;p=null;o=efor cross-lingual entity links.
The pretraining task is then generating ogiven s
andp. This objective is consistent with the link
prediction task and also beneÔ¨Åts other entity-related
downstream tasks, as empirically validated later.
2.1 Pretraining Language Models
Prix-LM is initialized by a multilingual PLM such
as XLM-R (Conneau et al., 2020): starting from
XLM-R‚Äôs pretrained weights, we train on the struc-
tured knowledge from a multilingual KB.
Input Representation. We represent knowledge
from the KB as sequences of tokens. In particular,
given some knowledge fact fs;p;og, where each
element is the surface name of an entity or a re-
lation, we tokenizethe elements to sequences of
subtokens X,X, and X. We treat each element
in the knowledge fact as a di erent text segment
and concatenate them to form a single sequence.5413We further introduce special tokens to represent
dierent types of knowledge:
(1) Monolingual Triples. We use special tokens to
indicate the role of each element in the triple, which
converts the sequence to the following format:
<s>is the special token denoting beginning of se-
quence; </s> is the separator token, both adopted
from XLM-R. Additional special tokens [S],[P]
and[O] denote the respective roles of subject,
predicate, and object of the input knowledge fact.
[EOS] is the end-of-sequence token.
(2) Cross-Lingual Links. As the same surface form
of an entity can be associated with more than lan-
guage, we use special language tokens to indicate
the actual language of each entity. These extra
tokens can also be interpreted as the relation be-
tween entities. The processed sequence obtains the
following format:
<s>and</s> are the same as for monolingual
triples. [S-LAN ]and[O-LAN ]denote two place-
holders for language tokens, where they get re-
placed by the two-character ISO 639-1 codes of
the source and target language, respectively. For
example, if the cross-lingual connects an English
entity London to a Spanish entity Londres , the two
language tokens [EN][ES]will be appended to the
token [P]. The new special tokens are randomly
initialized, and optimized during training. The orig-
inal special tokens are kept and also optimized.
Training Objective. The main training objective
ofPrix-LM is to perform completion of both mono-
lingual knowledge triples and cross-lingual entity
links (see ¬ß2). In particular, given XandX, the
model must predict 1) Xfrom monolingual triples
(i.e., Xis a proper relation), or Xas the cross-
lingual counterpart of Xfor cross-lingual pairs
(i.e., Xis a pair of language tokens). This task
can be formulated into an autoregressive language
modeling training objective:
L= Xlog P (xjx);
where P(xjx)is the conditional probability ofgenerating xgiven previous subtokens. The proba-
bility of generating token xis calculated from the
hidden state of its previous token hin the Ô¨Ånal
layer of Transformer as follows:
P(xjx)=softmax( Wh);
whereWis a trainable parameter initialized from
PLMs for subtoken prediction. Note that this train-
ing objective is applied to both monolingual knowl-
edge triples and cross-lingual links as they can both
be encoded in the same fs;p;ogformat.
Since models like mBERT or XLM-R rely on
masked language modeling which also looks ‚Äòinto
the future‚Äô, subtokens can be leaked by attention.
Therefore, we create adaptations to support causal
autoregressive training using attention masks (Yang
et al., 2019), so that the Xsubtokens can only
access their previous subtokens. In particular, in
the Transformer blocks, given the query Q, keyK,
and value V, we adapt them to a causal LM:
whereQ;K;V2R;lis the length of the input
sequence, dis the hidden size, M2Ris an
attention mask, which is set as follows:
M=8>>>>><>>>>>:0 x<X[f[EOS]g
0 x2X[f[EOS]g;ji
 1 x2X[f[EOS]g;j>i
2.2 Inference
Dierent downstream tasks might require di erent
types of inference: e.g., while link prediction tasks
should rely on autoregressive inference, similarity-
based tasks such as cross-lingual entity linking rely
on similarity-based inference, that is, Ô¨Ånding near-
est neighbors in the multilingual space. In what
follows, we outline both inference types.
Autoregressive Inference. For link prediction
tasks test input is in the format of fs;p;?g, where
the model is supposed to generate the missing o
given sand p. For such tasks, ocomes from a
known set of candidate entities O. A simple way
to perform inference is to construct candidate tu-
plesfs;p;ogusing each o2 O and return the
one with the minimum LM loss. This straightfor-
ward approach requires encoding jOjsequences.
However, asjOjcan be large for high-resource lan-
guages (e.g., 2M items for English), this might5414yield a prohibitively expensive inference procedure.
We thus propose to speed up inference by applying
and adapting the constrained beam search (Ander-
son et al., 2017). In a nutshell, instead of calcu-
lating loss on the whole sequence, we generate
one subtoken at a time and only keep several most
promising sequences in the expansion set for beam
search. The generation process ends when we ex-
ceed the maximum length of entities.
More precisely, given sandp(or only swhen
dealing with cross-lingual links), we concatenate
them as the initial sequence Xand initialize the
sequence loss to 0. We then extend the sequence us-
ing subtokens from the PLM‚Äôs vocabulary V. For
each subtoken w2V, we create a new sequence
fX;wgand add logP(wjX)to the sequence
loss. For the next round, we only keep the se-
quences that can be expanded to an entity in the
expansion set, and retain at most Ksequences with
the smallest sequence loss, where Kis a hyperpa-
rameter. This process is repeated until there are
no more candidate sequences to be added to the
expansion set. Finally, for any candidate entity
o2O, if it has been generated from a correspond-
ing candidate sequence, we set its loss to the total
LM loss (sum of sequence losses), otherwise we
set its loss to1. Finally, we return the entity with
the smallest loss. A more formal description of this
procedure is summarized in Alg. 1 in the Appendix.
This inference variant only requires encoding at
most LKsequences, where Lis the maximum
number of subtokens in an entity. It is much more
ecient when LKjOj , which generally holds
for tasks such as link prediction.
Similarity-Based Inference. For some tasks it is
crucial to retrieve nearest neighbors (NN) via em-
bedding similarity in the multilingual space. Based
on prior Ô¨Åndings concerning multilingual PLMs
(Liu et al., 2021b) and our own preliminary ex-
periments, out-of-the-box Prix-LM produces en-
tity embeddings of insu cient quality. However,
we can transform them into entity encoders via
a simple and e cient unsupervised Mirror-BERT
procedure (Liu et al., 2021a). In short, Mirror-
BERT is a contrastive learning method that cali-
brates PLMs and converts them into strong univer-
sal lexical or sentence encoders. The NN search
is then performed with the transformed ‚ÄúMirror-
BERT‚Äù Prix-LM variant.3 Experiments and Results
In this section, we evaluate Prix-LM in both high-
resource and low-resource languages. The focus is
on four tasks that are directly or indirectly related
to KB construction. 1) Link prediction (LP) is
the core task for automatic KB construction since
it discovers missing links given incomplete KBs.
2) Knowledge probing from LMs (LM-KP) can
also be seen as a type of KB completion task as it
performs entity retrieval given a subject entity and
a relation. 3) Cross-lingual entity linking (XEL)
and 4) Bilingual lexicon induction (BLI) can be
very useful for multilingual KB construction as
they help to Ô¨Ånd cross-lingual entity links.
3.1 Experimental Setup
Training ConÔ¨Åguration. We train our model on
knowledge facts for 87 languages which are repre-
sented both in DBpedia and in XLM-R (Base). The
training set comprises 52M monolingual knowl-
edge triples and 142M cross-lingual links.
We implement our model using Huggingface‚Äôs
Transformers library (Wolf et al., 2020), and pri-
marily follow the optimization hyperparameters of
XLM-R.For LP we use the Ô¨Ånal checkpoint; for
LM-LP, results are reported using the checkpoint
at 20k steps; for BLI and XEL, the checkpoint at
150k steps is used. We discuss the rationales of
checkpoint selection in ¬ß3.6.
Inference ConÔ¨Åguration. For similarity-based in-
ference, as in previous work (Liu et al., 2021a) the
Mirror-BERT procedure relies on the 10k most fre-
quent English words for contrastive learning.For
constrained beam search, used with the LP task, we
set the hyperparameter Kto 50.
3.2 Link Prediction
(Short) Task Description. Following relevant
prior work (Bosselut et al., 2019; Yao et al., 2019),5415lang.! en it de fr fi et tr hu ja avg.
# entities (K) 2175 525 304 671 187 32 159 151 422 -
# triples (K) 7256 1543 618 1912 634 66 528 535 1159 -TransE 11.3 4.1 4.8 3.0 2.4 2.6 6.1 11.4 1.9 5.3
ComplEx 15.3 12.8 11.6 16.3 18.8 16.3 16.3 15.0 12.7 15.0
RotatE 19.7 17.3 17.5 23.0 19.8 21.5 26.2 29.8 15.8 21.2
Prix-LM (Single) 25.5 17.9 17.8 23.8 19.0 16.1 37.6 32.6 19.7 23.3
Prix-LM (All) 27.3 22.7 20.8 25.0 22.4 25.8 41.8 35.1 20.6 26.8TransE 28.0 25.0 24.0 27.2 26.0 20.0 31.0 36.1 20.6 26.4
ComplEx 22.3 22.2 20.7 24.0 30.1 24.8 26.9 29.0 22.9 24.8
RotatE 29.6 28.4 26.8 30.1 32.8 34.6 37.4 42.6 26.7 32.1
Prix-LM (Single) 34.1 27.7 24.8 29.6 27.6 25.6 46.1 44.1 29.4 32.1
Prix-LM (All) 35.6 32.2 29.7 32.4 31.8 36.7 49.8 47.5 29.4 36.1TransE 41.4 42.3 38.8 43.5 47.9 38.3 50.3 51.0 37.9 43.5
ComplEx 32.2 34.7 32.7 35.7 44.4 35.6 41.7 45.0 35.5 37.5
RotatE 39.1 42.2 40.0 44.9 47.7 46.4 52.3 55.2 40.0 45.3
Prix-LM (Single) 42.5 38.2 33.3 37.6 39.2 34.8 54.3 55.4 36.7 41.3
Prix-LM (All) 44.3 42.5 40.1 40.3 44.0 47.5 58.7 56.8 38.0 45.8
lang.! te lo mr avg.
XLM-R +Mirror 2.1 4.0 0.1 2.1
mBERT +Mirror 3.2 8.0 0.1 3.8
Prix-LM +Mirror 13.09 7.6 21.0 13.9
given a subject entity eand relation r, the aim of
the LP task is to determine the object entity e.
Task Setup. We evaluate all models on DBpe-
dia. We randomly sample 10% of the monolingual
triples as the test set for 9 languages and use re-
maining data to train the model.The data statistics
are reported in Tab. 1. The evaluation metrics are
standard Hits@1 ,Hits@3 , and Hits@10 .
Models in Comparison. We refer to our model
asPrix-LM (All) and compare it to the following
groups of baselines. First, we compare to three rep-resentative and widely used KG embedding mod-
els: 1) TransE (Bordes et al., 2013) interprets rela-
tions as translations from source to target entities,
2) ComplEx (Trouillon et al., 2016) uses complex-
valued embedding to handle binary relations, while
3) RotatE (Sun et al., 2019) interprets relations
as rotations from source to target entities in the
complex space. In fact, RotatE additionally uses a
self-adversarial sampling strategy in training, and
oers state-of-the-art performance on several KG
completion benchmarks (Rossi et al., 2021). Sec-
ond, Prix-LM (Single) is the ablated monolingual
version of Prix-LM , which uses an identical model
structure to Prix-LM (All), but is trained only on
monolingual knowledge triples of the test language.
Training adopts the same strategy from prior work
on pretraining monolingual LMs for KG comple-
tion (Bosselut et al., 2019; Yao et al., 2019). We
train the Prix-LM (Single) for the same number
of epochs as Prix-LM (All): this means that the
embeddings of subtokens in the test language are
updated for the same number of times.
Results and Discussion. The results in Tab. 15416lang.! en es de fi ru tr ko zh ja th avg.
XLM-R +Mirror 75.4 34.0 13.7 4.2 7.4 19.5 1.8 1.4 2.7 3.2 16.3
mBERT +Mirror 73.1 40.1 16.6 4.4 5.0 22.0 1.9 1.1 2.3 2.4 16.9
Prix-LM (Single) +Mirror 75.4 39.5 16.9 8.4 12.4 27.4 2.1 3.5 4.1 6.9 19.7
Prix-LM (All) +Mirror 71.9 49.2 25.7 15.2 24.5 34.1 9.3 6.9 13.7 14.5 26.5
lang.!
model#en-it en -tr en -ru en -fi fi -ru fi -tr
Acc MRR Acc MRR Acc MRR Acc MRR Acc MRR Acc MRR
XLM-R +Mirror 12.0 16.6 6.9 8.6 2.9 5.9 5.9 7.4 2.0 3.3 5.7 7.0
Prix-LM +Mirror 11.5 20.4 6.7 11.1 3.7 11.4 6.9 11.5 4.2 9.0 7.7 11.0
lang.! en it de fr fi et tr hu avg.
XLM-R 21.0 19.3 13.9 7.6 5.6 6.1 20.5 6.1 12.5
Prix-LM 23.8 21.8 20.7 17.8 16.1 7.423.9 13.1 18.1
show that the Prix-LM (All) achieves the best
Hits@1 on average, outperforming TransE, Com-
plEx, and RotatE by 21:5%,11:8%, and 5:6%, re-
spectively. It also outperforms the baselines on
Hits@3 andHits@10 . Moreover, Prix-LM (All)
outperforms in almost all languages its monolin-
gual counterpart Prix-LM (Single): the average
improvements are >3%across all metrics, demon-
strating that the model can e ectively leverage com-
plementary knowledge captured and transferred
through massive pretraining on multiple languages.
Interestingly, the advantages of Prix-LM (both Sin-
gle and All models) over baselines are not restricted
to low resource languages but are observed across
the board. This hints that, beyond integrating mul-
tilingual knowledge, Prix-LM is essentially a well-
suited framework for KB completion in general.
3.3 Cross-lingual Entity Linking
(Short) Task Description. In XEL, a model is
asked to link an entity mention in any language
to a corresponding entity in an English KB or in
a language-agnostic KB.XEL can contribute to
multilingual KB construction in two ways. First,since XEL links mentions extracted from free text
to KBs, it can be leveraged to enrich KBs with
textual attributes. Second, it also provides a way
to disambiguate knowledge with similar surface
forms but di erent grounded contexts.
Task Setup. We evaluate Prix-LM on two XEL
benchmarks: (i) the Low-resource XEL bench-
mark (LR-XEL; Zhou et al. 2020) and (ii) cross-
lingual biomedical entity linking (XL-BEL; Liu
et al. 2021b). LR-XEL covers three low-resource
languages te,lo, and mrwhere the model needs
to associate mentions in those languages to the
English Wikipedia pages. XL-BEL covers ten ty-
pologically diverse languages (see Tab. 3 for the
full list). It requires the model to link an entity
mention to entries in UMLS (Bodenreider, 2004),
a language-agnostic medical knowledge base.
Models in Comparison. For XEL and all follow-
ing tasks, we use multilingual MLMs (i.e. mBERT
and XLM-R) as our baselines as they are the canon-
ical models frequently used in prior work and have
shown promising results in cross-lingual entity-
centric tasks (Vuli ¬¥c et al., 2020; Liu et al., 2021b;
Kassner et al., 2021). We remind the reader that the
‚ÄòMirror-BERT‚Äô Ô¨Åne-tuning step is always applied,
yielding an increase in performance.
Results and Discussion. On LR-XEL, Prix-LM
achieves gains for all three languages over its base
model XLM-R. Especially on mr, where XLM-R
and mBERT are almost fully ine ective, Prix-LM5417leads to over 20% of absolute accuracy gain, again
showing the e ectiveness of incorporating multi-
lingual structural knowledge. On lo, mBERT is
slightly better than Prix-LM , but Prix-LM again
yields gains over its base model: XLM-R. On XL-
BEL, a large increase is again observed for almost
all target languages (see Prix-LM (All) +Mirror).
The only exception is English, where the model
performance drops by 3.5%. This is likely to be a
consequence of trading-o some of the extensive
English knowledge when learning on multilingual
triples. Beyond English, substantial improvements
are obtained in other Indo-European languages in-
cluding Spanish, German and Russian ( +10-20%),
stressing the necessity of knowledge injection even
for high-resource languages. Like LP, we also ex-
perimented with Prix-LM trained with only mono-
lingual data (see Prix-LM (Single) +Mirror). Ex-
cept for English, very large boosts are obtained on
all other languages when comparing All and Single
models, conÔ¨Årming that multilingual training has
provided substantial complementary knowledge.
3.4 Bilingual Lexicon Induction
(Short) Task Description. BLI aims to Ô¨Ånd a
counterpart word or phrase in a target language.
Similar to XEL, BLI can also evaluate how well a
model can align a cross-lingual (entity) space.
Task Setup. We adopt the standard supervised em-
bedding alignment setting (Glava≈° et al., 2019) of
VecMap (Artetxe et al., 2018) with 5k translation
pairs reserved for training (i.e., for learning lin-
ear alignment maps) and additional 2k pairs for
testing. The similarity metric is the standard cross-
domain similarity local scaling (CSLS; Lample
et al. 2018).We experiment with six language
pairs and report accuracy (i.e., Hits@1 ) and mean
reciprocal rank (MRR).
Results and Discussion. The results are provided
in Tab. 4. There are accuracy gains observed on
4/6 language pairs, while MRR improves for all
pairs. These Ô¨Åndings further conÔ¨Årm that Prix-LM
in general learns better entity representations and
improved cross-lingual entity space alignments.
3.5 Prompt-based Knowledge Probing
(Short) Task Description. LM-KP (Petroni et al.,
2019) queries a PLM with (typically human-designed) prompts /templates such as Dante was
born in .(the answer should be Florence ). It can
be viewed as a type of KB completion since the
queries and answers are converted from /into KB
triples: in this case, {D ante,born-in , Florence }.
Task Setup. We probe how much knowledge a
PLM contains in multiple languages relying on the
multilingual LAnguage Model Analysis (mLAMA)
benchmark (Kassner et al., 2021). To ensure a
strictly fair comparison, we only compare XLM-R
andPrix-LM . We exclude multi-token answers as
they require multi-token decoding modules, which
will be di erent for causal LMs like Prix-LM ver-
sus MLMs such as XLM-R. For both Prix-LM and
XLM-R, we take the word with highest probabil-
ity at the [Mask]token as the model‚Äôs prediction.
Punctuation, stop words, and incomplete Word-
Pieces are Ô¨Åltered out from the vocabulary during
prediction.
Results and Discussion. Tab. 5 indicates that
Prix-LM achieves better performance than XLM-R
on mLAMA across all languages. We suspect that
the beneÔ¨Åts of Prix-LM training are twofold. First,
multilingual knowledge is captured in the uniÔ¨Åed
LM representation, which improves LM-KP as a
knowledge-intensive task. The e ect of this is par-
ticularly pronounced on low-resource languages
such as fi,etand hu, showing that transferring
knowledge from other languages is e ective. Sec-
ond, the Prix-LM training on knowledge triples
is essentially an adaptive Ô¨Åne-tuning step (Ruder,
2021) that exposes knowledge from the existing
PLMs‚Äô weights. We will discuss this conjecture,
among other analyses, in what follows.
3.6 Additional Analysis
Inconsistency of the Optimal Checkpoint
across Tasks (Fig. 2). How many steps should we
pretrain Prix-LM on knowledge triples? The plots
in Fig. 2 reveal that the trend is di erent on tasks
that require language understanding (mLAMA) ver-
sus tasks that require only entity representations
(LP and XL-BEL). On mLAMA, Prix-LM ‚Äôs perfor-
mance increases initially and outperforms the base
model (XLM-R, at step 0). However, after around
20k steps it starts to deteriorate. We speculate5418
that this might occur due to catastrophic forgetting,
as mLAMA requires NLU capability to process
queries formatted as natural language. Training on
knowledge triples may expose the PLMs‚Äô capabil-
ity of generating knowledge at the earlier training
stages: this explains the steep increase from 0-20k
iterations. However, training on knowledge triples
for (too) long degrades the model‚Äôs language un-
derstanding capability. On the other hand, longer
training seems almost always beneÔ¨Åcial for LP and
XL-BEL: these tasks require only high-quality en-
tity embeddings instead of understanding complete
sentences. A nuanced di erence between LP and
XL-BEL is that Prix-LM ‚Äôs performance on XL-
BEL saturates after 100k-150k steps, while on LP
theHits@1 score still increases at 200k steps.
Link Prediction on Unseen Entities (Tab. 6).
KG embedding models such as RotatE require that
entities in inference must be seen in training. How-
ever, the Prix-LM is able to derive (non-random)
representations also for unseen entities. We evalu-
ate this ability of Prix-LM on triples (s;r;o)where
the subject entity sor object entity ois unseen dur-
ing training. The results indicate that Prix-LM can
generalize well also to unseen entities.
4 Related Work
Injecting Structured Knowledge into LMs.
Conceptually, our work is most related to recent
work on knowledge injection into PLMs. Know-
BERT (Peters et al., 2019) connects entities in
text and KGs via an entity linker and then re-contextualizes BERT representations conditioned
on the KG embeddings. KG-BERT (Yao et al.,
2019) trains BERT directly on knowledge triples
by linearizing their entities and relations into a se-
quence and predicting plausibility of the sequence.
Wang et al. (2021a) improve KG-BERT by split-
ting a subject-relation-object knowledge triple into
a subject-relation pair representation and an object
entity representation, then modeling their similar-
ities with a dual /Siamese neural network. Other
work on knowledge injection such as K-BERT
(Liu et al., 2020a) and ERNIE (Zhang et al., 2019)
mainly aims to leverage external knowledge to im-
prove on downstream NLU tasks instead of per-
forming KG completion.
While prior studies have focused on incorporat-
ing monolingual (English) structured knowledge
into PLMs, our work focuses on connecting knowl-
edge in many languages, allowing knowledge in
each language to be transferred and collectively
enriched.
Multilingual LMs pretrained via MLM, such as
mBERT (Devlin et al., 2019) and XLM-R (Con-
neau et al., 2020), cover 100 +languages and are
the starting point (i.e. initialization) of Prix-LM .
With the notable exception of Calixto et al. (2021)
who rely on the prediction of Wikipedia hyperlinks
as an auxiliary /intermediate task to improve XLM-
R‚Äôs multilingual representation space for cross-
lingual transfer, there has not been any work on aug-
menting multilingual PLMs with structured knowl-
edge. Previous work has indicated that o -the-shelf
mBERT and XLM-R fail on knowledge-intensive
multilingual NLP tasks such as entity linking and
KG completion, and especially so for low-resource
languages (Liu et al., 2021b). These are the crucial
challenges addressed in this work.
KB Completion and Construction. Before
PLMs, rule-based systems and multi-staged infor-
mation extraction pipelines were typically used
for automatic KB construction (Auer et al., 2007;
Fabian et al., 2007; Ho art et al., 2013; Dong et al.,
2014). However, such methods require expensive
human e ort for rule or feature creation (Carlson
et al., 2010; Vrande Àáci¬¥c and Kr√∂tzsch, 2014), or
they rely on (semi-)structured corpora with easy-to-5419lang.! en it de fr fi et tr hu ja avg.
Hits@1 17.2 22.9 17.0 16.0 18.3 31.3 19.2 28.5 12.4 20.3
Hits@3 24.7 30.1 24.0 22.3 23.5 37.7 24.7 38.5 19.0 27.1
Hits@10 31.0 34.9 28.9 27.8 31.9 42.3 30.8 44.2 23.6 32.8
consume formats (Lehmann et al., 2015). Petroni
et al. (2019) showed that modern PLMs such as
BERT could also be used as KBs: querying PLMs
with Ô¨Åll-in-the-blank-style queries, a substantial
amount of factual knowledge can be extracted. This
in turn provides an e cient way to address the
challenges of traditional KB methods. Jiang et al.
(2020) and Kassner et al. (2021) extended the idea
to extracting knowledge from multilingual PLMs.
Work in monolingual settings closest to ours is
COMET (Bosselut et al., 2019): Prix-LM can be
seen as an extension of this idea to multilingual and
cross-lingual setups. Prix-LM ‚Äôs crucial property is
that it enables knowledge population by transfer-
ring complementary structured knowledge across
languages. This can substantially enrich (limited)
prior knowledge also in monolingual KBs.
In another line of work, multilingual KG embed-
dings (Chen et al., 2017, 2021; Sun et al., 2020a,
2021) were developed to support cross-KG knowl-
edge alignment and link prediction. Such methods
produce a uniÔ¨Åed embedding space that allows link
prediction in a target KG based on the aligned prior
knowledge in other KGs (Chen et al., 2020). Re-
search on multilingual KG embeddings has made
rapid progress recently, e.g., see the survey of Sun
et al. (2020b). However, these methods focus on
a closed-world scenario and are unable to lever-
age open-world knowledge from natural language
texts. Prix-LM combines the best of both worlds
and is able to capture and combine knowledge from
(multilingual) KGs and multilingual texts.
5 Conclusion
We have proposed Prix-LM , a uniÔ¨Åed multilingual
representation model that can capture, propagate
and enrich knowledge in and from multilingual
KBs. Prix-LM is trained via a casual LM objec-
tive, utilizing monolingual knowledge triples and
cross-lingual links. It embeds knowledge from the
KB in di erent languages into a shared representa-
tion space, which beneÔ¨Åts transferring complemen-
tary knowledge between languages. We have runcomprehensive experiments on 4 tasks relevant to
KB construction, and 17 diverse languages, with
performance gains that demonstrate the e ective-
ness and robustness of Prix-LM for automatic KB
construction in multilingual setups. The code and
the pretrained models will be available online at:
https://github.com/luka-group/prix-lm .
Acknowledgement
We appreciate the reviewers for their insightful
comments and suggestions. Wenxuan Zhou and
Muhao Chen are supported by the National Science
Foundation of United States Grant IIS 2105329,
and partly by Air Force Research Laboratory under
agreement number FA8750-20-2-10002. Fangyu
Liu is supported by Grace & Thomas C.H. Chan
Cambridge Scholarship. Ivan Vuli ¬¥c is supported
by the ERC PoC Grant MultiConvAI (no. 957356)
and a Huawei research donation to the University
of Cambridge.
References5420542154225423Algorithm 1: Constrained Beam Search
Input: Subject entity s, relation p, set of
object entitiesO, maximum entity
length L, size of expansion set K,
PLM vocabulary set V.
Output: Predicted entity.
Create the initial sequence Xby
concatenating sandp.
Create a set of sequences X=;.
X=f(X;0)g.
fort=1;:::;Ldo
X=;.
forX;l2Xdo
forw2V do
Add fX;wg;l log P( wjX)to
XandX.
Remove the sequences in Xthat cannot
expand to entities in O.
Keep at most Ksequences inXwith
the smallest loss.For object entities that appear in X, return
the one with the smallest loss.
A Language Codes
en English
es Spanish
it Italian
de German
fr French
fi Finnish
et Estonian
hu Hungarian
ru Russian
tr Turkish
ko Korean
ja Japanese
zh Chinese
th Thai
te Telugu
lo Lao
mr Marathi
B Constrained Beam Search Algorithm
The detailed algorithm of constrained beam search
is described in Alg. 1.5424