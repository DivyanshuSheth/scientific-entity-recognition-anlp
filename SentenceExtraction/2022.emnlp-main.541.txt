
Ahmet Üstün, Arianna Bisazza, Gosse Bouma,
Gertjan van Noord, Sebastian RuderUniversity of GroningenGoogle Research
a.ustun@rug.nl
Abstract
Massively multilingual models are promising
for transfer learning across tasks and languages.
However, existing methods are unable to fully
leverage training data when it is available in dif-
ferent task-language combinations. To exploit
such heterogeneous supervision, we propose
Hyper-X , a single hypernetwork that unifies
multi-task and multilingual learning with effi-
cient adaptation. This model generates weights
for adapter modules conditioned on both tasks
and language embeddings. By learning to
combine task and language-specific knowledge,
our model enables zero-shot transfer for un-
seen languages and task-language combina-
tions. Our experiments on a diverse set of lan-
guages demonstrate that Hyper-X achieves the
best or competitive gain when a mixture of mul-
tiple resources is available, while being on par
with strong baselines in the standard scenario.
Hyper-X is also considerably more efficient in
terms of parameters and resources compared to
methods that train separate adapters. Finally,
Hyper-X consistently produces strong results in
few-shot scenarios for new languages, showing
the versatility of our approach beyond zero-shot
transfer.
1 Introduction
Transfer learning across languages and tasks has
long been an important focus in NLP (Ruder et al.,
2019). Recent advances in massively multilingual
transformers (MMTs; Devlin et al., 2019; Conneau
et al., 2020) show great success in this area. A
benefit of such models is their ability to transfer
task-specific information in a high-resource source
language to a low-resource target language (Fig-
ure 1, 1). Alternatively, such models can lever-
age knowledge from multiple tasks for potentially
stronger generalization (Figure 1, 2).
Over time, many research communities have
been developing resources for specific languagesFigure 1: Experimental settings of different (zero-shot)
cross-lingual transfer scenarios. Single-task (1) is the
standard setting; multi-task (2) enables cross-task trans-
fer. Mixed-language multi-task (3) additionally allows
leveraging task data from multiple source languages for
different tasks.
of focus (Strassel and Tracey, 2016; Nivre et al.,
2018; Wilie et al., 2020). In practice, it is thus
common for data to be available for different tasks
in a mixture of different languages. For instance,
in addition to English data for both POS tagging
and Named Entity Recognition (NER), a treebank
with POS annotation may be available for Turkish,
while NER data may be available for Arabic. This
example is illustrated in Figure 1, 3.
In contrast to existing cross-lingual transfer
paradigms such as single-task zero-shot transfer
(Hu et al., 2020) or few-shot learning (Lauscher
et al., 2020a), multi-task learning on such a mix-
ture of datasets (mixed-language multi-task) poses
an opportunity to leverage all available data and7934
to transfer information across both tasks and lan-
guages to unseen task–language combinations
(Ponti et al., 2021).
Standard fine-tuning strategies, however, are lim-
ited in their ability to leverage such heterogeneous
task and language data. Specifically, MMTs are
prone to suffer from catastrophic forgetting and
interference (Wang et al., 2020) when they are fine-
tuned on multiple sources. Adapters (Houlsby et al.,
2019), a parameter-efficient fine-tuning alternative
are commonly used for transfer either across tasks
(Mahabadi et al., 2021b) or languages (Üstün et al.,
2020) but require training a new adapter for each
new language (Pfeiffer et al., 2020b).
In this paper, we propose a unified hypernet-
work, H -Xthat is particularly suited to this
setting by leveraging multiple sources of informa-
tion including different languages and tasks within
a single model. The core idea consists of taking
language and task embeddings as input, and gen-
erating adapter parameters via a hypernetwork for
the corresponding task-language combination. By
parameterizing each task and language separately,
Hyper-X enables adaptation to unseen combina-
tions at test time while exploiting all available data
resources.
Additionally, Hyper-X can make seamless use of
masked language modelling (MLM) on unlabelled
data, which enables it to perform zero-shot adapta-
tion to languages not covered by the MMT during
pre-training. MLM also enables Hyper-X to learn
a language representation even without available
task-specific data.
In sum, our work brings together a number of
successful transfer ‘ingredients’ that have been
explored in very recent literature (see Table 1),
namely multi-task learning, multilingual learn-ing, further pre-training, along a high degree of
compute- and time-efficiency.
We evaluate Hyper-X for cross-lingual transfer
on two sequence labelling tasks, namely part-of-
speech (POS) tagging and named-entity recogni-
tion (NER) in 16 languages—7 of which are not
covered in pre-training—across the three experi-
mental setups depicted in Figure 1. Our experi-
ments demonstrate that Hyper-X is on par with
strong baselines for cross-lingual transfer from En-
glish. In the multi-task and mixed-language set-
tings, Hyper-X shows a large improvement com-
pared to the standard baselines and matches the per-
formance of the less efficient adapter-based model
due to its ability to leverage heterogeneous sources
of supervision. Analysis highlights that Hyper-X is
superior in terms of efficiency–performance trade-
offs. Finally, we evaluate our model in a few-shot
setting, where Hyper-X consistently achieves com-
petitive performance across different languages and
tasks, which suggests the usability of our approach
in continuous learning scenarios.
2 Background
2.1 Adapters
Adapters (Rebuffi et al., 2018) are light-weight bot-
tleneck layers inserted into a MMT to fine-tune
the model for a new task (Houlsby et al., 2019),
language (Pfeiffer et al., 2020b) or domain (Bapna
and Firat, 2019). The pre-trained weights of the
transformer remain fixed and only adapter parame-
ters are updated. This setup prevents catastrophic
forgetting (McCloskey and Cohen, 1989) by encap-
sulating specialized knowledge.
Formally, an adapter module Aat layer icon-
sists of a down-projection D∈Rof the in-7935
putz∈Rwith the bottleneck dimension b, a
non-linear function (ReLU) and an up-projection
U∈R:
A(z) =U.ReLU (D.z) +z (1)
where this feed-forward network is followed by a
residual link connecting to the input z.
2.2 Hypernetworks
A hypernetwork is a network that generates the
weights for a larger main network (Ha et al., 2016).
When using a hypernetwork, the main model learns
the desired objective (e.g. classification) whereas
the hypernetwork takes an auxiliary input (usu-
ally an embedding) that represents the structure of
the weights and generates parameters of the main
model. A hypernetwork thus enables learning a sin-
gle parameter space shared across multiple transfer
dimensions such as tasks (Mahabadi et al., 2021b)
or languages (Platanios et al., 2018) while also al-
lowing input-specific reparametrization.
More concretely, a hypernetwork is a generator
function Hthat takes an embedding s∈R
representing the input sources, and generates the
model parameters Θ:
Θ≜H(s) (2)While Hcan be any differentiable function, it is
commonly parameterized as a simple linear trans-
form ( W) that generates a flat vector with the
dimension of d, which corresponds to the total
number of model parameters. Wis shared across
all input sources, enabling maximum sharing.
3 Hyper-X
We propose, Hyper-X, an efficient adaptation of
a MMT by exploiting multiple sources of infor-
mation for transfer to an unseen language or task-
language pairs. Specifically, Hyper-X learns to
combine task and language-specific knowledge in
the form of embeddings using a hypernetwork.
Conditioned on the task and language embeddings,
the hypernetwork generates composite adapter lay-
ers for the corresponding task-language combina-
tion (e.g. NER in Turkish), thereby enabling trans-
fer to arbitrary task-language pairs at test time. Fig-
ure 2 provides an overview of our model.
By jointly learning from task and language in-
formation, Hyper-X overcomes some of the lim-
itations of prior work: Unlike adapter-based ap-
proaches (Pfeiffer et al., 2020b; Üstün et al., 2020)
that transfer cross-lingual information only to the
task of the task adapter, our model is capable
of leveraging supervision—and positive transfer—
from both multiple tasks and languages. Moreover,
unlike Ponti et al. (2021) who require annotated
data in one of the target tasks for each language,
Hyper-X is able to perform zero-shot transfer even
when there is no annotated data from any of the
target tasks, by using MLM as an auxiliary task for
each language.
3.1 A Hypernetwork for Task-Language
Adapters
We use a standard hypernetwork as the parameter
generator function. However, instead of generat-
ing the full model parameters, our hypernetwork
generates the parameters for each adapter layer.
Concretely, the hypernetwork Hgenerates adapter
parameters where each adapter layer Aconsists of
down and up-projection matrices ( D,U):
D,U≜H(s) (3)
Decoupling Tasks and Languages In Hyper-X,
we condition the parameter generation on the in-
put task and language. Therefore, given a com-
bination of task t∈ {t, ..., t}and language
l∈ {l, ..., l}, the source embedding contains7936knowledge from both sources: s≈(t, l). We
parameterize each task and language via separate
embeddings, which enables adaptation to any task-
language combination. Task and language embed-
dings ( s,s) are low-dimensional vectors that
are learned together with the parameters of the hy-
pernetwork. During training, for each mini-batch
we update these embeddings according to the task
and language that the mini-batch is sampled from.
MLM as Auxiliary Task Hyper-X learns sepa-
rate tasks and languages embeddings—as long as
the task and language have been seen during train-
ing. As annotated data in many under-represented
languages is limited, we employ MLM as an auxil-
iary task during training to enable computing em-
beddings for every language. Moreover, MLM en-
ables a better zero-shot performance for languages
that are not included in MMT pre-training (see
§ 6.2 for a detailed analysis of the impact of MLM).
Sharing Across Layers In addition to the task
and language embedding, we learn a layer embed-
ding s(Mahabadi et al., 2021b; Ansell et al.,
2021) corresponding to the transformer layer index
iwhere the respective adapter module is plugged in.
Since Hyper-X generates an adapter for each Trans-
former layer, learning independent layer embed-
dings allows for information sharing across those
layers. Moreover, as layer embeddings allow the
use of a single hypernetwork for all Transformer
layers, they reduce the trainable parameters, i.e.,
size of the hypernetwork, by a factor corresponding
to the number of layers of the main model.
Combining Multiple Sources To combine lan-
guage, task and layer embeddings, we use a simple
source projector network Pas part of our hypernet-
work. This module consisting of two feed-forward
layers with a ReLU activation takes the concatena-
tion of the three embeddings and learns a combined
embedding s∈Rwith a potentially smaller
dimension:
s=s⊕s⊕s(4)
s=P(s) (5)
where s∈Rrefers to the concatenated em-
bedding before the P, with d=d+d+d.
This component enables learning how to combine
source embeddings while also reducing the total
number of trainable parameters.4 Experiments
Dataset and Languages We conduct experi-
ments on two downstream tasks: part-of-speech
(POS) tagging and named entity recognition (NER).
For POS tagging, we use the Universal Dependen-
cies (UD) 2.7 dataset (Zeman et al., 2020) and for
NER, we use WikiANN (Pan et al., 2017) with the
train, dev and test splits from Rahimi et al. (2019).
In addition to these two tasks, we also use masked
language modelling (MLM) on Wikipedia articles
as an auxiliary task. We limit the number of sen-
tences from Wikipedia to 100K for each language,
in order to control the impact of dataset size and to
reduce the training time.
For the language selection, we consider: (i) typo-
logical diversity based on language family, script
and morphosyntactic attributes; (ii) a combination
of high-resource and low-resource languages based
on available data in downstream task; (iii) pres-
ence in the pre-training data of mBERT; and (iv)
presence of a language in the two task-specific
datasets.We provide the details of the language
and dataset selection in Appendix A.
Experimental Setup We evaluate Hyper-X for
zero-shot transfer in three different settings: (1)
English single-task, where we train the models
only on English data for each downstream task
separately. (2) English multi-task, where the mod-
els are trained on English POS and NER data at
the same time. (3) Mixed-language multi-task,
where we train the models in a multi-task setup,
but instead of using only English data for both
POS and NER, we use a mixture of task-language
combinations. In order to measure zero-shot per-
formance in this setup, following Ponti et al. (2021)
we create two different partitions from all possible
language-task combinations in such a way that a
task-language pair is always unseen for one of the
partitions (e.g. NER-Turkish and POS-Arabic in
Figure 1). Details of partitions and our partitioning
strategy are given in Appendix A.
4.1 Baselines and Model Variants
mBERT (Devlin et al., 2019) is a MMT that is
pre-trained for 104 languages. We use mBERT
by fine-tuning all the model parameters on the7937available sources. As this standard approach
enables cross-lingual transfer from both a single
source or a set of language-task combinations,
we compare it to Hyper-X in all three settings.
Moreover, we use mBERT as the base model for
both Hyper-X and the other baselines.
MAD-X (Pfeiffer et al., 2020b) is an adapter-based
modular framework for cross-lingual transfer learn-
ing based on MMTs. It combines a task-specific
adapter with language-specific adapters that are
independently trained for each language using
MLM. We train MAD-X language adapters on the
same Wikipedia data that is used for Hyper-X, for
all languages with a default architecture.Finally,
for the mixed-language setup, as the original
MAD-X does not allow standard multi-task
training, we train the task adapters by using
multiple source languages but for NER and POS
separately. We call this model MAD-X MS .
Parameter Space Factorization (Ponti et al.,
2021) is a Bayesian framework that learns a
parameter generator from multiple tasks and
languages for the softmax layer on top of a
MMT. However, if a language lacks annotated
training data, this model cannot learn the required
latent variable for the corresponding language.
Therefore, we evaluate this baseline only for
the mixed-language multi-task setting using the
same partitions as Hyper-X. We use the original
implementation with default hyper-parameters and
low-rank factorization.
Model Variants We evaluated two variants of
Hyper-X in order to see the impact of Hypernet-
work size: Hyper-X Base model fine-tunes 76m
parameters ( d= 192 ), compatible with MAD-X
in terms of total number of trainable parameters,
and Hyper-X Small updates only 13m parameters
(d= 32 ). Table 3 shows the parameter counts
together with the corresponding runtime.
4.2 Training Details
For all the experiments, we used a batch size of
32 and a maximum sequence length of 256. We
trained Hyper-X for 100,000 updates steps by us-ing a linearly decreasing learning rate of 1e-4 with
4000 warm-up steps. We evaluated checkpoints ev-
ery 5,000 steps, and used the best checkpoint w.r.t.
the average validation score for testing. As for
baselines, we trained mBERT and MAD-X tasks
adapters for 20 epochs by using learning rate of
1e-5 and 1e-4 respectively with the same scheduler
and warm-up steps. Since MAD-X requires pre-
requisite language adapters, we trained language
adapters for 100,000 steps for each language sepa-
rately.
In terms of model size, we use a bottleneck
dimension of 256 to learn adapters for Hyper-X.
Similarly, we train language and adapters with di-
mension of 256 and 48 for MAD-X to create a
comparable baseline. In Hyper-X, as input to the
hypernetwork, dimensions for task, language and
layer embeddings are all set to 64 (total 192). Dur-
ing training, we create homogeneous mini-batches
for each task-language combination to learn the
corresponding embeddings together with the hy-
pernetwork. Moreover, following Mahabadi et al.
(2021b), we also update the original layer-norm
parameters. During multi-task training, we use
temperature-based sampling with T= 5 to bal-
ance each task-language pair during training (See
Appendix § B.1 for details).
5 Zero-shot Transfer Results
Table 2 shows the aggregate zero-shot results in
NER and POS tagging respectively. In addition
to the average scores across all 15 zero-shot lan-
guages, we show the average of the 8 ‘seen’ and 7
‘unseen’ languages separately with respect to lan-
guage coverage of mBERT. We present results for
English single-task, English multi-task and Mixed-
language multi-task settings.
Overall, Hyper-X Base performs on par with the
strongest baseline when transferring from English.
In the presence of additional sources, such as a mix-
ture of task-language pairs, Hyper-X outperforms
both mBERT and parameter space factorization
(PSF). In comparison to MAD-X, Hyper-X gener-
ally performs better on seen languages. We relate
this to the unified hypernetwork enabling maxi-
mum sharing between languages and higher utiliza-
tion of the pre-trained capacity in contrast to the
isolated adapters. On unseen languages, Hyper-X
is outperformed by MAD-X in most cases. How-
ever, we emphasize that MAD-X requires training
separate language adapters for each new language,7938
which makes it considerably less resource-efficient
than Hyper-X (see § 6.1).
English Single-Task When English is used as
the only source language for each task separately,
Hyper-X (Base) performs on par with MAD-X for
NER (52.7 vs 52.8 F1) but falls behind for POS
tagging (63.5 vs 65.4 Acc.) on average. Both mod-
els significantly outperform mBERT. Looking at
the individual language results, Hyper-X performs
slightly better on ‘seen’ languages compared to
MAD-X in NER and POS tagging respectively.
For ‘unseen’ languages, both MAD-X and Hyper-X
benefit from MLM, which results in large improve-
ments with respect to mBERT. Between the two
models, MAD-X achieves a higher average score
in both NER and POS tagging.
English Multi-Task In a multi-task setting where
only English data is available, fine-tuning mBERT
for both target tasks at the same time gives mixed
results compared to single-task training—in line
with previous findings noting catastrophic forget-
ting and interference in MMTs (Wang et al., 2020).
Hyper-X Base, on the other hand, shows a small
but consistent improvement on the majority of lan-
guages, with 0.2 (F1) and 0.1 (Acc.) average in-
crease in NER and POS tagging respectively. This
confirms that Hyper-X is able to mitigate inter-
ference while allowing for sharing between taskswhen enough capacity is provided.
Mixed-Language Multi-Task In this setting, a
mixture of language data is provided for NER
and POS via two separate training partitions while
keeping each task-language pair unseen in one of
these partitions. All the models including mBERT
achieve better zero-shot scores compared to the
previous settings. Among the baselines, parameter
space factorization (PSF) gives a larger improve-
ment compared to mBERT on both tasks, indicat-
ing the importance of task- and language-specific
parametrization for adapting a MMT. Hyper-X
Base produces the largest performance gain among
the models that trains only a single model: it
achieves 9.0 (F1) and 4.3 (Acc.) average increase
for NER and POS. Although both PSF and Hyper-
X enable adaptation conditioned on a mixture of
task and language combinations, we relate the dif-
ference between PSF and Hyper-X to the contrast
in parameter generation. PSF only generates pa-
rameters of the softmax layer and is thus unable
to adapt deeper layers of the model. Hyper-X, on
the other hand, generates adapter layer parameters
inserted throughout the model, which provide a
higher degree of adaptation flexibility. Hyper-X
outperforms PSF particularly on unseen languages
as it benefits from MLM as an auxiliary task.7939
Finally, Hyper-X tends to perform slightly better
on seen languages compared to the adapted multi-
source version of MAD-X. However, MAD-X out-
performs Hyper-X on unseen languages by 1.2 (F1)
and 2.8 (Acc.) for NER and POS respectively. Be-
sides the expected benefits of independently trained
language adapters in MAD-X, we relate this to the
limited cross-task supervision for unseen languages
in Hyper-X for this setting. Especially, when the
target task is POS, most of the unseen languages
have only 100 sentences available in NER dataset,
which leaves only a little margin for improvements.
6 Analysis
6.1 Parameter and Time Efficiency
Table 3 shows the fine-tuned parameter counts
and the training time required for the baselines
and Hyper-X models. Unlike mBERT, PSF and
Hyper-X, MAD-X consists of 16 and 2 indepen-
dently trained language and task adapters respec-
tively. In terms of parameter efficiency, MAD-X
and Hyper-X Base models correspond to 43% of
mBERT’s parameters. However, in terms of train-
ing time, Hyper-X Base is trained only once for
about 18 hours, as opposed to MAD-X’s consid-
erably high total training time (116 hours in to-
tal). Thus, considering the competitive zero-shot
performances across different languages and set-
tings, Hyper-X Base provides a better efficiency-
performance trade-off. Furthermore, in the case of
adding more languages, MAD-X’s parameter count
and training time increase linearly with the number
of new languages, while Hyper-X’s computational
cost remains the same.
As Hyper-X model variants, we evaluated
two different sizes of the source embedding
(d; 32→192). Although Hyper-X Small is much
more parameter-efficient (7.2% of mBERT’s pa-
rameters) and takes slightly less time to train (16h),
its zero-shot performance is significantly lower
than the base model, especially for unseen lan-
guages. Nevertheless, Hyper-X Small remains a
valid alternative for particularly ‘seen’ languages.
6.2 Impact of Auxiliary MLM Training
Figure 3 demonstrates the impact of auxiliary
MLM training in Hyper-X Base for the mixed-
language multi-task setting. As this setting pro-
vides training instances for each task and language,
we evaluated the impact of MLM by removing
the corresponding Wikipedia data first for ‘seen’
languages, then for ‘all’ languages. As shown in
the figure, although the availability of MLM data
slightly increases seen language performance, it
mainly boosts the scores in unseen languages: +6.2
F1 and +10.5 Acc. for NER and POS respectively.
Furthermore, when MLM data is removed for only
seen languages, Hyper-X can mostly recover perfor-
mance on seen languages, confirming the dominant
effect of MLM on unseen languages.
6.3 Impact of Source Languages
In the mixed-language multi-task setting, we delib-
erately avoid grouping languages from same fam-
ilies to different partitions, in order to restrict the
transfer from the same-language family instances,
and to observe the effect of cross-task supervision.
However, we also evaluate the impact of source
languages in this setup, to measure the degree of
potential positive transfer. To this end, we switched
the partitions of kk,mt,yue , so that all of them
will likely benefit from a high-resource language7940
from the same family for the same target task. Fig-
ure 4 and 5 shows the aggregated results in both
Hyper-X Base and mBERT. Firstly, both models
benefit from positive transfer. Secondly, although
the relative increase in mBERT is slightly higher
Hyper-X still outperforms mBERT with a large
margin, showing the robustness of our model with
regard to different partitions.
6.4 Few-shot Transfer
Fine-tuning an MMT with a few target instances
has been shown to increase zero-shot performances
(Lauscher et al., 2020b). Therefore, we evaluate
Hyper-X for few-shot transfer on 5 languages—3
of which are high-resource and covered by mBERT
and 2 are low-resource and unseen. To this end, we
further fine-tune Hyper-X and the corresponding
baselines that are trained initially in the English
multi-task by using 5, 10, 20, and 50 training in-
stances for each language separately on NER and
POS-tagging (see details in Appendix §D).
Figure 6 presents the average results compar-
ing mBERT to MAD-X. Similar to the zero-shot
results, on seen languages, Hyper-X constantly pro-
vides better adaptation than both baselines for NER
and POS. On unseen languages, MAD-X gives the
best result on average. This is because MAD-X
starts with better initial representations for Maltese
and Uyghur. When more samples are provided
Hyper-X reduces the initial gap. Overall, Hyper-X
consistently achieves the best or competitive perfor-
mance on the majority of the experiments, except
‘unseen’ languages for POS tagging, showing the
effectiveness of our approach beyond the standard
zero-shot transfer. Taken together with the parame-
ter and training efficiency, these results show that
Hyper-X can be easily extended to new languages
without incurring large computing costs.
7 Related Work
Adapters As a parameter-efficient alternative to
standard fine-tuning, adapters have been used for
quick training (Rücklé et al., 2021), multi-task
learning (Stickland and Murray, 2019) and knowl-
edge composition (Pfeiffer et al., 2021a; Wang
et al., 2021; Poth et al., 2021). Moreover, Ma-
habadi et al. (2021a) and He et al. (2022a) ex-
tended adapters for better performance with fewer
parameters. In the context of multilingual transfer,
adapters enable allocation of additional language-
specific capacity, thereby mitigating the ‘curse of
multilinguality’ (Üstün et al., 2020). Such lan-
guage adapters (Pfeiffer et al., 2020b; Ansell et al.,
2021) achieve high zero-shot results when com-
bined with task adapters and enable generalization
to languages unseen during pre-training via MLM-
based adaptation (Pfeiffer et al., 2021b). Philip et al.
(2020) and Üstün et al. (2021) also used monolin-
gual adapters for zero-shot and unsupervised NMT.
Hypernetworks in NLP Tay et al. (2021) pro-
pose a multi-task model that uses a hypernet-
work to condition on input to learn task-specific
reparametrizations. Similarly, Mahabadi et al.
(2021b) generate task-specific adapters via a hy-
pernetwork. Recently, He et al. (2022b) use a
hypernetwork to generate prompts. For multilin-
gual learning, where the input sources correspond
to language embeddings, Üstün et al. (2020) and
Ansell et al. (2021) learn these embeddings from
the typological feature vectors of languages, en-
abling generalization to unseen languages based
on a hypernetwork. In a similar spirit to our work,
parameter space factorization (PSF; Ponti et al.,
2021), learns task and language-specific embed-
dings from seen task-language combinations. How-
ever, unlike our model, these embeddings are used7941
for task/language-specific parametrization in the
softmax layer.
8 Conclusion
We have proposed Hyper-X, a novel approach for
multi-task multilingual transfer learning, based
on a unified hypernetwork that leverages hetero-
geneous sources of information, such as multi-
ple tasks and languages. By learning to generate
composite adapters for each task-language com-
binations that modify the parameters of a pre-
trained multilingual transformer, Hyper-X allows
for maximum information sharing and enables zero-
shot prediction for arbitrary task-language pairs at
test time. Through a number of experiments, we
demonstrate that Hyper-X is competitive with the
state-of-the-art when transferring from a source
language. When a mixture of tasks and languages
is available, Hyper-X outperforms several strong
baselines on many languages, while being more
parameter and time efficient. Finally, we show that
for few-shot transfer, Hyper-X is a strong option
with a less computing cost than baselines for the
initial task adaptation.
9 Limitations
Firstly, although our experiments show the poten-
tial of Hyper-X to benefit from multiple tasks for
zero-shot transfer, so far we evaluated our model
on a limited set of tasks: NER and POS-tagging,
which may limit the generalizability of our model
to other tasks.
Secondly, for the few-shot transfer, we limit our
experiments to languages that we learn via MLM
and to existing tasks. Our work does not include
languages without MLM data as well as completely
new tasks. Learning the task and language embed-
dings separately, however, creates a possibility to
interpolate existing embeddings for new languagesor new tasks, which especially may work for the
few-shot learning. We leave exploration of these
two limitations to future work.
Acknowledgements
We would like to thank Noah Constant, Asa Cooper
Stickland and the anonymous reviewers for their
helpful feedback on a previous version of this paper.
We also would like to thank the Center for Infor-
mation Technology of the University of Groningen
for providing access to the Peregrine HPC cluster.
References79427943794479457946A Language Selection
Table 4 shows that the details for languages such
as language code, UD treebank id and language
family. For POS tagging, we use the Universal De-
pendencies (UD) 2.7 dataset (Zeman et al., 2020)
and for NER, we use WikiANN (Pan et al., 2017)
with the train, dev and test splits from Rahimi
et al. (2019). To partition languages for the mixed-
language multi-task setting, we group languages
from the same families into the same partitions to
avoid a strong supervision from the same language
family when evaluating zero-shot predictions for
unseen task-language combinations. When there is
no available training data in the target treebank, we
use the test split for the mixed-language multi-task
setting.
B Experimental Details
B.1 Impact of Sampling
Hyper-X is a single model that is trained at once
for multiple languages and task simultaneously.
However, as the amount of total MLM training
data is considearbly larger than NER and POS-
tagging data, we experimented with two differ-
ent sampling methods: size propotional sampling
and temperature-based sampling ( t= 5). For
the temperature-based sampling, we independently
sample a batch for each task-language combination.
Figure 7 shows the impact of different sampling
methods on the zero-shot performance for ‘seen’,
‘unseen’ language groups together with average
over all languages. As seen, temperature-based
sampling, greatly increase performance for all lan-
guage groups on both NER and POS-tagging. This
suggest that when MLM data does not restricted by
sampling, it highly influences the learning objec-
tive which results a catastrophic forgetting on the
target tasks.
B.2 Implementation and Computing
Infrastructure
All the experiments are conducted using Tesla
V100 GPUs. We did not use parallel training on
multiple GPUs, so each experiment was conducted
on a single GPU. Parameters that are fine-tuned for
each model and total runtime are reported in the
section (§ 6.1). We implemented Hyper-X by using
Transformers library (Wolf et al., 2020) and the
code will be released upon publication. We used
adapterhub (Pfeiffer et al., 2020a) for MAD-X, and
the original repository for parameter space factor-
ization (Ponti et al., 2021). Hyper-parameters that
are used in experiments are given in the section 4.
We did not conduct a hyper-parameter search due
to the computational limitations, and used the refer-
ence values in most cases: only the dimension for
language adapters in MAD-X is changed to match
with the same parameter count of Hyper-X. Finally
for mBERT, we did a preliminary experiments with
learning rate of 1e-4 and 1e-5, and pick the latter
one as it produced better performance.
C Detailed Results
The results that are averaged over 3 runs for each
language are given in Table 6
D Few Shot Experiments
For the few-shot transfer experiments, we fine-tune
each model for 50 epochs with the same hyper-
parameters. We disable the learning rate decay as
only a few training instances are provided to the
models. Note that, in these experiments, we always
start with the models that are already trained in the
zero-shot setting and perform fine-tuning for each
language and task separately. For the selection of
training samples, we randomly sample instances
regardless of the labels, as the initial models are
already trained for these tasks on English data.
Table 5 show that few-shot results for NER and
POS-tagging respectively.794779487949