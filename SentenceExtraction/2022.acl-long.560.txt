
Yulin Xu, Zhen Yang, Fandong Meng, and Jie ZhouPattern Recognition Center, WeChat AI, Tencent Inc, China
{xuyulincs}@gmail.com
{zieenyang,fandongmeng,withtomzhou}@tencent.com
Abstract
Complete Multi-lingual Neural Machine
Translation (C-MNMT) achieves superior
performance against the conventional MNMT
by constructing multi-way aligned corpus,
i.e., aligning bilingual training examples from
different language pairs when either their
source or target sides are identical. However,
since exactly identical sentences from differ-
ent language pairs are scarce, the power of
the multi-way aligned corpus is limited by
its scale. To handle this problem, this paper
proposes "Extract and Generate" (EAG), a
two-step approach to construct large-scale and
high-quality multi-way aligned corpus from
bilingual data. Speciﬁcally, we ﬁrst extract
candidate aligned examples by pairing the
bilingual examples from different language
pairs with highly similar source or target
sentences; and then generate the ﬁnal aligned
examples from the candidates with a well-
trained generation model. With this two-step
pipeline, EAG can construct a large-scale and
multi-way aligned corpus whose diversity
is almost identical to the original bilingual
corpus. Experiments on two publicly available
datasets i.e., WMT-5 and OPUS-100, show
that the proposed method achieves signiﬁcant
improvements over strong baselines, with +1.1
and +1.4 BLEU points improvements on the
two datasets respectively.
1 Introduction
Multilingual Neural Machine Translation (MMMT)
(Dong et al., 2015; Firat et al., 2017; Johnson et al.,
2017; Aharoni et al., 2019) has achieved promis-
ing results on serving translations between multi-
ple language pairs with one model. With sharing
parameters of the model, MNMT can facilitate in-
formation sharing between similar languages and
make it possible to translate between low-resourceand zero-shot language pairs. Since the majority
of available MT training data are English-centric,
i.e., English either as the source or target language,
most non-English language pairs do not see a sin-
gle training example when training MNMT models
(Freitag and Firat, 2020). Therefore, the perfor-
mance of MNMT models on non-English trans-
lation directions still left much to be desired: 1)
Lack of training data leads to lower performance
for non-English language pairs(Zhang et al., 2021);
2) MNMT models cannot beat the pivot-based base-
line systems which translate non-English language
pairs by bridging through English (Cheng et al.,
2016; Habash and Hu, 2009).
Recently, Freitag and Firat (2020) re-kindle the
ﬂame by proposing C-MNMT, which trains the
model on the constructed multi-way aligned corpus.
Speciﬁcally, they extract the multi-way aligned ex-
amples by aligning training examples from differ-
ent language pairs when either their source or target
sides are identical (i.e., pivoting through English,
for German!English and English !French to ex-
tract German-French-English examples). Since
they directly extract the multi-way aligned exam-
ples from the bilingual corpus, we refer to their ap-
proach as the extraction-based approach. Despite
improving the performance, the scale of multi-way
aligned corpus extracted by Freitag and Firat (2020)
is always limited compared to English-centric bilin-
gual corpus, e.g., only 0.3M German-Russian-
English multi-way aligned corpus extracted from
4.5M German-English and 33.5M English-Russian
bilingual corpus. A simple idea for remedying this
problem is to add the roughly-aligned corpus by
extracting the training examples when either their
source or target sides are highly similar. However,
our preliminary experiments show that the perfor-
mance of the model decreases dramatically when
we train the model with appending the roughly-
aligned corpus.One possible solution, referred8141to as the generation-based approach, is to gener-
ate the multi-way aligned examples by distilling
the knowledge of the existing NMT model, e.g.,
extracting German-English-French synthetic three-
way aligned data by feeding the English-side sen-
tences of German-English bilingual corpus into
the English-French translation model. Although
thegeneration-based approach can theoretically
generate non-English corpus with the same size as
original bilingual corpus, its generated corpus has
very low diversity as the search space of the beam
search used by NMT is too narrow to extract di-
verse translations (Wu et al., 2020; Sun et al., 2020;
Shen et al., 2019), which severely limits the power
of the generation-based approach.
In order to combine advantages of the two
branches of approaches mentioned above, we pro-
pose a novel two-step approach, named EAG
(Extract and Generate), to construct large-scale
and high-quality multi-way aligned corpus for C-
MNMT. Speciﬁcally, we ﬁrst extract candidate
aligned training examples from different language
pairs when either their source or target sides are
highly similar; and then we generate the ﬁnal
aligned examples from the pre-extracted candidates
with a well-trained generation model. The motiva-
tion behind EAG is two-fold: 1) Although identical
source or target sentences between bilingual ex-
amples from different language pairs are scarce,
highly similar sentences in source or target side are
more wide-spread; 2) Based on the pre-extracted
candidate aligned examples which have highly sim-
ilar source or target sentences, EAG can generate
the ﬁnal aligned examples by only reﬁning the sen-
tences partly with a few modiﬁcations. Therefore,
the non-English corpus constructed by EAG has
almost identical diversity to the original bilingual
corpus. Experiments on the publicly available data
sets, i.e., WMT-5 and OPUS-100, show that the pro-
posed method achieves substantial improvements
over strong baselines.
2 Background
Bilingual NMT Neural machine translation
(Sutskever et al., 2014; Cho et al., 2014; Vaswani
et al., 2017) achieves great success in recent years
due to its end-to-end learning approach and large-
scale bilingual corpus. Given a set of sentence
pairsD=f(x;y)2(XY)g, the NMT model
is trained to learn the parameter by maximizingthe log-likelihoodlogP (yjx;).
MNMT Considering training a separate model
for each language pair is resource consuming,
MNMT (Dong et al., 2015; Johnson et al., 2017;
Gu et al., 2020) is introduced to translate between
multiple language pairs using a single model (John-
son et al., 2017; Ha et al.; Lakew et al., 2018). We
mainly focus on the mainstream MNMT model
proposed by Johnson et al. (2017), which only in-
troduces an artiﬁcial token to the input sequence to
indicate which target language to translate.
C-MNMT C-MNMT is proposed to build a com-
plete translation graph for MNMT, which contains
training examples for each language pair (Freitag
and Firat, 2020). A challenging task remaining
is how to get direct training data for non-English
language pairs. In Freitag and Firat (2020), non-
English training examples are constructed by pair-
ing the non-English sides of two training examples
with identical English sides. However, this method
can’t get large-scale training examples since the
quantity of exactly identical English sentences
from different language pairs is small. Another
feasible solution is to generate training examples
with pivot-based translation where the source sen-
tence cascades through the pre-trained source !
English and English !target systems to generate
the target sentence (Cheng et al., 2016). Despite a
large quantity of corpus it can generate, its gener-
ated corpus has very low diversity (Wu et al., 2020;
Sun et al., 2020; Shen et al., 2019).
3 Methods
The proposed EAG has a two-step pipeline. The
ﬁrst step is to extract the candidate aligned exam-
ples from the English-centric bilingual corpus. The
second step is to generate the ﬁnal aligned exam-
ples from the candidates extracted in the ﬁrst step.
3.1 Extract candidate aligned examples
Different from Freitag and Firat (2020) who ex-
tract non-English training examples by aligning the
English-centric bilingual training examples with
identical English sentences, we extract the can-
didate aligned examples by pairing two English-
centric training examples with highly similar En-
glish sentences. Various metrics have been pro-
posed to measure the superﬁcial similarity of two
sentences, such as TF-IDF (Aizawa, 2003; Huang
et al., 2011), edit distance (Xiao et al., 2008; Deng8142
et al., 2013), etc. In this paper, we take edit dis-
tance as the measurement to decide the superﬁcial
similarity of two English sentences. Three main
considerations are behind. Firstly, since edit dis-
tance measures the similarity of two sentences with
the minimum number of operations to transform
one into the other, it tends to extract sentences with
similar word compositions and sentence structures.
Secondly, since edit distance only utilizes three op-
erations, i.e., removal, insertion, or substitution, it
is easier to mimic these operations in the process of
generating the ﬁnal aligned examples (we leave the
explanation in the next subsection). Finally, unlike
TF-IDF which only considers word bags in two
sentences, edit distance also considers the word
order in each sentence.
Formally, given two English-centric bilin-
gual corpora from two different language pairs
fX;YgandfX;Yg, whereXandXare
English sides, YandYbelong to language L
andLrespectively. For sentence pair (x;y)2
fX;Ygand(x;y)2 fX;Yg, we take
(x;y;x;y)as a candidate aligned example if
the two English sentences xandxmeets:
f(x;x)min(jxj;jxj);2(0;1)(1)
wherefrefers to the function of edit distance
calculation,jxjrepresents the length of the sen-
tencex,is the similarity threshold which can be
set by users beforehand to control the similarity of
sentences in the candidate aligned examples. With
setting= 0, we can directly extract the same
multi-way aligned examples with Freitag and Fi-
rat (2020). With larger , more candidate aligned
examples can be extracted for looser restriction.
Accordingly, there are more noises in the extracted
candidate aligned examples.3.2 Generate ﬁnal aligned examples
In the extracted candidate aligned example
(x;y;x;y),(x;y)is not well aligned to
(x;y)iff(x;x)does not equal to zero. To
construct the ﬁnal three-way aligned example, we
search for one sentence pair (~x;~y)in the lan-
guage pairfX;Yg, where ~xhas the same mean-
ing tox(thus (x;y;~y)is a three-way aligned
example). Unfortunately, it is very difﬁcult for us
to directly ﬁnd such a sentence pair in the large
search space. However, considering ~xandxare
both in English, we can take an extreme case where
~xis identical to xin the superﬁcial form. Now,
the remained question is that we need to search
for the sentence ~yin language L, which has the
same meaning to x. By comparing (x;~y)with
(x;y), asxcan be transformed from xwith the
operations performed by edit distance, it is natu-
rally to suppose that we can ﬁnd such a ~ywhich
can be transformed from ywith these operations
similarly. Therefore, we can limit the search space
for~ywith two restrictions: Firstly, sentence ~yhas
the same meaning with x; Secondly, ~yis trans-
formed from ywith the operations performed by
edit distance. Considering the restrictions men-
tioned above, we apply an NMT model mto search
and generate ~y. There are two main questions left
to be resolved: how to train such a model mand
how to generate ~ywith a well-trained m.
Training Motivated by the recent success of self-
supervised training (Devlin et al., 2018; Conneau
and Lample, 2019; Song et al., 2019; Yang et al.,
2020) in natural language processing, we automati-
cally construct the training corpus for mfrom the
candidate aligned examples. Given the candidate
aligned example (x;y;x;y), the training ex-8143ample formis built as:
([x; ^y];y) (2)
whereyis the target sentence, the concatenation
ofxand^yis the source-side input. ^yis the
noisy form of ywhich we build by mimicking
the operations of edit distance, i.e, performing in-
sertion, removal, or substitution on some pieces
ofyrandomly. Speciﬁcally, with probability ,
each position of sentence ycan be noised by ei-
ther removed directly, inserted or substituted with
any other words in the dictionary W, which is
constructed from the corpus Y. With the self-
constructed training examples, the model mis
trained to generate the target sentence, which is
recovered from the right-side of the concatenated
input with the operations performed by edit dis-
tance, and has the same meaning to the left-side of
the input.
Generating With a well-trained m, we generate
the ﬁnal aligned examples by running the inference
step ofm. Formally, for the ﬁnal aligned example
(x;y;~y), the sentence ~yis calculated by:
~y=m([x;y]) (3)
where [;]represents the operation of concatena-
tion, andm(x)refers to running the inference step
ofmwithxfed as input. With this generation pro-
cess, ~yis not only has the same meaning to x
(thus also aligned to y), but also keeps the word
composition and sentence structure similar to y.
Therefore, EAG can construct the ﬁnal aligned cor-
pus for each non-English language pair, and keep
the diversity of the constructed corpus almost iden-
tical to the original English-centric corpus. For a
clear presentation, Algorithm 1 in Appendix A.2
summarizes the process of generating the ﬁnal
aligned examples. We also provide a toy exam-
ple in Figure 1 to illustrate how the proposed EAG
works.
4 Experiments and Results
For fair comparison, we evaluate our methods on
the publicly available dataset WMT-5, which is
used by Freitag and Firat (2020). Additionally, we
test the scalability of our method by further con-
ducting experiments on Opus-100, which contains
English-centric bilingual data from 100 language
pairs (Zhang et al., 2020). In the extraction pro-
cess, we run our extraction code on the CPU with24 cores and 200G memory.In the generation
process, we take transformer-big (Vaswani et al.,
2017) as the conﬁguration for m, andmis trained
with the self-constructed examples mentioned in
Section 3.2 on eight V100 GPU cards.
We choose Transformer as the basic structure
for our model and conduct experiments on two
standard conﬁgurations, i.e, transformer-base and
transformer-big. All models are implemented
based on the open-source toolkit fairseq (Ott et al.,
2019) and trained on the machine with eight V100
GPU cards.All bilingual models are trained for
300,000 steps and multi-lingual models are trained
for 500,000 steps. We add a language token at the
beginning of the input sentence to specify the re-
quired target language for all of the multi-lingual
models. For the hyper-parameters and, we set
them as 0.5 and 0.3 by default and also investigate
how their values produce effects on the translation
performance.
4.1 Experiments on WMT-5
4.1.1 Datasets and pre-processing
Following Freitag and Firat (2020), we take
WMT13EnEs, WMT14EnDe, WMT15EnFr,
WMT18EnCs and WMT18EnRu as the training
data, the multi-way test set released by WMT2013
evaluation campaign (Bojar et al., 2014) as the test
set. The size of each bilingual training corpus (the
non-English corpus constructed by Freitag and
Firat (2020) included) is presented in Table 1. For
the bilingual translation task, the source and target
languages are jointly tokenized into 32,000 sub-
word units with BPE (Sennrich et al., 2016). The
multi-lingual models use a vocabulary of 64,000
sub-word units tokenized from the combination of
all the training corpus. Similar to Freitag and Firat
(2020), we use a temperature-based data sampling
strategy to over-sample low-resource language
pairs in standard MNMT models and low-resource
target-languages in C-MNMT models (temperature
T= 5 for both cases). We use BLEU scores
(Papineni et al., 2002) to measure the model
performance and all BLEU scores are calculated
with sacreBLEU (Post, 2018).8144cs de en es fr ru
cs 0.7 47 0.8 1 0.9
de 0.7 4.5 2.3 2.5 0.3
en 474.5 13.1 38.1 33.5
es 0.8 2.3 13.1 10 4.4
fr 12.5 38.1 10 4.8
ru 0.9 0.3 33.5 4.4 4.8
cs de en es fr ru
cs 2.2 47 2.5 4.1 3.2
de 2.2 4.5 7.1 6.1 1.4
en 474.5 13.1 38.1 33.5
es 2.5 7.1 13.1 22 10.1
fr 4.1 6.1 38.1 22 11.0
ru 3.2 1.4 33.5 10.1 11.0
4.1.2 Corpus constructed by EAG
Table 2 shows the training data after constructing
non-English examples from English-centric corpus
by the proposed EAG. By comparing Table 2 with
Table 1, we can ﬁnd that EAG can construct much
more multi-way aligned non-English training ex-
amples than Freitag and Firat (2020), e.g., EAG
constructs 1.4M bilingual training corpus for the
language pair German !Russian which is almost
up to 4 times more than the corpus extracted by
Freitag and Firat (2020). In all, EAG constructs no
less than 1M bilingual training examples for each
non-English language pair.
4.1.3 Baselines
In order to properly and thoughtfully evaluate the
proposed method, we take the following ﬁve kinds
of baseline systems for comparison:
Bilingual systems (Vaswani et al., 2017) Apart
from training bilingual baseline models on the orig-
inal English-centric WMT data, we also train bilin-
gual models for non-English language pairs on the
direct bilingual examples extracted by Freitag and
Firat (2020).
Standard MNMT systems (Johnson et al., 2017)
We train a standard multi-lingual NMT model on
the original English-centric WMT data.Bridging (pivoting) systems (Cheng et al., 2016)
In the bridging or pivoting system, the source sen-
tence cascades through the pre-trained source !
English and English !target systems to generate
the target sentence.
Extraction-based C-MNMT systems (Freitag
and Firat, 2020) Freitag and Firat (2020) con-
struct the multi-way aligned examples by directly
extracting and pairing bilingual examples from dif-
ferent language pairs with identical English sen-
tences.
Generation-based C-MNMT systems The
generation-based C-MNMT baselines construct
non-English bilingual examples by distilling the
knowledge of the system which cascades the
source!English and English !target models.
Different from the bridging baselines which just
feed the test examples into the cascaded system
and then measure the performance on the test
examples, the generation-based C-MNMT base-
lines feed the non-English sides of the bilingual
training examples into the cascaded systems
and then get the non-English bilingual training
examples by pairing the inputs and outputs. The
combination of the generated non-English corpus
and original English-centric corpus is used to train
the C-MNMT model.
4.1.4 Results
We ﬁrst report the results of our implementa-
tions and then present the comparisons with pre-
vious works. In our implementations, we take the
transformer-base as the basic model structure since
it takes less time and computing resources for train-
ing. To make a fair comparison with previous
works, we conduct experiments on transformer-big
which is used by baseline models.
Results of our implementation Table 3 shows
the results of our implemented systems. Apart from
the average performance of the translation direc-
tions from each language to others, we also report
the average performance on the English-centric and
non-English language pairs.As shown in Table
3, we can ﬁnd that the proposed EAG achieves bet-
ter performance than all of the baseline systems.
Compared to the extraction-based C-MNMT, the
proposed method achieves an improvement up to
1.1 BLEU points on non-English language pairs.8145
The generation-based C-MNMT performs worse
than the extraction-based one even if it generates
much larger corpus. Since there is no any training
example for non-English language pairs in standard
MNMT, the standard MNMT system achieves infe-
rior performance to the pivot and bilingual systems
on non-English translation directions. However,
with the constructed non-English training exam-
ples, EAG achieves 3.2 and 7.5 BLEU points im-
provements compared with the pivot and bilingual
systems respectively.
targetcs de en es fr ru
cs 27.6 31.9 31.6 33.8 28.4
de 25.8 31.4 31.2 33.3 25.1
en 26.7 27.4 35.1 36.0 26.6
es 25.9 26.4 35.1 36.8 25.6
fr 24.9 26.5 34.6 33.8 23.7
ru 24.9 25.1 30.1 30.4 31.0
Results compared with previous works Table
4 shows the results of the proposed EAG. We can
ﬁnd that the proposed EAG surpasses Freitag and
Firat (2020) almost on all of the translation direc-
tions, and achieves an improvement with up to 2.4
BLEU points on the Russian-to-German direction.
4.2 Experiments on Opus-100
Datasets and pre-processing Zhang et al.
(2020) ﬁrst create the corpus of Opus-100 by sam-pling from the OPUS collection (Tiedemann, 2012).
Opus-100 is an English-centric dataset which con-
tains 100 languages on both sides and up to 1M
training pairs for each language pair. To evalu-
ate the performance of non-English language pairs,
Zhang et al. (2020) sample 2000 sentence pairs of
test data for each of the 15 pairings of Arabic, Chi-
nese, Dutch, French, German, and Russian. Fol-
lowing Zhang et al. (2020), we report the sacre-
BLEU on the average of the 15 non-English lan-
guage pairs.The statistics about the non-English
corpus constructed by Freitag and Firat (2020) and
EAG are presented in Table 5. We can ﬁnd that
EAG is able to construct much more bilingual cor-
pus for non-English language pairs (almost nine
times more than Freitag and Firat (2020) for each
language pair). We use a vocabulary of 64,000
sub-word units for all of the multi-lingual models,
which is tokenized from the combination of all the
training corpus with SentencePiece.
Results Apart from the baselines mentioned
above, we also compare with other two systems
proposed by Zhang et al. (2020) and Fan et al.
(2020). Zhang et al. (2020) propose the online8146
back-translation for improving the performance of
non-English language pairs in Opus-100. Fan et al.
(2020) build a C-MNMT model, named m2m,
which is trained on 7.5B training examples built in
house. Following Zhang et al. (2020), we take the
transformer-base as the basic model structure for
the experiments and results are reported in Table
6. We can ﬁnd that EAG achieves comparable per-
formance to Fan et al. (2020) which utilizes much
more data than ours. This is not a fair comparison
as the data used Fan et al. (2020) is 75 times as
much as ours. Additionally, our model surpasses
all other baseline systems and achieves +1.4 BLEU
points improvement compared with the extraction-
based C-MNMT model.
5 Analysis
We analyze the proposed method on Opus-100 and
take the transformer-base as the model structure.
5.1 Effects of the hyper-parameters
The similarity threshold and the noise ratio are
important hyper-parameters in EAG. In this section,
we want to test how these two hyper-parameters af-
fect the ﬁnal translation performance and how they
work with each other. We investigate this problem
by studying the translation performance with differ-
entand, where we vary andfrom 0 to 0.7
with the interval 0.2. We report the average BLEU
score for the translation directions from Arabic to
other ﬁve languages on the development sets built
in house. Figure 2 shows the experimental results.
With= 0, it means that the generation process is
not applied and we directly train the NMT model
with the extracted roughly aligned examples. And
this is the setting of our motivated experiments
mentioned in Section 1. We can ﬁnd that, the ﬁnalperformance drops sharply when we directly train
the model with the roughly aligned sentence pairs.
For each curve in Figure 2, we can ﬁnd that the
model achieves the best performance when the is
around, and then the performance decreases with
growing. A relatively unexpected result is that
the model usually achieves the best performance
when= 0:5rather than when = 0:7(with
a larger,mis trained to handle more complex
noise). We conjecture the main reason is that the
noise in the training data when = 0:7is beyond
the capacity of m, which makes mconverge poorly
during training. Overall, with set as 0.5 and set
as 0.3, the model achieves the best performance.
5.2 The ability of m
We test how the ability of maffects the ﬁnal perfor-
mance. Apart from the Transformer-base, i.e., the
default setting used in EAG, we also test other two
settings, namely Transformer-big (Vaswani et al.,
2017) and Transformer-deep (20 encoder layers
and 4 decoder layers). With different settings, mis
expected to perform different abilities in the gen-
eration process. The experimental results are pre-
sented in Table 7. We can ﬁnd that if we remove m,
the ﬁnal performance drops dramatically (compar-
ing #0 with #1). This shows that the generation step8147
plays a signiﬁcant role in the proposed EAG. How-
ever, by comparing among #0, #2 and #3, we can
ﬁnd that the ability for mshows little effect on the
ﬁnal performance. Taking all of #0, #1, #2 and #3
into consideration, we can reach a conclusion that,
the generation step is very important for EAG and
a simple generation model, i.e., a baseline NMT
model, is enough to achieve strong performance.
5.3 Back-translation
We are very curious about how EAG works with
back-translation (BT). To investigate this problem,
we utilize the extraction-based C-MNMT model to
decode the non-English monolingual sentences in
the candidate aligned examples extracted by EAG,
and then get the synthetic non-English sentence
pairs by pairing the decoded results with the orig-
inal sentences. The reversed sentence pairs are
appended into the training corpus for the MNMT
models. The experimental results are presented
in Table 8. We ﬁnd that BT improves the perfor-
mances of both the two systems. Additionally, BT
can work as a complementary to the proposed EAG.
5.4 Case study and weaknesses
Figure 3 presents some examples constructed by
EAG, each of which includes the extracted candi-
date aligned example and the generated sentence
for Arabic!Chinese. The extracted candidate
aligned example contains two bilingual examples,which are extracted from Arabic !English and
Chinese!English respectively. In Figure 3 ，the
two bilingual examples in case one are extracted as
a candidate aligned example as their English sen-
tences have high similarity. And there is a composi-
tion gap between xandxsince "Bobby Jordan"
is mentioned in x, but not inx. By comparing
the generated Chinese sentence ~ywith the original
sentencey, we can ﬁnd that ~yis modiﬁed from
yby inserting the Chinese words “ 鲍比乔丹 ",
which has the same meaning with "Bobby Jordan".
Therefore, the generated ~yis aligned to xand
y. In case 2, the Chinese word “ 出去(out)" iny
has been replaced with Chinese words " 得到正义
(justice)" in ~y, which makes the ~yaligned tox
andy. Case 3 in Figure 3 behaves similarly.
While achieving promising performance, the
proposed approach still has some weaknesses in
the real application: 1) The two-step pipeline per-
formed by EAG is somewhat time-consuming com-
pared to Freitag and Firat (2020); 2) The generated
multi-way aligned examples by EAG are some-
times not strictly aligned as the generation process
does not always perform perfectly.
6 Conclusion and Future work
In this paper, we propose a two-step approach,
i.e., EAG, to construct large-scale and high-quality
multi-way aligned corpus from English-centric
bilingual data. To verify the effectiveness of the
proposed method, we conduct extensive experi-
ments on two publicly available corpora, WMT-
5 and Opus-100. Experimental results show that
the proposed method achieves substantial improve-
ments over strong baselines consistently. There
are three promising directions for the future work.
Firstly, we plan to test whether EAG is applicable
to the domain adaptation problem in NMT. Sec-8148ondly, we are interested in applying EAG to other
related tasks which need to align different example
pairs. Finally, we want to investigate other model
structures for the generation process.
Acknowledgments
The authors would like to thank the anonymous re-
viewers of this paper, and the anonymous reviewers
of the previous version for their valuable comments
and suggestions to improve our work.
References81498150A Appendix
A.1 Structure and training process for m
We take transformer-big as the conﬁguration for m.
The word embedding dimension, head number, and
dropout rate are set as 1024, 16, and 0.3 respec-
tively. The model is trained on 8 V100 GPU cards,
with learning rate, max-token, and update-freq set
as 0.01, 8192, and 10 respectively.
We trainmon the self-constructed examples
with early-stopping. For the original example
(x;y), we feedmwith the input format " x<sep>
~y", where ~yis the noised form of y, "<sep>"
is a special token utilized to denote the sentence
boundary. Similar to the traditional NMNT model,
mis trained to predict the original target sentence
y.
A.2 Algorithm for our approach
The algorithm for the proposed EAG is detailed as
A.2.
A.3 Concrete BLEU score for each language
pair
In this section, we present the concrete BLEU score
for each language pair on the corpus of WMT-5.
Table 9, 10, 11, 12, and 13 show the translation per-
formance for the bilingual system, standard MNMT
system, pivot system, extraction-based system and
the EAG respectively.8151Algorithm 1 Generating ﬁnal aligned corpus: Given the aligned candidates fx;y;x;yg; an NMT
modelm, noisy probability , word listW; return the ﬁnal aligned corpus fy;~ygprocedure N (y,,W) ^y y fort20;:::;j^yj 1do generate random ﬂoat 2(0;1) if< then perform insertion, removal or substitution on the position tof^ybased onW return ^yprocedure T (x,y) initializemrandomly while not convergence do fori21;:::;jxjdo ^y N (y;) trainmwith the example ([x; ^y];y) returnmprocedure G (x,x,y,y,m) fori21;:::;jxjdo get~yby performing the inference step of the well-trained mwith input [x;y] get the ﬁnal aligned example by pairing ywith ~y returnfy;~yg
cs de en es fr ru
cs 17.1 30.8 21.2 22.4 14.3
de 16.2 30.4 28.0 29.3 8.1
en 25.2 25.9 33.2 35.6 24.5
es 16.3 23.1 35.4 35.1 20.0
fr 15.2 22.7 33.5 33.0 18.7
ru 13.2 7.3 29.0 23.5 25.0
cs de en es fr ru
cs 19.5 30.5 22.0 20.8 8.9
de 6.1 31.4 17.5 21.0 4.0
en 24.2 27.1 33.2 34.4 24.1
es 4.4 8.3 34.4 19.4 8.9
fr 3.8 10.9 32.5 23.9 6.2
ru 4.5 10.0 29.0 19.2 8.4
cs de en es fr ru
cs 22.8 30.8 27.1 29.2 22.3
de 21.4 30.4 26.5 29.4 20.8
en 25.2 25.9 33.2 35.6 24.5
es 23.1 23.0 35.4 32.6 22.9
fr 21.7 22.8 33.5 30.5 22.0
ru 21.6 20.4 29.0 27.1 28.08152cs de en es fr ru
cs 24.9 30.9 29.3 31.2 26.0
de 23.5 30.0 30.2 31.5 23.3
en 25.6 26.8 34.1 35.2 25.1
es 24.2 25.4 35.0 35.3 24.6
fr 22.9 24.9 34.0 32.5 23.3
ru 24.2 22.6 29.7 28.6 29.8
cs de en es fr ru
cs 26.6 30.5 30.9 32.8 27.6
de 25.0 30.1 30.1 32.5 24.0
en 25.1 26.9 34.7 35.5 25.6
es 24.9 25.7 35.3 36.5 25.2
fr 23.9 25.4 34.2 33.8 23.7
ru 24.7 23.8 30.0 29.8 30.48153