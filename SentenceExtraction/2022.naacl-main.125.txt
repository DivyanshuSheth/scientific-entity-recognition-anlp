
Chuyun Deng, Mingxuan Liu, Yue Qin, Jia Zhang, Haixin Duan, Donghong SunTsinghua University,Indiana UniversityBeijing National Research Center for Information Science and Technology
{dengcy20, liumx18}@mails.tsinghua.edu.cn, qinyue@iu.edu, zhangjia@cernet.edu.cn
{duanhx, sundonghong}@tsinghua.edu.cn
Abstract
Adversarial texts help explore vulnerabilities
in language models, improve model robustness,
and explain their working mechanisms. How-
ever, existing word-level attack methods trap in
a one-to-one attack pattern, i.e., only a single
word can be modified in one transformation
round, and they ignore the interactions between
several consecutive words. In this paper, we
propose ValCAT , a black-box attack frame-
work that misleads the language model by ap-
plying variable-length contextualized transfor-
mations to the original text. Compared to word-
level methods, ValCAT expands the basic units
of perturbation from single words to spans com-
posed of multiple consecutive words, enhanc-
ing the perturbation capability. Experiments
show that our method outperforms state-of-the-
art methods in terms of attack success rate, per-
plexity, and semantic similarity on several clas-
sification tasks and inference tasks. The com-
prehensive human evaluation demonstrates that
ValCAT has a significant advantage in ensur-
ing the fluency of the adversarial examples and
achieves better semantic consistency. We re-
lease the code at https://github.com/
linerxliner/ValCAT .
1 Introduction
Deep learning is successfully applied in a variety of
fields, while previous works have found that neural
network models are vulnerable to well-constructed
adversarial examples (Goodfellow et al., 2015; Ku-
rakin et al., 2017). In general, adversarial examples
are constructed by adding imperceptible perturba-
tions to the benign inputs, which can mislead the
victim model. And exploring adversarial examples
is essential to improving the reliability and robust-
ness of neural network models. Compared to the
long-studied image domain, generating adversarial
examples on texts is more difficult because texts are
discrete, where small changes can alter the originalmeaning and make it unnatural (Xu et al., 2020;
Zhang et al., 2020).
Table 1: Instances of adversarial examples. The first
is the original text, followed by adversarial examples
generated by ValCAT and BERT-Attack. Blue indicates
replace operation, and orange indicates insert operation.
For ValCAT, words in a bracket are a span perturbed
as a whole in one transformation round. The decision
results of the victim model are in parentheses.
Most of the sentence-level attack methods (Zhao
et al., 2018; Han et al., 2020; Xu et al., 2021) gen-
erate adversarial examples by perturbing the latent
representation of the text, and the text quality is
thus relatively difficult to control. Word-level at-
tack methods (Samanta and Mehta, 2017; Zhang
et al., 2019) have received much attention in the
recent past. Several previous works explore the per-
turbation based only on the properties of individual1735
words, with the help of inflectional morphology
(Tan et al., 2020), counter-fitting word vectors (Jin
et al., 2020), sememe (Zang et al., 2020), etc. The
encoder language models, like BERT, give us new
insights of incorporating context information into
the generation of adversarial candidates in transfor-
mation (Li et al., 2020; Garg and Ramakrishnan,
2020; Li et al., 2021). However, the basic units of
perturbation in these works are still single words,
which limits the scope of the perturbation and over-
looks the interactions between words, as examples
shown in Table 1. To address this issue, two re-
search questions are raised: 1) How to take into
account interactions between words in vulnerable
position discovery? 2)How to perturb a vulner-
able position with variable-length contextualized
transformations?
We propose ValCAT, which generates high-
quality adversarial examples by applying variable-
length contextualized transformations to the origi-
nal text. Table 1 demonstrates some sample cases.
Specifically, given a benign text, we enumerate
all possible spans by traversing the text with slid-
ing windows of different lengths to evaluate the
importance of the spans. We utilize two opera-
tions, R andI , to apply adversarial
spans generated by the encoder-decoder language
model to the text. The encoder can fill a mask token
with a single word, while the decoder can predict
a sequence after a prompt. Therefore, joint use of
encoder and decoder enables ValCAT to generate
variable-length contextualized spans at the arbitrary
vulnerable position.
Furthermore, we evaluate ValCAT by attacking
fine-tuned BERT on several classification tasks and
inference tasks. Experiment results show that it
outperforms other baseline methods in attack suc-
cess rate, perplexity, and semantic similarity. In
particular, we observe that the variable-length fea-
ture can significantly reduce perplexity, which is
less than 50% compared to the best baseline. As
for the efficiency, although the multi-word attackpattern perturbs more words than the one-to-one
attack pattern in one transformation, it just requires
fewer rounds toward success. In general results,
it is in a tolerable perturbation rate, and ValCAT
is even lower than baselines on some of the infer-
ence datasets. A comprehensive human evaluation
further verifies that ValCAT has significant advan-
tages in readability and semantic consistency. The
main contributions of this paper are summarized as
follows:
•ValCAT is the first variable-length adversarial
attack method against language models, i.e., ex-
tending the basic units of perturbation from single
words to spans.
•Our work proposes the Sliding Window for
vulnerable-position discovery and the variable-
length contextualized transformation which fully
exploits the advantage of encoder-decoder lan-
guage models.
•Automatic evaluation and human evaluation
demonstrate the excellent attack effectiveness of
ValCAT and the superior quality of our generated
adversarial examples.
2 ValCAT
To further improve the attack effectiveness and si-
multaneously improve the readability and semantic
similarity of the adversarial examples, we propose
ValCAT , which can generate high-quality adversar-
ial text by applying variable-length contextualized
transformations with the encoder-decoder language
model.
Problem Formalization Given a victim model
F:X→Yand a text x=ww...ww
that can be correctly classified by F, the attack
goal is to generate an adversarial text ˜x, which can
confuse the model prediction, i.e. F(˜x)̸=F(x).
A consecutive word sequence with length lcan be
denoted as a span s. In the soft-label black-box
setting, the attacker only has access to the logit
output P(y|x). The architecture, parameters, and
configurations of Fare unknown to the attacker.1736To achieve human imperceptibility, the attacker
should minimize textual perturbations and maintain
semantic consistency.
ValCAT The attack workflow of ValCAT is illus-
trated in Figure 1. To locate the most-vulnerable
positions for the perturbations in each iteration, Val-
CAT first rank the spans in xaccording to their im-
portance with sliding windows of length 1 to MAX,
as lines 2-7 in Algorithm 1. Based on the sorted
span ranking R, ValCAT uses the encoder-decoder
language model to generate variable-length contex-
tualized spans, as lines 8-13. Using two transfor-
mation operations in line 9, R andI ,
we obtain a adversarial text candidate set T. If
some of the candidates can mislead the victim
model, ValCAT declares the one with the high-
est cosine similarity with the original text as the
final successful result, as line 11. However, if no
candidate successes at this iteration, we select the
one with the highest negative impact to the victim
model as the basic text for the next iteration, as
line 13. Note that, if a span in the ranking Rhas
been selected as the target span, the subsequent
spans which are overlapped with it will be removed
from the ranking to avoid multiple modifications
on one word. The attack ends when the successful
example appears, or when the limit of the pertur-
bation constraints (See Section 3.1) reached. The
latter case is considered a final failure. Below we
elaborate on the two stages of the attack in Section
2.1 and Section 2.2 in detail.
2.1 Important Span Ranking
Echoing the observation to prior works (Jin et al.,
2020; Li et al., 2020), only some key words act
as vulnerable positions for the victim model F.
Perturbations over these words can be most benefi-
cial in crafting adversarial examples. Considering
the interactions between words, ValCAT performs
transformations on several important spans instead
of single words.
Given a text x, we evaluate the importance of
a span swithin xaccording to how removing the
span can impact the model prediction in the black-
box setting. Formally, we define the decision dif-
ference of xand˜xon the class yas:
D(x,˜x) =/braceleftbiggd(x,˜x), if y = ˜y
d(x,˜x) +d(˜x, x), if y ̸= ˜y
where yand˜yare the predictions of xand˜x, re-
spectively, and d(x,˜x) =P(y|x)−P(y|˜x)is thedifference of the probabilities that xand˜xare clas-
sified as y. Let ˜xdenote the text of replacing span
sby a unknown token [unk] . The importance score
ofswith respect to xisIP(s) =D(x,˜x).
To compare spans of different lengths, we pro-
pose the Sliding Windows to measure the impor-
tance of variable-length spans. Specifically, we
apply multiple sliding windows of the correspond-
ing sizes, which traverse the text from left to right.
Each span bounded by a sliding window is sequen-
tially removed from the original text for its im-
portance calculation. Finally, we obtain a set of
triples each consisting of the start index, the span
length, and the importance score of the span. We
rank the triples according to the importance score
in descending order.
Algorithm 1: VCAT
Input: Victim Model F; Text x; Label y;
Maximum length of sliding window
MAX
Output: Adversarial exampleR← ∅;t←xforl=1toMAX do fori=1to(x)−l+ 1do s←x ip←IP(s) R←R∪ ⟨i, l, ip⟩SortRaccording to the importance score in
descending orderfor(i,l, _) in Rdo T←R (t, i, l)∪I (t, i) if∃˜x∈Ts.t. F(˜x)̸=ythen return argmax (x,˜x) else t←arg maxD(x,˜x)return NULL
2.2 Variable-Length Contextualized
Transformations
Based on the ranking of triples, ValCAT performs
perturbations in a sequential manner, where each
step a vulnerable position in the original text is
replaced by or inserted with a set of adversarial
spans generated by the encoder-decoder language
model. The variable-length feature of the adversar-
ial spans renders the language model enough space
to produce more contextually appropriate candi-1737dates to improve fluency. Meanwhile, our variable-
length method expands the perturbation units, for it
supports multi-word transformations while is com-
patible with traditional one-to-one transformations.
Compared with previous methods, this further im-
proves the attack success rate under the same per-
turbation constraints. Below we elaborate on the
details of the adversarial span generation.
Adversarial Span Generation To generate ad-
versarial candidates of each target span, ValCAT ap-
plies the encoder-decoder language model to fill the
mask token with a set of spans of varying lengths.
First, the encoder language model owns the ca-
pability of predicting the masked tokens, which is
trained with the masked language modeling (MLM)
objective. However, the encoder fills one mask to-
ken with only one suitable word rather than multi-
ple words. The ability of the decoder could fill the
gap of the single word since the decoder is trained
with a causal language modeling (CLM) which can
predict the sequence after a prompt. But, the de-
coder can only generate sequences at the end of
the text. Hence, ValCAT combined these two mod-
els with their complementary advantages, with the
predictive capability at arbitrary positions of en-
coder and variable-length generation of a decoder.
With the candidate adversarial spans for target span,
ValCAT performs two kinds of transformation op-
erations, R andI , to generate the
adversarial examples for consideration.
Replace TheR operation substitutes the
target span s=w...wwith another span
s. For example, the target span “ awesome ” in the
text “This place is awesome.” could be replaced
by the adversarial span “ pretty good ”. Specifically,
we first replace a mask token [mask] tos:
˜x=w...w[mask] w...w,
and generate a set of contextualized spans of vary-
ing length, S, to fill the mask token. The adversar-
ial example is denoted as:
˜x =w...ws w...w,
where s∈ S is a adversarial span.
Since the language model is blind to the infor-
mation of the target span, some of the generated
adversarial spans may deviate from the original
meaning to a large extent. To avoid this situation,
we only leave the adversarial spans with a high
degree of semantic similarity to the original span.Specifically, we use Universal Sentence Encoder
(Cer et al., 2018) to restrict their cosine similarity.
We also impose a limit on the perturbation rate (See
Section 3.1). To prevent the text from being too
long, we constrain the adversarial spans to be at
most two words longer than the target span.
Insert TheI operation inserts a new span
sin front of the target span s. For example, “I
like this quite interesting movie.”. Similar to the
R operation, it inserts a mask token in front
of the target span:
˜x=w...w[mask][ w...w]w...w,
and corresponds with the adversarial text ˜x=
w...ws w...w. This operation also follows
the same perturbation constraints, mentioned in
Section 3.1.
3 Experiments
In this section, we evaluate ValCAT on two NLP
tasks, text classification and natural language infer-
ence. To demonstrate the effectiveness of ValCAT
in terms of fluency, grammaticality, and semantic
consistency, following Li et al. 2021, we design
and conduct a comprehensive human evaluation.
3.1 Implementation
Victim Model In this work, we choose fine-tuned
BERT as the victim model for the all evaluation
tasks. As BERT has achieved good results on a
variety of NLU tasks and has been proven to be one
of the most representative pre-trained transformers
(Devlin et al., 2019).
Span Generation Model To obtain variable-
length contextualized spans, we choose T5 (Raffel
et al., 2020) as the generation model. T5 is a rep-
resentative encoder-decoder language model that
can predict the missing spans within a corrupted
piece of text, benefiting from the fill-in-the-blank
pre-training. Also, the large corpus C4 renders T5
rich prior knowledge to enable the diversity and the
high quality of the generated spans.
Constraints To achieve human imperceptibility
and semantic preservation of the adversarial exam-
ple, we impose constraints on the word perturbation
rate and semantic similarity, as defined in Section
3.3. Following previous practices (Jin et al., 2020;
Li et al., 2021), we set the thresholds of these con-
straints respectively for each dataset, with details
shown in Appendix A.1738
Settings and Computation Cost All results are
derived from a single run since there is no random-
ness in our method. The maximum length of the
sliding window is set as 3. In our implementation,
we apply SpaCy (Honnibal and Montani, 2017)
and NLTK (Loper and Bird, 2002) for text manip-
ulation. We run ValCAT on Intel Xeon E5-2690
2.6GHz Processor with V100 GPU. Averagely it
takes 34 secs to generate a successful adversarial
example.
3.2 Datasets and Baselines
To investigate the effectiveness of ValCAT on dif-
ferent types of text, we evaluate it on several En-
glish datasets. We randomly sample 1000 instances
from each of the following datasets: three for text
classification, i.e., AG News ,Yelp Polarity , and
IMDB ; and three for natural language inference,
i.e.,SNLI ,MNLI , and QNLI , with detailed informa-tion shown in Appendix B. We compare ValCAT
with several state-of-the-art word-level attack meth-
ods, i.e., TextBugger ,TextFooler , and BERT-Attack .
Details of these methods are shown in Appendix C.
Note that, all the datasets and baselines are pub-
licly available and are used following their usage
specifications.
3.3 Automatic Evaluation
Metrics We evaluate the effectiveness of ValCAT
based on the following metrics:
•Attack success rate (Suc) : is the percentage of
adversarial examples successfully interfered with
the victim model’s prediction over the text dataset.
•Perplexity (PPL) : is an automatic metric to evalu-
ate the probability of a text appearing in a natural
corpus. So PPL can reflect the text’s natural flu-
ency, the lower the better. We use GPT-2 (Radford
et al., 2019) for this calculation.1739•Semantic similarity (Sim) : is the cosine similar-
ity between the original text and adversarial text
calculated by Universal Sentence Encoder (USE)
embeddings (Cer et al., 2018).
•Word perturbation rate (Pert) : is the proportion
of the modified words over the original text. To
evaluate the perturbation rate on variable-length
perturbations more accurately, we design separate
calculations for these two transformation opera-
tions. For the R operation, the number
of modifications is max(l, l)−l, where l,
l, and l are the lengths of the target span,
the adversarial span, and their longest-common-
subsequence (LCS), respectively. For the I
operation, the number of modifications is the length
of the inserted span.
•Grammar error (GErr) : is the incremental num-
ber of grammatical errors of the adversarial exam-
ple relative to the original text. We use Language-
Toolto count the grammatical errors within a
text.
Results The main experimental results are re-
vealed in Table 2. ValCAT outperforms baselines
on multiple datasets in both classification tasks
and inference tasks. Compared with BERT-Attack,
the best baseline method, ValCAT achieves higher
attack success rates in all experiments, and the im-
provement range from 5.8% even to 27.0%. This
is attributed to ValCAT’s ability to apply variable-
length contextualized transformations on spans at
any position, which largely enriches the pattern of
perturbations. In addition, ValCAT achieves the
highest semantic similarity with the original text in
all tasks. The perplexity of the adversarial example
generated by ValCAT is far superior to that of the
baselines. It is roughly only 50% of the perplexityof BERT-Attack and is almost consistent with the
original text. This indicates that the adversarial
example generated by ValCAT is with high fluency.
The underlying reason is that multi-word transfor-
mations render the language model enough space to
generate the adversarial spans with approximated
distribution with natural language. At the same
time, ValCAT reaches the lowest average increase
in the number of grammatical errors on five out of
nine attacks, with only a slight disadvantage on the
natural language inference tasks.
In general, ValCAT generates adversarial exam-
ples that achieve more promising attacks and are
more imperceptible to humans. Figure 2 shows the
performance of ValCAT and baselines on the Yelp
dataset under different constraint settings. As we
can see, the success rate of ValCAT decays most
slowly as the limits of perturbation rate and seman-
tic similarity increase, still achieving a success rate
of almost 70% under the strictest constraint. More-
over, ValCAT can maintain a low perplexity under
any constraint, however, other attack methods have
to sacrifice attack effect to maintain their fluency.
3.4 Human Evaluation
Design We evaluate the quality of the adversar-
ial examples from three perspectives: readability
(fluency and grammaticality), semantic similarity,
and label consistency. The first two metrics are
evaluated by ratings, while the third one lets the
annotator categorize the texts into a set of labels.
Five annotators with CET-4 certification performed
the evaluation on AG News dataset. All annota-
tors were informed and consented to the use of
the annotation, and were paid with remuneration
higher than the regional average. To construct the
instances for human evaluation, we randomly select
100 samples from the dataset, whose adversarial
examples generated by both ValCAT and BERT-
Attack can mislead the victim model to make the
same wrong classification decision among multiple
classes. For the evaluation of fluency and grammat-
icality, each original text and its two adversarial
examples are presented in one group in a shuffled
order and the annotators are asked to rate them.
For the evaluation of semantic similarity, given the
original text, the judges need to separately evaluate
its semantic similarity with the two adversarial ex-
amples presented in random order. The above two
tasks ask the annotators to rate the text or the text
pair from 1 to 5. In each questionnaire, we give1740
explicit criteria and clear examples for each grade.
For the evaluation of label consistency, we mix all
the original texts with the adversarial examples and
let the annotators label the category of them, e.g.,
business or technology.
Results For the two rating tasks, we normalize
the rates of the annotators and calculate the aver-
age score, while for the labeling task we take the
majority vote as the final result. As shown in Table
3, ValCAT is rated with much higher fluency and
grammaticality scores than BERT-Attack. Also, it
achieves obviously higher semantic similarity. In
terms of labeling accuracy, ValCAT slightly outper-
forms BERT-Attack, still indicating that ValCAT
makes smaller changes to the meanings of the text
from the aspect of human perception. All these
results demonstrate the superior quality of the ad-
versarial examples generated by ValCAT.
4 Analysis
4.1 Ablation Study
We evaluate different transformation strategies of
ValCAT as in Table 4. 3-Many results in the low-
est perplexity, as it renders the language model
more space to generate an appropriate span. It
also requires the smallest number of queries, and
we speculate the reason as it perturbs more words
in a single transformation. For the same reason,
3-Many is less likely to succeed since it’s easier
to reach the perturbation constraint. As expected,
the combination of multiple types of R op-
erations facilitates the attack success rate while
increasing the number of queries. The I
operation achieves high semantic similarity while
requiring a large number of queries. A comprehen-
sive utility of all operations (i.e., ValCAT) achieves
the highest success rate with the lowest perturba-
tion rate, which fully demonstrates the advantages
of the transformation diversity.
4.2 Impact of Candidate Numbers
From the results under different numbers of candi-
date spans shown in Figure 3, we observe that the
attack success rate is higher when there are more
candidates. Since we greedily select the adversar-
ial span with the greatest impact on the decision
of the victim model, as the number of candidates
increases, spans with low occurrence probability
can be added to the text, making the perplexity
higher. This is especially true for difficult-to-attack
datasets like AG News.
4.3 Properties of Transformations
We observe the properties of the transformations
in the successful adversarial examples from three
perspectives: type of operation, length of span, and
POS (part-of-speech) of span. Averagely, 62% of
the transformations in each example are R
operations, and 38% are I operations, indi-
cating that R operations are more effective
in most cases. As shown in Table 10, the replaced
spans are most likely the longer ones, while the
adversarial spans being applied are relatively short.
Additionally, Table 11 shows that the adversarial
spans of R operation share identical POS1741distribution with the replaced spans, which guar-
antees the fluency of the adversarial example. The
I operation tends to use adverbs and adjec-
tives, which is consistent with the human intuition
of inserting words into text.
4.4 Adversarial Training
We investigate how adversarial training can mit-
igate the adversarial attack, that is, the power of
our adversarial examples for adversarial training
as a general defense. We randomly sample 10,000
instances from the Yelp training dataset and apply
ValCAT to generate adversarial examples. These
adversarial examples and their intermediate results,
approximately 60,000 texts, are then used to fine-
tune the attacked BERT under the gold labels. Af-
ter adversarial training, the accuracy of the vic-
tim model on the test dataset slightly decreases
from 98.3% to 98.0%, which means that adversar-
ial training does not affect the performance of the
model on clean data. As shown in Table 5, the
attack success rate of ValCAT drops by about 20%,
while both the perturbation rate and the number
of queries have significantly increased, indicating
that the victim model is harder to attack after the
adversarial training.
4.5 Generalization
We evaluate the generalization of ValCAT in two
aspects: 1) attack on other victim models and 2)
transferability.
Attack on Other Victim Models We try to attack
other common language models on the Yelp dataset
using ValCAT. Table 6 shows that LSTM and word-
CNN, two traditional NLP models, are extremelyvulnerable to ValCAT with an attack success rate of
around 99%. Interestingly, RoBERTa shows better
robustness, with a 6.5% decrease in attack success
rate relative to BERT, which we speculate is due to
its more optimized pre-training approach.
Transferability We evaluate the transferability
of generated adversarial examples on the Yelp
dataset. Specifically, we utilize successful adver-
sarial examples generated by ValCAT targeting dif-
ferent victim models to attack other tested mod-
els. Results in Table 7 show that adversarial ex-
amples aimed at transformer models (BERT and
RoBERTa) are more effective in attacking tradi-
tional NLP models (LSTM and wordCNN) than
the opposite case.
4.6 Limitation
ValCAT makes the best effort to improve the effec-
tiveness of the attack, including the Sliding Win-
dow with a variable-length mechanism to discover
vulnerable positions and multiple types of transfor-
mation operations. However, all these efforts result
in a larger number of queries to the victim model,
compared to existing word-level attack methods.
According to the results of the automatic evalua-
tion and human evaluation, such an increase in the
computation expense is tolerable, considering the
significant improvement in attack success rate and
adversarial example quality. Furthermore, a sim-
plified version of ValCAT, 1-Many (See Table 4),
outperforms the best baseline method in all aspects,
including the number of queries.
Further, the encoder-decoder language models
targeting the fill-in-the-blank-style denoising ob-
jectives cannot see the masked original spans dur-
ing the training and inference. This limitation of
encoder-decoder leaves improvement space of se-
mantic similarity for ValCAT, i.e., establishing a di-
rect semantic mapping between the generated span
and the original span themselves in each transfor-1742mation. We leave finding better ways of training to
compensate for the invisibility of encoder-decoder
as our future work.
5 Related Work
Textual adversarial attacks have been intensively
studied (Wang et al., 2019; Zhang et al., 2020).
Early on, the character-level attack methods
(Ebrahimi et al., 2018; Gao et al., 2018; Li et al.,
2019) are widely used, but they would destroy
words and are very perceptible (Pruthi et al., 2019).
Word-level attack methods are now in the spotlight,
evolving from substitution based only on the prop-
erties of individual words themselves (Zang et al.,
2020; Jin et al., 2020; Tan et al., 2020) to perturb-
ing words by multiple strategies with knowledge of
the context (Li et al., 2020; Garg and Ramakrish-
nan, 2020; Li et al., 2021). Although these existing
works may enrich the transformation form with
various operations like insertion, deletion, merging,
etc., they still treat single words as basic units of
each transformation. Some sentence-level works
(Wang et al., 2020a; Wang et al., 2020b; Huang and
Chang, 2021) generate adversarial examples by per-
turbing the latent representation of the original text,
but they are slightly worse at controlling the text
quality. Our work introduces the Sliding Window to
rank important spans and generates variable-length
contextualized spans by the encoder-decoder lan-
guage model, enabling more effective adversarial
attacks and higher-quality adversarial examples.
6 Conclusion
In this paper, we propose ValCAT, the first variable-
length contextualized adversarial attack based on
the encoder-decoder language model. ValCAT
considers the interaction between words for the
vulnerable-position discovery and expands the ba-
sic units of perturbation from single words to spans.
Experimental results on several datasets demon-
strate the effectiveness of our methods, which out-
performs baselines in terms of attack success rate,
perplexity, and semantic similarity. Our adversarial
examples also have good transferability and help
to improve robustness. A comprehensive human
evaluation further verifies the high quality of the
adversarial examples generated by ValCAT.
Acknowledgements
This work is supported in part by National
Natural Science Foundation of China (GrantNo. U1836213, U19B2034), National Key R&D
Plan (Grant No. 2018YFB2101501), Indus-
trial Internet Innovation and Development Project
(Grant No. TC200H02X, TC200H02Y), and the
Huawei Technologies Co., Ltd under Grant No.
TC20200917004.
References17431744A Constraints
We impose constraints on the generating adver-
sarial examples in terms of perturbation rate and
semantic similarity, as shown in Table 8.
B Datasets
We employ three datasets for text classification
and three datasets for natural language inference as
described below. In our experiments, we randomly
sample 1000 instances.
•AG News : a collection of news articles catego-
rized into 4 types: World, Sports, Business, and
Sci/Tech (Zhang et al., 2015).
•Yelp Polarity : positive and negative restaurant
reviews collected from yelp (Zhang et al., 2015).
•IMDB : a dataset of movie reviews for binary
sentiment classification (Maas et al., 2011).
•SNLI : a collection of human-written English sen-
tence pairs (Bowman et al., 2015). Each sentence
pair consists of a premise and a hypothesis, which
is necessary to determine whether it is entailment,
contradiction, or neutral.
•MNLI : a similar dataset to SNLI (Williams et al.,
2018), but from a variety of genres.
•QNLI : a version of SQuAD which has been con-
verted to a binary classification task (Wang et al.,
2018).
C Baselines
We compare ValCAT with state-of-the-art word-
level attack methods:
•TextBugger : an attack method compounded by
five bug generation strategies (Li et al., 2019). Such
strategies include character insertion, character
deletion, character swapping, homograph character
replacement, and synonym replacement.
•TextFooler : classical adversarial attack algorithm
based on synonym substitution (Jin et al., 2020).1745Candidate synonyms are the closest neighbors of
the replaced word in the counter-fitting word em-
bedding space. It limits the cosine semantic simi-
larity and makes the POS consistent.
•BERT-Attack : a state-of-the-art contextualized
attack method using BERT to fill mask tokens (Li
et al., 2020). It serves as a representative of similar
BERT-based algorithms, such as BAE (Garg and
Ramakrishnan, 2020) and CLARE (Li et al., 2021).
D Performance of Encoder-Decoder
Models
We present the results of generating adversarial
spans by several language models in the T5 family
in Table 9. T5 andmT5take a longer time
to generate adversarial spans due to their larger ca-
pacity. The higher perplexity of T5v1.1is, we
conjecture, resulted from its specifically different
structure. In short, T5is good enough for ad-
versarial span generation.
E Properties of Transformations
We observe the properties of the transformations in
the successful adversarial examples from three per-
spectives: type of transformation, length of span,
and POS (part-of-speech) of span. Results are
shown in Table 10 and Table 11.1746