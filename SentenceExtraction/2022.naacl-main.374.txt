
Jian Guan, Ziqi Liu, Minlie Huang
The CoAI group, DCST; Institute for Artiﬁcial Intelligence; State Key Lab of
Intelligent Technology and Systems; Beijing National Research Center for
Information Science and Technology; Tsinghua University, Beijing 100084, China.
{j-guan19, liuzq19}@mails.tsinghua.edu.cn ,
aihuang@tsinghua.edu.cn
Abstract
Teaching morals is one of the most important
purposes of storytelling. An essential ability
for understanding and writing moral stories is
bridging story plots and implied morals. Its
challenges mainly lie in: (1) grasping knowl-
edge about abstract concepts in morals, (2)
capturing inter-event discourse relations in sto-
ries, and (3) aligning value preferences of sto-
ries and morals concerning good or bad be-
havior. In this paper, we propose two under-
standing tasks and two generation tasks to as-
sess these abilities of machines. We present , a new dataset of Chinese and English
human-written moral stories. We show the dif-
ﬁculty of the proposed tasks by testing vari-
ous models with automatic and manual eval-
uation on . Furthermore, we present a
retrieval-augmented algorithm that effectively
exploits related concepts or events in training
sets as additional guidance to improve perfor-
mance on these tasks.
1 Introduction
Stories play an essential role in one’s moral devel-
opment (Vitz, 1990). For example, individuals usu-
ally learn morals from life experiences or literature
such as fables and tell their morals by representing
their lived experience in a narrative form (Tappan
and Brown, 1989). Accordingly, it is a crucial
ability for humans to bridge abstract morals and
concrete events in stories. However, this ability has
not yet been investigated for machines.
There have been many tasks proposed for evalu-
ating story understanding and generation, including
story ending selection (Mostafazadeh et al., 2016)
and story generation from short prompts (Fan et al.,
2018). Unlike these tasks, which focus on reason-
ing plots from context, we emphasize the ability
to associate plots with implied morals. As exem-
pliﬁed in Table 1, the challenges mainly lie in (1)
Table 1: An example in
grasping knowledge about abstract concepts (e.g.,
“unity,” “strength”) and relations among them (e.g.,
“is”) in morals; (2) capturing inter-event discourse
relations in stories (e.g., the contrast between end-
ings of the “cows” when they are “united” and “di-
vided”); and (3) aligning value preferences (Jiang
et al., 2021) of stories and morals (e.g., the story
implies support for “unity”, not opposition, which
agrees with “is strength” in the moral). To test
these abilities of machines, we propose two un-
derstanding tasks and two generation tasks. Both
understanding tasks require selecting the correct
moral from several candidates given a story. And
they have respective candidate sets for testing ma-
chines in two aspects, including concept under-
standing (Cfor short) and preference align-
ment (P for short). The generation tasks
require concluding the moral of a story (2for
short), and conversely generating a coherent story
to convey a moral (2for short).
Furthermore, we collected a new dataset named composed of 4k Chinese and 2k English
human-written stories paired with morals through
human annotation to address the above challenges.
We call the Chinese dataset -and the
English dataset -, respectively. And we
construct datasets for the proposed tasks based on . Our focus of morals is on the social set
of standards for good or bad behavior and charac-
ter, or the quality of being right, honest or accept-5069able (Ianinska and Garcia-Zamor, 2006). We con-
duct extensive experiments on the proposed tasks.
Furthermore, we present a retrieval-augmented al-
gorithm to improve model performance by retriev-
ing related concepts or events from training sets
as additional guidance. However, the experiment
results demonstrate that existing models still fall
short of understanding and generating moral sto-
ries, which requires a better modeling of discourse
and commonsense relations among concrete events
and abstract concepts.
2 Related Work
Story Datasets ROCStories (Mostafazadeh et al.,
2016) and WritingPrompts (Fan et al., 2018) are
two frequently used story datasets in related studies.
The former consists of artiﬁcial ﬁve-sentence sto-
ries regarding everyday events, while the latter con-
tains ﬁctional stories of 1k words paired with short
prompts. Besides, some recent works collected
extra-long stories such as roleplayerguild (Louis
and Sutton, 2018), PG-19 (Rae et al., 2020), and (Akoury et al., 2020). Guan et al. (2022)
proposed a collection of Chinese stories. These
stories usually aim to narrate a coherent event se-
quence but not convince readers of any morals.
Story Understanding and Generation There
have been many tasks proposed for evaluating story
understanding and generation. Firstly, some works
tested the machinery commonsense reasoning abil-
ity regarding inter-event causal and temporal rela-
tions through story ending selection (Mostafazadeh
et al., 2016), story ending generation (Guan et al.,
2019) and story completion (Wang and Wan, 2019).
Secondly, a series of studies focused on the coher-
ence of story generation (Fan et al., 2018; Yao et al.,
2019; Guan et al., 2020). Another line of works
concentrated on controllability to impose speciﬁed
attributes into story generation. These attributes
involved outlines (Rashkin et al., 2020), emotional
trajectories (Brahman and Chaturvedi, 2020) and
story styles (Kong et al., 2021). Our tasks investi-
gate not only the above aspects but also the ability
to understand abstract concepts and reason value
preferences of stories.
A task similar to2is text summariza-
tion (Finlayson, 2012) since both tasks require gen-
erating a short text to condense crucial informationof a long text. But summarization requires reorga-
nizing a few words of the original text instead of
concluding a character-independent moral. For ex-
ample, a plausible summary of the story in Table 1
is “Four cows were killed by two tigers and a lion”
(generated by BART (Lewis et al., 2020) ﬁne-
tuned on a summarization dataset XSUM (Narayan
et al., 2018)), which includes speciﬁc characters
and events of the original story. Moreover,2
is similar to persuasive essay generation (Stab and
Gurevych, 2017), which also requires conveying
a viewpoint in generated texts. However, persua-
sive essays usually convince readers by directly
presenting arguments but not narrating a story.
Morals Haidt and Joseph (2004) provided a the-
oretical framework named Moral Foundations The-
ory (MFT) to summarize ﬁve basic moral foun-
dations such as “Care/Harm,” “Fairness/Cheating,”
etc. Based on the theory, recent studies have ex-
plored to classify the moral foundations of par-
tisan news (Fulgoni et al., 2016), tweets (John-
son and Goldwasser, 2018; Hoover et al., 2020),
and crowd-sourced texts (Pavan et al., 2020). And
V olkova et al. (2017) proposed identifying suspi-
cious news based on the features of moral foun-
dations. However, we focus on morals which are
free-form texts far beyond the scope of the ﬁve
categories in MFT. In addition, recent studies pro-
posed multiple datasets for machine ethics research
such as SBIC (Sap et al., 2020), Social Chem-
istry (Forbes et al., 2020), Moral Stories (Emelin
et al., 2020), ETHICS (Hendrycks et al., 2021) and
Scruples (Lourie et al., 2021). But these datasets
focus more on how machines behave ethically in
some scenario, while emphasizes the abil-
ity to conclude the moral implied by a story. More-
over, most cases in these datasets consist of short
texts of descriptive ethical behavior, typically in
the form of one sentence. In contrast, pro-
vided longer and more context-speciﬁc stories for
moral understanding.
3 Dataset
We collected from multiple web pages of
moral stories. All stories are allowed to use and
redistribute for research and have been reviewed by
the website editors as stated on the pages. We show
the full list of links to these pages in Section A.1.
After de-duplication, we collected 19,197 Chinese
and 2,598 English raw texts. Then we adopted hu-
man annotation for decoupling the story and moral5070
in each raw text. Due to resource limitations, we
only constructed 4,209 Chinese and 1,779 English
story-moral pairs. We will ﬁrst show the details
of human annotation, then present the topic analy-
sis and statistics of , and ﬁnally describe
the details of dataset construction for the proposed
tasks.
3.1 Human Annotation
To narrow down our focus, we deﬁne a story as
a series of coherent events involving several inter-
related characters, and implies support or opposi-
tion of some behavior. Such a deﬁnition constrains
the story to exhibit a moral without any explicit
arguments. And we deﬁne a moral as a judgment
to describe what the story implies concerning good
or bad behavior. Note that we do not require morals
in to be always reﬂective of normatively
virtuous behavior. We emphasize that the morals
should align with the story. Then, a key issue is how
to extract the story and moral from a raw text. We
observe that there are no markers such as “The story
tells us” to separate the story and moral in most
cases. The moral may be tightly weaved into the
plot (e.g., included in a dialogue). Therefore, we
adopted human annotation for this extraction task.
We hired a commercial team to annotate -. All annotators are native Chinese speakers
and well trained for our task. For -, we
hired three graduates with good English language
proﬁciency. We did not use AMT since it is incon-
venient to train online annotators. Figure 1 shows
the annotation pipeline.
We ﬁrst ask annotators to judge whether the raw
text contains a story and moral and whether they
meet our constraints shown in Figure 1. We show
the examples given to the annotators to inform
them of our requirements for stories and morals
in Section A.2. If the constraints are not met, we
then ask annotators to reﬁne the story and moral.In the reﬁnement stage, annotators have to clean
up the data with following heuristics: (1) refus-
ing examples which may violate general ethical
principles (e.g., discrimination); (2) deleting noisy
words (e.g., links, codes); (3) reﬁning the stories
and morals to be coherent and formal. And to en-
sure the quality of collected data, annotators may
refuse to reﬁne the example if it requires much cre-
ative writing. Finally, we review the annotation
results and provide detailed feedback to the annota-
tors before approving their submissions. We show
an annotation example in Table 2.
3.2 Topic Analysis
To provide insight into the taxonomy of morals
within , we adopt LDA (Blei et al., 2003)
for topic modeling of morals. Let Bdenote the
number of topics and Vdenote the vocabulary size.
Based on the variational parameter for topic word
distribution β∈R, we determine Bas the
minimum value that makes the following formula5071
holds true for any b∈{1,2,···,B}:
s=/summationtextβ
/summationtextβ≥h,
V=argmax/summationdisplayβ,
whereβis the element at the b-th row and v-th
column ofβ,k∈{1,2,···,V}is the size of the
top-kvocabularyV, andh∈[0,1]is a prede-
ﬁned threshold. sis used to measure the speci-
ﬁcity of theb-th topic. Intuitively, the larger s, the
more speciﬁc the topic. We set kto 20 andhto
0.5. Finally, we derive 40/24 topics for -/ -, respectively. And the minimum
proportion of examples of one topic is 1.6%/3.2%
for -/ -, respectively.
Table 3 shows the topic words in Vof each
topic and two morals assigned to each topic with
the highest probabilities for the ﬁve topics with the
largest speciﬁcity scores. The topics cover diverse
situations ranging from facing others (“honesty,”
“help”), parents (“love”), ourselves (“self-help,”
“self-discovery”) to facing difﬁculties (“think”) anddanger (“safety”). And examples of the same topic
present related semantics to some extent, such as
“being honest” and “not believing liars” for the ﬁrst
topic in -. We also show the analysis of
high-frequency words of stories and morals in Sec-
tion A.3 and discussion about the commonsense
and discourse relations in stories in Section A.4.
3.3 Dataset Statistics of
Table 4 shows the statistics of . We regard
the unlabeled data which contain entangled stories
and morals as an in-domain resource for research
on unsupervised or semi-supervised learning for
the proposed tasks. And the data are also suitable
for learning to generate morals stories where the
morals are weaved naturally into the story plots.
3.4 Task-Speciﬁc Dataset Construction
Based on , we build task-speciﬁc datasets
for our understanding tasks (CandP)
and generation tasks (2and2). We
randomly split the labeled data in -and -for training/validation/testing by 8:1:1
and 3:1:1, respectively. Table 5 shows the task5072
descriptions and data sizes.C It requires selecting the correct moral
from ﬁve candidates given a story. We constructed
the dataset by taking the original moral as the cor-
rect candidate and four negatively sampled morals
as incorrect candidates for each example. To avoid
more than one plausible candidate, we ensured that
the negative morals are assigned to different top-
ics from the original one by the LDA model (Sec-
tion 3.2). In this way,Ccan effectively test
the ability to distinguish different concepts.P It requires selecting the correct moral
from two candidates. Its difference fromC
is that we created the incorrect candidate by sub-
stituting one random token in the original moral
to its antonym. For example, the moral “unity is
strength” can be transformed to “unity is weak-
ness”. We perform the transformation using a rule-
based method (Ribeiro et al., 2020). Because there
exist examples where no words have antonyms, the
number of examples forPare a little fewer
thanC.P will serve for testing the
ability to capture the value preference of stories.2 It requires generating the moral of a
given story. We regard the original story as input
and the original moral as target output.2 It requires generating a story to convey
a given moral. Unfortunately, automatic evalua-
tion for open-ended story generation is still highly
challenging due to the notorious one-to-many is-
sue (Zhao et al., 2017): There may be multipleplausible stories with the same moral. For exam-
ple, the moral in Table 1 can also be conveyed by
another story: “bees unite to build their beehives.”
Such openness makes automatic metrics unreliable
for quality evaluation (Guan and Huang, 2020).
To alleviate this issue, we extract the ﬁrst sen-
tence and an outline from a target story, and pair
them with the moral as input for generating the
story. We follow Rashkin et al. (2020) to extract a
set of at most eight phrases from a story through
RAKE (Rose et al., 2010) as the outline. We set
the maximum number of words in each phrase to
eight. We also ﬁltered those phrases that are sub-
strings of others. For example, the outline for the
story in Table 1 is {“lions,” “friends fought,” “good
friends,” “grazed,” “perfect opportunity”}. Finally,
for -/ -, the average number
of phrases for each example is 7.5/6.8 and the aver-
age number of words in each phrase is 2.87/2.44,
respectively.
4 Retrieval Augmentation
A critical challenge for tackling the proposed tasks
is the sparsity of morals and events makes it difﬁ-
cult to learn relations between them. Prior studies
have shown that retrieval improves performance
towards infrequent data points across various tasks
such as open-domain question answering (Chen
et al., 2017) and text classiﬁcation (Lin et al., 2021).
We present a retrieval-augmented algorithm that ex-
ploits the moral-event relations in training sets. We
illustrate our model for theP task in Fig-5073
ure 2. Our models for other tasks are similar.
For bothCandP, we encode the
story and candidates using an input encoder, and
then predict a probability distribution over the can-
didates by normalizing the dot-product scores be-
tween the representations of the story and each
candidate. We optimize the model by minimizing
the cross-entropy loss. We insert special tokens
[S] and[C] before the story and each candidate,
respectively, and take the corresponding hidden
states as their representations. Furthermore, we
propose to retrieve related concepts from the train-
ing set using the input story. We encode the story
using a query encoder, then take the output as the
query to retrieve mmost related stories based on
a story index, i.e., a set of dense vectors as the
representations of stories in the training set. We
adopt BERT (Devlin et al., 2019) followed by a
mean-pooling layer to build the query encoder and
story index, which are frozen in the training stage.
Finally, we extract the nouns, verbs, adjectives and
adverbs from the morals of the top- mstories and
lemmatize them as the retrieved concepts. We feed
the concepts together with the original input to
the input encoder. For example, the retrieved con-
cepts for the story in Table 1 include “support” and
“strength”, which may serve as additional guidance
for models’ prediction.
The retrieval-augmented algorithm can easily
adapt to the generation tasks. For2, we take
the input story paired with the retrieved concepts
into the encoder and then generate the output us-
ing the decoder. And for2, we use the input
moral as the query to retrieve top- mstories, and
regard their outlines as the retrieved additional in-
formation to guide the subsequent story generation.
5 Experiments
5.1 Evaluated Models
We evaluated the following baselines for the un-
derstanding tasks: BERT (Devlin et al., 2019),RoBERTa (Liu et al., 2019) and T5(Raffel et al.,
2020). When evaluating T5, we feed the input to
both the encoder and decoder of T5 and optimize
the model using the cross-entropy loss. To investi-
gate potential biases of the proposed datasets, we
added a baseline called BERT w/o story , which is
ﬁne-tuned to make prediction without taking the
story as input. For the generation tasks, we evalu-
ated ConvS2S (Gehring et al., 2017), Fusion (Fan
et al., 2018), GPT2 (Radford et al., 2019) and T5,
which are trained or ﬁne-tuned with the standard
language modeling objective. Moreover, we also
evaluate a task-speciﬁc model PlotMachines (PM
for short) (Rashkin et al., 2020), which is pro-
posed for tackling outline-conditioned generation
by tracking the dynamic plot states. We use GPT2
as the backbone model of PM.
We also design models to test the adaption of the
unlabeled data of to the proposed tasks.
Speciﬁcally, we ﬁrst post-train RoBERTa and T5 on
the unlabeled data with their original pretraining ob-
jectives, respectively (i.e., masked language model
and text inﬁlling) and then ﬁne-tune them on the
labeled data for the downstream tasks (Gururangan
et al., 2020). We call the baselines RoBERTa-Post
andT5-Post . We perform our retrieval-augmented
algorithm based on the post-trained models, called
RA-RoBERTa andRA-T5 , respectively.
5.2 Experiment Settings
We implement the pretrained models based on
the codes and pretrained checkpoints of Hugging-
Face’s Transformers (Wolf et al., 2020). We use
LongLM(Guan et al., 2022) as the T5 model
for experiments on -, and set all pre-
trained models to the base version due to lim-
ited computational resources. As for the hyper-
parameters, we set the batch size to 16, the maxi-
mum sequence length to 1,024, the learning rate to
3e-5,mto 10 for our retrieval-augmented model.
We generate outputs using top- ksampling (Fan
et al., 2018) with k= 40 and a softmax temper-
ature of 0.7 (Goodfellow et al., 2016). We show
more details in Section B.1.
5.3 Automatic Evaluation
Evaluation Metrics We adopt accuracy to eval-
uate the understanding tasks. For generation tasks,
we do not use perplexity since perplexity scores
are not comparable among models with different
vocabularies. We adopt the following metrics for
automatic evaluation: (1) BLEU (B- n):It is used5074to measuren-gram overlaps ( n= 1,2) between
generated and ground-truth texts (Papineni et al.,
2002). (2) BERTScore-F1 (BS): It is used to mea-
sure the semantic similarity between generated and
ground-truth texts (Zhang et al., 2019). (3) Repe-
tition (R-n):It calculates the ratio of texts that re-
peat at least one n-gram in all generated texts (Shao
et al., 2019). (4) Distinct (D- n):It measure the di-
versity using the percentage of distinct n-grams to
alln-grams in generated texts (Li et al., 2016). For
both R-nand D-n, we setn= 2 for2and
n= 4 for2considering the much shorter
length of morals than stories. Besides, we also re-
port the average number of generated words ( Len).
We also adopt the following metrics for auto-
matic evaluation of2:(1) Coverage (Cov):
It computes Rouge-L recall (Lin, 2004) between
generated stories and phrases in the corresponding
outlines. A higher score means the generated sto-
ries cover more phrases in the given outlines. (2)
Order (Ord): It measures the disparity between
the positional orders of given phrases in the ground
truth and generated story using the percentage of in-
versions in the generated story (Guan et al., 2022).
An inversion is a position pair of two phrases that is
out of the ground-truth order. Higher order scores
mean that the stories arrange the outline more rea-
sonably. In Section B.2, we also construct a learn-
able automatic metric to measure the faithfulness
between morals and stories.
Results Table 6 and 7 show the results on the
understanding and generation tasks, respectively.
To get the human performance onCand-
P, we sampled 100 examples from the test set
and recruited three annotators with good Chinese
or English language proﬁciency to complete these
tasks. We made ﬁnal decisions among the annota-
tors through major voting. The annotation results
show an almost perfect agreement with Fleiss’s
κ>0.85(Fleiss and Joseph, 1971).
We summarize the results on the understanding
tasks as follows: (1)ThePdatasets suffer
from innate biases as indicated by the high accu-
racy of BERT w/o story. Such biases may result
from the noise introduced by the automatic con-
struction technique, i.e., antonym substitution. And
models may learn patterns of good behavior (e.g.,
“unity” is good and “disunity” is bad in general)
and make predictions easily without depending on
stories. However,P is still meaningful as
an evaluation task since BERT can achieve much
better accuracy when taking stories as input. And
we experiment using manually constructed exam-
ples for evaluating preference alignment in the ap-
pendix. (2)T5 performs better than RoBERTa onCbut worse onP, indicating T5 may
not be good at capturing value preferences. (3)
Post-training on the unlabeled data (i.e., RoBERTa-
Post and T5-Post) does not always bring improve-
ment on both tasks, suggesting that it is necessary
to develop a better way to exploit these data in
future work. (4)Retrieving additional concepts
improves models’ performance effectively, particu-
larly for theCtask on -. However,
there is still a big gap between our models and hu-
man performance.
As for the generation tasks, we draw the follow-
ing conclusions: (1)Almost all pretrained models
achieve better lexical and semantic similarity with
ground-truth texts than non-pretrained models, as
indicated by higher BLEU and BERTScore val-
ues.(2)Non-pretrained models have less repetition
than pretrained ones, and repeat even less than the
ground-truth texts when generating morals. It may
be because non-pretrained models generate shorter
sequences than pretrained models despite the same
decoding algorithm, which also accounts for the
higher distinct scores of the non-pretrained models
on the2task. (3)When generating stories,
T5-Post can cover more input phrases and arrange
them in a correct order than other baselines, as in-
dicated by higher coverage and order scores. (4)
Retrieval augmentation can improve the genera-
tion similarity with the ground-truth texts on both
tasks and improve the coverage and order scores
on2signiﬁcantly compared with T5-Post.5075
5.4 Manual Evaluation
On the generation tasks, we conducted a Likert-
scale based manual evaluation to measure the gap
between existing models and humans. For -, we hired three graduate students (native Chi-
nese speakers) as annotators. We conducted eval-
uation on -using Amazon Mechanical
Turk (AMT). For both tasks, we randomly sampled
100 examples from the test set, and obtained 300
generated texts from Fusion, T5 and RA-T5. For
each text we require three annotators to rate its qual-ity along with the input using a binary score in three
following aspects: (1) linguistic ﬂuency : correct-
ness in grammaticality; (2) coherence : reasonable
relations between sentences regarding relatedness,
causality and temporal orders; and (3) moral faith-
fulness : exhibition of a faithful moral to the input.
Three aspects are independently evaluated. We
decided the ﬁnal score of a text through majority
voting. The annotation instruction is shown in Sec-
tion B.3.
Table 8 shows the manual evaluation results. We
showp-values of the results in Section B.4. For2, T5 achieves a substantial improvement
compared with Fusion ( p<0.01), and our model
further outperforms T5. The superiority becomes
less signiﬁcant for2. However, the big gap
between these models and humans, particularly in
terms of faithfulness, proves both tasks challeng-
ing for existing models. Furthermore, we evaluate
whether machines can capture the value preference
of a story using manually constructed examples.
And we show error analysis and case study for the
proposed tasks in Section C. We believe that ex-
plicit modeling of the relations among events and
abstract concepts will further promote progress on
these tasks, which we regard as future work.
6 Conclusion
We present , a collection of Chinese and
English moral stories. To test the ability to bridge5076concrete events and abstract morals, we propose
new understanding and generation tasks based on , including selecting the correct moral
from several candidates with different topics or
opposite value preferences, concluding the moral
of a story and generating a story to convey a moral.
Extensive experiments prove these tasks still to
be challenging for existing models. We propose
a retrieval-augmented algorithm to improve per-
formance by retrieving related concepts or events
from training sets. Although it is possible to fur-
ther increase the dataset size, we expect to make
meaningful progress by developing better repre-
sentations of commonsense and discourse relations
among events and abstract concepts in future work.
7 Acknowledgement
This work was supported by the National Science
Foundation for Distinguished Young Scholars (with
No. 62125604) and the NSFC projects (Key project
with No. 61936010 and regular project with No.
61876096). This work was also supported by the
Guoqiang Institute of Tsinghua University, with
Grant No. 2019GQG1 and 2020GQG0005. This
work was also sponsored by Tsinghua-Toyota Joint
Research Fund. We would also like to thank the
anonymous reviewers for their invaluable sugges-
tions and feedback.
8 Ethics Statements and Broader Impact
We collected from public web resources.
All stories are under licenses that allow use and
redistribution for research purposes. We asked
commercial annotation teams to extract stories and
morals from the crawled raw texts. We required
the annotators to refuse the examples which violate
general ethical principles (e.g., showing discrimina-
tion for someone, containing disrespectful content,
or encouraging to disturb public order, etc.). To-
tally, we payed more than $7 (CNY 45) per hour on
average for annotating each example in ,
which was far beyond the minimum hourly wage
in China (CNY 21). Furthermore, we resorted
to AMT for manual evaluation of generated and
human-written texts for two proposed generation
tasks. We hired three annotators and payed each
annotator $0.2 on average for annotating each ex-
ample.
In this paper, we emphasize the ability to model
relations between concrete events and abstract
morals, which is also helpful for various scenar-ios such as reading comprehension (e.g., drawing
authors’ viewpoints from narratives) and essay writ-
ing (e.g., writing essays to convince readers of
some arguments by presenting examples or anec-
dotes). provides a good start point for
exploring these directions.
References507750785079
A Construction
A.1 Data Source
We show the full list of web pages used for con-
structing in Table 11. We initially collect
52,017 Chinese and 2,630 English raw texts from
the web pages. Then we de-duplicate the texts by
removing those texts which overlap with others
more than twenty words. After de-duplication, we
ﬁnally collected 19,197 Chinese and 2,598 English
texts. And we construct based on these
texts.A.2 Data Annotation
Table 9/10 shows the examples given to the an-
notators to inform them of the requirements for
stories/morals, respectively. If the constraints are
not met, we ask annotators to reﬁne the story and
moral. All workers were paid more than $7 per
hour on average.5080
A.3 Analysis of High-Frequency Words
To investigate the topic features of ,
we count the top 50 most frequent nouns in (excluding stop words) as shown in Fig-
ure 3. We roughly categorize these words into four
types: (1) Animals : animals are popular as pro-
tagonists in moral stories since they usually have
various but clear characteristics (e.g., “sly foxes”),
which embody rich commonsense knowledge; (2)
Relationships : such nouns are used to describe
the inter-character relationships in a story (e.g.,
“friend”), which are useful for modeling charac-
ters’ motivation and behavior; (3) Concrete nouns :
they refer to physical entities that can be observed,
such as “water”; and (4) Abstract nouns : they re-fer to abstract concepts, such as “difﬁculty”. We
manually check the proportional distribution of the
four types for stories and morals, respectively. The
results in Figure 3 demonstrate that morals contain
signiﬁcantly less concrete nouns and more abstract
nouns than stories. And morals contain little animal
words but almost as many relationship words as sto-
ries, indicating that morals may be independent of
speciﬁc characters but relate to general interper-
sonal relations. The result shows that morals are
more abstract than stories.
Furthermore, Table 12 shows the most frequent
4-grams in , further indicating that morals
are more abstract than stories. Each of the 4-grams
in Table 12 comprises less than 0.01% of all 4-5081
grams in the corresponding dataset, showing the
diversity of .
A.4 Discussion about
The high-quality examples in are full of
commonsense and discourse relations. As exem-
pliﬁed in Table 1 in the main paper, the common
sense is mainly regarding the characters’ reaction
and intention (e.g., “the cows dispersed” and then
the “tiger” and “lion” intend to kill them), as well
as the nature of physical objects and abstract con-
cepts (e.g. “cows” may be the food of “lions” and
“tigers”, and “unity” refers to “keeping together for
a common goal”). Additionally, the stories usually
have a speciﬁc discourse structure, i.e., the premise
to introduce the story settings (e.g., the characters
“four cows” and the location “a meadow”), the right
or wrong behavior (“stay together or not”) and the
endings (“living well or being killed”). We believe
it is an essential topic of future work to develop a
better approach to model such commonsense and
discourse relations.
B Experiments
B.1 Implementation
We implement the pretrained models used in our
experiment mainly based on the register models ofHuggingFace (Wolf et al., 2020). Table 13 shows
the names of the used register models. Note that
we use LongLM(Guan et al., 2022) as the T5
model for experiments on -, which has
not been registered on HuggingFace.
All results in the main paper and the appendix
are based on one NVIDIA Tesla v100 (16G mem-
ory). All reported results are based on one single
running. The CPU is Intel Xeon Gold 5218. It cost
less than 5 hours for ﬁne-tuning each model on . We set the hyper-parameters following
the default parameters of HuggingFace.
B.2 Automatic Evaluation for Moral
Faithfulness
We follow Guan and Huang (2020) to train a learn-
able metric to evaluate moral faithfulness. Speciﬁ-
cally, we ﬁne-tune RoBERTa as a classiﬁer to
distinguish whether a story matches a moral. We
regard ground-truth examples as positive where
the story and moral are matched, and construct
negative examples by replacing the story or moral
with a randomly sampled one. Finally, the classi-
ﬁer achieves an accuracy of 77.32/79.21% on the
data constructed based on the test set of -/ -respectively. Then we calculate
the faithfulness score as the average classiﬁer score
of all generated texts for the inputs.
Table 14 presents the evaluation results. We
can see that pretrained models achieve better faith-
fulness than the non-pretrained models as shown
by the much higher faithfulness scores. However,
we also observe that the faithfulness score of the
ground-truth texts is lower than some models (e.g.,
T5) when generating morals. Therefore, it is still
necessary to manually evaluate faithfulness.
Results on Validation Sets We show the perfor-
mance of several baselines and RA-T5 on the vali-
dation sets of the understanding tasks and the gen-
eration tasks in Table 15 and Table 16, respectively.
B.3 Manual Evaluation Instruction
We show the manual annotation interface in Fig-
ure 4. To ensure that the annotators guarantee a
consistent standard in the annotation process, we
asked annotators to rate four examples with the
same input at the same HIT (human intelligence
task). In these four examples, one is written by
humans and three are generated by models (i.e.,
Fusion, T5 and RA-T5). We payed each annotator
$0.2 on average for annotating each example.5082
B.4 Signiﬁcance of Manual Evaluation
Results
Table 17 shows the p-values (sign test) when com-
paring the manual evaluation results (Table 8 in the
main paper) between each pair of the ground truth,
Fusion, T5 and RA-T5.
B.5 Evaluating Value Preference Alignment
Although we have usedP to evaluate
whether machines can capture the value preference
of a story, the automatically constructed dataset
may bias machines to focus on distinguishing gen-
eral standards of good behaviour without consider-
ing story plots. Therefore, in this section, we con-
struct examples manually to test this ability beyond
the token level. Speciﬁcally, we randomly sampled
50 examples from the test sets of -and -respectively. For each example, we
manually rewrote the moral to convey a synony-
mous or antonymous value preference. For exam-
ple, a synonymous moral with “unity is strength”
in Table 1 can be “we are powerful as long as we
unite with each other” and an antonymous one can
be “everyone can also be powerful enough.” Then
we expect a model to be able to accept the synony-
mous moral but reject the antonymous one. We use
three typical models, including BERT w/o Story,
RA-RoBERTa and RA-T5, to compute the winning5083
rate of pair-wise comparisons between any two of
ground-truth, synonymous and antonymous morals.
These models are trained on the training set of thePtask.
Table 18 shows the evaluation results. We ob-
serve that BERT can not distinguish different types
of morals without input stories. RA-RoBERTa
fails to accept the synonymous morals on -(winning rate of only 36% w.r.t the ground truth,
p < 0.1), and can not distinguish synonymous
and antonymous morals on both -and -(winning rate near 50% with p>0.1).
Additionally, it prefers antonymous morals to the
ground truth signiﬁcantly on both datasets (winning
rate less than 50% and p <0.1). The results in-
dicate that existing models still struggle to capture
the value preference of moral stories.
C Error Analysis and Case Study
In this section, we conducted a case study and in-
vestigated the errors of existing models on the pro-
posed tasks to provide insight into future work. We
show several typical error cases in Table 20.
C.1 Understanding Tasks
The example in Table 20 forCshows that
the model may not grasp abstract concepts such
as “good will” and “good acts” and align them
to the story plots. It makes predictions possibly
based on only token-level features such as relations
between “ask after” and “attention”. On the other
hand, the example forPindicates that the
model can not capture the value preference of the
story in terms of “whether it is intelligent to regard
others are illiterate”. The results demonstrate the
necessity of introducing concept knowledge and
modeling high-level semantic information.
C.2 Generation Tasks
Table 21 shows cases generated by several base-
lines and our model for the generation tasks. We
can see that retrieval can provide effective guid-
ance for both moral and story generation. Baseline
models including GPT2 and T5 tend to generate
unrelated concepts or non-moral texts.
However, as shown by the manual evaluation
results, there is still a big gap between RA-T5 and
humans. To provide quantitative error analysis,
in the process of manual evaluation on -, we required annotators to annotate the error
type of a text when it exhibit an unfaithful moral.
We summarize three main error types as follows:
(1) Not a moral text ( ): not stating or imply-5084
ing what is right or what is wrong; (2) Unrelated
concepts ( ): containing unrelated concepts
with the input; and (3) Conﬂicting value prefer-
ence ( ): conveying a value preference con-
ﬂicting with the input despite related concepts. In
addition, we also provide annotators with another
option Others . The annotators are allowed to an-
notate a text with multiple errors. When at leasttwo of three annotators annotate the text with some
error, we decide it has the error. We show the dis-
tribution of the error types in Table 19, suggesting
that existing models still struggle to generate mean-
ingful morals and stories, and align the concepts
and value preferences between them.
Furthermore, as exempliﬁed in Table 20, when
generating morals, we can see from Case 1 that the5085
models still often state events involved with spe-
ciﬁc characters (e.g., “owls” ) but do not tell what
is right and what is wrong. And Case 2 shows that
they struggle to conclude related concepts from the
story (e.g., “greedy” is not embodied in the story at
all). Furthermore, in Case 3, the models conclude aconﬂicting value preference with the story despite
correct concepts (e.g., the story shows that “it is
bad to be scared” but not “good” ). On the other
hand, models also are shown to suffer from simi-
lar issues when generating stories. In Case 4, the
model only describes some scenes (e.g., “it was5086very big” and“it was very powerful” ) but does not
aims to convince readers of anything. And Case
5 seems to tell a story centered on some concepts
such as “active” and“busy” , but the concepts do
not relate to the input. Case 6 implies “empty so-
lutions may be useful, ” which is conﬂicting with
the input. These cases indicate the necessity of
modeling the relations between events and abstract
concepts for understanding and generating moral
stories.5087