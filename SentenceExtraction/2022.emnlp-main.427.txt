
Bowen Shi
TTI-Chicago
bshi@ttic.eduDiane Brentari
Univeristy of Chicago
dbrentari@uchicago.edu
Greg Shakhnarovich
TTI-Chicago
greg@ttic.eduKaren Livescu
TTI-Chicago
klivescu@ttic.edu
Abstract
Existing work on sign language translation—
that is, translation from sign language videos
into sentences in a written language—has fo-
cused mainly on (1) data collected in a con-
trolled environment or (2) data in a specific
domain, which limits the applicability to real-
world settings. In this paper, we introduce Ope-
nASL, a large-scale American Sign Language
(ASL) - English dataset collected from online
video sites (e.g., YouTube). OpenASL contains
288 hours of ASL videos in multiple domains
from over 200 signers and is the largest pub-
licly available ASL translation dataset to date.
To tackle the challenges of sign language trans-
lation in realistic settings and without glosses,
we propose a set of techniques including sign
search as a pretext task for pre-training and fu-
sion of mouthing and handshape features. The
proposed techniques produce consistent and
large improvements in translation quality, over
baseline models based on prior work.
1 Introduction
Sign language, a type of visual language that con-
veys meaning through gestures, is the most widely
used form of linguistic communication among deaf
and hard of hearing people. According to the
World Health Organization, over 5% of the world’s
population ( ∼430 million people) suffer from dis-
abling hearing loss.Automatic sign language pro-
cessing can facilitate the daily activities of deaf
people and make artificial intelligence technolo-
gies more accessible to deaf users. For example,
such techniques would allow deaf users to interact
with intelligent virtual assistants using sign lan-
guage and would support automatic interpretation
between sign languages and spoken languages. In-
terest in sign language research has recently beengrowing in the computer vision (CV) (Bragg et al.,
2019; Adaloglou et al., 2021; Rastgoo et al., 2021)
and natural language processing (NLP) communi-
ties (Shterionov, 2021; Yin et al., 2021)
In this paper, we study sign language transla-
tion (SLT),the task of translating continuous sign-
ing video into written language sentences. Un-
like other sign language processing tasks such as
sign spotting (Buehler et al., 2009) or continuous
sign language recognition (sign-to-gloss transcrip-
tion) (Dreuw et al., 2007), SLT has not been stud-
ied until recently (Camgoz et al., 2018) and is
still restricted to specific domains (e.g., weather
forecasts (Camgoz et al., 2018), emergency situ-
ations (Ko et al., 2019)), characterized by small
vocabulary size and lack of visual variability. The
lack of large-scale translation datasets in the wild is
a central challenge in developing SLT technologies
serving real-world use cases.
In terms of translation modeling, most existing
SLT approaches (Camgoz et al., 2018, 2020a,a;
Zhou et al., 2021; Yin et al., 2021; Gan et al., 2021;
Chen et al., 2022) rely on glosses, which are a
transliteration system for sign language. Annotat-
ing sign language videos with glosses is expensive
and hard to scale up. Building effective methods for
SLT without glosses is an under-studied challenge.
In this work, we introduce OpenASL, a large-
scale ASL-English translation dataset. OpenASL
has288hours of real-world ASL videos from over
200 signers, making it the largest ASL-English
translation dataset to date. OpenASL covers multi-
ple domains drawn from a mix of news and VLOGs.
To handle challenges in SLT modeling without
glosses, we propose a set of techniques including
pre-training with spotted signs and fusion of mul-
tiple local visual features, which improves over
existing SLT baselines by a large margin.6365
2 Related Work
2.1 Datasets for SLT
There has been a large body of work collecting sign
language corpora in general. Here we mainly focus
on video-based datasets that can be used for SLT
(see Table 1), which contain paired continuous
signing videos and sentences in a written language.
Most of the early datasets (Wilbur et al., 2006;
Dreuw et al., 2007) were collected in a studio-like
environment, where native signers are asked to sign
some given content. Recording conditions such as
lighting, background and camera perspectives are
carefully controlled in such datasets. These corpora
provide valuable resources, but do not account for
real-world conditions, which has been noted as a
limiting factor in recent work on sign language (Yin
et al., 2021). Moreover, the high cost of data collec-tion also makes studio-based datasets hard to scale
up.
With the advancement of computer vision tech-
niques, there is increasing attention on collecting
real-life SLT datasets. Many such datasets (Cam-
goz et al., 2018, 2021; Albanie et al., 2021) are
drawn from TV programs accompanied by sign
language interpretation. Despite being highly re-
alistic compared to studio datasets, they are gen-
erally limited to a specific domain. For example,
the popular Phoenix-2014T DGS-German bench-
mark contains signed German weather forecasts
and includes only 11 hours of signing videos from
9 signers. The largest real-world sign language
corpus we are aware of is BOBSL (Albanie et al.,
2021), which consists of 1,467 hours of BBC broad-
casts from 39 signers interpreted into British Sign
Language (BSL). However, access to the dataset is
heavily restricted.
Unlike prior datasets, OpenASL contains a mix
of spontaneous and (presumably) interpreted sign
language videos. It is collected from online video
sites and thus contains a diverse set of signers and
domains. In addition, the annotations we provide
are fully accessible to the public.
2.2 Methods for SLT
Direct translation from videos of continuous sign-
ing is practically appealing and has received grow-
ing interest recently. Ko et al. (2019) study transla-
tion of common Korean sign language sentences (in
video) that may be used in an emergency scenario.
In this specific domain with restricted vocabulary
size (419 words), the model can achieve BLEU-4
score higher than 60. In a larger-vocabulary set-
ting, Camgoz et al. (2018) study translation of Ger-
man sign language weather forecast videos under
various labeling setups. In particular, one of their
main findings is the drastic improvement achieved
when using gloss labels in training an SLT model.
It is hypothesized in (Camgoz et al., 2020b) that
glosses, as an intermediate representation of sign
language, can provide more direct guidance in6366learning sign language video representation. There-
fore, most followup work (Camgoz et al., 2020b;
Chen et al., 2022; Zhou et al., 2021; Yin and Read,
2020) largely relies on gloss sequences in training.
Given the high cost of gloss labeling, con-
ducting gloss-free SLT is practically appealing
but introduces modeling challenges. Glosses,
which are monotonically aligned to the video,
provide stronger supervision than text in writ-
ten language translation and facilitate learning of
a more effective video representation. On the
Phoenix-2014T benchmark, a model trained with-
out glosses (Camgoz et al., 2018) falls behind its
counterpart with glosses by over 10.0 (absolute)
BLEU-4 score (Camgoz et al., 2020b). Improv-
ing translation in real-world sign language video
without gloss labels is the modeling focus of this
paper. There is little prior work addressing SLT
without glosses. In a gloss-free setting, Li et al.
(2020b) study the use of segmental structure in
translation to boost translation performance. Or-
bay and Akarun (2020) incorporate handshape fea-
tures into the translation model. In this paper, we
consider sign spotting pre-training and fusion of
multiple local features for gloss-free translation.
A typical SLT model is composed of a visual
encoder and a sequence model. The visual encoder
maps input video into intermediate visual features.
In (Camgoz et al., 2018), a sign recognizer CNN-
LSTM-HMM trained with gloss labels was used
to extract image features. The continuous sign
recognizer was replaced by a CTC-based model
in (Camgoz et al., 2020b). In addition to RGB-
based images, pose is also used (Ko et al., 2019;
Gan et al., 2021) as a complementary input modal-
ity, which is commonly encoded by graph convo-
lutional neural networks. The sequence models
in SLT are usually based on standard sequence-to-
sequence models in machine translation with either
recurrent neural networks (Camgoz et al., 2018) or
transformers (Camgoz et al., 2020b; Yin and Read,
2020; Chen et al., 2022) as the backbone.
2.3 Other related work
Two key components of our proposed approach are
searching for spotted signs from video-sentence
pairs and fusing multiple local visual features.
There has been a substantial amount of prior
work (Buehler et al., 2009; Albanie et al., 2020;
Varol et al., 2021; Momeni et al., 2020; Shi et al.,
2022a) devoted to spotting signs in real-world signlanguage videos. In contrast to this prior work
where sign search is the end goal, here we treat
sign spotting as a pretext task in the context of SLT.
The use of multi-channel visual features has also
been previously explored for multiple tasks, in-
cluding sign spotting (Albanie et al., 2020) and
continuous sign language recognition (Koller et al.,
2020). Specifically for SLT, Camgoz et al. (2020a)
learn a multi-channel translation model by includ-
ing mouthing and handshape features. However,
these local modules are trained with in-domain
data whose labels are inferred from glosses, which
makes it inapplicable for gloss-free translation. In
contrast, we utilize models pre-trained on out-of-
domain data to extract local features and study the
effect of feature transfer to translation.
3 The OpenASL Dataset
Our videos are collected from video websites,
mainly YouTube. A large portion of our data con-
sists of ASL news, which come primarily from the
YouTube channels TheDailyMoth andSign1News .
We download all videos with English captions in
these two channels through June 2021. The rest of
the dataset is collected from short YouTube videos
uploaded by the National Association of the Deaf
(NAD). Those videos are mostly in the form of sign
VLOGs of various types including announcements,
daily tips, and short conversations.
The raw video is divided into roughly sentence-
sized clips based on the associated subtitles. Specif-
ically, we split the transcript into sentences with
the NLTKsentence segmentation tool and retrieve
the corresponding (video clip, sentence) pairs. This
procedure produces 98,417 translation pairs in to-
tal, with 33,549 unique words. Figure 4 shows
the distribution of sentence length in our data. We
randomly select 966 and 975 translation pairs from
our data as validation and test sets respectively.
The annotation of the validation and test sets is
manually verified. Specifically, the English trans-
lation and time boundaries of each video clip are
proofread and corrected as needed by professional
ASL interpreters. Each annotator views the video
clip and is given the original English sentence from
the subtitle for reference. The annotator marks the
corrected beginning and end of the sentence, and
provides a corrected English translation if needed
as well as the corresponding gloss sequence. Dur-
ing translation of each sentence, the annotator has6367access to the whole video in case the context is
needed for accurate translation.
Figure 2 shows the distribution of several proper-
ties in our dataset. Note that these are not ground-
truth labels, but rather approximate labels as per-
ceived by an annotator. The goal is to give an idea
of the degree of diversity in the data, not to provide
ground-truth metadata for the dataset. The label
"other" covers a variety of categories, including
examples where the annotator is unsure of the label
and examples that contain multiple signers.
One feature of our data is the use of subtitles
associated with the video as the English transla-
tion, thus saving effort on human annotation. Sub-
titled videos have also been employed in prior
work (Camgoz et al., 2021; Albanie et al., 2021) for
constructing sign language datasets. As prior work
has mostly focused on interpreted signing videos
where content originally in the spoken language is
interpreted into sign language, the subtitles used
there are naturally aligned to the audio instead of
the signing stream. As is shown in (Bull et al.,
2021), there exists a large time boundary shift be-
tween the two. The videos used in OpenASL are
"self-generated" rather than interpreted, so the En-
glish subtitles are already aligned to the video accu-
rately. As can be seen from Figure 3, the sentence
alignment in the subtitles is of overall high quality
(usually less than 2 second time shifts, although a
small percentage ( <5%) are larger).
We measure the degree of agreement between
the original and corrected translations using BLEU
score (Papineni et al., 2002). The original transla-
tion achieves 81.0 BLEU-4 score when it is com-
pared against the corrected one. The high agree-
ment in translation, as well as the small alignment
error from Figure 3, shows the overall high quality
of the subtitles. Thus to save annotation effort, we
do not proofread the training data.
4 Model and pre-training for gloss-free
translation
A translation model maps a sequence of Timage
frames Ito a sequence of nwords w. In
the most recent state-of-the-art approaches (Cam-
goz et al., 2020b; Li et al., 2020b) for SLT, a vi-
sual encoder Mfirst maps Ito a visual feature
sequence f, and a transformer-based sequence-
to-sequence model decodes fintow. Our
approach is based on the same overall architecture
(see Figure 6). We further incorporate several tech-
niques for pre-training and local feature modeling,
described next.
4.1 Sign spotting pre-training
ForM, we use an inflated 3D convnet (I3D) de-
veloped for action recognition (Carreira and Zis-
serman, 2017). Ideally, the visual encoder should6368capture signing-related visual cues (arm movement,
handshape, and so on). However, the translated
sentence in the target language may not provide
sufficiently direct guidance for learning the visual
representation, as is observed in prior work (Cam-
goz et al., 2018).
To alleviate this issue, we pre-train the I3D
network on relevant tasks that provide more di-
rect supervision for the convolutional layers than
full translation. Specifically, we pre-train I3D
for the task of isolated sign recognition on WL-
ASL (Li et al., 2020a), a large-scale isolated ASL
sign dataset. Empirically, we observe considerable
gains from isolated sign recognition pre-training
(see Section A.7).
Despite the aforementioned benefits, the isolated
sign recognition pre-training causes two potential
problems for the translation model. First, there
is substantial domain mismatch between isolated
signs and the continuous signing data used in trans-
lation. The coarticulation in a continuous sign-
ing stream is not reflected in isolated sign datasets.
In addition, the isolated sign videos are collected
from sources such as online lexica, which usually
have simpler visual backgrounds and less motion
blur than real-world signing video. Finally, exist-
ing isolated sign datasets mainly consist of lexical
signs and have few instances of fingerspelling. Fin-
gerspelling is used frequently in day-to-day sign-
ing and many important content words are com-
monly fingerspelled. Features related to finger-
spelling may not be encoded well due to the lack
of fingerspelling-specific pre-training data.
To mitigate the above issues, we propose to
search for signs from the signing video (see Fig-ure 5). The searched signs are used to pre-train
the visual backbone for translation. The search
relies on a lexical sign recognizer Mand a fin-
gerspelling recognizer M, which map a video
segment into a word probability vector p∈[0,1]
(V: vocabulary size) and a letter sequence c.
Given a translation video-sentence pair ( I,w),
the task is to spot lexical andfingerspelled signs
P={(I, w)}, where the ware se-
lected from w. The search process is described
briefly below (see Section A.3 for details).
We generate a list of candidate time intervals
for lexical signs and fingerspelling signs respec-
tively with a sliding window approach and a finger-
spelling detector M. For each interval, we infer
its word probability pfor lexical signs or word
hypothesis (i.e., a sequence of characters) ˆwfor
fingerspelling. We assign a word from the trans-
lated sentence to the target interval if the word
probability pis high or its edit distance with the
fingerspelling hypothesis is low.
Unlike the isolated sign dataset, the spotted signs
are sampled from the same data used for translation
training. Additionally, the detected fingerspelling
signs should also enhance the model’s ability to
transcribe signs that are fingerspelled.
4.2 Hand and mouth ROI encoding
In sign language, meaning is usually conveyed via a
combination of multiple elements including motion
of the arms, fingers, mouth, and eyebrows. The cor-
responding local regions in the image frame play an
important role in distinguishing signs. For instance,
SENATE and COMMITTEE have the same place
of articulation and movement; the difference lies
only in the handshape. Furthermore, mouthing (i.e.,
mouth movement) is commonly used for adjectives
or adverbs to add descriptive meaning (Nadolske
and Rosenstock, 2008).
Our model’s visual backbone does not explicitly
employ local visual cues. In principle, learned
global features can include sufficient information
about the important local cues, but this may require
a very large amount of training data. However, it
may be helpful to guide the translation model more
explicitly by learning local discriminative features
using external tasks.
Here we focus on learning features for two lo-
cal visual modalities: handshape and mouthing.
To extract handshape features, we train a fin-6369gerspelling recognizeron two large-scale finger-
spelling datasets (Shi et al., 2019) and use it to ex-
tract features for the hand region of interest (ROI).
ASL fingerspelling includes many handshapres that
are also used in lexical signs. Recognizing finger-
spelling requires distinguishing quick hand motions
and nuance in finger positions. The features are
extracted for both hands in each frame and are con-
catenated before feeding into the translation model.
We denote the hand feature sequence as f, where
Tis the video length in frames.
For mouthing, we use an external English lip-
reading model(Shi et al., 2022b) to extract fea-
turesffrom the lip regions of the signer. Al-
though mouthing in ASL is not used to directly
"say" words, we assume there is sufficient shared
lip motion between speaking and ASL mouthing.
4.3 Fusion and sequence modeling
Given the global/handshape/mouthing feature se-
quences f/f/f, the sequence model maps
them to text w, as illustrated in Figure 6. Since
we have multiple feature sequences each with its
own sequential properties, we adopt three indepen-
dent transformer (Vaswani et al., 2017) encodersfor the three types of features:
e=TF-Enc(f), x∈ {g, m, h }
where TF-Enc,TF-Enc,TF-Encdenote
the transformer encoders for global, mouthing and
hand feature sequences respectively.
For decoding, we use a single transformer de-
coder that takes all three encoder representations
as input. At decoding timestep n, we compute
modality-specific context vectors:
c=Cross-Attn(d,e), x∈ {g, m, h }
where Cross-Attn, Cross-Attnand
Cross-Attnare cross-attention layers (Vaswani
et al., 2017) for global/mouthing/hand features.
We concatenate the context vectors from the
three modalities to form the decoder context
vector c= [c,c,c], which is passed
to a feedforward layer and then the next layer.
The final layer output is then passed to a linear
projection, followed by a final softmax to produce
a probability vector over words in the vocabulary.
5 Experiments
5.1 Setup
For evaluation, we report BLEU-{1,2,3,4} (Pap-
ineni et al., 2002) and ROUGE (Lin, 2004) scores,
as in prior work on SLT (Camgoz et al., 2018; Ko
et al., 2019; Camgoz et al., 2021). As there is
only one English sentence as reference for evalua-
tion, we also report BLEURT (Sellam et al., 2020)
score, a metric that provides a measure of seman-
tic similarity between the prediction and ground
truth. Implementation details can be found in the
appendix (Section A.4).
5.2 Main Results
The performance of our proposed approach is
shown in Table 2. We compare it to two base-
line approaches adapted from prior work. Conv-
GRU, which uses ImageNet-pretrained AlexNet as
a visual backbone, is an RNN-based sequence-to-
sequence model proposed by Camgoz et al. (2018)
for sign language translation without glosses. I3D-
transformer is a similar model architecture to ours,
but it uses only global visual features and the CNN
backbone is pre-trained only on the WLASL iso-
lated sign recognition task. See Section A.5 in the
appendix for the performance of these two baseline6370
methods on the popular DGS-German benchmark
Phoenix-2014T.
From the results in Table 2, we observe: (1)
Conv-GRU has the worst performance among the
three models. One key difference lies in the data
used to pre-train the visual encoder: Conv-GRU is
pre-trained on ImageNet while the latter two are
pre-trained on sign language-specific data. There
are, of course, also differences in the model archi-
tecture and training pipeline. To isolate the effect
of sign language-specific pre-training, we compare
I3D-transformer pre-trained with different types of
data, and find that isolated sign pre-training leads
to consistent gains. See Section A.7 in the Ap-
pendix for details. (2) Our proposed approach
achieves the best performance. On average, the
relative gain over I3D transformer is ∼15% in
ROUGE and BLEU scores. This demonstrates the
effect of including spotted signs in visual backbone
pre-training and of incorporating the multiple local
visual features. (3) The performance measured by
BLEU, ROUGE and BLEURT scores are consis-
tent for different models.
Despite the improvement over baseline ap-
proaches, our model’s performance is still quite
poor. We show some qualitative examples of trans-
lated sentence in Section A.9 of the Appendix. The
low performance of current translation models has
also been observed in prior work on other sign
languages (Albanie et al., 2021; Camgoz et al.,
2021), highlighting the challenging nature of sign
language translation.
In the next sections, we analyze the effects of the
main components in our model. For the purpose
of these analyses, we report BLEU and ROUGE
scores on the validation set.
5.3 Ablation Study
Effect of sign spotting pre-training In Table 3,
we compare the performance of models with dif-
ferent pre-training data: WLASL only, WLASL +
spotted lexical signs. For both models, the visualbackbone is I3D and we do not incorporate local
visual features.
The results show that sign search consistently
improves performance. Compared to training with
WLASL only, including lexical sign and finger-
spelling spotting produces ∼10% relative improve-
ments, averaged across metrics. We attribute these
gains to the adaptation of I3D to our translation
data, which includes coarticulation and visual chal-
lenges that the isolated sign data lacks.
An alternative strategy could be to fine-tune the
visual backbone on our translation data. However,
this strategy downgrades translation performance
by a large margin (see Section A.6 for details).
Qualitatively, the spotted sign pairs are high-quality
in general (see Section A.10).
Effect of local feature incorporation Table 4
compares models without local visual features, and
with both mouthing and handshape features. All
models are pre-trained with spotted signs. Over-
all the incorporation of local features produces 5%
gains in different metrics. The gain is relatively
larger in BLEU scores of lower orders (e.g., BLEU-
1). See Section A.10 for qualitative examples
of improved translation when using mouthing fea-
tures.63715.4 Analysis
For a more detailed analysis of our model, we mea-
sure its performance on different evaluation subsets,
divided by several criteria.
Duplicate vs. non-duplicate Certain sentences
appear frequently in our dataset, which leads to
duplicated sentences appearing in both training and
evaluation. The duplicate sentences account for
10.9% (105 out of 967) of the dev set and 10.6%
(103 out of 976) of the test set. Most of these are
sentences that are used frequently in the news, such
as "Hello", "Thank you", and "See you tomorrow".
Our model translates videos associated with du-
plicate sentences with high accuracy (see Figure 7).
On the duplicate subset, the BLEU-4 score is 72.91.
Duplicates tend to be short clips, which are easy for
the model to memorize. In contrast, the BLEU-4
score on the non-duplicate subset is only 4.09.
News vs. VLOGs Our data are collected from
online sign language resources from two cate-
gories: news ( Sign1News andTheDailyMoth ) and
VLOGs ( NAD). The two sources differ in multiple
aspects, including visual conditions and linguis-
tic content. In the dev set, videos from these two
categories account for 63.6%/36.4% of sentences
respectively. We break the performance down ac-
cording to the source (see Figure 8). To avoid the
impact of duplicate sentences, we also perform this
comparison separately on non-duplicate sentences.
Our model performs better on scripted news
videos regardless of whether the duplicates are in-
cluded or not, which may be attributed to multi-
ple factors. On the one hand, the data from NAD
VLOGs contain a larger set of signers than the news
videos. The variability in signing among different
signers increases the difficulty of translation. NAD
VLOG videos also have higher visual variance in
terms of image resolution and background diver-
sity. It is also possible that the news videos are
more likely to be scripted beforehand while the
VLOG videos are more likely to be spontaneous.Spontaneous ASL videos are expected to be more
challenging to translate than scripted videos.
Fingerspelling vs. non-fingerspelling In our
dev set, 54.7%of the clips have at least one finger-
spelled word. Our model’s translation performance
on the fingerspelling-free subset is overall higher
than on clips with fingerspelling (BLEU-4: 7.74
vs. 6.33). We expect that proper nouns, typically
fingerspelled in ASL, are difficult to translate for
our model. A more detailed analysis can be found
in Section A.8.
6 Conclusion
Our work advances sign language translation "in
the wild" (i.e., directly translating real-world sign
language videos into written language) both (1) by
introducing a new large-scale ASL-English trans-
lation dataset, OpenASL, and (2) by developing
methods for improved translation in the absence
of glosses and in the presence of visually challeng-
ing data. OpenASL is the largest publicly avail-
able ASL translation dataset to date. By using
online captioned ASL videos, we have been able
to collect a large amount of high-quality and well-
aligned translation pairs (as verified by professional
translators) that represent a wide range of signers,
domains, and visual conditions. Our translation
approach, which combines pre-training via sign
spotting and multiple types of local features, out-
performs alternative methods from prior work by a
large margin. Nevertheless, the overall translation
quality for sign language videos, in both our work
and prior work, is significantly lower than that of
machine translation for written languages. There
is therefore much room for future improvement,
and we hope that OpenASL will enable additional
progress on this task.6372Limitations
Despite being the largest ASL translation dataset
to date, OpenASL is still of relatively small scale
compared to commonly used translation corpora
for written languages. Due to resource constraints,
we provide only one English translation for each
video for the time being. Future work may aug-
ment the dataset with multiple English translations
per video. In addition, although we strive to
collect a diverse dataset, we do not have ground-
truth labels for signer gender, race, and handedness,
so we cannot be certain about the distribution of
these properties in OpenASL. In terms of method-
ology, the proposed lexical sign search relies on
the availability of isolated sign data. Moreover,
our approach may have difficulty in handling ASL
signs that do not have an equivalent word in En-
glish (e.g., PAH!). Finally, the overall quality of
English translations produced by our model is still
very low, which highlights the challenging nature
of open-domain sign language translation.
Ethics Statement
The copyright for all videos in our dataset belongs
to their respective owners. The video URL, times-
tamps and English translations are released under
a Creative Commons BY-NC-ND 4.0 license. Our
data are collected from online video sites, and the
signers may not be representative of the general
deaf or ASL signing population. Please be aware
of unintended racial or gender bias caused by this
fact. Finally, the translation model proposed in this
paper still has low performance and hence is unable
to serve as an alternative to human interpreters in
real-life scenarios.
References63736374A Appendix
A.1 Existing datasets
Figure 9 shows typical image frames in existing
SLT datasets. Overall, existing SLT data are col-
lected in controlled environments with relatively
little visual variability (e.g., background, lighting).
A.2 Instructions for meta annotation
For the meta annotation, the annotators were pro-
vided the complete videos, and were required
to annotate the following information: (1) Num-
ber of signers appearing at any time in the
video (“one” or “multi” for multiple signers) (2)
Name of signer, if it appears in the video de-
scription. If you cannot find it, mark this field
“UNK” (3) Handedness (left or right; if multi-
ple signers, mark this field “multi”) (4) Perceived
gender (“male”, “female”, or “other/unknown”)
(5) Perceived race (white/caucasian (including
latino/hispanic), black/African American, south
Asian, east Asian, other/unknown) (6) Perceived
age group (child, young adult, middle aged, older
adult) (7) Perceived ASL proficiency (native or
near-native, high proficiency, low proficiency).A.3 Sign Search
The search process for lexical and fingerspelling
signs is detailed in Algorithm 1.
Algorithm 1: Sign Search
Data: Translation dataset D
Model: isolated sign recognizer M,
fingerspelling recognizer M,
fingerspelling detector M
Hyperparameters: lexical/fingerspelling
threshold δ/δ
Output: Spotted lexical and fingerspelling
sign dataset D
Function SearchSign( D,M,
δ):
D← ∅;
for(I, w)∈ Ddo
Sliding windows
Ω={(s, e)};
for(s, e)∈Ωdo
Letp←M(I)be
probability vector;
Let˜w←the subset of w
within the vocabulary of M;
Letq←(p, p, ..., p);
Letk←argmax {q};
ifq> δthen
D← D∪ {(I,˜w)}
end
end
Fingerspelling proposals
Ω={(s, e)}=M(I);
for(s, e)∈Ωdo
Word hypothesis
ˆw=M(I);
Accuracies y=
(A( ˆw, w), ..., A ( ˆw, w)),
A: letter accuracy function;
Letk←argmax {y};
ify> δthen
D← D∪ {(I, w)}
end
end
end
return D;
A.4 Implementation details
Preprocessing For training, we use the time bound-
aries in the associated video caption to segment raw6375videos into short clips. We extend the time bound-
aries of each video clip by 0.5 second at both the
beginning and the end to reduce the proportion of
potential missing frames caused by misalignment
between subtitle and signing video. Each video
clip is cropped to include only the signing region
of the target signer. Specifically, we employ the
DLIB face detector (King, 2009) to detect the face
of the target signer and crop an ROI centered on
the face which is 4 times the size of the original
bounding box. In case there are multiple faces de-
tected, we employ a simple heuristic to determine
the target face track (tracks with the highest optical
flow (Farnebäck, 2003) magnitude). The selected
ROI is resized to 224×224. We use words as out-
put units and keep words appearing at least twice in
the training set in the vocabulary (21,475 words).
Visual Backbone For global visual feature ex-
traction, we adopt I3D network (Carreira and Zis-
serman, 2017) as our backbone. The I3D, pre-
trained on Kinetics-400 (Carreira and Zisserman,
2017) is further fine-tuned on WLASL (Li et al.,
2020a), an isolated ASL sign dataset with 14,289
isolated training videos of 2000 distinct ASL signs.
The isolated sign recognizer achieves 42.6% accu-
racy on the WLASL test set.
For hand feature extraction, we train a finger-
spelling recognizer on the ChicagoFSWild (Shi
et al., 2018) and ChicagoFSWild+ (Shi et al., 2019)
datasets, which include 61,536 ASL fingerspelling
sequences. The recognizer is based on a Conv-
LSTM architecture (Shi et al., 2021) consisting
of the first 11 conv layers of VGG-19 followed
by a one-layer Bi-LSTM with 512 hidden units
per direction. The model is trained with CTC
loss (Graves et al., 2006) and achieves 64.5% letter
accuracy on the ChicagoFSWild test set. In order
to extract hand features on our data, we use the HR-
Net whole-body pose estimator (Sun et al., 2019) to
detect the hands of the signer and extract features
in the hand ROI. Features for left and right hands
are concatenated before feeding into the translation
model.
To obtain mouthing feature, we employ A V-
HuBERT (Shi et al., 2022b), a state-of-the-art
lip reading model for English. The mouth ROI,
cropped and resized to 96×96based on the fa-
cial landmarks detected with DLIB facial keypoint
detector (King, 2009), are fed into the lip reading
model for feature extraction.
Sign Search To search lexical signs, we run in-ference with the aforementioned I3D isolated sign
recognizer on 32-frame windows. The window is
swept across the whole video clip at a stride of 8
frames. To search fingerspelling, we use the off-
the-shelf fingerspelling detector (Shi et al., 2021)
trained on raw ASL videos of ChicagoFSWild+,
which has achieved 0.448 AP@0.5. The aforemen-
tioned fingerspelling recognizer is used for search-
ing fingerspelling signs. We keep proposals with
confidence score higher than 0.5. The thresholds
δ/δare tuned to be 0.6/0.2 respectively. The total
number of signs detected from our translation data
is 32,602. We combine WLASL and the spotted
signs for pre-training I3D (see section 5.3). The
model is trained with SGD for 50 epochs at batch
size of 8. The learning rate and momentum of SGD
are 0.01 and 0.9 respectively. The learning rate is
reduced to half at epoch 20 and 40.
Sequence Model The visual backbones are
frozen in training translation model. Both trans-
former encoder and decoder have 2 layers with 512
hidden dimension and 2048 feedforward dimen-
sion. The model is trained with Adam (Kingma
and Ba, 2015) for 14K iterations at batch size of 64.
The learning rate is linearly increased to 0.001 for
the first 2K iterations and then decayed to 0 in the
later iterations. At test time, we use beam search
for decoding. The beam width and length penalty
are tuned on the validation set.
Real-time performance Although real-time per-
formance is not a goal of this work, we note
that the whole proposed system (including all pre-
processing such as mouth/hand ROI estimation)
processes ∼25 frames per second on average for
a sign language video from scratch on one RTX
A4000 GPU.
A.5 Baseline performance on Phoenix-14T
Table 5 shows the performance of the two baseline
approaches on Phoenix-14T. In contrast to results
on OpenASL, I3D-transformer does not generally
outperform Conv-GRU, which is probably due to
the linguistic discrepancy between the isolated sign
data used to pre-train I3D (WLASL: ASL) and the
translation data (Phoenix-14T: DGS).6376A.6 Does fine-tuning the visual encoder help?
By default, the visual backbone is frozen in train-
ing the translation model. Table 6 compares pre-
training and fine-tuning I3D visual encoder for
translation. Fine-tuning visual backbone deteri-
orates the model performance by a large margin.
This probably suggests that the proposed bench-
mark is in a low-resource regime, which does not
have enough data for full fine-tuning. We hypoth-
esize that the paired text does not provide strong
supervision to learn visual encoder, thus leading
to performance degradation. Training a fully end-
to-end SLT model potentially requires much larger
amount of paired data.
A.7 Which pre-training data to use?
To show the effect of isolated sign pre-training, we
compare I3D pre-trained with Kinetics-400 and
WLASL in Table 7. Kinetics-400 (Carreira and
Zisserman, 2017) is a large-scale action recogni-
tion dataset including over 306,245 video clips
from 400 action categories, while WLASL contains
14,289 clips from 2,000 ASL signs. Though the
size of WLASL is one order of magnitude smaller,
using WLASL for pre-training outperforms pre-
training with Kinetics only by a large margin. Uti-
lizing isolated sign data, despite its amount being
scarce, greatly boosts the visual representation that
further benefits translation.
A.8 Fingerspelling vs. non-fingerspelling
Fingerspelling is an important component in real-
world ASL videos. We measure the performance
on the subsets with and without fingerspelling re-
spectively. According to Figure 10, the translation
quality in non-fingerspelling subsets is consistently
higher than the other part. Typical fingerspelledwords which our model fails to translate are either
proper nouns with low frequency in training (e.g.,
SCHMIDT, WHALEY), or long words (e.g., MAS-
SACHUSETTS, SALT LAKE CITY). Though the
visual backbone of our translation model is pre-
trained with fingerspelling sequences, transcribing
the fingerspelling segment(s) is still problematic.
As our model is based on whole word, it is inca-
pable of translating words unseen during training.
Thus proper nouns, typically fingerspelled in ASL,
are difficult to translate by our model. In practice,
we observed many fingerspelled words are simply
replaced with <UNK >. How to improve transla-
tion for ASL videos with fingerspelling requires
more research.
A.9 Translation examples
We randomly select 15 examples from dev set and
compare the model prediction against the reference
(see Table 8). The exactly correct translations are
mostly short and commonly used sentences in daily
communication (e.g., thank you). For longer and
more complex sentences, the model frequently fails
to capture their general meaning though some key-
words can be predicted correctly.
A.10 Visualization
The spotted lexical signs and fingerspelling se-
quences are shown in figure 11. Note that those
examples are randomly selected. The spotted signs
are mostly accurate. Below are our main observa-
tions. First, in lexical sign spotting, the target clip
often includes a partial (or whole) segment from
adjacent signs. For instance, the third clip of UNI-
VERSITY has an extra sign of GALLAUDET. This
is due to the fixed window size we use for lexical
sign search. 2. False positives occur especially
when two signs are of similar appearance. The
second clip of BEFORE, which has a similar body
posture to BEFORE, is a pointing sign indicating
that one thing is happening prior to something else.
3. Using a sophisticated fingerspelling detector
enables us to spot fingerspelling sequences more
precisely compared to lexical signs.637763786379