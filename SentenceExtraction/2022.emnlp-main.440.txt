
Fan Yin
University of California, Los Angeles
fanyin20@cs.ucla.eduYao Li
University of North Carolina, Chapel Hill
yaoli@email.unc.edu
Cho-Jui Hsieh
University of California, Los Angeles
chohsieh@cs.ucla.eduKai-Wei Chang
University of California, Los Angeles
kwchang@cs.ucla.edu
Abstract
Adversarial Examples Detection (AED) is a
crucial defense technique against adversarial
attacks and has drawn increasing attention from
the Natural Language Processing (NLP) com-
munity. Despite the surge of new AED meth-
ods, our studies show that existing methods
heavily rely on a shortcut to achieve good per-
formance. In other words, current search-based
adversarial attacks in NLP stop once model
predictions change, and thus most adversar-
ial examples generated by those attacks are
located near model decision boundaries. To
surpass this shortcut and fairly evaluate AED
methods, we propose to test AED methods with
FarBoundary ( FB) adversarial examples. Ex-
isting methods show worse than random guess
performance under this scenario. To overcome
this limitation, we propose a new technique,
ADDMU ,adversary detection with data and
modeluncertainty, which combines two types
of uncertainty estimation for both regular and
FB adversarial example detection. Our new
method outperforms previous methods by 3.6
and 6.0 AUC points under each scenario. Fi-
nally, our analysis shows that the two types
of uncertainty provided by ADDMU can be
leveraged to characterize adversarial examples
and identify the ones that contribute most to
model’s robustness in adversarial training.
1 Introduction
Deep neural networks (DNN) have achieved re-
markable performance in a wide variety of NLP
tasks. However, it has been shown that DNNs
can be vulnerable to adversarial examples (Jia and
Liang, 2017; Alzantot et al., 2018; Jin et al., 2020),
i.e., perturbed examples that flip model predictions
but remain imperceptible to humans, and thus im-
pose serious security concerns about NLP models.
To improve the robustness of NLP models, differ-
ent kinds of techniques to defend against adversar-
ial examples have been proposed (Li et al., 2021b).
In this paper, we study AED, which aims to add adetection module to identify and reject malicious
inputs based on certain characteristics. Different
from adversarial training methods (Madry et al.,
2018a; Jia et al., 2019) which require re-training
of the model with additional data or regularization,
AED operates in the test time and can be directly
integrated with any existing model.
Despite being well explored in the vision domain
(Feinman et al., 2017; Raghuram et al., 2021), AED
started to get attention in the field of NLP only re-
cently. Many works have been proposed to conduct
detection based on certain statistics (Zhou et al.,
2019; Mozes et al., 2021; Yoo et al., 2022; Xie
et al., 2022). Specifically, Yoo et al. (2022) propose
a benchmark for AED methods and a competitive
baseline by robust density estimation. However, by
studying examples in the benchmark, we find that
the success of some AED methods relies heavily
on the shortcut left by adversarial attacks: most
adversarial examples are located near model deci-
sion boundaries , i.e., they have small probability
discrepancy between the predicted class and the
second largest class. This is because when creat-
ing adversarial data, the searching process stops
once model predictions changed. We illustrate this
finding in Section 2.2.
To evaluate detection methods accurately, we
propose to test AED methods on both regular ad-
versarial examples and Far-Boundary ( FB)adver-
sarial examples, which are created by continuing to
search for better adversarial examples till a thresh-
old of probability discrepancy is met. Results show
that existing AED methods perform worse than
random guess on FB adversarial examples. Yoo
et al. (2022) recognize this limitation, but we find
that this phenomenon is more severe than what is
reported in their work. Thus, an AED method that
works for FB attacks is in need.6567We propose ADDMU, an uncertainty estimation
based AED method. The key intuition is based
on the fact that adversarial examples lie off the
manifold of training data and models are typically
uncertain about their predictions of them. Thus,
although the prediction probability is no longer a
good uncertainty measurement when adversarial
examples are far from the model decision bound-
ary, there exist other statistical clues that give out
the ‘uncertainty’ in predictions to identify adver-
sarial data. In this paper, we introduce two of them:
data uncertainty andmodel uncertainty . Data un-
certainty is defined as the uncertainty of model
predictions over neighbors of the input. Model
uncertainty is defined as the prediction variance
on the original input when applying Monte Carlo
Dropout (MCD) (Gal and Ghahramani, 2016) to
the target model during inference time. Previous
work has shown that models trained with dropout
regularization (Srivastava et al., 2014) approximate
the inference in Bayesian neural networks with
MCD, where model uncertainty is easy to obtain
(Gal and Ghahramani, 2016; Smith and Gal, 2018).
Given the statistics of the two uncertainties, we ap-
ply p-value normalization (Raghuram et al., 2021)
and combine them with Fisher’s method (Fisher,
1992) to produce a stronger test statistic for AED.
To the best of our knowledge, we are the first work
to estimate the uncertainty of Transformer-based
models (Shelmanov et al., 2021) for AED.
The advantages of our proposed AED method
include: 1) it only operates on the output level of
the model; 2) it requires little to no modifications
to adapt to different architectures; 3) it provides
an unified way to combine different types of un-
certainties. Experimental results on four datasets,
four attacks, and two models demonstrate that our
method outperforms existing methods by 3.6 and
6.0 in terms of AUC scores on regular and FB cases,
respectively. We also show that the two uncertainty
statistics can be used to characterize adversarial
data and select useful data for another defense tech-
nique, adversarial data augmentation (ADA).
The code for this paper could be
found at https://github.com/uclanlp/
AdvExDetection-ADDMU
2 A Diagnostic Study on AED Methods
In this section, we first describe the formulation of
adversarial examples and AED. Then, we show that
current AED methods mainly act well on detectingadversarial examples near the decision boundary,
but are confused by FB adversarial examples.
2.1 Formulation
Adversarial Examples. Given an NLP model f:
X → Y , a textual input x∈ X, a predicted class
from the candidate classes y∈ Y , and a set of
boolean indicator functions of constraints, C:X ×
X → { 0,1}, i= 1,2,···, n. An (untargeted)
adversarial example x∈ X satisfies:
f(x)̸=f(x),C(x, x) = 1 , i= 1,2,···, n.
Constraints are typically grammatical or seman-
tic similarities between original and adversarial
data. For example, Jin et al. (2020) conduct part-of-
speech checks and use Universal Sentence Encoder
(Cer et al., 2018) to ensure semantic similarities
between two sentences.
Adversarial Examples Detection (AED) The task
of AED is to distinguish adversarial examples
from natural ones, based on certain characteris-
tics of adversarial data. We assume access to 1)
the victim model f, trained and tested on clean
datasets D andD; 2) an evaluation set
D; 3) an auxiliary dataset Dcontains only
clean data. Dcontains equal number of adver-
sarial examples Dand natural examples
D.Dare randomly sampled from
D.Dis generated by attacking a dis-
joint set of samples from DonD. See
Scenario 1 in Yoo et al. (2022) for details. We use
a subset of D asD. We adopt an unsuper-
vised setting, i.e., the AED method is not trained
on any dataset that contains adversarial examples.
2.2 Diagnose AED Methods
We define examples near model decision bound-
aries to be those whose output probabilities for
the predicted class and the second largest class are
close. Regular iterative adversarial attacks stop
once the predictions are changed. Therefore, we
suspect that regular attacks are mostly generating
adversarial examples near the boundaries, and ex-
isting AED methods could rely on this property to
detect adversarial examples.
Figure 1 verifies this for the state-of-the-art un-
supervised AED method (Yoo et al., 2022) in NLP,
denoted as RDE . Similar trends are observed for an-
other baseline. The X-axis shows two attack meth-
ods: TextFooler (Jin et al., 2020) and Pruthi (Pruthi
et al., 2019). The Y-axis represents the probability6568
difference between the predicted class and the sec-
ond largest class. Average probability differences
of natural examples (Natural), and three types of
adversarial examples are shown: RDE fails to iden-
tify (Failed), successfully detected (Detected), and
overall (Overall). There is a clear trend that success-
fully detected adversarial examples are those with
small probability differences while the ones with
high probability differences are often mis-classified
as natural examples. This finding shows that these
AED methods identify examples near the decision
boundaries, instead of adversarial examples.
To better evaluate AED methods, we propose
to avoid the above shortcut by testing detection
methods with FB adversarial examples, which are
generated by continuously searching for adversarial
examples until a prediction probability threshold is
reached. We simply add another goal function to
the adversarial example definition to achieve this
while keep other conditions unchanged:
f(x)̸=f(x), p(y=f(x)|x)≥ϵ
C(x, x) = 1 , i= 1,2,···, n.
p(y=f(x)|x)denotes the predicted probabil-
ity for the adversarial example. ϵis a manually
defined threshold. We illustrate the choice of ϵin
Section 4.1. In Table 1, it shows that the existing
competitive methods (RDE and DIST) get lower
than random guess F1 scores when evaluated with
FB adversarial examples.
2.3 Quality Check for FB Attacks
We show that empirically, the quality of adversarial
examples do not significantly degrade even search-
ing for more steps and stronger FB adversarial ex-
amples. We follow Morris et al. (2020a) to evaluate
the quality of FB adversarial examples in terms of
grammatical and semantic changes, and compare
them with regular adversarial examples. We use
a triple (x, x, x)to denote the original
example, its corresponding regular adversarial and
FB adversarial examples. For grammatical changes,
we conduct an automatic evaluation with Language-
Tool (Naber et al., 2003) to count grammatical er-
rors and report the relative increase of errors of
perturbed examples w.r.t. original examples. For
semantic changes, we do a human evaluation using
Amazon MTurk. We ask the workers to rate to
what extent the changes to xpreserve the meaning
of the sentence, with scale 1 (‘Strongly disagree’)
to 5 (‘Strongly agree’). Results are summarized in
Table 2. The values are averaged over three adver-
sarial attacks, 50 examples for each. We find that
the FB attacks have minimal impact on the quality
of the adversarial examples. We show some exam-
ples on Table 7, which qualitatively demonstrate
that it is hard for humans to identify FB adversarial
examples.65693 Adversary Detection with Data and
Model Uncertainty (ADDMU)
Given the poor performance of previous methods
on FB attacks, we aim to build a detector that can
handle not only regular but also FB adversarial
examples. We propose ADDMU, an uncertainty
estimation based AED method by combing two
types of uncertainty: model uncertainty and data
uncertainty. We expect the adversarial examples to
have large values for both. The motivation of using
uncertainty is that models can still be uncertain
about their predictions even when they assign a
high probability of predicted class to an example.
We describe the definitions and estimations of the
two uncertainties, and how to combine them.
3.1 Model Uncertainty Estimation
Model uncertainty represents the uncertainty when
predicting a single data point with randomized mod-
els. Gal and Ghahramani (2016) show that model
uncertainty can be extracted from DNNs trained
with dropout and inference with MCD without
any modifications of the network. This is because
the training objective with dropout minimizes the
Kullback-Leibler divergence between the posterior
distribution of a Bayesian network and an approxi-
mation distribution. We follow this approach and
define the model uncertainty as the softmax vari-
ance when applying MCD during test time.
Specifically, given a trained model f, we do N
stochastic forward passes for each data point x.
The dropout masks of hidden representations for
each forward pass are i.i.d sampled from a Bernolli
distribution, i.e., z∼Bernolli (p)where pis
a fixed dropout rate for all layers, zis the mask for
neuron kon layer l. Then, we can do a Monte Carlo
estimation on the softmax variance among the N
stochastic softmax outputs. Denote the probability
of predicting the input as the i-th class in the j-th
forward pass as pand the mean probability for the
i-th class over Npasses as ¯p=/summationtextp,
the model uncertainty (MU) can be computed by
MU (x) =1
|Y|/summationdisplay1
N/summationdisplay(p−¯p).
3.2 Data Uncertainty Estimation
Data uncertainty quantifies the predictive probabil-
ity distribution of a fixed model over the neighbor-
hood of an input point.
Specifically, similar to the model uncertainty
estimation, we do Nstochastic forward passes.But instead of randomly zeroing out neurons in the
model, we fix the trained model and construct a
stochastic input for each forward pass by masking
out input tokens, i.e., replacing each token in the
original input by a special token with probability
p. The data uncertainty is estimated by the mean
of (1−maximum softmax probability) over the N
forward passes. Denote the Nstochastic inputs as
x, x,···, x, the original prediction as y, and
the predictive probability of the original predicted
class as p(·), the Monte Carlo estimation on data
uncertainty (DU) is:
DU(x) =1
N/summationdisplay(1−p(x)).
3.3 Aggregate Uncertainties with Fisher’s
Method
We intend to aggregate the two uncertainties de-
scribed above to better reveal the low confidence
of model’s prediction on adversarial examples. We
first normalize the uncertainty statistics so that they
follow the same distribution. Motivated by Raghu-
ram et al. (2021) where the authors normalize test
statistics across layers by converting them to p-
values, we also adopt the same method to normal-
ize the two uncertainties. By definition, a p-value
computes the probability of a test statistic being at
least as extreme as the target value. The transforma-
tion will convert any test statistics into a uniformly
distributed probability. We construct empirical dis-
tributions for MU and DU by calculating the cor-
responding uncertainties for each example on the
auxiliary dataset D, denoted as T, and T.
Following the null hypothesis H: the data being
evaluated comes from the clean distribution , we
can calculate the p-values based on model uncer-
tainty ( q) and data uncertainty ( q) by:
q(x) =P(T≥MU (x)|H),
q(x) =P(T≥DU(x)|H).
The smaller the values qandq, the higher the
probability of the example being adversarial.
Given qandq, we combine them into a sin-
gle p-value using the Fisher’s method to do com-
bined probability test (Fisher, 1992). Fisher’s
method indicates that under the null hypothesis,
the sum of the log of the two p-values follows a
χdistribution with 4 degrees of freedom. We
useqto denote the aggregated p-value. Adver-
sarial examples should have smaller q, where
logq=logq+logq.65704 Experiments
We first describe the experimental setup (Section
4.1), then present our results on both regular and FB
AED (Section 4.2). Results show that our ADDMU
outperforms existing methods by a large margin
under both scenarios.
4.1 Experimental Setup
Datasets and victim models. We conduct experi-
ments on classification tasks in different domains,
including sentiment analysis SST-2 (Socher et al.,
2013), Yelp (Zhang et al., 2015), topic classifica-
tion AGNews (Zhang et al., 2015), and natural lan-
guage inference SNLI (Bowman et al., 2015). We
generate both regular and FB adversarial examples
on the test data of each dataset with two word-
level attacks: TextFooler (TF) (Jin et al., 2020),
BAE (Garg and Ramakrishnan, 2020), and two
character-level attacks: Pruthi (Pruthi et al., 2019),
and TextBugger (TB) (Li et al., 2019). We only
consider the examples that are predicted correctly
before attacks. The numbers of evaluated examples
vary among 400 to 4000 across datasets. See Ap-
pendix B. For FB adversarial examples, we choose
theϵso that adversarial examples have approxi-
mately equal averaged prediction probability with
natural data. Specifically, ϵ= 0.9for SST-2, Yelp,
AGNews, and ϵ= 0.7for SNLI. We mainly exper-
iment with two Transformer-based victim models,
BERT (Devlin et al., 2019) and RoBERTa (Liu
et al., 2019) as they are widely adopted in the cur-
rent NLP pipelines and show superior performance
than other architectures. More details are presented
in Appendix B. In Appendix H, we also present
some simple experiments with BiLSTM.
Baselines. We compare ADDMU with several un-
supervised AED methods. 1) MSP: Hendrycks and
Gimpel (2017) use the Maximum Softmax Proba-
bility (MSP) for detection; 2) PPL: GPT-2 large
(Radford et al., 2019) as a language model to mea-
sure the perplexity of the input; 3) FGWS: Mozes
et al. (2021) measure the difference in prediction
probability after replacing infrequent words of the
inputs with frequent words and find that adver-
sarial examples have higher performance change;
4)RDE: Yoo et al. (2022) fit class conditional
density estimation with Kernel PCA (Schölkopf
et al., 1998) and Minimum Covariance Determi-
nant (Rousseeuw, 1984) in the feature space and
use the density scores; 5) DIST: we propose a
distance-based baseline that uses the difference be-tween class conditional, averaged K nearest dis-
tances. See Appendix C for details.
Unsupervised AED methods assign a score to
each evaluated data. Then, a threshold is selected
based on the maximum False Positive Rate (FPR)
allowed, i.e., the rate of mis-classified natural data.
Implementation Details. ForFGWS andRDE ,
we follow the hyper-parameters in their papers to
reproduce the numbers. For DIST andADDMU ,
we attack the validation set and use those examples
to tune the hyper-parameters. See Appendix D
for details. Specifically, for DIST , we use 600
neighbors. For ADDMU , we find N= 10 ,p=
0.2for MU works well for all datasets. For DU, we
find that it is beneficial to ensemble different mask
rates for text classification tasks, we set N= 100
in total, and 25 for each p∈ {0.1,0.2,0.3,0.4}
for all the text classification tasks, N= 25 ,p=
0.1for SNLI.
Metrics. In the main experiments, we select the
threshold at maximum FPR=0.1. A lower FPR rep-
resents a more practical case where only a small
proportion of natural samples are mis-classified as
adversarial samples. Following the setup in Xu
et al. (2018) and Yoo et al. (2022), we report True
Positive Rate (TPR), i.e., the fraction of the real
adversarial examples out of predicted adversarial
examples, and F1 score at FPR=0.1, and Area Un-
der the ROC curve (AUC), which measures the area
under the TPR and FPR curve. For all the metrics,
the higher the better.
4.2 Results
Performances of AED methods on BERT are pre-
sented in Table 3. We average the results among
three runs with different random seeds. See Ap-
pendix F for the results on RoBERTa.
Detector performance. Our proposed ADDMU
achieves the best performance on both regular and
FB adversarial examples under the three metrics
(TPR, F1, AUC) on the four datasets, which demon-
strates the effectiveness of ADDMU. Further, AD-
DMU preserves more than 90% of the performance
or even achieves better results, e.g SST-2-Pruthi
and Yelp-BAE, under FB adversarial attacks, which
shows that ADDMU is not affected by FB attacks.
The performances of MSP, DIST, and RDE are
severely degraded under FB attacks. This demon-
strates that those methods can be fooled and circum-
vented by carefully designed attacks. Under regular
attacks, the performances of RDE and DIST are6571
worse than the baseline MSP in most cases, which
simply uses the maximum softmax probability for
detection. One explanation is that those class con-
ditional methods are just approximating softmax
probabilities so might not be as effective as MSP
in detecting near the decision boundary examples.
Finally, PPL and FGWS are also not severely
affected by FB attacks. However, FGWS is only
applicable to word-level attacks. Also, PPL and
FGWS are not effective enough in general.
Ablation study. Data uncertainty ( DU) and model
uncertainty ( MU) can also be used as features indetection separately. Also, both RDE andDIST
can be enhanced by calculating the average score
over the neighborhood of the input using the same
random masking technique as used in data uncer-
tainty estimation. We denote them as RDE-aug
andDIST-aug . In this part, we study the effec-
tiveness of uncertainty aggregation and neighbor
augmentation by comparing ADDMU with DU
and MU, and by comparing RDE and DIST with
RDE-aug and DIST-aug. Full results are shown in
Appendix G. We show a representative proportion
of the results in Table 4. The summary of findings6572
are discussed in the following.
We find that ADDMU, the aggregation of two
uncertainties, achieves the best results in 70 out of
the 96 metric scores. DU and MU are the best in
12 scores each. This shows that the combination
of the two uncertainties provides more information
to identify adversarial examples. We also observe
that on SNLI, DU values are typically less useful,
and thus the combination of DU and MU performs
slightly worse than MU. One explanation is that the
SNLI task requires more sophisticated neighbor-
hood construction method to generate meaningful
neighbors in data uncertainty estimation. Finally,
we also notice that RDE-aug and DIST-aug are in
general better than RDE and DIST, especially under
FB attacks, which demonstrates the effectiveness
of neighbor augmentation.
Why do detection results vary among datasets
and attacks? Among different attacks, we find
that Pruthi is the hardest to detect, followed by
BAE. However, there is no obvious difference be-
tween detection performances against word-level
and character-level attacks. Also, attacks on the
sentence pair task (SNLI) are in general harder to
detect. Thus, future work could focus more on im-
proving the performance of detecting adversarial
examples in sentence pair tasks, like SNLI.
We investigate why the detection performances
vary among attacks. Our hypothesis is that attacks
on some datasets fail to be imperceptible and have
changed the groundtruth label for an input. Thus,
these ‘adversarial’ (can not be called adversarial
any more as they do not meet the definition of be-
ing imperceptible) examples actually lie close to
the training manifold of the target class. Therefore,
AED methods find it hard to detect those exam-
ples. To verify this assumption, we choose two
tasks (SST-2 and Yelp) and two attacks (TF and
BAE) to do sentiment analysis. We ask Amazon
MTurk workersto re-label positive ornegative
for attacked examples. Then, we summarize the
proportion of examples that workers assign oppo-
site groundtruth labels in correctly and wrongly
detected groups. As shown in Table 5, there is
an obvious correlation between bad performance
and the number of ‘adversarial’ examples whose
groundtruth labels changed. For example, AD-
DMU performs weak on detecting BAE attacks
on SST-2 (58.9 F1), but it turns out that this is be-
cause more than half of the examples already have
their groundtruth labels flipped. We give one exam-
ple in Table 5. This shows that adversarial attacks
need to be improved to retain the semantic meaning
of the original input.
5 Characterize Adversarial Examples
In this section, we explore how to characterize ad-
versarial examples by the two uncertainties.
MU-DU Data Map Plotting a heatmap with MU
on X-axis and DU on Y-axis, we visualize data in
terms of the two uncertainties. We show in Figure 2
the heatmaps with natural data, FB and regular
adversarial examples generated from three attacks
on three datasets (AGNews TF, Yelp BAE, SNLI
Pruthi). The performance of ADDMU varies on
the three attacks, as shown on the left of Figure 2.
We find that natural examples center on the bot-
tom left corner of the map, representing low MU
and DU values. This phenomenon does not vary
across datasets. Whereas for FB and regular ad-
versarial examples, they have larger values on at
least one of the two uncertainties. When ADDMU
performs best (AGNews TF, the first row), the cen-6573
ter of adversarial examples in the MU-DU map
is relatively rightward and upward compared to
other cases. For maps on the third row, the shadow
stretches along the MU axis, indicating that Pruthi
examples on SNLI have relatively large MU values.
Identifying Informative ADA Data ADA is an-
other adversarial defense technique, which aug-
ments the training set with adversarial data and
re-train the victim model to improve its robustness.
In this part, we show that our ADDMU provides
information to select adversarial data that is more
beneficial to model robustness. We test it with TF
on SST-2. The procedure is as follows: since SST-
2 only has public training and validation sets, we
split the original training set into training (80%)
and validation set (20%), and use the original val-
idation set as test set. We first train a model on
the new training set. Then, we attack the model on
validation data and compute DU and MU values for
each adversarial sample. We sort the adversarial ex-
amples according to their DU and MU values and
split them by half into four disjoint sets: HDHM
(high DU, high MU), HDLM (high DU, low MU),
LDHM (low DU, high MU), and LDLM (low DU,
low MU). We augment the clean training set with
each of these sets and retrain the model. As a base-
line, we also test the performance of augmenting
with all the adversarial examples generated from
the validation set (All). We report clean accuracy
(Clean % ), the number of augmented data ( #Aug ),
attack success rate ( ASR ), and the average query
number ( #Query ) for each model.
The results are in Table 6. We find that the most
helpful adversarial examples are with low DU and
high MU . Using those samples, we achieve better
ASR and clean accuracy than augmenting with the
whole validation set of adversarial examples, with
only one quarter of the amount of data. It is ex-
pected that examples with low DU and low MU are
less helpful as they are more similar to the clean
data. Similar observations are found in the FB
version of TF attacks. We also compare augmen-
tations with regular and FB adversarial examples.
See details in Appendix E.
6 Related Work
Adversarial Detection. Adversarial examples de-
tection has been well-studied in the image domain
(Feinman et al., 2017; Lee et al., 2018; Ma et al.,
2018; Xu et al., 2018; Roth et al., 2019; Li et al.,
2021a; Raghuram et al., 2021). Our work aligns
with Feinman et al. (2017); Li et al. (2021a); Roth
et al. (2019) that introduce uncertainty estimation
or perturbations as features to detect adversarial
examples. We postpone the details to Appendix I,
but focus more on the AED in NLP domain.
In the NLP domain, there are less work exploring
AED. Zhou et al. (2019) propose DISP that learns
a BERT-based discriminator to defend against ad-
versarial examples. Mozes et al. (2021) propose
a word-level detector FGWS that leverages the
model confidence drop when replacing infrequent
words in the input with frequent ones and surpass
DISP. Pruthi et al. (2019) combat character-level
attacks with word-recognition models. More re-
cently, Yoo et al. (2022) propose a robust density
estimation baseline and a benchmark for evaluat-
ing AED methods. There are other works like Xie
et al. (2022); Biju et al. (2022); Wang et al. (2022);
Mosca et al. (2022), that leverage other features
or train a detector. We show limitations of these
works on FB adversarial examples and propose our
ADDMU that overcomes this limitation.6574Other Defenses against Attacks. AED is a cate-
gory of approaches to defending against adversarial
attacks. Other methods are also considered. Jin
et al. (2020); Yin et al. (2020); Si et al. (2021) do
ADA that augments original training datasets with
adversarial data for better robustness. Madry et al.
(2018b); Miyato et al. (2017); Zhu et al. (2019);
Zhou et al. (2020) conduct adversarial training
which is formulated as a min-max problem. Re-
cently, several works perform certified robustness
defense with either interval bound propagation
(Huang et al., 2019; Jia et al., 2019; Shi et al., 2020),
or randomized smoothness (Ye et al., 2020). In this
work, we connect our AED method with ADA by
selecting more informative data to augment.
7 Conclusion
We proposed ADDMU, an uncertainty-based ap-
proach for both regular and FB AED. We began by
showing that existing methods are significantly af-
fected by FB attacks. Then, we show that ADDMU
is minimally impacted by FB attacks and outper-
forms existing methods by a large margin. We
further showed ADDMU characterizes adversar-
ial data and provides information on how to select
useful augmented data for improving robustness.
Acknowledgement
We thank anonymous reviewers, UCLA PLUS-
Lab and UCLA-NLP for their helpful feedback.
This work is partially supported by DMS-2152289,
DMS-2134107, IIS-2008173, IIS-2048280, Cisco
Faculty Award, and a Sloan Research Fellow.
Limitations
We summarize the limitations of this paper in this
section.
1.We only test the AED methods under classifi-
cation tasks. This is because we find that the
attacks on other tasks like language genera-
tion are not well-defined, for example what
would be the goal function of attacks on a
language generation task? Is minimizing the
BLEU score sufficient? It is hard to conduct
detection when there is no standard for a valid
adversarial example. Future work might come
up with attacks for diverse tasks first and pro-
pose corresponding AED methods.
2.More experiments should be conducted to an-
alyze the FB adversarial examples, includingits characteristics and the security concerns it
imposes to DNNs. However, given the time
and space limitations, we are not able to do
that.
3.Our method has slightly more hyperparame-
ters to tune (four in total), and requires a bit
more time to finish one detection. But, we
confirm that it is in an acceptable range.
References6575657665776578A Regular vs. FB Adversarial Examples
In this section, we qualitatively shows some cases
of far-boundary adversarial examples in Table 7.
We show that it is hard for human beings to identify
such far-boundary examples, which calls for an
automatic way to do the detection.
B Experimental Setup Details
B.1 Datasets and Target Models
We conduct experiments on four datasets, SST-
2, Yelp-Polarity, AGNews, and SNLI. Statistics
about those datasets are summarized on Table 8.
All those datasets are available at Huggingface
Datasets (Lhoest et al., 2021). Our target mod-
els are BERT (Devlin et al., 2019) and RoBERTa
(Liu et al., 2019). We use the public accessible
BERT-base-uncased and RoBERTa-base models
fine-tuned on the above datasets provided by Tex-
tAttack (Morris et al., 2020b) to benefit repro-
ducibility. The performance of those models are
summarized on Table 9.
B.2 Attacks and Statistics
We consider four attacks. TextFooler (Jin et al.,
2020), BAE (Garg and Ramakrishnan, 2020),
Pruthi (Pruthi et al., 2019), and TextBugger (Li
et al., 2019). TextFooler and BAE are word-level
attacks. Pruthi and TextBugger are character-level
attacks. For BAE, we use BAE-R, i.e., replace a
word with a substitution. For attacks on SNLI, we
only perturb the hypothesis sentence. For FB at-
tacks, as stated in the main paper, we add another
goal function to make sure the softmax probabil-
ity of the attacked class is larger than a threshold
ϵ. We select ϵ= 0.9for SST-2, Yelp, AGNews,
andϵ= 0.7for SNLI. We implement those attacks
with TextAttack, with the default hyperparameter
settings. Please refer to the document of TextAttack
for details. Here we report the after-attack accu-
racy ( Adv. Acc ), the attack success rate ( ASR ),
the number of queries ( #Query ), and the number
of adversarial examples we select ( #Adv ) for each
attack on each dataset, as well as for FB attacks.
Notice, the total evaluted examples will be twice
the number of adversarial examples. See Table 10
and Table 11.
C DIST
We propose the DIST baseline, which is a distance-
based detector motivated by Ma et al. (2018). Wealso find that the Local Intrinsic Dimension value
proposed in Ma et al. (2018) does not work well
when detecting NLP attacks. The DIST method
leverages the whole training set as D. Then,
it selects the K-nearest neighbors of the evaluated
point from each class of Dand calculates the av-
erage distance between the neighbors and the eval-
uated point, denote as d, d,···, d, where kis
the number of classes. Suppose the evaluated point
has predicted class i. Then, it uses the difference
between the distance of class iand the minimum of
the other classes to do detection. i.e., d−min(d),
where k̸=i. The intuition is that since adversar-
ial examples are generated from the original class,
they might still be closer to training data in the
original class, which is min(d), k̸=i.
D Implementation Details
For DIST and ADDMU, we tune the hyperparam-
eters with an attacked validation set. For datasets
with an original train/validation/test split (SNLI),
we simply attacked the examples in the validation
set and select 100 of them to help the tuning. For
datasets without an original split, like SST-2, Yelp,
and AGNews, we randomly held out 100 examples
from the training set and attack them to construct a
set for hyperparameter tuning. For DIST, we select
the number of the neighbors from {100, 200, ···,
1000}. For ADDMU, we select NandNfrom
{10, 20, 80, 100}, and choose pandpfrom {0.1,
0.2, 0.3, 0.4}. In our preliminary experiment, we
find that ensemble different pvalues also help.
So, we also consider ensemble different pvalues
in combinations {(0.1, 0.2), (0.1, 0.2, 0.3, 0.4)}.
We also find that augment the model uncertainty
estimation with some neighborhood data is help-
ful, so for the model uncertainty value, we actually
average over 10 neighborhood data with 0.1 mask
rate.
E Selecting Useful data with Uncertainty
values
In this section, we present the results of selecting
useful data for ADA using DU and MU values for
FB version of TF, shown in Table 13. Similar to the
regular version, we find that the most useful data
are still those with low data uncertainty and high
model uncertainty. We achieve better ASR and the
number of queries using only one quarter of data
compared to the full augmentation. In Table 14,
we show the attack success rate of four settings. 1)6579
Dataset Train/Dev/TestAvg
Len#Labels
SST-2 67.3k/0.8k/- 19.2 2
Yelp 560k/-/38k 152.8 2
AGNews 120k/-/7.6k 35.5 4
SNLI 550k/10k/20k 8.3 3
Augment with FB examples to defend against regu-
lar attack; 2) Augment with FB examples to defend
against FB attack; 3) Augment with regular exam-
ples to defend against regular attack; 4) Augment
with regular examples to defend against FB attack.
The finding is that augment with FB and regular
adversarial examples most benefits its own attacks.
This implies that FB attacks might already change
the characteristics of regular attacks. We need to
defend against them with different strategies.Dataset SST-2 Yelp AGNews SNLI
BERT 92.43 96.30 94.20 89.40
RoBERTa 94.04 - 94.70 -
F RoBERTa Results
We conduct adversarial examples detection with
RoBERTa-base. The setting is the same as BERT.
Through hyperparameters search as described be-
fore, for ADDMU, we select N= 20 and
N= 100 , and choose p= 0.1andp= 0.1,
without augmentation for MU estimation and no
ensemble of various p. Table 16 presents the re-
sults for RoBERTa-base. ADDMU also outper-
form other methods with RoBERTa. We combine
the ablation table together with the main table for
RoBERTa.6580
G Ablation Study
We present the full results for the ablation study of
uncertainty aggregation in Table 15. We also show
that our neighborhood construction process in data
uncertainty can be used to enhance two baselines
RDE and DIST.
H Preliminary Results on BiLSTM
We experiment with a one-layer BiLSTM model
with hidden dimension 150 and dropout 0.3. The
model achieves 89.3 clean accuracy on SST-2. In
our preliminary experiments, we test on detecting
TextFooler and BAE attacks and their correspond-
ing FB attacked examples. We compare our AD-
DMU detector with three baselines PPL, FGWS,
and RDE. Results are shown on Table 12. We show
that ADDMU still achieves the best performance,
while the previous SOTA on detecting BERT and
RoBERTa adversarial examples, RDE, is corrupted
when detecting BiLSTM adversarial examples.
I Related work in CV
Feinman et al. (2017) train a binary classifier using
density estimation and Bayesian uncertainty esti-
mation as features for detection. Li et al. (2021a)
replace DNNs with Bayesian Neural Networks,
which enhance the distribution dispersion between
natural and adversarial examples and benefit AED.
TF TF-FB BAE BAE-FB
PPL 75.8 77.1 41.9 40.9
FGWS 86.2 87.1 83.7 81.4
RDE 15.6 24.0 21.2 33.3
ADDMU 93.7 89.3 92.2 87.6
Roth et al. (2019) use logodds on perturbed ex-
amples as statistics to conduct detection. Further,
Athalye et al. (2018) have similar observations with
us concerning image attacks. They find that the
distance-based feature, local intrinsic dimension
proposed in Ma et al. (2018) for AED fails when
encounters FB adversarial examples.6581658265836584