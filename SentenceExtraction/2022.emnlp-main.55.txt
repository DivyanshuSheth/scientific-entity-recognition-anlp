
Jianyang Zhang, Tao Liang, Mingyang Wan, Guowu YangandFengmao LvUniversity of Electronic Science and Technology of ChinaBytedanceSouthwest Jiaotong University
jianyangzhang@std.uestc.edu.cn ;taoliangdpg@126.com ;
wanmingyang@bytedance.com ;guowu@uestc.edu.cn ;fengmaolv@126.com
Abstract
Existing sentiment analysis models have
achieved great advances with the help of suf-
ficient sentiment annotations. Unfortunately,
many languages do not have sufficient sen-
timent corpus. To this end, recent studies
have proposed cross-lingual sentiment analy-
sis to transfer sentiment analysis models from
resource-rich languages to low-resource lan-
guages. However, these studies either rely on
external cross-lingual supervision (e.g., parallel
corpora and translation model), or are limited
by the cross-lingual gaps. In this work, based
on the intuitive assumption that the relation-
ships between emojis and sentiments are consis-
tent across different languages, we investigate
transferring sentiment knowledge across lan-
guages with the help of emojis. To this end, we
propose a novel cross-lingual sentiment anal-
ysis approach dubbed Curriculum Knowledge
Distiller (CKD). The core idea of CKD is to
use emojis to bridge the source and target lan-
guages. Note that, compared with texts, emojis
are more transferable, but cannot reveal the pre-
cise sentiment. Thus, we distill multiple Inter-
mediate Sentiment Classifiers (ISC) on source
language corpus with emojis to get ISCs with
different attention weights of texts. To transfer
them into the target language, we distill ISCs
into the Target Language Sentiment Classifier
(TSC) following the curriculum learning mech-
anism. In this way, TSC can learn delicate
sentiment knowledge, meanwhile, avoid being
affected by cross-lingual gaps. Experimental
results on five cross-lingual benchmarks clearly
verify the effectiveness of our approach.
1 Introduction
Nowadays, sentiment analysis approaches perform
very well by fine-tuning large-scale pre-trained lan-
guage models (Yang et al., 2019; Xie et al., 2020;
Wang et al., 2021). However, their success heavilyrelies on manual sentiment annotations. Therefore,
they fail to recognize the sentiment polarities in
low-resource languages that do not have sentiment
supervision.
Still, recent studies have attempted to iden-
tify sentiment in unlabeled languages by transfer-
ring sentiment knowledge from resource-rich lan-
guages, (e.g., English), to low-resource languages
(e.g., Japanese and Arabic) and propose the Cross-
Lingual Sentiment Analysis (CLSA) task (Zhou
et al., 2014). Generally, these CLSA approaches
can be divided into two patterns, parallel corpus
based approaches, and unsupervised approaches.
Through cross-lingual supervision provided by par-
allel corpus, the former kind of approaches can
align the semantic gaps between source language
and target language by Auto-Encoder (Zhou et al.,
2014), Neural Translation Model (Eriguchi et al.,
2018) and Bilingual Word Embedding (Barnes
et al., 2018). However, the availability of paral-
lel corpus limits the usability of these works.
To avoid relying on external cross-lingual super-
vision, recent studies have paid attention to a more
challenging setting dubbed Unsupervised Cross-
Lingual Sentiment Analysis (UCLSA) (Feng and
Wan, 2019; Fei and Li, 2020; Zhang et al., 2021).
Under this setting, sentiment classifiers need to
adapt to new languages without the help of paral-
lel corpus. Cross-lingual language models such as
m-BERT (Devlin et al., 2019), XLM-R (Conneau
et al., 2020) and LaBSE (Feng et al., 2020) have
been prevalent in UCLSA. These models are pre-
trained by multilingual corpus and then fine-tuned
by source language sentiment supervision. In addi-
tion, Unsupervised Machine Translation Fei and Li
(2020) and Multilingual Language Model Feng and
Wan (2019) have been proposed to align source and
target languages in UCLSA.
Although the above unsupervised approaches
have calibrated semantic representation over source
and target languages, their performances are still864
unsatisfactory due to the large language discrep-
ancy (Chen et al., 2017). In this work, we propose
to use emojis to promote cross-lingual sentiment
knowledge transfer, which can be dubbed Emoji-
Supervised Cross-Lingual Sentiment Analysis (ES-
CLSA). ESCLSA is motivated by the intuitive as-
sumption verified by Choudhary et al. (2018) that
the relationships between emojis and sentiments
are consistent across different languages . Thus,
we can use multilingual corpus with emojis to
bridge the cross-lingual gaps without manual cross-
lingual supervision by aligning sentiment polarities
with emojis. Notably, multilingual corpora with
emojis is easy to obtain as it can be crawled from
public social networks.
The most intuitive way to learn ESCLSA is man-
ually labeling polarities of emojis, and using the
emoji label as the pseudo-label of target language
sentences. However, emojis fail to represent neu-
tral sentiment or distinguish delicate polarities, e.g.,
very positive and positive. Meanwhile, the pseudo-
label based method ignores the sentiment informa-
tion in texts. Thus, it can not be well adapted to
practical application. To handle these challenges,
an end-to-end framework of ESCLSA needs to be
established. ELSA (Chen et al., 2019) is the only
previous approach for ESCLSA. It uses emoji pre-
diction task to learn sentence representations. Afterthat, ELSA uses these representations to classify
sentiment polarities. The emoji-oriented represen-
tations are transferable. However, they ignore the
information carried in text, which limits the perfor-
mance of ELSA.
In this paper, we propose a novel ESCLSA ap-
proach dubbed Curriculum Knowledge Distiller
(CKD). The overall architecture of CKD is shown
in Figure 1. Texts can reveal the precise sentiment
but are hard to transfer due to the cross-lingual gaps.
On the other hand, emojis are more transferable,
but the sentiment information in texts will be under-
estimated if we focus too much on the emojis (Chen
et al., 2019). To address this limitation in the previ-
ous work Chen et al. (2019), we propose to distill
multiple Intermediate Sentiment Classifiers (ISCs)
from Source Language Sentiment Classifier (SSC)
on source language tweets with emojis. By adjust-
ing the attention weights of texts in ISCs, ISCs
has different transferability and sentiment veracity.
The models which are trained with low attention
weights of texts (i.e., emoji-dominant ISCs), are
more transferable, while the models with high at-
tention weights of texts (i.e., text-dominant ISCs)
pay more attention to extract sentiment informa-
tion from texts. Based on these ISCs, we distill
theTarget Language Sentiment Classifier (TSC)
following the curriculum learning mechanism (Ben-865gio et al., 2009). The TSC is trained by distillation
losses computed from different intermediate mod-
els. Specifically, in the beginning of distilling TSC,
the weights of emoji-dominant ISCs are large, as
they are easier to transfer. During the distillation
process, we progressively increase the weights of
text-dominant ISCs to learn more precise sentiment
knowledge from texts. Compared with the previ-
ous works, CKD can integrate more transferable
information from emojis with more precise senti-
ment information in texts. Thus, CKD achieves the
balance between transferbility and sentiment verac-
ity. We verify our approach on five cross-language
sentiment analysis benchmarks. The experimen-
tal results clearly support the effectiveness of our
approach.
To sum up, the contributions of this work are
three-fold:
•We propose a novel cross-lingual sentiment
classification approach CKD which avoids the
language discrepancy problem with the help
of corpus with emojis in multilingual public
social networks.
•We propose to distill multiple ISCs with dif-
ferent attention weights of texts to balance the
transferability and the sentiment veracity. Fur-
thermore, we propose to use the curriculum
learning mechanism to distill these ISCs into
the target language sentiment classifier. In this
way, our model can transfer delicate sentiment
knowledge across cross-lingual gaps.
•We conduct extensive experiments on five lan-
guage pairs involving 11 tasks to evaluate our
approach. CKD outperforms all the baseline
models. That is, we provide a practical ap-
proach for cross-lingual sentiment analysis
without requiring cross-lingual supervision.
2 Related Work
This section briefly reviews works related to ours,
including cross-lingual sentiment analysis, knowl-
edge distillation, and curriculum learning.
2.1 Cross-Lingual Sentiment Analysis
CLSA aims at transferring source language trained
sentiment models to adapt target language (Zhou
et al., 2014; Eriguchi et al., 2018; Barnes et al.,
2018; Fei and Li, 2020). The early works of
CLSA rely on cross-lingual supervision. Hajmo-
hammadi et al. (2014); Al-Shabi et al. (2017); Chenet al. (2019) use Neural Translation Model to align
source and target languages. In addition, Bilingual
Word Embedding (Ziser and Reichart, 2018) and
parallel corpus (Xu and Yang, 2017) are also used
to bridge the cross-lingual gaps. Recent studies
propose the unsupervised CLSA (UCLSA) setting
to avoid relying on the expensive parallel corpus.
Adversarial Training (Chen et al., 2018), Cross-
Lingual Language Model (Feng and Wan, 2019)
and Unsupervised Machine Translation (Fei and
Li, 2020) have been introduced to achieve CLSA
without any cross-lingual supervision. Still, these
UCLSA approaches are limited when the gaps be-
tween source and target languages are very large,
e.g., English to Japanese (Chen et al., 2019). Thus,
we propose to use emojis to promote cross-lingual
sentiment knowledge transfer. Emojis represent
consistent sentiments across different languages
(Choudhary et al., 2018), which can be the bridge
across cross-lingual gaps.
2.2 Emoji Based Sentiment Analysis
Existing works have noticed the relevance between
emojis and sentiment. Liu et al. (2021) directly
inputs emojis as the following words into the senti-
ment classifier to improve sentiment analysis, while
Lou et al. (2020) proposes to fuse emoji sentiment
into text embedding by the attention algorithm and
Yuan et al. (2021) proposes to improve emoji em-
bedding by the graph network. In addition, Chaud-
hary et al. (2019) uses emojis for irony detection.
Although these studies have explored the role of
emojis in sentiment analysis, these works are de-
signed for the monolingual setting and require man-
ual annotations on texts with emojis. Thus, these
works cannot promote CLSA with the help of emo-
jis.
ELSA (Chen et al., 2019) is the first work of
ESCLSA, which uses the emoji prediction task to
learn sentence representations. After that, it uses
these representations to classify sentiment polari-
ties. The emoji-oriented representations are trans-
ferable. However, it ignores the information carried
in texts, which limits the performance of ELSA. In
this paper, we propose a novel Emoji-supervised
CLSA approach dubbed Emoji Knowledge Distilla-
tion to achieve the balance between transferability
and sentiment veracity.
2.3 Knowledge Distillation
Knowledge distillation transfers knowledge from
teacher model to student model by prompting stu-866dent model to learn the soft output of teacher model
(Hinton et al., 2015). In recent years, knowledge
distillation techniques have been wildly used in
cross-lingual studies. These studies use supervi-
sion in source language to train the teacher model
and distill it into unlabeled target language to cal-
ibrate the domain gaps for Cross-Lingual Conver-
sation (Sun et al., 2021), Cross-Lingual Sentiment
Analysis (Xu and Yang, 2017), Cross-Lingual Se-
mantic Relation Classification (Vyas and Carpuat,
2019), and Cross-Lingual Named Entity Recogni-
tion (Wu et al., 2020; Li et al., 2022). Note that Xu
and Yang (2017) is the previous work introducing
distillation in CLSA. However, it distills the target
language classifier on parallel corpora, which is
not available in our setting. Following these works,
we further propose to distill multiple intermediate
models from source language sentiment classifier
to obtain both high transferability and high senti-
ment veracity models.
2.4 Curriculum Learning
Curriculum learning aims at solving the challenge
of “how to study”, by referring to the human learn-
ing strategy of studying from the easy samples
to the hard samples (Bengio et al., 2009). The
key challenge of curriculum learning is finding the
rank of samples from easy to hard, as well as de-
termining the timing of introducing hard samples
(Soviany et al., 2022). Recent studies determine
the learning sequence by the sentence length and
the word rarity (Platanios et al., 2019), the learn-
ing curve of model (Matiisen et al., 2019), and
Meta Learning (Zhan et al., 2021). In our approach,
the transferability of intermediate models increases
with the attention weight of texts decreasing. There-
fore, we distill ISCs into TSC in the order of easy-
to-transfer to hard-to-transfer, following the cur-
riculum learning mechanism.
3 Methodology
In this section, we introduce the methodology of
our proposed CKD. The key idea of our approach is
using emojis that express the consistent sentiment
in source and target languages to distill sentiment
knowledge across languages. Therefore, we use
source language supervision to train SSC, firstly.
Then, as emojis can not explain such precise sen-
timent as texts but are easier to transfer, we pro-
pose to distill multiple ISCs with various attention
weights of texts. The sentiment knowledge learnedAlgorithm 1 CKD Training Pipeline
SSC, ISC, TSC←SSC, the j-th ISC,
and TSC models at iteration m;
SSC, ISC, TSC←SSC, the j-th
ISC, and TSC models at iteration m−1;
X←source language labeled samples;
W←source language unlabeled tweets with
emojis;
W←target language unlabeled tweets with
emojis;
# Train SSC
form= 1,2, ..., M do
x, y← X;
SSC←Adam model update ∇L(x, y);
SSC←SSC;
end for
# Distill {ISC, ISC, ..., ISC}
form= 1,2, ..., M do
w, w← W;
ISC←Adam update ∇L(w, w);
ISC←ISC;
end for
forj= 2, ..., n do
form= 1,2, ..., M do
w← W;
ISC←Adam update ∇L(w);
ISC←ISC;
end for
end for
# Distill TSC
form= 1,2, ..., M do
w, w← W;
forj= 1, ..., n do
β←();
end for
TSC←Adam update ∇L(w, w);
TSC←TSC;
end for
by ISCs with higher text attention weights is more
precise but less transferable. Finally, we distill
ISCs into TSC following the curriculum learning
mechanism. Specifically, for the ISCs that are easy
to transfer, we assign high weights of their distilla-
tion losses first, then decrease the weights during
training. On the contrary, for the ISCs that are
hard to transfer, we reverse the direction of weight
change. In this way, TSC can learn precise senti-
ment knowledge from those hard-to-transfer ISCs,867meanwhile avoiding the language discrepancy prob-
lem with the help of those easy-to-transfer ISCs.
In the following subsections, we will introduce
the implementation details of our proposed ap-
proach. The overall architecture of CKD is shown
in Figure 1.
3.1 Problem Description
In ESCLSA, we are given the labeled source lan-
guage samples without emojis X={(x, y)},
where y∈ {1, ..., c},cis the number of senti-
ment polarities. The unlabeled source/target lan-
guage tweets with emojis W= (w, w)/W=
(w, w), where wandwrepresent a source lan-
guage tweet retained and eliminated emojis, respec-
tively.
3.2 Distillation in Source Language
In this section, we learn several intermediate clas-
sifiers {ISC, ISC, ..., ISC}with different at-
tentions on the emojis to consider both sentiment
veracity and cross-lingual transferability. We train
a source language sentiment classifier SSC byX
to distill source language sentiment knowledge,
firstly, formally,
where Lis a cross-entropy loss function, and
Iis an indication function.
As discussed in Section 3, we distill
SSC into multiple intermediate models
{ISC, ISC, ..., ISC}, where nis the
number of ISCs. For a latter model ISC, we
set the attention weights of words to be smaller.
Therefore, with the increase of j, model ISC
pays more attention to emojis, the veracity of
sentiment is lower, and the transferability is higher.
We use the Transformer (Vaswani et al., 2017) like
architecture for ISCs. Thus, the attention weights
of words can be adjusted by modifying Eq. (1) in
Vaswani et al. (2017) to
where αis the hyper-parameter that represents the
attenuation rate of words for M, andE∈ {0,1}
is the word mask vector, in which 1means the
current token is a word, on the country 0means
it is an emoji, kis the sentence length. Usingadjusted attention function Eq. 2, we distill ISC
from SSC by cross-entropy loss, formally,
Note that as SSC is trained on X, which is a text
dataset without emojis, emojis are eliminated for
the input of SSC . Then we distill the rest inter-
mediate models ISC, ..., ISCfrom the previous
model to gradually improve the transferability, for-
mally,
where 2≤j≤n. Through the above steps, a
series of intermediate models with different trans-
ferability and sentiment veracity is established.
3.3 Curriculum Distillation in Target
Language
We use curriculum learning to integrate the above
classifiers {ISC, ISC, ..., ISC}into the target
language sentiment classifier TSC to infer the sen-
timent polarity in the target language. Inspired by
the “from easy to hard” learning strategy of cur-
riculum learning (Bengio et al., 2009), we trained
TSC by distillation losses computed from different
intermediate models. Specifically, in the beginning
of distilling TSC, the weights of emoji-dominant
ISCs are large. During the distillation process, we
progressively increase the weights of text-dominant
ISCs. To achieve this goal, the weight of each ISC
changes smoothly in the iteration by an adjustable
weight function, formally,
where λ∈[−1,1]is the hyper-parameter to deter-
mine the slope, mis the current iteration number, γ
is the hyper-parameter to determine the stop state,
andMis the total iteration number. The distillation
loss function Lof each ISCin iteration mis
shown as,
Thus, the overall objective function Lcan be
formulated as,868Dataset Language Number of Classes Number of Samples
Amazon Review (Duh et al., 2011) Multi 2 4,000
Yelp (Zhang et al., 2015) English 5 700,000
Hotel Review (Lin et al., 2015) Chinese 5 20,000
Social Media Posts (Mohammad et al., 2016) Arabic 3 1,000
Note that, in the consideration of insuring Tto
learn sentiment knowledge in the target language,
we use the tweets eliminated emojis was the input
ofT.
3.4 Training
In this section, we introduce the training strategy
for CKD. First, we use source language supervised
dataset Xto train the source language sentiment
classifier SSC . Then we distill SSC to multi-
ple intermediate models {ISC, ISC, ..., ISC}
where the text attention weights of these models
are from high to low. Moreover, we set the curricu-
lum schedule in the reverse order, i.e., from ISC
toISC. Finally, we distill the target language
sentiment classifier Taccording to this schedule
by the adjustable weight function β. Algorithm 1
describes our iterative training pipeline in detail.
More training details are described in Section 4.2.
4 Experiments
In this section, we conduct extensive experiments
to verify the effectiveness of our proposed ap-
proach. Following Fei and Li (2020), eleven cross-
lingual tasks based on five language pairs are used
in the experiments. All those pairs take English to
be the source language, and the target languages
are German, French, Japanese, Chinese and Arabic,
respectively.
4.1 Datasets
To leverage the supervision of emojis, we establish
a Twitter corpora with emojis for each source and
target language pair. We collect 20,000 unlabeled
tweets with emojis for each language posted in
December 2018 and use sentiment emojis listed
by Yin et al. (2021).
For labeled data, as shown in Table 1, we employ
samples in six different languages (i.e., English,German, French, Japanese, Chinese and Arabic)
from the following datasets:
Amazon Review (Duh et al., 2011): This dataset
consists of four languages (i.e., English, German,
French and Japanese) for the binary sentiment clas-
sification problem, while each language contains
three domains (i.e., Books, DVD, and Music). For
each cross-lingual task, there are 2,000 samples for
train and 2,000 for test.
Yelp (Zhang et al., 2015): It is a large-scale En-
glish sentiment dataset containing 700K reviews
from five classes. We use the original class tags of
Yelp for the English-Chinese pair, and convert it
into 3 sentimental levels, i.e., 1,2→“negative”,
3→“neutral”, and 4,5→“positive”, for the
English-Arabic pair.
Hotel Review (Lin et al., 2015): This dataset con-
tains 170K Chinese hotel reviews from 5 classes as
in the Yelp dataset, and it is used for the English-
Chinese pair in the experiments. Following Fei and
Li (2020) we randomly sample 20K reviews for
test.
Social Media Posts (Mohammad et al., 2016): It
is an Arabic sentiment dataset with three sentimen-
tal labels(i.e., positive, neutral and negative). We
randomly sample 1000 instances and use them for
testing on the English-Arabic pair.
4.2 Experimental Setup
We compare our proposed CKD with a number
of baseline methods under different categories: i)
methods with explicit cross-lingual supervision
(i.e., LR+MT, CR-RL (Xiao and Guo, 2013), Bi-
PV (Pham et al., 2015) and CLDFA (Xu and Yang,
2017)); ii) methods with implicit cross-lingual
supervision (i.e., UMM (Xu and Wan, 2017),
PBLM (Ziser and Reichart, 2018), DAN (Chen
et al., 2018), mSDA (Chen et al., 2012) and
ADAN (Chen et al., 2018)); iii) methods with
no cross-lingual supervision (i.e., BWE (Conneau869
Approach Chinese (5) Arabic (3)
LR+MT 34.01 51.67
DAN 29.11 48.00
mSDA 31.44 48.33
ADAN 42.49 52.54
m-BERT 38.85 50.40
XLM-R 46.60 51.16
MVEC 43.36 49.70
CKD 49.86 53.14
et al., 2017), MAN-MoE (Chen and Qian, 2019),
m-BERT (Devlin et al., 2019), XLM-R (Conneau
et al., 2020), CLIDSA (Feng and Wan, 2019),
MVEC (Fei and Li, 2020)); iv) the emoji super-
vised cross-lingual sentiment analysis model (i.e.,
ELSA (Chen et al., 2019)). It is worth noting that
our CKD belongs to the fourth category, whichmakes use of emojis to bridge different languages.
In addition, the compared baselines are validated
on different benchmarks. Hence, we use different
baselines in Table 2 and Table 3 for fairness.
In our proposed CKD, we use XLM-R (Conneau
et al., 2020) to be the backbone for all SSC, ISCs,
and TSC. XLM-R is pre-trained on CommonCrawl
(Wenzek et al., 2020) which is a large-scale unsu-
pervised multilingual corpus. All the CKD compo-
nents are optimized by Adam optimizer (Kingma
and Ba, 2014) with the learning rate of 3×10
and the mini-batch size of 28. The iteration time M
for each step is 4,000, and the early stop rate γis
0.8. In addition, we set the number of ISCs to be 4,
to balance the training costs and the performance of
our approach. For these ISCs, the attenuation rate
of words αare(0,0.1,0.5,1), and the slope param-
eters λare(−1,−0.5,0.5,1). Moreover, we use
class average accuracy to evaluate the performance
of CKD and baselines.
4.3 Comparison
The classification results on Amazon Review
Dataset, English-Chinese, and English-Arabic pairs
are shown in Table 2 and Table 3 respectively
(where (n) indicates the number of sentiment polar-
ity). Our proposed method outperforms all the com-
pared methods in all target languages. Specifically,
as shown in Table 2, our model significantly im-
proves binary sentiment classification performance
in German, French, and Japanese, achieving 2.36%
to 3.43% improvements on each language aver-870agely. As for multi-class sentiment classification,
our approach achieves 0.6% to 3.26% improve-
ments in Chinese and Arabic respectively. Note
that, Japanese, Chinese, and Arabic are further
from the source language, compared with German
and French. The improvements of CKD are greater
in these languages. This shows that emojis is a
powerful bridge to transfer sentiment knowledge
across languages, especially when the cross-lingual
gaps are large.
In addition, compared with the early emoji pow-
ered work ELSA (Chen et al., 2019), our approach
also brings significant improvements. The rea-
son for this is that our model distills sentiment
knowledge from source language and emojis, rather
than using the emoji prediction task to learn cross-
lingual consistent representations like ELSA does.
The sentence representations learning from emoji
prediction task only involves knowledge correla-
tion to emojis. On the contrary, our TSC learns
more sentiment knowledge from source language
with the help of those ISCs with high attention
weights to texts.
As for the baseline models, we can see that
language model pre-trained approaches, i.e., m-
BERT and XLM-R achieve impressive perfor-
mances, which shows the effectiveness of pre-
training on large-scale unsupervised multilingual
corpus and the high semantic comprehension abil-
ities of Transformer (Vaswani et al., 2017) based
architectures. On the other hand, ELSA also per-
forms well. ELSA uses the emoji prediction task to
generate sentence embedding for both source and
target languages. The high cross-lingual consis-
tency of emojis brings ELSA the excellent ability
of cross-lingual sentiment transfer. Still, they ig-
nore the information carried in texts. Thus, we can
achieve better performance than ELSA as discussed
above.
4.4 Analysis
As shown in Table 4, we conduct the ablation study
to demonstrate the contributions of each compo-
nent in CKD. The first line represents the model
trained with all proposed components. The next
two lines represent the models without curriculum
learning (Eq. (5)) and without attenuation of text
attention (Eq. (2)), respectively. Note that as remov-
ing Eq. (2) makes ISCs no different, the third line
represents the results of using the single ISC with
attenuation rate α= 0. In addition, the last line rep-
resents directly evaluating SSC on target language.
It is clear that all the key parts of CKD generally
make good contributions to promote cross-lingual
sentiment knowledge transfer. Specifically, from
the second line, the curriculum learning mecha-
nism promotes TSC to learn ISCs flexibly, which
achieves higher performances than averagely dis-
tilling ISCs. From the third line, we can see that
even without attenuating the attention weight of871texts, distillation still works. This is mainly caused
by the soft labels in knowledge distillation that can
filter samples which are hard to transfer. Another
reason is that the model gains the cross-lingual
transfer ability by paying attention to emojis.
In addition, we recommend distilling multiple
ISCs with different attenuation of text attention
weights. To evaluate the effectiveness of these
ISCs, we test the performance of distilling TSC
from each single ISC. Results are shown in Fig-
ure 2. TSC distilled from all ISCs performs bet-
ter than those distilled from a single ISC. As for
the performances of distilled from a single ISC,
they show different patterns in different languages.
In French, the ISC with 0.1attenuation of texts
performs best, but in Japanese, the ISC with the
highest attenuation of texts performs best. The rea-
son for this is that French is closer to English and
shares more tokens in the tokenizer of XLM-R. On
the contrary, Japanese is far from English, which
gives emojis a more important role in cross-lingual
sentiment transfer. Notably, in real applications,
the test set of target language is unavailable. Thus,
it is not feasible to manually select the attenuation
of texts in ISCs.
As shown in Figure 3, we further analyze the
effect of the size of unlabeled Twitter corpora on
model performance. Generally, with a larger size
of Twitter corpora, CKD can transfer more knowl-
edge from the source language to the target lan-
guage, and the performance will be better. 20,000
tweets for both source and target languages is the
trade-off between the training cost and the perfor-
mance. We can see that for Japanese, the perfor-
mance decreases more severely with the decreas-
ing of Twitter corpora size. This is mainly caused
by the larger cross-lingual gap between English
and Japanese compared with English-French pair.
When the cross-lingual gap is large, more tweet
samples are required for achieving cross-lingual
knowledge transfer.
Moreover, we conduct the sensitivity analysis
for the early stop rate γto demonstrate the robust-
ness of our model displayed in Figure 4. From
this, we can see that the accuracy is sensitive to
γ, as it determines the weight of Eq. 6 for each
ISC at the end of distilling TSC. In the beginning,
when TSC learns more from ISCs that are hard to
transfer, the accuracy is higher. However, if γis
too large, the performance will degrade. This is
because ISCs which pay much attention to textswill overfit the source language. Following the
curriculum learning mechanism, TSC is first dis-
tilled from emoji-dominant ISCs , and then from
text-dominant ISCs. Experimental results shown
in Figure 4 verify the effectiveness of this strategy.
5 Conclusion
Emojis are widely used in social networks of vari-
ous languages. Based on the intuitive assumption
that the relationships between emojis and senti-
ments are cross-lingual consistent, we use the unsu-
pervised multilingual corpus with emojis to transfer
sentiment knowledge across languages. To achieve
this goal, we propose CKD, a novel curriculum
sentiment knowledge distillation framework. Us-
ing the source language supervision trained model,
we distill a series ISCs. These ISCs have differ-
ent transferability and sentiment veracity. Finally,
we distill RSCs into TSC under the curriculum
learning mechanism. That is, in the beginning of
training, the weights of emoji-dominant ISCs are
large, as they are easier to transfer. Then we in-
crease the weights of text-dominant ISCs to learn
more precise sentiment knowledge. In this way,
CKD learns precise sentiment knowledge, mean-
while avoiding the language discrepancy problem.
Extensive experiments on five languages involving
11 tasks verify the effectiveness of our approach.
Limitations
Our approach needs to distill ISCs one after another.
Although the time complexity of such a distillation
strategy used in our proposed CKD is a bit high, it
boosts the classification performance by a consid-
erable margin. Moreover, it is worth mentioning
that the scope of this cross-lingual work focuses on
pairs of languages. In the future, we will extend
our CKD to learn one universal model that caters
for all target languages at the same time.
Acknowledgments
This project is supported by the National Natu-
ral Science Foundation of China (No. 62106204
and 62172075), the Sichuan Natural Science Foun-
dation (No. 2022NSFSC0911), the Chengdu In-
novation and Technology Project (No. 2021-
YF05-02414-GX) and the Fundamental Research
Funds for Central Universities of China (No.
2682022CX068). J. Zhang and T. Liang contribute
equally to this work.872References873874875