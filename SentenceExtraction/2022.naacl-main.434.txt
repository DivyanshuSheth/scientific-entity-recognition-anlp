
Encarna Segarra Vicent AhuirLluís-F. Hurtado José Ángel González
VRAIN: Valencian Research Institute for Artificial Intelligence
Universitat Politècnica de València, Spain
{esegarra, lhurtado, jogomba2}@dsic.upv.esviahes@eui.upv.es
Abstract
The application of supervised methods to au-
tomatic summarization requires the availabil-
ity of adequate corpora consisting of a set of
document-summary pairs. As in most Natu-
ral Language Processing tasks, the great ma-
jority of available datasets for summarization
are in English, making it difficult to develop
automatic summarization models for other lan-
guages. Although Spanish is gradually forming
part of some recent summarization corpora, it
is not the same for minority languages such
as Catalan. In this work, we describe the con-
struction of a corpus of Catalan and Spanish
newspapers, the Dataset for Automatic sum-
marization of Catalan and Spanish newspaper
Articles (DACSA) corpus. It is a high-quality
large-scale corpus that can be used to train sum-
marization models for Catalan and Spanish. We
have carried out an analysis of the corpus, both
in terms of the style of the summaries and the
difficulty of the summarization task. In particu-
lar, we have used a set of well-known metrics in
the summarization field in order to characterize
the corpus. Additionally, we have evaluated the
performance of some extractive and abstractive
summarization systems on the DACSA corpus
for benchmarking purposes.
1 Introduction
Automatic summarization is one of the central prob-
lems in Natural Language Processing (NLP). The
development of automatic summarization systems
is an important issue due to the great amount of
information in different formats that is accessible
on the web or in other repositories. It is necessary
to develop techniques that help us to tackle that
huge amount of information. For this reason, there
is an increasing interest in the NLP community
to develop techniques that allow the users to find,
read, understand, or process the documents. In this
context, automatic summarization can be an impor-
tant aid because it provides a condensed versionof documents that reduce the time to explore or
analyze them.
Access to large-scale high-quality data is an es-
sential prerequisite for making substantial progress
in summarization. The application of supervised
methods to automatic summarization, as those
based on Neural Networks, requires the availabil-
ity of adequate corpora consisting of document-
summary pairs. The construction of large-scale and
high-quality corpora for learning neural summariza-
tion models is not an easy task. It is necessary a
great human effort to generate thousands of man-
ual summaries, or to design new approaches to ob-
tain these summaries in a semiautomatic way. The
first important resource for learning corpus-based
summarization models was the CNN/DailyMail
summarization corpus (Hermann et al., 2015), orig-
inally constructed for the task of passage-based
question answering and adapted to the document
summarization task. It consists of news stories
from CNN and DailyMail and contains 312,077
article-summary pairs. Afterwards, another En-
glish corpus was provided to the research commu-
nity for summarization purposes, the NewsRoom
corpus (Grusky et al., 2018). It consists of 1.3
million article-summary pairs that have been writ-
ten by the authors and the editors of 38 different
major news publications. The corpus was created
through a web-scale crawling of over 100 million
pages from a set of online publishers by gather-
ing the news and using the summaries provided in
the HTML metadata. The summaries contained in
this corpus combine both extractive and abstrac-
tive strategies to describe the content of the articles.
Also in 2018, the XSUM corpus (Narayan et al.,
2018a) was presented, it is a large scale dataset
obtained by harvesting online articles from the
British Broadcasting Corporation (BBC) with one-
sentence news summary.
As in most NLP tasks, the great majority of avail-
able datasets for summarization are in English. The5931lack of this kind of resources for other languages
is an encumbrance to modeling that constraints
the impact of language technologies on minority
language communities. The creation of a large-
scale Indonesian summarization dataset of 215,827
document-summary pairs, has just been published
(Koto et al., 2020). Recently, some datasets that
aim to fill the gap among English and other lan-
guages for the automatic summarization task have
been proposed: MLSUM (Scialom et al., 2020),
MassiveSumm (Varab and Schluter, 2021), and
XL-Summ (Hasan et al., 2021). Although Spanish
is the world’s second-most spoken native language
and is the official language in 21 countries, it has
only recently been considered in general domain
summarization datasets, as the aforementioned, and
in specific domains as in (González et al., 2019).
The situation is worse for Catalan, although it is not
an endangered language, it is spoken by 10 million
people in Spain and other three European countries,
it is minority worldwide and is underrepresented
or even not considered in summarization corpora.
In this work, we describe the construction of
a corpus of Catalan and Spanish newspapers, the
Dataset for Automatic summarization of Catalan
and Spanish newspaper Articles (DACSA) corpus.
With the aim of building a quality large-scale cor-
pus that could be used to learn automatic summa-
rization neural models for Catalan and Spanish,
we used a strategy inspired by the construction of
the NewsRoom corpus (Grusky et al., 2018). We
conducted a crawling process on 30 different news-
paper websites to extract articles and summaries in
a straightforward way. The crawling included from
Spanish mass media to regional newspapers. In or-
der to obtain the summaries, we took advantage of
the highlights and summaries, provided by authors
or editors of the articles.
To ensure the quality of the DACSA corpus, we
perform two subsequent filtering processes on the
downloaded articles. The first filter was used to
ensure, at least, a minimum length in both the arti-
cle and the abstract. All the articles or summaries
that were considered too short were discarded. Ob-
viously, an article or summary too short implies
discarding the article-summary pair. The second
filter was used to ensure that the summaries were
not almost verbatim copies of the first sentences of
the articles. To do this, the article-summary pairs in
which the overlapping between the summary and
the article prefix of the summary length was highwere also discarded. This way, we try to avoid a po-
sitional bias in the summaries by discarding those
samples in which the summary is reduced to select
the first sentences of the article.
Once both filters were applied, we found that
some newspaper sources had very few samples, less
than 1000 in some cases. To balance the corpus
partitions, we decided to remove the sources with
few samples from the training, validation, and tests
sets. Nevertheless, we joined together the samples
from those sources to create a special test set, a test
set with sources not present in the training process.
Therefore, the corpus consists of four partitions per
language: training, validation, and test sets along
with an extra test set. Considering all the partitions,
the DACSA corpus consists of a set of 725,184
article-summary pairs extracted from 9 different
Catalan newspaper websites and 2,120,649article-
summary pairs extracted from 21 different Spanish
newspaper websites. The DACSA corpus contains
articles and summaries about politics, economics,
sports, culture and other topics usually addressed
in journalistic domains. To our knowledge, the
DACSA corpus is the largest summarization dataset
for both languages.
We have used four well known metrics in the
summarization field in order to characterize the cor-
pus. These metrics are: extractive fragment cover-
age and density (Grusky et al., 2018), abstractivity-
p (Bommasani and Cardie, 2020), and novel n-
grams (Kry ´sci´nski et al., 2018). Additionally, for
benchmarking purposes, we have evaluated the per-
formance of 6 automatic summarization systems on
the DACSA corpus. Concretely, we have used two
unsupervised systems (lead-2 and textRank), an ex-
tractive summarization system, SHANN (González
et al., 2019), two abstractive summarization sys-
tems, mBART (Liu et al., 2020) and mT5 (Xue
et al., 2020), and one oracle to compute upper
bounds of the performance in the DACSA corpus.
The DACSA corpus can be requested for
research purposes at https://xarrador.
dsic.upv.es/resources/dacsa .
2 Related Work
The automatic text summarization problem has
been addressed in the literature using abstractive,
extractive, or mixed approaches. On the one hand,
extractive approaches compose summaries by se-
lecting sentences or words directly from the doc-
uments (Cheng and Lapata, 2016; Nallapati et al.,59322017; Liu and Lapata, 2019; Narayan et al., 2018b;
Zhang et al., 2018; Dong et al., 2018; Yao et al.,
2018; Chen and Bansal, 2018). Most of these ap-
proaches address a sequential binary sentence clas-
sification problem in order to select the most salient
sentences of the documents, following different
criteria such as negative log likelihood on prese-
lected sentences (Cheng and Lapata, 2016; Nallap-
ati et al., 2017; Liu and Lapata, 2019) or ROUGE
(Lin, 2004) rewards in reinforcement learning en-
vironments (Narayan et al., 2018b; Zhang et al.,
2018; Dong et al., 2018; Yao et al., 2018). Other
extractive architectures are based on siamese hier-
archical attention networks built in terms of Long
Short Term Memories and Transformer encoders
(González et al., 2019, 2020). These models have
been succesfully applied in summarization tasks
of Spanish newspapers and talk shows (González
et al., 2019). On the other hand, the abstractive
approaches build the summaries by paraphrasing
the sentences of the documents (See et al., 2017;
Paulus et al., 2018; Ive et al., 2019). The vast ma-
jority of existing neural abstractive summarization
models are based on encoder-decoder architectures
(Sutskever et al., 2014). Finally, there are also
mixed approaches that combine extractive and ab-
stractive techniques, performed in a decoupled way
or simultaneously inside the models (Mendes et al.,
2019).
Due to the recent success of self-supervised
learning, the focus of text summarization research
has exhibited a gradual shift from extractive tech-
niques to abstractive techniques (Lewis et al., 2020;
Zhang et al., 2020a; Raffel et al., 2020). These kind
of objectives allows to pretrain deep architectures
(mainly Transformers) to learn vast amounts of gen-
eral linguistic knowledge from large corpora, that
can be transferred to downstream tasks by means
of finetuning. The most successful model of this
type is BERT (Devlin et al., 2019), that is pre-
trained with Masked Language Model and Next
Sentence Prediction objectives on raw texts from
English Wikipedia and BooksCorpus. Based on
BERT, some architectural improvements have been
proposed like RoBERTa (Liu et al., 2019) or AL-
BERT (Lan et al., 2020).
In some recent works, BERT and RoBERTa have
been finetuned for extractive summarization (Liu
and Lapata, 2019; Zhong et al., 2020), but, although
it boosted the performance of the previous extrac-
tive approaches, the pretraining+finetuning philos-ophy has shown to be most effective for abstractive
systems. Nowadays, the best performing abstrac-
tive models are BART (Lewis et al., 2020), T5
(Raffel et al., 2020) and PEGASUS (Zhang et al.,
2020a), being all of them Transformers (Vaswani
et al., 2017) pretrained self-supervisedly as denois-
ing sequence to sequence autoencoders. Some
multilingual variants of these models have been
recently proposed, mBART (Liu et al., 2020) and
mT5 (Xue et al., 2020). Both of them were pre-
trained following a multilingual denoising proce-
dure on large-scale multilingual corpora. On the
one hand, the mBART model was pretrained by
using a corpus of 25 languages, extracted from the
Common Crawl (Wenzek et al., 2020) (CC25). On
the other hand, a multilingual variant of the Colos-
sal Clean Crawled corpus (Raffel et al., 2020) was
used to pretrain mT5.
Self-supervised pre-training requires obtaining
large amounts of raw data in order to learn good
initializations of deep models from denoising ob-
jectives. Also, the fine-tuning of these architectures
in downstream tasks like text summarization im-
plies the availability of adequate corpora consisting
of document-summary pairs. As we mention above,
the great majority of datasets for summarization are
in English: CNN/DailyMail, NewsRoom, XSUM
(Narayan et al., 2018a), and so forth. Although
some multilingual datasets have been recently cre-
ated, as MLSUM, MassiveSumm, and XL-Summ,
they do not provide a large enough portion of Span-
ish data and only MassiveSumm provides a few
samples for Catalan. It is in this context where we
propose to build the DACSA corpus.
The most used metrics in the literature to quan-
tify the performance of the models in the sum-
marization task are ROUGE (Lin, 2004) and
BertScore (Zhang et al., 2020b). On the one hand,
ROUGE measures the performance by counting
exact matches. On the other hand, BertScore is
a more semantic measure which is based on con-
textual embeddings provided by a BERT language
model. These metrics are convenient to evaluate
the performance, but they do not explicitly measure
the abstractivity. Measuring the abstractivity of the
summaries generated by the models is generally
not trivial. In this work, we used a set of metrics as
abstractivity indicators to asses the level of abstrac-
tivity: extractive fragment coverage and density
(Grusky et al., 2018), abstractivity(Bommasani
and Cardie, 2020), and novel n-grams (Kry ´sci´nski5933et al., 2018). Additionally, we also used ROUGE
and BertScore to compare the different summariza-
tion models.
3 Building the DACSA corpus
The DACSA corpus was collected using a dis-
tributed web crawler that captured over 6 million
news articles, close to 2 million of articles pub-
lished in Catalan, and more than 4 million written
in Spanish. The articles were captured from 30
newspapers sources, 9 sources for Catalan and 21
sources for Spanish. The range of years of publica-
tion was between 2010 to 2020.
We divided the crawling process into two ser-
vices. The first service was designed to retrieve
the list of articles on the website of the newspa-
pers source; we refer to this service as the URLs
extractor service . The second one aims to extract
the content (article content and summary) of the
article; we refer to this service as the content ex-
tractor service . The whole crawler was developed
with Python 3 and JavaScript (Node.js runtime)
programming languages.
For the configurations (one per source) of the
content extractor service , we used CSS selectors
and the library cheerio (https://cheerio.
js.org/ ). In order to capture the article and sum-
mary text, we designed the selectors that captured
the visible information that a person would read,
avoiding metadata. Using visual information in-
stead of metadata is important because we detected
that likely the metadata was automatically created
by some naive process that could lose information,
such as just extracting the first tokens of the arti-
cle; meanwhile, the visual text is likely complete,
readable and coherent.
We searched websites of electronic newspapers
published in Spain, in Catalan or Spanish lan-
guages. To find the addresses of each article, we
decided to use the list of news that electronic news-
papers usually have on their website. The benefit
of using the list of articles provided by these web-
sites, contrary to the common crawling approach
of following every link, was that we aimed the arti-
cles themselves, and there was no need to identify
whether the web page is a news article or other
kind of content. Thus, from the list of news in that
newspapers source, we created two configurations,
one for the URLs extractor service and another for
thecontent extractor service .
We intended DACSA to be a large-scale, high-quality corpus for Catalan and Spanish. Thus, after
the massive capture of samples, we defined two
requirements that the articles and summaries must
satisfy. We first established a threshold in the mini-
mum number of words of the article and the sum-
mary, and second, a threshold in the maximum sim-
ilarity between the summary and the first sentences
of the article.
On the one hand, we discarded those samples
with a short text in the article or the summary.
Specifically, every sample inside the corpus con-
tains at least 100 words in the article and 10 words
in the summary. With this restriction, we ensure
that the samples have enough content to generate a
summary with a reasonable length.
On other hand, we rejected from the corpus those
samples in which the summary is generated by
simply extracting the first sentences of the article.
Specifically, we restricted the overlapping between
the summary and the starting sentences of the arti-
cle by using a similarity metric based on the Lev-
enshtein distance to quantify the degree of overlap-
ping. The Equation (1) presents the definition of
this metric.
f(A, S ) = 1−Levenshtein (A, S)
|S|(1)
where Ais the sequence of words of the arti-
cle text, Sis the sequence of words of the sum-
mary, |S|is number of words of the summary,
andLevenshtein is the operation which returns
the well-known Levenshtein distance between two
texts. In this corpus, we established a maximum
threshold of 0.9 of f(A, S )between the article and
the summary.
4 Dataset
After the above processes, the DACSA corpus was
built. This corpus provides pairs of news article and
its summary from different newspapers for both,
the Catalan and the Spanish languages. Regarding
the Catalan set, there are 725,184 sample pairs
from 9 newspapers, regarding the Spanish set, the
corpus provides 2,120,649sample pairs from 21
newspapers.
The amount of samples by newspapers source is
far from being homogeneous. If these distributions
would be preserved over the partitions (training,
validation, and test set), the models will focus their
learning in the predominant newspapers. To avoid5934
this bias and achieve more general models, we pro-
pose that the test and validation sets be created in
a way that all newspapers have roughly the same
number of samples. To achieve this balance, we
discarded some sources in order to guarantee that
all sources represent at least 5% of samples in each
one of these two sets. Additionally, we discarded
those sources that have lower compression ratio
than 10% in their summaries, since we considered
these summaries too long compared to their corre-
sponding articles.
The three sets for Catalan (training, validation
and test set) are composed by 6 of the 9 newspapers,
the training set contains 636,596samples, and the
validation and test sets have 35,376samples each
one. For Spanish, the three sets are composed by
13 of the 21 newspapers, the training set contains
1,802,919samples, and the validation and test sets
have 104,052samples each one.
All the sources excluded were used as a separate
test set. This partition allows evaluating the gener-
alization capabilities of the summarization models
against unseen newspaper sources. In this work,
we refer to the test set with newspapers included
in the training set as TESTand to the test set that
contains newspapers not included in the training
set as TEST. The statistics of all the sets are
shown in Tables 1 and 2.
In the Appendix A, Tables 7 and 8 show the
distribution and the average lengths in terms of
sentences and words of the articles and their sum-
maries for Catalan and Spanish sets, detailed by
the different newspaper sources.5 Analysis of Dataset
In this section, an analysis of the level of abstrac-
tivity of the summaries of the corpus is done. First,
the definition of the different measures used in this
work is given, and second, we provide the applica-
tion of these measures to the DACSA corpus.
5.1 Definition of Abstractivity Metrics
We used a set of metrics as abstractivity indicators
to asses the level of abstractivity, they capture the
degree of text overlapping between the summary
and article. In particular, the following metrics
have been selected: extractive fragment coverage
and density, abstractivity, and novel n-grams.
Extractive Fragment Coverage (Grusky et al.,
2018): the coverage measure quantifies the extent
to which a summary is derivative of a text, that is, it
measures the percentage of words in the summary
that are part of an extractive fragment of the article.
Extractive Fragment Density (Grusky et al.,
2018): contrary to the coverage, the density mea-
sure takes into account the length of the extractive
fragments. A summary might contain many indi-
vidual words from the article and therefore have a
high coverage, however it might have a low density
if the extractive fragments are short.
Abstractivity(Bommasani and Cardie, 2020):
theabstractivitymetric measures abstractivity as
the absence of overlapping between the summary
and the original text. Higher values indicate less
overlapping and higher abstractivity. The pparam-
eter weights the length of each extractive fragment,5935the higher value of p, the more the length of the
extractive fragment is penalized.
Novel n-grams : (Kry ´sci´nski et al., 2018) the
novel n-grams metric quantifies the n-grams in-
troduced in the summary that did not appear in
the original text. The value of the metric is a per-
centage over the total of n-grams contained in the
summary.
Additionally, we also used the Compression Ra-
tio, that is, the ratio between the length of article
and summary. Summarizing with higher compres-
sion is challenging as it requires capturing more
precisely the critical aspects of the article text.
5.2 Dataset Abstractivity
This section presents the results of the abstractivity
metrics described in Section 5.1 for the DACSA
corpus. The results are shown separately for both
languages; Table 3 shows the average values of
the partitions for Catalan and Table 4 for Spanish.
Tables 9 and 10 in the Appendix B also show these
results for each newspaper source.
As Tables 3 and 4 show, the training andvali-
dation partitions have a similar type of summaries
regarding their degree of abstractivity. The sum-
maries in the test partitions, except the TESTset
for Spanish, also show similar degree of abstractiv-
ity as the previous partitions.
In order to better characterize the corpus, we also
present in Figure 1 the distributions of the samples
by combining the values of extractive fragment
coverage andextractive fragment density of their
summaries, and in Figure 2 the distribution of the
samples by combining the values of abstractivity
(p=2) andnovel 2-grams . These plots help to iden-
tify visually the degree of abstractivity of the sum-
maries in the Catalan and Spanish sets. On the
one hand, the metrics used in the first plots corre-
late negatively with the abstractitivy; thus, higher
abstractivity is shown in the partition when the dis-
tribution is centered around the bottom left corner
of the plot (where the values are lower on both
metrics). On the other hand, the second plots cor-
relate positively with the abstractivity; thus, the
distributions are centered near the right top corner
if the summaries are highly abstractive. Finally, we
should point that due to the outliers, the distribu-
tions were hard to visualize. Hence, we exclude
the 10% with the lowest values and the 10% with
the highest values.
Figure 1 shows that the Catalan set mainly con-(a) Catalan (b) Spanish
(a) Catalan (b) Spanish
tains summaries with short extractive fragments
since the distribution centers in 75% of coverage
and a density lower than 2. Also, we observe that
the distribution tends to go up and right; thus, the
samples of the set diversify to less abstractive sum-
maries. In the case of Spanish, we observe that
the extractive fragments are longer than in the first
language due to the higher density, and also, the dis-
tribution centers in the 85% of coverage, which in-
dicates that the summaries in the Spanish set reuse
more words from the article than in the Catalan
set. However, the distribution tends to go down and
left, which indicates a big presence of abstractive
summaries in this set.
Figure 2 helps to show the diversity of the sam-
ples by combining abstractivity(p= 2) and novel
2-grams, which brings us more information. Al-
though in Figure 1 the distributions were different
from language to language, in this figure, we ob-
serve that the two sets are similar regarding these
two metrics; note that the darker zones follow the
same pattern around the same range of values.
Based on Tables 3 and 4 and Figures 1 and 2,
it can be concluded that the DACSA corpus pro-
vides samples that do not contain a predominance5936
of extractive summaries, and show great diversity
regarding their degree of abstractivity.
6 Summarization models and
performance results
We evaluate several summarization systems to
understand the challenges posed by the DACSA
dataset for summarization tasks. We consider both
extractive and abstractive models, along with an
extractive oracle to show an upper bound of the
extractive performance in the corpora.
Extractive systems : Lead-k, TextRank (Mihal-
cea and Tarau, 2004) and SHANN (González et al.,
2019) have been evaluated. Lead-k is a heuristic
that extracts the first ksentences of a text, being
especially well suited to summarize newspaper ar-
ticles. TextRank is a graph-based system inspired
by PageRank, where nodes represent sentences,
and edges measure similarities in terms of shared
words. Finally, SHANN is a supervised system
based on siamese hierarchical attentional networks.
The document sentences are scored using sentence-
level attentions and those with highest scores are
extracted to build the summary. As the average
number of sentences in the summaries of DACSA
is near to two, we extracted two document sen-
tences by using the extractive systems. We built
the extractive systems upon code that is available
on Github (Barrios et al., 2016), (González et al.,
2019).
Abstractive systems : we considered two repre-
sentative models with high performance on abstrac-
tive summarization, based on encoder-decoder ar-
chitectures with Transformers as backbone: BARTand T5. Due to there are neither BART nor T5
models pretrained from scratch for the Spanish
and Catalan languages, we finetuned and evaluated
their multilingual variants, mBARTand mT5. It
should be noted that, although both of them con-
sidered the Spanish language during pretraining,
the Catalan language is not represented in the case
of mBART, as this language is not contained in
the CC25 dataset. We built the abstractive systems
using the HugginFace toolkit (Wolf et al., 2020).
Oracle : we implemented an extractive oracle
that aligns each summary sentence with the most
similar document sentence using ROUGE. The
aligned document sentences are concatenated to
build the oracle summary.
In order to evaluate the models, we use ROUGE
and BERTScore metrics. ROUGE-1, ROUGE-2
and ROUGE-L are reported to measure lexical over-
lapping, while BERTScore is used to measure se-
mantic similarity.
Tables 5 and 6 show the performance results
of the different models on the Catalan and Span-
ish DACSA TESTand TESTsets in terms
of ROUGE and BERTScore metrics. The oracle
outperforms the other systems by a large margin.
The worse results obtained by the oracle are in the
Catalan TEST, showing that this partition is the
most abstractive test partition in the DACSA cor-
pus. Generally, extractive systems are worse in5937
theTESTthan in the TEST, which suggests a
higher extractivity in TESTthan in TEST. The
high results of Lead-2, especially in the TEST
sets, show that there is a positional bias in these
sets.
7 Conclusions
Languages other than English have a lack of re-
sources for learning models based on deep learning.
This is true for endangered languages but it is also
true even for those languages that have millions of
speakers but are minority worldwide such as Cata-
lan. In this work, we describe the construction of
a corpus of Catalan and Spanish newspapers, the
Dataset for Automatic summarization of Catalan
and Spanish newspaper Articles (DACSA) corpus.
We have included an analysis of the corpus using
a set of well-known metrics in the summarization
field in order to characterize the corpus. This char-
acterization shows that DACSA provides samples
that do not contain a predominance of extractive
summaries, and show great diversity regarding theirdegree of abstractivity. We have also carried out
an evaluation of the performance of some extrac-
tive and abstractive summarization systems on the
DACSA corpus that could be used for benchmark-
ing. To our knowledge, the DACSA corpus is the
largest summarization dataset for Catalan and Span-
ish languages and is freely available for research
purposes.
Ethical considerations
The main objective of this work was to build a
quality large-scale corpus that could be used to
learn automatic summarization neural models for
Catalan and Spanish. To achieve this objective, we
selected a set of Spanish news sites, including from
Spanish mass media to regional newspapers, and
we collected as many data as possible from them.
To increase the quality of the corpus, we filtered
the article-summary pairs following basic statistics
of the text. However, we did not apply any kind
of content filtering. Therefore, our filtering could
include biased content such as political tendency,5938geographic imbalance or gender biases (Stanczak
and Augenstein, 2021). A future direction towards
improving the dataset quality would be to alleviate
that biases, for example, by means of deduplicat-
ing content, augmenting artificially the samples to
balance gender (Sun et al., 2019), politics, and ge-
ographic aspects, or either manually selecting an
unbiased subset of the dataset.
The articles collected in the dataset are un-
der Creative Common or private licenses. Nowa-
days, we are working on obtaining authoriza-
tion for the distribution of all sources. Those
newspaper sources under Creative Common li-
cense or the private ones with authorization
are freely provided. DACSA can be re-
quested at https://xarrador.dsic.upv.
es/resources/dacsa .
Acknowledgements
This work is part of the AMIC-PoC
project (PDC2021-120846-C44), funded by
MCIN/AEI/10.13039/501100011033 and by the
European Union "NextGenerationEU/PRTR". It
is also partially supported by the Vicerrectorado
de Investigación de la Universitat Politècnica de
València (PAID-11-21).
References593959405941A Statistics of DACSA
We show in Tables 7 and 8 a more detailed view of the statistics of the DACSA corpus, distinguishing
among the sources from which it was built. The sources that were only considered in the TEST
partitions are marked with an asterisk.5942B Abstractivity in DACSA
We show in Tables 9 and 10 a fine-grained view of the abstractivity of the DACSA corpus, distinguishing
among the sources from which it was built.5943