
Yanyang Li, Jianqiao Zhao, Michael R. Lyu, Liwei Wang
Department of Computer Science and Engineering, The Chinese University of Hong Kong
{yyli21,jqzhao,lyu,lwwang}@cse.cuhk.edu.hk
Abstract
Recent advances in large-scale pre-training pro-
vide large models with the potential to learn
knowledge from the raw text. It is thus nat-
ural to ask whether it is possible to leverage
these large models as knowledge bases for
downstream tasks. In this work, we answer
the aforementioned question in unsupervised
knowledge-grounded conversation. We explore
various methods that best elicit knowledge from
large models. Our human study indicates that,
though hallucinations exist, large models post
the unique advantage of being able to output
common sense and summarize facts that can-
not be directly retrieved from the search engine.
To better exploit such generated knowledge
in dialogue generation, we treat the generated
knowledge as a noisy knowledge source and
propose the posterior-based reweighing as well
as the noisy training strategy. Empirical results
on two benchmarks show advantages over the
state-of-the-art methods.
1 Introduction
Knowledge-grounded conversation (Dinan et al.,
2019; Moghe et al., 2018) is the task where the
model could reply to a dialogue history based
on extra knowledge. Compared to standard con-
versational modeling, this extra knowledge pre-
vents the model from generating generic and non-
informative responses (Li et al., 2016). Typically, at
each turn of the conversation, a pool of knowledge
candidates will be retrieved from a knowledge base
like unstructured documents (e.g., Wikipedia) (Di-
nan et al., 2019) or a structured knowledge graph
(Dziri et al., 2021). The model then learns to select
the most related knowledge from this pool, in an
unsupervised manner, to generate its response.
However, constructing and maintaining knowl-
edge bases are time-consuming and expensive. Re-
cent studies have shown that large pre-trained mod-els are capable of grasping knowledge from un-
supervised text corpora and memorizing facts to
their weights (Petroni et al., 2019; Roberts et al.,
2020; Lewis et al., 2021; Wang et al., 2021; Liu
et al., 2022a). These large models can even per-
form reasoning implicitly (Wei et al., 2022). In
light of this remarkable capacity of large models,
we explore the possibility of leveraging large mod-
els as a new knowledge source for unsupervised
knowledge-grounded conversation.
In this work, we first investigate the quality of
knowledge generated by large models. We exam-
ine which tuning method, including the conven-
tional fine-tuning (Devlin et al., 2019; Zhang et al.,
2021) and the recently proposed prefix-tuning (Li
and Liang, 2021), best prompts knowledge from
large models for a given dialogue history. We then
design a human evaluation protocol and conduct
an extensive quality assessment of the generated
knowledge. Despite some extent of hallucinations
(plausible statements with factual errors) persist
(Maynez et al., 2020), large models can mostly
generate related and correct knowledge for the fu-
ture development of dialogue. Moreover, some of
this knowledge is not simply paraphrased or copied
from web pages: they summarize scattered facts
on the Internet (See Section 5.1). These observa-
tions advocate the unique value of employing large
models as knowledge bases.
Owing to the hallucinations, it is risky to put
generated knowledge directly into the dialogue sys-
tem as the misinformation could contaminate the
response. We instead consider the generated knowl-
edge as a noisy knowledge source and use it to
aid the knowledge selection process. Specifically,
we measure its similarity to each knowledge candi-
date and refine the knowledge selection accordingly
(See Section 3.2). We further estimate the poste-
rior of the refined knowledge selection distribution,
inspired by the fact that the posterior detangles the
one-to-many relation between dialogue context and10551
knowledge selection (Kim et al., 2020). In addition,
we propose a noisy training strategy to strengthen
the model’s ability on handling noisy knowledge
(See Section 3.3). All these strategies significantly
elevate the performance of the existing state-of-the-
art model to a new level on two widely-adopted
benchmarks, Wizard of Wikipedia (Dinan et al.,
2019) and Holl-E (Moghe et al., 2018).
2Eliciting Knowledge from Large Models
In this section, we first introduce the tuning meth-
ods and large pre-trained models we used to gener-
ate knowledge for a given dialogue history. Then
we show the tagset developed for evaluating the
generated knowledge.
2.1 Methods and Models
Since the objective function of large pre-trained
models is to predict possible words instead of
knowledge given the context (Devlin et al., 2019;
Radford et al., 2019), tuning these large models on
annotated data is necessary. Here we focus on two
tuning methods, as shown in Figure 1:
•Fine-Tuning (Devlin et al., 2019; Zhang et al.,
2021) which updates all weights in the model.
•Prefix-Tuning (Li and Liang, 2021) which
freezes the pre-trained weights and tunes only
a small set of parameters that are added as the
prefix of the model’s input.
Fine-tuning remains the standard approach for
leveraging pre-trained models in downstream tasks,
while prefix-tuning has a comparable performance
but avoids the risk of catastrophic forgetting (Good-
fellow et al., 2013), which is desirable in our task.
Another challenge is selecting large models. Be-
cause our task requires large models to understand
the dialogue history and then recommend a related
knowledge piece for the user to follow up, we study
two types of large models:
•Pre-trained Language Models (PLMs) thatare trained on web documents with access to
abundant knowledge during pre-training.
•Pre-trained Dialogue Models (PDMs) that
are trained on dialogue data to better under-
stand the dialogue history.
We choose T5 (Raffel et al., 2020) as the repre-
sentative of PLMs and DialoGPT (Zhang et al.,
2020) for PDMs, because they release a series of
checkpoints with different model sizes.
Besides, we experiment with various decoding
methods to see which of them best suits each type
of large models, including greedy decoding, beam
search and top-K sampling (Fan et al., 2018). We
find that PDMs work best with top-K sampling and
beam search for PLMs.
2.2 Annotation Tagset
To assess the quality of the generated knowledge,
we develop an annotation tagset for human evalua-
tion in Table 1. Each generated knowledge along
with its associated dialogue history will be anno-
tated by at least two tags, each from a different
category: context understanding ,tuning effective-
ness (and fact-checking if outputs contain facts).
Context Understanding Related andUnrelated
in rows 1-2 of Table 1 measure whether large pre-
trained models understand the conversation and
generate related knowledge. Although we can use
automatic metrics like the F1 score that measures
the distance between the generated knowledge and
the ground truth knowledge as an alternative, a
single reference only captures one possible future
direction of the dialogue. In this sense, human eval-
uation provides a more comprehensive assessment.
Tuning Effectiveness Non-Verifiable (e.g.,
chitchat) and Verifiable in rows 3-4 indicate the
reliability of the tuning methods for eliciting knowl-
edge from large models. If a tuning method is effec-
tive, models should generate outputs that contain
Verifiable facts.
Fact-Checking Among those Verifiable outputs,
we classify them into Supported (facts is sup-
ported by evidence), Refuted (facts is refuted by
evidence) and Not Enough Information (NEI,
evidence is not found), as shown in rows 5-12 of
Table 1. These tags are mainly adapted from Gupta
et al. (2022). Annotators will gather trustworthy
evidence via search engines to determine the label.
To better understand the detailed behavior of
large models, we divide Supported intoExplicit
Supported andImplicit Supported . The for-10552
mer means that large models memorize existing
documents, while the latter implies that they do
more than memorization, e.g., summarization. We
also let the annotators check whether NEIoutputs
could be validated by common sense. If common
sense could be used for validation, these NEIout-
puts will be further classified into Reasonable NEI
(facts match common sense) or Unreasonable NEI
(facts contradict common sense), and Hard NEI if
common sense is not applicable.
3 Exploiting Generated Knowledge for
Conversation
In this section, we first review the state-of-the-
art approach - PLATO-KAG (Huang et al., 2021).
Then we develop our method on top of PLATO-
KAG to exploit generated knowledge.
3.1 PLATO-KAG
As shown in Figure 2, PLATO-KAG is a model
consisting of two modules: a knowledge selector
which selects top-K most relevant knowledge to
the dialogue history from a pool of retrieved knowl-
edge candidates, and a response generator that gen-
erates the response based on the dialogue history
and the selected knowledge.
Knowledge Selector The knowledge selector
adopts a dual encoder with shared parameters to ex-
tract features. The dialogue history hand a knowl-
edge candidate zwill pass to this encoder indepen-
dently to get their own representations. Then it
estimates the relevance between the dialogue his-toryhand a knowledge candidate zby:
f(h, z) = (WE(h))(WE(z)) (1)
where E(·)is the fixed-length vector representation
of the input, i.e., the encoder’s output on the [CLS]
token. WandWare two linear projections.
To select the top-K knowledge candidates, the
knowledge selector computes the relevance be-
tween hand all possible z. Then only the top-
K most related knowledge Z={z,···, z}is
retained to construct the knowledge selection dis-
tribution P(z|h)as follows:
P(z|h) =e
/summationtexte(2)
Response Generator After the knowledge selec-
tion, the response generator will predict the proba-
bility of the response rby:
P(r|h) =/summationdisplayP(z|h)P(r|h, z) (3)
where P(r|h, z) =/producttextP(r|h, z, r)is a decoder
that generates response rgiven the dialogue history
hand one knowledge candidate z.
3.2 Posterior-based Reweighing
Reweighing Generated knowledge contains hallu-
cinated facts, as later shown in Section 5.1.2. It
is thus not viable to take generated knowledge g
as the direct input of the model. Instead, we inter-
pretgas noisy ground truth and define a refined10553
knowledge selection distribution P(z|h):
P(z|h) =P(z|h)P(z|g) (4)
where P(z|h)is the original knowledge selection
distribution and P(z|g)measures the similarity be-
tween gandz. This refined distribution P(z|h)
will score zhigh only if zis close to the history h
as well as the noisy ground truth g.
Similar to Eqs. 1-2, we define P(z|g)to measure
the closeness between gand each z:
f(g, z) = (WE(g))(WE(z)) (5)
P(z|g) =e
/summationtexte(6)
where αis a hyper-parameter that controls the
sharpness of P(z|g).
Posterior Kim et al. (2020) suggests that the poste-
rior selection distribution P(z|h, r)could select a
more appropriate knowledge than the prior selec-
tion distribution P(z|h), because the presence of
future response rnarrows down the scope of all
possible z. We drive the posterior of the refined
selection distribution P(z|h)via the Bayes rule:
P(z|h, r) =P(r|h, z)P(z|h)/summationtextP(r|h, z)P(z|h)(7)
where the denominator is tractable as there are only
a small number of zandP(r|h, z)is exactly the
response generator.
The main challenge is how to estimate P(r|h, z)
when ris not yet observed. We let the response
generator greedy decode a most likely response ¯r
for a given z. However, different zmight result in
¯rwith various lengths. A long ¯rtends to have a
lower probability and is not competitive with the
short one (Yang et al., 2018). We therefore use
the mean token probability as the approximation of
P(r|h, z)≈/summationtextP(¯r|h, z,¯r), where Nis
the length of ¯r.In the end, we add a hyper-parameter βto con-
trol the sharpness of the posterior P(z|h, r)∝
P(z|h, r). Since we only apply the Bayes rule
once to obtain the posterior, adjusting the sharpness
help to amplify or diminish the impact of updating.
3.3 Noisy Training
Note that Eq. 7 is only applicable when the re-
sponse generator P(r|h, z)is able to denoise, i.e.,
the model should give the likelihood estimate of a
low value if zis not appropriate. In this case, the
Bayes rule will update the posterior by lowering
the chance of this inappropriate zbeing selected.
Since the knowledge selector always presents its
most confident selection to the response generator
and the knowledge selector performs much better in
the training set (a top-K accuracy of 90.3% for the
baseline) than in the test set (a top-K accuracy of
68.1%), such a bias will lead to a training-inference
discrepancy and therefore the response generator
is not resilience to noisy knowledge during testing.
To alleviate this issue, we employ the Gumbel-
TopK trick (Kool et al., 2019), which adds noise
to the top-K operation in the knowledge selector
during training. Specifically, we sample noise from
the Gumbel distribution with location µ= 0and
scale ϕ= 1. This noise will add to f(h, z)in Eq.
1 to permute the ranking of knowledge candidates
and perturb the selection distribution P(z|h).
4 Experimental Setup
4.1 Datasets
We conduct experiments on two popular bench-
marks: Wizard of Wikipedia (Dinan et al., 2019)
(WoW), and Holl-E (Moghe et al., 2018). The
WoW dataset covers a wide range of topics (1,365
in total). Each conversation in WoW happens
between a wizard who has access to knowledge
from Wikipedia about a specific topic and an ap-
prentice who learns from the wizard about the10554topic. Specifically, for our knowledge genera-
tion task in Section 2.1, the input is the dialogue
history and the target is the ground truth knowl-
edge that the wizard used to generate his response.
There are 18,340/1,948/1,933 dialogues in the train-
ing/validation/test set. The validation and test sets
are split into two categories: Seen which contains
new dialogues with topics that appeared in the train-
ing set and Unseen whose dialogues have topics
that never appear in the training set. We follow
Dinan et al. (2019)’s scripts to preprocess the data.
Compared to WoW, conversations in Holl-
E happened between two participants dis-
cussing a specific movie, where a single docu-
ment about that movie is given as knowledge.
There are 7,228/930/913 dialogues in the train-
ing/validation/test split. We follow Kim et al.
(2020)’s scripts for data preprocessing.
4.2 Evaluation Metrics
We assess all results (generated knowledge and re-
sponses) via both the automatic metric and human
evaluation.
Knowledge Generation Assessment In automatic
evaluation, we compute the unigram F1 between
the generated knowledge and the ground truth
knowledge. In human evaluation, we recruit three
well-trained annotators who are fluent in English
to evaluate 100 random samples from the seen and
unseen test sets each, according to the scheme we
proposed in Section 2.2. The tag of an example
is determined by the majority vote of the three an-
notators. The agreement among the annotators is
measured via Fleiss’ kappa (Fleiss, 1971).
Response Generation Assessment In the auto-
matic evaluation, we report the perplexity (PPL)
and Unigram F1 of ground truth responses. We
also collect the top-1 knowledge accuracy (P@1)
statistics, which evaluate the performance of the
knowledge selector. In the human evaluation, 100
random examples from WoW seen and unseen test
sets are distributed to three annotators respectively.
They will evaluate these samples in four aspects,
following Huang et al. (2021):
•Coherent measures whether the response is
consistent with the dialogue history.
•Informativeness evaluates whether the re-
sponse is generic and non-informative or not.
•Engagingness assesses how likely the annota-
tor is willing to continue the discussion.
•Hallucination checks the correctness of thecontained factual information.
Coherence, informativeness and engagingness are
in the range of [0, 1, 2]. A higher value implies
a better result. Hallucination is in the range of [0,
1], where 0 means the response is factually correct
and 1 means the response contains hallucinated
facts. We refer the readers to Huang et al. (2021)
for more details. The final score of each sample is
determined through majority voting.
4.3 Response Generation Baselines
TMN is the baseline released along with the WoW
dataset (Dinan et al., 2019). It stores knowledge
candidates’ features in the memory for selection.
We include the released unsupervised trained check-
point in our experiments.
SKT models the knowledge selection process in
multi-turn dialogue generation as a sequential la-
tent variable model (Kim et al., 2020). We use their
open-sourced models in our experiments.
KnowledGPT fine-tunes a GPT-2 (Radford et al.,
2019) and leverages the reinforcement learning ap-
proach to train an unsupervised sequential knowl-
edge selector (Zhao et al., 2020). We adopt their
released model for experiments.
4.4 Implementation Details
Knowledge Generation For fine-tuning, all mod-
els use a batch size of 64, a learning rate of 5e-5,
and the inverse square root learning rate scheduler
(Vaswani et al., 2017) with 1000 warmup steps. We
validate the model on the validation set every 1000
steps and early stop the training if the performance
does not improve after 15 validations. For prefix-
tuning, the prefix length is set to 5 as in Li and
Liang (2021). Other hyper-parameters are almost
the same as in fine-tuning, except that the learning
rate is kept constant and reduced by 1/10 only if the
validation set performance does not improve after
10 validations. At inference, DialoGPT is decoded
with top-K sampling where K is 10 and the beam
size is 20. For T5, we use beam search with a beam
size of 10.
Response Generation Since Huang et al. (2021)
did not release their codes and models before we
start the experiments, we reimplement their ap-
proach in ParlAI (Miller et al., 2017) and report
our own results as well. We follow Huang et al.10555
(2021)’s hyper-parameters settings in our exper-
iments. For our proposed reweighing method,
we perform a grid search on the validation set
(α∈[1,10], β∈(0,1)) and set α= 5, β= 0.4.
According to Table 2, we choose the generated
knowledge of DialoGPT-large for our experiments,
as it performs the best on average. All experiments
are conducted on 8 NVIDIA A100 80G. It takes
roughly one day to train one model.
5 Results and Analysis
5.1 Knowledge Generation Results
We conduct a case study of eliciting knowledge
from large models on the WoW dataset and present
the evaluation results.
5.1.1 Automatic Evaluation Results
Table 2 shows F1 scores of various large models
tuned by different methods on the seen and unseen
test sets. Results of fine-tuning T5-XXL are miss-
ing because we do not have enough resources to
train this model. The first row of Table 2 is the base-
line result of tuning a randomly initialized T5-large
model. We observe that nearly all large models
perform better than this baseline, especially on theunseen test set. This observation indicates the per-
trained weights do store a lot of factual information
as they make a non-trivial improvement.
We also see that PDMs perform much better
than PLMs on data with a seen topic, while PLMs
are better on the unseen topic in most cases. This
might be the consequence that PLMs are trained on
diverse text data, which allows them to generalize
better on unseen topics. PDMs, on the other hand,
are trained on dialogue data only and have a smaller
discrepancy between pre-training and fine-tuning.
Thus PDMs perform better on seen topics. We also
find that the results of fine-tuning are much better
than prefix-tuning in general. But this gap is closed
when the model gets larger, which is aligned with
the conclusion in Lester et al. (2021).
Interestingly, large models scale poorly on our
task. On the unseen test set, the performance in-
creases only around 3 points while the model size
is 50×larger (0.1796 for fine-tuned T5-small with
60M parameters vs. 0.2053 for fine-tuned T5-XL
with 3B parameters).
5.1.2 Human Evaluation Results
The human evaluation results are presented in Fig-
ure 3. This evaluation has a kappa value of 1 for10556
the context understanding dimension and 0.698 for
the remaining dimensions. Here we study the out-
puts of fine-tuned DialoGPT-large, as it performs
the best on average. We also analyze prefix-tuned
T5-XXL in Appendix A and the results are similar.
Context Understanding From the first subplot
of Figure 3, large models can reliably generate re-
lated knowledge for a given dialogue history, where
around 90% tags are Related in both test sets.
Tuning Effectiveness The second diagram in Fig-
ure 3 shows that large models exhibit desirable
behaviors after fine-tuning: it generates knowledge
(Verifiable ) in all cases.
Fact-Checking The rightmost three panels of Fig-
ure 3 demonstrate the factual correctness of the
generated knowledge. As shown in the third
panel, large models generate factually correct
(Supported ) statements in most cases, though
there is still around 10% of the chance to produce
hallucinated information ( Refuted ). Among all the
factually correct knowledge (the second last panel),
more than 50% of them are Implicit Supported .
This is exciting as large models are able to assem-
ble multiple facts in their outputs, which cannot be
substituted by simple search engine retrieval. This
ability to summarize justifies the value of large
models in serving as knowledge bases.
The last panel of Figure 3 checks whether NEIclaims could be verified by common sense. There is
a certain amount (39 ∼57%) of NEIclaims that are
common sense. This observation advocates another
advantage of utilizing large models as knowledge
bases: they can provide common-sense information
that lies behind the human mind, with no need for
humans to explicitly write them down. We show
some annotated examples in Appendix B.
5.2 Response Generation Results
5.2.1 Main Results
Table 3 is the response generation results of the
WoW test sets. We can see that applying our pro-
posed method to PLATO-KAG obtains the high-
est F1 score, even if our reimplemented PLATO-
KAG baseline already performs much better than
reported in the paper. On the other hand, our pro-
posed method seems to lower the top-1 knowledge
accuracy, i.e., P@1 drops from 0.266 to 0.254 in
the seen test set and from 0.233 to 0.231 in the
unseen test set. Note that PLATO-KAG is a model
whose input consists of K knowledge candidates.
If the ground truth knowledge is not ranked in the
first place but presented in the top-K results, the
model can still use the ground truth for the gener-
ation. In this case, the top-K knowledge accuracy
is a more important metric for evaluating knowl-
edge selection. Though not presented in Table 3,
P@K increases from 0.681 to 0.690 in the seen test
set and from 0.645 to 0.656 in the unseen test set.
Table 4 displays the automatic evaluation results10557
in Holl-E datasets. Similar to the results of WoW,
our proposed method significantly outperforms the
baseline systems in terms of the F1 score.
Table 3 also reports the human evaluation results
of WoW. The kappa value of this human evalu-
ation is 0.415. In the seen test set, our strategy
improves over baselines in nearly all metrics. How-
ever, our method degrades the performance of the
unseen test set. In Section 5.2.2, we will show
that our method put significantly more ground truth
knowledge into the responses. In spite of that more
knowledge helps to reduce hallucinations (from
0.0962 to 0.0385 as shown in Table 3), this could
also lead to a degenerated result in human evalua-
tion (Huang et al., 2021), as the knowledge makes
the response far less interesting.
5.2.2 Analysis
We conduct an ablation study in Table 5 for a bet-
ter understanding of our proposed method. We
additionally report Knowledge F1, the F1 score be-
tween the generated response and the ground truth
knowledge (Lian et al., 2019; Shuster et al., 2021),
which indicates how much ground truth knowledge
is embedded into the response.
As shown in Table 5, all steps in our proposed
method, including noisy training and posterior-
based reweighing, contribute to the final perfor-
mance. In particular, reweighing greatly improves
Knowledge F1, which implies that it helps to select
and incorporate ground truth knowledge into the
response generation. We give some examples in
Appendix C for demonstration.
6 Related Work
Knowledge-Grounded Conversation The dia-
logue system field has witnessed a growing inter-
est in knowledge-grounded conversation in recent
years. Many related benchmarks have been pro-
posed to study this problem (Zhang et al., 2018;
Zhou et al., 2018; Dinan et al., 2019; Gopalakrish-
nan et al., 2019; Komeili et al., 2022). Early work
(Dinan et al., 2019) had harnessed the annotated
knowledge for training. Unsupervised approaches
become attractive as acquiring these annotations
is expensive. Zhao et al. (2020) use reinforcement
learning to fine-tune GPT-2 (Radford et al., 2019)
for unsupervised knowledge selection. Huang et al.
(2021) achieve a new state-of-the-art by selecting
top-K knowledge when annotations are not avail-
able. Another line of research improves the knowl-
edge selection modeling by estimating the posterior,
which makes use of the future utterance. Lian et al.
(2019) train the knowledge selector as a variational
auto-encoder (Kingma and Welling, 2014). Kim
et al. (2020) further model the knowledge selection
in multi-turn dialogue as a sequential latent vari-
able. More recently, dialogue model pre-training
also attempts to involve knowledge for generating
informative responses. Shuster et al. (2021) uti-
lize the pre-trained retriever DPR (Karpukhin et al.,
2020). Thoppilan et al. (2022) directly access to
the search engine to collect relevant knowledge.
Knowledge in Pre-Trained Models The LAMA
prob (Petroni et al., 2019) first study knowledge
stored in pre-trained models. They show that pre-
trained models contain a certain amount of factual
knowledge without any fine-tuning. This finding
has motivated a series of work that adopts knowl-
edge from pre-trained models for downstream tasks.
Roberts et al. (2020) show that pre-trained models
fine-tuned on question-answering datasets without
accessing any external knowledge base could ob-
tain a remarkable result. Wang et al. (2022) prob
relational structures from pre-trained models for
Text-to-SQL parsing. Liu et al. (2022a) further
demonstrate that pre-trained models can generate
knowledge via prompting to help in common sense
reasoning tasks. Perhaps the most related work
is Liu et al. (2022b), where they adapt a large
model to knowledge-grounded conversation via
multi-stage prompting and which includes an inter-
mediate knowledge generation step. Compared to
this work, our work treats large models as a general-
purpose knowledge base, then elicits and transfers
knowledge from it to improve a small but strong
downstream task model with a distinct architecture.10558Knowledge Distillation Our work also closely
resembles knowledge distillation (Hinton et al.,
2015), as we similarly transfer knowledge from
a large pre-trained model to a small downstream
task model. Most existing approaches employ con-
tinuous vectors to represent knowledge, e.g., logits
(Hinton et al., 2015), attention distribution (Wang
et al., 2020), hidden features (Romero et al., 2015)
or weights (Lin et al., 2021), which are not straight-
forwardly interpretable. In this work, the large
model generates discrete, readable sentences to
transfer knowledge.
7 Conclusion
In this work, we show that large pre-trained models
could serve as knowledge bases for unsupervised
knowledge-grounded conversation. The study on
the generated knowledge of large models has the
following observations:
•Fine-tuning better elicits knowledge from
large models than prefix-tuning.
•Knowledge pieces generated by pre-trained
language models have a higher quality on un-
seen topics, while those from pre-trained dia-
logue models are better on seen topics.
•Large pre-trained models can synthesize com-
mon sense and summarize facts scattered on
the web.
We also propose posterior-based reweighing and
noisy training, which helps to incorporate the gen-
erated knowledge into the dialogue system. These
simple strategies show a promising result over the
strong baselines.
Limitations
We realize that there still are some limitations in
our work despite the exciting results:
Generated Knowledge A certain amount of an-
notated data is required to fine-tune large models
before they could output knowledge stored in their
weights. Such kind of data could be difficult to col-
lect as it requires highly educated annotators. Be-
sides, although large models could generate knowl-
edge that needs multiple pieces of evidence for
verification (samples with Implicit Supported
tag), to what extent large models understand facts
and the relation to hallucination remain unknown.
Proposed Method The approach we proposed to
leverage the generated knowledge is still primi-
tive, as it is purely training-free and applied only
at inference. In addition, we only use one gener-ated knowledge sentence in experiments. Aside
from this, in human evaluation we have shown
that injecting more knowledge into responses re-
duces hallucination, but results in the degradation
of other dimensions like engagingness. How to
carefully balance these quality measurements is
another topic that is worth investigating.
Acknowledgements
This work is supported by Centre for Perceptual
and Interactive Intelligence Limited, UGC under
Research Matching Grant Scheme, and partially
by Research Grants Council of the Hong Kong
Special Administrative Region, China (No. CUHK
14210920 of the General Research Fund).
References105591056010561A Human Evaluation on T5-XXL
Figure 4 is the human evaluation results on the
generated knowledge of prefix-tuned T5-XXL. The
kappa value of the context understanding dimen-
sion is 0.922 and 0.726 for the remaining two
dimensions. Compared to DialoGPT-large, T5-
XXL performs more robustly in generating related
knowledge for unseen topics, as indicated by the
leftmost panel of Figure 4. It also seems that prefix-
tuning is slightly less effective, as it produces a few
non-verifiable claims, as shown in the second left-
most subplot of Figure 4. In the rightmost diagram
of Figure 4, we note that T5-XXL is more likely
to generate knowledge that is hard to verify. This
might indicate a higher risk of hallucination when
using larger models. Other measurements remain
similar for DialoGPT-large and T5-XXL.
B Knowledge Generation Examples
Table 6 gives some generated knowledge examples,
each with a different tag annotated. Most of these
examples are generated by fine-tuned DialoGPT-
large on WoW unseen test set.
C Response Generation Examples
Table 7 are response examples from different re-
sponse generation models in two WoW test sets.
We can see that our PLATO-KAG +generates re-
sponses that are more interesting than other base-
lines. We also note that the first few words are sim-
ilar for the PLATO-KAG baseline and our PLATO-
KAG+. It seems that our strategy improves the
long-term modeling of PLATO-KAG to achieve a
better result.105621056310564