
Jonas Pfeiffer, Naman Goyal, Xi Victoria Lin, Xian Li,
James Cross, Sebastian Riedel, Mikel ArtetxeNew York University,TU Darmstadt,Meta AI
Abstract
Multilingual pre-trained models are known to
suffer from the curse of multilinguality , which
causes per-language performance to drop as
they cover more languages. We address this is-
sue by introducing language-speciﬁc modules,
which allows us to grow the total capacity of
the model, while keeping the total number of
trainable parameters per language constant. In
contrast with prior work that learns language-
speciﬁc components post-hoc, we pre-train the
modules of our Cross -lingual Mod ular ( X-
M) models from the start. Our experiments
on natural language inference, named entity
recognition and question answering show that
our approach not only mitigates the negative
interference between languages, but also en-
ables positive transfer, resulting in improved
monolingual and cross-lingual performance.
Furthermore, our approach enables adding lan-
guages post-hoc with no measurable drop in
performance, no longer limiting the model us-
age to the set of pre-trained languages.
1 Introduction
Recent work on multilingual NLP has focused on
pre-training transformer-based models (Vaswani
et al., 2017) on concatenated corpora of a large
number of languages (Devlin et al., 2019; Conneau
et al., 2020). These multilingual models have been
shown to work surprisingly well in cross-lingual
settings, despite the fact that they do not rely on
direct cross-lingual supervision (e.g., parallel data
or translation dictionaries; Pires et al., 2019; Wu
and Dredze, 2019; Artetxe et al., 2020; Hu et al.,
2020; K et al., 2020; Rust et al., 2021).
However, recent work has uncovered fundamen-
tal limitations of multilingual transformers. Con-
neau et al. (2020) observe that pre-training a model
with a ﬁxed capacity on an increasing amount of
languages only improves its cross-lingual perfor-
mance up to a certain point, after which perfor-Figure 1: A transformer layer of our proposed modular
architecture. The dark blue and green components illus-
trate the modular layers, which are language speciﬁc.
The Multi-Head Attention and Feed-Forward compo-
nents are shared by all languages.
mance drops can be measured—a phenomenon
known as the curse of multilinguality (Figure 2).
As such, prior work had to ﬁnd a trade-off between
supporting more languages and obtaining better
performance on a smaller set of languages.
In this work, we address this problem by in-
troducing language-speciﬁc, modular components
during pre-training (Figure 1). Our Cross -lingual,
Mod ular ( X-M) language model shares the ma-
jority of the transformer parameters between all pre-
training languages, while providing each language
with individual capacity to learn idiosyncratic in-
formation without increasing the total number of
trainable parameters per language. While previous
adapter-based approaches (Figure 3a) extend pre-
trained multilingual language models (LMs) with
modular components after pre-training, we add
modular components during pre-training, thereby3479
preparing the model to be extended to new lan-
guages post-hoc. Our experiments on natural lan-
guage inference (NLI), named entity recognition
(NER), and question answering (QA) demonstrate
that our modular architecture not only is effective at
mitigating interference between languages, but also
achieves positive transfer, resulting in improved
monolingual and cross-lingual performance. In ad-
dition, we show that X-Mcan be extended to
unseen languages, with no measurable drop in per-
formance, by learning its corresponding modules
and leaving the shared parameters frozen. All in
all, we propose a multilingual architecture that can
scale to a large number of languages without any
loss in performance, and can be further extended
to new languages after pre-training.
2 Background and related work
We provide a background on multilingual and mod-
ular language modelling, as well as approaches that
extend LMs to new languages.
2.1 Multilingual transformers
Recent LMs (Devlin et al., 2019; Conneau et al.,
2020), based on transformer architectures (Vaswani
et al., 2017) and pre-trained on massive amounts
of multilingual data, have surpassed (static) cross-
lingual word embedding spaces (Ruder et al., 2019;
Glavas et al., 2019) for cross-lingual transfer in
NLP (Pires et al., 2019; Wu and Dredze, 2019;
Wu et al., 2020; Hu et al., 2020; K et al., 2020).
Transformer-based models are 1)pre-trained on
textual corpora using Masked Language Modelling
(MLM). They are then 2)ﬁne-tuned on labelled
data of a downstream task in a source language and
3)directly applied to perform inference in a target
language (Hu et al., 2020).
2.2 Modular language models
Modular approaches have a long standing history
in NLP, preceding pre-trained models (Andreas
et al., 2016). They have recently re-gained in-
terest for transformer-based models, where mix-3480ture of experts (MoE; Shazeer et al., 2017) ap-
proaches have enabled training trillion parame-
ters models in a distributed fashion (Fedus et al.,
2021). More recently modular MoE approaches
have been shown to improve domain-speciﬁc pre-
training of LMs (Gururangan et al., 2021). In a
similar trend, ‘expert’ modules have been added
to (non-modular) pre-trained LMs post-hoc, pre-
dominantly referred to as adapters (Rebufﬁ et al.,
2017, 2018; Houlsby et al., 2019). Next to being ex-
tremely parameter (Houlsby et al., 2019; Mahabadi
et al., 2021a; He et al., 2022) and training efﬁcient
(Pfeiffer et al., 2020a; Rücklé et al., 2021), these
modular approaches allow models to be extended
to new data settings (Chen et al., 2019; Rücklé
et al., 2020), where newly learned knowledge can
be combined (Stickland and Murray, 2019; Wang
et al., 2021a; Pfeiffer et al., 2021a; Lauscher et al.,
2020a; Mahabadi et al., 2021b; Poth et al., 2021),
or stacked for combinatory cross-lingual (Pfeiffer
et al., 2020b, 2021b; Üstün et al., 2020; Vidoni
et al., 2020; Ansell et al., 2021b,a; Wang et al.,
2021b) as well as NMT scenarios (Bapna and Fi-
rat, 2019; Philip et al., 2020; Chronopoulou et al.,
2020; Le et al., 2021; Üstün et al., 2021; Stickland
et al., 2021; Garcia et al., 2021).
2.3 Weaknesses, improvements, and
extensions of language models
Next to the curse of multilinguality , recent works
have shown substantially reduced cross-lingual and
monolingual abilities of models for low-resource
languages with smaller pre-training data (Wu and
Dredze, 2020; Hu et al., 2020; Lauscher et al.,
2020b; Artetxe et al., 2020; Pfeiffer et al., 2020b,
2021b; Chau et al., 2020b; Ponti et al., 2020).
K et al. (2020); Artetxe et al. (2020) show that a
shared vocabulary is not necessary for cross-lingual
transfer. Chung et al. (2021) demonstrate that de-
coupling the input embeddings from the predic-
tion head improves the performance on a number
of downstream tasks. Dufter and Schütze (2020)
show that the number of parameters and training
duration is interlinked with the model’s multilin-
gual capability. Chung et al. (2020); Rust et al.
(2021) show that the tokenizer plays an important
role in the per-language downstream task perfor-
mance, which Clark et al. (2022); Xue et al. (2022);
Tay et al. (2021) take to the extreme by proposing
tokenizer-free approaches.
To extend a monolingual LM to other languages,Artetxe et al. (2020) train a new embedding layer
with a corresponding target-language tokenizer,
while freezing the pre-trained transformer weights.
Tran (2020) extend a monolingual model to new
languages using bilingual corpora. Wang et al.
(2020); Chau et al. (2020a) extend the vocabu-
lary of multilingual models with a small number
of target-language tokens, to improve the perfor-
mance in the target language. Muller et al. (2021)
propose a transliteration based approach, Vernikos
and Popescu-Belis (2021) propose subword map-
pings, and Pfeiffer et al. (2020b, 2021b); Vidoni
et al. (2020); Ansell et al. (2021b) propose adapter-
based approaches to extend multilingual models to
unseen languages.
While these approaches achieve considerable
performance gains over unseen languages, they are
outperformed by standard full ﬁne-tuning methods
for seen languages. One can further argue that, as
the pre-trained models have already been cursed by
multilinguality, the adapter-based approaches build
upon sub-optimal parameter initializations.In our
work, we consequently aim to 1)modularize the
model from the start to prepare the model to be 2)
extendable to new languages post-hoc.
3 Proposed approach
We propose X-M, a modular multilingual archi-
tecture that combines shared and language-speciﬁc
parameters. In contrast to prior work, we pre-
train modular models from the get-go. Our mod-
els can be extended to new languages after pre-
training, and used for cross-lingual transfer learn-
ing in downstream tasks.
Architecture. As illustrated in Figure 1, we
extend the transformer-based architecture from
mBERT (Devlin et al., 2019) and XLM-R (Con-
neau et al., 2020) by incorporating language-
speciﬁc modules—bottleneck feed-forward layers—
at every transformer layer. We learn a separate
module for each language, whereas the attention
and feed-forward components are shared. While
the total number of parameters of the model grows
linearly with the number of languages, the train-
ing and inference cost does not increase (as mea-
sured in FLOPs), as only the module in the relevant
language is used for each input. Inspired by the
adapterarchitecture of Pfeiffer et al. (2021a) we3481place our ‘modules’ after the LayerNorm of the
feed-forward transformer block, and the residual
connection is placed after the LayerNorm;the Lay-
erNorm before and after the modular component is
shared.
Pre-training procedure. Similar to Conneau et al.
(2020), we pre-train our model on MLM on com-
bined monolingual corpora in multiple languages.
Examples of each language are passed through
the shared embedding matrix as well as the multi-
head attention and feed-forward components at
each layer. As each layer contains a language-
speciﬁc modular component, the examples are
routed through the respective designated modular
bottleneck layer. Given that each example only
requires access to a single module, modules can
be efﬁciently stored on only a subset of GPUs in
distributed training.
Extending to new languages. The modular de-
sign of our model allows us to extend it to new
languages after pre-training. To that end, we learn
new embeddings and adapter modules for the tar-
get language through MLM, while the rest of the
components are frozen.Consequently, we are able
to extend the model to a new language by learning
a small number of new parameters, without affect-
ing performance in the set of pre-trained languages.
Following Pfeiffer et al. (2021b), we learn a new
subword vocabulary for the added languages, and
initialize the embeddings of lexically overlapping
tokens from the original embedding matrix.
Fine-tuning on downstream tasks. To transfer
the models to cross-lingual downstream tasks, we
ﬁne-tune the shared weights only on the source
language data, while keeping the modular compo-
nents and the embedding layer frozen. We follow
the standard ﬁne-tuning procedure of adding a pre-
diction head on top of the CLS token. We then
replace the source language modules (as well as
embedding layer for added languages) with the tar-
get language parameters, passing the text of the
target language through the model.4 Experimental design
We detail the baseline and models (§4.1), and their
training (§4.2) and evaluation settings (§4.3).
4.1 Model variants
We pre-train separate models for all combinations
along the following axes:
X-Mvs. .To evaluate the effective-
ness of our X-Mmodel, we aim to compare
ourselves to a conventional non-modular architec-
ture. However, simply removing the modular com-
ponent would be unfair, as the number of FLOPs
and trainable parameters per language would not
be the same—both in terms of pre-training, as
well as ﬁne-tuning. Consequently, for our base-
line model—where all parameters should be fully
shared between all languages—we include a single
bottleneck layer right after the Feed-Forward com-
ponent. Effectively, this is the same architecture
as our X-Mmodel, just with a single module
that is shared by all languages. We refer to this
as the model throughout this paper.To
extend the model to unseen languages,
we follow Artetxe et al. (2020) and only learn a
new embedding layer, freezing the transformer pa-
rameters. To ﬁne-tune the model on a
downstream task, we freeze the embedding layer,
as well as the (single) module, thereby ﬁne-tuning
an equal amount of parameters on the downstream
task as the X-Mmodel.
13 vs. 30 vs. 60 vs. 75 languages. So as to under-
stand how each approach is affected by the curse
of multilinguality, we pre-train the X-Mand models on 4 increasing sets of languages.
We start with an initial set of 13 typologically di-
verse languages that we evaluate on, and add addi-
tional languages for larger sets of 30, 60, and 75
languages. In addition, we keep a set of 7 held-out
languages that we extend the pre-trained models
to. Table 1 lists the speciﬁc languages in each3482
group. The selection and split of initial as well as
added languages is motivated by typological and
geographical diversity, as well as the availability of
downstream task evaluation data.
Controlling for total vs. per-language updates.
Conneau et al. (2020) investigated the effect of
adding more languages during pre-training, while
training on an equal number of update steps. How-
ever, increasing the number of languages while
keeping the number of updates constant results in
the model seeing less data in each individual lan-
guage. As such, it remains unclear if the curse of
multilinguality happens because of negative inter-
ference, or simply because the number of updates
for each speciﬁc language is smaller. So as to un-
derstand this, we compare (1) training on an equal
number of update steps and (2) training on an equal
number of seen examples per language. We start
with the set of 13 languages (Table 1) and train the
respective models for 125k update steps. When
adding more languages, we compare (1) training
models on each set of languages for 125k update
steps, and (2) increasing the number of update steps
such that the models are trained on the same num-
ber of examples in each of the initial 13 languages.
For the latter, this amounts to training for 195k,
265k and 269k update steps, respectively.
4.2 Training details
Data and hyperparameters. We sample lan-
guages with α= 0.7and train our models with
a batch size of 2048 across 64 V100 GPUs on
the CC100 dataset (Conneau et al., 2020) using
fairseq (Ott et al., 2019). All our models extend the
base transformer architecture, with 12 layers and
768 dimensions. Modules are implemented with
a bottleneck size of 384. The shared transformer
weights account for 270M parameters, whereas
each individual module accounts for 7M parame-
ters. We train our models with a linear learningrate decay peaking at 7e−4during pre-training and
1e−4when adding languages.
Vocabulary. As we aim to identify the impact
ofmodularity on the curse of multilinguality, we
control for consistent tokenization across the differ-
ent axes. We therefore tokenize using the XLM-R
vocabulary for all our pre-training experiments.
However, for languages added post-hoc, we learn a
newSentencePiece tokenizer for each of the target
language,as the languages potentially use scripts
unseen by the original tokenizer.
4.3 Evaluation
We conduct experiments on NLI, NER, and QA.
In all cases, we ﬁne-tune the model on English
and measure the zero-shot transfer performance in
other languages. For NLI we train on MultiNLI
(Williams et al., 2018) and evaluate on XNLI (Con-
neau et al., 2018). For QA, we train on SQuAD
(Rajpurkar et al., 2016) and evaluate on XQuAD
(Artetxe et al., 2020) and MLQA (Lewis et al.,
2020). For NER, we use WikiANN (Pan et al.,
2017; Rahimi et al., 2019). We experiment with
learning rates 1e−4,3e−4, and5e−4and train for
3 or 5 epochs for QA and 5 or 10 epochs for NER
and NLI. For NER and NLI we take the hyperpa-
rameter setting performing best on the development
sets, averaged across the pre-trained languages (Ta-
ble 1). For SQuAD we take the best performing
checkpoint evaluated on the English development
set, and report the cross-lingual test set results.
All results are averaged across 5 random seed runs.3483
5 Results and discussion
We present results for pre-trained languages in §5.1
and added languages in §5.2.
5.1 Pre-trained languages
In Figure 4 we plot downstream task results of
models pre-trained on different amounts of lan-
guages. Table 2 reports the individual language per-
formance for the models trained on 60 languages.
The Curse of Multilinguality. Conneau et al.
(2020) showed that multilingual LMs trained on in-
creasing amounts of languages, while maintaining
the number of update steps, exhibit drops in down-
stream task XNLI performance. We reproduce
these results, both in terms of language modelling
perplexity (Figure 2a),as well as downstreamtask performance on XNLI andNER (Figure 4a).
We further ﬁnd that the curse of multilinguality
does not only happen because the total number of
update steps per language decreases, but also when
all models are trained on the same num-
ber of examples per language (Figure 4b). This
conﬁrms that fully shared architectures suffer from
negative interference.
Lifting the Curse. While for the model
we witness negative interference between lan-
guages in terms of perplexity, the X-Mmodel is
able to maintain performance, and even improves
for a subset of languages. We observe similar
patterns in the downstream task performance: In
both our experimental setups—(1) we control for
the number of update steps (Figure 4a); (2) we
control for the number of per-language seen ex-
amples (Figure 4b)—our X-Mmodel—in con-
trast to the model—is able to maintain, or3484
even outperform model variants trained on less lan-
guages. These results demonstrate that the added
per-language capacity is sufﬁcient for the model to
adequately represent all languages.
Surprisingly, X-Mnot only maintains per-
formance, but actually slightly improves while we
increase the number of languages we pre-train on.
This is even the case for settings where the model
sees less examples in the target language. This
suggests that increasing the language diversity can
have a positive impact on the model’s cross-lingual
representation capability.
X-Mvs .Overall, the X-Mmodel
pre-trained on 60 languages achieves the best cross-
lingual performance.Our results on XNLI, NER,
MLQA, and XQuAD in Table 2 demonstrate con-
sistent performance gains over the model
for every task and across (almost) all high- as well
as low-resource languages.5.2 Extending to unseen languages
We further evaluate the cross-lingual performance
of languages added in the second step; (1) on the
architectural side—comparing the with
theX-Mmodelling variant—and (2) by com-
paring the performance when pre-training on the
language, vs. when adding the language post-hoc.
Modular vs Shared. We evaluate if the additional
per-language capacity improves the extendability
of the X-Mmodel. On the right in Figure 4a
we plot the results for added languages on XNLI
(top) and NER (bottom). Similarly, we plot the
results for the models where we control for the
number of seen examples per target language in
Figure 4b. We ﬁnd that the X-Mmodel consis-
tently outperforms the model, with a peak
performance when pre-training on 60 languages,
demonstrating that the language speciﬁc capacity
is beneﬁcial for adding new languages post-hoc.
We report results for the 60 language versions in
Table 3, demonstrating the consistent advantage of
the X-Mover the model.
Pre-training vs Adding Languages. To evaluate
if there is a measurable difference on downstream
performance for languages that we pre-train on vs.
those we add post-hoc , we train 2 models on differ-
entinitial sets of languages, adding the respectively
missing ones in the second step. So as to under-
stand if the typological similarity of languages has
impact on the downstream task performance, we
split the initial andadded languages (Table 1) of
our previous experiments into two parts. The ﬁrst
split consists of languages where the model was
pre-trained on at least one language of the same
language family (e.g. English vs. German). The
second split consists of languages that are part of
aunique language family, i.e. the model was not3485
pre-trained on a language of the same family (Ta-
ble 4). Consequently, we pre-train two models on
two sets of languages, adding the respective other
set post-hoc.
Our XNLI results (Figure 5) demonstrate that
the per-language performance is on par when pre-
training vs. when adding the language post-hoc.
We also ﬁnd that the family does not have a measur-
able effect on the performance of the language. Our
results therefore suggest that it is sufﬁcient to train
X-Mon only a subset of languages for which
sufﬁcient pre-training data exists. Essentially, X-
Mhas the potential to cover all languages of the
world, as the model has the capability to be adapted
to new languages post-hoc.
6 Further analysis
We further analyze the impact of the number of
update steps on X-M(§6.1) and compare our
method to adapter-based approaches (§6.2).
6.1 The importance of update steps
In Figure 4 we have witnessed a slight edge of
the model over the X-Mmodel, when
training on only 13 languages and only training
for 125k update steps. Dufter and Schütze (2020)
found that it requires a large number of update steps
for a model pre-trained on multiple languages to
become multilingual; with the added per-language
capacity we hypothesize that update steps also play
an important role for modular models. We com-
pare the downstream task performance of mod-
els pre-trained on 13 languages, when training for
125k with 250k update steps in Figure 6. When
training for longer we ﬁnd that the X-Mmodel
begins to outperforms the model in the
source language, while almost closing the gap in
the cross-lingual setting. This supports the hypoth-
esis that the X-Mmodel requires more update
steps when training only on a small number of lan-
guages, in order for modularity to “kick-in”.
6.2 X-Mvs. Adapters
As illustrated in Figure 3, from an architecture per-
spective X-Mis similar to previously proposed
multilingual Adapter-based methods ( MAD-X;
Pfeiffer et al., 2020b). MAD-X utilizes a pre-
trained massively multilingual transformer-based
model and ﬁne-tunes newly introduced adapter
weights on languages the model has seen during
pre-training, and ones the model has not been3486
trained on. For a fair comparison in terms of seen
examples andnumber of update steps we train a
transformer model without module components
(shared_nm ) for 100k update steps on the respec-
tive languages (Table 1). We subsequently train
adapters on each of the target languages for an-
other 25k update steps.We report results in com-
parison to X-Min Figure 7, here results for
shared_nm are for a model that was trained for
125k update steps to instantiate a fair comparison.
Our results demonstrate that the additional capac-
ity of adapters added after pre-training is not able
to mitigate the curse of multilinguality which has al-
ready had a catastrophic impact on the shared trans-
former weights; the performance of the adapters
strongly correlates with the performance of the cor-
responding fully shared model shared_nm . Conse-
quently, adding language-speciﬁc capacity during
pre-training is important, as the curse of multilin-
guality cannot be lifted post-hoc.
7 Conclusions
In this paper, we have evaluated the effectiveness
of modular multilingual language modelling across
multiple axes. We have demonstrated that by
providing additional per-language capacity, while
maintaining the total number of trainable parame-
ters per language, we are not only able to mitigate
negative interference between languages, but ad-
ditionally achieve positive transfer. Our results
suggest that it is sufﬁcient to train our proposed
X-Mmodel only on a subset of languages for
which sufﬁcient amounts of textual data is avail-able. Unseen languages can be added post-hoc,
with no measurable drop in performance on XNLI.
Bypre-training the model in a modular fashion, we
thus mitigate negative interference of idiosyncratic
information, while simultaneously preparing the
model to be extendable to unseen languages.
While in this work we have simulated language
adding scenarios with a held out set of languages, in
future work we aim to evaluate the performance on
truly low-resource languages such as MasakhaNER
(Adelani et al., 2021) and AmericasNLI (Ebrahimi
et al., 2021). We further aim to evaluate the cross-
lingual transfer performance from typologically
more diverse source languages, besides English.
Acknowledgments
We thank Samuel Broscheit for insightful feedback
and suggestions on a draft of this paper, as well
as the ARR reviewers and meta-reviewers for their
valuable comments.
References34873488348934903491
A Additional results
We report MLQA and XQuAD results on pre-
trained languages in Tables 5 and 6, respectively,
and MLQA results on added languages in Table 7.
Table 8 report NER results on more languages.
Figures 9, 10 and 11 report per-language results
as we increase the amount of languages on lan-
guage modeling perplexity, XNLI and NER, re-
spectively.
B Intermediate checkpoints
Our results in §6.1 suggest that, when the number
of languages is small, X-Mbecomes more com-
petitive with as the number of training
steps increases. So as to understand if this behav-
ior also holds for models covering more languages,
we evaluate intermediate checkpoints for the 60-
LANG model on XNLI. As shown in Figure 8,
we ﬁnd that the X-Mmodel continuously out-
performs the model. This suggests that
the model immediately suffers from neg-
ative interference between languages, while the
added, language-speciﬁc components of the X-
Mmodel are able to mitigate the curse of mul-
tilinguality, resulting in considerable performance
gains at all evaluated checkpoints.
C Language selection
We provide more details about our selection of
languages in Table 9.3492349334943495