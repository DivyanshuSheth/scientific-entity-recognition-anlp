
Xiangru TangArjun NairBorui WangBingyao WangJai Desai
Aaron WadeHaoran LiAsli CelikyilmazYashar MehdadDragomir RadevYale UniversityMeta AI
{xiangru.tang, arjun.nair, borui.wang, dragomir.radev}@yale.edu
{aimeeli, aslic, mehdad}@fb.com
Abstract
Factual inconsistencies in generated sum-
maries severely limit the practical applications
of abstractive dialogue summarization. Al-
though signiﬁcant progress has been achieved
by using pre-trained neural language models,
substantial amounts of hallucinated content
are found during the human evaluation. In
this work, we ﬁrst devised a typology of fac-
tual errors to better understand the types of hal-
lucinations generated by current models and
conducted human evaluation on popular dialog
summarization dataset. We further propose a
training strategy that improves the factual con-
sistency and overall quality of summaries via
a novel contrastive ﬁne-tuning, called C-
FT. To tackle top factual errors from our an-
notation, we introduce additional contrastive
loss with carefully designed hard negative sam-
ples and self-supervised dialogue-speciﬁc loss
to capture the key information between speak-
ers. We show that our model signiﬁcantly
reduces all kinds of factual errors on both
SAMSum dialogue summarization and AMI
meeting summarization. On both datasets, we
achieve signiﬁcant improvements over state-
of-the-art baselines using both automatic met-
rics, ROUGE and BARTScore, and human
evaluation.
1 Introduction
Text summarization is used to generate a concise
and accurate summary of a long text while focusing
on the sections that convey the most useful infor-
mation (Gurevych and Strube, 2004). In recent
years, the resurgence of dialogue summarization
has attracted signiﬁcant research attentions (Mc-
Cowan et al., 2005; Gliwa et al., 2019; Koay et al.,
2020; Zhang et al., 2021; Zhong et al., 2021; Zhu
et al., 2021; Chen et al., 2021a; Li et al., 2021; Chen
et al., 2021c; Fabbri et al., 2021; Chen et al., 2021d).
The goal of dialogue summarization is to condense
the conversational input into brief sentences ver-
sion but cover salient information (McCowan et al.,2005; Yuan and Yu, 2020). Signiﬁcant progress has
been made recently on abstractive dialogue summa-
rization with various pre-trained models. However,
such pre-trained models are susceptible to generat-
ing hallucinate content that is not supported by the
source documents (Cao et al., 2018; Maynez et al.,
2020; Kryscinski et al., 2020). To tackle the issue
of factual inconsistency in dialogue summarization,
recent works correctly encode the names of speak-
ers (Zhu et al., 2020), explicitly incorporate coref-
erence information (Liu et al., 2021b), and order
the personal named entities (Liu and Chen, 2021).
But it is still challenging to improve the quality
of summaries generated by different models and
decrease the hallucination at the same time.
To better understand the types of hallucinations
generated by the pre-trained models, we devised
a linguistically motivated taxonomy of factual er-
rors for dialogue summarization, instead of simply
classifying the summary as faithful or not. Based
on our typology, we deﬁned an annotation protocol
for factuality evaluation of dialogue summariza-
tion. We then conducted a human evaluation of
several pre-trained abstractive summarizers, includ-
ing BART (Lewis et al., 2020), Pegasus (Zhang
et al., 2020), and T5 (Raffel et al., 2020), aiming
at identifying the proportion of different types of
factual errors and studying the weaknesses of the
pre-trained models. Our typology and annotation
helps us gain deeper insights into the causes of
factual inconsistency. Unlike news summarization
(Pagnoni et al., 2021), we found that the challenges
posed by dialogue summarization are more related
to dialogue ﬂow modeling, informal interactions
between speakers, and complex coreference resolu-
tion. Figure 1 shows a dialogue-summary pair with
three speciﬁc errors.
In order to tackle the top factual errors produced
by existing models, we propose to replace the most
commonly used ﬁne-tuning with a linguistically-
informed contrastive ﬁne-tuning approach. For5657
example, the reason for producing wrong refer-
ence errors is that models cannot understand the
role in the dialogue, which goes beyond the events.
Our goal is to drive the model to pay attention
to the grounds of speciﬁc errors during the ﬁne-
tuning, and learn how to reduce the generation of
such errors. To be more speciﬁc, CFTlearns
to distinguish whether there are factual errors in
the summaries and capture the key information in
the dialogue content, such as numbers and person
names. Experiments on SAMSum (Gliwa et al.,
2019) and AMI (McCowan et al., 2005) show the
generalizability of CFTwhen it is applied to
different pre-trained models and datasets. Further-
more, we employ both automatic evaluation and
human evaluation on faithfulness and show that
CFTsigniﬁcantly reduces all different factual
errors and generates summaries that are more fac-
tually consistent. Moreover, we analytically ﬁnd
that optimizing the contrastive ﬁne-tuning is quite
beneﬁcial for improving the robustness of models,
which brings further beneﬁts.
Our contributions are as follows:
•We introduce the ﬁrst typology of factual er-
rors for dialogue summarization and use it
to conduct comprehensive annotation and fo-
cused analysis.
•Targeting different categories of factual errors
in the annotations, we reduce occurrence of
such errors generated by various pre-trained
models with a novel linguistically-informed
contrastive ﬁne-tuning CFT approach.
•We validate our method on a widely used dia-logue summarization corpus, SAMSum, and
extend it to a meeting summarization corpus
AMI. Evaluations of output summaries on au-
tomatic metrics like ROUGE, BARTScore as
well as human evaluations show that CFT
outperforms baseline pre-trained models.
2 New Taxonomy of Factuality Errors for
Abstractive Dialogue Summarization
In order to gain deeper insights into the types of
factuality errors introduced by different abstractive
dialogue summarization systems, we proposed a
new taxonomy of factuality errors for abstractive
dialogue summarization based on our empirical
experiments and annotations of the performance
of a set of representative baseline summarization
models on the SAMSum dataset, which is a widely-
used large-scale dialogue summarization dataset
of chat message dialogues in English (see Section
4.1). Speciﬁcally, we generate summaries of SAM-
Sum dialogues using state-of-the-art abstractive
dialogue summarization models, including models
ﬁne-tuned based on T5 (Raffel et al., 2020), Pe-
gasus (Zhang et al., 2020), BART (Lewis et al.,
2020), D-HGN (Xiachong et al., 2021), and S-
BART (Chen and Yang, 2021b). We then man-
ually annotate all different types of errors in these
generated summaries that are inconsistent with the
source dialogue, compute detailed statistics of all
these factuality errors, and then classify them into
different categories. Based on our annotation and
analysis, we propose a new taxonomy of errors with
the majority focusing on factuality error, which in-
cludes the following 8 error types:5658Category 1 - Missing Information: The content
of the generated summary is incomplete compared
to the reference.
Example:
[Reference Summary] Williams invites
Ms. Blair for a coffee. They will go to her
favourite coffee place near the square in
a side alley at 2 p.m.
[Model-Generated Summary] Ms. Blair
is going to a coffee place near the square
in a side alley.
Category 2 - Redundant Information: There is
redundant content in the generated summary com-
pared to the reference.
Example:
[Reference Summary] Paula helped
Charlotte with correct pronunciation of
"Natal Lily."
[Model-Generated Summary] Charlotte
asks Paula how to pronounce the name
of the plant "Natal Lily." Paula conﬁrms
that the stress on the second syllable is
2nd.
Category 3 - Circumstantial Error: Circumstan-
tial information (e.g., date, time, location) about
the predicate doesn’t match the reference.
Example:
[Reference Summary] The USA was
founded in 1776.
[Model-Generated Summary] The USA
was founded in 1767.
Category 4 - Wrong Reference Error: A pro-
noun is with an incorrect or nonexistent antecedent,
or a personal named entity in the generated sum-
mary is in the place of a different personal entity in
the reference.
Example:
[Reference Summary] Mohit asked Dar-
lene about the test.
[Model-Generated Summary] Darlene
asked Mohit about the test.
Category 5 - Negation Error: This encompasses
factual errors resulting from missing or erroneous
negation in the generated summary compared to
the reference.
Example:[Reference Summary] Justin likes books.
[Model-Generated Summary] Justin
does not like books.
Category 6 - Object Error: This covers factual
errors resulting from incorrect direct or indirect ob-
jects (for non-personal entities only; errors of this
nature involving personal entities are designated as
Wrong Reference Errors).
Example:
[Reference Summary] Tara raised her
glass.
[Model-Generated Summary] Tara
raised her spoon.
Category 7 - Tense Error: This encompasses fac-
tual errors resulting from discrepancies in gram-
matical tense between the generated summary and
the reference.
Example:
[Reference Summary] The children will
go to the library.
[Model-Generated Summary] The chil-
dren went to the library.
Category 8 - Modality Error: This includes fac-
tual errors resulting from modal discrepancies,
such getting words like "may", "should", "could"
wrong, between the generated summary and the
reference.
Example:
[Reference Summary] School may be
cancelled today.
[Model-Generated Summary] School is
cancelled today.
2.1 Annotation and Analysis
Using our proposed taxonomy of factuality errors,
we compute the proportion of each type of factual-
ity errors across different summarization models.
We then investigate the model generation behavior
that is indicative of errors, which guides the design
of our proposed model.
We performed a human evaluation of four model
outputs from 19 SAMSum dialogues in order to
identify the limitations of abstractive summariza-
tion models in dialogue summarization tasks. The
four models used in this human evaluation are
two BART models with different random seeds5659
(ROUGE-L 48 and 49) (Lewis et al., 2020), D-
HGN (ROUGE-L 40) (Xiachong et al., 2021), and
S-BART (ROUGE-L 48 (Chen and Yang, 2021b)).
BART and S-BART are pre-trained models (PLM),
and D-HGN is trained from scratch. Since we are
focusing on the dialogue domain, most of the fac-
tual errors in the model summaries are related to
coreference, anaphora, and other dialogue-speciﬁc
characteristics. In fact, approximately 45% of all er-
rors fall into the categories of Missing Information
and Wrong Reference. The distribution of these er-
rors throughout these pre-existing models informs
the limitations of each model. Our proposed C-
FTmodel targets the top errors generated by the
current state-of-the-art models to reduce factual
inconsistency.
3 CFT Model
Standard ﬁne-tuning parameterizes the probabil-
itypof the generator on a task-speciﬁc labeled
dataset by maximizing cross-entropy loss.
L=−/summationdisplay
logP(˜t|t,D) (1)
However, the cross-entropy loss has several
shortcomings that can lead to factual inconsistency
in dialogue summarization due to its sub-optimal
generalization and instability. We propose a more
efﬁcient ﬁne-tuning method CFTfor factual
consistency driven by the intuition that good gen-
eralization requires capturing the similarity in one
class and contrasting them in other classes. In
CFT, we introduce two additional losses: con-
trastive loss and self-supervised loss. We use two
weights, actually which is coefﬁcients, to adjust the
ratio ofLandLin the total loss of CFT.The ﬁnal training objective J(θ)of the proposed
framework is as follows:
J(θ) =L+αL+βL (2)
Our linguistically-informed typology and anno-
tation help us gain deeper insights into the causes of
different factual errors. To help our models gener-
ate more faithful summaries, the proposed CFT
learns to concentrate on the essential elements of
dialogue and capture the dynamic role information
as illustrated in Figure 3.
3.1 Contrastive Loss
In order to reduce the occurrence of factual errors,
we propose a contrastive loss that uses the follow-
ing negative sample generation techniques to target
each error type in our proposed taxonomy:
•Swap the nouns in the reference summary
with each other randomly. This aims to re-
duce wrong reference and object errors by
providing negative samples.
•Swap the verbs in the reference summary with
each other randomly. This aims to the model
reduce circumstance (and, to a lesser extent,
tense and modality) errors.
•Mask numbers and years in the dialogue and
then pass it into the model to generate a neg-
ative sample summary. This aims to reduce
circumstance errors.
•Randomly delete 30% of the sentences in the
dialogue and then pass it into the model to
generate a negative sample summary. This
aims to reduce missing information errors.5660•Mask-and-ﬁll coreferent entities with BART
in the dialogue and then pass it into the model
to generate a negative sample summary. This
aims to reduce wrong reference errors.
Equation 3 demonstrates our contrastive loss
function. During the ﬁne-tuning, we have the pos-
itive samples, which is the reference summaries
and another set of incorrect summaries, which is
the negative samples. The contrastive objectives
are learning representations that are invariant to
different views of positive pairs; while maximizing
the distance between negative pairs (Gunel et al.,
2020). Our goal is to maximize the likelihoods of
the positive samples and minimize the likelihoods
of the negative samples as well. We use the follow-
ing contrastive learning objective
whereyandyare positive summary pairs gen-
erated by back translation technology and yis
from negative set of examples and c,c,care
their BART encoder representations.
3.2 Self-supervised Loss
One unique challenge in abstractive dialogue sum-
marization is the use of ﬁrst-person pronouns (such
as "I" or "we") in speaker utterances, which the
model has to correctly identify as being a reference
to the speaker. This can lead to wrong reference
errors in the summary, as the model cannot under-
stand which participant is speaking and thus cannot
accurately resolve ﬁrst-person references. To ad-
dress this problem, we design a self-supervised loss
that aims to determine whether two tokens belong
to the same speaker. Based on these ﬁndings, we
design a self-supervised loss to enable CFTto
capture the dynamic roles in the dialogue.
After the BART encoder, the input dialogue is
encoded into hidden vectors C. Here, we ﬁrst ran-
domly select kpairs of two tokens tandtfrom
the input dialogue, with labels sandsdenoting
which speaker they are coming from. We also do
the same for utterances. Given the concatenation
of the encoder representation of dialogue, tand
t, we use the following loss function to classify
whether the two tokens or two utterances are from
the same speaker.This supplementary loss function helps CFT
keep track of speaker information, thus improving
the faithfulness of its summaries for dialogues that
contain several ﬁrst-person references.
4 Experiments
4.1 Dataset
We evaluate our new model on the popular SAM-
Sum dialogue summarization dataset. Then, we
extend our model to meeting summarization with
the AMI Meeting Corpus. SAMSum (Gliwa et al.,
2019) is a recently proposed large-scale dialogue
summarization dataset consisting of 16,369 chat
message dialogues in English written by linguists,
and each message dialogue is annotated with a
multi-sentence summary written by language ex-
perts. 75% of the dialogues in the SAMSum
dataset (Gliwa et al., 2019) are between two in-
terlocutors, and the other 25% are among three
or more interlocutors. The AMI Corpus is an-
other well-known dialogue summarization dataset
consisting of 137 multiparty meeting transcripts
extracted from 100 hours of meeting recordings.
Each meeting transcript in the dataset is also anno-
tated with a generic abstractive summary. We use
these two representative dialogue summarization
datasets to empirically test our new model’s abstrac-
tive summarization performance in the settings of
both short conversation-style dialogues and long
meeting-style dialogues. See Table 2 for detailed
statistics of the two datasets.
4.2 Experiment Settings
In our experiment using SAMSum, we trained
BART for 3 epochs with a learning rate of 1e−05,
Pegasus for 20 epochs with a learning rate of
1e−04, and T5 for 20 epochs with a learning
rate of 1e−05. In our experiment using AMI, we
trained BART for 6,000 steps with a learning rate
of1e−05, Pegasus for 24,000 steps with a learn-
ing rate of 1e−05, and T5 for 20,000 steps with a
learning rate of 1e−05.
4.3 Evaluation Metrics
To evaluate our model, we use three metrics:
ROUGE (Lin, 2004): ROUGE measures N-
gram overlap between the reference and the au-
tomatically generated summaries.
BARTScore (Yuan et al., 2021): Because
ROUGE scores only measure token overlap, other
automated metrics (Rebuffel et al., 2021; Kryscin-
ski et al., 2020; Wang et al., 2020; Scialom et al.,5661
2021) have been proposed to evaluate faithfulness
more precisely. BARTScore is a transformer-based
measure that scores a dialogue and the correspond-
ing automatically generated summary and has been
shown to be strongly correlated with human evalu-
ations of faithfulness (Yuan et al., 2021).
Human Evaluation : Finally, we conduct hu-
man evaluations on 100 SAMSum (Gliwa et al.,
2019) and 20 AMI (McCowan et al., 2005) dia-
logues. Tang et al. (2021) found that Likert scales
are a more consistent measure of factuality for ab-
stractive dialogue summarization than Best-Worst
Scaling. We have human evaluators directly rate
the summaries on a scale from 1 to 10 correspond-
ing to their faithfulness. In addition, using the error
taxonomy proposed in Section 2, we have them
mark whether each error type appeared in the given
summary. We do this in a blinded fashion, so that
the annotators do not see the corresponding model
of the summary. Additionally, in order to prevent
model information from leaking to the annotators,
we randomly shufﬂe outputs within each dialogue
before assigning them to annotators.5 Results
Table 1 shows the ROUGE scores of our models,
the baseline models they were ﬁne-tuned from, and
a number of other abstractive summarization mod-
els on the SAMSum and AMI datasets. Tables 5
and 6 show the average human faithfulness and
BART scores respectively for each model’s outputs
on 100 SAMSum and 20 AMI dialogues.
We observe that for all three pretrained models
CFTsigniﬁcantly beat baselines on ROUGE-
1, ROUGE-L, and human faithfulness score for
both datasets. For BARTScore, we note that, while
performance increases on SAMSum for all mod-
els, it decreases on AMI. However, given the fact
that human evaluators rated the outputs of all three
CFTmodels as more faithful than those of
their corresponding baselines on both datasets, the
decreases in BARTScore on AMI can likely be at-
tributed to the imperfection of automated metrics
at capturing faithfulness in text.
5.1 Error Analysis
Tables 3 and 4 show the percentage of summaries
that were labeled with each error type in our tax-
onomy of factual errors (discussed in Section 2.)
for both the baseline and CFTmodels on the
SAMSum and AMI datasets respectively.
We observe that on SAMSum, our ﬁne-tuning
method greatly reduces missing information, re-
dundant information, wrong reference, and circum-
stance errors for all models. The largest reduction
is on the "wrong reference" error type (20%, 7%,
and 33% for BART, Pegasus, and T5 respectively),5662
likely owing to the self-supervised loss function
introduced in Section 3.2 that was designed to help
the model more effectively capture speaker infor-
mation. For AMI, however, our ﬁne-tuning method
is not as consistent at reducing the frequency ofeach error type across models. It is possible that
this is due to sample size (20 AMI dialogues vs.
100 SAMSum dialogues).
5.2 Case Study
Figure 4 shows the results of human annotation on
the model outputs of a selected SAMSum dialogue.
Note that all of the autogenerated summaries, both
baseline and CFT, were marked as having miss-
ing information errors by the annotator, likely due
to the omission of Ernest’s relief upon hearing that
the car that was crashed into did not belong to
Mike. As a result, none of the models achieved a
perfect factuality score on this dialogue; however,
the scores for each CFTmodel were higher
than those of their corresponding baselines.
It can be observed that while baseline BART out-
puts a summary with a circumstance error, mistak-
enly asserting that Mike parked his car on Ernest’s
street, the BART+ CFTﬁxes this error, cor-
rectly asserting that Mike took his car to the garage
today; as a result, the human annotator gave this
summary a higher score than the predicted sum-
mary from baseline BART. Baseline T5 outputs a
summary with two coreference errors; speciﬁcally,
it contains a missing subject in the ﬁrst sentence
and incorrectly implies that the car that got crashed
into belonged to Mike in the second sentence. The
T5+CFTis able to ﬁx both of these errors,5663
adding "Mike" to the beginning of the ﬁrst sentence
and changing "his red Honda" to"a red Honda just
like Mike’s" in the second sentence. Similarly, the
output of baseline Pegasus contains a coreference
error in the ﬁrst sentence, implying that Mike owns
the car that was crashed into while the output of
Pegasus+CFT does not.
6 Related Work
Multi-party dialogues are especially challenging to
summarize using automated models, given that they
often contain pauses, false starts, reconﬁrmations,
hesitations, and speaker interruptions (Sacks et al.,
1978; Feng et al., 2021a; Chen and Yang, 2021a).
Previous work in the ﬁeld has addressed these chal-
lenges by incorporating semantic features, includ-
ing keywords (Zhu et al., 2020), domain termi-
nologies (Koay et al., 2020), topics (Zhao et al.,
2020; Liu et al., 2021a), entailment knowledge (Li
et al., 2018), and background knowledge (Feng
et al., 2021c). Other works exploit personal named
entities (Liu and Chen, 2021) and coreference infor-
mation (Liu et al., 2021b) to learning to distinguish
complex coreferent relationships expressed through
personal pronouns (including the ﬁrst person "I")
in the conversation (Lei et al., 2021). Researchers
have also explored conversational structure (Zhao
et al., 2021), utterance ﬂow modelling (Chen et al.,
2021b), syntactic structure (Lee et al., 2021), gran-
ularity control (Wu et al., 2021), but they have not
yet converged to a simple and practical solution.Our proposed taxonomy of factual errors and
annotations help us gain deeper insights into the
causes of factual inconsistency in abstractive dia-
logue summarization outputs.
7 Conclusion
We presented CFT, a novel method to improve
the faithfulness of abstractive dialogue summariza-
tion models via contrastive and self-supervised
ﬁne-tuning. By adapting the objective function
during ﬁne-tuning to incorporate a contrastive
loss that learns to distinguish positives from ex-
amples with factual errors, and a self-supervised
dialogue-speciﬁc loss that captures important di-
alogue information ﬂow between multiple inter-
locutors, CFTcan signiﬁcantly improve the
faithfulness of the abstractive summaries gener-
ated by transformer-based sequence-to-sequence
language models, and reduce multiple categories
of factuality errors in the abstractive summaries
by large margins. In our experiment on SAMSum
and AMI, we demonstrated that CFTachieves
better empirical performance compared to the base-
line models ﬁne-tuned with the traditional cross-
entropy loss, based on both automatic evaluation
metrics and human evaluation. Our work provides
new insights into improving the faithfulness of ab-
stractive summarization systems using carefully
designed novel objective functions for ﬁne-tuning
that captures important structures and features of
the text to summarize.56648 Ethics Statement
Human Evaluation We recruited seven volun-
teer participants for our error annotation, request-
ing speakers of English. The internal annotators are
Xiangru Tang, Arjun Nair, Borui Wang, Jai Desai,
Aaron Wade, Anushka Nijhawan, and Dragomir
Radev. These annotators are participating volun-
tarily. Our participants are free to opt out of the
study at any point in time. We have written four
scripts for use in the annotation process: (1) the
ﬁrst script generates an annotation spreadsheet and
a key spreadsheet from the model outputs. The
annotation spreadsheet does not contain the model
names; however, it contains an id that can be used
to recover the model name from the key spread-
sheet. For ease of annotation, summaries from
the same dialogue are grouped together; however,
they are randomly shufﬂed within each dialogue
so that the annotators cannot guess from the order-
ing as to which model is which. (2) The second
script splits an annotation spreadsheet into multi-
ple spreadsheets so that the work can be distributed
amongst annotators. (3) The third one merges these
spreadsheets back together after the annotation pro-
cess is ﬁnished. (4) The last script recovers the
model names from the key spreadsheet and inserts
them into the annotation spreadsheet. Each evalua-
tor is asked to examine whether there is an error and
the full context (dialogue, generated summaries,
and reference) and give a score on a scale of 1 to
10 for each of the criteria. We only consider faith-
fulness, instead of general quality. E.g. 1: very
poor, 3: poor, 5: neutral; 7: good; 10: very good.
We asked each internal annotator to evaluate 300
samples.
Other Ethical Issues (1) We did not use any per-
sonally identiﬁable information in the experiments.
(2) The goal of the project, improving the faithful-
ness of automatically generated summaries, is to
make the output of the summarization system more
reliable and minimize confusion for the readers
of the summaries. (3) We used existing summa-
rization datasets that do not contain any sensitive
information and are unlikely to cause any harm to
the annotators.References5665566656675668