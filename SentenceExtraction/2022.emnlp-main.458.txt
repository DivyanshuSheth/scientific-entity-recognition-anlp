
Yichong Huang Xiaocheng Feng Xinwei Geng Bing Qin
Harbin Institute of Technology, China
{ychuang, xcfeng, xwgeng, qinb}@ir.hit.edu.cn
Abstract
Although all-in-one-model multilingual neu-
ral machine translation (MNMT) has achieved
remarkable progress, the convergence in-
consistency in the joint training is ignored,
i.e.,different language pairs reaching conver-
gence in different epochs. This leads to the
trained MNMT model over-fitting low-resource
language translations while under-fitting high-
resource ones. In this paper, we propose a novel
training strategy named LSSD ( Language-
Specific Self-Distillation), which can allevi-
ate the convergence inconsistency and help
MNMT models achieve the best performance
on each language pair simultaneously. Specif-
ically, LSSD picks up language-specific best
checkpoints for each language pair to teach the
current model on the fly. Furthermore, we sys-
tematically explore three sample-level manipu-
lations of knowledge transferring. Experimen-
tal results on three datasets show that LSSD
obtains consistent improvements towards all
language pairs and achieves the state-of-the-art.
1 Introduction
Neural machine translation (NMT) (Kalchbrenner
and Blunsom, 2013; Sutskever et al., 2014) has
witnessed enormous and significant progress, in-
cluding network structures (Cho et al., 2014; Bah-
danau et al., 2015; Gehring et al., 2017; Vaswani
et al., 2017), attention mechanism (Luong et al.,
2015; Liu et al., 2016; Shen et al., 2018) and decod-
ing strategies (Xia et al., 2017; Geng et al., 2018;
Zhou et al., 2019). While achieving promising per-
formance (Wu et al., 2016; Hassan et al., 2018),
widely-used bilingual NMT actually causes huge
computational cost especially when tackling numer-
ous language pairs, thereby facilitating the recent
emergence of multilingual NMT (Ha et al., 2016;
Firat et al., 2016; Johnson et al., 2017; Lu et al.,Figure 1: Loss curves of a multilingual NMT model
trained to translate 4 languages into English. The upper
two boxes illustrate low-resource directions. And the
high-resource ones are placed in the bottom part. The
x-axis and y-axis indicate training epochs and dev losses
respectively. Dotted lines mark language-specific best
checkpoints (colorful ones) as well as the overall best
checkpoint (the black one).
2018; Aharoni et al., 2019). By directly translating
multiple language pairs with one model, multilin-
gual NMT (Tan et al., 2019; Zhang et al., 2020;
Fan et al., 2021; Zhang et al., 2021a) quadratically
accelerates deployment and effectively encourages
transfer learning between similar languages, which
greatly benefits low-resource directions (Arivazha-
gan et al., 2019) and successfully enables zero-shot
translation (Gu et al., 2019).
Despite the remarkable success, multilingual
NMT (Liu et al., 2020; Lin et al., 2021) clearly
expresses a strong disagreement on the uniform
convergence point across various translation cor-
pora. The underlying reason is the imbalance and
heterogeneity of available data in multilingual train-
ing (Wang et al., 2020a; Wu et al., 2021), which
also explains why the disagreement between high-
resource languages (HRLs) and low-resource lan-
guages (LRLs) is more pronounced, as illustrated in
Figure 1. Particularly, a fairly common and impor-6822tant observation about the learning of multilingual
NMT is that HRLs typically encounter underfitting,
while extremely severe overfitting generally arises
in LRLs.
In this paper, we cast this convergence incon-
sistency as the performance deficit between the
multilingual NMT model and its own language-
specific best checkpoints, and aim to reduce this
deficit. Towards tacking this problem, we pro-
pose a novel training strategy dubbed Language-
Specific Self-Distillation (LSSD). At each train-
ing step, LSSD appoints recent best checkpoints
oriented to each language pair as teacher models
and treats the current training model as the student
learning from each teacher model in a knowledge
distillation manner. Differently depending on data
selection and strength of performing knowledge
distillation indeed, several practical strategies are
employed to potentially provide a relatively fine-
grained manipulation of knowledge transferring:
1)LSSD-W , which performs distillation on
all data samples equally; 2) LSSD-S ,
which selects samples for distillation conditioned
on whether the teacher performs better than the
student; 3) LSSD-A , which varies the
distillation strength according to the sample-level
performance ratio between the teacher and student.
Experimental results on TED talksand WMT
under many-to-one and one-to-many settings
demonstrate that our method can obtain consistent
and significant improvement through remedying
the convergence inconsistency, ultimately achiev-
ing state-of-the-art translation performance.
2 Preliminaries
2.1 Neural Machine Translation
Bilingual neural machine translation (Bilingual
NMT) translates a sentence xin source language
into a sentence yin target language. Given a par-
allel corpus D={(x, y)∈ X × Y} , the neural
machine translation model is commonly trained
with the Maximum Likelihood Estimation (MLE):
θ= arg maxE/summationdisplaylogP(y|x, y;θ),
(1)
where P(·|·;θ)is the conditional probability with
model θ, which is usually implemented in anencoder-decoder architecture (Bahdanau et al.,
2015; Vaswani et al., 2017).
2.2 Multilingual Neural Machine Translation
Multilingual neural machine translation (multilin-
gual NMT) translates multiple language pairs with
one unified model. In this work, we follow John-
son et al. (2017) to train a multilingual NMT model
jointly using training datasets of Llanguage pairs
D={D, ..., D..., D}, where
Dis the dataset of language pair (S, T). To
encode and decode diverse languages to/from a
shared semantic space, a large multilingual vocab-
ularyVis constructed. And a language tag is ap-
pended to the beginning of source sentences to
specify the target language. Similar with bilingual
NMT, the MNMT model is also trained with the
same objective as Eq.1.
Model Selection Strategy The common practice
saves a checkpoint at the end of each training epoch,
and evaluates its performance on a set of develop-
ment sets D={D, ..., D, ..., D}. Fi-
nally the checkpoint with minimal average dev loss
is selected as the overall best checkpoint . This
average dev loss could be formalized as:
L(θ, D) =1
L/summationdisplayL(D;θ). (2)
In our work, we record language-specific dev losses
Land save the language-specific best check-
point towards each language pair ladditionally.
2.3 Self-Distillation
Knowledge distillation is an effective model com-
pression technology that distills knowledge from a
high-capacity teacher model into the compact stu-
dent model (Hinton et al., 2015). Self-distillation
is an intriguing variation on knowledge distilla-
tion with the fundamental difference that self-
distillation uses the same network for both the
teacher and student model. (Yang et al., 2019;
Zhang et al., 2019). Yang et al. (2019) use mod-
els in earlier epochs to guide the training of later
epochs, which boosts the predictive accuracy in im-
age classification by a large margin. In this paper,
we extend this idea to let the student model learn
multiple different targets in the same training epoch
and the number of targets is constantly changing in
different epochs.6823
3 Language-Specific Self-Distillation
In this section, we first introduce the overall process
of LSSD. Then, we provide a detailed formalization
of LSSD, with special emphasis on the three fine-
grained manipulations on knowledge transferring.
3.1 Overall
In this subsection, we present an overview of our
training strategy for multilingual NMT, as illus-
trated in Figure 2. Specifically, we first take bilin-
gual self-distillation as an example to show the
distillation learning process. Then, we describe the
multilingual self-distillation model and how it per-
forms self-distillation towards multiple language
pairs.
Bilingual LSSD Traditional bilingual neural ma-
chine translation builds a Sequence-to-Sequence
model for training. In our bilingual LSSD, we intro-
duce an additional teacher model and a distillation
switch. The teacher model is used to guide the
original machine translation model and the switch
is designed to decide whether the teacher model is
working or not in the current training epoch. We
depict the bilingual LSSD process in the upper half
of Figure 2.
Normally, at the beginning of training, the loss
monotonically decreases, and we refer to this pro-
cess as “initial training stage”. And kdenotes thenumber of epochs the initial training stage lasts.
It should be noted that the value of kis not a
hyperparameter but is up to the training process.
During these kepochs, we do not perform distil-
lation (keep the distillation switch off) but replace
iteratively the teacher with the better-performance
student which has a lower loss. As we all know, the
lower loss, the better. Therefore, if the teacher’s
loss is lower than the student’s loss, we turn on the
switch, as shown in the k+1 epoch. In fact, we
do not perform distillation due to the switch being
closed at the beginning of this epoch. In the k+2
epoch, since the switch is on, the teacher model dis-
tills the student model. And since the teacher’s loss
is still lower than the student’s loss, the switch re-
mains on. In the k+3 epoch, the switch also shows
turned on, the distillation learning is performed.
But the teacher’s loss is higher than the student’s
loss, we turn off the switch. At the same time, we
replace the teacher model with the current student
model to complete the teacher updating.
Multilingual LSSD Multilingual LSSD is a com-
plex version of bilingual LSSD, which needs to
maintain multiple language-specific teachers and
conduct multi-objective distillation learning. We
illustrate the multilingual LSSD process in the bot-
tom half of Figure 2. The blue box also means the
student model (the current training MNMT model)6824Algorithm 1 Language-Specific Self-Distillation
and the value of loss represents the average loss
of the student over multiple language-specific dev
sets in the corresponding training epoch. The dif-
ferent kinds of orange boxes represent different
language-specific teacher models, and each loss
means different language dev loss in the current
training epoch. For example, the darkest orange
boxbos refers to the teacher model in Bosnian -
to-English translation.
At the beginning of training, multilingual LSSD
also has an “initial training stage” for each lan-
guage pair just like the bilingual LSSD, at which
time the language-specific teacher doesn’t work.
Note that the initial training stage of different lan-
guage pairs may last different epochs. After the
initial training stage, the language-specific teacher
model begins working, and the student model can
be guided by the teacher. The different kinds of
orange lines mean the language-specific teacher
is distilling the student model and the number of
working teachers is determined by the language-
specific switches, which are consistent with the
distillation switch of bilingual LSSD. The blue line
represents the language-specific teacher is replaced
by the better-performance student from the last
training epoch. In this paper, multilingual LSSD is
the superposition of multiple bilingual LSSDs and
they do not affect each other. We also summarize
this process in Algorithm 1.3.2 Formalization of LSSD
Formally, we denote the current model by θand
maintain a set of language-specific best check-
points {ˆθ}. In the validation stage, which is
at the end of each epoch, we evaluate the perfor-
mance of θon each language pair. For each lan-
guage pair l, ifθoutperforms ˆθin dev set D,θ
replaces ˆθ. To control whether to perform teaching
in the current epoch, we define a set of distillation
switches {Ω}. In the validation stage, if the
current model θexceeds the language-specific best
checkpoint ˆθ, we turn off the distillation switch,
not performing teaching in the next epoch. Con-
versely, when the language-specific best checkpoint
wins, we turn on the switch, performing teaching
in the next epoch.
When the language-specific distillation switch is
on, the parameters of the current model θis updated
by optimizing both L andα· L. When
the language-specific distillation switch is off, the
model θis updated only by L,i.e.,α= 0. The
training loss is calculated as:
L=L+αL, (3)
where the αis the weight of distillation loss. And
L is computed as the cross-entropy between
the output distribution of ˆθandθ, which is formal-
ized as:
L =−/summationdisplay/summationdisplayP(w|x, y;ˆθ)
·logP(w|x, y;θ).(4)
3.3 Sample-level Manipulations for LSSD
To prevent the potential negative transferring on
some translation samples where the teacher under-
performs the student, we devise three sample-level
manipulations for LSSD: LSSD-W ,LSSD-
S andLSSD-A . All of them
could be generalized as rescaling the distillation
loss with a sample-level weight:
L =L× G, (5)
where Gis the sample-level weight which is de-
termined by the performance difference between
teacher and student. And different operations cor-
respond different implementation of G.6825
LSSD-W In this manner, we equally exe-
cute distillation with the same sample-level weight
on all data samples. In practice, it is equivalent to
setting G= 1.LSSD-W can be viewed as
the base version of LSSD.
LSSD-S To avert the negative impact
of performing distilltaion on data samples where
the teacher model errs on, we explore to only dis-
tilling samples where the teacher performs better
than the student. And this is implemented as:
G(x, y, θ, ˆθ) =/braceleftigg
0, P(y|x;ˆθ)< P(y|x;θ)
1, P(y|x;ˆθ)≥P(y|x;θ),
(6)
where P(y|x;θ)is the likelihood probability that
model θoutputs with respect to the target sentence
ygiven the input x. And we compute the sentence
probability by averaging probabilities over all to-
kens.
LSSD-A Since a one-size-fits-all rule
prohibiting distillation on a subset of samples could
limit flexibility, we design LSSD-A
changing the distillation weight according to the
teacher-student performance ratio, which is imple-
mented as:
G(x, y, θ, ˆθ) =min(P(y|x;ˆθ)
P(y|x;θ), σ),(7)
where σis a hyperparameter to truncate the Gwhen
higher than σ, value of which is set to 2 empiri-cally.
4 Experiments
4.1 Settings
Datasets We conduct experiments on three
datasets: the widely-used TED-8-Diverse and TED-
8-Related (Wang et al., 2020a), and a relative large-
scale WMT dataset. The TED-8-Diverse contains
4 low-resource languages ( bos,mar,hin,mkd)
and 4 high-resource languages ( ell,bul,fra,
kor) to English. The TED-8-Related contains 4
low-resource languages ( aze,bel,glg,slk)
and 4 related high-resource language ( tur,rus,
por,ces) to English. Both of these two datasets
have around 570K sentence pairs. We detail the
data statistics and the interpretation of language
codes in Appendix A.
For the WMT dataset, we consider 3 low-
resource languages ( et,ro,tr) and 3 high-
resource languages ( fr,de,zh) to English. To-
tally around 5M training sentences are sampled
from the parallel corpus provided by WMT14,
WMT16, WMT17, and WMT18. And we use
the corresponding dev and test sets for validation
and evaluation. The detailed data statistics are
also placed in Appendix A. Compared to TED-
8-Diverse and TED-8-related, the size of the WMT
dataset is larger and distributed more unevenly over
various languages.6826
For each dataset, we experiment in two mul-
tilingual translation scenarios: 1) M--
O(M2O): translating multiple languages to En-
glish in this work; 2) O--M (O2M):
translating English to different languages.
Hyperparameters We verify the effectiveness of
LSSD on the Transformer (Vaswani et al., 2017)
as implemented in fairseq (Ott et al., 2019) with 6
layers and 8 attention heads. And we use the same
hyperparameters with the previous SOTA (Zhou
et al., 2021) to obtain a strong baseline. The only
difference with Zhou et al. (2021) is that we train
all models for 300 epochs which is less than theirs.
For the TED-8-Diverse and TED-8-Related, we
follow previous works Wang et al. (2020a); Zhang
et al. (2021b) to preprocess both datasets using
sentencepiece (Kudo and Richardson, 2018) with
a vocabulary size of 8 Kfor each language. For
the WMT dataset, we preprocess data using senten-
cepiece with a vocabulary size of 64 Kfor all lan-
guages. The complete set of hyperparameters can
be found in Appendix B. All models are trained on
8 Tesla V100 GPUs. And the performance is evalu-
ated with BLEU score using sacreBLEU (Papineni
et al., 2002; Post, 2018). We set distillation weight
αto 2.0 in M2O and 0.6 in O2M respectively (see
section 5.4 for analysis on these choices).
Baselines We compare our LSSD with: 1)
the standard-trained multilingual NMT model(i.e.,M ) (Johnson et al., 2017); 2)
M -D (Tan et al., 2019), which is also
a distillation-based strategy that guides the multi-
lingual model in each translation direction utiliz-
ing bilingual models. To re-implement M -
D , we first train bilingual models of each
language pair on all three datasets. Follow previ-
ous works (Tan et al., 2019; Zhang et al., 2021b),
we train bilingual models using the same model
configuration and hyper-parameters with multilin-
gual models. For all baselines and our LSSD, the
same model configuration and hyper-parameters
are applied.
4.2 Main Results
Overall results We summarize main results into
Table 1. As we can see, 1) on all three datasets, our
LSSD significantly outperforms the baselines under
M2O and O2M settings, demonstrating the effec-
tiveness of our approach; 2) compared with previ-
ous works, LSSD achieves higher BLEU scores on
the TED-8-Diverse and TED-8-Related datasets,
which indicates the superiority of our method;
3) in the comparison among the three variants
of LSSD, LSSD-A excels in M2O and
LSSD-W performs best in O2M overall. To
better understand this phenomenon, we analyzed
the teacher-student performance ratio (detailed in
Equation 7) in M2O and O2M respectively, and dis-
covered that this ratio varies more significantly in
the challenging O2M translation than in the M2O6827
(the variance is 0.097 in O2M and is 0.039 in M2O),
which may incur an unstable training for LSSD-
Adaptive.
Results on each language Looking closer at re-
sults per languages for the M -D and
our LSSD, we calculate the difference between
M andM -D or LSSD
separately in terms of BLEU, which is shown in
Table 2. Firstly, on all datasets and settings, LSSD
consistently outperforms the M
baseline across all language pairs . The improve-
ment is up to 2.37 ( eng→mkd). Secondly, Multi-
Distill struggles in low-resource directions across
all three datasets, which is not unexpected consider-
ing that the training data is too scarce to train trust-
worthy bilingual teacher models. However, LSSD
breaks this limitation, obtaining stable improve-
ments on both low-resource and high-resource lan-
guage pairs, by employing multilingual NMT mod-
els that own more balanced performance as teacher
models.5 Analysis
5.1 Convergence Inconsistency
In this work, we propose to formalize the conver-
gence inconsistency as the loss gap between the
overall best checkpoint and language-specific best
checkpoints, which is referred to as “ Performance
Deficit )”. Concretely, for each language pair l, we
accumulate the dev loss differences between the
overall best checkpoint θand the corresponding
language-specific best checkpoint ˆθ. We give a
formal definition as:
DUB (θ,{ˆθ})=/summationdisplay(L(θ, D)−L(ˆθ, D)).
(8)
Note that the performance deficit is a non-negative
value because L(θ, D)is always greater than
or equal to L(ˆθ, D). We list the performance
deficit of the M baseline and our
LSSD in Table 3. As observed, the performance
deficit of LSSD is significantly lower than the base-
line (decreased 57% on average), which proves
the efficacy of LSSD in remedying convergence
inconsistency.
5.2 Multiple Teachers vs. Single Teacher
To provide light on the necessity of using language-
specific best checkpoints as teacher models, we
conduct ablation study in Table 4. The STSD (Sin-
gle Teacher Self-Distillation) uses the overall best6828
checkpoint instead of language-specific best check-
points to guide the training of multilingual NMT
models. To make a fair comparison, we report the
results of LSSD-W . As indicated, the gains
from STSD over the baseline only account for 20%
to 60% of LSSD’s gains in the M2O translation.
Even more, STSD fails to enhance the O2M trans-
lation. These results show that the multilingual
model gains a great deal from tailored language-
specific teachers indeed.
5.3 Comparison of Loss Curves
To better comprehend how each approach affects
the model training process, we analyze different
methods from the perspective of loss curves, as
illustrated in Figure 3. Firstly, comparing the
loss curves of the M baseline with
LSSD (solid lines vs. dotted lines), it is clear that
the baseline suffers from serious over-fitting in
low-resources ( e.g.,bos↔eng). By letting the
model recall previous checkpoints, LSSD mitigates
the over-fitting, which delays the convergence to
lengthen the training time for high-resource lan-
guages. Secondly, comparing the loss curves of
baselines in M2O and O2M (dotted lines in left
vs. right half section), it is observed that M2O
suffers from more serious over-fitting than O2M.
This explains why M2O benefits more from LSSD
than O2M. Lastly, contrasting the three modes of
LSSD ,LSSD-A performs better in M2O
and achieves comparable with LSSD-W in
O2M in terms of dev loss.
5.4 Effect of Distillation Weight α
As Equation 3 shows, LSSD trains multilingual
NMT models with NMT loss and the α-weighted
distillation loss jointly. We demonstrate the effect
of different αon LSSD in Figure 4. As we can see,
the optimal weight for O2M ( α= 0.6) is smaller
than which for M2O ( α= 2.0). We conjecture
this is due to the fact that O2M converges later
than M2O (see Section 5.3), meaning that O2M
models might learn from immature teachers for
more epochs. Consequently, a lower distillation
strength is more suited to eliminating this danger.
6 Related Works
6.1 Advances in Multilingual NMT
Recently, multilingual NMT mainly focuses on:
1) designing effective parameters sharing strategy
(Zhang et al., 2021a; Zhu et al., 2021; Xie et al.,
2021; Lin et al., 2021); 2) obtaining language-
agnostic representations (Zhu et al., 2020; Pan
et al., 2021); 3) incorporating pre-training mod-
els (Siddhant et al., 2020; Wang et al., 2020b); 4)
resolving the data imbalance among diverse lan-
guages (Wang et al., 2020a; Wu et al., 2021; Zhang
et al., 2021b; Zhou et al., 2021). Different from
them, LSSD is designed for alleviating the conver-
gence inconsistency, which is ignored by existing
works.
6.2 Knowledge Distillation in NMT
To the best of our knowledge, Kim and Rush (2016)
first apply knowledge distillation in bilingual NMT
and propose a sequence-level distillation. Wei et al.
(2019) propose to avoid over-fitting by guiding the
training process with best checkpoints.
In multilingual NMT, Tan et al. (2019) use
knowledge distillation to close the gap between
the multilingual NMT model and bilingual models.
However, their work is based on the hypothesis
that there are sufficient training data for each lan-
guage pair to prepare a promising bilingual teacher.
In fact, most languages face the resource-scarcity
problem. In our work, LSSD’s teacher is the mul-
tilingual model, which has a better performance
for low resource language translation via transfer
learning. And we further develop three sample-
level operations for LSSD via weighing teacher
and student performance.68297 Conclusion
In this work, we propose a novel training strategy
Language- Specific Self-Distillation (LSSD) to rem-
edy the convergence inconsistency in multilingual
neural machine translation. Moreover, we devise
three sample-level manipulations for LSSD. Exper-
imental results on three datasets demonstrate that
LSSD achieves SOTA performance. Through anal-
ysis experiments, we also find that: 1) LSSD sig-
nificantly mitigates the convergence inconsistency
(decreased 57% on average), which is quantified
by performance deficit; 2) both low-resource and
high-resource languages benefit from LSSD.
8 Limitations
Same with other distillation-based works (Hinton
et al., 2015; Tan et al., 2019), LSSD takes some
extra training overhead. Taking the training in TED-
8-Diverse M2O as an example, the B and
LSSD-W spend 5.7 hours and 7.6 hours re-
spectively. However, it is worth noting that our
method doesn’t affect the inference speed of the
model.
Acknowledgements
Xiaocheng Feng is the corresponding author of this
work. We thank the anonymous reviewers for their
insightful comments. This work was supported
by the National Key R&D Program of China via
grant 2020AAA0106502, National Natural Science
Foundation of China (NSFC) via grant 62276078
and the Major Key Project of PCL, PCL2022D01.
References6830683168326833A Data Statistics
Data statistics of the TED-8-Diverse and TED-8-
Related are listed in Table 5. Data statistics of the
WMT dataset is listed in Table 6.
B Hyperparameters
In this section, we list the details of hyperparame-
ters we use for the experiments.
•We adopt the architecture with 6 layers and 8
attention heads.
•The embedding dimension is 512 and the FFN
has a dimension of 2048.
•We use Adam optimizer (Kingma and Ba,
2015) with β= 0.9, β= 0.98, and the
same learning rate schedule as Vaswani et al.
(2017).
•Batch size is set to 64K and half-precision
training is adopted (Ott et al., 2018).
•For regularization, we use the dropout as
0.3 (Srivastava et al., 2014) and the label
smoothing as 0.1 (Szegedy et al., 2016).
•For sampling strategy, we use temperature-
based sampling (Arivazhagan et al., 2019) andsetτ= 1on the TED-8-Diverse and TED-8-
Related. And we set τ= 5 on the WMT
dataset as the it is more imbalanced.
•For inference, we use beam search with beam
size 5.
C Bilingual vs. Multilingual
We list the results of bilingual models and the mul-
tilingual model in Table 7, 8, 9.68346835