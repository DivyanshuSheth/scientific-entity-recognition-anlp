
Chantal ShaibMillicent L. LiSebastian Joseph
Iain J. MarshallJunyi Jessy LiByron C. WallaceNortheastern University,The University of Texas at Austin,King’s College London
Abstract
Large language models, particularly GPT-
3, are able to produce high quality sum-
maries of general domain news articles in few-
and zero-shot settings. However, it is un-
clear if such models are similarly capable in
more specialized, high-stakes domains such as
biomedicine. In this paper, we enlist domain
experts (individuals with medical training) to
evaluate summaries of biomedical articles gen-
erated by GPT-3, given zero supervision. We
consider both single- and multi-document set-
tings. In the former, GPT-3 is tasked with gen-
erating regular and plain-language summaries
of articles describing randomized controlled
trials; in the latter, we assess the degree to
which GPT-3 is able to synthesize evidence
reported across a collection of articles. We
design an annotation scheme for evaluating
model outputs, with an emphasis on assessing
the factual accuracy of generated summaries.
We ﬁnd that while GPT-3 is able to summarize
and simplify single biomedical articles faith-
fully, it struggles to provide accurate aggrega-
tions of ﬁndings over multiple documents. We
release all data and annotations used in this
work.
1 Introduction
Large language models have been shown to be ca-
pable of producing high-quality and reasonably ac-
curate summaries in zero-shot settings (Goyal et al.,
2022; Liang et al., 2022), with GPT-3 besting fully
supervised models in generic news summarization,
according to human judgments (Goyal et al., 2022).
In this work we evaluate if such models are simi-
larly able to summarize medical literature, a high-
stakes domain that demands factual accuracy.
Speciﬁcally, we use the newest iteration of GPT-
3 (text-davinci-003 ;GPT3-D3 from here) to gen-
erate summaries of (a) individual articles describ-
ing individual randomized controlled trials (RCTs)Figure 1: We enlist domain experts to evaluate the fac-
tual accuracy of summaries and simpliﬁcations of med-
ical articles describing clinical trials. We consider both
single- and multi-document settings.
evaluating the efﬁcacy of interventions, and, (b) col-
lections of such articles that describe several trials
addressing the same underlying clinical question
(e.g., evaluating the same medication). These con-
stitute single- and multi-document summarization
tasks, respectively. In the single-document case,
we also evaluate the ability of GPT3-D3 to summa-
rize in plain language . We enlist domain experts
(with medical training) to annotate model outputs,
and seek to address the following questions.
RQ1 Does GPT3-D3 produce faithful summaries of
medical articles?
RQ2 Can GPT3-D3 accurately simplify while also
summarizing such texts?
RQ3 CanGPT3-D3 synthesize —aggregate the ﬁnd-
ings presented in—multiple input articles in a way
that accurately reﬂects the totality of the evidence?
RQ4 What sort of factual mistakes does GPT3-D3
make when performing these tasks (if any), and
what are the risks implied by such errors?
Overall, we ﬁnd that GPT3-D3 performs single-
document summarization and simpliﬁcation with
reasonably good accuracy. However, it is less able
to accurately synthesize evidence reported in col-
lections of trials (in the multi-document case). We1387release all model outputs and accompanying anno-
tations to facilitate additional work on this topic.
2 Single Document Summarization
Data We sample 100 articles describing random-
ized control trials (RCTs) indexed in the Trial-
streamer database (Marshall et al., 2020), which
also provides automatically extracted “key re-
sults”alongside titles and abstracts. We search for
trials published after November 28 2022, following
the release date of GPT3-D3 , to ensure the model
has not seen any of the studies during pre-training.
Experimental Setup Using the RCT data de-
scribed above, we evaluate the ability of GPT3-D3
to faithfully summarize and simplify biomedical
texts in a zero-shot setting. We also compare
GPT3-D3 summaries to summaries generated using
Flan-T5 (Wei et al., 2021), but qualitatively ﬁnd
thatGPT3-D3 summaries are much higher quality.
We provide results of this comparison in Appendix
F.3. Speciﬁcally, we prompt GPT3-D3 to separately
produce: (i) a technical summary, and, (ii) a plain
language summary (August et al., 2022). See Ap-
pendix C for all prompts.
Study Design We designed an evaluation scheme
that captures the sensitivity of medical informa-
tion. To assess factuality, we collect annotations
about omissions and errors with respect to main
results, and key components of the trials including
populations, interventions, and outcomes (“PICO”
elements; Richardson et al. 1995). Where appropri-
ate, we ask annotators to highlight spans of gener-
ated text that are inconsistent with the input—these
might be “new” concepts introduced or spans that
directly contradict the input. To gauge overall lin-
guistic quality, we solicit assessments regarding
the ﬂuency and usefulness of a summary on a Lik-
ert scale (1932). We include additional questions
about the simpliﬁcation of technical terms for the
plain language summaries. We provide a complete
taxonomy of the survey in Appendix H.
Annotations We recruited 3 domain experts with
medical training on the Upwork platform,and
task them each with annotating 100 samples. In
total, we collect 300 annotations (3 annotations per
sample). We use Label Studioas our interface.3 Multiple Document Summarization
and Evidence Synthesis
Data For multi-document summarization, we
download meta-analyses from the Cochrane Li-
brary (these are reviews of medical evidence, usu-
ally RCTs).Our ﬁnal sample contains 50 multi-
document studies comprising meta-review titles,
reference abstracts (inputs), and target conclusions
(target summaries) written by domain experts, 10
of which were published post- GPT3-D3 release.
Experimental Setup Because inputs comprise
multiple abstracts, these (together with generated
tokens) often exceed the token capacity of GPT3-D3 .
In our dataset, about 41% of the samples exceeded
this upper-bound. We report information about our
data, including average length, in Appendix B. To
address the upper-bound problem, we adopt a sim-
ple two-phase strategy for multi-document summa-
rization. First, we generate independent summaries
for each abstract, using the single-document sum-
marization prompt described in Section 2. Then,
we include all the generated single-document sum-
maries in our multi-document synthesis prompt
(examples in Appendix C).
Study Design Our evaluation rubric asks for as-
sessments of generated outputs as compared to:
(a) inputs, and, (b) target summaries. Speciﬁcally,
we ask if generated summaries are supported by
thesummaries provided as inputs in the multi-
document case, and to what extent they agree with
target (reference) summaries. We also ask annota-
tors to highlight spans of text in generated outputs
that disagree with paired target summaries. We
reproduce the full rubric in Appendix H.
With respect to annotators, we use the same pro-
cedure described in Section 2; we recruited 3 new
medical experts and tasked them each with anno-
tating 50 samples, for a total of 150 annotations.1388
4 Results
RQ1: Does GPT3-D3 produce faithful sum-
maries of medical articles? In the single doc-
ument setting, we ﬁnd that GPT3-D3 generates sum-
maries of biomedical abstracts that are fairly high-
quality. Figure 2 (a) shows that annotators rated a
majority of the summaries as being coherent, use-
ful, and capturing “key results”.
When GPT3-D3 does err, it tends to make mi-
nor mistakes or omit details. The latter is more
common than the former, as shown in Figure 3 (a).
RQ2: Can GPT3-D3 accurately simplify while
summarizing medical texts? Shown in Figure
2 (b), GPT3-D3 produces simpliﬁed summaries that
are similarly deemed to be coherent and useful,
and which appear to contain key results. Simpliﬁed
outputs are scored highly in terms of readability, in-
dicating that these summaries would be understood
by someone without medical training.
In comparison to the technical summaries, Fig-
ure 3 (b) shows that there are fewer omissions but
a slightly higher amount of errors. These may be
problematic, but — importantly — some omissions
are expected in a simpliﬁed summary, as certain
details that are important for an accurate summary
for a technical audience may not be necessary to
convey key information to a more general audience.
RQ3: Can GPT3-D3 synthesize ﬁndings pre-
sented in multiple input articles in a way that
accurately reﬂects the totality of the evidence?
We now evaluate GPT3-D3 ’s performance on multi-
document summarization, i.e., its ability to synthe-
size evidence (Wang et al., 2022). Figure 4 shows
that most summaries generated by GPT3-D3 in this
setting are supported by the inputs. This is consis-
tent with our ﬁndings in RQ1 :GPT3-D3 is able to
summarize faithfully with respect to given input.
However, we ﬁnd that generated summaries do not
consistently agree with the target summaries. In-
deed, Figure 4 shows that generated summaries dis-
agree with the targets in over half of cases. This dis-
crepancy suggests that human-written summaries
in the biomedical domain require a level of synthe-
sis that is not captured by GPT3-D3 .
RQ4: What sort of factual mistakes does
GPT3-D3 make and what are the risks? In RQ1,
we reported that GPT3-D3 sometimes omits key
information. Figure 5 characterizes the types of
omissions and errors made, with respect to PICO
elements. GPT3-D3 tends to underspecify elements
in the summary more often than generating inaccu-
racies. Appendix F provides further details regard-
ing underspeciﬁcation. In the simpliﬁcation task,
GPT3-D3 capably simpliﬁes most technical terms
in the generated output (Figure 6).
Regarding RQ3, we showed that there are often
discrepancies between generated and target sum-
maries, despite the former being supported by the
inputs. Human-written summaries of trials may be1389
more cautious in their conclusions. We measure the
evidence strength and direction of both the target
and generated summaries, and ﬁnd that GPT3-D3
tends to recommend marginal or substantive bene-
ﬁcial effects regarding interventions in the majority
of the summaries (Figure 7).
Overall, we ﬁnd that GPT3-D3 copies frequently
from inputs. This results in summaries that are of-
ten faithful to the input. It may also be one reason
that summaries tend to have more omissions (rather
than errors) in the single document case, and it may
also explain how summaries in the multi-document
case often disagree with the reference synopsis
while also being supported by (some subset of) the
inputs. We calculate the degree of overlap and sim-
ilarity between inputs and generated summaries
from GPT3-D3 for both single-document and multi-
document summarization at the sentence level (Fig-
ure 8). GPT3-D3 often copies sentences verbatim.
In other cases, it changes phrasings but only very
slightly (see Appendix F for examples).
Further, Figure 8 shows how many sentences in
each summary have a BLEU score of ≥30; which
indicates the sentences are highly aligned. Over
70% of the summaries have at least a quarter of
the sentences copied from the input. Appendix F
shows some examples of highly similar summaries
and sentence pairs.
5 Related Work
More broadly in summarization, several efforts
have called for increased emphasis on human
(rather than automated) evaluation of generated
texts, increased deployment of human-centered
systems for text generation evaluation (Khashabi
et al., 2021), and greater focus on building bench-
marks that incorporate human preferences (Liang
et al., 2022; Fabbri et al., 2021). And indeed,
Goyal et al. (2022) ﬁnd that summaries produced
byGPT3-D3 are often preferred by humans over
alternative model outputs even when automated
metrics disagree. Such ﬁndings have motivated
the manual analysis we conduct for this work. As
far as we know, there has not been any work that
assess the degree to which GPT-3 is proﬁcient at
summarizing biomedical and clinical data in both
single-document and multi-document cases.
Our analysis of summarization in the biomed-
ical space complements recent work analyzing
the question answering capabilities of such mod-
els in this domain (Singhal et al., 2022; Liévin
et al., 2022) and the degree to which they encode
medical knowledge implicitly (Sung et al., 2021).
Other work has considered using summarization1390of biomedical texts as assistive tools for reading
(August et al., 2022).
6 Conclusions
We evaluate the ability of GPT3-D3 to faithfully
summarize and simplify medical literature. The ex-
pert annotations we collect indicate that GPT3-D3
performs single-document tasks quite well, but
struggles with multi-document summarization.
This highlights the ability to aggregate across doc-
uments as a direction for future work. We release
all data and annotations to facilitate such work in
the medical space going forward.
Limitations
This evaluation focussed on expert manual assess-
ments of model outputs and their factual accuracy.
Domain expertise (in medicine) was invaluable for
this task, but is also expensive and therefore lim-
ited the scale of our evaluation. Consequently, all
ﬁndings are derived over a modest sample (100s)
of triple-annotated instances.
Another limitation here is that we have consid-
ered only articles describing randomized control
trials (RCTs) . We focused on such articles because
RCTs are the most reliable means of assessing med-
ical interventions, and therefore inform the practice
of evidence-based medicine; summarizing such ar-
ticles is therefore critical to help physicians stay
on top of the evidence. Moreover, RCTs provide a
natural grounding with respect to factuality, given
that all such trials will investigate the relative efﬁ-
cacy of an intervention for a particular condition
(i.e., on a speciﬁc population of patients) and with
respect to an outcome of interest. That said, this is
restrictive by design, and our analysis has therefore
excluded large swaths of other types of medical
texts.
Ethical Considerations
In Appendix D, we note the costs of hiring domain
experts for annotation.
Large language models (such as GPT3-D3 ) have
been shown capable of generating concise and ﬂu-
ent summaries. But these often contain factual in-
accuracies. This poses unique risks in the domain
of medicine, where inaccurate summaries of pub-
lished evidence have the potential to (mis-)inform
patient care. This work has attempted to empiri-
cally assess the tendency of models to introduce
inaccuracies into summaries of medical literatureby enlisting domain experts to identify and char-
acterize omissions and errors in model generated
summaries. Understanding such issues is a ﬁrst
step toward designing methods to mitigate them.
While we found that GPT3-D3 appears to pro-
duce summaries of single biomedical article ab-
stracts that are reasonably factual, relying on such
outputs still poses risks, and even in this setting we
would caution against trusting model outputs with-
out further veriﬁcation at present. Moreover, we
found that in the multi-document case—i.e., on the
task of synthesizing evidence reported across mul-
tiple clinical trials— GPT3-D3 struggles to provide
synopses that agree with reference (expert written)
summaries. In sum, despite their ability to produce
consistently plausible outputs, our view is that sum-
maries of medical literature produced by LLMs
should not yet be used to directly inform care given
the risks of factual inaccuracies. More research is
needed to better characterize the kinds of mistakes
such models make, and ultimately to mitigate them.
Acknowledgements
This research was partially supported by National
Science Foundation (NSF) grants IIS-2145479
and RI-2211954, and by the National Institutes
of Health (NIH) under the National Library of
Medicine (NLM) grant 2R01LM012086.
References13911392Appendix
A Model details
We use the following parameters to prompt
GPT3-D3 : temperature = 0.7, top-p = 1.0, frequency
penalty = 0.0, presence penalty = 0.0. We set our
maximum token length to 1000 to avoid artiﬁcially
introducing any omission errors.
B Dataset statistics
We provide some basic information about the
dataset in Table 2. Because we used GPT3-D3 , we
do not have a clear idea about how the tokenization
is done. To be as transparent as possible, how-
ever, we still provide the number of tokens when
tokenized with SpaCy. Since we use GPT3-D3 ,
we opt to use a tokenization scheme that focuses
mainly on general English (so we did not use a spe-
cialized tokenizer for biomedical texts to replicate
as similar a tokenization as possible).
C Prompts
For single-document summarization, we follow
prior work to select our prompts. From (Goyal
et al., 2022; August et al., 2022), we use the fol-
lowing prompts for the technical summary and the
plain language summary:
• Summarize the above.
•My ﬁfth grader asked me what this passage
means: “““ [TEXT TO SIMPLIFY] ””” I
rephrased it for him, in plain language a ﬁfth
grader can understand.
To our knowledge, there is no prior work investi-
gating prompt constructions for multi-document
summarization generally (or evidence synthesis
speciﬁcally). Table 1reproduces prompts we con-
sidered for this, but we ultimately used:
•“““ [GENERATED INPUT SUMMARIES]
””” What does the above evidence conclude
about “““ [TITLE] ”””?
Figure 9 shows an example of the input structure
and prompts we provide to GPT3-D3 in the multi-
document setting. For the few-shot setting, we
evaluate using up to 5 examples in context. Figure
10 shows the input structure for this setting in the
second phase.1393
D Annotation details
We calculate the inter-annotator agreement score
(Cohen’s kappa), which averaged 0.59 amongst all
annotators.
We also transparently reveal the cost of annotat-
ing on Upwork. The total cost of hiring 3 workers
on Upwork was a little more than $3,700USD. Be-
cause annotations on a more specialized platform
cost signiﬁcantly more, we hired fewer annotators
than one would hire on generic crowdworking web-
sites.
Since each Upworker requested different pay-
ment amounts (which is the nature of the platform),
we provide the averages per hour for the work. For
the single-document case, each annotation took on
average 15-20 minutes per sample, and with 100
samples, the upper-bound was 33.3 hours for the
entire task per annotator. For the multi-document
case, each annotation took on average 10-15 min-
utes per sample, and with 50 samples, the upper-
bound was 12.5 hours for the entire task per anno-
tator. Both tasks had three annotators annotating
each.E Survey details
For each data point (and for each question in the
interface), the annotator ﬁrst evaluates the standard
summary and then evaluates the plain language
summary, before completing the survey in its en-
tirety. We reproduce our survey questions and the
corresponding answer options. These include the
evaluation categories that we care about: For stan-
dard (technical) summaries , we focus on factual-
ity, linguistic quality, and holistic evaluation; For
plain language summaries , we include an addi-
tional section on readability because the purpose of
these is to simplify technical language such that a
layperson might understand the summary. We pro-
vide details regarding the structures of the surveys
we used and our rationales behind their construc-
tion below.
E.1 Single-document summarization
In the single-document summarization case, the in-
puts comprise study abstracts ,titles , and we also
show to the user key results , which were automati-
cally extracted (Marshall et al., 2020). (We do not
have reference summaries for these examples.) The
goal of expert evaluation was to quantify the ex-
tent to which GPT3-D3 accurately summarizes these
article inputs. We reiterate that we consider two
different types of summarization strategies: stan-
dard (technical) summarization and plain-language
summarization. We reproduce the questions asked
for these summary types below, which vary only
slightly in their focus.
Factuality Many of our questions chosen in our
taxonomy revolve around factuality since factual
accuracy is extremely in domain-speciﬁc work.
1. The model summary accurately conveys
the key results in the input. Given the model
summary, we seek to evaluate whether the key re-
sults that are automatically extracted are reﬂected
in the output. This is a matter of degree, so we
solicit assessments rated on a Likert scale.
2. Highlight sentences in the model summary
(if any) that directly contradict the input (high-
light model summary on the right). We collect
additional annotations on which portions of the
model summary contradict the input. We did not
further analyze these highlights here, but do release
them as part of the data collected.
3. Highlight any concepts that are new in the
model summary that don’t appear in the input1394
(highlight model summary on the right). Here
the idea is to allow the annotator to mark “hallu-
cinated” content in outputs (not supported by the
input).
4. How are details about the population de-
scribed in the summary, relative to the input
text? The patient population is a critical com-
ponent of clinical trials in medicine, and so it is
important that summaries accurately describe this
element. In particular we ask both whether the pop-
ulation is described (at all), and also the degree to
which it is described accurately .
5. How are details about the intervention de-
scribed in the summary, relative to the input
text? Another key element of trials is the inter-
vention (e.g., medicine or treatment) being evalu-
ated. Therefore, as for study populations, we col-
lect annotations regarding whether this is captured
(and if it is captured accurately).
6. How are details about the outcome (what
was measured) described in the summary, rela-
tive to the input text? The outcome measured
(e.g., mortality) is the ﬁnal foundational compo-
nent of trials. As in the preceding two cases, we
ask annotators to assess whether this is reported
upon faithfully.
7. Are there any omission(s) unrelated to
the population, intervention, or outcome? We
evaluate whether the model omits any information
regarding the key trial elements—population, inter-
vention, and outcome—just described. For more
details about types of omissions, refer to section
F.2.
8. Are there any errors? We also ask whether
there are any errors (in general) in the model sum-
mary.
Linguistic quality
9. The model summary is coherent, ﬂuent,
and without grammatical errors. This is in-tended to capture the readability or ﬂuency of the
generated output, independent of its veracity.
Holistic evaluation Finally, we ask for a holistic
evaluation of the output.
10. The output is a concise, accurate, and po-
tentially useful summary of the input. Contin-
uing with more holistic questions, this is intended
to capture the perceived (potential) utility of gener-
ated summaries, according to the domain experts
we hired as annotators.
In the case of plain summarization, we ask the
annotator to rate whether 10. The simpliﬁed text
is accurate and would be understandable by a
(lay) patient. This effectively conveys the po-
tential utility of automatically produced lay sum-
maries, because the purpose of these outputs would
be make medical evidence more accessible to (in-
expert) patients.
11. If there was anything not elaborated or
covered, feel free to leave a comment in the box.
We conclude with an open-ended text box to collect
notes or thoughts not otherwise captured.
Readability For plain language summaries ,
we include a section on readability, given the focus
on making evidence more digestible in this case.
12. The simpliﬁed model text is less technical
and more approachable, thus making it easier
to understand. This question measures the de-
gree to which the annotator judges the model to
have successfully simpliﬁed the text.
13. Technical terms in the input are being
substituted with simpler language in the simpli-
ﬁed model text. This is a more focussed ques-
tion regarding simpliﬁcation to quantify whether
the model consistently swaps jargon terms for more
accessible language.
E.2 Multi-document summarization
The inputs in the multi-document case comprises
collections of articles describing trials, and the tar-
gets are syntheses of these (which put together1395the ﬁndings they report). We sampled these meta-
reviews from previously conducted evidence syn-
theses, and so in this case we have target sum-
maries, which we provide to the annotator. We
not consider simpliﬁcation in the multi-document
setting.
Factuality We again focus on factuality of model
outputs.
1. Highlight any spans in the generated sum-
mary that disagree with the target summary.
We ask for annotators to mark any explicit con-
tradictions featured in the generated output.
2. The generated summary is supported by
putting together the given summaries of the in-
dividual articles. The core of multi-document
summarization is the piecing together of multiple
documents into a coherent summary that accurately
reﬂects the inputs in aggregate. This question is
intended to measure the degree to which the model
does so.
3. The generated summary agrees with the
target summary. Because we have reference
(target) summaries in this case, we directly ask
whether and to what degree the model generated
synopsis seems to agree with this.
4. Rate the degree to which the generated
summary shows the extent that there is evi-
dence supporting the effectiveness of the inter-
vention(s) of interest (as indicated in the stud-
ies). The generated summary suggests... Here
we aim to assess whether the model output implies
that the intervention studied in the constituent trials
is supported by the ﬁndings reported within them.
5. Rate the degree to which the target sum-
mary shows the extent that there is evidence
supporting the effectiveness of the interven-
tion(s) of interest (as indicated in the studies).
The target summary suggests... Similarly, we
ask whether the reference summary implies that
the intervention in question is effective.
Holistic evaluation As above we seek to elicit
an overall impression of summary accuracy and
quality.
6. If there was anything not elaborated or
covered, feel free to leave a comment in the box.
Much like for single-document summarization, the
survey provides an additional box for annotators togive information about the speciﬁc data point that
was asked.
F Additional evaluation
F.1 Few-shot
Few-shot We experimented brieﬂy with few-shot
prompting (Appendix G), but qualitatively this did
not seem to outperform zero-shot summarization,
hence our focus on evaluating the latter.
For few-shot generation, we insert in-context
training examples after the ﬁrst summarization
phase by concatenating the summaries and the tar-
get conclusions of inputs (see Appendix C). We
evaluate using up to 5 shots.
F.2 Underspeciﬁed elements
Table 3 and Table 4 show the additional options
selected when an element (e.g., population) was
marked as “underspeciﬁed” in the survey for the
technical and simpliﬁed cases, respectively.
There can be many reasons why an element
could be marked underspeciﬁed. Because we try to
remove as much ambiguity as possible, we opt to
identify the reasons under each category ( Popula-
tion,Intervention ,Outcome ) the speciﬁc reasoning.
The questions we ask in both the regular and plain
summarization case are both different because of
the audience we address in either case. In the regu-
lar summarization case, the reader is intended to be
a domain expert; in the plain summarization case,
the reader is intended to be laymen, and so we alter
the types of questions we ask as a result.
We ﬁnd that plain summaries (Table 4) have
fewer errors than that of regular summaries (Ta-
ble 3), whereas regular summaries have a higher
number of speciﬁc omissions. However, plain sum-
maries seem to have more omissions in areas out-
side of the scope of what we identify as salient
omissions. We can hypothesize that given more
complex language, it could be that annotators can
more easily identify salient information in the text.
On the other hand, there are nuances in regular
summaries that cannot be extrapolated via plain
summarization prompts, and instead we must use
regular summaries to gather more critical informa-
tion (in addition to the fact that the questions asked
in the plain summarization case tends to be sim-
pler). Although, with regular summaries, summa-
rizing on a deeper level may result in using more
convoluted language. Nonetheless, each type of
prompt (regular and plain) seem to be well-suited
for the task at hand; what matters is the context in1396
which the prompt is used, and what information is
needed for the user.
F.3 Flan-T5
We compared GPT-3 zero-shot results to Flan-T5
(Wei et al., 2021). We ﬁnd that Flan-T5 produces
substantially shorter summaries (2-3 sentences on
average). We provide examples of generated sum-
maries in Figure 11. Qualitatively, these seemed
far worse than GPT-3 generated outputs, so we did
not evaluate these further in this work.
F.4 ROUGE scores
We provide the standard automatic metric of
ROUGE (Lin, 2004) to analyze multi-document sum-
marization. We do not have ROUGE scores for single-
document summarization since we lack groundtruth data. However, the focus of this work is on
the capability of GPT3-D3 to faithfully summarize
biomedical literature (i.e., to generate accurate sum-
maries); human experts remain the best judges of
factuality. Noting this and prior work by Goyal et al.
(2022) make ROUGE scores (and other automatic
metrics) rather unreliable to judge the capabilities
of these large language models on summarization.
F.5 Similarity
We provide additional examples of sentences and
summaries with high similarity to the input ab-
stract.
G Examples of generated summaries
We include examples of generated summaries we
annotated, both standard summaries and plain lan-
guage in the single and multi-document case (Table
14, 13).
We also provide examples of few-shot genera-
tions along with the zero-shot and target summaries
for comparison (Figure 15). Note that the few-shot
examples reﬂect the same evidence strength and
recommendation as the zero-shot examples, thus13971398we do not evaluate them at this point.
H Additional ﬁgures1399140014011402140314041405ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7 (after conclusion)
/squareA2. Did you discuss any potential risks of your work?
RQ4, section 7
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 2, 3
/squareB1. Did you cite the creators of artifacts you used?
Section 2, 3
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 2, 3
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
No response.1406/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
No response.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
No response.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
No response.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix, and will be released with the data
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Appendix and Section 2, 3
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Section 2, 3
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No - annotation work like this does not require IRB, and i have discussed this with our folks here
before
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
We only hired annotators based on their expertise, demographic/geographic characteristics were not
part of this.1407