
Ye Wang, Xinxin Liu, Wenxin Hu, Tao ZhangEast China Normal University, Shanghai, ChinaTsinghua University, Beijing, China
{yewang, xxliu}@stu.ecnu.edu.cn, wxhu@cc.ecnu.edu.cn
tao-zhan20@mails.tsinghua.edu.cn
Abstract
Document-level relation extraction (RE) aims
to identify relations between entities across
multiple sentences. Most previous methods
focused on document-level RE under full super-
vision. However, in real-world scenario, it is ex-
pensive and difficult to completely label all re-
lations in a document because the number of en-
tity pairs in document-level RE grows quadrat-
ically with the number of entities. To solve
the common incomplete labeling problem, we
propose a unified positive-unlabeled learning
framework −shift and squared ranking loss
positive- unlabeled (SSR-PU) learning. We use
positive-unlabeled (PU) learning on document-
level RE for the first time. Considering that
labeled data of a dataset may lead to prior shift
of unlabeled data, we introduce a PU learning
under prior shift of training data. Also, us-
ing none-class score as an adaptive threshold,
we propose squared ranking loss and prove its
Bayesian consistency with multi-label ranking
metrics. Extensive experiments demonstrate
that our method achieves an improvement of
about 14 F1 points relative to the previous base-
line with incomplete labeling. In addition, it
outperforms previous state-of-the-art results un-
der both fully supervised and extremely unla-
beled settings as well.
1 Introduction
Relation extraction (RE) aims to identify the re-
lations between two entities in a given text. It
has rich applications in knowledge graph construc-
tion, question answering, and biomedical text un-
derstanding. Most of the previous work was to
extract relations between entities in a single sen-
tence (Miwa and Bansal, 2016; Zhang et al., 2018).
Recently, document-level RE aiming to identify the
relations among various entity pairs expressed in
multiple sentences has received increasing researchFigure 1: A case from DocRED. Entities are highlighted
in different colors depending on their type. Black arrows
indicate relations annotated with the original dataset,
orange arrows indicate relations that are re-annotated by
(Tan et al., 2022b).
attention (Yao et al., 2019; Zhou et al., 2021; Xu
et al., 2022).
Previous document-level RE methods mainly
deal with fully supervised scenarios. However,
in real-world scenarios, incomplete labeling is a
common problem in document-level RE because
the number of entity pairs grows quadratically with
the number of entities. DocRED (Yao et al., 2019)
is a popular dataset for document-level RE. Re-
cent studies (Huang et al., 2022; Tan et al., 2022b)
found that DocRED, which annotates data with a4123recommend-revise scheme, contains a large num-
ber of false negative samples, i.e., many positive
samples being unlabeled. As shown in Figure 1,
document Alecu Russo contains a large number
of unlabeled positive relations. Consequently, the
models trained on this dataset tend to overfit in
real scenarios and get lower recall. As a result,
document-level RE with incomplete labeling has
become an emergency need.
To solve this problem, we propose a unified
positive-unlabeled learning framework −shift and
squared ranking loss positive- unlabeled (SSR-PU)
learning, which can be adapted to labeling under
different levels. We use positive-unlabeled (PU)
learning for the first time on the document-level
RE task. Since document-level RE is a multi-label
classification task, we apply a binary PU learning
method for each class (one-vs-all), converting it
to multi-label PU learning. In addition, accord-
ing to our observations, a considerable portion of
the relations in DocRED, a dataset annotated by
recommend-revise scheme, have already been anno-
tated. This leads to the deviation between the prior
distribution of the unlabeled data and the overall
prior distribution. To address this problem, we in-
troduce an adaptive PU learning under prior shift
of training data that adjusts the model based on the
estimated overall prior distribution and the labeled
positive sample distribution to be similar to ordi-
nary PN learning or ordinary PU learning. Here
positive-negative (PN) learning means treating all
unlabeled samples as negative samples.
Also, to distinguish between none-class and pre-
defined classes, we propose a squared ranking
loss for none-class ranking such that positive pre-
defined labels are ranked higher than none-class
label and negative pre-defined labels are ranked
lower. This is an ideal multi-label surrogate loss
metric, and we theoretically prove its Bayesian con-
sistency with the multi-label ranking metric pro-
posed by (Zhou and Lee, 2022). This loss function
can be well adapted to PU learning.
We conduct extensive experiments on two multi-
label document-level RE datasets with incomplete
labeling, DocRED (Yao et al., 2019) and ChemDis-
Gene (Zhang et al., 2022), a newly proposed multi-
labeled biomedical document-level RE dataset. Ex-
perimental results show that our method SSR-PU
outperforms previous baseline that did not consider
the labeling incompleteness phenomenon by about
14 F1 points. In addition, we perform fully super-vised experiments, as well as experiments on an
extremely unlabeled data that is newly constructed,
in which the number of each relation type labeled
in each document is limited to 1. Experiments un-
der two complementary settings demonstrate the
effectiveness of our method with different levels
of labeling. The contributions of this paper are
summarized as follows:
•We propose a unified positive-unlabeled learn-
ing framework, SSR-PU, to adapt document-
level RE with different levels of incomplete
labeling.
•We apply PU learning for the first time to
the document-level RE task and introduce a
PU learning under prior shift of training data
that can reach a balance between ordinary PN
learning and ordinary PU learning based on
the estimated prior and labeling distribution.
•We propose squared ranking loss, which effec-
tively improves performance relative to other
loss functions, and prove its Bayesian consis-
tency with multi-label ranking metrics.
•Our method achieves state-of-the-art results
in a variety of settings and provides a robust
baseline for document-level RE with incom-
plete labels.
2 Related Work
Document-level relation extraction. Previous
generally effective methods for document-level RE
are mainly graph-based models and transformer-
based models. Graph-based models (Nan et al.,
2020; Li et al., 2020; Zeng et al., 2020, 2021; Xu
et al., 2021b) gather entity information for rela-
tional inference with graph neural networks, and
transformer-based methods (Zhou et al., 2021; Xu
et al., 2021a; Zhang et al., 2021; Tan et al., 2022a)
implicitly capture long-range dependencies. Re-
cently, (Huang et al., 2022; Tan et al., 2022b) found
that a large number of positive relations remain
unlabeled in document-level RE datasets, espe-
cially unpopular relations. However, the previous
methods did not consider unlabeled data separately.
They simply treated them all as negative samples,
which led to a lower recall and a significant drop
in performance in realistic scenarios.
PU learning. Positive-unlabeled (PU) learn-
ing (Elkan and Noto, 2008; du Plessis et al., 2014;
Plessis et al., 2015; Kiryo et al., 2017; Garg et al.,41242021) aims to learn a classifier from positive and
unlabeled data. PU learning is a kind of semi-
supervised learning but there is a fundamental
difference between them: while semi-supervised
learning requires labeled negative data, PU learning
requires only labeled positive data. Many current
PU learning methods rely on an overall prior esti-
mate, while some recent studies (Charoenphakdee
and Sugiyama, 2019; Nakajima and Sugiyama,
2021) have noticed a prior shift between the train-
ing set and the test set. On the other hand, PU
learning has been used in many NLP applications,
e.g., text classification (Li and Liu, 2003), sen-
tence embedding (Cao et al., 2021), named entity
recognition (Peng et al., 2019; Zhou et al., 2022),
knowledge graph completion (Tang et al., 2022)
and sentence-level RE (He et al., 2020). However,
this method is rarely applied to the document-level
RE task.
Multi-label classification. Multi-label classifi-
cation is a widely investigated problem, and here
we focus on the loss function. Binary cross en-
tropy (BCE) is the most popular multi-label loss,
reducing the multi-label problem to a number of
independent binary (one-vs-all) classification tasks.
Recently, (Hui and Belkin, 2020) have found that
squared loss can also achieve better results in clas-
sification tasks. Another common multi-label loss
function is pairwise ranking loss, which transforms
multi-label learning into a ranking problem via pair-
wise (one-vs-one) comparison (Fürnkranz et al.,
2008; Li et al., 2017). For multi-label PU learn-
ing, (Kanehira and Harada, 2016) treated it as a
multi-label PU ranking problem, and (Aota et al.,
2021) applied PU learning to multi-label common
vulnerabilities and exposure classification by using
one-vs-all strategy. For document-level RE task,
(Zhou and Lee, 2022) proposed a none-class rank-
ing multi-label metric. This multi-label metric has
not yet been applied to PU learning.
3 Methodology
In this section, we introduce the details of our
method shift and squared ranking loss positive-
unlabeled (SSR-PU) learning for document-level
RE with incomplete labeling. Firstly, we intro-
duce the definition of positive-unlabeled learning
for document-level RE. Next, we present the PU
learning under prior shift of training data. Finally,
squared ranking loss using the none-class score as
an adaptive threshold is proposed.3.1 Positive-unlabeled learning for
document-level RE
Document-level RE can be viewed as a multi-label
classification task, where each entity pair is an in-
stance and the associated relations are label sam-
ples. Previous supervised learning methods only
treated unlabeled relations as negative samples,
which may lead to low recall in the presence of
a large number of false negatives. To address this
problem, we adopt PU learning (du Plessis et al.,
2014; Plessis et al., 2015) for each class.
LetXbe an instance space and Y={−1,+1}
be a label space, where Kis the number of pre-
defined classes. An instance x∈ X is associated
with a subset of labels, identified by a binary vec-
tory∈ Y = (y, . . . , y), where y= +1 if
thei-th label is positive for x, and y=−1oth-
erwise. A score function is defined as f(x) =
(f(x), f(x), ..., f(x)). In the following we use
finstead, to omit the dependency on x.
Fori-th class, assume that the data follow
an unknown probability distribution with density
p(x, y),p=p(x|y= +1) as the positive
marginal, p=p(x|y=−1)as the negative
marginal, and p(x)as the marginal. In positive-
negative (PN) learning, the goal is to minimize the
expected classification risk:
R(f) =/summationdisplayE[ℓ(f, y)],(1)
Here, Eq.1 can be calculated by equivalently us-
ing the sum of the errors of positive and negative
samples:
R(f) =/summationdisplay(πE[ℓ(f,+1)]
+ (1−π)E[ℓ(f,−1)]),(2)
where π=p(y= +1) and(1−π) = (1−p(y=
+1)) = p(y=−1)is the positive and negative
prior of the i-th class. E[·] =E[·],
E[·] =E[·]and the loss function is
represented by ℓ. Rewriting Eq.2 into a form that
uses the data for approximation, we get:
/hatwideR(f) =/summationdisplay(π
n/summationdisplayℓ(f(x),+1)
+(1−π)
n/summationdisplayℓ(f(x),−1)),(3)4125where xandxdenote cases that the j-th sam-
ple of class iis positive or negative. nandn
are the number of positive and negative samples of
class i, respectively.
In positive-unlabeled (PU) learning, due to the
absence of negative samples, we cannot estimate
E[·]from the data. Following (du Plessis et al.,
2014), PU learning assumes that unlabeled data
can reflect the true overall distribution, that is,
p(x) =p(x). The expected classification risk
formulation can be defined as:
R(f) =/summationdisplay(πE[ℓ(f,+1)]+
E[ℓ(f,−1)]−πE[ℓ(f,−1)]),(4)
HereE[·] = E[·]andE[ℓ(f,−1)]
−πE[ℓ(f,−1)]can alternatively represent (1−
π)E[ℓ(f,−1)]because p(x) = πp(x) +
(1−π)p(x).
By rewriting Eq.4 in a form that can be approxi-
mated using the data, we get the following:
/hatwideR(f) =/summationdisplay(π
n/summationdisplayℓ(f(x),+1)
+ [1
n/summationdisplayℓ(f(x),−1)
−π
n/summationdisplayℓ(f(x),−1)]),(5)
where xdenote cases that the j-th sample is un-
labeled as class iandnis the number of samples
unlabeled as class i.
However, the second term in Eq.5 can be nega-
tive and can be prone to overfitting when using a
highly flexible model. Thus, a non-negative risk es-
timator (Kiryo et al., 2017) is proposed to alleviate
the overfitting problem:
/hatwideR(f) =/summationdisplay(π
n/summationdisplayℓ(f(x),+1)+
max(0 ,[1
n/summationdisplayℓ(f(x),−1)
−π
n/summationdisplayℓ(f(x),−1)])).(6)
Forℓ, we use the convex function squared loss:
ℓ(f, y) =1
4(yf−1), (7)
and we compare the performance of using squared
loss and log-sigmoid loss functions in Section 4.4.
The latter is a convex loss function commonly used
in classification.
In addition, to solve the heavy class imbalance
problem, we multiply γ= ()before posi-
tive risk estimations as the class weight.
3.2 Class prior shift of training data
Ordinary PU learning requires an assumption that
the overall distribution needs to be the same as
the distribution of the unlabeled data. In contrast,
with the document-level RE dataset constructed
by a recommend-revise scheme, many relations are
probably already annotated, especially the common
ones. This leads to a prior shift in the unlabeled
data of the training set. When this assumption is
broken, ordinary PU learning will yield a biased
result. To address this problem, inspired by the
method (Charoenphakdee and Sugiyama, 2019) for
handling a prior shift between the test set and the
training set, we introduce the PU learning under
prior shift of training data.
For each class, assume that the original prior
π=p(y= +1) . We set π =p(s= +1)
and(1−π ) = (1 −p(s= +1)) = p(s=
−1)where s= +1 ors=−1mean that the
i-th class is labeled or unlabeled, respectively. As
shown in Figure 2, the conditional probability of
a positive sample under unlabeled data is different
from the probability of an overall positive sample.
The conditional probability of a positive sample
under unlabeled data is:
p(y= 1|s=−1) =p(y= 1, s=−1)
p(s=−1),(8)
where p(y= 1, s=−1) = π−π , we
can obtain the prior of positive samples in the new
unlabeled data after labeling as π=p(y= 1|
s=−1) =.4126For document-level RE, the goal is to minimize
the following misclassification risk for the original
distribution of the training data:
R(f) =/summationdisplay(πE[ℓ(f,+1)]
+ (1−π)E[ℓ(f,−1)]).(9)
We can express R(f)using the expectation
of positive and unlabeled data by the following
theorem.
Theorem 1. The misclassification risk R(f)can
be equivalently expressed as
R(f) =/summationdisplay(πE[ℓ(f,+1)]
+1−π
1−πE[ℓ(f,−1)]
−π−ππ
1−πE[ℓ(f,−1)]).(10)
Proof. Proof appears in Appendix A.1.
As a result, we can obtain the non-negative risk
estimator (Kiryo et al., 2017) under class prior shift
of training data as follows:
/hatwideR(f) =/summationdisplay(1
nπ/summationdisplayℓ(f(x),+1)
+ max(0 ,[1
n1−π
1−π/summationdisplayℓ(f(x),−1)
−1
nπ−ππ
1−π/summationdisplayℓ(f(x),−1)])).
(11)
We can observe that PN learning and PU learning
are special cases of this function. When π= 0,
this equation reduces to the form of ordinary PN
learning, and when π=π, this equation reduces
to the form of ordinary PU learning.
3.3 Squared ranking loss
To better measure the performance of document-
level RE, (Zhou and Lee, 2022) proposed a new
multi-label performance measure:
L(f,y) =/summationdisplay(/llbrackety>0/rrbracket/llbracketf< f/rrbracket
+/llbrackety≤0/rrbracket/llbracketf> f/rrbracket+1
2/llbracketf=f/rrbracket),(12)DatasetDocRED ChemDisGene
train test train test
# docs 3,053 500 76,942 523
# rels 96 14
Avg # ents 19.5 19 .6 7 .5 10 .0
Avg # rels 12.5 34 .9 2 .1 7 .2
where positive pre-defined labels should be ranked
higher than the none-class label and negative ones
should be ranked below. /llbracket·/rrbracketis an indicator function
that takes the value of 1 when the conditions in the
parentheses are met, otherwise 0.
However, it is difficult to optimize the above
equation directly. Thus, we propose the squared
ranking surrogate loss by rewriting Eq.7 as:
ℓ(f, y) =1
4(y(f−f)−margin ),(13)
where margin is a hyper-parameter and fis the
none-class score, when fis greater than fthe
label exists, and otherwise not.
Next we prove the Bayesian consistency of ℓ
with the multi-label ranking metric Lwhen
margin ̸= 0. Given an instance x, let ∆=
P(y= 1|x)be the marginal probability when
thei-th label is positive, the Bayes optimal score
function fthat minimizes the multi-label risk
E[L(P,f)|x]is given by:
f∈ {f:f> fif∆>1
2,
and f< fif∆<1
2}.(14)
The next theory guarantees that the classifier
obtained by minimizing the surrogate loss ℓcon-
verges to the classifier with the lowest multi-label
risk, thus making it possible to achieve a better
classification performance w.r.t. corresponding to
the multi-label performance metric.
Theorem 2. ℓ(Eq.13) is Bayes consistent w.r.t.
L(Eq.12) when margin ̸= 0.
Proof. Proof appears in Appendix A.2.
As a supplement, we likewise compare the log-
sigmoid ranking loss performance in Section 4.4.
4 Experiments
In this section, we evaluate our method on two
multi-label document-level RE datasets with in-4127Model Ign F1 F1 P R
BiLSTM32.57±0.22 32 .86±0.22 77 .04±1.01 20 .89±0.17
GAIN+BERT 45.57±1.36 45 .82±1.38 88 .11±1.07 30 .98±1.36
DocuNET+RoBERTa 45.88±0.33 45 .99±0.33 94 .16±0.32 30 .42±0.29
ATLOP+BERT 43.12±0.24 43 .25±0.25 92.49±0.33 28.23±0.23
PN+ATLOP+BERT 51.11±0.49 51 .68±0.40 77 .55±3.10 38 .79±0.49
SR-PN+ATLOP+BERT 52.70±0.28 53 .10±0.26 83 .76±0.49 38 .87±0.23
PU+ATLOP+BERT 51.80±1.11 53 .14±1.01 58 .81±2.41 48 .15±0.14
SR-PU+ATLOP+BERT 53.87±0.27 55 .06±0.25 63 .42±0.64 48.66±0.11
S-PU+ATLOP+BERT 53.36±1.22 54 .44±1.12 65 .95±2.84 46 .38±0.22
SSR-PU+ATLOP+BERT 55.21±0.12 56 .14±0.12 70.42±0.18 46 .67±0.14
ATLOP+RoBERTa 45.09±0.26 45 .19±0.27 94.75±0.25 29.67±0.24
PN+ATLOP+RoBERTa 54.21±0.34 54 .47±0.35 89 .22±0.36 39 .20±0.41
SR-PN+ATLOP+RoBERTa 56.06±0.21 56 .39±0.23 87 .47±0.60 41 .61±0.39
PU+ATLOP+RoBERTa 56.97±0.47 58 .04±0.43 67 .39±1.22 50 .98±0.39
SR-PU+ATLOP+RoBERTa 57.64±0.25 58 .77±0.26 66 .39±0.47 52.72±0.44
S-PU+ATLOP+RoBERTa 58.19±0.24 58 .95±0.25 75 .68±0.36 48 .29±0.40
SSR-PU+ATLOP+RoBERTa 58.68±0.43 59 .50±0.45 74.21±0.53 49 .67±0.77
complete labeling. We also demonstrate the ef-
fectiveness of our method with different levels of
labeling.
4.1 Experimental Setups
Datasets. DocRED (Yao et al., 2019) is a large-
scale document-level RE dataset with 96 pre-
defined relations constructed by a recommend-
revise scheme from Wikipedia. (Tan et al., 2022b)
observed a large number of false negatives in the
annotation of DocRED and provided a high-quality
revised version, Re-DocRED. In our experiments,
we use the incompletely labeled DocRED original
training set for training and the revised test set for
testing. ChemDisGene (Zhang et al., 2022) is a
newly proposed biomedical multi-label document-
level RE dataset. This corpus is automatically de-
rived from CTD database (Davis et al., 2021) by
distantly supervised method and has 523 abstracts
labeled by domain experts as an additional All rela-
tionships test set. We use the distantly supervised
training set for training and the All relationships
test set for testing. The average number of re-
lations per document in the test set on both two
datasets is much larger than the average number of
relations in the training set, which indicates the in-
complete labeling phenomenon in the training set,
with a large number of false negatives present. The
statistics of the two datasets are listed in Table 1.
Implementation details. For each dataset, weuse ATLOP (Zhou et al., 2021) as the encoding
model for the representation learning of relations.
Further, we apply cased BERT (Devlin et al.,
2019) and RoBERTa (Liu et al., 2019) for
DocRED and PubmedBert (Gu et al., 2021) for
ChemDisGene. We use Huggingface’s Transform-
ers (Wolf et al., 2020) to implement all the models
and AdamW (Loshchilov and Hutter, 2019) as the
optimizer, and apply a linear warmup (Goyal et al.,
2017) at the first 6% steps followed by a linear de-
cay to 0. For DocRED, we set the learning rates for
BERT andRoBERTa settings to 5e-5
and 3e-5, respectively, in the same way as ATLOP.
For ChemDisGene, the learning rate is set to 2e-5.
The batch size (number of documents per batch) is
set to 4 and 8 for two datasets, respectively. Dur-
ing our experiment, we set π= 3π and
margin = 0.25. To evaluate the efficacy of our
methods in realistic settings, we do not use any
fully labeled validation or test sets in any stage of
the training process. The training stopping criteria
are set as follows: 30 epochs for both two dataset.
We report the performance of the final model in-
stead of the best checkpoint. All experiments are
conducted with 1 Tesla A100-40G GPU.
Baseline. We re-implemented the existing fully
supervised methods BiLSTM (Yao et al., 2019),
GAIN (Zeng et al., 2020), DocuNET (Zhang et al.,
2021) and ATLOP (Zhou et al., 2021) as the base-
line models for DocRED in this new setup, where4128Model F1 P R
BRAN32.5 41 .8 26 .6
PubmedBert42.1 64 .3 31 .3
BRAN+PubmedBert43.8 70 .9 31 .6
ATLOP+PubmedBert42.73±0.36 76.17±0.54 29.70±0.36
PN+ATLOP+PubmedBert 44.25±0.24 73 .46±0.95 31 .67±0.16
SR-PN+ATLOP+PubmedBert 46.56±0.35 69 .84±0.54 34 .93±0.40
PU+ATLOP+PubmedBert 44.60±0.70 46 .56±1.17 42 .80±0.35
SR-PU+ATLOP+PubmedBert 45.86±0.38 46 .91±0.79 44.86±0.37
S-PU+ATLOP+PubmedBert 46.73±0.49 53 .95±1.14 41 .23±0.36
SSR-PU+ATLOP+PubmedBert 48.56±0.23 54.27±0.40 43 .93±0.32
for GAIN and BiLSTM we use a fixed threshold
of 0.5 and all methods take the final result of the
model instead of the best checkpoint. For ChemDis-
Gene, we used BRAN (Verga et al., 2018), Pubmed-
Bert (Gu et al., 2021) and PubmedBert + BRAN
mentioned in (Zhang et al., 2022) as the baseline
models, and ATLOP is re-implemented as a supple-
mentary baseline.
Evaluation metric. For DocRED, we use the
micro F1 (F1), micro ignore F1 (Ign F1), preci-
sion (P) and recall (R) as the evaluation metrics to
evaluate the overall performance of a model. Ign
F1 measures the F1 score excluding the relations
shared by the training and test set. For ChemDis-
Gene, we use micro F1 (F1), precision (P) and
recall (R) as the evaluation metrics.
4.2 Main Results
In this subsection, we present the results of com-
parison of PN learning (PN), squared ranking loss
PN learning (SR-PN), PU learning (PU), squared
ranking loss PU learning (SR-PU), PU learning un-
der prior shift of training data (S-PU) and SSR-PU.
All methods use the same encoder and different
loss functions. For each method, we use the same
hyper-parameter settings and report the mean and
standard deviation on the test set by conducting 5
runs with different random seeds (62, 63, 64, 65,
66).
Results on DocRED. As shown in Table 2, our
SSR-PU method achieves a state-of-the-art F1 and
Ign F1 in both BERT andRoBERTa
settings and outperforms the original ATLOP by
13.58 and 14.52 F1 points, respectively. Mean-
while, consistent with the observation in the paper
(Huang et al., 2022), existing document-level REmethods under full supervision have a significant
performance degradation in the incompletely la-
beled scenario.
The original ATLOP method has the highest pre-
cision (P) but low recall (R), which implies that
supervised learning methods that simply treat un-
labeled data as negative samples lack the general-
ization ability to extract instances of relations that
are systematically missed in the dataset. PN learn-
ing uses an estimated prior, but will yield a biased
result because there are still positive samples in
the unlabeled data. While PU learning uses both
unlabeled and labeled data to better estimate the
expectation of negative samples, which results in a
higher recall rate. In addition, ordinary PU meth-
ods without prior shift overestimate the content of
positive samples in unlabeled data, which means
that the model will tend to identify more samples
as positive, i.e., higher recall, but also leads to more
false-positive prediction results, i.e., lower preci-
sion. In contrast, the S-PU method with prior shift
effectively mitigates this phenomenon by bring-
ing the positive samples estimated by the model in
the unlabeled data closer to their true distribution.
For example, in experiments under the BERT
setting, there is a small decrease in recall of less
than 2 percentage points, while the precision im-
proves by about 7 percentage points, leading to
an improvement in the final results. And this phe-
nomenon is more evident in common relations as
analyzed in Section 4.4. Finally, applying squared
ranking loss in PN learning, PU learning and S-
PU learning can further improve the performance
of the model, demonstrating the effectiveness of
the method with none-class score as an adaptive
threshold for document-level RE.4129Model Ign F1 F1
ATLOP+BERT 72.70 73 .47
SSR-PU+BERT 72.91 74 .33
ATLOP+RoBERTa 76.92 77 .58
DocuNET+RoBERTa77.27 77 .92
KD-DocRE+RoBERTa77.63 78 .35
SSR-PU+RoBERTa 77.67 78 .86
Model Ign F1 F1
ATLOP+BERT 16.99 17 .01
SSR-PU+BERT 46.47 47 .24
ATLOP+RoBERTa 17.29 17 .31
SSR-PU+RoBERTa 48.98 49 .74
Results on ChemDisGene. As shown in Table
3, the improvement of our method agrees with the
results on DocRED, reaching the state-of-the-art
F1, which is 5.83 F1 points higher than the original
ATLOP. Notice that the improvement on ChemDis-
Gene is not as dramatic as that on DocRED. We
argue that this may be due to the fact that some
of the documents in the extra annotated All rela-
tionships test set are from another corpus DrugProt
(Miranda et al., 2021), and that the annotation by
human experts has a large deviation from the origi-
nal training set distribution. This suggests that it is
a challenging direction to make the document-level
RE model more generalizable when it is difficult to
estimate the true distribution of the test set.
4.3 Different Levels of Labeling
Fully supervised setting. In this setting, we set
π=π and other hyper-parameters iden-
tically. As shown in Table 4, we use the (Tan
et al., 2022b) revised Re-DocRED dataset in the
same fully supervised setting to compare with the
current state-of-the-art baseline models ATLOP
(Zhou et al., 2021), DocuNET (Zhang et al., 2021)
and KD-DocRE (Tan et al., 2022a). Our method
achieves the same state-of-the-art results, demon-
strating the effectiveness of our method with full
labeling. The result with this setting can be seen
as an upper bound for document-level RE with in-Model Freq. F1 Freq. P Freq. R
SR-PN 60.79 87 .83 46 .49
SR-PU 62.43 60 .28 64 .74
SSR-PU 64.88 68 .36 61 .74
Model Freq. F1 Freq. P Freq. R
SR-PN 47.62 71 .76 35 .64
SR-PU 47.65 44 .35 51 .48
SSR-PU 50.91 52 .09 49 .78
complete labeling. More details of the experiment
are shown in Appendix A.3.
Extremely unlabeled setting. In this setting,
we use the original training set of DocRED to con-
struct an extremely unlabeled training set, i.e., the
number of labels for each relation type in the doc-
ument being limited to 1. The average number of
relations in the processed documents is reduced to
5.4. We consider this a more difficult and challeng-
ing scenario. We set π= 12 π and other
hyper-parameters identically. As shown in Table 5,
traditional supervised learning methods fail, while
our proposed SSR-PU method still yields a robust
result. It is worth noting that since the labeled sam-
ple is only a fraction of the true positive sample, i.e.,
the biased distribution, which means p(x|y= 1)
is not equal to p(x|s= 1) , the first term in
Eq.11 is actually a biased approximation to the first
term in Eq.10. We consider this bias as one of the
bottlenecks of the current method and the main rea-
son why the method degrades a lot in extremely
unlabeled scenarios, i.e., the bias is widened in ex-
tremely unlabeled scenarios. This is a good direc-
tion for future research, where possible solutions
might involve adding some data augmentation or
bootstrapping methods for labeling to alleviate this
bias. More details of the experiment are shown in
Appendix A.4.
4.4 Additional Analysis
Analysis of common relations. As shown in ta-
ble 6 and table 7, we show the results for common
relations on DocRED and ChemDisGene, these
frequent relation types account for about 60% of
the relation triples (Tan et al., 2022b; Zhang et al.,
2022). It can be seen that the SR-PU method has4130Model Ign F1 F1
S-PU 52.23 53 .43
S-PU 54.00 55 .01
S-PU 52.42 53 .66
SSR-PU 55.43 56 .36
a slightly higher recall and much lower precision,
which corresponds to an overestimation of the posi-
tive sample size in the unlabeled data. The SSR-PU
method, on the other hand, can alleviate this prob-
lem well, contributing to a better balance among
precision and recall and better performance. This
indicates a large amount of prior shift in common
relations, which is consistent with (Huang et al.,
2022) observation that common relations are more
likely to be labeled in the dataset.
Comparison with other loss functions. We
compare the squared loss with the log-sigmoid loss,
which is commonly used in multi-label classifica-
tion at the document-level RE. And again, this loss
function is rewritten into a none-class ranking form
for further comparison with squared ranking loss.
The details of the loss function are listed in Ap-
pendix A.5. As shown in Table 8, both the squared
loss function and the squared ranking loss function
are significantly improved compared to the other
loss functions, which demonstrates the effective-
ness of our proposed loss function in the multi-label
document-level RE task.
5 Conclusion and Future Work
In this paper, we propose a unified positive-
unlabeled learning framework, SSR-PU, which
can effectively solve the incomplete labeling
of document-level RE. We use PU learning on
document-level RE for the first time and intro-
duce a PU learning under prior shift of training
data to adapt to different levels of labeling. Also,
we propose squared ranking loss, using none-class
score as an adaptive threshold. Experiments demon-
strate that our method achieves state-of-the-art re-
sults with different levels of labeling and provides
a robust new baseline for incompletely labeled
document-level RE. In the future, we will con-
sider methods that do not require estimation of
priors, allowing generalization to unknown distri-
butions more accurately, as well as addressing the
problem of biased distributions with incompletelabeled positive samples and further improving the
extraction performance of long-tail relations.
Limitations
Regarding the limitations of our proposed method,
our method requires an estimation of an overall
prior that will affect the final result. In a realistic
scenario, a very accurate prior estimation may be
difficult to obtain. In addition, the biased distribu-
tion caused by the incomplete labeling of positive
samples is one of the bottlenecks of the current
method, and there is still much left to be improved
for extremely unlabeled scenarios and scenarios
where the gap between the test set and the training
set distribution is too large, which can be a direc-
tion for further research. However, for now, we
believe that our task is a valuable contribution to
advancing the application of document-level RE
in more realistic scenarios and provides a robust
baseline for this direction.
Acknowledgements
We sincerely thank all anonymous reviewers for
their valuable comments to improve our work. This
research is funded by the Basic Research Project
of Shanghai Science and Technology Commission
(No.19JC1410101). The computation is supported
by ECNU Multifunctional Platform for Innovation
(001).
References413141324133
A Appendix
A.1 Proof of Theorem 1
Proof. Based on the fact that p(x) =πp(x)
+(1−π)p(x),(1−π)E[ℓ(f,−1)]can
be alternatively expressed as E[ℓ(f,−1)]−
πE[ℓ(f,−1)]. We can rewrite R(f)as fol-
lows:
R(f) =/summationdisplay(πE[ℓ(f,+1)]
+ (1−π)E[ℓ(f,−1)])
=/summationdisplay(πE[ℓ(f,+1)]
+1−π
1−π(E[ℓ(f,−1)]
−πE[ℓ(f,−1)]))
=R(f).(15)
We conclude that R(f) =R(f).
A.2 Proof of Theorem 2
Proof. Let∆= P( y= 1|x)be the marginal
probability when the i-th label is positive. Theconditional risk of ℓis:
R(P,f) =/summationdisplay(∆1
4((f−f)−margin )
+ (1−∆)1
4(−(f−f)−margin )).
(16)
Fori= 1, ..., K , the partial derivative can be com-
puted by
∂
fE[ℓ(P,f)|x] =/summationdisplay(∆1
2((f−f)−margin )+
(1−∆)1
2((f−f)−margin )),(17)
since ℓis convex and differentiable, we can ob-
tain the optimal fby setting the partial derivatives
to zero, which leads to
f−f= 2∆margin −margin, i = 1, ..., K.
(18)
When margin ̸= 0, for the optimal score function
f,f> fif and only if ∆>, which mini-
mizes the ℓrisk according to Eq.14. Therefore,
ℓis Bayes consistent w.r.t. L.
A.3 Results under the Fully Supervised
Setting
The detailed results under the fully supervised set-
ting are shown in Table 9. We report the mean and
standard deviation on the validation and test set by
conducting 5 runs with different random seeds (62,
63, 64, 65, 66).
A.4 Results under the Extremely Unlabeled
Setting
The detailed results under the extremely unlabeled
setting are shown in Table 10. We report the mean
and standard deviation on the test set by conducting
5 runs with different random seeds (62, 63, 64, 65,
66).
A.5 Details of Other Loss Functions
We first show the convex loss function log-sigmoid
loss, which is commonly used in classification task:
ℓ(f, y) =−log(σ(yf)), (19)
where σ(x)is the sigmoid function.
Since log-sigmoid loss is convex and differen-
tiable, we can obtain its none-class ranking form.4134Model Dev Test
Ign F1 F1 Ign F1 F1
ATLOP+BERT 73.12±0.35 73 .93±0.38 72 .70±0.23 73 .47±0.25
SSR-PU+BERT 73.27±0.19 74 .69±0.20 72 .91±0.23 74 .33±0.20
ATLOP+RoBERTa 76.98±0.20 77 .68±0.21 76 .92±0.15 77 .58±0.16
DocuNET+RoBERTa77.53 78 .16 77 .27 77 .92
KD-DocRE+RoBERTa77.92 78.65 77 .63 78 .35
SSR-PU+RoBERTa 77.44±0.25 78.66±0.23 77 .67±0.25 78 .86±0.23
Model Ign F1 F1 P R
ATLOP+BERT 16.99±0.24 17 .01±0.24 93.17±0.48 9.36±0.14
SSR-PU+BERT 46.47±0.21 47 .24±0.23 59.52±0.87 39.18±0.61
ATLOP+RoBERTa 17.29±0.28 17 .31±0.28 94.85±0.19 9.52±0.17
SSR-PU+RoBERTa 48.98±0.30 49 .74±0.30 61.57±1.34 41.75±0.42
Model Ign F1 F1
SSR-PU 0.18 0 .20
SSR-PU 55.76 56 .81
SSR-PU 55.43 56 .36
SSR-PU 55.27 56 .19
SSR-PU 54.25 55 .24
Model F1 P R
SSR-PU 55.44 78 .98 42 .71
SSR-PU 56.36 70 .53 46 .93
SSR-PU 54.74 61 .45 49 .35
Log-sigmoid ranking loss:
ℓ(f, y) =−log(σ(y(f−f))).(20)
This ranking loss function remain Bayesian con-
sistent with L(Eq.12).
A.6 Sensitivity to Hyper-Parameter margin
As shown in Table 11, the model fail to train when
margin = 0 , and the model is insensitive to
margin when margin ̸= 0. This is consistent
with our proof.A.7 Influence of Prior Estimation
As shown in Table 12, the experimental results with
different πshow that our method is insensitive to
the estimation of π. Smaller estimates of πlead to
higher precision rates as well as lower recall rates,
while the opposite is true for higher estimates of
π.4135