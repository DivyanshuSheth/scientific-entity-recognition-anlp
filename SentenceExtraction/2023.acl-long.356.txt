
Tzu-Hsuan Chouand Chun-Yi Linand Hung-Yu Kao
Intelligent Knowledge Management Lab
Institute of Computer Science and Information Engineering
National Cheng Kung University
Tainan, Taiwan
ProFatXuanAll@gmail.com ,NE6101050@gs.ncku.edu.tw ,
hykao@mail.ncku.edu.tw
Abstract
Recent research on multi-criteria Chinese word
segmentation (MCCWS) mainly focuses on
building complex private structures, adding
more handcrafted features, or introducing com-
plex optimization processes. In this work, we
show that through a simple yet elegant input-
hint-based MCCWS model, we can achieve
state-of-the-art (SoTA) performances on sev-
eral datasets simultaneously. We further pro-
pose a novel criterion-denoising objective that
hurts slightly on F1 score but achieves SoTA
recall on out-of-vocabulary words. Our result
establishes a simple yet strong baseline for fu-
ture MCCWS research. Source code is avail-
able at https://github.com/IKMLab/
MCCWS .
1 Introduction
Chinese word segmentation (CWS) is a prelimi-
nary step for performing Chinese NLP tasks. Re-
searchers have proposed many CWS datasets to
enhance word segmentation performance in dif-
ferent text domains. However, due to the diver-
gence in linguistic perspectives, the same text pas-
sage can be segmented in entirely different ways
across datasets. For example, in their written forms,
Chinese human names have no spaces in between.
Some datasets segment human names into last and
first names, while others leave human names as
a whole (see Table 1). The simplest way to ad-
dress such an issue is through single-criterion CWS
(SCCWS) model, i.e., to train different models for
different datasets. But the cost of maintaining mul-
tiple versions of the same model becomes cumber-
some as recent deep learning models get deeper
and larger. Thus, recent CWS works started to shift
their focuses to multi-criterion Chinese word seg-
mentation (MCCWS), which aims to fit one model
for all CWS datasets (Chen et al., 2017; He et al.,Dataset Samples Labels
PKU 江-泽民 S-BE
MSRA 江泽民 BME
AS 何-樂-而-不-為 S-S-S-S-S
CITYU 何樂而不為 BMMME
Table 1: Actual samples from SIGHAN bakeoff 2005
datasets (Emerson, 2005) demonstrating labeling incon-
sistency. The hyphen “-” denotes segmentation. Labels
are defined in Section 3.1. In the first two rows, the
human name 江泽民 (Jiang Zemin) in PKU dataset is
segmented into the last name 江(Jiang) and the first
name泽民 (Zemin), but not in MSRA dataset. In the
last two rows, the idiom 何樂而不為(Why not do some-
thing?) is segmented in AS dataset but not in CITYU
dataset. More examples can be found in these datasets.
2019; Gong et al., 2019; Huang et al., 2020b,a; Ke
et al., 2020; Qiu et al., 2020; Ke et al., 2021).
MCCWS can be seen as a multi-task learning
problem (Chen et al., 2017) that benefits from
leveraging large amounts of heterogeneous data,
meanwhile dealing with subtle linguistic diver-
gence. Prior works are mainly divided into private-
structure-based and input-hint-based models. In a
typical SCCWS workflow, an input character se-
quence is first converted to character embeddings
and fed to an encoder to get contextualized repre-
sentation. The encoder output is then passed to a
decoder to generate the final prediction (see Fig-
ure 1(a)). In private-structure-based MCCWS, an
encoder-decoder pair is created for each dataset, but
an additional encoder is shared across datasets to
better leverage general knowledge (see Figure 1(b)).
In input-hint-based MCCWS, instead of creating
private structures for each dataset, all datasets share
one encoder-decoder pair, and a criterion-specific
hint is given as part of the input (see Figure 1(c)).
Despite its simplicity, input-hint-based MCCWS
models outperform private-structure-based MC-
CWS models.
Proven to be simple and effective, the input-6460
hint-based approach has become the most popular
choice of recent MCCWS works (He et al., 2019;
Gong et al., 2019; Huang et al., 2020a; Ke et al.,
2020; Qiu et al., 2020; Ke et al., 2021). While
existing works kept adding complex features and
structures, we show that without such complexity,
we can still achieve state-of-the-art (SoTA) results
across 10 CWS datasets. We do this by jointly
training MCCWS with a criterion classification ob-
jective on a simple model. In particular, we used a
pre-trained Chinese BERT (Devlin et al., 2019) as
our encoder and a softmax decoder. Neither hand-
crafted features nor complex non-greedy decoding
algorithms were used.
One problem remains for input-hint-based MC-
CWS models. When fitting on a training set or
evaluating a test set, each character sequence is
sampled from a particular dataset, so one would al-
ways know which criterion-specific hint was given
as input. However, when performing inference,
one would not know the source of a given char-
acter sequence. Therefore, one has to choose the
criterion in such cases manually. With hundreds
of linguistic rules (Emerson, 2005), it is difficult
for non-linguists to determine which criterion to
use. Thus, inspired by the masked language model,
we proposed a novel criterion-denoising objective
to make our MCCWS model automatically choose
a suitable criterion for each input. We show that
adding such a denoising objective surprisingly re-
tains near SoTA performance on the F1-score, and
even outperforms SoTA performance on the recall
of out-of-vocabulary (OOV) words.2 Related Works
After Xue (2003) proposed to treat CWS as a char-
acter tagging problem, many works followed the
same problem formulation to address CWS. Chen
et al. (2017) is the first to propose a multi-criteria
learning framework for CWS. They proposed mul-
tiple private-structure-based MCCWS models and
trained them in an adversarial setting. A criterion
discriminator was used in their adversarial training
so that common knowledge across datasets could
be shared through different private structures. But
the nature of adversarial training forces their cri-
terion discriminator to predict each criterion with
equal probability (Goodfellow et al., 2014; Chen
et al., 2017). Thus their criterion discriminator
failed to provide accurate criterion prediction and
cannot be used to choose a suitable criterion for
each input.
Inspired by the success of the BiLSTM-based
SCCWS model (Ma et al., 2018) and input-hint-
based multilingual neural machine translation sys-
tem (Johnson et al., 2017), He et al. (2019) pro-
posed to build an input-hint-based MCCWS on top
of the BiLSTM. They added two artificial tokens
representing a criterion and put them at the begin-
ning and the end of an input sentence. Such a sim-
ple idea advanced the SoTA performance on seven
datasets simultaneously. Gong et al. (2019) pro-
posed switch-LSTMs, which can dynamically route
between multiple BiLSTMs to encode criterion-
specific features when given different input hints.
Their work set the SoTA limit that can be achieved
via LSTM architecture.
After the remarkable effectiveness of pre-trained
language models was found, MCCWS works6461started to replace LSTM encoders with Trans-
former encoders (Vaswani et al., 2017). Huang
et al. (2020a) used RoBERTa (Liu et al., 2019) to
build an input-hint-based MCCWS model, which
advanced SoTA performance. Huang et al. (2020b)
shows that adding private structures on top of a
large pre-trained model can push SoTA even fur-
ther. Ke et al. (2021) pre-trained an input-hint-
based MCCWS on BERT (Devlin et al., 2019) with
meta-learning (Finn et al., 2017), but only after fine-
tuning did they become the new SoTA on SCCWS
models.
Ke et al. (2020) and Qiu et al. (2020) are the
most similar to ours among many MCCWS works.
We use a nearly identical input-hint-based model
as in Qiu et al. (2020). However, like all the works
mentioned before, they do not include a criterion
classification objective, and therefore fail to pro-
vide a way to choose criteria automatically. Ke
et al. (2020) is the only work using criterion clas-
sification objective, but we further simplified its
model structure, which outperforms their models
on average F1-score. We further proposed a novel
criterion-denoising objective that helps choose cri-
teria automatically. By trading off 0.07% F1-score
on average, we achieved the new SoTA on the OOV
recall, which improved by a large margin compared
to the previous SoTA ( 1.61%).
In summary, previous research on MCCWS ei-
ther did not provide a way to choose a criterion or
always manually chose a criterion. In our work,
we proposed a simple yet elegant way to make our
MCCWS model automatically choose a suitable
criterion for the given character sequence. Compar-
ing our works to others, we find that (1) our model
has the simplest structure and is the easiest to im-
plement among other works; (2) we achieved MC-
CWS SoTA performance on several CWS datasets
and on average F1-score over 10 datasets; (3) we
improved SoTA OOV recall by a large margin.
3 MCCWS
In this section, we describe the detail of our method-
ology. We first give a formal definition of input-
hint-based MCCWS (Section 3.1). Then we intro-
duce our MCCWS model (Section 3.2). Finally,
we formally define our criterion-denoising objec-
tive and describe how to jointly train our MCCWS
on top of the proposed denoising objective (Sec-
tion 3.3).3.1 Problem Definition
Letxbe a character sequence. Denote the i-th
character of sequence xasx, and the i-th output
corresponds to xasy. Each ybelongs to a tagset
T={B,M,E,S}where B,M,Erepresent the be-
ginning, the middle, and the end of a word, and S
represents a word with a single character. When
receiving a character sequence x, a SCCWS model
will pass xto its encoder (with parameter θ)
to generate the contextualized representation of x,
then feed the encoder output to its decoder (with
parameter θ) to generate prediction ybased on
x, following the constraint of the tagset T(see
Figure 1(a)). Typically, a decoder such as the con-
ditional random field (CRF) (Lafferty et al., 2001)
will search through all possible combinations and
return the combination with the highest probability:
y= arg maxPr(y|x;θ, θ), (1)
where |x|denotes the number of characters of x.
The goal of a SCCWS model with parameters θ
andθis to maximize the probability of ygiven
xover all pairs of (x, y)in a CWS dataset D. One
can achieve this by minimizing the negative log-
likelihood Lover dataset D:
L(D, θ, θ)
= min −/summationdisplaylog Pr( y|x;θ;θ).(2)
Now suppose there are Kdifferent CWS
datasets {D}. When receiving a character se-
quence xfrom the k-th dataset D, an input-hint-
based MCCWS model will combine xwith the k-th
criterion token [k]to form a new sequence (see Fig-
ure 1(c)). The new sequence is then processed as
in Equation (1). Therefore, we can rewrite Equa-
tion(2)to define the minimization objective of an
input-hint-based MCCWS model with parameters
θandθ:
L({D}, θ, θ)
= min −/summationdisplay/summationdisplaylog Pr( y|x,[k];θ;θ).
(3)
Observe that the negative log-likelihood of yis con-
ditioned on both xand[k], and the minimization is
performed on all Kdatasets simultaneously instead
of a single dataset.64623.2 Model Definition
Input Format. For each dataset Dand each
character sequence x∈ D, let
x= [[CLS]; [k];x; [SEP]] (4)
be the new sequence formed by concatenating the
[CLS]token, the k-th criterion token [k], character
sequence x, and the [SEP]token. xis treated as a
sequence with 3 +|x|characters and fed into our
MCCWS encoder.
Encoder. We used a pre-trained Chinese BERT
as our encoder, and we denote the output of BERT
ash:
h= BERT( x;θ)∈R,(5)
where d is the hidden dimension of BERT.
Devlin et al. (2019) includes all details of BERT.
Both [CLS]and[SEP]tokens are only used to follow
the BERT input format with no further computa-
tions done on both tokens. We note that we neither
use any private structures nor handcrafted features.
Thus, our encoder architecture can be considered
as the simplest among other MCCWS works.
Decoder. To keep our model simple, we choose
a greedy decoding algorithm over a non-greedy
one. We use one linear layer followed by a softmax
normalization as our decoder. The output of BERT
encoder h, with starting index 3, is fed directly into
our decoder:
y= softmax( W·h+b)∈R
for all i∈ {3, . . . ,|x|+ 2}.(6)
W∈R andb∈Rare trainable pa-
rameters, and 4is the size of tagset T. Our de-
coder will generate a sequence of probability vec-
torsy= (y, . . . ,y)∈R. Since we use
greedy decoding, we optimize our input-hint-based
MCCWS model with cross-entropy loss instead of
negative log-likelihood. So we change Equation (3)
as follows:
L({D}, θ, θ)
= min −/summationdisplay/summationdisplay/summationdisplay1⊙logy,(7)where 1denotes the one-hot encoding corre-
sponding to y,⊙denotes the Hadamard product,
andlogydenotes performing log operation on
probability vector yin an element-wise fashion.
Criterion Classification To make our model re-
member the meaning of criterion hint [k]during the
forward pass, we introduce a criterion classification
task. We let our model predict which criterion hint
it received. So we pick h, the output of BERT
that corresponds to the criterion token [k], and feed
it into a criterion classifier which consists of one
linear layer (different from our decoder) following
a softmax normalization:
c= softmax( W·h+b)∈R. (8)
Both W∈R andb∈Rare train-
able parameters. Our criterion classifier is set to
minimize cross-entropy loss, just like Equation (7):
L({D}, θ, θ)
= min −/summationdisplay/summationdisplay1⊙logc,(9)
where 1denotes the one-hot encoding that cor-
responds to [k]andlogcdenotes the element-wise
log operation on the probability vector c.
Total Loss Combining Equations (7)and(9), we
get our final loss L:
L({D}, θ, θ)
=L({D}, θ, θ)
+L({D}, θ, θ).(10)
We jointly train both objectives on our input-hint-
based MCCWS model. Surprisingly, this joint
objective gives us SoTA performance on several
datasets.
3.3 Criterion Denoising
To avoid manually giving criterion tokens, we de-
sign a criterion-denoising objective to make our
model choose the suitable criterion for each in-
put. We define a token [UNC], which stands for
“unknown criterion,” and we randomly replace each
pairing criterion [k]with [UNC]. In this situation,
the goal of our criterion classifier (see Equation (8))
is to find the best fitting criterion for the given input
x. So Equation (9)becomes a denoising objective,
in a similar way to the masked language model6463objective used in BERT. After training with [UNC],
the model can choose a suitable criterion for xand
perform CWS simultaneously, all in just a single
forward pass. We show that such an auto mecha-
nism does not harm the performance, making our
model effective and practical.
4 Experiments
4.1 Datasets
We perform experiments on 10 CWS datasets (this
means K= 10 ). Four datasets are from the
SIGHAN2005 bakeoff (Emerson, 2005), including
AS, CITYU, PKU, and MSRA; SXU is from the
SIGHAN2008 bakeoff (Jin and Chen, 2008); the
rest are CNC, CTB6 (Xue et al., 2005), UD (Ze-
man et al., 2018), WTB (Wang et al., 2014) and
ZX (Zhang et al., 2014). Following Emerson
(2005), we report the F1-score and OOV recall.
Our preprocessing mainly follows the works
of He et al. (2019) and Chen et al. (2017), as done
by others. We first convert all full-width charac-
ters into half-width. Then, we replace different
consecutive digits into one token (we do the same
for alphabets). Unlike others who set the maxi-
mum sentence length to 128 or lower to speed up
the training process, we decide to utilize the full
computing power of BERT and include as many
characters in the same context as possible. So we
set the maximum sentence length to 512. For sen-
tences longer than 512, we try to find the nearest
punctuation as our delimiter, otherwise, we split on
the 512th character. The statistics for all datasets
can be found in Appendix A.
4.2 Hyperparameters
We use PyTorch (Paszke et al., 2019) to im-
plement our model. We fine-tune BERT with
AdamW (Loshchilov and Hutter, 2019) on the
pre-trained checkpoint bert-base-chinese
provided by huggingface (Wolf et al., 2020) (this
means d = 768 and the number of parame-
ters is around 110M). Moving average coefficients
(β, β)of AdamW are set to (0.9,0.999). The
learning rate is set to 2×10, and the weight
decay coefficient is set to 0.01. We schedule the
learning rate with linear warmup and linear decay.
The warmup ratio is set to 0.1, and the total training
step is set to 170000 . Dropout (Srivastava et al.,
2014) is applied with a probability of 0.1. We set
the batch size to 32, and use gradient accumulationwith two steps (this is almost equivalent to setting
the batch size to 64). We use label smoothing only
on the decoder but not on the criterion classifier,
and we set the smoothing value to 0.1. We pick the
checkpoint with the highest F1 on the development
set to calculate test set F1. For each experiment
reported later, we ran each over 5 random seeds
and reported only the best result. The results of
all trials are listed in Appendix A. All experiments
were run on a single Intel Xeon Silver 4216 CPU
and an Nvidia RTX 3090 GPU.
4.3 Main Results
SoTA F1-score. Table 2 shows our results on
F1 over 10 CWS datasets. Our MCCWS model
(denoted as “Ours”) achieves SoTA results on 5
out of 10 datasets. Since not all works performed
experiments on all the same 10 datasets, we also
report average results on the most common 4 (de-
noted as Avg.4) and 6 (denoted as Avg.6) datasets.
Results show that our model is ranked 2nd under
Avg.4 and Avg.6, which is only 0.14% and0.05%
less than the best-performing model respectively.
We note that Huang et al. (2020b) used a private-
structure-based MCCWS with CRF decoder, there-
fore, has way more parameters than our proposed
model. Nevertheless, our model achieves the SoTA
performance on average over 10 datasets (denoted
as Avg.10). Therefore, despite the simplicity, our
model still performs well against strong baselines.
Noisy but near SoTA. In Section 3.3, we pro-
posed a criterion-denoising objective. We ran-
domly select 10% criterion tokens for each mini-
batch and replace them with [UNC]. Table 2 shows
the performance of our criterion denoising MC-
CWS model (denoted as ours+ 10%[ UNC]). We see
that the denoising version of our model beats the
previous SoTA on Avg.10 and even achieved the
new SoTA on 5 datasets. This shows that our
criterion-denoising objective does not hinder the
performance, but helps our model advance to near
SoTA results.
SoTA OOV Recall. Table 3 shows our results
on OOV recall over 10 CWS datasets. Our models
achieve SoTA results on 9 out of 10 datasets with or
without criterion-denoising objective. CWS task is
challenging when the word boundary is ambiguous,
which can only be eased by giving enough context.
Thus, we attribute the remarkable OOV recall im-
provement to our preprocessing step, for which we
set the maximum input length to 512, giving our6464
model enough context to identify unseen words.
We will further discuss this result in Section 4.4.
But with the help of our criterion-denoising ob-
jective, we see that OOV recall is boosted even
higher, showing the effectiveness of our criterion-
denoising objective.
Auto Mechanism In Section 3.3, we claimed that
our criterion-denoising objective could be used for
choosing criteria automatically. We do this by pair-
ing each input sequence on the test set with [UNC]
and performing the evaluation. Table 2 shows that
most datasets maintain their performances almost
on par with the original even when using [UNC],
and the average F1-score remains competitive with
other baselines. This suggests that some common
knowledge is shared throughout the 10 heteroge-
neous datasets, and our model can learn and lever-
age this knowledge.
Efficiency Unfortunately, almost all recent works
do not release their source code. So it might
be unfair to perform a quantifiable comparison.
However, we can still do a time-complexity analy-
sis. Since recent MCCWS works, including ours,
use the same encoder architecture (BERT-base or
RoBERTa-base), comparing the time complexity
between different decoding algorithms is fair. CRF
takes O(|x| · |T |), where |x|stands for sequence
length, and |T |stands for the number of classes
(which is 4for BMES tagging). Almost all recent
works use CRF as their decoding strategy, but weuse greedy decoding, which only takes O(|x|·|T | ).
Thus, our MCCWS model has lower time complex-
ity and is more efficient.
4.4 Ablation Study
Increase Criterion Denoising Rate. This sec-
tion studies what happens when the criterion de-
noising rate increases. Figure 2 shows that both the
average F1-score and the average OOV recall de-
crease as criterion noise increases. This is expected
as in the masked language model experiment of
BERT, where increasing the masked rate results in
fine-tune performance drop. However, as shown
in Figure 2, using [UNC]to perform inference only
gets affected slightly by different denoising rates.
This suggests that when using criterion-denoising
objective, our model learns to segment on the most
common patterns showed across datasets. Thus,
our model is robust to diverse inputs, which proven
itself to be a “general CWS model” that shares
knowledge across different CWS datasets.
Reduce Maximum Sentence Length. As shown
in Table 3, our model’s OOV recall outperformed
others by a large margin. We suspect that it is
due to our preprocessing step, which allows our
model to take inputs up to 512 characters. Figure 3
shows that the longer a model’s character sequence
is allowed to take, the better the performance on
the average F1-score and the average OOV recall.
Performance on input length longer than 256 stays64656466
mostly the same since only a few sequences have
their length longer than 256 (the average sentence
length on all 10 datasets is 37.09, see Appendix A).
However, we found an easy fix for models trained
on shorter sentences: That is, allow input sequence
length up to 512. Despite not being trained on such
a long sequence, we found that all models’ perfor-
mance increased after feeding longer input. This is
consistent with the common sense that longer input
reduces the chance of ambiguity and thus performs
better on CWS.
Criterion Classifier When removing the crite-
rion classifier, our average F1-score drops nearly
0.1%(Table 4, row 1), which is the gap between
our model and the previous SoTA. F1-score drops
even more when we use [UNC]to perform infer-
ence (Table 4, row 3). On the other hand, average
OOV recall seems to increase when removing the
criterion classifier (Table 4, rows 5-6). This sug-
gests that without the criterion classifier, the ability
to differentiate criterions was hindered (thus aver-
age F1 drops), and MCCWS model started to treat
different datasets as a whole (thus average OOV
recall improves). This shows the effectiveness of
the criterion classification.
Case Study We provide examples to demonstrate
our MCCWS model’s capability of segmenting dif-
ferently when given different criterion tokens. Ta-
ble 5 shows that in some cases, one sentence can
be segmented in at least five different ways, which
proves that our model can perform CWS based on
various criteria. Table 6 shows that in some other
cases, most criteria agree with each other, which
proves that our model can leverage the common
knowledge shared across datasets. We leave more
examples in Appendix A for interested readers.Original Sentence 也是言之有據
AS-gold 也-是-言-之-有-據
CITYU -gold 也是-言之有據
AS-infer 也-是-言-之-有-據
CITYU -infer 也是-言之有據
CNC -infer 也是-言之有據
CTB6 -infer 也-是-言之有據
MSRA -infer 也是-言之有據
PKU -infer 也-是-言之有據
SXU -infer 也-是-言之有據
UD-infer 也是-言-之有據
WTB -infer 也是-言之有據
ZX-infer 也-是-言-之-有據
[UNC]-infer 也是-言之有據
5 Conclusion
In this paper, we proposed a simple yet effective
input-hint-based MCCWS model which achieves
several SoTA results across 10 CWS datasets. We
also proposed a novel criterion-denoising objective
which makes our model capable of choosing cri-
terion automatically for each character sequence.
Experiment results show that our novel denoising
objective does not suffer dramatic performance loss
but helps our MCCWS model retain near SoTA per-
formance and even outperform previous work on6467Original Sentence 江泽民总书记
MSRA -gold 江泽民 -总书记
PKU -gold 江-泽民 -总书记
AS-infer 江泽民 -总书记
CITYU -infer 江泽民 -总书记
CNC -infer 江泽民 -总书记
CTB6 -infer 江泽民 -总书记
MSRA -infer 江泽民 -总书记
PKU -infer 江-泽民 -总书记
SXU -infer 江泽民 -总书记
UD-infer 江-泽民 -总-书记
WTB -infer 江泽民 -总书记
ZX-infer 江泽民 -总书记
[UNC]-infer 江泽民 -总书记
OOV recall by a large margin. Our model can serve
as a simple and robust baseline for MCCWS work
or as the starting point to further fine-tune into SC-
CWS models. In the future, we will try to gather
more CWS datasets and perform more extensive
experiments on more datasets.
Limitations
Unfortunately, we cannot access most
SIGHAN2008 bakeoff datasets, which were
proprietary but used by many previous works. This
makes the comparison in Table 2 a little unfair.
We argue that we replaced these non-accessible
datasets with the ones publicly accessible (includ-
ing UD, WTB, and ZX). We note that Huang et al.
(2020b) faced the same limitation as us. Thus they
also replaced datasets just as we did, which makes
them the only directly comparable work to ours.
Acknowledgement
This work was funded in part by the National Sci-
ence and Technology Council, Taiwan, under grant
MOST 111-2221-E-006-001 and in part by Google
and Qualcomm through a Taiwan University Re-
search Collaboration Project NAT-487842. This
work cannot be done without the support of all of
our labmates and families. So we would like to
thank all of them. In particular, we thank Meng-
Hsun Tsai, Daniel Tan, Runn Prasoprat, and Ching-
Wen Yang for their help in reviewing the draft; wethank Hsiu-Wen Li for his suggestion on changing
different denoising rates; we thank Chia-Jen Yeh
and Yi-Ting Li for their insightful discussion.
References64686469
A Appendix
We list the preprocessing statistics in Table 7. The
datasets’ description and preprocessing steps can
be found in Section 4.1. All datasets’ licenses
can be found in Table 8. Experiments on multi-
ple trials can be found in Tables 9 and Table 10.
Tables 11,12,13,14 give more examples to demon-
strate our input-hint-based MCCWS model’s capa-
bility of segmenting Chinese words with multiple
criteria.6470Dataset Split #C #S #W #UC #UW OOV% Avg.SL
AStrain 7,453,690 638,058 4,898,372 5,957 124,512 0 11.68
dev 805,692 70,895 551,209 4,353 32,000 1.86 11.36
test 193,723 14,429 122,610 3,579 18,093 3.73 13.43
CITYUtrain 2,132,370 47,718 1,317,626 4,799 60,650 0 44.69
dev 220,243 5,301 138,004 3,234 16,372 3.79 41.55
test 66,353 1,492 40,936 2,643 8,633 7.38 44.47
CNCtrain 8,908,376 207,001 5,841,321 6,643 113,223 0 43.04
dev 1,109,292 25,875 727,783 5,109 47,773 0.76 42.87
test 1,107,772 25,876 726,038 5,154 47,268 0.75 42.81
CTB6train 1,108,461 24,416 678,811 4,201 42,086 0 45.40
dev 82,765 1,904 51,229 2,491 8,639 4.89 43.47
test 86,157 1,975 52,861 2,538 8,747 5.17 43.62
MSRAtrain 3,615,524 78,227 2,144,776 5,023 71,399 0 46.22
dev 363,425 8,691 223,615 3,676 22,515 2.57 41.82
test 180,988 3,985 106,873 2,805 11,858 2.12 45.42
PKUtrain 1,616,528 17,255 1,004,155 4,569 48,758 0 93.68
dev 170,803 1,917 105,792 3,019 13,613 3.15 89.10
test 168,992 1,949 104,372 2,881 12,456 3.31 86.71
SXUtrain 744,162 15,407 474,758 4,026 28,207 0 48.30
dev 85,470 1,711 53,480 2,206 6,460 6.23 49.95
test 179,688 3,654 113,527 2,776 11,600 4.93 49.18
UDtrain 147,295 3,997 98,608 3,390 15,930 0 36.85
dev 19,027 500 12,663 1,922 4,040 10.95 38.05
test 18,080 500 12,012 1,806 3,748 11.05 36.16
WTBtrain 22,512 813 14,774 1,635 3,045 0 27.69
dev 2,875 95 1,843 770 837 18.39 30.26
test 2,838 92 1,860 733 731 15.05 30.85
ZXtrain 96,647 2,373 67,648 2,289 6,770 0 40.73
dev 28,309 788 20,393 1,651 3,184 7.85 35.93
test 47,992 1,394 34,355 1,787 4,126 6.45 34.43
Alltrain 25,845,565 1,035,265 16,540,849 9,286 310,538 0 24.97
dev 2,887,901 117,677 1,886,011 7,134 95,398 1.30 24.54
test 2,052,583 55,346 1,315,444 6,789 77,145 1.21 37.096471Dataset Provider License
AS SIGHAN2005 Research Purpose
CITYU SIGHAN2005 Research Purpose
CNC CNCorpus Research Purpose
CTB6 StanfordCoreNLP Apache License
MSRA SIGHAN2005 Research Purpose
PKU SIGHAN2005 Research Purpose
SXU Shan Xi University Research Purpose
UD UD Project BY-NC-SA 4.0
WTB Wang et al. (2014) Research Purpose
ZX Zhang et al. (2014) Research Purpose64726473Original Sentence 何樂而不為
AS-gold 何-樂-而-不-為
CITYU -gold 何樂而不為
AS-infer 何-樂-而-不-為
CITYU -infer 何樂而不為
CNC -infer 何-樂-而-不-為
CTB6 -infer 何-樂而-不為
MSRA -infer 何樂而不為
PKU -infer 何樂而不為
SXU -infer 何樂而不為
UD-infer 何-樂-而-不-為
WTB -infer 何樂而不為
ZX-infer 何-樂-而-不-為
[UNC]-infer 何樂而不為
Original Sentence 一去不復返
AS-gold 一-去-不復-返
CITYU -gold 一去不復返
CNC -gold 一去不復返
MSRA -gold 一去不復返
PKU -gold 一去不復返
AS-infer 一-去-不復-返
CITYU -infer 一去不復返
CNC -infer 一去不復返
CTB6 -infer 一-去-不復-返
MSRA -infer 一去不復返
PKU -infer 一去不復返
SXU -infer 一去不復返
UD-infer 一-去-不復-返
WTB -infer 一去-不復返
ZX-infer 一-去-不-復-返
[UNC]-infer 一去-不復返Original Sentence 四月二十六日
AS-gold 四月-二十六日
CITYU -gold 四月-二十六-日
CNC -gold 四-月-二十六-日
MSRA -gold 四月二十六日
AS-infer 四月-二十六日
CITYU -infer 四月-二十六-日
CNC -infer 四-月-二十六-日
CTB6 -infer 四月-二十六日
MSRA -infer 四月二十六日
PKU -infer 四月-二十六日
SXU -infer 四-月-二十六-日
UD-infer 四-月-二十六-日
WTB -infer 四月-二十六日
ZX-infer 四月-二十六日
[UNC]-infer 四月-二十六-日
Original Sentence 並不足以
AS-gold 並-不-足以
CITYU -gold 並-不足以
CNC -gold 並不 -足以
AS-infer 並-不-足以
CITYU -infer 並-不足以
CNC -infer 並不 -足以
CTB6 -infer 並不 -足以
MSRA -infer 並不 -足以
PKU -infer 並-不足以
SXU -infer 並-不足以
UD-infer 並-不-足-以
WTB -infer 並不 -足以
ZX-infer 並-不-足以
[UNC]-infer 並-不-足以6474ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.6475/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.6476