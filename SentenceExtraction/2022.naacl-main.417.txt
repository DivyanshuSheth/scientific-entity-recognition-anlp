
Xiangru TangAlexander R. FabbriZiming MaoGriffin AdamsBorui Wang
Haoran LiYashar MehdadDragomir RadevYale UniversityColumbia UniversityFacebook AI
{xiangru.tang, alexander.fabbri, ziming.mao,
borui.wang, dragomir.radev}@yale.edu
griffin.adams@columbia.edu
{aimeeli, mehdad}@fb.com
Abstract
Current pre-trained models applied for sum-
marization are prone to factual inconsistencies
that misrepresent the source text. Evaluating
the factual consistency of summaries is thus
necessary to develop better models. However,
the human evaluation setup for evaluating fac-
tual consistency has not been standardized. To
determine the factors that affect the reliabil-
ity of the human evaluation, we crowdsource
evaluations for factual consistency across state-
of-the-art models on two news summarization
datasets using the rating-based Likert Scale and
ranking-based Best-Worst Scaling. Our analy-
sis reveals that the ranking-based Best-Worst
Scaling offers a more reliable measure of sum-
mary quality across datasets and that the reli-
ability of Likert ratings highly depends on the
target dataset and the evaluation design. To im-
prove crowdsourcing reliability, we extend the
scale of the Likert rating and present a scoring
algorithm for Best-Worst Scaling that we call
value learning . Our crowdsourcing guidelines
will be publicly available to facilitate future
work on factual consistency in summarization.
1 Introduction
Pre-trained language models have achieved
promising results in abstractive text summariza-
tion (Edunov et al., 2019; Dong et al., 2019; Song
et al., 2019; Zhang et al., 2019, 2020). A serious
limitation of these models, however, is their ten-
dency to produce text that is factually inconsistent
with the input. Thus, evaluating the factual consis-
tency of the generated summaries with respect to
the source is an important task (Falke et al., 2019;
Cao et al., 2020; Gabriel et al., 2021; Durmus et al.,
2020; Huang et al., 2021; Pagnoni et al., 2021).
Recently, metrics have been proposed for evalu-
ating factual consistency, including applying natu-
ral language inference (Falke et al., 2019; Kryscin-
ski et al., 2020) and question-answering mod-
els (Eyal et al., 2019; Scialom et al., 2019; Dur-
mus et al., 2020; Wang et al., 2020). However,current metrics still do not correlate highly with
human judgments on factual consistency (Koto
et al., 2020; Pagnoni et al., 2021). To overcome
the inherent limitation of automatic metrics, re-
searchers typically crowdsource human evaluations
using platforms such as Amazon’s Mechanical
Turk (MTurk) (Gillick and Liu, 2010; Sabou et al.,
2012; Lloret et al., 2013). However, papers often
differ in their preferred evaluation protocols (Louis
and Nenkova, 2013; Hardy et al., 2019). These
differences in the evaluation task design affect the
quality of the resulting human judgments and sys-
tem comparisons (Santhanam and Shaikh, 2019).
Two of the primary paradigms of crowdsourced
evaluations are ranking-based and rating-based.
Best-Worst Scaling (Louviere and Woodworth,
1991) is a ranking-based method by which the an-
notator selects the best and worst example out of
a set of examples. Prior research has claimed that
Best-Worst Scaling produces higher-quality evalua-
tions than rating scales such as the Likert Scale for
tasks such as sentiment analysis (Kiritchenko and
Mohammad, 2017). In the context of summariza-
tion, Steen and Markert (2021) find that, compared
to the Likert Scale, ranking-based protocols are
more reliable for measuring summary coherence
but less so for repetition. However, previous stud-
ies have not analyzed annotation reliability in the
context of factual consistency for summarization.
Our contributions are the following: 1) We are,
to the best of our knowledge, the first to study the
reliability of human evaluation for summarization
factual consistency. 2) We study rating and ranking-
based protocols across two summarization datasets
and four state-of-the-art abstractive models. We de-
termine the factors affecting human evaluation reli-
ability and present a novel ranking-based protocol
with the highest reliability. 3) We will release our
evaluation guidelines and annotations to promote
future work on factual consistency evaluation.5680
2 Study Design
Each study consists of 100 input documents ran-
domly sampled from each dataset, and four associ-
ated model-generated summaries.
2.1 Datasets and Models
Datasets: We conduct our study on two bench-
mark summarization datasets. CNN/DailyMail
(Hermann et al., 2015; Nallapati et al., 2016)
consists of 311,672 pairs of online articles and
bullet-point summaries, typically three sentences.
XSum (Narayan et al., 2018) consists of 227K on-
line articles and single-sentence summaries.
Models: The following abstractive summarization
models are chosen due to their strong cross-dataset
performance: BART (Lewis et al., 2020), a de-
noising autoencoder for pretraining sequence to se-
quence and natural language understanding tasks;
ProphetNet (Qi et al., 2020), a pre-trained encoder-
decoder model that performs n-gram language mod-
eling; PEGASUS (Zhang et al., 2020), a model
pre-trained with a summarization-specific objec-
tive function; and BERTSUM (Liu and Lapata,
2019), a two-stage fine-tuning approach. Table 1
shows the models’ ROUGE scores (Lin, 2004).
2.2 Reliability
We follow Steen and Markert (2021) and report
Krippendorff’s alpha and Split-Half Reliability as
measures of the reliability of crowdsourced anno-
tations. Krippendorff’s alpha ( α)is a reliability
coefficient developed to measure the agreement
among multiple annotators (Krippendorff, 2011).
This measures instance-level reliability, especially
how reliable judgments are over individual sum-
mary instances. For system-level rankings, to mea-
sure the reliability of the rankings of summarization
models, we compute Split-Half Reliability (SHR) .
To compute SHR, annotations are split into two
independent groups, and Pearson correlations are
calculated between the groups.
We follow a similar block-design described in
Steen and Markert (2021). We note that we include
the input document as the context of the summaries
as opposed to the coherence and repetition dimen-
sions studied in that work, which do not require
reading the input article. We divided our corpus
into 20 blocks of 5 documents. We include all
4 generated summaries for each document in the
same block, resulting in 5 ×4 = 20 summaries
per block. We require 3 annotators per block as
in Steen and Markert (2021), and each annotator
is limited to annotating at most two blocks total
across all tasks. A further study of the effect of
the number of annotators or block design is left for
future work. Crowdsourcing is done via MTurk.
2.3 Protocols
TheLikert Scale (LS) is a common rating-based
evaluation protocol (Asghar et al., 2018). Likert
Scales applied to summarization typically range
from 1-5 (Steen and Markert, 2021). Best-Worst
Scaling (BWS) is a type of ranking-oriented evalua-
tion that requires annotators to specify only the best
and the worst example in a set of summaries (Hollis
and Westbury, 2018; Kiritchenko and Mohammad,
2017). For BWS, the annotator labels the most
factually consistent summary and the least factu-
ally consistent summary. Another type of ranking-
based protocol is pairwise comparison, where each
example is compared to every other example. How-
ever, this protocol is very expensive; given Nitems
to annotate, Ntotal annotations must be collected
as opposed to BWS which requires a constant fac-
tor of Ntotal annotators. Due to this exorbitant
cost as any reasonable scale, we restrict our study
of ranking-based protocols to BWS, and we refer
the reader to Kiritchenko and Mohammad (2017)
for an in-depth discussion of the cost comparison
for the task of sentiment analysis.5681
2.4 Research Questions
We study three three main research questions (RQ):
RQ1: Ranking (BWS) vs. LS? We aim to deter-
mine the more reliable evaluation protocol.
RQ2: What affects reliability? We aim to de-
termine the factors that affect the reliability of the
human evaluation.
RQ3: What are the protocols’ limitations and
how to improve them? Based on the analysis,
we propose two protocols to improve the reliability.
3 Analysis
We show the average ratings across LSscales, in-
cluding a modified LS scale we will later introduce,
in Table 2. Despite the consistently higher ROUGE
scores, Pegasus was not always ranked highest,
which aligns with previous work suggesting that
ROUGE score does not correlate with factual con-
sistency (Durmus et al., 2020). The primary results
for reliability evaluation are found in Table 3.
RQ1: BWS outperforms LS on CNN/DM. We
see on the left-hand side of the first two rows of
Table 3 that BWS outperforms LSby a large mar-
gin on both instance-level ( α) and system-level
(SHR) reliability. As seen in the distribution of the
LSratings in Figures 1, many models are rated as
factually consistent with scores of 4 or 5. This co-
incides with previous investigations on CNN/DM
which conclude that recent summarization systems
produce fluent texts with relatively few factual er-
rors (Fabbri et al., 2021). We hypothesize that
the greater reliability of BWS on CNN/DM data
may result from the ranking task forcing the anno-
tator to choose the best summary and distinguish
these close summaries rather than allowing e.g. the
annotator to give both a score of 5. This result
suggests that BWS is preferable in cases where the
summaries analyzed have similar factual consis-
tency, such as CNN/DM.
Though agreement on individual summaries ( α)
is relatively low for all annotation methods, these
numbers are comparable to those obtained in (Steen
and Markert, 2021). Furthermore, we look at the
relative difference between ( α) of BWS and LS,
and we find that studies still arrive at consistent
system scores as demonstrated by the SHR. This
reflects similar observations made by Gillick and
Liu (2010). System-level ranks such as SHR, are
also more important for evaluation purposes as the
goal is generally to rank models to determine the
best performing (or most factually consistent) sys-
tem as opposed to examining individual examples
as Krippendorff’s alpha measures.
RQ2: Dataset Characteristics Affect Reliability.
We extend our experiments to the XSum dataset to
see whether the reliability of the protocols changes
as the characteristics of the dataset change. XSum-
trained models are known to suffer from factual
inconsistencies because of the high compression
ratio and high level of abstraction of the reference
summaries (Maynez et al., 2020). As seen on the
right-hand side of the first two rows of Table 3,
BWS andLSboth perform well, with LSslightly
outperforming BWS according to SHR. As seen in
Figure 1, the model scores are more spread out
along the scale. This coincides with the large range
of ROUGE scores and larger differences between
models, as seen in Table 1, which likely explains
why annotators can differentiate the model outputs
better. Thus, we believe that LSis a viable op-
tion when the corpus contains a diverse quality of
summaries, like XSum.5682
RQ3: Improvements and Current Limitations.
We propose two modified protocols to improve re-
liability and then study the presence of common
limitations for evaluation protocols. Prior work has
noted the effect of scale granularity (Kiritchenko
and Mohammad, 2017), so for LS, we extend the
scale from for 5 to 10 and call it LS-10 . Table 3
shows that that LS-10 is more reliable than LSA
finer-grained scale may capture more nuanced dif-
ferences in data points with more choices. Scores
tend to move towards the extremes when we use
a finer-grained scale (10 vs 5), as seen in the dif-
ference in distributions in Figures 1 and 2. Thus,
forLS-10 , a larger range and being less biased to-
wards a specific region, promoting better reliability.
Previous work suggests that Best-Worst Scaling
fails to yield an unbiased estimate of the true qual-
ity value (Hollis, 2018). Thus, for BWS, we incor-
porate information about the quality of competing
examples or value learning into a BWS proto-
col. The annotator is asked to give a score (3-point
scale) for the difference between the best and the
worst summary. The final ranking uses a weighted
sum. The results at the bottom of Table 3 also
confirm the effectiveness of this protocol.
To verify the limitations of evaluation protocols
noted by Kiritchenko and Mohammad (2017), we
conduct the following studies. We first analyze (a)
the inconsistencies in annotations by different
annotators , measured by the percentage of sum-
maries that receive different ratings or rankings
from different annotators, which we call change
rate. As shown in Table 4, annotators are more
likely to agree on the same ranking in BWS as op-
posed to the same rating for LS. We further test (b)
inconsistencies by the same annotator , in partic-
ular whether annotations done by the same worker
are consistent over time. We ask workers who have
previously annotated XSum and CNN/DM sam-
ples to re-do their annotations one week after their
initial annotations. We notified the workers to re-
annotate only one week after they finished, instead
of at the beginning, as we do not want to introduce
design bias. In total, 43 workers redid 860 anno-
tations. For LS, the average change in the rating
of the two annotations one week apart by the same
worker was 0.92.
Additionally, we examine whether LSsuffers
from (c) scale region bias , where different anno-
tators are often biased towards different parts of
the rating scale. For a given block and two anno-
tators, we calculate the rating range given by each
annotator. We then calculate the overlap length
between those two ranges divided by the length of
the overall range from both annotators. We call
this the percentage scale overlap and average over
all pairs of annotators and blocks. For LS, the per-
centage scale overlap is (0.67, 0.88) for (CNN/DM,
XSum), respectively, and (0.61, 0.82) forLS-10 .
The difference in scale region bias between LSand
LS-10 is small, but the bias difference between
CNN/DM and XSum is notable. Greater diver-
sity in summary quality as in XSum may force the
annotators to expand their use of the scale and mit-
igate region bias, which may explain why LSis
better than BWS on XSum as opposed to CNN/DM.
Future work may investigate further what exactly
constitutes too wide of a scaling range.
4 Conclusion
In this paper, we conduct studies to understand and
improve the reliability of ranking and rating-based
human evaluations of summarization factual con-
sistency. We find that Best-Worst Scaling is largely
reliable, and the Likert scale also has merits, but
the proper scaling and dataset characteristics must
be carefully studied to ensure its reliability. We im-
prove these two protocols based on our findings and
believe that our studies advance the understanding
of both models and metrics as we aim to facilitate
factually consistent text generation.56835 Ethical Considerations
Intellectual Properties and Privacy Rights All
of the datasets (CNN/DM and XSum) used in our
study are publicly available. Regarding privacy
rights, the authors of the paper completed IRB hu-
man subject protection training for conducting this
study. We will release the annotations, but rather
than releasing the MTurk ID of the worker, we will
completely anonymize this ID.
Compensation for Annotators Workers were
compensated $5 per block, calibrated to equal a
$15/hour payrate. We first annotated examples in-
house to determine the required annotation speed.
A summary block usually takes around 20 minutes.
Steps Taken to Avoid Potential Problems An-
notations were completed in the form of a survey
on a Google Form. We provided space for the Turk-
ers to provide feedback. We manually uploaded the
data points (articles and summaries) used in this
study to avoid any offensive content.
The Number of Examples We sampled 100 ex-
amples from each dataset that did not contain ex-
actly matching summaries. Both Likert and BWS
follow the same block design, which includes the
same number of examples per block. With the ex-
ception that the BWS annotation asks for the most
and least factually consistent summary and the Lik-
ert asks for ratings for each individual summary.
Due to space requirements, we included further de-
tails, images of the interface, in the supplementary
material. We pay the same amount per block of
annotations.
Qualifications of MTurk workers We use the
following qualifications to recruit in total 350
MTurk workers with good track records: HIT ap-
proval rate greater than or equal to 98%, num-
ber of HITs approved greater than or equal to
500, and located in one of the following English
native-speaking countries: Australia, Canada, New
Zealand, United Kingdom, United States.
References568456855686A Appendix
Besides the average model rank and average rating
scores across BWS, LS-5, and LS-10 evaluations,
we also provide standard deviations in Table 5.
To demonstrate our annotation template and fa-
cilitate future research, we show the interface for
BWS annotations in Figures 3 and 4 and the inter-
face for Likert annotations in Figures 5 and 6. We
made use of the survey feature in Amazon Mechan-
ical Turk (MTurk) to link to these Google Forms in
Figure 7.568756885689569056915692