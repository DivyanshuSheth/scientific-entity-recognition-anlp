
Siyu Lai, Zhen Yang, Fandong Meng, Xue Zhang, Yufeng Chen,
Jinan XuandJie ZhouBeijing Key Lab of Trafﬁc Data Analysis and Mining,
Beijing Jiaotong University, Beijing, ChinaPattern Recognition Center, WeChat AI, Tencent Inc, China
{siyulai,xue_zhang,chenyf,jaxu}@bjtu.edu.cn ,
{zieenyang,fandongmeng,withtomzhou}@tencent.com
Abstract
Generating adversarial examples for Neu-
ral Machine Translation (NMT) with single
Round-Trip Translation (RTT) has achieved
promising results by releasing the meaning-
preserving restriction. However, a potential
pitfall for this approach is that we cannot de-
cide whether the generated examples are ad-
versarial to the target NMT model or the aux-
iliary backward one, as the reconstruction er-
ror through the RTT can be related to ei-
ther. To remedy this problem, we propose a
new criterion for NMT adversarial examples
based on the Doubly Round-Trip Translation
(DRTT). Speciﬁcally, apart from the source-
target-source RTT, we also consider the target-
source-target one, which is utilized to pick out
the authentic adversarial examples for the tar-
get NMT model. Additionally, to enhance the
robustness of the NMT model, we introduce
the masked language models to construct bilin-
gual adversarial pairs based on DRTT, which
are used to train the NMT model directly.
Extensive experiments on both the clean and
noisy test sets (including the artiﬁcial and natu-
ral noise) show that our approach substantially
improves the robustness of NMT models.
1 Introduction
In recent years, neural machine translation (NMT)
(Cho et al., 2014; Bahdanau et al., 2014; Vaswani
et al., 2017) has achieved rapid advancement in
the translation performance (Yang et al., 2020; Lu
et al., 2021). However, the NMT model is not al-
ways stable enough, as its performance can drop
signiﬁcantly when small perturbations are added
into the input sentences (Belinkov and Bisk, 2017;
Cheng et al., 2020). Such perturbed inputs are often
referred to as adversarial examples in the literature,
and how to effectively generate and utilize adver-
sarial examples for NMT is still an open question.Figure 1: An example of the source-target-source RTT
process on a perturbed input xby replacing “ 巨大
(huge)” to “ 轻便(light)”.
Conventional approaches (Ebrahimi et al., 2018;
Cheng et al., 2019) for generating NMT adversarial
examples always follow the meaning-preserving
assumption, i.e., an NMT adversarial example
should preserve the meaning of the source sentence
but destroy the translation performance drastically
(Michel et al., 2019; Niu et al., 2020). With the
meaning-preserving restriction, the researchers try
to add perturbations on the source inputs as small
as possible to ensure the meaning of the source
sentence is unchanged, which severely limits the
search space of the adversarial examples. Addi-
tionally, it is much problematic to craft a minor
perturbation on discrete text data, since some ran-
dom transformations (e.g., swap, deletion and re-
placement) may change, or even reverse seman-
tics of the text data, breaking the aforementioned
meaning-preserving assumption. To break this lim-
itation, Zhang et al. (2021) introduce a new crite-
rion for NMT adversarial examples: an effective
NMT adversarial example imposes minor shifting
on the source and degrades the translation dramati-
cally, would naturally lead to a semantic-destroyed
round-trip translation result . Take the case in Fig-
ure 1 as an example: xreverses the semantics
of input xby replacing “ 巨大(huge)” to “ 轻便
(light)”. Since the semantics of xandxare com-4256pletely different, it is unreasonable to use the orig-
inal target sentence of xto evaluate the attacks
directly. Therefore, Zhang et al. (2021) propose to
evaluate the BLEU score between xand its recon-
structed sentence ˆ xfrom the source-target-source
round-trip translation (RTT), as well as the BLEU
score between the original sentence xand its re-
constructed sentence ˆ x. They take the decrease
between the two BLEU scores mentioned above
as the adversarial effect. Speciﬁcally, if the BLEU
decrease exceeds a predeﬁned threshold, xis con-
cluded to be an adversarial example for the target
NMT model.
While achieving promising results by breaking
the meaning-preserving constraint, there are two
potential pitfalls in the work of Zhang et al. (2021):
(1) Since the source-target-source RTT involves
two stages, i.e., the source-to-target translation
(S2T) performed by the target NMT model and
target-to-source translation (T2S) performed by an
auxiliary backward NMT model, we cannot decide
whether the BLEU decrease is really caused by the
target NMT model. As we can see from the ex-
ample in Figure 1, the translation from xtoyis
pretty good, but the translation from ytoˆ xis re-
ally poor. We can conclude that the BLEU decrease
is actually caused by the auxiliary backward model
and thus xis not the adversarial example for the
target NMT model. Even if Zhang et al. (2021) try
to mitigate this problem by ﬁne-tuning the auxil-
iary backward model on the test sets, we ﬁnd this
problem still remains. (2) They only generate the
monolingual adversarial examples on the source
side to attack the NMT model, without proposing
methods on how to defend these adversaries and
improve the robustness of the NMT model.
To address the issues mentioned above, we ﬁrst
propose a new criterion for NMT adversarial ex-
amples based on Doubly Round-Trip Translation
(DRTT), which can ensure the examples that meet
our criterion are the authentic adversarial exam-
ples for the target NMT model. Speciﬁcally, apart
from the source-target-source RTT (Zhang et al.,
2021), we additionally consider a target-source-
target RTT on the target side. The main intuition is
that an effective adversarial example for the target
NMT model shall cause a large BLEU decrease on
the source-target-source RTT while maintaining a
small BLEU decrease on target-source-target RTT.
Based on this criterion, we craft the candidate ad-
versarial examples with the source-target-sourceRTT as Zhang et al. (2021), and then pick out the
authentic adversaries with the target-source-target
RTT. Furthermore, to solve the second problem, we
introduce the masked language models (MLMs) to
construct the bilingual adversarial pairs by perform-
ing phrasal replacement on the generated monolin-
gual adversarial examples and the original target
sentences synchronously, which are then utilized to
train the NMT model directly. Experiments on both
clean and noisy test sets (including ﬁve types of
artiﬁcial and nature noise) show that the proposed
approach not only generates effective adversarial
examples, but also improves the robustness of the
NMT model over all kinds of noises. To conclude,
our main contributions are summarized as follows:
•We propose a new criterion for NMT adversarial
examples based on the doubly round-trip transla-
tion, which can pick out the authentic adversarial
examples for the target NMT model.
•We introduce the masked language models to
construct the bilingual adversarial pairs, which
are then utilized to improve the robustness of the
NMT model.
•Extensive experiments show that the proposed
approach not only improves the robustness of the
NMT model on both artiﬁcial and natural noise,
but also performs well on the clean test sets.
2 Related Work
2.1 Adversarial Examples for NMT
The previous approaches for constructing NMT
adversarial examples can be divided into two
branches: white-box and black-box. The white-
box approaches are based on the assumption that
the architecture and parameters of the NMT model
are accessible (Ebrahimi et al., 2018; Cheng et al.,
2019; Chen et al., 2021). These methods usually
achieve superior performance since they can con-
struct and defend the adversaries tailored for the
model. However, in the real application scenario, it
is always impossible for us to access the inner archi-
tecture of the model. On the contrary, the black-box
approaches never access to inner architecture and
parameters of the model. In this line, Belinkov
and Bisk (2017) rely on synthetic and naturally
occurring language error to generate adversarial ex-
amples and Michel et al. (2019) propose a meaning-
preserving method by swapping the word internal4257
character. Recently, Zhang et al. (2021) craft ad-
versarial examples beyond the meaning-preserving
restriction with the round-trip translation and our
work builds on top of it.
2.2 Masked Language Model
Masked Language Model (MLM) (Devlin et al.,
2018; Conneau and Lample, 2019) has achieved
state-of-the-art results on many monolingual and
cross-lingual language understanding tasks. MLM
randomly masks some of the tokens in the input,
and then predicts those masked tokens. Recently,
some work adopt MLM to do word replacement
as a data augmentation strategy. Jiao et al. (2019)
leverage an encoder-based MLM to predict word
replacements for single-piece words. Liu et al.
(2021) construct augmented sentence pairs by sam-
pling new source phrases and corresponding target
phrases with transformer-based MLMs. Following
Liu et al. (2021), we introduce the transformer-
based MLMs to construct the bilingual adversarial
pairs. The main difference between our work and
Liu et al. (2021) is that we choose to mask the ad-
versarial phrases or words at each step and Liu et al.
(2021) mask the words randomly.
3 Method
In this section, we ﬁrst describe our proposed cri-
terion for NMT adversarial examples, and then
present the way of constructing the bilingual adver-
sarial pairs.
3.1 Adversarial Examples for NMT
For clarity, we ﬁrst introduce the traditional crite-
ria for NMT adversarial examples, i.e., the criteriabased on the meaning-preserving (Michel et al.,
2019; Karpukhin et al., 2019) and RTT (Zhang
et al., 2021), and then elaborate our new criterion
based on DRTT. We will use the following nota-
tions: xandydenotes the source and target sen-
tence, respectively. xandydenote the perturbed
version of xandy, respectively. f(·)is the forward
translation process performed by the target NMT
model andg(·)is the backward translation process
performed by the auxiliary backward NMT model.
sim(·,·)is a function for evaluating the similar-
ity of two sentences, and we use BLEU (Papineni
et al., 2002) as the similarity function.
Criterion based on meaning-preserving. Sup-
posey=f(x)andy=f(x)is the forward
translation of the input xand its perturbed version
x, respectively. xis an adversarial examples
when it meets:
/braceleftbiggsim(x,x)>η,
sim(y,y)−sim(y,y)>α,(1)
whereηis a threshold to ensure a high similar-
ity between xandx, so that they can meet the
meaning-preserving restriction. A larger αindi-
cates a more strict criterion of the NMT adversarial
example.
Criterion based on RTT. Zhang et al. (2021)
point out that the perturbation δmay change, even
reverse the meaning of x, so it is incorrect to use y
as a target sentence to measure the semantic alter-
ation on the target side. Therefore, they introduce
the criterion based on RTT which gets rid of the
meaning-preserving restriction. The percentage de-
crease of similarity between xandxthrough the4258source-target-source RTT is regarded as the adver-
sarial effect d(x,x), is calculated as:
d(x,x) =sim(x,ˆ x)−sim(x,ˆ x)
sim(x,ˆ x),(2)
where ˆ xandˆ xare reconstructed sentences gener-
ated with source-target-source RTT: ˆ x=g(f(x)),
ˆ x=g(f(x)). A large d(x,x)indicates that
the perturbed sentence xcan not be well recon-
structed by RTT when compared to the reconstruc-
tion quality of the original source sentence x, so
xis likely to be an adversarial example.
Criterion based on DRTT. In Eq.(2), sim(x,ˆ x)
is a constant value given the input xand the NMT
models. Therefore, the d(x,x)is actually deter-
mined by−sim(x,ˆ x), which can be interpreted
as the reconstruction error between xandˆ x. As
we mentioned above, the reconstruction error can
be caused by two independent translation processes:
the forward translation process f(·)performed by
the target NMT model and the backward translation
processg(·)performed by the auxiliary backward
model. Consequently, there may be three occa-
sions when we get a large d(x,x): 1) A large
semantic alteration in f(x)and a small semantic
alteration in g(y); 2) A large semantic alteration
inf(x)and a large alteration in g(y); 3) A small
semantic alteration in f(x)and a large alteration
ing(y). We can conclude xis an adversarial ex-
ample for the target NMT model in occasion 1 and
2, but not in occasion 3. Therefore, the criterion
based on RTT may contain many fake adversarial
examples.
To address this problem, we add a target-source-
target RTT starting from the target side. The per-
centage decrease of the similarity between yand
ythrough the target-source-target RTT, denoted
asd(y,y), is calculated as:
d(y,y) =sim(y,ˆ y)−sim(y,ˆ y)
sim(y,ˆ y),(3)
where ˆ y=f(g(y))andˆ y=f(g(y))are re-
constructed sentences generated with the target-
source-target RTT. We take both d(x,x)and
d(y,y)into consideration and deﬁne xas an
adversarial examples when it meets:
/braceleftbiggd(x,x)>β,
d(y,y)<γ,(4)
whereβandγare thresholds ranging in [−∞,1]. The interpretation of this criterion is intuitive:
ifd(y,y)is lower than γ, we can conclude
that the reconstruction error between yandˆ yis
very low. Namely, we can ensure a small semantic
alteration of g(y). Therefore, if d(x,x)is
larger thanβ, we can conclude the BLEU decrease
through the source-target-source RTT is caused by
the target NMT model, so that we can conclude x
is an authentic adversarial example.
3.2 Bilingual Adversarial Pair Generation
Since the proposed criterion breaks the meaning-
preserving restriction, the adversarial examples
may be semantically distant from the original
source sentence. Thus, we cannot directly pair the
adversarial examples with the original target sen-
tences. In this section, we propose our approach
for generating bilingual adversarial pairs, which
performs the following three steps: 1) Training
Masked Language Models: using monolingual and
parallel data to train masked language models; 2)
Phrasal Alignment: obtaining alignment between
the source and target phrases; 3) Phrasal Replace-
ment: generating bilingual adversarial pairs by per-
forming phrasal replacement on the source and
target sentences synchronously with the trained
masked language models. The whole procedure is
illustrated in Figure 2.
Training Masked Language Models. We train
two kinds of masked language models, namely
monolingual masked language model (M-MLM)
(Devlin et al., 2018) and translation masked lan-
guage model (T-MLM) (Conneau and Lample,
2019), for phrasal replacement on the source and
target sentence, respectively. The M-MLM intro-
duces a special [MASK] token which randomly
masks some of the tokens from the input in a cer-
tain probability, and predict the original masked
words. Following Liu et al. (2021), we train the M-
MLM on monolingual datasets and use an encoder-
decoder Transformer model (Vaswani et al., 2017)
to tackle the undetermined number of tokens dur-
ing generation. The T-MLM takes the identical
model structure and similar training process as the
M-MLM. The main difference is T-MLM relies on
the parallel corpus. T-MLM concatenates parallel
sentences by a special token [SEP] and only masks
words on the target side. The objective is to predict
the original masked words on the target side.4259Phrasal Alignment. Phrasal alignment projects
each phrase in the source sentence xto its align-
ment phrase in the target sentence y. We ﬁrst gener-
ate the alignment between xandyusing FastAlign
(Dyer et al., 2013). Then we extract the phrase-
to-phrase alignment by the phrase extraction algo-
rithm of NLTK, and get a mapping function p.
Phrasal Replacement. Given the source sen-
tencex={s,s,...,s}and the target sentence
y={t,t,...,t},sis thei-th phrase in x,t
is thep(i)-th phrase in ywhich is aligned to sby
the mapping function p. We construct the candidate
bilingual adversarial pairs (x,y)by performing
the phrasal replacement on (x,y)repeatedly until
cpercentage phrases in xhave been replaced. For
each step, we select the phrase that yields the most
signiﬁcant reconstruction quality degradation.
Here, we take the replacing process for sand
tas an example. Considering the not attacked
yet phrasesinx, we ﬁrst build a candidate set
R={r,r,...,r}forswith the prepared
M-MLM. Speciﬁcally, we extract the kcandidate
phrases with top khighest predicted probabilities
by feeding xinto M-MLM, where xis the
masked version of xby masking s. We select
the best candidate rforsas:
r= arg maxd(x,x), (5)
where xis the noised version by replacing s
withr. Withsbeing replaced, we need to replace
tto ensure they are still semantically aligned.
To this end, we feed the concatenation of xand
yinto T-MLM, and choose the output phrase
with the highest predicted probability as the substi-
tute phrase for t.
Finally, to decide whether (x,y)is an authen-
tic bilingual adversarial pair for the target NMT
model, we perform a target-source-target RTT start-
ing from the target side and calculate d(y,y)
between yand its reconstruction sentence ˆ yac-
cording to Eq.(4). We take (x,y)as an authentic
bilingual adversarial pair if d(x,x)is greater
thanβandd(y,y)is less thanγ. We formalize
these steps in Algorithm 1 in Appendix A.
After generating adversarial data through the
above steps, we combine it with original training
data and use them to train the NMT model directly.4 Experimental Settings
We evaluate our model under artiﬁcial noise in
Zh→En and En→De translation tasks, and under
natural noise in En→Fr translation task. The details
of the experiments are elaborated in this section.
4.1 Dataset
For the Zh→En task, we use the LDC corpus with
1.25M sentence pairs for training, NIST06 for val-
idation, and NIST 02, 03, 04, 05, 08 for testing.
For the En→De task, we use the publicly available
dataset WMT’17 En-De (5.85M) for training, and
take the newstest16 andnewstest17 for validation
and testing, respectively. In En →Fr task, we follow
Liu et al. (2021) to combine the WMT’19 En →Fr
(36k) robustness dataset with Europarl-v7 (2M) En-
Fr pairs for training. We take the development set
of the MTNT (Michel and Neubig, 2018) for val-
idation and the released test set of the WMT’19
robustness task for testing. As for MLMs, we use
the Chinese sentences of the parallel corpus to train
the Chinese M-MLM, and use the whole parallel
corpus to train Zh-En T-MLM. We train the English
M-MLM with News Commentary and News Crawl
2010 (7.26M in total) monolingual corpus follow-
ing Liu et al. (2021). T-MLM for En-De and En-Fr
are trained with their original parallel corpus.
4.2 Model Conﬁguration and Pre-processing
The MLMs and NMT models in this paper take
Transformer-base (Vaswani et al., 2017) as the
backbone architecture. We implement all models
base on the open-source toolkit Fairseq (Ott et al.,
2019). As for hyper-parameters, βis set to 0.01 and
γis set to 0.5 for Zh→En. For En→De and En→Fr,
βandγare set to 0.5. The replacement ratio cis set
to 0.2 following Liu et al. (2021), and the candidate
numberkis set to 1. The details of model conﬁgu-
ration and the number of the generated adversarial
examples are shown in the Appendix B. Following
previous work, the Zh →En performance is evalu-
ated with the BLEU (Papineni et al., 2002) score
calculated by multi-bleu.perl script. For En→De
and En→Fr, we use SacreBLEU (Post, 2018) for
evaluation.4260
4.3 Comparison Methods
To test the effectiveness of our model, we take
both meaning-preserving and meaning-changeable
systems as comparison methods:
Baseline: The vanilla Transformer model for
NMT (Vaswani et al., 2017). In our work, we
use the baseline model to perform the forward and
backward translation in the round-trip translation.
CharSwap: Michel et al. (2019) craft a minor
perturbation on word by swapping the internal char-
acter. They claim that character swaps have been
shown to not affect human readers greatly, hence
making them likely to be meaning-preserving.
TCWR: Liu et al. (2021) propose the approach
of translation-counterfactual word replacement
which creates augmented parallel translation cor-
pora by random sampling new source and target
phrases from the masked language models.
RTT: Zhang et al. (2021) propose to generate ad-
versarial examples with the single round-trip trans-
lation. However, they do not provide any approach
for generating the bilingual adversarial pairs. Tomake a fair comparison, we generate the bilingual
adversarial pairs from their adversarial examples
in the same way as ours.
5 Results and Analysis
5.1 Main Results
Artiﬁcial Noise. To test robustness on noisy in-
puts, we follow Cheng et al. (2018) to construct ﬁve
types of synthetic perturbations with different noise
ratios on the standard test set: 1)Deletion: some
words in the source sentence are randomly deleted;
2)Swap: some words in the source sentence are
randomly swapped with their right neighbors; 3)
Insertion : some words in the source sentence are
randomly repeated; 4) Rep src: short for ‘replace-
ment on src’. Some words in the source sentence
are replaced with their relevant word according to
the similarity of word embeddings; 5)Rep both:
short for ‘replacement on both’. Some words in the4261
source sentence and their aligned target words are
replaced by masked language models.
Table 1 shows the BLEU scores of forward trans-
lation results on Zh →En and En→De noisy test
sets. For Zh→En, our approach achieves the best
performance on 4 out of 5 types of noisy test sets.
Compared to RTT, DRTT achieves the improve-
ment up to 1.1 BLEU points averagely on deletion .
For En→De, DRTT also performs best results on
all types of noise except Rep src . We suppose the
reason is Rep src sometimes reverses the semantics
of the original sentence as we claimed above.
Since the perturbations we introduced above may
change the semantics of the source sentence, it may
be problematic for us to calculate the BLEU score
against the original reference sentence in Table 1.
Therefore, following Zhang et al. (2021), we also
report the BLEU score between the source sentence
and its reconstructed version through the source-
target-source RTT, which is named as RTT BLEU.
The intuition behind it is that: a robust NMT
model translates noisy inputs well and thus has mi-
nor shifting on the round-trip translation, resulting
in a high BLEU between inputs and their round-
trip translation results. Following Zhang et al.
(2021), we ﬁne-tune the backward model (vanilla
Transformer model) with its test set to minimize
the impact of the T2S process. As shown in Ta-
ble 2, DRTT outperforms the meaning-preserving
method and other methods on all types of noise
on Zh→En and En→De tasks. Considering the
results of Table 1 and Table 2 together, DRTT sig-
niﬁcantly improves the robustness of NMT models
under various artiﬁcial noises.
Natural Noise. In addition to the artiﬁcial noise,
we also test the performance of our model on
WMT’19 En→Fr robustness test set which contains4262
various noise in real-world text, e.g., exhibits typos,
grammar errors, code-switching, etc. As shown
in Table 3, DRTT yields improvements of 1.34
BLEU compared to the baseline, it proves that our
approach also performs well in real noise scenario.
Besides, DRTT achieves 0.63 BLEU improvement
over RTT by ﬁltering out 10% of fake adversarial
examples (according to Table 6), which demon-
strates that ﬁltering out fake adversarial examples
further improves the robustness of the model.
5.2 Effectiveness of Adversarial Examples
In this sub-section, we evaluate the effectiveness
of the generated adversarial examples on attacking
the victim NMT model (i.e., the target NMT model
without being trained on the generated adversarial
pairs). In our approach, γin Eq.(4) is a hyper-
parameter to control the strictness of our criterion
on generating adversarial examples. Thus, we eval-
uate the effectiveness of adversarial examples by
studying the translation performance of the victim
NMT model on the set of adversarial pairs gener-
ated with different γ. That is to say, if a sample
is an adversary, it should destroy the translation
performance drastically, resulting in a low BLEU
score between the translation result and its paired
target sentence. The average BLEU scores of the
victim model on the different adversarial pair sets
(generated with γfrom -10 to 1 on NIST 06) are
shown in Figure 3. Speciﬁcally, the average BLEU
on the adversarial sets generated with γ=−10is
8.0. When we remove the restriction of γ, i.e., the
DRTT is degenerated into RTT, the average BLEU
for the constructed adversarial examples reaches
up to 11.2. This shows that the adversarial exam-
ples generated with lower γ(more strict restriction)
attack the model more successfully. Therefore,
we can select more effective adversarial examples
compared to Zhang et al. (2021) by lowering the
thresholdγto create a more strict criterion.
5.3 Clean Test set
Adding a large amount of noisy parallel data to
clean training data may harm the NMT model per-
formance on the clean test sets seriously (Khayral-
lah and Koehn, 2018). In this sub-section, we
test the performance of the proposed model on
the clean test sets and the results are presented in
Table 4. The meaning-preserving method Char-
Swap has negative effect on clean test set while
DRTT achieves the best translation performance
on Zh→En and En→De clean test sets. It demon-
strates that our approach not only improves the
robustness of the NMT model, but also maintains
its good performance on clean test sets.
6 Case Study and Limitations
In Table 5, we present some cases from Zh-En ad-
versarial pairs generated by our approach. From the
case 1, we can see “ 拥护 ” in the source sentence
is replaced by its antonym “ 反对”, which reverse
the meaning of the original sentence, and DRTT
makes a corresponding change in the target sen-
tence by replacing “support” with “oppose”. In the
other case, DRTT replaces “ 良好” by its synonym4263
“不错”, thus, “satisfactory” in the target sentence
remains unchanged. From these cases, we ﬁnd that
DRTT can reasonably substitute phrases in source
sequences based on the contexts and correctly mod-
ify the corresponding target phrases synchronously.
Although the proposed approach achieves
promising results, it still has limitations. A small
number of authentic adversarial examples may be
ﬁltered out when the large d(y,y)is caused
byf(ˆx), we will ameliorate this problem in the
further.
7 Conclusion and Future Work
We propose a new criterion for NMT adversarial
examples based on Doubly Round-Trip Transla-
tion, which can ensure the examples that meet our
criterion are the authentic adversarial examples.
Additionally, based on this criterion, we introduce
the masked language models to generate bilingual
adversarial pairs, which can be used to improve the
robustness of the NMT model substantially. Exten-
sive experiments on both the clean and noisy test
sets show that our approach not only improves the
robustness of the NMT model but also performs
well on the clean test sets. In future work, we will
reﬁne the limitations of this work and then explore
to improve the robustness of forward and backward
models simultaneously. We hope our work will
provide a new perspective for future researches on
adversarial examples.Acknowledgements
The research work descried in this paper has been
supported by the National Key R&D Program of
China (2020AAA0108001) and the National Na-
ture Science Foundation of China (No. 61976016,
61976015, and 61876198). The authors would like
to thank the anonymous reviewers for their valuable
comments and suggestions to improve this paper.
References42644265A Bilingual Adversarial Pair Generation
Algorithm 1: Bilingual Adversarial Pair
Generation
Input: A sequence pair (x,y), a sampling
probabilityc, an alignment mapping
p, candidate words k, masked
language models M-MLM and
T-MLM, thresholds βandγ.
Output: A bilingual adversarial pair
(x,y)Function BilAdvGen( x,y): whilen≤len(x)∗cdor←M-MLM (x);x←Replace (x,r)r←arg max d(x,x)(2);x←Replace (x,r) Get aligned index p(i);w←T-MLM (x,y);y←Replace (y,w)n←n+ 1 endifd(x,x)>β andd(y,y)<γ then returnx,yend
B Implementation Details
As for Zh→En, we apply the separate byte-pair
encoding (BPE) (Sennrich et al., 2016) encoding
with 30K merge operations for Zh and En, respec-
tively, the peak learning rate of 5e-4, and the train-
ing step is 100K. For En →De and En→Fr, we
apply the joint BPE with 32K merge operations,
the learning rate of 7e-4 and the training step is
200K. The dropout ratio is 0.1. We use Adam op-
timizer (Kingma and Ba, 2014) with 4k warm-up
steps. All models are trained on 8 NVIDIA Tesla
V100 (32GB) GPUs.
Method Zh→En En→De En→Fr
original 1252977 5859951 2037962
-CharSwap 1252977 5859951 2037962
-TCWR 1252977 5859951 2037962
-RTT 1236485 2670044 1639661
-DRTT(ours) 956308 2336285 14667564266