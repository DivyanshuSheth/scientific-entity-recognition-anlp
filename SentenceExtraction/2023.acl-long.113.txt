
Shabnam Behzad
Georgetown UniversityKeisuke Sakaguchi
Tohoku University
Nathan Schneider
Georgetown UniversityAmir Zeldes
Georgetown University
Abstract
We present ELQA, a corpus of questions and
answers in and about the English language.
Collected from two online forums, the >70k
questions (from English learners and others)
cover wide-ranging topics including grammar,
meaning, fluency, and etymology. The answers
include descriptions of general properties of
English vocabulary and grammar as well as ex-
planations about specific (correct and incorrect)
usage examples. Unlike most NLP datasets,
this corpus is metalinguistic —it consists of lan-
guage about language. As such, it can facilitate
investigations of the metalinguistic capabilities
of NLU models, as well as educational applica-
tions in the language learning domain. To study
this, we define a free-form question answering
task on our dataset and conduct evaluations on
multiple LLMs (Large Language Models) to an-
alyze their capacity to generate metalinguistic
answers.
1 Introduction
Language is so powerful that it can be reflected
back on itself. Statements like “In informal usage,
asteep learning curve means something that is dif-
ficult (and takes much effort) to learn” or “In some
cases, an adjective has both -ic and -ical forms,
with no difference in meaning” expressly concern
linguistic inventories, structures, and behaviors. In
other words, they are metalinguistic —they use lan-
guage to discuss language (cf. Wilson, 2013). They
may concern a particular instance of language use,
or properties of a language or speaker in general;
either way, they are metalinguistic in making lin-
guistic phenomena the subject matter of a linguistic
utterance. For the rest of this paper, the term meta-
language is used for natural language text in which
natural language is also the subject matter.
While NLP models have become powerful at pre-
dicting text in many settings, it remains to be seen
whether such capability extends to metalanguage—
where linguistic strings are not being deployed tocontribute to the discourse with their normal deno-
tations, but rather, are treated as entities with lin-
guistic properties (e.g., grammar, meaning). One
way this can be explored is in a question answering
framework, which requires suitable datasets, ide-
ally based on questions that are realistic and paired
with high-quality answers.
In this paper, we present a corpus of metalinguis-
tic questions and answers about English. The cor-
pus is collected and carefully processed from two
Stack Exchange forum sites: English Language
& Usage () and English Language Learners
(). It covers more than 70k questions on numer-
ous topics about English such as grammar, mean-
ing, fluency, and etymology along with answers.
Our corpus, ELQA (English Language Questions
and Answers), can serve as a tool to facilitate met-
alinguistic studies. Moreover, since questions in
ELQA cover a variety of topics in English, it can
be used in the educational and English language
learning domains.
As the first case study of ELQA, we investigate
the performance of current state-of-the-art NLP
technology on free-form question answering in the
English language domain. Additionally, we ex-
plore the possibility of building NLP models that
can directly answer questions from language learn-
ers. We process a subset of ELQA and make it
appropriate for this task. Then, we report on the
results of both automatic and human evaluations
using different experimental settings of T5and
GPT-3models. Although most of these models
achieve high ratings for well-formedness, the va-
lidity of their answers is significantly lower than
that of human-authored answers, indicating that
this type of metalinguistic QA task is challenging
even for large language models.
Our main contributions are: 1) we release the2031
first publicly available metalinguistic QA dataset,
focused on the English language; 2) we present a
taxonomy of questions in the corpus along with an-
alysis; and 3) we investigate to what extent LLMs
are able to articulate appropriate generalizations
about language in response to these questions.
2 Related Work
Stack Exchange is a network of numerous CQA
sites (originally and most famously, Stack Over-flow) built on a common platform. Stack Exchange
forums have been featured in a number of previous
datasets (Yao et al., 2013; Hoogeveen et al., 2015;
Ahmad et al., 2018; Penha et al., 2019; Campos
et al., 2020; Kumar and Black, 2020; Rogers et al.,
2023), including the English site (our) along
with others such as Ask Ubuntu ,Android ,Gam-
ingandWordPress (dos Santos et al., 2015; Nakov
et al., 2017). We focus on and as they
concern the English language itself; we show that
these datasets cover a wide range of metalinguistic
questions.
Our use of these forums contrasts with previ-
ous work on metalanguage in corpora, which an-
notated and quantified mentions (Anderson et al.,
2004; Wilson, 2010, 2011, 2012, 2017), but did
not consider entire questions and answers about
language. Taylor (2015) studied metalanguage in
online forums, but with a focus on the usage of met-
alinguistic expressions of mock politeness. More
recently, Bogetic (2021) published the first corpus
of contemporary Slovene, Croatian and Serbian
media metalanguage texts.
So far, metalanguage has not been a focus in the
QA domain—ours is the first publicly available En-
glish metalinguistic QA dataset. Most QA tasks are
set up to have a question and a reference document,
where the objective is to find the answer based
on the document (Fan et al., 2019; Kwiatkowski
et al., 2019). In this paper, we explored a type of
“closed-book” question answering task (Roberts
et al., 2020; Khashabi et al., 2021). To the best of
our knowledge, this task has not been explored to
date within the realm of English language questions2032
that require significant generalization and adapta-
tion rather than looking up facts.
3 Constructing the Dataset
We collect our data from two sites on Stack Ex-
change: English Language & Usage ()and
English Language Learners ().Sample screen-
shots of the site are shown in Figure 1. The Stack
Exchange data is publicly released under the CC-
BY-SA 3.0 license. We preprocessed the data until
2021-12-06 collected from the Internet Archiveto
be suitable for NLP studies and release it as ELQA.
Additionally, some cleanup (e.g., removing posts
marked as “spam” or “offensive”) was done. Fields
for each entry (question) include the title, body,
user bio (if available), score (which is calculated
based on up-votes and down-votes by other users),
tags (user-assigned, related to the area/topic of the
question), favorite count, and a list of answers. Tex-
tual content (body and user bio) is provided in two
formats: HTML and plain text without HTML tags.
We release two versions of ELQA based on dif-
ferent preprocessing steps. In ELQA-large, we
keep questions as long as they don’t include any
images (<img> HTML tag) and have an answer
with a score of at least 2 (meaning at least two peo-
ple other than the user posting the answer found
it helpful). For ELQA-small, we applied further
filtering to ensure that the data has the least amount
of noise: a) questions should have a score of at least2 (ensuring questions are clear and coherent), b)
question has an answer with a score higher than 3
and c) there are no hyperlinks in at least one of the
high-rated answers. The last step reduces noise and
facilitates a fair comparison for the closed-book
question-answering task (§4) with model-generated
answers, as models cannot be expected to have ac-
cess to the web to suggest valid URLs compared to
humans who would search the web for appropriate
resources to include in their answers.
For quality assurance, we also did a human an-
notation on ELQA-small. Two of the authors anno-
tated 250 question and answer pairs for the follow-
ing: 1) Is the question answerable? and 2) Does
the answer fully address the question? We found
99.2% of the questions answerable and 91.8% of
the answers acceptable.
Table 1 contains overall statistics on both ver-
sions. Figure 2 shows the distribution of the 10
most common tags in each of the sites. Since users
assign these tags to their questions (0 to multiple),
similar or near-duplicate tags are common within
the collection. Some form more general and more
fine-grained variants, e.g. ‘meaning’ and ‘meaning-
in-context’. In addition to available user-assigned
tags, we manually inspected a large subset of the
data to identify salient types of questions. These
are defined below and illustrated in Table 2. We
then labeled 100 random questions to get a rough
estimate of their frequencies (two annotators anno-
tated these 100 samples and they agreed on 92% of
cases in an overlapping subset).
•Fluency (≈38% of questions) : Usually asking
about a particular sentence, comparison of multi-
ple sentences, and/or probing how an expression
should be used in general. The user wants to
know if X is correct, or to decide between mul-
tiple choices, which one is correct. “Correct”
could mean grammatical, most natural/idiomatic,
stylistically appropriate, conveying the intended
meaning, etc. In Qs where options are provided
by the user, there are cases in which 1) none of
the choices are correct, 2) multiple choices are
correct, and 3) only one is correct.
•Form to Meaning (Interpretation) (≈19% of
questions) : Questions such as “What does X
mean?” (of an expression in general, or an en-
countered passage) or “What’s the difference in
meaning between X and Y?”.
•Meaning to Form (Encoding )(≈20% of ques-
tions) : In these questions, the user gives some2033
explanation/definition and asks for the term or
for form to express it.
•Grammatical Analysis (≈11% of ques-
tions) : Questions about parts of speech and
other aspects of syntactic analysis. (e.g. “Is
this a verb or an adjective?”; “Can an article
ever go after the noun it modifies?”). Note that
Fluency questions may mention grammatical
terminology, but the grammatical categories are
not the focus.
•Other (≈10% of questions) : Any other type of
question not listed above. This includes ques-
tions about pronunciation, etymology, etc.
As can be seen from the examples in Table 2, it
is common for questions and answers to contain
example usages, often visually distinguished with
Markdown formatting (such as blockquotes, bul-
lets, and italics) which we retain in the processed
corpus markup. Examples can be incorporated into
a post in a variety of ways—e.g., asking for an inter-
pretation of one usage, as in the Form to Meaning
example in Table 2, or contrasting multiple usages
such as in the following question:
Usage examples provided in a question may be
instances that the author encountered “in the wild”
(such as in a novel or film), or in a grammar book
or dictionary, or they may have been constructedby the user. Answers sometimes include examples
found through a corpus search.
4 English Language Question Answering
Large language models can produce output that
is fluent and (at times) informationally adequate
when presented with factual questions about enti-
ties in the world (Roberts et al., 2020). But how do
such models perform when asked questions about
the language itself? In this section, we investigate
the free-form English language question answering
task.
This task has the potential to benefit educational
applications for language learners. Research on
NLP for educational purposes has investigated
tasks such as automated grammatical error correc-
tion (Dale et al., 2012; Ng et al., 2014; Bryant
et al., 2019; Wang et al., 2021, inter alia ), question
and quiz generation for language learning (Sak-
aguchi et al., 2013; Chinkina and Meurers, 2017;
Marrese-Taylor et al., 2018; Vachev et al., 2021),
and automated essay scoring (Burstein, 2003; Farag
et al., 2018, inter alia ). Nevertheless, an applica-
tion that has not been taken up by the educational
NLP community is free-form question answering
about language. Second language learners possess
a degree of metalinguistic awareness about the lan-
guage they are learning, and often turn to teachers
or more advanced speakers with explicit questions
about vocabulary, grammar, and usage. Commu-
nity Question Answering (CQA) websites such as
Stack Exchange have sites for language learners’
questions and answers. These sites require consid-2034
erable effort by volunteers, and learners may have
to wait for an answer—if an answer is provided at
all. In fact, looking at the data from 2021-12-06 for and, 9% of questions have no answers.
4.1 Data
We randomly divided ELQA-small into
train/test/dev splits. This resulted in 21,175
Q&A pairs in the train split and 3,107 Q&A pairs
in each of the dev and test splits. Answers in
these splits have a score of at least 4. If there
are multiple high-rated answers to a question,
we include all of them for training. Some of
these questions can be answered by looking at
a dictionary or vocabulary list for descriptions.
But many of them are explanations in relation to
particular instances of language use and require
significant reasoning rather than looking up facts.
Thus in this setup, we do not have any external
context/reference available at evaluation time,
i.e. this is a closed-book QA task.
The input for the task is Title: [Q title] <sep>
Body: [Q body] . We use the HTML version
of ELQA for this task since metalinguistic men-
tions are usually distinguished via formatting (e.g.,
blockquotes, bullets) and the ultimate goal is a sys-
tem that humans can easily use to get answers to
their language-related questions.
4.2 Setup
We use T5 (Raffel et al., 2020; Roberts et al., 2022)
and GPT-3 (Brown et al., 2020) as our models since
they have been shown to be strong baselines in
other QA domains. We believe the questions in
ELQA offer new challenges for the QA task since
they require different types of knowledge/under-
standing to be able to generate answers. Addition-ally, these questions contain noise (grammatical
errors) and cases of textual metalanguage which is
likely harder to comprehend for a model.
We fine-tune T5-l andT5-xxl for this task.We
saved multiple checkpoints during fine-tuning and
evaluated them with the interpolation of BLEU (Pa-
pineni et al., 2002), BERTScore (Zhang et al.,
2020) and ROUGE (Lin, 2004) on the dev set to
choose the best-performing one (checkpoint at 75k
updates, hyperparameters available in Table 8 in
the Appendix).
With GPT-3 we used text-davinci-003 and ex-
perimented with both fine-tuning (FT) on 100 and
1000 samples and a few-shot (FS) setting in which
the model is given a few demonstrations of the
questions and answers at inference time as condi-
tioning, but no weights are updated (Radford et al.,
2019). In the FS setting, we show the model four
Q&A pairs since we wanted the model to see differ-
ent question types but there were also limits on the
input length. To select these 4 pairs, we randomly
created 5 different sets of Q&A pairs, evaluated
on a subset of dev, and chose the best-performing
set for the experiments (dev results available in
Appendix, Table 9).
4.3 Results
4.3.1 Automatic Evaluation
Results are shown in Table 3. GPT-3 FS outper-
forms all other methods in all metrics with a large
margin except for BLEU Score. We also observed
that using GPT-3 in a few-shot setup worked much
better than the fine-tuned version. Looking at some
of the model-generated answers, we noticed that
the fine-tuned model tends to generate longer an-2035
swers containing redundant text. We observed im-
provements when we used 1000 samples instead of
100 for fine-tuning and hence, fine-tuning on larger
data might result in better performance, however,
we only experimented with 100 and 1000 samples
in this paper due to having limited resources.
Based on Table 3, T5-xxl seems to perform sim-
ilarly to GPT-3 FT-1000 . However, a small man-
ual evaluation showed otherwise ( GPT-3 FT-1000
answers were slightly better). Furthermore, we ob-
serve that the scores for even the best system are
very low, but manual evaluations showed that the
GPT-3 FS generates fairly good answers in many
cases. Due to these observations and also given
the well-known limitations of automatic metrics
for evaluating generation tasks (Kasai et al., 2022;
Celikyilmaz et al., 2020; Bhakthavatsalam et al.,
2021), we believe conducting human evaluation for
deeper analysis is necessary for this task.
In Table 4, we show results for each site to see
if one is more challenging than the other. Overall,
models perform slightly better on based on
automatic metrics—but we see in the next section
(Table 5) that there isn’t really a meaningful differ-
ence between the sites when humans evaluate the
answers.
4.3.2 Human Evaluation
Human evaluators were presented with the question
title and body, and then asked to rate 5 answers:
a top-rated human-provided answer, a low-rated
human-provided answer, and answers generated by
3 of our best models: GPT-3 FS, GPT3 FT-1000,
T5-xxl .They were asked to give ratings (via a slider
widget, on a 1–5 integer scale—the higher, the
better) for two criteria (C1 & C2):
The first criterion aims to get a score for fluency
and coherence and the second one for correctness
and completeness .
We collected ratings for a set of 75 questions
(375 different answers). Each question with its
set of answers was evaluated by at least 2 raters,
and then the average score was calculated based
on their responses.We also report the average
z-score which is calculated over each annotator’s
raw score distribution for each metric, intended to
normalize interannotator variation in how the scale
is interpreted for each of the two metrics (details in
Appendix B).
The results of this study are shown in Table 5.
Overall, answers generated by GPT-3 FS have a
small gap with human answers in both C1 and C2.
GPT-3 FT-1000 comes next, with less accurate an-
swers containing redundant text and hallucinations.
The smallest model, T5-xxl , ranks last.
Rankings based on human evaluations are avail-
able in Table 6. These results are also indicating
that model-generated answers are fluent in most
cases, but they are not as good as human answers2036
when correctness/completeness is considered ( GPT-
3 FS is ranked first or as good as a top-rated human
answer in only 45% of cases).
For each criterion and Top-rated human, Low-
rated human and GPT-3 FS , histograms of the av-
erage score of the two annotators are plotted in
Figure 3. We can observe that GPT-3 FS and Low-
rated human have very similar numbers of high-
scoring answers (human evaluation scores), but
the number of low-scoring human answers drops
off gradually as quality decreases, while the distri-
bution is more spread out for GPT-3 FS. I.e., the
model has some moderately bad answers as well
as some good ones, whereas Low-rated human an-
swers cluster more on the upper end of the scale.
C1 (fluency/coherence). All models generated
fairly fluent and well-structured answers. We
even notice that GPT-3 FS scores higher in well-
formedness than human answers. We looked at
those samples and we believe there are two main
reasons for this: 1) Some human answers were
very long, containing multiple different quotes
from different sources. On average, our evalua-
tors preferred the structure of answers from GPT-3
FS, which took the form of a short paragraph ad-
dressing the question. 2) Some human answers
have a more casual/conversational nature. On theother hand, GPT-3 FS generated more authoritative-
sounding, teacher-like answers with complete sen-
tences, which were preferred by our evaluators in
some cases.
C2 (correctness/completeness). On average,
models are worse on this metric, though sometimes
they did produce acceptable answers (perhaps be-
cause variants of the question are commonly dis-
cussed on the web).
One challenge in this domain is that questions,
particularly from, may not be fully fluent. In
theFluency example from Table 7 we see that there
are some grammatical errors in the question that are
unrelated to the topic of the question. In addition,
the questioner uses incorrect terminology, mention-
ing verb tense in the post title even though the
question is actually about subject-verb agreement
with respect to number (as can be inferred from
the examples). While the good human response
correctly flags the incorrect terminology and an-
swers the underlying question, GPT-3 models give
irrelevant answers about tense.
Another correctness failure from GPT-3 FS can
be seen in the following pair involving a Meaning-
to-Form question:2037
We see that the model begins by ignoring the
user’s explanation that analogously does not have
the desired meaning, and suggests it anyway. The
rest of the model’s answer gives a (valid) general
definition of analogously and clumsily attempts
to apply it to the two kinds of signals, which is
not responsive to the question. It may be the case
that models particularly struggle with questions
for which there is no straightforward solution (in
this case, no adverb derived from analogue has the
user’s intended meaning).
4.4 Discussion
4.4.1 Metrics
Our human evaluations found that the high-rated
human answers are the best, and GPT-3 FS is
roughly on par with the low-rated human answer,
while other models are worse.
As noted in §4.3.1, we did not find the automatic
metrics very informative. We believe this is due to
the long and free-form nature of this task: concepts
can be explained in different ways and answers can
include different examples which make automaticevaluations very challenging. For example, for the
question Why is this sentence: "Additional nine fea-
tures were added. . . " incorrect? a human answer
is:
Model-generated answers mostly mirror the ex-
amples in the question. But answers given by hu-
mans mention new examples—which are not in
the question (and therefore unlikely to come up
in other answers). Thus a good answer may men-
tion superficially off-topic terms metalinguistically,
which presents evaluation challenges unlike those
of summarization or MT, for example (Reiter, 2018;
Celikyilmaz et al., 2020). For QA tasks in metalin-
guistic domains such as ours, caution is warranted
when applying automatic metrics that are unaware
of the use/mention distinction. Human evaluation
is likely to be more illuminating. We release our
full human evaluation data to enable detailed com-
parisons in future research.
4.4.2 Qualitative Analysis
We examined behavioral patterns for the best
model, GPT-3 FS , organized by question type, with
observations as follows.
Interpretation and Encoding: These are the cat-
egories where the model performs the best, espe-
cially if the question has a straightforward answer
(for example, there aren’t several possible answers
based on context). Topics in these categories of-
ten center on definitions of vocabulary and idioms.
These questions may involve less reasoning to an-
swer, and models may be leveraging content from
the online dictionaries and thesauri in their web
training data.2038Grammatical Analysis: When asked about gen-
eral concepts and rules of grammar, the model-
generated answers are usually acceptable:
However, when questions become more specific
to examples, sometimes the model struggles:
The example in the question uses the word in
twice. Rather than addressing the first token, which
the questioner has bolded, the answer addresses
the second one (denoting the physical relationship
between one species and another).
Fluency: The model correctly answers many of
the fluency questions. Others require reasoning and
understanding of context, and the model struggles
to generate a correct answer. For example:
We also observe that when the correct usage of
a case is not mentioned in the question itself, the
model sometimes suggests incorrect corrections
and wrong explanations:
4.4.3 Outlook
Overall, we were surprised by the quality of many
of the answers from GPT-3 FS : many would likely
have received high user ratings if submitted as an-
swers on the site. At the same time, the model is
not to the point where we would want to trust its
answers without human review. We believe that
answer confidence estimation—so that users can be
shown only the best model-generated answers—is
an important direction for using learner QA models
in practice (Jiang et al., 2021).
5 Conclusion
We presented ELQA, a dataset containing metalin-
guistic questions and answers about the English
language. We provided analysis and a taxonomy
of the data, along with experiments on free-form
answer generation and investigated the extent to
which language models can articulate their gener-
alizations about language. Since many of the ques-
tions in ELQA were asked by language learners,
it forms a potentially useful and so far untapped
resource for educational NLP purposes and metalin-
guistic question answering. We release the dataset
to enable further studies of this task.
Ethics Statement
We have released a processed version of an already
public online forum dataset, in a manner consis-
tent with the terms of the license, which require
attribution of all posts (§3). The models we have
presented are intended only as baselines for future
research, not for deployment. Models should be
carefully stress-tested for undesirable heuristics/
biases before deployment. Systems for the gen-
eration task, in particular, would risk misleading
language learners with plausible but incorrect an-
swers, so it is important to not deploy a generation2039system until it is approximately as reliable as exist-
ing non-automated alternatives, and to present the
output with caveats. Potential biases reflecting the
demographics of authors represented in the training
data (in terms of native language, level of English
proficiency, etc.) also need to be considered if mod-
els are deployed for different target populations.
Limitations
One limitation of our dataset, ELQA, is that the cor-
pus only contains questions in English and about
English. However, Stack Exchange has sites with
questions about other languages and our main data
extraction scripts are general enough that they can
be used to create corpora for other sites on Stack
Exchange. Of course, language-specific process-
ing steps, quality assurance and analysis must be
applied before releasing such data.
Most importantly, the models we have presented
here are intended only as baselines for future re-
search, not for deployment. Potential biases re-
flecting the demographics of authors represented
in the training data (in terms of native language,
level of English proficiency, etc.) also need to be
considered if models are deployed for different tar-
get populations. Moreover, many of these types of
questions are found on the web, and a lot of the
same topics are brought up by many users, so a
model’s ability to generate correct answers cannot
necessarily be attributed to abstract reasoning.
Acknowledgements
We thank the anonymous reviewers for their in-
sightful comments. We thank Daniel Khashabi for
helpful discussions and feedback. This research
was supported in part by NSF award IIS-2144881.
References204020412042A Data Credits
The Stack Exchange license requires that any Inter-
net use of the content should include a hyperlink
directly to the original question and the profile of
the authors. Below are URLs for all the examples
used in this paper. The post URL incorporates the
post title.
B On our use of z-scores
In our human evaluation, raters were presented with
a question and five candidate answers and asked torate each on a scale from 1 to 5 for each of our two
criteria (C1 and C2).
Our main goal is to compare the quality of the
answers across 5 conditions (3 systems, 2 posts
from the site). Raters may have different interpre-
tations of the absolute scales—for example, some
raters could be more generous than others overall
in terms of the numerical rating, even if they agree
on the ranking of systems.
There are several possible ways to factor out
this bias. One way is to compute standard scores,
a.k.a. z-scores, for each annotator’s distribution of
responses on each criterion. Consider C1: from
the ratings of an annotator awe have the empirical
distribution
p(y∣x)
where iindexes the items (answers, of which mul-
tiple ones may belong to the same question), and
likewise for C2. For each of these distributions we
fit a normal distribution by computing mean and
standard deviation. For an absolute rating y, its z-
score zis its number of standard deviations above
the mean rating for that annotator on that metric
(a negative z-score indicates it is below the mean).
Averaging the z-scores for a particular condition,
we can see whether annotators tended to rate out-
puts in that condition with higher or lower scores
than the other outputs they saw in the sample. Note
that the z-score computation ignores the grouping
of answers from different conditions into questions,
so it is not directly measuring annotators’ rankings
of candidate answers to a particular question.
C Further Details204320442045ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
7
/squareA2. Did you discuss any potential risks of your work?
6
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3
/squareB1. Did you cite the creators of artifacts you used?
3, Appendix
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
3
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
3
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
3
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
3
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
3.1, 4.1
C/squareDid you run computational experiments?
4.2
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
4.2, appendix2046/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4.2
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
4
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
4, appendix
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
4
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
3,4
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
4 (footnote)2047