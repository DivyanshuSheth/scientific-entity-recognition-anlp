
Jinghui Lu, Linyi Yang, Brian Mac Namee, Yue ZhangThe Insight Centre for Data Analytics, University College DublinSchool of Computer Science, University College DublinSchool of Engineering, Westlake UniversityInstitute of Advanced Technology, Westlake Institute for Advanced StudySenseTime Research
{jinghui.lu, brian.macnamee}@ucd.ie
{yanglinyi, zhangyue}@westlake.edu.cn
Abstract
We present a novel rationale-centric framework
with human-in-the-loop – Rationales-centric
Double-robustness Learning (RDL) – to boost
model out-of-distribution performance in
few-shot learning scenarios. By using static
semi-factual generation and dynamic human-
intervened correction, RDL exploits rationales
(i.e. phrases that cause the prediction), human
interventions and semi-factual augmenta-
tions to decouple spurious associations and
bias models towards generally applicable
underlying distributions, which enables fast
and accurate generalisation. Experimental
results show that RDL leads to significant
prediction benefits on both in-distribution and
out-of-distribution tests compared to many
state-of-the-art benchmarks—especially for
few-shot learning scenarios. We also perform
extensive ablation studies to support in-depth
analyses of each component in our framework.
1 Introduction
Recent work finds that natural artefacts (Guru-
rangan et al., 2018) or spurious patterns (Keith
et al., 2020; Srivastava et al., 2020) in datasets
can cause sub-optimal model performance for
neural networks. As shown in Figure 1, the
bold phrases— “100% bad ”and “brain cell
killing ”—are underlying causes for a negative sen-
timent prediction that most human readers would
recognise. These are defined as rationales in this
paper. The underlined phrase—“ acting and plot ”—
has been incorrectly recognised as a causal term by
the model used fort this example, and is referred
to as a spurious pattern .
Spurious patterns (or associations) are caused
by natural artefacts or biases in training data
(Lertvittayakumjorn and Toni, 2021), and are
usually useless, or even harmful, at test time. This
issue can be severe in few-shot learning (FSL)Figure 1: A negative movie review with human an-
notated causal terms (bold text) and spurious patterns
recognised by the model (underlined text).
scenarios. For instance, Kulesza et al. (2010)
suggests that when a model is trained with a small
subset of labelled data, it is prone to exploiting
spurious patterns leading to poor generalisability
that is evident in the performance decay in out-
of-distribution (OOD) datasets. In spite of these
issues, training deep neural networks using few
labelled examples is a compelling scenario since
unlabelled data may be abundant but labelled data
is expensive to obtain in real-world applications
(Lu and MacNamee, 2020; Lu et al., 2021).
There is a strand of research addressing this sce-
nario that seeks to improve model performance
by“introducing methods and resources for train-
ing models less sensitive to spurious patterns”
(Kaushik et al., 2020). Most of this work relies on
generating counterfactual augmented data (CAD),
either manually (Kaushik et al., 2021) or automat-
ically (Feng et al., 2021; Qian et al., 2021; Yang
et al., 2021, 2020a; Delaney et al., 2021). For ex-
ample, Kaushik et al. (2020) proposed a human-
in-the-loop framework where human annotators
are required to make minimal changes to original
movie reviews to produce sentiment-flipped coun-
terfactual reviews, which enables models to learn
useful associations between input texts and output
labels (Kaushik et al., 2021).
Generating manual counterfactuals, however,
is expensive and time-consuming—Kaushik et al.
(2020) report the cost of revising 2.5kinstances
at over $10,000. On the other hand, fully auto-
matic methods are task-specific and therefore have
weak robustness across domains and less reliabil-6986
ity compared to manual counterfactuals. To ad-
dress these issues, we propose Rationales-centric
Double-robustness Learning (RDL), a human-in-
the-loop framework for data augmentation in a
few-shot setting, which is efficient, robust, model-
agnostic, and general across tasks.
Our main idea is a rationale-centric strategy
for eliminating the effect of spurious patterns by
leveraging human knowledge as shown in Figure 2.
Our double-robustness framework consists of two
main modules. The first is a Static Semi-factual
Generation module that generates a set of semi-
factual data automatically for a given instance by
using human-identified rationales. Such labelling
requires less human input compared to fully
manual counterfactual generation (see Section 3.1).
In contrast with counterfactuals (Roese, 1997) that
rely on what might have been different (i.e. the
label would be changed if certain terms have been
changed), semi-factuals (McCloy and Byrne, 2002;
Kenny and Keane, 2021), as used in our work, aim
to guide a model to identify terms less causally
related to the label (i.e. even if certain terms had
been changed, the label would be kept the same).
Second, we apply a Dynamic Human-intervened
Correction module , where the most salient features
are identified for model predictions over a set of
training examples, and human workers intervene
by checking the correctness of the rationale in case
first-round modifications introduce new artefacts.
We evaluate the two modules in a few-shot setting,
where a minimum number of training instances are
labeled for maximum generalisation power, both
for in-distribution and OOD predictions.
Results on a sentiment analysis task, which isalso used in Kaushik et al. (2020), demonstrate that
the double-robust models can be less sensitive to
spurious patterns. In particular, models trained with
RDL with only 50 labelled examples achieve the
same or even better results than fully-supervised
training with a full training set of 1,707 examples,
and improvements are especially significant for
OOD tests. The predictive model trained with RDL
using only 100 labelled examples outperforms mod-
els trained with manual (Kaushik et al., 2020) and
automatic CAD (Yang et al., 2021) using the full
augmented training set of 3,414 examples.
To the best of our knowledge, we are the first
to exploit the efficacy of semi-factuals and human-
intervention for improving the generalisation abili-
ties of deep neural networks in few-shot learning
scenarios.
2 Related Work
Data augmentation has been used for resolving
artefacts in training datasets before (Gururangan
et al., 2018; Srivastava et al., 2020; Kaushik et al.,
2021). In particular, previous work (Kaushik et al.,
2020) relied on large-scale crowd-sourcing to gen-
erate useful augmented data. More recently, Yang
et al. (2021), and Wang and Culotta (2021) inves-
tigated the efficacy of the automatically generated
counterfactuals for sentiment analysis. Similar to
our work, these methods also consider the most
salient features that a model uses when generating
augmented data, which is in line with our ratio-
nale definition. However, they use sentiment lexi-
con matching for identifying rationales, which is
task-specific and not necessarily fully relevant. In
contrast, we employ human annotators to identify
rationales, which can be task-agnostic and robust.
Moreover, our method generates semi-factuals in-
stead of counterfactuals used in previous work.
Human-the-loop Machine Learning (Wu et al.,
2021) has received increasing research attention.
Active learning (Settles, 2009; Margatina et al.,
2021), the most common example of human-in-the-
loop machine learning, asks human annotators only
to provide high-level annotations (i.e. labels) for
important examples. There is also some work ex-
ploring more explainable AI systems by exploiting
feature-based information. Such methods use rela-
tively simple models such as Naïve Bayes (Stumpf6987et al., 2009; Kulesza et al., 2015) and Linear Re-
gression with bag-of-words features (Jia and Liang,
2017; Teso and Kersting, 2019; Ghai et al., 2021;
Shao et al., 2021), because these classifiers are
relatively intuitive in generating explanations and
amenable to incorporating human feedback.
Some other work uses simple neural networks
such as multi-layer perceptrons (Shao et al., 2021)
and shallow CNNs (Lertvittayakumjorn et al.,
2020; Stammer et al., 2021; Teso et al., 2021) be-
cause the predictions of such models can be ex-
plained in the form of features. Very recently, Yao
et al. (2021) proposed a human-in-the-loop method
to inspect more complicated models (e.g. BERT)
with the help of model-agnostic post-hoc explana-
tion algorithms (Ribeiro et al., 2018) that can ex-
plain predictions of any linear or non-linear model
without exploiting its weights. However, previous
work focuses on increasing the explainability of AI
systems for high-stakes domains such as health and
finance (Li et al., 2020; Yang et al., 2020b), instead
of improving model robustness or generalisation
ability. Also, they assume access to a large amount
of labelled data. In contrast, we focus on few-shot
learning scenarios which are more compelling.
3 Method
The RDL pipeline is shown in Figure 2 and consists
of two modules: Static Semi-factual Generation
andDynamic Human-intervened Correction .
Static semi-factual generation is a more efficient
alternative to manually generated counterfactuals
(Kaushik et al., 2020). In the first phase, Rationale
Marking (Section 3.1), human annotators review
each document in the training set to provide ratio-
nales (i.e. phrases that support the document clas-
sification decisions shown as bold text in Figure
2). The second phase is a semi-factual generation
method based on synonym replacement (Section
3.2) that produces augmented examples (blue text
in Figure 2 indicates replaced words), which are
added into the training set.
Dynamic human-intervened correction (Section
3.3) is a rationales-powered human-in-the-loop
framework to dynamically correct the model’s be-
haviours. At the outset, sampling and sensitivity of
contextual decomposition (SCD) (Jin et al., 2019) is
applied to detect the rationales given by the model
that is obtained in the previous step. Then, all
model-identified rationales (underlined texts in Fig-
ure 2) are examined by human annotators to iden-tifyfalse rationales (i.e. words or phrases that do
not support the classifications but are falsely in-
cluded by the model) and missing rationales (i.e.
words or phrases that support the classifications
but are not included by the model). Both false
rationales and missing rationales are corrected to
produce augmented examples. Finally, newly gen-
erated examples are added into the training set to
re-train the deep learning model.
3.1 Rationale Marking
Following Kaushik et al. (2020) and Yang et al.
(2021), we use the IMDb movie review dataset
(Maas et al., 2011) in our experiments. It consists
of positive and negative movie reviews that are easy
for human participants to understand, re-annotate,
and provide feedback upon (Zaidan et al., 2007).
We use a crowdsourcing company to recruit
editors and annotators for marking rationales that
support classification decisions. At the outset,
annotators were given instructions and examples
that gently guided them to annotate rationales.
Only adjectives, adverbs, nouns, and verbs were
considered as rationales. Besides, rationales were
required to carry complete semantic information.
For example, for a phrase starting with a negation
word such as “ not great ”, annotators are instructed
to mark the whole phrase “ not great ” as a rationale
instead of just marking “ not”. We also limited
rationales to at most three consecutive words
(i.e. unigrams, bigrams and trigrams). Phrases
consisting of numerical scores are not counted
as rationales (e.g. 5 or 10 stars) since different
datasets may use different rating scales, and
annotating digits may hurt OOD performance.
Overall, we encouraged annotators to try their
best to mark as many rationales as possible to ex-
plain classification labels. However, to guarantee
the quality of rationale marking and prevent anno-
tators from over including non-rationales for more
payment, we also manually inspected annotated
examples and rejected examples that contained in-
correct rationales. After inspection, we rejected
10.6% of negative reviews and 7.6% of positive
reviews. Editors and annotators re-annotated the
rejected examples, which were then presented to
us for another inspection. All re-annotated exam-
ples were approved only if all authors were happy
with the quality of the annotations. Otherwise, the
examples were re-annotated again.
Our annotation procedure generated 5,0736988rationales in 855 movie reviews involved in Section
3.1 and 3.3 (note that we did not annotate all 1,707
examples in the training set because only 855 exam-
ples were necessarily involved in our experiments).
Human annotators spent on average 183.68 seconds
to identify rationales in a review and our method
generated semi-factual examples automatically. On
the contrary, workers spent on average 300 seconds
to revise a review to generate a counterfactual man-
ually as reported by Kaushik et al. (2020). Note that
our approach using 100 labelled examples can out-
perform manual CAD (Kaushik et al., 2020) using
the entire training set of 1,707 examples (see Sec-
tion 5.3), making our approach≈27.88
times more efficient than manually generated CAD.
3.2 Static Semi-factual Generation
We take a simple replacement strategy, which has
been taken by Yang et al. (2021), to generate semi-
factual examples. Given a human-identified ratio-
nale, our method constructs augmented examples
by automatically replacing non-rationale words,
thus leading to examples with the same labels. This
augmentation is consistent with semi-factual think-
ing: even if those non-rationales were changed, the
label would not change.
Formally, given a training example
x= [t, t, ..., t](where tis the jtoken
of the idocument) and its ground truth label y,
we create a rationale vector r= [a, a, ..., a]
where ais the value that indicates whether tis
a rationale or not (we set a= 1to indicate that
tis a rationale and 0otherwise). To generate a
semi-factual example, x, we randomly replace a
certain number of non-rationales (where a= 0),
except for punctuation, with synonymous terms.
The synonyms can be provided by a human,
retrieved automatically from a lexicon such as
WordNet (Miller, 1995), or generated using the
mask-filling function of a pretrained context-aware
language model (Liu et al., 2019).
In our experiments, we randomly replace 5% of
non-rationales using mask-filling and generate a
set of augmented examples, x, with some replaced
non-rationales and all the other tokens identical to
x. The label, y, of a newly generated example
is the same as the label of the original example,
x. Examples of generated data are shown in Table
1. Afterwards, the augmented examples are added
into the training set used to train the model.3.3 Dynamic Human-intervened Correction
Dynamic human-intervened correction further im-
proves the robustness of the model by allowing
human annotators to correct the model rationales
online. Firstly, SCD is applied to detect unigrams,
bigrams or trigrams that are salient to the model.
SCD is a technique to assess the importance of
terms by continuously removing terms and mea-
suring changes in prediction (Jin et al., 2019). Hu-
man annotators examine all rationales given by the
model from all documents to discover two types
of incorrect rationale: false rationales and missing
rationales. The next phase allows human feedback
to influence the learning process. To this end, for
each type of incorrect rationale, we propose a cor-
responding strategy to correct them.
For false rationales (i.e. phrases that actually do
not support classifications but are incorrectly iden-
tified by the model), we use synonym replacement
again to generate semi-factual examples. Unlike
the static semi-factual generation (Section 3.2), in
this component we replace all false rationales with
their synonyms instead of randomly replacing 5%
of non-rationales in a document. Examples of gen-
erated data are shown in Table 2.
For missing rationales (i.e. phrases that actually
support classifications but are not identified by the
model), we take another simple semi-factual gener-
ation strategy, that is, extracting sentences that con-
tain missing rationales to form semi-factual data.
Specifically, given a sentence containing missing
rationales, we use this sentence as a new example,
and the label of this newly generated example is
identical to that of the document where the sentence
is extracted. For example, there is a positive movie
review (bold font for rationales) “Robert Urich was
afineactor, and he makes this TV movie believable
. I remember watching this film when I was 15 .... ” .
The model fails to identify “fine”and“believable ”
as rationales. Thus we extract the text ““Robert
Urich was a fineactor, and he makes this TV movie
believable . ”as a new example, and the class of
this example is still positive. We extract the whole
sentence rather than just the missing rationales to
reserve more semantic information.
Note that the two correction methods in dynamic
human-intervened correction can operate in parallel
and the generated examples are added to the small
training set to re-train the model.6989
4 Why Does RDL Work?
Broadly speaking, our RDL framework takes ad-
vantage of invariance that makes a model less sen-
sitive to non-rationale words or spurious patterns
(Tu et al., 2020; Wang et al., 2021) in favour of fo-
cusing on useful mappings of rationales to labels.
More specifically, by using static semi-factual
generation (Section 3.2) and false rationale correc-
tion (Section 3.3), we expect to break spurious asso-
ciations. For example, if a model incorrectly deter-
mines that “ Soylent Green ” is associated with pos-
itive sentiment (Table 2), the augmented examples
that replace “ Soylent Green ” with other phrases
such as “ Gang Orange ” break the spurious asso-
ciation. Besides, using synonym replacement can
generate examples that are similar to the original
one, which is equivalent to adding noisy data to pre-
vent models from overfitting (Wei and Zou, 2019).
Missing rationale correction (Section 3.3) em-
phasizes the ground truth associations between ra-
tionales and labels, enabling the model to better
estimate the generally useful underlying distribu-
tions for OOD datasets, even in few-shot learning
scenarios. In the next section, we present experi-
ments and empirical evidence to demonstrate the
utility of the proposed RDL framework in improv-
ing model robustness.
5 Experiments
Our intention is to improve the generalisability of
models, and we use both in-distribution and OODperformance for evaluation. Our experiments are
designed to address the following research ques-
tions:
•RQ1 Can we use static semi-factual genera-
tion to achieve better in-distribution and OOD
performance?
•RQ2 Does dynamic human-intervened correc-
tion improve generalisability of models?
5.1 Datasets
For fair comparison with previous work (Kaushik
et al., 2020; Yang et al., 2021), we use the IMDb
sentiment classification dataset (Maas et al., 2011)
as the in-distribution dataset. Following Kaushik
et al. (2020), all models were trained with the IMDb
dataset predefined training, validation and test par-
titions containing 1,707,245, and488reviews re-
spectively and an enforced 50:50 class ratio.
To measure the generalisation ability of different
models, we focus on OOD performance. To this
end, we test models on another four binary senti-
ment classification datasets: the sampled Amazon
reviews dataset (Ni et al., 2019) (100,000 positives
and 100,000 negatives) from six genres: beauty,
fashion, appliances, gift cards, magazines, and soft-
ware; the Yelp review dataset (Zhang et al., 2015)
(19,000 positives and 19,000 negatives); the SST-2
dataset (Socher et al., 2013) (1,067 positives and
1,143 negatives), and the SemEval-2017 Twitter
dataset (Rosenthal et al., 2017) (2,339 positives6990
and 2,339 negatives). These datasets were sampled
to ensure a nearly 50:50 class balance.
5.2 Evaluating Static Semi-factual Generation
To address RQ1 , we compare the performance of
models trained by the static semi-factual genera-
tion strategy with models trained with the original
50 examples, referred to as Static . We also com-
pare to a model trained with the full training set
(1,707 labelled examples), referred to as Full.
5.2.1 Experiment Setup
To simulate the few-shot training scenario, we
randomly sample 50 examples (we also forced
a 50:50 class balance) from the IMDb dataset as
training data. For each experiment, the training is
repeated 10 times with training datasets sampled
by 10 different random seeds. We report the
average result of these 10 repetitions and use
accuracy to measure the classification performance.
Our experiments rely on an off-the-shelf cased
“RoBERTa-base” model implemented by Hugging
Faceto either perform mask-filling to provide
synonyms or as a predictive model. Following
Kaushik et al. (2020), we fine-tune RoBERTa for
up to 20 epochs and apply early stopping with
patience of 5 (i.e. stop fine-tuning when validation
loss does not decrease for 5 epochs).
We also explore the impact of the number of
semi-factual examples on model performance. To
this end, we conduct static semi-factual generation
with a different number of augmented examples for
each instance: {3, 7, 11, 15, 19, 23}. Considering
we have 50 original examples, this would result
in {150, 350, 550, 750, 950, 1,150} additional
examples in the training set, respectively (we callthisStatic+ n, where nis the number of generated
semi-factuals).
We use the Adam optimizer (Kingma and Ba,
2014) with a batch size of 4. We found that setting
the learning rate to {5e-5, 5e-6 and 5e-6} could
optimise Static, Static+ n, and Full, respectively.
5.2.2 Results and Analysis
As shown in Table 3, all static semi-factual genera-
tion (Static+ n) methods can outperform the base-
line method (Static) in both in-distribution and
OOD tests, demonstrating the utility of static semi-
factual generation. Among all Static+ nmethods,
Static+350 seems the best-performing method and
exceeds Static with a 1.56% in-distribution im-
provement in average accuracy. Static+350 also
outperforms Static with 3.26%, 1.97%, 1.5%, and
0.46% OOD improvement in the SemEval-2017 ,
SST-2 ,Yelp andAmazon datasets respectively. Al-
though the improvement on the Amazon dataset
appears modest, given that there are 200,000 exam-
ples in the Amazon test set, this actually stands for
nearly 1,000 documents being correctly classified.
The Static+ nmethods can even outperform Full
(i.e. normal training with the full training set) on
theSemEval ,SST-2 , and Amazon datasets and are
comparable on the Yelp dataset. The performance
of models with the full training set is best on the
in-distribution dataset but the worst on the SemEval
dataset, which can be caused by the big differ-
ence between underlying distributions of these two
datasets. In other words, a model that fits well with
one dataset can cause performance decay on oth-
ers. In this case, training with a smaller training
set is more likely to reduce overfitting with the in-
distribution dataset and fit well with the SemEval
dataset, which explains the big improvement. It is
interesting to note that models trained with the en-6991tire training set perform slightly better on the OOD
Yelp dataset (93.66) than on the in-distribution
dataset (93.23), which could also be explained
by the high similarity between the underlying dis-
tributions of these two datasets.
Benefits of Static Semi-factual Generation
First, we test whether the improvement in model
performance is brought about by static semi-factual
generation (Static+ n) or simply by an increase
in the size of the training set. We compare
Static+350 (due to its relatively good performance)
with another baseline called Duplication ( DP
heareafter). We multiply the original training set
(50 examples) up into 400 examples identical
to the size of the training set of Static+350, and
fine-tune RoBERTa on this dataset with the same
hyperparameters as Static+350.
As shown in Table 3, in most cases, DP un-
derperforms other algorithms and is even worse
than Static, demonstrating that solely increasing
the dataset size cannot improve the performance.
We believe that the duplication of original examples
increases the risk of overfitting and easily magnifies
artefacts or spurious patterns hidden in the small
training set, which leads to worse models.
Second, synonym replacement has been used
previously for data augmentation (Wei and
Zou, 2019), and we compare static semi-factual
generation with simply replacing any words (i.e.
both rationales and non-rationales). Following
Wei and Zou (2019), we replace 5% of words
at random and set the training set size to 400 to
ensure fair comparison (we use RoBERTa and the
same hyperparameters of Static+350). We call this
Random Replacement ( RRhereafter).
As shown in Table 3, RR is slightly better than
the baseline Static approach. This result is similar
to that reported in Wei and Zou (2019), since the
augmented data generated by random replacement
is similar to the original data, introducing noise
that helps prevent overfitting to some extent. How-
ever, the magnitude of improvement of the Static+ n
method is much larger than that of RR, demonstrat-
ing the utility of only replacing non-rationales to
generate semi-factuals. These observations show
that the model trained with Static+ ndoes improve
both in-distribution and OOD performance, and
the improvement is actually derived from static
semi-factual generation.
5.3 Evaluating Dynamic Human-intervened
Correction
As shown in Table 3 and Figure 3, the performance
gain of static semi-factual generation (Static+ n)
marginalises when augmented data is increased.
Using too much augmented data even hurts the
Static+1150 performance. This observation is con-
sistent with existing work on data augmentation
(Wei and Zou, 2019). We believe one reason could
be that the use of static augmented examples could
also introduce new spurious patterns that degrade
model performance, necessitating a method that ex-
ploits rationales without generating too many aug-
mented examples. Human-in-the-loop can address
this issue by dynamically correcting the model.
To address RQ2 , we compare the performance
of models trained by dynamic human-intervened
correction with a popular few-shot human-in-the-
loop learning framework, Active Learning, as well
as two other state-of-the-art CAD-based methods
(Kaushik et al., 2020; Yang et al., 2021). Lastly, we
provide an ablation study to examine the influence
of different correction methods, as well as an analy-
sis regarding model sensitivity to spurious patterns.
5.3.1 Experiment Setup
We build up an active learning procedure as a base-
line based on the model trained with Static. In
particular, we select another 50 examples by Un-
certainty Sampling (i.e. prediction scores for two
classes in these examples were close) and add
them into the training set (called ALhereafter).
The training set size of the baseline becomes 100.
The best performing static semi-factual generation
method Static+350 is also listed as a baseline.
For fair comparison, we also use Uncertainty
Sampling to select another 50 examples (i.e. 100
original examples in the training set now) for the
proposed dynamic human-intervened correction in-6992
cluding both False Rationale Correction and Miss-
ing Rationale Correction (called Dynamic ). For
Dynamic, we control the number of augmented
examples for each review to 7 (4 from Missing
Rationale Correction and 3 from False Rationale
Correction), resulting in 800 examples in the train-
ing set. For Automatic CAD (Yang et al., 2021) and
Manual CAD (Kaushik et al., 2020), we use the en-
tire training set to produce counterfactuals to build
up two challenging baselines (one counterfactual
for one example, which is limited by the method),
resulting in 3,414 examples in the training set.
To investigate the influence of each correction
method, we also construct another two datasets that
augment the same 100 original examples to 800 ex-
clusively by False Rationale Correction ( Dynamic-
FRhereafter) and Missing Rationale Correction
(Dynamic-MR hereafter). Again, experiments all
rely on a RoBERTa model and all hyperparameters
are identical to those described in Section 5.2.1,
except for the learning rate of AL which is set to
1.25e-5 (we found this value optimised AL perfor-
mance).
5.3.2 Results and Analysis
As shown in Table 4, both AL and Dynamic out-
perform Static in in-distribution and OOD datasets
which makes sense, because we use Uncertainty
Sampling to add new labelled data to minimise
model uncertainty and increase model performance.
However, AL fails to compete with Static+350
even if more original data is added, which again
demonstrates the utility of static semi-factual gen-
eration. On the contrary, Dynamic does better
than Static+350 with a 0.68% in-distribution im-
provement in average accuracy. Dynamic also out-
performs Static+350 with 1.14%, 0.16%, 0.42%
OOD improvement in the SST-2 ,Yelp andAma-
zondatasets, but no improvement for the SemEval
dataset. Finally, the performance of our methods is
better that the state-of-the-art manual CAD method
in few-shot learning scenarios on all OOD datasets.
Overall, these observations demonstrate that ap-
plying dynamic human-intervened correction (i.e.
Missing Rationale Correction and False Rationale
Correction) can further increase the robustness of a
model on generalisation ability, effectively avoid-
ing the improvement marginalisation caused by the
increased volume of augmented data.
Missing Rationales vs. False Rationales
We conduct an ablation study by examining the
performance of Dynamic-MR and Dynamic-FR in
Table 4. Interestingly, Dynamic-FR is specifically
good at improving model performance on the
in-distribution and SemEval datasets while
Dynamic-MR does a good job on the SST-2 dataset.
We believe that it is because Dynamic-MR biases
the model to estimate an underlying distribution
that is useful for SST-2 and in-distribution datasets,
while Dynamic-FR biases the model to estimate
a distribution similar to SemEval dataset. The
performance of Dynamic can be explained as a
compromise of two correction methods.
Sensitivity to Spurious Patterns
We conduct an analysis to explore whether the
double-robust models are less sensitive to spurious
patterns. We compute models mean sensitivity
to all rationales and non-rationales through SCD
in the IMDb test set. As shown in Table 5,
the corrected model is much more sensitive to
rationales with 13.9% average increase in the6993sensitivity to rationales, which demonstrates that
our double-robust method can decouple models
from spurious patterns.
6 Conclusion
We proposed a rationale-centric human-in-the-loop
framework, RDL, for better model generalisability
in few-shot learning scenarios. Experimental re-
sults show that our method can boost performance
of deep neural networks in both in-distribution
and OOD datasets and make models less sensitive
to spurious patterns, enabling fast generalisation.
In the future, we expect to see rationale-centric
frameworks defined for different tasks, including
NER, question answering, and relation extraction.
7 Ethical Statement
We honor the ACL Code of Ethics. No private data
or non-public information was used in this work.
All annotators have received labor fees correspond-
ing to the amount of their annotated instances.
Acknowledgements
We acknowledge with thanks the discussion with
Chenyang Lyu from Dublin City University, as well
as the many others who have helped. We would
also like to thank anonymous reviewers for their in-
sightful comments and suggestions to help improve
the paper. This publication has emanated from re-
search conducted with the financial support of the
Pioneer and "Leading Goose" R&D Program of
Zhejiang under Grant Number 2022SDXHDX0003
and Science Foundation Ireland (SFI) under Grant
Number [12/RC/2289_P2]. Yue Zhang is the corre-
sponding author.
References699469956996