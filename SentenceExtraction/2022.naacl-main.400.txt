
Yibo Hu, MohammadSaleh Hosseini, Erick Skorupa Parolin,
Javier Osorio, Latifur Khan, Patrick T. Brandt, Vito J. D’OrazioThe University of Texas at Dallas,The University of Arizona
{yibo.hu,seyyedmohammadsaleh.hosseini,erick.skorupaparolin,
lkhan,pbrandt,dorazio}@utdallas.edu,josorio1@email.arizona.edu
Abstract
Analyzing conflicts and political violence
around the world is a persistent challenge in the
political science and policy communities due
in large part to the vast volumes of specialized
text needed to monitor conflict and violence
on a global scale. To help advance research
in political science, we introduce ConfliBERT,
a domain-specific pre-trained language model
for conflict and political violence. We first
gather a large domain-specific text corpus for
language modeling from various sources. We
then build ConfliBERT using two approaches:
pre-training from scratch and continual pre-
training. To evaluate ConfliBERT, we collect
12 datasets and implement 18 tasks to assess
the models’ practical application in conflict re-
search. Finally, we evaluate several versions
of ConfliBERT in multiple experiments. Re-
sults consistently show that ConfliBERT outper-
forms BERT when analyzing political violence
and conflict. Our code is publicly available.
1 Introduction
The study of political violence is a central concern
of conflict scholars and security analysts in the
academic and policy communities. For decades,
scholars and governments have devoted incalcu-
lable resources to monitoring, understanding, and
predicting the dynamics of social unrest, political
violence, and armed conflict worldwide. Conflict
research is a sub-field of political science that ana-
lyzes a broad scope of interactions between govern-
ment agents, their challengers, and the civilian pop-
ulation, including material and verbal conflict and
cooperation. Conflict research covers protest, riots,
repression, insurgency, civil war, terrorism, human
rights, genocide, criminal violence, forced displace-
ment, conventional and unconventional warfare, nu-
clear deterrence, peacekeeping, diplomatic disputes
and cooperation, among others.Traditionally, researchers used manual coding to
track conflict processes worldwide (Raleigh et al.,
2010). Unfortunately, the high costs and slow pace
of domain experts conducting these tasks make
it extraordinarily difficult and costly to monitor
highly complex and rapidly changing conflicts in
an ever-growing volume of information available
on a global scale. Furthermore, these efforts tend
to focus on quantifying particular types of conflict
events between specific kinds of actors (Sundberg
and Melander, 2013).
Initial efforts to address these challenges mo-
tivated political scientists to develop automated
systems to classify or extract structured event data
from news articles (Bond et al., 2003; Boschee
et al., 2016; O’Brien, 2010; Osorio and Reyes,
2017; Schrodt, 2006, 2009; Alliance, 2015; Norris
et al., 2017; Lu and Roy, 2017; Ward et al., 2013).
These systems capture a broader range of event
types, including different conflict and cooperation
events, between a larger number of political ac-
tors. They can also extract volumes of data that are
orders of magnitude greater than manual coding ef-
forts. Automated event data such as the Integrated
Crisis Early Warning System have been used for
conflict forecasting and other kinds of research in
political science (Bagozzi et al., 2021; Beger et al.,
2016; Brandt et al., 2022).
However, these existing systems rely on dated
pattern matching techniques and large dictionar-
ies, which often yield low-accuracy results and are
too costly to maintain. Recent efforts by politi-
cal scientists employ traditional machine learning
(Hanna, 2017; Osorio et al., 2020) and deep learn-
ing (Beieler, 2016; Radford, 2020b; Glavaš et al.,
2017; Skorupa Parolin et al., 2020) to analyze po-
litical conflict and violence. Standard supervised
learning requires labeled data, which are expensive
to obtain due to the expertise required for quality
annotation. This led conflict scholars to seek alter-
native solutions based on the latest developments5469in natural language processing (NLP).
Recent progress in NLP has been driven by
pre-trained transformer language models (Vaswani
et al., 2017; Radford et al., 2019; Devlin et al.,
2018; Yang et al., 2019). Self-supervision using
large-scale unlabeled text can significantly alleviate
the annotation bottleneck using transfer learning.
The training parallelization of transformers also
improves their efficiency on large datasets. As a
result, the use of powerful computational devices
and the advantage of transformer structures make
large-scale language models’ pre-training possi-
ble. Furthermore, the introduction of extensive
benchmarks (Wang et al., 2018, 2019; Rajpurkar
et al., 2018; Lai et al., 2017) validates the signifi-
cant improvement of pre-trained language models
on various downstream tasks.
While many language models are built on gen-
eral domain corpora, such as Wikipedia, Book-
Corpus (Zhu et al., 2015), and WebText (Radford
et al., 2019), recent works show that pre-training
on domain-specific corpora can boost downstream
performance on those domains (Lee et al., 2019;
Gururangan et al., 2020). Domain-specific work
in bio-medicine focuses not only on developing
pre-trained models (Lee et al., 2019; Beltagy et al.,
2019; Alsentzer et al., 2019; Lewis et al., 2020;
Gu et al., 2021) but also on proposing domain-
relevant evaluation benchmarks (Peng et al., 2019;
Gu et al., 2021). Pre-training models also have
advanced research in other domains such as aca-
demic papers (Beltagy et al., 2019) and legal stud-
ies (Chalkidis et al., 2020). Despite some efforts
to apply transformers-based approaches in political
science (Büyüköz et al., 2020; Olsson et al., 2020;
Örs et al., 2020; Radford, 2020a; Halterman and
Radford, 2021; Hürriyeto ˘glu et al., 2021; Parolin
et al., 2021a, 2022), we are unaware of any stud-
ies that develop and evaluate domain-specific pre-
trained language models for political science or
conflict research.
By combining the expertise of conflict schol-
ars and computer scientists, we developed Con-
fliBERT, a pre-trained language model designed
for conflict and political violence. ConfliBERT
improves downstream tasks for conflict research
while significantly alleviating the annotation bot-
tleneck. We expect it to support a broad commu-
nity of academic and policy researchers, enabling
the analysis of conflict processes using a domain-
specific NLP tool that yields accurate and validresults at minimum operational cost. Our paper
provides the following key contributions: (1) We
curate a large domain-specific corpus for language
modeling in the domain of political violence, con-
flict, cooperation, and diplomacy. (2) Based on our
domain-specific corpora, we devise a pre-trained
language model, ConfliBERT, and make it avail-
able to the general public, which directly benefits
the political science and policy communities. (3)
To evaluate our model in practical applications, we
collect 12 datasets and conduct 18 tasks relevant to
conflict research. We are the first to carry out such
a comprehensive evaluation of language models for
conflict studies. (4) We evaluate different versions
of ConfliBERT and show it outperforms models
trained on generic domains. We also perform in-
depth analyses of different tasks to investigate the
factors affecting the performance.
2 Preliminaries
Recent pre-trained transformer language models,
such as the Bidirectional Encoder Representation
from Transformers (BERT) (Devlin et al., 2018),
follow a two-steps framework: (1) pre-train on a
large unlabeled corpus; and (2) fine-tune on task-
specific labeled data. These models learn seman-
tics during the pre-training step and require smaller
labeled data to significantly improve their perfor-
mance on downstream tasks. The fine-tuning step
requires minor network modifications to create
state-of-the-art models for various tasks.
Technically, BERT uses the multi-layer, multi-
head self-attention mechanism, which provides sub-
stantial advantages for language modeling, such as
allowing parallel GPU computation and capturing
long-range dependencies. This allows to efficiently
pre-train large language models on large corpora
using powerful devices.
Another key element of BERT-like models is the
design of self-supervision tasks. Self-supervision
refers to generating labels for unlabelled data and
using them to train a model in a supervised man-
ner. BERT uses two self-supervision tasks during
pre-training. On the one hand, masked language
model (MLM) is a fill-in-the-blank task based on
randomly masking a token and then using the sur-
rounding words to predict the word hidden behind
the mask. On the other hand, next sentence pre-
diction (NSP) determines whether one sentence
follows another one in the same document.
Recent works also propose variants of self-5470supervision tasks. For example, Joshi et al. (2020)
mask out contiguous sequences of tokens to im-
prove span representations. Clark et al. (2020) use
replaced token detection, where the model distin-
guishes real input tokens from plausible but syn-
thetically generated replacements. However, Liu
et al. (2019) prove that MLM is competitive with
other recently proposed training objectives with
more data and improved training strategies.
Finally, most BERT-like models focus on a
generic domain, such as Wikipedia, BookCor-
pus (Zhu et al., 2015) and WebText (Radford et al.,
2019). However, BERT without domain adaptation
tends to underperform in target domains with dis-
tinct characteristics such as specialized vocabulary,
language style, and specific semantics. Our domain
includes political violence, armed conflict, interna-
tional cooperation, and diplomacy—all of which
have these characteristics. This performance gap
is the primary motivation for developing domain-
specific language models.
Specifically, the language of political actors in-
volves strategic and complex semantics. Policy
positions that show support for one actor while
also threatening another are sometimes embedded
in simple statements. For example, “NATO will not
tolerate this aggression” mixes a negation, a condi-
tional, and the action of potential interest. Signals
are highly context-dependent, adapted for a target
audience, and vary in strength depending on the
specific actor sending the signal (McManus, 2017;
Blankenship, 2020). Compared to a generic lan-
guage model, we expect ours to incorporate impor-
tant contextual information and learn more about
the strategic ways that political actors convey in-
formation. This context and the related political
biases in it are exactly a need that ConfliBERT
aims to fulfill. Political conflict and violence text
gains from this domain knowledge: one political
actor’s definition of “rebels” is another’s “freedom
fighters”.
Table 1 summarizes various recent domain-
specific BERT models, including our model, Con-
fliBERT. These models mainly differ in their cor-
pora and pre-training strategies, including: (1) con-
tinuing pre-training ( Cont ); and (2) pre-training
from scratch ( SCR ). In the next section, we elabo-
rate on the strategies and our method of developing
ConfliBERT in the domain of political conflict and
violence.
3 Approach
As described in Section 2, MLM-based BERT
achieves competitive performance among other
transformer models with different self-supervision
tasks. Besides, BERT has been validated in vari-
ous domains (Lee et al., 2019; Beltagy et al., 2019;
Peng et al., 2019; Chalkidis et al., 2020; Gu et al.,
2021) shown in Table 1. Therefore, we develop
our domain-specific model based on BERT. The
key components of developing and validating our
model, ConfliBERT, include pre-training strategies,
corpora, and evaluation tasks.
3.1 Domain-specific Pretraining
We explore both strategies (SCR and Cont) of
adapting BERT to the political conflict and violence
domain. A Cont model initializes with BERT’s
checkpoint and vocabulary, and trains for addi-
tional steps on a domain-specific corpus. Since
BERT has already been pre-trained about one mil-
lion steps on the generic domain, Cont usually re-
quires fewer steps than training a new model from
scratch. For example, Lee et al. (2019) report that
continual pre-training of BERT on a biomedical
dataset for 470K steps yields comparable perfor-
mance to pre-training for one million steps.
On the other hand, when pre-training BERT from
scratch (SCR) on the domain-specific corpora, we
generate a new vocabulary from the target domain
instead of using the original BERT’s vocabulary.
Various papers (Beltagy et al., 2019; Gu et al.,
2021) argue that SCR generates substantial gains
over Cont for domains with sizeable unlabeled text.
We refer to the original BERT vocabulary as
BaseV ocab and our domain vocabulary as Con-
fliV ocab. We generated both cased and uncased
versions of ConfliV ocab on our training corpus
using the Wordpiece algorithm (Wu et al., 2016).
We set the ConfliV ocab size to 30,000 words to5471
match that of BaseV ocab. The resulting token over-
lap between BaseV ocab and ConfliV ocab is 58.3%,
which indicates a considerable difference (41.7%)
in high-frequency words between the general and
conflict-specific corpora.
In particular, we find a substantial advantage of
using ConfliV ocab during the tokenization. Table 2
shows examples of conflict-related terms that ex-
clusively appear in ConfliV ocab. For example, the
term "separatists" is not included in BaseV ocab,
and BERT erroneously splits it into four sub-words
["se", "##par", "##ati", "##sts"]. This fragmenta-
tion may hinder learning in downstream tasks. We
will validate the advantage of ConfliV ocab in the
downstream tasks in our experiments section.
3.2 Corpora
The first step to develop ConfliBERT is to build a
domain-specific corpus for pre-training. As illus-
trated in Table 1, there exist large-scale publicly
available biomedical datasets, such as PubMed and
MIMIC (Johnson et al., 2016). SciBERT (Beltagy
et al., 2019) is built from a large corpus of academic
papers (Ammar et al., 2018; Lo et al., 2020). His-
tory Labprovides many government documents,
but lacks the breadth we need for the politics and
conflict domain (Connelly et al., 2021). Thus we
curated a domain corpus that consists of 33.7 GB
of clean, plain text in the BERT required format.
We bin the sources into five categories below and
provide more details in Appendix.
Expert Domain Corpora (EDC). We curated
2,293 MB of plain text from multiple profes-
sional sources relevant to conflict and diplomacy.The sources include United Nations’ websites
and databases, international humanitarian non-
governmental organizations, think tanks, and gov-
ernment sources such as the Foreign Relations of
the United States. These are examples of objective
records of government and diplomatic activity from
non-partisan observers.
Mainstream Media Collection (MMC). We
crawled 35 worldwide news agencies reporting in
English and with coverage from 1966 to 2021. We
pre-processed and filtered 20 GB of stories using
metadata such as document tags for War and Poli-
tics. These cover a period during and after the Cold
War with global coverage that focuses on primarily
state-based conflict.
Gigaword. This corpus provides a distinct cover-
age of seven international English newswires from
1994 to 2010 (Parker et al., 2011). We removed the
overlapping stories (which also existed in MMC)
and filtered an 8,818 MB domain-specific subset.
Phoenix Real-Time (PRT). PRT is a developing
event dataset crawled from more than 400 news
agencies worldwide from October 2017 (Salam
et al., 2018). It contains many news agencies in ar-
eas other than Europe and the U.S., thus improving
the scope of our coverage. We removed the dupli-
cated news agencies (which also existed in MMC
and Gigaword) and filtered a 2,425 MB relevant
subset. This allows the capture of post-Cold War
actors, the Global War on Terrorism Service Medal
(GWOT), and more recent events.
Wikipedia. Wikipedia has a different language
style for describing political events and can enrich
the diversity of our corpus. Based on its category
labels, we curated 2,845 MB of relevant articles
from an 18 GB size of the Wikipedia dump released
on March 20, 2021.
3.3 Evaluation Tasks
The introduction of comprehensive benchmarks ac-
celerated the development of pre-trained language
models in the general NLP domain (Wang et al.,
2018, 2019; Rajpurkar et al., 2018; Lai et al., 2017)
and biomedical applications (Peng et al., 2019; Gu
et al., 2021). However, few comprehensive bench-
marks exist for evaluating language models in the
political conflict and violence domain. The focus
of political science professionals is different from
that of general NLP researchers. For example, they5472are more interested in classifying, tracking, and
predicting conflict events from the text.
To conduct a comprehensive evaluation of Con-
fliBERT, we collected a broad range of NLP tasks
related to political conflict and violence from
both publicly available and our newly-annotated
datasets. Table 3 shows the datasets and their corre-
sponding tasks. Some datasets may contain subsets
and are related to various tasks. The table also lists
the number of examples in the training, develop-
ment, and test datasets as well as the evaluation
metrics used for each task. In particular, we use
F1 scores as performance metrics for binary clas-
sification tasks. We use example-based F1 metrics
for multi-label classification tasks (Sorower, 2010).
For all the other tasks, we rely on Macro F1 to
assess the model’s performance. Next, we describe
the datasets and their tasks.
Binary classification (BC). We collected BBC
News (Greene and Cunningham, 2006) and
20 Newsgroups (Lang, 1995) for identifying
political news, a subset from Gun Violence
Database (Pavlick et al., 2016) for finding arti-
cles related to gun violence. We also used the
samples from Global Contention Politics Dataset
(GLOCON ) (Hürriyeto ˘glu et al., 2019) to conduct
one sentence-level and one document-level classifi-
cation task to predict whether the story is related to
protests. These BC tasks are essential for political
scientists as a first step to classify and filter docu-
ments containing political and conflict events from
large-scale news wires.
Multi-class classification (MCC). GTD refers
to Global Terrorism Database which collects ter-
rorist incidents from 1970 onward (START, 2019).
We sampled a subset with description text longer
than 40 words and single labels to classify 9 types
of attacks such as bombing/explosion, armed as-
sault, and hostage-taking.
India Police Events (Halterman et al., 2021)
consists of sentences from English-language Times
of India articles about police activity events in Gu-
jarat during March 2002 (a relevant period due to
widespread Hindu-Muslim violence). The labels
are available for both document and sentence levels
and consider five categories of police activity: kill,
arrest, fail to act, force, and any action.
Event Status includes English news articles
about civil unrest events annotated with temporal
tags (Huang et al., 2016). Following the originalsetting, we conduct a temporal status classification
(TS MCC) to detect the primary temporal distinc-
tions among past, ongoing, and future. Besides,
we also build a BC task of predicting if the story
contains civil unrest events.
Multi-label classification (MLC). SATP stands
for South Asia Terrorism Portalfrom which we
manually annotated a sample of 7,445 narratives
between 2011 and 2019. We focus on incidents ini-
tiated by terrorist organizations. 23.6% of the sam-
ple are relevant stories classified into one or more
categories: armed assault, bombing/explosion, kid-
napping, and others. The rest samples are irrelevant
(stories not about terrorism attacks such as arrests
or armed clashes). Based upon this, we built three
tasks. The first is a BC task to find relevant sto-
ries. The second is an MLC task to predict attack
types on the relevant subset (Rel MLC). The third
is the same as the second but conducted on the
more imbalanced full dataset (All MLC).
InSight Crime (Parolin et al., 2021b) contains
annotated stories about organized criminal activity
in Latin America and the Caribbean from InSight
Crime.We applied an MLC task to predict multi-
ple crime categories expressed in the stories, such
as drug trafficking, corruption, and law enforce-
ment.
Sequence Labeling or Named Entity Recogni-
tion (NER). MUC-4 consists of documents re-
porting terrorism events, annotated with entities
such as Perpetrator Individuals, Perpetrator Orga-
nizations, Physical targets, Victims, and Weapons
(MUC-4, 1992). We split the dataset following Du
and Cardie (2020).
Re3d stands for Relationship and Entity Extrac-
tion Evaluation Dataset (DSTL, 2018), comprising
task-specific documents focused on the topic of the
conflict in Syria and Iraq. The data contains annota-
tions in span format with their corresponding entity
types: Organization, Weapon, Military platform,
Person, among others.
CAMEO (C onflict andMediation Event
Observations) is the industry standard for event
extraction in political science (Gerner et al., 2002).
An event classification, known as pentacode, con-
sists of five event types: 0-Make a Statement, 1-
Verbal Cooperation, 2-Material Cooperation, 3-
Verbal Conflict, and 4-Material Conflict, and spans5473of texts containing sources (who conducted the
action) and targets (to whom the action was con-
ducted). We formulated two tasks for CAMEO
event extraction on our newly-annotated dataset:
sources and targets labeling (ST NER), and penta-
code classification (PC MCC).
4 Experimental Setup
4.1 Pre-training Setup
We implemented ConfliBERT using two methods,
Cont and SCR. Each approach has an uncased
and a cased version. The architecture is the same
as BERT-Base with 12 layers, 768 hidden units,
12 attention heads, and 110M parameters in total.
Specifically, for our Cont models, we ran additional
pre-training steps of the released checkpoints of
BERT-Base models on our domain-specific corpus.
The vocabulary is the same as the original BERT’s
vocabulary. For our SCR models, we use an in-
domain vocabulary, ConfliV ocab (See Section 3.1
for more details).
We discarded the next sentence prediction (NSP)
task. We found that the predicted NSP accuracy
quickly reached 90% in the middle of our training,
which indicated that NSP might be less challeng-
ing for the model to learn in our domain. How-
ever, learning NSP simultaneously affected the
speed of optimization of masked language mod-
els (MLM) loss. Following many recent works
discarding NSP (Lample and Conneau, 2019; Yang
et al., 2019; Joshi et al., 2020; Liu et al., 2019) and
our observation, we optimized MLM only.
We used four V-100 GPUs with 32 GB mem-
ory to train each model. We used Adam opti-
mizer (Kingma and Ba, 2015). The learning rate
was warmed up over the first 10,000 steps to the
peak value of 5e-4, and then linearly decayed. We
pre-trained each SCR model for about 150K steps
over the 7 billion word corpus. We followed Devlin
et al. (2018) to train the model with a sequence
length of 128 for 80% of the steps. Then, we
trained the remaining 20% steps with a sequence of
512. The overall training time for each SCR model
took about eight days. We trained Cont models the
same as SCR models but in two fewer days because
they were trained from intermediate checkpoints.
See Appendix for more details.
4.2 Fine-Tuning Setup
Architecture. We followed the same architecture
modification as BERT (Devlin et al., 2018) in thedownstream tasks. Our task mainly consists of
classification and sequence labeling. The sequence
labeling tasks predict the sequence of BIO tags for
each token in the input sentence. The classification
tasks require a sequence classification/regression
head on top of the pooled output of BERT. We
used cross-entropy loss for binary/multi-class clas-
sification. We used mean-square loss and set the
discrimination thresholds as 0.5 in all the multi-
label classification tasks.
Casing. Devlin et al. (2018) use the cased models
for NER and the uncased models for all other tasks.
However, other works report that uncased models
perform slightly better than cased models in spe-
cific domains, even on NER tasks (Beltagy et al.,
2019; Gu et al., 2021). Therefore, we evaluated
both cased and uncased versions of all models.
Hyperparameters. Devlin et al. (2018) propose
a hyperparameter tuning strategy relying on a grid-
search on the ranges such as the number of training
epochs∈{3, 4}, and batch size ∈{16, 32}. How-
ever, this strategy for general domain benchmarks
(e.g. GLUE (Wang et al., 2018)) has not been
sufficiently justified in other datasets (Chalkidis
et al., 2020). The optimal hyperparameters are
highly dataset- and task-dependent in our tasks.
For instance, the models may be underfitting after
the suggested maximum of four epochs. Addi-
tionally, based on our observations from the con-
flict datasets (e.g., GTD, SATP, MUC-4, InSight
Crime), ConfliBERT models converge to the best
results faster than BERT. Therefore, to compare
with BERT fairly, we used early stopping based
upon the development dataset within a range of
the maximum training epochs when all the models
have achieved stable results. A more detailed de-
scription of other hyperparameters can be found in
the Appendix. Finally, we repeated all the experi-
ments ten times with different seeds.
5 Results and Analysis
5.1 Pre-training Results
We use perplexity (ppl) to measure how well the
language models predict a masked token in an un-
seen test set. We sampled 0.02% of stories from
each source during the data preparation, ending
with an 8.62 MB held-out dataset representing our
corpus’s distribution. Table 4 shows the ppl of our
models on the held-out dataset. We also list the val-
ues reported by the original models (Devlin et al.,5474
2018). Low ppl scores indicate that our models
have been sufficiently pre-trained and have better
generalization on our corpora.
5.2 Fine-Tuning Results and Analysis
Table 3 reports the F1 scores for each task using the
mean of 10 seeds. We have the below observations:
ConfliBERT’s superiority over BERT. Con-
fliBERT provides additional improvement to the
original BERT in our target domain. In Table 3,
although the performance is task-, dataset- and
casing- dependent, our models consistently report
the best results (in bold). In Figure 1, we compare
ConfliBERT SCR-uncased with the best results
from both cased and uncased versions of BERT
in each experiment. We use different colors to
denote four p-value thresholds (p <0.01, p<0.05,
p<0.1, and p ≥0.1) of statistical significance. SCR-
uncased demonstrates superior performance across
all the tasks, and the difference is statistically sig-
nificant at p<0.1 in all but three. Specifically
for GTD, we observed that SCR-uncased slightly
beats the best BERT, but it still shows a significant
level of confidence, as depicted in Figure 1. We
also observed that on InSight Crime, ConfliBERT
achieves the best results in SCR-cased. Yet for
SCR-uncased, the margin is not significant when
compared with the best BERT in Figure 1. How-
ever, we conduct certain experiments on GTD and
Insight Crime showing ConfliBERT’s significant
superiority when tackling limited training data in
section 5.2.
Evaluating differences between the two pre-5475
training strategies, Cont and SCR, remains to be
studied. Table 3 shows that SCR slightly beats
Cont in most cases (13 out of 18 tasks), and SCR-
uncased provides the most stable improvements
over BERT among our four models. However, the
performance is still dataset- and task-dependent.
For example, Cont beats SCR significantly in Gun
violence and Event Status. We present an in-depth
analysis of these two cases in Appendix.
Effect of ConfliVocab One major difference be-
tween SCR and Cont is the use of in-domain vo-
cabulary. Section 5.2 shows that both Cont and
SCR outperform BERT significantly while SCR
slightly beats Cont. We have discussed the sub-
stantial advantage of using ConfliV ocab during the
tokenization in Section 3.1. Besides the examples
in Table 2, in ConfliV ocab we also find terrorist
groups and criminal organizations frequently men-
tioned in the reports of violence and crime. Ex-
amples include Boko Haram, Al Qaeda, Sinaloa
Cartel, PCC, FARC, Mara, among others. On the
other hand, the range of actor entities in the politics
domain is much larger and sparser than terrorist
and criminal organizations. Given that we have a
more distinct in-domain vocabulary in the conflict
domain, we expect a more significant benefit from
ConfliV ocab in the conflict domain instead of the
general politics domain.
ConfliBERT requires less annotated data than
BERT. ConfliBERT performs well with limited
data in various conflict datasets. Figure 2 shows
three groups of experiments on GTD, SATP, and
Insight Crime, where we used varying training data
sizes but the same valid and testing set as the orig-
inal experiments respectively. We repeated each
experiment with five seeds and plotted the averagemetric scores.
Figure 2a shows that ConfliBERT beats BERT
using limited size of GTD training data. Especially
in the case of 1/32 size of GTD training data (88
examples), both SCR models still have 69% F1
scores, while BERT models drop to 55% F1 scores.
In Figure 2b, we sampled various subsets of SATP-
relevant, the SATP subset related to terrorist attacks.
Results show that three of our models remain 65%
to 73% F1 scores when using only 1/32 size of the
training data (34 examples), while BERT drops to
only 44% F1 scores. Finally, we also observe that
both ConfliBERT SCR models significantly beat
Cont and BERT models with a large margin on
Insight Crime in Figure 2c.
These results show large improvements when us-
ing ConfliBERT with limited training data. Given
the resources required to annotate data in conflict
research, this is a particularly encouraging finding.
These experiments also show that ConfliBERT out-
performs BERT on GTD, SATP, and Insight Crime,
strengthening the results in Figure 1.
6 Conclusion and Future Work
This paper presents the development, application,
evaluation, and further exploration of ConfliBERT,
a pre-trained language model for political conflict
and violence. The development of ConfliBERT
rests on an unprecedented effort on three fronts.
First, we collect and curate a large domain-specific
corpus to support the pre-training process. Sec-
ond, we conduct a comprehensive evaluation across
several datasets and various NLP tasks of distinct
nature and varying degrees of complexity.
The results show that ConfliBERT consistently
outperforms BERT in the conflict and political vio-
lence domain. Furthermore, the biggest improve-5476ments are with limited training data, which conflict
researchers often have due to the high costs of data
annotation. In this way, ConfliBERT constitutes
a valuable development that will contribute to a
broad community of researchers in political science
and policy sectors interested in tracking, analyzing,
and predicting political violence and conflict on a
global scale.
Due to limited time and computational resources,
we did not conduct more experiments to explore
various hyperparameters that could affect fine-
tuning results, such as vocabulary size and pre-
training epochs, to name a few. Future work should
analyze how to optimize ConfliBERT, expand Con-
fliBERT to multi-lingual settings, and apply Con-
fliBERT to more challenging tasks such as un-
derstanding, inference, question answering, uncer-
tainty qualification (Hu et al., 2021; Hu and Khan,
2021), and few-/ zero-shot tasks to speed up the
study of NLP application for the political science
community.
7 Ethical Impact
Our research considers several measures to miti-
gate concerns of bias in machine learning: (i) we
implement standard social science practices to se-
lect corpora and training data (Barberá et al., 2021);
(ii) for the pre-training stage, we gather a corpus
with unprecedented global coverage to reduce re-
gional biases; (iii) we move beyond the biases in-
troduced from dictionary-based methods by using
machine learning, as suggested by Wilkerson and
Casas (2017); (iv) finally, we use multiple coders
for the training data. However, copyright issues
prevent us from sharing the raw data and hinder
FAIR data principles (Wilkinson et al., 2016).
The broader goal of producing accurate and valid
conflict data is to prevent or mitigate harm. These
types of data provide a more objective means to
understand and study conflict and armed violence.
Our effort is an attempt to produce higher-quality
data resources to serve this purpose.
Acknowledgments
The research reported herein was supported in part
by NSF awards DMS-1737978, DGE-2039542,
OAC-1828467, OAC-1931541, and DGE-1906630,
ONR awards N00014-17-1-2995 and N00014-
20-1-2738, Army Research Office Contract No.
W911NF2110032 and IBM faculty award (Re-
search).References5477547854795480
A Dataset
Table 5 and Table 6 list the detailed sources in our
Expert Domain Corpora and Main Stream Media
Collection described in Section 3.2, respectively.
Filtering News Wires. We considered all the sto-
ries in EDC as relevant. However, for the general
news in MMC, Gigaword, and PRT, we needed tofilter our specific domain of political conflict and
violence based on the websites’ metadata informa-
tion such as URLs, subjects, and tags. For example,
we collected the stories with the tags such as Con-
flicts, Violence, War, Politics, Defense, Crime, et
al. We also defined a bag-of-words classifier to
assess unlabeled stories’ relevance to our domain.
Therefore, we statistically summarized two lists of
the most frequent keywords’ regular expressions
from relevant stories and irrelevant stories. There
are 266 patterns in the relevant list and 246 in the
not relevant list. For example, our relevant list con-
tains patterns such as " activist ", "protest ",
"counter.?terrorism ", and " jails?\b ".
Sports news use bellicose language similar to that
of conflict stories with words such as attack ,
shoot , and defeat , thus presenting a classifi-
cation challenge. The not relevant list contains
frequent patterns such as " shot \w+ goal " to
remove sports news. We compared the number
of unique matching in the two lists and tuned the
thresholds with the help of conflict experts. Finally,
we filtered a small subset from MMC, Gigaword,
and PRT in the conflict domain.
Filtering Wikipedia. We modified Wikiextractor
(Attardi, 2015) to extract 18 GB size of documents
with category labels from the Wikipedia dump
released on March 20, 2021. We used PetScan
to fetch pages of interest in the category hierarchy
graph. We searched all the sub-categories within 0
to 4 depths under the union of five high-level top-
ics: politics, activism, crime, government, and war.
And we got 5 GB size of stories within 208,008 sub-
categories from the query. Then, we summarized
the top 300+ frequent keywords from our targeted
categories to prune irrelevant or too far-away child
nodes based on the sub-category labels. We also
removed unrelated categories such as fictional char-
acters, movies, video war games, and historical
events or people before the 20th Century, et al.
B Hyperparameters
Table 7 and Table 8 describe the detailed hyper-
parameters used in our pre-training and fine-tuning
experiments, respectively. We implement our mod-
els using Huggingface API (Wolf et al., 2020).5481
C Other detailed results
This section analyzes in a detailed manner the
model’s performance on certain datasets. Specifi-
cally, we analyze two rare cases where all ConfliB-
ERT models outperform BERTs and where Cont
models significantly outperform SCR models. Ta-
ble 9 indicates how Cont significantly outperforms
SCR in all performance metrics (p<0.05 for all met-
rics). Table 10 shows how Cont-cased beats all the
other counterparts for classifying event status ofpieces of civil unrest. While there may be many
factors, we postulate that some words in the origi-
nal SCR-cased vocabulary are accidentally good at
tokenizing the out-of-domain text in Gun Violence,
while that vocabulary is also good at classifying
ongoing (OG) and future (FU) events.5482