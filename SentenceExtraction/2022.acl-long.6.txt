
George-Eduard Zaharia, R ˘azvan-Alexandru Sm ˘adu
Dumitru-Clementin Cercel, Mihai Dascalu
University Politehnica of Bucharest, Faculty of Automatic Control and Computers
{george.zaharia0806, razvan.smadu}@stud.acs.upb.ro
{dumitru.cercel, mihai.dascalu}@upb.ro
Abstract
Complex word identiﬁcation (CWI) is a cor-
nerstone process towards proper text simpli-
ﬁcation. CWI is highly dependent on con-
text, whereas its difﬁculty is augmented by
the scarcity of available datasets which vary
greatly in terms of domains and languages. As
such, it becomes increasingly more difﬁcult to
develop a robust model that generalizes across
a wide array of input examples. In this paper,
we propose a novel training technique for the
CWI task based on domain adaptation to im-
prove the target character and context represen-
tations. This technique addresses the problem
of working with multiple domains, inasmuch
as it creates a way of smoothing the differences
between the explored datasets. Moreover, we
also propose a similar auxiliary task, namely
text simpliﬁcation, that can be used to com-
plement lexical complexity prediction. Our
model obtains a boost of up to 2.42% in terms
of Pearson Correlation Coefﬁcients in contrast
to vanilla training techniques, when consider-
ing the CompLex from the Lexical Complex-
ity Prediction 2021 dataset. At the same time,
we obtain an increase of 3% in Pearson scores,
while considering a cross-lingual setup rely-
ing on the Complex Word Identiﬁcation 2018
dataset. In addition, our model yields state-of-
the-art results in terms of Mean Absolute Er-
ror.
1 Introduction
The overarching goal of the complex word iden-
tiﬁcation (CWI) task is to ﬁnd words that can be
simpliﬁed in a given text (Paetzold and Specia,
2016b). Evaluating word difﬁculty represents one
step towards achieving simpliﬁed, which in return
facilitates access to knowledge to a wider audience
texts (Maddela and Xu, 2018). However, complex
word identiﬁcation is a highly contextualized task,
far from being trivial. The datasets are scarce and,
most of the time, the input entries are limited or
cover different domains/areas of expertise. There-
Table 1: Examples of complex words annotated for
each of the domains from CompLex LCP and CWI
datasets. The shades indicate the complexity; the
darker the shade, the more complex the sequence of
words. Best viewed in color.
fore, developing a robust and reliable model that
can be used to properly evaluate the complexity of
tokens is a challenging task. Table 1 showcases
examples of complex words annotations from the
CompLex LCP (Shardlow et al., 2020, 2021b) and
English CWI (Yimam et al., 2018) datasets em-
ployed in this work.
Nevertheless, certain training techniques and
auxiliary tasks help the model improve its general-
ization abilities, forcing it to focus only on the most
relevant, general features (Schrom et al., 2021).
Techniques like domain adaptation (Ganin et al.,
2016) can be used for various tasks, with the pur-
pose of selecting relevant features for follow-up
processes. At the same time, the cross-domain sce-
nario can be transposed to a cross-lingual setup,
where the input entries are part of multiple avail-
able languages. Performance can be improved by70also employing the power of domain adaptation,
where the domain is the language; as such, the task
of identifying complex tokens can be approached
even for low resource languages.
We propose several solutions to improve the per-
formance of a model for CWI in a cross-domain or
a cross-lingual setting, by adding auxiliary compo-
nents (i.e., Transformer (Vaswani et al., 2017) de-
coders, Variational Auto Encoders - V AEs (Kingma
and Welling, 2014)), as well as a domain adaptation
training technique (Farahani et al., 2021). More-
over, we use the domain adaptation intuition and
we apply it in a multi-task adversarial training sce-
nario, where the main task is trained alongside an
auxiliary one, and a task discriminator has the pur-
pose of generalizing task-speciﬁc features.
We summarize our main contributions as fol-
lows:
•Applying the concept of domain adaptation
in a monolingual, cross-domain scenario for
complex word identiﬁcation;
•Introducing the domain adaptation technique
in a cross-lingual setup, where the discrimi-
nator has the purpose to support the model
extract only the most relevant features across
all languages;
•Proposing additional components (i.e., Trans-
former decoders and Variational Auto En-
coders) trained alongside the main CWI task
to provide more meaningful representations
of the inputs and to ensure robustness, while
generating new representations or by tuning
the existing ones;
•Experimenting with an additional text simpliﬁ-
cation task alongside domain/language adapta-
tion, with the purpose of extracting cross-task
features and improving performance.
2 Related Work
Domain Adaptation . Several works employed do-
main adaptation to improve performance. For ex-
ample, Du et al. (2020) approached the sentiment
analysis task by using a BERT-based (Devlin et al.,
2019) feature extractor alongside domain adapta-
tion. Furthermore, McHardy et al. (2019) used
domain adaptation for satire detection, with the
publication source representing the domain. At the
same time, Dayanik and Padó (2020) used a tech-
nique similar to domain adaptation, this time forpolitical claims detection. The previous approaches
consisted of actor masking, as well as adversarial
debiasing and sample weighting. Other studies
considering domain adaptation included sugges-
tion mining (Klimaszewski and Andruszkiewicz,
2019), mixup synthesis training (Tang et al., 2020),
and effective regularization (Vernikos et al., 2020).
Cross-Lingual Domain Adaptation . Chen
et al. (2018) proposed ADAN, an architecture
based on a feed-forward neural network with three
main components, namely: a feature extractor, a
sentiment classiﬁer, and a language discriminator.
The latter had the purpose of supporting the ad-
versarial training setup, thus covering the scenario
where the model was unable to detect whether the
input language was from the source dataset or the
target one. A similar cross-lingual approach was
adopted by Zhang et al. (2020), who developed a
system to classify entries from the target language,
while only labels from the source language were
provided.
Keung et al. (2019) employed the usage of mul-
tilingual BERT (Pires et al., 2019) and argued that
a language-adversarial task can improve the per-
formance of zero-resource cross-lingual transfers.
Moreover, training under an adversarial technique
helps the Transformer model align the representa-
tions of the English inputs.
Under a Named Entity Recognition training sce-
nario, Kim et al. (2017) used features on two levels
(i.e., word and characters), together with Recur-
rent Neural Networks and a language discriminator
used for the domain-adversarial setup. Similarly,
Huang et al. (2019) used target language discrim-
inators during the process of training models for
low-resource name tagging.
Word Complexity Prediction . Gooding and
Kochmar (2019) based their implementation for
CWI as a sequence labeling task on Long Short-
Term Memory (LSTM) (Hochreiter and Schmid-
huber, 1997) networks, inasmuch as the context
helps towards proper identiﬁcation of complex
tokens. The authors used 300-dimensional pre-
trained word embeddings as inputs for the LSTMs.
Also adopting a sequence labeling approach, Finni-
more et al. (2019) considered handcrafted features,
including punctuation or syllables, that can prop-
erly identify complex structures.
The same sequence labeling approach can be ap-
plied under a plurality voting technique (Polikar,
2006), or even using an Oracle (Kuncheva et al.,712001). The Oracle functions best when applied to
multiple solutions, by jointly using them to obtain
a ﬁnal prediction. At the same time, Zaharia et al.
(2020) explored the power of Transformer-based
models (Vaswani et al., 2017) in cross-lingual envi-
ronments by using different training scenarios, de-
pending on the scarcity of the resources: zero-shot,
one-shot, as well as few-shot learning. Moreover,
CWI can be also approached as a probabilistic task.
For example, De Hertog and Tack (2018) intro-
duced a series of architectures that combine deep
learning features, as well as handcrafted features
to address CWI as a regression problem.
3 Method
3.1 Datasets
We experimented with two datasets, one monolin-
gual - CompLex LCP 2021 (Shardlow et al., 2020,
2021b) - and one cross-lingual - the CWI Shared
Dataset (Yimam et al., 2018). The entries of Com-
pLex consist of a sentence in English and a target
token, alongside the complexity of the token, given
its context. The complexities are continuous values
between 0 and 1, annotated by various individuals
on an initial 5-point Likert scale; the annotations
were then normalized.
TheCompLex dataset contains two types of en-
tries, each with its corresponding subset of entries:
a) single, where the target token is represented by
a single word, and b) multiple, where the target to-
ken is represented by a group of words. While the
single-word dataset contains 7,662 training entries,
421 trial entries, and 917 test entries, the multi-
word dataset has lower counts, with 1,517 training
entries, 99 trial entries, and 184 for testing. At the
same time, the entries correspond to three different
domains (i.e., biblical, biomedical, and political),
therefore displaying different characteristics and
challenging the models towards generalization.
The CWI dataset was introduced in the CWI
Shared Task 2018 (Yimam et al., 2018). It is a mul-
tilingual dataset, containing entries in English, Ger-
man, Spanish, and French. Moreover, the English
entries are split into three categories, depending
on their proﬁciency levels: professional (News),
non-professional (WikiNews), and Wikipedia ar-
ticles. Most entries are for the English language
(27,299 training and 3,328 validation), while the
fewest training entries are for German (6,151 train-
ing and 795 validation). The French language does
not contain training or validation entries.3.2 The Domain Adaption Model
The overarching architecture of our method is in-
troduced in Figure 1. All underlying components
are presented in detail in the following subsections.
Our model combines character-level BiLSTM fea-
tures (i.e., F) with Transformer-based features for
the context sentence (i.e., F). The concatenated
features ( F+F) are then passed through three lin-
ear layers, with a dropout separating the ﬁrst and
second. The output is a value representing the com-
plexity of the target word.
Three conﬁgurations were experimented. Within
Basic Domain Adaptation , the previous features
are passed through an additional component, the
domain discriminator, composed of a linear layer
followed by a softmax activation function. A gra-
dient reversal layer (Ganin and Lempitsky, 2015)
is added between the feature concatenation and the
discriminator to reverse the gradients through the
backpropagation phase and support extracting gen-
eral features. The loss function is determined by
Equation 1 as:
L=L L (1)
where Lis the regression loss, Lis the general
domain loss, is a hyperparameter used for con-
trolling the importance of L, andis another hy-
perparameter that varies as the training process
progresses.
The following setups also include the Basic Do-
main Adaptation training setting.
V AE and Domain Adaptation considers the
previous conﬁguration, plus the V AE encoder, that
yields the Ffeatures, and the V AE decoder, which
aims to reconstruct the input. The concatenation
layer now contains the BiLSTM and Transformer
features, along with the V AE encoder features ( F),
namely F+F+F. The loss function is depicted
by Equation 2 as:
L=L L+L (2)
where, additionally, Lrepresents the V AE loss
described in Equation 6.
Transformer Decoder and Domain Adapta-
tion adds a Transformer Decoder with the purpose
of reconstructing the original input, for a more ro-
bust context feature extraction. The loss is denoted
by Equation 3 as:
L=L L+L (3)
where Lrepresents the decoder loss described in
Equation 9.72
3.2.1 Character-level BiLSTM for Target
Word Representation
The purpose of this component is to determine the
complexity of the target token, given only its con-
stituent characters. A character-level Bidirectional
Long Short-Term Memory (BiLSTM) network re-
ceives as input an array of characters corresponding
to the target word (or group of words), and yields
a representation that is afterwards concatenated to
the previously mentioned Transformer-based repre-
sentations. Each character cis mapped to a certain
value obtained from the character vocabulary V,
containing all the characters present in the input
dataset.
The character sequence is represented as C=
[c, c, . . . , c], where nis the maximum length of
a target token. Cis then passed through a character
embedding layer, thus yielding the output Emb.
Emb is then fed to the BiLSTM, followed by a
dropout layer, thus obtaining the ﬁnal target word
representation, F.
3.2.2 Transformer-based Context
Representation
We rely on a Transformer-based model as the main
feature extractor for the context of the target word
(i.e., the full sentence), considering their superior
performance on most natural language processing
tasks. The selected model for the ﬁrst dataset is
RoBERTa (Liu et al., 2019), as it yields better
results when compared to its counterpart, BERT.
RoBERTa is trained with higher learning rates and
larger mini-batches, and it modiﬁes the key hyper-parameters of BERT. We employed the usage of
XLM-RoBERTa (Conneau et al., 2020), the multi-
lingual counterpart of RoBERTa, now trained on
a very large corpus of multilingual texts, for the
second cross-lingual task. The features used for
our task are represented by the pooled output of the
Transformer model. The feature vector Fof 768
elements captures information about the context of
the target word.
3.2.3 Variational AutoEncoders
We aim to further improve performance by
adding extra features via Variational AutoEncoders
(V AEs) (Kingma and Welling, 2014) to the con-
text representation for a target word. More speciﬁ-
cally for the CWI task, we use the latent vector z,
alongside the Transformer and the Char BiLSTM
features. Moreover, we also need to ensure that
the Encoder representation is accurate; therefore,
we consider the V AE encoding and decoding as an
additional task having the purpose of minimizing
the reconstruction loss.
The V AE consists of two parts, namely the en-
coder and the decoder. The encoder g(x)produces
the approximation q(z|x) of the posterior distribu-
tionp(z|x) , thus mapping the input xto the latent
space z. The process is presented in Equation 4.
We use as features the representation z, denoted as
F.
p(zjx)q(zjx) =N((x);(x)) (4)
The decoder f(z)maps the latent space to the
input space (i.e., p(z)top(x)), by using Equation 5.73p(x) =Z
p(xjz)p(z)dz
=Z
N(f(z);I)p(z)dz(5)
Equation 6 introduces the loss function, where
Drepresents the Kullback Leibler divergence.
Furthermore, Erepresents the expectation with
relation to the distribution q.
L(f;g) =Xf D[q(zjx)jjp(z)]
+E[lnp(xjz)]g(6)
3.2.4 Discriminators
The features extracted by our architecture can vary
greatly as the input entries can originate from dif-
ferent domains or languages. Consequently, we
introduced a generalization technique to extract
only cross-domain features that do not present a
bias towards a certain domain. We thus employ
an adversarial training technique based on domain
adaptation, forcing the model to only extract rele-
vant cross-domain features.
A discriminator acts as a classiﬁer, containing
three linear layers with corresponding activation
functions. The discriminator classiﬁes the input
sentence into one of the available domains. Un-
like traditional classiﬁcation approaches, our pur-
pose is not to minimize the loss, but to maximize
it. We want our model to become incapable of
distinguishing between different categories of in-
put entries, therefore extracting the most relevant,
cross-domain features.
Our architecture is encouraged to generalize in
terms of extracted features by the gradient reversal
layer that reverses the gradients during the back-
propagation phase; as such, the parameters are up-
dated towards the direction that maximizes the loss
instead of minimizing it.
Three scenarios were considered, each one tar-
geting a different approach towards domain adapta-
tion.
Domain Discriminator . The ﬁrst scenario is
applied on the ﬁrst dataset, CompLex, with entries
only in English, but covering multiple domains.
The discriminator has the purpose of identifying
the domain of the entry, namely biblical, biomed-
ical or political. The intuition is that, by grasping
only cross-domain features, the performance of the
model increases on all three domains, instead ofperforming well only on one, while poorer on the
others.
Language Discriminator . The intuition is sim-
ilar to the previous scenario, except that we ex-
perimented with the second multilingual dataset.
Therefore, our interest was that our model extracts
cross-lingual features, such that the performance is
equal on all the target languages.
Task Discriminator . In this scenario, we
trained a similar, auxiliary task, represented by
text simpliﬁcation. A task discriminator is imple-
mented to detect the origin of the input entry: either
the main task or the auxiliary task (i.e., simpliﬁed
version). The dataset used for text simpliﬁcation
is represented by BenchLS (Paetzold and Specia,
2016a). The employed simpliﬁcation process con-
sists of masking the word considered to be complex
and then using a Transformer for Masked Language
Modeling to predict the best candidate. The corre-
sponding ﬂow is described in Algorithm 1, while
the loss function is presented in Equation 7:
L=L L+L (7)
where Lis the Sparse Categorical Cross Entropy
loss.
All previous discriminators use the same loss,
namely Categorical Cross Entropy (Zhang and
Sabuncu, 2018).
The overall loss consists of the difference be-
tween the task loss and the domain/language loss.
Moreover, the importance of the latter can be con-
trolled by multiplication with a hyperparameter,
that changes over time, and a ﬁxed hyperparam-
eter. The network parameters, are updated ac-
cording to Equation 8, where is the learning rate,
Lis the domain loss, Lis the task loss and is
the weight for the domain loss. A similar equation
for language loss ( L) is in place for the second
dataset, where instead of the domain loss Lwe
used the language identiﬁcation loss L, having the
same formula.
= (@L
@ @L
@) (8)
3.2.5 Transformer Decoder
Our model also considers a decoder to reconstruct
the original input, starting from the Transformer
representation. The intuition behind introducing74Algorithm 1: The Multi-Task Adversarial
algorithm (Task 1 - lexical complexity pre-
diction; Task 2 - text simpliﬁcation).Inputs: Preprocessed dataset, split into
batches ( x,y),i=1,n (where nis the
number of batches, xare the input features
for the target word and the context, and y
is the complexity);Outputs: Updated parameters ;Initialization: Initializewith random
weights;forevery batch do Select entries E1 from Task 1; Select entries E2 from Task 2; out1 = Apply initial architecture on E1; out2 = Apply Masked Language
Modeling Transformer on E2; F = Combine the features from applying
architecture on E1 and E2; out_task = Pass F through task
discriminator; loss1 = L(out1, ref1); loss2 = L(out, ref2); task_loss = L (out_task, ref_task); loss = loss1+loss2- task_loss; Backpropagate loss; Update;end
this decoder is to increase the robustness of the
context feature extraction.
The decoder receives as input the outputs of the
hidden Transformer layer alongside an embedding
of the original input, which are passed through a
Gated Recurrent Unit (GRU) (Chung et al., 2014)
layer for obtaining the ﬁnal representation of the
initial input. Additionally, two linear layers sepa-
rated by a dropout are introduced before obtaining
the ﬁnal representation, y=F. The loss is com-
puted by using the Negative Log Likelihood loss
between the outputs of the decoder and the original
Transformer input id representation of the entries
(see Equations 9 and 10).
L(x;y) =Xl (9)
l= wx;
w=weight [c] 1fc6=ignore _indexg(10)3.3 Experimental Setup
The optimizer used for our models is represented
by AdamW (Kingma and Ba, 2014). The learning
rate is set to 2e-5, while the loss functions used for
the complexity task are the L1 loss (Janocha and
Czarnecki, 2016) for the CompLex LCP dataset
and the Mean Squared Error (MSE) loss (Kline and
Berardi, 2005) for the CWI dataset. The auxiliary
losses are summed to the main loss (i.e., complex-
ity prediction) and are scaled according to their
priority, with a factor of , whereis set to 0.1
for the V AE loss, and 0.01 for the Transformer
decoder and task discriminator losses. The pa-
rameter used for domain adaptation was updated
according to Equation 11:
=2
1 +e 1 (11)
whereis the number of epochs the model was
trained;was set to 0.1, while was set to 0.2.
Moreover, each model was trained for 8 epochs, ex-
cept for the one including the V AE features, which
was trained for 12 epochs.
4 Results
4.1 LCP 2021 CompLex Dataset
We consider as baselines two models used for the
LCP 2021 competition (Shardlow et al., 2021a),
as well as the best-registered score. Almeida et al.
(2021) employed the usage of neural network so-
lutions; more speciﬁcally, they used chunks of the
sentences obtained with Sent2Vec as input features.
Zaharia et al. (2021) created models that are based
on target and context feature extractors, alongside
features resulted from Graph Convolutional Net-
works, Capsule Networks, and pre-trained word
embeddings.
Table 2 depicts the results obtained for the En-
glish dataset using domain adaptation and various
conﬁgurations. "Base" denotes the initial model
(RoBERTa + Char BiLSTM) on which we apply do-
main adaptation, as well as the auxiliary tasks. The
domain adaptation technique offers improved per-
formance when applied on top of an architecture,
considering that the model learns cross-domain
features. The only exception is represented by a
slightly lower Pearson score on the model that uses
domain adaptation alongside the Transformer de-
coding auxiliary task (Base + Decoder + DA), with
a value of .7969 on the trial dataset, when compared75
to the initial .7987 (Base). However, the remain-
ing models improve upon the starting architecture,
with the largest improvements being observed for
domain adaptation and the text simpliﬁcation auxil-
iary task (Base + Text simpliﬁcation + DA), with a
Pearson correlation coefﬁcient on the test dataset of
.7744, 2.42% better than the base model. The im-proved performance can be also seen for the Mean
Absolute Error score (MAE = .0652).
While the Transformer decoder auxiliary task
does not offer the best performance for the sin-
gle word dataset, the same architecture offers
the second-best performance for the multi-word
dataset, with a Pearson score of .8252 compared76to the best one, .8285. The domain adaptation and
V AE conﬁguration provide improvements upon the
base model (.7554 versus .7502 Pearson), but the
V AE does not have an important contribution, con-
sidering that the Base + domain adaptation model
has a slightly higher Pearson score of .7569.
4.2 CWI 2018 Dataset
We also experimented with a multilingual dataset,
where the discriminant is considered to be the lan-
guage. The baseline consists of three models used
from the CWI 2018 competition. The performance
is evaluated in terms of MAE; however, we also
report the Pearson Correlation Coefﬁcient. First,
Kajiwara and Komachi (2018) based their mod-
els on regressors, alongside features represented
by the number of characters or words and the fre-
quency of the target word in certain corpora. Sec-
ond, the approach of Bingel and Bjerva (2018) is
based on Random Forest Regressors, as well as
feed-forward neural networks alongside speciﬁc
features, such as log-probability, inﬂectional com-
plexity, or target-sentence similarity; the authors
focused on non-English entries. Third, Gooding
and Kochmar (2018) approach the English section
of the dataset by employing linear regressions. The
authors used several types of handcrafted features,
including word n-grams, POS tags, dependency
parse relations, and psycholinguistic features.
Table 3 presents the results obtained on the mul-
tilingual validation dataset and compares the perfor-
mance of different conﬁgurations. The best over-
all performance in terms of Pearson correlation
coefﬁcient is yielded by the Base model (XLM-
RoBERTa + Char BiLSTM) alongside the text sim-
pliﬁcation auxiliary task and the domain adaptation
technique (Base + Text simpliﬁcation + LA), with
values of .8602 on English News, .8555 on English
WikiNews, as well as .7842 on English Wikipedia
and .7147 on German. The best Pearson score for
the Spanish language is obtained by the base model,
with .6944. The Base + V AE + LA architecture of-
fers improvements over the Base model, but falls
behind when compared to the Base + Text sim-
pliﬁcation + LA model, with Pearson correlation
ranging from .8557 on the English News dataset to
.6805 on the Spanish dataset.
However, when switching to MAE, the metric
used for evaluation in the CWI 2018 competition,
the best performance is split between the ﬁrst three
models, namely Base, Base + LA, and Base + V AE+ LA. The Base + LA approach yields the best,
lowest MAE score on the German and Spanish
datasets, while the Base architecture performs the
best on English WikiNews and English Wikipedia.
The English News achieves the best MAE results
from the Base + V AE + LA model.
Nevertheless, the best overall performance is ob-
tained by the Base + V AE + LA model on the test
dataset (see Table 4), with dominating Pearson and
MAE scores on the Spanish and French languages:
0.6912 Pearson, 0.595 MAE for Spanish, as well
as .5559 Pearson, and .0752 MAE for French, re-
spectively. The Base + Text simpliﬁcation + LA
model performs the best in terms of Pearson Cor-
relation Coefﬁcient on the English WikiNews and
Wikipedia datasets, with Pearson scores of .8338
and .7420. However, the best MAE scores for
the same datasets are generated by the Base + LA
model (.0513 English WikiNews) and Base + V AE
+ LA (.0671 English Wikipedia).
5 Discussions
The domain adaptation technique supports our
model to learn general cross-domain or cross-
language features, while achieving higher perfor-
mance. Moreover, jointly training on two different
tasks (i.e., lexical complexity prediction and text
simpliﬁcation), coupled with domain adaptation to
generalize the features from the two tasks, can lead
to improved results.
However, there are entries for which our mod-
els were unable to properly predict the complexity
score, namely: a) entries with a different level of
complexity (i.e. biomedical), and b) entries part
of a language that was not present in the training
dataset (i.e., French). For the former, scientiﬁc
terms (e.g., "sitosterolemia"), abbreviations (e.g.,
"ES"), or complex elements (e.g., "H3-2meK9")
impose a series of difﬁculties for our feature extrac-
tors, considering the absence of these tokens from
the Transformer vocabulary. The latter category of
problematic entries creates new challenges in the
sense that it represents a completely new language
on which the architecture is tested. However, as
seen in the results section, the cross-lingual domain
adaptation technique offers good improvements,
helping the model achieve better performance on
French, even though the initial architecture was not
exposed to any French example.776 Conclusions and Future Work
This work proposes a series of training techniques,
including domain adaptation, as well as multi-task
adversarial learning, that can be used for improv-
ing the overall performance of the models for CWI.
Domain adaptation improves results by encourag-
ing the models to extract more general features,
that can be further used for the lexical complexity
prediction task. Moreover, by jointly training the
model on the CWI tasks and an auxiliary similar
task (i.e., text simpliﬁcation), the overall perfor-
mance is improved. The task discriminator also
ensures the extraction of general features, thus mak-
ing the model more robust on the CWI dataset.
For future work, we intend to experiment with
meta-learning (Finn et al., 2017) alongside do-
main adaptation (Wang et al., 2019), consider-
ing the scope of the previously applied training
techniques. This would enable us to initialize the
model’s weights in the best manner, thus ensuring
optimal results during the training phase.
Acknowledgments
This research was supported by a grant of the Ro-
manian National Authority for Scientiﬁc Research
and Innovation, CNCS - UEFISCDI, project num-
ber TE 70 PN-III-P1-1.1-TE-2019-2209, "ATES -
Automated Text Evaluation and Simpliﬁcation".
References787980