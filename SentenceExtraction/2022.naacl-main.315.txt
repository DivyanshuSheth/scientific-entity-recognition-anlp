
Tomohiro Yamasaki
Toshiba Corporation, Research and Development Center, Japan
tomohiro2.yamasaki@toshiba.co.jp
Abstract
We propose a novel Thai grapheme-to-
phoneme conversion method based on a neu-
ral regression model that is trained using neu-
ral networks to predict the similarity between
a candidate and the correct pronunciation. Af-
ter generating a set of candidates for an input
word or phrase using the orthography rules,
this model selects the best-similarity pronunci-
ation from the candidates. This method can be
applied to languages other than Thai simply by
preparing enough orthography rules, and can
reduce the mistakes that neural network mod-
els often make. We show that the accuracy of
the proposed method is .931, which is com-
parable to that of encoder-decoder sequence
models. We also demonstrate that the pro-
posed method is superior in terms of the di ﬀer-
ence between correct and predicted pronunci-
ations because incorrect, strange output some-
times occurs when using encoder-decoder se-
quence models but the error is within the ex-
pected range when using the proposed method.
1 Introduction
Grapheme-to-phoneme conversion (G2P) is the
task of converting grapheme sequences into cor-
responding phoneme sequences. Many languages
have the di ﬃculty that some grapheme sequences
correspond to more than one di ﬀerent phoneme se-
quence depending on the context.
G2P plays a key role in speech and text process-
ing systems, especially in text-to-speech (TTS)
systems. These systems have to produce speech
sounds for every word or phrase, even those not
contained in a dictionary. In low-resource lan-
guages, it is fundamentally di ﬃcult to obtain
large vocabulary dictionaries with pronunciations.
Therefore, pronunciations need to be predicted
from character sequences.
In many languages, each word is composed of
syllables and each syllable is composed of char-
acters following the orthography rules of that lan-guage. This means that G2P for languages with
syllabic orthography rules can be formulated as
the task of selecting the best path in a lattice gen-
erated for a given input word or phrase if we pre-
pare enough orthography rules to make sure that
any lattice generated almost certainly includes the
path for the correct pronunciation.
Figure 1: G2P can be formulated as the task of se-
lecting the best path in a lattice. In English, because
“ea” can be pronounced as /e/and/i:/and “ow” can
be pronounced as /oU/ and/aU/ , the word “meadow”
has 4 pronunciation candidates, excluding stress posi-
tion estimation.
As the result of some e ﬀort, we prepared Thai
orthography rules. Almost all possible paths in a
lattice can be generated from these, and each path
needs to be evaluated using a phonological lan-
guage model to select the best path. With this in
mind, we propose a novel G2P method based on a
neural regression model that is trained using neu-
ral networks to predict how similar a pronuncia-
tion candidate is to the correct pronunciation. Af-
ter generating a set of candidates for an input word
or phrase using the orthography rules, this model
selects the best-similarity pronunciation from the
candidates.
In the following sections, we describe the
proposed method and explain experiments on a
dataset of Thai vocabulary entries with pronun-
ciations collected from Wiktionary. After that,
we show that the proposed method outperforms
encoder-decoder sequence models in terms of the
diﬀerence between correct and predicted pronun-
ciations, and demonstrate that incorrect, strange
output sometimes occurs when using encoder-
decoder sequence models while error is within the4251expected range when using the proposed method.
The code is available at https://github.com/
T0106661 .
2 Related Work
For G2P, converting words into pronunciations
does not require as much expertise as needed
to prepare correspondences between graphemes
and phonemes. Therefore, a method of learn-
ing correspondences from a large number of
word-pronunciation pairs has been used (van den
Bosch and Daelemans, 1993). To solve the prob-
lem that graphemes and phonemes often have
many-to-many correspondence, a hidden Markov
model-based method (Jiampojamarn et al., 2007)
and weighted ﬁnite-state transducer-based meth-
ods (Novak et al., 2012a,b) have been devel-
oped. Bisani and Ney (2008) assumed that gra-
phones underlie both graphemes and phonemes,
and achieved great performance by learning gra-
phones to minimize joint errors.
In addition, there have been some attempts to
apply encoder-decoder models to learn end-to-end
G2P models. For example, Toshniwal and Livescu
(2016) applied a sequence-to-sequence architec-
ture (Sutskever et al., 2014; Luong et al., 2015).
Yolchuyeva et al. (2020) and Vesik et al. (2020)
applied a transformer architecture (Vaswani et al.,
2017) to train models that can deal with English or
many other languages.
3 Proposed Method
It is well known that word segmentation (WS)
is a necessary preprocessing for languages with-
out word delimiters, such as Chinese, Japanese,
and Thai. To solve WS and homograph disam-
biguation for Thai simultaneously, Tesprasit et al.
(2003) enumerated pronunciation candidates for
text data with ambiguity and trained a model to
select the correct pronunciation from candidates
based on the context in which the text data appear.
In contrast, the proposed method trains a model
that predicts how similar each candidate is to the
correct pronunciation. Although the main process
of the proposed method is language independent,
we take Thai as an example in this section.
First, we prepared correspondences between
graphemes and phonemes by combining Thai
characters consisting of 44 consonants, 15 vowels,
and several symbols, resulting in an approximately
300-line program and more than 180,000 entries.The prepared entries consisted of 77 characters,
5,772 syllables, and 31 phoneme symbols, and
each syllable was composed of up to 7 phoneme
symbols.
Characters Syllables
/uni0E40/uni0E2A
/uni0E21/uni0E2D
/uni0E40/uni0E2A/uni0E21/uni0E2D
/uni0E1C/uni0E25 ,
/uni0E23/uni0E31 (silent)
Suppose that a dataset of vocabulary entries
with pronunciations D={(v,p)|i=0, . . .}
is given. We begin by tracing characters in each
vocabulary vone at a time to generate a lattice
of nodes corresponding to entries. We then enu-
merate all possible paths in the lattice from one
end to the other, and obtain a set of pronuncia-
tion candidates C={c|j=0, . . .}by joining
syllables assigned to nodes. For example, when
we have v=“/uni0E01/uni0E25/uni0E32/uni0E07/uni0E04/uni0E37/uni0E19”(night) ,we obtain the set
C={,,}.
After that, we calculate the similarity
s=s(p,c)=1−d(p,c)
max(|p|,|c|)
to the correct pronunciation pfor each can-
didate c, where d(·,·) denotes symbol-based
edit distance. This stakes a maximum of 1
when c=pand approaches a minimum of
0 as cdiverges from p. For the previous
example, we obtained p=;
thus s(,)=1,
s(,)=13/16,
for example, were obtained.
Using the similarity deﬁned above, we train a
neural regression model that predicts how simi-
lar each candidate is to the correct pronunciation.
More speciﬁcally, we train the model to return the
similarity sfrom the encoded vectors of vand
cusing RNNs with mean absolute error as the
loss function to keep each sample error as small as
possible.
Figure 2 shows the model architecture. V ocabu-
laryvis converted into a d-dimensional vector by
a character embedding layer and a bi-directional4252GRU (Bi-GRU). Candidate cis converted into a
d-dimensional vector by a syllable embedding
layer, a phoneme embedding layer, and Bi-GRUs,
like the network of Lample et al. (2016). Finally,
both vectors are concatenated and converted into
similarity sby two dense layers. Each layer di-
mension can be changed depending on the target
language.
The model trained in this way is expected to
represent the phonological nature of the target lan-
guage. Therefore, we can predict the pronuncia-
tion pfor a given vocabulary vas follows. As
in the training phase, we trace characters in each
vocabulary v, generate a lattice, and obtain a set
of candidates C={c|j=0, . . .}. Next, we cal-
culate the similarity sfor each pair ( v,c) using
this model, and ﬁnd j=argmaxs. Finally,
we output the predicted pronunciation p=c.
As can be seen, both training and predicting
processes are language independent. In other
words, the proposed method can be applied to
languages other than Thai simply by preparing
enough orthography rules.
However, one potential problem with the pro-
posed method is that the number of candidates in-
creases exponentially with input length, which can
be undesirable for long words and phrases. This is
considered in the next section.
4 Experimental Setup and Results
We collected 18,066 Thai vocabulary entries with
pronunciations from Wiktionary as an experimen-
tal dataset. The vocabulary consisted of not only
words but also phrases. We then converted the
pronunciations described in a Thai-speciﬁc way
into International Phonetic Alphabet sequences.
For entries without pronunciations but with syl-
lable boundaries, we determined their pronuncia-
tions when all candidates for each boundary were
uniquely generated. For entries where the pro-
nunciations were unable to be determined, two
workers ﬂuent in Thai described the correct pro-
nunciations. The average length of each data was
7.42 in characters, 2.47 in syllables, and 12.48 in
phoneme symbols.
First, we examined the number of candidates
generated and the coverage rate of the correct pro-
nunciations. Figure 3 shows the distribution of
the number of candidates. As seen in the ﬁgure,
the number is usually less than 10 and seldom
greater than 100. In fact, the minimum, mode,
median, mean, and maximum were 1, 2, 4, 19.6,
and 19,242 respectively. This means that the aver-
age stayed slightly less than 20, although the lat-
tice generated for longer input tends to have many
branches and can cause an exponential increase
in candidates. For the coverage rate, 17,720 of
18,066 correct pronunciations were included in the
sets of candidates and 346 were not.
Next, we evaluated our method compared with
three G2P baseline models available from SIG-
MORPHON (2020), namely, a pair ngram model
(fst) and two encoder-decoder sequence models
(encoder-decoder and transformer, hereinafter ab-
Models AccuracyDiﬀerence
Ave Max
fst .670±.014 .619 12 .3
enc-dec .932±.012 .313 78 .9
xformer .911±.015 .360 42 .5
ours .931±.006 .217 8 .74253breviated to enc-dec and xformer). Table 2 shows
the results of 10-fold cross-validation on the test
data. For each fold in the experiments, we used
8/10 of entries for training, 1 /10 for validation,
and 1 /10 for testing. We also used accuracy and
the di ﬀerence between correct and predicted pro-
nunciations counted by phoneme symbols as eval-
uation metrics. In other words, accuracy is the per-
centage of 0-di ﬀerence entries.
As a result, 1 .17×10parameters were trained.
Each training run was composed of about 40
epochs and each epoch took about 13 minutes on 1
GPU (Titan V , 11GB). The absence of decoders in
our model is a possible reason why the number of
parameters is small and the training time is short.
Table 2 shows that our method achieved high
accuracy, small average di ﬀerence, and small
maximum di ﬀerence, and the accuracy in particu-
lar is comparable to those of enc-dec and xformer.
In contrast, the low accuracy and large average dif-
ference of fst indicate that an ngram model was
not able to su ﬃciently learn the Thai phonologi-
cal nature, and large maximum di ﬀerences of enc-
dec and xformer indicate the well-known problem
of neural network models returning good output
when they work, but sometimes making mistakes
when they do not.
/uni0E01/uni0E32/uni0E23/uni0E40/uni0E25/uni0E37/uni0E2D/uni0E01/uni0E15/uni0E31/uni0E49/uni0E07/uni0E1B/uni0E23/uni0E30/uni0E18/uni0E32/uni0E19/uni0E32/uni0E18/uni0E34/uni0E1A/uni0E14/uni0E35/uni0E1D/uni0E23/uni0E31/uni0E48/uni0E07/uni0E40/uni0E28/uni0E2A
(French presidential election)
fst
enc-dec
xformer
ours
correct
As shown in Table 3, further investigation re-
vealed that the outputs of the encoder-decoder se-quence models sometimes included unnatural syl-
lable repetitions and sometimes lacked syllables in
the middle, which are undesirable for TTS systems
because they might give the impression that the
system is failing. In contrast, our method was able
to reduce such mistakes because all candidates fol-
lowed the orthography rules.
4.1 Additional Experiments
To conﬁrm that the main process of our method
can be applied to other languages, we performed
additional experiments on the Japanese Hiragana
dataset available from SIGMORPHON (2021).
This dataset consisted of 10,000 entries and the
average length of each data was 4.21 in characters,
6.53 in syllables, and 16.43 in phoneme symbols.
As the result of some e ﬀort, we prepared
Japanese Hiragana orthography rules. The pre-
pared entries consisted of 85 characters, 405 sylla-
bles, and 45 phoneme symbols, and each syllable
was composed of up to 6 phoneme symbols.
Models AccuracyDiﬀerence
Ave Max
fst .915±.002 .187 19 .0
enc-dec .925±.006 .161 14 .8
xformer .921±.006 .169 13 .2
ours .945±.002 .103 14 .0
Table 4 shows performance comparison with
three G2P baseline models. As can be seen, our
method also achieved high accuracy and small av-
erage di ﬀerence. However, the maximum di ﬀer-
ences of enc-dec and xformer are comparable to
that of our method. A possible reason why the
encoder-decoder sequence models worked well is
that the number of long inputs in this dataset was
smaller compared with Thai.
5 Conclusion and Future Work
In this study, we proposed a novel Thai G2P
method based on neural regression models. We
conﬁrmed that the model trained using neural net-
works to predict the similarity was able to select
the correct pronunciations from candidates. The
accuracy was .931 and the di ﬀerence between cor-
rect and predicted pronunciations was .217 on av-
erage and 8 .7 at maximum.4254This means that the performance of our pro-
posed method was comparable to that of encoder-
decoder sequence models and superior in terms of
the di ﬀerence between correct and predicted pro-
nunciations. In particular, error is within the ex-
pected range when using our proposed method.
Use of neural regression models not only for G2P
but also for summarization and generation opens
the possibility that neural network models could
reduce strange mistakes.
Our proposed method has the strength that it can
be applied to any language by preparing enough
orthography rules. However, it also has the weak-
ness of the number of candidates increasing expo-
nentially with input length, which can be a concern
for languages with many exceptional orthography
rules, such as English.
A method for reducing candidates might thus be
needed. There may be an e ﬃcient solution to ﬁnd
the correct pronunciation using a given candidate
and the predicted similarity. However, these stud-
ies and experiments are left as future work.
References4255