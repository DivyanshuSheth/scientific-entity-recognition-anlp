
I-Hung HsuZhiyu XieKuan-Hao Huang
Premkumar NatarajanNanyun PengInformation Science Institute, University of Southern CaliforniaComputer Science Department, University of California, Los AngelesComputer Science Department, Tsinghua University
{ihunghsu, pnataraj}@isi.edu {khhuang, violetpeng}@cs.ucla.edu
xiezy19@mails.tsinghua.edu.cn
Abstract
Event argument extraction (EAE) identifies
event arguments and their specific roles for a
given event. Recent advancement in generation-
based EAE models has shown great perfor-
mance and generalizability over classification-
based models. However, existing generation-
based EAE models mostly focus on problem re-
formulation and prompt design, without incor-
porating additional information that has been
shown to be effective for classification-based
models, such as the abstract meaning represen-
tation (AMR) of the input passages. Incorpo-
rating such information into generation-based
models is challenging due to the heterogeneous
nature of the natural language form prevalently
used in generation-based models and the struc-
tured form of AMRs. In this work, we study
strategies to incorporate AMR into generation-
based EAE models. We propose A ,
which generates AMR-aware prefixes for every
layer of the generation model. Thus, the prefix
introduces AMR information to the generation-
based EAE model and then improves the gen-
eration. We also introduce an adjusted copy
mechanism to A to help overcome po-
tential noises brought by the AMR graph.
Comprehensive experiments and analyses on
ACE2005 and ERE datasets show that A
can get 4%−10% absolute F1 score improve-
ments with reduced training data and it is in
general powerful across different training sizes.
1 Introduction
Event argument extraction (EAE) aims to recog-
nize event arguments and their roles in an event.
For example, in Figure 1, EAE models need to ex-
tract districts ,u.s. supreme court , and washington
and the corresponding roles — Plaintiff ,Adjudi-
cator , and Place for the Justice:Appeal event with
trigger appeal . EAE has long been a challenging
task in NLP, especially when training data is lim-
ited (Wang et al., 2019; Ma et al., 2022). It isan important task for various downstream appli-
cations (Zhang et al., 2020; Berant et al., 2014;
Hogenboom et al., 2016; Wen et al., 2021; Wu
et al., 2022).
Recently, generation-based EAE models (Hsu
et al., 2022a; Lu et al., 2021; Li et al., 2021; Paolini
et al., 2021; Parekh et al., 2022) are proposed and
have shown great generalizability and competitive
performance compared to traditional classification-
based methods (Chen et al., 2015; Ma et al., 2020;
Hsu et al., 2022b; Fincke et al., 2022). However,
existing generation-based EAE models mostly fo-
cus on problem reformulation and prompt design
without incorporating auxiliary syntactic and se-
mantic information that is shown to be effective in
classification-based methods (Huang et al., 2016;
Xu and Huang, 2022; Huang et al., 2018; Ahmad
et al., 2021; Veyseh et al., 2020).
In this work, we explore how to incorporate aux-
iliary structured information into generation-based
EAE models. We focus on abstract meaning repre-
sentation (AMR) (Banarescu et al., 2013), which
extracts rich semantic information from the input
sentence. As the Figure 1’s example shows, AMR
graph summarizes the semantic structure of the in-
put passage, and many of its nodes and edges share
strong similarities with the event structures. For
example, the trigger word appeal can be mapped
to the node “appeal-01” , and the subject who ap-
peals can be found using edge “ARG0” . Hence,
the AMR graph could provide important clues for
models to figure out event arguments, resulting in
performance improvements (Zhang and Ji, 2021)
and better generalizability (Huang et al., 2018) for
classification-based methods. However, it is un-
clear how to best integrate AMR into generation-
based methods. The heterogeneous nature between
the AMR graph and the natural language prompts
in the generation-based EAE models causes the10976
difficulty of the model design.
To overcome the challenge, we propose A- (AMr-aware Prefix for generation-based
Event a Rgument Extraction), which encodes AMR
graph into prefix (Li and Liang, 2021) to regulate
the generation-based EAE models. Specifically,
an additional AMR encoder is used to encode the
input AMR graph into dense vectors. Then, these
vectors will be disassembled and distributed to ev-
ery Transformer layer in generation-based EAE
models as the prefix. These generated prefixes are
transformed into additional key and value matri-
ces to influence the attention calculation, hence,
guiding the generation.
We also introduce an adjusted copy mechanism
forA to overcome potential noises brought
by the AMR graph. Specifically, as we can observe
in Figure 1, AMR parsers will include additional
normalization (turning washington intoWashing-
ton) and word disambiguation (using appeal-01
rather than appeal ) to create AMR graphs. Such
normalization could impact the generation to pro-
duce some words that are not in the original input,
especially when the training data is limited. Hence,
we apply a copy mechanism (See et al., 2017) and
add an additional regularization loss term to en-courage copying from the input passage.
We conduct experiments on ACE 2005 (Dod-
dington et al., 2004) and ERE (Song et al., 2015)
datasets using different ratios of training data. Our
results show that A outperforms several
prior EAE works in both datasets. Under low-
resource settings that use only 5%or10% of train-
ing data, we can get 4%−10% absolute F1-scores
of improvement, and our method is in general pow-
erful across different training sizes and different
datasets. We also present a comprehensive study
of different ways to incorporate AMR information
into a generation-based EAE model. We will show
thatA is the best way among the various
methods we explored. Our code can be found at
https://github.com/PlusLabNLP/AMPERE .
2 Method
A uses D (Hsu et al., 2022a) as the
base generation-based EAE model(Section 2.1),
and augments it with AMR-aware prefixes, as
shown in Figure 1. To generate the AMR-aware
prefixes, we first use a pre-trained AMR parser to
obtain the AMR graph of the input sentence (Sec-10977tion 2.2). Then, the graph is transformed into dense
vectors through graph linearization and an AMR
encoder. Then, these dense vectors will be disas-
sembled and distributed to each layer of our base
generation-based EAE model so the generation is
guided by the AMR information (Section 2.3). Fi-
nally, we introduce the training loss for A
and our adjusted copy mechanism that can help
A overcome additional noise brought from
AMR graphs (Section 2.4).
2.1 Generation-Based EAE Model
Despite our AMR-aware prefix being agnostic to
the used generation-based EAE model, we select
D (Hsu et al., 2022a) as our base model be-
cause of its great generalizability and performance.
Here, we provide a brief overview of the model.
Given a passage and an event trigger, D
first prepares the prompt , which includes an event
type description (a sentence describing the trig-
ger word), and an event-type-specific template, as
shown in Figure 1. Then, given the passage and
the prompt, D summarizes the event in the
passage following the format of the EAE template,
so that final predictions can be decoded easily by
comparing the template and the output text. Take
the case in Figure 1 as an example, by comparing
“districts in wash ington appealed the adjudication
from u.s.supreme court. ” with the template “some -
body in some where appealed the adjudication from
some adjudicator. ”, we can know that the “districts”
is the argument of role “Plaintiff” . This is because
the corresponding placeholder “somebody” of the
role“Plaintiff” has been replaced by “districts” in
the model’s prediction.
2.2 AMR Parsing
The first step of our method is to prepare the AMR
graph of the input passage. We consider SPRING
(Bevilacqua et al., 2021), a BART-based AMR
parser trained on AMR 3.0 annotation,to be
our AMR parser. As illustrated by Figure 1, the
AMR parser encodes the input sentence into an
AMR graph, which is a directed graph where each
node represents a semantic concept (e.g., “give-
01”,“appeal-01” ) and each edge describe the cate-
gorical semantic relationship between two concepts
(e.g., ARG0 ,location ) (Banarescu et al., 2013).2.3 AMR-Aware Prefix Generation
Our next step is to embed the information into pre-
fixes (Li and Liang, 2021) for our generation-based
EAE model. To encode the AMR graph, we follow
Konstas et al. (2017) to adopt a depth-first-search
algorithm to linearize the AMR graph into a se-
quence, as shown in the example in Figure 1. Then,
an AMR encoder is adapted to encode the repre-
sentation of the sequence. One of the advantages
of our method is the flexibility to use models with
different characteristics to our generation-based
EAE model to encode AMR. Here, we consider
two AMR encoders to form different versions of
A :
•A (AMRBART) : We consider using the en-
coder part of the current state-of-the-art AMR-
to-text model — AMRBART (Bai et al., 2022) that
pre-trained on AMR 3.0 data.The model is
based on BART-large and its vocabulary is en-
larged by adding all relations and semantic con-
cepts in AMR as additional tokens. Employing
the model as our AMR encoder enables A- to leverage knowledge from other tasks.
•A (RoBERTa) :RoBERTa-large (Liu
et al., 2019b) is also considered as our AMR
encoder as pre-trained masked language models
are typical choices to perform encoding tasks.
In order to make RoBERTa better interpret the
AMR sequence, we follow Bai et al. (2022) to
add all relations in AMR (e.g. ARG0 ,ARG1 ) as
special tokens. However, since the model is not
pre-trained on abundant AMR-to-text data, we
do not include semantic concepts (e.g. concepts
end with -01) as extra tokens.
After getting the representation of the linearized
sequence, we then prepare llearnable vectors as
queries and an attention layer, where lis a hyper-
parameter that controls the length of the used pre-
fixes. These queries will compute attention with the
representations of the linearized AMR sequence,
then, we will obtain a set of compressed dense
vector P. This Pwill be transformed into the pre-
fixes (Li and Liang, 2021) that we will inject into
our generation-based EAE model.
To be more specific, we first disassemble P
intoLpieces, where Lis the number of lay-10978ers in the base generation-based EAE model, i.e.,
P={P, P, ...P}. Then, in the n-th layer of
the EAE model, the prefix is separated into two
matrices, standing for the addition key and value
matrices: P={K, V}, where K&V
are the addition key and value matrices, and they
can be further written as K={k, ...,k}and
V={v, ...,v}.kandvare vectors with the
same hidden dimension in the Transformer layer.
These additional key and value matrices will be con-
catenated with the original key and value matrices
in the attention block. Therefore, when calculat-
ing dot-product attention, the query at each posi-
tion will be influenced by these AMR-aware pre-
fixes. The reason of generating layer-wise queries
and keys is to exert stronger control. We generate
layer-wise key-value pairs as each layer may em-
bed different information. These keys influence
the model’s weighting of representations towards
corresponding generated values. Empirical studies
on layer-wise versus single-layer control can be
found in Liu et al. (2022b).
It is worth noting that Li and Liang (2021)’s
prefix tuning technique uses a fixed set of prefixes
disregarding the change of input sentence, A- willgenerate a different set of prefixes when
the input passage varies. And the variation reflects
the different AMR graph’s presentation.
We can inject prefixes into the encoder self-
attention blocks, decoder cross-attention blocks,
or decoder self-attention blocks in our generation-
based EAE model. Based on our preliminary ex-
periments, we observe that using prefix in encoder
self-attention blocks and decoder cross-attention
blocks works best in A .
2.4 Adjusted Copy Mechanism
We follow D ’s setting to use
BART-large (Lewis et al., 2020) as the pre-
trained generative model, and the training objective
of our generation-based EAE model is to maximize
the conditional probability of generating a ground-
truth token given the previously generated ones
and the input context in the encoder x, x, ..x:
where yis the output of the decoder at step
i. In D ’s setting, the probability of pre-
dicting an token tfully relies on the generative
model. Although this setting is more similar to howBART-large is pre-trained and thus better lever-
ages the power of pre-training, the loose constraints
on the final prediction could generate hallucinated
texts (Ji et al., 2022) or outputs not following the
template. Such an issue could be enlarged if less
training data is used and more input noise is pre-
sented, such as when incorporating AMR graphs.
To enhance the control, one commonly-used
technique is to apply copy mechanism (See et al.,
2017) to generation-based event models (Huang
et al., 2022, 2021). , i.e.,
where w∈[0,1]is the probability to generate,
computed by passing the last decoder hidden state
to an additional network. P(j|·)is the proba-
bility to copy input token x, and it’s computed by
using the cross-attention weights in the last decoder
layer at time step i. When w= 1, it is the orig-
inal model used by D , while if w= 0,
this model will only generate tokens from the input.
Our core idea of the adjusted copy mechanism
is to encourage the model to copy more, and this
is achieved by introducing a regularization term on
wto the loss function of A :
where λis a hyper-parameter. Compared to fully
relying on copy from input, our method still allows
the generative model to freely generate tokens not
presented in the input. Compared to ordinary copy
mechanisms, the additional regularizer will guide
the model to copy more. Using this loss, we train
the whole A end-to-end.
3 Experiments
We conduct experiments to verify the effectiveness
ofA . All the reported numbers are the aver-
age of the results from three random runs.
3.1 Experimental Settings
Datasets and Data split. We adopt the event an-
notation in ACE 2005 dataset (Doddington et al.,
2004) ( ACE05-E ), and the English split in ERE10979
dataset (Song et al., 2015) ( ERE-EN ). ACE 2005
contains files in English, Chinese, and Arabic, and
ERE includes files in English and Chinese. In this
paper, we only use the documents in English, and
split them to sentences for use in our experiments.
We follow prior works (Wadden et al., 2019; Lin
et al., 2020) to preprocess each dataset. After pre-
processing, ACE05-E has 33 event types and 22
argument roles, and ERE-EN are with 38 event
types and 21 argument roles in total. Further, we
follow Hsu et al. (2022a) to select 5%, 10%, 20%,
30%, and 50% of training samples to generate the
different data split as the training set for experi-
ments. The data statistics are listed in Table 4 in
the Appendix A.
Evaluation metrics. We report the F1-score for
argument predictions following prior works (Wad-
den et al., 2019; Lin et al., 2020). An argument is
correctly identified ( Arg-I ) if the predicted span
matches the span of any gold argument; it is cor-rectly classified ( Arg-C ) if the predicted role type
also matches.
Implementation details. We use the AMR tools
as we mentioned in Section 2. When training our
models, we set the learning rate to 10. The
number of training epochs is 60 when training on
ACE05E, and 75 when training on ERE-EN. We
simply set λas 1 for all our models. We do hyper-
parameter searching using the setting that trains
on 20% of data in ACE05E and selects the best
model based on the development set results. We
setl= 40 , and batch size is set to 4 for A
(AMRBART) and 6 for A (RoBERTa) in the
end. This is searching from l={30,40,50}and
batch size ={4,6,8,12}.
Baselines. We compare A with the fol-
lowing classification-based models: (1) Dy-
GIE++ (Wadden et al., 2019), which extracts in-
formation by scoring spans with contextualized
representations. (2) OneIE (Lin et al., 2020), a
joint IE framework that incorporates global fea-
tures. (3) Query and Extract (Wang et al., 2022),
which uses attention mechanisms to evaluate the
correlation between role names and candidate en-
tities. (4) AMR-IE (Zhang and Ji, 2021), which
captures non-local connections between entities by
aggregating neighborhood information on AMR10980graph, and designed hierarchical decoding based
on AMR graph information. We also consider the
following generation-based models: (5) PAIE (Ma
et al., 2022), a framework that integrated prompt
tuning, and generates span selectors for each role.(6)D (Hsu et al., 2022a). The generation-
based EAE model we used as our base model.
To ensure a fair comparison across models, we
adopt the official codes of the above baselines to
train them on the identical data and did hyper-
parameter tuning. For all the classification-based
methods, we use RoBERTa-large , and for all the
generation-based methods, we use BART-large as
the pre-trained language models. Appendix §B
shows details about the implementation.
3.2 Results
Table 1 shows the argument classification ( Arg-
C) F1-scores in ACE05-E and ERE datasets under
different data proportions. Overall, both A
(RoBERTa) andA (AMRBART) consistently
outperform all other baselines except the test set
results of using 50% data in ERE-EN.
From the table, we can notice that A sig-
nificantly outperforms our base model DEGREE in
all experiments in ACE05-E, and in ERE-EN, the
improvement is also considerable. When trained
with less than 20% data in ACE05-E, A
(RoBERTa) can consistently achieve more than 4
points of improvement over D in both the de-
velopment and test sets. In the following Section 4,
we will further discuss the detailed contribution of
our method over D .
To quantitatively evaluate the effectiveness of
AMR’s incorporation, we can first check the per-
formance of AMR-IE. AMR-IE achieves competi-
tive performance among classification-based mod-
els, especially under extremely low-resource set-
tings. This is coincident with how A ’s re-
sult shows. A outperforms both D
and PAIE, and the gap is more obvious under low-
resource settings. For example, in the 5% data
proportion setting, A (RoBERTa) made over
11 points of improvement over D in ACE05-
E’s test Set. In the meanwhile, A (RoBERTa)
achieves 4.4 points of performance gain compared
with PAIE. All this shows the empirical evidence
that AMR information can hint to the models’ se-mantic structure of the input passage, and this is
especially helpful for models when training sam-
ples are limited. Despite the strong performance of
AMR-IE, A can still outperform it across
all the settings, indicating the effectiveness of our
method.
Comparing A (AMRBART) andA
(RoBERTa) , we show that our proposed method
does not necessarily rely on pre-trained AMR-to-
Text models. Particularly, A (RoBERTa) ,
which employs a pre-trained RoBERTa-large as
the AMR Encoder still achieves competitive results
toA (AMRBART) , which uses AMR-to-Text
data. Yet, the advantage of using AMR-to-Text data
as an auxiliary is that we can get similar results with
less parameters. The AMR encoder component of
A (RoBERTa) has approximately 1.7 times
more parameters than that of A (AMRBART) ,
as we only use the encoder part of AMRBART in AM-
PERE(AMRBART). Nevertheless, the pre-trained
knowledge from AMR-to-text data enables A- (AMRBART) to perform competitively with
A (RoBERTa) .
4 Analysis
In this section, we present comprehensive ablation
studies and case studies to validate our model de-
signs. Two essential parts of our design, the AMR-
aware prefix, and the adjusted copy mechanism will
be examined in the following studies. For all the
experiments in this section, we use the setting of
training on 5% and 20% ACE05-E data to simulate
very low-resource and low-resource settings.
4.1 Different Ways for AMR Incorporation
We compare different ways to incorporate AMR
information into generation-based EAE models:
•AMR Prompts. We follow the same process as
A to obtain the linearized AMR graph
sequence. Then, we directly concatenate the
linearized AMR graph sequence to the input
text as part of the prompts.
•AMRBART Encoding Concatenation. After ob-
taining the AMR sequence representations after
the AMR encoder using AMRBART , we concate-
nate this encoding with the output representation
in our generation-based EAE model and feed
them together to the decoder.
•RoBERTa Encoding Concatenation. The
method is similar to the AMRBART Encod-
ing Concatenation method, except that we use10981
RoBERTa as the AMR encoder.
For comparison, we provide A ’s perfor-
mance without any AMR incorporation as a base-
line. Additionally, we also consider A with
frozen AMR encoderin the comparisons to ex-
clude the concern of extra learnable parameters
ofA compared to baselines such as AMR
Prompts. Note that all the mentioned models above
are implemented with our proposed adjusted copy
mechanism. Table 2 shows the results.
From the table, we observe that A gets
the best performance among all the ways we ex-
plored and achieves 4.9% and 4.2% F1-score im-
provements over the model without AMR incor-
poration under the case of using 5% & 20% of
training data, respectively.
An interesting finding is that the performance
of AMR Prompts is worse than the model without
any AMR incorporation in the very low-resource
setting (5% data). As mentioned in Section 1, the
heterogeneous nature between AMR graph and nat-
ural language sentences is an important intuitive for
our model design. AMR often uses special tokens
such as :ARG0 orappeal-01 , and in implementa-
tion like AMR Prompts, it would be confusing for
models when training samples are not sufficient.
Furthermore, due to the heterogeneous vector
space between AMRBART and RoBERTa ,RoBERTa
Encoding Concatenation method could not work
well. In comparison, the prefix design of A
shows strong adaptability, as A (AMRBART)
andA (RoBERTa) both outperform the other
implementation methods.Finally, we focus on the results from A
with frozen AMR Encoder. We can observe that
despite slight performance degradation compared
to fully-trainable A ,A with frozen
AMR Encoder still obtain at least 1% absolute F1-
scores improvements over other AMR incorpora-
tion methods.
4.2 Studies of Adjusted Copy Mechanism
To justify the effectiveness of our adjusted copy
mechanism, we compare our adjusted copy mecha-
nism with the following method:
•A w/o any copy. For comparison, we
adopt a normal generation-based model adapted
with AMR-aware prefixes.
•A w/ pure copy. : In Equation 2, we
directly set w= 0. In other words, tokens
not presented in the input can not be generated.
•A w/ ordinary copy mechanism. We
apply the copy mechanism but train the model
with the loss function in Equation 1.
In Table 3, the experiment with A
(AMRBART) andA (RoBERTa) lead to similar
conclusions. Any kind of copy mechanism can lead
to noticeable improvement, and the performance
gap between methods with and without copy mech-
anism is larger in the lower data proportion setting.
Our adjusted copy mechanism stably outperforms
the other methods in studies. Compared to the tradi-
tional copy mechanism, our method encourages the
model to copy more, hence can stably overcome
the very low-resource challenges. Compared to
fully relying on copy from input, our method al-
lows the generative model to freely generate tokens
not presented in the input, so as to better leverage
the pre-trained language model’s power, leading
to better performance when data is slightly more
available.10982
4.3 Case Study
4.3.1 Output Examples
To intuitively explain the benefit of our method
over previous generation-based EAE models, we
present examples here to showcase the influence
of incorporating AMR information. We compare
A andD that both trained on 20%
ACE05-E data and demonstrate two examples in
Figure 2 to show the difference of their generated
output text.
Example A presents a case where the edges in
the AMR graph helps the model to classify the cor-
rect role type of argument "government" . Without
AMR information, D incorrectly predicts
the"government" to be the agent that launched
some organization. In the AMR graph, edge ARG1
points to the object of the action concept form-01 .
Thus, in the A ’s output, "government" is
correctly classified as the object of "form" .
Example B in Figure 2 shows how the AMR
graph hints A about the argument "judge" .
By looking up the subject of verb "order" in the
AMR graph, the model is able to find the adjudi-
cator of the event. Thus, A could correctly
replace the adjudicator placeholder in the template
with real adjudicator, “judge” .
4.3.2 Error Analysis
To point out future research direction for
generation-based EAE models, we performed er-
ror analysis on 30 cases where our A
(RoBERTa) made mistakes. We identified two com-
mon types of errors: (1) ambiguous span bound-
aries, and (2) incorrect distinction between events
of the same type.
For instance, in the case of “ambiguous
span boundaries,” A (RoBERTa) incor-rectly predicted "Christian Ayub Masih" instead
of the correct label "Ayub Masih." We observe
that generation-based models struggle to accu-
rately predict span boundaries, as both A
(RoBERTa) ’s output and the ground truth can fit into
the sentence template coherently. Even with the
inclusion of AMR, the model’s ability to identify
potential boundaries from the AMR graph through
learning remains limited.
Regarding the issue of “incorrect distinction be-
tween events of the same type,” we present an ex-
ample to illustrate this. In the given input sentence,
“As well as previously holding senior positions at
Barclays Bank, BZW and Kleinwort Benson, Mc-
Carthy was formerly a top civil servant at the De-
partment of Trade and Industry. ” , the model be-
comes confused between the two "Personnel:End-
Position" events, each triggered by “previousl” and
“formerly” , respectively, due to subtle differences.
We suggest that incorporating additional structural
knowledge, such as dependency parsing informa-
tion, to separate the sentences structurally, could
be a potential solution. However, we leave this
research as future works.
5 Related Work
Generation-based event (argument) extraction
models. Traditionally, most models for EAE are
classification-based (Chen et al., 2015; Ma et al.,
2020; Hsu et al., 2022b; Fincke et al., 2022). Re-
cently, generation-based EAE models (Hsu et al.,
2022a; Lu et al., 2021; Li et al., 2021; Paolini
et al., 2021) become more and more popular due
to their flexibility to present different output struc-
tures (Yan et al., 2021), to be unified considered
with similar tasks (Lu et al., 2022), and their com-
petitive performance (Hsu et al., 2022a; Liu et al.,
2022a).10983
The development of generation-based event (ar-
gument) extraction models starts from works in-
vestigating how to reformulate event extraction
problems as a generation task (Du et al., 2021a,b).
Follow-up works put efforts to show the influence
of different prompt designs to the generative event
models. (Ma et al., 2022; Yao et al., 2022; Hsu
et al., 2022a) More recently, researchers start to
improve this series of work by designing different
model architectures (Du et al., 2022; Zhu et al.,
2022). However, very few efforts have been put
into the ways and the effectiveness of incorporat-
ing auxiliary syntactic and semantic information
into such models, even though this information has
been shown to be beneficial in classification-based
models. Hence, in this paper, we present the study
and explore ways to incorporate this additional in-
formation for generation-based event models.
Improving event extraction with weakly-
supervisions. Being a challenging task that re-
quires deep natural language understanding to
solve, many prior efforts have been put into in-
vestigating which auxiliary upstream task informa-
tion is useful for event predictions. (Xu and Huang,
2022; Liu et al., 2019a; Huang et al., 2018; Veyseh
et al., 2020) Liu et al. (2019a); Ahmad et al. (2021)
leverages dependency syntactic structures of the
input sentence to help cross-lingual event predic-
tions. Huang et al. (2016, 2018) uses the similarity
between AMR and event structures to perform zero-
shot event extraction. More recently, Zhang and
Ji (2021); Veyseh et al. (2020); Xu et al. (2022)
investigates different message passing methods on
AMR graph to help learn better representations
for final classifications. Despite many efforts thathave been put into the community, these methods
are designed for classification-based models. This
highlights the open area for research — how and
whether incorporating such auxiliary information
can also be helpful. We take a step forward in
this direction and present A to showcase the
possibility to improve the generation-based event
models by such way.
6 Conclusion
In this paper, we present A , a generation-
based model equipped with AMR-aware prefixes.
Through our comprehensive studies, we show that
prefixes can serve as an effective medium to con-
nect AMR information and the space of generative
models, hence achieving effective integration of the
auxiliary semantic information to the model. Addi-
tionally, we introduce an adjusted copy mechanism
to help A more accurately and stably gener-
ate output disregarding the additional noise brought
from the AMR graph. Through our experiments,
we show that A achieves consistent im-
provements in every setting, and the improvement
is particularly obvious in low-resource settings.
Acknowledgments
We thank anonymous reviewers for their helpful
feedback. We thank the UCLA PLUSLab and
UCLA-NLP group members for their initial review
and feedback for an earlier version of the paper.
This research was supported in part by AFOSR
MURI via Grant #FA9550-22-1-0380, Defense Ad-
vanced Research Project Agency (DARPA) via
Grant #HR00112290103/HR0011260656, the In-
telligence Advanced Research Projects Activity10984(IARPA) via Contract No. 2019-19051600007,
National Science Foundation (NSF) via Award
No. 2200274, and a research award sponsored by
CISCO.
Limitations
Our goal is to demonstrate the potential of incor-
porating AMR to improve generation-based EAE
models. Although we have shown the strength of
our method, there are still some limitations. First,
our proposed techniques are based on the AMR
graph generated by pre-trained AMR parsers. The
generated AMR graphs inevitably have a certain
possibility of being not perfect. Hence, the error
propagation issues would happen to A . We
hypothesize this is one of the reasons why the im-
provement of A is not necessarily signifi-
cant when data is abundant. Yet, through our exper-
imental results, we still show the benefit of incor-
porating this information, especially in the case of
low-resource settings. Second, although our AMR-
aware prefix design should be agnostic to the used
generation-based EAE model, in our experiment,
we only set D as our base generation-based
EAE model. We leave the investigation on the gen-
eralizability of our AMR-prefix method to other
base models as future work.
Ethics Considerations
Our method relies on a pre-trained AMR parser,
which is built using pre-trained large language mod-
els (AMRBART &RoBERTa ). It is known that the
models trained with a large text corpus may capture
the bias reflecting the training data. Therefore, it is
possible that the AMR graph used in our method
could contain certain biases. We suggest carefully
examining the potential bias before applying A- to any real-world applications.
References10985109861098710988A Datasets
We present detailed dataset statistics in Table Ta-
ble 4.
B Implementation Details
This section introduces the implementation details
for all the baseline models we use in this paper.
Our experiments are run using our machine that
equips 8 NVIDIA a6000 GPUs.
•DyGIE++ : we use their official code to reim-
plement the model.Their original model is
built using BERT (Devlin et al., 2019). As
we mentioned in Section 3.1, we replace
the used pre-trained language model into
RoBERTa-large and tune with learning rates
={1e−5,2e−5,3e−5}.
•OneIE : we use their official codeto train
the model. Their original model is built us-
ingBERT (Devlin et al., 2019). As we men-
tioned in Section 3.1, we replace the used pre-
trained language model into RoBERTa-large
and tune with learning rates ={1e−5,2e−
5,3e−5}.
•Query and Extract : we use their official
codeto train argument detection model with
learning rate = 1e−5, batch size = 16 , train-
ing epoch = 10 . Different from the official
code, we used RoBERTa-large for pre-trained
language model to ensure a fair comparison.
•AMR-IE : the original AMR-IE is an end-to-
end event extraction model, so we adapt their
official codeto event argument extraction
task by giving gold triggers in model evalua-
tion. We fixed pre-trained language model
learning rate = 1e−5, then did hyperpa-
rameter searching from graph learning rate
={1e−3,4e−3}and batch size ={8,16}.
•PAIE : we use their official codeto train
the model with the default parameters for
BART-large .•DEGREE : we use their official codeto
train the model with the default parameters
forBART-large .
C Detailed Result
Table 5 shows the detailed results of our main ex-
periments. We repeat running every experiment
setting with three random seeds, and report their
average Arg-I and Arg-C F1-scores, and the corre-
sponding standard deviation scores.109891099010991ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations Section
/squareA2. Did you discuss any potential risks of your work?
Ethics Considerations Section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract Section & Section 1 Introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3
/squareB1. Did you cite the creators of artifacts you used?
Section 3 (Datasets)
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Appendix A
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Appendix A
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Appendix A
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 3 (Datasets)
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A
C/squareDid you run computational experiments?
Section 3 & Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix B10992/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3 (Implementation Details)
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 2 (AMR Parser, AMRBART)
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.10993