
Jihyuk Kim
Yonsei University
jihyukkim@yonsei.ac.krMinsoo Kim
Seoul National University
minsoo9574@snu.ac.kr
Seung-won Hwang
Seoul National University
seungwonh@snu.ac.kr
Abstract
Deep learning for Information Retrieval (IR)
requires a large amount of high-quality query-
document relevance labels, but such labels
are inherently sparse. Label smoothing re-
distributes some observed probability mass
over unobserved instances, often uniformly,
uninformed of the true distribution. In con-
trast, we propose knowledge distillation for in-
formed labeling, without incurring high com-
putation overheads at evaluation time. Our
contribution is designing a simple but ef-
ﬁcient teacher model which utilizes collec-
tive knowledge, to outperform state-of-the-
arts distilled from a more complex teacher
model. Speciﬁcally, we train up to ×8faster
than the state-of-the-art teacher, while distill-
ing the rankings better. Our code is pub-
licly available at https://github.com/
jihyukkim-nlp/CollectiveKD .
1 Introduction
Facilitated by recent developments in pre-trained
language models (PLM) such as BERT (Devlin
et al., 2019), neural ranking models have seen
signiﬁcant improvements in effectiveness (Yates
et al., 2021). Neural ranking models can be cat-
egorized into two major groups: Cross-encoders
and bi-encoders, which jointly or separately en-
code a query qand a passage p, respectively. While
the former shows stronger performance, the lat-
ter makes the passage representations indexable
and thus enables efﬁcient retrieval supported by
approximated nearest neighbor (ANN) search, e.g.,
FAISS (Johnson et al., 2019). In this work, target-
ing web-scale retrieval, we follow the bi-encoder
design and adopt the state-of-the-art bi-encoder,
ColBERT (Khattab and Zaharia, 2020) as our tar-
get retriever.
While label supervision has a critical role in
training bi-encoders, only incomplete labels are
Table 1: Comparison of existing KD approaches with
ours. Teacher for each method denoted in parentheses.
available in benchmark training datasets due to the
prohibitive cost of exhaustive human annotation
of large-scale passage corpora. As a prominent
example, in MSMARCO (Nguyen et al., 2016)
training dataset, the number of labeled relevant
passages per query averages only 1.1, among 8.8M
passages. In contrast, in TREC-DL 2019 (Craswell
et al., 2020) which provides complete annotations
for a select number of queries on the same pas-
sage collection, the same value is 58.2, indicating
that signiﬁcant amounts of relevant passages can
remain unlabeled in MSMARCO. Meanwhile, we
stress that the problem of incomplete labels ob-
served from benchmark datasets gets worse in real-
life retrieval tasks, where new relevant documents
are constantly added without annotation.
Existing work addressing incomplete labels can
be categorized into uninformed smoothing and in-
formed labeling. Uninformed smoothing redis-
tributes the observed probability mass uniformly
over the unobserved, and has been shown to im-
prove model calibration and prediction (Szegedy
et al., 2016; Müller et al., 2019). However, given
diverse topics in the passage corpora, uniform
smoothing does not accurately reﬂect the true dis-
tribution. Alternatively, informed labeling distills
knowledge from a trained model, evaluating the
relevance of a q-ppair, to assign a higher proba-
bility to an unlabeled pair with a higher estimated
relevance, also known as knowledge distillation
(KD) (Hinton et al., 2015).
Table 1 contrasts two existing KD strategies with
different teachers – cross- and bi-encoders – and4141
positions our proposed KD approach. Standard
KD approaches adopt a high-capacity teacher, e.g.,
cross-encoder, to teach a bi-encoder student, e.g.,
ColBERT (Hofstätter et al., 2020). However, the
cross-encoder teacher sacriﬁces efﬁciency, requir-
ing BERT encodings to be recomputed for each
possibleq-ppair. Alternatively, student and teacher
can have equal capacity (Furlanello et al., 2018),
by adopting a bi-encoder teacher, i.e., in a self-
knowledge distillation setting (self-KD). However,
while increasing efﬁciency due to independent en-
coding ofqandp, the bi-encoder teacher provides
little additional knowledge to the student. Our con-
tribution is devising a more informed bi-encoder
teacher achieving both high capacity and efﬁciency.
To augment capacity without sacriﬁcing efﬁ-
ciency, our distinction is utilizing a set of rele-
vant passages from the bi-encoder teacher as col-
lective knowledge . To illustrate, in Table 2, we
show a query q, along with a set of three passages
F={p,p,p}, labeled as relevant by ColBERT.
Considered individually, each passage may be mis-
takenly judged as relevant to the query, e.g., p.
Instead, we consider Fcollectively, identifying
their representative semantics , or collective cen-
troid ˜q. Next, we treat q∪˜qas if it were a query,
andredistribute relevance labels based on the sim-
ilarity toq∪˜q, such that relevant passages (e.g.,
{p,p}) and non-relevant passages (e.g., {p})
can be discriminated. We henceforth denote our
ColBERT teacher leveraging collective knowledge
ascollective bi-encoder .
We validate the effectiveness of our proposed
collective bi-encoder for KD, comparing retrieval
efﬁcacy with existing models on the TREC and
MSMARCO datasets. While achieving the state-of-the-art performance, we train up to ×8times faster
than existing KD that uses cross-encoder teachers.
2 Approach
Taking the state-of-the-art bi-encoder ColBERT as
our target retriever (§2.1), our goal is to tackle the
problem of incomplete labels during training (§2.2).
To do so, we devise a stronger ColBERT teacher
that leverages collective knowledge to reﬁne rele-
vance labels that are transferred to the ColBERT
student via KD (§2.3).
2.1 Baseline: ColBERT
While following a bi-encoder design to enable pto
be indexed ofﬂine, ColBERT additionally models
term-level interactions between separately encoded
q,pterm representations (called late-interaction )
to leverage exact-match signals as in cross-encoder.
Speciﬁcally, terms in q,pare ﬁrst encoded using
BERT, as contextualized representations q,p∈
R, thenq-prelevanceφ(p)is computed by ag-
gregating similarity between the terms in q,p:
φ(p) =/summationdisplaymax(Wq)(Wp),(1)
where|q|,|p|denote the number of tokens in q,
prespectively, and W∈Rcompresses
features for efﬁcient inner-product computation.
Our goal is to improve training by addressing the
challenge of incomplete labels, which we discuss
in the following subsection.
2.2 Challenge: Incomplete Labels
We formally revisit the challenge of incomplete la-
bels in IR. Given complete labels on true relevance,
φ, the correlation between φandφcan be in-
creased via an objective Lusing KL-divergence.
R(p) =softmax (φ(p)) (2)
L=−/summationdisplayR(p) logR(p)
R(p),(3)
whereRdenotes a probability distribution ob-
tained by normalizing φ. Let us now consider the
scenario of incomplete label supervision: The stan-
dard training approach would approximate Ras
R(p)=1for a labeled positive p, andR(p)
=0for the others by assuming them to be negatives4142{p}:
L=−logR(p) (4)
=−loge
e+/summationtexte, (5)
where, in the ofﬁcial train samples of MSMARCO,
pis randomly sampled from the top-1000 ranked
passages by BM25 given qas a query. Our claim
is that, in the case of incomplete labels, where the
number of unlabeled pbecomes larger, this as-
sumption causesRto be poorly approximated.
2.3 Proposed: Collective Self-KD
To better estimate R(p), we utilize collective
knowledge, introduced in Section §1. Collective
knowledge makes it possible to distill residual rel-
evance knowledge between teacher and student,
leading to a high degree of efﬁciency.
As illustrated in Table 2, our collective bi-
encoder teacher ﬁrst obtains the collective centroid
˜qfrom top-ranked passages to expand qtoq∪˜q.
Givenq∪˜q, the collective knowledge of the teacher
is reﬂected in the updated R(p), which is now ap-
proximated asR(p), the similarity between p
andq∪˜q. In practice, we implement our teacher
using the recently proposed ColBERT-PRF (Wang
et al., 2021), which uses PRF for query expansion
(at test time) by using top-ranked passages from
ColBERT as pseudo-relevance feedback (PRF).
Our distinction is leveraging PRF for labeling
(at train time) to augment the capacity of bi-encoder
teacher. This is an important distinction, as it pro-
vides substantial beneﬁts as follows: (1) Train-time
PRF is more reliable than test-time PRF, since at
test time, the initial ranking is less accurate for
novel queries that have not been observed during
training. (2) Train-time PRF additionally provides
hard negatives (Xiong et al., 2021) (e.g., pin Ta-
ble 2), as by-products of the initial ranking from
ColBERT, producing better {p}for training. With
the goal of distilling collective knowledge while
preserving test time efﬁciency, we now describe
in detail the operation of our collective bi-encoder
teacher, andR(p)estimation process.
2.3.1 Collective Knowledge Extraction
As a preliminary, we pre-train a ColBERT model
using Eq (5) and use it to rank passages in the
collection for each q, producing a ranking Π=
{p}whereland|P|denotes the rank of eachpassage and the size of the passage collection, re-
spectively. From these, we obtain the top- franked
passages, as feedback passages Π={p}.
To extract collective knowledge present in Π,
we apply k-means clustering on the token em-
beddings in Π, i.e.,{Wp},
producing fcentroid embeddings C=
{c}. Then, in order to sift out trivial knowl-
edge, discriminative embeddings among Care fur-
ther ﬁltered. More precisely, we sort the centroids
inC, by the IDF score of the token that is the near-
est neighbor, among the entire passage collection,
to each c. According to these IDF scores, we
select the top- fembeddings among Cas the ﬁnal
collective centroid embeddings, i.e., vector repre-
sentations for ˜q, denoted byE={e}⊂C.
Finally, to obtain the improved approximation
ofRviaR(p), the similarity between Eand
pis added toφ(p)in Eq (1):
R(p)≈softmax (φ(p)
+β/summationdisplayσmaxe(Wp)),(6)
whereσis the IDF of the nearest neighbor to-
ken of e, weighing the contribution of each
ein terms of discriminability, and βis a hyper-
parameter controlling overall contribution. Note
that, we can reuse the collective centroids for es-
timating theR(p)of anyp, avoiding redundant
BERT encoding.
2.3.2 Collective Knowledge Distillation
We now propose efﬁcient distillation strategies for
teachingRto the student, where we leverage the
fact that the student and the teacher share the iden-
tical architecture design. For efﬁcient KD, we let
our student inherit most of the knowledge through
the parameters of the teacher, by initializing the
student’s parameters with those of the pre-trained
ColBERT, denoted by θ. Sinceθalready captures
R, the knowledge distillation of Rcan be sim-
pliﬁed as distilling residual relevance, i.e., R, to
the student.
Meanwhile, when using θfor initialization, p
sampled by BM25 now becomes a trivial negative
that can be easily discriminated by θ. To learn
additional knowledge, we introduce hard negatives,
by utilizing the initial ranking Πfrom the pre-
trained ColBERT. Speciﬁcally, we replace the pool
ofpwith top-100 ranked passages Π , so as
to improve top-ranking performance of the student.4143
3 Experiments
3.1 Experimental Settings
We conduct our experiments by trainining retriev-
ers using the MSMARCO training dataset, and
evaluating on the MSMARCO Dev and TREC-
DL 2019/2020 datasets, which provide binary and
graded relevance labels, respectively. For TREC-
DL datasets, we adopt grade 2 as the binary cut-off.
For evaluation metrics, we report MRR@10 for
MSMARCO and NDCG@10 for TREC-DL, and
Recall@1000 for both. We also report the per query
mean response time (MRT) of retrievers. The re-
sults are from a single run.
3.2 Dataset Details
MSMARCO (Nguyen et al., 2016)is a passage
ranking dataset initially introduced for reading
comprehension and subsequently adopted for re-
trieval. The collection consists of 8.8M passages,
obtained from the top-10 search results retrieved
by the Bing search engine, in response to 1M real-
world queries. The training and evaluation sets
contain approximately 500,000 queries and 6,980
queries, respectively, with roughly one relevant pos-
itive passage label per query, with binary relevance
labels.
TREC-DL 2019/2020 (Craswell et al., 2020,
2021) datasets are provided by the passage ranking
task of the 2019 and 2020 TREC Deep Learning
tracks, providing densely-judged annotations of 43
and 54 queries, respectively. They share the same
passage pool as MSMARCO, and there are 215/211human relevance annotations per query. The rele-
vance judgments are graded on a four-point scale:
Irrelevant, Related, Highly Relevant, and Perfectly
Relevant.
3.3 Results
In this section, we validate the effectiveness of
our collective self-KD with ColBERT as our target
retriever. The results of ranking experiments are
reported in Table 3.
Baselines We compare several different training
strategies: training using incomplete labels (i.e., by
Eq (5)) or using better approximation of Rfrom
a teacher. For the former, we report results from
different{p}: ColBERT trained using top-1000
ranked passages by BM25 ( ColBERT ) and Col-
BERT further ﬁne-tuned using Π (ColBERT-
HN). For the latter, we compare different teachers:
(1) an identically parameterized teacher ( Self-KD ),
(2) a cross-encoder adopting BERT-Base encoder
(Standard KD; CE ), (3) an ensemble of three
cross-encoders with different PLMs, i.e., BERT-
Base, BERT-Large, and ALBERT-Large (Lan et al.,
2019), ( Standard KD; CE-3 ), and ﬁnally (4) our
collective bi-encoder teacher ( Collective Self-KD ).
For relevance labels from cross-encoder teachers,
we used open-sourced data by (Hofstätter et al.,
2020). Analysis on estimated Rfrom different
teachers can be found in Appendix A.1. For val-
ues of (f,f,f,β), we set (f=3,f=24,f=10)
and ran experiments with β= 0.5andβ= 1.0,
by referring to the reported results in Wang et al.
(2021). For further analysis on ( f,f,f,β),
see Appendix A.2. In addition, we also compare
ColBERT-PRF Ranker with β= 0.5andβ= 1.0.4144Ranking Performance By leveraging PRF to
augment query contexts, ColBERT-PRF shows
strong performance, outperforming the others on
TREC-DL datasets. Meanwhile, on MSMARCO
Dev queries, ColBERT-PRF shows comparable or
higher recall but lower MRR@10, compared to
ColBERT. This is a well-known limitation of PRF
for sparse query datasets, such as MSMARCO eval-
uation dataset with a few relevant documents (1.1
per query) (Amati et al., 2004).
Among ColBERT retrievers, as expected, basic
self-KD fails to improve performance. For Col-
BERT students distilled from cross-encoder teach-
ers, Recall@1k shows marginal difference on MS-
MARCO Dev, and decreases on the TREC-DL
datasets. Our teacher is the most effective for KD,
where our student outperforms the others on all
metrics. As a result, our student shows closest per-
formance to ColBERT-PRF. This indicates collec-
tive knowledge from PRF helps to better labeling
relevance and our collective self-KD effectively
transfers collective knowledge to our student.
Importantly, our method produces a much more
efﬁcient student than ColBERT-PRF, by distilling
collective knowledge into the parameters of the stu-
dent, resulting in a 3-fold reduction of MRT. More
speciﬁcally, recall that ColBERT-PRF performs
PRF at evaluation time , and thus performs two
rounds of retrieval (one using qand the other using
q∪˜q), increasing latency approximately two fold.
Furthermore, obtaining ˜qfrom PRF requires addi-
tional online computation at evaluation time. In
contrast, we transfer such computations to training
time for relevance labeling, eliminating all over-
heads at evaluation time.
Training Efﬁciency We compare training efﬁ-
ciency between our collective self-KD and stan-
dard KD using cross-encoder teachers. On the
same device, we measured elapsed times for the
annotation phase of relevance labels via teacher,
and student training phase using the labels.
In annotation phase, for a single cross-encoder
teacher using a BERT-Base encoder, obtaining R
takes roughly 40 hours. For a teacher using a BERT-
Large encoder, this time increases to 90 hours, and
for ALBERT-Large encoder, to 110 hours. When
ensembling cross-encoder teachers, the cost is com-
pounded. In contrast, our teacher takes only 15hours for obtaining collective knowledge, i.e., E
along withσin Eq 6, indicating a speedup be-
tween×2.67∼×16. Such efﬁciency gain comes
from the difference in the encoding phase. More
precisely, given|Q|queries and|D|documents,
cross-encoder spends quadratic complexity for en-
coding, e.g.,|Q|×|D|BERT encoding, while our
teacher adopting bi-encoder design only spends lin-
ear complexity, e.g., |Q|+|D|BERT encoding. As
a result, cross-encoder teachers do not scale to real-
world retrieval tasks, for labeling large numbers of
queries/documents.
For training student, we enable efﬁcient train-
ing via informed initialization. As a result, time
consumed for training student only takes 20 hours,
whereas the same value was around 50 hours in
standard KD, indicating an overall speedup of ×2.5.
In total, our collective self-KD is at least ×2.5
faster and up to×8faster than standard KD ap-
proaches.
4 Conclusion and Future Work
We study collective relevance labeling to overcome
incomplete labels in passage retrieval. Our ap-
proach bypasses the computational overhead as-
sociated with PRF, leading to a state-of-the-art stu-
dent retriever without sacriﬁcing efﬁciency. We
validate the effectiveness of our method over exist-
ing approaches on the MSMARCO and TREC-DL
Passage Ranking datasets.
As future work, we consider zero-shot transfer.
While our collective self-KD effectively distills
knowledge, one requirement we have is to boot-
strap the collective knowledge with a good initial
ranking model. As it usually requires some form
of label supervision to train such a model, the case
where no initial supervision is available, may be
considered a limitation of our method. We believe
bootstrapping an effective self-KD without any su-
pervision is a promising direction.
Acknowledgements
This work was supported by Microsoft Research
Asia and IITP [(2022-00155958, High Potential In-
dividuals Global Training Program) and (NO.2021-
0-01343, Artiﬁcial Intelligence Graduate School
Program (Seoul National University)].4145References
A Appendix
A.1 Analysis on Rapproximated by
different teachers
Given the top-100 passages ranked by the pre-
trained ColBERT, i.e., Π , the goal of our
teacher is to obtain better relevance boundary
through collective knowledge, by which unlabeled
positives can be discriminated from true negatives.
To analyze the reliability of relevance boundary
from the teacher, we evaluate how well the teacher
classiﬁes positives among the top-100 passages, us-
ing TREC 2019 that provides complete relevance
labels.
Figure 1 shows trade-offs between precision and
recall when applying different thresholds on φ.
We set different cut-offs on graded relevance for
deciding positive passages. For example, in the left
most ﬁgure with boundary 1, we treat passages with
labeled relevance 1,2,3 as positives and 0 as nega-
tives. As baselines, we compare our collective bi-
encoder teacher with the pretrained ColBERT, and
a cross-encoder teacher, monoBERT(Nogueira4146
et al., 2019). Better precision-recall curves should
bow towards the top right corner.
For the cut-offs of 1 and 2, we observed little
differences between retrievers. For example, when
retrievers are tasked to classify non-relevant pas-
sages with relevance label 0 and the others (the left
most ﬁgure), all retrievers show 100% recall with
near 80% precision. In contrast, for the cut-off of
3 (the rightmost ﬁgure), our teacher shows much
better trade-offs compared to the others. For ex-
ample, collective bi-encoder teacher shows near
50% precision to achieve 10% recall, while the oth-
ers show 20% precision to achieve the same recall.
The noticeable drop of precision for both ColBERT
and cross-encoder in low (<0.5) recall regimes indi-
cate that these models have difﬁculty distinguishing
moderate relevance (1, 2) from perfect relevance
(3), that is, they are not well calibrated to distin-
guish between those two groups. On the other
hand, our teacher’s reﬁnement of qthrough collec-
tive knowledge is effective in calibrating relevance,
to ﬁnely reﬂect the distinction between perfectly
relevant passages and the others.
A.2 Analysis on (f,f,f,β)
Exploring ranking performance of students under
all different conﬁgurations of (f,f,f,β)is ex-
pensive. Instead, the optimal conﬁguration can
be decided by ranking performance of the teacher.
Here, we explore f∈{1,3,5},f∈{12,24},
f∈{5,10}, andβ∈{0.5,1.0}. Meanwhile, ac-cording to reported results by (Wang et al., 2021),
(f= 3,f= 24,f= 10,β∈{0.5,1.0})shows
strong performance. Thus, instead of testing all
different conﬁgurations, we set (f= 3,f=
24,f= 10,β= 1.0)as default values, and
change one variable at a time. When evaluating
teachers using TREC-DL 2019 dataset, We found
similar results to (Wang et al., 2021) (Table 4).4147