
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang HuangTsinghua University,Alibaba Group
yuanhy20@mails.tsinghua.edu.cn
{yuanzheng.yuanzhen,chuanqi.tcq,f.huang,songfang.hsf}@alibaba-inc.com
Abstract
Language models with the Transformers struc-
ture have shown great performance in natu-
ral language processing. However, there still
poses problems when fine-tuning pre-trained
language models on downstream tasks, such
as over-fitting or representation collapse. In
this work, we propose HyPe, a simple yet ef-
fective fine-tuning technique to alleviate such
problems by perturbing hidden representations
of Transformers layers. Unlike previous works
that only add noise to inputs or parameters,
we argue that the hidden representations of
Transformers layers convey more diverse and
meaningful language information. Therefore,
making the Transformers layers more robust
to hidden representation perturbations can fur-
ther benefit the fine-tuning of PLMs en bloc.
We conduct extensive experiments and analy-
ses on GLUE and other natural language infer-
ence datasets. Results demonstrate that HyPe
outperforms vanilla fine-tuning and enhances
generalization of hidden representations from
different layers. In addition, HyPe acquires neg-
ligible computational overheads, and is better
than and compatible with previous state-of-the-
art fine-tuning techniques. Codes are released
athttps://github.com/Yuanhy1997/HyPe .
1 Introduction
Pretrain-then-finetune has become the mainstream
paradigm in recent natural language processing
(NLP) practices, and there emerges various pre-
trained language models (PLMs) such as BERT
(Devlin et al., 2019), RoBERTa (Liu et al., 2019),
and XLNet (Yang et al., 2019). Vanilla PLM fine-
tuning with common strategies (e.g., dropout (Sri-
vastava et al., 2014) and AdamW (Loshchilov and
Hutter, 2019)) can empower PLMs with excellent
downstream performance. However, vanilla fine-
tuned PLMs acquire performances with large vari-
ances on the downstream tasks (Dodge et al., 2020).Figure 1: The overview of proposed HyPe fine-tuning
technique. The random noise is added to the hidden
representations fed into each Transformers layer in the
forward computation of PLMs.
Such unstable performances may results from over-
fitting or representation collapse (Aghajanyan et al.,
2021). These problems can be aggravated in low-
resource scenarios (Zhang et al., 2021).
In recent literature, effective fine-tuning tech-
niques have been proposed to improve the perfor-
mance and generalization (transferability) of fine-
tuned PLMs (Jiang et al., 2020; Lee et al., 2020;
Chen et al., 2020). Besides other explicit regular-
ization, adding noise is a widely-used strategy to
smoothen the optimization landscape and mitigate
over-fitting. For example, some works apply the
perturbation to pre-trained parameter weights (e.g.,
NoisyTune (Wu et al., 2022)), input embedding
features (e.g., R3F (Aghajanyan et al., 2021)) or
gradients (e.g., ChildTuning (Xu et al., 2021)) dur-
ing the fine-tuning process.
Injecting noise to input features is a conven-
tional technique for generalization and can be
seen as implicit parameter regularization (Bishop,
1995). Common PLMs are stacked basic neural
network layers (i.e., Transformer layers (Vaswani
et al., 2017)), and previous research (Tenney et al.,
2019) points out that different Transformers layers3246of PLMs resolve different language information
which is encoded in hidden representations. We
turn to inject noise between layers to enhance the
hidden semantic representations for better general-
ization on Transformers layer level.
Based on the above findings, we propose to im-
prove fine-tuning by perturbing the hidden repre-
sentations. As shown in Figure 1, we propose a
simple yet effective fine-tuning technique named
HyPe (Hi(y) dden representation Perturbation) that
adds random noise to the hidden representations be-
tween layers (i.e., the inputs of each Transformers
layer) to alleviate the performance of fine-tuned lay-
ers from degrading. To be concrete, we introduce
no inductive biases to the distributions of noise in
HyPe and focus on the pivotal influences of noise
per se. Although noise can be compatible with
auxiliary constrains (Aghajanyan et al., 2021) or in-
clude informative priors (Xu et al., 2021), they may
lead to non-negligible computational overheads.
We simply use the uniform and normal distribu-
tions as two variants of noise distributions and de-
note them as HyPe-U and HyPe-N, respectively.
The computational overheads are marginal in HyPe.
HyPe can also be regarded as a decoupling analysis
of the above methods.
We conduct extensive experiments on GLUE
benchmark (Wang et al., 2018) and HyPe improves
vanilla fine-tuning up to 1.60 on BERT in terms
of average scores of the relatively small datasets
MRPC, RTE, CoLA, and STS-B, surpasses previ-
ous state-of-the-art techniques (i.e. R-Drop (Liang
et al., 2021)) by 0.15, and improves performance
in low-resource scenarios. Further analyses demon-
strate that HyPe is also compatible with different
scales of PLM (Section 5.1) and other fine-tuning
techniques (Section 5.2), increases the robustness
towards adversarial attacks (Section 5.3), and im-
proves generalization across tasks and domains on
different layers (Section 5.4).
To summarize our work, the main contributions
are listed as follows:
1.We propose HyPe, a simple yet effective
fine-tuning technique requiring little compu-
tational overhead to improve the performance
and transferability of fine-tuning PLMs.
2.Extensive experimental results show that 1)
HyPe improves fine-tuning in the aspect of
task performance and generalization and is
complementary to PLM scaling; 2) HyPe sur-passes and is compatible with current state-of-
the-art fine-tuning techniques.
2 Related Works
For large-scale PLMs, fine-tuning on downstream
tasks may acquire unstable performances, resulting
from over-fitting problems or failed training runs
(Dodge et al., 2020; Zhang et al., 2021). Recent
research has focused on how to alleviate such prob-
lems and effectively improve fine-tuning of PLMs
on the downstream tasks.
A general idea is to make the best of the pre-
trained weights and constrain the fine-tuned pa-
rameters from deviating much from the pre-trained
weights. For example, Top-K Tuning (Houlsby
et al., 2019) only fine-tunes the top-k layers of
PLMs and keeps the lower pre-trained layers in-
tact. Inspired by DropConnect (Wan et al., 2013),
mixout (Lee et al., 2020) randomly replaces the
weights of parameters with their pre-trained values
instead of zero. RecAdam (Chen et al., 2020) intro-
duces Ldistance to penalize the change of weights
from the pre-trained ones. ChildTuning (Xu et al.,
2021) applies task-free or task-driven masks on
the gradients thus only a subset of parameters are
changed during fine-tuning. SAGE (Liang et al.,
2022) uses differential updating step sizes for each
parameter. Parameters with higher sensitivities are
updated less aggressively where the computation of
sensitivities is related to the pre-trained parameters
in the cases of PLM fine-tuning.
Another line of work use noise to improve fine-
tuning. R-Drop (Liang et al., 2021) uses KL di-
vergence to regularize the discrepancy between
the noised outputs produced by different dropout
(Srivastava et al., 2014) masks during fine-tuning.
Recently proposed NoisyTune (Wu et al., 2022)
directly adds weight-aware noise to the pre-trained
parameters before fine-tuning to improve perfor-
mance. Based on the ideas of trust regions and
adversarial training, FreeLB (Zhu et al., 2019),
SMART (Jiang et al., 2020) and R3F (Aghajanyan
et al., 2021) are proposed to improve fine-tuning
by introducing adversarial noise to the input rep-
resentations during training. Tong et al. (2022)
create noised input representations by interpolat-
ing the representations between in-batch samples.
The augmented fine-tuning data can alleviate over-
fitting and help PLMs learn a smoother decision
boundary.
Previous research has proven the pivotal role of3247noise in improving PLM fine-tuning. Our proposed
technique looks into the PLMs and adds noise to the
hidden representations. Previous works introduce
regulations along with the added noise. Generat-
ing random noise only requires little computational
overheads, while additional regulations can cause
non-negligible computational overheads in mem-
ory footprints or training time, such as R-Drop
requiring two forward computations in each train-
ing step (Liang et al., 2021), and Child-Tuning
(Xu et al., 2021) requiring to pre-compute Fisher
information matrices.
3 Hidden Representation Perturbation
HyPe is motivated to improve fine-tuning of PLMs.
Perturbing input features for better training perfor-
mance is proven in effect in wide machine learning
applications (Nazaré et al., 2017; Aghajanyan et al.,
2021). The structure of PLMs is complicated and
different layers may have diverse impacts on under-
standing languages (Tenney et al., 2019). There-
fore, by perturbing the hidden representations, we
can improve the performance of each layer hence
the whole PLMs in fine-tuning processes.
In the vanilla fine-tuning setting of language
models, we denote the mapping of a PLM com-
prising of nnetwork layers as f(·)and the clas-
sification head for the downstream task as c(·),
where θstands for the pre-trained parameters of
the PLMs and ψrepresents the parameters of the
classification head on top of the PLM. Here we
have the whole forward mapping ˆy=c(f(x)),
where xandˆyare the embedded language inputs
and predicted target labels respectively. The train-
ing objective is L(θ, ψ) =L(c(f(x)), y), where
Lis the loss function defined by tasks.
The basic layer block of nowadays PLMs
(e.g., BERT) is Transformers (Vaswani et al.,
2017) which mainly comprises of multi-head self-
attention mechanism and feed-forward neural net-
work. By stacking the Transformers layers, the
scales of PLMs can get larger (e.g., the base and
large versions of BERT contain 12 and 24 layers re-
spectively). Given the stacking structure of PLMs,
f(x)can be decomposed as:
f(x) =g◦g◦ ···g(x),
where g(·)is the mapping function of the i-th
Transformers layer of the PLM, θrepresents the
parameters within layer iand we have ∪θ=θ.
Lethrepresents the hidden states fed into the layerAlgorithm 1 Forward Propagation with HyPe
Input: Word Token Sequences xh=EmbeddingLayer (x)foreachiin layer number ndo Generate εfromN(0, σ)orU(−σ, σ),h=h+ε,
//▷Add Random Noise to Hidden Statesh=g(h),end forˆy=c(h).return ˆy
i, then h=g(h). As the input sequences may
comprise multiple word tokens, without the loss of
generality, we omit the token position and sample
index marks for x,yandhfor simplicity.
During fine-tuning, HyPe injects parameter-
independent noise to the hidden states (representa-
tions) of each layer, then for the i-th layer:
h=g(h+ε)
:=g(h),
therefore the whole feed-forward process of the
PLM becomes:
f(x) =g◦g◦ ···g(x),
where εis the random noise for layer iand each
entry is distributed as N(0, σ)orU(−σ, σ). With
HyPe, the training objective is simply:
L(θ, ψ) =L/parenleftig
c(f(x)), y/parenrightig
.
As shown above, HyPe is a simple and straight-
forward fine-tuning technique. It can be easily
applied to different tasks and PLMs.
4 Experiments
In this section, we empirically demonstrate the ef-
fectiveness of HyPe through extensive experiments.
We use GLUE benchmark (Wang et al., 2018) to
illustrate the performance of HyPe in comparison
to vanilla fine-tuning.
4.1 Datasets
GLUE GLUE is a widely-used benchmark de-
signed for evaluating the natural language under-
standing abilities of models. Tasks in GLUE cover
different aspects of language understanding includ-
ing sentiment analysis, language acceptability, etc.3248
Following Xu et al. (2021), we mainly use four
relatively small datasets STS-B (Cer et al., 2017),
MRPC (Dolan and Brockett, 2005), RTE (Socher
et al., 2013a) and CoLA (Warstadt et al., 2019), as
the over-fitting problem is more notable in the small
data settings (Dodge et al., 2020). We also use
other larger datasets SST2 (Socher et al., 2013b),
QNLI (Rajpurkar et al., 2016), QQPand MNLI
(Williams et al., 2018) to further illustrate the per-
formance of HyPe. We report performance on the
development set since the test set labels are not
released. The statistics of GLUE are listed in Ap-
pendix B.
4.2 Experiment Settings
For all experiments listed in the following, we do
grid search on the learning rates and report the av-
erage results over three different random seeds. We
use the hidden representations of the first special
token (e.g., [CLS] in BERT) for sentence represen-
tation. For our HyPe, we conduct experiments on
two variants with different distributions of noise,
denoted as HyPe-N where ε∼ N(0, σ)and HyPe-
U where ε∼ U(−σ, σ). HyPe is only added dur-
ing training. When using HyPe, we empirically
find that turning off dropout will improve the tech-
nique’s performance, which will be discussed in
Section 5.5. Therefore, we run experiments with
HyPe using no dropout on hidden representations.For the more detailed settings concerning individ-
ual experiments, we list them in Appendix A ∼G.1.
4.3 Performance on GLUE
To illustrate the generality of HyPe, we conduct
experiments on the GLUE benchmark with four
popular PLMs, BERT-large (Devlin et al., 2019),
RoBERTa-large (Liu et al., 2019), ELECTRA-large
(Clark et al., 2020) and XLNet-large (Yang et al.,
2019). We use the PLMs from Huggingface Hub
(Wolf et al., 2020).
We first evaluate HyPe on the four relatively
small datasets from GLUE. As shown in Table 1,
both variants of HyPe with different noise consis-
tently improve the performance over vanilla fine-
tuning. On average scores across tasks, the im-
provements are 1.60 on BERT, 0.89 on RoBERTa,
7.45 on XLNet, and 5.80 on ELECTRA, respec-
tively. In addition, HyPe can help the model con-
verge better on the CoLA dataset using XLNet and
ELECTRA with smaller standard deviations.
We also evaluate HyPe on relatively large
datasets. We fine-tune RoBERTa on the larger
datasets of GLUE benchmark, with and without
HyPe. The results listed in Table 2 also show that
HyPe improves performance with large amounts
of fine-tuning samples. The average gains across
datasets are 0.22 and 0.17 for HyPe-U and HyPe-N3249
respectively.
In summary of the aforementioned results, we
can conclude that HyPe improves and stabilizes
fine-tuning consistently across different datasets
and PLMs. In addition, we observe that the im-
provements are more significant on small datasets,
which indicates that HyPe has the capability of miti-
gating the over-fitting problem of PLM fine-tuning.
4.4 Performance with Low Resources
As the amount of training data becomes smaller,
the over-fitting problem can be more severe. Since
HyPe shows good performance in mitigating over-
fitting on relatively small GLUE datasets, we create
a low-resource setting to further illustrate the per-
formance of HyPe. We follow previous research
(Xu et al., 2021) for the low-resource setting. In
detail, we subsample the training samples of each
dataset in GLUE benchmark to a training subset
with 1k samples, and evaluate the performance us-
ing the original development set.
As shown in Table 3, both variants of HyPe with
RoBERTa-large outperform vanilla consistently.
On average, the improvements brought by HyPe-N
and HyPe-U are up to 7.37 and 7.96 respectively.
On some datasets, the improvements are signifi-
cant: for example, the improvements of HyPe-N
and HyPe-U are up to 13.00 and 16.97 on RTE
respectively. In summary, HyPe can effectively pre-
vent PLMs from over-fitting when fine-tuning in
low-resource scenarios.
5 Further Analysis
We provide further analyses and discussions on the
performances of HyPe for model scaling, methods
comparison and combination, adversarial attacks,
and hyper-parameters in this section.
5.1 Performance on Parameter Scaling
We investigate how HyPe performs as parameters
of PLM scale up. We experiment on DeBERTa
(He et al., 2021) with 4 sizes: base, large, XL,
and XXL. The experimental details are shown in
Appendix F. Results in Table 4 show that HyPe
uniformly improves vanilla fine-tuning across dif-
ferent model sizes. The averaged improvements are
+0.72, +0.29, +0.63, and +0.39 as the size scales
up. This demonstrates that HyPe is complimentary
to PLMs parameter scaling.
5.2 Methods Comparison
To compare HyPe with previous techniques for ef-
fective fine-tuning, we review and compare with the
following baselines: (1) Top-K Tuning (Houlsby
et al., 2019); (2) Mixout (Lee et al., 2020); (3)
RecAdam (Chen et al., 2020); (4) R3F (Agha-
janyan et al., 2021); (5) ChildTuning (Xu et al.,
2021); (6) R-Drop (Liang et al., 2021); (7) LNSR
(Hua et al., 2021); (8) NoisyTune (Wu et al., 2022).
The comparison experiments are conducted on the
GLUE datasets STS-B, CoLA, MRPC, and RTE.
Comparison From the results shown in Table
5, HyPe achieves the best results on STS-B and
CoLA, and consistently outperforms Top-K Tuning,
Mixout, RecAdam, Child-Tuning, and NoisyTune
across different datasets. HyPe-N achieves the best
average score of four tasks and surpasses the previ-
ous state-of-the-art R-Drop by 0.15. On MRPC and
RTE, HyPe achieves competitive results with R3F,
R-Drop, and Child-Tuning. However, R3F and
R-Drop include a KL divergence regularization ob-
jective and need to make two forward computations
in a fine-tuning step. Both methods may have addi-
tional computational overhead. Take GPU memory
footprints as an example, under the same training
setting (e.g., batch size of 16), R3F and R-Drop
require 16GB of memory while HyPe only requires
about 11GB of memory. Child-Tuningis a task-
specific method and needs additional computation3250
of the Fisher information matrix. HyPe only adds
task-agnostic random noise to the hidden represen-
tations, and is more computationally efficient.
Compatibility To show the complementarity of
HyPe with other effective fine-tuning techniques,
we conduct experiments on the combination of tech-
niques. We integrate HyPe-N with four recently
proposed state-of-the-art techniques, R-Drop, R3F,
Child-Tuning, and NoisyTune. We use MRPC,
STS-B, CoLA, and RTE datasets and apply differ-
ent combinations to RoBERTa and BERT. The av-
erage results of the four tasks in Figure 2 show that
combining HyPe with other effective fine-tuning
techniques can further boost performance. This il-
lustrates that the improvements brought by adding
noise to hidden representations do not overlap with
other techniques, thus another advantage of HyPe is
being compatible with others. The details of exper-
iment settings and results are shown in Appendix
D.
5.3 Performance on Adversarial Samples
Fine-tuning PLMs may prone to bad generalization
of adversarial attacks. Results listed in Table 6
on textually crafted adversarial samples from ad-
vGLUE (Wang et al., 2021) show that vanilla fine-
tuned PLMs suffer from adversarial attacks, and
compared to vanilla, the performance gains broughtby HyPeN are up to +1.42, +3.79/+0.73, +8.10,
+8.20 and +2.26 on advSST-2, advMNLI(m/mm),
advRTE, advQNLI and advQQP respectively. The
results demonstrate that injecting noise into the hid-
den representations can increase the robustness of
fine-tuning towards adversarial attacks.
5.4 Performance on Generalization
Probings on generalization abilities is another
scope to access the over-fitting problem of fine-
tuning (Xu et al., 2021; Aghajanyan et al., 2021).
In this subsection, we discuss the transferability of
HyPe fine-tuned PLMs from the perspective of task
generalization and domain generalization.
Task Generalization Probing One side effect of
over-fitting is the degeneration of the dense repre-
sentations of PLMs after fine-tuning, and the phe-
nomenon is named representation collapse (Agha-
janyan et al., 2021). We probe fine-tuned PLMs
task generalization by training a PLM on one task
and then evaluating on another with parameters
fixed. Previous works freeze the whole parameters
of PLMs and only tune a linear classifier for other
tasks (Aghajanyan et al., 2021; Xu et al., 2021).
As HyPe perturbs hidden representations among
layers, we extend this experiment by training sepa-
rated linear classifiers for hidden representation of
each layer, and show their representational abilities.
We use MRPC, STS-B, RTE, and CoLA for
the target tasks and start from the checkpoints of
RoBERTa fine-tuned on SST2. As depicted in Fig-
ure 3, it is shown that 1) both variants of HyPe
achieve better performance than vanilla fine-tuning
overall; 2) the improvement is more significant on
higher layers of the PLM. In the lower layers, the
three lines seem entangled. This is reasonable as
the lower layers of PLMs are changed less in fine-3251
tuning, as discussed by previous research (Durrani
et al., 2021). The results show that PLMs fine-
tuned with HyPe maintain better representation
ability across layers, thus demonstrating that they
suffer less from the over-fitting problem.
Domain Generalization Probing Besides gen-
eralization across tasks, Xu et al. (2021) also ex-
periments on transferability across domains for the
same. Good domain generalization may indicate
that PLMs are fine-tuned to learn general semantic
features and not easily over-fit the domain-specific
information within training data. Following theirwork, we use natural language inference (NLI)
tasks from different domains. Beyond NLI datasets
MNLI and QQP in GLUE, we additionally intro-
duce datasets SNLI (Bowman et al., 2015), SciTaiL
(Khot et al., 2018) and SICK (Marelli et al., 2014).
For MNLI, we use both development sets of MNLI-
match (MNLI) and MNLI-mismatch (MNLI-mm)
for evaluation. Following previous research, we
fine-tune RoBERTa-large with different techniques
on a 5k sample subset of MNLI and SNLI datasets,
respectively. Then, we test the fine-tuned PLMs on
the aforementioned datasets to show the domain
generalization ability. The detailed introductions
of the datasets, experiment settings, and necessary
label mappings are shown in Appendix C.
The results listed in Table 7 illustrate that both
variants of HyPe outperform vanilla fine-tuned
models on most of the out-of-domain datasets, ex-
cept for SICK when fine-tuned on MNLI. This
shows that HyPe can mitigate model over-fitting to
domain-related features. Therefore when the do-
main of downstream tasks varies, PLMs fine-tuned
with HyPe can still have good performance.
Both generalization probing experiments above
demonstrate that HyPe can help PLMs avoid repre-
sentation collapse and over-fitting to the fine-tuning
data, hence obtaining good generalization across
tasks and domains.
5.5 Discussions
Do the noise forms and scales matter? Here
we discuss how performance varies given different
noise distributions and scales σ.
In Table 8, we can conclude from the results that
1) given different distributions and scales, HyPe
consistently outperforms vanilla fine-tuning; 2) for
different tasks the best choice for distributions and
scales may differ: for example, on CoLA, the lan-
guage acceptability task, the best choice is using
a normal distribution with small scale σ= 10,
while on MRPC, the semantical equivalence task, it3252
is better to use uniform distribution with the scale
ofσ= 10.
Relation with Dropout Note that in the afore-
mentioned experiments we turn off dropout when
using HyPe. When combining HyPe-N with
dropout, we empirically find that the performance
degrades. The average score drops from 80.75 to
79.92, as shown in Table 9. The possible explana-
tion is that the improvement brought by dropout
and that by HyPe partly overlap, since dropout
randomly sets entries of hidden representations to
zero, which can be regarded as a discrete form of
0/1 noise multiplied to different hidden representa-
tions where each entry of noise obeys a Bernoulli
distribution. In terms of HyPe, we add continu-
ousrandom noise to the hidden representations.
Empirically our HyPe shows superior performance
than dropout, as in vanilla fine-tuning we apply 0.1
dropout rate. Therefore, adding continuous noise to
the hidden representations in HyPe can be a good
alternative for the discrete noise of dropout.
We leave the discussions of adding noise only
to hidden representations of a subset of layers and
adding additional noise to the representations of
self-attention mechanism outputs inside each Trans-
formers layer to Appendix G.
6 Conclusion
To conclude, we introduce HyPe, a technique to
improve PLM fine-tuning. HyPe enhances fine-
tuning by perturbing the intermediate hidden repre-
sentations of a PLM with task and model agnostic
random noise. Through experiments on GLUE
and other NLI tasks, we demonstrate that PLMs
fine-tuned with HyPe have better performance and
transferability in comparison to vanilla fine-tuning,
especially in a low-resource scenario. In further
analyses, without additional regulation like KL-
divergence and computational overheads, HyPe ob-
tains superior performances compared to existing
state-of-the-art fine-tuning techniques, and can fur-
ther boost fine-tuning combined with others. Fine-
tuning with HyPe improves hidden representations
across different layers and provide stable improve-3253ments for generalization, adversarial attack and
different model scales.
Limitations
Collapsed fine-tuning runs mostly occur in the low
resource scenario where PLMs may easily over-
fit to the small data. The improvement with the
proposed technique becomes marginal when the
amount of training data scales up, as shown in Ta-
ble 2. The other limitation is that HyPe introduces
two new hyper-parameters: The noise distribution
form and the scale of variance. To achieve the best
performance, we may need to search for different
combinations of hyper-parameters.
Ethic Statement and Broader Impact
As the parameter scale of PLMs and the pre-
training cost get much larger hence showing better
brilliant performance in language modeling, it is
necessary to improve the fine-tuning performance
of the language model in an effective and efficient
way. Our proposed HyPe improves large PLM
fine-tuning by only adding noise to the hidden rep-
resentations. Unlike previous works, we do not
include additional regulations since additional reg-
ulations may require non-negligible computational
resources which may increase as the scale of PLM
gets larger. It is important to develop effective
fine-tuning techniques that are efficient and easy
to implement. Through extensive discussions of
HyPe, we illustrate that including perturbations in
the features or representations could be the key
part of why previous techniques work. Besides,
we show that our HyPe can be a good continu-
ous noise alternative for the widely-used dropout
which can be regarded as 0/1 discrete noise mul-
tiplied to hidden representations. How and where
to include perturbations and which forms of per-
turbations to apply to the fine-tuning of language
models is worth studying and would be beneficial
for advancing NLP frontiers.
Acknowledgments
This work was supported by Alibaba Group
through Alibaba Research Intern Program.
References325432553256A General Experiment Settings
On each experiment with each PLM, we run
for three different random seeds for the aver-
aged results and we grid search on learning
rates of {1,2,3,4} ×10for the best results.
Across different PLMs and tasks, we use AdamW
(Loshchilov and Hutter, 2019) as the optimizer with
Adam βof (0.9,0.99), Adam ϵof1×10and 0.1
weight decay. For the learning rate scheduler, we
use a linear decay scheme. We truncate all the
inputs to a length of 128 tokens. In vanilla fine-
tuning, we use 0.1 dropout rate. For HyPe-N and
HyPe-U, we use the best results of the scale 10
and10and turn off dropout if not otherwise
specified.
All our experiments are conducted on 32G
NVIDIA V100 GPU in a single GPU setting.
B Experiments on GLUE
B.1 Data Introduction
The summary statistics of GLUE and the re-
ported evaluation metric is listed in Table 10. The
license for GLUE is CC-BY-4.0.
B.2 Experiment Settings
For different fine-tuning techniques, we experiment
with the same hyper-parameter setting, which are
listed in Table 11.
B.3 GLUE Test Set Results
The conventional evaluation procedures of the pre-
vious research (R3F, RDrop, ChildTuning, Noisy-
Tune) only report results on development sets of
GLUE. Here we compare the vanilla fine-tuned re-
sults with HyPe fine-tuned results on test sets. Re-
sults listed in Table 12 show that on the averaged
scores (column A VG.-ALL) of 8 GLUE tasks ex-
cept WNLI and AX, HyPe-N and HyPe-U achieve
82.27 and 82.20 for BERT, as well as 85.71 and
86.02 for RoBERTa, which is obviously better thanvanilla fine-tuning of 81.40 for BERT and 84.94
for RoBERTa. The improvements are more higher
on 4 relatively small datasets (column A VG.), and
HyPe-(N/U) achieves 2.30/1.90 and 1.17/1.92 for
BERT and RoBERTa respectively. The results are
consistent with those in Table 1 and 2 where HyPe
can bring more performance gains on small data
setting, since PLMs are prone to over-fitting more
given small data.
C Generalization Probings
C.1 Dataset Introduction
The summary statistics of the NLI datasets SNLI,
SICK and ScitaiL used in domain generalization
probing experiments are presented in Table 13. The
licenses for SICK and ScitaiL are CC-BY-NC-SA-
3.0 and Apache-2.0 respectively.
C.2 Experiment Settings
Task Generalization We freeze the model pa-
rameters fine-tuned on SST2 except for fine-tuning
a re-initialized linear head for each task. For each
experiment, we use a learning rate of 0.001 for
3 epochs and batch size 16 for tuning the linear
heads.
Domain Generalization We train on the sub-
sets for 3 epochs with batch size 16. For different
datasets we used, their label spaces are different as
shown in Table 14. Therefore, we follow the exper-
iment settings in Xu et al. (2021). Since SciTaiL
only contains two labels entailment andneutral
in their label spaces, we map the contradiction
label in MNLI, MNLI-mm, SICK and SNLI to
neutral to reduce their label space to entailment
and neutral . For QQP, following Gong et al.
(2018), we map duplicate toentailment and
not duplicate tocontradiction . With the
above procedures, we create a consistent label
space for each dataset to run evaluations. Besides,
for some samples in SNLI, there exists no golden la-
bels, and we filter them for training and evaluation.
For the datasets used, we use their corresponding
development sets for evaluation.
D With Other Techniques
D.1 Baseline Techniques
Different previously proposed effective fine-tuning
techniques have exclusive hyper-parameters,
we list the hyper-parameters we used in our
re-implementation in Table 15. For each, we3257
follow the best settings reported in their papers.
For ChildTuning, we use the Python code
implementation from https://github.com/
alibaba/AliceMind/tree/main/ChildTuning .
For R-Drop, we use the implementation
in https://github.com/dropreg/R-Drop .
For R3F, we use the implementation from
https://github.com/facebookresearch/
fairseq/tree/main/examples/rxf . Note thatin the original R3F implementation, they leave
out STS-B task as this is a regression task and
is not compatible with KL divergence. In our
implementation, for STS-B task, we use mean
squared error (MSE) in place of KL divergence for
regulation.
D.2 Combination Experiments
We use the HyPe variant HyPe-N with scale σ=
10to integrate with others. When combining
with Child-Tuning, we add HyPe to the forward
computations. When combining with R3F, we use
HyPe for the noised forward computation. When
combining with R-Drop, we add HyPe to two
forward computations in a training step with no
dropout. When combining with NoisyTune, we
add the noise to the parameters before fine-tuning
with HyPe. For the combination experiments, we
also search on the same ranges of hyper-parameters
for the best result.
D.3 Detailed Results for Technique
Combination
The detailed results for Figure 2 are listed in Table
16.3258
E Experiment Details for advGLUE
AdvGLUE (Wang et al., 2021) contains the five
adversarial perturbed datasets in GLUE which are
SST-2, QQP, MNLI, RTE and QNLI. For MNLI
there are MNLI-match and MNLI-mismatch. They
use the original training data from the correspond-
ing datasets in GLUE for model training. In our
experiments, each results listed in Table 6 are aver-
aged out of 3 random seed runs.
F Experiment Details for Parameter
Scaling Experiments
When using vanilla fine-tuning schemes as settings
listed in Table 11 will lead to corrupted and
sub-optimal performances for DeBERTa. To
reproduce a strong vanilla baseline for solid
comparison, (1) we extend the training epochs
to 6 and use a fixed warm-up step 100; (2)
for MRPC, RTE and STS-B, we fine-tune
based on MNLI-tuned models, which are
deberta-base-mnli , deberta-large-mnli ,
deberta-v2-xlarge-mnli and
deberta-v2-xxlarge-mnli from Hugging-face repository, and for CoLA, we use the origin
pre-trained versions , which are deberta-base ,
deberta-large , deberta-v2-xlarge and
deberta-v2-xxlarge from Huggingface
repository; (3) for the xlarge and xxlarge
versions of DeBERTa’s, we additionally
search for best results on learning rates
{1×10,3×10,5×10,8×10}.
G More Discussions
G.1 Token Representation Similarity
As mentioned above in the generalization probing
experiments, the representation abilities of hidden
states are ameliorated. To further investigate how
HyPe improves PLMs fine-tuning, we investigate
the change of hidden representations. As illus-
trated by previous research (Ethayarajh, 2019; Gao
et al., 2019), PLMs may suffer from the problem
of anisotropic distribution of token representations
(i.e., the representations only distributed in a nar-
row cone of the entire high-dimensional space).
Research finds a correlation between isotropic dis-
tribution of representations and downstream perfor-
mance (Su et al., 2021; Yu et al., 2022). Isotropic-3259
distributed hidden representation is a good property
in terms of good representation abilities. Represen-
tation anisotropy can be accessed by calculating
the token-wise cosine similarity within a sample.
The lower similarity indicates a more isotropic dis-
tribution.
For the calculation of layer-wise token cosine
similarity, we denote the index of each sample as
i, the token index in each sample as j. The layer
index is denoted as l. The calculation of similarity
scoreSfor layer land sample iis:
S=2
n(n−1)/summationdisplaycos(h, h),
where nis the token count of sample i,hstands
for the hidden representation of token jin sample
iin layer landcosstands for the cosine similarity
cos(q, p) =. Then the score is averaged
over different samples:
S=1
M/summationdisplayS,
where Mis the number of samples.
With isotropic distribution where similarity val-
ues are larger, transformers layers do not show
degeneration and maintain good representation ca-
pacities. Hidden states may carry diverse useful
information to each token in the next layer throught
attention mechanism. We investigate the similar-
ity to provide insight on how HyPe improve final
results.
In Figure 4, we provide a line plot on how hid-
den presentation similarity varies across layers. For
each point, the results are averaged across samples
and 3 different runs. We can see that the anisotropic
distribution problem gets severe for the higher lay-
ers. Models fine-tuned with HyPe have lower hid-
den representation similarity compared to vanilla
fine-tuned PLMs on the top layers. For the lower
layers, three lines are entangled, and this finding is
consistent with that in Section 5.4.
It is worth noticing that for token similarity on
CoLA, although HyPe-U has lower similarity on
the last layer, while has lower performance than
HyPe-N in Table 1. There may seem a contradic-
tion between results. However, HyPe-N achieves
better similarity on other higher layers. As HyPe is
added to all different layers and information from
intermediate layers influences that from the last
layer, the results are also consistent.
In summary, inspired by previous research on in-
terpreting PLMs, we empirically provide an insight
that HyPe may improve fine-tuning by making hid-
den representations isotropic-distributed.
Adding noise after self-attentions. In HyPe, we
add noise to the hidden representations between
each Transformer layer, and compared to dropout,
HyPe empirically shows better performance. These
findings lead to this discussion of adding noise to
the representations between self-attention and feed-
forward network within Transformers layer like
dropout, as illustrated in Figure 5. We run exper-
iments on CoLA, STS-B, MRPC, and RTE with
different schemes of adding noise. Experiments are3260conducted on BERT-large.
As shown in Table 17, in terms of average scores,
HyPe-N with scale σ= 10(i.e., only adding
noise between Transformers layers) shows the best
performance, while adding noise only within Trans-
formers shows the worst result among the three.
When combining both positions to add noise, the
performance shows no improvements on perfor-
mances.
Adding noise to a subset of hidden represen-
tations. HyPe adds random noise to the hidden
representations of all Transformers layers. We run
further analyses by only adding noise to hidden
representations fed into a subset of layers. We
add normal noise with scale σ= 10to the hid-
den representations in the higher 6/12 layers and
lower 6/12 layers of BERT-large. The higher lay-
ers mean the layers near the classifier head, while
the lower layers mean the layers near the token
embedding layer. As shown in Table 18, from the
average scores across MRPC, STS-B, CoLA, and
RTE datasets, we can conclude that 1) when adding
noise on the higher layers is better than adding on
the lower layers; 2) Noise added to more layers
will obtain better performance.32613262ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitation section in main texts
/squareA2. Did you discuss any potential risks of your work?
Ethic Statement and Broader Impact section in main texts
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
In Introduction and Abstract section in main texts
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 4.1 and Appendix B
/squareB1. Did you cite the creators of artifacts you used?
Section 4.1 and Appendix B C1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 4.1 and Appendix B C1
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 4.1 and Appendix B C1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 4.1 and Appendix B C1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4.1 and Appendix B C1
C/squareDid you run computational experiments?
Section 4.1 and Appendix A
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 4.1 and Appendix A3263/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4.1 and Appendix A
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4, 5 Appendix A
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4.3 and Appendix F
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.3264