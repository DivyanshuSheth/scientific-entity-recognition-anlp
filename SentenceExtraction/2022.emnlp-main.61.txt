
Aniruddha Mahapatra, Sharmila Reddy Nangi
Aparna Garimella, Anandhavelu NatarajanCarnegie Mellon University, USAStanford University, USAAdobe Research, India
amahapat@andrew.cmu.edusrnangi@stanford.edu
{garimell, anandvn}@adobe.com
Abstract
Transformer-based language models trained
on large natural language corpora have been
very useful in downstream entity extraction
tasks. However, they often result in poor perfor-
mances when applied to domains that are differ-
ent from those they are pretrained on. Contin-
ued pretraining using unlabeled data from tar-
get domains can help improve the performances
of these language models on the downstream
tasks. However, using all of the available unla-
beled data for pretraining can be time-intensive;
also, it can be detrimental to the performance
of the downstream tasks, if the unlabeled data
is not aligned with the data distribution for the
target tasks. Previous works employed exter-
nal supervision in the form of ontologies for
selecting appropriate data samples for pretrain-
ing, but external supervision can be quite hard
to obtain in low-resource domains. In this pa-
per, we introduce effective ways to select data
from unlabeled corpora of target domains for
language model pretraining to improve the per-
formances in target entity extraction tasks. Our
data selection strategies do not require any ex-
ternal supervision. We conduct extensive ex-
periments for the task of named entity recog-
nition (NER) on seven different domains and
show that language models pretrained on tar-
get domain unlabeled data obtained using our
data selection strategies achieve better perfor-
mances compared to those using data selection
strategies in previous works that use external
supervision. We also show that these pretrained
language models using our data selection strate-
gies outperform those pretrained on all of the
available unlabeled target domain data.
1 Introduction
Named entity recognition (NER) (Lample et al.,
2016; Nadeau and Sekine, 2007; Finkel and Man-
ning, 2009) is the task of extracting entities from
a given piece of text. NER is useful for severalnatural language processing (NLP) applications
such as information extraction (Chiticariu et al.,
2013), retrieval (Banerjee et al., 2019), and lan-
guage understanding. Over the years, various
approaches including rule-based techniques (Far-
makiotou et al., 2000), unsupervised learning ap-
proaches (Luo et al., 2019), feature-based extrac-
tions (Li et al., 2020) have been explored for NER.
The recent deep learning-based methods are shown
to result in state-of-the-art performances for vari-
ous domains (Chiu and Nichols, 2016; Peters et al.,
2018; Yadav and Bethard, 2018).
Specifically, large pretrained language models
such as BERT (Devlin et al., 2019) are widely
used for NER, as they mark state-of-the-art perfor-
mances (Jia et al., 2019). However, these models re-
quire large amounts of annotated data which stands
as a bottleneck to the training process, particularly
when domains having few labeled data are involved.
Directly fine-tuning BERT model on small collec-
tion of labeled data can yield sub-optimal results.
To address this issue, there have been works on
pretraining strategies (Liu et al., 2020; Gururangan
et al., 2020) to adapt BERT-like language models
to target domains by further pretraining them on
unlabeled target domain data prior to fine-tuning
on labeled data to improve performances in tasks
like NER. However, further pretraining language
models on all of the available unlabeled data from
target domains can be sometimes detrimental to the
downstream NER performances, if the entire unla-
beled data is not aligned with the entity distribution
of the labeled data (Liu et al., 2020).
Specifically, Liu et al. (2020) used two types of
unlabeled corpora belonging to the target domain,
namely task-level corpus , consisting of unlabeled
sentences from the target domain that are highly
aligned with the distribution of the labeled down-
stream NER dataset, and domain-level corpus ,
consisting of a very large collection of unlabeled
sentences from the target domain, excluding sen-942tences from the task-level corpus. Gururangan et al.
(2020) showed the advantages of pretraining on
both domain-level and task-level corpora for im-
proving NER performance. However, using the
task-level corpus only for further pre-training may
be inappropriate, as this is usually very small in
size. Additionally, pretraining on the entire domain-
level corpus can be challenging, particularly in
resource-constrained settings, and can sometimes
produce sub-optimal results due to non-alignment
and distribution mismatch. Liu et al. (2020) intro-
duced a strategy to select samples from domain-
level corpus to augment the task-level corpus using
external supervision in the form of ontologies to
improve the downstream NER performance. While
this marks the importance of data selection for pre-
training using ontology-based filtering, it is con-
strained by the availability of meta-data and entity
information from Wikipedia, and thus, is not easily
extensible to other low-resource domains.
In this paper, we introduce new methods for data
selection via (i)cosine similarity-based retrieval in
embedding space of domain-level corpus generated
using pretrained and fine-tuned BERT (fine-tuned
on labeled target domain data), (ii)entity predic-
tion on domain-level corpus using fine-tuned BERT.
Note that both these strategies do not require any
additional supervision ( e.g., in the form of ontolo-
gies) or metadata, thus making them easily extensi-
ble to other low-resource domains. Similar to our
method, Dai et al. (2019) proposes unsupervised
methods of computing similarities between differ-
ent corpora based on word-embedding and perplex-
ity scores of bert-base-cased . However, unlike our
method of selecting sentences from the domain-
level corpus based on similarity scores, Dai et al.
(2019) uses the similarity scores to select the most
appropriate source domain (out of multiple source
domains) for a particular target domain (in our set-
ting, both domain-level and task-level corpus be-
long to the same domain). We evaluate our method
on i2b2 (Uzuner et al., 2007), Atticus (Hendrycks
et al., 2021), and five other domains curated by (Liu
et al., 2020) with different amounts of selected sen-
tences for pretraining. Experimental results show
that our proposed methods for data selection result
in better performances in downstream NER tasks,compared to those proposed by previous works that
require external supervision.
2 Task Definitions
Following the notions in Liu et al. (2020), let
D={(x)}, and T={(x)}denote
thedomain-level and the task-level unlabeled cor-
pora having same domain as L={(x, y)},
which denotes labeled corpus tagged with BIOES
for NER, where L⊆T, andN≫N≥N.
Given a large unlabeled Dand relatively small
T, our task involves selectively sampling sen-
tences S={(x)}from D, where S⊂
DandN≫N, such that pretraining on T
augmented with Sincreases the performance for
NER, without having to pretrain on entire D,
which can be very time and resource intensive.
For a clearer understanding of D,T, and
Lwe present an example scenario involving
these 3 different corpus types: Let Xbe a small-
scaled legal firm that has a limited collection of
customer contracts. To ease their workflow, they
want to automate the process of extracting named
entities (like contracting-party, contract-amount ,
etc.) from contracts using NER models. How-
ever, they don’t have permission to outsource all
the customer contracts to external annotators to
create labeled data for NER due to legal restric-
tions. Out of all the customer contracts, only
a very small number (without legal restrictions)
can be outsourced for annotations. They also
have a large collection of unlabeled SEC contracts
(https://www.sec.gov/edgar.shtml) that are publicly
available (however, these SEC contracts are some-
what different from customer contracts of Xin
terms of format, layout, etc.). In the context of
our setting all the customer contracts to form T,
the small subset of these contracts that can be used
for annotations form L, and the SEC contracts
constitute D.
3 Methodology
This section describes the details of our stage-wise
method of selecting the pretraining corpus ( S)
from the domain-level corpus ( D) to pretrain
BERT (Devlin et al., 2019), followed by fine-tuning
it on a small labeled corpus ( L) for NER. Figure
1 gives an overview of our data selection strate-
gies for the selective pretraining and the following
fine-tuning stages to train BERT for NER.943
3.1 Data Selection Strategies
Continual pretraining on Dwith Tcorpora
prior to fine-tuning BERT for NER has shown sig-
nificant improvement over just fine-tuned BERT
w/o task-specific pretraining(Gururangan et al.,
2020). However, when size of Dis extremely
large, pretraining on entire corpus can be very
time-consuming and resource intensive (see Table
5). Additionally, Dcan contain noisy and un-
related sentences compared to the T, leading to
reduced effectiveness of domain continual pretrain-
ing. Therefore, we devise unsupervised methods
to select only the useful sentences from Dfor
effective pretraining, without requiring additional
domain metadata, such that it can even be applied
to any real world low-resource domains.
Aharoni and Goldberg (2020) showed the use
of pretrained BERT (Devlin et al., 2019) for ef-
fectively clustering sentences consisting of diverse
domains. Drawn on this observation, we use the
BERT model for selecting sentences from Dthat
are more closely associated to Tbased on cosine
similarity in sentence embedding space. Moreover,
based on results from Liu et al. (2020), sentencesinDthat contain more domain-specialized enti-
ties are shown to be more effective for pretraining
than randomly selected Dsentences. Based on
these observations, we propose four novel cosine-
similarity and NER-Selected based unsupervised
methods for data selection from the domain-level
corpus for further pretraining (Stage I of figure 1):
•Cosine Similarity w/ Pretrained BERT : We
use vanilla pretrained bert-base-cased model
to obtain embeddings of sentences from T
andDas the activations from the last layer
of BERT. We compute the mean task-level
embedding as the average of all task-level cor-
pus sentence embeddings. We then use it as
a query vector to obtain sentences with max-
imal cosine similarity from the domain-level
corpus ( D).
•Cosine Similarity w/ task-level MLM
BERT : We first initialize BERT with bert-
base-cased parameters and then additionally
pretrain the BERT model using masked lan-
guage modeling (details of MLM mentioned
in Section 3.2 and 4.1) on the task-level corpus
(T). This model is referred to as B. We use
Bto select sentences from a domain-level
corpus based on the previous method.944•Cosine Similarity w/ task-level MLM and
NER BERT : We fine-tune Bfor the NER
task (on labeled corpus L), termed B. Us-
ingBandB, we obtain task-level and
domain-level sentence embeddings. We per-
form an element-wise concatenation of em-
beddings obtained from Bwith the ones ob-
tained using B, and then select sentences
using these embeddings like the previous
method on the aggregated embeddings.
•NER-Selected : Instead of using DBpedia On-
tology to select sentences with plentiful enti-
ties as used in Liu et al. (2020), using B, we
obtain entity predictions for the sentences in
the domain-level corpus. Based on the labels
predicted on the domain-level corpus as soft
entity labels, we select the sentences with the
maximal number of predicted entities. Since
Bis trained on Lwith domain-specialized
entities, it will help in selecting sentences that
likely contain similar domain-specialized enti-
ties from D, thus mitigating the problem of
distribution alignment between the NER task
data and data used for further pretraining.
3.2 Domain Pretraining and Fine-tuning
Combining selected sentences ( S) from domain-
level corpus ( D) in Stage I with the task-level cor-
pus (T), in 1:1 ratio, we pretrain the BERT model
(initialized with bert-base-cased parameters) using
masked-language modeling (Stage II in figure 1) as
specified in Devlin et al. (2019). We mask out 15%
of the tokens randomly in the selected sentences
and then replace 80% of the masked tokens with
a special tag ( [MASK ]), 10% with random, and
10% with original tokens.
Finally, we add a Conditional Random Field
(CRF) layer on top of this BERT model and train it
for NER on labeled corpus L(Stage III of figure
1). More details are specified in Section 4.2.
4 Experiments
4.1 Experimental Setup
For all the experiments, we use BERT model
initialized with bert-base-cased (Devlin et al.,
2019) parameters. The total number of trainable
parameter in the BERT model used for experiments
are 110M. In all experiments involving pretraining
using any corpus type, we pretrain BERT with
MLM for 5 epochs with a batch size of 64. Wethen fine-tune this pretrained BERT on NER
task by adding CRF layer on top for 200 epochs
(stopping at early convergence on dev set) with
learning rate 2e−05,L2regularization of 1e−08
and AdamW betas (0.9,0.999) . The size of model
used for NER task is 442.6 MB. All experiments
were performed on Tesla V100 GPU. We evaluate
the performance of all the methods with F1 Score
(https://github.com/allanj/pytorch_neural_crf).
Each experiment is conducted thrice with random
seed and the average score is reported.
4.2 Dataset Details
We evaluate our proposed method on seven di-
verse domain datasets including contracts, medical
records, AI, science, politics, music, and litera-
ture domains. The Atticus dataset comprises of
contracts, i2b2 (Uzuner et al., 2007) contains medi-
cal records of patients, and the other five domain
datasets are taken from Liu et al. (2020) which
contain Wikipedia articles from each of the corre-
sponding domains of AI, science, politics, music,
and literature (for example, Science domain con-
tains Wikipedia articles that fall under Science or
similar categories).
•i2b2 (Uzuner et al., 2007): We transform the
original dataset of 37.1K labeled sentences
into 35.1K domain-level and 2K task-level
sentences unlabeled sentences. We use la-
beled version of the same 2K task-level sen-
tences as Lfor the training of the NER task.
We use the original dev and test data provided
by Uzuner et al. (2007).
•CrossNER (Liu et al., 2020): We directly use
the five different domain datasets provided by
(Liu et al., 2020), namely AI, Science, Lit-
erature, Politics and Music. The dataset is
already split into task-level and domain-level
corpora, with separate train, test, and dev sets
for NER.
•Atticus (Hendrycks et al., 2021): Atticus
is a recently published Question-Answering
dataset for contracts. We only use entity type
contracting-party from this dataset for NER.
We divide all the contracts into splits 70:10:20
for train, dev, and test sets. Out of the 28
different sub-categories of contracts, we se-
lect 6 documents from three sub-categories
(‘Strategic Alliance’, ‘Development’, ‘Distrib-
utor’) as the labeled training set ( L) and all945
the documents under these 3 sub-categories
as task-level sentences ( T) from the 70%
total train split. The rest of the documents
from the total train split (from the remaining
25 sub-categories) that are not included in
the task-level corpus are considered from the
domain-level corpus ( D).
Note that we modify the original i2b2 and At-
ticus datasets to our problem setting (using the
method described above). More details on the num-
ber of domain-level D, task-level T, selected
Ssentences and labeled train, test and dev splits
along with entity types are provided in Tables 1
and 2. For fairness of comparison of our methods
with Entity-level data selection strategy (Section
4.3) used in Liu et al. (2020), we use the same
number of selected sentences ( S) for each of the
Liu et al. (2020) datasets (as mentioned for respec-
tive domains in Entity-level corpus of (Liu et al.,
2020)) for experimentation. For i2b2 and Atticus
datasets, we take the number of sentences in Sto
be approximatelythat of D.
4.3 Baseline Methods
We compare the effectiveness of our data-selection
and pre-training strategies against the following
baselines:
•w/o task-specific pretraining: We use a
bert-base-cased model (Devlin et al., 2019)
and fine-tune it for the NER task on L.
•Task-level pertaining (TAPT) : We initialize
the BERT model with bert-base-cased param-
eters and pretrain on the task-level corpus withMLM, then, fine-tune for the NER task (anal-
ogous to TAPT in Gururangan et al. (2020)).
•Domain-level pretraining (DAPT) : We first
initialize the BERT model with bert-base-
cased parameters and pretrain on the domain-
level corpus with MLM, followed by fine-
tuning for the NER task (analogous to DAPT
in Gururangan et al. (2020)).
•Task w/ Domain-level pretraining (TAPT
+ DAPT) : We combine the domain-level and
task-level corpora in 1:1 ratio. We initialize
BERT with bert-base-cased parameters and
pretrain on the combined corpus with MLM,
then, fine-tune for the NER task (analogous to
TAPT + DAPT in Gururangan et al. (2020)).
•Entity-level w/ Task-level pretraining : We
use the entity-level corpus, provided by Liu
et al. (2020), that comprises of sentences from
domain-level corpus having plentiful entities
which are obtained by leveraging knowledge
from DBpedia Ontology. We combine the
entity-level and task-level corpora in 1:1 ratio
for pretraining (integrated corpus mentioned
in Liu et al. (2020)). Since we use token-level
MLM in all our methods, for fair comparison
of the effectiveness of our selected sentences
that of Liu et al. (2020), we compare with
their method which performs token-level pre-
training with integrated corpus.
•Perplexity Score (PPL) : Dai et al. (2019)
uses language model perplexity score to select
the most appropriate source domain, from a946
collection of different domains for a given tar-
get domain. Specifically, they train a model
on target corpus and compute perplexity score
of each source corpora using this model, and
then select the source that gives the lowest
perplexity score. Instead, we modify this
method for our scenario by first training a
BERT model on the task-level corpus and us-
ing it to select sentences from domain-level
corpus having minimum perplexity.
5 Results and Analysis
5.1 Performance v/s Data Selection Strategies
From the results of our experiments in Table 3, we
see that pretraining using selected sentences us-ing our methods outperform all baselines for all of
the domain datasets. Figure 3 (b) shows qualita-
tive examples of NER predictions from the Atti-
cus test set, using our method: ‘Cosine-Similarity
w/ MLM and NER tuned BERT’ and ‘w/o task-
specific pretraining’. Our method is able to predict
‘contacting-party‘ much more accurately than the
‘w/o task-specific pretraining’ model. According
to Table 2, although the size of the selected cor-
pusSis around half or less than half that of the
domain-level corpus, the pretraining using Sim-
proves NER performance than pretraining on D
orDandTcombined. Based on this it can be
hypothesized that selection of sentences using co-
sine similarity over BERT embeddings filter many
noisy sentences that might not be very related to the
task-domain corpus as only the top-most sentences
are selected that lie close to mean task-level corpus
embedding. Additionally, NER-Selected method
removes sentences that might not contain domain-
specific entities related to the particular NER task,
increasing the effectiveness of pretraining (See fig-947
ure 3 (a)). Furthermore, integrating the selected
corpus and task-level corpus is able to consistently
boost the downstream NER performance compared
to utilizing other corpus types although the size of
this corpus is still smaller than the domain-level
corpus. This is because it ensures that the pre-
training corpus contains content that is explicitly
related to the NER task in the target task-level cor-
pus while also making it relatively larger than the
task-level corpus itself. The results suggest that the
corpus content is essential for the effectiveness of
continual pretraining. Surprisingly, this pretraining
strategy is still effective for the i2b2, Atticus, and
AI domains even though the corpus sizes in these
domains are relatively small, which illustrates the
effectiveness of our method in settings with less
availability of unlabeled domain-level corpus.5.2 Performance v/s Pretraining Corpus Size
Since such large domain-level corpora might not be
always available in real-world scenarios, we investi-
gate the performance of pretraining on the different
quantities of selected sentences ( S). From figure
2, it is evident that as the number of selected sen-
tences increases, the performance keeps improving
(in this figure it is shown for both Literature and
Music domains). This implies that a larger corpus
size is better for pretraining. However, the slope of
the performance gain curve becomes less when we
increase the number of sentences for MLM beyond
50% of S. Our method of pretraining can also be
used in resource-constrained settings, where it is
infeasible to pretrain with MLM on extremely large
corpora. Table 5 shows the performance with the
time required for MLM pretraining on different cor-948
pus sizes and types for two different domains with
two different methods of data selection. For the Lit-
erature domain with ‘Cosine-Similarity w/ MLM
and NER tuned BERT’, we see that we achieve
an F1 of 64.67when using TwithScompared
to pretraining using entire Dalong with T(F1
64.57), even though it takes less than 2.5 times the
time required for MLM pretraining with almost
half the number of total sentences. A similar trend
can be observed for the Music domain.
5.3 Fine-grained Comparison
In this section, we explore the effectiveness of our
pretraining strategy using selected sentences on in-
dividual entities belonging to the same domain over
‘Domain-level pretraining (DAPT)’ method. Table4 shows the performance of the majority of the en-
tities for the Literature domain using two methods
‘Domain-level pretraining (DAPT)’ and ‘Cosine-
Similarity w/ MLM and NER tuned BERT’. We
see that the performance has increased for all entity
types when pretraining on selected sentences using
our method has been used. We observe that the
performance on the person entity type is compar-
atively lower than all other entity types for both
methods. It may be due to the hierarchical category
structure of this domain that causes the model to get
confused between person andwriter entity types.
It can be also seen that the performance of domain-
specific entities (like ‘book’, ‘writer’, ‘event’ and
‘magazine’) has improved much more than the im-
provement of the non domain-specific generic en-949tity types (like ‘person’, ‘location’, ‘organization’)
when our method of ‘Cosine-Similarity w/ MLM
and NER tuned BERT’ is used over ‘Domain-level
pretraining (DAPT)’ for pretraining.
5.4 Performance v/s Labeled Corpus Size
From figure 4, it can be inferred that the perfor-
mance decreases drastically as the number of la-
beled samples in Lused for fine-tuning BERT-
CRF is reduced. Specifically, the performance be-
comes extremely low for Literature when there
are only 10 labeled samples (18.3 F1 for ‘Cosine-
Similarity w/ MLM tuned BERT’ and 15.45 F1
for ‘Domain-level pretraining (DAPT)’). Although
the performance of our method is always better
than ‘Domain-level pretraining (DAPT)’ when the
number of labeled samples are extremely low (10),
the improvement from our method over ‘Domain-
level pretraining (DAPT)’ is significant ( ∼18%
increase), and this margin of improvement gets
smaller as we increase the number of labeled sen-
tences ( ∼2% at 100 labeled samples). This can
be explained by the fact that the BERT model is
able to gain domain knowledge from the pretrain-
ing data and thus has the ability to better predict
entity labels.
6 Conclusion
We propose novel methods of data selection tech-
niques, which do not require additional external
supervision, for pretraining BERT for NER. In ad-
dition to being illustrated on 7 diverse domains, our
method can be easily extended to NER for any new
domain with very scarce labeled data and plenty
of domain unlabeled data. Experiments on multi-
ple domain datasets demonstrate that our selection
techniques are better than naive pretraining on the
entire domain corpus and also achieve better perfor-
mance compared to state-of-the-art data selection
methods using external knowledge bases. Since
we may not get domain labels for every corpora, in
future we will try to extend our work with corpus
consisting of a mixture of different domains.
7 Limitations
Our method works well when both the task-level
and domain-level corpus to belong to the same do-
main or in similar domain, i.e, if the task-level
corpus belongs to the science domain, then the
domain-level corpus should also belong to sci-
ence/similar domain. However, since clear domainlabels may not be present for all corpora in the wild,
one may have to choose an appropriate domain-
level corpus corresponding to the task-level cor-
pus. The second limitation of this work is that
we assume the domain-level corpus consists only
of a single domain, though, a real-world corpus
might consist of mixture of sentences belonging
to different domains. In this work, we have not
verified how effective our selection strategies will
be in selecting sentences for pretraining in such a
scenario where domain-level corpus consists of a
mixture of domains (with domains present other
than the domain label of task-level corpus). Ad-
ditionally, all our data selection strategies, except
the ‘Cosine-Similarity w/ pretrained BERT’ use
domain-specific models of data selection, hence
there can not be a unified model that can be ap-
plied for data selection for any domain without
additional pretraining.
References950951