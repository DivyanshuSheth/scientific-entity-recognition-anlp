
Puneet Mathur, Riyaz Bhat, Gautam Kunapuli, Manish Shrivastava,
Dinesh Manocha, and Maneesh SinghUniversity of Maryland, College Park, MD, USAIBM, Bengaluru, IndiaMotive, San Francisco, USAIIIT, Hyderabad, India
Abstract
We present DocInfer - a novel, end-to-end
Document-level Natural Language Inference
model that builds a hierarchical document
graph enriched through inter-sentence rela-
tions (topical, entity-based, concept-based),
performs paragraph pruning using the novel
SubGraph Pooling layer, followed by optimal
evidence selection based on REINFORCE al-
gorithm to identify the most important context
sentences for a given hypothesis. Our evidence
selection mechanism allows it to transcend the
input length limitation of modern BERT-like
Transformer models while presenting the en-
tire evidence together for inferential reasoning.
We show this is an important property needed
to reason on large documents where the evi-
dence may be fragmented and located arbitrar-
ily far from each other. Extensive experiments
on popular corpora - DocNLI, ContractNLI,
and ConTRoL datasets, and our new proposed
dataset called CaseHoldNLI on the task of legal
judicial reasoning, demonstrate significant per-
formance gains of 8-12% over SOTA methods.
Our ablation studies validate the impact of our
model. Performance improvement of ∼3−6%
on annotation-scarce downstream tasks of fact
verification, multiple-choice QA, and contract
clause retrieval demonstrates the usefulness of
DocInfer beyond primary NLI tasks.
1 Introduction
Natural Language Inference (NLI) is a fundamental
textual reasoning task seeking to classify a pre-
sented hypothesis as entailed by, contradictory
to or neutral to a premise (Dagan et al., 2010).
Prior NLI datasets and studies have focused on
sentence-level inference where both the premises
and hypotheses are single sentences (SNLI (Bow-
man et al., 2015), MultiNLI(Williams et al., 2018),
QNLI and WNLI(Wang et al., 2018)) Document-
level NLI extends the reasoning of NLI beyondsentence granularity where the premises are in the
document granularity, whereas the hypotheses can
vary in length from single sentences to passages
with hundreds of words(Yin et al., 2021).
Document level NLI is an important problem
for many tasks including verification of factual cor-
rectness of document summaries, fact-checking
assertions against articles, QA on long texts, legal
compliance of contracts, etc. Even so, it challenges
modern approaches due to the limited input bot-
tleneck of modern Transformer models. Consider
that the universally used BERT model (Devlin et al.,
2018) can only encode 512 input sub-tokens due
to its quadratic self-attention complexity. Conse-
quently, evidence in the document premise relevant
to the hypothesis can potentially be distributed in
several textual spans located arbitrarily far away
from each other in long documents, and may not
be simultaneously available to draw inference.
Recent approaches, notably SpanNLI (Koreeda
and Manning, 2021), HESM (Hanselowski et al.,
2018)) and others, have shown that chunking the
premise into multiple document spans, scoring
them, and aggregating the scores helps mitigate the
limited input length problem. Such approaches do
not allow the inference module to reason over the
complete evidence. In contrast to encoding the doc-
ument as a set of sentences fed into a transformer
for inferential reasoning, a recent line of work, e.g.
EvidenceNet (Chen et al., 2022), GEAR (Zhou
et al., 2019) and HGRGA (Lin and Fu, 2022)),
encodes documents as graphs and uses graph rea-
soning to perform textual inference. Graphs allow
encoding of various morphological and semantic re-
lationships at various granularities. However, these
approaches use graph-based processing subsequent
to evidence selection.
We address the above challenge with a reason-
able assumption that the portion of the premise
(the ground truth evidence) necessary and suffi-
cient for inference can fit entirely into the length809limit of language model for effective representa-
tion learning. Our proposed system achieves this
by selecting sentences in the document that are con-
textually relevant for a given hypothesis through
pruning irrelevant paragraphs and reinforce learn-
ing based optimal sentence selection. Our main
contributions:
•DocInfer – a novel DocNLI model that si-
multaneously performs successive optimal ev-
idence selection and textual inference on large
documents. It utilizes a novel graph represen-
tation of the document encoding structural,
topical, concept and entity-based relation-
ships. It performs subgraph pooling and asyn-
chronous graph updates to provide a pruned,
hypothesis-relevant and richer sub-document
graph representation and uses a reinforcement-
learning based subset selection module to pro-
vide the contextually-relevant evidences for
inference. Experimental results show that
DocInfer outperforms the current SOTA on
DocNLI, ContractNLI and ConTRoL datasets
with a significant improvement of 8-12%.
•We propose CaseHoldNLI - a new
document-level NLI dataset in the domain
of legal judicial reasoning with over 270K
document-hypotheses pair with maximum
premise length of 3300 words. We observe
similar performance gains on this dataset.
•Application on downstream tasks: We
demonstrate the usefulness of the DocInfer
evidence selection module on downstream
tasks of fact verification, multiple choice QA
and few shot clause retrieval from legal texts
using no or small amounts of data for super-
vised fine-tuning. Results on FEVER-binary,
MCTest and Contract Discovery dataset show
significant improvement of ∼3-6% F1.
2 Related Work
Document-level NLI Datasets : Yin et al. (2021)
introduced Doc-level NLI on news and Wikipedia
articles. Liu et al. (2021) proposed the multi-
paragraph ConTRoL dataset focused on complex
contextual reasoning (logical, coreferential, tem-
poral, and analytical reasoning). Several datasets
comprising legal documents like case laws, statutes,
and contracts have been proposed. COLIE-2020
(Rabelo et al., 2020) and Holzenberger et al. (2020)
support identification of relevant paragraphs from
cases that entail the decision of a new case. How-ever, the combined input length of their premise-
hypothesis pairs remains within 512 tokens with
the premise lengths at paragraph-level, reasonably
suited for input to BERT-like models. Koreeda and
Manning (2021) released ContractNLI dataset for
document-level NLI task on multiple page NDA
contract documents along with ground truth evi-
dence labeling for interpretability. We benchmark
DocInfer on DocNLI, ContractNLI and ConTRoL
datasets. CaseHOLD dataset (Zheng et al., 2021) is
a multiple choice QA dataset for selecting relevant
governing laws required to reason about the legal
decision text. Document-scale and Corpus-Scale
Reasoning : In order to handle document-scale
premises in the doc-NLI corpora, approaches like
SpanNLI (Koreeda and Manning, 2021), HESM
(Hanselowski et al., 2018)) chunk the premise into
multiple document spans for reasoning. A similar
approach was followed by legal language models
such as Legal-BERT (Chalkidis et al., 2020) and
Custom Legal-BERT (Zheng et al., 2021) for legal
reasoning tasks. More recently, language models
(e.g., Longformer (Beltagy et al., 2020) with 4096
token input) have been proposed to overcome the
limited input field bottleneck. Fact Extraction and
Verification (FEVER) (Thorne et al., 2018) tasks
require extracting evidence and claim entailment
given an input claim and the Wikipedia corpus.
Prior works in this domain address the length lim-
itation for claim verification by relevant evidence
identification and its chunking which are individu-
ally scored and probabilistically aggregated (Sub-
ramanian and Lee, 2020; Jiang et al., 2021). Hi-
erarchical graph modeling may be used to handle
the large scale of the premise (Liu et al., 2019b;
Zhou et al., 2019; Zhong et al., 2020; Zhao et al.,
2020; Chen et al., 2022; Lin and Fu, 2022; Si et al.,
2021). Context Selection for Document-level
NLP : Recent works have investigated selection of
relevant context for document-level NLP tasks such
as Neural Machine Translation (Kang et al., 2020),
Event Detection (Ngo et al., 2020; Veyseh et al.,
2021), Relation Extraction (Trong et al., 2022). Re-
cently, some of the work on document-level NLP
has looked at temporal relation extraction (Mathur
et al., 2021) , temporal dependency parsing (Mathur
et al., 2022b), and speech synthesis (Mathur et al.,
2022a) using graphs and sequence learning. How-
ever, none of them have considered an end-to-end
trainable approach for graph learning with to iden-
tify the relevant evidence extraction.810
3DocInfer
Given a textual hypothesis H, the task of document-
level NLI is to classify whether the hypothesis is
entailed by ,contradicting to ornot mentioned by
(neutral to) the document D. We present DocInfer ,
a neural architecture (Figure 1) that can select a
set of evidence sentences Efrom document Dto
form a shortened document Dwhich is then used
for NLI prediction. Here, for the document level
NLI task, we need to constrain Dto fall within the
length limit of BERT-like context encoder to enable
it to consume the evidence entirely for improved
representation learning for NLI.
Our model can been seen as a sequence of four
phases: (a) Representation of document Din the
presence of the Hypothesis Hto form a hierarchi-
cal document graph with sentences and paragraphs
as nodes and Structural, Topical, Entity-centric and
Concept-similarity relations as edges. (b) Para-
graph node pruning using the novel Subgraph Pool-
inglayer to select highly relevant paragraphs. (c)
Asynchronous graph update for improved node rep-
resentations and finally. (d) Optimal evidence se-
lection using REINFORCE from the graph for the
task of document-level NLI.
Document Representation : Let premise docu-
ment Dbe defined as a sequence of nsentences
s, s,···, ssuch that D= [s, s,···, s].
These sentences are naturally grouped into mcon-
secutive paragraphs P= [p, p,···, p]such
that each each sentence sbelongs to only one
paragraph p. We leverage pre-trained BERT lan-
guage model to obtain the embedding of every
sentence and paragraph nodes. The final rep-
resentation for each sentence sand paragraphpis obtained by extracting the hidden vector
of the CLS token as given by Emb( s) =BERT
([[CLS ];H; [SEP ];s; [SEP ]])and Emb( p) =
BERT ([[CLS ];H; [SEP ];p; [SEP ]]), respec-
tively. Here Hdenotes the hypothesis text which
is also encoded as h=BERT ([[CLS ];H; [SEP ]]).
[CLS ]and[SEP ]are symbols that indicate the
beginning and ending of a text input, respectively.
Document Graph Construction : The document is
then modeled as a hierarchical graph D= (V, E)
to capture the premise document structure. Here,
V={V, V, V}, where V, V, Vare nodes cor-
responding to all the paragraphs, all the sentences
and the hypothesis, respectively. The set of edges
(E) of the Document Graph encodes four types of
relations between the nodes mentioned below:
(1) Structural Relations ( R): Hypothesis-
Paragraph edges and Paragraph-Sentence Affilia-
tion edges model the hierarchical structure of the
document through a directed edge from the hypoth-
esis node to each paragraph node and from a para-
graph node to each constituent sentence, respec-
tively. Further, Paragraph-Paragraph Adjacency
and Sentence-Sentence Adjacency links preserve
the sequential ordering for consecutive paragraph
and sentence nodes through directed edges.
(2) Topical Relations ( R): Sentence-Sentence
Topical Consistency connections model the topical
consistency between a pair of sentences by con-
structing sentence-level topical representations via
latent Dirichlet allocation (Blei et al., 2003). Given
a pair of sentences sands, we extract latent
topic distribution lda, lda∈Rfor each sentence
which are joined if the Helinger H(lda, lda)dis-
tance between them is greater than 0.5.
(3) Entity-centric Relations ( R): Sentence-811Sentence Entity Overlap connections explicitly
model the sentence-level interactions between en-
tity spans by adding an undirected edge between
two sentence nodes if they share one or more
named entities. Further, Sentence-Sentence En-
tity Coreference connections join two sentences by
an undirected edge if the sentences share mentions
referring to the same real world entity.
(4) Concept-Similarity Relations ( R): Sen-
tences conceptually similar to other sentences and
the hypothesis are connected to each other to ac-
count for presence of related events and topics
in two sentences. We propose Sentence-Sentence
ConceptNet Similarity using ConceptNet Number-
batch (CN). Let A= [a, a,···, a]be the Con-
ceptNet Numberbatch embeddings for the words
in sentence s= [w, w,···, w]respectively.
Here, if a word wdoes not have its corresponding
embedding in CN, we simply set its vector ato
zero. Further, we introduce Hypothesis-Sentence
Knowledge Similarity (using KnowBert embed-
ding) connections that add weighted undirected
edges between sentence-sentence and hypothesis-
sentence node pairs, respectively. KnowBert repre-
sentations are obtained by encoding text using the
pre-trained KnowBert language model as A=
KnowBERT ([s]). The edge weights εi, j)be-
tween the input vector pairs ( a,a) is cosine simi-
larity between the knowledge-based semantic em-
beddings of the input texts.
εi, j) =/braceleftbigg
Paragraph Pruning using Subgraph Pooling :
Long documents are structured as a sequence of
paragraphs such that each paragraph may be topi-
cally coherent to itself and neighboring paragraphs.
As such, paragraphs unrelated to a given hypoth-
esis may be ignored to reduce distractor cues.
Graph pooling (Grattarola et al., 2021) is a pop-
ular method for graph coarsening. Unlike previ-
ous methods such as gPool (Gao and Ji, 2019)
and SAGPool (Lee et al., 2019) that pool entire
graph, we propose attention-based Subgraph Pool-
inglayer which can select top rank nodes from a
predefined subset of nodes in the graph. Subgraph
Pooling layer can selectively drop irrelevant para-
graph nodes while retaining the remaining para-
graph nodes, their corresponding sentence nodes
and the hypothesis node in the graph.Suppose there are Nnodes in document graph
Dwith node embedding of size Cwith adjacency
matrix A∈ ℜand feature matrix X∈ ℜ
.We apply GAT (Veli ˇckovi ´c et al., 2017) over D
to obtain self-attention scores Zfor all nodes. The
pooling ratio ηis a hyperparameter that determines
the number of paragraph nodes to keep based on
the value of Z. We want to select the top-rank
nodes only from the set of paragraph nodes. Hence,
we use a hard mask µ={1|x∈P∀X; 0}that is 1
for all paragraph nodes P, otherwise zero. We per-
form an element-wise multiplication ( ⊙) between
attention scores and mask values to get a soft mask
Z=Z⊙µ. Top-rank operation ranks returns
the indices of top ηparagraphs based on Z. Node
indices corresponding to the set of selected top-
ηparagraphs added to the set of sentence nodes
minus those belonging to the pruned paragraphs
(idx) and hypothesis ( idx) are selected
as follows: idx=top-rank (Z, η)+idx+
idx. The combined index tensor (idx) contains
the indices of all the nodes selected in the final
graph D.X(idx,:)and/tildewideA=A(idx,idx)per-
form the row and/or column extraction to form the
adjacency matrix and the feature matrix of D.
The attention scores for selected nodes Zact
as gating weights for node features after filtering
which controls the information flow and makes the
whole procedure trainable by back-propagation as
given by: /tildewideX=X⊙(Z).
Asynchronous Graph Update : Graph Neural Net-
works (GNN) are useful for multi-hop reasoning on
hierarchical graphs comprising of different levels
of granularity (questions, paragraphs, sentences, en-
tities)Fang et al. (2019); Zhang et al. (2020); Chen
et al. (2021). However, GNN’s perform message
passing synchronously at each step of the graph
update, ignoring the fact that different relationship
(edge) types may have different priorities. In or-
der to overcome this challenge, we propose to use
Asynchronous Graph Update (Li et al., 2021) to per-
form sequential graph updates corresponding to all
relationship types in R∈ {R,R,R,R}
to enhance the effectiveness of multi-hop reasoning.
Optimal Evidence Selection ( E): To select
the set of most relevant evidence sentences E, we
hypothesize that a sentence sfrom document D
is important for NLI prediction if including the
corresponding sentence as part of evidence set can
improve the performance of NLI label prediction
model ( M). We design an iterative process812for sentence selection such that at step k+ 1 in
the process (k≥0), a sentence sis chosen
which has not been selected previously in evidence
setE={s∗,···s∗}at step k. We employ a
Long Short Term Memory Network (LSTM) over
previously selected ksentences to select a relevant
sentence at time step k+ 1. At step 0, the initial
hidden state hfor LSTM is set to zero. At step
k+ 1, we use the hidden state hof LSTM from
prior step to assign a score scfor each sentence
nodes∈S−E. The sentence with highest selec-
tion score is considered for selection at this step as
given by sc=sigmoid (FFN ([x:h]))and
s∗=argmax(sc), where FFN is
a two-layer feed-forward network. In particular, if
selecting s∗causes the number of words in the
selected sentences so far to exceed the context en-
coder length limit (eg., 512 tokens for BERT), the
selection process stops and s∗is not included
in the evidence set E (i.e., E={s∗,···, s∗}in
this case). Otherwise, the selection process con-
tinues to the next step and s∗will be chosen
and included in E(i.e.,E={s∗,···, s∗}).
The hidden state of LSTM is also updated for the
current step, i.e., h=LSTM (h, x∗), to
prepare for the continuation of sentence selection.
Evidence Selection Reward Function : In order
to train the evidence selection module, we employ
the REINFORCE algorithm (Williams, 1992) and
incorporate the following information signals in
the reward function of REINFORCE to better su-
pervise the training process. In order to train the
evidence selection module, we employ the REIN-
FORCE algorithm (Williams, 1992). We incorpo-
rate the following information signals in the reward
function of REINFORCE to better supervise the
training process:
(1) Task Reward ϕ: We compute this reward
based on the NLI task prediction performance. In
order to measure the impact of the selected con-
text, we use a T-5 model (Raffel et al., 2019a) pre-
trained on MNLI corpus (Williams et al., 2017) to
predict the NLI label for the given hypothesis +
context pair. ϕ(E)is set to 1 if the final predic-
tion is correct; and 0 otherwise.
(2) Semantic Reward ϕ: We propose that the
evidence sentences should be semantically similar
to the hypothesis. Our motivation is that similar
context sentences (e.g., discussing the same events
or entities) provide more relevant information for
the NLI prediction. We include the semantic simi-larity between the selected evidence sentences in E
and the hypothesis as measured by the cosine simi-
larity (i.e.,/circledottext) between their sentence embeddings
computed using SimCSE(Gao et al., 2021).
(3) Evidence Reward ϕ: We seek to promote
evidence sentences having a high overlap with the
target ground truth evidence. In many cases, the
target evidence length may be way less than 512
token limit. Hence, our motivation is to reward the
lexical overlap while penalizing verbosity arising
at evidence selection stage. We calculate the BLEU
score between the selected evidence Eand ground
truth evidence E:ϕ=BLEU (E, E)This
reward can only be applied for cases where ground
truth evidence annotation is present.
(4) Multihop Reward ϕ : The motivation for
this reward is that a sentence should be preferred
to be included in Eby the selection process if
there are common entities mentions with the hy-
pothesis. Moreover, connected sentences by the
virtue of common entity mentions are more likely
to refer to the same events. Hence, we leverage
the subgraph similarity of the learned node embed-
dings of the selected evidence and their first degree
node connections through entity-centric relations
with the hypothesis node in G. We perform max-
pooling operation over the concatenated node em-
beddings of the corresponding evidence sentences
and their first degree node connections joined
byR:ˆE=maxpool (v/circleplustextv,···, v|s∈
E, i∈ {1,···, k}), where/circleplustextmeans embedding
concatenation. Finally, we compute the dot-product
between ˆEand node embedding of the hypothesis
node hasϕ =ˆE.h.
3.1 Training DocInfer
NLI Prediction Loss : We combine the final
representations corresponding to the learnt graph
structure ( g) and selected evidence text ( t).
We aggregate the embeddings corresponding
to the selected sentence nodes in Dand the
hypothesis node using a summation-based
graph-level readout function (Xu et al., 2018)
asg=ρ(/summationtextWV). The words in the
evidence sentences are joined in order of their
appearance in document Dand input to the context
encoder t=Encoder ([CLS ]s;s,···, s).
g and t are concatenated and passed
through two dense fully-connected layers:
z=ReLU (Dense (t/circleplustextg). This is813followed by a Softmax layer to predict entail-
ment/contradiction/neutral by utilizing the negative
log-likelihood loss: L=−P(y|z).
Evidence Selection Loss : The overall reward
function to train our evidence selection module is
ϕ(E) =ϕ+ϕ+ϕ+ϕ . Using
REINFORCE, we seek to minimize the negative ex-
pected reward ϕ(E)over the possible choices of E
asL=−E[ϕ(E)], and L=
−E[ϕ(E)]∇log(P(E|H, D )).
Finally, the probability of the selected se-
quence Eis computed via P(E|H, D ) =/producttextP(s∗ |H, D, s∗), which is
obtained via softmax over selection scores for
sentences in Sat selection step k+ 1.
Joint NLI Prediction and Evidence Selection :
During training, the NLI prediction model M
and the evidence selection module Eare
trained alternatively. At each update step, E
first selects optimal evidence sentences Ethat form
a shortened document D.MusesEto pre-
dict the NLI label. The parameters of Mare
updated using the gradient of NLI prediction loss
L, keeping parameters of evidence extraction
module constant. Next, the parameters of the evi-
dence selection module are updated using the gra-
dient of L, keeping parameters of Mcon-
stant. This process repeats until convergence. At
test time, evidence sentences are first selected and
then consumed by the prediction model to perform
NLI prediction.
4 Experiments
4.1 Datasets for Document-level NLI
We use the following three datasets to benchmark
document-level NLI approaches. (1) DocNLI (Yin
et al., 2021): A large-scale document-level NLI
dataset obtained by reformatting mainstream NLP
tasks such as question answering and document
summarization. (2) ContractNLI (Koreeda and
Manning, 2021): NLI dataset of 607 contract
documents annotated with ground truth evidence
sentences. (3) ConTRoL (Liu et al., 2021) : A
passage-level NLI dataset of exam questions that
requires logical, analytical, temporal, coreferential
reasoning, and information integration over mul-
tiple premise sentences. (4) CaseHoldNLI , the
fourth and novel NLI dataset introduced in this
paper, in the legal judicial reasoning domain for
identifying the governing legal rule (also called
“Holding”) applied to a particular set of facts. It is
sourced from the CaseHOLD dataset (Zheng et al.,
2021) comprising over 53,000+ multiple choice
questions. Each multiple choice question com-
prises of a snippet from a judicial decision along
with 5 semantically similar potential holdings, of
which only one is correct. We obtain the NLI-
version by combining the question and the positive
(negative) answer candidate as a positive (nega-
tive) hypothesis. To evaluate the dataset quality,
we asked an expert to select the NLI using only
the hypothesis for 10% of the test data sampled at
random. The poor performance of this human base-
line (∼0.24F1) validates that the dataset doesn’t
suffer from hypothesis bias. CaseHoldNLI dataset
is comparable to challenging document-level NLI
datasets with average premise length at document-
scale and exceeds the maximum input length limit
of BERT models. We report train/dev/test splits of
each dataset.
4.2 Experiments on Downstream Tasks
(1) Fact Verification : The NLI-version of FEVER
(Thorne et al., 2018) task, released by Nie et al.
(2019), considers each claim as a hypothesis while
the premises consist of ground truth textual evi-814
dence and other randomly sampled related text.
(2) Multi-choice Question Answering : The NLI-
version MCTest (Richardson et al., 2013) com-
bines the question and the positive (negative) an-
swer candidate as a positive (negative) hypothesis.
Presence of limited labeled data makes them both
good benchmarks to investigate the performance of
document-level NLI models on annotation-scarce
tasks. We evaluate DocInfer trained on DocNLI
dataset and report F1 scores for both tasks. We
follow the "FEVER-binary" and ”MCTest-NLI"
settings proposed in Yin et al. (2021).
(3) Contract Clause Retrieval (Łukasz Borch-
mann et al., 2020): is a task to identify spans in
a target document representing clauses analogous
(i.e. semantically and functionally equivalent) to
the provided seed clauses from source documents.
We reformulate this as an NLI task where the seed
clauses are concatenated to form the hypothesis,
and the target document is the premise. We test the
evidence selection capabilities of DocInfer trained
on ContractNLI dataset for identifying relevant
sentence-level spans in the premise for the clause
retrieval task. The dataset has 1300 examples each
for validation and test to tune and test the paragraph
selection hyperparameter η. We followed the eval-uation framework specified in Łukasz Borchmann
et al. (2020) of few (1-5) shot setting and report
Soft F1 score.
5 Results
Table 1-4 compares the performance of DocInfer
against other baselines on DocNLI, ContractNLI,
ConTRoL, and CaseHoldNLI datasets. Similar
to (Yin et al., 2021), we truncate the hypothesis-
premise pair sequence to appropriate maximum
input length for input to Transformer models.
BERT (Devlin et al., 2018), RoBERTa (Liu et al.,
2019a), DeBERTa (He et al., 2020), BART (Lewis
et al., 2020) show superior performance for Doc-
NLI, ContractNLI, and ConTRoL datasets, respec-
tively. Legal-BERT (Chalkidis et al., 2020) outper-
forms other Transformer language models on Case-
HoldNLI dataset due to its high domain-specificity
of legal language. However, they are challenged by
their input length restriction of 512 tokens for con-
textually reasoning over long premise lengths. Con-
sistent with observations of (Yin et al., 2021), large
input Transformer models such as Longformer
(Beltagy et al., 2020) and BigBird (Zaheer et al.,
2020) that can handle up to 4096 tokens under-
perform traditional BERT-like models on all four
datasets. We attribute this to the presence of distrac-
tors in long documents and the inability of these
models to reason in a multihop fashion. BART-
NLI which is pretrained on sentence-level NLI (Liu
et al., 2021) improves over naive Transformers but
still struggles due to limited captured context.
We also re-purpose several strong baseline meth-
ods from the Fact Extraction and Verification
(FEVER 1.0) task. by reformulating the docu-
ment retrieval and claim verification steps to para-
graph retrieval and textual entailment, respectively.
GEAR, KGAT, and HGRGA model the document
as a dense fully-connected graph, leading to dis-
tractor interactions confounding the reasoning pro-
cess. They are also devoid of linguistic information
about entities, topics or commonsense knowledge.
HESM uses document chunking which hinders con-
textual reasoning for far-away chunks. DREAM
and TARSA use semantic role labeling and topic
modeling, respectively, to identify phrase interac-
tion but lack entity-level information required to re-
solve coreferences across document. EvidenceNet
and SpanNLI emerge as strong baseline models
for our work. DocInfer outperforms SpanNLI
and EvidenceNet due to its ability to iteratively815select important evidence sentences in the premise
and simultaneously utilize multihop interactions be-
tween related evidences. Impact of Input Length :
DocInfer achieves SOTA performance on all four
datasets and maintains steady improvements over
corresponding baseline models with increasing in
input lengths. Choice of context encoder in NLI
prediction : One of the merits of the our approach
is that it is extensible and can utilize any domain-
specific transformer language models for context
encoding to further augment performance. We eval-
uate the choice of context encoder for different
datasets. DocInfer gives SOTA performance us-
ing RoBERTa for DocNLI, BERT for ContractNLI,
BART for ConTRol, and Legal-BERT for Case-
HoldNLI, in the prediction model.
Ablation Study of DocInfer : Table 5 shows ab-
lations for the document graph relations, module
components and reward functions. We observe
that concept relation is critical in all data settings
due to the need for external knowledge-based se-
mantic representation for connecting related con-
cepts across sentences. Removing any of the rela-
tions does not degrade the performance below Evi-
denceNet (Chen et al., 2022) or SpanNLI baselines.
This is important for adapting our method to new
domains where existing linguistic parsers maybe
noisy or non-existent. Cells in Table 5 highlighted
inred shows the ablation of individual compo-
nents such that removing paragraph pruning mech-
anism severely deteriorates model performance as
the model has to evaluate an exponentially larger
number of candidate evidences during evidence
selection stage. In absence of optimal evidence
selection, we treat evidence extraction as a binary
classification task over each sentence node along
with NLI label given by the “readout” function sim-
ilar to KGAT (Liu et al., 2019b). The severe per-
formance drop of DocInfer model in absence of
evidence selection component highlights its impor-
tance for document NLI task. Asynchronous graph
update adds incremental value to DocInfer owing
to its relation-specific message passing. Evidence
Selection and Paragraph Pruning components are
most critical for SOTA performance of DocInfer .
Greedy selection instead of REINFORCE signifi-
cantly decreases performance. Concept relations
are most beneficial for DocInfer , followed by top-
ical and entity relations. Evidence, semantic, multi-
hop and task rewards most help ContractNLI, Con-
TRoL, DocNLI, and CaseHoldNLI.Impact of reward function : Table 5 shows that re-
moving any reward component (i.e., task, semantic,
evidence, multihop) significantly hurts the overall
performance, thus clearly demonstrating their in-
dividual importance. To assess the necessity of
the multi-step selection using REINFORCE, we
eliminate multistep selection strategy and perform
one-shot sentence selection where the top ksen-
tences with highest selection scores from the first
step are selected. We call this setting as greedy
evidence selection and show that the elimination of
multistep selection drops performance, suggesting
that selecting sentences incrementally conditioning
on previously selected sentences is advantageous.
Performance of DocInfer on downstream tasks :
Table 6 shows the evaluation of DocInfer along
with RoBERTa-large and EvidenceNet (Chen et al.,
2022) baselines and RoBERTa model from Yin
et al. (2021) on FEVER-binary and MCTest tasks.
We train all models on DocNLI dataset to benefit
from cross-task transfer and for minimizing domain
shift. We then inference all models in two settings:
(i) without task specific fine-tuning, and (ii) with
fine-tuning on the end task. DocInfer model con-
sistently outperforms baselines across both tasks in
case of without fine-tuning (FEVER-binary: +0.8
F1, MCTest v160: +1 F1, MCTest v500: +0.6
F1) and with fine-tuning (FEVER-binary: +0.9 F1,
MCTest v160: +0.5 F1, MCTest v500: +0.2 F1).
We observe that both the tasks require the mod-
els to capture topic coherence, knowledge-based
semantics, and entities interactions as removing
graph relations severely degrades the performance.
Evidence selection for clause retrieval focuses
on selecting evidence spans in the target document
(premise) given the entailment relation with seed
clauses (hypothesis). The task is unsupervised in
nature (has no training set). We test the evidence
selection module ( E) of the DocInfer model
and its ablated variants (without paragraph prun-
ing and reward functions), all pre-trained on Con-
tractNLI dataset. Table 7 shows that DocInfer
model with BERT as the context encoder outper-
forms strong baselines by approximately 5%. Re-
moving paragraph pruning significantly degrades
the performance, highlighting the need to prune dis-
tractor paragraphs for retrieving relevant informa-
tion. Presence of each reward function to maintain
the performance of DocInfer indicates the linguis-
tic importance of each reward. Formulating the task
as NLI helps contextualize the seed clauses with816
premise as opposed to earlier techniques of isolated
vectorization and naive aggregation by (Łukasz
Borchmann et al., 2020) .
Qualitative Analysis : Figure 2 shows qualitative
analysis across different reasoning types on the test
set of ConTRoL dataset. The results provide ev-
idence that the multihop and semantic similarity
rewards are important for coreference reasoning
(CR) due to reasoning over multiple mentions and
noun phrases. Multihop reward also helps improve
Information aggregation (II) which requires com-
bining information from multiple paragraphs. Task
reward benefits logical reasoning as it focuses on
logical inference of human language. DocInfer is
unable to handle temporal and analytical reasoning
cases. We further analyze the evidence extraction
mAP on ContractNLI dataset across diverse chal-
lenging phenomena. Entity relations are critical
for resolving reference to definitions (RD) as they
are anchored together through common mentions.
Concept similarity links play an important role in
resolving information spread out between discon-
tinuous spans based on commonsense reasoning.
DocInfer handles evidence identification for all
studied phenomena better than SpanNLI.
6 Conclusion and Future Work
We introduce DocInfer , a document-level NLI
model that uses enriched hierarchical document
graph through inter-sentence relations, performs
paragraph pruning using SubGraph Pooling layer,
and optimally selects evidence sentences using RE-
INFORCE algorithm to outperform SOTA meth-
ods on four doc-NLI datasets, including our
propose CaseHoldNLI on legal judicial reason-
ing.DocInfer is useful for downstream fact verifi-
cation, multi-choice QA and legal clause retrieval
tasks. For future work, we intend to integrate tem-
poral knowledge and analytical reasoning into our
model to improve the performance.817References818819820Appendix
A Limitations
Through careful analysis of error cases, we found
that there are two main types of prediction errors
from the proposed model. First, the model is unable
to reason over temporal and causal aspects. For ex-
ample, in the hypothesis “Repayment terms will
be finalized before disbursement but prior to loan
approval” while the evidence states “Repayment
terms are subject to loan approval and monthly
disbursement of interest amount”. DocInfer does
not recognize the fact that there is a temporal or-
der between events “repayment", “approval", and
“disbursement". Tackling this type of error re-
quires temporal relation prediction between dif-
ferent events. The second type of errors is mainly
due to contradictory/missing information in the re-
trieved evidence required for analytical inference.
For example, the model predicts that the hypothe-
sis “Insurance prices are all time high" contradicts
with evidences “Insurance prices increase with in-
crease in pollution" and “Protest marches for restor-
ing pollution control board were censored”. The
model prunes a relevant piece of evidence - “Pol-
lution control board tabled policy for curbing air
contamination in residential areas" in an otherwise
irrelevant paragraph which causes loss of logical
flow. Potential Risks: Our models are exploratory
and academic in nature and should not be used for
real-world legal/contractual/healthcare purposes
without extensive investigations into its shortcom-
ings/randomness/biases. Unhandled Cases : The
current work is limited to English language and
would need suitable tools in other languages to
process semantic similarity, concept knowledge
and topic models. Moreover, our method has been
tested on limited domains of Wikipedia text, narra-
tive stories, exam-style questions, case laws, and
contracts. Applying it to life-critical scenarios such
as healthcare, public safety will need further inves-
tigations.
B Ethics Statement
We utilize three publicly available datasets - Doc-
NLI, ContractNLI and ConTRoL for evaluating
document-level NLI. We also curated dataset for
doc-level NLI on legal judicial case documents. We
source these contract documents from a publicly
available resource - CaseHOLD dataset (Zheng
et al., 2021). We repurpose the dataset for ourtask and provide new annotations. CaseHoldNLI
dataset does not violate any privacy as these doc-
uments are already in public domain as part of
Harvard Case Law Corpus. There is no human
bias involved in such documents as they are al-
ready annotated expertly and provided openly after
anonymizying any identifiable information. These
documents do not restrict reuse for academic pur-
poses and any personal information was already
redacted before their original release. All docu-
ments and our experiments are restricted to English
language. FEVER-binary, MCTest, and Contract
Discovery datasets are also publicly available for
research purposes. There was no sensitive data
involved in the studies.
C Impact of Input Length
DocInfer achieves SOTA performance on all four
datasets and maintains steady improvements over
corresponding baseline models with increasing in
input lengths for performance vs premise length
comparison for ConTROL, DocNLI, ContractNLI,
andCaseHoldNLI datasets.
D Experiments on Downstream Tasks
D.1 Fact Verification
The NLI-version of FEVER (Thorne et al., 2018)
task, released by Nie et al. (2019), considers each
claim as a hypothesis while the premises consist of
ground truth textual evidence and some other ran-
domly sampled related text. Yin et al. (2021) com-
bined the "refute" and "not-enough-info" labels into
a single class of "not entail", organized the dataset
into train/dev/test split of 203,152/8,209/10,000
and renamed it as "FEVER-binary".
D.2 Multi-choice Question Answering
MCTest (Richardson et al., 2013) is a multi-choice
QA benchmark in the domain of fictional story
with one correct and three incorrect answers. The
NLI-version MCTest combines the question and
the positive (negative) answer candidate as a posi-
tive (negative) hypothesis. We test two versions of
MCTest – MCTest-160 (70 train, 30 dev, 60 test)
and MCTest-500 (300 train, 50 dev, 150 test). Pres-
ence of limited labeled data makes it a good bench-
mark to investigate the performance of document-
level NLI models on annotation-scarce tasks. We
evaluate DocInfer trained on DocNLI dataset and
report F1 scores for both tasks.821D.3 Contract Clause Retrieval
Clause Discovery (Łukasz Borchmann et al., 2020)
is a task to identify spans in a target document
representing clauses analogous (i.e. semantically
and functionally equivalent) to the provided seed
clauses from source documents. We reformulate
this as an NLI task where the seed clauses are con-
catenated to form the hypothesis, and the target
document is the premise. We test the evidence
selection capabilities of DocInfer for identifying
relevant sentence-level spans in the premise. To
this end, we test DocInfer trained on ContractNLI
dataset without supervision for the clause retrieval
task. The dataset has 1300 test examples and
we tuned the paragraph selection hyperparameter
ηon the validation set of 1300 examples. We
followed the evaluation framework specified in
Łukasz Borchmann et al. (2020) of few (1-5) shot
setting and report Soft F1 score.
E Data Statistics
We present the dataset statistics in Table 8.
A1: Limitations:
Through careful analysis of error cases, we found
two main types of prediction errors from the pro-
posed model: (1) unable to reason over temporal
and causal relations; (2)contradictory/missing in-
formation in the retrieved evidence required for
analytical inference.
A2: Potential Risks:
Our models are exploratory and academic
in nature ans should not be used for real-
world legal/contractual/healthcare purposes with-
out extensive investigations into its shortcom-
ings/randomness/biases.
B1: Citation to creators of artifacts:
We use four datasets: (i) DocNLI (Yin et al., 2021),
(ii) ContractNLI (Koreeda and Manning, 2021),
(iii) ConTRoL (Liu et al., 2021), (iv) CaseHold
(Zheng et al., 2021). We repurpose the fourth
dataset for doc-level NLI task All datasets and doc-
uments are publicly available.
Further, we use three datasets for downstream
applications: (i) FIVER-binary (Nie et al., 2019),
(ii) MCTest (Richardson et al., 2013), (iii) Contract
Discovery (Łukasz Borchmann et al., 2020).
DocNLI Dataset:ContractNLI Dataset:
ConTRoL Dataset:
CaseHold Dataset:
Contract Dicovery Dataset:
B2: License and terms for use of data artifacts:
All the datasets are available to use for research
purposes.
B3: Intended use of data artifacts:
The intended use of NLI datasets is to improve
NLP reasoning and semantic understanding of text
and languages. Use cases in legal and contract
domains can make increase accessibility amongst
non-experts and lead to AI for social good.
B4: Steps taken to protect / anonymize names,
identities of individual people or offensive
content:
We do not use any identifiable user data for any
experiments. All persons mentioned in the dataset
are anonymous or have their information publicly
available.
B5: Coverage of domains, languages, linguistic
phenomena, demographic groups represented
in data:
Our work uses NLI dataset from Wikipedia, story
narrations, exam questions, contracts and case laws
in English language. Adaptation to other languages
may need appropriate processing.
B6: Data statistics (train/test/dev splits):
The data statistics are given in Table 8.
E.1 Training Setup
Hyperparameters : We tune the hyperparameters
for the proposed model using a grid search. All
the hyperparameters are selected based on the F1
scores on the development set to find the best con-
figurations for different datasets. In our model we
use the BERT-base to encode document embed-
dings; LSTM and 2 layers for feed forward neural
networks. The trade-off parameters α,βandγare
set to 0.5, 0.1, 0.05, respectively. We use Adam op-
timizer and the batch size of 16 is employed during
training. We vary paragraph selection ηbetween 1
to 10.822
We summarize the range of our model’s hy-
per parameters such as: size of hidden layers in
LSTM {100,200,300}, size of hidden layers in
FFN{100,200,300,400}, BERT embedding size,
dropout δ∈ {0.2,0.3,0.4,0.5.0.6}, learning rate
λ∈ {1e−5,2e−5,3e−5,4e−5,5e−5}, weight
decay ω∈ {1e−6,1e−5,1e−4,1e−3}, batch
sizeb∈ {16,32,64}and epochs ( ≤100).
Contextual Encoder : We used BERT-base-
uncased for generating token embedding of size
1x 768. As BERT-base Transformer provides a
stronger baseline as compared to RoBERTa, we uti-
lized BERT Transformer for Contextual Encoder in
DocInfer architecture. We use the default dropout
rate (0.1) on BERT’s self attention layers but do
not use additional dropout at the top linear layer
The output from the Contextual Encoder is a 1-D
vector of size 768.
Loss Function and Inference :DocInfer is
trained end to end using Cross Entropy loss for con-
text encoder and REINFORCE loss for evidence
selection with Adam optimizer. Across all four
datasets, we found the best results correspond with
the use of Adam optimiser set with default values
β= 0.9,β= 0.999,ϵ= 1e−8, weight-decay of
5e−4and an initial learning rate of 0.001. We eval-
uate the performance of NLI prediction with the
following corresponding metrics for each dataset:
•DocNLI : devF1, test F1
•ContractNLI : NLI label using Acc (%),
F1(entails), F1(contradicts), Evidence extrac-tion using mAP,PR@80 precision and recall
score.
•ConTRoL : Acc(%), F1(E), F1(C), F1(N),
F1(Overall)
•CaseHold : P, R, F1
•MCTest : F1
•FEVER-binary : F1
•Contract Discovery : Soft F1
Parameter Size : Our model’s total parameters is
asymptotically close to the context encoder it uses
in the backbone. Our model does not add a lot
more parameters than the existing context encoder.
C4: Implementation Software and Packages
We implemented our solution in Python 3.6 us-
ing PyTorch framework. We used the following
libraries and modules:
•Huggingface’s implementation
for BERT/RoBERTa/BART/Legal-
BERT/T5/Longformer/BigBird transformers.
•PyTorch Geometricfor graph learning meth-
ods.
• LDA-Pypipackage for topical relations.823• Standford CoreNLP, Spacy, and
•AllenNLP Libraryfor coreference and
Named Entity Recognition in entity relations.
•ConceptNet Numberbatchand KnowBert
for concept relations.
• SimCSE embeddingsfor semantic reward.
• Bleu-Pypilibrary for evidence reward.824