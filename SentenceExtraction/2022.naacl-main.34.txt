
Sharan Narasimhan Suvodip Dey Maunendra Sankar Desarkar
Indian Institute of Technology Hyderabad, India
Abstract
Recent studies show that auto-encoder based
approaches successfully perform language gen-
eration, smooth sentence interpolation, and
style transfer over unseen attributes using un-
labelled datasets in a zero-shot manner. The
latent space geometry of such models is or-
ganised well enough to perform on datasets
where the style is “coarse-grained” i.e. a
small fraction of words alone in a sentence
are enough to determine the overall style la-
bel. A recent study uses a discrete token-based
perturbation approach to map “similar” sen-
tences (“similar” defined by low Levenshtein
distance/ high word overlap) close by in la-
tent space. This definition of “similarity” does
not look into the underlying nuances of the
constituent words while mapping latent space
neighbourhoods and therefore fails to recog-
nise sentences with different style-based se-
mantics while mapping latent neighbourhoods.
We introduce EPAAEs (Embedding Perturbed
Adversarial AutoEncoders) which completes
this perturbation model, by adding a finely ad-
justable noise component on the continuous em-
beddings space. We empirically show that this
(a) produces a better organised latent space that
clusters stylistically similar sentences together,
(b) performs best on a diverse set of text style
transfer tasks than similar denoising-inspired
baselines, and (c) is capable of fine-grained
control of Style Transfer strength. We also ex-
tend the text style transfer tasks to NLI datasets
and show that these more complex definitions
of style are learned best by EPAAE. To the best
of our knowledge, extending style transfer to
NLI tasks has not been explored before.
1 Introduction
The Text Style transfer (TST) task is a form of con-
trolled language generation. The goal is to produce
fluent style-altered sentences from a given base sen-
tence, while also preserving its style-independentcontent. The definition of “style” depends on the
class labels of the end task. Our work focuses on
the unsupervised scenario i.e. performing the train-
ing on completely unlabelled corpora. By inducing
the latent space organization through input pertur-
bation, TST can be performed using a simple vector
arithmetic method (discussed in Section 6).
Background. Several well-known architectures
use auxiliary objectives that serve as regularizers to
ensure that the latent space geometry is smoothly
interpolatable and learns high-level semantic fea-
tures (refer Appendix D). Inspired by successes
in denoising approaches in vision (Creswell and
Bharath, 2019; Vincent et al., 2008), we look at in-
put perturbation based approaches for unsupervised
style transfer. Unlike text, in vision, freedom exists
to finely control the degree of Gaussian “blur” on
the continuous input image space. We conjecture
that this notion of “controlling blur” may be useful
in the text domain as well, serving as our motiva-
tion for our chosen model for Input Perturbation.
Shen et al. (2020) map “similar” (defined by low
Levenshtein distance/ high word overlap) sentences
together in the latent space by introducing a simple
denoising objective over an underlying Adversarial
Autoencoder (AAE) (Makhzani et al., 2015). This
noise model includes simple token manipulationFigure 1: A TSNE plot of encoded latent vectors for
all 1950 sentences in the Toy Dataset. EPAAE shows
tighter and more organised clustering. Refer to B.1 and
B.2 for larger plots with legend.475(token dropout and substitution) with some proba-
bility p, to reconstruct the original sentences from
the perturbed inputs. We can reason intuitively that
discrete word dropout allows sentences with high
word overlap (or low Levenshtein distance) to have
a higher chance of being perturbed into one another,
thereby mapping them close in the latent space dur-
ing the training time. However, this method allows
for stylistically dissimilar sentences, albeit with
high word overlap, to be mapped together in the
latent space.
Idea. We argue that this negatively impacts the
quality of final latent geometry and the results of
the subsequent style transfer task. As an alternative,
we explore Embedding Perturbation, where a noise
vector is sampled, appropriately scaled and added
to the embeddings of each input sentence, such that
the resultant embeddings are constrained to live in-
side an E-dimensional hypersphere. The radius of
this hypersphere is controllable using a tune-able
hyper-parameter ζ. This hypersphere constraint is
partially inspired by concepts in adversarial robust-
ness and ensures that each resultant noised word
embedding is not altered to the extent of causing
the underlying semantics of the sentence to change.
We argue that this allows sentences with stylisti-
cally similar constituent embeddings to be mapped
together, also encouraging the formation of style-
preserving latent neighbourhoods (more on this in
Section 4). The resulting latent representation from
“embedding-perturbed” autoencoders consequently
are better semantically organized and stylistically
robust, enabling us to perform TST using an induc-
tive method i.e. simple vector arithmetic on latent
vectors.
Contributions. We show that this extended model
of input perturbation with both discrete and contin-
uous components, allows for overall better quality
text style transfer, particularly in its ability to pre-
serve style-independent “content” information. To
expand the traditional definitions of styles such
as Polarity, Formality, which are based on sim-
ple “intra-sentence” attributes, we introduce the
“Discourse Style Transfer Task” by salvaging NLI
datasets in which the flow of logic between sen-
tences are captured using “Entailment”, “Contra-
diction”, and “Neutral” labels. This enables inter-
esting applications such as discourse manipulation
in which a pair of sentences agreeing with each
other can be made to contradict, and vice versa. We
also test our model on fine-grained styles presentin the Style-PTB dataset. We empirically show that
our model performs the best on a diverse set of
datasets with styles ranging from coarse-grained
styles (like sentiment) to fine-grained styles (like
tenses) and complex inter-sentence styles (like dis-
course or flow of logic).
2 Related work
Seminal work in TST. Autoencoder based ap-
proaches for TST on labelled non-parallel datasets
are quite well explored (Shen et al., 2017; Fu et al.,
2018). Some techniques involve implicit Style-
Content disentanglement of the latent space using
Back Translation (Prabhumoye et al., 2018) and
adversarial learning (John et al., 2019). Li et al.
(2018) achieve disentanglement using simple key-
word replacement. Most studies look at simple non-
parallel classification datasets, defining their style
to be the class label. Studies also look at Syntax-
Semantic disentanglement of the latent space (Chen
et al., 2019; Bao et al., 2019). A λpenalty is im-
posed on the log variance of the perturbations to
prevent it from vanishing. The latent vacancy prob-
lem (Xu et al., 2020) of the β−V AE is mitigated by
introducing auxiliary losses and provided for one
of the earliest methods for unsupervised TST. Simi-
lar to our work, Rubenstein et al. (2018) introduces
the Latent noised AAE (LAAE), Gaussian perturba-
tion is instead added to latent encodings to promote
organization. Unsupervised work includes using
a language model as a discriminator for a richer
feedback mechanism (Yang et al., 2018), allow-
ing it to increase performance in word substitution
decipherment, sentiment modification, and related
language translation. More seminal work related
to autoencoders in the context of Style Transfer is
mentioned in Appendix D.
Contemporary work in TST. More recent work,
treat the style transfer problem as paraphrase gen-
eration and fine-tune pre-trained language models
(Krishna et al., 2020). Malmi et al. (2020) trains
masked language models or MLMs on the source
and target domains to identify input tokens to be
removed and replace them with tokens from the
target MLM in an unsupervised manner. Liu et al.
(2020) uses gradient-based update rules in the con-
tinuous latent space zfrom style and content predic-
tor networks, enabling the transfer of fine-grained
styles without using an adversarial approach. Reid
and Zhong (2021) performs TST on sentiment and
politeness datasets using token editing methods476(similar to Li et al. (2018)) using Levenshtein edit-
ing operations. Lee et al. (2021) also focuses on
enhancing content preservation by introducing a
method to remove style at the token level using re-
verse attention and fuse this content representation
with style using a conditional layer normalization
technique. Riley et al. (2021) adapts the T5 model
(Raffel et al., 2020) for few shot text style transfer
by extracting a style representation and perform-
ing conditioned decoding, using only a handful of
examples at inference time.
3 Method
The underlying language model is a generative
auto-encoder which models an input distribution
p(x)assumed to be from an underlying latent distri-
bution p(z). A deterministic encoder Erepresent-
ing the distribution q(z|x), in the form of an RNN,
whose output is reparameterized by another dis-
tribution z∼ N (µ(x), σ(x))to give the aggre-
gated posterior distribution q(z). Various auxiliary
loss functions are used to enforce the learned prior
q(z)to match p(z). The Generator Grepresent-
ingp(x|z), also in the form of an RNN, decodes
back the sample from the learnt prior q(z)into its
corresponding input from p(x). Gradient descent
is applied on the reconstruction loss of the autoen-
coder given by:
L(θ, θ) =E[−logp(x|E(x))](1)
We use the AAE (Makhzani et al., 2015) as our
choice for the underlying generative autoencoder
over which the input perturbation techniques were
applied. AAE uses a discriminator Dto enforce
q(z)to match p(z), a standard Gaussian, by learn-
ing to distinguish between samples from the two
different distributions. This adversarial loss serves
as a regularizer for the latent space, giving it the
ability the organize itself better for smooth sentence
interpolation.
L(θ, θ) =E[−logD(z)] +
E[−log(1−D(E(x)))]
(2)
The final min-max objective is a sum of the re-
construction loss (given below) and λweighted
adversarial loss:
minmaxL(θ, θ)−λL(θ, θ)(3)We found empirically that AAEs performed well
and were stable during training. On the other hand,
β-V AEs required careful tuning of the βhyperpa-
rameter to prevent posterior collapse and did not
perform as well.
3.1 Finely-controlled continuous noise on
embedding space
To further organize the latent geometry of the un-
derlying AAE to encode style-based semantic simi-
larity among sentences, we propose a perturbation-
based approach on the continuous embedding
space. Word embeddings of dimensionality E, of
each input token xare denoted as e. Consider
an input sentence of length lcontaining tokens
x,···, xhaving embeddings e,···, ere-
spectively. Our objective is to blur every embed-
ding vector eby adding some appropriately scaled
noise vector nto produce a resultant noised em-
bedding vector e, such that edoes not lie too far
away from eto not change the underlying seman-
tics of the word completely. We do this by ensuring
that each new elives inside an E-dimensional hy-
persphere. The center of the hypersphere is the
original embedding eand its radius is defined as
|e|*s, where s∈Ris a random variable sam-
pled from a distribution P(s).P(s)is probability
distribution of the form Y(µ= 0, σ)where Yis
some arbitrarily chosen distribution and σas func-
tion of hyperparameter ζ∈R. This distribution
P(s)parameterized by Y, ζ)models the probability
density cloud inside the embedding hypersphere,
controlling its radius and interior densities. Figure
1 neatly summarizes the aforementioned embed-
ding perturbation mechanism for a simple example
with only two individual word embeddings. In477practice, we use a vectorized representation of the
above mechanism to blur the embeddings of a mini-
batch of sentences of size Lin constant O(1) time.
The embedding perturbation method is summarized
below in a vectorized form:
z∼ N(µ, σ)
ˆz=z/|z|
s∼Y(µ= 0, σ= (ζ/3))
n= (s⊙ |e|)ˆz
e=e+n
r=ζ∗ |e|(4)
where z,ˆz,s,nandeare vectorized represen-
tations of z,ˆz,s,nanderespectively for a
mini-batch of sentences of size l.⊙and * denote
element wise and scalar multiplication respectively.
|x|denotes the magnitude of a vector/batch of vec-
torsx.ris the vectorized mini-batch representa-
tion of r, the expected radius of hypersphere.
We choose the Gaussian distribution as our choice
of the probability distribution Y, as on testing it
produced the best results. To couple the radius
rofhypersphereto our hyperparameter ζto en-
able fine-grained control using ζ, we constrain the
probability density of Yto live inside the hyper-
sphere within three standard deviations. To achieve
this, we equate 3σtoζ, and consequently set the
variance σofYto be (ζ/3).
3.2 Discrete word dropout probability
The “Denoising Autoencoder” or DAAE (Shen
et al., 2020) considered discrete token-level per-
turbations such as token masking, substitution and
deletion. We consider token deletion as discrete
noise. Drop probability p= 0.3is found to be the
best in both their experiments and ours.
Token deletion is the only type of input pertur-
bation that can alter the number of tokens in a
sentence. Any continuous model for input noise
cannot mathematically generalize the effects of this
kind of discrete word dropout. Furthermore, dur-
ing our experiments, we find that for some datasets,
both discrete and continuous noise components are
required to produce the overall best model. In such
a case, we first perform token deletion and then
subject the leftover token embeddings to perturba-
tion. We refer to this generalized noise model as“Embeddings-Perturbed Adversarial Autoencoder”
or “EPAAE” parameterized by ζandp.
4 Semantic Similarity in Latent Space
Neighbourhoods
We contrast and compare the resultant latent space
neighbourhoods of DAAEs and EPAAEs.
4.1 Preliminary reasoning
Here, we first investigate the question - Does token
deletion during training group truly put semanti-
cally similar sentences together in latent space Z?.
Intuitively we can reason that the answer may be in
the affirmative if the drop probability is small. For
example, the sentences “The food was good” and
“The food was superb” might get perturbed into a
common version, i.e. “The food was” and therefore
be mapped nearby in Zduring training. As anal-
ysed and concluded in (Shen et al., 2020), latent
neighbourhoods in the latent space of DAAEs suc-
cessfully cluster sentences with high word overlap
(low Levenshtein/Edit distance) together. However,
the Levenshtein distance metric is not an accurate
measure of the true semantic similarity between
sentences. A pair of sentences with high word
overlap might convey different ideas w.r.t the un-
derlying style-based semantics of the dataset. For
example, in the context of the Yelp dataset, the sen-
tences “The food was good” and “The food was
bad” are stylistically opposing (style being polarity
in this case) and yet still get mapped close by in Z
for DAAEs.
4.2 Testing the hypothesis with a Toy Dataset
We conduct specifically curated experiments on
a synthetic dataset to verify our hypothesis that
EPAAEs map stylistically similar sentences to-
gether.
Details of Toy dataset: Inspired by Yelp, each
sentence in this dataset either represents a posi-
tive or negative sentiment. It also contains dif-
ferent sentiment independent components, such
as the identity of the person and the subject of
the review. Each sentence is of the format: “The
<identity_token> said the <subject_token> is<de-
cision_token> " where identity ,subject anddeci-
sion token classes are the only variable parameters
in each sentence. The entire set of all permutations
of these token classes forms the dataset. For ex-
ample, the decision class is further subdivided into
two subclasses, i.e. positive/ negative sentiment,478
each subclass containing 7 and 8 choosable tokens,
respectively. The resultant dataset consisted of
1950 sentences. The details of the subclasses and
the representative tokens inside each subclass are
shown in Table 1. We produce output labels for
each sentence by using a 3-bit representation, one
bit for each of three token classes, where the values
of each bit represent the token label subclass within
that class. For example, a label of 5 is encoded as
101 corresponding to a sentence with Female ,Food
andNegative labels. There are 2= 8 labels in total
labelled 0-7.
Qual. Analysis of Latent space. We consider
two models, DAAE with token deletion probability
p= 0.3and EPAAE with ζ= 2.5. Both models
are initially pre-trained over the unlabelled Yelp
dataset. The synthetic dataset is used during infer-
ence time only. We encode all 1950 sentences into
their respective latent space vectors and use TSNE
plots to visualise the latent space of each model
(Figure 1). We pick a random query sentence, e.g.
“The man said the pasta is spicy”, and encode it into
Z. We then retrieve the top five nearest encoding
to the query and observe their decoded outputs to
check for the preservation of style-based semantics
(Table 2) around the query. We find that the EPAAE
maps the latent neighbourhood such that stylisti-
cally similar (positive/negative sentiment in this
context) are grouped. This is not the case with the
DAAE, evident from Table 2 in which neighbours
1 and 3 have a differing sentiment from the query.
This offers an explanation as to why the DAAE is
not able to produce tightly confined clusters.
Quant. Analysis of Latent space. To further
validate our hypothesis that EPAAEs better pre-
serve style-based semantics in latent neighbours,
we generate useful quantitative metrics over a k-
nearest neighbours experiment, done on the test
split across all datasets. We document these met-
rics in Table 3. Column 3 conveys the mean dis-
tance to the closest neighbour with the opposite
label. Column 4 conveys the mean number of hops
to reach to the closest neighbour with opposite la-
bels. In all but a few datasets, the DAAE reports a
smaller “mean hops for label flip”, supporting our
hypothesis that stylistically dissimilar sentences
are mapped closer together in the latent neighbour
than our proposed model. We also see this trend
mostly holds true for the "Mean L2 Norm for Label
Flip" metric in Column 4 as well, providing further
evidence to our hypothesis.4795 Setup, Datasets and Metrics
5.1 Experimental Setup
Baselines. This work focuses on simple denois-
ing approaches for Unsupervised TST and subse-
quently constrains our choice of baselines to follow
these criteria. We consider three other autoencoder
based models for our experimentation, i.e. Denois-
ing AAE (DAAE), Latent-Noised AAE (LAAE)
and the β-V AE.
Hyperparameters and Setup. Details on hy-
perparameter selection can be found in A.2. Other
common hyperparameters (detailed in A.1) related
to encoder/decoder architectures remained identi-
cal across all models. Training is completely unsu-
pervised, and labels are only used during inference
time. Details on computation time, number of pa-
rameters and infrastructure used can be found in
Appendix. E.
5.2 Datasets
In this section, we briefly describe the different
kinds of datasets used for experimentation. Further
details is provided in in Appendix C.
Complexity of Styles in Datasets: Current
studies mainly focus on high-level styles to validate
the approaches. To validate our hypothesis that the
EPAAE can perform fine-grained style transfer due
to semantics learnt from embeddings, we consider
three tasks: sentiment, discourse and fine-grained
text style transfer. As prepossessing, we remove
non-essential special characters and lowercase all
sentences. Except for the Yelp dataset, no pruning
is done based on sentence length.
Sentiment Style Datasets: We use the prepro-
cessed version from (Shen et al., 2017) of the Yelp
dataset. The sentiment labels (positive, negative)
were considered as style.
Discourse Style Datasets: To check for the
model’s ability to alter the discourse or flow of
logic between two sentences we make use of NLI
datasets such as SNLI, DNLI, and Scitail. Each
instance in the SNLI dataset (Bowman et al., 2015)
consists of two sentences that either contradict,
entail (agree) or are neutral towards each other.
Similarly, the DNLI dataset (Welleck et al., 2019)
consists of contradiction, entailment and neutrality
labelled instances. Scitail (Khot et al., 2018) is
an entailment dataset created from multiple-choice
science exams and the web, in a two-sentence for-
mat similar to SNLI and DNLI. The first sentence
is formed from a question and answer pair fromscience exams and the second sentence is either a
supporting (entailment) or non-supporting (neutral-
ity) premise.
Fine-grained Style Datasets: The Style-PTB
dataset (Lyu et al., 2021) consists of 21 styles/labels
with themes ranging from syntax, lexical, semantic
and thematic transfers as well as compositional
transfers which consist of transferring more than
one of the aforementioned fine-grained styles. To
check whether the EPAAE can capture fine-grained
styles better by leveraging its better organised latent
space, we make use of three styles i.e. Tenses,
V oices (Active or Passive) and Syntactic PP tag
removal (PPR). In the Tenses dataset, each sentence
is labelled with “Present”, “Past”, and “Future”.
The V oices dataset contains “Active” and “Passive”
voices labels and the PPR dataset contains “PP
removed” and “PP not removed” labels.
5.3 Automatic Evaluation metrics
Evaluation for text style transfer includes checking
for a) Style Transfer accuracy, b) Content preser-
vation metrics and c) Fluency of output sentences.
Recent studies show that automatic Evaluation met-
rics are still an open problem and can be gamed
(Xu et al., 2020).
Style Transfer Measure: A pre-trained clas-
sifier is used to check the presence of the target
label in the output sentence. (Mir et al., 2019) in-
troduces the notion of checking the style transfer
intensity apart from just the presence of the target
label. While we find this notion intriguing, we wish
to first accomplish the style transfer task convinc-
ingly for the current set of tasks before assuming a
more complex metric.
Content Preservation: In recent work, we ob-
serve that models typically struggle more in content
preservation and the ability to preserve the contex-
tual meaning of the base sentence. The BLEU
score alone does not suffice to correlate strongly
with actual qualitative results. To truly validate
our hypothesis that EPAAE’s are better able to pre-
serve content better, we augment the bucket of stan-
dard content preservation metrics. Apart from
the standard of using BLEU between sentences of
source and target styles, we borrow evaluation tech-
niques from fields similar to Text Style Transfer
such as Machine Translation and Text Summariza-
tion, such as METEOR (Banerjee and Lavie, 2005),480ROUGE-L (Lin, 2004), CIDEr (Vedantam et al.,
2015) which have been shown to correlate more
strongly with human judgement. Following the
study of (Sharma et al., 2017), in which they show
BLEU does not necessarily correlate with human
evaluations in dialogue response generation, we
also adopt Embedding Average, Vector Extrema
(Forgues et al., 2014) and Greedy matching score
(Rus and Lintean, 2012).
Fluency of generations: Past work measures
the perplexity using a pre-trained language model
to gauge the fluency or grammatical correctness of
the style transferred outputs. (Mir et al., 2019) ar-
gues such perplexity calculations for style transfer
tasks may not necessarily correlate with a human
judgement of fluency. Adversarial classifiers in the
form of logistic regression networks are trained
with the goal of distinguishing between human-
produced and machine produced sentences. These
classifiers are then used to score the naturalness of
the output sentences. We follow this metric during
our evaluation of fluency or naturalness
6 Experiments
In this section, we look at the quantitative and quan-
titative results for the text style transfer task for four
autoencoder models in seven datasets. We use the
vector arithmetic method on latent space represen-
tations, inspired by (Mikolov et al., 2013) where it
showed that word embeddings learnt can capture
linguistic relationships using simple vector arith-
metic. Analogous to the standard example where
“King” - “Man” + “Woman” ≈“Queen”, we ma-
nipulate an arbitrary sentence encoding zof Style
X to Style Y:
z=z+k(1
N/summationdisplayz−1
N/summationdisplayz)(5)
where z,zdenotes the latent vector of the i
sentence in style yandxrespectively and Nand
Nrepresent the number of encoding present in the
corpus for style xandyrespectively. kis a scaling
parameter used to control the style transfer strength.The resultant latent vector is passed through the
decoder to produce the output sentence.
6.1 Quantitative Analysis
TST accuracy, content preservation and natural-
ness were computed on the converted sentences
(shown in Table 4, 5, 6). Content preservation met-
rics can be found from Column 4 onwards. We
consider two versions of the EPAAE i.e. only con-
tinuous embedding noise, continuous embedding
noise + token deletion, and find that in some cases
a mixture of both is required for optimal perfor-
mance. We find that a slightly lowered value pfor
the EPAAE combined with its optimal ζparameter
outperforms other models as well.
6.1.1 Sentiment TST
Table 4 summarises the results of TST on the Yelp
dataset. On visual inspection, there appears to be a
general tradeoff between TST% and content preser-
vation metrics. For example, the β-V AE achieves
the best TST% but suffers from bad content preser-
vation capability. In this case, we observe that
EPAAE ( ζ= 2.0, p= 0.1)has the best con-
tent preservation capabilities across all metrics and
achieves a reasonable tradeoff of TST%=77.1 as
well.
6.1.2 Discourse TST
Table 5 summarises the results on four NLI datasets.
Similar to the sentiment task, we see that EPAAEs
have the best content preservation capabilities as
well as the naturalness metric. It achieves this
while achieving a comparable TST% as well. β-
V AE shows best TST% but again suffers in content
preservation. DAAE also display reasonable TST%
vs Content preservation tradeoffs but overall cannot
match the tradeoffs achieved by EPAAEs. Human
evaluations on the SNLI dataset (Table 8) confirm
this as well. The TST% achieved by any model in
any task peaks at only 60.4% compared to 81.9%
in sentiment task, a significant difference that hints
at the fact that the Discourse TST task might be
intrinsically more complex than sentiment style.
Future work that aims to increase performance on
the NLI task will be beneficial.
6.1.3 Style-PTB TST
Table 6 summarises the results on three datasets
from the Style-PTB benchmark. (Lyu et al., 2021)
produces a hierarchy of styles based on transfer
difficulty measured by the average token-level ham-481
ming distance between the base and converted sen-
tence. According to this hierarchy Tense inversion,
PP removal/addition and V oice change are labelled
in ascending order as easy, medium and hard re-
spectively. Our results seem to partially validate
this observation, in that the max TST% is obtained
on the Tenses dataset (100%). Generally speak-
ing, we also observe that TST has much better
performance on fine-grained Style-PTB datasets
than sentiment and discourse styles. Similar to be-
fore, the EPAAE shows best content preservation
at competitive values of TST% as well.
It is also noteworthy to consider the direction
of style transfer, particularly in the case of com-
plex styles such as Discourse styles present in NLI
datasets. Results and analysis on direction-specific
metrics for Discourse TST are presented in Ap-
pendix B.2.2.
6.2 Qualitative Analysis
Samples of Output. For qualitative analysis, sam-
ple outputs by DAAE and EPAAE for the Yelp,
SNLI and Tenses dataset are given in Table B.4.
For a full list of qualitative examples on all datasets
along with setup details, please refer to Appendix
B.2.1.
Varying kfor Fine-Grained TST. By varying
the value of kin Equation 5, it is possible to finely
control the strength of Text Style transfer. We showexamples of this for the Yelp (Table B.9) and the
SNLI (Table 7) dataset, for the baseline DAAE
(p= 0.3) and the proposed model EPAAE. For the
proposed EPAAE model, the best performing mod-
els (specifically in content preservation metrics)
were chosen i.e. zeta = 2.0,p= 0.1for the Yelp
dataset and zeta = 2.5for the SNLI dataset. The
chosen qualitative examples highlight the EPAAEs
slight superiority in performing fine-grained TST
compared to the baseline.
Smooth Interpolation. Sentence interpolation
experiments are reported in Appendix B.1, in which
latent space points in an interpolation along a spe-
cific direction are decoded to gauge the smoothness
of the space and its ability to generate coherent sen-
tences
6.3 Human Evaluations
Each human annotator was given a set of base sen-
tences and asked to vote for which model produced
the most appropriate corresponding style inverted
sentences. Please refer to Appendix A.3 for full
details on the setup. Results are shown in Table
8. We observe that the proposed EPAAE model
was overall more preferred across all three chosen
datasets. This margin was most significant in the
case of the SNLI dataset.482
7 Conclusion
We introduce the “Embedding Perturbed AAE” or
EPAAE and show that it best captures underlying
style-based semantic features in the latent space
in an unsupervised manner compared to its base-
lines. By inducing robust latent space organization
through embedding perturbation in an unsupervised
manner, we also demonstrate the possibility of fine-
grained TST, where we can control the strength of
the target style. Using a diverse set of datasets with
varying formulations and complexities of style, we
empirically that EPAAE performs overall best in
the text style transfer task, particularly in its abil-
ity to preserve style-independent content across all
datasets.8 Future Work and Limitations
Regarding work in TST. We wish to augment
existing state of the art methods with embedding
perturbation to check if doing so aids performance.
We see degrading TST performance in the Entail-
ment to Contradiction Task across all models. Fu-
ture work will focus on methods to improve this
task.
Generally. It is also interesting to further anal-
yse the effects of embedding perturbation to latent
representations and resultant properties. A theoret-
ical analysis would be beneficial to cement the use
of embedding perturbation in a more general set-
ting. There also remain more important questions
that need answering for, e.g. "What if you apply
continuous perturbation to hidden states instead of
embeddings?", "What is the relation between this
type of perturbation and techniques like dropout?".
We wish to explore these important questions in
the future.
Ethics Statement
Any TST model can be used for nefarious pur-
poses, e.g. performing a "Non-toxic to toxic"483modification of text in a real-world setting and
causing social harm. Therefore, it is important
we keep in mind a code of ethics (e.g. ) for usage, re-
search and development in this type of research.
We have made all our code open-source and pro-
vided all details of experimentation and implemen-
tation to the best of our knowledge.
Acknowledgements
We would like to thank the reviewers whose feed-
back we believe substantially increased the quality
of this work. We would like to thank the human
annotators for their participation.
References484485
Appendix
A Additional details on Text Style
Transfer Experiments
A.1 Model Architecture
All baseline models were trained with all underly-
ing architectures apart from their individual objec-
tive losses. Bi-directional GRUs were used for the
encoder and decoder with input embeddings of size
300, a hidden representation of size 256 and a latent
space of size 128. For all models using AAEs as
the underlying autoencoder, the discriminator was
a single-layered perceptron with 512 units. The
ADAM optimiser with β,βas 0.5, 0.999 and a
learning rate of 0.001. All models were trained
for 30 epochs as any more training steps caused
the reconstruction loss to dominate and decrease
overall performance on the TST task. All input
perturbations were disabled in inference time.
A.2 Hyperparameter Selection
For the hyperparameters p,λ, andβfor the DAAE,
LAAE and β-V AE, we fixed the values as 0.3, 0.05,
0.15 respectively. This decision was aligned with
the results in Shen et al. (2020), which showed
that these values produced the best reconstruction
vs BLEU trade-off. We found this to be the case
during subjective manual testing as well. λwas
set to 10 for all models having AAEs as the un-
derlying architecture. For EPAAE, we found that
ζ∈[2.0,3.0]overall showed the best results across
all datasets. Therefore a manual search around this
range was conducted to determine the optimal ζ
for each dataset.
A.3 Human Evaluations
TST outputs of two models, the baseline DAAE and
the proposed EPAAE model, on the Yelp, SNLI and
Tenses dataset were considered. The best perform-
ing EPAAE was chosen according to the results in
Table 4, 5 and 6, particularly with respect to the
content preservation metrics (Since TST% were
similar across all models). Two hundred sentences
(hundred from each base style) were randomly sam-
pled from the test split of each dataset and style
inverted (with scaling factor k= 2). Six hundredinstances (each instance being a base and converted
sentence pair) were equally split between three hu-
man evaluators. Each evaluator was given the task
of labelling all two hundred instances from each
dataset. The models were anonymous to evaluators
and randomly named as "Model 1" and "Model 2".
Each instance was to be labelled by an evaluator
with four possible decision outcomes, i.e. "1 is
best", "2 is best", "All are bad" and "All are good".
For each instance, the majority of three votes from
each of the three annotators were taken as the final
decision for that corresponding instance. Instances
without a majority were marked as "NA". The eval-
uation guidelines was formulated to consider which
model a) successfully transferred the target style b)
preserved the style-independent content and c) was
overall fluent and grammatically coherent.
B Additional Experiments:
B.1 Sentence Interpolation in Latent Space -
Qualitative Examples
We perform sentence interpolation using DAAE
and EPAAE, starting from the same input sentence,
incrementally moving along the same direction in
the latent space in five fixed-size steps. We see that
both the baseline and proposed model are able to
produce fluent and coherent sentences, indicative
of a smoothly populated latent space.
B.2 Text Style Transfer
B.2.1 Qualitative Examples
TST with scaling factor k= 2was performed on
DNLI, Scitail datasets as seen in B.5 and B.6 re-
spectively. Similarly, it was also performed on
V oices and PP Removal datasets from the Style-
PTB benchmark as shown in B.7 and B.8 respec-
tively. The proposed model for each dataset, was
chosen to be the EPAAE with best performance in
the content preservation metrics (shown in Table
4, 5 and 6). Samples were specifically selected
in which at least one of the models was able to
generate the ideal, style converted sentence with
near-perfect content preservation and coherence.
This was done by human evaluators, where both
the models were anonymized.
B.2.2 Direction specific Discourse Transfer
metrics
Particularly in the case of Discourse Style, it is
natural to speculate that the difficulty of style
transfer might be sensitive to the direction i.e.486487488489
is it Contradiction/Neutrality to Entailment or
vice versa. Intuitively, this makes sense as the
Entailment to Contradiction/Neutrality tasks can
be achieved simply by randomly editing either the
subject or predicate or both, in any one sentence,
to trigger a contradiction/neutrality between the
two. However, in the reverse task, the edited part
must be carefully chosen to precisely match the
context of the other sentence to trigger entailment.
To analyze this, direction-specific quantita-
tive metrics for Discourse TST are conducted for
the SNLI (B.10), DNLI (B.11) and SciTail (B.12)
datasets. We notice a disparity in performances
in fact, does exist, mainly highlighted by the
differences in the TST% metric. This sensitivity
to direction is present in all models across all
datasets but is most significant in the SNLI dataset
in which TST% goes as low as 20.6% for the
Contradiction to Entailment task and as high as
83.7% for the opposite task. Future work can focus
on trying to specifically improve the Contradiction
to Entailment task, as doing so will be a measure
of a model’s ability to detect and carefully align
the content of one sentence to match another.
C Details on Datasets
Here we provide some additional details of all the
datasets used in this work.
Complexity of Styles in Datasets: As discussed
in Section 5.2, we consider three tasks- sentiment,
discourse and fine-grained text style transfer. As
prepossessing, we remove non-essential special
characters and lowercase all sentences. Except
for the Yelp dataset, no pruning is done based on
sentence length. The vocab size during training
was limited at 25k unless mentioned otherwise.
Sentiment Style Datasets: We use the prepro-
cessed version from (Shen et al., 2017) of the Yelpdataset. It contains 200k, 10k, 10k sentences in the
train, dev and test split respectively. The sentiment
labels (positive, negative) were considered as style.
Discourse Style Datasets: We used three NLI
datasets - SNLI, DNLI, and Scitail. Each instance
in the SNLI dataset (Bowman et al., 2015) consists
of two sentences. These sentences either contra-
dict, entail (agree) or are neutral towards each other.
The resultant dataset contained 341k, 18k, 18k in
the train, dev, test splits respectively. The DNLI
dataset (Welleck et al., 2019) consists of contradic-
tion, entailment and neutrality labelled instances
instead in the form of a first-person dialogue like
representation. The dataset contains 208k, 11k, 11k
sentences in train, dev and test respectively. Scitail
(Khot et al., 2018) is an entailment dataset created
from multiple-choice science exams and the web,
in a two-sentence format similar to SNLI and DNLI.
The first sentence is formed from a question and
answer pair from science exams and the second
sentence is either a supporting (entailment) or non-
supporting (neutrality) premise obtained from the
internet. The dataset contained 24k, 1.3k, 1.3k sen-
tences in the train, dev, test splits respectively. For
SNLI and DNLI, all instances with the “neutrality”
label were removed. Style transfer task performed
from “contradict” to “entail” and vice versa. For
SciTail (Khot et al., 2018), the transfer task was
from “neutral” to “entail” and vice versa.
Fine-grained Style Datasets: We used Style-
PTB dataset (Lyu et al., 2021) for fine-grained style.
It consists of 21 styles/labels with themes rang-
ing from syntax, lexical, semantic and thematic
transfers as well as compositional transfers which
consist of transferring more than one of the afore-
mentioned fine-grained styles. To check whether
the EPAAE can capture fine-grained styles better
by leveraging its better organized latent space, we
make use of three styles i.e. Tenses, V oices (Active490491492
or Passive) and Syntactic PP tag removal (PPR). In
the Tenses dataset, each sentence is labelled with
"Present", "Past" and "Future". The V oices dataset
contains "Active" and "Passive" voices labels and
the PPR dataset contains "PP removed" and "PP not
removed" labels. The resultant sizes of the test, dev,
test splits were 71k,8.8k,8.8k (tenses), 90k,11k,11k
(PPR) and 44k,5.5k,5.5k (voices).
D More details on related work
Bowman et al. (2016) extend Variational Auto-
Encoders (Kingma and Welling, 2014) for text gen-
eration and address the posterior collapse problem
wherein the decoder completely ignores the latent
channel leading to poor generation. Adversarial
auto-encoders (Makhzani et al., 2015) substitute
the KL loss with an adversarial approach to enforce
the latent Gaussian prior. On the other hand, AAEs
are shown to naturally avoid the posterior collapse
problem and promote strong coupling between the
encoder and decoder. Wasserstein autoencoders
(Tolstikhin et al., 2018) introduce a family of regu-
larized autoencoders that learn a flexible prior by
solving an optimal transport problem to match P(z)
and Q(z) using an adversarial approach. Adver-
sarially regularised autoencoders (ARAEs) follow
the Wasserstein Autoencoder framework to learn a
learnt prior unlike AAEs, which assume the prior
to be a fixed standard Gaussian distribution.
E Computational Expense and
Infrastructure used
The most parameter-heavy EPAAE model was
from the SNLI dataset and we therefore report
statistics for this model. The model has 13 mil-
lion parameters and each epoch took approximately
60 seconds to train on an Nvidia V100-SMX2
GPU and an Intel(R) Xeon(R) E5-2698 CPU. Forcomplete details, please refer to the log.txt files
present in each model’s directory present in .493