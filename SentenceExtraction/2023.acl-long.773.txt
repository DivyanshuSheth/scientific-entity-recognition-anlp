
Shiqi WangZheng LiHaifeng QianChenghao YangZijian Wang
Mingyue ShangVarun KumarSamson TanBaishakhi RayParminder Bhatia
Ramesh NallapatiMurali Krishna RamanathanDan RothBing XiangAWS AI LabsCornell UniversityUniversity of ChicagoAWS AI Research & Education
{wshiqi,qianhf,zijwan,bxiang}@amazon.com zl634@cornell.edu
Abstract
Code generation models have achieved impres-
sive performance. However, they tend to be
brittle as slight edits to a prompt could lead
to very different generations; these robustness
properties, critical for user experience when de-
ployed in real-life applications, are not well un-
derstood. Most existing works on robustness in
text or code tasks have focused on classification,
while robustness in generation tasks is an un-
charted area and to date there is no comprehen-
sive benchmark for robustness in code genera-
tion. In this paper, we propose ReCode , a com-
prehensive robustness evaluation benchmark
for code generation models. We customize
over 30 transformations specifically for code on
docstrings, function and variable names, code
syntax, and code format. They are carefully
designed to be natural in real-life coding prac-
tice, preserve the original semantic meaning,
and thus provide multifaceted assessments of
a model’s robustness performance. With hu-
man annotators, we verified that over 90% of
the perturbed prompts do not alter the semantic
meaning of the original prompt. In addition, we
define robustness metrics for code generation
models considering the worst-case behavior un-
der each type of perturbation, taking advantage
of the fact that executing the generated code can
serve as objective evaluation. We demonstrate
ReCode on SOTA models using HumanEval,
MBPP, as well as function completion tasks
derived from them. Interesting observations
include: better robustness for CodeGen over In-
Coder and GPT-J; models are most sensitive to
syntax perturbations; more challenging robust-
ness evaluation on MBPP over HumanEval.
1 Introduction
Code generation has emerged as an important AI
application. Multiple models (Nijkamp et al., 2022;Fried et al., 2022; Wang and Komatsuzaki, 2021)
have been proposed and achieved impressive perfor-
mance on generating code using a natural-language
description, on completing partial lines and func-
tions, and even on solving complex coding-contest
problems. They can offer real-life help to soft-
ware engineers and enhance their productivity, and
multiple commercial offerings exist today for AI-
powered code generation (Chen et al., 2021).
However, one important aspect, robustness of the
code generation models, is commonly overlooked.
Anecdotally, people know that these models are
sensitive to perturbations over prompts: sometimes
just an extra space in a line or a slight change to a
function name would lead to completely different
generations, with potentially negative impacts to
usability. In Fig. 1 and Fig. 2, we show two fail-
ure cases on InCoder-6B (Fried et al., 2022) and
CodeGen-16B-mono (Nijkamp et al., 2022) where
they perform correctly on regular prompts but fail
on our perturbed ones after docstring paraphrasing
and function camel case renaming in our ReCode
benchmark. The perturbed prompts are natural and
retain the original meaning, indicating weakness of
these models if deployed in real-life applications.
There exists no comprehensive and quantitative
robustness benchmark for code generation models.
Li et al. (2022) includes a brief study on robustness
but it has limited perturbation types and is in a set-
ting with massive numbers of samples, unrealistic
in practice. Other existing works on robustness
in text or code tasks have focused on classifica-
tion and are not directly applicable to code genera-
tion (Zhang et al., 2020; Jha and Reddy, 2022).
In this paper, we present ReCode , aRobustness
Evaluation framework for Code , aiming to provide
comprehensive assessment for robustness of code
generation models. ReCode includes only transfor-
mations that (1) appear naturally in practice and
(2) preserve the semantic meaning of the original
inputs. We carefully collect and customize a com-13818
prehensive list of natural transformations on doc-
strings, function and variable names, code syntax,
and code format, providing multifaceted assess-
ments of a model’s robustness performance. We
verify the quality of the perturbed data using both
human evaluation and objective similarity scores.
We take advantage of the fact that executing the
generated code can serve as objective evaluation
and define three robustness evaluation metrics that
aggregate a model’s correctness across randomized
transformations and transformation types. These
metrics quantify a model’s accuracy on perturbed
prompts, its relative accuracy drop from original
prompts, as well as its general instability.
We summarize our contributions below:
•We present the first robustness evaluation
benchmark ReCode for code generation tasks.
Our evaluation framework is general and can
be easily extended to any code generation
datasets and models.
•We collect and customize over 30 natural
transformations from the aspects of docstrings,
function and variable names, code syntax, and
code format. Human evaluation shows that
most of the perturbed prompts do not alter the
semantic meaning and that their level of natu-
ralness is close to the originals. Quantitativesimilarity metrics confirm the same.
•We propose robustness evaluation metrics for
code-generation tasks: Robust Pass@k, Ro-
bust Drop@k, and Robust Relative@k.
•We demonstrate the ReCode benchmark on
HumanEval and MBPP datasets and present
extensive empirical robustness comparisons
on state-of-the-art models including CodeGen,
InCoder, and GPT-J across different sizes. We
find that 1) diverse pretraining corpus and
larger model size can help improve the model
worst-case robustness, but models may learn
to generalize in a non-robust way; 2) code
generation models are most sensitive to syn-
tax perturbations; 3) due to diversity, MBPP
poses greater changes than HumanEval.
2 Related Work
Robustness for NLP. Recent research have iden-
tified the severe robustness problem in Large Lan-
guage Models (LLMs) using adversarial examples.
For example, LLMs can be easily fooled by syn-
onym replacement (Jin et al., 2020; Zang et al.,
2020). To better illustrate the severity of adver-
sarial robustness problems for NLP models, exist-
ing works (Nie et al., 2020; Gardner et al., 2020;
Kiela et al., 2021; Wang et al., 2021a) build ro-
bustness benchmarks, which encourage people to
further build robust and trustworthy models. Zhang13819et al. (2020) presents a comprehensive overview of
works in this field. Most existing works in this field
focus on classification tasks rather than genera-
tion tasks . The main challenge for benchmarking
robustness over generation tasks is that the evalu-
ation of text generation is highly subjective and is
usually hard to quantify. However, code genera-
tion provides a special opportunity because we can
do objective and quantitative evaluation on gener-
ated codes, and code generation models use similar
model architecture as NLP models.
Robustness for code. There are a series of previ-
ous work on different aspects of robustness prob-
lems for code. Specifically, Bielik and Vechev
(2020) studies the adversarial robustness problem
for type inference in programming languages. Yang
et al. (2022) focuses on improving the naturalness
of adversarial examples in code vulnerability pre-
diction, clone detection and authorship attribution.
Zhou et al. (2022) focuses on the adversarial robust-
ness problems of source code comment generation
and (Jha and Reddy, 2022) focuses on code trans-
lation, repair and summarization. These papers
mainly focus on proposing attack and defense meth-
ods for different tasks in code domain, but there is
no previous work on a comprehensive robustness
benchmark for code generation domain.
Code generation. Code generation, also known
as program synthesis, is a task of generating code
based on natural language statements or code from
context. Researchers have adapted transformer-
based large language models to the code generation
field. Various architectures have been explored:
For example, CodeBERT (Feng et al., 2020),
PLBART (Ahmad et al., 2021), CodeGPT (Lu
et al., 2021) explore BERT, BART and GPT ar-
chitectures for language models pretrained on code
corpus. There are also works that propose to incor-
porate code structures for models to better under-
stand the semantic information, including Graph-
CodeBERT (Guo et al., 2021) and CodeT5 (Wang
et al., 2021b). Most recently, models with much
larger size (i.e., billion-scale parameter numbers)
are shown to significantly improve the performance
on code generation benchmarks. Codex-12B (Chen
et al., 2021) and CodeGen-16B (Nijkamp et al.,
2022) are two representative very large pretrained
code generation models and have established new
state of the arts. However, few works have system-
atically explored robustness in code generation.3 Methodology
In this section, we introduce the transformations to
perturb prompts on both text (docstring) and code.
We then propose new robust evaluation metrics.
3.1 Problem Formulation
We consider the end-to-end model-based code gen-
eration task. The input prompt can include natural
language statements that describe the functionality,
signature of the function to generate, helper func-
tions, and possibly a half-written function. The
goal is left-to-right generation that creates or com-
pletes the function. This setting is agnostic to
model architectures and is applicable to encoder-
decoder or decoder-only models.
We perturb the input prompt with transforma-
tions. We focus on natural transformations that pre-
serve the semantic meaning of the original prompt
and that are likely to appear in practice, e.g., fre-
quent typos in docstrings, tab to four spaces, func-
tion name style changes, and many more. We do
not consider adversarial attacks that require model
feedbacks in this paper because it is non-trivial to
control the naturalness of adversarial attacks and
they often require higher computational cost. In-
stead, we randomly generate perturbed prompts
based on the restrictions for each type of perturba-
tions and propose new metrics to evaluate model
robustness based on these prompts. We leave ad-
versarial attacks for future work.
3.2 Natural Transformations on Docstrings
Docstring describes the target function to gener-
ate. Since docstrings can vary greatly when written
by different users, robustness against changes in
docstrings is critical for usability in applications.
For docstrings, we use the NL-
Augmenter (Dhole et al., 2021) library which is
designed for data augmentation and robustness
evaluation on text.We carefully select ten
transformations, including character-level, word-
level and sentence-level ones, that are likely to
preserve semantic similarity. The selected pertur-
bations include CharCaseChange , where random
characters are replaced with their upper cases,
SynonymSubstitution , where random words are
substituted with their WordNet synonyms (Miller,
1992), BackTranslation , where sentences are
translated to a different language (e.g., German by
default) then back to English for paraphrasing the13820
whole sentence (Li and Specia, 2019; Sugiyama
and Yoshinaga, 2019), and more. To perform
perturbations, we extract docstring sentences from
the input prompt and then put the perturbed version
back to the prompt. See Appendix A for details.
We observe that directly applying NL-
Augmenter to docstrings without constraints can
potentially lead to low quality due to keywords
in the programming languages. For example,
"Create a list a[][]" could be perturbed by "Create
a list [a][]" by character case swap, which is
not natural. Therefore, to guarantee naturalness
of perturbations, we use tree-sitter to parse the
whole code snippet (the prompt & the canonical
solution) to extract any existing function names,
variable names ("a"), and type names ("list").
We then exclude them from being perturbed
by the transformations. In Tab. 1, we list all
ten transformations that are customized from
NL-Augmenter and are included in our robustness
benchmark along with sample illustrations.
3.3 Natural Transformations on Function
Names
Perturbing function names also results in perfor-
mance drops for code generation models. We sum-
marize our perturbations in Tab. 2.
Some perturbations switch function names be-
tween naming conventions. For example, the
perturbation called CamelCase transform function
names between camel-case (e.g., "findCharLong")
and snake-case ("find_char_long").
Other perturbations apply character-level or
word-level natural text transformations on com-
ponent words in a function name, including
ChangeCharCase ,InflectionalVariation , and
SynonymSubstition as discussed in Sect. 3.2.
3.4 Natural Transformations on Code Syntax
Code generation models are often used on function
completion task where the prompt includes a partial
implementation of the target function and the goal
is to complete it. In such scenarios, the partial13821code in prompt is work in progress and can be
subject to frequent editing, and ideally a model
should be robust with respect to perturbations in
the partial code. For this evaluation, we derive new
customized datasets from HumanEval and MBPP
by adding halfof the canonical solutions to the
prompts (Fig. 3a). Then we perturb such partial
code inside prompts. Details and examples for each
perturbations can be found in Appendix A.
Transformations on partial code must be syntac-
tically correct and must not alter semantic meaning.
The next section will address code format, and let
us first focus on code refactoring: these are syntac-
tic changes that are semantically invariant.
We adopt three transformations from Nat-
Gen (Chakraborty et al., 2022): (1) Deadcode
Insertion where dummy loops (0 iterations) or
if conditions are randomly inserted; (2) Operand
Swap where we randomly swap one operation (e.g.,
a<btob>a); (3) For-While Switch where we ran-
domly transform one for-loop structure in code to
equivalent while-loop structure and vice versa.
Additionally, we implement three different
schemes of variable renaming. We select the most
frequent variable in the partial code and replace it
using: (1) using CodeBERT (Feng et al., 2020) pre-
dictions with highest aggregated scores according
to the context around all its appearance, a method
inspired by (Jha and Reddy, 2022; Li et al., 2020),
(2) using NatGen style renaming as "V AR_0", and
(3) random name generation with half alphabetic
and half numeric characters. The first strategy tends
to provide more natural variable names, yet names
from the other two strategies are also plausible.
3.5 Natural Transformations on Code Format
A natural way to perturb partial code is by code
format transformations as they preserve the original
semantic meaning. We implement following code
format transformations in ReCode .
Newline Insertion: We consider three methods
of new line insertions: (1) empty lines at randomly
selected positions, (2) an empty line inserted be-
tween docstring and partial code, and (3) an empty
line inserted after partial code.
Tab-Indent: We randomly replace any space
indent with tab or replace tab with 4 spaces for
indent-sensitive languages like Python.
Line Split: We select the longest line of code
and split it into two lines in the middle.Docstrings to Comments: We convert doc-
strings to comments (e.g., """ docstring """
to# docstring for Python).
3.6 Evaluation Metrics
Many proposed transformations are randomized
operations. Hence, we need to measure model ro-
bustness over multiple samples to reduce variance.
Specifically, for each transformation and each
prompt, we create srandomly perturbed prompts.
The model under evaluation generates outputs for
each of them. We measure the worst-case perfor-
mance across each group of sperturbed prompts:
the model is considered robust on a prompt if and
only if it generates a correct solution for allsper-
turbed prompts, where correctness is measured by
executing associated unit tests.
Based on such worst-case measurements, we pro-
pose three new metrics for robustness evaluation.
Robust Pass@k (RP@k): Pass@k is a widely
used metric for measuring the performance of code
generation tasks (Chen et al., 2021). We extend
its definition to Robust Pass@k (RP@k) with s
random perturbations. For an original prompt xand
for each transformation, let the perturbed prompts
bex,···, x. We sample ngenerations by the
model for each prompt, and in total there are n·s
generations f(x), where 1≤i≤nand1≤j≤
s. Instead of regular pass@k, we first consider the
worst-case correctness across f(x), ..., f(x)for
1≤i≤n: Letc(x) = 1 iff(x), ..., f(x)are
all correct and c(x) = 0 otherwise. Let rc(x) =/summationtextc(x). Following definition of pass@k, we
define the RP@k metric as Eq. (1).
Robust Drop@k (RD@k): RP@kdirectly
measure worst-case robustness in absolute values.
It provides a worst-case estimation for models un-
der certain perturbation. But in some applications,
users may care more about relative performance
change to compare worst-case performance and
average-case performance. We propose Robust
Drop@k defined in Eq. (2) as another important
robustness metric to quantify relative changes.
Robust Relative@k (RR@k): Lastly, there
are cases where models generate incorrect code on13822original prompts yet predict correctly on perturbed
ones. This can (arguably) be considered as non-
robust behavior that we should include when report-
ing model robustness. Let’s first consider the case
of greedy decoding with n=k= 1. Let RC
denote the number of correct-to-incorrect changes
under the worst-case measurement as discussed.
Symmetrically, let RCdenote the number of
incorrect-to-correct changes under best-case mea-
surement: if the prediction with the original prompt
is incorrect yet is correct for any of the sperturbed
prompts. We define the Robust Relative@1 metric
as the fraction of changes in both directions out of
the size of the dataset ( N):
This definition can be generalized to sampling.
Letrc(x)andrc(x)be similarly defined as
RCandRCexcept that they are the number
of changes within nsamples for a prompt xinstead
of counting across the dataset. We define@k:=E
2−/parenleftbig/parenrightbig
/parenleftbig/parenrightbig−/parenleftbig/parenrightbig
/parenleftbig/parenrightbig

(4)
Eq. (4) falls back to Eq. (3) when n=k= 1.
Discussion. RP@k, RD@kand RR@kfo-
cus on different robustness requirements in prac-
tice. High RP@kdoes not necessarily mean low
RD@kor RR@k, because the model may learn
to utilize spurious correlation in the datasets to
demonstrate better Pass @kor RP @k, which is not
robust. We advocate to report all of them to provide
a comprehensive estimation of model robustness.
4 Evaluation
Evaluation setup. In this work, we use
execution-based code generation benchmarks Hu-
manEval (Chen et al., 2021) and MBPP (Austin
et al., 2021) to demonstrate our ReCode robust-
ness evaluation framework. We perform a com-
prehensive study of robustness evaluation on pop-
ular public models including CodeGen (Nijkamp
et al., 2022), InCoder (Fried et al., 2022), and GPT-
J (Wang and Komatsuzaki, 2021) to show the ro-
bustness comparisons across different model archi-
tectures and sizes. The perturbations and metrics
implemented in ReCode are general and applicable
to any code generation datasets and models.4.1 Code Generation Robustness Evaluation
Tab. 3 and Tab. 4 show the general perturbation
performances on all the models in terms of the four
general perturbation categories including transfor-
mations on docstrings, function names, code syn-
tax, and code format. The nominal baselines for
docstrings and function name perturbations are the
pass@k on nonperturbed datasets. For perturba-
tions on code syntax and format, the nominal base-
line is the pass@k on nonperturbed customized
datasets with partial code (see Sect. 3.4). We use
greedy sampling for all the models to eliminate
randomness effect and enable fair comparisons as
the default setting. We consider s= 5, i.e., we gen-
erate five different datasets with different random
seeds for each type of perturbations and evaluate
worst-case robustness performance according to the
robustness evaluation metric defined in Sect. 3.6.
To evaluate and compare model robustness in a uni-
fied fashion, we aggregate the worst performance
across different perturbations under each category.
Taking the docstring perturbation category as an
example, we say the model is robust only when
the model predicts correctly on all the sperturbed
datasets for each transformation listed in Tab. 1.
We present detailed numbers for each perturbation
type in Appendix D, Tab. 11-18. In Appendix B, we
showcase and analyze failure cases on CodeGen-
16B-mono under three top perturbations, causing
significant performance drops.
(1)Diverse pretraining corpus helps with both
generalization and worst-case robustness. Com-
paring all code generation models with the same
size 6B, CodeGen models have much better nom-
inal performance, and have better robustness on
RP@1, a very strict worst-case robustness met-
ric. That is possibly because CodeGen models
are pretrained over a more diverse corpus than In-
Coder and GPT-J and thus have more capacity to
deal with unseen instances and perturbations. How-
ever, CodeGen models have worse performance
on RD@1and RR@1, two robustness metrics
relative to nominal performance, indicating that
CodeGen models cannot generalize in a robust way
(e.g., may learn to use spurious features in data).
(2)Larger model size brings improvement in13823
worst-case robustness, but may risk overfitting.
In general, we observe higher RP@1for larger
models within the same model family (e.g., im-
proved from 0.174to0.217for CodeGen-mono 2B
to 16B on average across all perturbations), indi-
cating larger model helps improve worst-case ro-
bustness. Similarly, we observe that larger models
usually have larger RR@1(e.g., increased from
27.90% to35.91% for CodeGen-mono 2B to 16B
on average), indicating that larger models may risk
overfitting as the relative performance drops under
perturbations are significant.
(3)Code generation models are most sensitive
to syntax perturbation. Among all perturbation
types and across MBPP and HumanEval, we ob-
serve that syntax perturbations often result in the
most performance drops. That reveals a significant
limitation of syntax understanding ability of the
state-of-the-art code generation models.
(4)Datasets having more variances in codestyle poses more challenges on model robust-
ness. In Tab. 5, we can see that models show better
robustness on HumanEval over MBPP on average.
MBPP has more variances in code style (e.g., in-
dent with 1 space), closer to natural code distribu-
tion hence more challenging for model robustness.13824
4.2 Ablation Study
Robustness with sperturbed datasets. As de-
scribed in Sect. 3.6, our robustness metrics consider
worst-case performance across sperturbed datasets
for each perturbation. Larger sleads to stronger
perturbations evaluated, larger performance drops,
and more extensive coverage to practical failures.
The performance drops will start converging when
large enough sevaluated. We can clearly see such
trends in Fig. 4 where we evaluate CodeGen-16B-
mono RD@1 and RR@1 under greedy sampling
withs= 1, ...,10. Perturbation categories like
docstring and syntax that involve larger searching
space and more randomness tend to benefit more
with larger s(see Appendix A for details). As a
trade-off, evaluation cost linearly increase with s.
Thus, we recommend s= 5 as a good balance
between cost and evaluation strength. We sum-
marize the ablation study in terms of larger sam-
pling nin Appendix D.3 which can also benefit
our proposed robustness estimation with additional
sampling cost.
Stable RD@k and increasing RR@k under dif-
ferent k.Pass@k allows the model to have k
trials and model performance is often reported with
different k. With the sampling setting of n= 100 ,
we plot the RD@k and RR@k in Fig. 5. Interest-
ingly, we observe that RD@k stays stable across
different k while RR@k increases with k. This is
because larger k leads to higher nominal pass@k
and RP@k but their relative ratio stays similar lead-
ing to stable RD. On the other hand, larger k in-
volves more samples potentially changing results
on perturbed datasets causing larger RR. Similar
trends on CodeGen-2B and 6B in Appendix D.2
further confirm the observations.
4.3 Perturbation Sample Quality
Human evaluation. To verify the naturalness of
the perturbations in ReCode , we randomly sam-
ple and shuffle 100 and50perturbed and non-
perturbed MBPP and HumanEval data points and
create a shuffle mix of 300samples. Each sample is
shown to 5 human annotators who are familiar with
Python and who are asked to rate naturalness out
of 0: not natural, 0.5: possible to appear in practice
but rare, and 1: natural. The scores for naturalness
drop 14% on average for our perturbed data where
drops mainly come from typos by Butterfingers,
CharCaseChanges, SwapCharacter, etc.
In addition, we randomly sample 100and50
pairs perturbed and non-perturbed MBPP and Hu-
manEval data points. Each pair is shown to 5 hu-
man annotators who are asked to rate semantics out
of 0: totally changed, 0.5: slightly changed, and 1:
exactly preserved. We summarize the main results
in Tab. 6, and we present statistic details and setup
in Appendix C.1. Notably, the majority vote (at
least three out of five) is 1 for 90% of data points.
We further provide automatic evaluation below to
support the quality of our perturbed datasets, but
human evaluation is in general more reliable.
Docstring/function names similarity. We mea-
sure the sentence cosine similarity between
perturbed and non-perturbed docstrings and
function names. We obtain the embed-
dings by sentence transformers using model
all-mpnet-base-v2(Song et al., 2020). Note
that we split each function name into words to get13825sentence embeddings. On average, we have 0.93
and 0.81 for docstring and function name pertur-
bations, showing that they well preserve the se-
mantics. Scores for some function name pertur-
bations are sensitive to typos due to the lack of
sentence context (e.g., 0.21 for interperse and
intErpErse ). Appendix C.2 summarizes detailed
numbers for each perturbation.
Code syntax/format similarity. In Tab. 7, we
also measure the code similarity using CodeBLEU
scores (Lu et al., 2021) for perturbed and non-
perturbed data involving code syntax/format trans-
formations. Here we consider the CodeBLEU score
with syntax and dataflow separately as the evalua-
tion metrics. On average, we have score 0.96 and
0.97 for CodeBLEU syntax and dataflow, show-
ing good quality of perturbed datasets. Note that a
few perturbations should expect low CodeBLEU
scores: doc2comments transforms docstrings into
comments causing changes of syntax; Deadcode
insertion andfor-while switch involve new
if-conditions, loops, and new variables causing
changes of code syntax and dataflow. Please re-
fer to Appendix C.3 for details.
5 Conclusion
In this paper, we propose ReCode , a comprehensive
robustness evaluation benchmark for code gener-
ation models. We collect and customize over 30
natural transformations under categories of doc-
strings, function names, code syntax, and code for-
mat perturbations. These transformations are care-
fully selected and designed to be natural in practice
and preserve the semantic meaning after pertur-
bations. We further propose general worst-case
robustness metrics to give a unified overview of
the model robustness performance. We empirically
demonstrate our ReCode benchmark on popular
models including CodeGen, InCoder, and GPT-J
using HumanEval and MBPP datasets and function
completion tasks derived from them. With human
evaluation, over 90% of our perturbed data are con-
firmed to preserve the original semantic meaning;
sentence similarity and CodeBLEU scores addition-
ally support the quality of perturbations in ReCode .
Limitations
ReCode benchmark has several limitations: (1) It
contains perturbed datasets based on HumanEval
and MBPP which focuses on Python function com-
pletion use cases. Therefore, we only performevaluation on Python language and not be able
to capture robustness in a wide variety of code
completion use cases. However, our transforma-
tions are generalizable and could be easily ex-
tended to other languages and also other code-
related datasets (Athiwaratkun et al., 2023). We
encourage researchers to apply and extend ReCode
benchmark to additional languages and other code-
related tasks; (2) ReCode benchmark is designed
for robustness evaluation and cannot mitigate the
lack of robustness. Given that our benchmark can
be used to generate comprehensive collection of
perturbed data, we believe that it can be used for
training data augmentation to enhance model ro-
bustness. We will consider corresponding robust
training strategy design and evaluation in the future
work.
Ethics Statement
OurReCode robustness benchmark aims to provide
a comprehensive robustness evaluation framework
for any code-generation models, which we believe
is critical towards building robust and user-friendly
language models for code. With the new robustness
evaluation metrics, users can rely on ReCode and as-
sess model predictions with more confidence. The
model trainers, on the other hand, will be aware of
the potential vulnerabilities that might cause mis-
predictions in practice and mitigate them before
deployments. Therefore, we believe our ReCode
benchmark is beneficial in terms of broader impact.
References138261382713828ATransformation Details and Qualitative
Examples
In this section, we give detailed descriptions and
settings for each type of perturbations that are in-
cluded in our ReCode benchmark with qualitative
examples for illustrations.
A.1 Natural Transformations on Docstrings
For natural transformations on docstrings, we aim
to perturb the docstrings to their variances that
preserve the semantics and also appear natural in
practice. Specifically, we will first extract and per-
turb the docstrings with the following natural trans-
formations in each prompt, and then attach their
perturbed versions to the prompt. To preserve se-
mantics for the code generation task prompts, we
extract a blacklist of program keywords using tree-
sitter as discussed in Sect. 3.2 that are excluded
from perturbations. We extend most transforma-
tions from NL-Augmenter (Dhole et al., 2021), a
standard library designed for data augmentation
and robustness evaluation on text. We list some
qualitative examples in Tab. 1.
BackTranslation. BackTranslation paraphrases
the docstrings by translating them to another lan-
guage (in this case, German) and then back to En-
glish. It is a common method for data augmentation
in generating sentence variances with the same se-
mantics (Li and Specia, 2019; Sugiyama and Yoshi-
naga, 2019). Overall, it can reliably generate high
quality perturbed docstrings. We use the default
implementation in NL-Augmenter (Dhole et al.,
2021). BackTranslation contains no randomness in
transformations.
ButterFingers. ButterFingers transformation
randomly selects characters of the docstrings
and perturbs each of them to a random subset of
similar characters, it is from (Dhole et al., 2021)
and is also used in (Mille et al., 2021). Since this
transformation tends to introduce character-level
typos, we set randomness for perturbing each
character to be low as 0.05 for naturalness
consideration.
ChangeCharCase. ChangeCharCase transfor-
mation randomly changes the selected characters
to upper case in the docstrings. We use the default
probability 0.35 where majority annotators vote 0.5
for naturalness in the setting of Sect. 4.3.EnglishInflectionalVariation. This transforma-
tion randomly selects words in the docstring and
change them to a random inflection variance. This
can be from plural to singular (or vice versa) for
nouns and tense changes for verbs. To maintain
naturalness, the perturbation is constrained to be
the same Part of Speech (POS) tag in the Penn
Treebank (Marcus et al., 1993).
SwapCharacters. This transformation randomly
selects pairs of adjacent characters in the docstring
and swap them. This represents a common type of
typos by humans. To ensure naturalness, we set the
probability as 0.05 for making the swap.
SynonymInsertion. This transformation ran-
domly select words in the docstrings and inserts
their synonyms in WordNet (Miller, 1992). Punc-
tuations and stopwords are excluded. We set the
probability to be 0.35 considering low success rate
after keywords filtering.
SynonymSubstitution. This transformation ran-
domly selects words in the docstring and replaces
each one with a synonym from WordNet (Miller,
1992). Similar to SynonymInsertion, we set the
probability as 0.35 to balance naturalness and per-
turbation success rates.
TenseTransformationPast. This is a determin-
istic transformation that converts sentences in the
docstring to past tense.
TenseTransformationFuture. This is a deter-
ministic transformation that converts sentences in
the docstring to future tense.
Whitespace. This transformation inserts or
deletes a single white space at randomly selected lo-
cations in the docstring. This represents a common
type of typos by humans. Folowing NL-Augmenter,
we use probability 0.1 for adding whitespaces and
0.05 for removing whitespaces.
A.2 Natural Transformations on Function
Names
These transformations modify the name of the tar-
get function to generate. Any references to the
function name in the prompt, e.g., in docstring, are
also modified to maintain consistency. Qualitative
examples can be found in Tab. 2.
CamelCase. A function name is often composed
of multiple words. If the original function name
concatenates the words in camel-case style, this13829transformation changes it to snake-case, and vice
versa. This transformation is deterministic.
ButterFingers. ButterFingers transformation
randomly selects characters of the docstrings
and perturbs each of them to a random subset of
similar characters, it is from (Dhole et al., 2021)
and is also used in (Mille et al., 2021). Since this
transformation tends to introduce character-level
typos, we set randomness for perturbing each
character to be low as 0.05 for naturalness
consideration.
SwapCharacters. This transformation randomly
selects pairs of adjacent characters in the function
name and swap each pair. This represents a com-
mon type of typos by humans. To control natural-
ness, we set the probability to be 0.05, same setting
as the docstring perturbations.
ChangeCharCase. ChangeCharCase transfor-
mation randomly changes the selected characters
to upper case in the docstrings. We use the default
probability 0.35 where majority annotators vote 0.5
for naturalness in the setting of Sect. 4.3.
InflectionalVariation. This transformation ran-
domly selects words in the function name and ap-
plies a random inflection on them. This can be
from plural to singular (or vice versa) for nouns
and tense change for verbs. To control naturalness,
the perturbation is constrained to be the same Part
of Speech (POS) tag in the Penn Treebank (Marcus
et al., 1993).
SynonymSubstitution. This transformation ran-
domly selects words in the docstring and replaces
each one with a synonym from WordNet (Miller,
1992). Similar to SynonymInsertion, we set the
probability as 0.35 to balance naturalness and per-
turbation success rates.
A.3 Natural Transformations on Code Syntax
These transformations modify the code content in
the prompt. We derived function completion tasks
with half the code from the canonical solutions
such that the following code transformations and
robustness evaluation can be performed. To guar-
antee fair comparisons to the nominal baseline, we
make sure that we have the same block of code
before and after code perturbations. In the follow-
ing part we show qualitative examples on the same
MBPP sample baseline ( Fig. 6).
DeadCodeInserter. This transformation inserts
a block of useless code at a random location. The
added block can be a loop of zero iteration or an
if condition that is always false. The code content
inside the dummy loop or if condition is randomly
selected from the adjacent code statements with
limited tree-sitter node sizes.
For-While Switch. This transformation ran-
domly selects a for-loop or while-loop in the
prompt and transforms it to its equivalent coun-
terpart.
OperandSwap. This transformation randomly
selects a binary logical operation, swaps the two
operands, and modifies the operator if necessary to
maintain semantic equivalence.
VarRenamerCB. This transformation selects the
most frequently referenced variable name in the
partial code and replaces it throughout the prompt
with a new name obtained by CodeBERT (Feng13830
et al., 2020). Specifically, we replace all occur-
rence of the variable name with a mask token, and
then run CodeBERT inference to obtain candidate
names at each location, where each candidate name
comes with a probability score. We pick the candi-
date name with the highest aggregated score across
locations. This transformation is inspired by (Jha
and Reddy, 2022; Li et al., 2020).VarRenamerNaive. This transformation selects
the most frequently referenced variable name in the
partial code and replaces it with "V AR_0". This is
the original implementation in the NatGen package.
This transformation is deterministic.
VarRenamerRN. This transformation selects the
most frequently referenced variable name in the
partial code and replaces it with a random string
with half alphabetic and half numeric characters.
A.4 Natural Transformations on Code Format
Tab-Indent. This transformation replaces any
space indents with tabs or replaces tabs with 4
spaces for indent-sensitive languages like Python.
This transformation is deterministic.
Line Split. This transformation splits the longest
line in the partial code into two lines. This transfor-
mation is deterministic.
Doc2Comments. This transformation changes
the style of the documentation in the prompt. For
Python, it converts docstring (e.g., """ docstring
""") to commented lines (e.g., # docstring ) and
vice versa. For Java, it converts comments in the13831
format of /* docstring */ to// docstring and
vice versa. This transformation is deterministic.
NewlineRandom. This transformation inserts
empty lines at randomly selected positions.
NewlineAfterCode. This transformation inserts
an empty line at the end of the prompt. This trans-
formation is deterministic.
NewlineAfterDoc. This transformation inserts
an empty line between the docstring and the partial
code. This transformation is deterministic.
B Failure Case Study under
Perturbations
In this section, we showcase and analyze some
failure cases on CodeGen-16B-mono and perturbed
HumanEval datasets under three top perturbations
that will cause significant performance drops.
DeadCode insertion is one of the most effective13832perturbations. It can commonly mislead the model
predictions with the inserted dead code, especially
when the completions are required right after the
inserted dead code. Fig. 19 shows an failure ex-
ample where CodeGen-mono-16B only predicts a
newline after inserted meaningless for loop, which
might be mislead by the inserted return statement.
Fig. 20 shows a failure example of CodeGen-
16B-mono on a prompt where an empty newline
is inserted right before completion. Such simple
perturbation causes wrong predictions for the fol-
lowing if-else conditions. It is especially effective
when the required completion code is complicated.
ButterFingers perturbation on docstring causes
large performance drops as well. Fig. 21 shows an-
other falure example on CodeGen-16B-mono. The
typos introduced in the perturbation might cause
the model to misunderstand the targeted docstrings,
leading to wrong model completions.
C Perturbation Sample Quality
C.1 Details for Human Evaluation
The annotators are all recruited from software engi-
neers online who have good experience in Python
via strict coding interview. To guarantee the re-
liability of the human evaluation results, we first
conducted annotation trials with our annotators.
We gave them clear definitions for each level of
naturalness and semantic similarity.13833
We measure the inter-annotator agreement rate
Fless Kappa in Tab. 8. The overall average Fleiss
Kappa for the annotations is 0.52,0.36for seman-
tic and naturalness measurements on perturbed
samples. The confidence interval (95%) with boot-
strap sampling (10K samples) is [0.515,0.528]
and[0.358,0.364], indicating that our annotation
reaches “moderate agreement” and thus our anno-
tations are reliable (Gwet, 2014). The scores from
annotators are not perfectly consistent especially
for naturalness since people have different prefer-
ences for code.
C.2 Sentence Transformers for
Docstring/Function Names Similarity
In this subsection, we give experimental details for
measuring the sentence similarity of perturbed and
unperturbed data points using sentence transform-
ers.
To measure the similarity scores for the doc-
string perturbations, we first extract docstrings
from each pair of perturbed and unperturbed
data points, and we use sentence transformer
all-mpnet-base-v2 (Song et al., 2020) to predictan embedding vector for each docstring. Then co-
sine similarity is calculated and reported for each
pair of perturbed and unperturbed datapoints.
Same process cannot be directly applied to func-
tion name perturbations since function names are
concatenations of words instead of common sen-
tences, barely seen by the sentence transformer
training data. In order get more accurate sentence
embeddings for function names, we first split each
name into words (e.g., has_close_elements to
has close elements ) and then calculate the cor-
responding cosine similarities.
In Table 9, we present the detailed results for
each type of perturbations for sentence similarity.
On average, we can have 0.93 and 0.92 similar-
ity scores for docstring perturbations and 0.80 and
0.81 for function name perturbations on the Hu-
manEval and MBPP datasets. The overall high
similarity numbers provide support that our per-
turbations have good quality in naturalness and
semantic preservation from the unperturbed inputs.
Some function name perturbations including
ButterFinger, SynonymSubstitution, and Char-
CaseChange have relatively low sentence similarity.
This is mainly because the function names only in-
clude keywords without complete sentence context
and thus minor changes to each words could po-
tentially cause large change in measured cosine
similarity. For instance, character case changes
on function name intersperse tointErspErse
which lacks of context only has 0.21 similar-
ity. On the other hand, the function names
with more context has much higher scores, e.g.,
1.0 similarity score for has_close_elements and
has_ClosE_Elements .
C.3 CodeBLEU Scores for Code Similarity
Here we present the experimental details for the
CodeBLEU syntax and dataflow scores to quantita-
tively measure the quality of our code syntax and
format transformations.
The measurement is straightforward. The unper-
turbed baseline is each data point from our cus-
tomized partial code datasets derived from Hu-
manEval and MBPP. The perturbed one is the
same data point transformed by each type of
our perturbations. The CodeBLEU syntax and
dataflow scores are then directly measured using
the CodeXGLUE (Lu et al., 2021) implementa-
tion.13834
In Table 10, we present the detailed CodeBLEU
results for each type of perturbations. The aver-
age numbers are summarized in Table 7. Overall,
77% and 89% of our transformations have over
0.9 CodeBLEU syntax and dataflow scores, show-
ing good quality in preserving semantics from the
unperturbed code.
However, CodeBLEU syntax and dataflow are
not perfect in quantitatively measuring naturalness
and semantic preservation for the perturbations
and thus some perturbations have expected rela-
tively low scores: Doc2Comments transforms doc-
strings into comments causing changes of syntax;
Deadcode insertion andfor-while switch in-
volve new if-conditions, loops, and new variables
causing changes of code syntax and dataflow.
D Additional Results
D.1 Fine-grained Robustness Evaluation
We present the robustness evaluation for each type
of perturbations from Table 11 to 18, . The eval-
uation setting is the same as Table 3 and 4 where
we evaluate various sizes of CodeGen (Nijkamp
et al., 2022), InCoder (Fried et al., 2022), and GPT-
J (Wang and Komatsuzaki, 2021) with greedy sam-
pling. For each type of perturbations, we randomly
generate s= 5 different perturbed datasets de-
rived from HumanEval and MBPP. For perturba-
tions without randomness, only one single version
of perturbed dataset is evaluated. The list of indeter-
ministic perturbations can be found in Appendix A.D.2 Additional Results for Different k
As discussed in Sect. 4.2, we observe that Robust
Drop stays stable across different k while Robust
Relative increases linearly with k. We present addi-
tional results on CodeGen-2B-mono, CodeGen-6B-
mono along with CodeGen-16B-mono in Fig. 22.
We evaluate each model with large n(n= 100)
using top-p sampling strategy with probability 0.95
and temperature 0.2.
D.3 Additional Results for Large Sampling n
Larger sampling nis commonly used for prevent-
ing model generation variances and providing ac-
curate estimations. The evaluation cost increases
linearly to n. Here we show that larger ncan also
benefit our proposed three robustness metrics but
not causing significant differences. In specific,
we measure Robust Pass@1, Robust Drop@1,
and Robust Relative@1 on CodeGen-16B-mono
and HumanEval dataset. The model is run with
n= 100 using top-p sampling strategy with prob-
ability 0.95and temperature 0.2. We present de-
tailed results in Tab. 19.13835138361383713838138391384013841ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.13842/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.13843