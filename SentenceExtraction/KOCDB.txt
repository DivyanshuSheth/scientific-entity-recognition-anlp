
Viraj MehtaOjash NeopaneVikramjeet DasSen Lin Jeff SchneiderWillie Neiswanger
Abstract
Preference-based feedback is important for many
applications where direct evaluation of a reward
function is not feasible. A notable recent exam-
ple arises in reinforcement learning from human
feedback on large language models. For many
of these applications, the cost of acquiring the
human feedback can be substantial or even pro-
hibitive. In this work, we take advantage of the
fact that often the agent can choose contexts at
which to obtain human feedback in order to most
efficiently identify a good policy, and introduce
theoffline contextual dueling bandit setting. We
give an upper-confidence-bound style algorithm
for this setting and prove a regret bound. We also
give empirical confirmation that this method out-
performs a similar strategy that uses uniformly
sampled contexts.
1. Introduction
In many decision making problems in information retrieval,
question answering, clinical trials, advertising, and other
fields, feedback about the performance of a particular choice
is only available in the form of a preference between sev-
eral options. Such feedback often takes the form of a user
clicking on a link in the wild but can also be collected from
labelers in an attempt to understand their underlying pref-
erences and train a system to optimize them. This is an
especially useful method for collecting human feedback be-
cause humans are unreliable in giving scalarized feedback
compared to the accuracy of their preferences (Ouyang et al.,
2022). Often, these settings come with some context that
can provide information about the associated distribution
over preferences. This could be the search term, some infor-
mation about a trial participant, or the prompt in a questionanswering setting.
Recently, these techniques have seen large amounts of at-
tention when popularized through reinforcement learning
from human feedback (RLHF). RLHF is one of the ma-
jor techniques for aligning large language models (LLMs)
for use as a chat assistant or for other specialized applica-
tions after pretraining on a sequence modeling objective.
In these applications, human raters are provided with a
prompt and several possible responses taken from the LLM.
They are asked to rank the human responses based on their
preferences given the prompt. This process requires a rela-
tively large number of samples from human raters (tens of
thousands) in order for the alignment process to succeed.
This can incur large costs for the data collection. For more
specialized problems than a general chatbot assistant, the
feedback required may be impractical or expensive relative
to the desired application.
In many applications, the feedback is collected online as
the policy is learned. Under these circumstances, the con-
texts are typically assumed to be drawn from an (unknown)
stationary probabity distribution and the agent’s object is
to quickly find a near optimal decision rule. Currently, in
the RLHF setting, the prompts presented to the model in
order to sample responses and then to the raters are typi-
cally sampled uniformly from a response set intended to be
representative of the test-time distribution.
In such cases, the aforementioned online setting does not
allow us to fully take advantage of the problems structure –
we can control which prompts and responses are presented
to the human raters for feedback and we are not interested
in the performance of the actions chosen during labeling,
just the performance of the policy at test time afterwards.
Instead of the standard contextual bandit problem it is then
more appropriate to consider the so-called offline contex-
tual bandit (Char et al., 2019) where we are additionally
allowed to select the contexts for which we receive feed-
back. By leveraging this control over this less restrictive
data-generating process, we show how to select contexts
and actions in order output policies with stronger optimality
guarantees without the need to collect more data.
Here, we tackle the special case of pairwise feedback where
a pair of actions are compared given a particular context.
Following Xu et al. (2020), we first reduce the problem
2. Related Work
3. Problem Setting
4. Method and Analysis
5. Experiments
6. Conclusion and Future Work
References
A. Proof of Theorem 4.1
B. RKHS norms of rand f