Kernelized Offline Contextual Dueling Bandits Viraj Mehta 1 Ojash Neopane 2 Vikramjeet Das 2 Sen Lin Jeff Schneider 1 2 Willie Neiswanger 3 Abstract answering setting. Preference-based feedback is important for many applications where direct evaluation of a reward function is not feasible. A notable recent example arises in reinforcement learning from human feedback on large language models. For many of these applications, the cost of acquiring the human feedback can be substantial or even prohibitive. In this work, we take advantage of the fact that often the agent can choose contexts at which to obtain human feedback in order to most efficiently identify a good policy, and introduce the offline contextual dueling bandit setting. We give an upper-confidence-bound style algorithm for this setting and prove a regret bound. We also give empirical confirmation that this method outperforms a similar strategy that uses uniformly sampled contexts. Recently, these techniques have seen large amounts of attention when popularized through reinforcement learning from human feedback (RLHF). RLHF is one of the major techniques for aligning large language models (LLMs) for use as a chat assistant or for other specialized applications after pretraining on a sequence modeling objective. In these applications, human raters are provided with a prompt and several possible responses taken from the LLM. They are asked to rank the human responses based on their preferences given the prompt. This process requires a relatively large number of samples from human raters (tens of thousands) in order for the alignment process to succeed. This can incur large costs for the data collection. For more specialized problems than a general chatbot assistant, the feedback required may be impractical or expensive relative to the desired application. In many applications, the feedback is collected online as the policy is learned. Under these circumstances, the contexts are typically assumed to be drawn from an (unknown) stationary probabity distribution and the agent’s object is to quickly find a near optimal decision rule. Currently, in the RLHF setting, the prompts presented to the model in order to sample responses and then to the raters are typically sampled uniformly from a response set intended to be representative of the test-time distribution. 1. Introduction In many decision making problems in information retrieval, question answering, clinical trials, advertising, and other fields, feedback about the performance of a particular choice is only available in the form of a preference between several options. Such feedback often takes the form of a user clicking on a link in the wild but can also be collected from labelers in an attempt to understand their underlying preferences and train a system to optimize them. This is an especially useful method for collecting human feedback because humans are unreliable in giving scalarized feedback compared to the accuracy of their preferences (Ouyang et al., 2022). Often, these settings come with some context that can provide information about the associated distribution over preferences. This could be the search term, some information about a trial participant, or the prompt in a question In such cases, the aforementioned online setting does not allow us to fully take advantage of the problems structure – we can control which prompts and responses are presented to the human raters for feedback and we are not interested in the performance of the actions chosen during labeling, just the performance of the policy at test time afterwards. Instead of the standard contextual bandit problem it is then more appropriate to consider the so-called offline contextual bandit (Char et al., 2019) where we are additionally allowed to select the contexts for which we receive feedback. By leveraging this control over this less restrictive data-generating process, we show how to select contexts and actions in order output policies with stronger optimality guarantees without the need to collect more data. *Equal contribution 1Robotics Institute, Carnegie Mellon University, USA 2Machine Learning Department, Carnegie Mellon University, USA 3Department of Computer Science, Stanford University, USA. Correspondence to: Viraj Mehta <virajm@cs.cmu.edu>. Here, we tackle the special case of pairwise feedback where a pair of actions are compared given a particular context. Following Xu et al. (2020), we first reduce the problem The Many Facets of Preference Learning Workshop at the International Conference on Machine Learning (ICML), Honolulu, Hawaii, USA, 2023. Copyright 2023 by the author(s). 
Kernelized Offline Contextual Dueling Bandits of finding the optimal action given pairwise feedback to finding the action that optimizes the Borda function given a particular context. The Borda function is the probability that for a particular context, a selected action is preferred over another action selected uniformly at random. We select contexts which maximize the uncertainty over the Borda ‘value function’ and then select one action optimistically and the other uniformly. and overall quality. The results are spectacular, with the 1.3B parameter InstructGPT matching the 175B GPT-3 in performance on a variety of tasks. Because the focus was ensuring representation both in the inputs to models used in real life and in the human feedback received, the team used 40 labelers and worked with a dataset of more than 100,000 examples. A related paper from Zhu et al. (2023) also explores a simplified version of this problem and showed that under the strong assumption of a linear model given a known feature mapping, the policy obtained by optimizing the pessimistic MLE given a fixed dataset is provably optimal for learning in the k-wise comparison context. Given these strong assumptions, the authors point out that a G-optimal experimental design for online data collection as in Soare et al. (2014) would be maximally informative. However, these assumptions are unrealistic and do not represent the methods used in practice as reward model training is usually conducted over all layers of a deep model. In this work, we show that our method provably achieves (cid:16) L1√ suboptimality at most O everyT where in the context space after T iterations with probability 1 − δ under the assumption that the Borda function is bounded by B in RKHS norm. We also demonstrate on a distribution of synthetic problems that it performs well when implemented and outperforms a baseline with uniformly selected contexts and an optimistic policy as well as entirely uniform sampling. (cid:113) (cid:17)(cid:17) (cid:16) log 1 δ B + ΦT 2. Related Work Dueling Bandits At the same time, the bandit literature has also explored the effectiveness of comparative feedback (“dueling bandit”) while considering the cost of acquiring such information. This was first studied by Yue et al. (2012) in settings where comparative information is relatively easy to extract but absolute rewards (i.e., direct queries) are illdefined and have no absolute scale. Later, Bengs et al. (2021) surveyed methods used in the online learning setting, where the trade off with cost of information is most acute, including those used in the online contextual dueling bandit setting by Dud´ık et al. (2015). These constraints motivate a kernelized approach that can incorporate the nonlinearities in the models used in practice. Learning from Comparative Feedback There is a rich literature on reinforcement learning from comparative human feedback, including work by F¨urnkranz et al. (2012), Akour (2014) and Christiano et al. (2017). Many of these works grappled with the heightened need for sample efficiency given the cost of acquiring human feedback. In particular, Christiano et al. (2017) made it feasible to use human feedback for deep reinforcement learning by training a reward model that is then used as the target for reinforcement learning. In their Atari test case, where naive deep RL would have required thousands of hours of gameplay, they were able to achieve superior performance with only 5,500 or several hours of human queries. More recently, methods of using comparative human feedback have gained prominence as a means of improving the performance of language models. These methods have been shown to be effective at improving stylistic continuation (Ziegler et al., 2019), text summarization (Stiennon et al., 2020), translation (Kreutzer et al., 2018), semantic parsing (Lawrence & Riezler, 2018), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019). However, while effective, incorporating human feedback brought substantial costs. For example, Stiennon et al. (2020) achieved significant improvements to baseline, but needed summaries on 123,169 posts from the TL;DR dataset generated by a small team of labelers (more than 21 persons) from Upwork, Scale, and Lionbridge to train. Offline Contextual Bandit Optimization When there are distinct phases of learning and then deployment, an agent can make maximal use of every example during learning to acquire information that can be exploited once deployed. Char et al. (2019) introduce this idea for black-box function approximation by considering a setting where at test time the goal is to perform well on average across a context distribution while during learning the goal is to choose contexts and actions that are most useful for that goal. Given a reward function for each task, the authors proposed a multi-task version of Thompson sampling during the offline training phase, which allows provable regret bounds in that problem setting. We extend this setting from cardinal to ordinal rewards as is appropriate for comparative feedback. This heavy-resource requirement is again reflected even in later, state-of-the-art work. Ouyang et al. (2022) focused on using RLHF to improve alignment of the GPT-3 model (at 175B parameters) with human values on a variety of directions, including toxicity, hallucinations, moral opinion, In Li et al. (2023), the agent queries the states (or contexts, in a bandit setting) where the value function is most uncertain and acts optimistically. Combined with least-squares value iteration, this method leads to provable polynomial
Kernelized Offline Contextual Dueling Bandits to the contextual setting as follows, given as fr : X × A → [0, 1] where fr(x, a) = Ea′∼U (A) [P (a ≻ a′ | x)], where U (A) is the uniform measure over the action space. It is clear from the definition that fr and r will have the same maximizers. sample convergence in the worst-case error of the value function estimate in reinforcement learning in general, and as a corollary the setting from Char et al. (2019) as a special case. This sets the foundation that we will adapt to the comparative feedback setting. We conclude this section by discussing the structural assumptions we place on the reward function as well as the contextual Borda function. Our first assumption restricts the reward and contextual Borda functions to be ‘smooth’ in an underlying Reproducing Kernel Hilbert Space (RKHS). 3. Problem Setting In this paper, we consider a dueling variant of the so-called offline contextual bandit problem introduced in Char et al. (2019). An instance of this problem is defined by a tuple (X , A, f ) where X denotes the context space, A denotes the action space and f : X × A × A → [0, 1] is a preference function so that f (x, a, a′) denotes the probability that the action a is preferred to the action a′ when the underlying context is x. We will design algorithms that operate under the following interaction protocol, which occurs for T time steps. During each time step t ∈ [T ], the agent selects a context xt ∈ X and a pair of actions at, a′ t ∈ A and observes a binary random variable Rt ∼ Bern(f (xt, at, a′ t)) which equals one if at is preferred to a′ t) and zero otherwise. Assumption 3.1. Let κ denote a Positive Semi-Definite kernel and let Hκ denote its associated RKHS. We assume that ∥r∥κ , ∥fr∥κ ≤ B, where B is a known constant. Note that this assumption is different than the standard assumption which only requires that r has bounded RKHS norm. This is due to the generality of our setting which allows for multiple different link functions. While this assumption is not ideal, it is difficult to bound the norm of fr given a bound on the norm of r. We investigate this issue more in Section B where we empirically find that the norm of the Borda function is almost always smaller than the norm of the reward function. t (denoted at ≻ a′ We assume that the preference function takes the following form Our second assumption relates the reward function to the contextual Borda function. Assumption 3.2. Let f ∗ r (x) = maxa fr(x, a) and r∗(x) = maxa r(x, a). There exists a constant L1 such that for every x ∈ X , a ∈ A we have 1 r (x) − L1 fr(x, a). f (x, a, a′) = σ (r(x, a) − r(x, a′)) , (1) where σ : R → [0, 1] is the link function and r : X × A → R is the reward function. Common link functions include the logistic function, which leads to the BradleyTerry-Luce (BTL) model (Bradley & Terry, 1952) as well as the Gaussian CDF (Thurstone, 1927). We also place some additional assumptions on the reward function which we discuss at the end of this section. (r∗(x) − r(x, a)) ≤ f ∗ This assumption implies that differences in r will cause a similar magnitude of difference in fr In fact, when σ is Lipschitz continuous, it is sufficient for the Lipschitz constant of σ to be at least 1/L1 for this condition to hold. Our objective within this protocol is to design algorithms that are able to quickly identify policies with small suboptimality. We define the suboptimality of a policy π as 4. Method and Analysis (cid:18) (cid:19) SubOpt(π) = sup x∈X sup a∈A r(x, a) − r(x, π(x)) . (2) At a high level, our approach reduces the dueling feedback problem to contextual optimization over a single action via the contextual Borda function introduced in Section 3. Once reduced appropriately, we apply techniques adapted from recent work on active exploration in reinforcement learning to construct a sampling rule and policy selection rule which allows us to output a policy with provably small suboptimality. Broadly, our sampling rule samples contexts at which there is maximum uncertainty over the Borda ‘value function’ and then compares the optimistic action with an action sampled uniformly from the action set. We remark that this notion of suboptimality is much stronger than usual notions that look at the expected suboptimality of the final policy when the contexts are sampled from some known distribution. In contrast, the form of suboptimality we consider here looks at the worst-case context for each policy. Before discussing the assumptions we place on the reward function, we first introduce a closely related function, called the contextual borda function fr, which generalizes the borda function introduced in by Xu et al. (2020). The Borda function as introduced in Xu et al. (2020) for dueling-choice optimization is defined as the probability that a particular action a will be preferred over a random action a′ uniformly sampled from the action space. We generalize this definition 4.1. Estimating The Contextual Borda Function By design, we can easily estimate the contextual Borda ? ≻ a′ function from data of the form {xt, at t}, where the 
Kernelized Offline Contextual Dueling Bandits Algorithm 1 Borda-AE contexts xt and first actions at are arbitrary and the second actions a′ t are uniformly selected. In this work, we model the contextual Borda function using standard kernelized ridge regression (KRR) (Rasmussen et al., 2006). The key feature of KRR is that besides an estimate of the contextual Borda function after t observations µt(x, a), we can also estimate the uncertainty over the prediction σt(x, a) using standard results; under the assumptions in Section 3 and given an value for β appropriate chosen for our confidence level, we can bound |fr(x, a) − µt(x, a)| ≤ βσt(x, a) with the desired confidence. 1: Input: kernel function κ(·, ·), exploration parameters β(r) t , number of inital data n0 ? ≻ a′ i}n0 2: Let Dn0 = {si, ai 3: for t = n0 + 1, . . . , T do 4: 5: 6: i=1 for si, ai, a′ i uniform. Compute µt(·, ·), σt(·, ·) using KRR. Choose xt according to (3). Choose at according to (4), a′ Let Dt = Dt−1 ∪ {(xt, at t ∼ U (A). ? ≻ a′ t)}. 7: 8: end for 9: Output a final policy ˆπT according to (5). 4.2. Selecting Contexts and Actions Our sampling rule builds on top of the one established in Li et al. (2023) —put simply the rule is to sample the state with the maximum uncertainty over the value function and then act optimistically. We will now present our algorithm which highlights how to extend these idea to the dueling setting via the contextual Borda function fr. For now, we assume that there is a known bonus term β(r) for all t. We can then define upper and lower confidence bounds f t r(x, a) = µt(x, a) − β(r) where rA = [r(x)]x∈A , ϵA ∼ N (0, η2I) and I(X; Y ) = H(X) − H(X|Y ) is the mutual information. With this definition, we are now ready to state our result. Theorem 4.1. Suppose we run Algorithm 1 with (cid:115) (cid:19) (cid:18) 2 δ β(r) t = 2B + 2Φt + 1 + log , (7) t r(x, a) = µt(x, a) + β(r) then, with probability at least 1 − δ, we have that t σt(x, a) and f t (cid:32) (cid:32) (cid:33)(cid:33) (cid:114) t σt(x, a). Our rule is to sample a context L1√ T 1 δ B + ΦT SubOpt(ˆπT ) ≤ O log . (8) (cid:18) (cid:19) f t r(x, a) f t r(x, a) − max a∈A xt ∈ arg maxx∈X max a∈A . (3) Proof Sketch. At a high-level the proof of this result is as follows. First, we use standard results on KRR to show that our choice of β(r) guarantees that our confidence bands contain f r(x, a) with high probability simultaneously for all t and x, a ∈ X × A. Next, we use assumption 3.2 to show that the suboptimality of the pessimistic policy induced by our estimated contextual borda function is small whenever we are able to estimate the contextual borda function well. Finally, we conclude the proof by showing that our sampling rule indeed allows us to estimate thet contextual borda function well. Here, we are choosing a context that maximizes the difference between the optimistic ‘value function’ and the pessimistic ‘value function’ (both of which require a maximization over actions to compute). We then optimistically choose an action by at ∈ arg maxa∈A f t r(xt, a). (4) After repeating this process T times, we output a pessimistic policy against the tightest lower bound we can find, which is the maximizer of all our lower bounds through the optimization process. Put formally we return ˆπT : X → A such that Concrete Performance Bounds. To more concretely understand the performance of our algorithm, we instantiate our results for three commonly studied kernels: the linear, squared-exponential. For both of these settings, the scaling of the information gain is well known (see for example (Srinivas et al., 2010)). In the linear setting, we have that ΦT = d log T leading to a bound of O . For squared exponential kernels we have ΦT = O (cid:0)log(T )d+1(cid:1) leading to a suboptimality bound of O f t r(x, a). ˆπT (x) ∈ arg maxa∈A max t≤T (5) From these pieces we construct the full algorithm, BordaAE, which we present in Algorithm 1. (cid:113) (cid:16) L1√ (cid:17)(cid:17) (cid:16) 4.3. Bounding the regret of Borda-AE log 1 δ dB log T T Before proceeding with our algorithm’s formal guarantees, we first introduce the maximal-information gain which plays an important role in our results. The maximum information gain over t rounds, denoted Φt, is defined as B log(T )d+1(cid:113) (cid:16) (cid:16) L1√ (cid:17)(cid:17) . log 1 δ T When compared to existing results for dueling bandits (Xu et al., 2020) as well as standard bandits (Chowdhury & Gopalan, 2017), we see that our suboptimality bounds Φt = max A⊂X ×A:|A|=t I(rA + ϵA; rA), (6) 
Kernelized Offline Contextual Dueling Bandits match, thus showing that our algorithm is able to achieve the same performance under a stronger performance metric. 5. Experiments In order to assess the validity of our theory we have begun an experimental campaign starting with synthetic experiments that allow us to come as close as possible to the theoretical setting and empirically confirm our results. To do so, we implemented the regression using the BernoulliGP model provided by GPyTorch (Gardner et al., 2018). We use a Mat´ern kernal with automatic relevance detection with hyperparameters fit via maximum a posteriori optimized by the Adam algorithm (Kingma & Ba, 2014). We tested on distributions of synthetic reward functions generated by sampling a random linear combination of Random Fourier Features (Rahimi & Recht, 2007) derived from a squared exponential kernel. For each sampled reward function r, we used the Bradley-Terry model where p(a ≻ a′ | x) = 1+exp(r(x,a′)−r(x,a)) to generate comparison data. For each trial we uniformly sampled n0 = 25 datapoints and then selected data to observe until T = 500 total datapoints had been collected according to one of three methods: 1 • Borda-AE: our method, as described in Section 4. • Borda-Uniform: uniform sampling of context and actions. • Borda-UCB: uniform sampling of contexts with UCB Figure 1. Performance of all methods across 10 random functions r with 1D Context and 1D action. The top plot shows the median regret across contexts and the bottom shows the maximum. Error regions are standard errors. actions as in Borda-AE. This last method reduces to the method presented in Xu et al. (2020) when na¨ıvely generalized to the contextual setting. All methods have the same test-time behavior of executing the action found by optimizing the pessimistic Borda function estimate for the test context. By optimizing the ground-truth reward function we were able to approximate the optimal policy and therefore estimate the regret of our policy against it. We give an example of the progression of our method for 1D context and 1D actions in Figure 2 as well as a comparison against Borda-Uniform and BordaUCB in Figure 1. One can see that Borda-AE performs best both on median regret and on the maximum regret, which was the metric of interest in our theoretical analysis. able to in theory and in controlled practical settings achieve promising performance, it has a number of issues that prevent its wider application. First, we reflect on the fact that the Borda function is a blunt tool: in order to make sampling tractable we must sample a′ uniformly during exploration. Since we have the possibility of choosing both a and a′ this seems somewhat wasteful of the a′ samples. In the context of RLHF with language models this is especially grim as uniformly chosen sequences are likely to be gibberish and the obvious preference for plausible sequences vs uniform ones will mean that the data selected via this strategy would be less likely to be useful. It is clear the method is quickly able to concentrate samples in regions that could plausibly be the optimum and it is similarly clear that the peaks in the acquisition function over contexts are sensible given the mean and uncertainty estimates of fr. Many of these applications also benefit from the parallel nature of modern hardware and only make sense when presented with large batches of data. The sequential nature of Borda-AE makes it unsuitable for these applications as well. With all this in mind, however, we still believe that there is tremendous potential in finding methods for solving these problems, especially with the worst-case style guarantees such as the ones provided here. We hope to continue the progress as we work towards solving these issues. 6. Conclusion and Future Work In this work we introduced the offline contextual dueling bandit setting and presented a first efficient algorithm for solving it in the kernelized setting. Though our method is 
Kernelized Offline Contextual Dueling Bandits Time = 50 Time = 150 Time = 600 Figure 2. Progress of Borda-AE across 50, 150, and 600 datapoints. From the top the charts show the ground truth function, the mean of the posterior estimate of fr, the uncertainty function, the estimate of the value function as well as the acquisition function given in (3), and the regret. 
