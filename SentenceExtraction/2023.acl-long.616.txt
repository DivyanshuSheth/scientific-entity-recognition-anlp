
David Jurgens
University of Michigan
jurgens@umich.eduAgrima Seth
University of Michigan
agrima@umich.edu
Jackson Sargent
University of Michigan
jacsarge@umich.eduAthena Aghighi
University of California, Davis
aaghighi@ucdavis.eduMichael Geraci
University of Buffalo
megeraci@buffalo.edu
Abstract
Understanding interpersonal communication
requires, in part, understanding the social con-
text and norms in which a message is said.
However, current methods for identifying of-
fensive content in such communication largely
operate independent of context, with only a
few approaches considering community norms
or prior conversation as context. Here, we
introduce a new approach to identifying in-
appropriate communication by explicitly mod-
eling the social relationship between the in-
dividuals. We introduce a new dataset of
contextually-situated judgments of appropri-
ateness and show that large language mod-
els can readily incorporate relationship infor-
mation to accurately identify appropriateness
in a given context. Using data from online
conversations and movie dialogues, we pro-
vide insight into how the relationships them-
selves function as implicit norms and quan-
tify the degree to which context-sensitivity
is needed in different conversation settings.
Further, we also demonstrate that contextual-
appropriateness judgments are predictive of
other social factors expressed in language such
as condescension and politeness.
1 Introduction
Interpersonal communication relies on shared ex-
pectations of the norms of communication (Hymes
et al., 1972). Some of these norms are widely
shared across social contexts, e.g., racial epithets
are taboo, enabling NLP models to readily iden-
tify certain forms of offensive language (Fortuna
and Nunes, 2018). Yet, not all norms are widely
shared; the same message said in two different
social contexts may have different levels of accept-
ability (Figure 1). While NLP has recognized the
role of social context as important (Hovy and Yang,
2021; Sheth et al., 2022), few works have directlyFigure 1: The same message can be appropriate or not
depending on the social context in which it is said.
incorporated this context into modeling whether
messages violate social norms. Here, we explicitly
model relationships as the social context in which
a message is said in order to assess whether the
message is appropriate.
NLP models have grown more sophisticated in
modeling the social norms needed to identify offen-
sive content. Prior work has shown the beneﬁts of
modeling context (Menini et al., 2021), such as the
demographics of annotators and readers (Sap et al.,
2019; Akhtar et al., 2021) and the online commu-
nity in which a message is said (Chandrasekharan
et al., 2018; Park et al., 2021). However, these
works overlook normative expectations within peo-
ple’s relationships.
In this paper, we introduce a new dataset of over
12,236 instances labeled for whether the message
was appropriate in a given relationship context. Us-
ing this data, we show that computation models
can accurately identify the contextual appropriate-
ness of a message, with the best-performing model
attaining a 0.70 Binary F1. Analyzing the judg-
ments of this classiﬁer reveals the structure of the
shared norms between relationships. Through ex-
amining a large corpus of relationship-labeled con-
versations, we ﬁnd that roughly 19% of appropriate
messages could be perceived as inappropriate in
another context, highlighting the need for models10994that explicitly incorporate relationships. Finally,
we show that our model’s relationship-appropriate
judgments provide useful features for identifying
subtly offensive language, such as condescension.
2 Social Norms of Appropriateness
Relationships are the foundation of society: most
human behaviors and interactions happen within
the context of interpersonal relationships (Reis
et al., 2000). Communication norms vary widely
across relationships, based on the speakers’ so-
cial distance, status/power, solidarity, and per-
ceived mutual beneﬁt (Argyle et al., 1985; Fiske,
1992). These norms inﬂuence communication in
content, grammar, framing, and style (Eckert and
McConnell-Ginet, 2012) and help reinforce (or sub-
vert) the relationship between speakers (Brown and
Levinson, 1987). Prior computational work mostly
frames appropriateness as exhibiting positive affect
and overlooks the fact that, in some relationships,
conversations can be affectively negative but still
appropriate (King and Sereno, 1984). For example,
swearing is often considered a norm violation (Jay
and Janschewitz, 2008), but can also be viewed as
a signal of solidarity between close friends (Mon-
tagu, 2001) or co-workers (Baruch and Jenkins,
2007). In such cases, the violation of taboo rein-
forces social ties by forming a sense of in-group
membership where norms allow such messages
(Coupland and Jaworski, 2003).
In sociolinguistics, appropriateness is a func-
tion of both context and speech. Trudgill (1997)
argues that “different situations, different topics,
different genres require different linguistic styles
and registers,” and Hymes (1997) argues that the
extent to which “something is suitable, effective
or liked in some context” determines its appropri-
ateness. Whether a discourse is appropriate de-
pends strongly on the social context in which it
is produced and received (Fetzer, 2015), making
the assessment of appropriateness a challenging
task due to the need to explicitly model contex-
tual norms. Behavioral choices are subject to the
norms of “oughtness” (Harré and Secord, 1972;
Shimanoff, 1980), and Floyd and Morman (1997)
suggest relationship types as an important factor in-
ﬂuencing the normative expectations for relational
communication. For example, while it may be con-
sidered appropriate for siblings to discuss their past
romantic relationships in detail, the topic is likely
to be perceived as taboo or inappropriate betweenromantic partners (Baxter and Wilmot, 1985).
3 Building a Dataset of Contextual
Appropriateness
Prior work has shown that interpersonal relation-
ships are a relevant context for the appropriateness
of content (Locher and Graham, 2010). While not
all messages differ in this judgment—e.g., “hello”
may be appropriate in nearly all settings—building
a dataset that embodies this context sensitivity re-
mains a challenge. Here, we describe our effort to
build a new, large dataset of messages rated for con-
textual appropriateness, including how we select
relationships and operationalize appropriateness.
Due to the challenge of identifying and rating these
messages, our dataset is built in two phases.
Selecting Relationships Formally categorizing
relationships has long been a challenging task for
scholars (Regan, 2011). We initially developed a
broad list of relationships, drawing from 1) folk
taxonomies (Berscheid et al., 1989), e.g., com-
mon relationship types of friends (Adams et al.,
2000), family (Gough, 1971), or romantic part-
ners (Miller et al., 2007); and 2) organizational
and social roles (Stamper et al., 2009), e.g., those
in a workplace, classroom, or functional settings,
as these frequently indicate different social status,
distance, or solidarity between individuals in the
relationship. Using this preliminary list, four anno-
tators performed a pilot assessment of coverage by
discussing quotes from movie scripts, social media,
or their imagination and identifying cases where an
excluded relationship would have a different judg-
ment for appropriateness. Ultimately, 49 types of
relationships were included, shown in Table 1.
Deﬁning Appropriateness Appropriateness is a
complex construct that loads on many social norms
(Fetzer, 2015). For instance, in some relationships,
an individual may freely violate topical taboos,
while in other relationships, appropriateness de-
pends on factors like deference due to social status.
Informed by the theory of appropriateness (March
and Olsen, 2004), we operationalize inappropri-
atecommunication as follows: Given two people
in a speciﬁed relationship and a message that is
plausibly said under normal circumstances in this
relationship, would the listener feel offended or
uncomfortable? We use plausibility to avoid judg-
ing appropriateness for messages that would likely
never be said, e.g., “would you cook me a ham-10995
burger?” would not be said from a doctor to a
patient. We constrain the setting to what an annota-
tor would consider normal circumstances for peo-
ple in such a relationship when deciding whether
the message would be perceived as appropriate;
for example, having a teacher ask a student to say
something offensive would be an abnormal context
in which that message is appropriate. Thus, during
annotation, annotators were asked to ﬁrst judge if
the message would be plausibly said and only, if
so, rate its appropriateness.
Judging appropriateness necessarily builds on
the experiences and backgrounds of annotators.
Culture, age, gender, and many other factors likely
inﬂuence decisions on the situational appropriate-
ness of speciﬁc messages. In making judgments,
annotators were asked to use their own views and
not to ascribe to a judgment of a speciﬁc identity.
Raw Data Initial conversational data was se-
lectively sampled from English-language Reddit.
Much of Reddit is not conversational in the sense
that comments are unlikely to match chit-chat.
Further, few comments are likely to be context-
sensitive. To address these concerns, we ﬁlter
Reddit comments in two ways. First, we train
a classiﬁer to identify conversational comments,
using 70,949 turns from the Empathetic dialogsdata (Rashkin et al., 2019) and 225,907 turns from
the Cornell movie dataset (Danescu-Niculescu-
Mizil and Lee, 2011) as positive examples of
conversational messages, and 296,854 turns from
a random sample of Reddit comments as non-
conversational messages. Full details are provided
in Appendix B. Second, we apply our conversa-
tional classiﬁer to comments marked by Reddit as
controversial in the Pushshift data (Baumgartner
et al., 2020); while the decision logic for which
comments are marked as controversial is propri-
etary to Reddit, controversial-labeled comments
typically receive high numbers of both upvotes and
downvotes by the community—but are not neces-
sarily offensive. These two ﬁlters were applied to
identify 145,210 total comments gathered from an
arbitrary month of data (Feb. 2018).
3.1 Annotation Phase 1
In the ﬁrst phase of annotation, four annotators indi-
vidually generated English-language messages they
found to differ in appropriateness by relationship.
Annotators were provided with a website interface
that would randomly sample conversational, con-
troversial Reddit comments as inspiration. Details
of the annotation instructions and interface are pro-
vided in Appendix A. The annotation process used
a small number of in-person annotators rather than
crowdsourcing to allow for task reﬁnement: During
the initial period of annotating, annotators met regu-
larly to discuss their appropriateness judgments and
disagreements. This discussion process was highly
beneﬁcial for reﬁning the process for disentangling
implausibility from inappropriateness. Once an-
notation was completed, annotators discussed and
adjudicated their ratings for all messages. Annota-
tors ultimately produced 401 messages and 5,029
total appropriateness ratings for those messages in
the context of different relationships.
3.2 Annotation Phase 2
Phase 2 uses an active learning approach to iden-
tify potentially relationship-sensitive messages to
annotate from a large unlabeled corpus. A T5
prompt-based classiﬁer was trained using Open-
Prompt (Ding et al., 2022) to identify whether a
given message would be appropriate to say to a per-10996
son in a speciﬁc relationship. Details of this clas-
siﬁer are provided in Appendix C. This classiﬁer
was run on all sampled data to identify instances
where at least 30% of relationships were marked
as appropriate or inappropriate; this ﬁltering biases
the data away from universally-appropriate or in-
appropriate messages, though annotators may still
decide otherwise.
Two annotators, one of which was not present
in the previous annotation process, completed two
rounds of norm-setting and pilot annotations to
discuss judgments. Then, annotators rated 30 mes-
sages each, marking each for plausibility and, if
plausible, appropriateness; they met to adjudicate
and then rated another 41 messages. This pro-
duced 2,159 appropriateness ratings across these
messages. Annotators had a Krippendorff’s αof
0.56 on plausibility and, for messages where both
rated as plausible, 0.46 on appropriateness. While
this agreement initially seems moderate, annota-
tors reviewed all disagreements, many of which
were due to different interpretations of the same
message, which inﬂuenced appropriate judgments
rather than disagreements in appropriateness itself.
Annotators then revised their own annotations in
light of consensus in message meaning, bringing
the plausibility agreement to 0.72 and appropri-
ateness to 0.92. We view these numbers as morereliable estimates of the annotation process, rec-
ognizing that some messages may have different
judgments due to annotators’ values and personal
experiences. We mark the 2,159 ratings in this data
asAdjudicated data for later evaluation.
Both annotators then independently annotated
different samples of the Reddit data in order to
maximize diversity in messages. Annotators were
instructed to skip annotating messages that they
viewed as less context-sensitive (e.g., offensive in
all relationship contexts) or where the message did
not appear conversational. Annotators provided
5,408 ratings on this second sample. We refer to
this non-adjudicated data as Phase 2 data.
3.3 Dataset Summary and Analysis
The two phases produced a total of 12,236 appro-
priateness judgments across 5299 messages. Of
these, 7,589 of the judgments were appropriate,
and 4647 were inappropriate. Table 2 shows ex-
amples of annotation judgments. In line with prior
cultural studies of appropriateness (Floyd and Mor-
man, 1997; Fetzer, 2015), three themes emerged
during training. First, annotators noted the percep-
tion of the role of teasing in deciding appropriate-
ness. Teasing messages are directed insults (mild
or otherwise) aimed at the other party; comments
such as “you are so dumb” are likely made in jest10997within close relationships such as best friends or
siblings but inappropriate in many others. Sec-
ond, messages’ appropriateness depended in part
on whether the relationship was perceived to be
supportive; for example, the message “At least you
called him by his correct name” could be one of
encouragement in the face of a mistake (e.g., if
said by a spouse) or a subtle insult that implies the
listener should have known more about the third
party. Third, differences in the power/status in the
relationship inﬂuenced appropriateness, where very
direct messages, e.g., “you made a mistake there.”
were often perceived to be inappropriate when said
to a person of higher status, a known violation of
politeness strategies (Brown and Levinson, 1987).
Ultimately, appropriateness was judged through a
combination of these aspects.
As an initial test of regularity in how the relation-
ship inﬂuence perceived appropriateness, we mea-
sured the probability that a message appropriate
for relationship ris also appropriate for rusing
all the annotations, shown in Figure 2 and grouped
by thematic categories. Clear block structure ex-
ists with some categories, e.g., O ,
indicating shared norms of appropriateness for rela-
tionships within the same category. In contrast, the
F andS categories contain relation-
ships with different power (e.g., parent) and social
distance (e.g., friend vs. stranger), leading to varied
judgments. Figure 2 also reveals the asymmetry
in which message themes are appropriate: While
much of what is said for R- relation-
ships is also appropriate in S orR ,
the reverse is not true.
4 Identifying Contextual
Inappropriateness
Given the high potential agreement of annotators,
we test whether models can similarly recognize
whether a given message is appropriate if said in
the context of a speciﬁc relationship.
Experimental Setup Two classes of models
were trained. Given the recent successes of prompt-
based models, we build models using the Open-
Prompt library (Ding et al., 2022) and, to sup-
port larger models, using the PEFT library (Liu
et al., 2022). The OpenPrompt library was used
to train t5-base andgpt2-med models using
the prompt “Is it appropriate for person1 to say
"quote" to person2, "yes" or "no"? [MASK] ” us-
ing the verbalization “yes’ or ”no” for the masked
token. Here, we ﬁll in person1 and person2 to refer
to the two parties in the relationship. Examples of
ﬁlled-in templates and other prompts are reported
in Appendix D, though performance varied only
slightly across different prompts. The PEFT library
was used to train the large andxlvariants of
theflan-t5 model (Chung et al., 2022). This
model has been pretrained for instruction follow-
ing; therefore, based on suggested guidelines from
Ziems et al. (2023), we use the following prompt:
“Rate whether it is inappropriate for this message
to be said in the following social setting?\n setting:
relationship description \n message: "quote"\n an-
swer (yes or no):” Due to the resources required for
training these larger models, no additional prompts
were rigorously evaluated outside of initial pilot
testing.
The second class of models uses masked lan-
guage model (MLM) ﬁne-tuning on the [CLS]
token from an MLM to predict appropriateness.
Here, we frame the instance using the same lan-
guage as the OpenPrompt-based models but ﬁll
in the MASK with “yes” (i.e., indicating that the
message is appropriate to say in the relationship).
The classiﬁcation model is then ﬁne-tuned to clas-
sify whether this hard-coded judgment is correct
or not. We test two recent MLMs, MiniLM (Wang
et al., 2020), a small distilled model, and DeBERTa-
v3 (He et al., 2021), a much larger model. These
two models reﬂect extremes among relatively small
MLMs and allow us to assess whether more social10998relationship knowledge might be embedded within
a larger parameter space.
Annotated data was split at the message level
70:10:20 into train, development, and test sets,
resulting in 9,107 train, 1,100 development, and
2,029 test instances. We frame the task similar
to offensive language detection and use Binary F1
as our metric where inappropriate is the positive
class. Model performance is reported as the aver-
age across ﬁve random runs. Additional training
details and per-seed performance are provided for
all systems in Appendix E.
Two baseline systems are included. The ﬁrst is
random labels with respect to the empirical distribu-
tion in the training data. The second uses Perspec-
tive API (Lees et al., 2022) to rate the toxicity of the
message, labeling it as toxic if the rating is above
0.7 on a scale of [0,1]; the same label is used for
all relationships. While this baseline is unlikely to
perform well, it serves as a reference to how much
explicit toxicity is in the dataset, as some (though
not all) of these messages are inappropriate to all
relationships.
Results Models accurately recognized how re-
lationships inﬂuence the acceptability of a mes-
sage, as seen in Table 3. Prompt-based models
were largely equivalent to MLM-based models,
though both approaches far exceeded the baselines.
The largest model, flan-t5-xl , ultimately per-
formed best, though even the MiniLM offered
promising performance, despite having several or-
ders of magnitude fewer parameters. In general,
models were more likely to label messages as in-
appropriate even when appropriate for a particular
setting (more false positives). This performance
may be more useful in settings where a model ﬂags
potentially inappropriate messages which are then
reviewed by a human (e.g., content moderation).
However, the performance for models as a whole
suggests there is substantial room for improvement
in how relationships as social context are integrated
into the model’s decisions.
Error Analysis Different relationships can have
very different norms in terms of what content is
acceptable, as highlighted in Figure 2. How did
model performance vary by relationship? Figure
3 shows the binary F1 score of the flan-t5-xl
model by relationship, relative to the percent of
training instances the model saw that were inap-
propriate; Appendix Table 11 shows full results
per relationship. Model performance was highly
correlated with the data bias for inappropriateness
(r=0.69; p<0.01). The model had trouble iden-
tifying inappropriate comments for relationships
where most messages are appropriate (e.g., friend,
sibling) in contrast to more content-constrained
relationships (boss, student, doctor). These low-
performance relationships frequently come with
complex social norms—e.g., the boundary between
appropriate teasing and inappropriate hurtful com-
ments for siblings (Keltner et al., 2001)—and al-
though such relationships have among the most
training data, we speculate that additional training
data is needed to model these norms, especially
given the topical diversity in these relationships’
conversations.
5 Generalizing to Unseen Relationships
Through their pretraining, LLMs have learned se-
mantic representations of relationships as tokens.
Our classiﬁcation experiments show that LLMs
can interpret these relationship-as-token represen-
tations to effectively judge whether a message is ap-
propriate. To what extent do these representations10999allow the model to generalize about new relation-
ships not seen in training? In particular, are models
able to generalize if a category of relationship, e.g.,
all family relations, was never seen? Here, we
conduct an ablation study where one of our folk
categories is held out during training.
Setup Theflan-t5-xl model is trained with
the same hyperparameters as the best-performing
system on the full training data. We use the same
data splits, holding out all training examples of
relationships in one category during training. We
report the Binary F1 from the test set on (1) rela-
tionships seen in training and (2) relationships in
the held-out category. Note that because training
set sizes may change substantially due to an im-
balance of which relationships were annotated and
because categories have related norms of accept-
ability, performance on seen-in-training is likely to
differ from the full data.
Results Ablated models varied substantially in
their abilities to generalize to the unseen relation-
ship types, as well as in their baseline perfor-
mance (Figure 4). First, when ablating the larger
categories of common relationships (e.g., F-,S ), the model performs well on seen-
relationships, dropping performance only slightly,
but is unable to accurately generalize to relation-
ships in the unseen category. These unseen cat-
egories contain relationships that span a diverse
range of norms with respect to power differences,
social distance, and solidarity. While other cat-
egories contain partially-analogous relationships
along these axes, e.g., parent-child and teacher-
student both share a power difference, the drop in
performance on held-out categories suggests the
model is not representing these social norms in
a way that allows easy transfer to predicting ap-
propriateness for unseen relationships with similar
norms. Second, relationships in three categories
improve in performance when unseen: O- ,R-B , and P .
All three categories feature relationships that are
more topically constrained around particular situ-
ations and settings. While the categories do con-
tain nuance, e.g., the appropriateness around the
power dynamics of boss-employee, the results sug-
gest that models may do well in zero-shot settings
where there is strong topic-relationship afﬁnity—
and messages outside of normal topics are inappro-
priate. Viewing these two trends together, we posit
that the semantic representations of relationships
inflan-t5-xl currently capture only minimal
kinds of social norms—particularly those relating
to topic—and these norms are not represented in a
way that lets the model easily generalize to reason-
ing about relationships not seen in training.
6 How Much of Conversation is Context
Sensitive in Appropriateness?
Our annotation and computational models have
shown that the relationship context matters in de-
termining appropriateness. However, it is unclear
how often conversations are sensitive to this con-
text. For example, the majority of conversation
may be appropriate to all relationships. Here, we
aim to estimate this context sensitivity by testing
the appropriateness of a message in counterfactual
settings using an existing dataset labeled with rela-
tionship types.
Experimental Setup To estimate context sensi-
tivity, we use our most accurate model to label
a large selection of dialog turns from the PRIDE
dataset (Tigunova et al., 2021). PRIDE consists of
64,844 dialog turns from movie scripts, each anno-
tated for the relationship between the speaker and
receiver, making it ideal as a high-plausibility con-
versational message said in relationships. However,
some turns of the dialog are explicitly grounded
in the setting of the movie, e.g., “How’s it going,
Pat?” which makes the turn too speciﬁc to that
particular setting to accurately estimate appropri-11000ateness. Therefore, we run SpaCy NER (Honnibal
and Montani, 2017) on the dialog and remove all
turns containing references to people, companies,
countries, and nationalities in order to keep the
dialog generic and maximally plausible in many
different relationship contexts. Further, we remove
turns with only a single token or over 100 tokens.
This ﬁltering leaves 47,801 messages for analysis.
PRIDE contains 18 unique relationships, 16 of
which were already included in our categories (cf.
Table 1); the two previously-unseen relationship
types, described as “religious relationships” and
“client/seller (commercial),” were also included
since our model can accommodate zero-shot pre-
diction.
To text for context sensitivity, we apply our
flan-t5-xl model and measure the appropri-
ateness of the actual relationship context and then
the counterfactual cases as if the message had been
said in an alternative relationship context seen in
their data. This setup allows us to assess whether
if a message was appropriate in its intended rela-
tionship context, would it still be appropriate in
another.
Results Considering only appropriate messages
and excluding the unusual enemy relationship from
consideration, we ﬁnd that roughly 19% of the
appropriate-as-said messages in the data would be
inappropriate if said in the context of a different
relationship. Figure 5 shows the probability that a
message acceptable in some other relationship con-
text would also be acceptable in the given context;
the striking decrease in the likelihood of accept-
ability follows the increasingly constrained social
norms around a relationship. For example, while
friends and loved ones have broad latitude to dis-
cuss sensitive topics (Hays, 1984), R-
relationships and those with larger power differ-
ences are more constrained in what is considered
acceptable conversation. While the movie dialog in
the PRIDE dataset likely differs from a natural dia-
log, these results point to relationships as important
contexts in natural language understanding.
More generally, we suggest a need for socially-
aware models to identify offensive language. While
substantial effort has been put into identifying ex-
plicit toxic or abusive language (Vidgen et al.,
2021), few models, if any, incorporate the context
in which the message is said. These models typi-
cally rely on previous conversation turns (Zhang
et al., 2018) or modeling community-level social
norms (Chandrasekharan et al., 2018) to under-
stand how the context may shift whether the mes-
sage is perceived as appropriate. Our result sug-
gests that the social context—and particularly so-
cial relationships—are highly inﬂuential in mea-
suring appropriateness. Indeed, together with the
result showing the (expected) low performance of
the Perspective API toxicity detector, these results
suggest NLP models deployed in social settings
are likely missing identifying many offensive mes-
sages due to their lack of explicitly modeling of
social relations. As NLP tools make their way into
the workplace setting, which frequently features
a mix of O ,S , and R- ties, explicitly modeling context will likely
be necessary.
7 Identifying Subtle Offensiveness using
Contextual Appropriateness
Prior NLP studies of subtly inappropriate language
often omit the social context in which a statement
is said (Breitfeller et al., 2019; Pérez-Almendros
et al., 2022), yet it is often this context that makes
a statement inappropriate. For example, a teacher
asking a student “Do you need help writing that?”
is appropriate, whereas a student asking a teacher
the same question may seem rude. We hypothesize
that modeling the relative appropriateness of a mes-
sage across relationships can help identify types
of subtly offensive language. We test this hypothe-
sis using datasets for two phenomena: condescen-11001sion (Wang and Potts, 2019) and (im)politeness
(Danescu-Niculescu-Mizil et al., 2013).
Experimental Setup Theflan-t5-xl model
is used to predict the appropriateness of each mes-
sage in the training data in the TalkDown dataset
for condescension (Wang and Potts, 2019), and the
Stanford Politeness Corpus (Danescu-Niculescu-
Mizil et al., 2013). Each message is represented
as a binary vector of inappropriateness judgments
for each relationship. TalkDown is based on Red-
dit comments, which our model has seen, whereas
the politeness data is drawn from Wikipedia and
StackExchange conversations. We adopt the same
train and test splits as in the respective papers and
ﬁt a logistic regression classiﬁer for each dataset
to predict whether a message is condescending or
impolite, respectively, from the per-relationship ap-
propriateness vector. The logistic regression model
uses Scikit-learn (Pedregosa et al., 2011); for each
task, we adopt the evaluation metric used in the re-
spective paper. Appendix F has additional details.
Results The relationship appropriateness scores
were meaningfully predictive of subtle offensive-
ness, as seen in Table 4 for condescension and Ta-
ble 5 for impoliteness. In both settings, the appro-
priateness features provide a statistically signiﬁcant
improvement over random performance, indicating
that adding relationships as context can help iden-
tify subtly offensive messages. Further, despite
the classiﬁer’s relative simplicity, the appropriate-
ness features alone outperform the bert-large
classiﬁer used in Wang and Potts (2019) in the bal-
anced setting, underscoring how explicitly model-
ing relationships can still be competitive with LLM-
based approaches. Performance at recognizing
(im)politeness from relationship-appropriateness
was lower than the hand-crafted or purely bag-
of-words approaches. Yet, this gap is expected
given that dataset’s design; Danescu-Niculescu-
Mizil et al. (2013) focus on identifying discourse
moves, and the politeness classiﬁcation task comes
from messages at the top and bottom quartiles of
their politeness rating. Messages in the bottom
quartile may be less polite, rather than impolite,
and therefore appropriate in more context, thereby
making relationship-appropriate judgments less dis-
criminating as features.
8 Conclusion
“Looking beautiful today!”, “You look like you
need a hand with that”, and “When can I see you
again?”—in the right contexts, such messages can
bring a smile, but in other contexts, such messages
are likely to be viewed as inappropriate. In this
paper, we aim to detect such inappropriate mes-
sages by explicitly modeling the relationship be-
tween people as a social context. Through a large-
scale annotation, we introduce a new dataset of
over 12,236 ratings of appropriateness for 49 re-
lationships. In experiments, we show that mod-
els can accurately identify inappropriateness by
making use of pre-trained representations of rela-
tionships. Further, through counterfactual analy-
sis, we ﬁnd a substantial minority of content is
contextually-sensitive: roughly 19% of the appro-
priate messages we analyzed would not be appro-
priate if said in some other relationship context.
Our work points to a growing need to consider
meaning within the social context, particularly for
identifying subtly offensive messages. All data
and code are released at .
Acknowledgments
The authors thank Aparna Anathasubramaniam,
Minje Choi, and Jiaxin Pei for their timely and
valuable feedback on the paper. This work was sup-11002ported by the National Science Foundation under
Grant Nos. IIS-1850221, IIS-2007251 and IIS-
2143529.
9 Limitations
This paper has three main limitations worth not-
ing. First and foremost, while our paper aims to
model the social context in which a message is
said, the current context is limited to only the par-
ties’ relationship. In practice, the social context
encompasses a wide variety of other factors, such
as the sociodemographics of the parties, the cul-
ture and setting of the conversation, and the his-
tory of the parties. Even relationships themselves
are often much more nuanced and the appropri-
ateness may vary widely based on setting, e.g.,
statements said between spouses may vary in ap-
propriateness when made in public versus private
settings. These contextual factors are likely neces-
sary for a full account of the effect of social con-
text on how messages should be perceived. Our
work provides an initial step in this direction by
making the relationship explicit, but more work
remains to be done. Future work may examine
how to incorporate these aspects, such as by di-
rectly inputting the situation’s social network as
context using graph embedding techniques (Kulka-
rni et al., 2021), where the network is labeled with
relationships (Choi et al., 2021), or by modeling
relationships particular types of settings such as
in-person, phone, texting, or other online commu-
nication, which each have different norms.
Second, our data includes annotations on a ﬁnite
set of relationships, while many more unique rela-
tionships are possible in practice, e.g., customer or
pastor. Our initial set was developed based on dis-
cussions among annotators and aimed at high but
not complete coverage due to the increasing com-
plexity of the annotation task as more relationships
were added. Our results in Section 5 suggest that
our best model could be able to generalize to new
types of relationships in some settings and zero-
shot results on two new relationship types not seen
in training (a fellow church member and a com-
mercial relationship) match expectations of context
sensitivity, (cf. Figure 5 . However, performance is
likely limited for less-common relationships with-
out additional training data to describe the norms
of appropriateness in this context; and, based on
the error analysis in Section 4, models are currently
unlikely to generalize to unseen relationships thathave complex sensitivity norms. In addition, new
settings such as online spaces may require addi-
tional deﬁnitions of relationships as individuals
interact with each other anonymously.
Third, our judgments of appropriateness were
drawn from ﬁve annotators total, each of whom had
different views of appropriateness based on their
values and life experience. While our analysis of
agreement with the Adjudicated data (Section 3.2)
suggests that when annotators can reach a consen-
sus on a message’s meaning, they are highly likely
to agree on appropriateness, we nonetheless view
that our annotations are likely to primarily reﬂect
the values of the annotators and may not generalize
to other social or cultural contexts where the norms
of relationships differ. Future work is needed to
explore how these norms differ through additional
annotation, and we hope that our dataset will pro-
vide a reference for comparison to these judgments.
For example, future work may make use of annota-
tion schemes that explicitly model disagreements
(Fornaciari et al., 2021) or personalized judgments
(Plepi et al., 2022); such approaches may be able
to better represent common factors inﬂuencing ap-
propriateness judgments.
10 Ethical Considerations
We note three points on ethics. First, we recognize
that appropriateness is a value judgment, and there-
fore our data is limited here by the viewpoints of
the annotators. Multiple works on offensive lan-
guage have shown that the values and identities of
annotators can bias the judgments and potentially
further marginalize communities of practice whose
views and norms are not present (Sap et al., 2019;
Garg et al., 2022). We have attempted to mitigate
this risk by adding diversity to our annotator pool
with respect to gender, age, and culture, yet our lim-
ited pool size necessitates that not all viewpoints
will be present. Given that we show relationships
do matter in judging appropriateness, we hope that
future work will add diversity through new addi-
tions and data to study relationships. We will also
release demographic information on annotators as
a part of our dataset to help make potential biases
more explicit and more easily addressed.
The annotators themselves were authors of the
study and were compensated as a part of their nor-
mal work with a living wage. Due to the nature of
our ﬁltering, the vast majority of our content was
not explicitly toxic. Nonetheless, some comments11003did contain objectionable messages, and annotators
were provided guidance on how to seek self-care if
the messages created distress.
With any new tool to identify offensive or abu-
sive language comes a dual use by an adversarial
actor to exploit that tool to ﬁnd new ways to harass
or abuse others while still “abiding by the rules.”
Our work has shown that relationships are effective
context (and features) for identifying previously-
unrecognized inappropriateness. This new capabil-
ity has the beneﬁt of potentially recognizing more
inappropriate messages before they reach their des-
tination. However, some adversaries could still use
our data and model to screen their own messages
to ﬁnd those that still are classiﬁed as appropriate
(while being inappropriate in practice) to evade de-
tection. Nevertheless, given the new ability to iden-
tify context-sensitive offensive messages—which
we show can represent a substantial percentage of
conversation (Section 6)—we view the beneﬁts as
outweighing the risk.
References110041100511006
A Annotation Details
This section describes the details of the annotation
process. Annotators were the authors of this paper
and were compensated for their work as a part of
their normal duties; no additional payments were
provided.
The annotation interface was designed using
P (Pei et al., 2022), shown in Figure 6, and
was accessed through a browser, which allowed an-
notators to start and stop their labeling at any time.
Annotators were allowed to revise their annotations
at any time.
During annotation, annotators were presented
with the message to be annotated and collapsible
instructions for annotation. Figure 7 shows the full
written instructions shown to annotators. The in-
structions were reﬁned through an iterative process
throughout the project, and annotators regularly
communicated about ambiguity. The instructions
were designed to let the annotators know the intent
of the study and the downstream tasks that data
would be used for.
B Conversation Classiﬁer Details
The conversational classiﬁer was used during the
initial data sampling phase to identify comments on
Reddit that could plausibly have been said in a con-
versation. This classiﬁer is intended only as a ﬁlter
to improve data quality by reducing the number of
non-conversation comments (e.g., those with Red-
dit formatting, long monologues, and comments
written in a non-conversational register). We have
two datasets of known conversations: 70,949 turns
from the Empathetic dialogs data (Rashkin et al.,
2019) and 225,907 turns from the Cornell movie
dataset (Danescu-Niculescu-Mizil and Lee, 2011)
as positive examples of conversational messages.
We then sample an equivalent number of 296,854
turns from a random sample of Reddit comments
as non-conversational messages. While some of
these Reddit messages are likely conversational,
this classiﬁcation scheme is only a heuristic aimed
at helping ﬁlter data. A held-out set of 74,212 in-
stances was used for evaluation, balanced between
conversational and not.
A MiniLM classiﬁer (Wang et al., 2020) was
trained using Huggingface Transformers (Wolfet al., 2019) for ﬁve epochs, keeping the model
with the lowest training loss at any epoch; Epoch
5 was selected. The model attained an F1 of 0.94
for the held-out data indicating it was accurate at
distinguishing the conversational turns from the
random sample of Reddit comments. We apply
this classiﬁer to 1,917,346 comments from Reddit
during the month of February 2018 and identify
145,210 whose probability of being a conversation
is>0.5. We retain these comments as potential
comments to annotate in Phase 2 (Section 3.2).
B.1 Computational resources
All of our experiments were conducted on an
Ubuntu 16.04.7 LTS machine installed with
NVIDIA RTX A5000 and RTX A6000 GPUs hav-
ing CUDA 11.3. The Python packages used in our
experiments include Pytorch 1.17.0, Transform-
ers 4.25.1, PEFT 0.3.0, OpenPrompt 1.0.1, pandas
1.1.4, spacy 3.3.2, and Sci-kit learn 1.2.0.
B.2 Speciﬁcation of LLMs
The LLMs used in this paper were downloaded
from huggingface.co. The model and their parame-
ter sizes are listed in Table 6.
B.3 Classiﬁers from Sklearn
For the classiﬁcation of politeness and condescen-
sion tasks, we used logistic regression from sklearn
with the solver as ‘lbfgs’ and max_iter set to 400.
C Phase 1 Classiﬁer
The phase-1 LLM classiﬁer was trained using the
pilot training data and the OpenPrompt frame-
work. In this framework, we use a batch size of
4, the maximum sequence length was set to 256,
decoder_max_length=3, truncate_method="head",
and teacher_forcing and predict_eos_token were
set to default values. The prompt used for the
model was framed as a yes/no question - "is it
appropriate for PERSON1 to say QUOTE to PER-
SON2?".11007
D Additional Prompt-based Model
Details
We train gpt2-base andt5-base using the
OpenPrompt framework. In this framework, we
use a batch size of 16, the maximum sequence
length was set to 256, decoder_max_length=3, trun-
cate_method="head", and teacher_forcing and pre-
dict_eos_token were set to default values. The
model was trained using early stopping and the
AdamW optimizer with a learning rate set to 1e-4.
The different prompts that we used before ﬁnaliz-
ing the prompt as "Is it appropriate for PERSON1
to say "QUOTE" to PERSON2?, "yes" or "no"? are
reported in table 10.
We train the flan-t5-large and
flan-t5-xl models using the PEFT li-
brary. Models were trained with a batch size
of 96 and 32, respectively. Both models used a
maximum sequence length of 192 and learning rate
of 1e-2 with AdamW, using all other default library
parameters. The model was trained for 20 epochs,
keeping the best-performing model by binary F1
on the development dataset for each seed.
E Additional Results
E.1 Development Set Performance
The performance of the different models on the
development dataset is reported in Table 8 and per-
formance on the test set with standard errors is11008
reported in Table 9.
E.2 Analysis of Relationship Predictions
The data annotation process showed clear associ-
ations between pairs of relationships in terms of
how often a message would be appropriate (Fig-
ure 2). However, the training data for that ﬁgure
only includes annotations on relationships annota-
tors selected. What structure or regularity might
we see from analyzing similarities between all our
relationships through model predictions?
As a qualitative experiment, we use the
flan-t5-xl model to label the subset of the
PRIDE dataset (Section 6) for the appropriateness
of all 49 relationships in our training data. This
produces a binary matrix of 49 ×47,801. We use
PCA to capture regularity and then project relation-
ships onto a 2D visualization using t-SNE (van der
Maaten and Hinton, 2008), which is aimed at pre-
serving local similarity in the spatial arrangement.
If model predictions are capturing shared norms,
we view t-SNE as potentially more useful than a
PCA projection, as we want to visualize which re-lationships with similar judgments as being nearby
(what t-SNE does) rather than optimizing the visu-
alization to the global structure of distances (what
PCA does). The t-SNE projection was designed
using guidance from Wattenberg et al. (2016); a
perplexity of 40 was used.
The resulting visualization, shown in Figure 8,
captures expected regularity. While the projection
is only a visual tool, and aspects such as distance
are not meaningful in t-SNE visualizations, the
grouping and neighbors suggest the model is sen-
sitive to power/status and social distance in how it
decides appropriateness based on the relationship.
E.3 Per Relationship Results
Table 11 shows the peformance of the
flan-t5-xl model on the test set, broken
down by relationship11009
F Additional Experimental Setup Details
for Identifying Subtly Offensive
Messages
For experiments with both the TalkDown corpus
(Wang and Potts, 2019) and Stanford Politeness
Corpus (Danescu-Niculescu-Mizil et al., 2013), the
trained flan-t5-xl model was used in a zero-
shot setting with no additional training. For the
Politeness corpus, politeness ratings are made at
the utterance level, outside of any dialog context.
As a result, the existing prompt was used (§4, Ex-
perimental Setup) to assess relationship-speciﬁc
appropriateness.
Two modiﬁcations were necessary for the Talk-
Down corpus. First, the TalkDown corpus’s data is
rated at the turn level, with condescension judg-
ments based on the interpretation of a reply to
a speciﬁc piece of quoted text. Wang and Potts
(2019) note that incorporating both the quote and
reply into the input resulted in better performance.
Therefore, we modify our initial prompt slightly
as follows: “Rate whether it is inappropriate for
message A to be said in response to the message
B in the speciﬁed social setting: \n A: quoted text
\n B: reply text \n setting: relationship description
\n answer (yes or no):”. Since the flan-t5-xl
model was trained speciﬁcally for instruction fol-
lowing (Chung et al., 2022), we expected the model
to generate similar outputs as our original prompt.
Second, some of the quoted and reply text in Talk-
Down can be quite long (hundreds of words). Since
the adapted prompt contains both quote and reply,
we use an ﬂexible truncation process to maximize
the content that can still ﬁt within the maximum in-
put token sequence length (196). First, quoted textover 50 tokens is truncated to the ﬁrst 50, using the
flan-t5-xl tokenizer to segment words. Then,
if the full input (with prompt instructions) still ex-
ceeds the maximum input length, we truncate both
the quoted text and reply evenly, still keeping at
least the ﬁrst then 10 tokens of each.1101011011ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
9
/squareA2. Did you discuss any potential risks of your work?
10
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
0,1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3
/squareB1. Did you cite the creators of artifacts you used?
3,4,5,6,7
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
3
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
3,4
C/squareDid you run computational experiments?
4,5,6,7
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
B, C11012/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4, B, C
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
No response.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
A
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
A
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
A
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
The authors annotated the data so demographics were held out during submission to preserve
double-blind status.11013