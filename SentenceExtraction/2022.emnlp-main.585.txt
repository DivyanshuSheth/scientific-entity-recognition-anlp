
Yaxin Zhu andHamed Zamani
Center for Intelligent Information Retrieval
University of Massachusetts Amherst
{yaxinzhu, zamani}@cs.umass.edu
Abstract
Concept prerequisite learning (CPL) plays a
key role in developing technologies that assist
people to learn a new complex topic or con-
cept. Previous work commonly assumes that
all concepts are given at training time and solely
focuses on predicting the unseen prerequisite
relationships between them. However, many
real-world scenarios deal with concepts that
are left undiscovered at training time, which
is relatively unexplored. This paper studies
this problem and proposes a novel alternating
knowledge distillation approach to take advan-
tage of both content- and graph-based models
for this task. Extensive experiments on three
public benchmarks demonstrate up to 10% im-
provements in terms of F1 score.
1 Introduction
As the amount of online educational data rapidly
grows, it is more important than ever to develop
information access systems that assist users to learn
new complex topics or concepts (Gwizdka et al.,
2016; Collins-Thompson et al., 2017; Eickhoff
et al., 2017; Urgo and Arguello, 2022). A fun-
damental step towards developing these systems
isconcept prerequisite learning (CPL) —the task
of building a concept graph by structuring open
knowledge in prerequisite relations. A prerequisite
is a directed relation between two concepts, e.g.,
Heap Tree is a prerequisite of Heap Sort .
CPL was first introduced by Talukdar and Cohen
(2012) with the aim of formulating probabilistic
planning problems for machine learning solvers. A
number of CPL models use various kinds external
resources including Wikipedia links (Liang et al.,
2015), textbook structures (Wang et al., 2016), and
course dependencies (Liang et al., 2017; Liu et al.,
2016) in two ways: content-based (Pan et al., 2017;
Roy et al., 2019; Gasparetti, 2022) and graph-based
(Liang et al., 2015). There also exists research on
active learning (Liang et al., 2018a,b), unsuper-vised learning (Li et al., 2020), and domain adap-
tation (Li et al., 2021) to meet data insufficiency
challenges in CPL problems. However, most exist-
ing approaches assume the system to reconstruct
concept prerequisite paths with vague knowledge
(i.e. incomplete relations) of each concept, or to
transfer graph structure information to a new do-
main. In practical scenarios, knowledge is updated
with both new concepts and relations introduced.
To jump out of the offline setting of graph comple-
tion with given concepts, we define a new task -
CPL for unseen concepts, i.e. predicting prerequi-
site relationships for concepts that never appear in
the training set.
Most existing graph-based CPL approaches can-
not be simply used for unseen concepts, because
the randomly initialized concept representations do
not get updated for unseen concepts. A simple solu-
tion would be initializing the concept embeddings
based on content-based models (Li et al., 2019; Jia
et al., 2021; Zhang et al., 2022; Sun et al., 2022).
To better take advantage of content information,
we propose a novel CPL model that consists of
two components: one that solely focuses on textual
content associated with each concept and another
one that focuses on the concept graph structure. To
train our model, we propose an iterative knowledge
distillation approach by alternating between these
two components as “teacher” and “student”. Our
main contributions include:
1.Exploring a new task to predict prerequisite re-
lationships for unseen concepts.
2.Introducing a simple yet effective retrieval-
augmented content-based approach for CPL.
3.Proposing an alternating knowledge distillation
procedure that benefits both content- and graph-
based models for CPL.
4.Advancing state-of-the-art on three public
benchmarks. Extensive experiments shed light
on the empirical contributions of each proposed8542
component. Our code and model parameters are
released for reproducibility purposes.
2 Methodology
This section introduces a CPL model that uses
two complementary components. The first com-
ponent models prerequisite relations conditioned
on textual content retrieved for each concept (i.e.,
a retrieval-augmented model), and the second com-
ponent casts the problem as a link prediction task
and models prerequisite relations conditioned on
the graph structure. The proposed method uses a
knowledge distillation approach that alters between
these two components as the teacher and student
models, iteratively. An overview of the proposed
solution is presented in Figure 1.
Notation and Problem Statement : Let G= (V, E)
be a directed concept graph whose vertices are asso-
ciated with concepts and edges represent prerequi-
site relations, Vbe a set of unseen concepts to dig
out. The training set for concept prerequisite learn-
ing is equal to D ={(c, c, r) :c, c∈
V, r}, where r= 1ifc→c∈Eandr= 0
otherwise. The test set is D={(c, c, r) :
c∈V∨c∈V, r}.
2.1 Alternating Knowledge Distillation for
Concept Prerequisite Learning
There are two formulations of the concept prereq-
uisite learning problem, as follows:
A content-based formulation for CPL : In this for-
mulation, the aim is to develop a model that pre-
dicts prerequisite relations based on textual infor-
mation associated with the concepts, as follows:
arg min/summationdisplayL(p(c→c|ϕ, θ), r),where Landθdenote the loss function and the
content-based model parameters, respectively. ϕ(·)
is a function that takes a concept and provide a
textual description of the concept.
A graph-based formulation for CPL : An alterna-
tive formulation of the CPL problem is to predict
prerequisite relationships based on the prerequisite
graph structure, as follows:
arg min/summationdisplayL(p(c→c|G, θ), r),
where Landθdenote the loss function and the
graph-based model parameters, respectively.
Alternating knowledge distillation for training :
θandθcan be trained independently or jointly
on the training set D. We introduce a more
effective alternative optimization called alternating
knowledge distillation (AKD), in which the roles
of teacher and student models alternate between θ
andθrepeatedly. Our motivation is to improve
generalization in both of these models that are
complementary. Therefore, we first train our
content-based CPL model θ(see Section 2.2)
using the ground-truth training data (i.e., D).
Then, we consider θas the teacher model and
produce a pseudo-labeled training set ˆDbased on
p(c→c|ϕ, θ)as follows: For every concept
c, we consider kpositive pseudo labels using
topk({p(c→c|ϕ, θ) :∀c̸=c}). Note
that we exclude the concepts with less than 0.5
prerequisite probability, if any. We also take k
negative instances. These negative instances can
be selected randomly or from the ones with the
lowest probability. We then train the student model
θ(see Section 2.3) on D∪ˆD, useθas the
teacher, produce the pseudo-labeled training set
and train the student model θ. We repeat this
teacher-student alternation process for Nsteps.
Empirically Nis set to 4.
Since θandθare complementary , we in-
terpolate their scores linearly to acquire the final
probability:
αp(c→c|ϕ, θ) + (1 −α)p(c→c|G, θ),
where α∈[0,1]is a hyper-parameter. In the fol-
lowing two subsections, we describe how we model
θandθ, respectively.
2.2 Retrieval-Augmented Concept
Prerequisite Learning
A simple approach for modeling p(c→c|ϕ, θ)
is to use pre-trained language models (e.g.,8543
Dataset #concepts #p+ #p-
University Course 407 1005 996
LectureBank 205 904 967
Mooc ML 244 1737 4975
BERT (Devlin et al., 2018)) to represent the con-
cept names. In our initial experiments, we ob-
served that concept names are not sufficient for
prerequisite prediction and more descriptive con-
tent should be produced by ϕ. Therefore, with
the aim of taking advantage of a massive unstruc-
tured corpus from textual world knowledge, we
augment the training data with passages retrieved
from Wikipedia. To be concise, each concept
name is regarded as a query in order to retrieve
100-token passages (similar to the Wikipedia DPR
collection (Karpukhin et al., 2020)) using BM25.
We use the first ranked passage for augmenta-
tion and compute p(c→c|ϕ, θ)by feeding
“[CLS] np[SEP] np[SEP] ” to BERT and us-
ing a fully-connected layer and sigmoid on top of
the [CLS] representation. Note that, nandpare
the concept name and the first retrieved passage for
concept c, respectively. For the loss function, we
use binary cross entropy.
2.3 Graph-based Concept Prerequisite
Learning
For modeling θ, we aim at predicting missing
links in a concept graph. Various approaches based
on matrix factorization, geometry, and graph neural
networks have been developed for the link predic-
tion problem. In this work, we use neural collab-
orative filtering (NCF) (He et al., 2017) to obtain
node representations and corresponding link exis-
tence likelihood. NCF is efficient, less prone to
overfit, and can be used for directed graphs. Ithas demonstrated successful results in a number of
recommendation problems. We use NCF to learn
a representation for every concept. The represen-
tations are initialized randomly and we train the
model using a binary cross entropy loss function.
3 Experiments
3.1 Data
We evaluate the effectiveness of our approach on
the following three manually annotated bench-
marks.
University Course (Liang et al., 2017): This
dataset includes concepts from computer science
course descriptions provided by 11 universities in
the United States. The concepts were extracted
using Wikipedia Miner (Milne and Witten, 2013).
LectureBank (Li et al., 2019): This dataset was
constructed by collecting online lecture files from
60 courses covering NLP and related topics.
MOOC ML (Pan et al., 2017): This dataset
contains concept prerequisite relations extracted
from video subtitles of Coursera’s machine learn-
ing courses using the approach presented by
Parameswaran et al. (2010).
The statistics of these three benchmarks are pre-
sented in Table 2.
3.2 Experimental Setup
To evaluate the ability of predicting prerequisites
of undiscovered concepts for our method, we ran-
domly split the concept set of each dataset with a
proportion of 9:1 as VandV, then reconstruct the
training and test set by dropping any pair with an
unseen concept into test set. Thus, the represen-
tation of implicit concepts will never be updated
during training. The experiments are repeated for
three times with different random splits and the
average is reported. We use Precision, Recall, and
F1 Score (macro averaged) as evaluation metrics.
We set the NCF’s concept embedding dimension-
ality to 32, and the learning rates for θandθto8544
5e-5 and 1e-3, respectively. We use BERT-base in
all experiments. For the AKD process, kis propor-
tioned to the size of positive training instances for
each concept (i.e., 10%). We also set k=k. The
hyper-parameter αwas selected using grid search.
3.3 Baselines
We use the following competitive baselines:
VGAE (Li et al., 2019) uses variational graph au-
toencoder to encode edges in the training set with
a 2-layer Graph Convolutional Network (GCN),
then adopts inner product to reconstruct the graph.
CPRL (Jia et al., 2021) is the most recent CPL
approach that produces state-of-the-art results by
creating a heterogeneous graph for representing
concepts in addition to learning objects. It uses a
Relational Graph Convolutional Network (R-GCN)
to encode nodes and a Siamese network to iden-
tify prerequisites. Note that CPRL uses an external
resource. To provide a fairer comparison, we imple-
mented a simplified version of CPRL that excludes
the learning objects. We call this method CPRL --.
BERT (Devlin et al., 2018) takes a pair of concept
names and is fine-tuned to classify prerequisite re-
lations using the binary cross entropy loss. NCF
with BERT embs is implemented to compare dif-
ferent combination methods of content and graph
based models. We follow the strategies in (Li et al.,
2019; Jia et al., 2021) that initialize NCF node rep-
resentations with fine-tuned BERT embeddings.
3.4 Results
Comparison with the Baselines: According to Ta-
ble 1, graph-based baselines perform poorly whendealing with unseen concepts. Unsurprisingly, car-
rying information from the pre-training step helps
the BERT model produce the best results on both
University Course and MOOC ML datasets. Implic-
itly using BERT representations is helpful, but the
prediction ability of graph-based model is limited.
Our method outperforms the baselines on all three
datasets in terms of all metrics. The improvements
come from the augmentation using passages re-
trieved from Wikipedia, the alternating knowledge
distillation approach, and the explicit combination
of complementary models.
Ablation Study: In our ablation study, we answer
the following empirical research questions:
Q1:Does retrieval augmentation improve the gen-
eralizability of the content-based model? The F1
scores for the proposed retrieval-augmented BERT
on University Course, LectureBank and MOOC
ML are 0.796, 0.747, 0.800 respectively. Com-
paring them to the BERT’s performance reported
in Table 1 demonstrates the generalizability of re-
trieval augmentation for this task.
Q2:How is the effect of different components in
our AKD approach? In Table 3 we eliminate each
of components and demonstrate substantial drop
in nearly all cases. The large performance drops
by removing the retrieval-augmented BERT model
is due to its role of capturing content information.
This experiment demonstrates that all the compo-
nents used in developing our approach contributes
to the final performance.
Q3:How does distilled data contribute to NCF
training? In Table 4, ˆDacts as a better training
set than D, indicating that even weakly anno-8545
tated unseen concept pairs can play an important
role in guiding graph based models. A combina-
tion of ground truth D andˆDleads to strong
improvement.
Learning Curve: We plot the learning curves of
our model for all three datasets in Figure 2. The
model’s effectiveness is substantially improved by
increasing the training data size. Given the slope of
the learning curves, the proposed model is likely to
achieve significantly higher F1 scores by increasing
the training data size. This is an encouraging obser-
vation, especially given that the developed model
is already very effective and obtains F1 scores of
higher than 0.86 on all datasets.
Case Study: Table 5 demonstrates a few exam-
ple concept pairs from the LectureBank dataset.
The first two examples show that the graph-based
model could not accurately predict the prerequisiterelations, while the content-based model produces
probabilities higher than 0.5. On the other hand, the
last example shows that the graph-based model can
sometimes correct the errors made by the content-
based model. Of course, the interpolation of these
two models can sometimes introduce noise (e.g.,
Example 3). In our ablation study (Table 3), we
show that this interpolation leads to overall im-
provement.
Results for Seen Concepts: Even though this pa-
per focuses on unseen concepts, we also compare
our methods against the baselines for predicting
prerequisite relations for seen concepts. Following
previous work (Li et al., 2019; Jia et al., 2021), we
split the concept pairs in each dataset into training
and test sets with a proportion of 9:1 for Lecture-
Bank and 7:3 for others. Results in Table 6 show
that our method outperforms baselines on all three
datasets in terms of all metrics, indicating that our
approach is equipped with the ability to deal with
seen concepts.
4 Conclusions and Future Work
This paper explored the challenge of predicting pre-
requisites for unseen concepts in CPL. It proposed
an alternating knowledge distillation approach that
enables us to train more effective content-based
and graph-based models, as well demonstrated that
content-based CPL models can benefit from re-
trieval augmentation. In the future, we intent to
extend the proposed solution to an online setting,
where concept prerequisites can be extracted for
every learning-oriented query in a search engine.8546Limitations
One of the limitations of this work is that this work
overlook the concept detection or extraction in the
model design. Even though previous work also
made similar assumptions (Liang et al., 2017; Li
et al., 2019; Pan et al., 2017), we believe that this
is an important aspect that should be considered
in the future. Extracting concepts from unstruc-
tured data accurately can be challenging. Another
limitation is related to the number of concepts in
the datasets. In some real-world scenarios, the
number of concepts would be significantly higher
than those represented by the existing benchmarks.
Increasing the number of concepts is likely to neg-
atively impact the model’s effectiveness or raise
efficiency concerns.
Mistakes made by the CPL models, including
ours, if they are used in learning-oriented search
engines, are likely to negatively impact the learn-
ing outcome. For instance, missing a prerequisite
relation during a learning session may lead to some
misunderstanding about the concepts being learned.
Therefore, we suggest raising awareness of such
mistakes to the users so they can make wise deci-
sions while learning online.
Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part
by NSF grant #2106282. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect those of the sponsor.
References85478548