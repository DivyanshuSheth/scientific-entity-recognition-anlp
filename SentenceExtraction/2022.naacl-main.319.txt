
James Lee-Thorp and Joshua Ainslie and Ilya Eckstein and Santiago Ontañón
Google Research
{jamesleethorp, jainslie, ilyaeck, santiontanon}@google.com
Abstract
We show that Transformer encoder architec-
tures can be sped up, with limited accuracy
costs, by replacing the self-attention sublayers
with simple linear transformations that “mix”
input tokens. Most surprisingly, we ﬁnd that
replacing the self-attention sublayer in a Trans-
former encoder with a standard, unparameter-
ized Fourier Transform achieves 92-97% of
the accuracy of BERT counterparts on the
GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard
512 input lengths. At longer input lengths, our
FNet model is signiﬁcantly faster: when com-
pared to the “efﬁcient Transformers” on the
Long Range Arena benchmark, FNet matches
the accuracy of the most accurate models,
while outpacing the fastest models across all
sequence lengths on GPUs (and across rela-
tively shorter lengths on TPUs). Finally, FNet
has a light memory footprint and is particularly
efﬁcient at smaller model sizes; for a ﬁxed
speed and accuracy budget, small FNet mod-
els outperform Transformer counterparts.
1 Introduction
The Transformer architecture (Vaswani et al., 2017)
has achieved rapid and widespread dominance in
NLP. At its heart is a attention mechanism – an
inductive bias that connects each token in the input
through a relevance weighted basis of every other
token. Many papers have prodded and probed the
Transformer, and in particular the attention sublay-
ers, in an effort to better understand the architec-
ture; see, for example, Tenney et al. (2019); Vig
and Belinkov (2019); Clark et al. (2019); V oita et al.
(2019). Although potentially limited in their ef-
fectiveness (Hewitt and Liang, 2019), these probes
generally back the intuition that, by allowing higher
order units to form out of compositions of the input,Transformer models can ﬂexibly capture diverse
syntactic and semantic relationships.
In this work, we investigate whether simpler to-
ken mixing mechanisms can wholly replace the
relatively complex self-attention layers in Trans-
former encoder architectures. We ﬁrst replace the
attention sublayer with two parameterized matrix
multiplications – one mixing the sequence dimen-
sion and one mixing the hidden dimension. See-
ing promising results in this simple linear mixing
scheme, we further investigate the efﬁcacy of faster,
structured linear transformations. Surprisingly, we
ﬁnd that the Fourier Transform, despite having no
parameters at all, achieves nearly the same perfor-
mance as dense linear mixing and scales very efﬁ-
ciently to long inputs, especially on GPUs (owing
to theO(NlogN)Fast Fourier Transform (FFT)
algorithm). We call the resulting model FNet.
While Fourier Transforms have previously been
used to approximate or speed up computations
in Convolutional Neural Networks (El-Bakry and
Zhao, 2004; Mathieu et al., 2014; Highlander and
Rodriguez, 2015; Pratt et al., 2017; Lin et al., 2018;
Chitsaz et al., 2020; Goldberg et al., 2020), Recur-
rent Neural Networks (Koplon and Sontag, 1997;
Zhang and Chan, 2000; Zhang et al., 2018), Trans-
formers (Choromanski et al., 2020; Tamkin et al.,
2020), and MLP layers more generally (Cheng
et al., 2015; Moczulski et al., 2016; Sindhwani
et al., 2015), we believe our work is the ﬁrst to
wholly replace particular neural network sublayers
with a Fourier Transform. This approach of view-
ing the Fourier Transform as a ﬁrst class mixing
mechanism is reminiscent of the MLP-Mixer (Tol-
stikhin et al., 2021) for vision, which replaces at-
tention with MLPs; although in contrast to MLP-
Mixer, FNet has no learnable parameters that mix
along the spatial dimension.
Given the favorable asymptotic complexity of
the FFT, our work also connects with the literature
on “long sequence” or “efﬁcient” Transformers,4296which aim to make the attention mechanism scale
better via sparsity patterns (Child et al., 2019; Qiu
et al., 2020; Parmar et al., 2018; Beltagy et al.,
2020; Ainslie et al., 2020; Zaheer et al., 2020;
Wang et al., 2020; Tay et al., 2020b,a; Kitaev et al.,
2020; Roy et al., 2021; Vyas et al., 2020; Liu
et al., 2018) or via linearization of the attention
matrix (Katharopoulos et al., 2020; Choromanski
et al., 2021; Peng et al., 2021). As we will show
in our experiments, while some of those works
achieve O(N)scaling of attention, this complexity
often hides large constants, which make them less
scalable in practice than FNet.
The contributions of our paper are:
•We show that simple linear transformations,
including even (parameter-free) Fourier Trans-
forms, along with standard MLPs in feed-
forward layers, are competent at modeling
diverse relationships in text. That such a sim-
ple linear transformation works at all is sur-
prising, and suggests that, for at least some
NLP problems, attention may not be the prin-
cipal component driving the performance of
Transformers.
•We introduce a new model, FNet, that uses
the Fourier Transform as a mixing mecha-
nism. FNet offers an excellent compromise
between speed, memory footprint, and accu-
racy, achieving 92% and97%, respectively, of
the accuracy of BERT-Base and BERT-Large
(Devlin et al., 2019) on the GLUE benchmark
(Wang et al., 2018), while training 80% faster
on GPUs and 70% faster on TPUs.
•We ﬁnd that FNet hybrid models contain-
ing only two self-attention sublayers achieve
97−99% of their BERT counterparts’ accu-
racy on GLUE, while still running 40−70%
faster. This indicates that, while attention can
improve accuracy, it may not be necessary to
use in every layer.
•We demonstrate FNet scales very well to long
inputs and offers a better compromise between
speed and accuracy than the efﬁcient Trans-
formers evaluated on the Long-Range Arena
(LRA) benchmark (Tay et al., 2021a). Specif-
ically, FNet achieves accuracy comparable
to the most accurate efﬁcient Transformer ar-
chitectures but is signiﬁcantly faster at bothtraining and inference than all of the evalu-
ated Transformer architectures across all se-
quence lengths on GPUs. On TPUs, FNet is
faster for relatively shorter sequence lengths;
for longer sequences, the only efﬁcient Trans-
formers that are faster than FNet on TPUs are
less accurate on the LRA benchmark. Based
on this, we argue that rather than seeking more
efﬁcient approximations of the attention, there
may be more value in seeking out completely
new mixing mechanisms.
2 Related work
2.1 Fourier Transforms in neural networks
Fourier analysis features heavily in studies of the
universal approximation properties of neural net-
works; see, for example, (Cybenko, 1989; Barron,
1993). In terms of practical applications, discrete
Fourier Transforms (DFT), and in particular the
Fast Fourier Transform (FFT), have been used to
tackle signal processing problems such as ﬁtting
neural networks to FFTs of electrocardiogram sig-
nals (Minami et al., 1999; Gothwal et al., 2011;
Mironovova and Bíla, 2015) and vibration signals
(Zhang et al., 2013), or to evolve solutions of Par-
tial Differential Equations (Li et al., 2021).
Because ordinary multiplication in the frequency
domain corresponds to a convolution in the time do-
main, FFTs have been deployed in Convolutional
Neural Networks to speed up computations, in Re-
current Neural Networks to speed up training and
reduce exploding and vanishing gradients, and gen-
erally to approximate dense, linear layers to reduce
computational complexity; see references cited in
Section 1. DFTs have also been used indirectly in
several Transformer works. The Performer (Choro-
manski et al., 2020) linearizes the Transformer self-
attention mechanism by leveraging random Fourier
features to approximate a Gaussian representation
of the softmax kernel. In our work, rather than
approximating attention, we replace attention with
the Fourier Transform, which acts as an alternate
hidden representation mixing mechanism. Tamkin
et al. (2020) use spectral ﬁlters to generate hier-
archical features, showing that the ﬁltered embed-
dings perform well in different tasks (word-level,
sentence-level or document-level), depending on
which frequency scales are ﬁltered. In contrast to
FNet, they separate Fourier frequencies, rather than
using the transform to combine features.42972.2 Modeling semantic relations via attention
Attention models have achieved state of the art re-
sults across virtually all NLP tasks and even some
image tasks (Dosovitskiy et al., 2021). This success
is generally attributed to the ﬂexibility and capac-
ity of attention. Although some works (Ramsauer
et al., 2021) have endeavoured to gain a deeper
understanding of attention, the pervasive intuition
is that the success of attention models derives from
the token-dependent attention patterns in differ-
ent layers; see, for example, (Tenney et al., 2019).
However, it is natural to ask: Do we really need the
ﬂexibility, and associated cost, of attention?
Tay et al. (2020a) empirically investigated the
importance of the dot product operation in the atten-
tion mechanism in their Synthesizer model (related
to our “Linear” baseline below). They ﬁnd that
learnt token-dependent attention weights are highly
expressive, but not necessarily crucial for realiz-
ing accurate NLP models. You et al. (2020) re-
place attention weights in the Transformer encoder
and decoder with unparameterized Gaussian dis-
tributions, showing minimal performance degrada-
tion provided they retain learnable cross-attention
weights. Similarly, Raganato et al. (2020) ﬁnd lit-
tle to no accuracy degradation when replacing all
but one of the attention heads of each attention
layer in the encoder with ﬁxed, non-learnable po-
sitional patterns. Finally, Tolstikhin et al. (2021)
present MLP-Mixer, where attention is replaced
by MLPs, with limited performance degradation in
image classiﬁcation tasks.
2.3 Efﬁcient and long sequence models
The standard attention mechanism (Vaswani et al.,
2017) has a quadratic time and memory bottleneck
with respect to sequence length. This limits its ap-
plicability in tasks involving long range dependen-
cies. Most efforts to improve attention efﬁciency
are based on sparsifying the attention matrix. Tay
et al. (2020c) survey many of the recent efﬁcient
attention works; see also citations in Section 1. Sev-
eral “efﬁcient Transformers” achieve O(N√
N)or
evenO(N)theoretical complexity. However, the
constants hidden by this notation can be large. For
example, in models such as Longformer (Beltagy
et al., 2020), ETC (Ainslie et al., 2020), and Big-
Bird (Zaheer et al., 2020), attention is O(N)as a
function of the input length, but quadratic in the
number of “global tokens”; the latter must be sufﬁ-
ciently large to ensure good performance.The Long-Range Arena benchmark (Tay et al.,
2021a) attempts to compare many of the efﬁcient
Transformers in a series of tasks requiring long
range dependencies, ﬁnding that the Performer
(Choromanski et al., 2021), Linear Transformer
(Katharopoulos et al., 2020), Linformer (Wang
et al., 2020), and Image Transformer (Local Atten-
tion) (Parmar et al., 2018) were the fastest on TPUs
and had the lowest peak memory usages per de-
vice.Instead, in this paper we completely replace
self-attention with a different mixing, namely the
Fourier Transform, which offers: (1) performance,
(2) reduced model size (no learnable parameters),
and (3) simplicity.
Finally, we note that, in an effort to investigate
different token mixing mechanisms, we compare
a vanilla BERT model (Devlin et al., 2019) with
a vanilla FNet, ignoring more recent Transformer
optimizations, which we consider orthogonal to
this work; see, for example, (Narang et al., 2021;
Kim and Hassan, 2020; Shleifer and Rush, 2020).
3 Model
3.1 Discrete Fourier Transform
The Fourier Transform decomposes a function into
its constituent frequencies. Given a sequence {x}
withn∈[0, N−1], the discrete Fourier Transform
(DFT) is deﬁned by the formula:
X=/summationdisplayxe,0≤k≤N−1.(1)
For each k, the DFT generates a new representation
Xas a sum of all of the original input tokens x,
with so-called “twiddle factors”. There are two pri-
mary approaches to computing the DFT: the Fast
Fourier Transform (FFT) and matrix multiplication.
The standard FFT algorithm is the Cooley–Tukey
algorithm (Cooley and Tukey, 1965; Frigo and
Johnson, 2005), which recursively re-expresses the
DFT of a sequence of length N=NNin terms
ofNsmaller DFTs of sizes Nto reduce the com-
putation time toO(NlogN).
An alternative approach is to simply apply the
DFT matrix to the input sequence. The DFT matrix,
W, is a Vandermonde matrix for the roots of unity
up to a normalization factor:
W=/parenleftBig
e/√
N/parenrightBig
, (2)4298
where n, k= 0, . . . , N−1. This matrix multipli-
cation is anO(N)operation, which has higher
asymptotic complexity than the FFT, but turns out
to be faster for relatively shorter sequences on
TPUs.
3.2 FNet architecture
FNet is an attention-free Transformer architec-
ture, wherein each layer consists of a Fourier mix-
ing sublayer followed by a feed-forward sublayer.
The architecture is shown in Figure 1. Essen-
tially, we replace the self-attention sublayer of each
Transformer encoder layer with a Fourier sublayer,
which applies a 2D DFT to its (sequence length,
hidden dimension) embedding input – one 1D DFT
along the sequence dimension, F, and one 1D
DFT along the hidden dimension, F:
y=/Rfractur/parenleftbig
F(F(x))/parenrightbig
. (3)
As indicated by Equation (3), we only keep the real
part of the result; hence, we do not need to modify
the (nonlinear) feed-forward sublayers or output
layers to handle complex numbers. We found that
FNet obtained the best results when the real part
of the total transformation was only extracted at
the end of the Fourier sublayer; that is, after ap-
plying bothFandF. We also experimented
with the Hadamard, Hartley and Discrete Cosine
Transforms. Of these three, the Hartley Transform
was the strongest alternative, obtaining comparableaccuracy to Equation (3); see Appendix A.3 for
details.
The simplest interpretation for the Fourier Trans-
form is as a particularly effective mechanism for
mixing tokens, which provides the feed-forward
sublayers sufﬁcient access to all tokens. Because
of the duality of the Fourier Transform, we can also
view each alternating encoder block as applying
alternating Fourier and inverse Fourier Transforms,
transforming the input back and forth between the
“time” and frequency domain. Because multiplying
by the feed-forward sublayer coefﬁcients in the fre-
quency domain is equivalent to convolving (with a
related set of coefﬁcients) in the time domain, FNet
can be thought of as alternating between multipli-
cations and convolutions.
We use the same embedding layers as in Devlin
et al. (2019); namely, we combine the word embed-
dings, absolute position embeddings of the tokens
and type embeddings of the sentences. Because of
the positional information encoded by the Fourier
Transform in Equation (1)(seen,kindices), FNet
performs just as well without position embeddings.
Nevertheless, we include the position embeddings
to allow for a cleaner comparison with BERT.
3.3 Implementation
Empirically, we found that on GPUs: the FFT is
faster than matrix multiplications for all sequence
lengths we consider ( 512−8192 tokens), whereas
on TPUs: for relatively shorter sequences ( ≤4096
tokens), it is faster to cache the DFT matrix and
then compute the DFT through matrix multiplica-
tions than using the FFT; for longer sequences, the
FFT is faster. As a result, our GPU FNet imple-
mentation always uses the FFT, while our TPU
implementation computes the 2D DFT using ma-
trix multiplications for sequences up to lengths of
4096 and the FFT for longer lengths. Presumably
the GPU vs TPU difference is primarily a result of
two factors: (1) TPUs are even more highly opti-
mized for matrix multiplications than GPUs, and
(2) GPUs offer a more efﬁcient FFT implementa-
tion than TPUs. We suspect that FNet will only
become more performant on TPUs as the TPU im-
plementation of the FFT improves. Our model uses
JAX and, in particular, the Flax framework. Core4299
Mixing layer ops Model params
Model (per layer) Base Large
BERT 2nd+ 4nd112M339M
Linear nd+nd94M269M
FNet (mat) nd+nd83M238M
FNet (FFT) ndlog(n)+ 83M238M
ndlog(d)
Random nd+nd83M238M
FF-only 0 83M238M
model code is given in Appendix A.7 and the full
source core is available online.
4 Results
4.1 Transfer learning
We compare FNet and Transformer architectures
in a common transfer learning setting. For a fuller
picture, we compare multiple models (see Table 1
for parameter counts in “Base” conﬁguration):
•BERT-Base: a Transformer encoder model.
•FNet encoder: we replace every self-attention
sublayer with a Fourier sublayer.
•Linear encoder: we replace each self-attention
sublayer with a two learnable, dense, linear
sublayers, one applied to the hidden dimen-
sion and one to the sequence dimension.
•Random encoder: we replace each self-
attention sublayer with a two constant random
matrices, one applied to the hidden dimension
and one applied to the sequence dimension.
•Feed Forward-only (FF-only) encoder: we
completely remove the self-attention sublayer;
so that this model has no token mixing.
Despite its simplicity, the Linear baseline turns
out to be surprisingly accurate and fast. Our Lin-
ear model is similar to the MLP-Mixer (Tolstikhin
et al., 2021) (for vision) and also the Random Syn-
thesizer (Tay et al., 2020a), but simpliﬁes the latter
model further by removing the multiple heads andsoftmax projections, resulting in just two matrix
multiplications in the mixing sublayer.
It is reasonable to expect that the Linear encoder,
which uses densely parameterized mixing layers,
will learn more ﬂexibly than FNet, which uses
parameter-free mixing layers. As we will show,
although the Linear-Base model outperforms FNet-
Base slightly on GLUE ( 0.3points), it has several
efﬁciency drawbacks relative to FNet: it has a much
larger memory footprint (see Table 4b), it is slower
to train on regular 512sequence lengths (see Table
3), and scales signiﬁcantly worse on long sequence
lengths (see Tables 4b-4c).We also found that
Linear-Large was more difﬁcult to train due to gra-
dient blow up (see “Large” scores in Table 2).
We adopt the same ﬁxed “Base” and “Large”
model and training conﬁgurations as for the origi-
nal BERT (Devlin et al., 2019), except that we pre-
train on the much larger C4 dataset (Raffel et al.,
2020) and use a 32000 SentencePiece vocabulary
model (Kudo and Richardson, 2018) (see Appendix
A.1 for full pre-training details). For ﬁne-tuning
on the GLUE benchmark (Wang et al., 2018), we
found that different BERT runs with the same base
learning rate could yield slightly different results.
Consequently, for the Base (Large) models, we
performed 3 (6) trials, respectively, for each base
learning rate and reported the best result across
all experiments. This reﬂects our observation that
BERT-Large was less stable than BERT-Base, as
noted in Devlin et al. (2019).
We report the results for the best base learning
rate (no early stopping) on the GLUE Validation
split in Table 2.For Base models, results mirror
the pre-training metrics (see Appendix A.1): BERT
performs best. FNet and the Linear model both
underperform BERT by 7.5−8%. Referring to
Table 3, we see that although less accurate, FNet
trains signiﬁcantly faster than BERT – 80% faster
on GPUs and 70% faster on TPUs – and performs
63% of BERT’s FLOPS. Measured in isolation,
the Fourier sublayers perform forward and back-
ward passes an order of magnitude faster than the
self-attention sublayers (see Appendix A.4), but
FNet’s overall training speed is impeded by the
feed-forward sublayers that all models share.4300
Model MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.
BERT-Base 84/81 87 91 93 73 89 83 69 83.3
Linear-Base 74/75 84 80 94 67 67 83 69 77.0
FNet-Base 72/73 83 80 95 69 79 76 63 76.7
Random-Base 51/50 70 61 76 67 4 73 57 56.6
FF-only-Base 34/35 31 52 48 67 FAIL 73 54 49.3
FNet-Hybrid-Base 78/79 85 88 94 76 86 79 60 80.6
BERT-Large 88/88 88 92 95 71 88 86 66 84.7
Linear-Large 35/36 84 80 79 67 24 73 60 59.8
FNet-Large 78/76 85 85 94 78 84 88 69 81.9
FNet-Hybrid-Large 79/80 87 89 92 81 88 86 70 83.6
Pre-training Inference GFLOPS
Model GPU TPU GPU TPU /example
BERT-Base 305 213 82 32 98
Linear-Base 199 (1.5x) 149 (1.4x) 52 (1.6x) 20 (1.6x) 71 (73%)
FNet-Base 169 (1.8x) 128 (1.7x) 46 (1.8x) 23 (1.4x) 62 (63%)
Random-Base 182 (1.7x) 130 (1.6x) 52 (1.6x) 22 (1.4x) 71 (73%)
FF-only-Base 162 (1.9x) 118 (1.8x) 43 (1.9x) 16 (2.0x) 59 (60%)
FNet-Hybrid-Base 198 (1.5x) 149 (1.4x) 51 (1.6x) 24 (1.3x) 68 (69%)
BERT-Large OOM 503 263 111 337
Linear-Large 592 397 (1.3x) 170 (1.5x) 108 (1.0x) 247 (73%)
FNet-Large 511 275 (1.8x) 149 (1.8x) 82 (1.4x) 217 (64%)
FNet-Hybrid-Large 541 294 (1.7x) 157 (1.7x) 84 (1.3x) 227 (67%)
Returning to Table 2: the FF-only model
severely underperforms all other models: as ex-
pected, token mixing is critical to the expressivity
of the model. For example, 50% accuracy scores on
the binary classiﬁcation tasks (QNLI, SST-2, RTE),
indicate that the model fails to learn the tasks. The
weak accuracy of the Random model suggests that
not just any mixing will do; rather, a structured
mixing is required. We also include metrics from a
hybrid FNet attention model. In the hybrid model,
we replace the ﬁnal two Fourier sublayers of FNet
with self-attention sublayers – other conﬁgurations
are possible, but we generally found that replac-
ing the ﬁnal layers worked best; see Appendix A.5.
With the addition of just two self-attention sublay-
ers, the hybrid FNet models achieve 97% and99%
of their respective BERT counterpart’s accuracies
with only limited speed degradations (see Table 3).Interestingly, the gap between BERT and FNet
shrinks to just 3%for Large models; this is likely
due to FNet-Large being more stable during train-
ing than BERT-Large.The Linear-Large model
severely underperforms its Base counterpart on
GLUE benchmark due to training instabilities. We
generally found that the Linear model and BERT
were less stable than the models with no param-
eters in their mixing sublayers, namely the FNet,
Random and FF-only models.
The speed vs MLM accuracy curve for GPU ( 8
V100 chips) pre-training is shown in Figure 2 (see
Appendix A.2 for TPU results). Both TPU and
GPU models are trained for 1million steps as in4301
Devlin et al. (2019). Motivated by the models con-
sidered in Turc et al. (2019), we evaluated several
model sizes; see Table 6 in Appendix A.1. We
found that the smaller model architectures bene-
ﬁted from larger learning rates, so we select the
best result using 10and10for all models.
The GPU (Figure 2), and TPU (Figure 3 in Ap-
pendix A.2) results display the same trends. For
larger, slower models, BERT and FNet-Hybrid de-
ﬁne the Pareto speed-accuracy efﬁciency frontier.
For smaller, faster models, FNet and the Linear
model deﬁne the efﬁciency frontier.
4.2 Long-Range Arena (LRA) benchmark
Of the efﬁcient Transformers evaluated on LRA
benchmark by Tay et al. (2021a), their results
suggest that (1) the vanilla Transformer is (by a
small margin) the second most accurate model, and
(2) the Performer (Choromanski et al., 2021) is
the fastest model. We benchmark FNet’s accu-
racy against both of these models using Tay et al.
(2021a)’s codebase and running on the same hard-
ware ( 4×4TPU v3 chips); the results are shown
in Table 4a.To ensure a fair comparison, we also
report the results of our own experiments for thevanilla Transformer (see Appendix A.6 for details).
Table 4a suggests that, in aggregate, the (vanilla)
Transformer and FNet obtain comparable results.
Given that the Transformer is the second most ac-
curate model evaluated by Tay et al. (2021a) and
that the relative differences in the average accuracy
scores within Table 4a are small, our results suggest
that FNet is competitive with the most accurate of
the efﬁcient Transformers on LRA.
Turning to efﬁciency, in Table 4b, we pro-
vide training speed and memory usage statis-
tics from our experiments on GPUs ( 8V100
chips); see Appendix A.2 for results on TPUs.
We perform a sweep over sequence lengths
{512,1024,2048,4096,8192}. On GPUs, FNet
is much faster than all other models across all se-
quence lengths, due to the highly efﬁcient FFT
implementation on GPUs. Table 4b also indicates
that FNet has a lighter memory footprint (this holds
for both GPUs and TPUs; see extended results in
Appendix A.2). This is partly because FNet has
no learnable parameters in its mixing sublayer, but
also due to the FFT’s efﬁciency, especially at longer
sequence lengths. Lastly, Table 4c shows that train-
ing speed gains generally carry over to inference
gains (see Appendix A.2 for detailed TPU results).
5 Conclusions
In this work, we studied simpliﬁed token mix-
ing modules for Transformer-like encoder archi-
tectures, making several contributions. First, we
showed that simple, linear mixing transformations,4302
Model ListOps Text Retrieval Image Pathﬁnder Path-X Avg.
Transformer (ours) 36.06 61.54 59.67 41.51 80.38 OOM 55.83
Linear (ours) 33.75 53.35 58.95 41.04 83.69 FAIL 54.16
FNet (ours) 35.33 65.11 59.61 38.67 77.80 FAIL 55.30
Transformer (*) 36.37 64.27 57.46 42.44 71.40 FAIL 54.39
Local Attention (*) 15.82 52.98 53.39 41.46 66.63 FAIL 46.06
Sparse Trans. (*) 17.07 63.58 59.59 44.24 71.71 FAIL 51.24
Longformer (*) 35.63 62.85 56.89 42.22 69.71 FAIL 53.46
Linformer (*) 35.70 53.94 52.27 38.56 76.34 FAIL 51.36
Reformer (*) 37.27 56.10 53.40 38.07 68.50 FAIL 50.67
Sinkhorn Trans. (*) 33.67 61.20 53.83 41.23 67.45 FAIL 51.39
Synthesizer (*) 36.99 61.68 64.67 41.61 69.45 FAIL 52.88
BigBird (*) 36.05 64.02 59.29 40.83 74.87 FAIL 55.01
Linear Trans. (*) 16.13 65.90 53.09 42.34 75.30 FAIL 50.55
Performer (*) 18.01 65.40 53.82 42.77 77.05 FAIL 51.41
Training Speed (steps/s) Peak Memory Usage (GB)
Seq. length 512 1024 2048 4096 8192 512 1024 2048 4096 8192
Transformer 21 10 4 OOM OOM 1.6 4.0 12.2 OOM OOM
Linear 34 (1.6x) 19 (1.8x) 9 (2.0x) 4 OOM 0.9 1.6 2.8 6.9 OOM
FNet (FFT) 43 (2.0x) 24 (2.3x) 14 (3.2x) 7 4 0.8 1.3 2.2 3.9 7.4
Performer 28 (1.3x) 15 (1.5x) 9 (1.9x) 4 2 1.1 1.9 3.1 5.5 10.4
Seq. length 512 1024 2048 4096 8192 16384
Transformer 12 28 76 244 OOM OOM
Linear 9 (1.4x) 14 (2.0x) 30 (2.6x) 72 (3.4x) 208 OOM
FNet (FFT) 8 (1.5x) 12 (2.3x) 23 (3.4x) 43 (5.7x) 83 164
Performer 11 (1.2x) 17 (1.6x) 32 (2.4x) 60 (4.0x) 116 238
along with the nonlinearities in feed-forward lay-
ers, can competently model diverse semantic rela-
tionships in text. Second, we introduced FNet, a
Transformer-like model wherein the self-attention
sublayer is replaced by an unparameterized Fourier
Transform. FNets achieve 92and97% of their
respective BERT-Base and BERT-Large counter-
parts’ accuracy on the GLUE benchmark, but train
70−80% faster on GPUs/TPUs. Third, because of
its favorable scaling properties, FNet is very com-
petitive with the “efﬁcient Transformers” evaluated
on the Long-Range Arena benchmark, matching
the accuracy of the most accurate models while
being much faster and lighter on memory.
Our work highlights the potential of linear unitsas a drop-in replacement for the attention mech-
anism in text classiﬁcation tasks. We found the
Fourier Transform to be a particularly efﬁcient and
effective mixing mechanism, due to the speed of
the FFT. However, we only performed a cursory
survey of other linear transformations (see also Ap-
pendix A.3), and additional fast alternatives are
worth exploring.
Given the speed and accuracy advantages of
smaller FNet models relative to Transformers, we
suspect that FNet will be effective as a lightweight,
distilled student model deployed in resource-
constrained settings such as production services
or on edge devices. The need for such lightweight
serving models is only forecast to grow given the4303interest in giant models (Raffel et al., 2020; Brown
et al., 2020; Lepikhin et al., 2021). A natural av-
enue to explore in this regard is knowledge distilla-
tion of small FNet models from larger Transformer
teacher models, following, for example, Sanh et al.
(2019); Jiao et al. (2020); Turc et al. (2019).
Another aspect of interest and worthy of further
study is hybrid FNet-attention models. We found
that adding only a few self-attention sublayers to
FNet offers a simple way to trade speed for accu-
racy. Speciﬁcally, replacing the ﬁnal two Fourier
sublayers with self-attention provided 97−99% of
BERT’s accuracy with limited speed penalties.
Throughout this work we have restricted our fo-
cus to encoders. FNet decoders can be designed
by “causally” masking the Vandermonde matrix,
but a lower level implementation is required to in-
troduce causal masking to FFTs. How to adapt
Fourier mixing for encoder-decoder cross-attention
is an open question as evidence suggests that cross-
attention may be crucial to performance (You et al.,
2020). We have focused on tasks which do not
require generation so we leave FNet decoders and
encoder-decoder setups to future work; although
we do remark that the FNet encoder could be used
as a drop in replacement in a Transformer as other
works have successfully demonstrated; see, for ex-
ample, (Zaheer et al., 2020; Guo et al., 2021).
References4304430543064307
Loss Accuracy
Model Total MLM NSP MLM NSP
BERT-B 1.76 1.48 0.28 0.68 0.86
Linear-B 2.12 1.78 0.35 0.62 0.83
FNet-B 2.45 2.06 0.40 0.58 0.80
Random-B 5.02 4.48 0.55 0.26 0.70
FF-only-B 7.54 6.85 0.69 0.13 0.50
FNet-H-B 2.13 1.79 0.34 0.63 0.84
BERT-L 1.49 1.23 0.25 0.72 0.88
Linear-L 1.91 1.60 0.31 0.65 0.85
FNet-L 2.11 1.75 0.36 0.63 0.82
FNet-H-L 1.89 1.58 0.31 0.67 0.85
A Appendices
A.1 Pre-training details
We adopt the same ﬁxed “Base” and “Large” model
and learning conﬁgurations as for the original
BERT (Devlin et al., 2019). We train on the much
larger C4 dataset (Raffel et al., 2020) and use a
32000 SentencePiece vocabulary model (Kudo and
Richardson, 2018) trained on a 100million sen-
tence subset of C4. Our TPU experiments use a
batch size of 256as in Devlin et al. (2019) and are
each run on 4×4TPU v3 chips. Our GPU experi-
ments use a smaller batch size of 64and are run on
8V100 chips. Because the training conﬁguration is
lifted from Devlin et al. (2019), it may be slightly
biased towards the BERT attention model.
Table 5 summarizes the pre-training metrics for
the different models; the pre-training speeds are
shown in Table 3 in the main text. Although they
have weaker accuracy metrics, the Linear model
and FNet train nearly 80% faster than BERT on
GPUs, and 70% faster on TPUs (see Table 3). We
also ﬁnd that the three models with no learnable
parameters in their mixing layer, namely FNet, the
Random model and the FF-only model, are the
most stable during training.
BERT’s higher accuracy on the MLM pre-
training task is not simply a result of having more
parameters than the other models. Indeed, Table 5
shows that BERT-Base is actually more accurate
than FNet-Large, which contains more than twice
as many parameters. BERT is presumably more ex-
pressive because the mixing (attention) weights are
both task speciﬁc and token dependent, determined
Dimensions Parameters (millions)
dLayers BERT Linear FNet FNet-H
768 12 111 93 83 88
512 12 55 49 42 44
512 8 42 38 34 36
256 8 15 15 13 13
512 4 30 28 26 28
256 4 12 12 11 11
256 2 10 10 10 -
128 2 5 5 4 -
by token-token (query-key) dot products; see also
Tay et al. (2020a). FNet’s mixing weights, on the
other hand, are neither task speciﬁc nor token de-
pendent.
Finally, Table 6 shows the model sizes that were
used to construct Figure 2 (main text) and Figure 3
(Appendix A.2).
A.2 TPU results
In this section, we report FNet efﬁciency results
for TPUs; the main text focuses on GPUs. Figure
3 shows the speed vs MLM pre-training accuracy
curve when training on TPU ( 4×4v3 chips). As
on GPUs, FNet and the Linear model deﬁne the
Pareto efﬁciency frontier for smaller, faster models,
while BERT deﬁnes the frontier for larger, slower
models.
Table 7 shows Long Range Arena Text classiﬁ-
cation efﬁciency results on TPUs ( 4×4v3 chips).
The Linear model and FNet train faster than all the
efﬁcient Transformers for sequence lengths ≤2048
and512, respectively. For longer sequences, FNet
is slower than the Performer and, based on results
in Tay et al. (2021a), likely also slower than the
other efﬁcient Transformers that linearize attention,
namely Local Attention (Parmar et al., 2018), Lin-
former (Wang et al., 2020) and Linear Transformer
(Katharopoulos et al., 2020). However, it is worth
noting that Table 4a suggests that FNet is more
accurate than all of the aforementioned models.
Moreover, we expect that the GPU speed gains will4308
Seq. length 512 1024 2048 4096 8192 16386
Training Speed (steps/s)
Transformer 8.0 5.6 1.7 OOM OOM OOM
Linear 9.4 (1.2x) 9.1 (1.6x) 7.6 (4.5x) 3.9 1.4 OOM
FNet (mat) 9.5 (1.2x) 9.1 (1.6x) 6.1 (3.6x) 3.0 0.8 0.2
FNet (FFT) 8.6 (1.1x) 6.0 (1.1x) 3.2 (1.9x) 1.6 0.8 0.3
Performer 9.2 (1.2x) 8.4 (1.5x) 6.9 (4.1x) 4.2 2.2 1.1
Inference Speed (ms/batch)
Transformer 7.0 13.2 39.4 129.9 490.2 OOM
Linear 5.6 (1.2x) 6.5 (2.0x) 9.6 (4.1x) 20.4 (6.4x) 54.6 (9.0x) OOM
FNet (mat) 6.0 (1.2x) 7.7 (1.7x) 15.4 (2.6x) 40.7 (3.2x) 137.0 (3.6x) 454.5
FNet (FFT) 10.8 (0.7x) 16.8 (0.8x) 29.9 (1.3x) 58.8 (2.2x) 113.6 (4.3x) 263.2
Performer 6.1 (1.2x) 7.2 (1.8x) 10.1 (3.9x) 17.5 (7.4x) 31.8 (15.4x) 61.0
Peak Memory Usage (GB)
Transformer 1.1 2.1 5.8 9.1 OOM OOM
Linear 0.9 1.1 1.9 4.9 14.8 OOM
FNet (mat) 0.8 0.9 1.3 2.2 4.8 11.9
FNet (FFT) 0.8 0.9 1.3 2.0 3.5 6.3
Performer 1.0 1.3 1.8 3.0 5.1 9.64309transfer to TPUs as the TPU FFT implementation
improves.
A.3 Additional conﬁgurations that we
experimented with
We experimented with a number of additional ideas
to improve FNet.
Fourier Transform algorithm. On GPUs, the
FFT was the fastest algorithm for computing the
DFT across all sequence lengths that we experi-
mented with ( 512−8192 ). On TPUs, it is faster
to compute the DFT directly using matrix multipli-
cations for relatively shorter sequence lengths (up
to lengths of 4096 ; see Table 7). This efﬁciency
boundary between matrix multiplication and FFT
on TPUs will change depending on the XLA pre-
cision for the matrix multiplications. We found
that, although (slower) HIGHEST XLA precision
was required to very accurately reproduce FFT in
computing the DFT, (faster) DEFAULT XLA pre-
cision was sufﬁcient to facilitate accurate model
convergence.
Modifying the Fourier Transform computa-
tion. To keep the entire FNet architecture simple,
the Fourier sublayer accepts real input and returns
real output. The standard Fourier sublayer in FNet
simply extracts the real part after computing the
2D DFT. We found that FNet was less accurate and
less stable during training if only the real part of the
DFT was used throughout the computation. Simply
extracting the absolute value (instead of the real
part) also led to a signiﬁcantly less accurate model.
Because the feed-forward sublayer mixes the hid-
den dimension, we experimented with applying
a 1D DFT along the token dimension only in the
Fourier sublayer (i.e. no hidden dimension mixing
in the Fourier sublayer). This yielded some train-
ing speed gains but hurt accuracy. The 1D (token
mixing only) DFT model still signiﬁcantly outper-
formed the (no token mixing) FF-only model, indi-
cating that token mixing is most important mecha-
nism in the Fourier sublayer.
Other transforms. We experimented with three
natural alternatives to the Fourier Transform:
•Discrete Cosine Transform (DCT). The DCT
is closely related to the DFT but transforms
real input to real output. However, we found
that the DCT model underperformed FNet ( ∼
4%accuracy degradation).•Hadamard Transform. Although the
Hadamard Transform was slightly faster than
the DFT, it yielded less accurate results ( ∼2%
accuracy degradation).
•Hartley Transform. The Hartley Transform,
which transforms real input to real output, can
be described in terms of the Fourier Trans-
form:H=/Rfractur{F}−/Ifractur{F} . We found that
the Hartley Transform matched the Fourier
Transform on GLUE ( 76.7vs.76.7).
Introducing learnable parameters to the
Fourier sublayer. Our attempts to introduce learn-
able parameters into the Fourier sublayer were
either detrimental or inconsequential, and gener-
ally slightly slowed the model. For the (sequence
length, hidden dimension) input in each Fourier
sublayer, we tried two approaches to introduce
learnable parameters: (1) element wise multipli-
cation with a (sequence length, hidden dimension)
matrix, and (2) regular matrix multiplication with
(sequence length, sequence length) and (hidden
dimension, hidden dimension) matrices. We exper-
imented with these approaches in various conﬁgura-
tions: preceding and/or following the DFT, and also
in combination with inverse DFT (e.g. transform
to frequency domain, apply element wise multipli-
cation, transform back to time domain), but most
setups degraded accuracy and reduced training sta-
bility, while a few did not change accuracy but lead
to small speed decreases. In a slightly different set
of experiments and in an effort to provide more
ﬂexibility to the model, we added (complex) learn-
able weights to the 2D DFT matrix. This model
was stable but did not yield any accuracy gains,
suggesting that the DFT is locally optimal in some
sense.
FNet block modiﬁcations. The standard FNet
encoder block structure follows that of the Trans-
former: a Fourier sublayer followed by a feed-
forward sublayer, with residual connections and
layer norms after each sublayer; see Figure 1. We
tried several modiﬁcations to this structure, based
on the intuition of moving in and out of the fre-
quency domain between multiplications. For ex-
ample, the sandwiching of Fourier, feed-forward,
Fourier (or inverse Fourier) sublayers and only ap-
plying the residual connections and layer norms to
the ﬁnal result, yields a structure that more closely4310
Training speed (ms/batch) Inference speed (ms/batch)
GPU TPU GPU TPU
Self-attention (Base) 136 76 43 16
Linear (Base) 36 (3.7x) 12 (6.1x) 15 (2.8x) 4 (3.9x)
FNet (Base) 11 (12.2x) 8 (9.9x) 11 (4.0x) 8 (2.1x)
Self-attention (Large) 404 212 128 43
Linear (Large) 103 (3.9x) 35 (6.1x) 36 (3.6x) 10 (4.5x)
FNet (Large) 18 (22.2x) 22 (9.7x) 18 (7.3x) 22 (2.0x)
mimics convolutions. However, these setups de-
graded accuracy and lead to a more unstable model
during training. Adding extra feed-forward sub-
layers to this layering, or swapping out the feed-
forward sublayers for simpler dense sublayers, did
not help either.
A.4 Mixing layer speeds
Table 8 summarizes the inference and training
speeds for the different mixing layers. For each
of the Base and Large conﬁgurations, we have re-
moved all other sublayers and transformations and
then calculated the speed per batch of input exam-
ples. The FNet training speeds are particularly fast
because no parameters are updated. The Linear
model has faster inference than FNet on TPUs be-
cause it is performing real matrix multiplications,
whereas FNet performs complex matrix multiplica-
tions; see Equation (2).
Although the Fourier mixing sublayer itself per-
forms forward and backward passes signiﬁcantly
faster than the self-attention sublayer, FNet is over-
all 70-80% faster than BERT because the overall
training and inference speeds are bottle-necked by
the feed-forward sublayers that all models share.
A.5 FNet-Hybrid ablations
Table 9 shows the effects of varying the number
of attention sublayers and the attention layout in
the FNet-Hybrid model. For the “BOTTOM” lay-
out, all attention sublayers are placed in the ﬁrst
few encoder layers, where they replace the Fourier
mixing sublayers. For the “TOP” layout, attention
sublayers are placed in the ﬁnal encoder layers; for
the “MIDDLE” layout they are placed in the mid-
dle layers; and for the “MIXED” layout, they are
distributed through the model.
Attention Accuracy Speed
Layers Layout MLM NSP (ms/batch)
2 BOTTOM 0.497 0.733 193
2 MIDDLE 0.499 0.686 196
2 MIXED 0.509 0.727 194
2 TOP 0.526 0.738 193
0 TOP 0.486 0.679 173
2 TOP 0.526 0.738 193
4 TOP 0.539 0.740 214
6 TOP 0.546 0.746 235
From the Table 9, we can make two observations:
(1) more attention improves accuracy at the cost
of speed, and ultimately with diminishing returns;
(2) placing attention layers at the top of the model
gives the best accuracy results. Given our focus on
speed, we chose to focus FNet-Hybrid experiments
in the main text of the paper on the 2 attention layer,
“TOP” conﬁguration variant.
A.6 A note on Long-Range Arena
hyperparameter settings
Concerning the Long-Range Arena setup, several
hyperparameters are not described in Tay et al.
(2021a) and there a few mismatches between the
conﬁgurations described in the paper and the code
repository. Where possible, we prioritize conﬁg-
urations described in the paper with only two ex-
ceptions. Firstly, for the CIFAR10 (Image) task,
we perform a sweep of the number of layers in the4311range [1,2,3,4]. We found that 1 layer worked
best for all models; Tay et al. (2021a) suggest 3
layers yielded the best results. Secondly, for the
Pathﬁnder task, we found that a base learning rate
of0.001(as given in the code repository) yielded
better results for all models than the 0.01value in-
dicated in Tay et al. (2021a). We also perform a
very small sweep over the embedding dimension
and batch size, which are not listed in Tay et al.
(2021a).
We also remark that the accuracy comparisons
between our runs and those from Tay et al. (2021a)
should be performed with the caveat that we found
that results for certain tasks – Text and Retrieval
in particular – can vary quite a bit between runs,
especially for the Transformer; we report the best
results.
A.7 FNet code43124313