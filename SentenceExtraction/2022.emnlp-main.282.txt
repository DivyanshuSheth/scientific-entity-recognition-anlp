
Yunqi Zhang, Yubo Chen, Yongfeng HuangDepartment of Electronic Engineering & BNRist, Tsinghua University, Beijing, ChinaZhongguancun Laboratory, Beijing, China
yq-zhang19@mails.tsinghua.edu.cn ybch14@gmail.com
yfhuang@tsinghua.edu.cn
Abstract
Relational triple extraction is a critical task for
natural language processing. Existing methods
mainly focused on capturing semantic informa-
tion, but suffered from ignoring the syntactic
structure of the sentence, which is proved in the
relation classification task to contain rich rela-
tional information. This is due to the absence
of entity locations, which is the prerequisite for
pruning noisy edges from the dependency tree,
when extracting relational triples. In this paper,
we propose a unified framework to tackle this
challenge and incorporate syntactic informa-
tion for relational triple extraction. First, we
propose to automatically contract the depen-
dency tree into a core relational topology and
eliminate redundant information with graph
pooling operations. Then, we propose a sym-
metrical expanding path with graph unpooling
operations to fuse the contracted core syntactic
interactions with the original sentence context.
We also propose a bipartite graph matching
objective function to capture the reflections be-
tween the core topology and golden relational
facts. Since our model shares similar contract-
ing and expanding paths with encoder-decoder
models like U-Net, we name our model as Re-
lation U-Net (RelU-Net). We conduct experi-
ments on several datasets and the results prove
the effectiveness of our method.
1 Introduction
Relational Triple Extraction (RTE) is defined as au-
tomatically recognizing entity pairs and the seman-
tic relations in the form of ( subject ,relation ,object )
from unstructured text. It is a critical task for natu-
ral language processing, especially for constructing
large-scale Knowledge Graphs (KGs) from unla-
beled corpus (Dong et al., 2014).
Recent work proposed several neural network
methods to extract relational triples. For example,
Zheng et al. (2017) formulated this task as sequenceFigure 1: An example of the dependency-based relation
classification method and our syntax-aware relational
triple extraction method. The dashed arrows indicate the
location injection of the known entities when pruning
the dependency tree.
tagging problems but failed to extract overlapping
triples. Wei et al. (2020) proposed a cascade tag-
ging framework to solve the overlapping problem.
Chen et al.(2021) proposed to extract the implicit
relational triples which are not explicitly expressed
in the sentence with a binary pointer network.
Existing methods achieved considerable success
in capturing semantic information from relational
mentions. However, they usually failed to incor-
porate syntactic structures of the sentence, which
is proved to contain rich relational information
in the Relation Classification (RC) task (Zhang
et al., 2018; Guo et al., 2019). These syntax-aware
RC methods usually pruned irrelevant dependency
edges according to the known locations of the en-
tity pair to eliminate the noise of the dependency
tree. For example, the relational facts in Figure 1
(top) can be easily inferred given the dependency
paths, which are pruned from the original depen-
dency tree using the locations of the known entity
pairs. Unfortunately, the locations of entities are
unknown in the RTE task, as shown in Figure 1
(bottom). This absence makes it difficult to prune4208dependency noise thus leads to an insufficient ex-
ploration of syntactic and relational information.
In this paper, we propose a unified framework
to tackle this challenge and incorporate syntactic
information for relational triple extraction. First,
we propose to reduce the dependency tree and elim-
inate syntactic noise with graph pooling operations
(Figure 1). We utilize MinCut pooling (Bianchi
et al., 2020) to cluster similar nodes (words) and
hence obtain a core relational topology. Next, we
apply the inductive GraphSAGE algorithm (Hamil-
ton et al., 2017) to propagate cluster information
and capture the syntactic interactions underlying
the core topology. Then, we propose to symmet-
rically expand the core topology with graph un-
pooling operations to integrate the syntactic inter-
actions with the original sentence context. Finally,
we propose a bipartite graph matching loss to in-
duce the connections in the core topology to re-
flect golden relational facts, which is similar to
the reflections between the dependency paths and
golden facts (e.g. Figure 1) in the RC task. Since
our model shares similar contracting and expand-
ing paths with encoder-decoder models like U-
Net (Ronneberger et al., 2015), we name our model
as Relation U-Net (RelU-Net).
The main contributions of this paper are:
•We propose a unified framework to incorpo-
rate syntactic structures of the sentence for
relation triple extraction.
•To eliminate disturbance caused by irrelevant
syntactic information, we propose to automat-
ically contract the dependency tree into a core
topology with graph pooling operations.
•To fuse the syntactic interactions underlying
the core topology with the original sentence
context, we propose a symmetrical expanding
path with graph unpooling operations.
•To establish reflections between core topology
connections and golden relational facts, we
propose a bipartite graph matching loss.
2 Related Work
In the early work of relational triple extraction, the
task is addressed in a pipelined manner (Zelenko
et al., 2003; Zhou et al., 2005; Chan and Roth,
2011; Gormley et al., 2015). They recognized all
entities in the sentence and classified the relations
between pairs of extracted entities separately. How-
ever, the pipeline methods usually suffered fromerror propagation and failed to capture the interac-
tions between the entities and relations.
To address these issues, end-to-end models for
joint extraction of entities and relations have be-
come the dominant paradigm of this task, including
feature-based models (Yu and Lam, 2010; Li and Ji,
2014; Ren et al., 2017) and neural network-based
models (Miwa and Sasaki, 2014; Gupta et al., 2016;
Miwa and Bansal, 2016; Zheng et al., 2017). For
example, Ren et al. (2017) proposed to detect re-
lation mentions and their entity arguments with
distant supervision. Gupta et al. (2016) proposed
to model the inter-dependencies of entities and rela-
tions through the entity-relation table proposed by
(Miwa and Sasaki, 2014). Zheng et al. (2017) first
formulated this task as a sequence tagging problem,
but they failed to extract overlapping triples.
More recent work developed several strategies
to address the overlapping triple problem, includ-
ing sequence tagging based models (Wei et al.,
2020; Wang et al., 2020; Zheng et al., 2021; Chen
et al., 2021) and triple generation based mod-
els (Zeng et al., 2018, 2019, 2020; Sui et al.,
2020; Huguet Cabot and Navigli, 2021). For ex-
ample, Wei et al. (2020) proposed a cascade tag-
ging scheme to simultaneously identify all possible
overlapping triples. Chen et al. (2021) proposed a
binary pointer network to extract overlapping rela-
tional triples and introduced a relational network
to capture relational reasoning patterns for this
task. Zeng et al. (2018) proposed a sequence-to-
sequence triple generation model with copy mech-
anism, while Sui et al. (2020) proposed to treat this
task as a direct set prediction problem. However,
these methods mainly focused on learning seman-
tic information of the sentence and usually suffered
from ignoring syntactic patterns of the sentence.
Although Fu et al. (2019) applied the dependency
tree to extract regional features of the text, they did
not prune irrelevant contents from the dependency
tree, which may be sub-optimal.
Different from previous work, we propose to re-
move redundant information from the dependency
tree with graph pooling operations and integrate
core syntactic connections with the original sen-
tence context with graph unpooling operations. We
also propose a bipartite matching loss to guide the
core syntactic connections to reflect golden rela-
tional facts, like in the RC task. Experimental
results on several benchmark datasets prove the
effectiveness of our method.4209
3 Our Approach
The overall framework of our approach is illus-
trated in Figure 2. We introduce the Relation U-
Net, the triple extractor and the details of model
training in Section 3.1, 3.2 and 3.3, respectively.
3.1 Relation U-Net (RelU-Net)
The syntactic structure of the sentence has already
been proved to contain rich relational information
by existing RC methods (Zhang et al., 2018; Guo
et al., 2019; Yu et al., 2020). They usually pruned
irrelevant contents from the dependency tree to
eliminate noise according to the known locations
of each entity pair (Figure 1). However, the loca-
tions of entities are unknown in RTE, which makes
it challenging to prune noisy dependency edges and
sufficiently exploit syntactic information. To tackle
this challenge, we first down-sample the depen-
dency tree to summarize informative structures and
reduce noise. The down-sampled core topology
contains relation-relevant syntactic information of
the sentence. Then, we symmetrically up-sample
the core topology to enable precise triple localiza-
tion. Our model has a contracting and expanding
path of the dependency tree, which shares simi-
lar architecture with encoder-decoder models like
U-Net (Ronneberger et al., 2015).
Therefore, we propose Relation U-Net, named
as RelU-Net, to incorporate syntactic information
for RTE. We first propose to automatically reducethe dependency tree into a core relational topol-
ogy with graph pooling operations (Section 3.1.1).
Then, we adopt inductive graph convolutions to
capture the syntactic interactions underlying the
core topology (Section 3.1.2). Finally, we propose
to expand the core topology with graph unpooling
operations for the fusion of the syntactic interac-
tions and the semantic context (Section 3.1.3).
3.1.1 Graph Pooling
Given the input sentence {w, . . . , w}and its de-
pendency tree, we first convert the words (nodes)
into contextual representations with a text encoder
such as BERT (Devlin et al., 2019), whose output
denoted as E= [E, . . . ,E]. Then, we repre-
sent the dependency tree with an adjacency matrix
A, where (A)= 1if there exists a depen-
dency edge between the i-th and the j-th word oth-
erwise 0. Next, we utilize the spectral-clustering
MinCut pooling (Bianchi et al., 2020) operations to
aggregate nodes with strong syntactic connections
and similar semantic features. By clustering depen-
dency nodes, we merge all irrelevant edges into the
supernodes, guiding our model to focus more on
critical syntactic interactions between the supern-
odes, thus reducing the negative impact of noisy
contents. Specifically, MinCut pooling (denoted as
gPool) captures syntactic interactions with a Graph
Convolution Network (GCN, Section 3.1.2) and4210uses a Multi-Layer Perceptron (MLP) to learn a
cluster assignment matrix S∈R:
S=softmax(MLP(GCN(X,A);Θ)) (1)
where N, C are numbers of input nodes and output
clusters, X∈R,A∈Rare the input
node features and adjacency matrix, and the soft-
max operation is to normalize the contributions of
all input nodes to each output cluster. Then we
compute the pooled node features X and adja-
cency matrix A:
X=SX,A=SAS (2)
To emphasize the inter-supernode connections, we
zero the diagonal and apply degree normalization
to the pooled adjacency matrix:
ˆA=A−Idiag(A)
/tildewideA=ˆDˆAˆD(3)
where ˆDis the degree matrix of ˆA. Since
these pooling operations are fully differentiable,
the MinCut layer can be stacked multiple times:
X=ReLU (X),A=/tildewideA(4)
where X=E,A=A. We perform
Ltimes pooling on the raw dependency tree and
obtain a core topology G={X,A}, as
shown in Figure 2 (left).
Note that our model is similar to the multi-layer
graph pooling proposed by Gao and Ji (2019), but
we do not use their top- Kgraph pooling. This is
because the top- Kpooling reduces graph size by
simply removing nodes, but the RTE task involves
node merging, such as multi-word entities.
3.1.2 Graph Convolution
We apply multi-layer graph convolutions to cap-
ture the syntactic interactions underlying the core
topology. However, this pooled topology changes
dynamically with the parameter updates during
training, thus making the conventional transduc-
tive graph convolution methods (Kipf and Welling,
2016) not suitable to our model. Therefore, we
adopt the inductive GraphSAGE (Hamilton et al.,
2017) algorithm, which first uniformly samples
neighbor nodes w.r.t. each node and then aggre-
gates the features of the sampled nodes. Formally,
the convolution of the p-th layer is formulated as:
G =ReLU/parenleftig
W[G;Ave(G)]/parenrightig
(5)where Wstands for the parameters of the p-th
layer,Sdenotes the i-th node’s sampled neighbors
in the topology, and G=X. We also use
GraphSAGE in the GCN of graph pooling (Equa-
tion 1) because the intermediate graphs of the pool-
ing path are also dynamic.
3.1.3 Graph Unpooling
To fuse the syntactic representations of the core
topology with the original sentence context, we
propose a reverse expanding path with graph un-
pooling operations to enable precise triple local-
ization, as illustrated in Figure 2 (right). Dur-
ing graph pooling process, apart from learning the
cluster assignment matrix S, we also learn a clus-
ter decomposition matrix Swith another MLP:
S=softmax (MLP(GCN(X,A);Θ)). Then, we
transpose Equation 2 to achieve graph unpooling:
X =SX,A =SAS(6)
We also add symmetrical skip-connections between
the corresponding pooling and unpooling layers
to combine semantic and syntactic information at
different levels (Figure 2).
3.2 Relational Triple Extractor
We adopt CR(Wei et al., 2020) to the out-
put representations Hof the RelU-Net to ex-
tract relational triples, which consists of a bi-
nary subject tagger and a set of relational-specific
object taggers. The subject tagger predicts the
start and end positions of all possible subjects
with two binary classifiers: p=σ/parenleftbig
WH+
b/parenrightbig
where σis the sigmoid function, W
andbare the parameters of the subject clas-
sifiers. Then, each relational-specific object tag-
ger takes the averaged representation of the k-
th subject’s start and end tokens as sand pre-
dicts the start and end positions of all objects cor-
responding to the current subject under the r-th
relation: p=σ/parenleftig
W(H+s) +b/parenrightig
where Wandbare the parameters of the
r-th relation-specific object classifiers. We set the
binary tags to 1 if their probabilities exceed some
threshold ( 0.5in our model) otherwise 0. Then we
extract the subjects and objects by matching the
nearest start-end position pair. We refer readers
to (Wei et al., 2020) for more detailed descriptions.
3.3 Model Training
We calculate a Binary Cross-Entropy (BCE)
f(y,p) =−/summationtextylogp+(1−y) log(1 −p)4211as the loss of a triple extractor’s predictions:
L=/summationdisplay(f(y,p) +/summationdisplayf(y,p)),(7)
where yare the binary labels corresponding to the
position probabilities p.
In addition, we observe from the RC task that a
dependency path connecting two entities reflects a
relational fact (Figure 1). Similarly, we expect the
connections in the core topology to have reflections
to the golden relational facts. Therefore, we pro-
pose an objective function that minimizes the adja-
cency similarity between the core topology and the
golden relational triples. We represent the golden
triples with a relational graph G={X,A},
whose nodes are subject and object entities, Xis
computed by averaging the word representations E
corresponding to the nodes’ entities, and A= 1
if there exists a golden triple between the i-th and
thej-th node otherwise 0. However, the node or-
ders of GandGmay mismatch, making it
difficult to directly compare their adjacency matri-
ces because they are sensitive to node permutations.
To address this issue, we propose to find the op-
timal bipartite matching between two sets of graph
nodes and compute the similarity score between the
core topology and the permuted golden graph, as
illustrated in Figure 2 (top). We formulate the opti-
mal bipartite matching as a permutation πwith the
minimum cost: π= argmin/summationtextC(X,X),
where Πis the permutation space with length m.
Cis a pair-wise matching cost between the two sets
of graph nodes and is defined as a bilinear score:
C:=C(X,X) =−XWX (8)
where Ware learnable parameters of the cost
function. The optimal permutation πis computed
via the Hungarian algorithmin polynomial time.
Moreover, we add a bipartite loss to minimize the
approximatelower bound of the matching cost:
L =/summationdisplaylog/parenleftig
softmaxC/parenrightig
=/summationdisplayC−m·log/summationdisplayexpC(9)
Finally, we permute the golden graph’s adjacency
Awith the optimal matching πand compute itssimilarity to the core topology’s adjacency A
with a point-wise BCE:
L =BCE(A,A). (10)
We train our RelU-Net with the joint loss L=
L+αL +βL, in which αandβare
hyper-parameters.
4 Experiments
4.1 Datasets and Evaluation Metrics
We evaluate our method on two widely-used bench-
mark datasets: NYT (Riedel et al., 2010) and
WebNLG (Gardent et al., 2017). NYT consists
of sentences sampled from New York Times news
articles and contains 24 relation types. WebNLG
was originally proposed for natural language gener-
ation and first introduced in the RTE task by Zeng
et al. (2018), which contains 171 relation types.
Following previous work (Zeng et al., 2018; Wei
et al., 2020; Chen et al., 2021), we divide the sen-
tences into three classes: Normal ,EntitypairOver-
lap(EPO) and SingleEntityOverlap (SEO) accord-
ing to different overlapping patterns of triples, as
shown in Table 1. We adopt the same partial match
score following previous work (Zheng et al., 2017;
Wei et al., 2020; Chen et al., 2021) for evaluation.
We regard the extracted triple as correct if and only
if the relation and the heads of subject and object
are all correct. We report the standard micro preci-
sion, recall, and Fscores on both datasets.
4.2 Experimental Settings
The hyper-parameters are determined on the vali-
dation set. We use the spaCy toolkitto parse the
dependencies of sentences. We adopt BERT
and RoBERTa as our text encoders follow-
ing (Chen et al., 2021). We use 2 layers of graph
pooling, graph unpooling, and GraphSAGE convo-
lutions in our model. The output node numbers of4212MethodNYT WebNLG
Prec. Rec. FPrec. Rec. F
CR (Wei et al., 2020) 81.5 75.7 78.5 84.7 79.5 82.0
CR (Wei et al., 2020) 89.7 89.5 89.6 93.4 90.1 91.7
TPLinker (Wang et al., 2020) 91.3 92.5 91.9 91.8 92.0 91.9
SPN (Sui et al., 2020) 93.3 91.7 92.5 93.1 93.6 93.4
CGT (Ye et al., 2021) 90.8 77.7 83.7 87.6 70.5 78.1
CGT (Ye et al., 2021) 94.7 84.2 89.1 92.9 75.6 83.4
PFN (Yan et al., 2021) - - 92.4 - - 93.6
TDEER (Li et al., 2021) 93.0 92.1 92.5 93.8 92.4 93.1
PRGC (Zheng et al., 2021) 89.6 82.3 85.8 90.6 88.5 89.5
PRGC (Zheng et al., 2021) 93.3 91.9 92.6 94.0 92.1 93.0R-BPtrNet (Chen et al., 2021) 92.7 92.5 92.6 93.7 92.8 93.3R-BPtrNet (Chen et al., 2021) 92.7 91.6 92.1 93.6 92.9 93.3R-BPtrNet (Chen et al., 2021) 94.0 92.9 93.5 94.3 93.3 93.8R-BPtrNet (Chen et al., 2021) 93.3 93.0 93.2 94.2 93.2 93.7
RelU-Net 87.9 87.3 87.6 90.3 89.1 89.7
RelU-Net 89.4 87.6 88.5 93.4 88.9 91.1RelU-Net 93.3 92.9 93.1 94.9 93.7 94.3RelU-Net 94.2 93.3 93.7 95.4 94.4 94.9
the graph pooling layers are set to 16 and 8, respec-
tively. The weights αandβof bipartite matching
loss and graph similarity loss are set to 1.0 and
0.1, respectively. We use Adam optimizer (Kingma
and Ba, 2014) to fine-tune the pre-trained BERT
weights with the learning rate of 10and train
other parameters with the learning rate of 5×10.
We train our model for 200 epochs with batch
size as 10 on both datasets. We choose the model
with the best validation performance and report the
scores on the test set.
4.3 Performance Evaluation
We report the evaluation results on the NYT and
WebNLG test sets in Table 2. We also compare
our RelU-Net with several previous state-of-the-art
models: (1) CR(Wei et al., 2020) proposed
a cascade binary tagging framework with a sub-
ject tagger and a set of relational-specific object
taggers. (2) TPLinker (Wang et al., 2020) pro-
posed a novel handshaking tagging scheme to linktoken pairs. (3) SPN (Sui et al., 2020) proposed
a relational triple set prediction network with non-
autoregressive transformers. (4) CGT (Ye et al.,
2021) proposed a novel generative transformer
for contrastive triple extraction. (5) PFN (Yan
et al., 2021) proposed a partition filter network
to model two-way interaction between NER and
RE. (6) TDEER (Li et al., 2021) proposed a
translating decoding schema with negative sam-
ples. (7) PRGC (Zheng et al., 2021) proposed to
model potential relation and global correspondence
through decomposing this task into three subtasks.
(8)R-BPtrNet (Chen et al., 2021) proposed a rea-
soning pattern enhanced binary pointer network to
extract implicit relational triples.
From Table 2 we have several observations.
First, the performance of RelU-Netsignifi-
cantly drops on both datasets compared with C-
R. It indicates that the full dependency
tree contains much noise and will greatly hurt
the model performance if used without pruning.4213
Second, RelU-Net significantly outperforms
other randomly initialized models on the NYT
and WebNLG datasets. Moreover, RelU-Net
even outperforms RelU-Netwhere the latter
uses pre-trained BERT weights while the former
does not. It indicates the effectiveness of our
method to incorporate the syntactic structure of
the sentence by automatically contracting the de-
pendency tree and reducing noisy contents. Finally,
RelU-Net and RelU-Net further outper-
forms RelU-Net and other baseline models.
It indicates that the pre-trained language models
bring more prior knowledge from unlabeled corpus
and enhance the model performance.
4.4 Performance on Different Sentence Types
Following previous work (Wei et al., 2020; Zheng
et al., 2021; Chen et al., 2021), we divide the test
sets of two datasets according to the overlapping
patterns and the triple counts to investigate the abil-
ity of our model in handling complex sentences.
The results are shown in Table 3. We observe
that our RelU-Net model outperforms the baseline
models in almost all the subsets, especially on the
WebNLG dataset. We speculate that this is because
we train the model with the graph similarity loss
(Section 3.3) to bootstrap the core topology con-
nections to reflect relational facts, thus the complex
interactions between multiple relational triples can
be naturally captured through the graph convolu-
tions over the core topology. In general, the results
on different sentence types show the effectiveness
of our model in complicated scenarios.
4.5 Ablation Study
In this section, we conduct an ablation study on
the NYT test set to demonstrate the contribution
of each component of our model. The result isshown in Table 4. Note that when removing Graph-
SAGE, we use the transductive convolution (Kipf
and Welling, 2016) instead. From Table 4, we ob-
serve that the GraphSAGE convolution constitutes
a significant contribution to the model performance.
It indicates that the inductive convolution can bet-
ter fit the dynamic core topology and intermediate
graphs in the pooling path and improve the ability
of capturing syntactic interactions. The bipartite
matching loss L also brings improvements
Method Prec. Rec. F
RelU-Net 93.3 92.9 93.1
w/o GraphSAGE 92.2 91.0 91.6
w/oL 93.1 91.5 92.3
w/oL +L 91.8 90.6 91.2
w/o All 91.2 90.0 90.64214
to the model because it helps minimize the approxi-
mate lower bound of the node matching cost, which
is approximately equivalent to minimizing the ac-
tual cost with a relaxation since the real optimal per-
mutation is unknown. Moreover, removing the bi-
partite matching and graph similarity loss together
causes a significant performance drop. It demon-
strates the effectiveness of our training objectives
in capturing the reflections between syntactic con-
nections and relational facts. Finally, the model
without all three components produces the worst
performance, which proves that our model can suf-
ficiently explore syntactic information and improve
RTE performance.
4.6 Influence of Core Topology Size
To investigate the balance between pruning irrel-
evant contents and preserving informative edges,
we conduct experiments on the NYT test sets with
different core topology sizes, which stand for dif-
ferent ratios of graph reduction. The results are
illustrated in Figure 3. We observe that when the
size is too small, the performance drops signifi-
cantly. We hypothesize that too aggressive pooling
causes undesired pruning of informative contents,
or even merging of multiple entities, and hurts the
performance. When the topology size increases up
to the most extreme setting of no pooling, the noisy
content cannot be sufficiently eliminated thus leads
to performance degradation.4.7 Case Study
Figure 4 shows the comparison of the RelU-
Net andCR models on two exam-
ples. In the first example, the dependency tree it-
self contains conjunct patterns between the entities
“Ghana ”, “Togo ” and “ Angola ” and the patterns
are successfully preserved in the core topology. In
the second example, despite the redundancy of the
dependency structure between the entities “ Danny
Glover ” and “ San Francisco ”, the graph pooling
operations eliminate the irrelevant contents and the
corresponding connection in the core topology is
informative enough for the triple to be extracted.
We can observe that our model sufficiently explores
the syntactic information of the dependency trees,
while CRonly captures semantic information
and thus yields worse predictions than our model.
The above observations demonstrate the effective-
ness of our method in incorporating syntactic struc-
tures for relational triple extraction.
5 Conclusion
In this paper, we propose a unified framework to
incorporate syntactic structures of the sentence for
relation triple extraction. We propose a graph pool-
ing network to automatically prune the dependency
tree to a core topology and remove useless infor-
mation. We propose a symmetrical graph unpool-
ing network to integrate the syntactic interactions
underlying the core topology with the original sen-4215tence context. We also propose a bipartite graph
matching objective function to learn the reflections
between the core syntactic interactions and golden
relational facts. We conduct experiments on two
benchmark datasets, and the results demonstrate
the effectiveness of our method.
6 Limitations
There are two main drawbacks of our model. First,
our model employs multiple graph pooling, unpool-
ing, and convolution layers, resulting in high time
complexity and high computational resource de-
mand. It usually takes a longer time to achieve
the best performance compared to other models.
Second, the bipartite matching loss in Equation
9 is approximated. This may cause the values to
enter the saturation zone of the softmax function,
which will lead to the vanishing gradient problem.
Therefore, the instability and the slow convergence
during training are also limitations of our model.
Acknowledgement
We would like to thank the anonymous reviewers
for their valuable comments on this paper. This
work was supported by National Key Research and
Development Program of China under Grant num-
ber 2021ZD0113902 and the National Natural Sci-
ence Foundation of China under Grant numbers
U1936216 and U1936208. We would like to thank
the VMWare gift funding’s partial support to the
authors.
References42164217