
Chan-Jan Hsu, Hung-yi Lee, Yu TsaoNational Taiwan University, TaiwanAcademia Sinica, Taiwan
{r09946011, hungyilee}@ntu.edu.tw,
yu.tsao@citi.sinica.edu.tw
Abstract
Transformer-based models are widely used in
natural language understanding (NLU) tasks,
and multimodal transformers have been effec-
tive in visual-language tasks. This study ex-
plores distilling visual information from pre-
trained multimodal transformers to pretrained
language encoders. Our framework is in-
spired by cross-modal encoders’ success in
visual-language tasks while we alter the learn-
ing objective to cater to the language-heavy
characteristics of NLU. After training with a
small number of extra adapting steps and fine-
tuned, the proposed XDBERT (cross-modal
distilled BERT) outperforms pretrained-BERT
in general language understanding evaluation
(GLUE), situations with adversarial genera-
tions (SWAG) benchmarks, and readability
benchmarks. We analyze the performance of
XDBERT on GLUE to show that the improve-
ment is likely visually grounded.
1 Introduction
Transformer-based models are extensively used
in natural language understanding (NLU) tasks,
and some prominent pretraining strategies include
BERT (Devlin et al., 2019), RoBERTa (Liu et al.,
2019), ALBERT (Lan et al., 2020), and ELEC-
TRA (Clark et al., 2020). Despite their differences
in curating the learning objectives, they all utilize
text-based datasets only. In the real world, however,
humans can benefit from the visual modality when
acquiring knowledge from language; an obvious
example is learning visually grounded words, such
as colors and shapes.
Some studies have succeeded with visually
grounded information used in NLU. ViCo (Gupta
et al., 2019) learned visual co-occurrences in text
and reported superior performance to GloVe in
word analogy problems. Zhang et al. (2020) and
Huang et al. (2020) used images to boost transla-
tion performance in supervised and unsupervised
Bird: "A winged [MASK] 
    soars in the sky ."
"The [MASK]  formed a       
beautiful arc in the sky":
What is
a bir d?
CLIPBER TFigure 1: Humans can answer cloze questions and
match a word with an image, and the multi-views of a
word could be simulated by neural networks. While
BERT excels in masked word reconstruction, CLIP
(Section 3) specializes at image-text matching. The
two modalities have different collocations of concepts,
which incentivize joint learning from the two systems.
settings. Tan and Bansal (2020) reported improve-
ments over BERT on NLU by proposing the con-
cept of vokenization.
Another branch of research focuses on solving
multimodal downstream tasks such as visual ques-
tion answering and image retrieval. Li et al. (2019);
Lu et al. (2019); Su et al. (2020); Li et al. (2020)
trained visual-text transformers, while LXMERT
(Tan and Bansal, 2019) used different encoders for
text and image and a cross-modal encoder. Tan
and Bansal (2020) tested these models with general
language understanding evaluation (GLUE Wang
et al. (2018)) and found that the performance does
not exceed using BERT (Appendix A), drawing
the conclusion that vision-and-language pretrain-
ing on visually-grounded language dataset failed to
distill useful information for general NLU. CLIP
(Radford et al., 2021) utilizes contrastive loss to
reach SOTA on zero-shot image classification in a
retrieval fashion.
In this work, we establish the link between
pretrained multimodal transformers and visually-
grounded language learning. We devise a way
to distill visual information from components of
a pretrained multimodal transformer (CLIP text-
transfomer, abbreviated as CLIP-T) to pretrained479language transformers (BERT/ELECTRA), to in-
corporate versatile perception of words into the
model (Figure 1). The usage of a visually
grounded text-transformer as a teacher allows us to
implement straightforward and non-fuzzy adapting
tasks for distillation. We show that it is mathemati-
cally logical that the CLIP-T output approximates
visual features (Sec. 2.2), and also the linguistic
competence of CLIP-T is low (Sec. 3), to prove
that the distilled information is predominantly vi-
sual and thus non-trivial to the pretrained-language
transformer despite having textual inputs.
Methodologically, we use the cross-modal en-
coder structure inspired by Tan and Bansal (2019),
to concatenate the two models and further adapt
the ensemble for some extra steps (a lot fewer
than the original pretraining steps). While adapt-
ing pretrained-BERT, we favor a document-level
corpus (wiki103) over a vision-language corpus
(MSCOCO) due to claims from Devlin et al.
(2019)and results from Tan and Bansal (2020)
(Appendix A). The adapting tasks are joint masked
language modeling (MLM), same sentence predic-
tion, and CLIP token classification tasks, which
are resemblant of BERT pretraining tasks to cater
to the language-heavy characteristics of NLU. We
do ablation studies to show that each of the task
provides improvement (Section 5).
During finetuning, we finetune XDBERT (cross-
modal distilled BERT), which is the language en-
coder after adaptation. We evaluate the linguistic
capabilities of the model by finetuning on GLUE,
situations with adversarial generations (SWAG
(Zellers et al., 2018)) benchmarks, and readabil-
ity benchmarks. The resulting XDBERT outper-
forms pretrained BERT, proving that our adaptation
strategy distills useful visual knowledge into BERT
(right of Figure 2). We provide analysis to show
that the improvements are visually grounded.
We summarize our contribution as follow:
•We explore distilling visual information from
a pretrained multimodal transformer to a pre-
trained language transformer and improved
NLU performance.
•Our adapting method is efficient and exten-
sible to different combinations of pretrained-
language encoders (BERT/ELECTRA).2 Proposed Method
The training process consists of three phases: pre-
training, adaptation, and finetuning (Figure 2). Our
proposed method focuses on the adaptation phase
with pretrained models, so pretraining is not a
part of our experiment, but we explain all three
phases for completeness. The adaptation phase in-
corporates the cross-modal transformer structure to
jointly learn from CLIP-T and BERT outputs.
2.1 Model Architecture
The cross-modal transformer (middle of Figure 2)
consists of a cross-modal encoder, CLIP-T and
BERT. CLIP-T has the same module connections
as BERT with only parameter differences (specifi-
cations in Appendix B). The cross-modal encoder
consists of repeating cross-modal encoder layers,
which is an extension to single-modality encoder
layers (layers of BERT/CLIP-T) in Figure 3. The
added cross-attention module follows the attention
formula (Vaswani et al., 2017):
Attention output =softmax
Q∗K/√
D
V
(1)
for queries ( Q), keys ( K) and values ( V) of dimen-
sion D, however, Qis generated from a modality
other than KandV. We choose the number of
cross-modal encoder layers to be 2.
2.2 Pretraining
BERT is trained using the next sentence prediction
and masked language modeling. CLIP is an image-
text matching system with two components, a text
encoder (CLIP-T), and an image encoder (CLIP-
ViT), which learn to encode paired inputs to closer
output embeddings via contrastive loss. The trained
representation has the following properties:
cos(H, V)>> cos (H, V)(i̸=j) (2)
cos(H, V)>> cos (H, V)(i̸=j) (3)
where His the CLIP text encoder output of X,
andVis the CLIP image encoder output of Y. The
text-image input ( X,Y) is paired, and every ( X,
Y)(j̸=k)is a non-pair. Since HandVare
normalized and have a length of 1, Hcan be used
to approximate V. The similarity of HandVis
also shown in multi-modal arithmetic propreties
discovered in Tewel et al. (2021) Therefore, we
use the CLIP text encoder output to approximate
CLIP image encoder output for a straightforward
adaptation process.480
2.3 Adaptation
We define three adapting tasks that can be learned
in a self-supervised manner, which is visualized in
Figure 2. In these tasks, BERT and CLIP-T takes
sentences A and B respectively as input, and losses
are calculated from both BERT output and CLIP-T
output. Our adapting tasks closely follow BERT
text pretraining strategies to retain linguistic com-
petence. Unlike pretraining, the adaptation is com-
putationally inexpensive, as we found that training
1 epoch on wiki103 was already effective. Further
training details can be found in Appendix C.
2.3.1 Joint Masked Language Modeling
(MLM)
The MLM objective teaches the model to recon-
struct masked tokens. The masked ratio and
masked token replacement probabilities follow De-
vlin et al. (2019). Since there is no equivalent of a
[MASK] token in CLIP, we leave the sentence as
is.
2.3.2 Same sentence prediction (MATCH)
The Image-Text Matching (ITM) objective is
widely used in multimodal learning (Tan and
Bansal, 2020; Radford et al., 2021). We modify
this objective to same sentence prediction as both
streams of our model takes text as input. When
choosing the input sentences for BERT and CLIP-
T, we make the inputs nonidentical 50% of the
time. A binary classifier over [CLS] differentiates
between the two cases. This motivates the [CLS]
output to encode sentence related information, and
trains the cross-attention weights.
Cross-modality encoder layer
Single-modality encoder layer
Cross-
attentionSelf-
attentionFeed-
ForwardQ,K,V K,V
Q
2.3.3 CLIP Token Classification
This is the MLM objective done on the CLIP-T
side of the full model, omitting the masking part
because CLIP has no mask token. Same as MLM,
15% of the tokens are randomly selected for recon-
struction. We address concerns on trivial solutions
learned by the model in Section 5 and 9 in the
appendix.
2.4 Finetuning
Finetuning follows the methods described in De-
vlin et al. (2019), and is applied to the language
encoder only (XDBERT), therefore the number of
parameters are kept equal to pretrained-BERT.
3 Experimental Results
We evaluated our model on three NLU benchmarks,
namely GLUE, SWAG and READ. We tested our
adaptation strategy on three different language en-
coders coupled with CLIP-T, including BERT-base,
ELECTRA-base, and ELECTRA-large. We fix the
finetuning parameters between models where com-
parison is intended, and select the median result of481
multiple runs. Details of finetuning are provided in
Appendix C.
Table 1 shows experimental results. Each of our
XD-model constantly outperforms the original en-
coder (For fair comparison, we train the original
encoder with one more epoch of wiki103). We
found that performance gains are more significant
on smaller datasets (RTE, MRPC, STSB, CoLA),
indicating that visual features help increase general-
ization when the amount of training data is limited.
The gains are also significant on the readability
benchmark (READ).
We show that the results of finetuning CLIP-
T alone on GLUE does not perform well. Since
the language capability of the CLIP-T model is
weak, the distilled information obtained by XD-
BERT/XDELECTRA is predominantly visual.
It is also possible to finetune the entire cross-
modal transformer after adaptation. The perfor-
mance further increases but the model has more
parameters. The results are in Appendix C.3.
4 Analysis
To justify the use of a cross-modal encoder, we first
conducted a pairwise projection weighted canoni-
cal correlation analysis (PWCCA) on word embed-
dings. The PWCCA is a good measure to determine
how close the distributions of two vector groups
are to each other. The PWCCA results in Table 2
show low scores on both BERT/CLIP and ELEC-
TRA/CLIP before co-training, so the cross-modal
encoder is useful in learning from both distribu-
tions.
We inspect RTE, MRPC, and CoLA results
of 5 runs in detail to show that the improve-
ments are likely from visual information of CLIP-
T. Over the 5 runs, XDBERT-b has accumulated
+38 more correct classifications than BERT-b, or
+2.74%(38/5/277) gain in performance. MPRCSystems PWCCA
BERT/ELECTRA 0.5498
BERT/CLIP 0.4980
ELECTRA/CLIP 0.4645
BERT/RANDOM 0.3569
and CoLA show +0.3% and +0.9% gains in accu-
racy respectively, and translates to a larger gain
in performance with their original metric (MRPC
F1: +0.83%, CoLA Corr: +2.2%). We then sepa-
rate each of the glue datasets entries into two cat-
egories: entries that XDBERT-b improves classi-
fication over BERT-b, and entries of the opposite.
Entries where both models obtain the same per-
formance are set aside. Analyzing the separated
entries as a whole, we discovered that the better-
performing entries have a larger visually grounded
ratio (Figure 4), as the quartile, median and mean
values are generally higher for improved samples.
The enhancement of visually grounded token rep-482RTE MPRC STSB CoLA
MLM+MATCH+CLIPTC(proposed) 69.31 88.02 89.32 56.27
MLM+MATCH 70.04 86.93 88.8 54.62
MLM 68.23 87.25 89.29 54.78
1 cross attention layer 66.79 87.66 89.32 53.62
2 Epochs (2x) 69.31 88.04 89.31 55.91
20 Epochs (20x) 57.4 87.74 - -
wiki(14G), same steps as above 65.3 87.78 89.1 -
resentations is a rough indicator that XDBERT has
obtained distilled visual information from CLIP-
T. We show examples of each category in Ap-
pendix D.
5 Ablation study
We tried various combinations of adaptation tasks
and found out that using all three yielded the best
results. We also tried to reduce the number of
cross-modal encoder layers to one; however, no
further improvements were made upon the visually
grounded language encoder. Other experiments
include changing the number of layers in the cross-
modal encoder, training for longer, and swapping
to a much larger wiki (14G). Swapping to wiki
reduces potential overfitting from the 20 Epochs
setting trained on wiki103, as training for the same
amount of steps on wiki is less than 1 epoch. We
tested these changes on RTE, MPRC, STSB, and
CoLA on 5 random seeds, and the results are shown
in Table 3, where MLM refers to the joint MLM
objective, MATCH refers to the cross-modal match-
ing objective, and CLIPTC refers to the CLIP token
classification objective.
Besides experimental evidence, we also jus-
tify the CLIPTC loss via further analysis, as the
CLIPTC objective can theoretically be trivially
solved by identity mapping. Despite this possi-
bility, we find that the loss is crucial to cross at-
tention learning. Since we do not impose negative
hard samples from sampled sentences, the MATCH
objective can be solved sufficiently simply by guid-
ing the cross attention to focus on common trivial
words. With the CLIPTC objective, the diversity
of the input embeddings corresponding to different
tokens must be retained in the cross-modal encoder,
leading to more robust cross-modal attention. We
show comparisons of the attention maps generated
from the cross-modal encoders with a random se-quence from RTE in Table 9 in the Appendix to
verify this claim.
6 Conclusion
In this study, we explored using cross-modal en-
coders to distill visual information to BERT. We
adapted the model with multiple objectives, and
we were able to achieve improved performance on
NLU tasks. Our adaptation techniques are compu-
tationally inexpensive and straightforward. Further-
more, our method is language encoder agnostic, as
we show similar performance gains on XDELEC-
TRA.
Acknowledgements
This work was supported in part by Ministry of
Science and Technology (MOST), Taiwan, under
Contract 110-2223-E-002 -007-MY3.
References483
A Visual-Text Transformers Results on
NLU
We show the result of Visual-Text Transformers on
GLUE, reported by Tan and Bansal (2020) in Ta-
ble 7. All of the listed methods (except LXMERT)
have their text-transformers initialized from BERT.
The results show that multi-modal training for solv-
ing vision-language tasks does not improve the
performance of the models on natural language
understanding tasks.484BERT-b BERT-l CLIP
dim 768 1024 512
max_len 512 512 77
#layers 12 24 12
B Modeling sequences on CLIP
While BERT and CLIP have similar forwarding
mechanisms, the specifications of the transformer
architecture are different, resulting in challenges to
jointly model both models (Table 4).
Mismatching dimensions pose a problem in
cross-attention. We use a linear transformation
to generate Q,K, and Vof matching dimensions,
but clarify that this linear transformation layer ex-
ists in the original LXMERT setting where hidden
representations have unified dimensions.
We modify the input to address the mismatched
max_len of the two systems. In the joint MLM, we
used a fixed sequence length of 512 for the BERT.
However, the same cannot be done for CLIP as the
maxmum model sequence length is 77 for CLIP.
We found that most BERT sequences (>99%) of
length 512 encode into CLIP sequences of length
less than 693, so we pad the CLIP sequence to
length 693, and then split the CLIP sequence into 9
sub-sequences of length 77. Therefore, a batch of
inputs will contain BERT inputs of size (batch_size,
512) and CLIP inputs of size (batch_size, 9, 77).
The output was resized to (batch_size, 693) in the
cross-modal encoder. The issue is also present in
the finetuning phase, and the maximum sequence
length of GLUE and SWAG is 128; therefore we
used 2 blocks of CLIP sub-sequences to model it.
For bi-sequence classification tasks such as RTE
and MRPC, we ensure that separate sentences do
not use the same block in the CLIP encoder. There-
fore, uni-sequence classification tasks will have a
CLIP input size of (batch_size, 2, 77) and the bi-
sequence classification task will have a CLIP input
size of (batch_size, 4, 77).C Further Training Details
C.1 Adaptation
We use publicly available wiki103 and preprocess-
ing methods similar to Tan and Bansal (2020).
Wiki103 (500MB) is a subset of the Wikipedia cor-
pus consisting of only good and featured articles.
The adaptation of 1 epoch on wiki103 finished in
35 minutes on 8 V100s (BERT-base). We trained
for at most 20 epochs( 16k steps) and found that fur-
ther adaptation steps did not increase scores in early
epochs, and significantly decreased performance
in late epochs. We used the following parameters
for adaptation : learning rate = 1e-4, max_epoch =
40 (although we stopped early due to plummeting
performance), warmup ratio = 0.05
C.2 Finetuning
The learning rates are listed in Table 5.
base-sized large-sized
RTE,MRPC,STSB 1e-4 5e-5
others 2e-5 1e-5
We used a warmup ratio of 0.1, with a learn-
ing rate decay of 0.9, and trained the model for 3
epochs. We report the median results of 5 runs on
different random seeds, except for RTE, which is
unstable; therefore, we report the median results of
9 runs instead. The reproduce results of ELECTRA
on RTE and STSB are lower than values reported
by Clark et al. (2020) because we did not start from
an MNLI checkpoint.
C.3 Finetuning with Full Model
Since our cross-modal transformer itself is can also
be viewed as a language encoder, finetuning can
be done on the full model. This approach, how-
ever, adds extra parameters to pretrained-BERT,
so comparison with pretrained-BERT is not in-
tended, instead, we focus on showing the feasi-
bility of this approach. The number of additional
parameters is only a function of the hidden size in
BERT/ELECTRA, so when the language encoder
is large, the ratio of additional parameters is much
more insignificant. To simplify notations, we use X-
(language encoder) to represent the full model. The485number of parameters of the full model is shown
in Table 6 and the results on NLU tasks are shown
in Table 8.
model parameters
BERT-b / ELECTRA-b 109482240
XBERT-b / XELECTRA-b 202059009
ELECTRA-l 334092288
XELECTRA-l 442671617
D RTE Examples
We provide three RTE example of each type in
Figure4, and we choose extreme examples where
performance difference is huge over 5 runs for
both "Improved" and "Worsened" categories. We
follow Tan and Bansal (2020) to classify tokens
as visually-grounded if it is not a stopword and
has more than 100 occurrences in MSCOCO. In
the following examples, Bold words are visually-
grounded, while normal words are non-visually-
grounded. Words in brackets are stopwords and
does not count towards either category.
D.1 Improved : XDBERT outperforms BERT
Example1 :
Visually-grounded ratio : 11/(11+16) = 0.4074
BERT answered correctly : 0/5
XDBERT answered correctly : 5/5
hands across (the) divide (was) formed (in)
march 2001 (,) (and) one(of) (its) immediate
aims (was) (to) press (for) (more) freedom
(of) contact (and) communication right away
(between) (the) two parts (of) cyprus (,) (and)
(for) early progress towards (a) solution (to)
(’) (the) cyprus problem (’) (.)
cyprus (was) divided (into) two parts (in)
march 2001 (.)
Example2 :
Visually-grounded ratio : 4/(10+4) = 0.2857
BERT answered correctly : 0/5
XDBERT answered correctly : 5/5
(it) (is) hoped (that) women (,) (who) consti-
tute (more) (than) half (of) (the) population
(,) (will) vote (for) (other) women (and) en-
sure (that) (their) issues (are) represented (in)
parliament (.)
women (are) poorly represented (in) parlia-ment (.)
Example3 :
Visually-grounded ratio : 13/(13+17) = 0.4333
BERT answered correctly : 0/5
XDBERT answered correctly : 5/5
ho##dler claimed (there) (were) also irregu-
larities (in) (the) campaigns organized (by)
atlanta (for) (the) 1996 summer games (,) syd-
ney (for) (the) summer olympics (in) 2000
(and) salt lake city (for) (the) 2002 winter
games (.)
(before) salt lake city (,)winter olympic
games took place (in) naga ##no (.)
D.2 On Par : XDBERT and BERT perform
equally
Example1 :
Visually-grounded ratio : 6/(6+32) = 0.1375
BERT answered correctly : 0/5
XDBERT answered correctly : 0/5
(on) october 1 2001 (,) eu (and) (other) coun-
tries introduced (the) option (for) domestic
animal owners (to) apply (for) petpassports
(under) (the) pets travel scheme (() pets (for)
short ()) (,) (for) pets returning (from) abroad
(to) (the) united kingdom (.) (this) replaced
(the) old system (of) 6 months compulsory qu
##aran ##tine (for) (all) domestic pets (.)
(in) 2001 (,) (the) eu introduced (a) pass-
port(for) pets (.)
Example2 :
Visually-grounded ratio : 5/(5+16) = 0.2381
BERT answered correctly : 5/5
XDBERT answered correctly : 5/5
security forces (were) (on) high alert (after)
(an) election campaign (in) (which) (more)
(than) 1 (,) 000 people (,)including seven
election candidates (,) (have) (been) killed (.)
security forces (were) (on) high alert (after)
(a) campaign marred (by) violence (.)
Example3 :
Visually-grounded ratio : 8/(8+16) = 0.3333
BERT answered correctly : 5/5
XDBERT answered correctly : 5/5
(in) 1979 (,) (the) leaders signed (the) egypt
(-) israel peace treaty (on) (the) white house
lawn (.) (both) president begin (and) sad ##at
received (the) nobel peace prize (for) (their)486work (.) (the) two nations (have) enjoyed
peaceful relations (to) (this) day(.)
(the) israel (-) egypt peace agreement (was)
signed (in) 1979 (.)
D.3 Worsened : XDBERT underperforms
BERT
Example1 :
Visually-grounded ratio : 11/(11+29) = 0.2750
BERT answered correctly : 5/5
XDBERT answered correctly : 0/5
jean (-) claude tri ##chet (,) (the) european
central bank president (,) made (it)clear (,)
(on) wednesday (,) (that) (he) would oppose
un##war ##rant ##ed political attempts (to)
remove antonio fa##zio (:) (the) bank (of)
italy governor (,) engulfed (in) controversy
(over) (his) handling (of) bank takeover bids
(.)
antonio fa##zio (is) subordinate (to) jean
(-) claude tri ##chet (.)
Example2 :
Visually-grounded ratio : 11/(11+29) = 0.4167
BERT answered correctly : 5/5
XDBERT answered correctly : 0/5
(about) half (were) along (a) 20 (-) mile
stretch (of) santa monica bay(from) topanga
canyon boulevard (to) (the) palo sverde s
peninsula (.)
(the) coastline (of) santa monica bay(is)
50 miles long (.)
Example3 :
Visually-grounded ratio : 32/(32+55) = 0.3678
BERT answered correctly : 5/5
XDBERT answered correctly : 0/5
cairo (is) (now) home (to) (some) 15 mil-
lionpeople (-) (a) bu##rgeon ##ing popula-
tion (that) produces approximately 10 (,) 000
tonnes (of) rubbish per day (,)putting (an)
enormous strain (on) public services (.) (in)
(the) past 10 years (,) (the) government (has)
tried hard (to) encourage private investment
(in) (the) refuse sector (,) (but) (some) estimate
4(,) 000 tonnes (of) waste (is) left behind ev-
eryday(,) fest ##ering (in) (the) heat (as) (it)
waits (for) someone (to)clear (it) (up) (.) (it)
(is) often (the) people (in) (the) poor ##est
neighbourhoods (that) (are) worst affected (.)
(but) (in) (some) areas (they) (are) fightingback (.) (in) shu ##bra (,) one(of) (the) north-
ern districts (of) (the) city(,) (the) residents
(have) taken (to) (the) streets armed (with)
dust ##pan ##s(and) brushes (to)clean (up)
public areas (which) (have) (been) used (as)
public dump ##s (.)
15 million tonnes (of) rubbish (are) pro-
duced daily (in) cairo (.)487Diff. to BERT weight SST-2 QNLI QQP MNLI
VL-BERT 6.4e-3 90.1 89.5 88.6 82.9
VisualBERT 6.5e-3 90.3 88.9 88.4 82.4
Oscar 41.6e-3 87.3 50.5 86.6 77.3
LXMERT 42.0e-3 82.4 50.5 79.8 31.8
BERT/ViLBERT – 90.3 89.6 88.4 82.4488MLM+MATCH+VC MLM+MATCH
Cross-Encoder, Layer1, Head0 Cross-Encoder, Layer1, Head0
Cross-Encoder, Layer2, Head0 Cross-Encoder, Layer2, Head0489