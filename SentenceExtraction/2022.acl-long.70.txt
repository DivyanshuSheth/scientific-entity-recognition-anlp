
Linjuan Wu, Shaojuan Wu, Xiaowang Zhang, Deyi Xiong, Shizhan Chen,
Zhiqiang Zhuang,Zhiyong FengCollege of Intelligence and Computing, Tianjin University, Tianjin, China, 300350Tianjin University-Aishu Data Intelligence Joint Laboratory, Tianjin, China
Tianjin_Research@aishu.cn
{wulinjuan1997,shaojuanwu,dyxiong,shizhan}@tju.edu.cn
Abstract
Multilingual pre-trained models are able
to zero-shot transfer knowledge from rich-
resource to low-resource languages in machine
reading comprehension (MRC). However, in-
herent linguistic discrepancies in different lan-
guages could make answer spans predicted by
zero-shot transfer violate syntactic constraints
of the target language. In this paper, we pro-
pose a novel multilingual MRC framework
equipped with a Siamese Semantic Disentan-
glement Model (SDM) to disassociate seman-
tics from syntax in representations learned by
multilingual pre-trained models. To explicitly
transfer only semantic knowledge to the tar-
get language, we propose two groups of losses
tailored for semantic and syntactic encoding
and disentanglement. Experimental results on
three multilingual MRC datasets (i.e., XQuAD,
MLQA, and TyDi QA) demonstrate the effec-
tiveness of our proposed approach over models
based on mBERT and XLM-100.
1 Introduction
Multilingual pre-trained language models (PLMs)
(Devlin et al., 2019; Conneau and Lample, 2019;
Conneau et al., 2020) have been widely explored
in cross-lingual understanding tasks. However,
zero-shot transfer method based on multilingual
PLMs does not work well for low-resource lan-
guage MRC. Such multilingual MRC models could
roughly detect answer spans but may fail to pre-
dict the precise boundaries of answers (Yuan et al.,
2020).
In order to address this issue, existing methods
mainly resort to external resources. Based on the
finding that 70% of answer spans are language-
specific phrases (e.g., named entities, noun phrases)
in MLQA (Lewis et al., 2020), Yuan et al. (2020)
propose an additional language-specific knowledgeFigure 1: Relations between answer spans and syntactic
constituents. (a) An example from XQuAD (Artetxe
et al., 2020) where the ground-truth answer is a syntac-
tic constituent. (b) A case from BiPaR (Jing et al., 2019)
where the answer predicted by a model transferred from
English to Chinese violates syntactic constituent bound-
aries in the target language.
phrase masking (LAKM) task to enhance bound-
ary detection for low-resource languages. Liang
et al. (2021) present a boundary calibration model
stacked over a base sequence labeling module, in-
troducing a phrase boundary recovery task to pre-
train the calibration module on large-scale multi-
lingual datasets synthesized from Wikipedia doc-
uments. These two methods rely on external re-
sources, which are not always easily available.
As illustrated in Figure 1(b), the transfer model
may violate syntactic constraints for answer spans
in the target language (e.g., the predicted answer
"月光不住" crossing the boundaries of two sub-
trees). An intuitive assumption is that the major-
ity of answer spans respect syntactic constituency
boundaries (i.e., syntactic constraint, illustrated by
the case in Figure 1(a)). On four multilingual MRC
datasets, we use Stanford CoreNLPto collect syn-991
tax parse trees and calculate the percentages of
ground-truth answers that respect syntactic con-
stituent boundaries. As shown in Table 1, over 87%
of answer spans respect the syntactic constraint.
On the bilingual parallel MRC corpus BiPaR
(Jing et al., 2019), we have compared two MRC
models: a monolingual MRC model trained on
the Chinese data of BiPaR vs. an mBERT-based
MRC model trained on the English data of BiPaR
and adapted to Chinese via zero-shot transfer. For
questions where the monolingual model correctly
predicts the answer and respect syntactic constraint,
23.15% of them are incorrectly predicted by the
transfer model, and the predicted answers violate
the syntactic constraint, illustrated by the case in
Figure 1(b). This suggests that the source language
syntax may have a negative impact on the answer
boundary detection in the target language during
zero-shot transfer, due to the linguistic discrepan-
cies between the two languages.
However, linguistic discrepancies are diverse
and it is difficult to learn them. We hence pro-
pose to decouple semantics from syntax in pre-
trained models for multilingual MRC, transform-
ing the learning of linguistic discrepancies into uni-
versal semantic information. Specifically, we pro-
pose a Siamese Semantic Disentanglement Model
(SDM) that utilises two latent variables to learn
semantic and syntactic vectors in multilingual pre-
trained representations. As shown in Figure 2(a),
stacking a linear output layer for MRC over the
disentangled semantic representation layer, we
can fine-tune the multilingual PLMs on the rich-
resource source language and transfer only disen-
tangled semantic knowledge into the target lan-
guage MRC. Our model aims to reduce the negative
impact of the source language syntax on answer
boundary detection in the target language.
To disassociate semantic and syntactic informa-
tion in PLMs well, we introduce objective functions
of learning cross-lingual reconstruction and seman-
tic discrimination together with losses of incorpo-
rating word order information and syntax structure
information (Part-of-Speech tags and syntax parsetrees). We use a publicly available multilingual
sentence-level parallel corpus with syntactic labels
to train SDM.
To summarize, our main contributions are as
follows.
•We propose a multilingual MRC framework
that explicitly transfers semantic knowledge
of the source language to the target language
to reduce the negative impact of source syntax
on answer span detection in the target lan-
guage MRC.
•We propose a siamese semantic disentangle-
ment model that can effectively separate se-
mantic from syntactic information of multi-
lingual PLMs with semantics/syntax-oriented
losses.
•Experimental results on three multilingual
MRC datasets ( XQuAD, MLQA, and TyDi
QA) demonstrate that our model can achieve
significant improvements of 3.13 and 2.53 EM
points over two strong baselines, respectively.
2 Related Work
Cross-lingual/Multilingual Machine Reading
Comprehension Recent advances in multilin-
gual MRC evaluation datasets (Artetxe et al., 2020;
Lewis et al., 2020; Clark et al., 2020) trigger re-
search interests in multilingual and cross-lingual
MRC (Hsu et al., 2019; Cui et al., 2019; Yuan et al.,
2020; Liu et al., 2020; Huang et al., 2021; Wu
et al., 2021). Hsu et al. (2019) investigate cross-
lingual transfer capability of multilingual BERT
(mBERT) on MRC tasks and find that zero-shot
learning based on PLM is feasible, even between
distant languages, such as English and Chinese.
Various approaches have been proposed on top
of multilingual MRC based on PLMs. Cui et al.
(2019) propose a method that combines multilin-
gual BERT and back-translation for cross-lingual
MRC. In order to effectively leverage translation
data and reduce the impact of noise in translations,
Liu et al. (2020) propose a cross-lingual training ap-
proach based on knowledge distillation for multilin-
gual MRC. Yuan et al. (2020) present two auxiliary
tasks: mixMRC and LAKM to introduce additional
phrase boundary supervision into the fine-tuning
stage. Liang et al. (2021) propose a pre-trained
boundary calibration module based on the output992
of a base zero-shot transfer model, refining the
boundaries of initial answers.
Different from the above studies, we mainly con-
sider the impact of syntactic divergences between
the source and target language in zero-shot cross-
lingual transfer based on multilingual PLMs, and
attempt to disassociate semantics from syntax and
only transfer semantics to the target language.
Disentangled Representation Learning Re-
cently, there has been a growing amount of work
on learning disentangled latent representations in
NLP tasks (Zhang et al., 2019; Hu et al., 2017; Yin
et al., 2018). In this aspect, the most related work
to our syntax-semantics decoupling method is the
vMF-Gaussian Variational Autoencoder (VGV AE)
model proposed by Chen et al. (2019). It is a gener-
ative model using two latent variables to represent
semantics and syntax of the sentence, developed for
monolingual setting and trained with paraphrases.
It uses paraphrase reconstruction loss and a dis-
criminative paraphrase loss to learn semantic repre-
sentations and word order information for syntactic
representations. We adapt this model to multilin-
gual syntax-semantics disentanglement. We use
bilingual sentence pairs to train our model with a
cross-lingual reconstruction loss and semantic dis-
crimination loss. To better disentangle semantics
from complex and diverse syntax in multilingual
PLMs, we introduce two additional syntax-related
losses for incorporating POS tags and syntax trees.3 Approach
Figure 2 shows the architecture of our multilin-
gual MRC framework with the proposed siamese
semantic disentanglement model.
3.1 Multilingual MRC Framework
Our multilingual MRC framework consists of three
essential components: the multilingual PLM layer,
the siamese semantic disentanglement module, and
the linear output layer. The output representations
from the multilingual PLM are fed into SDM to
disassociate semantic and syntactic information.
Only the disentangled semantic representations are
input to the linear output layer for predicting an-
swer spans in passages.
In order to facilitate the zero-shot cross-lingual
transfer of only semantic knowledge from the rich-
resource source language to the low-resource target
language, we take a two-stage training strategy.
First, we pre-train SDM with parallel data (see
Section 3.2) while the parameters of the multilin-
gual PLM are frozen. Once SDM is trained, only
the output of source language MLP network is fed
into the linear output layer for MRC. In the second
step, we freeze the parameters of the SDM and
fine-tune the entire multilingual MRC framework
on MRC data of the source language.
3.2 Siamese Semantic Disentanglement Model
In SDM, we assume that a sentence xis generated
by a semantic and syntactic variable, i.e., yand
z, independently. We follow VGV AE Chen et al.993(2019) to use the von Mises-Fisher (vMF) distri-
bution for the semantic variable and the Gaussian
distribution for the syntactic variable. Formally, the
joint probability of the sentence and its two latent
variables can be factorized as:
p(x, y, z ) =p(y)p(z)p(x|y, z) (1)
where p(x|y, z)is a generative model consisting
of bag-of-words decoder.
The variational inference process of VG-
V AE uses a factorized approximated posterior
q(y|x)q(z|x) = q(y, z|x)with the objective
function that maximizes a lower bound of the
marginal log-likelihood:
L =L+ KL( q(z|x)||p(z))
+ KL( q(y|x)||p(y)),(2)
L=E
−logp(x|y, z)
(3)
where q(y|x)is subject to vMF (µ(x), κ(x))
while q(z|x)follows N(µ(x), diag (κ(x))).
The prior p(y)andp(z)follows the uniform dis-
tribution vMF (·,0)and a standard Gaussian dis-
tribution respectively. Eq.(3) is the reconstruc-
tion loss (RL) of the generator. In our model,
we adopt a multilayer perceptron (MLP) network
to learn the mean ( µ) and variance ( κ) of two
distributions. As pre-trained representations are
contextually-encoded token vectors, latent variable
vectors obtained by sampling from the distributions
need to be averaged so as to output sentence-level
semantic and syntactic vector.
Since SDM uses a Siamese network for both
the source and target language, the disentanglement
between semantics and syntax is conducted for the
two languages simultaneously with two parameter-
shared subnetworks, as shown in Figure 2(b).
We attempt to extract rich semantic information
from multilingual representations which is univer-
sal for multiple languages and contains less syn-
tactic information. Except for the conventional re-
construction loss, we propose two additional losses
on parallel data to encourage the latent variable y
to capture semantic information: a Cross-lingual
Reconstruction Loss (CRL) andSemantic Dis-
crimination Loss (SDL) . The former estimates
the cross-entropy loss when we use the semantic
representation yof the target language to recon-
struct the source input and use the source semantic
representation yfor target reconstruction. The
latter is used to force the learned source seman-
tic representation yto be as close as possible to
the target semantic representation ysince the se-mantic meanings of the parallel source and target
sentence is equivalent to each other. The two losses
are estimated as follows:
L=E
−logp(x|y, z)
+E
−logp(x|y, z)
,(4)
L= max
0, δ−sim(y, y) + sim( y, n)	
+ max
0, δ−sim(y, y) + sim( n, y)	
(5)
where sim(·,·)is a cosine similarity score function.
The margin δis a hyperparameter to control the gap
between parallel sentence pair (y, y)and two non-
parallel sentence pairs (y, n)and(n, y).nis
the semantic vector of a negative sample, which
has the highest cosine similarity to y. Specially,
as partial sentences in our corpus are parallel in
more than two languages, we limit the data range
of negative sampling to only 2-way parallel pairs.
nare obtained in the similar way to n.
In order to guide SDM to disassociate syntac-
tic information into the syntactic latent variable z,
we also define three losses tailored for capturing
different types of syntactic information. First, we
employ Word Position Loss (WPL) , defined as
follows:
L =E
−Xlog softmax( f(h))
,
(6)
where softmax( ·)indicates the probability of the
ith word at position i, and f(·)is a three-layer
feedforward neural network with input h= [e;z]
that is the concatenation of the syntactic variable
zand the embedding vector eof the multilingual
PLM for the ith token in the input sentence.
In addition, we define a Part-of-Speech and syn-
tax tree loss to encourage SDM to isolate deeper
syntactic information from pre-trained represen-
tations. POS tagging is a sequence labeling task,
which can be regarded as a multi-class classifica-
tion problem for each token in a sentence. Hence,
we define Part-of-Speech (POS) Loss as a cross-
entropy style loss as follows:
L=XX−log softmax( g(h))
(7)
where g(·)is a linear layer, softmax (·) esti-
mates the probability of gold POS tag class ,mis
the number of different POS tags.
For learning structural information, we design
Syntax Tree Loss (STL) . Many studies have found994that PLMs can encode syntactic structures of sen-
tences (Hewitt and Manning (2019); Chi et al.
(2020)). Inspired by Hewitt and Manning (2019),
we formulate syntactic parsing from pre-trained
word representations as two independent tasks:
depth prediction of a word and distance predic-
tion of two words in the parse tree. Given a matrix
B∈Ras a linear transformation, the losses of
these two subtasks are defined as:
L =X(∥w∥ − ∥ Bh∥), (8)
L =Xd(w, w)−d(h, h)(9)
where ∥w∥is the parse depth of a word defined
as the number of edges from the root of the parse
tree to w, and∥Bh∥is the tree depth L2 norm
of the vector space under the linear transformation.
d(w, w)is the number of edges in the path be-
tween the ith and jth word in the parse tree T. As
ford(h, h), it can be defined as the squared L
distance after transformation by B:
d(h, h) = (B(h−h))(B(h−h))(10)
To induce parse trees, we minimize the summa-
tion of the above two losses L andL ,
andL is defined as:
L=L +L (11)
According to the different syntactic tasks,
we train two SDM variants: SDM_POS and
SDM_SP (SP for syntactic parsing), where their
training objectives are defined as follows:
L=L +L+L+L +L,
L=L +L+L+L +L
3.3 Generalization Analysis
In this section, we analyze the generalization of our
decoupling-based multilingual MRC model.
By two reconstruction losses Eq.(3) and Eq.(4),
we will prove that the syntactic and semantic
vectors obtained by SDM are language-agnostic.
Since the mathematic structures of Eq.(3) and
Eq.(4) are the same, we take one part of Eq.(4)
for analysis. Due to zandyare independent of
each other, p(x, z|y) =p(x, z). We obtain:
E
−logp(x|y, z)
=E Xp(z)logp(z)
p(x, z|y)
= KL( p(z)||p(x, z))Similarly,
Minimizing KL(q(z|x)||p(z)) and
KL(q(y|x)||p(y))will eventually fit both
p(x, z)andp(x, z)into the same distribution.
In the same way, both p(x, y)andp(x, y)
also fit to the same distribution, no matter what
the target language is. This is consistent with our
motivation to use the siamese network.
Furthermore, the semantic discrimination loss in
Eq.(5) guarantees that the semantic vectors of the
source language and the target language are similar
to each other. Minimizing Eq.(5) can be equivalent
to:sim(y, y)>sim(y, n) +δ
sim(y, y)>sim(n, y) +δ
which is to maximize sim(y, y)to encourages the
target semantic vector to approach parallel source
semantic vector.
In summary, SDM can obtain language-
agnostic semantic and syntactic vectors. Therefore,
our multilingual MRC model is suitable even for
low-resource languages without training data for
the decoupling model.
4 Experiment
4.1 Datasets
To verify the effectiveness of our multilingual MRC
model, we conducted experiments on three multi-
lingual question answering benchmarks:
XQuAD (Artetxe et al., 2020) consists of 11
datasets of different languages translated from the
SQuAD v1.1 (Rajpurkar et al., 2016) development
set, including Spanish (es), German (de), Greek
(el), Russian (ru), Turkish (tr), Arabic (ar), Viet-
namese (vi), Thai (th), Chinese (zh), Hindi (hi),
and Romanian (ro).
MLQA (Lewis et al., 2020) consists of over 5K
extractive MRC instances in 7 languages: English
(en), Arabic (ar), German (de), Spanish (es), Hindi
(hi), Vietnamese (vi) and Chinese (zh). MLQA is
also highly parallel, with MRC instances parallel
across 4 different languages on average.
TyDi QA-GoldP is the gold passage task in
TyDi QA (Clark et al., 2020) covering 9 typologi-
cally diverse languages: Arabic (ar), Bengali (bg),
English (en), Finnish (fi), Indonesian (id), Korean
(ko), Russian (ru), Swahili (sw), Telugu (te). It995is a more challenging MRC benchmark as ques-
tions have been written without seeing the answers,
leading to 3 and 2 times less lexical overlap than
XQuAD and MLQA, respectively (Hu et al., 2020).
4.2 Baseline Models
We used the following two multilingual PLMs to
build our MRC model to conduct experiments:
mBERT is the multilingual version of BERT
Devlin et al. (2019), with 177M parameters, is pre-
trained on the Wikipedia of 104 languages to opti-
mize the masked language modeling objective.
XLM-100 uses a pre-training objective similar
to that of mBERT but with a larger number of pa-
rameters (578M) and a larger shared vocabulary
than mBERT, and is trained on the same Wikipedia
data covering 100 languages as mBERT.
Furthermore, we compared with a strong base-
line that uses external knowledge to enhance cross-
lingual MRC:
LAKM is a pre-trained task proposed in (Yuan
et al., 2020) by introducing external sources for
phrase-level masked language modeling task. The
external corpus contain 363.5k passages and 534k
knowledge phrases in four languages: English (en),
French (fr), German (de), and Spanish (es).
4.3 Setup
For SDM, we collected approximately 26k la-
belled parallel sentence pairs from the Univer-
sal Dependencies (UD 2.7) Corpus (Zeman et al.,
2020) as the training set. The training set covers 20
languages and overlap with 13 languages of three
MRC datasets. We used Universal POS tags and
HEAD tags in UD 2.7 for the POS tagging and
syntactic parsing task. We chose data from the Chi-
nese semantic textual similarity (STS) task (Tang
et al., 2016) as the development set. For hyper-
parameters in SDM, the learning rate was set to
5e-5, the margin δwas 0.4, and the latent variable
dimensions was 200.
For our multilingual MRC models and two base-
line models, we fine-tuned them on the SQuAD
v1.1 (Rajpurkar et al., 2016) and evaluated them on
the test data of the three multilingual MRC datasets.
For models based on mBERT, we fine-tuned them
for 3 epochs with a training batch size of 32 and a
learning rate of 2e-5. We fine-tuned models based
on XLM-100 for 2 epochs with a training batch
size of 16 and a learning rate of 3e-5.
4.4 Experiment Results
The overall experimental results are shown in
Table 2. All our tests were conducted under
the conditions of zero-shot transfer. Our mod-
els (SDM_POS, SDM_SP combined with XLM-
100 or mBERT) significantly outperform both
XLM-100 and mBERT baselines on three datasets.
SDM_SP achieves the best performance, indicat-
ing that the learning of deeper syntax information
is compelling. Especially, compared with baselines
on the TyDi QA-Gold dataset, SDM_SP based
on XLM-100 and mBERT gains 4.1%, 4.2% EM
improvements on average across 9 languages, re-
spectively.
The results of 12 languages in XQuAD and
MLQA are shown in Table 3. For cross-lingual
transfer performance, our models are better than
the two baselines in terms of either EM or F1 on all
11 low-resource target languages. On the MLQA
dataset, LAKM uses a larger extra corpus to train a
better backbone language model, while our method
with less external data can still achieve similar per-
formance in German (de) and Spanish (es).
The TyDi QA-GoldP dataset is more challeng-
ing than XQuAD and MLQA. The results of TyDi
QA-GoldP are shown in Table 4, and our models
are superior to the baselines in terms of either EM
or F1 for all 8 low-resource target languages. Sig-
nificantly, XLM+SDM_SP outperforms the XLM-
100 baselines by 8.4%, 9.5% in EM for Finnish (fi),
Russian(ru), respectively. The language families of
these two languages are different from that of En-
glish. The evaluation results on these three datasets
verify the effectiveness of our proposed method.
In Section 3.3, we theoretically analyze the
generalization of our model. The results on the
three datasets show the effectiveness on five lan-
guages not included in the training target lan-
guages for SDM. The five languages are Roma-
nian (ro), Vietnamese (vi) in XQuAD and Ben-
gali (bg), Swahili (sw), Telugu (te) in TyDi QA-
GoldP, which are resource-scarce and have differ-996
ent language families from English. Significantly,
mBERT+SDM_SP outperforms the mBERT base-
line by 13.6% in EM for Swahili (sw).
5 Analysis
5.1 Ablation Study
We further conducted an ablation study based on
the mBERT and VGV AE model with different
combinations of losses (introduced in the Sec-
tion.3.2). The results are shown in Figure 3.
Our mBERT+SDM_SP MRC model achieves the
strongest performance among all variants, surpass-
ing the model w/ all losses. According to the results
shown in Figure 3, we can summarize that each loss
is essential and suitable to our model.
The results without POS and STL loss (e.g., w/
CRL+SDL+WPL) on the MLQA dataset validate
the effectiveness of our losses (POS or STL loss)
tailored for capturing syntactic information. The
performance of models that only contain two losses
in CRL, SDL, and WPL drops significantly com-
pared with the w/ CRL+SDL+WPL model. The
results of models that only contain one of the losses
in CRL, SDL drop slightly, but the EM of the model
with only WPL is better than w/ CRL+WPL and
w/ SDL+WPL, which further demonstrates the im-
portance of the syntax-oriented loss. All ablation
models do not exceed our best model, illustrating
the importance of all proposed losses.997
5.2 Why Use a Siamese Network in SDM?
In order to separate semantic information from
PLMs, an alternative way is to train a single net-
work based on the VGV AE model as shown in Fig-
ure 4. Compared with SDM, the single-network
model does not use the CRL and SDL loss and only
requires labeled monolingual data. Corresponding
to SDM, there are also two single-network vari-
ants: SDM_single_POS and SDM_single_SP.
Since there is no explicit semantics learning across
the source and target language, we conjecture that
the single-network SDM will affect the quality
of learned semantic vectors and the degree of
semantics-syntax decoupling. As shown in Table 5,
the performance of the single-network SDM is
worse than the siamese-network model.
5.3 Why the SDM Works?
Our method mainly aims to reduce the potential
negative impact of syntactic differences of lan-guages in the zero-shot transfer process by explic-
itly isolating semantics from syntax in representa-
tions from multilingual pre-trained models. There-
fore, we hope to obtain multilingual semantic repre-
sentations with rich semantic information to guide
the machine to read and understand texts. In or-
der to examine (1) whether semantic vectors yin
SDM encode rich semantic information, and (2)
whether semantics is sufficiently separated from
syntax, and (3) whether semantic disentanglement
can improve predicted answer spans in matching
syntactic structures of the target language, we con-
ducted additional experiments and analyses.
Here we used three datasets of cross-lingual se-
mantic textual similarity (STS) in SemEval-2017
to evaluate the quality of semantic vectors learned
by SDM. The three datasets are for Arabic to
English (ar-en), Spanish to English (es-en), and
Turkish to English (tr-en) cross-lingual STS. We
report the results of our models in Figure 5 based
on mBERT. We also evaluated learned syntactic
vectors in cross-lingual STS, hoping that the per-
formance gap between semantic vectors (i.e., yin
SDM) and syntactic vectors (i.e., zin SDM) is
as large as possible. As shown in Figure 5, disen-
tangled semantic representations significantly im-
prove Pearson correlation over the baseline in ar-en,
es-en, and tr-en by 11.46%, 3.40%, 4.98%, respec-
tively. Additionally, disentangled syntactic repre-
sentations are negatively correlated to STS in most
cases. These results suggest that disentangled se-
mantic vectors indeed learn rich universal semantic
information.
We visualize hidden representations of the last
layer of mBERT and semantic representations of
mBERT+SDM_POS and mBERT+SDM_SP in
Figure 6, in which the parallel sentences are from998a 15-way parallel corpus (Conneau et al., 2018).
It is clear to see that disentangled semantic rep-
resentations learned by SDM make parallel sen-
tences in 15 languages (semantically equivalent to
each other) closer to one another in space, blend-
ing language boundaries clearly seen from mBERT
representations (Figure 6(a)). Combined with the
negative/positive results of syntactic/semantic vec-
tors in the cross-lingual STS task in SemEval-2017,
the visualization demonstrates that SDM can effi-
ciently disassociate semantics from syntax.
Finally, we evaluated the degree of consistency
to syntactic constituents of predicted answer spans.
As described in Section 1, 23.15% of the non-
transfer predicted correct answers violate syntactic
constraint of the target language during the raw
zero-shot cross-lingual transfer on BiPaR. By con-
trast, SDM_POS and SDM_SP drop this percent-
age to 12.98% and 6.60%, respectively. Moreover,
on the entire test set of BiPaR (Jing et al., 2019) in
Chinese, 93.27% answers predicted by SDM_SP
exactly span syntactic constituents, which is 8.14%
higher than the mBERT model.
6 Conclusions
In this paper, we have presented a novel multi-
lingual MRC model for zero-shot cross-lingual
transfer, which can disentangle semantic from syn-
tactic representations and explicitly transfer se-
mantic information from rich-resource language
to low-resource languages, reducing the influence
of syntactic differences between languages on the
answer span prediction of the target language.
To disassociate semantics from syntax in multi-
lingual pre-trained representations, we propose
the siamese semantic disentanglement model that
semantics/syntax-oriented losses to guide latent
variables to learn corresponding information. For
low-resource languages without training data for
the decoupling model, our theoretical analysis and
experiments verify the generalization of our mul-
tilingual MRC model. Further in-depth analyses
suggest that the proposed SDM can efficiently dis-
entangle semantics from syntax and significantly
improve syntactic consistency of answer predic-
tions on the target language after zero-shot cross-
lingual transfer.
Acknowledgments
The present research was supported by the Na-
tional Natural Science Foundation of China (NSFC)(61972455), the Joint Project of AISHU.com,
Bayescom, Zhejiang Lab (No. 2022KH0AB01)
and the Natural Science Foundation of Tianjin (No.
19JCZDJC31400). Xiaowang Zhang is supported
by the program of Peiyang Young Scholars in Tian-
jin University (2019XRX-0032).
References9991000