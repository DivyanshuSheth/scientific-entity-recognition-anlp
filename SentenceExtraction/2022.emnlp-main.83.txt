
Yichi Zhang Jianing Yang Jiayi Pan Shane Storks Nikhil Devraj
Ziqiao Ma Keunwoo Peter Yu Yuwei Bao Joyce Chai
Computer Science and Engineering Division, University of Michigan
zhangyic@umich.edu
Abstract
Recent years have seen an increasing amount
of work on embodied AI agents that can per-
form tasks by following human language in-
structions. However, most of these agents are
reactive , meaning that they simply learn and im-
itate behaviors encountered in the training data.
These reactive agents are insufficient for long-
horizon complex tasks. To address this limita-
tion, we propose a neuro-symbolic deliberative
agent that, while following language instruc-
tions, proactively applies reasoning and plan-
ning based on its neural and symbolic represen-
tations acquired from past experience (e.g., nat-
ural language and egocentric vision). We show
that our deliberative agent achieves greater than
70% improvement over reactive baselines on
the challenging TEACh benchmark. Moreover,
the underlying reasoning and planning pro-
cesses, together with our modular framework,
offer impressive transparency and explainabil-
ity to the behaviors of the agent. This enables
an in-depth understanding of the agent’s ca-
pabilities, which shed light on challenges and
opportunities for future embodied agents for
instruction following. The code is available at
https://github.com/sled-group/DANLI .
1 Introduction
Natural language instruction following with embod-
ied AI agents (Chai et al., 2018; Anderson et al.,
2018; Thomason et al., 2019; Qi et al., 2020; Shrid-
har et al., 2020; Padmakumar et al., 2021) is a
notoriously difficult problem, where an agent must
interpret human language commands to perform
actions in the physical world and achieve a goal. Es-
pecially challenging is the hierarchical nature of ev-
eryday tasks,which often require reasoning aboutsubgoals and reconciling them with the world state
and overall goal. However, despite recent progress,
past approaches are typically reactive (Wooldridge,
1995) in their execution of actions: conditioned
on the rich, multimodal inputs from the environ-
ment, they perform actions directly without using
an explicit representation of the world to facili-
tate grounded reasoning and planning (Pashevich
et al., 2021; Zhang and Chai, 2021; Sharma et al.,
2022). Such an approach is inefficient, as natural
language instructions often omit trivial steps that
a human may be assumed to already know (Zhou
et al., 2021). Besides, the lack of any explicit sym-
bolic component makes such approaches hard to
interpret, especially when the agent makes errors.
Inspired by previous work toward deliberative
agents in robotic task planning, which apply long-
term action planning over known world and goal
states (She et al., 2014; Agia et al., 2022; Srivas-
tava et al., 2021; Wang et al., 2022), we introduce
DANLI , a neuro-symbolic Deliberative Agent for
following Natural Language Instructions. DANLI
combines learned symbolic representations of task
subgoals and the surrounding environment with
a robust symbolic planning algorithm to execute
tasks. First, we build a uniquely rich semantic spa-
tial representation (Section 3.1), acquired online
from the surrounding environment and language
descriptions to capture symbolic information about
object instances and their physical states. To cap-
ture the highest level of hierarchy in tasks, we pro-
pose a neural task monitor (Section 3.2) that learns
to extract symbolic information about task progress
and upcoming subgoals from the dialog and action
history. Using these elements as a planning domain,
we lastly apply an online planning algorithm (Sec-
tion 3.3) to plan low-level actions for subgoals in
the environment, taking advantage of DANLI ’s trans-
parent reasoning and planning pipeline to detect
and recover from errors.
Our empirical results demonstrate that our de-1280
liberative DANLI agent outperforms reactive ap-
proaches with better success rates and overwhelm-
ingly more efficient policies on the challenging
Task-driven Embodied Agents that Chat (TEACh)
benchmark (Padmakumar et al., 2021). Importantly,
due to its interpretable symbolic representation and
explicit reasoning mechanisms, our approach offers
detailed insights into the agent’s planning, manipu-
lation, and navigation capabilities. This gives the
agent a unique self awareness about the kind of ex-
ceptions that have occurred, and therefore makes it
possible to adapt strategies to cope with exceptions
and continually strengthen the system.
2 Problem Definition
The challenge of hierarchical tasks is prominent in
the recent Task-driven Embodied Agents that Chat
(TEACh) benchmark for this problem (Padmaku-
mar et al., 2021). Here, language instructions are
instantiated as a task-oriented dialog between the
agent and a commander (who has comprehensive
knowledge about the task and environment, but can-
not perform any actions), with varying granularity
and completeness of guidance given. We focus on
the Execution from Dialog History (EDH) setting
in TEACh, where the agent is given a dialog history
as input, and is expected to execute a sequence of
actions and achieve the goal set out by the com-
mander. This setting allows us to abstract away
the problem of dialog generation, and focus on the
already difficult problem of instruction following
from task-oriented dialog.
As shown in Figure 1, a task, e.g., Make a Sand-wich, may have several subtasks that the agent must
achieve in order to satisfy the overall task goal. The
success of a task/subtask is achieved by meeting a
set of goal conditions , such as slicing a bread and
toasting two pieces. At each timestep, the agent
receives an egocentric visual observation of the
world and the full dialog history up to that time,
and may execute a single low-level action . Actions
can either involve navigation, e.g., to step forward,
or manipulation, e.g., to pick up an object. Ma-
nipulation actions additionally require the agent to
identify the action’s target object by specifying a
pixel in its field of view to highlight the object. The
execution continues until the agent predicts a Stop
action, otherwise the session will terminate after
1000 timesteps or 30 failed actions. At this time,
we can evaluate the agent’s completion of the task.
It is worth noting that while we focus on TEACh,
our approach is largely transferable between bench-
mark datasets and simulation environments, albeit
requiring retraining of some components.
3 A Neuro-Symbolic Deliberative Agent
An overview of our neuro-symbolic deliberative
agent is shown in Figure 2. We first introduce the
symbolic notions used in our system. We use the
object-oriented representation (Diuk et al., 2008)
to represent the symbolic world state. Each object
instance is assigned an instance ID consisting of
its canonicalized class name and ordinal. We de-
fine a state in the form of Predicate(Arguments)
as an object’s physical state or a relation to an-
other object. We define subgoals as particular
states that the agent should achieve while com-
pleting the task, represented by a symbolic form
(Patient, Predicate, Destination), where
thePatient andDestination are object classes,
and the Predicate is a state that can be applied to
thePatient . We define an action in the agent’s
plan as ActionType(Arguments) where each ar-
gument is an object instance.
To complete tasks, our agent reasons over a
learned spatial-symbolic map representation (Sec-
tion 3.1) to generate a hierarchical plan. At the
high level, it applies a neural language model to the
dialog and action history to predict the complete
sequence of completed and future subgoals in sym-
bolic form (Section 3.2). For each predicted sub-
goal, it then plans a sequence of low-level actions
using both the symbolic subgoal and world repre-1281
sentations online, with robustness to various types
of planning and execution failures (Section 3.3).
Next, we describe how each component works and
highlight our key innovations.
3.1 World Representation Construction
The reasoning process of an embodied AI agent
relies heavily on a strong internal representation of
the world. As shown in Figure 2, we implement
the internal representation as a semantic map in-
corporating rich symbolic information about object
instances and their physical states. We introduce
our methods for the construction of this representa-
tion in this section.
3D Semantic Voxel Map As the agent moves
through the environment while completing a task,
it constructs a 3D semantic voxel map to model its
spatial layout. Following Blukis et al. (2022), we
use a depth estimator to project the pixels of ego-
centric observation images and detected objects to
3D point cloud and bin the points into 0.25mvox-
els. The resulting map can help symbolic planner
(Section 3.3) break down high-level navigation ac-
tions, such as GOTO Knife_0 , to atomic navigation
actions such as Forward, TurnLeft, LookUp .
Object Instance Lookup Table Everyday tasks
can involve multiple instances of the same object,
and thus modeling only object class informationmay be insufficient.As shown in the internal rep-
resentation update part of Figure 2, we store object
instance information for a single task episode in a
symbolic lookup table, where each instance in the
environment is assigned a unique ID once observed.
These symbols in the lookup table become the plan-
ning domain of the symbolic planner (Section 3.3).
To collect this symbolic lookup table, we use a
panoptic segmentation modelto detect all object
instances in the current 2D egocentric visual frame.
These 2D instance detections are then projected
into the 3D map, and we use each instance’s 3D
centroid and size information to match and update
existing object instances’ information in the lookup
table. As the agent moves through the scene and
receives more visual observations, the symbolic
lookup table becomes more complete and accurate.
Physical State Prediction Additionally, tasks
can hinge upon the physical states of particular ob-
ject instances. For example, when making coffee,
the agent should disambiguate dirty and clean cof-1282fee mugs and make sure to use the clean mug. To
recognize the physical state of each object instance,
we propose a physical state classification model
where inputs include the image region of a detected
object instance and its class identifier, and the out-
put is physical state labels for the instance. As
classifying the physical state from visual observa-
tion alone can introduce errors, we also incorporate
the effect of the agent’s actions into physical state
classifications. For example, the isToggledOn at-
tribute is automatically modified after the agent
applies the ToggleOn action, overriding the classi-
fier’s prediction.
3.2 Subgoal-Based Task Monitoring
Due to the hierarchical nature of tasks, natural lan-
guage instructions may express a mix of high-level
and low-level instructions. In order to monitor and
control the completion of a long-horizon task given
such complex inputs, we first model the sequence
of high-level subgoals , i.e., key intermediate steps
necessary to complete it.
As shown in Figure 3, we apply a sequence-to-
sequence approach powered by language models to
learn subgoals from the dialog and action history.
At the beginning of each session, our agent uses
these inputs to predict the sequence of all subgoals.
Our key insight is that to better predict subgoals-
to-do, it is also important to infer what has been
done . As such, we propose to additionally predict
the completed subgoals, and include the agent’s
action history as an input to support the prediction.
To take advantage of the power of pre-trained
language models for this type of problem, all in-
puts and outputs are translated into language form.
First, we convert the agent’s action history into syn-
thetic language (e.g. PickUp(Cup) →“get cup” ),
and feed it together with the history of dialog ut-
terances into the encoder. We then decode lan-
guage expressions for subgoals one by one in an
autoregressive manner. As the raw outputs from
the decoder can often be noisy due to language
expression ambiguity or incompleteness, we add
anatural-in, structure-out decoder which learns
to classify each of the subgoal components into
its symbolic form, and transform them to a lan-
guage phrase as decoder input to predict the next
subgoals.
3.3 Online Symbolic Planning
Symbolic planners excel at generating reliable and
interpretable plans. Given predicted subgoals and a
constructed spatial-symbolic representation, PDDL
(Aeronautiques et al., 1998) planning algorithms
can be applied to generate a plan for each subgoal.
These short-horizon planning problems reduce the
chance of drifting from the plan during execution.
Nonetheless, failures are bound to happen during
execution. A notable advantage of our approach is
the transparency of its reasoning process, which not
only allows us to examine the world representation
and plan, but also gives the agent some awareness
about potential exceptions and enables the develop-
ment of mechanisms for replanning. In this section,
we introduce several new mechanisms to make on-
line symbolic planning feasible and robust in a
dynamic physical world.
Finding Unobserved Objects The agent’s par-
tial observability of the environment may cause
a situation where in order to complete a subgoal,
the agent needs an object that has not been ob-
served yet. In this case, a traditional symbolic
planner cannot propose a plan, and thus will fail
the task. To circumvent this shortcoming, we ex-
tend the planner by letting the agent search for the
missing object(s). Specifically, during planning,
our agent assumes that all objects relevant to sub-
goals exist, mocking full observability. If it has1283not observed any instance of an object required
to satisfy the subgoal, then the agent plans with
an assumed dummy object instance, which is as-
signed an additional state predicate unobserved .
By incorporating these dummy objects, a plan can
still be produced. For example, if the subgoal
is(Mug,isPickedUp) but the agent has not ob-
served a mug, the plan will become Search(Mug),
GoTo(Mug), PickUp(Mug) . During execution, the
agent is equipped with a dedicated Search action,
which lets the agent explore the environment un-
til an instance of the target object class has been
observed and added to the world representation.
Search Space Pruning Large object and state
search space slows symbolic planner down and cre-
ates unbearable latency to the system, as also found
in Agia et al. (2022). We propose to solve this prob-
lem by pruning the search space to only include the
relevant object instances. First, the planner finds all
relevant object types for a subgoal predicate. For
example, if the subgoal is to cook an object, then
all object classes that can be used to cook (e.g.,
Microwave, Stove Burner, Toaster, etc.) are seen
as relevant, even if the class is not explicitly stated
in the subgoal. Once relevant object classes are
determined, the agent collects relevant instances
of those classes. If there are other instances tied
to the state of an instance, they are also deemed
relevant. For example, if an object is inside of a
receptacle, that receptacle object instance is also
deemed relevant in the search space. All irrelevant
instances are then discarded to speed up planning.
Action Failure Recovery The ability to detect
exceptions is critical, as the agent’s actions may
fail for unexpected reasons. For example, an agent
may try to place an object in a receptacle, but the
action fails because the receptacle is full. We can
address this by applying exception-specific strate-
gies; in this case, the agent should try to clear out
the receptacle first. Figure 4 shows a decision tree
of recovery mechanisms that can be used to handle
various types of these exceptions. If recovery fails,
this replanning process repeats until a step limit
threshold is hit, at which point the agent gives up
and moves on to the next subgoal. While we man-
ually define recovery strategies as an initial step,
other ways to acquire or learn these strategies can
be explored in the future.
4 Experiments And Result Analysis
4.1 Experimental Setup
We follow the evaluation methods originally pro-
posed for TEACh, reporting metrics for task com-
pletion success and efficiency. First, task success
rate is the proportion of task sessions for which
the agent met all goal conditions. Goal condition
success rate is more lenient, measuring the aver-
age proportion of goal conditions that were met
in each session. Each metric has a path length
weighted (PLW) counterpart, where the agent is
penalized if it takes a longer sequence of actions
than the human-annotated reference path. Given a
metric M, the PLW version of Mis calculated by
M =∗M, where ˆLandLare the
lengths of the predicted and reference path, respec-
tively. There are also seen and unseen divisions for
each split, where seen and unseen indicate whether
the rooms in that division appear in the train set.
Implementations Our sequence-to-sequence
subgoal predictor is implemented by the BART-
Large language model (Lewis et al., 2020). For
depth estimation, we fine-tune the depth model
from Blukis et al. (2022) on TEACh. For object
instance detection, we use mask2former (Cheng
et al., 2022) as our panoptic segmentation model
and fine-tune CLIP (Radford et al., 2021a) to
classify physical states. Lastly, we use Fast
Downward (Helmert, 2006) for symbolic planning.
More details can be found in the Appendix.
Baselines We compare DANLI to several baselines:
•Episodic Transformer (ET) : End-to-end multi-
modal transformer model that encodes both dia-1284
log and visual observation histories, and predicts
the next action and object argument based on the
hidden state (Pashevich et al., 2021). We repro-
duced the results based on the official code.
•Hierarchical ET (HET) : A hierarchical version
of ET where low-level navigation actions are ab-
stracted out by a navigation goal (e.g. GoTo Mug ),
then predicted by a separate transformer.
•Oracle Navigator (HET-ON) : HET with the
navigator replaced by an oracle navigator that
can obtain the ground truth target object location
from the simulator’s metadata.
•DANLI -PaP : A version of DANLI where symbolic
planning is replaced by procedures as programs
(PaP), i.e., manually defined rule-based policies
following Zhou et al. (2021).
•JARVIS (Zheng et al., 2022): A neuro-symbolic
model similar to DANLI in spirit that also lever-
ages a language model for subgoal planning and
constructs maps for navigation. However, low-
level plans for each subgoal are manually defined
instead of being generated by a symbolic planner.
4.2 Overall System Performance
We compare our agent with baseline models on the
EDH task from TEACh in Table 1.
Success Rate DANLI consistently outperforms all
baseline models across both splits in every task and
goal condition success rate metric. It outperforms
HET-ON, the best reactive baseline equipped with
an oracle navigator, by a sharp margin, demonstrat-
ing the advantage of our world representation and
navigation planning. It also outperforms the de-
liberative DANLI -PaP baseline, showing the value
of employing existing mature automatic planning
tools in the system rather than handcrafting proce-
dural knowledge for the agent.
As shown in Table 2, there is a high variance
in the success rate of different task types, ranging
from 10.6% up to 57.4%. The inverse correlation
between the task success rate and the average num-
ber of task goal conditions indicates that longer-
horizon tasks are more challenging, as expected.
Efficiency For task completion efficiency, we
find that DANLI again outperforms all baselines in
PLW metrics, including HET-ON by over 10% in
PLW goal condition success rate. We further com-
pare the predicted trajectory lengths of our agent
with the HET baseline and ground truth human data
in Figure 5. DANLI outperforms HET in efficiency
by a large margin, with 26% of tasks surpassing hu-
man efficiency. These results show the superiority
of the deliberative agents in creating both accurate
and efficient plans for task completion.
4.3 Generalization to New Tasks
To demonstrate the generalizability of our ap-
proach, we additionally applied DANLI to the trajec-
tory from dialog (TfD) subtask of TEACh without1285
re-training any modules. The major difference be-
tween EDH and TfD is the form of inputs given
at each timestep. In EDH, the agent receives the
contextualized interaction history (including dia-
log and physical actions) up to the current time,
while in TfD, the agent is given the full session
dialog without context, and must infer actions to
complete the task induced by the dialog (analo-
gous to instruction following). As shown in Table
3,DANLI significantly outperformed both the ET
baseline and JARVIS, achieving success rates up to
about 8%, compared to a maximum of 1.8% from
baselines which are specifically trained on the TfD
data.
4.4 Analysis of Submodules
We perform further analysis to better understand
the contribution and failures from each component.
When a session fails, the agent logs a possible rea-
son based on its awareness of the situation. Figure
6 shows the distribution of the errors.
Subgoal prediction is a major bottleneck (ac-
counting for 23.3% of failures), since the agent
only takes actions for subgoals that are predicted
by this module. Most remaining errors are related
to planning and exception handling. Inability to
find a plan accounts for 13.5% of failures, since the
agent can only execute actions that it has planned.
Meanwhile, 17.6% of failures are caused by not
finding an instance of a planned target object after
searching for it. Interaction and grounding failures,
where interaction with the virtual environment fails
and the planner’s exception handling mechanisms
fail to resolve the issue, account for a respective
13.6% and 13.5% of errors. Other types of failures
are categorized as others , which are typically due
to inaccuracies in the agent’s world representation.
As most system failures are caused by the sub-
goal predictor and planner (including built-in ex-
ception handling techniques), we will discuss these
modules’ performance in terms of module-specific
results (Tables 4-6, Figure 7), and their ablated
contribution to the end task performance (Table 5).
4.4.1 Subgoal-Based Task Monitoring
We conduct ablations on our subgoal-based task
monitor from the perspective of subgoal prediction
accuracy and overall task completion.
Subgoal Prediction Accuracy In Table 4, we
examine the subgoal prediction accuracy of our
subgoal prediction module (shown in Figure 3).
First, we find conditioning on action history in ad-
dition to the dialog history achieves much higher
prediction accuracy, suggesting this is a strong sig-
nal for the model to infer the progress and pre-
dict which subgoals still need to be completed in
the task. Further, we find that converting actions
from unnatural symbols into language improves
the prediction performance, showing the advantage
of translating symbols to natural language inputs
when using pre-trained language models. Finally,
we show that conditioning on the model’s predicted
completed subgoals also improves the performance
(as opposed to inferring them implicitly).
Task Completion In the second row of Table 5,
we see the impact of monitoring all subgoals rather
than the current one on the end task performance.
In 6 out of 8 metrics, DANLI , which learns to predict
all subgoals throughout the task session, achieves
a significant improvement. A possible explanation1286
of the improvement is that some subgoals can only
be addressed when a previous one is completed,
while not all subgoals are strictly conditioned on
previous ones.
4.4.2 Online Symbolic Planning
We also conduct ablations for our online planner,
specifically in its capabilities for search space prun-
ing and action failure recovery.
Finding Unobserved Objects The planner de-
pends heavily on the constructed spatial-symbolic
map, and errors here may cause errors in scene nav-
igation, especially when target objects are hidden in
the environment. As 17.6% of failures are caused
by not finding the target object, we made a further
analysis on our agent’s object search performance.
We found that DANLI performs at least one object
search action in 11.8% of subgoals across 21.7%
of the game sessions. Among these sessions, the
success rate is 3.7%, which is far below the over-
all success rate of 17.3%. This indicates object
search is indeed a big performance bottleneck. To
improve performance, future work may develop
stronger methods to locate missing objects.
Search Space Pruning As seen in Table 6, scene
pruning sharply improves both planning success
(i.e., the rate at which formed PDDL problems
had a solution generated for them) and the average
runtime of the planner, which greatly speeds up
online execution. Table 5 also depicts task and goal-
condition success rates for our model without scene
pruning, showing that scene pruning contributes to
DANLI ’s success, and that improving PDDL plan
search complexity improves overall task success.
Action Failure Recovery DANLI ’s exception han-
dling mechanisms enable it to identify and recover
from exceptions to prevent some types of potential
task failures. These mechanisms work in symbolic
space, and are fully interpretable. In Figure 7, we
show the distribution of exception types the agent
encountered and how it attempted to recover. A ma-
jor source of exceptions is the mismatch between
the agent’s perceived internal world state to the
real situation. This is because the internal state
is updated per timestep, while a generated plan
remains unchanged unless it is not applicable any-
more. Typical cases include failure to ground to the
interaction target, being blocked by an obstacle due
to the change of estimated destination, or attempt-
ing to navigate to an object that no longer exists.
We find that replanning can handle about half of
these failure cases. Within these cases, our pro-
posed mechanisms for addressing exceptions with1287receptacle occupancy and object reachability con-
tribute to around 15% of recoveries. Meanwhile,
when a manipulation fails (i.e., no effect observed)
and replanning does not help, our agent can only
handle about 12% of these cases. Future work can
apply further analysis and exception handling tech-
niques to mitigate this type of failure. As shown in
the last line of Table 5, replanning contributes up
to 2.4% of the overall task success rate.
5 Related Work
The topics most relevant to this work include: se-
mantic scene representation, hierarchical policies,
and symbolic planning.
Semantic Scene Representation A strong world
representation is necessary for spatial and task-
structure reasoning in embodied AI. Prior work
has explored using vectorized representation (Pa-
shevich et al., 2021; Suganuma et al., 2021; Suglia
et al., 2021; Khandelwal et al., 2022), 3D scene
graphs (Armeni et al., 2019; Kim et al., 2020; Li
et al., 2022; Agia et al., 2022) and semantic map
(Chaplot et al., 2020; Saha et al., 2021; Blukis et al.,
2022; Min et al., 2022). We follow the latter ap-
proach. But unlike prior work, our semantic map
additionally models object instance andphysical
state to inform planning, and enables the use of
off-the-shelf symbolic planners.
Hierarchical Policies Hierarchical policy learn-
ing and temporal abstraction have long been stud-
ied in reinforcement learning (Dietterich, 2000;
Sutton et al., 1999; McGovern and Barto, 2001;
Konidaris et al., 2012; Daniel et al., 2016; Kulkarni
et al., 2016; Bacon et al., 2017) to uncover the in-
herent structures of complex tasks. Recently, such
hierarchical approaches are gaining interest in nat-
ural language instruction following (Sharma et al.,
2022; Zhang and Chai, 2021; Zhou et al., 2021;
Blukis et al., 2022; Zheng et al., 2022). While
prior work applies planning at the high level for
subgoal prediction, a reactive approach is still used
for low-level action execution. As such, we build a
deeply deliberative agent by using specialized plan-
ners for interaction and navigation actions based
on our spatial-symbolic world representation, thus
enabling more reliable subgoal completion.
Symbolic Planning Symbolic planning has been
extensively studied in classical AI literature and ap-
plied in robotics, where entities, actions, and goals
are represented as discrete symbols (Aeronautiqueset al., 1998; Galindo et al., 2004; Kavraki et al.,
1996; Galindo et al., 2008). Recent work aims to
learn these symbols from language (She and Chai,
2017; Zellers et al., 2021; Silver et al., 2021; Mac-
Glashan et al., 2015) and vision (Lamanna et al.,
2021; Xu et al., 2022). Unlike these efforts, our ap-
proach combines both modalities by learning sym-
bolic task goals from natural language, and lever-
aging a persistent scene representation based on
visual object detection. Further, unlike past work,
we optimize our approach for embodied instruction
following by adding capabilities to search for miss-
ing objects, knowledgeably prune the search space,
and recover from failed actions.
6 Conclusion and Discussion
This paper introduced DANLI , a deliberative instruc-
tion following agent which outperformed reactive
agents in terms of both success rate and efficiency
on a challenging benchmark TEACh. The use of
explicit symbolic representations also supports a
better understanding of the problem space as well
as the capabilities of the agent.
Despite the strong results, we are still far away
from building embodied agents capable of follow-
ing language instructions, and we identify three ma-
jor bottlenecks that will motivate our future work.
First, the capability to handle uncertainty in percep-
tion and grounding of language is essential when
constructing symbolic representations for planning
and reasoning. We hope to explore tighter neuro-
symbolic integration in order to allow better learn-
ing of symbolic representations, while still keeping
the advantage of providing transparent reasoning
and decision-making in agent behaviors. Second,
the capability to robustly handle exceptions encoun-
tered during execution is vital. Humans proactively
seek help if they get stuck in completing a task,
while current AI agents lack this capability. Inte-
grating dialog-powered exception handling policies
is our next step to extend DANLI . Third, future work
needs to improve the scalability of symbolic repre-
sentations for use in novel situations. In this work,
the symbol set for predicates and subgoals is pre-
defined, which requires engineering effort and may
not be directly applicable to other domains. As re-
cent years have seen an exciting application of large
language models for acquiring action plans (Ahn
et al., 2022), we will build upon these approaches
for more flexible learning of action planning in an
open-world setting in the future.1288Limitations
As in many embodied agents which apply planning
algorithms, DANLI operates in a closed domain of
objects and actions, which requires manually spec-
ifying object affordances as well as state changes
caused by actions. This means that if faced with a
task involving new objects and/or predicates, the
agent will fail without updating the knowledge base
and re-training perception modules. Moreover, the
agent is tied to its action space in the Thor simula-
tor, and cannot easily acquire new actions. Future
work can address this by exploring the possibility
to acquire new objects and actions in this space,
including object affordances and action effects. In
addition, as most of the work currently is done in
the simulated environment, whether the learned
models can be successfully transferred to the phys-
ical environment remains a big question. Sim2real
transfer has not been well explored in this line of
work on embodied AI.
Furthermore, the evaluation applied to DANLI is
completely automatic and only measures task com-
pletion information. We do not evaluate the safety
of the agent’s movements, or the disruption of its
actions to the environment. This evaluation cannot
capture a human user’s trust and confidence to-
ward the agent’s behavior. Before such agents can
be brought from the simulator space into the real
world, extensive study will be required to ensure
the safety and respect of human users.
Ethics Statement
One main goal of the embodied agents in the sim-
ulated environment is to enable physical agents
(such as robots) with similar abilities in the future.
As such, one key ethical concern is the safety of
those physical agents, e.g., the need to minimize
the harm and damage they may cause to humans
and their surroundings. The current setup in the
simulated environment is not designed to address
safety issues, mostly based on task success. In the
future, we hope to incorporate such safety consid-
erations into the design of intelligent agents, by
means such as minimizing the occurrence of colli-
sions, performing more consistent and predictable
actions, and explicitly modeling human behaviors
to avoid harm. We also hope to see more datasets
and benchmarks emphasizing safety in their evalu-
ation.Acknowledgements
This work was supported in part by an Ama-
zon Alexa Prize Award, NSF IIS-1949634,
NSF SES-2128623, and DARPA PTG program
HR00112220003. The authors would like to thank
the anonymous reviewers for their valuable com-
ments and suggestions.
References128912901291
A Appendix
A.1 TEACh vs ALFRED
The ALFRED (Shridhar et al., 2020) dataset is a
feasible alternative to evaluate our method on, but
we do not conduct our evaluations on it in this
work. Here we provide some comparisons between
the TEACh and ALFRED datasets to suggest that
evaluating on TEACh is sufficient, due to its higher
complexity and difficulty, and because this dataset
necessitates a deliberative approach.
1.TEACh has many more state variations. For
example, the sink in an ALFRED environment
is always clear so the agent can always rinse
an object by placing it into the sink and tog-
gling the faucet on. In TEACh the sink might
be occupied so the agent may have to clear
up the sink first. Similarly, mugs are always
empty and clean (if it is not the target object
of a cleaning mug task) in ALFRED, while in
TEACh they can be filled with water or dirty.
The more physical state variations suggest that
the TEACh dataset poses more challenges in
understanding the situation.2.In TEACh the agent has to learn multiple
ways to fulfill one subgoal. As an example,
in ALFRED, the cooking task corresponds to
a rigid protocol of operating the microwave.
In TEACh, there are multiple ways of ma-
nipulating different kitchen utilities to cook,
such as either using a microwave or a pan to
cook a potato slice, using a toaster to make
toast, or boiling potatoes with either a pot and
stove or a bowl and microwave. This also
demonstrates TEACh’s sufficiency in terms of
planning complexity.
3.It is a known issue that solving a dataset syn-
thesized by an oracle planner may result in
reverse engineering of the data generation pro-
cess. As TEACh consists of human-human
interaction data, there is no concern that there
exists reverse engineering.
4.To monitor task progress in TEACh, the agent
has to infer what has been done before it can
predict future subgoals. In ALFRED, all lan-
guage instructions for the episode are pro-
vided at the beginning, so there is no need
to estimate progress. In particular, a reactive
agent suffers less in the less challenging AL-
FRED because it does not need to monitor
task progress and reason about its plan. A de-
liberative agent, on the other hand, shines in
TEACh.
A.2 System Overview
A detailed overview of all system components and
sub-components is provided in Figure 8.
A.3 Perception System Details
Our perception system consists of a panoptic seg-
mentation model, a depth estimation model and a
physical state estimation model. At each time t, af-
ter receiving the RGB image Ifrom the egocentric
vision, our perception system predicts egocentric
depth estimation I, panoptic segmentation I,
and corresponding state estimation Ifor each pre-
dicted objects in I. The output of this system
serves as the foundation for building the symbolic
representation for the agent,
We use main-stream pretrained models for
the standard depth estimation and panoptic seg-
mentation task and implement a custom Vision-
Transformer (Dosovitskiy et al., 2021) based model
for physical state estimation. A dedicated dataset,1292
TEACh-Perception, is collected for training these
models.
A.3.1 Data Collection
We collect the visual perception data by replaying
the TEACh dataset with additional filtering and
data augmentation. Just like in the main experi-
ment, we use the train split to build our training set,
validation seen and validation unseen splits for two
test sets.
We replay each of the games within the split and
record egocentric images frame by frame, with their
corresponding depth data, panoptic segmentation
and object physical states ground truth extracted
from the rendering engine. We find consecutive
frames within the game can be very similar, degrad-
ing the dataset quality tremendously. Therefore,
we propose to filter the frames on the fly. Namely,
we use ImageHashto calculate the similarity be-
tween the consecutive frames and only record the
frame if its hash differs from the previous recorded
frame by at least 3. To further improve dataset
quality, we call the simulator’s built-in lighting ran-
domization and material randomization methods
if the consecutive frames are similar by the afore-
mentioned criteria. Following this method, we col-
lect 179,144 annotated frames for the training set,
22,169 and 71,452 frames for the validation seenand unseen set respectively.
A.3.2 Model Design & Training
Depth Estimation We follow the customized U-
Net (Ronneberger et al., 2015) depth estimation
model introduced in (Blukis et al., 2022) and used
its weight to initialize our model. To save com-
putation, before passing Iinto the U-Net, we
first downsample I’s resolution from 900x900 to
300x300 and upsample the model’s output back to
900x900 in the end. The model was finetuned with
the same hyper-parameter for 30,000 steps on a
single Nvidia A40 GPU on our TEACh-Perception
dataset.
Panoptic Segmentation Mask2Former (Cheng
et al., 2022) is used as the panoptic segmentation
model. We use the Swin-B 21K modelin MMDe-
tection (Chen et al., 2019) for initialization, de-
crease the learning rate to 3×10and finetune it
for 80,000 steps on 8 Nvidia A40 GPUs.
Physical State Estimation As each object has
different affordances,we only predict the possi-
ble valid physical states for each object. For exam-
ple, we will only predict a physical state value for1293the attribute isFilledWithWater for objects that
areFillable , e.g., bowls and cups.
We implement a custom model for state estima-
tion tasks. The input consists of an object-centric
image with its object’s class id. After the im-
age passes through an image encoder and class
id through an embedding layer, two features are
concatenated and go through two fully-connected
layers to produce its state estimation. We use ViT-
B/32 (Dosovitskiy et al., 2021) with an additional
projection layer as the image encoder and initial-
ize its weights from CLiP (Radford et al., 2021b).
The projection layer is a linear layer followed by a
ReLU function, projecting the 512-dimension im-
age feature down to 128 dimensions. The class id
embedding has a dimension of 8. The following
2 fully-connected layers are with ReLU activation
function, each with output dimensions 128 and 3.
The model is then trained for 150,000 steps on our
TEACh-Perception dataset with a learning rate of
1×10.
Lastly, our system also requires predicting ob-
jects’ spatial relationship of whether or not one
object is on another object. We use a simple
rule-based algorithm to predict these spatial rela-
tions, which is, given object Ain class Cwith
bounding box B= (x, y, x, y), ob-
jectBin class Cwith bounding box B=
(x, y, x, y), where bounding boxes are in
absolute coordinates notation, the model predicts
Ais on Bif and only if 1.) from the prior recep-
tibility knowledge, it is possible for Cto be on
C, 2.)(x≤(x+x)/2≤x)∧(y≥
y)∧((y+y)/2≤y).
During experimentation, we find three factors
constraining state estimation accuracy and provide
solutions respectively. First, the object-centric im-
ages are extracted from Iby objects’ bounding
boxes, which include little information about its sur-
rounding. For example, to predict the isToggled
state of a cabinet, the spatial relationship between
the cabinet and the desk must be included in the im-
age. To solve this problem, we enlarge the bound-
ing box by 30% on each side to include more spatial
information. Secondly, we find when in distance,
some objects will be too small to extract meaning-
ful state information, hurting the accuracy of physi-
cal state estimation. Therefore, we filter out objects
whose corresponding bounding boxes are less than
1000 pixels large in the egocentric image before
passing into this model. Lastly, although whether
or not a StoveBurner is turned on is mostly re-
flected on the existence of fire flames above it,
the corresponding isToggled state is stored in the
Knob object controlling the StoveBurner . To solve
this misalignment between the visual feature and
the corresponding state, we manually move the
isToggled state from the knob to the correspond-
ingStoveBurner .
A.3.3 Results
We additionally decouple our panoptic segmenta-
tion model and physical state estimation model
from the whole system and evaluate them sepa-
rately. Note that we do not perform a dedicated
evaluation for the depth estimation model as it is
directly adopted from (Blukis et al., 2022) with
minimal change.
Panoptic Segmentation In Table 7, we report the
panoptic segmentation performance of our model
on both validation seen and unseen splits. The
significant performance drop on PQ of about 20%
when transferring from seen to unseen room setup
indicates the difficulty in building a more gener-
alized model, which we leave future works to ex-
plore.
Physical State Estimation In Table 8, we report
the performance of our model on both validation
seen and unseen splits. It is worth noting that al-
though our model can perform well on isDirty
state with an accuracy of over 95%, isToggled
state only has an accuracy of 77.0 % and 74.7%.
After sampling and inspecting data points and
the model’s corresponding predictions, we argue
that the performance gap between different states
mainly comes from the difference in the states’ vi-
sual saliency. For example, while isDirty state
affects almost every part of the object, the effect
ofisToggled is much more inconspicuous, e.g.
isToggled only affects the color of a small LED1294
light on the CoffeeMachine .
A.4 Map Construction and Navigation
We describe below the transformation procedure
of converting egocentric visual observations to a
3D semantic map. Inspired by Blukis et al. (2022),
we project the pixels from the panoptic segmen-
tation model, with the help of a depth estimation
model, to a 3D point cloud. The points in the point
cloud are then binned into 0.25mcubes, called
voxels . Note that throughout this conversion pro-
cess, object class information is preserved. The
resulting map is V∈[0,1], where Cis
the number of object classes and X, Y, Z are the
length, width and height of the map, respectively.
Given this 3D semantic voxel map, we can plan
navigation paths using a deterministic path planner
like the Value Iteration Network (VIN) introduced
in Blukis et al. (2022). The path produced can be
further converted to navigation action sequences,
which can be directly followed by the agent through
primitives. See Figure 9 for an example of map
construction and path planning results in a single
episode. When exploring for missing objects, the
agent randomly samples an object from the 5 far-
thest objects in its current world representation and
navigates to it. There is no specific timeout for
the search action, but each subgoal is limited to a
maximum of 200 action steps.
A.5 Instance Match Algorithm
We use the following algorithm to match object
instances from the current observation to instances
already existing in the symbolic instance lookup
table. If it is the first time that an instance is ob-
served, we will register it into the lookup table.
First, we group all the detections in the current ob-
servation based on its object type, and denote nas the count of object of type c. Then for each ob-
ject type, we get object instances of the same type
from the lookup table that is visible (at least one
voxel falls into the visibility mask) at the moment,
and denote the count as n. Ifn==n, we
match each pair of them to minimize the pair-wise
distance computed as the Euclidean distance be-
tween their centroids. If n> n, we match
up to ndetections to the existing instances, and
then register new instances into the lookup table. If
n< n, we will match up to ndetections
to the existing instances, and remove the remain-
ing instances. When a detection is matched to a
instance in the lookup table, we update its voxel
mask by performing a logical-or operation of the
projected voxel mask of the detection and the exist-
ing voxel mask. Thus different partial observations
of the same instance at different positions can get
merged.
A.6 Subgoal Prediction
A.6.1 Subgoal Labeling
To annotate the key subgoals for each task
type in TEACh, we define a minimal set of
object states that is essential for the comple-
tion of the task. For example, as previously
shown in Figure 2, (Knife,isPickedUp) and
(Bread,isSliced) are subgoals for making toast,
while (Knife,isPlacedTo,Counter) is not. This
is because while placing a knife on the counter may
often happen when making toast in the TEACh data,
it is not essential to achieve the goal conditions for
making toast. We then use an automatic subgoal
labeling algorithm to get these subgoal annotations
for the entire dataset.
A.6.2 Subgoal Annotation
We propose a method to automatically annotate
subgoals from the trajectories. Based on the meta-
data of task goal conditions and the oracle world
state change of each step recorded in the dataset,
we extract whether a goal condition is achieved at
every step. If there is a goal condition completed,
it is annotated as a subgoal being achieved in that
step. We show an annotation example in Figure 10.
A.7 Symbolic Planning Details
In this section we describe details of our symbolic
planning module, including how PDDL planning
works, the types of subgoal predicates we used in
our experiments, and pseudocode to describe how
our online planning pipeline works.1295
A.7.1 PDDL Planning
PDDL planners, such as FastDownward (Helmert,
2006) and ENHSP (Scala, 2020), require as inputs
problems , which symbolically specify the exist-
ing preconditions and desired goal, and domains ,
which describe the symbols that can be used and the
possible actions that can be planned over. However,
before forming PDDL problems, we first adjust the
scene that the agent works with. This serves to
speed up planning, as described in section 3.3.
The domain is formed via provided object and
action symbols along with manually encoded inter-
actions within the simulator. Primitive actions are
qualified with conditional effects , which allows one
to encode more complex, contextual interactions
into the domain; e.g. if a plate is moved underneath
a toggled faucet, then the planner should assumeits state is changed to isClean . Within the domain
one may also encode action costs, which specify
priorities for actions.
An example problem, which contains relevant
objects, initial conditions, and goal conditions,
looks like the following:
An example domain, which contains types of ob-
jects to plan with, predicates, and action schemas,
may look like:1296
Subgoal predicates Below we describe the sub-
goal predicates used specifically in our TEACh
experiments.1297Algorithm 1: Hierarchical Planning
Input: PDDL domain D, Context CFunction HierarchicalPlan( D,C): S←InitializeRepr (C) G←SubgoalPredictor (C) foreach g∈Gdo ▷PDDL Planning P←FormPDDLProblem (g,S,D) Π←PDDLPlanner (P,D) foreach a∈Πdo o, e←ExecuteAction (a) ife̸=None then ▷Error handling ife∈solvable then g←PlanRecovery (o) goto line 5 with g:=g else return with failure end end ▷Internal representation update S←UpdateRepr (o, S) end end returnEnd Function
A.7.2 Online Planning Algorithm
Our full planning algorithm, including the re-
planning functionality to recover from failed ac-
tions, is listed in Algorithm 1. The subprocesses
used in the algorithm are the following:
A.8 Implementation Details of Baselines
The architecture of the HET model is shown in
Figure 11. We use two transformer models to im-
plement the controller and the navigator respec-
tively. For the HET-ON model, the navigator is
replaced with an oracle navigator which can get
access to the ground truth environmental state for
reliable navigation. We conduct an extensive hyper-
parameter search over the model architecture and
optimization process. We find that using a 4-layer
Transformer with a hidden size of 768 and 12 atten-
tion heads obtains the best results on the validation
set (average of seen and unseen split). We find
the navigator achieves the best performance when
increasing the layer number from 4 to 6.
A.9 DANLI GUI Interface
Figure 12 show a snapshot of the GUI interface we
built for visualizing DANLI ’s intermediate represen-
tations.
A.10 Results on Original Validation Split
The original test splits of the TEACh dataset are
used for the SimBot Challenge,and are thus not
publicly available at the time of writing this pa-
per. To facilitate model evaluation, the authors of
TEACh re-split the original validation set into new
validation and test sets, which are used in this paper
as the standard evaluation protocol. In Table 9, we
additionally provide the results on the EDH subtask
of TEACh using the original validation splits. Fu-
ture work evaluated on the original split can refer
to these results when comparing with our model.1298