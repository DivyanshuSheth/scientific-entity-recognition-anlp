
Albert Websonand Ellie Pavlick
{albert_webson, ellie_pavlick}@brown.eduDepartment of Computer Science, Brown UniversityDepartment of Philosophy, Brown University
Abstract
Recently, a boom of papers has shown ex-
traordinary progress in zero-shot and few-shot
learning with various prompt-based models. It
is commonly argued that prompts help mod-
els to learn faster in the same way that hu-
mans learn faster when provided with task in-
structions expressed in natural language. In
this study, we experiment with over 30 prompt
templates manually written for natural lan-
guage inference (NLI). We ﬁnd that models
can learn just as fast with many prompts that
are intentionally irrelevant or even pathologi-
cally misleading as they do with instructively
“good” prompts. Further, such patterns hold
even for models as large as 175 billion parame-
ters (Brown et al., 2020) as well as the recently
proposed instruction-tuned models which are
trained on hundreds of prompts (Sanh et al.,
2021). That is, instruction-tuned models of-
ten produce good predictions with irrelevant
and misleading prompts even at zero shots. In
sum, notwithstanding prompt-based models’
impressive improvement, we ﬁnd evidence of
serious limitations that question the degree to
which such improvement is derived from mod-
els understanding task instructions in ways
analogous to humans’ use of task instructions.
1 Introduction
Suppose a human is given two sentences: “No
weapons of mass destruction found in Iraq yet.”
and “Weapons of mass destruction found in Iraq.”
They are then asked to respond 0 or 1 and receive a
reward if they are correct. In this setup, they would
likely need a large number of trials and errors be-
fore ﬁguring out what they are really being re-
warded to do. This setup is akin to the pretrain-and-
ﬁne-tune setup which has dominated NLP in recent
years, in which models are asked to classify a sen-
tence representation (e.g., a CLS token) into somearbitrary dimensions of a one-hot vector. In con-
trast, suppose a human is given a prompt such as:
Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “Given that “ no weapons of mass destruction found
in Iraq yet. ”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “”, is it deﬁnitely correct that “ weapons
of mass destruction found in Iraq. ”?”?”?”?”?”?”?”?”?”?”?”?”?”?”?”?”?Then it would
be no surprise that they are able to perform the task
more accurately and without needing many exam-
ples to ﬁgure out what the task is.
Similarly, reformatting NLP tasks with prompts
such as the underlined text above has dramatically
improved zero-shot and few-shot performance over
traditional ﬁne-tuned models (Schick and Schütze,
2021b; Le Scao and Rush, 2021; Sanh et al., 2021;
Wei et al., 2021). Such results naturally give rise to
the hypothesis that the extra prompt text included
within each input example serves as semantically
meaningful task instructions which help models
to learn faster, in the way task instructions help
humans to learn faster. This hypothesis is implic-
itly assumed by many and explicitly argued by
Mishra et al. (2021), Schick and Schütze (2021a),
and Brown et al. (2020).
While last years saw a gold rush of papers (sum-
marized in §2) that proposed automatic methods for
optimizing prompts, Logan IV et al. (2021) com-
pare a representative sample of these newly pro-
posed methods and report that Schick and Schütze
(2021b)’s manually written prompts still on aver-
age outperform the automatically searched prompts
across a range of SuperGLUE tasks (Wang et al.,
2019). Such ﬁndings suggest that expert-crafted
prompts are among the best, if not thebest, which
reinforces the above hypothesis that models beneﬁt
from meaningful instructions.
In this paper, we test this hypothesis by evaluat-
ing various models on NLI in zero-shot and few-
shot settings using more than 30 manually written
templates and 13 sets of LM target words for a2300total of over 390 prompts. We ﬁnd that in most
cases models learn identically as fast when given
irrelevant or misleading templates as they do when
given instructively good templates. Further, models
ranging from 235 million to 175 billion parame-
ters all exhibit this behavior, as do the instruction-
tuned models, which are trained on hundreds of
manually written prompts. While we conﬁrm Sanh
et al. (2021)’s ﬁnding that instruction tuning sub-
stantially improves the performance and robustness
of prompts, we also ﬁnd that instruction-tuned mod-
els can be, in some sense, too robust and less sensi-
tive to the semantics of the prompts, as compared
to their non-instruction-tuned equivalents. Finally,
models are much more sensitive to the choice of
the LM target words as opposed to the meaning of
the instruction templates. In sum, despite prompt-
based models’ dramatic improvement in zero-shot
and few-shot learning, we ﬁnd limited evidence
that models’ improvement is derived from models
understanding task instructions in ways analogous
to humans’ use of task instructions.
2 Related Work
2.1 Prompt-Based Models
At the time of writing, the terms “prompt tuning”
and “prompting” can refer to any one or combina-
tion of three approaches described below:
Discrete Prompts reformat each example
with some template text. For example, in a
sentiment analysis task, the template can be
{sent} In summary, the restaurant
is [prediction] , where the predicted mask
word is then converted to a class prediction by
a predeﬁned mapping, e.g., {“great” →positive,
“terrible” →negative}. The prompts can be
manually written (Schick and Schütze, 2021a;
Bragg et al., 2021) or automatically generated (Gao
et al., 2021b; Shin et al., 2020). This approach
typically tunes all parameters of the model, but
its few-shot performance can exceed that of very
large models (e.g., GPT-3 175B) despite using a
3 orders of magnitude smaller LM (Schick and
Schütze, 2021b; Tam et al., 2021).
Priming (a.k.a. in-context learning) prepends
kpriming examples to the evaluation example,
where each example is optionally wrapped in a
template such as Question: {sent} True
or false? {label} ... Question:
{sent} True or false? {label}
Question: {eval_sent} True orfalse? [prediction] . Notably, although
models see labeled examples, their parameters
do not receive gradient updates based on those
examples. Although this approach is intriguing,
Brown et al. (2020) report that it only performs
well on the largest GPT-3 model, the API of which
is costly and difﬁcult to use for academic research
(see Appendix B for details).
Continuous Prompts prepend examples with
special tokens, optionally initialized with word em-
beddings; but during learning, those tokens can be
updated arbitrarily such that the ﬁnal embeddings
often do not correspond to any real word in the
vocabulary (e.g., Lester et al., 2021; Li and Liang,
2021; Qin and Eisner, 2021). This approach often
efﬁciently tunes a much smaller set of model pa-
rameters, but these methods have not yet reported
success in few-shot settings. Moreover, foregoing
prompts as expressed in natural language makes it
much harder to study their semantics, and it is not
clear if continuous prompts serve as task-speciﬁc
instructions or simply more efﬁcient model param-
eters (see He et al., 2021 for a detailed analysis).
2.2 Analyses of Prompts
In this paper, we focus on discrete prompts because
we can manually write and control their wording
and semantics. We measure the effect of prompt se-
mantics by the model’s k-shot performance where
k={0,4,8,16,32,64,128,256}. This setup re-
sembles that of Le Scao and Rush (2021), but their
study focuses on comparing Schick and Schütze
(2021b)’s existing small set of prompts against tra-
ditional ﬁne-tuning over the training trajectories of
entire training sets, whereas our study focuses on
the few-shot learning trajectories among a much
more diverse set of prompts designed to test spe-
ciﬁc hypotheses about the effect of prompt seman-
tics on few-shot learning speed.
At a high-level, our ﬁndings contradict Mishra
et al. (2021)’s claim that models beneﬁt from elab-
orate instructions adapted from crowdsourcing an-
notation guides. But note that they deﬁne “instruc-
tions” more broadly as including priming examples,
and they ﬁnd that “GPT-3 beneﬁts the most from
positive examples, mildly from deﬁnition, and de-
teriorates with negative examples.” (p. 18). In other
words, if we ablate priming and narrow “instruc-
tions” to just the description of a task, we in fact
have the same ﬁnding that instructions are only
modestly beneﬁcial over no instructions (cf. our2301irrelevant templates). In a similar vein, concurrent
work by Lampinen et al. (2022) ﬁnds that other
components of a prompt such as explanations of
priming examples are helpful, but models are indif-
ferent to whether the instructions in fact describe
their tasks.
Finally, a growing body of concurrent work also
questions the degree to which models need mean-
ingful instructions (Khashabi et al., 2021; Prasad
et al., 2022). One particularly noteworthy ﬁnding
is that Min et al. (2022) show that models learn
just as well with incorrect labels as opposed to cor-
rect labels in priming, concluding that prompts are
helping models to learn the distribution of the input
text and space of possible labels (as opposed to
specifying instructions of the task).
3 Overall Setup
We implement a manual discrete prompt model-
which in essence is the same as that of Schick and
Schütze (2021b), except their implementation in-
cludes several augmentations such as self-labeling
and ensembling of multiple prompts for compet-
itive results. In order to focus on measuring the
effect of prompts themselves, our implementation
does not include those augmentations. Following
Sanh et al. (2021) and Wei et al. (2021), we evalu-
ate by a rank classiﬁcation of the target words.
Baseline Model In preliminary experiments, we
ﬁne-tuned and prompt-tuned BERT, DistilBERT,
RoBERTa, ALBERT, and T5 (Devlin et al., 2019;
Sanh et al., 2019; Liu et al., 2019; Lan et al., 2020;
Raffel et al., 2020; all implemented via Wolf et al.,
2020). Conﬁrming prior work (Schick and Schütze,
2021b; Tam et al., 2021), we ﬁnd that ALBERT
consistently yields the best performance, so we use
it as our baseline model.
To verify that our implementation is compara-
ble with prior work, Figure 10 reports the RTE
validation accuracy of our baseline model. At 32
shots, our implementation yields a median accu-
racy of 70.22% (mean = 69.29%, std. dev. = 6.3%),
which is comparable to the 69.8% reported by
Schick and Schütze (2021b). Further, Figure 10
conﬁrms Le Scao and Rush (2021)’s ﬁnding that,
while both ﬁne-tuning and prompt-tuning converge
to similar results when fully trained on the entire
set (n= 2490 for RTE), prompt-tuning yields the
largest improvement in the few-shot setting. Go-
ing forward, we focus on studying the few-shot
learning trajectory between 4 and 256 examples.Instruction-Tuned Model We additionally ex-
periment with T0, a recently proposed instruction-
tuned model which is trained on over 60 datasets
formatted with hundreds of manually written
prompts (Sanh et al., 2021). We experiment with
both sizes of T0 (3B and 11B), as well as their non-
instruction-tuned version, T5 LM-Adapted (Lester
et al., 2021), as a baseline.
Very Large Model Lastly, we experiment with
the largest GPT-3 (175B) via priming (a.k.a. in-
context learning). Although ﬁne-tuning is techni-
cally available, it is extremely limited by OpenAI’s
various quotas. See Appendix B for details on how
we circumvent challenges in reproducing Brown
et al. (2020)’s results.
Data NLI is a task where a model is asked to
classify whether one piece of text (the “premise”)
entails another (the “hypothesis”). We focus on NLI
because all T0 variants holds out all NLI prompts
and all NLI datasets in its training, which makes it
a fair comparison to other models in this paper.
We use Recognizing Textual Entailment (RTE,
Dagan et al., 2006, inter alios), a series of expert-
annotated NLI datasets. Speciﬁcally, we use the
SuperGLUE collection of RTE (i.e., RTE1, 2, 3,
and 5; all converted to binary classiﬁcation) and
report their validation accuracy for comparability
with prior work on prompts.
We also experiment with Adversarial NLI
(ANLI, Nie et al., 2020), Heuristic Analysis for
NLI Systems (HANS, McCoy et al., 2019), and
Winograd Schema Challenge (WSC, Levesque
et al., 2012), reported in Appendices G.2, K, and
L, respectively. We ﬁnd no qualitative difference
between their and the main RTE results except that
ANLI requires much larger number of shots be-
fore obtaining any above-random accuracy, as it is
designed to be a highly challenging set.
Random Seeds & Example Sampling All
experiments are run over the same set of 4 random
seeds. Within a given seed, all models see the
same set of examples. For instance, under seed
1, the 4-shot models see examples 550–553, the
8-shot models see examples 550–557, and so on.
Across different seeds, a different starting example
index is drawn. The exact training example indices
are also recorded in our GitHub repository for
reproducibility.2302Statistical Tests We use both ANOV A and its
nonparametric equivalent, the Kruskal–Wallis test.
After ﬁnding a signiﬁcant difference among multi-
ple categories of templates, we report pairwise sig-
niﬁcance with the independent two-sample t-test
and the Wilcoxon rank-sum test. We set α= 0.05
and apply the Bonferroni correction to account for
multiple comparisons. For all results reported in
this paper, both t-test and Wilcoxon agree.
4 Effect of Templates
Our research question is whether models under-
stand prompts as meaningful task instructions anal-
ogous to how humans would. For intuition, sup-
pose an experimenter provides a human annotator
with an informative instruction of a reasonably easy
task. If the annotator understands the instruction,
we expect them to perform better than when the
experimenter provides intentionally misleading in-
structions, makes irrelevant chitchat, or says noth-
ing at all. Accordingly, we write various prompt
templates that correspond to these different scenar-
ios and evaluate models’ performance with these
templates in zero-shot and few-shot settings.
4.1 Method
We write 5 categories of templates (Table 1), with
at least 5 templates for each category (10 for in-
structive):
•Instructive: how we would describe the NLI
task to a human who has never seen this task
before.
•Misleading-Moderate: instruct the models to
perform a task related or tangential to NLI
such that, if the model were to perform the
task as explicitly instructed, it would perform
poorly on NLI in general.
•Misleading-Extreme: instruct the models to
perform a task unrelated to NLI.
•Irrelevant: concatenate the premise, a sentence
unrelated to any NLP task, and the hypothesis.
•Null: concatenate the premise and the hypoth-
esis without any additional text.
See Table 1 for examples and Appendix F
for the full list. We use “prompt” to mean a
unique combination of a template and a pre-
deﬁned LM target word for each class label.
For example, {“yes” →entailment, “no” →
non-entailment} are the default targets for the
template {premise} Should we assume
that {hypothesis}? [prediction] . In
this section, to control for the effect of target words,
a template’s performance is always reported with
“yes”/“no” as its target words, which consistently
perform best. In Section 5, we control for the tem-
plates and study the effect of different target words.
We further control for punctuation, declarative vs.
interrogative templates, and the order of concate-
nation (always {premise} some template
text {hypothesis}[prediction] ).
After preliminary experiments, to avoid cherry
picking, all prompts reported in this paper were
written prior to evaluation, i.e., we do not allow
retroactively editing prompts for performance ma-
nipulations, except for an ablation study that explic-
itly studies the effect of punctuation (Appendix A).
4.2 Result
Irrelevant Templates We ﬁnd that models
trained with irrelevant templates learn just as fast
as those trained with instructive templates, with no
practical differenceat any number of shots (Fig-
ure 1). This is true for all models and all datasets
in our experiments, including the largest GPT-3
(Figure 2).2303
Misleading Templates There is no consistent re-
lation between the performance of models trained
with templates that are moderately misleading (e.g.
{premise} Can that be paraphrased
as "{hypothesis}"? ) vs. templates that are
extremely misleading (e.g., {premise} Is
this a sports news? {hypothesis} ).
T0 (both 3B and 11B) perform better given
misleading-moderate (Figure 3), ALBERT and
T5 3B perform better given misleading-extreme
(Appendices E and G.4), whereas T5 11B and
GPT-3 perform comparably on both sets (Figure 2;
also see Table 2 for a summary of statistical
signiﬁcances.) Despite a lack of pattern between
the two misleading categories, however, it is con-
sistent that each model exhibits signiﬁcantly better
performance on instructive templates compared to
at least one category of misleading templates.
Null Templates Models trained with null tem-
plates perform far worse than all other categories
of templates (see Appendix G for all null re-
sults). Here, we focus on ALBERT (an encoder-
only masked language model), which allows more
permutation of concatenation orders by placing
mask in the middle of sentences. We see that, al-
though null templates are much worse in aggregate,
some subset of them (e.g., {premise} [mask]
{hypothesis} ) are still able to learn nearly as
fast as the average instructive template after 32
shots (Figure 13).
Zero-Shot So far, we have focused on few-shot
results. At zero shots, all models (including GPT-3
175B) perform only marginally above random, ex-
cept the instruction-tuned T0. Thus, for our analysis
of zero shot performance, we focus on T0. Figure 4
shows that there is no practical difference between
the performance of T0 3B given instructive tem-
plates and either category of misleading templates.
T0 11B performs better, although it also shows no
practical difference between misleading-moderate
and instructive templates. Lastly, T0++ (trained on
more datasets than other T0 variants), is the only
model in this paper that shows statistically signiﬁ-
cantly different performance across all categories
of prompts. However, there remains the caveat that
it still performs arguably too well in absolute terms
with pathological prompts, which we discuss in the
next section.2304
4.3 Discussion
Recall that a common assumption in the literature
is that prompts require experts to clearly and cor-
rectly describe the task at hand (§1). In contrast,
Table 2 summarizes that, with the exception of
T0++ at zero shots, all models perform essentially
as well with some pathological prompts as they do
with proper prompts. Notably, despite being much
larger than its competitors, GPT-3 shows the same
patterns of behaviors, suggesting that mere scaling
does not address this issue. Meanwhile, the evi-
dence from instruction tuning is mixed. Although
Sanh et al. (2021) are right that instruction tuning
yields substantial improvement in performance aswell as robustness as measured by variance, T0 is
somewhat too robust and less sensitive to the se-
mantics of the prompts in terms of distinguishing
proper instructions from pathological ones, com-
pared to T5 of the same size in the few-shot setting
(Figure 2).
In the zero-shot setting, we do see that that
the largest model instruction-tuned with the most
datasets (T0++) improves a model’s sensitivity
to prompt semantics. This is a positive result,
but it comes with the caveat that there still exist
numerous examples of pathological prompts that
perform just as well as the proper ones do. To be
charitable to randomness in neural models, we hold2305
this study to a higher standard by comparing means
and medians among categories with statistical tests.
Nevertheless, for our research question, existence
proofs alone are still alarming. For example,
without any gradient update nor priming, it is
striking that out-of-the-box T0++ scores a high
accuracy of 78% with the extremely misleading
{premise} Is that grammatically
correct? {hypothesis} , the same accu-
racy as it achieves with a proper instruction
{premise} Are we justified in
saying "{hypothesis}"? If models were
truly classifying whether the text is grammatical, it
would have only scored 52.7% because RTE is writ-
ten by experts and all examples are grammatical.
Even templates that underperform the instructive
ones seem to be too good. For example, it is
difﬁcult to imagine a human scoring 72% zero-shot
with the prompt {premise} Inflections
are annoying and thank god that
Middle English got rid of most of
them. {hypothesis} for a nuanced task like
NLI.
5 Effect of Target Words
5.1 Method
In this experiment, we study the effect of different
LM target words given a ﬁxed template. We write
4 categories of targets, with at least 3 pairs of target
words for each category (except the singleton yes-
no category):
1.Yes-no: Model is expected to predict the
word “yes” for entailment and “no” for non-
entailment.
2.Yes-no-like: Semantically equivalent to yes-
no but using superﬁcially different words, e.g.,
“true”/“false”, “positive”/“negative”.
3.Arbitrary: Model is expected to predict arbi-
trary words that have no semantic relation to
the entailment task, e.g., “cat” for entailment,
“dog” for non-entailment.
4.Reversed: Model is expected to predict the
opposite of the (intuitive) yes-no and yes-no-
like labels, e.g., “no” for entailment, “yes” for
non-entailment.
See Appendix F.3 for the full list. Within the arbi-
trary category, in addition to the common anglo-
phone ﬁrst names as Le Scao and Rush (2021) use,
we also include word pairs with high semantic sim-
ilarity, low similarity, and pairs which are highly
frequent in the English language, but we ﬁnd no
consistent difference among these various subcate-
gories of the arbitrary category.
5.2 Result
For both ALBERT and T0, we ﬁnd that models
trained with yes-no targets learn a good deal faster
than those trained with yes-no-like targets and dra-
matically faster than those with arbitrary and re-
versed targets. For example, Figure 5 shows the
top-performing instructive template trained with
different target words. At 32 shots, the difference
between the median accuracies of “yes”/“no” vs.
“no”/“yes” is 22.2%, far larger than the effect size
of varying categories of templates in Section 4. Ag-
gregating over all combination of templates and2306targets, Figure 16 conﬁrms that the choice of target
words matter much more than the meaning of the
templates.
5.3 Discussion
The fact that models consistently learn slower with
arbitrary and reversed target words is a positive
result: this type of performance differential is con-
sistent with what we expect for models that are
correctly sensitive to the semantics of the words.
However, there are several important negative re-
sults in these experiments as well. First, the effect
of the target words overrides the semantics of the
overall prompt. Consider two kinds of template-
target combinations:
1.An irrelevant or misleading template + yes-no
targets, e.g., {premise} Does the
paragraph start with "the"?
[yes/no] {hypothesis}
2.An instructive template + arbitrary tar-
gets, e.g., {premise} Based on the
previous passage, is it true
that "{hypothesis}"? [cat/dog]
Figure 6 shows that combinations such as (1) often
dramatically outperform (2). However, (2) simply
requires ﬁguring out a mapping: “Reply ‘cat’ if en-
tailed and reply ‘dog’ if not entailed”. For humans,
this can be learned in a few shots, e.g., Ferrigno
et al. (2017) showed that adults can reach 60% ac-
curacy in 18 trialsfor an arbitrary map of {more
numerous →star shape, less numerous →diamond
shape} without receiving any language instructions.
In contrast, models under many arbitrary LM tar-
gets struggle to reach 60% median accuracy even
by 64 shots with instructive templates (Figure 6
green; Figure 5 red, purple).
Further, even given intuitive yes-no-like targets
such as “agree”/“disagree” and “good”/“bad”, mod-
els learn much slower compared to when given
“yes”/“no”. As Figure 5 (green vs. dark green) and
Figure 16 (ﬁrst vs. second x-axis group) show, there
exists a large performance gap between yes-no and
yes-no-like targets which is not closed until 256
shots. Moreover, when we try to help the models
by appending target hints such as “True or false?”
to the templates, performance often drops instead,
echoing Sanh et al. (2021) and Wei et al. (2021)’sﬁndings that including answer choices in input se-
quence make models perform worse for certain
tasks.
6 General Discussion
6.1 Summary and Interpretation
Our main research question is whether models un-
derstand prompts as meaningful task instructions
analogous to how humans would. Again, suppose
an experimenter provides a human annotator with
an informative instruction of a reasonably easy task.
If the annotator understands the instruction, we
expect them to perform better than when the ex-
perimenter provides misleading instructions, irrele-
vant instructions, or no instructions at all. Section 4
shows that the performance of most models is insen-
sitive to the difference between instructive and irrel-
evant templates, moderately sensitive between in-
structive and misleading templates, and highly sen-
sitive between instructive and null templates. Com-
paring to the effect of the templates, however, Sec-
tion 5 shows that models are much more sensitive
to the semantics of the target words: they learn far
slower with arbitrary or reversed target words as de-
sired. However, they are overly sensitive to seman-
tically equivalent yes-no-like words (i.e., perform-
ing much worse with “agree”/“disagree” than with
“yes”/“no”), and the choice of target words over-
ride the semantics of the templates (e.g., perform-
ing much better given a irrelevant template with
“yes”/“no” targets than with an instructive template
with arbitrary targets such as “cat”/“dog”).
Our main argument throughout the paper shares
the same logic as a recent line of studies (Sinha
et al., 2021; O’Connor and Andreas, 2021; Pham
et al., 2021; Gupta et al., 2021) which argue that
the fact that LMs achieve good performance un-
der ideal conditions is insufﬁcient to establish lan-
guage understanding because they also succeed
under pathological conditions (e.g., sentences with
shufﬂed word order) where humans fail catastroph-
ically.In other words, the fact that models are so
good at inferring the gold labels from pathologi-2307cal inputs casts major doubts on whether models
make inferences in any way that resembles how
humans make inferences. For our results, the fact
that models are so good at learning from patho-
logical instructions likewise casts major doubts on
whether models understand prompts as instructions
in any way that resembles how humans understand
instructions.
6.2 Alternative Interpretations and Future
Directions
As with any extrinsic evaluation, accuracy cannot
directly measure understanding. For example, a hu-
man could perfectly understand an instruction but
still, e.g., have the same accuracy with instructive
vs. irrelevant templates because the task itself is
too hard (a lack of competence) or because they for
some reason ignore the instructions (a lack of com-
pliance). We discuss these two possibilities below.
Lack of Competence This is primarily a con-
cern for non-instruction-tuned models at zero shots,
where all models perform only slightly above ran-
dom, and thus a lack of statistical signiﬁcance
among template categories is ambiguous as to
whether models lack understanding of NLI instruc-
tions vs. if models lack the competence in NLI per
se. This is why our study largely focuses on the few-
shot setting, where a lack of competence is less of
a concern, as models do competently achieve good
accuracies that are only moderately below the state-
of-the-art non-few-shot models.
Another counterargument is that maybe no mod-
els ever actually reason about if a premise entails a
hypothesis. Maybe they just always exploit spuri-
ous or heuristic features and, if only they were com-
petent in properly reasoning about entailment rela-
tions, then the meaning of NLI instructions would
matter. This argument is possible, although, ﬁrst, it
hinges on to what extent NLI (or any other behav-
ioral evaluation) can measure language understand-
ing, which is a complex debate beyond the scope
of this paper. Second, in preliminary experiments
(Appendix K), our models actually zero-shot trans-
fer reasonably well to HANS (McCoy et al., 2019),
a dataset designed to diagnoses models use of NLI
heuristics. Thus, it is unlikely that models are en-
tirely incompetent in reasoning about entailment
relations and solely rely on heuristics. Regardless,
further differentiating competence in understand-
ing task instructions vs. competence in tasks per se
is an important direction for future work.Lack of Compliance Another interpretation is
that irrelevant prompts perform the same as the in-
structive ones because models simply ignore the
prompts altogether. However, a lack of compliance
alone cannot explain our results. If models truly ig-
nore the prompts, we should not see any systematic
differences between any categories of prompts. In-
stead, we do see consistent patterns that instructive
and irrelevant templates make models learn signiﬁ-
cantly faster than misleading and null templates do
(Table 2).
A more nuanced counterargument is that al-
though models do not ignore their prompts entirely,
perhaps it “takes less effort” for models to use the
spurious or heuristic features for predictions as
opposed to the more complex syntactic or seman-
tic features (Lovering et al., 2021; Warstadt et al.,
2020) required to properly comply with the instruc-
tions. However, spurious features alone likewise
cannot explain the observed performance gaps. Re-
call that, within each random seed, all models see
exactly the same training examples (with the same
spurious features). Thus, to the extent that models
perform differently with some prompts compared
to others, it may be due to some complex interac-
tions between the (spurious or semantic) features
in prompts and the spurious features in data ex-
amples. One possible example of this interaction
is that punctuation has a large effect for irrelevant
templates, but instructive templates seem to be able
to suppress such effect (Appendix A). Investigating
the nature of this interaction is a promising direc-
tion for future work, and it suggests a way in which
the semantics of the prompt might matter, e.g., by
affecting the models’ inductive biases, even if mod-
els do not interpret or use the instructions in the
same way as humans would.
7 Conclusion
In this study, we train several models with over
30 manually written templates and 13 sets of LM
targets for NLI. We ﬁnd that models often learn
equally fast with misleading and irrelevant tem-
plates as they do with instructive ones, and that
the choice of the target words overrides the mean-
ing of the overall prompts. Although models do
not entirely ignore the meaning of the prompts,
our results contradict a hypothesis commonly as-
sumed in the literature that models use prompts as
semantically meaningful task instructions in ways
analogous to humans’ use of instructions.2308Ethical Considerations
The fact that even the largest LMs appear to fol-
low yet do not actually follow users’ instructions
has important implications, especially considering
the increasing commercial use of LMs. While tra-
ditional ﬁne-tuned models also pose challenges
in interpretability, with prompt-based models, an
illusion of instruction following can be more per-
nicious than having no instructions at all. The in-
tuitive interface that prompts provide might make
them more accessible to lay users, and can mis-
lead users to think that their instructions arebeing
understood and followed. Our results suggest that
cautions are needed even more than they were with
traditional ﬁne-tuned models.
Acknowledgments
We are grateful to Colin Raffel, Victor Sanh, Sasha
Rush, Stephen Bach, Roman Feiman, Teven Le
Scao, Ian Tenney, Dan Garrette, Jason Wei, Satoshi
Sekine, Mike Tien-Chien Chiang, Xavier Fontaine,
Pierre Colombo, Ryan Teehan, Debajyoti Datta,
William Rudman, Ruochen Zhang, Daniel Cohen,
George Zerveas, Eric Rosen, Kaiyu Zheng, Nihal
Nayak, Roma Patel, Charles Lovering, Tian Yun,
Jack Merullo, and Aaron Traylor for comments and
discussions on early drafts of this paper. Special
thanks to Victor, Colin, and Teven for technical
clariﬁcations and code review.
Furthermore, Albert is indebted to Colin and
Sasha for their patience on the many iterations of
the zero-shot Figure 4 as well as invaluable men-
torship throughout the T0 project.
This work was supported in part by the IARPA
BETTER program.
References230923102311Contents
1 Introduction 1
2 Related Work 2
2.1 Prompt-Based Models . . . . . . 2
2.2 Analyses of Prompts . . . . . . . 2
3 Overall Setup 3
4 Effect of Templates 4
4.1 Method . . . . . . . . . . . . . . 4
4.2 Result . . . . . . . . . . . . . . . 4
4.3 Discussion . . . . . . . . . . . . . 6
5 Effect of Target Words 7
5.1 Method . . . . . . . . . . . . . . 7
5.2 Result . . . . . . . . . . . . . . . 7
5.3 Discussion . . . . . . . . . . . . . 8
6 General Discussion 8
6.1 Summary and Interpretation . . . 8
6.2 Alternative Interpretations and Fu-
ture Directions . . . . . . . . . . 9
7 Conclusion 9
A Effect of Punctuation 14
B Details and Lessons from Experiment-
ing with GPT-3’s API 15
B.1 Choice of Model . . . . . . . . . 15
B.2 Priming vs. Fine-Tuning . . . . . 15
B.3 Other Tips for Working with GPT-3 16
C Hyperparameters 16
D Compute Used 16
E Additional Figures Discussed in the
Main Text 17
F All Prompts 19
F.1 Main Experiment Templates . . . 19
F.2 Ablation Experiment Templates . 20
F.3 All Target Words . . . . . . . . . 20
G Aggregated Results 21
G.1 ALBERT on RTE . . . . . . . . . 21
G.2 ALBERT on ANLI R1 . . . . . . 22
G.3 T5 770M on RTE . . . . . . . . . 23
G.4 T5 3B on RTE . . . . . . . . . . . 24
G.5 T0 3B on RTE . . . . . . . . . . . 25
G.6 T0 3B on ANLI R1 . . . . . . . . 26G.7 T5 11B, T0 11B, and GPT-3 175B
(Figure 2) . . . . . . . . . . . . . 27
H Results of Individual Templates 28
H.1 ALBERT . . . . . . . . . . . . . 28
H.2 T0 (3B) . . . . . . . . . . . . . . 32
H.3 T5 LM-Adapted (3B) . . . . . . . 36
I Zero-Shot Results (Figure 4) 40
J Comparison of LM targets, Controlling
for the Template 41
K Preliminary Results on HANS 44
L Preliminary Results on Winograd 452312
A Effect of Punctuation
For irrelevant templates, we ﬁnd a large effect
from the use of quotation and question marks in
templates. It is natural to write such punctuation
in instructive templates as they help humans
to parse an NLI hypothesis as an embedded
clause within an instruction sentence (e.g.,
Given {premise} Should we assume
that "{hypothesis}" is true? ). For
control, we also use quotation and question
marks (“qmarks” hereafter) in irrelevant tem-
plates where they would not have made sense
naturally, e.g., {premise} Single-family
zoning is bad for American cities.
"{hypothesis}"? As an ablation, when we
remove these qmarks from irrelevant templates,
the performance of ALBERT and T0 drops
substantially (Figures 7 and 8). In contrast, for
T5, qmarks make no difference for irrelevant
templates; yet, removing qmarks from instructive
templates—where qmarks are natural—boosted
performance instead for T5 (Figure 9), but not for
T0 nor ALBERT.
Additionally, as a coincidence, most mislead-
ing templates contain both quotation and question
marks, while most misleading-far templates con-
tain only question marks (Appendix F). But as
noted in Section 4.2, there is no consistent pat-
tern between those two misleading categories. In
other words, punctuations alone cannot explain ev-
erything. As discussed in Section 6.2, the full ex-
planation is likely a combined interactions between
the spurious features and the semantics of the tem-
plates.
Lastly, note that Schick and Schütze (2021b)
and many subsequent papers’ prompts for
NLI (e.g., "{hypothesis}" ? | [mask].
"{premise}" ) are basically null templates with
some variation in punctuation between the hy-
pothesis and the premise. We ﬁnd that models
learn poorly with the vanilla {hypothesis}
[mask] {premise} , but they learn as fast
as the instructive templates with Schick &
Schütze’s punctuated version. That being said,
note again that punctuation alone cannot explain
the performance gap, since models trained with
[mask] {hypothesis} {premise} (Fig-2313ure 13, pink) perform second to best, yet swapping
their premises and hypotheses (Figure 13, purple)
makes it the worst performing among all null tem-
plates.
B Details and Lessons from
Experimenting with GPT-3’s API
B.1 Choice of Model
We use the davinci model provided by OpenAI
LP’s API, which corresponds tothe 175 billion
parameter model reported in Brown et al. (2020).
Concurrent to our work, OpenAI released a new
product called the “Instruct Series”, but we decided
to not experiment with the Instruct Series because
no academic paper or technical documentation of
any kind is available with the Instruct Series at the
time of writing aside from the following claim on
their website:
The Instruct models share our base
GPT-3 models’ ability to understand and
generate natural language, but they’re
better at understanding and following
your instructions. You simply tell the
model what you want it to do, and it
will do its best to fulﬁll your instruc-
tions. This is an important step forward
in our goal of building safe models that
are aligned with human interests.
Crucially, the Instruct Series is inappropriate for
reproducible research because it is unknown what
datasets and prompts these models are trained on,
and whether any task categories are systematically
held out as done by Sanh et al. (2021) and Wei et al.
(2021). If it is trained on any prompt or dataset of
NLI, it would not be zero-shot, making it an un-
fair comparison to other models in our experiments.
Second, it is still in beta and its training, held-out,
and prompt mixtures could change. At least two
Instruct Series models were made available in se-
quence during our writing, and it is not clear if we
experiment on an older version, whether it will still
be available and reproducible in the future.B.2 Priming vs. Fine-Tuning
As mentioned in Section 3, we use priming (a.k.a.
in-context learning) in lieu of ﬁne-tuning because,
at the time of writing, OpenAI’s ﬁne-tuning API is
limited to 10 runs per month. To train 30 prompts
at only two number of shots would take 6 months,
assuming we get hyperparameters right at ﬁrst try.
Further, each training run is limited to a maximum
of 5 epochs, which often entails an insufﬁcient
number steps for few-shot training. We were unable
to ﬁne-tune GPT to any reasonable accuracy with
our allowed 10 tries in the ﬁrst month. Finally, the
ﬁne-tuning API is limited to GPT variants up to
6.7B, not the 175B model we plan to experiment
with.
With priming, we are able to reproduce Brown
et al. (2020)’s zero-shot performance on RTE but
only with their exact prompt reported in their Fig-
ure G.31, all other (even instructive) prompts per-
form at random at zero shots, suggesting that their
reported prompt is highly cherry-picked. We are
unable to reproduce their reported few-shot result
because they report it at 32 shots, but their API only
permits a context length up to 2049 tokens, which
is insufﬁcient for RTE. We ﬁnd that 16 shots are
the highest one can reach within the token limit.
Like the gradient updated models, we document
the exact examples we use for few-shot priming in
our GitHub repository. Unlike the gradient updated
models, which are trained on the same kexam-
ples, priming models use different sets of kprim-
ing examples for each inference example (Brown
et al., 2020, p. 20). This means that GPT’s perfor-
mance reﬂects the fact that, overall, it has seen far
more thankexamples, making it not directly com-
parable to the few shots of the gradient updated
models. This is not ideal, but our GPT few-shot
performance already underperforms what Brown
et al. (2020) report, so we choose to not further
restrict it to have the same ﬁxed priming examples
for all inference examples, which could run into
a lack of competence issue (§6.2) that make its
results unusable for our research question.
Lastly, unlike the gradient updated models, we
do not run multiple seeds with our GPT experi-
ments because, ﬁrst, they are expensive. As the
API bills by token, using kshots of priming exam-
ple effectively multiplies the total cost by k. Sec-2314ond, OpenAI imposes a monthly quota for each lab,
so running multiple seeds will take several more
months to complete.
B.3 Other Tips for Working with GPT-3
Using the logprobs argument in their API, we
obtain the top 99 predicted target word and their
log probabilities.Following Sanh et al. (2021) and
Wei et al. (2021), we evaluate by a rank classiﬁ-
cation of the target words, i.e., if the gold target
word is “yes”, we consider it as correct as long as
the probability of “yes” is higher than that of “no”,
regardless of whether “yes” is the top-1 prediction
generated by the model.
Alarmingly, we ﬁnd that these top-99 predictions
are semantically inconsistent ranked, e.g., for one
data example and its top-99 word predictions, it is
often the case that, e.g., P(yes) > P(no) but P(Yes)
< P(No). Thus, the choice of the target words’ sur-
face form makes a substantial difference in the
overall performance. (Not to mention the prob-
lem of choosing between yes/no, true/false, cor-
rect/incorrect, etc. as studied in Section 5.) OpenAI
recommends having no trailing space in the input
and let the model predict the ﬁrst token with a lead-
ing space as in “ Yes”. We ﬁnd that although strip-
ping the leading space sometimes leads to higher
performance for some prompts, overall not apply-
ing stripping or other token normalization performs
the best.
Another point researchers should pay attention
to is the use of what OpenAI calls a “separator”
inserted between priming examples. In preliminary
experiments, we initially use newline characters as
appeared in Brown et al. (2020)’s Appendix G. We
later discover that OpenAI recommends using ###
or\n###\n as separators. We use the latter and
ﬁnd consistent performance improvement over just
using newline characters, and we use it throughout
in our main experiments.
C Hyperparameters
For encoder-only models, we follow Schick and
Schütze (2021b) and Le Scao and Rush (2021)’s
recommendations and use a learning rate of 1e.
For T5 and T0 models, we follow Raffel et al.
(2020) and Sanh et al. (2021)’s recommendationsand use a learning rate of 1e. We run sev-
eral preliminary experiments with learning rates
(3e,1e,5e,1e) deviating from their rec-
ommendations and they perform worse, although
our search is not exhaustive due to the high cost
of running multiple prompts with multiple random
seeds.
Note that T5 and T0 are trained with the Adafac-
tor optimizer (Shazeer and Stern, 2018) in Mesh
TensorFlow. Our implementation is in PyTorch, and
we ﬁnd that ﬁne-tuning T5 with PyTorch’s imple-
mentation of Adafactor yields substantially worse
results than the usual choice of the AdamW opti-
mizer. We corresponded with Raffel et al. (2020),
who advised us that it might be due to the fact that
PyTorch does not have the same learning rate sched-
uler implementation as TensorFlow’s Adafactor
does. They recommended us to simply use AdamW,
which is what we did. This is somewhat unfortunate
because Adafactor is much more memory efﬁcient,
which would have drastically reduced the compute
resources required and thus enable more compre-
hensive experiments of the 11B models, which are
currently limited to 0 shots and 16 shots only.
Although most models seem to obtain the high-
est validation accuracy at very early epochs, we
train all models to 30 epochs (20 epochs for 11B
models) to be safe and select the checkpoint with
the highest validation accuracy.
All models use a batch size of 4 with 4 gradient
accumulation steps for an effective batch size of
16.
Note that because we use a rank classiﬁcation
of single-token target words, decoding sampling
methods (e.g., beam search, top- k, top-p) are un-
necessary.
We follow Raffel et al. (2020) and add EOS to-
kens for input sequences, which yields higher few-
shot performance compared to not adding EOS as
done by Sanh et al. (2021). However, we omit EOS
in the zero-shot setting, which exactly reproduces
the results reported by Sanh et al. (2021). See T0’s
GitHub repository readmefor more information.
D Compute Used
Each ALBERT 235M model is trained on a single
Nvidia RTX3090. Their main experiments took
approximately 192 GPU hours.2315Each T5 LMA 770M model is trained on a sin-
gle A6000. Their main experiments took approxi-
mately 48 GPU hours.
The 3B models are each trained by partitioning
their layers over four RTX3090s. T5 and T0’s main
experiments took approximately 2,304 GPU hours
in total.
The 11B models are each trained on eight V100s
(each with 32GB of memory). T5, T0, and T0++’s
main experiments took approximately 1,728 GPU
hours in total. (Due to their large GPU memory
requirement, we were only able to complete one
number of shots.)
E Additional Figures Discussed in the
Main Text23162317F All Prompts
F.1 Main Experiment Templates2318F.2 Ablation Experiment Templates
F.3 All Target Words2319G Aggregated Results
G.1 ALBERT on RTE2320G.2 ALBERT on ANLI R12321G.3 T5 770M on RTE2322G.4 T5 3B on RTE2323G.5 T0 3B on RTE2324G.6 T0 3B on ANLI R12325G.7 T5 11B, T0 11B, and GPT-3 175B (Figure 2)2326H Results of Individual Templates
H.1 ALBERT2327232823292330H.2 T0 (3B)2331233223332334H.3 T5 LM-Adapted (3B)2335233623372338I Zero-Shot Results (Figure 4)2339J Comparison of LM targets, Controlling for the Template234023412342K Preliminary Results on HANS2343L Preliminary Results on Winograd2344