
Eric Mitchell, Joseph J. Noh, Siyan Li, William S. Armstrong,
Ananth Agarwal, Patrick Liu, Chelsea Finn, Christopher D. Manning
Stanford University
eric.mitchell@cs.stanford.edu
Abstract
While large pre-trained language models are
powerful, their predictions often lack logical
consistency across test inputs. For example,
a state-of-the-art Macaw question-answering
(QA) model answers YestoIs a sparrow a
bird? andDoes a bird have feet? but answers
NotoDoes a sparrow have feet? . To address
this failure mode, we propose a framework,
Consistency Correction through Relation De-
tection, or ConCoRD , for boosting the consis-
tency and accuracy of pre-trained NLP models
using pre-trained natural language inference
(NLI) models without fine-tuning or re-training.
Given a batch of test inputs, ConCoRD sam-
ples several candidate outputs for each input
and instantiates a factor graph that accounts
for both the model’s belief about the likeli-
hood of each answer choice in isolation and
the NLI model’s beliefs about pair-wise answer
choice compatibility. We show that a weighted
MaxSAT solver can efficiently compute high-
quality answer choices under this factor graph,
improving over the raw model’s predictions.
Our experiments demonstrate that ConCoRD
consistently boosts accuracy and consistency of
off-the-shelf closed-book QA and VQA mod-
els using off-the-shelf NLI models, notably in-
creasing accuracy of LXMERT on ConVQA
by 5% absolute. See the project websitefor
code and data.
1 Introduction
Reliable and trustworthy AI systems should demon-
strate internal self-consistency , in the sense that
their predictions across inputs should imply logi-
cally compatible beliefs about the world. However,
even powerful large language models are known
to lack self-consistency (Ray et al., 2019; Elazar
et al., 2021; Kassner et al., 2021). For example, a
question-answering (QA) model that answers the
question Is a sparrow a bird? andDoes a bird have
feet? with Yesis implicitly expressing the belief
thatA sparrow is a bird andA bird has feet . If the
same model answers the question Does a sparrow
have feet? with No, the model expresses the logi-
cally incompatible belief A sparrow does not have
feet. In such cases, ascertaining the model’s “true”
belief is difficult, making interpreting and validat-
ing its behavior correspondingly challenging.
Prior work has improved model self-consistency
by training with specialized loss functions (Elazar
et al., 2021) or data augmentation (Ray et al., 2019),
or alternatively re-ranking model predictions based
on their mutual self-consistency using pre-written
logical constraints, such as “all mammals have fur”
(Kassner et al., 2021). However, the first class
of methods requires expensive fine-tuning which
might be impractical for many practitioners for very
large pre-trained models, and re-ranking methods
require an explicit collection of the logical rela-
tions of interest, making scaling a challenge. Still,1754re-ranking-based approaches have the benefit of
not requiring fine-tuning, and we hypothesize that
their scalability limitations may be addressed by
estimating logical relationships between model pre-
dictions on the fly. Specifically, we hypothesize
that existing pre-trained natural language inference
(NLI) models can estimate logical relationships be-
tween an arbitrary pair of model predictions well
enough to provide an effective, scalable substitute
for explicit collection of such constraints. Leverag-
ing these estimated constraints, we can construct a
factor graph representing a probability distribution
over model outputs that incorporates both the origi-
nal model’s confidence scores and the NLI model’s
beliefs about logical relationships.
Our primary contribution is Consistency Correc-
tion through Relation Detection, or ConCoRD , a
framework to improve the consistency and perfor-
mance of a pre-trained base language model with-
out fine-tuning by using more confident and better
attested model predictions to override less confi-
dent model beliefs. See Figure 1 for an overview.
To enable propagation of model beliefs, we esti-
mate pair-wise logical relationships between model
predictions using a pre-trained NLI model. Us-
ing these pair-wise relationships, we define an
undirected graphical model representing a distribu-
tion over responses accounting for both the base
model’s beliefs and the NLI model’s estimates of
answer compatibility. We efficiently find the ap-
proximate mode of this distribution among the base
model’s top answer choices for each input as the
solution of a MaxSAT problem, which consistently
produces more accurate and self-consistent predic-
tions than using the raw model predictions. In
Section 4.1 we find that ConCoRD produces an
8.1% absolute improvement in F1 of a pre-trained
Macaw model (Tafjord and Clark, 2021) on the
BeliefBank QA dataset (Kassner et al., 2021). In
Section 4.2 we find a 5.0% absolute improvement
in accuracy of a pre-trained LXMERT model (Tan
and Bansal, 2019) on the ConVQA dataset (Ray
et al., 2019), and in Section 4.3 we find that Con-
CoRD enables test-time model editing (Sinitsin
et al., 2020; Mitchell et al., 2022), updating model
predictions at test time when presented with new
information.
2 Related Work
Prior work for maintaining consistency in the
question-answering space often involves additionaltraining to improve performance. Chen et al. (2021)
transform the Natural Questions (Kwiatkowski
et al., 2019) dataset question-answer pairs into
premise-hypothesis pairs, then uses an NLI model
trained on this dataset as a decider for unanswer-
able questions. Alberti et al. (2019) generate ques-
tions from unlabeled texts, then filter them to en-
sure roundtrip consistency; pre-training on this
synthetic set improves performance on SQuAD
2.0 (Rajpurkar et al., 2018) and Natural Ques-
tions. Asai and Hajishirzi (2020) augment QA-
pairs with their logically symmetric and transitive
counterparts through linguistic approaches to en-
hance cross-dataset QA performance. ConCoRD
differs significantly from these question-answering-
specific approaches because no fine-tuning of the
base model is needed and the methodology is not
specific to question-answering.
Similarly to ConCoRD, Kassner et al. (2021)
re-rank model predictions by solving an optimiza-
tion problem defined by a combination of the base
model confidence scores and pair-wise constraints
representing the logical compatibility of different
model predictions stored in a persistent memory,
which they call BeliefBank. The key distinguish-
ing property of ConCoRD is the fact that pair-wise
constraints between model predictions are dynami-
cally estimated by a pre-trained NLI model, rather
than drawn from a fixed, pre-collected set of con-
straints. Dynamically estimating the constraints
has a variety of benefits, eliminating the need for
manually collecting the logical constraints of inter-
est, automating the process of determining whether
a particular constraint applies to a particular pair of
predictions, and likely inheriting improvements in
Natural language inference (NLI, MacCartney and
Manning (2008)) models over time.
NLI has long been used to maintain logical con-
sistency in generated dialogue utterances (Welleck
et al., 2019; Dziri et al., 2019; Song et al., 2020), ra-
diology report domain entities (Miura et al., 2021),
and summarization (Laban et al., 2022; Honovich
et al., 2022). Perhaps most similarly, Jung et al.
(2022) use NLI to estimate constraints between fac-
tual statements produced by GPT-3. These prior ap-
proaches support our intuition for using NLI mod-
els to improve logical consistency among batches
of answers. While the authors explore applica-
tions of this framework to multi-step reasoning
for True/False questions or statements, our work
focuses on applying this methodology to more gen-1755
eral settings, such as VQA, open-ended QA, and
model editing.
3 Consistency Correction through
Relation Detection
ConCoRD contains three key components, the base
model , arelation model (typically a pre-trained
NLI model), and an inference procedure that com-
bines the predictions of the two models into a more
accurate and self-consistent set of beliefs. Impor-
tantly, both the base model and relation model are
pre-trained, off-the-shelf models; ConCoRD does
not update any weights or require training data for
either model, using only a small validation set for
hyperparameter tuning. We next explain the func-
tion of each of these components when executing
ConCoRD.
3.1 Base Model
The core function of the base model in ConCoRD
is generating a set of candidate outputs for a given
input, which are ultimately re-ranked by the infer-
ence process (Sec. 3.3). Given a batch of Nmodel
queries Q={q}, the first step of ConCoRD is to
generate a set of Jcandidate outputs for each query
ˆA={ˆa, ...,ˆa}, along with their correspond-
ing likelihoods p(ˆa|q). Note that the candidate
outputs need not be an IID sample from the base
model; for example, we might use beam search
with a diversity bonus to produce a more diverse
set of candidates (Vijayakumar et al., 2018). Each
pair of query and candidate output forms a model
belief b= (q,ˆa); the output of the base model
is the complete set of model beliefs B={b}
and their corresponding normalized probabilitiesp. The base models in our experiments are pre-
trained question-answering models based on T5-
large (Raffel et al., 2020) and pre-trained visual
question-answering models such as LXMERT (Tan
and Bansal, 2019) and ViLT (Kim et al., 2021).
3.2 Relation Model
The relation model p(·|x, x)estimates the
most likely logical relationship between an or-
dered pair of natural language utterances from
the choices {none,fwd-entail ,contradict ,
equivalence }.In addition to the model be-
liefs B, we define optional context statements
c=C(b),Krelevant statements that may
be retrieved, generated, or manually written for
each model belief. The ability to incorporate con-
text statements enables ConCoRD to modulate
model behavior independently for each input in
the test batch, rather than reasoning transductively
about pairs of test inputs. See Table 3 for exam-
ples of model beliefs and context statements. In-
puts to the relation model are either pairs of two
model beliefs (b, b)or pairs of one model be-
lief and one context statement (b, c). We de-
fine the most likely inter-belief relation as r=
argmaxp(r|b, b), and similarly for belief-
context relations r= argmaxp(r|b, c).
The output of the relation model is the set of most-
likely relations R={r} ∪ { r}and their
associated probabilities, which we denote as p
andp. Our experiments use various pre-trained
NLI models based on RoBERTa (Liu et al., 2019)1756and ALBERT (Lan et al., 2019) as the relation
model.
Question-answer to statement conversion.
While concatenating query qand candidate
output ˆato produce inputs to the relation model
is perhaps the simplest approach to estimating
soft constraints, we use a statement conversion
model to provide inputs to the relation model that
are closer to its training distribution. Instead of
defining the belief b= (q,ˆa)as concatenation
ofqandˆa, we define bto be the statement
f(q,ˆa), where fis the conversion model. We
fine-tune a small T5 model on a combination of
data from (Demszky et al., 2018) and BeliefBank
(Kassner et al., 2021) to produce a model that maps
a (question, answer) pair into a natural language
statement. Details about the fine-tuning procedure
and data are provided in Appendix C.
3.3 Inference
ConCoRD’s inference procedure maps the set of
beliefs Band pair-wise relations Rinto a choice of
the most likely belief for each question. To define
the inference problem, we first define a binary de-
cision variable zrepresenting the estimated truth
value of model belief b. A value of 1 for node z
in the maximum likelihood configuration means
thatˆais returned for query q; the problem in-
cludes a constraint that exactly one candidate an-
swer is true for each query. The factor graph in-
cludes the set of variables Z={z}and
various factors (functions mapping a subset of Zto
a non-negative scalar) derived from the base model
and relation model’s beliefs and the hard constraint
of returning only one answer per question. Factors
are defined such that more desirable configurations
ofzyield a larger product of the individual fac-
tors. First, unary factors ϕ(z)encode the base
model’s beliefs about the likelihood of specific an-
swers, and are defined as:
ϕ(z) =/braceleftiggifz= 1
1 otherwise(1)
where p=p(ˆa|q); in other words, the factor
takes the odds ratio if the corresponding statement
variable zis assigned a truth value of 1; otherwise,
the factor takes value 1. In order to encode the
hard constraint that exactly one output should be
returned for each query, we include a J-ary factor
ϕ(Z)for each group of nodes Z={z},which is equal to 1 for configurations where exactly
one of the nodes takes a value of 1, and 0 for all
other configurations.
Binary factors ϕ(z, z)and optionally
ϕ(z, c)encode compatibility between pairs
of model beliefs (or model belief-context pairs):
ϕ(z, z) =/braceleftigg
1 ifr(z, z)
1−potherwise
where we define the relation function rto eval-
uate to trueif its arguments satisfy the underlying
relation, and false otherwise; ϕ(z, c)is de-
fined similarly to ϕ(z, z). The inference
problem amounts to finding argmaxϕ(Z),where
ϕ(Z) =/productdisplayϕ/productdisplayϕ/parenleftigg/productdisplayϕ/parenrightigg/parenleftigg/productdisplayϕ/parenrightigg
.
(2)
An approximate solution to this inference problem
can be efficiently found for most problems with a
MaxSAT solver such as RC2 (Ignatiev, 2019). We
omit arguments to the factors for conciseness. See
Figure 2 for a simple example of a factor graph
with a single inter-belief constraint and no belief-
context constraints.
Entailment correction. Consider a belief b, a
set of its entailed statements S={s}, unary
factors ϕ(z)and{ϕ(z)}, and binary factors
P={ϕ(z, z)}. Recall that an entailment re-
lation r(z, z)is satisfied (and the binary fac-
tor is maximized) if either z= 0 orallz= 1.
Consequently, as the cardinality of {z|z= 0}
increases, the more likely it is that z= 0will max-
imize the product of all binary factors/producttextϕ(z, z).
This is true even if most entailed statements are true,
i.e.,|{z|z= 1}| ≫ |{ z|z= 0}|. If most of
the statements entailed by a belief are true, assign-
ing the belief to be false due to a small number of
(potentially spuriously) false entailed statements
may be undesirable. To mitigate this outcome,
we experiment with an additional type of factor
in which configurations satisfying entailments with
bothz= 1andz= 1are ‘rewarded’ more than1757other configurations satisfying the entailment:
ϕ(z, z) =

1 ifz, z= 1
1−pifz, z= 0/radicalig
1−potherwise
Applying entailment correction consistently im-
proves ConCoRD’s performance; see Appendix
Table 8 for a dataset-by-dataset breakdown.
3.4 Hyperparameters of ConCoRD
We introduce two key hyperparameters to Con-
CoRD. Because we do not know a priori the rel-
ative reliability of the base model and relation
model, we introduce the hyperparameter β∈[0,1],
corresponding to a trade-off between the predic-
tions of the base model and relation model. A
value of β= 1 corresponds to simply taking the
raw predictions of the base model, while β=
0corresponds to optimizing purely for answers
that are self-consistent according to the relation
model, without considering the base model’s be-
liefs. The unary factors in the factor graph become
ϕ(z) = ( ϕ(z))andϕ(z, z) =
/parenleftbig
ϕ(z, z)/parenrightbig(and similarly for ϕ). In
addition to β, we introduce a threshold λfor rela-
tion model confidence to filter out low-confidence
relation estimates. That is, we discard a relation
rorrifp< λ orp< λ, respec-
tively. In practice, we find that the optimal βand
λvary across problems, perhaps due to the vary-
ing complexity of the model belief and context
statements (and therefore the reliability of the re-
lation model’s predictions). Therefore, we use the
hyperopt library (Bergstra et al., 2013) for auto-
mated hyperparameter optimization, using the Tree
Parzen Estimator (TPE) algorithm to tune βandλ
jointly. We use the optimal hyperparameters found
on the validation data for each problem to compute
test performance. Appendix H.1 details hyperpa-
rameter tuning for each experiment.
4 Experiments
Our experiments are broadly designed to answer
the high-level question: can ConCoRD leverage
the relational knowledge in pre-trained NLI models
to produce more accurate, self-consistent system
behavior, without additional data or fine-tuning?
Further, we investigate ConCoRD’s applicability to
performing test-time model editing (Sinitsin et al.,2020; Mitchell et al., 2022), or injection of new in-
formation, and ConCoRD’s sensitivity to the choice
of hyperparameters and types of relations detected.
4.1 Internal Consistency in Closed-Book
Question-Answering
Protocol. To evaluate the accuracy and consistency
of a set Bof beliefs, Kassner et al. (2021) syn-
thesize a gold standard for those beliefs andthe
inferred relations R. Following this prior work, we
assume the following is given:
• A set of entities s∈S
• A set of unary predicates P∈P
•A collection of “facts” (P(s)), whose bi-
nary truth value is known
•A directed graph of gold-standard con-
straints G(P, E), whose edges (P, P)∈
Erepresent first-order logical formulae
∀x(P(x)→P(x))
From these, we construct simple yes/no questions
using natural language templates. For example,
for fact P(s), if entity srepresents a lion and
predicate Prepresents an ability to drink liquids ,
the template-generated gold question answer pair
(q, a)is Q: Is it true that a lion is able to drink
liquids? ; A:Yes.
These questions are given as input to one of two
sizes of a multi-angle question answering model
(Tafjord and Clark, 2021), given a multiple choice
angle with choices Yes.andNo.The questions and
retrieved answers (q,ˆa)form a set of beliefs B
for each entity. Since these are closed-book ques-
tions, no context statements are supplied; because
they are yes/no questions, only one candidate an-
swer is obtained, i.e., J= 1. Question-answer
to statement conversion is applied to all questions
with a default answer of Yes.regardless of the an-
swer ˆa, in order to provide the relation model with
positive natural language assertions from which to
infer sets of relations R; where the base model
answers ˆaareNo.we replace node zin the factor
graph with its complement. Configurations Z
are found for each s∈Swhich maximize Equa-
tion 2 given B, Rand together form a global
solution Z.
Datasets. Kassner et al. (2021) provide a suitable
database with 12,636 facts (“silver facts”), each
indicating whether one of 601 predicates relates
to one of 85 entities, as well as 4,060 confidence-
weighted first-order constraints manually gathered
from ConceptNet (Speer et al., 2017), forming a1758
constraint graph G. Additionally, they provide
1,072 distinct “calibration facts”, each relating one
of 7 entities to one of 334 predicates.
We tune βandλusing a validation set of ques-
tions generated from the calibration facts, and eval-
uate test time performance with questions gener-
ated from silver facts.
Metrics. We measure accuracy using binary F1
between elements zof the configuration Z maxi-
mizing ϕ(Z)(as in Equation 2), and the truth value
of facts (P(s)). As in Kassner et al. (2021);
we use F1 for evaluation because gold answers are
highly biased towards true No.answers.
We compute consistency within batches of
questions using the complement of of Li et al.
(2019)’s conditional constraint violation metric τ,
defined here as the proportion of relevant gold
constraints in Gwhich are violated ; a constraint
∀x(P(x)→P(x))is relevant iff, for some en-
titys, there is some belief b∈Bfrom fact
(P(s))such that z= 1,andthere is some be-
liefb∈Bthat corresponds to fact (P(s));
the constraint is violated when z= 0.
Comparisons. ConCoRD is evaluated against a
naive baseline where only base model answers ˆa
and probabilities are considered. A second baseline
(G.C.) performs the inference described in Sec. 3.3,
replacing the inferred relations Rwith the gold
constraints from constraint graph G, rather than
those estimated by the relation model.
Results. Results are shown in Table 1. ConCoRD
provides an absolute improvement of over 8% in
F1 and consistency for Macaw-Large and 7% for
Macaw-3B compared to the baseline. Notably, the
margin of superiority of the Macaw-3B base model
is mostly preserved after applying ConCoRD, sug-
gesting that ConCoRD may provide a significant
benefit even for very large models. A surprising
result is that ConCoRD shows marked improve-
ments in F1 over the gold constraint baseline, sug-
gesting that the detection and filtering of relations
ConCoRD provides may, in this setting, be an im-
provement over rigid adherence to the logical con-
nections specified a priori in Kassner et al. (2021).
4.2 Internal Consistency in VQA
Protocol. The Visual Question Answering (VQA)
task involves a language model generating answers
to questions that are directly associated with im-
ages. VQA tests for robustness and generalizability
of ConCoRD as it introduces an additional layer of
difficulty; the task moves away from purely text-
based tasks while expanding the answer space to
the vocabulary of the LM being used. The ques-
tions from the ConVQA dataset (Ray et al., 2019)
and its associated images from the Visual Genome
dataset (Krishna et al., 2016) provide an apt setting
to assess ConCoRD, as the relatedness of ques-
tions for each image provide ample opportunity for
model self-inconsistency.
The ConVQA dataset consists of a set of images
each associated with a group of related questions
about the image, such as What color is the horse?
andIs the horse brown? for a picture of a brown
horse in a stable. We evaluate ConCoRD with two
VQA models, LXMERT (Tan and Bansal, 2019)
and ViLT (Kim et al., 2021). For each group of
questions Q={q}, we sample the top-2 can-
didate outputs {ˆa,ˆa}for each question, and
use a pre-trained NLI model to infer the most likely
pair-wise relations Rbetween outputs from differ-
ent questions. We use the RC2 MaxSAT Solver to
estimate the configuration that maximizes Equation
2.
Metrics. We report accuracy as the proportion of
questions answered correctly across all groups. We
infer consistency using a metric previously used in
the literature for the ConVQA dataset called "per-
fect consistency" (Ray et al., 2019). For all groups
of related questions, a group is perfectly consistent
if all its questions are answered correctly. Perfect
consistency then reports the proportion of question
groups that were perfectly consistent. While this1759
is not a perfect measure of consistency as it ex-
cludes cases in which incorrect answers are consis-
tent with each other, it still serves as a meaningful
proxy since the dataset was designed such that any
incorrect answer in a question group implies the
presence of inconsistency.
Datasets. We divide the ConVQA dataset into a
"clean" (i.e. human verified and filtered) test set
and a non-test set (train + val + test as defined by
Ray et al. (2019)). From the non-test set, we sam-
ple 10,000 random images equivalent to 123,746
questions to be used as our validation set for tuning
our two hyperparameters. We use the clean test set
– 725 images and 6,751 questions – to report our
final results.
Comparisons. ConCoRD is compared with a naive
baseline and a top-2 oracle upper bound. The naive
baseline is the answer with the highest VQA model
probability. Top-2 oracle upper bound selects the
correct answer if present within the top-2 predic-
tions of the VQA model. Top-2 is appropriate given
our use of the top-2 candidate outputs to generate
inferences with NLI models.
Results. The final results for ConCoRD, baseline,
and oracle upper bound are shown in Table 2. Con-
CoRD increases the accuracy of LXMERT and
ViLT by 5% and 2% respectively, and the consis-
tency of LXMERT and ViLT by 4.9% and 5.9% re-
spectively. Examples in which ConCoRD correctly
and incorrectly selects a candidate output different
from the baseline output are shown in Figure 4 and
Figure 5, respectively. In particular, the incorrectscenarios demonstrate several failure modes that
may be in part responsible for the gap between
ConCoRD and the oracle upper bound, suggesting
further improvements of the components of Con-
CoRD will also continually improve ConCoRD.
4.3 Test-Time Information Injection
Protocol. We perform an additional experiment
to evaluate ConCoRD’s ability to integrate exter-
nal factual information into its inference process,
rather than only using other predictions in the test
batch. Such an ability enables editing a model’s
behavior at test time, without re-training, as new
information becomes available. We use the Natural
Questions (NQ; Kwiatkowski et al. (2019)) dataset,
rather than BeliefBank, to provide more challeng-
ing inputs to the relation model. Given a question
from NQ, a sentence from the ground truth con-
text document containing information about the
answer is retrieved and provided as an additional
input to ConCoRD; we constrain the node repre-
senting this context variable in the factor graph to
be true. Constraints are predicted between each
answer choice and the context statement. As in the
other experimental settings, hyperparameters are
tuned on the validation set and applied on the test
set. See Appendix H for tuning procedures.
Metrics. Model performance is evaluated using
the SQuAD F1 score for overlapping tokens, fol-
lowing the same answer normalization protocols,
including lower-casing and removing punctuation.
Datasets. The NQ development set consists of
7830 open-book question-answer pairs, with both
long and short gold annotations in their context
passages. Since the NQ test set is not available,
we create a test and validation set from the NQ
validation questions as follows: we take the first
5000 questions to form our test set, and the rest to
be our val set, which we use for hyperparameter
tuning. Then each set is filtered such that only1760the answerable questions remain. “Answerable” is
defined as having a “short answer" span defined in
the annotations. This filtering process gives 2713
test entries and 1576 val entries.
Comparisons. ConCoRD is compared with a naive
baseline and an oracle upper bound. All of these
approaches operate on the fixed set of QA model
answers for a specific QA model (one of T5-Sm-
NQ, T5-Lg-NQ, and T5-3B-NQ), specifically the
set of top-4 answers for each question. The naive
baseline selects the answer with the highest QA
model probability, argmaxp(ˆa|q). The ora-
cle upper bound approach selects the answer that
has the best score with the gold short answer span,
argmaxF(ˆa, a).
Results. The results on the test set using the naive
baseline, ConCoRD, and oracle upper-bound are
reported in Table 4. ConCoRD always outper-
forms the naive approach, demonstrating that the
framework is useful even when each query input is
processed independently (i.e., non-transductively).
However, despite providing a relative gain of as
high as 8.7% over the naive baseline, there is still
a gap between ConCoRD and the oracle. This gap
may be attributable to the complexity of the NQ
questions and context information compared with
the statements in prior experimental settings. Chen
et al. (2021) demonstrate a significant gain in cal-
ibration performance from training on MultiNLI
(Williams et al., 2018) to training on a combination
of MultiNLI and their NLI corpus adapted from
NQ, perhaps hinting that crucial knowledge present
in Natural Questions is not covered in MultiNLI,
partially explaining the gap between ConCoRD
and oracle F1 performance. Overall, these results
suggest that ConCoRD can reason between context
statements and model beliefs in addition to pairs of
model beliefs, improving performance even with
the increased complexity of the data.
Qualitative Analyses. Examples of “good” and
“bad” edits (edits that improve and decrease the
resulting F1-scores respectively) are presented in
Table 3, with more in Appendix F. When the cor-
rect answer is not available in the candidate outputs,
ConCoRD is capable of pushing towards more par-
tially correct answers and those that have more
overlap with the context.
4.4 Ablating Relation Types
Given that we consider two types of relations in
our experiments, contradiction and entailment, it
is natural to wonder the relative contribution of
these to ConCoRD’s performance improvement;
Table 5 shows the results of this ablation. We re-run
ConCoRD with either entailment or contradiction
relations removed, re-tuning the hyperparameters
for both of the new settings (contradiction-only or
entailment-only). We find that the relative con-
tribution of contradiction and entailment relations
varies significantly across models even within the
same task, but using both relation types always per-
forms approximately as well or better than using
just one, suggesting that both types of detected rela-
tions from the NLI model carry useful information.
However, we observe in several cases, such as ViLT
and the T5 models, that the entailment and contra-
diction relations may encode somewhat redundant
information, as the performance when including
either type of constraint alone nearly matches that
of using both types.
4.5 Hyperparameter Sensitivity
We perform several experiments to clarify the rela-
tionship between the key hyperparameters, includ-
ing the specific relation NLI model, β, and λ.
Impact of varying relation model. Table 6
shows a comparison of ConCoRD’s test perfor-
mance for several NLI models for each setting; no-
tably, the best-performing NLI model is not consis-
tent across problems. While the Albert-XXL model
from Nie et al. (2020) is the strongest performing
model on NQ, the simpler RoBERTa-Large models
outperform it on BeliefBank and ConVQA.
Sensitivity to βandλ.Figure 3 shows the per-
formance of ConCoRD on ConVQA with ViLT as1761
β(the tradeoff between base model and relation
model beliefs) and λ(the NLI confidence thresh-
old) are varied, using the values explored during
hyperparameter optimization. Section H.2 of the
Appendix shows similar visualizations for differ-
ent VQA experiments. If multiple hyperparame-
ters within a grid element were explored, the best
performing configuration is shown. While the max-
imum value in each column is the same (0.04),
indicating that there exists a good value of βfor
almost any λ, the converse is not true; for some
values of β, no good value of λexists. Thus, we
conclude that the tradeoff parameter βis the more
important parameter to tune carefully.
5 Discussion & Conclusion
We have presented the ConCoRD framework for
enforcing self-consistency in pre-trained language
models using relations estimated by pre-trained
NLI models, showing that it improves over off-the-
shelf performance in a variety of settings without
requiring any fine-tuning. Our findings suggest that
existing pre-trained NLI models can be a useful
building block for boosting performance of NLP
systems by providing useful estimates of logical
relationships between model predictions across var-
ious models and datasets for QA and visual QA.
ConCoRD also suggests several directions for
future work. Integrating ConCoRD with meth-
ods that generate questions likely to elicit use-
ful knowledge for answering the question at hand
(Ray et al., 2019; Shwartz et al., 2020) may further
improve performance. In addition, integrating a
framework such as ConCoRD with recent methods
for differentiation through black box combinatorial
solvers (Pogan ˇci´c et al., 2020) may enable train-
ing of the entire base model, relation model, and
inference pipeline end-to-end, potentially further
improving aggregate performance. Finally, Con-
CoRD’s general mechanism of re-ranking predic-
tions by estimating the self-consistency of groups
of model predictions is applicable beyond natu-
ral language, and future work might investigate
its application to problems in vision or sequential
decision-making. We hope that ConCoRD may
serve as another promising example of integrat-
ing both neural and explicit symbolic inference
machinery into a broader intelligent system that
outperforms any of its components individually.
6 Limitations
While our results suggest ConCoRD can effectively
leverage additional compute to boost model per-
formance without fine-tuning, our work has some
limitations. Although ConCoRD is conceptually
applicable to generations from any language model,
our work focuses on question-answering settings to
leverage existing self-consistency benchmarks. In
addition, ConCoRD increases the compute costs of
inference, although it does not require fine-tuning.
Further, our results suggest that the best NLI model
to use for ConCoRD may vary across domains, re-
quiring some tuning. As NLI models improve, we
might hope that the final performance of ConCoRD-
like systems should also inherit these gains, but Ta-
ble 6 suggests that the factors that make a particular
NLI model well-suited to a particular problem are
not obvious, requiring further investigation.1762Acknowledgements
The authors would like to thank the anonymous
reviewers for their helpful feedback during the re-
view period, Gabe Mudel, Julie Wang, Cameron
Tew, Anthony Tzen, Kevin Yang, and Ian Ng for
helpful discussions and assisting with exploratory
experiments early on in the project, and Nora Kass-
ner for providing helpful early guidance in con-
figuring the BeliefBank experiments. CF and CM
are CIFAR Fellows. EM gratefully acknowledges
funding from the Stanford Knight-Hennessy Grad-
uate Fellowship. JN is supported by Stanford Uni-
versity Medical Scientist Training Program grants
T32-GM007365 and T32-GM145402. SL acknowl-
edges brownie bites from Target for providing a cru-
cial fuel source for late night experiment-running.
References17631764
A Reproducing Macaw-Large Examples
The following configuration reproduces the Macaw-
Large behavior noted in the abstract and the intro-
duction at https://huggingface.co/allenai/
macaw-large .
$answer$ ; $question$ = Is a sparrow a
bird? ; $mcoptions$ = (A) Yes. (B) No. ;
$answer$ ; $question$ = Does a bird have
feet? ; $mcoptions$ = (A) Yes. (B) No. ;
$answer$ ; $question$ = Does a sparrow
have feet? ; $mcoptions$ = (A) Yes. (B)
No. ;
B Factor Graph Overview
A factor graph is a factorization of a function f
mapping a set of nvariables Z={z}to a non-
negative scalar. The factorization is represented
as a bipartite graph containing variable nodes and
factors ; each zis represented by one variable node,
and each factor ϕmaps a subset of the variable
nodes Zto a non-negative scalar. The value of
the function is computed as f(Z) =/producttext(Z). See
Loeliger (2008) for a more complete reference.
C Question-Answer to Statement
Conversion Model Details
To convert question-answer pairs into declara-
tive statements, we combine data from the Ques-
tion to Declarative Sentence (QA2D) (Demszky
et al., 2018) and BeliefBank (Kassner et al.,
2021) datasets to fine-tune a T5-base sequence-to-
sequence model. QA2D contains question-answer
pairs from five QA datasets; 95% of the pairs are
from SQuAD (Rajpurkar et al., 2016). The gold
statements are from Amazon Mechanical Turk. The
BeliefBank questions are created from silver facts
using natural language templates as in Section 4.1,and the yes/no answers are from the known binary
truth values of these facts. Our training dataset is
composed of the full QA2D training dataset of 61k
question-answer pairs and half of the BeliefBank
silver facts, for a total of 67k training examples.
Likewise, the validation dataset consists of the full
QA2D validation dataset of 10k pairs and half the
BeliefBank silver facts, for a total of 16k validation
pairs.
The input to the QA statement conversion model
is the concatenation of the question-answer pair
q∥a. Accuracy is evaluated by comparing the
output sequence tokens to the gold sequence tokens.
Training occurs with a learning rate of 1efor a
maximum of 50 steps, where each step consists
of 32k training examples, with early stopping if
validation loss does not decrease for 6 consecutive
steps. We ran the fine-tuning on NVIDIA GeForce
RTX 3090 GPU. Fine-tuning ended after 14 steps
with a final training accuracy of 0.764 and valida-
tion accuracy of 0.628. This took approximately
40 minutes. Table 7 demonstrates the model’s per-
formance on a few validation examples.
D Additional Modifications to ConCoRD
A timeout for solvers is imposed in order to prevent
the RC2 MaxSAT solver from running optimization
indefinitely. The average solve time per question
was <4 ms for closed-book QA, <1 ms for VQA
and <20 ms for NQ (for NQ, the solve time is
< 1/10th of the time needed for a forward pass
through the QA and NLI models). We found only
one batch of test questions for the closed-book QA
task and VQA task where the solver couldn’t find a
solution efficiently, so we set a short timeout (30s
for CBQA, 10s for VQA, none required for NQ).
We also de-duplicate the list of inferred con-
straints before passing the statement and constraint
groups through the MaxSAT solver so that only the
highest-weighted constraints would remain among
their duplicates.
E Entailment Correction Ablations
Table 8 shows the effects of entailment correc-
tion on ConCoRD test performance in closed-book
question answering and VQA experiments for dif-
ferent choices of base model, using the NLI rela-
tion model resulting in the best test set performance
(RoBERTa-Large-MNLI).1765
F Additional “Good” and “Bad” Edit
Pairs
More examples of good and bad edits in the Edit-
ing experiment are presented in Table 10. We also
include good (Figure 4)and bad flip (Figure 5) ex-
amples from the VQA dataset. For the bad flip ex-
amples in VQA, we include different failure modes
to demonstrate the types of potential ConCoRD
errors.
G Good and Bad Flips
For each set of experiments on the test set, we
report the numbers of good and bad flips made by
ConCoRD in Table 9. It can be observed that the
number of good flips is consistently significantly
higher than that of bad flips.
H Hyperparameter Search Details
H.1 Experiments
H.1.1 Closed-Book Question Answering
Hyperparameters (Section 3.4) are tuned jointly us-
inghyperopt on the BeliefBank calibration dataset
(Section 4.1). The search space of βis uniform
between [0.05,1.0], and for λit is uniform be-
tween [0.5,1.0].hyperopt optimizes cumulative
F1 across all entity batches for 300 trials. To speed-
up tuning, we created caches of model beliefs B
and relation sets Rfor each calibration entity
s. This was run on NVIDIA GeForce RTX 3090
GPU, and the largest NLI models took up to two
hours to complete. Using these caches, hyperopt
tuning completes in less than an hour on CPU. The
best performance on the calibration facts for each
of the base Macaw models is reported in Table 11.
The results show that βis higher for the better base
model Macaw-3B.
H.1.2 VQA
Hyperparameters are tuned jointly using hyperopt .
The search space for βis uniform over [0.05,1], for
λit is uniform over [,1]. A total of 100 trials were
performed, updating parameters using TPE, on an
AWS g4dn.xlarge EC2 instance. Each search
took less than one hour. Table 12 shows the se-
lected parameters and their exact-match accuracy
on validation questions.17661767
H.1.3 Information Injection with Natural
Questions
For this round of experiments, we lower the bounds
forβandλafter some initial trials. The bounds
ofβare[0,0.5]and the bounds of λare[0,0.6].
We run hyperopt for 200 trials (often taking ap-
proximately 2 to 3 hours on an NVIDIA GeForce
RTX 3090 GPU) for each of the three NLI mod-
els. Hyperopt optimizes for the highest token-
overlapping F1 score in this experiment.
We report the best validation performance of
each of the QA base models in Table 13.
H.2 Visualizing Hyperparameter Search
Figure 6 shows increases in exact-match accuracy
as they vary with choices of λ,β, for additional
choices of base model for a VQA task, with and
without entailment correction, complementing fig-
ure 3. Interestingly, choosing a different base
model does noticeably effect the optimum value
ofβ; between figures 6b and 6c we see the near-
optimal region shift towards a value of βthat gives
higher confidence in the base model where the base
model produces “better” answers. However, the
increase in accuracy is similar, suggesting that with
appropriate selection of β, ConCoRD can offer sim-
ilar improvements over a range of choices of base
model.1768