
Odunayo Ogundepo, Xinyu Zhang, Shuo Sun, Kevin Duh,andJimmy LinDavid R. Cheriton School of Computer Science, University of WaterlooJohn Hopkins University{oogundep, xinyucrystina.zhang, jimmylin}@uwaterloo.ca{ssun32@jhu.edu, kevinduh@cs.jhu.edu }
Abstract
Language diversity in NLP is critical in en-
abling the development of tools for a wide
range of users. However, there are limited re-
sources for building such tools for many lan-
guages, particularly those spoken in Africa. For
search, most existing datasets feature few or
no African languages, directly impacting re-
searchers’ ability to build and improve infor-
mation access capabilities in those languages.
Motivated by this, we created AfriCLIRMatrix,
a test collection for cross-lingual information
retrieval research in 15 diverse African lan-
guages. In total, our dataset contains 6 mil-
lion queries in English and 23 million rele-
vance judgments automatically mined from
Wikipedia inter-language links, covering many
more African languages than any existing in-
formation retrieval test collection. In addition,
we release BM25, dense retrieval, and sparse–
dense hybrid baselines to provide a starting
point for the development of future systems.
We hope that these efforts can spur additional
work in search for African languages. Afri-
CLIRMatrix can be downloaded at https://
github.com/castorini/africlirmatrix .
1 Introduction
The ever-increasing amounts of information on the
web in different languages highlight the need for
systems that enable users to search in one language
and retrieve relevant documents in another. This
search task, commonly known as cross-lingual in-
formation retrieval (CLIR), is becoming increas-
ingly important. CLIR can break down language
barriers between information seekers and the exten-
sive collections of documents that are available in
diverse languages.
One common approach to CLIR takes advan-
tage of machine translation and monolingual in-
formation retrieval (Zhou et al., 2012; Jiang et al.,
2020). The documents and queries are translated
into the same language before search occurs. Thistranslation is often performed using a variety of
sources, including parallel corpora, bilingual dic-
tionaries, and machine translation (MT) systems.
The effectiveness of this approach relies heavily on
translation quality, which may be a bottleneck for
low-resource languages where high-quality transla-
tions are not readily available.
To address this challenge, researchers have re-
cently explored the use of pretrained multilingual
models (MacAvaney et al., 2020; Shi et al., 2020).
Examples such as mBERT (Devlin et al., 2019) and
XLM-R (Conneau et al., 2020) are often pretrained
on a large collection of multilingual texts, enabling
the models to learn representations across differ-
ent languages. The use of multilingual models for
CLIR often builds on techniques that have previ-
ously been applied to monolingual retrieval (Lin
et al., 2021b).
Regardless of approach, modern neural-based
CLIR models are data hungry, typically requiring
large amounts of query–document pairs that have
been annotated with relevance labels. Such anno-
tated data are expensive to obtain, especially for
low-resource African language pairs. Although
there is ongoing research on training multilingual
models for dense retrieval in low-resource set-
tings (Zhang et al., 2022a,b), there are still not
enough resources for these languages. Existing
CLIR datasets do contain some African languages,
such as CLIRMatrix (Sun and Duh, 2020) and the
MATERIAL corpora (Zavorin et al., 2020). How-
ever, these collections contain only a few African
languages, a tiny fraction of the 2000+ languages
spoken on the continent with hundreds of millions
of speakers (Eberhard et al., 2019). The paucity
of data hinders progress in developing information
access capabilities for Africa.
As a small step towards plugging this gap, we
introduce AfriCLIRMatrix, a new test collection
for cross-lingual information retrieval containing
geographically diverse African languages. This8721resource comprises English queries with query–
document relevance judgments in 15 African lan-
guages automatically mined from Wikipedia. Al-
though we only cover a small set of languages, our
resource already represents a substantial enhance-
ment over existing datasets, as AfriCLIRMatrix
covers geographically diverse languages that are
collectively spoken by 340 million people in Africa
and worldwide.
We hope that this resource will spur research in
retrieval techniques and motivate the development
of more robust datasets for information retrieval in
African languages. As a start, we provide a number
of baselines for researchers to build on: BM25, a
multilingual adaptation of DPR known as “mDPR”,
and a hybrid approach combining the two.
2 Related Work
NLP for African Languages: Natural language
processing for African languages has garnered
some attention in recent years and is gradually be-
coming an area of active research (Adebara and
Abdul-Mageed, 2022). This has resulted in ef-
forts directed at creating resources to aid research
in these languages. These resources include pre-
trained language models (Ogundepo et al., 2022;
Ogueji et al., 2021) as well as datasets for a range
of common tasks (Nekoto et al., 2020; Adelani
et al., 2022, 2021; Muhammad et al., 2022).
Cross-Lingual Information Retrieval: The main
goal of information retrieval systems is to help
users identify relevant information. In some cases,
information exists in multiple languages, hence the
need for cross-lingual information retrieval (Nie,
2010). While such systems enable users to ac-
cess documents in foreign languages, sufficient
quantities of high-quality bilingual data required to
build effective CLIR systems are often unavailable
for low-resource languages (Zavorin et al., 2020).
It is often expensive, time-consuming, and labor-
intensive to build high-quality annotated datasets
in multiple languages.
Researchers have since explored the use of au-
tomated pipelines to construct datasets for mul-
tilingual and cross-lingual information retrieval.
One such pipeline is the translation of docu-
ments/queries into the desired language. For in-
stance, Bonifacio et al. (2021) used an automatic
neural machine translation system to create a mul-
tilingual version of the MS MARCO dataset (Bajaj
et al., 2018) in 13 languages. Other researchers sim-ply incorporated translation in their CLIR systems
(Zhang et al., 2019; Nair et al., 2020).
Another common approach is to exploit exist-
ing large multilingual corpora, e.g., the Common
Crawland Wikipedia. For example, the HC4
corpus for cross-lingual information retrieval was
created from Common Crawl data (Lawrie et al.,
2022). Examples of exploiting Wikipedia for CLIR
include WikiCLIR (Schamoni et al., 2014), CLIR-
Matrix (Sun and Duh, 2020), Large Scale CLIR
(Sasaki et al., 2018), among others. Although
these collections typically feature a diversity of
languages, they do not in general contain many
African languages. Our work builds on Sun and
Duh (2020) and is to our knowledge the first cross-
lingual information retrieval dataset to specifically
focus on African languages.
3 AfriCLIRMatrix
AfriCLIRMatrix is a new information retrieval test
collection comprising queries and documents in 15
diverse African languages mined from Wikipedia,
the largest such dataset that we are aware of.
We focus on cross-lingual information retrieval
with queries in English and documents in various
African languages, listed in Table 1. We use an
automated pipeline to extract document titles from
English Wikipedia articles as queries, and use cross-
language Wikidata links to find relevant articles in
other languages.
Extraction Pipeline: Our mining pipeline is sim-
ilar to the one used in Sun and Duh (2020). For
every “source” Wikipedia article in language L,
there exist inter-language links that connect the
source article to articles about the same topic in
other languages. We leverage these connections to
extract queries and a set of relevant articles in En-
glish, and then use Wikidata backlinks to find rele-
vant articles in other languages if they are available.
We use English article titles as queries because
they are readily available, span multiple domains,
and have articles linked to more languages than
any other language in Wikipedia. However, our
pipeline also supports other forms of queries, for
example, Sasaki et al. (2018) used the first sentence
in each article in their dataset.
To find relevant articles, we use each query to
retrieve a set of 100 articles in English using a bag-
of-words retrieval system (Elasticsearch).Inter-8722
language links for the retrieved articles are then
used to extract similar articles in other languages.
Given that BM25 scores reflect how relevant a doc-
ument (article) is to a given query, we use the scores
to generate relevance judgments for the retrieved
documents (articles). The scores are normalized
and then converted into discrete relevance grades
using the Jenks natural break optimization algo-
rithm (McMaster and McMaster, 2002). The doc-
uments are originally judged on a scale of 0 to 6,
with 0 being irrelevant and 6 being the most rel-
evant. A score of 0 is assigned to all documents
not retrieved by the monolingual English pipeline
using Elasticsearch, while a score of 6 is assigned
to documents from articles directly connected to
the title queries.
Dataset Statistics: A breakdown of AfriCLIR-
Matrix in terms of languages is shown in Table 1.Our dataset is based on the Wikipedia dump re-
leased on April 4, 2022. The number of Wikipedia
documents (articles) for each language is shown
in Table 1; the number of documents in the corpus
for each language is exactly equal to the number of
Wikipedia articles in the corresponding dump. Due
to the lack of sufficient articles for some languages,
we filter out low-quality queries for each language
by discarding queries whose relevant documents
all have low scores (1, 2, and 3). Thus, we retain
only queries where there is at least one relevant
document with score ≥4.
Comparison with other datasets: Table 2 shows a
comparison of AfriCLIRMatrix with existing mul-
tilingual and cross-lingual datasets. The main com-
parison here is the number of African languages
present in each dataset. Of all the African lan-
guages, Swahili appears to be the best-covered8723
language in the listed datasets. This is because
Swahili has relatively more accessible monolingual
data compared to the other languages. As far as
we know, our dataset covers the most African lan-
guages of any comparable resource.
4 Baselines
As a starting point for future research, we release
BM25, mDPR, and sparse–hybrid baselines for
AfriCLIRMatrix. For each language, we split the
extracted queries into training and test sets, as
shown in Table 1. We perform experiments on
the test set and report nDCG@10 and Recall@100
scores for all conditions. Detailed instructions for
reproducing all of these experiments can be found
in our repository.
BM25: We report a bag-of-words BM25 (Robert-
son and Zaragoza, 2009) baseline obtained using
the implementation provided by the Anserini IR
toolkit (Yang et al., 2018), which is built on the
Lucene open-source search library. We use the de-
fault Anserini configuration ( k= 0.9,b= 0.4)
and whitespace tokenization for analyzing the doc-uments (and queries) since Lucene does not cur-
rently provide language-specific analyzers for any
of the languages in AfriCLIRMatrix. Note that in
this condition we are applying the same exact ana-
lyzer to both queries and documents (in different
languages); see discussion of results below.
mDPR: We also report zero-shot results from
mDPR, which is a multilingual adaptation of the
Dense Passage Retriever (DPR) model (Karpukhin
et al., 2020), where BERT in DPR is simply re-
placed with multilingual BERT (mBERT). The
mDPR implementation in our experiments adopts
a shared-encoder design (i.e., the same encoder
for queries and passages) and was fine-tuned on
the MS MARCO passage ranking dataset (Bajaj
et al., 2018). Zhang et al. (2022a) showed this to
be an effective baseline. Retrieval is performed in
a zero-shot manner using the Faiss flat index imple-
mentation provided by the Pyserini IR toolkit (Lin
et al., 2021a).
Hybrid: Hybrid results are a combination of sparse
and zero-shot dense retrieval runs described above.
The dense and sparse retrieval runs are combined8724using Reciprocal Rank Fusion (RRF) (Cormack
et al., 2009).
Although queries and documents in our experi-
ments are notin the same language, we observe
that BM25 provides a strong baseline. This makes
sense since, due to the nature of Wikipedia article
titles, most of the queries are named entities. En-
glish entities often appear in non-English articles,
either because the entity has the same surface form
or due to code switching. This makes it possible
to retrieve relevant content based solely on exact
lexical matches.
Results in Table 3 show that mDPR effective-
ness varies across languages, but overall it is not as
effective as BM25. Given the prevalence of entity-
centric queries, this finding is consistent with Sci-
avolino et al. (2021). We observe a clear connec-
tion between the script of the language and the
relative effectiveness of BM25 vs. mDPR in terms
of nDCG@10. Among the 11 languages that use
the Latin script, BM25 outperforms mDPR on all
butsnaandwol; Similarly, among the other 4 lan-
guages, mDPR outperforms BM25 on all but arz.
These results are expected, as lexical matching is
straightforward when queries and documents are
in the same script. Overall, we see that dense re-
trievers still have a long way to go for effective
cross-lingual information retrieval.
Finally, results demonstrate the effectiveness of
combining sparse and dense retrieval. For 11 lan-
guages, the hybrid approach is more effective than
either in terms of nDCG@10. This means that,
even though mDPR is less effective than BM25
in most cases, it can still provide complementary
relevance signals to improve BM25 rankings.
5 Conclusion and Future Work
To spur interest in information retrieval research
and development for African languages, we intro-
duce a new dataset for cross-lingual information
retrieval in 15 languages across different African
regions. AfriCLIRMatrix is a collection of bilin-
gual datasets with English queries and documents
in 15 African languages. In addition to releasing
the resource, we also provide baselines as a starting
point for further research.
6 Limitations
Language Coverage & Diversity: Although our
dataset covers 15 African languages, we still fall
far short of the over 2000+ languages spoken onthe continent. However, it is worth noting that
our dataset covers the largest African languages in
terms of the number of speakers. Collectively, lan-
guages in our dataset are spoken by an estimated
340 million people. In terms of typological di-
versity, we cover three language families (Niger–
Congo, Indo–European, Afro–Asiatic), but are
missing others due to the lack of data in Wikipedia.
English-Centric Queries: Our dataset only con-
tains English queries. Ideally, we would like to
provide queries in all 15 African languages, but
this is technically challenging due to the way we
construct the collection: We first query for docu-
ments in-language, then propagate the relevance
labels to a new language via Wikidata links.
We did explore running our data extraction
pipeline on all pairs of languages, but the results
were too sparse to be useful. One ramification of
bootstrapping the collection from English queries
and associated relevance judgments on English
Wikipedia documents is that there may exist bias
in the types of queries (e.g., fewer questions about
African people and events compared to English)
and in the way they are answered. We acknowledge
this limitation; in future work, it will be important
to investigate other data creation methods that yield
African-centric queries.
Incomplete Inter-language Links: Wikipedia pro-
vides inter-language links connecting articles on
the same topic in different languages. While run-
ning our data creation pipeline, we observed that
some links to existing articles in other languages
are missing. In particular, these links are often
limited and exist only for high-resource languages.
Therefore, we might have missed the labeling of
some relevant documents. For future work, we
will explore the use of cross-lingual link discovery
systems (Lefever et al., 2012) to update existing
inter-language links and improve the dataset. Also,
the absence of human-annotated relevance judg-
ments directly impacts the quality of the dataset.
We instead present this work as a starting point for
future research in creating more IR resources for
African languages.
Acknowledgements
This research was supported in part by the Canada
First Research Excellence Fund and the Natural Sci-
ences and Engineering Research Council (NSERC)
of Canada; computational resources were provided
by Compute Ontario and Compute Canada.8725References872687278728