
Lucy Lu WangYulia OtmakhovaJay DeYoungThinh Hung Truong
Bailey E. KuehlErin BransomByron C. WallaceUniversity of WashingtonAllen Institute for AIUniversity of MelbourneNortheastern University
Abstract
Evaluating multi-document summarization
(MDS) quality is difficult. This is especially
true in the case of MDS for biomedical liter-
ature reviews, where models must synthesize
contradicting evidence reported across different
documents. Prior work has shown that rather
than performing the task, models may exploit
shortcuts that are difficult to detect using stan-
dardn-gram similarity metrics such as ROUGE.
Better automated evaluation metrics are needed,
but few resources exist to assess metrics when
they are proposed. Therefore, we introduce
a dataset of human-assessed summary quality
facets and pairwise preferences to encourage
and support the development of better auto-
mated evaluation methods for literature review
MDS. We take advantage of community sub-
missions to the Multi-document Summariza-
tion for Literature Review (MSLR) shared task
to compile a diverse and representative sam-
ple of generated summaries. We analyze how
automated summarization evaluation metrics
correlate with lexical features of generated sum-
maries, to other automated metrics including
several we propose in this work, and to aspects
of human-assessed summary quality. We find
that not only do automated metrics fail to cap-
ture aspects of quality as assessed by humans,
in many cases the system rankings produced by
these metrics are anti-correlated with rankings
according to human annotators.
1 Introduction
Multi-document summarization (MDS) requires
models to summarize key points across a set of re-
lated documents. Variants of this task have drawn
significant attention in recent years, with the intro-
duction of datasets in domains like newswire (Fab-
bri et al., 2019), Wikipedia (Gholipour Ghalandari
et al., 2020), science (Lu et al., 2020), medical liter-
ature reviews (DeYoung et al., 2021; Wallace et al.,Figure 1: Spearman correlations between rankings pro-
duced by human-assessed quality facets (F1-F4), auto-
mated metrics (M1-M7), and combined pairwise sys-
tem rankings (PW-combined) on the Cochrane MSLR
dataset. Rankings from automated metrics are highly
correlated as a group except for PIO-Overlap (A). PIO-
Overlap rankings are strongly correlated with rank-
ings from human-assessed facets, especially PIO agree-
ment (B). Metrics most strongly associated with PW-
Combined rankings are Delta-EI and PIO-Overlap (C).
Rankings from commonly reported automated metrics
like ROUGE and BERTScore are not correlated or anti-
correlated with human-assessed system rankings (D).
2020), and law (Shen et al., 2022); and substantial
methodological work to design model architectures
tailored to this task (Xiao et al., 2022; Pasunuru
et al., 2021; Liu and Lapata, 2019).
In this work, we focus on MDS for literature
reviews (MSLR), a challenging variant of the task
in which one attempts to synthesize all evidence
on a given topic. When manually performed, such
reviews usually take teams of experts many months
to complete. Good review summaries aggregate the
results of different studies into a coherent passage,
while the evidence presented in the input studies
will often be in conflict (Wallace et al., 2020; DeY-
oung et al., 2021; Wadden et al., 2022), complicat-9871ing the synthesis task.
Evaluating conditional text generation models
is notoriously difficult, impeding progress in the
field. Prior work on summarization evaluation has
proposed various lexical and modeling-based ap-
proaches to assess generation quality, but these
metrics predominately use correlation with human-
assessed quality facets over relatively small num-
bers of examples to demonstrate utility (Fabbri
et al., 2021; Wang et al., 2020; Deutsch and Roth,
2020; Yuan et al., 2021). This limitation of current
metric evaluation implies that existing automated
measures may not generalize well. Further, evalua-
tion in the multi-document setting adds additional
complexity, e.g., prior work has shown that MDS
models may sometimes exploit shortcuts that do
not reflect as detectable changes in automated met-
rics (Wolhandler et al., 2022; Giorgi et al., 2022a).
To address these challenges, we collect human
annotations to evaluate current models and to sup-
port automated metrics development for the medi-
cal MDS task. We construct a dataset of such eval-
uations using public submissions from the 2022
MSLR shared task on literature review MDS.Se-
lecting top-performing models, we label the sum-
mary quality of a sample of these models’ outputs
on the Cochrane subtask (Wallace et al., 2020). As
part of our analysis, we compare system rankings
produced by automated metrics and human evalu-
ations. Strikingly, our results highlight consistent
and significant disagreements between automated
metrics and humans, motivating the need for better
automated evaluation metrics in this domain.
We contribute the following:
•A dataset of summaries and quality annotations
on participant submissions to the MSLR shared
task. We include human annotations for 6 models
on 8 individual quality facets (§3.2) and pairwise
preferences provided by five raters (§3.3).
•An analysis of lexical features among inputs, gen-
erated, and target summaries (§4), showing a
large amount of undesirable copying behavior.
•An analysis of correlations between automated
evaluation metrics and human-assessed quality
(§5), and the differences in system rankings pro-
duced by automated metrics versus human eval-uation (§6). We propose several novel evalua-
tion metrics based on desired features of MSLR
summaries (§5). We find that system rankings
derived from commonly reported automated met-
rics are notcorrelated or even anti-correlated
with rankings produced by human assessments
of quality, though some of the metrics we pro-
pose demonstrate promise in capturing certain
quality facets.
2 Background
The MSLR shared task was introduced to bring
attention to the challenging task of MDS for litera-
ture reviews. The shared task comprised two sub-
tasks, based on the Cochrane (Wallace et al., 2020)
and MSˆ2 (DeYoung et al., 2021) datasets. The
Cochrane dataset consists of 4.6K reviews from
the Cochrane database of systematic reviews. In-
puts are abstracts of papers cited by the review and
target summaries are the Authors’ Conclusions sub-
sections of review abstracts. The MSˆ2 dataset in-
cludes 20K reviews and is semi-automatically con-
structed from biomedical literature reviews indexed
by PubMed. We refer the reader to the original pub-
lications for details concerning dataset construction
(Wallace et al., 2020; DeYoung et al., 2021).
Shared task organizers provided training and val-
idation splits for both datasets, and solicited model
submissions to two public leaderboards, where
models were evaluated on a hidden test split. Mod-
els were ranked on the leaderboard using ROUGE
(-1, -2, -L; Lin 2004), BERTScore (Zhang et al.,
2020a), and Delta-EI (DeYoung et al., 2021; Wal-
lace et al., 2020), a metric based on evidence infer-
ence (Lehman et al., 2019) classifications.
3 Dataset
We construct our dataset from system submissions
to the Cochrane subtask leaderboard for the 2022
MSLR shared task (provided to us by task organiz-
ers). We only sample from the Cochrane subtask
due to the greater number and variety of successful
submissions. We include all summaries from the
leaderboard, though we only perform human eval-
uation on summaries generated by 6 models (dis-
cussion in §3.1). We define and apply two human
evaluation protocols to a sample of summaries from
these 6 systems. The first (§3.2) is a facet-based
evaluation derived from the analysis conducted in
Otmakhova et al. (2022b) and the second (§3.3) is
a pairwise preference assessment.98723.1 MDS systems
We perform human evaluation on the outputs of
6 MDS systems. Five of these are community
submissions to the MSLR-Cochrane leaderboard,
while a sixth is a baseline system (BART-Cochrane)
included for reference. These systems represent
different Transformer model architectures (BART,
BART-large, Longformer, BigBird), input selection
strategies (Shinde et al., 2022), and differential rep-
resentation/attention on input tokens (Otmakhova
et al., 2022a; DeYoung et al., 2021). We exclude
some systems from human evaluation due to poor
summary quality (disfluent) or being baselines. We
briefly describe our 6 systems below.
ITTC-1 / ITTC-2 Otmakhova et al. (2022a)
fine-tuned PRIMERA (Xiao et al., 2022) for the
Cochrane subtask and exploited the use of global
attention to highlight special entities and aggregate
them across documents. We include two settings
from the leaderboard, one that adds global attention
to special entity marker tokens (ITTC-1) and one
that adds global attention to entity spans (ITTC-2).
BART-large Tangsali et al. (2022) fine-tuned
BART-large (Lewis et al., 2020) for the subtask.
SciSpace Shinde et al. (2022) defined an extract-
then-summarize approach, combining BERT-based
extraction of salient sentences from input docu-
ments with a BigBird PEGASUS-based summa-
rization model (Zaheer et al., 2020).
LED-base-16k Giorgi et al. (2022b) fine-tuned
Longformer Encoder-Decoder (Beltagy et al.,
2020) for the Cochrane subtask following a similar
protocol described in Xiao et al. (2022).
BART (baseline) The baseline follows the pro-
tocol in DeYoung et al. (2021) to fine-tune BART
(Lewis et al., 2020) for the Cochrane subtask.
Model rankings originally reported on the MSLR-
Cochrane leaderboard are provided in Table 1.
3.2 Facet-based Human Evaluation
We adapt a facet-based human evaluation proce-
dure from the analysis in Otmakhova et al. (2022b).
In their work, the authors analyzed baseline model
outputs from MSˆ2 (DeYoung et al., 2021) with re-
spect to fluency, PIO alignment, evidence direction,
and modality (or strength of claim). PIO standsfor Population (who was studied? e.g. women with
gestational diabetes), Intervention (what was stud-
ied? e.g. metformin), and Outcome (what was mea-
sured? e.g. blood pressure), and is a standard frame-
work for structuring clinical research questions
(Huang et al., 2006). These are important elements
thatmust align between generated and target sum-
maries for the former to be considered accurate.
Evidence direction describes the effect (or lack
thereof) that is supported by evidence (e.g., the
treatment shows a positive effect, no effect, or a
negative effect, comparatively). The strength of the
claim indicates how much evidence or how strong
the evidence associated with the effect might be.
We derive 8 questions based on this analysis:
1.Fluency : if the generated summary is fluent
2.Population : whether the population in the gen-
erated and target summaries agree
3.Intervention : as above for intervention
4.Outcome : as above for outcome
5.Effect-target : effect direction in the target
6.Effect-generated : effect direction in the gener-
ated summary
7.Strength-target : strength of claim in the target
8.Strength-generated : strength of claim in the gen-
erated summary
Of the 470 reviews in the Cochrane test set, we
sample 100 reviews per system for facet annota-
tions (600 summaries in total). For 50 reviews, we
fully annotate all summaries from the 6 systems
(the overlapping set); for the other 50 reviews per
system, we sample randomly from among the re-
maining reviews for each system (the random set).
All together, at least one system’s outputs are an-
notated for 274 reviews in the test set. We elect
for this sampling strategy to balance thoroughness
(having sufficient data points to make direct com-
parisons between systems) and coverage (having
annotations across more review topics).
For each sampled instance, we show annotators a
pair of (target, generated) summaries from a review
and ask them to answer 8 questions regarding these
(details in App. A). A sample of 10 reviews from
the overlapping set (60 summary pairs) and 10 from
the random set (10 summary pairs) are annotated
by two annotators. We compute inter-annotator
agreement from these and report Cohen’s Kappa
and agreement proportions for all eight facets in Ta-
ble 2. Several facets have lower agreement (Popu-
lation, Outcome, and Strength-target), though most
disagreements are between similar classes (e.g. par-9873
tial agree vs. agree); more on this in App. A.
Two annotators with undergraduate biomedical
training annotated these samples. We arrived at
the final annotation protocol following two rounds
of pilot annotations on samples from the MSˆ2
dataset and discussing among authors to resolve
disagreements and achieve consensus.
3.3 Pairwise Human Evaluation
We perform pairwise comparisons to elicit human
preferences between system-generated summaries
and to study how facet-based quality maps to holis-
tic summary quality.
We sample pairs of system generations from our
dataset, half from the overlapping set of reviews
annotated for facet evaluations, and half from other
reviews. A different subsample of these pairwise
comparisons is provided to each of 5 raters, who
are asked to complete up to 100 judgments each.
For each comparison, the annotator is given the
target summary, the system A summary, the system
B summary, and asked “Which of A or B more
accurately reflects the content of the target sum-
mary?” where the options are A, B, or Neither. All
annotators are knowledgable in BioNLP and one
annotator has biomedical training. Four annota-
tors completed 100 pairwise comparisons; a fifth
completed 50 comparisons.We first determine system rankings per individ-
ual annotator. To tally annotations: if A is preferred
over B, system A gets 1 point; if B over A, sys-
tem B gets 1 point; if Neither is preferred, neither
system gets a point. Systems are ranked by total
points; tied systems receive the same ranking. To
determine a combined ranking based on the prefer-
ences of all 5 annotators, we adopt the Borda count
(Emerson, 2013), a ranked choice vote counting
method that maximizes the probability of selecting
the Condorcet winner.In this method, for each an-
notator (voter), we award each system the number
of points corresponding to the number of systems
ranked below it, e.g., for a set of systems ranked
1-6, the rank 1 system receives 5 points, the rank
2 system 4 points, and so on. System rankings re-
sulting from the Borda count are shown in Table 1
under Pairwise-Combined.
We perform bootstrapping over each annotator’s
pairwise annotations to estimate the error of the
overall system rankings. We resample each indi-
vidual’s pairwise preferences with replacement and
compute a new combined ranking. Over 10000
bootstrap samples, the average Spearman ρof the
resampled rankings against the initial rankings is
0.716 (s/d = 0.197).
3.4 Dataset Statistics
Our final dataset consists of 4658 summaries gener-
ated by 10 systems over 470 review instances from
MSLR-Cochrane. Of these summaries, 597 from
6 systems are annotated on 8 quality facets. We
also include 452 pairwise comparisons from five
annotators. In addition to annotations, we compute
and include automated metrics for each generated
summary to facilitate analysis (more in §5).9874
4 Analysis of generated summaries
We perform lexical analysis of input abstracts, sys-
tem generated summaries, and target summaries in
our dataset, summarizing our findings below.
Input copying and synthesis To assess similar-
ity between inputs and summaries, we first apply
the evidence inference pipeline (Lehman et al.,
2019; DeYoung et al., 2020)to identify an evi-
dence statement in each input document and clas-
sify it with an effect direction. Between each input
evidence statement and the target and generated
summaries, we compute ROUGE-1 scores. We
compute the Synthesis rate as how often the effect
direction agrees between the most similar evidence
statement (by ROUGE-1 score) and the generated
summary. In Table 3, we find that system genera-
tions match the effect of the closest input at a high
rate (0.41-0.46), though no more frequently than
we would expect based on the synthesis rate for the
target summaries (0.48). Using ROUGE-1 scores,
we also determine how often a generated summary
is closer to an input document than the target ( Input
Match ), which might indicate whether a system is
performing an implicit synthesis by selecting an in-
put and copying it. We find that systems sometimes
copy inputs, but not in any consistent way.
n-gram self-repetition Previously, Salkar et al.
(2022) noted that models fine-tuned on the
Cochrane corpus tend to generate summaries con-
taining repeating patterns; however, they claim that
the amount of such self-repetitionis fairly consis-
tent between model-generated and human-written
text. We analyze self-repetition rates for long n-
grams (5- to 10-grams) and show that their occur-
rence rates are much higher in generated summaries
than in human-written summaries. These long n-
grams do not just represent stylistic patterns, but
can contain important information such as the ef-
fect direction, e.g., “there is insufficient evidence
to support the use” (see App. B for details), so the
high rate of self-repetition is very concerning.
We find a clear distinction between generated
and target summaries in the self-repetition of longer
sequences, such as 7- to 10-grams (Figure 5 in
App. B). Though the amount of self-repeating 10-
grams in human-written summaries is negligible,
it reaches over 80% in some of the examined mod-
els’ outputs. The self-repetition rate for specific
n-grams (the number of documents in which an
n-gram appears) in generated summaries is also
much higher than in the targets: some 7-grams oc-
cur in up to 70% of generated summaries (Figure 2;
trends for other long n-grams are in App. B).
To determine the origin of these long n-grams,
we calculate their overlap with summaries in the
Train set and their corresponding input documents.
While overlap with inputs is nearly zero, up to 90%
of long n-grams are also found in Train set sum-
maries (Figure 6 in App. C). Interestingly, models
with global attention (LED or PRIMERA-based)
seem to replicate more long sequences from the
Train set summaries than BART-based ones, while
in the Pegasus-based system (SciSpace) a smaller
amount of self-repetition can be explained by fine-9875tuning. Finally, we observe that though the dis-
tributions of self-repeating n-grams in the target
summaries of the Testset and Train set are very sim-
ilar (Figure 3; left), in generated summaries the rate
of self-repetition increases up to 500x compared to
occurrence in the Train set summaries (Figure 3;
right). Models amplify repeating patterns from the
Train set to unnatural proportions!
5 Automated evaluation metrics
We compute automated metrics for each generated
summary and include instance-level scores in our
dataset. We investigate how these metrics correlate
with other metrics (§5.1) and with human evalua-
tion facets (§5.2).
Metrics from the MSLR leaderboard:
ROUGE : The leaderboard reported system-level
ROUGE-1, ROUGE-2, and ROUGE-L F-scores
(Lin, 2004). We report these same three metrics; in
some plots, due to space constraints, we show the
average of these three ROUGE metrics, which we
call Avg-ROUGE-F.
BERTScore : We compute and report BERTScore-
F (Zhang et al., 2020a) for each generated summary
as computed using the RoBERTa-large model.
Delta-EI : We compute Delta-EI as introduced by
Wallace et al. (2020) and modified by DeYoung
et al. (2021) for the MSLR shared task. The metric
computes the probability distributions of evidence
direction for all intervention-outcome (I/O) pairs
between inputs and the target and generated sum-
maries. The final score is a sum over the Jensen-
Shannon Divergence of probability distributions
over all I/O pairs. Lower values indicate higher
similarity to the target summary.
Other metrics we propose and examine:
NLI/STS/ClaimVer : These metrics leverage
Sentence-BERT (Reimers and Gurevych, 2019)
and are computed as the cosine similarity between
the embedding of the target summary and the em-
bedding of the generated summary when encoded
with trained SBERT models. We use three pre-
trained variants of SBERT: RoBERTa fine-tuned on
SNLI and MultiNLI (NLI); RoBERTa fine-tuned on
SNLI, MultiNLI, and the STS Benchmark (STS);
and PubMedBERT fine-tuned on MS-MARCO and
the SciFact claim verification dataset (ClaimVer).
PIO-Overlap : Following Otmakhova et al.
(2022a), we employ a strong PIO extractor (Bio-
LinkBERT (Yasunaga et al., 2022) trained on EBM-
NLP (Nye et al., 2018)) to extract PIO spans. For
each target-generated pair, we define PIO-Overlap
as the intersection of the two extracted sets of PIO
spans normalized by the number of PIO spans in
the target summary. Spans are only considered to
overlap if they have the same label and one span is
a subspan of the other.
5.1 Correlation between automated metrics
We compute Pearson’s correlation coefficients be-
tween pairs of metrics (Figure 8 in App. E). Most
automated metrics are significantly correlated (p <
0.01), except Delta-EI and PIO-Overlap. ROUGE
and BERTScore show a strong positive correlation
(r = 0.75), and NLI and STS have a strong positive
correlation (r = 0.92), unsurprising since the under-
lying models are trained on similar data. Delta-EI
presents as bimodal, with two peaks around 0 and 1.
Distributions of instance-level automated metrics
per system are shown in App. D.
System ranks (§6) produced by automated met-
rics are highly correlated except for PIO-Overlap,
which is anti-correlated (Figure 1). Ordering sys-
tems based on these metrics generally result in the
same or similar rankings ( ρ≥0.77 for all pairs of
metrics besides PIO-Overlap), e.g., rankings from
ClaimVer, NLI, and STS are identical ( ρ= 1).
5.2 Correlation between automated metrics
and human judgements
We investigate the relationship between automated
metrics and human facet-based annotations. For
this analysis, we normalize human facets to 4 agree-
ment scores: Fluency, PIO, Direction, and Strength,
each in the range [0, 1] (details in App. F).9876Correlation coefficients between automated met-
rics and these four agreement scores are given in
Table 4; PIO correlations are plotted in Figure 10 in
App E. In general, there is weak to no correlation
between metrics and human-assessed Fluency, PIO,
Direction, and Strength, suggesting that automated
metrics may not be adequately capturing aspects of
summaries that humans determine to be important.
The exception is PIO-Overlap, which has a statisti-
cally significant correlation with human-assessed
PIO agreement, and presents as a promising future
metric for the MSLR task; ClaimVer is also weakly
correlated with PIO agreement.
Disappointingly, Delta-EI does not correlate
with human-assessed Direction agreement. We
investigate this further by computing empirical cu-
mulative distribution functions (ECDFs) for each
of the metrics w.r.t. Direction agreement (App. E).
Delta-EI exhibits a small but desirable difference
between instances where Direction agrees and in-
stances where Direction disagrees (Agrees is more
likely to have lower Delta-EI scores than Dis-
agrees). In sum, Delta-EI shows some promise
in detecting differences in Direction agreement,
though further refinement of the metric is needed.
6 Comparing system rankings
Evaluation metrics for summarization can be used
in two settings, to judge performance at the in-
stance level (comparing individual summaries) or
at the system level (comparing model performance
over many instances). Here, we compare system-
level rankings produced by automated metrics, hu-
man facet evaluation, and pairwise preference an-
notations to determine whether automated metrics
effectively rank systems as humans would.
System rankings are computed by averaging the
instance-level metric values or scores across all re-
view instances for each system, and ranking from
best to worst average score (direction depends on
metric; higher is better for all scores except Delta-
EI). We only average metrics over the subset of
reviews for which we have human annotations.
This ensures a fair comparison in the circumstance
where we have selected an annotation sample that
a system performs particularly well or poorly on.
By doing this, the system rankings we present here
are different than those computed using the same
metrics from the MSLR leaderboards. We do not
intend our computed rankings to be interpreted as
the true system ranking; our analysis focuses on
whether automated metrics and human evaluation
are able to produce similar rankings of systems.
Table 1 shows rankings as assessed by all auto-
mated metrics and human scores; Figure 1 shows
Spearman correlation coefficients.
Rankings by automated metrics are not cor-
related with rankings by human evaluation
In general, system rankings from commonly re-
ported automated metrics are not correlated or
anti-correlated (lighter blue) with system rank-
ings produced by human judgments. System rank-
ings from automated metrics are highly correlated
among themselves ( ρclose to 1), aside from PIO-
Overlap. PIO-Overlap rankings are strongly cor-
related with rankings from human PIO agreement.
PIO-Overlap and Delta-EI ranks also correlate with
the combined pairwise rankings, again suggesting
that these two metrics may be the most promising
for capturing human notions of summary quality.
Pairwise assessments do not weigh facets equally
Pairwise-combined rankings are correlated with
facet-based rankings for Fluency and PIO, but not
Direction and Strength of claim. This may indicate
that Fluency and PIO are more detectable problems,
or that issues in Fluency and PIO are more preva-
lent in our data. The rank correlations also show
that Direction and Strength are highly correlated
and may capture similar aspects of system-level
summary quality, making the case for dropping one
of the two (likely Strength) in future annotations.
Pairwise preferences suggest that annotators
weigh facets differently In Figure 4, we show
Spearman correlation coefficients of facet-based
rankings against the rankings of five pairwise anno-
tators and the combined pairwise ranking. These9877coefficients suggest that annotators weigh facets
differently when comparing system output. Anno-
tator 1 ranks similarly to Fluency and PIO facets,
Annotators 2 and 5 rank similarly to PIO and Di-
rection facets, while Annotators 3 and 4’s rankings
are uncorrelated with most facets.
7 Related work
Beyond ROUGE (Lin, 2004) and BERTScore
(Zhang et al., 2020a), an extensive list of n-gram
(Papineni et al., 2002; Banerjee and Lavie, 2005)
and model-based (Zhao et al., 2019; Gao et al.,
2020; Martins et al., 2020; Sellam et al., 2020;
Yuan et al., 2021) summarization evaluation met-
rics have been proposed in the literature. In particu-
lar, model-based approaches that use question gen-
eration and question answering (Wang et al., 2020;
Durmus et al., 2020; Deutsch et al., 2021) or NLI-
based models (Kryscinski et al., 2020) have been
proposed to assess summary factual consistency.
Fabbri et al. (2021) and Deutsch et al. (2022) pro-
vide more thorough evaluations of many of these
metrics on select summarization tasks. We perform
evaluations using metrics previously reported on
the MSLR task, and leave a systematic evaluation
of metrics on this task and others to future work.
In Zhang et al. (2020b), the authors performed
fact verification on generated radiology reports us-
ing an information extraction module, by aligning
the extracted entities with entities found in the ref-
erence summary. Our PIO-Overlap metric simi-
larly uses a PIO entity extraction module to assess
concept overlap between generated and reference
summaries. Falke et al. (2019) proposed to use NLI
models to rank summaries by average entailment
score per sentence against the input documents;
this shares similarities with the Delta-EI score we
evaluated, which attempts to quantify agreement
relative to the reference summary with respect to
the direction of evidence reported.
Deutsch et al. (2022) investigated system-level
rankings produced by automated metrics and hu-
man evaluation and found minimal correlation be-
tween them, a finding corroborated by our work.
Liu et al. (2022) introduced the robust summa-
rization evaluation (RoSE) benchmark, contain-
ing human judgments for system outputs on the
CNN/DM, XSum, and SamSum datasets. We
extend such work into a novel domain (medical
MDS for literature review) and demonstrate dif-
ferences in automated metric performance and hu-man evaluation in our domain and task. For exam-
ple, though ROUGE correlates with human pref-
erences in single-document (CNN/DM) and multi-
document (MultiNews) news summarization, we
find that it is poorly correlated with human judg-
ments and preferences in the MSLR task.
Recent developments in large language mod-
eling have also shifted the goalposts for evalua-
tion. Goyal et al. (2022) found that although hu-
mans overwhelmingly prefer zero-shot GPT-3 sum-
maries for news summarization, automated metrics
were unable to capture this preference; they in-
troduced a benchmark of human judgments and
rationales comparing system outputs on the single-
document news summarization task. More recently,
Shaib et al. (2023) demonstrated that GPT-3 can be
adapted for the MSLR task, and though the model
outputs are generally found by human annotators to
be faithful to the inputs, in the MDS setting the ev-
idence direction often disagrees with the reference.
Detecting these disagreements and developing auto-
mated metrics that can capture such disagreements
are valuable pursuits and one of the motivations
for our work. Further investigation into whether
automated metrics developed using limited human
evaluation benchmarks such as the dataset we in-
troduce here will be a goal for future work.
8 Discussion
MDS for literature review may involve notions of
summary quality not readily captured by standard
summarization evaluation metrics. For example,
our lexical analysis of generated summaries re-
veals a concerning level of self-repetition behav-
ior, which is not penalized by standard metrics.
Through two independent human evaluations (facet-
based and pairwise preferences), we also show that
automated metrics such as ROUGE and BERT-
Score are poorly correlated or even anti-correlated
with human-assessed quality. This is not to say that
these metrics do not provide any utility. Rather,
further work is needed to understand what aspects
of summary quality these metrics capture, and how
to use them in combination with other metrics,
novel metrics yet unintroduced, as well as human
evaluation to better assess progress. We note that
ours is not a systematic analysis of all automated
summarization evaluation metrics, but is a focused
study on evaluation metrics reported for the MSLR
shared task and which we introduce under the hy-
pothesis that they may be useful for capturing some9878quality facets associated with this task. For those
interested in the former, please refer to studies such
as Fabbri et al. (2021) or Deutsch et al. (2022).
A positive finding from our work is the promise
of the PIO-Overlap and Delta-EI metrics. Delta-
EI shows some potential to capture evidence di-
rectional agreement between summaries, though
the metric as currently implemented is noisy and
does not cleanly separate summaries that agree
and disagree on direction. PIO-Overlap, a met-
ric we introduce, correlates with human-assessed
PIO agreement, suggesting that it could be a per-
formant, scalable alternative to human evaluation
of this quality facet. Still, more work is needed
to probe how variants of these metrics could be
adapted to evaluate performance on MSLR and
other MDS tasks.
Finally, we note that human evaluation is diffi-
cult because people value different qualities in sum-
maries. The rank-based analysis we perform does
not account for interactions between related quality
facets and is unable to elicit relationships between
overall quality and individual quality facets. The
majority of pairwise preference annotations in our
dataset also include short free text justifications for
preference decisions, which could be used to fur-
ther study this problem. Other promising directions
for future work involve studying how to optimally
elicit human preferences, such as how to sample
instances for labeling to maximize our confidence
in the resulting system-level rankings.
9 Conclusions
There have been major recent advances in the gener-
ative capabilities of large language models. Models
like ChatGPT,GPT-3 (Brown et al., 2020), and
PubmedGPTdemonstrate aptitude on many tasks
but have also been shown to confidently produce
factually incorrect outputs in specialized and tech-
nical domains.Medicine is a specialized domain
where incorrect information in generated outputs
is difficult to identify and has the potential to do
harm. There is therefore a pressing need for the
community to develop better methods to assess the
quality and suitability of generated medical texts.
Our investigation confirms that there is significant
room for improvement on medical MDS evalua-tion. We hope that the resources and findings we
contribute in this work can assist the community
towards this goal.
Limitations
Though we include 6 systems in our annotation
which reflect the current state-of-the-art, all of the
models are Transformer-based and fine-tuned on
just the Cochrane dataset, which may limit the
diversity of our generated summaries. Addition-
ally, none of the systems are generating summaries
that approach the accuracy of human-written sum-
maries. As a consequence, though the summaries
in our dataset span the spectrum of quality, they
may have less coverage on the higher end of quality
(summaries approaching the accuracy and utility of
human-written review summaries).
Our analysis of evaluation metrics also assumes
the existence of reference summaries. In many
real-world summarization scenarios, reference sum-
maries do not exist, and reference-free evaluation
metrics are needed for assessment. We refer the
reader to related work in reference-free summariza-
tion evaluation (Vasilyev et al., 2020; Gao et al.,
2020; Luo et al., 2022), which have been found in
some settings by Fabbri et al. (2021) to exhibit even
lower correlation with human notions of summary
quality; the performance of these metrics on MSLR
evaluation is unknown and is left to future work.
Our notions of summary quality also do not nec-
essarily correspond to clinical utility. As with any-
thing in the medical setting, it is of utmost im-
portance to verify correctness and the quality of
evidence before using any generated text to make
or guide clinical decisions.
Ethical Considerations
As with other applications of NLP in the medical
domain, results of MSLR systems must be verified
by domain experts before they should be consid-
ered for use in clinical guidance. We do not intend
the system outputs included in our dataset and anal-
ysis to be used for such end applications, as this
would be clearly premature given the low quality
of generated summaries and our lack of ability to
assess the prevalence of factuality errors in these
summary texts. Nonetheless, we believe that medi-
cal MDS holds eventual promise, and it is of vital
importance that we study its challenges and how to
measure and detect quality issues in generated text.9879Acknowledgements
This research was partially supported by National
Science Foundation (NSF) grant RI-2211954, by
the National Institutes of Health (NIH) under
the National Library of Medicine (NLM) grant
2R01LM012086. YO and THT are supported by
the Australian Government through the Australian
Research Council Training Centre in Cognitive
Computing for Medical Technologies (project num-
ber ICI70200030).
References988098819882
A Facet-based Annotation
The questions and answer options shown to anno-
tators for facet annotation are shown in Table 5.
If merging all Yes and Partial Yes classes, agree-
ment proportion between annotators increases for
Fluency (0.87 →0.97), Population (0.56 →0.64),
Intervention (0.77 →0.90), and Outcome agree-
ment (0.36 →0.44).
B Self-repetition rates in generated
summaries
Most of the long n-grams repeating across docu-
ments contain meaningful statements regarding the
direction or strength of effect findings rather than
purely stylistic patterns, which means that the sys-
tems are prone to introducing factuality mistakes
by replicating common statements. In Table 6 we
show the examples of the most repetitive 8-grams
for the 6 models, together with the percentage of
generated summaries they occur in.
We also show that the self-repetition rate for
n-grams with n > 4 have very dissimilar trends
for generated summaries in comparison to human-
written summaries (Figure 5) The amount of 5-
grams and higher self-repetition also differs be-
tween models .
C Copying self-repeating n-grams from
training set
In Figure 6, we show the percentages of self-
repeating n-grams from generated summaries
which can also be found in the target summaries in
the Train set.
D Automated metric distributions per
system
Distributions of automated metrics for all instances
per system are shown in Figure 7.
E Correlations between metrics in the
Cochrane dataset
We present correlations between all automated met-
rics along with correlation coefficients (Figure 8).
ROUGE and BERTScore are strongly correlated.
NLI and STS are strongly correlated. Delta-EI has
a bimodal distribution. PIO-Overlap is uncorre-
lated with other metrics.
Correlations between automated metrics and the
normalized PIO facet score are shown in Figure 9.
In general, automated metrics are poor predictors
of PIO agreement, except PIO-Overlap, which is
positively correlated with PIO agreement (p < 0.05).
This confirms that model extraction and alignment
of PIO spans is a promising direction for assessing
PIO agreement. ClaimVer also shows a weak but
statistically significant correlation with PIO agree-
ment. The ClaimVer metric is computed based on
embedding similarity between two texts using a
model trained on the SciFact scientific claim veri-
fication dataset (Wadden et al., 2020); the SciFact
task measures whether evidence entails or refutes98839884
a scientific claim, which is somewhat analogous
to our evaluation task for medical multi-document
summarization.
We also assess whether metrics can distinguish
between summaries where the Direction agrees
with the target and summaries where the Direc-
tion disagrees. We present the empirical cumula-
tive distribution functions (ECDF) for each auto-
mated metric, showing the separation of metrics
between when Direction agrees and disagrees (Fig-
ure 10. The Delta-EI metric is somewhat sensitive
to human-assessed directional agreement (a higherproportion of generated summaries where the Di-
rection agrees with the target have lower Delta-
EI scores), though we note that the difference is
small. PIO-Overlap also shows some separation
between the two Direction classes (a higher pro-
portion of disagrees have lower PIO-Overlap score
than agrees), though again the difference is subtle.
F Normalizing human facet scores
Responses to the Fluency question result in a 3-
class ordinal variable that we map to the range [0,
1], where 0.0 is disfluent, 0.5 is somewhat fluent,9885
and 1.0 is fluent. PIO aggregates agreement over
Population, Intervention, and Outcome, where each
of P, I, and O are 3-class ordinal variables that we
map to the range [0, 1] as we do Fluency; we av-
erage the three facets to get PIO agreement. For
evidence direction, though each of the two anno-tated questions has 4 answers (positive, no effect,
negative, or no direction given), we elect to define
Direction as a binary class. We normalize Direction
to 1 if the target direction and generated direction
agree and 0 if they disagree. For Strength, each of
the two annotated questions has 4 answers (strong,9886moderate, weak, and not enough evidence). We
take the difference between the answers for the tar-
get and generated summaries and normalize to the
range [0, 1] to yield our Strength agreement score.9887ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations section, and parts of 8. Discussion
/squareA2. Did you discuss any potential risks of your work?
Ethical considerations section, and parts of 8. Discussion
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and 1. Introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3. Dataset; we create a dataset in this work and describe how we go about collecting data
/squareB1. Did you cite the creators of artifacts you used?
Throughout the paper
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Licensing for our data artifact will be available on Github
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Throughout the paper, also in 3. Dataset and 8. Discussion
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. No names and unique identiﬁers are included in the dataset
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
We discuss the provenance of data in our dataset in 2. Background and 3. Dataset
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
All reported in 3. Dataset
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
No response.9888/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
No response.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
No response.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
No response.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3, 5, and 6 discuss our human annotation protocols
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Full text is in Appendix; a brief description in Section 3
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Annotators are included as authors on the paper
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Annotators are included as authors on the paper
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Section 3 describes annotator demographics and background9889