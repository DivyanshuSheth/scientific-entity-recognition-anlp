
Jingwei Ni
University College London
ucabjni@ucl.ac.ukZhijing Jin
MPI & ETH Zürich
zjin@tue.mpg.de
Markus Freitag
Google Research
freitag@google.comMrinmaya Sachan
ETH Zürich
msachan@ethz.chBernhard Schölkopf
MPI & ETH Zürich
bs@tue.mpg.de
Abstract
Human-translated text displays distinct fea-
tures from naturally written text in the same
language. This phenomena, known as transla-
tionese , has been argued to confound the ma-
chine translation (MT) evaluation. Yet, we
ﬁnd that existing work on translationese ne-
glects some important factors and the conclu-
sions are mostly correlational but not causal.
In this work, we collect C MT, a dataset
where the MT training data are also labeled
with the human translation directions. We in-
spect two additional critical factors, the train-
test direction match (whether the human trans-
lation directions in the training and test sets
are aligned), and data-model direction match
(whether the model learns in the same direc-
tion as the human translation direction in the
dataset). We show that these two factors have a
large causal effect on the MT performance, in
addition to the test-model direction mismatch
highlighted by existing work on translationese.
In light of our ﬁndings, we provide a set of
suggestions for MT training and evaluation.
1 Introduction
MT has long been concerned with the artifacts in-
troduced by translationese , the human-translated
text that is systematically different from naturally
written text in the same language (Toury, 1980;
Gellerstam, 1986; Toury, 1995; Baker, 1993; Ba-
roni and Bernardini, 2006). For a translation sys-
tem translating from language Xto language Y,
there can be two types of test data: sentences that
originated in language Xand are human-translated
into language Y(denoted as X− →Y), and sen-
tences that originated in language Yand human-
translated into language X(denoted as X← −Y).Figure 1: Three different factors illustrate the impact
of translationese on MT performance. Previous work
mainly focuses on how translationese in the test set
(Factor 1) inﬂates BLEU score and makes it favor some
translation systems over others. Our work investigates
the causal effects of the other two key factors, the train-
test direction match (Factor 2; i.e., whether the training
set and the test set share the same human translation
direction), and data-model direction match (Factor 3;
i.e., whether the dataset collection direction and model
translation direction are the same).
The main concern raised by this distinction of the
two sets is whether the reported performance on a
mixed test set truly reﬂects the actual translation
quality. Previous work in MT has shown that trans-
lationese is a confounder in evaluating translation
quality (Lembersky et al., 2012; Toral et al., 2018;
Läubli et al., 2018; Freitag et al., 2020).
Recent studies on causality have also brought
to attention the importance of distinguishing the
data-model direction match , namely whether the
data collection direction is the same as or opposite
to the model direction, also known as causal or
anti-causal learning (Jin et al., 2021b; Veitch et al.,
2021; Schölkopf et al., 2012). If the dataset is
collected by human annotators who see the input
Xand produce an output Y, then learning an X-to-5303Ymodel is causal learning, and learning a Y-to-X
model is anticausal learning.
In this work, we study the artifacts in MT
brought by translationese from the viewpoint of
causality, speciﬁcally, the match between the data
and model directions. We consider two factors
of variation in MT: human translation direction
(in both the training and the test set) and model
translation direction . Thus, we formulate the effect
of translationese in the test set as the test-model
direction match problem, and causal/anticausal
learning as the data-model direction match prob-
lem. Further, we identify the third important fac-
tor, the train-test direction match problem, namely,
whether the training set and the test set are col-
lected with the same human translation direction.
We study the causal inﬂuences of these three fac-
tors on MT performance in Figure 1. Previous
work has mainly studied the artifacts introduced
by the test-model direction match on MT perfor-
mance (Toral et al., 2018; Graham et al., 2020;
Edunov et al., 2020), but little considers the other
two factors, the train-test and data-model direction
match (Kurokawa et al., 2009; Larkin et al., 2021).
Moreover, most analyses are based on correlation
but not causation (Graham et al., 2020).
In this paper, we use causal inference (Pearl,
2009; Peters et al., 2017) to re-investigate previous
conclusions about the most studied relationship
between the test-model direction match and MT
performance. We calibrate the effect of test-model
direction match by analyzing the causal effects of
the two other factors, the train-test and data-model
direction match. In our causal analyses, we use in-
terventions to study the effect of train-test direction
match, and covariate matching to study the effect
of data-model direction match, by controlling for
covariates such as sentence lengths and content.
We build C MT, a new dataset on ﬁve lan-
guage pairs labeled with the human translation di-
rections, and statistically verify that translationese
tends to be simpler and more verbose, corroborat-
ing previous observations on translationese (Toury,
1980; Gellerstam, 1986; Toury, 1995; Baker, 1993).
Then, we rigorously analyze C MT, leading
to the following new insights and contributions:
C1. Previous work claims that translationese in the
test set inﬂates MT model performance and
thus suggests removing the translationese-to-
original half of the test set (Toral et al., 2018;
Zhang and Toral, 2019; Graham et al., 2020;Barrault et al., 2019). Our work shows that the
translationese-to-original half of the test set
does not necessarily inﬂate MT performance
in all cases. In some cases, it can even be more
challenging than the other half, depending on
the human translation direction in the training
corpus. Hence, we suggest still reporting per-
formance on both test sets, and also reporting
the training data direction if available.
C2.Previous work (Burlot and Yvon, 2018)
claims that back-translation (BT) (Sennrich
et al., 2016) is usually more effective than
self-training (ST) (He et al., 2019). Our work
shows that BT is not necessarily better than
ST in all cases. This result also depends on
how the pseudo-parallel corpus aligns with
the human translation direction in the test set.
We suggest choosing BT or ST depending on
this direction match.
C3.Previous work claims that BT’s performance
improvement is largely reﬂected on the
translationese-to-original half of the test set,
but the improvement is very small on the other
half (Toral et al., 2018; Freitag et al., 2019).
Our work shows that the improvement of BT
can be larger on the other half of the test set
as well, as long as the pseudo-parallel corpus
aligns with the human translation direction in
the test set.
C4.Our work shows that data-model direction
match also has a large causal effect on the
MT performance of up to 12.25 BLEU points
after adjusting for other covariates using the
backdoor adjustment (Pearl, 1995).
2 C MT Dataset
To investigate the effect of train-test direction
match and data-model direction match, we need to
collect translation data in different human transla-
tion directions.
2.1 Data Collection
To construct our C MT dataset consisting
of a large number of translation pairs labeled with
the human translation direction,we use the Eu-
roparlExtract toolkit (Ustaszewski, 2019) to ﬁlter5304
translation pairs by meta-information (e.g., the tag
specifying the original language of the speaker).
Speciﬁcally, in the Europarl corpus (Koehn, 2005),
we iterate over each transcript that has an origina-
tion label and mark a sentence as original text if
the original language of the speaker is the same
as the language this sentence is in, or otherwise
mark it as the translated text. After extracting the
direction-labeled language pairs, we remove all du-
plicates in the entire dataset. Since our study needs
to compare training on parallel corpora of the same
language pair but with two different human trans-
lation directions, e.g., De− →En and De← −En, we
control the size of the two corpora to be the same
by downsampling the larger set.
Among all language pairs we can obtain, we
keep ﬁve language pairs with the largest number
of data samples. As in Table 1, the C MT
dataset contains over 200K translation pairs in each
training set of three language pairs and over 90K
translation pairs in each training set of the other two
language pairs. The development set and test set
contain 1K and 2K translation pairs for all language
pairs in each direction, respectively.
2.2 Dataset Characteristics
We analyze the characteristics of the C MT
dataset in light of how translated text differs from
naturally written text in the same language.
Our ﬁndings echo the observations by previ-
ous work on the distinct features of translationese
(Toury, 1980; Gellerstam, 1986; Toury, 1995;
Baker, 1993; Baroni and Bernardini, 2006; V olan-
sky et al., 2015). For example, translationese tends
to be simpler and more standardized (Baker, 1993;
Toury, 1995; Laviosa-Braithwaite, 1998), such ashaving a smaller vocabulary and using certain dis-
course markers more often (Baker, 1993, 1995,
1996). Translationese also tends to be inﬂuenced
by the source language in terms of its lexical and
word order choice (Gellerstam, 1986).
In the C MTdata, we observe three prop-
erties. (1) Within each language pair (e.g., German
and English), the same language’s translationese
always has a smaller vocabulary than its naturally
written text corpus. For example, the translationese
German in De← −En has only 113K vocabulary,
which is 5K smaller than the vocabulary of the
German corpus in De− →En. (2) Translationese
tends to be more verbose . For each language pair,
we calculate the expansion factor from language
Xto languageY(X:Y) as the average word count
per sample in language Xdivided by the average
word count per sample in language Y. For exam-
ple, for each (English, German) translation pair,
the number of English words is 1.13 times that of
German words when English is the translationese
(i.e., en:de expansion factor=1.13). On the other
hand, the en:de expansion factor is only 1.04 when
English is the naturally written text. (3) We use
a syntax-based parser to detect the percentage of
samples with passive voice in English, details of
which are in Appendix B. There is a clear distinc-
tion that translationese English tends to use more
passive voice than original English , e.g., 14.94%
translationese samples in the passive voice in the
En← −Es corpus in contrast with 11.49% original
English samples in the reverse direction.
3 The Overshadowing Effect of
Train-Test Direction Match
The ﬁrst analysis of this paper aims to calibrate the
most studied relationship of the test-model direc-
tion match and MT performance by considering the
additional effect of the train-test direction match .5305Previous work observes that the translationese-
to-original test set inﬂates the score. To eval-
uate a model with the X-to-Ytranslation direc-
tion, traditionally, the test set is a mixture of two
halves, one with the human translation direction
X− →Y(aligned) and the other X← −Y(unaligned,
or translationese-to-original) (Bojar et al., 2018).
Previous studies propose that the unaligned,
translationese-to-original test set is easier to trans-
late than the other aligned test set because transla-
tionese inputs are easy for the MT model to han-
dle (Toral et al., 2018; Zhang and Toral, 2019; Gra-
ham et al., 2020). The inﬂated test performance
caused by translationese has long been speculated
(Lembersky et al., 2012; Toral et al., 2018; Läubli
et al., 2018), and recent work has statistically veri-
ﬁed the correlation (Graham et al., 2020).
With the previous understanding, some works
suggest removing the unaligned half of the test
set (Toral et al., 2018; Zhang and Toral, 2019; Gra-
ham et al., 2020), which was adopted by the 2019
WMT shared task (Barrault et al., 2019), whereas
others suggest keeping both but reporting the per-
formance separately (Freitag et al., 2019; Edunov
et al., 2020). The motivations from the two sides
are that in the unaligned half, although the source
text being translationese is an easy input to the
model, its target text being naturally written text
makes the evaluation more natural.
This “inﬂation” depends on train-test direction
match. We take a step back from the argument on
whether the unaligned test set positively or nega-
tively affects the MT performance evaluation. In-
stead, we call attention to the fact that, beyond the
test-model direction match, there can be other fac-
tors also playing a critical in the MT performance
evaluation, i.e., the train-test direction match.
For a given machine translation task to learn the
X-to-Ytranslation, there can be two questions: the
question from previous work is whether we should
use the test set aligned with the model translation
direction (T1) or the test set unaligned with the
model translation direction (T2) to evaluate the
model fairly, whereas the question answered by
our work is which training data should be used to
achieve the best performance .
Our analysis aims to obtain causal conclusions
on how intervening on the train-test direction match
affects the MT performance. Therefore, we con-
trol all other possible confounders. For each lan-
guage pair, we control the total training data size
to be the samewhen varying the portion of data
in two directions. We also enumerate all other pos-
sible interventions, such as varying the two model
translation directions and reporting performance
on two different halves of the test set with two hu-
man translation directions. We also control that all
translation models use the same Transformer archi-
tecture (Vaswani et al., 2017) by fairseq (Ott et al.,
2019), with experimental details in Appendix C.5306We report the experiment results of how inter-
vening the train-test direction match affects the MT
performance in BLEU score (Papineni et al., 2002)
in Table 2. The main takeaways are as follows:
(1) It is not always the case that, for the same
model, the unaligned test set T2 yields higher/more
inﬂated results than the aligned test set T1. When
the training data has 75–100% aligned training sam-
ples, performance reported on T2 is, in most cases,
no longer larger than that on the other half. With
such training data, usually, T1 inﬂates the BLEU
score more.
(2) The train-test direction match can have an
overshadowing effect over the artifacts introduced
by the translationese-to-original test set, since no
matter which test set we use, the more matched
the train and test directions are, the higher the per-
formance reported on T1 than T2 is. Speciﬁcally,
as we vary the portion of the aligned training data
from 0 to 100%, the performance on T1 keeps in-
creasing, and the performance on T2 keeps decreas-
ing. Additionally, if the training data has about
0–50% samples aligned with the model translation
direction, then, in many cases, T2 is higher than
T1, which might explain the previous observations
that T2 inﬂates the BLEU score (Toral et al., 2018;
Graham et al., 2020). To account for another possi-
ble interpretation, such as the domain shift between
the training and test sets, we also conduct an addi-
tional evaluation using the newstest2014 test sets,
which do not share any domain similarity with our
training sets, but still support our observation (in
Appendix Table 6).
Hence, the two constructive suggestions for fu-
ture work are that (1) it is important to still report
on both test sets, and also the training data direc-
tion if available, and (2) parallel training data in
the same direction with the test set is more helpful.
Monolingual data in the original language of
the test set is more helpful. With the intuition
that the train-test direction match is a crucial factor
for MT performance, we also look into its implica-
tions on semi-supervised learning.
Given additional monolingual data, a common
question in MT is what type of monolingual data
to use, and the accompanying question, whether
to use self-training (ST) for the source language
monolingual corpus (He et al., 2019; Yarowsky,
1995) or back-translation (BT) for the target lan-
guage monolingual corpus (Bojar and Tamchyna,
2011; Sennrich et al., 2016; Poncelas et al., 2018).
We reframe the question as “ with abundant mono-
lingual data from both languages, but limited com-
putation resources, which data (together with the
corresponding semi-supervised learning method)
should we choose? ”
In previous work, BT is the most widely used
technique (Bojar et al., 2018; Edunov et al., 2018;
Ng et al., 2019; Barrault et al., 2019, p. 15), and is
reported to outperform ST (Burlot and Yvon, 2018).
Another line of previous work inspects the perfor-
mance gain by BT. Some argue that BT is helpful
mostly on the test set aligned with the model (Toral
et al., 2018; Freitag et al., 2019; Edunov et al., 2020,
Appendix A Table 7) but not the unaligned test set,
while others show that BT improves performance
on both test sets (Edunov et al., 2020).
We re-inspect the two previous lines of work,
and ﬁnd (1) BT does not always outperform ST,
especially when ST can make use of the monolin-
gual data in the original language of the test set (to
produce pseudo-aligned training data), and (2) the
performance gain by BT is not always larger on the
unaligned test set, but depends on the model direc-
tion, especially when BT generates pseudo-aligned
training data with the test set.
We implement BT by Edunov et al. (2020), and
ST by He et al. (2019). To fairly compare the per-5307formance of ST vs. BT, for each language pair
XandY, we split half both training corpora into
X− →Y-Half1,X− →Y-Half2,Y− →X-Half1, and
Y− →X-Half2. We construct the supervised train-
ing data as an equal mix (i.e., α=50) combining
X− →Y-Half1 andY− →X-Half1. The development
data is the combination of both development sets,
which is also an equal mix.
To train ST or BT, we use the second halves of
the training data only as the monolingual corpora.
For example, if the translation task is English-to-
German translation, ST generates a pseudo-parallel
corpus with original English paired with machine-
translated pseudo-German, which we denote as
(en, de). For readability, we mark the machine-
translation direction with ST and BT byand the
human translation direction by.
Our hypothesis is that the machine-translated
text pairs (en, de) will also show similar proper-
ties as the human-translated training data (en, de).
Speciﬁcally, the more the pseudo-training data is
aligned with the test set, the higher performance
the semi-supervised learning method will achieve.
This is conﬁrmed by the experiment results in Ta-
ble 3, where, across all settings, no matter which
semi-supervised learning method is used, when the
pseudo-training data has the same translation di-
rection as the test set, the resulting performance is
generally higher. The experiments conducted on
C MTtest sets also generally show the same
trend, and, due to the space limit, we include the
results in the Appendix Table 7.
4 Causal Effect of Data-Model Direction
Match
The second contribution of this work is to inspect
how much another factor, the data-model direction
match, causally affects the MT performance. For-
mally, our research question is that, for a given
translation task X-to-Y, considering an equal mix
of the test set, does the human translation direction
of the training data still matter? If so, how large is
the effect, and is it language-/task-dependent?
This section will use causal inference to isolate
the effect of data-model direction match from other
possible confounders and discuss its effect in dif-
ferent languages and translation tasks.
4.1 Correlation in Previous Experiments
Our previous experiments show that data-model
direction match correlates with MT performance.Speciﬁcally, for each translation task in Table 2,
there is a clear difference between the causal learn-
ing and anticausal learning model. We copy this
naïve difference to the “Diff” column of Table 5.
This naïve difference represents
E [S]−E [S], which
compares the performance score Son the aligned
corpus and Son the unaligned corpus, without
controlling for potential confounders. The famous
“correlation does not imply causation” implies that
this formulation cannot answer the causal question,
as the two expectation terms are taken over two
different distributions that are not necessarily
comparable. However, by Reichenbach’s Common
Cause Principle (Reichenbach, 1956), correlation
implies the existence of some common cause
behind the two correlated variables, which
motivates us to investigate the causal relationship
in the next section.
4.2 Setup of the Causal Effect Estimation
Formulating the causal effect. Instead of just
correlational analyses, we aim to estimate the av-
erage causal effect (ACE) (Holland, 1988; Pearl,
2009) of the data-model direction match (i.e.,
causal vs. anticausal learning) Mon the translation
performance S:
ACE =P(S=s|do(M= 1))
−P(S=s|do(M= 0)),(1)
where, according to do-calculus (Pearl, 1995) in
causal inference, the operator do(M= 0 or1)
means to intervene on the data-model direction
match to be 0 (i.e., anticausal learning) or 1 (i.e.,
causal learning). The ACE formulation is about
how much the model performance Swill differ if
intervening the data-model direction match Mto
be 0 or 1.
To estimate the ACE, we ﬁrst draw the causal
graph considering all variables that can interfere
with the relationship between data-model direction
match and MT performance. The main additional
factors we need to control for are in the causal
graph in Figure 2. We make the assumption that
it is very likely that the two corpora of different
human translation directions also vary by sentence5308
lengths and the distribution of content (Bogoychev
and Sennrich, 2019) due to a hidden confounder
(i.e., a common cause) such as the nature of Eu-
roparl. Note that since our research question is
about which training data matters for given a trans-
lation task, the data-model direction match is equiv-
alent to the human translation direction of the train-
ing data, as the model translation direction is ﬁxed.
Given the causal graph in Figure 2, the ACE
in Eq. (1)can be calculated by conditioning on
the set of variables Zwhich blocks the backdoor
paths (Pearl, 1995) between MandS. (Zﬁts the
backdoor criterion (Pearl, 1993) in that the sen-
tence lengths and content block all non-directed
paths fromMtoS, and neither is a descendant of
any node on the directed path from MtoS.) An
intuitive interpretation can be that when we directly
look at the correlation between the data-model di-
rection match and MT performance, it might also
be due to that different corpora have different distri-
butions of sentence lengths and content. Therefore,
we need to control the sentence lengths and content
so that the performance difference will be solely
due to the data-model direction match.
Formally, the ACE using the do-notation can be
calculated by conditioning on Z. Speciﬁcally, we
integrate over the distribution of P(Z), and cal-
culate the difference in the conditional probability
distribution P(S=s|M= 1,Z=z)−P(S=
s|M= 0,Z=z)ofSgiven the data-model di-
rection match value Mconditioned on the other
key variables Zfor each of its possible value z, as
shown in Eq. (2):
ACE =/integraldisplay[(P(S=s|M= 1,Z=z)
−P(S=s|M= 0,Z=z))P(z)](2)
=E[S|M= 1,Z=z]
−E[S|M= 0,Z=z].(3)
Finally, we estimate it by comparing the expected
values of the model performance score Sgiven
M= 0or1over all possible values of the covari-
ateZas in Eq. (3).
Causal effect estimation by matching. To esti-
mate the ACE in Eq. (3), the intuition is that we
need to take care of the covariates in Zso that
the aligned setting and the unaligned setting are
comparable. We follow the covariate matching
method in causal inference (Rosenbaum and Ru-
bin, 1983; Iacus et al., 2012) and adjustment in
the high-dimensional setting of text (Roberts et al.,
2020; Veitch et al., 2020). Speciﬁcally, matching
is a method in causal inference to subsample the
treated (i.e., the aligned corpus with the model di-
rection) and control samples (i.e., the unaligned
corpus with the model direction) so that the covari-
ates of interest are matched.
We aim to match pairs of samples, one from the
causal corpus and the other from the anticausal
corpus, where we constrain them to share simi-
lar contents and similar sentence lengths. In our5309
implementation, for each sentence in the causal
corpus, we select its most similar match in the an-
ticausal corpus using Dinic’s maximal matching
algorithm (Dinic, 1970).
Empirically, we limit the sentence length ratio
of each matched pair to be no larger than 1.1 and
the content to have a cosine similarity larger than
0.7, following the threshold to match a content-
similar pseudo-parallel corpus in Jin et al. (2019).
To calculate the content-wise similarity of a pair
of samples, we represent each sentence by the sen-
tence BERT embedding (Reimers and Gurevych,
2019). In cases of multiple languages as candi-
dates to match the sentence embeddings in, we
set a prioritization order of “En >De>Fr>Es” for
sentence embedding matching. The prioritization
order roughly follows the training data sizes of the
multilingual version (Reimers and Gurevych, 2020)
of the sentence transformer in the four languages.
Note that since the set of factors to control is in
a high-dimensional vector space, it is less realis-
tic to use other common matching methods such
as propensity score stratiﬁcation and matching, as
pointed out by Roberts et al. (2020).
We check the quality of our matching heuristics.
Taking the German-English language pair as an ex-
ample, we plot the distributions of topics by Latent
Dirichlet Allocation (LDA) topic modeling (Blei
et al., 2001) and distributions of sentence lengthsacross the De− →En and De← −En corpora in Fig-
ure 3. We also list some example matched pairs in
English in Table 4. Further statistics of the matched
corpora are in Appendix E.1.
Finally, based on the matched datasets that con-
trol for the sentence lengths and contents, ACE can
be calculated by the average difference in MT per-
formance of models trained on the two directions
of the new datasets.
4.3 Causal Effect Results
We have three observations from the results in Ta-
ble 5: (1) The data-model direction match is a
clear cause for MT performance. The ACE of data-
model direction match on MT performance can
be up to 12.25 BLEU points, for example, in the
Spanish-to-English translation task. (2) The ACE
varies by language and translation tasks. For the
English-Spanish language pair, both translation di-
rections get higher BLEU points if the models are
trained in the causal learning direction. For other
language pairs, the causal effects of data-model
direction match are clear, although varying from
positive to negative values, the reasons for which
are worth future studies. (3) The results of naïve
differences (Diff) are, in most cases, smaller than
that of the causal analysis by ACE. This indicates
that the correlational analysis neglects other impor-
tant factors such as the sentence length and content,5310which might also be reﬂected in the overall differ-
ence. The causal analysis is a more appropriate
method to isolate the inﬂuence of the data-model
direction match.
5 Related Work
Linguistic studies have long observed the distinct
properties of translationese from text originally au-
thored in the same language (Toury, 1980; Geller-
stam, 1986; Baker, 1993; Toury, 1995). Recent
work in MT identiﬁes that the source side of the
translationese-to-original portion of the test sets
(i.e., test sets unaligned with the model direction)
is easier (Graham et al., 2020), echoing with many
previous observations (Toral et al., 2018; Lember-
sky et al., 2012; Läubli et al., 2018) and thus some
suggest to exclude this portion from future test
sets, in particular human evaluations (Toral et al.,
2018; Zhang and Toral, 2019; Graham et al., 2020;
Barrault et al., 2019). Nevertheless, Freitag et al.
(2019) demonstrate that it is worth reporting auto-
matic metric scores on both directions separately as
both types of test sets evaluate different properties
of translation quality. In follow-up work, Freitag
et al. (2020) introduce paraphrased test sets that
overcome the problems of translationese for test
sets, not solving the problem for the training data
though.
Based on this speculated inﬂation of MT perfor-
mance due to translationese in the test set, further
work inspects what previous conclusions about the
effectiveness of MT models should be recalibrated.
Some discover that models with BT mostly im-
prove on the inﬂated test set but not the other more
challenging portion (Toral et al., 2018; Freitag et al.,
2019; Edunov et al., 2020, Appendix A Table 7)
and raise concerns that BT is not as effective as
expected. Others argue that BT can still improve
on both test sets (Edunov et al., 2020).
While for almost all test sets the original lan-
guage of each example is known, the majority of
training data does not contain this meta information.
Studies of the impact of translationese on training
examples are thus mostly based on Europarl where
meta information is given. For instance, Ozdowska
and Way (2009) argue that the original side of each
training sample is important when building a statis-
tical machine translation (SMT) system. Kurokawa
et al. (2009); Koppel and Ordan (2011); Sominsky
and Wintner (2019) build classiﬁers to automati-
cally detect the direction of each training sample.Riley et al. (2020) use a CNN classiﬁer to sepa-
rate the training data at scale and bias the NMT
model via tagging to generate translations that look
like the original text. Human evaluation demon-
strates that this produces more accurate and natural
translations. Further, Amponsah-Kaakyire et al.
(2021) investigate the impact of training samples
generated with pivot (“relay”) languages.
Our work differs from all previous work in that
we conduct causal inference (Pearl, 2009; Peters
et al., 2017) to contribute causal insights on how
translationese affects MT.
6 Future Work
We list several directions for potential future work:
(1) It will be meaningful to explore whether the
conclusions of this paper can generalize to higher-
resource data and a wider variety of languages.
(2) In real-world MT systems, it is important not
only to answer whether aligned training data or un-
aligned training data is better (when deciding how
to distribute the budget to collect data for a usage
scenario with a ﬁxed direction), but also to investi-
gate how to utilize the mixed training data (when
trying to make the best use of the existing data) and
contribute to the best possible translation systems.
Riley et al. (2020); Larkin et al. (2021); Freitag
et al. (2022) suggest adding a tag per sample spec-
ifying the direction to make use of the unaligned
data. Future work can also explore if there can be
an end-to-end model jointly inferring the human
translation direction and signaling the model to
deal with the aligned and unaligned directions dif-
ferently. (3) In our study, Table 5 mainly conﬁrms
the causal effects of the data-model direction match
on the MT performance, but the actual reasons for
positive and negative ACEs are still worth further
investigation.
7 Conclusion
In conclusion, this work study the causal effects
of three important factors on MT performance:
the test-model, train-test, and data-model direction
match. We provide suggestions for future study in
MT, such as using more training data in the aligned
direction and paying attention to whether the nature
of the translation task is causal or anticausal.
Ethical Considerations
Data Privacy and Bias: This research mainly fo-
cuses on translation using the Europarl (Koehn,53112005) corpus, which is widely adopted in the com-
munity. There are no data privacy issues or bias
against certain demographics with regard to this
dataset.
Potential Use: The potential use of this study is
to improve future MT practice in terms of both
evaluation and training.
Generalizability: Most conclusions in this study
are language-agnostic and potentially help MT in
all language pairs, although due to the limitations
of available data, the study mainly uses the com-
mon languages, English, German, French, and
Spanish, in a relatively low-resource setting of
around 100K to 200K data. It will be meaning-
ful to explore whether the conclusions of this paper
can generalize to higher-resource data and a wider
variety of languages. There is a possibility that
the ﬁndings of the study will need to be further
adjusted in different settings, which we strongly
encourage future work to explore.
Limitations: First, the current study mainly looks
into clear cases of causal or anticausal learning, but
there can potentially be a third case where both
languages are translated from a third language, as
pointed out in Riley et al. (2020, Figure 1), which
is worth exploring for future work.
Second, this work extracts human translation di-
rections from the Europarl corpus, with the assump-
tion that the speakers at the European Parliament
tend to be native speakers. It might also be possible
that the Europarl corpus contains text annotated as
originals but from non-native speakers, but since
the European Parliament is a highly formal and
important venue, the speakers tend to be at least
proﬁcient users of that language, if not native. In
addition, for non-English languages in our corpus,
it is highly likely that the speech comes from a
native speaker.
Third, in addition to the length and content fac-
tors considered in this work, it could be interesting
to look at other factors that can constitute the Z
variable in Eq. (3). Some motivations include that
the data-model direction match seems to be a clear
cause for the MT performance, and the fact that
it does not always show a very large causal effect
might mean that there are additional hidden vari-
ables to take into consideration.
Lastly, due to ﬁnancial budgets, we did not use
human evaluation in addition to the BLEU score re-
ported in this work. We released all outputs of our
model so future work can feel free to evaluate ourresults by human evaluation or various other auto-
matic evaluation metrics. See more discussions on
recommended evaluation metrics in Appendix D.3.
Acknowledgments
We sincerely thank Prof Rico Sennrich for his
constructive suggestions on improving the mes-
sage we present from our experiments to the MT
community, and Isaac Caswell at Google Trans-
late for proofreading our paper. We thank Claudia
Shi for the insightful discussions on the matching
method and suggestions on papers to cite. We thank
Shaoshu Yang and Di Jin for supporting us with
computational resources. We thank Zhiheng Lyu
for helping to code a fast version of the sentence
similarity matching.
This material is based in part upon works sup-
ported by the German Federal Ministry of Educa-
tion and Research (BMBF): Tübingen AI Center,
FKZ: 01IS18039B; the Machine Learning Clus-
ter of Excellence, EXC number 2064/1 – Project
number 390727645; the John Templeton Founda-
tion (grant #61156); a Responsible AI grant by the
Haslerstiftung; and an ETH Grant (ETH-19 21-1).
Zhijing Jin is supported by the Open Phil AI
Fellowship and the Vitalik Buterin PhD Fellowship
from the Future of Life Institute.
Author Contributions
Jingwei Ni conducted most of the MT experiments,
collected the most updated version of C MT
dataset, and performed various analyses.
Zhijing Jin designed the project, conducted the
causal inference experiments, structured the ﬁrst
version of the codes to run the MT experiments, and
collected the ﬁrst version of C MTdataset
which is also used in our previous study (Jin et al.,
2021a).
Markus Freitag helped design the storyline of the
translationese part, gave insights on what analyses
are important, and provided suggestions on the
evaluation.
Mrinmaya Sachan and Bernhard Schölkopf
guided the project and provided substantial con-
tributions to the storyline of the paper.
Everyone contributed to writing the paper.5312References5313531453155316A Reproducibility, License, and
Copyright
We open-source our codes and datasets, which
are both uploaded to the submission system. In
our data, we include all three variations: the full
C MT dataset, the split used for the semi-
supervised learning experiments, and the subset
after matching the contents and sentence lengths.
In our codes, we include all commands with hyper-
parameters to help future work to reproduce our
results.
The codes and data are under MIT license. Note
that the Europarl dataset has no copyright restric-
tion, according to its ofﬁcial website.
B Linguistic Property Analysis
We also open-source the codes to calculate the lin-
guistic properties of our dataset in Table 1. We use
the Python library Stanza(Qi et al., 2020) to tok-
enize the sentences when calculating the number of
sentences per sample. For computational efﬁciency,
we use NLTK(Bird et al., 2009) to tokenize the
words and count the vocabulary.
We use the Python library spaCy(Honnibal and
Montani, 2017) to calculate the punctuation per
sample. We use a passive voice checker(only
available in English). For the expansion factor,
we formatted Table 1 using the ratio of the two
languages in a descending alphabetical order of
each language pair. In our table, it happens to be
the ratio of the more verbose language to the less
verbose language in each language pair.
C Implementation Details
C.1 Preprocessing
To prepare the text for the models, we follow the
preprocessing scripts of fairseq (Ott et al., 2019).
Speciﬁcally, we use the Moses tokenizer (Koehn
et al., 2007),the default byte pair encoding (BPE)
size of 40K subwords, and remove sentence pairs
that of larger than 1.5 length ratio from the training
set.C.2 Evaluation Script
We use the fairseq scriptto calculate the BLEU
score (Papineni et al., 2002) of each translation
model, with a beam width of 5, BPE removed, and
detokenized by moses.
C.3 Model Details
We use the sequence-to-sequence Transformer
model (Vaswani et al., 2017) implemented by the
fairseq library (Ott et al., 2019). Speciﬁcally, we
use a six-layer Transformer, a label smoothing
of 0.1, a weight decay of 0.0001, a dropout of
0.3, 4000 warming updates, and a learning rate of
0.0005. All results are reported by a single run but
a ﬁxed random seed.
For the semi-supervised learning, we implement
the BT model following Edunov et al. (2020) to
use the Facebook-FAIR system of the WMT’19
news shared translation task.All the hyperparam-
eters are the same as the supervised system, with a
learning rate of 0.0007 on both the supervised train-
ing data and the generated pseudo-parallel corpus.
We implement the ST model by He et al. (2019)
following their script,and also keep the hyperpa-
rameters the same as the supervised model.
C.4 Training Details
We train the supervised learning model and each
step in the semi-supervised learning scripts for
1000 epochs. We select the model with the best
performance on the development set and report the
ﬁnal evaluation results on the test set.
All experiments are run on NVIDIA RTX2080
GPUs. Each supervised learning experiment takes
around 32 GPU hours, and each semi-supervised
learning experiment takes about 128 GPU hours.
D Additional Experimental Results
D.1 Effect of Train-Test Direction Match on
Supervised Learning
To inspect the inﬂuence of train-test direction
match on the MT performance, we conduct all ex-
periments on our C MTtest sets and also the
standard newstest2014 test sets. For the supervised
learning performance, we list the performance on5317
theC MTtest sets in the main paper in Ta-
ble 2, and list the additional performance on the
newstest2014 test sets in Table 6.
For better visualization of the trends, we also pro-
vide line plots of the same experimental results in
Table 2. Speciﬁcally, we plot the results of German-
English translation in Figure 4a using our previous
experiment results in Table 2. We also include the
diagram of all ﬁve language pairs in Figure 4b.
In Figure 4a, we use lines with the same darkness
of color for the same model trained on different
data directions. Results show that the data-model
direction match matters signiﬁcantly. Taking the
German-to-English translation models (- - - and
—), the two data directions can cause up to 4.53
difference in BLEU points. In the current ﬁgures,
we also see that the data direction with a smaller
expansion factor is a better training corpus than the
other one.
We use the same line type (dashed or solid) for
models trained on the same data. Using the same
data, the performance of the two different direc-
tions of models cannot be compared directly be-
cause the target language is different, causing the
BLEU calculation to be different.
D.2 Effect of Train-Test Direction Match on
Semi-Supervised Learning
For the semi-supervised learning performance, we
show the performance on the newstest2014 test sets
in Table 3 in the main paper, and performance on
the test sets of C MTin Table 7. Note that
the decrease of ST performance on En-Es and Es-Fr
pairs is possible because ST is more sensitive to the
quality of the model learned on the supervised data,
and these language pairs have a smaller training
data size of 90K compared with 200K+ data for all
the other language pairs.
D.3 Non-BLEU Evaluation Metrics
Due to ﬁnancial constraints, we did not use human
evaluation in addition to the BLEU score in our
study. However, we encourage future studies in this
line of work to include human evaluation results,
as evaluation is important given the nature of such
work, and human evaluation is reported to be more
reﬂective of the real translation quality (Edunov
et al., 2020).
To make it convenient for follow-up work, we
open-source all the outputs generated by our model5318
to our GitHub. We also encourage future work to
adopt more evaluation metrics such as COMET
and BLUERT, which are among the metrics that
correlates the best with human judgements accord-
ing to the metric task at WMT Freitag et al. (2021).
COMET, for instance, correlates better with human
judgements than BLEU (Kocmi et al., 2021) and
goes further string matching.
E Implementation Details for Causal
Inference
E.1 Statistics of the Matched Corpora
We list the statistics of the matched corpora in Ta-
ble 8, and analyze its linguistic properties in Ta-
ble 9.
E.2 Conﬁrming the Causal Graph by Causal
Discovery
To check our causal graph assumption, we ﬁrst ver-
ify whether data-model direction match is a cause
for MT performance using causal discovery.
We use the causal discovery algorithm, fast
causal inference (FCI) (Spirtes et al., 2000a), to
verify that the data-model direction match causally
affects the translation performance, conditioned on
other factors such as the sentence length and topics.
FCI is the most appropriate causal inference
method for this analysis since there might exist hid-
den confounders that affect the MT performance,
which normal causal discovery methods such as
score-based methods (Heckerman et al., 1999;
Huang et al., 2018) and other constraint-based al-
gorithms like Peter-Clark (PC) algorithm (Spirtes
et al., 2000b, §5.4.2, pp. 84–88) cannot han-
dle (Glymour et al., 2019). FCI gives asymptoti-
cally correct results in the presence of confounders,
and outputs Markov equivalence classes, i.e., a set
of causal structures satisfying the same conditional
independences.
Given a language pair XandY, we generate
eight sets of experiment results, by varying the two
training directions, two test directions, and two
model directions. We extract the test samples of
all eight experiments, and since each test set is
2K, there are 16K samples in total. On the 16K
samples, besides keeping the label of their data-
model direction match, translation performance in
BLEU, we also calculate the other factors such as
the test-model direction match, train-test direction5319
match, source sentence length, and the topic vec-
tor by topic modeling on all the training data of
the language pair XandY. We run the FCI algo-
rithm using the causal-learn Python packageover
all the variables of interest. The implementation
details are in the Appendix.
The resulting causal graph on the German-
English language pair is in Figure 2. The results
conﬁrm our hypothesis that the data-model direc-
tion match (causal vs. anticausal direction) does
have a causal effect on the BLEU score, together
with other factors such as the sentence length and
topics.5320