
Yiren Jian
Dartmouth College
yiren.jian.gr@dartmouth.edu
Chongyang Gao
Northwestern University
cygao@u.northwestern.eduSoroush Vosoughi
Dartmouth College
soroush@dartmouth.edu
Abstract
The impressive performance of GPT-3 using
natural language prompts and in-context learn-
ing has inspired work on better ﬁne-tuning of
moderately-sized models under this paradigm.
Following this line of work, we present a con-
trastive learning framework that clusters in-
puts from the same class for better general-
ity of models trained with only limited ex-
amples. Speciﬁcally, we propose a super-
vised contrastive framework that clusters in-
puts from the same class under different aug-
mented "views" and repel the ones from dif-
ferent classes. We create different "views"
of an example by appending it with different
language prompts and contextual demonstra-
tions. Combining a contrastive loss with the
standard masked language modeling (MLM)
loss in prompt-based few-shot learners, the
experimental results show that our method
can improve over the state-of-the-art methods
in a diverse set of 15 language tasks. Our
framework makes minimal assumptions on the
task or the base model, and can be applied
to many recent methods with little modiﬁca-
tion. The code will be made available at:
https://github.com/yiren-jian/LM-SupCon.
1 Introduction
The prompt-based ﬁne-tuning method reduces the
gap between pre-training and ﬁne-tuning by form-
ing the ﬁne-tuning task into a masking language
problem. A language prompt is a piece of text
appended to the query input enabling the model
to come up with better predictions (Schick and
Schütze, 2021; Tam et al., 2021). For instance, by
feeding a language model with "The story is not
worth reading, a truly one." , the model assigns
a higher probability for the blank to be ﬁlled with
"terrible" than "great" . Here, "a truly one." is
called the template of the prompt and "terrible" or
"great" is the label word. Recently, LM-BFF (Gao
et al., 2021) shows that appending demonstrations(e.g."This is an amazing movie, a truly great one" )
to inputs can help the model to better understand
the label word, leading to further improved results.
In this work, we show that Supervised Con-
trastive Learning (SupCon) (Khosla et al., 2020) at
the feature space can be beneﬁcial during the ﬁne-
tuning of prompt-based few-shot language learners,
with proper data augmentation.
Data augmentation is the key component of
SupCon. While there exists many augmentation
techniques like Cutmix (Yun et al., 2019), Mixup
(Zhang et al., 2018) in computer vision and EDA
(Wei and Zou, 2019), AEDA (Karimi et al., 2021)
for text, data augmentation remains challenging.
However, prompt-based few-shot learners with
demonstrations actually provide us with a natural
way to create multiple "views" (augmentations) of
a single example, i.e., for a ﬁxed set of label words,
we can sample different templates and different
demonstrations to append to the input text (shown
in Figure 1). This allows us to construct diverse
input texts that are consistent and complete. By
applying SupCon to cluster the above two example
inputs with very different contents but the same
label, our method is able to obtain an additional
supervision at the feature space which is crucial if
we are only given a few labeled examples.
The main contributions of our paper are:
•A Supervised Contrastive Learning frame-
work for prompt-based few-shot learners.
•An effective data augmentation method using
prompts for contrastive learning with prompt-
based learners.
2 Related Work & Background
Few-shot Learning is often tackled by meta learn-
ing (Li and Zhang, 2021; Bansal et al., 2020; Sharaf
et al., 2020; Jian et al., 2020; Jian and Gao, 2021),
data augmentation (Jian et al., 2022; Jian and Tor-
resani, 2022; Arthaud et al., 2021; Wei et al., 2021;5577
Kumar et al., 2019). Inspired by the in-context
learning of GPT-3, prompt-based ﬁne-tuning (Gao
et al., 2021; Tam et al., 2021; Schick and Schütze,
2021) recently becomes dominant in NLP. Basu
et al. (2021) applies contrastive learning in their
few-shot semi-supervised intent classiﬁcation, by
using EDA (Wei and Zou, 2019) as augmentation
method. Different from Basu et al. (2021), our
method applies to prompt-based ﬁne-tuning, and
we show in experiments that our proposed augmen-
tation outperforms EDA.
Supervised Contrastive Loss . SupCon is a spe-
cial form of contrastive learning (Chen et al.,
2020a,b; Tian et al., 2020a,b,c; Liu et al., 2021;
Xiong et al., 2020) that clusters two augmented
batches at the class level in the feature space. Let
˜x,˜xbe two augmented views of an input
batchx; andz,zto be the features of
˜x,˜x. Then SupCon loss can be computed as
L =SupCon (z,z,y) (1)
whereyis the label for batch x. The details of
SupCon can be found in Khosla et al. (2020).
3 Method
Problem formulation . Following the few-shot set-
ting in LM-BFF, we assume to have access to a
pre-trained language model M, datasetsDand
Dwith label spaceY. There are only K= 16
examples per class in D.
Fine-tuning with prompts and demonstra-
tions . Prompt-based methods treat a classiﬁcation
problem as a masked language modeling (MLM)
problem. They take as input a sentence ( sent)
and a masked template ( temp ) (i.e.,x =sent,temp([mask] )), and ﬁnd the best token to
ﬁll in the [mask]. This leads to a MLM loss
L =MLM (x,y), whereyis the label
word corresponding to x . LM-BFF (Gao
et al., 2021) further appends demonstrations of la-
bel words to improve the results: x =
sent,temp([mask] ),sent,temp(word), where
wordis the label word for sent, and sentis sam-
pled from the training set. Then the classiﬁcation
loss becomes:
L =MLM (x,y) (2)
More mathematical formulation can be found in
LM-BFF or our Appendix B.
Language-based Supervised Contrastive
Loss . For applying SupCon on multi-views of an
input text, we need to ﬁrst obtain two views of a
text:
x=sent,temp([mask] ),sent,temp(word)
x=sent,temp([mask] ),sent,temp(word)
wherexis identical to x in LM-BFF.
We sample a new template ( temp), demonstration
(sent) and the corresponding label word ( word)
to replace those in x, to create a second view of
inputx. Withxandx, we can compute SupCon
loss by Equation 1. The total loss is then
L=L+L (3)
See our Appendix C for more mathematical details.
Computational overhead . We show the algo-
rithm of our method in Algorithm 1. In general,
our method learns from L=L+L ,
whereas baseline LM-BFF learns with L only.5578Learning fromL requires one additional for-
ward and backward pass (highlighted in blue in
Algorithm 1), leading to an increase of computa-
tional cost by×1.5.
Algorithm 1 Our methodMax _Step = 1000 ,LM: Language model,Train _Set: Training set,Sample : Randomly sampling function,Concatenate : The function to concatenate
two strings,CE: Cross Entropy loss,SupCon : Supervised Contrastive loss.fori inMax _Step dosent,y =Sample (Train _Set)demo=Sample (Train _Set)demo=Sample (Train _Set)input=concatenate (sent,demo)input=concatenate (sent,demo)
⊿Learning from MLM Lossoutput=LM(input)L =CE(output,y)L.backward ()optimizer.step ()
⊿Learning from SupCon Lossoutput=LM(input)L =SupCon (output,output)L.backward ()optimizer.step ()end for
4 Experiments
Evaluation datasets and protocol . We evaluate
our method on 15 classiﬁcation tasks studied in
LM-BFF and follow the same setup as them to
allow fair comparisons (see Appendix A for more
training details). Contrastive learning algorithms
beneﬁt from large batch training. Thus, we report
baselines with the same large batch size as ours.
Our method uses a single prompt/template (pri-
mary prompt) for the prediction of each task, and
a set of prompts (auxiliary prompts) for generat-
ing multi-views of inputs for contrastive learning.
The primary prompts we used are shown in Ap-
pendix D. The auxiliary prompts can be either man-
ually designed or generated by a searching algo-
rithm. In this work, we use the top-20 generated
prompts from LM-BFF’s project page and we ran-
domly sample templates in these 20 prompts to
produce second views of our inputs. Unless other-
wise noted, we apply both random templates and
random demonstrations to create second views of
inputs for the contrastive learning.
4.1 Main results on 15 tasks
We use RoBERTa-base (see Appendix E for
RoBERTa-large). We compare ours with LM-BFF
(a method w/ demonstrations) and PET (Schick and
Schütze, 2021) (a method w/o demonstration).
Table 1 shows that our SupCon loss can consis-
tently boost the performance of baseline prompt-
based ﬁne-tuning method LM-BFF. The introduc-
tion of SupCon loss has a maximum improvement
of6.3%in QQP and an average improvement of
2.5%across 15 tasks, likely due to the more gen-
eralized representations learned by SupCon. On
average, the greater improvements by our model
can be seen on the more difﬁcult tasks (see Ap-
pendix 5 for more detail).
We want to emphasize that the input for base-
line LM-BFF already appends different randomly
sampled demonstrations at each tuning iteration.
Thus, the improvement of our method can not be
attributed to the diversity of inputs when learning
fromL of Equation 3, but to the L .
Table 1 also shows that our method works well
even for prompt-based methods without demon-
strations. PET, which is a method without demon-
strations, works consistently worse than LM-BFF.
However, with the additional SupCon loss, the few-
shot performances of PET can be increased by an
average of 2.3%. And the gap between having and
not having demonstrations can be largely closed5579(see LM-BFF vs. PET+ours in Table 1). In some
tasks, e.g., SST-2, SST-5, QNLI, QQP, RTE MRPC,
MR, and CR, the contribution of our SupCon loss
can be even larger than the sole use of the demon-
strations for label words.
4.2 SupCon vs. other losses
We further show that our method outperforms
two latest methods that are designed to improve
prompt-based language models. In ADAPET (Tam
et al., 2021), the authors replace the traditional
CrossEntropy loss with Decoupling Label Loss
and Label Condition Loss in the prompt-based ﬁne-
tuning method PET, without demonstrations. Con-
textual Calibration (Zhao et al., 2021) calibrates
the output probabilities by considering context-free
inputs, i.e., " " or "N/A". (Further see Appendix I)
From Table 2 we observe that on 12 tasks our
L outperforms the other losses, while per-
forms on-par in other tasks. Contextual Calibration
does not achieve good results overall. We speculate
two reasons for this. First, Contextual Calibration
is designed for large models without ﬁne-tuning
like GPT (zero-shot setting). Second, the form
of in-context learning in Contextual Calibration is
different from the demonstrations we study here.
4.3 Ensemble vs. our single model
Our method uses 20generated templates (auxil-
iary prompts) to construct multi-views of input sen-
tences. But only a single prompt (primary prompt)
and one set of label words are used for main pre-
dictions. Thus, there is only a single model from
our method. Here, we compare our model to an en-
semble comprised of 20models trained separatelyTask LM-BFF LM-BFF
+ours ensemble
SST-5 (acc) 49.5 (1.1) 48.0 (0.8)
CoLA (Matt.) 10.2 (5.8) 7.5 (4,7)
MNLI (acc) 63.3 (2.4) 62.2 (1.8)
MNLI-mm (acc) 65.1 (2.4) 64.0 (1.8)
QNLI (acc) 66.4 (3.5) 63.8 (2.7)
MR (acc) 85.8 (0.6) 85.7 (0.7)
with the 20prompts. From Table 3, we ﬁnd that
our method even outperforms the ensemble with
20×more number of parameters, showing that it
is a more efﬁcient way to make use of the gener-
ated prompts. We speculate that because of the
over-ﬁtting nature of few-shot learners, members
in the ensemble fail to produce substantial diverse
prediction distributions.
5 Improvements vs. Task Difﬁculty
Here, we show that the improvements achieved
by our method are greater for tasks with higher
difﬁculty. To show this, we ﬁrst sort the 15 tasks by
base (LM-BFF) performance and use this ranking
as a proxy for the difﬁculty of the task. Next, we
report the average improvements achieved by our5580method on the top K hardest tasks, where K goes
from 1 to 15. Figure 2 shows these results. The
ﬁrst bar corresponds to the improvement achieved
by our method on the hardest task, the second bar
corresponds to the average improvement achieved
by our method on the hardest and second-hardest
tasks, and so on. The last bar corresponds to the
average improvement on all 15 tasks.
6 Comparative Experiments
6.1 Input augmentation
The success of contrastive learning heavily relies
on the data augmentation. Our method takes ad-
vantage of prompt-based language learners and nat-
urally creates multi-views of a single input by ap-
pending it with different templates and/or demon-
strations. Compared to EDA which includes syn-
onym replacement (SR), random insertion (RI), ran-
dom swap (RS) and random deletion (RD), our
strategy for augmentation does not lead to incom-
plete and inconsistent sentences, while introducing
adequate variations for effective learning.
The results in Table 4 are obtained by applying
SR, RI, RS, RD, EDA for 10% of input tokens (Re-
sults for 20% are in Appendix F). In contrast to
ours, EDA, etc., for SupCon lead to worse perfor-
mances than the baseline method in many tasks.
6.2 Variable templates, demonstrations
So far, we have shown the results by our method
generating multi-views of inputs by appending both
random templates and demonstrations. However,
we ﬁnd that in some tasks ﬁxed templates withrandom demonstrations or random templates with
ﬁxed demonstration lead to even stronger perfor-
mances (see Table 5). For example, sampling
demonstrations with ﬁxed templates for MRPC
achieves a very strong result ( 80.0), outperform-
ing all other methods in Table 4.
7 Limitations
Since SupCon clusters examples on class level,
our framework applies only to classiﬁcation tasks.
Also, our framework requires large GPU memory,
as SupCon is an in-batch contrastive loss that needs
a large batch size.
8 Conclusion
We proposed a novel supervised contrastive learn-
ing framework and an effective augmentation
method using prompts that can boost the perfor-
mance of prompt-based language learners and out-
perform recent work on 15 few-shot tasks.
9 Ethical Considerations
As far as we are aware, our proposed work does
not have any ethical considerations. However, our
work relies on pre-trained language models, which
have been shown to be biased in prior work (Liang
et al., 2021). As such, users of such models should
be aware of and if possible address such issues.
The data and the code for this work will be made
available to aid reproducibility.5581References55825583A Batch size and learning details
We use the same learning rate of 1efor MLM
loss as LM-BFF. To take full advantage of SupCon,
we apply large batch sizes (16, 32, 40). We show
the batch size and learning rate for SupCon in Table
A.1. Note that for results of LM-BFF shown in the
main paper, we use the same large batch size of our
method to allow for fair comparisons.
We set the batch size to be dividable by the total
number of examples in the task and small enough
to ﬁt into the GPU memory. The experiments with
RoBERTa-base are carried out on one NVIDIA
RTX-A6000 with 48 GB of memory. Experiments
with RoBERTa-large require 4x NVIDIA RTX-
8000 (or RTX-A6000) with 192 (4x 48) GB of
momery.
Following LM-BFF, our ﬁne-tuning runs a maxi-
mum of 1000 steps.
Task Batch LR
SST-2 16 1e
Subj 16 1e
SST-5 40 1e
CoLA 16 1e
TREC 32 1e
MNLI 24 1e
MNLI-mm 24 1e
SNLI 32 1e
QNLI 16 1e
QQP 32 1e
RTE 32 1e
MRPC 16 1e
MR 16 1e
MPQA 16 1e
CR 32 1e
B Fine-tuning with prompts and
demonstrations
We also consider LM-BFF as our baseline method
due to its state-of-the-art performance in a wide
range of few-shot tasks. The given masked lan-
guage modelMﬁrst encodes the input sentence
xinto a sequence of tokens ˜xand maps ˜xto a
sequence of hidden states {h,h,...h}, whereL
is the length of the sequence and h∈R, whered
is the dimension of the hidden states. For example,
in prompt-base ﬁne-tuning, for single sentence textxe.g.,"The story is not worth reading." ), the in-
put with the prompt ( e.g.,"a truly [MASK] one.")
takes the form of
x =[CLS]x, a truly [MASK] one.[SEP]
≡T(x)
Then, the model decides whether it is more likely
to put the label word "great" or"terrible" at the
[MASK] position. Fine-tuning with this ﬁll-in-the-
blank framework has been shown to be superior to
standard ﬁne-tuning (Schick and Schütze, 2021).
By mapping the label space Yto the label words
whereV(y)denotes the label word for class y, the
prediction of the model Mfor classy∈Y can be
written as
p(y|x) =p([MASK] =V(y)|x ) (4)
=exp( w·h )/summationtextexp( w·h )(5)
where wis the weight vector of MLM head.
In LM-BFF, the authors further append demon-
strations to the input x to help the model
better understand what is "great" and"terrible" .
Formally,x≡T(x)and˜T(x,y)denote
T(x)with[MASK] replaced by the label word
V(y). Then, the input to LM-BFF takes the form
of
T(x)⊕˜T(x,y)⊕...⊕˜T(x,y)(6)
In this paper, we use random sampling for the
demonstrations, i.e.,xis randomly chosen from
the training set. The masked language modeling
loss is then
L =/summationdisplay−logp(y|x) (7)
C Language-based Supervised
Contrastive Loss
Our method extends the loss L with an addi-
tional Supervised Contrastive Loss (SupCon). For
applying SupCon on multi-views of an input text,
we need to ﬁrst obtain a second view of a text:
˜x=Aug(˜x) (8)
As we show in ablations, traditional data augmen-
tation for text does not work well in the contrastive
framework. Thus, we propose obtaining a second5584
view by randomly changing the templates and/or
demonstrations:
˜x=T(x)⊕...⊕˜T(x,y)⊕...(9)
˜x=T(x)⊕...⊕˜T(ˆx,y)⊕... (10)
whereTdenotes a set of pre-deﬁned templates,
t∈Tandt/negationslash=t.ˆxis another randomly
sampled example as the demonstration text and
ˆx/negationslash=x. This strategy serves as a perfect form of
augmentation for our purpose as it does not gener-
ate incomplete or inconsistent sentences, and since
we do not edit the main input, the label for that
input stays the same. Furthermore, ˜xhas a sub-
stantial variation from ˜x, which allows for ef-
fective contrastive learning.
D Manual primary prompts
Table D.1 shows the primary prompts we used for
each task. Those prompts are manually chosen by
LM-BFF (Gao et al., 2021).
E Experiments with RoBERTa-large
While we use RoBERTa-base to conduct exten-
sive experiments in our main study and ablations,
here we compare our framework to LM-BFF using
RoBERTa-large. We also include results directly
reported from LM-BFF (Gao et al., 2021) (hence-
forth referred to as LM-BFF †) , though the compar-
ison between them could be unfair since the results
reported in the original LM-BFF paper are:
•obtained with an additional sampling strategy
to select similar demonstrations (section 6.2
of their paper), which put our results at a dis-
advantage.•obtained from a set of batch sizes (2,4,8) and
learning rates ( 1e,2e,5e) and the best
models are selected from the validation set.
Whereas we show experimental results with
models trained with a ﬁxed batch size and a
learning rate of 1e.
Nevertheless, we show the state-of-the-art results
we achieved in Table E.1. We only marginally
under-perform LM-BFF †in 2 tasks, possibly due
to the reasons listed above.5585Task SR RI RS R D EDA ours
SST-2 (acc) 90.6 (0.5) 90.8 (0.4) 90.8 (0.4) 90.8 (0.4) 90.7 (0.6) 90.6 (0.1)
Subj (acc) 90.4 (1.4) 90.4 (1.3) 90.4 (2.5) 90.3 (1.4) 90.4 (1.1) 90.4 (1.1)
SST-5 (acc) 47.6 (1.4) 47.0 (1.8) 46.5 (1.7) 46.9 (1.9) 45.2 (2.1) 49.5 (1.1)
CoLA (Matt.) 6.0 (5.3) 6.0 (5.4) 7.2 (3.8) 5.6 (3.0) 5.6 (2.9) 10.2 (5.8)
TREC (acc) 80.7 (2.2) 79.1 (3.8) 81.2 (2.2) 82.8 (2.1) 81.1 (4.3) 83.3 (1.5)
MNLI (acc) 60.3 (2.2) 61.2 (2.1) 60.7 (2.6) 60.2 (2.1) 58.3 (2.5) 64.0 (2.0)
MNLI-mm (acc) 62.2 (1.5) 63.3 (1.3) 63.0 (1.7) 62.9 (1.2) 60.2 (2.1) 65.5 (2.7)
SNLI (acc) 63.4 (4.1) 63.4 (3.8) 62.3 (3.6) 62.0 (4.3) 63.0 (4.2) 69.9 (2.4)
QNLI (acc) 63.2 (3.3) 64.5 (4.6) 64.0 (4.3) 64.8 (4.5) 60.8 (3.6) 66.4 (3.5)
QQP (acc) 64.8 (2.8) 62.9 (2.2) 62.0 (3.5) 63.9 (2.1) 60.4 (5.8) 68.8 (3.8)
RTE (acc) 61.2 (3.0) 62.2 (3.7) 47.9 (0.8) 47.9 (0.8) 64.3 (2.7) 65.1 (3.5)
MRPC (F1) 77.0 (3.8) 79.2 (4.6) 78.5 (2.1) 77.4 (3.2) 76.2 (6.0) 78.2 (3.1)
MR (acc) 83.3 (1.4) 85.5 (0.5) 85.6 (0.6) 85.2 (0.3) 85.7 (0.7) 85.8 (0.6)
MPQA (acc) 82.6 (2.8) 82.7 (2.4) 83.4 (2.4) 84.3 (2.0) 83.1 (2.9) 84.6 (1.5)
CR (acc) 87.8 (0.9) 88.1 (0.3) 87.5 (1.0) 88.5 (1.0) 87.9 (0.5) 89.4 (1.0)
F Augmentation with 20% of input
tokens
In the main paper, we compare our augmentation
strategy (random templates and random demon-
strations) to standard augmentation techniques on
10% of input tokens for creating multi-view of in-
puts to apply the SupCon loss. Here, we show
additional experimental results with synonym re-
placement (SR), random insertion (RI), random
swapping (RS), random deletion (RD) and EDA
(Wei and Zou, 2019) (with SR, RI, RS and RD
all together) at 20% of input tokens. Same as be-
fore, the model under-performs when using stan-
dard augmentations. Results are shown in Table
F.1.
G SimCLR vs. SupCon
Here, we compare our choice of contrastive loss
SupCon (Khosla et al., 2020) to an unsupervised
version SimCLR (Chen et al., 2020a). Unsuper-
vised contrastive loss clusters examples at the in-
stance level, i.e., it only pulls the same instance
under different views close to each other and push
away all the others in a mini-batch. Whereas Sup-
Con clusters examples at the class level.
As shown in Table G.1. SupCon is better than
SimCLR in all tasks. In many cases, SimCLR
even underperforms the baselines by large margins
(see LM-BFF in Table 1), indicating that learningTask SimCLR SupCon
SST-2 (acc) 89.7 (0.8) 90.6 (0.1)
Subj (acc) 86.4 (1.2) 90.4 (1.1)
SST-5 (acc) 42.1 (0.8) 49.5 (1.1)
CoLA (Matt.) 1.8 (3.6) 10.2 (5.8)
TREC (acc) 60.0 (4.4) 83.3 (1.5)
MNLI (acc) 52.5 (1.2) 64.0 (2.0)
MNLI-mm (acc) 53.0 (1.2) 65.5 (2.7)
SNLI (acc) 60.0 (4.5) 69.9 (2.4)
QNLI (acc) 53.5 (0.6) 66.4 (3.5)
QQP (acc) 56.4 (2.0) 68.8 (3.8)
RTE (acc) 58.6 (2.8) 65.1 (3.5)
MRPC (F1) 68.2 (6.3) 78.2 (3.1)
MR (acc) 84.7 (1.1) 85.8 (0.6)
MPQA (acc) 82.2 (1.6) 84.6 (1.5)
CR (acc) 88.1 (2.1) 89.4 (1.0)
discriminative features at instance level only can
hurt the ﬁne-tuning process.
H[CLS] vs.[MASK]
SupCon takes the representations of inputs to per-
form contrastive learning. We use the hidden states
at[MASK] tokens as the representations of sen-
tences in the main experiments. Another common
choice is to take the hidden states at [CLS] tokens.5586Task [CLS] [MASK]
SST-2 (acc) 90.4 (0.5) 90.6 (0.1)
Subj (acc) 90.0 (1.5) 90.4 (1.1)
SST-5 (acc) 48.2 (0.6) 49.5 (1.1)
CoLA (Matt.) 10.7 (5.7) 10.2 (5.8)
TREC (acc) 81.1 (1.4) 83.3 (1.5)
MNLI (acc) 59.9 (3.7) 64.0 (2.0)
MNLI-mm (acc) 61.4 (4.0) 65.5 (2.7)
SNLI (acc) 66.9 (2.5) 69.9 (2.4)
QNLI (acc) 65.1 (3.1) 66.4 (3.5)
QQP (acc) 66.0 (3.0) 68.8 (3.8)
RTE (acc) 63.4 (3.1) 65.1 (3.5)
MRPC (F1) 77.7 (1.8) 78.2 (3.1)
MR (acc) 85.5 (0.8) 85.8 (0.6)
MPQA (acc) 84.2 (1.6) 84.6 (1.5)
CR (acc) 88.4 (1.2) 89.4 (1.0)
For example, in standard ﬁne-tuning, the algorithm
takes the representation of a sentence at [CLS]
token and attaches a linear classiﬁer on top of it.
Based on Table H.1, applying the contrastive loss
at[MASK] tokens is generally better than apply-
ing it at [CLS] . This is fairly intuitive, as the ﬁnal
classiﬁcations are performed at [MASK] tokens
and enforcing class-level discriminative represen-
tations explicitly at [MASK] tokens helps models
generalize better after ﬁne-tuning.
I Adapting ADAPET
For results in Table 2, we adapt the open-source
code from ADAPET to our codebase. In origi-
nal ADAPET, there are multiple label words corre-
sponding to a label class (e.g positive class: "great",
"good", "nice" ). To make a fair comparison to
LM-BFF and ours, we only apply one label word
corresponding to a label class (e.g positive class:
"great" ). The original Label Condition Loss im-
plemented in ADAPET has a hyper-parameter α
to control the percentage of input tokens used for
masked language modeling (see ADAPET (Tam
et al., 2021) for more details). To match the training
objective in LM-BFF with only one [MASK] to-
ken, we also set the Label Condition Loss to apply
to one random token of inputs, i.e., α=.5587