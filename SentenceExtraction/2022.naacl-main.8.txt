
Jiangang Bai, Yujing Wang, Hong Sun, Ruonan Wu, Tianmeng Yang,
Pengfei Tang, Defu Cao, Mingliang Zhang, Yunhai Tong,
Yaming Yang, Jing Bai, Ruofei Zhang, Hao Sun, Wei ShenPeking University, Beijing, ChinaMicrosoft, Beijing, China
Abstract
Large-scale pre-trained language models have
attracted extensive attentions in the research
community and shown promising results on var-
ious tasks of natural language processing. How-
ever, the attention maps, which record the at-
tention scores between tokens in self-attention
mechanism, are sometimes ineffective as they
are learned implicitly without the guidance of
explicit semantic knowledge. Thus, we aim
to infuse explicit external knowledge into pre-
trained language models to further boost their
performance. Existing works of knowledge in-
fusion largely depend on multi-task learning
frameworks, which are inefficient and require
large-scale re-training when new knowledge is
considered. In this paper, we propose a novel
and generic solution, KAM-BERT, which di-
rectly incorporates knowledge-generated atten-
tion maps into the self-attention mechanism. It
requires only a few extra parameters and sup-
ports efficient fine-tuning once new knowledge
is added. KAM-BERT achieves consistent im-
provements on various academic datasets for
natural language understanding. It also out-
performs other state-of-the-art methods which
conduct knowledge infusion into transformer-
based architectures. Moreover, we apply our
model to an industry-scale ad relevance applica-
tion and show its advantages in the real-world
scenario.
1 Introduction
Language models pre-trained by a large text cor-
pus have shown superior performances on a wide
range of natural language processing tasks. Many
advanced models based on the transformer archi-
tectures achieve state-of-the-art results on various
NLP benchmarks. Existing literature (Jawahar
et al., 2019; Hewitt and Manning, 2019) shows
that pre-training enables a model to capture syntac-
tic and semantic information in the self-attention
mechanism. However, the attention maps, whichrecord the attention scores between tokens in a self-
attention mechanism, are sometimes ineffective as
they are learned implicitly without the guidance of
explicit semantics (Jain and Wallace, 2019). If the
knowledge can be leveraged in a reasonable way
to guide the self-attention mechanism, we have a
good chance to improve the quality of attention
scores as well as the performance of downstream
applications.
Recently, there have been multiple attempts for
incorporating knowledge into transformer archi-
tectures. ERNIE (Zhang et al., 2019) and KE-
PLER (Wang et al., 2019) utilize both large-scale
textual corpora and knowledge graphs to train a rep-
resentation model in a multi-task learning frame-
work. They need to be retrained from scratch when
injecting new knowledge, which is inefficient and
can not benefit from existing pre-trained check-
points. K-Adapter (Wang et al., 2020) integrates
additional neural models to capture different kinds
of knowledge. It enables adaptation based on pre-
trained language models. However, it does not
instruct the self-attention mechanism directly and
introduces a relatively large number of parameters
to the original model.
In this paper, we propose a novel and generic
self-attention mechanism enhanced by explicit
knowledge to address problems mentioned above.
First, we show a failure case of query-ad matching,
which motivates us to inject explicit knowledge into
self-attention mechanism. In Figure 1, the attention
map of a query-ad pair is visualized, and the goal
is to judge if the query and ad text are semantically
relevant. As shown in the figure, BERT misclas-
sifies this pair as irrelevant, probably because it
does not understand the query word “glipizide” ,
which rarely appears in the pre-training corpus. In
fact, “glipizide” is a kind of medicine and highly
related to the word “Pharmacy” in ad text, so this
case should be classified as relevant. In this case,
if we know “glipizide” is semantically correlated107
to“Pharmacy” as prior knowledge, we can enrich
the attention maps accordingly. In addition, terms
“Fred” and“Meyer” are from the same entity, so
the attention scores between these two terms should
be relatively high. Based on this fact, we believe
that simply using language models pre-trained by
a general corpus is not enough to meet the satisfac-
tion of a specific application. Thus, our motivation
is to inject explicit knowledge into the transformer
architecture, which guides the pre-trained language
model to perform better adaptation in an efficient
fine-tuning procedure.
To address the above motivation, we pro-
pose a novel architecture, namely KAM-BERT
(Knowledge-assisted Attention Maps for BERT).
First, it constructs semantic attention maps based
on corresponding relevance functions defined by
various kinds of semantic knowledge. Specifically,
we consider three kinds of semantic knowledge
to guide the self-attention mechanism, i.e., entity,
phrase segmentation, and term correlation. En-
tity and phrase segmentation indicate the cohesion
of continuous terms, while term correlation can
help to enrich the semantic representation of a sen-
tence. Then, the knowledge infusion procedure is
completed by concatenating these attention maps
with vanilla self-attention and then performing 2D-
convolution for integration. Finally, the result atten-
tion maps are served as inputs for value projection,
and the rest part is the same as a standard trans-
former. The KAM-BERT model can be fine-tuned
on existing pre-trained checkpoints in a plug and
play mode, which is highly efficient in practice.
As illustrated in Section 4, we compare KAM-
BERT with BERT and other knowledge-enhanced
SOTAs on various natural language understand-
ing tasks, where KAM-BERT shows consistent
superiority. Especially, we lift the average score
of BERT-Base from 77.5 to 78.7 on the GLUE
benchmark. We also demonstrate the advantage of
KAM-BERT for knowledge injection on LAMA,a probing benchmark to analyze the factual and
commonsense knowledge contained in a model.
Furthermore, KAM-BERT is successfully applied
to the query-ad relevance scenario in a commercial
search engine and shows significant lift in AUC
score.
The major contributions of this paper are sum-
marized as follows:
•First, we propose a novel self-attention mech-
anism enhanced by semantic attention maps,
which incorporates knowledge from entity,
phrase segmentation, and term correlation.
Ablation study will demonstrate the effective-
ness of these kinds of semantic attention maps.
•Second, KAM-BERT requires little extra
memory and computation cost compared to
vanilla BERT and other SOTAs. It can be fine-
tuned efficiently on existing language mod-
els for a given application. We have success-
fully applied it to improve the performance
of query-ad relevance in a commercial search
engine.
•Last but not least, the proposed framework is
generic and flexible for infusing various kinds
of knowledge into the transformer architec-
ture. Except for the three kinds of knowledge
considered in this paper, we will also show-
case how to incorporate other kinds of infor-
mation, such as a knowledge graph. It opens
up new opportunities for further exploration.
2 Related Works
After the NLP landmark models Trans-
former (Vaswani et al., 2017) and BERT (Devlin
et al., 2019), many efforts have been devoted to
pre-training representation models for natural
language and utilizing them to benefit specific
NLP tasks. Most of them capture semantic infor-
mation in the self-attention mechanism implicitly,
while recent works demonstrated that injecting
explicit knowledge significantly enhanced the108performances of downstream tasks. ERNIE (Zhang
et al., 2019) makes a preliminary attempt to utilize
knowledge graph to improve the performance
of knowledge-driven tasks. LIBERT (Lauscher
et al., 2019) injects pairs of words with synonym
and hyponym relations in WordNet. SenseBERT
(Levine et al., 2020) considered word-supersense
knowledge by predicting the supersense of the
masked word. KnowBERT (Peters et al., 2019)
incorporates knowledge bases into BERT through
Knowledge attention and re-contextualization.
WKLM (Xiong et al., 2019) replaces entity
mentions in the original document with names
of other entities of the same type, and is trained
to distinguish the correct entity mention from
random ones. These models are supposed to be
retrained when injecting new kinds of knowledge.
K-Adapter (Wang et al., 2020) addresses this
problem by plugging multiple ways of adapters
for different kinds of knowledge. Notably, most
of methods above need to be pre-trained from
scratch while our method do not. Instead, we
inject multiple kinds of knowledge directly into the
self-attention maps and support efficient adaptation
on a pre-trained language model.
3 KAM-BERT
As illustrated in Figure 2, KAM-BERT injects mul-
tiple kinds of knowledge into transformer-based
pre-trained models in the form of multi-channel
semantic attention maps. Different kinds of knowl-
edge can be extracted independently and infused
together into one self-attention map in the trans-
former architecture. KAM-BERT can be fine-tuned
directly from an existing checkpoint of BERT,
while additional parameters are initialized ran-
domly and learned in the fine-tuning stage. Thus,
it is quite efficient and flexible to incorporate new
kinds of knowledge into the model.
Below we first describe the standard self-
attention mechanism. Then, we will introduce the
generic definition of semantic attention maps, as
well as the methodology of multi-channel knowl-
edge infusion which integrates semantic attention
maps into transformer models. At last, the genera-
tion of different kinds of semantic attention maps
will be presented. Note that the time complexity
of KAM-BERT is on-par with a vanilla BERT. A
detailed analysis can be found in the supplementary
material.3.1 Self-Attention
The representation of a text sequence can be writ-
ten asX∈R, where Ndenotes the sequence
length and Cis the word embedding dimension
size. A standard Transformer block is composed
of a self-attention layer and a position-wise feed-
forward layer, while each attention map is gen-
erated by a self-attention layer without any other
prior knowledge introduced.
The self-attention mechanism plays an important
role in the transformer-based model. In a vanilla
Transformer, the self-attention map Aof layer
iis calculated by the dimension-normalized dot-
product operation.
A=Self-Attention (X) =QK
√
d(1)
where dis the dimension of representation vectors.
In a vanilla transformer, Ais then normalized by
softmax and fed into position-wise feed-forward
layers. In KAM-BERT, the self-attention map A
is infused with semantic attention maps to calculate
the final attention matrix, which will be described
in the following sub-sections.
3.2 Semantic Attention Maps
Given a sequence of tokens {x, x, ..., x}, the
semantic attention map can be defined in a generic
form M∈R, where M∈[0,1]denotes the
attention score from token ito token j, andnis the
number of tokens in the current sentence. Then, for
a specific kind of knowledge, we need a correspond-
ing relevance function to calculate the attention
score, i.e.,M=Relevance (x, x). Note that
ifxdenotes a sub-word as in the BERT model, we
define M=Relevance (W(x), W(x)), where
W(x)denotes the entire word which contains
the sub-word x. For example, BERT will con-
vert a sequence "I like tacos !" into a sequence of
sub-words, i.e., {I, like, ta, ##cos, !} , where "ta"
and"##cos" are sub-words from “tacos” , so both
W(ta)andW(##cos)denote the word "tacos" .
In Section 3.4, we will introduce three kinds of
semantic attention maps considered in this paper
and the generation method for other knowledge-
assisted attention maps.
3.3 Multi-Channel Knowledge Infusion
In order to incorporate external knowledge into
self-attention, we concatenate semantic attention
maps with vanilla self-attention, and then infuse
them into a single multi-head attention map us-
ing multi-channel 2D-convolutions. Applying 2D-
convolution to a self-attention map is first pro-109
posed by (Wang et al., 2021) and shows advan-
tages in both NLP and CV tasks. Here we use
2D-convolution to infuse different kinds of knowl-
edge.
First, we perform Channel Wise Concatenation
(CWC) : the vanilla self-attention map Awill be
concatenated with each semantic attention map M
separately along the channel dimension. Then, a
multi-channel 2D-convolution is applied to gener-
ate an knowledge-infused attention map, denoted
byA. The entire process can be formulated as
below.
A=Conv (CWC (A|M)) (2)
where Mis a set of semantic attention maps
obtained by kdifferent kinds of knowledge, includ-
ing but not limited to the three ones considered in
this paper; To infuse different types of knowledge,
we apply a standard 2D convolution operation, the
output dimension of which is the same as that of
A. IfAhasmattention heads, then Awill
also has mattention heads. We adopt 3×3kernel
for the convolution empirically as it performs better
than1×1and5×5kernels according to (Wang
et al., 2021).
At last, we adopt a hyper-parameter αto balance
the importance of AandA.
A=Softmax (α·A+ (1−α)·A)
(3)
After softmax activation, we get the final self-
attention map Awithmheads for the i-th layer.Given the self-attention map, the rest components
are identical to a vanilla Transformer, which can
be calculated as
h=AV,H= (/circleplusdisplayh)W, j∈m. (4)
In detail, we use the obtained attention map A
to multiply the value matrix Vin the attention
mechanism to get the representation hof the j-th
attention head. Next, the outputs of all attention
heads from each layer will be concatenated along
the embedding dimension. Finally, we multiply
this result with a linear transformation matrix W
to get the output representation of the i-th KAM-
BERT layer. Besides, we add a skip connection
from the result attention map in the i-th layer to the
self-attention map of the i+ 1layer to enhance the
flow of information between layers.
3.4 Generation of Semantic Attention Maps
The knowledge we inject into KAM-BERT in-
cludes entity information, phrase segmentation in-
formation, and term correlation information. We
consider these three types of knowledge because
they reflect language semantics from different per-
spectives. Entity and phrase represent coherence
between adjacent words while term correlations
build a semantic bridge for related words which
may be far away or even unseen in the current
sentence. The first two kinds of knowledge are
presented as labeled sequences, and the last one is
presented as relationship between tokens. As de-
fined in Section 3.2, a specific kind of knowledge110can be transferred to semantic attention maps once
the corresponding Relevance function is defined.
In the following paragraphs, we will demonstrate
how to define Relevance functions for the three
types of knowledge used in this paper. Also, we
need to emphasize that the proposed framework
is generic and is feasible to incorporate other se-
mantic information like a knowledge graph. Thus,
we will discuss how to generate other knowledge-
assisted attention maps as our future work.
Entity Attention Map Named Entity Recogni-
tion (NER) (Nadeau and Sekine, 2007) is a stan-
dard task for natural language processing which
has been studied for years. Mathematically, a
entity extractor transforms the sequence of to-
kens{x, x, ..., x}into a sequence of labels
{label, label, ..., label}, where labelfalls
into one of three classes, denoting non-entity words,
starting words in entities and other words in entities.
Based on the labeled sequence, the Relevance
function for entity attention map can be defined
as
Relevance (W, W) =/braceleftigg
1W≡W
0otherwise(5)
where A≡Bdenotes AandBbelong to the
same entity.
Phrase Segmentation Attention Map Similar
to the entity attention map, one can highlight the
term correlations within the same phrase segmenta-
tion to emphasize the locality inductive bias. Syn-
tax tree is a generic source to extract phrases in
different semantic levels, which can be generated
by a trained syntax parser. In a syntax tree, each
internal node represents a phrase segment for a
specific level. For example, we can select the par-
ents of leaf nodes in the syntax tree as the root of
each sub-tree which represents a phrase. We define
the distance of an internal node ito the leaf node
aslevel(i). Thus, the relevance function can be
computed by
Relevance (W, W) =/braceleftigg
1W≡W
0otherwise(6)
where A≡Bdenotes that AandBbelong to the
same sub-tree at level(i).
Term Correlation Attention Map In compu-
tational linguistics, Pointwise Mutual Information
(PMI) has been widely used for finding associa-
tions between words (Arora et al., 2016). In our
work, we adopt PMI to measure the semantic cor-
relations between terms. The PMI of a pair (x, y)
from discrete random variables (X, Y )quantifiesthe discrepancy between the probability of their
coincidence given joint distributions and individual
distributions. We define the PMI-based relevance
function as
PMI (x;y) = logp(x, y)
p(x)p(y)
Relevance (W, W) =PMI (W;W)/Z(7)
where Zdenotes the normalized factor of PMI ma-
trix. In our experiments, PMI is calculated us-
ing a large web corpus. We calculate the prob-
ability p(x, y)of a word pair appearing jointly,
and the probability of single word appearance
is denoted as p(x)andp(y). Finally, we use
logp(x, y)−logp(x)−logp(y)to compute the
PMI score.
To better incorporate semantic knowledge into
attention maps, we further enrich each attention
map by adding top kterms which do not appear
in the current sentence but hold the highest av-
erage PMI scores with terms in the original sen-
tence. Note that we should expand the selected
kwords to Ksubwords for BERT. Then the sub-
words will be appended to the original sentence.
After augmentation, the input sentence has N+K
words and the shape of an attention map becomes
(N+K)×(N+K), where NandKdenote
the number of original terms and auxiliary terms
respectively (see an example in Fig. 2(d)). In or-
der to align the shapes of different attention maps
(including the vanilla self-attention map), we add
zero-padding for smaller ones. After passing one
transformer layer, the output sequence length is
stillN+K. Note that the auxiliary words are only
utilized to enrich the semantics of original word
representations, which is done within each trans-
former layer. Thus, we trim the output sequence
length to Nbefore taking it as input to the next
transformer layer (while a new round of augmenta-
tion will be done in the next layer).
Other Knowledge-assisted Attention Maps.
The KAM-BERT framework is generic and can
be extended to other kinds of knowledge in future
works. For each semantic type, we can define a
specific Relevance function to transfer the corre-
sponding information into semantic attention maps.
For example, we can define the relevance function
for a Knowledge Graph (KG) as:
Relevance (W, W) =/braceleftigg
1E(W)≡E(W)
0otherwise
(8)111where E(W)is the corresponding entity of word
or sub-word Win a KG, and A≡Brepresents
that both AandBexist and are adjacent in a KG.
4 Experiments
We briefly introduced the extraction of semantic
information in Section 4.1. Then we report experi-
mental results on natural language understand and
question answering tasks in Section 4.2 and 4.3
respectively. In Section 4.4, we show evaluation
on LAMA, a benchmark especially designed to
study how much semantic knowledge is contained
in a language model. Experiments on query-ad
relevance is described in Section 4.5. At last, we
present ablation study in Section 4.6 .
4.1 Semantic Information Extraction
We use Stanza library (Qi et al., 2020) to extract
NER information and syntax information. Stanza
NER takes one sentence as input and returns the
start and end indices of the corresponding named
entity in the sentence. While Stanza Parser can
extract the corresponding syntax tree for each sen-
tence. We use query-ad logs from a commercial
search engine to calculate PMI matrix. These steps
gain the knowledge required to generate the seman-
tic attention maps mentioned in Section 3.4.
4.2 GLUE Benchmark
The GLUE benchmark offers a collection of tools
for evaluating the performance of models across a
diverse set of NLP applications. It contains single-
sentence classification tasks (CoLA and SST-2),
similarity and paraphrase tasks (MRPC, QQP and
STS-B) and pairwise inference tasks (MNLI, RTE
and QNLI). We use the default train/dev/test split
for each dataset. The hyper-parameters are cho-
sen based on the validation set (refer to appendix
for details). After the model is trained, we make
predictions on the test data and send the results
to GLUE online evaluation serviceto get testing
scores. Note that the original WNLI dataset in the
GLUE benchmark is problematic, which causes the
evaluation results to be 65.1. In order to make a fair
comparison, most papers (Devlin et al., 2019; Liu
et al., 2019; Yang et al., 2019) choose to ignore the
results of WNLI when calculating GLUE average
score.
The scores on all datasets in GLUE benchmark
are listed in Table 1. We report test scores on
BERT-Base, BERT-Large, RoBERTa related mod-
els and their corresponding enhanced models. The
performances of BERT-Base, BERT-Large andRoBERTa-Large are reproduced using the official
checkpoints provided by corresponding authors.
As shown in the table, our models outperform
all corresponding baselines. KAM-BERT-Base
achieves an average GLUE score of 78.7, lifting 1.2
scores from standard BERT-Base model with only
a few extra parameters introduced to the baseline
model. Particularly, the improvements on CoLA
datasets are fairly large, showing that our knowl-
edge integration method has good generalization
performance for natural language inference and
understanding. ERNIE have also added external in-
formation such as entity and knowledge graph, but
it needs much more time for a joint re-training. As
for BERT-Large and its counterpart KAM-BERT-
Large, the average improvement on GLUE bench-
mark is 0.9. We can see that the improvement
becomes smaller when the model grows larger, be-
cause larger models often capture more semantic
knowledge in the pre-training phrase. But incorpo-
rating explicit knowledge is still indispensable for
achieving a superior performance.
4.3 Question Answering
We conduct experiments on two kinds of ques-
tion answering tasks, i.e., commonsense QA
and open-domain QA. Commonsense QA aims
to answer questions with commonsense. We
adopt CosmosQA for evaluation, which requires
commonsense-based reading comprehension for-
mulated as multiple answer selection. Open-
domain QA aims to answer questions using ex-
ternal resources such as collections of documents
and webpages. We consider two public datasets for
this task, i.e., Quasar-T and SearchQA.
The results of CosmosQA are shown in Table 2.
Compared with BERT-Large, KAM-BERT-Large
achieves 10.6% improvement in accuracy. KAM-
RoBERTa-Large further improves the accuracy of
RoBERTa-Large by 5.4%, which indicates that our
models has better knowledge inference ability. For
open-domain QA, our model also achieves better
results compared to corresponding baselines. This
because that KAM-based models can make full use
of the infused knowledge. At the same time, one
can notice that KAM-based models have fewer pa-
rameters than K-Adaptor, demonstrating its effec-
tiveness for knowledge infusion. WKLM (Xiong
et al., 2019) forces the pre-trained language model
to incorporate knowledge from a knowledge graph.
This makes WKLM to achieve a better score on
QA tasks, but KAM-BERT performs even better
than WKLM.112
4.4 LAMA Benchmark
To further verify whether KAM-BERT better inte-
grate internal knowledge into pre-trained language
models, we conduct experiments on LAMA, a
widely used benchmark for knowledge probing.
LAMA examines models’ abilities on recalling re-
lational facts by cloze-style questions. The first
place micro-averaged accuracy is used as evalua-
tion metrics. The evaluation results are shown in
Table 3. KAM-BERT consistently outperforms cor-
responding baselines on all tasks. It indicates that
KAM-BERT can generate better attention maps
with semantic guidance.
4.5 Query-Ad Relevance
Query-ad relevance measures how relevant a search
ad matches with a user’s search query. Very of-
ten queries and ads have words with special mean-
ings, which are not easily understood well by tra-
ditional NLP techniques but can benefit from the
knowledge-assisted mechanism proposed in this
work. Besides, user queries and ads text often con-
tain noises, so evaluation on query-ad relevance
task would test our model’s robustness and resis-
tance of noise. We compare BERT and KAM-
BERT on a large-scale internal dataset of a com-
mercial search engine. As shown in Table 5, our
model outperforms corresponding baselines by a
large margin, which is statistically significant un-
der 95% confidence interval. One thing to call
out is that, although NER and syntax parsing re-
sults are nosier comparing to the ones in academic
datasets, we still have good improvements on this
dataset. This indicates the way we combine those
knowledge together makes our model more robust
to noisy inputs.
4.6 Model Analysis
In this section, we explore the sensitivity of hyper-
parameter α, and then conduct ablation experi-
ments on three types of added knowledge.
Hyper-parameter Analysis The optimal α
value after grid search is 0.2, which means that
the original attention maps still dominate the token
relationships. We chose three tasks from different
fields to do ablation study for α. Our model is
KAM-BERT-Base, and its performance is shown
in Table 4. In three different tasks, setting αto 0.2
achieves the best results. An intuitive understand-
ing is that when αis small, external knowledge
plays an unimportant role and cannot participate in113
the entire training process well. With the gradual
increase of α, the intervention of external knowl-
edge on the attention map will increase, and the
attention relationship in the original sequence will
be gradually lost, resulting in the decline of model
performance.
Ablation Study For a comprehensive under-
standing of our model design, we conduct abla-
tion study with the following settings in Table 6.
(1)KAM-BERT w/o PMI : the PMI-based attention
maps are removed; (2) KAM-BERT w/o Phrase :
the phrase-based attention maps are removed; (3)
KAM-BERT w/o Entity : the entity-based attention
maps are removed. (4) KAM-BERT w/o Convolu-
tion: the convolution layers for knowledge-assisted
attention map integration are removed. Instead,
we merge all the attention maps through average
aggregation.
The average scores of all ablation experiments
are better than BERT, but are relatively worse than
KAM-BERT, demonstrating all the components
are beneficial for the final performance. At the
same time, we observe that after deleting the en-
tity attention map, the score of KAM-BERT drops
drastically from 78.7 to 77.7. This shows that the
gain brought by entity information is the greatest.
In addition, the convolution layer is indispensable
for achieving a superior performance.
4.7 Case Study
In Figure 3, we visualize an example of query-ad
relevance, where the query is “buy glipizide” and
ad text is “Fred Meyer Pharmacy Near Me” . The
darker color in the figure represents a higher at-
tention score. Figure 3(a) is the attention map of
vanilla BERT without adding explicit knowledge.
When encountering rare words like “glipizide” ,
the self-attention mechanism cannot do a good job
to decide which terms should “glipizide” attend
to. But in Figure 3(b), the attention map of KAM-
BERT uses term correlations to learn that “glip-
izide” is a medicine, so it focuses on the medicine-
related tokens like “Pharmacy” and“antidiabetic” .
Figure 3(c) and 3(d) are the attention maps for the
sentence “The Academy of Fine Arts is located
in Northern Maidan. ” We only visualize the key
parts due to space limitation. Figure 3(c) shows
the attention map for vanilla BERT, and 3(d) is the
attention map for KAM-BERT. It can be observed
that the terms in the same entity are highly cor-
related with each other in KAM-BERT, which is
more reasonable than vanilla BERT.
5 Conclusion
In this paper, we proposed KAM-BERT, a flex-
ible and efficient approach to inject knowledge
into transformer-based pre-trained models. Exten-
sive experiments on GLUE and LAMA benchmark
show that our approach outperforms all BERT-
Style baselines and achieves new SOTA on QA
tasks, suggesting that our models indeed integrate
knowledge in an effective manner and have good
generalization ability. In future work, we hope to
investigate more types of knowledge which can be
effectively integrated in our framework.114References115