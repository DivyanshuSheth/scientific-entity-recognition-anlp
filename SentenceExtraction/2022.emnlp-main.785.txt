
Alexey Sorokin
Yandex
Moscow State University
alexey dot sorokin at list dot ru
Abstract
We offer a two-stage reranking method for
grammatical error correction: the first model
serves as edit generator, while the second clas-
sifies the proposed edits as correct or false. We
show how to use both encoder-decoder and se-
quence labeling models for the first step of our
pipeline. We achieve state-of-the-art quality on
BEA 2019 English dataset even using weak
BERT-GEC edit generator. Combining our
roberta-base scorer with state-of-the-art GEC-
ToR edit generator, we surpass GECToR by
2−3%. With a larger model we establish a
new SOTA on BEA development and test sets.
Our model also sets a new SOTA on Russian,
despite using smaller models and less data than
the previous approaches.
1 Introduction
Grammatical error correction (GEC) is the task of
converting the source text to its clean version with
no orthographic, punctuation, lexical or other er-
rors. It becomes increasingly popular during last
years due to its applications such as Second Lan-
guage Learning. However, even for English with
its numerous resources and wide research commu-
nity, modern models are far from being perfect. In
particular, the recall of the state-of-the-art GEC-
ToR model (Omelianchuk et al., 2020) on stan-
dard BEA2019 development set is lower than 40%.
While GECToR uses sequence labeling approach
with linguistically motivated label inventory, most
works for other languages, such as (Náplava and
Straka, 2019) and others, simply mimic machine
translation methods, training a Transformer model
(Vaswani et al., 2017) on the pairs of source and
corrected sentences. This approach suffers from
left-to-right decoding: the model can make a wrong
decision not observing the future context.
This problem may be mitigated using reranking:
the ranker observes entire corrected sequences and
thus may utilize richer context. It also helps todiscriminate between several possible edits with
similar basic model probability . Due to these rea-
sons, it was heavily used in machine translation
both in statistical (Och et al., 2004) and neural (Yee
et al., 2019) era.
In contrast to machine translation, sequence edit-
ing in GEC usually does not require complete
rewriting and can be decomposed to elementary
edits such as modifying a single word or a consec-
utive group of words. In this paper we propose
to score elementary edits produced by the basic
model and classify them as positive or negative on
the second stage of the pipeline. Than the calcu-
lated probabilities can be either used directly or
combined with the scores from the first stage.
We show that our scoring model achieves state-
of-the-art performance on BEA2019 dataset even
with rather weak first stage model. Its combina-
tion with GECToR edit generator outperforms the
models of the same size by about 2 Fpoints.
The large version of our model establishes the new
SOTA on BEA dataset among models of all size.
We also improve over current SOTA on Russian
RULEC-GEC dataset by more than 3 Fpoints.
We make most of our code and models freely avail-
able.
2 Pipeline description
As proposed by Alikaniotis and Raheja (2019),
probably the simplest approach to grammatical
error correction is to generate possible edits us-
ing a rule-based model and then extract those that
increase the sentence probability by a sufficient
margin. This probability may be estimated using
a large pretrained language model, such as GPT
(Radford et al., 2019) or BERT (Devlin et al., 2019).
This approach requires no training data, only a de-
velopment set for tuning the hyperparameters. As11416a reverse side of its simplicity, this algorithm has
two main limitations:
•Recall is limited to errors that can be specified
by the rules.
•The probability estimators are imperfect, espe-
cially when the edit changes sequence length.
Therefore the main idea of our paper is to replace
the scorer by a more powerful trainable model. An-
other key detail is that we apply the scorer not to
the full corrections, but to the elementary edits.
Namely, given the erroneous sentence *The boy
fall on floor yesterday and its correction The boy
fell on the floor yesterday , our model should return
True for sentences The boy fell on floor yesterday
andThe boy fall on the floor yesterday andFalse
for other elementary corrections, for example, *The
boy falls on floor yesterday .
So, our model includes three main stages:
1.Extracting elementary edits from the edit gen-
erator.
2.Classifying these edits as positive or negative.
3. Applying the positively classified edits to the
source sentence.
The first part is described in 3 and the remaining
two in Section 4. A schematic description of our
algorithm is given on Figure 1.
3 Edit generators
In this section we describe the first stage of our
pipeline – the edit generator. We seek to make our
pipeline independent of particular generator selec-
tion. Therefore we describe three possible vari-
ants: the rule-based generator (Subsection 3.1), the
seq2seq model (3.2) and the sequence labeling one
(3.3), based on the well-known GECToR model
(Omelianchuk et al., 2020). Note that GECToR
is available only for English and its development
for languages with complex morphology is prob-
lematic since it needs a word inflection module to
transform the predicted labels into surface forms.
3.1 Rule-based edit generator
We start with a rule-based edit generator, combined
with a left-to-right neural language model. It may
be viewed as our reimplementation of Alikaniotis
and Raheja (2019). Our edit generation module
takes as input a dependency tree of the sentence
and applies rule-based edits corresponding to themost frequent errors, such as missing or incorrect
determiners, commas and prepositions or wrong
choice of word form. The exact list of applied rules
is given in Appendix A.1.
These operations produce a fairly large num-
ber of possible corrections. To reduce computa-
tional burden we apply two-stage filtering based
on left-to-right probability model p, such as GPT
(Radford et al., 2019). First, for every hypothe-
sisuwe calculate the gain logp(u|w...w)−
logp(w|w...w), where πis the length of
longest common prefix of uand source sequence
w.We choose best Kdeletions, Kinser-
tions and Kreplacement edits according to this
score. For the selected hypotheses we calculate
their full log-probability and pick Kbest variants
provided their score exceeds logp(w)−θ, where
θis the predefined margin.
3.2 Sequence-to-sequence edit generator
To generate edits using a sequence-to-sequence ba-
sic model we run standard beam search, align all
the produced hypotheses with the source sentence
and extract non-trivial parts of such alignments.
The score of edit eequals logp(u|w)−logp(v|w),
where udenotes the most probable hypothesis
containing eandvis the most probable hypoth-
esis that changes nothing in the span of e. If
there is no such hypothesis, we set the score to
logp(u|w)−logp(v|w) + 1 , where vis the last
hypothesis in the beam.We experimented with
tracking only hypotheses with at most one edit,
however, it requires implementing an additional
control mechanism over beam states and does not
bring performance gains. The same holds for di-
verse beam search, which also has additional hy-
perparameters such as diversity penalty.
This approach can be applied to any sequence-
to-sequence model, in our paper we use BERT-
GEC(Kaneko et al., 2020) as basic edit generator.
3.3 Sequence labeling generator
In contrast to other methods, the recent GECToR
model (Omelianchuk et al., 2020) reduces grammar
error correction to sequence tagging. We give an
example of such reduction in Table 1 and refer the
reader to Sections 3and5of the original paper to11417 ? ? ? - ? + + + ? ?
better understand their approach. GECToR oper-
ations naturally correspond to elementary edits in
our terminology. For each position iwe extract all
the tags tsuch that
logp(t=t)≥logp(t= )−θ,
where pis the label probability GECTOR returns
andθis the predefined margin. For example, if on
the first step of the example in Table 1 we have
p(t=) = 0 .5, p(t=) = 0 .3, p(t= ) = 0 .1, then the transformation fall
→falls will also be extracted. Again, we keep
topKedits according to the difference between
logarithmic probabilities of the edit and the the
default “do nothing“ operation (the tag).
3.4 Common details
Existing Grammatical Error Correction datasets
may contain ‘ k-to-l‘ edits with either k > 1or
l > 1. Since datasets differ in how they treat
such multiword operations and GECToR model
produces only 0-to-1,1-to-1and1-to-0edits, we
split all such edits to elementary ‘ 0-to-1‘, ‘1-to-0‘
and ‘1-to-1‘ operations, treating as correct all ele-
ments of such splits. We also add the “do nothing”
edit that returns the source sentence. It is treated as
positive if the original sentence is already correct.
4 Model description
4.1 Edits classification
Given numerous successes of Transformer models
in NLP, we decide to use Roberta (Liu et al., 2019)
for edit classification. It takes as input the sequence
x=⟨⟩ ⟨⟩ _ ⟨⟩
and outputs the probability of the edited source to
be a plausible correction. Consider the sequence
x=x. . . xx. . . x and let
x. . . xandx. . . xbe the source and the tar-
get of the edit, respectively. Then our classificationmodel Mcan be decomposed as
M(x) =g(f( ( (x)))),
where
• is the Transformer encoder that
produces the embeddingssequence h=
hh. . . hhh. . . hh.
• is the readout function that con-
verts a sequence of embeddings to the vector-
ization of the whole input. We use the first em-
bedding of the target span ( xin our notation)
and consider other variants during ablation in
Appendix D.1.
•fis a multilayer perceptron and gis the final
classification layer with sigmoid activation.
4.2 Decoding
After classifying the edits we cannot simply apply
all ‘positive‘ operations as they may conflict each
other (e.g., the edits fall→fellandfall→falls for
the sentence The boy fall on the floor yesterday ).
The conflicts may also happen between adjacent ed-
its (boy→boys andfall→falls) thus we consider
as contradicting any two edits whose source spans
either intersect or are adjacent and non-empty.We
test two decoding strategies:
parallel pick the edits whose probability is greater
than the maximum of predefined threshold
and “do nothing” edit score. Keep those that
do not contradict any edits with higher scores.
stagewise if the most probable edit is “do nothing”
or its probability is below threshold, stop. Oth-
erwise pick the most probable edit, apply it to
the current input sentence and remove all the
edits with intersecting spans. Repeat this until
reaching the maximal number of iterations.11418Iter. Source Edits Result
1Boy fall the floor The boy fell the floor
2The boy fell the floor The boy fell on the floor
S In there moment , I thought that my best friends was my parents and sister .
A 0 1|||R:PREP |||At|||REQUIRED |||-NONE- |||0
A 1 2|||R:OTHER |||that|||REQUIRED |||-NONE- |||0
A 10 11 |||R:VERB:SVA |||were|||REQUIRED |||-NONE- |||0
S При новых законах , надо было держать женщин на работу .
A 0 1|||Предлог |||По|||REQUIRED |||-NONE- |||0
A 1 3|||Заменить |||новым законам |||REQUIRED |||-NONE- |||0
A 9 10 |||Сущ.:Падеж |||работе |||REQUIRED |||-NONE- |||0
The stagewise strategy is slower as it requires
rerunning the scorer on the modified sentence on
each iteration. However, it produces slightly better
scores, thus we use it for all experiments in the
paper. The optimal threshold is model-dependent
and is optimized on development set. We investi-
gate the effect of threshold selection and decoding
strategy in Appendix D.2.
4.3 Scoring
All edit generators described above also return
scores, corresponding to edit log-probabilities. By
default we do not use them, taking only the scorer
probabilities (the ‘ scorer-only ‘ variant). We also
try to combine generator and scorer probabilities
in a log-linear model (the ‘ combined ‘ method).
Precisely, we set the score of edit eequal to
logpscorer (e) +α·score gen(e), where αis the
tunable parameter.
The ‘scorer-only‘ variant is used by default for
most experiments in the paper, the ‘combined‘
method scores are reported only for the best se-
lected models to compare with SOTA scores.
5 Data and models
5.1 Data
We test our approach on English (a high-resource
language) and Russian with less resources and
worse edit generators available. For English we
use the BEA 2019 Shared Task data (Bryant et al.,2019). We use the same training data as in the
previous works: Write&Improve and LOCNESS
corpus (Bryant et al., 2019), First Certificate of En-
glish (FCE) (Yannakoudakis et al., 2011), National
University of Singapore Corpus of Learner English
(NUCLE) (Dahlmeier et al., 2013), Lang-8 Corpus
of Learner English (Tajiri et al., 2012) and synthetic
data (Awasthi et al., 2019). For experiments with
pretraining on synthetic data we utilize PIE dataset
(Awasthi et al., 2019). We test our models on BEA
2019 development and test sets and CoNLL 2014
(Ng et al., 2014) test data.
For additional experiments we also use cLang8
(Rothe et al., 2021) – the cleaned and extended
version of Lang8 corpus. The characteristics of
datasets are given in Table 2.
Dataset Size Usage
W&I+LOCNESS 34308 Train, finetune
FCE 28350 Train
NUCLE 57151 Train
Lang8 1037561 Train
PIE synthetic 9000000 Pretrain
BEA 2019 dev 4384 Development
BEA 2019 test 4477 Test
CoNLL14 1312 Test
cLang8 2372119 Train
For Russian we use the RULEC-GEC data (Ro-
zovskaya and Roth, 2019). Due to its small
size we generate our own synthetic dataset, cor-11419rupting the source sentences with rule-based
operations such as comma / preposition inser-
tion/deletion/replacement or changing the word to
another form of the same lexeme. The full list of
operations is given in Appendix A.2.
Dataset Sentences Errors
RULEC-GEC train 4980 4383
RULEC-GEC dev 2500 2182
RULEC-GEC test 5000 5301
Synthetic data 213965 187122
We follow the training procedure described in
(Omelianchuk et al., 2020). Namely, after pretrain-
ing on synthetic data only we perform the main
training on full BEA 2019 train set which is the
concatenation of W&I+LOCNESS, FCE, NUCLE
and Lang8 and afterwards finetune the model on
W&I+LOCNESS. When using cLang8 instead of
Lang8 we do not apply pretraining. For Russian we
pretrain on the concatenation of real and synthetic
data and finetune on RULEC-GEC train set.
5.2 Model architecture and training
For our scorer we use the Transformer model
and initialize it using the weights of pretrained
Roberta(Liu et al., 2019)model. We represent the
edit with the embedding of the leftmost word in the
target span. This vector is then passed through a
two-layer perceptron with intermediate ReLU and
sigmoid output activation. We implement our mod-
els using PyTorch and use HuggingFace to work
with pretrained transformer models.
The model is trained using total batch size of
3500 subtokens to fit into 32GB GPU memory. All
the examples for a single sentence are placed into
the same batch. Since the number of proposed
negative edits is much larger than the number of
positive ones, we independently average the binary
cross-entropy loss for positive and negative exam-
ples inside each batch. We optimize the model with
AdamW optimizer using default hyperparameters.
6 Experiments
In this section we describe our experiments. Note
that our main contribution is the scorer and we
claim that our method is not limited to a particular
edit generator. Thus we do not train edit generatorsDataset Rule-based BERT-GEC GECToR
BEA dev 45.8 55.5 54.9
W&I train 46.7 61.0 66.3
FCE 40.4 60.7 56.6
NUCLE 39.6 48.3 45.0
Lang8 33.0 50.2 43.3
BEA dev F 38.4 48 .6 54 .1
by ourselves and only adapt them to our pipeline
as described in Section 3.
Our main experiments are conducted for English,
in Subsection 6.3 we also present results for Rus-
sian. We compare the models by Fscore using
ERRANT (Bryant et al., 2019) for English BEA
data and M2Scorer (Dahlmeier et al., 2013) for
other datasets. Since the BEA 2019 test set does
not contain correct answers, we do most of the
comparisons on the development data.
6.1 Edit generators
We use three edit generators of different type: the
rule-based one with GPT2-medium edit scorer
(Subsection 3.1), the seq2seq BERT-GEC model
(Subsection 3.2) and the sequence labeler based on
our extension of GECToR(Subsection 3.3). For
all edit generators we set the number of hypotheses
(“beam width”) to 15and loss threshold θto3.0.
Before all we check that our edit generator has
sufficient recall. As shown in Table 4, BERT-GEC
and GECToR has similar recall on BEA data, while
on other datasets BERT-GEC coverage is better
despite lower quality of the corresponding model.
Recall of the rule-based model is low because it
cannot handle free text rewriting.
6.2 English
The first research question we ask is RQ1: does
our scorer achieve decent performance for all
edit generators . We answer it in the upper block
of Table 5. For all three scorers our model outper-
forms the BERT-GEC model(Kaneko et al., 2020)
on BEA2019 development set. It is still behind the
state-of-the-art GECToR model, however, the latter11420Edit gen. Scorer PT BEA 2019 dev CoNLL 2014
P R F P R F
Rule-based ‘scorer-only‘ NO 63.3 28 .1 50 .671.2 33 .3 58 .0
BERT-GEC ‘scorer-only‘ NO 62.1 33 .9 53 .270.2 38 .0 60 .0
GECToR ‘scorer-only‘ NO 60.4 34 .1 52 .573.6 34 .9 60 .2
BERT-GEC ‘scorer-only‘ YES 68.4 30 .4 55 .171.2 39 .4 61 .3
GECToR ‘scorer-only‘ YES 69.1 30 .9 55 .472.9 39 .1 62 .1
GECToR ‘combined‘ YES 68.4 34 .5 57 .279.1 38 .3 65 .2
BERT-GEC(Kaneko et al., 2020) YES 53.0 36 .5 48 .669.2 45 .1 62 .5
GECToR, roberta(Omelianchuk et al., 2020) YES 62.3 35 .6 54 .272.8 40 .9 63 .0
GECToR, XLNet(Omelianchuk et al., 2020) YES 66.0 33 .8 55 .577.5 40 .2 65 .3
uses more training data being pretrained on PIE
synthetic dataset.
Consequently, we ask the RQ2: does our model
outperform GECToR(Omelianchuk et al., 2020)
when being trained in the same conditions . The
second block of Table 5 shows that our ranker out-
performs GECToR variant based on the same Trans-
former with both model-based edit generators. If
it takes into account the scores of GECToR edit
generator by using ‘combined‘ decoding, we addi-
tionally improve on BEA dev by 1.8 Fpoints.
Notably, if BERT-GEC is used on the first stage
of our pipeline, the scorer still shows solid perfor-
mance being significantly better than its generator
model. Thus SOTA performance is possible even
for a weak generator model provided its recall is
high.
The third research question is RQ3: can
our model further improve performance using
larger language models and more training data .
In this setup we do two modifications: replace
Lang8 with larger and better cLang8 dataset (Rothe
et al., 2021) and utilize roberta-large model instead
of roberta-base. For all the models we use GECToR
edit generator. As shown in Table 6, roberta-large
produces further improvement over roberta-base
and outperforms current SOTA on BEA 2019 test
set. However, the improvement on CoNLL-2014 is
much smaller, we hypothesize that our models may
overfit to BEA domain.
6.3 Russian
English is relatively simple from the point of its
morphology. With 6main cases and 3genders, Rus-
sian is a more complicated case. Its rich nominalmorphology drastically extends the space of possi-
ble errors even for the rule-based generator. There
is no pretrained model for Russian GEC, thus we
compare two generators: the rule-based one (analo-
gous to English) and the finetuned ruGPT-large.
Their coverage statistics are given in Table 7. We
initialize the scorer with ruRoberta-largesince
there is no roberta-base for Russian. The results
are given in Table 8.
Contrasting with English experiments, even scor-
ing the rule-based edits outperforms the previous
SOTA models. We also note the latter are of larger
size and were trained with two magnitudes more
synthetic data. When using the model-based gen-
erator, the score is 1.5better; the combination of
ranker and generator scores yields further Fim-
provement. Thus we answer positively to RQ4:
is the suggested approach applicable in case of
little annotated data and languages other than
English .
7 Ablation studies
7.1 Joint generators
Our model is trained on edits from a particular
generator. A natural question is whether it overfits
to this generator or learns a model-independent
notion of grammaticality. We check this by training
a model with a single generator and applying it
to the union of different generators output (‘joint‘
generator in Table 9). We also investigate the effect
of finetuning and full training on joint edit sets.11421Model Scorer cLang8 BEA 2019 dev BEA 2019 test CoNLL 2014
P R FP R FP R F
roberta-base ‘combined‘ NO 68.4 34.5 57.282.4 54.5 74.779.1 38.3 65.2
roberta-base ‘scorer-only‘ YES 70.2 32.9 57.282.8 52.4 74.272.6 39.5 63.9
roberta-base ‘combined‘ YES 69.3 35.5 58.282.5 55.1 75.179.6 36.2 66.0
roberta-large ♢ ‘scorer-only‘ NO 70.2 33.1 57.383.8 52.0 74.777.3 36.3 63.0
roberta-large ♢ ‘combined‘ NO 69.6 35.6 58.583.5 54.4 75.579.3 39.5 66.0
roberta-large ♣♢ ‘scorer-only‘ YES 71.033.4 57.986.254.277.179.4 36.1 64.0
roberta-large ♣♢ ‘combined‘ YES 70.335.9 59.084.856.377.080.2 39.1 66.3
GECToR, ensemble NO NA NA NA 79.4 57.2 73.778.2 41.5 66.5
(Sun et al., 2021) ♣♢ NO NA NA NA NA NA NA 71.0 52.8 66.4
T5-XXL, cLang8 ♣♢ YES NA NA NA NA NA 75.9NA NA 68.9
Dataset Coverage
Rule-based ruGPT-based
RULEC-GEC train 54.4 81 .5
RULEC-GEC dev 55.5 59 .3
RULEC-GEC test 46.4 54 .3
Synthetic data 78.0 95 .8
Results are given in Table 9. We note that the
recall of joint generator on BEA development set
is69%, which significantly exceeds the coverage
of individual generators, which is about 55% (see
Table 4). Table 10 also illustrates the difference in
edits produced by different generators.
Joining generators output produce minor im-
provements for GECToR-based model and has neg-
ative impact on BERT-GEC-based one. It proves
that our models overfit to the edit generation al-
gorithm, the most severe overfitting happens in
case of BERT-GEC. As expected, full training on
joint set of edits performs better than only on edits
from GECToR generator. The same patterns hold
for large models and ‘combined‘ decoding, in par-
ticular, the roberta-large model trained with joint
edits achieves 76.2 Fon BEA test, reaching the
highest score among the models trained without
external data.
7.2 Decoding ablation
Our decoding algorithm has three hyperparame-
ters: the decoding algorithm (‘scorer-only‘ or ‘com-
bined‘), the threshold between positive and nega-
tive edits and the the maximal allowed number ofedits. Detailed results of their ablation are in Ap-
pendix D.2, summarizing:
1.‘Combined‘ decoding provides a stable im-
provement of 0.5−1%over ‘scorer-only‘.
2.Optimal threshold is usually 0.7before fine-
tuning and 0.9after finetuning.
3.Fscore monotonically improves up to 8
allowed edits due to increased recall, after 5
edits the scores almost saturate.
8 Related work
The task of grammatical error correction has a
long history. The main paradigm of recent years is
to treat it as low-resource machine translation (Fe-
lice et al., 2014; Junczys-Dowmunt et al., 2018) us-
ing extensive pretraining on synthetic data (Grund-
kiewicz et al., 2019). Synthetic data is usually
generated using random replacement, deletion, in-
sertion, spelling errors and perturbations (Grund-
kiewicz et al., 2019; Kiyono et al., 2019; Náplava
and Straka, 2019), other approaches include train-
ing on Wikipedia edits (Lichtarge et al., 2019) and
backtranslation (Kiyono et al., 2019). Another
trend is incorporating pretrained Transformer lan-
guage models either as a part of system architec-
ture (Kaneko et al., 2020) or for the initialization
of model weights (Omelianchuk et al., 2020). The
extreme case of the latter approach is the “brute
force” when one simply uses large encoder-decoder
Transformer that potentially is able to solve any
text-to-text task (Rothe et al., 2021).
Another paradigm in GEC is to reduce gram-
mar correction to sequence labeling (Omelianchuk11422Model Training data P R F
Transformer (Náplava and Straka, 2019) 10M synth., RULEC-GEC train, dev 63.3 27 .5 50 .2
mT5-XXL (Rothe et al., 2021) mC4 synth., RULEC-GEC train NA NA 51.6
ruGPT-large finetune (strong baseline) 200K synth., RULEC-GEC train 65.7 27 .4 51 .3
rule-based edits 200K synth., RULEC-GEC train 69.4 25 .9 51 .9
ruGPT-large edits, ‘scorer-only‘ 200K synth., RULEC-GEC train 70.9 26 .8 53 .4
ruGPT-large edits, ‘combined‘ 200K synth., RULEC-GEC train 73.7 27 .3 55 .0
Generator Metrics
Train Finetune Test P R F
GECToR GECToR GECToR 69.1 30 .9 55 .4
GECToR joint 67.6 33 .0 55 .9(+0.5)
joint joint 64.8 35 .5 55 .7(+0.3)
BERT-GEC BERT-GEC BERT-GEC 68.4 30 .4 55 .1
BERT-GEC joint 63.4 34 .2 54 .2(−0.9)
joint joint 64.2 34 .3 54 .6(−0.5)
joint joint joint 64.5 38 .2 56 .7(+1.3)
et al., 2020). However, it requires constructing a lin-
guistically meaningful set of tags that could be hard
to design for languages with complex morphology.
Our work mainly follows the third approach that
considers GEC as two-stage process including edit
generation as the first stage and their ranking or
classification as the second. Edits were usually gen-
erated by manually written rules and their scoring
was performed by linear classifiers (Rozovskaya
et al., 2014) or later by a pretrained language model
(Alikaniotis and Raheja, 2019). A recent work of
Yasunaga et al. (2021) generates edits using sepa-
rate sequence-to-sequence Transformer and then
filters them using a language model.
Our approach can be seen as a special case of
reranking . Feature-based reranking was common
in statistical machine translation before the advent
of neural networks (Och et al., 2004), in the field
of grammatical error correction it was applied by
Hoang et al. (2016), Xie et al. (2016) used a feature-
based binary classifier similar to ours to improve
precision of the GEC model. Grundkiewicz et al.
(2019) used a R2L language model scorer to rerank
the output of the first stage seq2seq model. How-
ever, recent studies on machine translation (Lee
et al., 2021) and summarization (Liu and Liu, 2021)
benefit from a training a Transformer rescoring
model, not choosing a fixed one. Our work is
partially inspired by theirs, the key difference isthat we use classification loss instead of ranking
and rerank individual edits, not complete sentences.
As far as we know, the only example of trainable
reranking for GEC is Liu et al. (2021), but it uses
a more complex architecture and focuses more on
error detection than correction.
9 Conclusion
We have developed a two-stage algorithm for gram-
matical error correction based on edit classifica-
tions. Our main results are the following:
•Our model reaches state-of-the-art perfor-
mance on English even without using the
scores of edit generator. With ‘roberta-base‘
backbone, it outperforms models of the same
size and achieves SOTA scores using ‘roberta-
large‘.
•It notably improves current state-of-the-art on
Russian, proving that our model is also ap-
plicable to small datasets with weaker edit
generators.
•Our approach works with different edit gener-
ators and their combinations.
Since our model shows competitive performance
even with rule-based edit generators, it may be
applied in settings that require control over possible
corrections. One such field is language learning,11423e.g., correcting error of particular type, such as verb
tense or determiner choice. In the future work we
plan to address this question in more details and
test the applicability of our approach on additional
languages, such as German or Czech. Last but not
the least, the main idea of ranking individual edits
can be applied not only to GEC, but to any task
where the concept of elementary edit has meaning,
for example, machine translation post-editing.
References11424
A Rule-based transformations used for
edit generation
A.1 English
Rule-based edit generator includes the following
operations:11425• Comma insertion and deletion.
•Preposition insertion, deletion and substitu-
tion. Insertion is allowed only before the first
token of a noun group.
•Determiner insertion, deletion and substitu-
tion. Insertion is allowed only before the first
token of a noun group.
•toinsertion before infinitives.
•Spelling correction for OOV words using Hun-
spell.
•Substitution a word with all its inflected forms,
inflection is performed using Lemminflect.
• Capitalization switching.
•Replacement of comma by period and capital-
izing the subsequent word ( I have a dog, it is
cute.→I have a dog. It is cute. ).
A.2 Russian
Rule-based edit generator for Russian includes the
following operations:
• Comma insertion and deletion.
•Preposition insertion, deletion and substitu-
tion. Insertion is allowed only before the first
token of a noun group.
• Conjunction substitution.
•Spelling correction for OOV words using Hun-
spell.
•Joining of consecutive words using Hunspell
(e.g. ne bol’shoj ‘no+big‘ ∝⇕⊣√∫⊔≀→nebol’shoj
‘small‘).
•Substitution a word with all its inflected forms,
inflection is performed using PyMorphy.
•Joint noun group inflection (e.g. bol’shoj
dom ‘large house‘ ∝⇕⊣√∫⊔≀→bol’shikh domov
‘large+GEN+PL houses+GEN’)
• Capitalization switching.
• Switching the order of consecutive words.
The rules take as input sentence dependency
trees, parsing is done using DeepPavlov.B Data sources
English
• W&I-LOCNESS train, dev and test.
• FCE.
• Lang8.
• CLang8.
• CoNLL14.
• PIE synthetic data.
Russian
• RULEC-GEC.
• Synthetic data: not available yet.
C Examples of elementary edits
See Table 10.
D Ablation studies
D.1 Additional losses
The choice of model architecture and training pa-
rameters may seem arbitrary. Therefore in this
section we study other possible variants of mod-
ern architecture. The architecture used in main
experiments has the following key components:
1.The model is trained with cross-entropy classi-
fication loss without any additional objectives.
2.The loss is normalized separately for positive
and negative instances.
3.Edit is represented using the first token em-
bedding in the output span.
4.The classification module contains a single
hidden layer.
5.Except for the classification module, no ad-
ditional layers are added on the top of main
Transformer encoder.
We test the following architecture modifications:
1.Adding an additional ranking objective. We
do it adding standard margin loss:11426
Here gis the logit of positive class before
sigmoid, Pis the set of contrastive pairs of
batch elements, θis a margin hyperparameter
andαis the additional loss weight. We
investigate 3variants of defining P:
•All pairs of positive and negative in-
stances ( +soft ),
•Only pairs of positive and negative in-
stances whose spans intersect( +hard ),
•All pairs of the form (e,e)and
(e,e), where e,eandeare pos-
itive, negative and “do nothing” edits,
respectively( +contrast ).
2. Removal of class normalization ( no_norm ).
3.Using the token ( cls), mean representa-
tion of output span ( mean ) and concatenation
of output and source span ( origin ) as edit en-
codings.
4.Adding one more hidden layer in the classifi-
cation block ( ‘2 layers‘ ).5.Adding an additional Transformer layer be-
tween all the edit representations for the same
sentence ( +attention ). That allows to poten-
tially use information from other hypotheses.
We run all ablation experiments on the concate-
nation of W&I+LOCNESS train and FCE datasets
using GECToR edit generator, results are given in
Table 11. For all the models we select the best per-
forming checkpoint and threshold according to the
Fscore and perform stagewise decoding. For
those models that improve over the basic one on
the small dataset, we run additional testing on full
BEA train data without finetuning.
We observe that additional losses that are helpful
in low-resource setting even decrease performance
for larger data. Thus the variant used in the paper
is the most effective despite being the simplest,
however, a more detailed study is required.
D.2 Decoding ablation
In the first experiment in Table 12 we vary the de-
coding algorithm and the decision threshold. We11427Model W&I+FCE BEA 2019 train+finetune
P R F P R F
Basic 55.5 26 .7 46 .1(+0.0)60.4 34 .1 52 .5(+0.0)
+hard 55.1 26 .4 45 .8(−0.3) NA NA NA
+soft 55.2 30 .8 47 .6(+1.5)58.2 35 .3 51 .6(−0.9)
+contrast 55.1 31 .1 47 .7(+1.6)60.9 30 .1 50 .5(−2.0)
no_norm 55.8 27 .4 46 .2(+0.1) NA NA NA 57.7 22 .0 43 .5(−2.6) NA NA NA
+mean 58.0 27 .0 47 .2(+1.1)61.6 31 .6 51 .8(−0.7)
+origin 57.4 26 .2 46 .4(+0.3) NA NA NA
2layers 55.6 27 .7 46 .3(+0.2) NA NA NA
+attention 52.8 31 .4 46 .4(+0.3) NA NA NA
provide the scores for the model trained with GEC-
ToR edit generation on full training data before and
after finetuning on W&I-LOCNESS training data.
Another notable pattern is that before finetuning the
bestF-score is achieved at threshold 0.6−0.7,
while afterwards the optimal threshold is 0.8−0.9.
These values are stable across datasets, so setting
the threshold to 0.7before finetuning and to 0.9
after it is nearly optimal, thus threshold tuning is
almost unnecessary.
In Table 13 we also analyze how the quality of
the model depends on the maximal number of edits
allowed. We observe that recall and Fscore are
improved up to 8edits per example. The differ-
ence between stagewise and parallel algorithms is
about 0.5−0.7 Fscore. It follows the experi-
ence of (Omelianchuk et al., 2020), where iterative
rewriting (the analogue of our stagewise decoding)
improved performance even more significantly.
E Limitations and risks of the work
Our method relies on either the existence of a
grammatical error correction model that can serve
as model-based generator or a pretrained LM to
be used with rule-based generator. With the ex-
istence of multilingual language models these re-
quirements are fulfilled for most of high- or middle-
resource languages. A more serious limitation is
the existence of labeled corpus of grammatical er-
rors and its quality.
Concerning practical applications of our work,
we mentioned that it can be used for automatic cor-
rection of learner sentences, for example, in the
field of Second Language Learning. However, we
acknowledge that real-word learner errors differfrom the ones in the academic datasets. It implies
that before applying our model or its extension in
any practical setting an additional study is required
to check whether its precision is enough for prac-
tical usage. In particular, its corrections should be
verified by a human in case of usage for automatic
essay scoring and related tasks.
The model was trained on examples from aca-
demic datasets that may be biased towards students
having particular mother tongue. Therefore an addi-
tional investigation is required, whether the model
has equal quality for the sentences from English
learners with different native languages and profi-
ciency levels.11428Threshold Before finetuning After finetuning
P R F P R F
0.5 59.2 30 .7 49 .957.1 39 .8 52 .6
0.6 60.5 29 .8 50 .258.6 38 .9 53 .2
0.7 63.1 27 .7 50 .260.7 37 .9 54 .2
0.8 68.8 22 .7 48 .963.1 35 .9 54 .8
0.9 79.9 10 .7 34 .869.2 30 .9 55 .4
1 2 3 4 5 6 7 8
Parallel Precision 72.9 70 .6 69 .6 69 .5 69 .4 69 .4 69 .4 69 .4
Recall 18.8 25 .7 28 .0 29 .0 29 .3 29 .5 29 .5 29 .5
Fscore 46.2 52 .4 53 .7 54 .3 54 .5 54 .6 54 .6 54 .6
Stagewise Precision 72.9 71 .0 70 .1 69 .4 69 .2 69 .1 69 .0 69 .0
Recall 18.8 25 .9 30 .4 28 .8 29 .9 30 .5 30 .9 31 .0
Fscore 46.2 52 .6 54 .5 54 .9 55 .2 55 .3 55 .4 55 .4
(Fgain) (+0.00) (+0 .2) (+0 .8) (+0 .6) (+0 .7) (+0 .7) (+0 .8) (+0 .8)11429