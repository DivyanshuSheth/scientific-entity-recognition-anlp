
Kaiqiang Song,Chen Li,Xiaoyang Wang,Dong Yu,Fei LiuTencent AI Lab, Seattle, WAUniversity of Central Florida, Orlando, FL
Abstract
Podcasts have shown a recent rise in popularity.
Summarization of podcasts is of practical bene-
fit to both content providers and consumers. It
helps people quickly decide whether they will
listen to a podcast and/or reduces the cognitive
load of content providers to write summaries.
Nevertheless, podcast summarization faces sig-
nificant challenges including factual inconsis-
tencies of summaries with respect to the inputs.
The problem is exacerbated by speech disfluen-
cies and recognition errors in transcripts of spo-
ken language. In this paper, we explore a novel
abstractive summarization method to alleviate
these issues. Our approach learns to produce
an abstractive summary while grounding sum-
mary segments in specific regions of the tran-
script to allow for full inspection of summary
details. We conduct a series of analyses of the
proposed approach on a large podcast dataset
and show that the approach can achieve promis-
ing results. Grounded summaries bring clear
benefits in locating the summary and transcript
segments that contain inconsistent information,
and hence improve summarization quality in
terms of automatic and human evaluation.
1 Introduction
Podcasts are one of the most popular forms of new
media. As of today, over 155 million people listen
to a podcast every week (Christian, 2021). With
the growing interest, there is an increased demand
for textual summaries that foretell the content of
podcasts. Those summaries help people decide, in
a few seconds, if they will listen to a podcast or
subscribe to the channel. They are helpful for users
who want to find podcasts previously listened to.
Furthermore, they can be re-purposed for social me-
dia posts or email marketing campaigns, enabling
content creators to make their podcasts accessible
to a larger audience.
It is desirable to generate grounded summaries
from podcast transcripts, where spans of summarytext are closely tethered to the original audio. Fig-
ure 1 provides an example of a grounded abstrac-
tive summary. When a user clicks on a summary
segment, she will be directed to an audio clip that
gives further detail of the conversational context.
Grounded summaries give us a preview of notable
podcast clips (Shalom, 2019) and they may further
release summarization service providers from po-
tential legal claims by directing users to the original
audio. This is because, speech recognizers induce
transcription errors and abstractive summarization
models may hallucinate facts that are not entailed
by the original (Kryscinski et al., 2020), both can
cause podcast summaries to contain misleading or
inaccurate information. With grounded summaries,
users are able to frame, interpret, and place into
context any system-generated summaries, thus re-
ducing the barriers to deploy podcast summariza-
tion technology.
One may attempt to align summary text and pod-
cast transcripts in a post-processing step to gener-
ate grounded summaries. Unfortunately, hallucina-
tions do not allow for proper alignments as they are
not found in the transcripts (Maynez et al., 2020).
Hierarchical attention models may seem promising
for this task (Liu and Lapata, 2019). However, the
excessive length of the transcripts makes it difficult
to produce attention distributions over the entire
transcripts. Recent evidence suggests that attention
weights are not reliable indicators of the relative
importance of inputs (Jain and Wallace, 2019), thus
it remains an open question whether attention can
be used to find alignments between transcripts and
summary segments.
In this paper, we seek to generate grounded sum-
maries from podcast transcripts by exploring an
on-demand abstractive summarizer. It mimics how
a human might approach a lengthy transcript – the
expert would identify a portion of the transcript that
is deemed most important and relevant to the exist-
ing summary, use it as a ground to produce a new4407
piece of the summary, and that process is repeated
until the summary is finished. Our summarizer em-
ploys a novel regularization technique that enables
it to visit portions of the transcript in chronological
order, while allowing zigzags in order to produce
a coherent summary. This has another implication.
It implies that we may estimate what percentage of
a podcast transcript is covered by the summary and
thus adjust that when necessary.
Distinguishing our work from earlier research
on extract-then-abstract methods (Hsu et al., 2018;
Chen and Bansal, 2018; Gehrmann et al., 2018;
Lebanoff et al., 2019; Jin et al., 2020; Pilault et al.,
2020), we require selected transcript chunks to have
high salience, but also those salient content must
appear at the beginning of the selected chunks, so
that the corresponding audio clips can provide good
jump-in points for users to start listening. Our ex-
periments are performed on a large podcast summa-
rization dataset containing over 100,000 English
podcasts (Clifton et al., 2020). We show that our
proposed grounded summarizer can perform com-
petitively or better than the state-of-the-art methods,
including the recent methods that leverage large,
pretrained models (Lewis et al., 2020; Beltagy et al.,
2020) as judged by automatic metrics and human
evaluation. Our contributions in this paper are as
follows.
•We address the problem of podcast summariza-
tion by investigating an on-demand summarizer
that produces grounded abstracts. The abstracts
help users quickly decide if they will listen to
the podcasts and offer a sampler of salient pod-
cast clips. The on-demand summarizer does not
need to encode the entire transcript, hence sub-
stantially reduces the GPU memory footprint.•We conduct a series of analyses to gain insights
into the impact of specific design decisions. They
include how a transcript chunk should be defined,
whether those transcript chunks overlap, to what
extent the summary content is taken verbatim
from selected chunks, and how the summary may
be extended to cover more information.
•Through extensive experiments on a benchmark
podcast dataset, we demonstrate the effectiveness
of our proposed approach and show results that
are comparable to human writer performance.
The approach opens an avenue towards generat-
ing a new kind of abstractive summaries that al-
low users to verify the information consistency of
summary parts against the original audio clips.
2 Related Work
With the rapid rise of podcasts comes the need for
automatic summarization of podcast transcriptions.
While comparatively understudied, recent work has
shown great progress. Clifton et al. (2020) present
the Spotify dataset that was adopted in TREC 2020
for the podcast summarization task.Our partici-
pating system in TREC 2020 focuses on identifying
salient segments from transcripts and using them
as input to an abstractive summarizer (Song et al.,
2020). Reddy et al. (2021) develop classifiers to de-
tect and eliminate extraneous marketing materials
in podcasts to aid summarization. In this paper, we
explore techniques that generate grounded podcast
summaries where pieces of summary text are tied
to short podcast clips.4408One of the most serious problems of neural ab-
stractive summarization is that the summaries can
contain factually incorrect information and halluci-
nations (Falke et al., 2019; Kryscinski et al., 2020;
Maynez et al., 2020; Lebanoff et al., 2020). With-
out grounded summarization, users have to listen
to the full episodes to find connections between
details of the summaries and the original podcasts.
If successful, grounded summaries will benefit a
number of summarization tasks where the input in-
volves lengthy transcripts, including meetings (Li
et al., 2019; Koay et al., 2020, 2021; Zhong et al.,
2021), medical conversations (Liu and Chen, 2019),
interviews (Zhu et al., 2021), livestreams (Cho
et al., 2021) and more.
An extract-then-abstract strategy could be used
to produce grounded abstractive summaries (Chen
and Bansal, 2018; Gehrmann et al., 2018; Hsu et al.,
2018; Jin et al., 2020; Pilault et al., 2020). Most of
these approaches are tailored to written documents,
e.g., news, Wikipedia, and scholarly articles. They
extract sentences from the documents and use them
as input to an abstractive summarization model to
produce a summary. Nevertheless, transcripts of
spoken language lack essential document structure
such as sentence, paragraph and section boundaries,
making it unclear how these approaches will per-
form on podcasts.
Attention provides another mechanism for align-
ing the summary and transcript segments. The use
of sparse attention allows a summarization model
to potentially scale to longer documents (Beltagy
et al., 2020; Kitaev et al., 2020; Huang et al., 2021).
Hierarchical Transformer encodes multiple para-
graphs in a hierarchical manner to allow them to
exchange information (Liu and Lapata, 2019; Fab-
bri et al., 2019; Chen and Yang, 2020). However,
it is shown that attention weights are not reliable
indicators of the relative importance of inputs, as al-
ternative attention distributions would have yielded
similar results (Jain and Wallace, 2019).
Our approach in this paper is to better align sum-
mary segments with chunks of the transcripts to al-
low easy tracing of inconsistent information. It fea-
tures a generator that writes a summary from begin-
ning to end, and a savvy selector that knows when
to switch to a new transcript chunk and where to
switch to. Differing from PG networks (See et al.,
2017) and retrieval-augmented generation (Guu
et al., 2020; Lewis et al., 2021), our selector places
heavy emphasis on modeling and selection of tran-script chunks. A desirable chunk is expected to be
about 2 minutes long and places important infor-
mation at the beginning to enable easy user verifi-
cation. In the following section, we present details
of the model implementation.
3 Our Approach
A major challenge facing podcast summarization is
the dramatic length difference between source and
target sequences. At a speaking rate of 122 words
per minute for spontaneous speech (Polifroni et al.,
1991), the full transcript of a 1-hour long episode
contains roughly 7,000 words and that of a 1.5-hour
long episode could reach 10,000 words. In contrast,
a podcast summary is short, containing on average
61 words according to Manakul and Gales (2020).
The ratio of their lengths could reach as high as
100-to-1, and this motivates our study of abstractive
grounded summarization where summary segments
are grounded to selected chunks of transcripts as a
way of combating the inevitable errors that occur
in podcast summarization.
Letxbe the sequence of tokens in the source
transcript and ybe the sequence of tokens in the
summary. These tokens share the same vocabulary
V. We use xto denote a chunk of the transcript,
andCgives the indices of tokens that belong to the
chunk. The full transcript can be decomposed into
a sequence of chunks, denoted by {C,···,C}.
The chunks may have varying sizes and overlap
with each other; they are the grounds for generat-
ing a podcast summary. Our assumption is twofold.
Firstly, we assume a summary segment is produced
by conditioning on the previously generated tokens
(y) and a specific chunk of the transcript. Sec-
ondly, there exists a function G(x,y)(Eq. (1))
that determines the most appropriate grounding
chunk for generating all tokens of the segment. Par-
ticularly, when the entire transcript is treated as a
single chunk, it reduces to the standard conditional
generation model p(y|y,x).
p(y|x) =Yp(y|y,G(x,y)) (1)
Thus, the crucial point is a coarse segmentation
of the source transcript and an alignment between
the transcript chunks and summary segments. In
this work we use a sliding window to produce tran-
script chunks, with window size Wand stride size4409S.The sizes can be measured in terms of to-
kens. E.g., W=256 and S=128 tokens will produce
a series of fixed-length chunks that overlap with
each other. The rationale for using overlapping
chunks is to find those that serve both as grounds
for summary generation and good jump-in points
for user verification. The sizes can also be mea-
sured by the number of sentences. E.g., W=20 and
S=20 sentences produce a set of varying-length,
non-overlapping chunks. In spoken language, a se-
ries of consecutive short sentences often indicates
the content is relatively unimportant (Marge et al.,
2010).
Given a summary segment ey, we designate x
as agrounding chunk if it attains the highest score
S(x,ey)(Eq. (2)). This position-biased coverage
score favors the transcript chunk that covers sum-
mary bigrams and puts summary content at the
beginning to aid humans in performing content ver-
ification. It measures the percentage of unique sum-
mary bigrams B(ey)covered by a chunk x. Partic-
ularly, I[b∈x]is an indicator that returns 1 if
the bigram bappears in xand 0 otherwise. Each
bigram bhas an associated weight w(Eq. (3)).
If it appears in the first position of x(= 0),
it receives a weight of one. Otherwise, the weight
is decayed according to the relative position of the
bigram’s first occurrence in the chunk () and γ
is a coefficient for the decay.
S(x,ey) =1
|B(ey)|Xw I[b∈x](2)
w= 1−γ
|C|;γ∈[0,1] (3)
We proceed by training a neural encoder-decoder
model to generate an abstractive summary from the
grounding transcript chunks. Each segment of the
summary (= sentence)is generated conditioned on
its grounding chunk xand all the previously gen-
erated tokens y. The process starts from the first
chunk of the transcript x. The encoder converts
this grounding chunk into a sequence of hidden vec-
tors[h, . . . ,h](Eq. (4)). The decoder predicts
the next summary token y(Eq. (5)) and continues
to do so until a “ switch point ” is detected. At this
point the current summary segment is finished and
the decoder is poised to select the next transcript
chunk xand generate a new summary segment
from it. The decoding process finishes when a spe-
cial symbol () is predicted that indicates the
end of the summary.
[h, . . . ,h] = (x) (4)
y= (y,[h, . . . ,h]) (5)
G(x,y) =

x, j = 1
x, j > 1
G(y), j > 1
There is a notable difference between our ap-
proach and most extract-then-abstract approaches
that select important sentences from the document
and provide them to the abstractor all-at-once. As
illustrated in Figure 2, strong position bias causes
the abstractor to use only content at the beginning
of the input to generate a summary. By exposing
the chunks progressively, our approach naturally
makes use of this characteristic to consolidate in-
formation from multiple source chunks. It reduces
the amount of computation necessary to train the
encoder-decoder model, as only selected transcript
chunks are encoded which is equal to the number
of summary segments. Moreover, it is possible to
encourage the summary to have a good coverage
of the source content by specifying a minimal set
of grounding chunks to be used for generation.4410Regularizing Chunk Selection. Learning func-
tionG(x,y)that predicts a transcript chunk x
to switch to is crucial for success at inference time.
Let there be Mtranscript chunks and Nsummary
segments in a training instance. We define pto
be the model probability that the c-th chunk is pre-
dicted as the ground for generating the j-th sum-
mary segment; cis the gold chunk obtained using
Eq. (2-3). Our learning objective is a cross-entropy
loss against the gold labels with a novel regulariz-
ing term Rto enable chunks to be selected as per
their original order in the transcript, while allowing
zigzags to produce a coherent summary (Eq. (6-7)).
L(ϕ) =−Plogp+αR (6)
R=PPmax(0 , s−s)(7)
Particularly, s=Ppdenotes the sum
of the probability assigned to all chunks up to the
c-th position, in order to generate the j-th summary
segment. We encouragePmax(0 , s−s)
to be a small value so that if a chunk (up to the c-th
position) is assigned to the j-th summary segment,
it is unlikely to be assigned to the ( j+1)-th segment.
Ris designed to regularize the loss and penalize
violations; αis its coefficient which will be tuned
on the validation set.
Given a partial summary y, selecting the next
transcript chunk depends on two factors. Firstly, it
should be a chunk that contains salient content at its
beginning. We use I(x)to denote the importance
of the chunk. It is obtained by encoding the chunk
into a vector husing RoBERTa (Liu et al., 2019),
then apply a feedforward network to it to estimate
the importance (Eq. (9)).
p∝exp(I(x) +R(x,y))(8)
I(x) =(h) (9)
R(x,y) =([h||h]) (10)
+ (hW h)
Secondly, the chunk may be relevant to the par-
tial summary y. We define the relevance score
R(x,y)to capture two levels of interaction be-
tween the candidate chunk, represented by handthe last hidden state of the partial summary, repre-
sented by h. Their linear interaction is captured
by a feedforward network () and bilinear in-
teraction is modelled by hW hwhere a low-
rank approximation is used: (pWq) =
(pU)(Vq). The score pis the likelihood that
thec-th chunk is assigned to the j-th summary seg-
ment considering saliency and content relevancy.
Switch Point. A skilled writer pauses after writ-
ing down a sentence. We borrow that intuition to
inform the construction of a switch-point predic-
tor. The model combines the last hidden state of
the summary sequence hand the embedding of
the anticipated token E(y), and use a feedforward
networkto predict if the j-th decoding step
corresponds to a “ switch point ” (Eq. (11)). During
training, the last token of each summary sentence is
a ground-truth switch point. At inference time, the
model predicts a switch point if p( )exceeds
a threshold, at which point we compute pto decide
the next chunk. Note that the model may choose
use the same transcript chunk after switching.
p( ) =σ(([h||E(y)])) (11)
4 Podcast Data
With over 100,000 podcast episodes, the Spotify
dataset (Clifton et al., 2020) is one of the largest cor-
pora available for podcast search and summariza-
tion. It encompasses a wide range of topics: travel,
business, sports, book reviews, mysteries, guided
meditations, nutrition and weight loss, among oth-
ers. Each episode is accompanied by an audio
file, an automatic transcript generated by Google’s
Speech-to-Text API,and metadata provided by the
podcast creator. We do not use the audio data in this
paper. Our summarizer takes as input a transcript
and uses the creator-provided episode description
as the reference summary.
Data Filtering. Episode descriptions provided
by podcast creators show wide variations in qual-
ity. When noisy descriptions are used as reference
summaries, they can cause a summarizer to hallu-
cinate content. We conduct aggressive filtering of
the training data to remove low-quality creator de-
scriptions so as to maintain a balance between the
amount of training examples available and quality
of those examples. We clean up reference sum-
maries on the token-, sentence- and summary-level.4411
Tokens that correspond to URLs, email addresses,
@mentions, #hashtags, and those excessively long
tokens (>25 characters) are directly removed from
the summaries. Each sentence in the summary
is given a salience score that is the sum of IDF
scores of its words. A low score (<10) indicates
the sentence contains few informative words and
it is thus removed from the summary. Finally, if,
after sentence removal, the reference summary is
too short or cannot be properly aligned to tran-
script chunks (§3), the instance is removed from
the dataset.This process filters out a substantial
amount of low-quality reference summaries, yield-
ing 40,302 episodes in the training set. The Spotify
dataset has a standard test set of 1,027 episodes and
179 of them are set for human evaluation.
Baselines. Our baselines consist of three of the
best performing systems in the TREC 2020 com-
petition on podcast summarization. These systems
were judged the best performing by both automatic
metrics and human evaluation performed by NIST
assessors. All systems make use of the BART-large
model (Lewis et al., 2020). The model is tuned first
on a news summarization dataset, i.e., CNN/DM or
XSum, then fine-tuned on the podcast dataset. Due
to the long length of the transcripts, Karlbom and
Clifton (2020) describe a combined Longformer-
BART model that replaces the BART attention lay-ers with attentions of Longformer (Beltagy et al.,
2020); their system is named . Song
et al. (2020) develop an extractive module to select
segments from transcripts, then integrate the extrac-
tor with BART abstractor to generate summaries
( ). Their baseline ( ) directly
truncates the transcript to the first 1,024 tokens.
Manakul and Gales (2020) develop a similar base-
line ( ) using the first 1,024 tokens.
Further, they perform sentence filtering using a hi-
erarchical attention model ( )
and ensembles of models from different data shuf-
fles and checkpoints ( ). In this
paper, our system is called for generating
grounded abstracts. It has 4 options: ,
indicating the sliding window is defined in terms
of tokens () or sentences (), overlapping () or
non-overlapping (). We obtain outputs from these
competitive baselines and our system to examine
both the successes and failures of these attempts.
5 Results and Analysis
Experimental Settings. Our encoder-decoder
model uses BART-large as the base model before
fine-tuning it on the podcast dataset. We use the
AdamW (Loshchilov and Hutter, 2017) optimizer,
where the momentum parameters are set to 0.9 and
0.999. The regularizing coefficient αis tuned on
the validation set in the range of {0,0.01,0.1,1}.
For summary decoding, we use beam search with
a beam size K=4 and a length penalty p=2. Our4412
sliding window, measured in terms of tokens or sen-
tences, only contain whole sentences. We use the
Byte-Pair Encoding (BPE) tokenizer with a vocab-
ulary size V=50,265. For transcripts and reference
summaries, we use the SpaCy tool to segment them
into sentences (model ).
Example Summaries. In Table 1, we provide
a direct comparison of system summaries. This
podcast is hosted by Natalie and Jessica who call
themselves “ Skincare Sommeliers. ” The episode is
named “ The Great Exfoliation Debate .” We find
that grounded abstractive summaries ( )
have a higher level of specificity compared to sum-
maries without grounding. Segments of grounded
summaries are tied to specific transcripts chunks.
If a listener finds a summary segment interesting,
they can tap to hear the selected summary segment
in context. Our baselines are highly competitive.
Their summaries tend to contain more generic con-
tent. The description provided by podcast creators
is relatively short and at times it does not directly
summarize the episode. There are clear benefits in
automatic summarization of podcasts, which can
reduce the cognitive load and the time it takes for
podcast creators to write the summary.
Automatic Metrics. In Table 2, we report results
on the standard test set containing 1,027 podcast
episodes. The metrics include ROUGE (Lin, 2004)variants that compare system summaries with cre-
ator descriptions based on n-gram overlap. Further,
we experiment with recently developed metrics:
BertScore (Zhang et al., 2020) and BLEURT (Sel-
lam et al., 2020) that draw on deep neural repre-
sentations to evaluate generated text. Our approach
does not outperform the baselines in ROUGE eval-
uation against creator descriptions. However, the
gap has been substantially reduced when more ad-
vanced metrics (BertScore and BLEURT) are con-
sidered. There are two possible explanations. First,
grounded summaries are about 50% longer than
plain abstractive summaries. Their average length
is about 80 words per summary, yielding low preci-
sion scores. Second, the quality of creator descrip-
tions can be poor. Jones et al. (2020) report only
40% of such descriptions are of Good or Excellent
quality, indicating future work may consider creat-
ing high-quality ground-truth summaries. Among
the four variants of our approach, we observe that
their difference is not prominent. The token-based,
non-overlapping windows (-tn) variant outperforms
others in terms of R-1 and R-L. This system is used
in subsequent experiments and analyses.
Human Evaluation. It is imperative to perform
human evaluation given that creator-provided de-
scriptions are of poor quality and ground-truth sum-
maries are nonexistent. We follow the TREC guide-
lines to ask human evaluators to assign each sum-
mary to one of the four grades: Excellent ,Good ,
Fair andPoor . The excellent summary will accu-
rately conveys the most important content of the
episode (topical content, genre, and participants).
It should contain almost no redundant material, be
coherent, comprehensible, and has no grammati-
cal errors (Jones et al., 2020). We also asked the
human evaluators to answer 8 yes/no questions re-4413
garding the quality of the summary as (Jones et al.,
2020) suggested, those questions are shown in Ta-
ble 5. We conduct these experiments on the test
set containing 179 podcast episodes as (Jones et al.,
2020) did, where each summary is evaluated by
five Master workers recruited on the mechanical
turk. As shown in Table 3, we find that humans pre-
fer the lengthier grounded abstractive summaries,
which substantially outperform all baselines. 25%
of grounded abstractive summaries are rated as Ex-
cellent and 76% of them receive a rating of either
Excellent orGood . Table 4 shows the results of
the 8 questions. Comparing to previous best sys-
tems, our grounded abstractive summaries have a
significant performance gain on retrieving impor-
tant information including People Names(+6.03%),
People Additional Information(+12.92%), Main
Topics(+1.75%), Podcast Format(+4.78%) and Ti-
tle related context(+2.47%) with slight redundancy.
Chunk Selection and Switch Point Prediction.
We are curious to know how well our system per-
forms on predicting grounding chunks: G(x,y).
In this study, we assume switch points are knownand report results on the validation set. Our decoder
starts from the first transcript chunk and predicts
the next chunk at each switch point. We find that
it achieves an accuracy of 86.02% on identifying
ground-truth chunks. Next, we examine the perfor-
mance of switch point prediction. On the validation
set, we observe that the predictor achieves 98.75%,
84.95% and 91.33%, respectively, for precision,
recall and F-score. Moreover, each summary has
an average of 3.67 switch points. A majority of the
time (92.42%) the model decides to use the current
chunk to continue to decode the next summary seg-
ment. At a small percentage (7.58%) the model
decides to find to a new grounding chunk. We find
1.24 unique grounding chunks per summary. The
statistics suggest that identifying grounding chunks
is crucial for summary generation.
Grounded Summaries. In Table 7, we measure
the percentage of summary n-grams that appear
in the transcripts (for all baselines) or grounding
chunks (for our approach). While the distributions
of unigrams are largely similar, we observe that
grounded abstractive summaries tend to reuse more
bigrams and trigrams of their grounding chunks.
Moreover, for trigrams that are found in the ground-
ing chunks, we find 70% of them tend to appear at
the beginning – the front half of the chunks. These
results suggest that the grounding chunks identified
by our approach can provide effective support for
summary generation.
What Made the Task Challenging?
We manually analyze a large amount of transcripts
and their creator descriptions to identify the chal-
lenging points of podcast summarization in Table 8:
•Substantial lexical mismatch exists between the
spoken and written form of descriptions. Speech
recognition errors are abundant. E.g., “ by Hans
Christian Andersen ” has been misrecognized into4414
“buy homes Christian Andersen .”
•The creator descriptions are sometimes highly
abstractive, do not always summarize the episode
and contain teasers. E.g., “ A male perspective
podcast to start a conversation... ” and “ Ever
wondered how Ed Sheeran became famous .”
•The transcripts contain advertising inserts, e.g.,
“I need to tell you about our sponsor... ” and the
same description is used for different episodes
that causes confusion to the model, e.g., “ The
goal of Daily Fortnite is to build a community... ”
6 Conclusion
In this paper, we investigate podcast summarization
to produce textual summaries for podcast episodes
that help listeners to understand why they might
want to play those podcasts. We present a new kind
of podcast summary where spans of summary text
are tethered to the original audio to allow users
to interpret system-generated abstracts in context.Experiments on a benchmark dataset demonstrates
the utility of our proposed approach.
Acknowledgments
The authors would like to thank all anonymous re-
viewers for their insightful comments which helped
improve this paper. This research was supported
in part by the National Science Foundation (NSF)
Grant #2143792.
References441544164417
A Appendix
What made the task of podcast summarization
challenging?4418