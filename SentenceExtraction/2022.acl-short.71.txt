
Isabel Papadimitriou
Stanford University
isabelvp@stanford.eduRichard Futrell
University of California, Irvine
rfutrell@uci.edu
Kyle Mahowald
The University of Texas at Austin
mahowald@utexas.edu
Abstract
Because meaning can often be inferred from
lexical semantics alone, word order is often a
redundant cue in natural language. For exam-
ple, the words chopped ,chef, and onion are
more likely used to convey “The chef chopped
the onion,” not “The onion chopped the chef.”
Recent work has shown large language mod-
els to be surprisingly word order invariant, but
crucially has largely considered natural pro-
totypical inputs, where compositional mean-
ing mostly matches lexical expectations. To
overcome this confound, we probe grammat-
ical role representation in English BERT and
GPT-2, on instances where lexical expectations
are not sufﬁcient, and word order knowledge is
necessary for correct classiﬁcation. Such non-
prototypical instances are naturally occurring
English sentences with inanimate subjects or
animate objects, or sentences where we system-
atically swap the arguments to make sentences
like “The onion chopped the chef”. We ﬁnd
that, while early layer embeddings are largely
lexical, word order is in fact crucial in deﬁn-
ing the later-layer representations of words in
semantically non-prototypical positions. Our
experiments isolate the effect of word order
on the contextualization process, and highlight
how models use context in the uncommon, but
critical, instances where it matters.
1 Introduction and Prior Work
Large language models create contextual embed-
dings of the words in their input, starting with a
static embedding of each token and progressively
adding more contextual information in each layer
(Devlin et al. ,2019 ;Brown et al. ,2020 ;Man-
ning et al. ,2020 ). While these contextual em-
bedding models are often praised for capturing
rich grammatical structure, a spate of recent work
has shown that they are surprisingly invariant to
scrambling word order ( Sinha et al. ,2021 ;Hes-
sel and Schoﬁeld ,2021 ;Pham et al. ,2021 ;Gupta
et al. ,2021 ;O’Connor and Andreas ,2021 ) and
Figure 1: Probabilities of probes trained to differentiate
subjects from objects in BERT embeddings. We sepa-
rate our evaluation examples by prototypicality: whether
the ground truth grammatical role is what we would ex-
pect given the word out of context. The majority of
natural examples are prototypical (solid lines), and so
if we average all cases we cannot see that grammati-
cal information is gradually acquired in the ﬁrst half
of the network for cases where lexical information is
non-prototypical. The equivalent ﬁgures for GPT-2 are
in Appendix A.
that grammatical knowledge like part of speech,
often attributed to contextual embeddings, is actu-
ally also captured by ﬁxed embeddings ( Pimentel
et al. ,2020 ). These results point to a puzzle: how
can syntactic contextual information be important
for language understanding when the words them-
selves, not their order, are what matter?
We argue that this apparent paradox arises be-
cause of the redundant structure of language it-
self. Lexical distributional information alone inher-
ently captures a great deal of meaning ( Erk,2012 ;
Mitchell and Lapata ,2010 ;Tal and Arnon ,2022 ),
and typically both humans and machines can re-
construct meanings of sentences under local scram-
bling of words ( Mollica et al. ,2020 ;Clouatre et al. ,
2021 ). In this paper, we study model behaviour in
cases where word order is informative and is not
redundant with lexical information.
We focus on the feature of grammatical role636(whether a noun is the subject or the object of a
clause). Most natural clauses are prototypical :
in a sentence like “the chef chopped the onion”,
the grammatical roles of chef andonion are clear
to humans from the words alone, without word
order or context (see Mahowald et al. ,2022 , for
experiments in English and Russian in which hu-
man participants successfully guessed which of
two nouns was the subject and which was the ob-
ject of a simple transitive clause, in the absence
of word order and contextual information). This
means syntactic word order is often redundant with
lexical semantics. Whether hand-constructed or
corpus-based, most studies probing contextual rep-
resentations have used prototypical sentences as
input, where syntactic word order may not have
much information to contribute to core meaning
beyond the words themselves.
Yet human language can use syntax to deviate
from the expectations generated by lexical items:
we can also understand the absurd meaning of a
rare non-prototypical sentence like “The onion
chopped the chef” ( Garrett ,1976 ;Gibson et al. ,
2013 ). Is this use of syntactic word order available
to pretrained models? In this paper, we train gram-
matical role probes on the embedding spaces of
BERT and GPT-2, and evaluate them on these rare
non-prototypical examples, where the meaning of
words in context is different from what we would
expect from looking at the words alone. We focus
on English because grammatical role is directly de-
pendent on word order in English, and because we
had access to sufﬁciently large English parsed cor-
pora such that we could generate non-prototypical
sentences, easily check them, and ﬁlter to grammat-
ical ones.
We probe for grammatical role because it is key
to the basic compositional semantic structure of a
sentence ( Dixon ,1979 ;Comrie ,1989 ;Croft ,2001 ).
While ﬁxed lexical semantics contains information
about grammatical role (animate nouns are likely to
be subjects, etc), the grammatical role of a word in
English is ultimately determined by syntactic word
order. Probing grammatical role lets us examine
the interplay between syntactic word order and lex-
ical semantics in forming compositional meaning
through model layers.
For all of our experiments, we train grammatical
role probes with standard data and test them oneither prototypical cases or non-prototypical cases
(where word order matters), to understand if gram-
matical embedding under normal circumstances is
sensitive to word order. Our experiments reveal
three key ﬁndings:
1.Lexical semantics plays a key role in orga-
nizing embedding space in early layer rep-
resentations, and non-lexical compositional
features are expressed gradually in later lay-
ers, as shown by probe performance on non-
prototypical sentences (Experiment 1, Figure
1).
2.Embeddings represent meaning that is im-
parted only by syntactic word order, overrid-
ing lexical and distributional cues. When we
control for distributional co-occurrence fac-
tors by evaluating our probes on argument
swapped sentences (like “The onion chopped
the chef”, real sample in Appendix B), probes
can differentiate the same word in different
roles (Experiment 2, Figure 2).
3.Syntactic word order is signiﬁcant beyond just
local coherence: the compositional informa-
tion of syntactic word order is lost when we
test our probes on locally-shufﬂed sentences,
that keep local lexical coherence but break
acute syntactic relations (Figure 3).
More generally, we highlight the importance of
examining models using non-prototypical exam-
ples, both for understanding the strength of lexical
inﬂuence in contextual embeddings, but also for
accurately isolating syntactic processing where it
is taking place.
2 Why non-prototypical probing?
As opposed to more general syntactic probing tasks
(e.g., dependency parsing), grammatical role is a
linguistically signiﬁcant yet speciﬁc task that is
both syntactic and semantic. As such, we can
choose these linguistically-informed sets of non-
prototypical examples where the lexical semantics
does not match the compositional meaning implied
by the syntax.
Non-prototypical examples give us a unique per-
spective on how syntactic machinery like word or-
der inﬂuences compositional meaning representa-
tionindependently from lexical semantics. Stud-637ies in probing have controlled for lexical seman-
tics by substituting content words for nonce words
(“jabberwocky” sentences, as in Hall Maudslay
and Cotterell ,2021 ;Goodwin et al. ,2020 ) or ran-
dom real words (“colorless green idea” sentences,
as in Gulordava et al. ,2018 ). A tradeoff is that
these methods lead to out-of-distribution sentences
whose words are unlikely to ever co-occur naturally.
Rather than bleaching the effect of lexical seman-
tics, our setup lets us examine the interplay between
lexical semantics and syntactic representation in
a controlled environment, isolating the effects of
syntactic word order while using in-distribution
examples.
Recent work on representation probing has fo-
cused on improving probing methodologies to
make sure that extracted information is not spurious
or not simply lexical ( Hewitt and Liang ,2019 ;Be-
linkov ,2022 ;V oita and Titov ,2020 ;Hewitt et al. ,
2021 ;Pimentel et al. ,2020 ). Our experiments are
a complementary approach, where we use standard
probing methods, but use linguistically-informed
data selection to address the ambiguity of what
classiﬁers are extracting.
3 Experiment 1: Grammatical
Subjecthood Probes
In Experiment 1, we evaluate grammatical role
probes on prototypical instances, where grammat-
ical role lines up with lexical expectations, and
non-prototypical instances, where it does not.
3.1 Methods
We train a 2-level perceptron classiﬁer probe with
64 hidden units to distinguish the layer embed-
dings of nouns that are transitive subjects from
nouns that are transitive objects , as in Papadim-
itriou et al. (2021 ). We train a separate classiﬁer
for each model layer, as well as training a classiﬁer
on the static word embedding space of the mod-
els without the position embeddings added (before
layer 0). The probe classiﬁers are binary, taking the
layer embedding of a noun and predicting whether
it is a transitive subject or a transitive object. Probe
training data comes from Universal Dependencies
treebanks: we pass single sentences from the tree-
banks through the models, and use dependency an-
notations to label each layer embedding for whether
it represents a transitive subject, a transitive object,
or neither (not included in training). The training
set is balanced, and consists of 864 embeddingsof subject nouns, and 864 embeddings of object
nouns. We train all probes for 20 epochs, for con-
sistency. The embedding models that we use are
bert-base-uncased andgpt2 . For our analysis,
we call a noun a prototypical subject if the probe
probability for its word embedding (pre-layer 0) is
greater than 0.5, and a prototypical object if it is
less.
3.2 Results
Prototypical and non-prototypical arguments differ
in probing behavior across layers, as demonstrated
in Figure 1. For prototypical instances (solid lines),
syntactic information is conﬂated with type-level
information and so probe accuracy is high starting
from layer 0 (word embeddings + position embed-
dings), and stays consistent throughout the network.
However, when we look at non-prototypical in-
stances (dashed lines), we see that the embeddings
from layer to layer have very different grammatical
encodings, with type-level semantics dominating in
the early layers and more general syntactic knowl-
edge only becoming extractable by our probes in
later layers.
Crucially, since prototypical examples dominate
in frequency in any corpus, the average probe accu-
racy across all examples is high for all layers, and
the grammatical encoding of subjecthood, which is
accurate only after the middle layers of the model,
would be hidden. Separating out non-prototypical
examples illustrates how the syntax of a phrase can
arise independently from type-level information
through transformer layers, while also showcas-
ing the importance of lexical semantics in forming
embedding space geometry in the ﬁrst half of the
network.
4 Experiment 2: Controlling for
Distributional Information by
Swapping Subjects and Objects
In Experiment 1 we show that the contextualiza-
tion process consists of gradual grammatical infor-
mation gain for non-prototypical examples, even
though this is largely obscured in the majority pro-
totypical examples where the lexical semantics also
contains accurate syntactic information. In this ex-
periment, we ask: does this contextualized informa-
tion about grammatical role stem from word order
and syntax, or from distributional (bag-of-words)
effects when seeing all words in the sentence? We
answer this question by creating example pairs638
where we control for distributional information by
keeping all the words the same, but swapping the
positions of the subject and the object. Such pairs
of the type “The chef chopped the onion” →“The
onion chopped the chef” (real sample in Appendix
B) have identical distributional information. To
accurately classify grammatical role in both sen-
tences, the model we’re probing would have to be
attuned to the ways in which small changes in word
order globally affect meaning.
4.1 Methods
We use the same probing classiﬁers from Experi-
ment 1, and evaluate on a special test set of pairs
of sentences that have the subject and direct ob-
ject of one clause swapped. To create the swapped
sentences, we search the UD treebank for verbs
that have lexical, non-pronoun direct subjects and
direct objects, check that the subject and object
have the same number (singular or plural), and also
check that neither of them are part of a compound
word or a ﬂat dependency word that would be sep-
arated (like a full name). If a sentence contains a
verb where its arguments fulﬁll all of these require-
ments, we swap the position of the subject and the
object to create a second, swapped sentence, and
add the sentence pair (original and swapped) to our
evaluation set. A random sample of our swapped
sentences is in Appendix B.4.2 Results
When testing our probes on pairs of normal and
swapped sentences, we ﬁnd that our probes from
Experiment 1 correctly classify both the normal
and the swapped sentences with high accuracy in
higher layers. Since we test our probes on con-
trolled pairs that have the same distributional in-
formation, we can isolate effect of syntactic word
order in inﬂuencing meaning representation. This
is demonstrated in Figure 2, where probe predic-
tions for the same set of words in the same distribu-
tional context diverges signiﬁcantly depending on
whether the word is in subject or object position.
Our results indicate that, separate from distribu-
tional effects, models have learned to represent the
ways in which syntactic word order can indepen-
dently affect meaning.
4.3 Are these results just due to general
position information?
Our results in Experiment 2 indicate that syntac-
tic word order information can affect model repre-
sentations of word meaning, even when we keep
lexical and distributional information constant. A
question still remains: does the divergence demon-
strated in Figure 2stem from the ﬁne-grained ways
in which word order inﬂuences syntax in English,
or from heuristics based on primacy (whether a
word is earlier or later in a sentence)? To further
investigate this, we train and test probes on sen-
tences where word order is locally scrambled so
that no word moves more than 2 slots, and so gen-
eral primacy and local coherence is preserved. As
shown in Figure 3, probes trained on these locally
shufﬂed sentences do not fare better than chance639
on non-prototypical examples. While prototypi-
cal lexical information can aid classiﬁcation (solid
line), general primacy information is not sufﬁcient
to overcome lexical cues and cause the word-order-
dependent representation we demonstrate in Figure
2.
5 Discussion
While recent work has shown that large language
models come to rely largely on distributional se-
mantic information, we consider the model’s abil-
ity to overcome these distributional cues. Research
showing that models rely on lexical and distribu-
tional information is not at odds with our ﬁndings
that this can be overridden. In fact, even though hu-
mans can accurately understand non-prototypical
sentences, human syntactic processing is often in-
ﬂuenced by the lexical semantics of words, as evi-
denced by studies on human subjects ( Frazier and
Rayner ,1982 ;Rayner et al. ,1983 ;Ferreira and
Henderson ,1990 ) as well as by lexically-inﬂuenced
syntactic processes in human languages, like dif-
ferential object marking ( Aissen ,2003 )—a phe-
nomenon whereby non-prototypical grammatical
objects are marked.
More generally, while we have shown that it is
tempting for a straightforward probing approach
to conclude that grammatical role information is
available to the lowest layers of BERT, separately
analyzing prototypical and non-prototypical argu-
ments makes it clear that the picture is more compli-cated. At lower layers, BERT representations can
typically classify subjects and objects, but when
a non-prototypical meaning is expressed, accurate
classiﬁcation is not available until the higher layers.
We argue that considering probing performance
on these non-prototypical instances is crucial. A
key design feature of human language is the abil-
ity to talk about things that aren’t there or don’t
exist ( Hockett ,1960 ), and it has been argued that
the combinatoric power of syntax exists to allow
humans to say things that are subtle, surprising, or
impossible ( Garrett ,1976 ;Chomsky ,1957 ). Thus,
considering probing accuracy on the average task
may be misleading. Insofar as being able to un-
derstand non-prototypical meanings is a hallmark
of human language and insofar as these meanings
may differ in systematic ways from prototypical
meanings, considering such cases is crucial for
understanding how language models represent lan-
guage.
6 Acknowledgments
This work was supported by National Science Foun-
dation Grants No. 2104995 to KM, No. 1947307 to
RF, and a Graduate Research Fellowship to IP. We
thank Dan Jurafsky and Adina Williams for helpful
discussions, and Kaitlyn Zhou, Dallas Card, and J.
Adolfo Hermosillo for comments on drafts.
References640641
A Figures for GPT-2 Experiments
We ran our experiments on both BERT and GPT-2
embeddings, and both models had similar behav-
iors that we discuss in the paper. For clarity, ﬁgures
in the paper only visualize the BERT results, and
we’re including the GPT-2 versions of those same
ﬁgures for comparison. Figure 4shows the GPT-2
results of Figure 1, Figure 5shows the GPT-2 re-
sults of Figure 2, and Figure 6shows the GPT-2
result of Figure 3.
.
.
.642BSample of argument-swapped sentences
A random sample (not cherry-picked) of our
argument-swapped evaluation set, where the sub-
ject and the object of clauses are automatically
swapped. The original subject is in bold and the
original object is in bold and italics . The process
for creating these sentences is detailed in Section
4.1
On Thursday, with 110 days until the start of the
2014 Winter Paralympics in Sochi, Russia, Profes-
sorinterviewed Assistant Wikinews in Educational
Leadership, Sport Studies and Educational / Coun-
seling Psychology at Washington State University
Simon Li ˇcen about attitudes in United States to-
wards the Paralympics.
This approach shows a more realistic video to
playing Quidditch.
Second, aggregate view provides only a high-
level information of a ﬁeld, which can make it
difﬁcult to investigate causality [23].
Ahand raises her girl.
area of the Mississippi River and the destruction
of wetlands at its mouth have left the Alteration
around New Orleans abnormally vulnerable to the
forces of nature.
It was known that a moving energy exchanges
its kinetic body for potential energy when it gains
height.
Thus, when ACPeds issued a statement con-
demning gender reassignment surgery in 2016 [21],
many beliefs mistook the organization ’s political
people for the consensus view among United States
pediatricians — although the peak body for pedi-
atric workers, the American Academy of Pediatrics,
has a much more positive view of gender dysphoria
[22].
Hispainting perfectly combines artand Chinese
calligraphy.
When the inches become a few plants tall and
their leaves mature, it ’s time to transplant them to
a larger container.
Since the television series’ inception, reviews at
The A V Club have written two critical writers for
each episode:643