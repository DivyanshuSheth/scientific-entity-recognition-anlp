
Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, Aram GalstyanUniversity of Southern California - Information Sciences InstituteMeta AI
ninarehm@usc.edu ,beirami@google.com ,
{fred,galstyan}@isi.edu
Abstract
Warning : this paper contains content that may
be offensive or upsetting.
Recent research in Natural Language Process-
ing (NLP) has advanced the development of
various toxicity detection models with the in-
tention of identifying and mitigating toxic lan-
guage from existing systems. Despite the abun-
dance of research in this area, less attention
has been given to adversarial attacks that force
the system to generate toxic language and the
defense against them. Existing work to gen-
erate such attacks is either based on human-
generated attacks which is costly and not scal-
able or, in case of automatic attacks, the at-
tack vector does not conform to human-like lan-
guage, which can be detected using a language
model loss. In this work, we propose attacks
against conversational agents that are impercep-
tible, i.e., they fit the conversation in terms of
coherency, relevancy, and fluency, while they
are effective and scalable, i.e., they can auto-
matically trigger the system into generating
toxic language. We then propose a defense
mechanism against such attacks which not only
mitigates the attack but also attempts to main-
tain the conversational flow. Through auto-
matic and human evaluations, we show that our
defense is effective at avoiding toxic language
generation even against imperceptible toxicity
triggers while the generated language fits the
conversation in terms of coherency and rele-
vancy. Lastly, we establish the generalizability
of such a defense mechanism on language gen-
eration models beyond conversational agents.
1 Introduction
Adversarial attacks on different Machine Learn-
ing (ML) and Natural Language Processing (NLP)
applications can reveal important vulnerability is-
sues related to these systems. Most existing re-
search focuses on adversarial attacks that degrade
performance of existing ML systems with regardsFigure 1: An example illustrating the attack performed
by the adversary on the third turn of the conversation
(red line) that leads the defender into generating a toxic
utterance (dotted box). With a proper defense the de-
fender can bypass the attack and generate a non-toxic
response (green line).
to accuracy (Chakraborty et al., 2018; Zhang et al.,
2020b). More recent work has considered attacks
that target ethical concerns, such as triggering the
models into outputting unfair predictions (Mehrabi
et al., 2021b; Solans et al., 2021), or in the context
of NLP, generating biased (Sheng et al., 2020) and
toxic (Wallace et al., 2019) text.
In this paper, we consider adversarial attacks
on human-centric chatbots and dialogue systems.
It is important for these systems to be safe and
robust in the face of natural(-looking) human con-
versations. Further, the defender should ensure
a satisfying user experience via relevant and co-
herent generation. An instance of the attack and
defense is demonstrated in Figure 1 in which the
adversary tries to trigger the defender while the
defender avoids the attack by not generating toxic
utterances.
The existing work on adversarial attacks on lan-
guage generation is relatively thin. Wallace et al.
(2019) offer attacks based on universal adversarial
triggers (UAT) that can result in toxic text gen-
eration with a relatively high success rate. How-2831ever, those triggers are unnatural, incoherent se-
quences of words that can be easily detected via
a language model loss. Furthermore, such attacks
cannot be successful in voice-based dialogue sys-
tems where the input to the dialogue model comes
from speech recognition and should necessarily
conform to human language norms. Xu et al.
(2020) use human-and-model-in-the-loop frame-
work to generate natural-looking attacks to break
chatbots, but this approach is costly and inherently
not scalable.
In this paper, we propose imperceptible adver-
sarial attacks on dialogue systems that leverage
natural-looking and coherent utterances as triggers,
which cannot be easily detected using anomaly de-
tection techniques. As such, these attacks can also
target voice-based assistants who see the world
through the lens of speech recognition systems.
Our proposed approach works by augmenting the
UAT from Wallace et al. (2019) with additional
selection criteria to generate imperceptible yet ef-
fective triggers. The method is fully automated and
scalable, thus affording the exploration of a large
number of attack vectors and system vulnerabilities
efficiently. Through human and automatic evalu-
ations we show the effectiveness of the proposed
attack in provoking the defender into generating
toxic responses while keeping the fluency and co-
herency of the conversation intact.
We then focus on a defense mechanism for the
non-adversarial (defender) model to avoid generat-
ing toxic utterances. While simple defense methods
such as (Xu et al., 2020) achieve near-perfect effec-
tiveness against adversarial triggers, those methods
work by essentially resetting the conversation topic
which breaks the flow. Instead, we are interested
in a defense mechanism that “detoxifies" the re-
sponse while preserving the natural conversation
flow. Our proposed method relies on two levels of
interpretable reasoning that helps the model to (1)
identify the key adversarial tokens responsible for
the attack and (2) avoid generating toxic responses
by masking those tokens during the generation pro-
cess. We perform automatic and human evaluations
to assess the effectiveness of our defense mecha-
nism and demonstrate that it compares favorably
with various state of the art baselines, both in terms
of detecting the attacks and generating conversa-
tionally fluent responses. We finally demonstrate
the generalizability of such a defense mechanism
on generation tasks beyond conversational models.We emphasize that while our problem formula-
tion focuses on the adversarial scenario, the imper-
ceptible and coherent-looking triggers used in our
proposed attacks can also be invoked inadvertently
by regular (non-adversarial) users. Thus, the de-
fense mechanism proposed against such triggers
will improve the overall robustness of conversa-
tional agents, not only against adversaries but also
in interactions with regular users.
2 Attack Approaches
In this section, we first discuss the universal ad-
versarial trigger attack proposed by Wallace et al.
(2019), which we use as our baseline. We then
propose alterations to this baseline to make the uni-
versal triggers more natural-looking and suitable
for conversational domain. Finally, we discuss our
performed experiments and results.
2.1 Methodology
Universal Adversarial Trigger (UAT) (Wallace
et al., 2019) The goal in universal adversarial trig-
ger attack is to find a universal trigger sequence for
a given trained model, which if attached to the start
of any given input can cause the model to output
the desired outcome (Wallace et al., 2019). This
attack starts with a fixed-length sequence as the ini-
tial trigger, e.g., “the the the the the the” and tries
to iteratively replace the tokens in the sequence
to satisfy an objective. The iterations terminate
when no improvement (replacement) can be made
to further optimize the objective. The objective in
this generative process is to search for triggers that
can maximize the likelihood of toxic tokens being
generated as follows:
f=/summationdisplay/summationdisplaylogP(y|y).
where Yis the set of toxic outputs, tdenotes the
trigger sequence, and θis a trained language model.
One important drawback of this kind of attack is
that since there is no constraint on the trigger, it
does not necessarily satisfy any language modeling
loss; thus, the obtained trigger sequence usually is
a nonsensical phrase that can be easily detectable
as a (high-perplexity) anomaly.
Universal Adversarial Trigger with Language
Model Loss (UAT-LM) An intuitive solution to
address the above shortcoming of UAT is to impose
a language modeling objective on the trigger tokens.
Thus, the objective for UAT-LM attack is2832f =f+/summationdisplay/summationdisplaylogP(t|t, θ).
Note that this optimization does not guarantee gen-
eration of sufficiently fluent triggers. Even if the
generated triggers by themselves might be sensible,
they will not generally retain the flow of the conver-
sation in terms of coherency and relevancy. Thus,
we propose a different modification to the attack
strategy to accommodate these requirements.
Unigram Trigger with Selection Criteria
(UTSC) To consider the history of the conversation
hand retain the fluency, coherency, and relevancy
aspects of the conversation in generating the attack,
we propose an alternative approach in which we
generate a collection of unigram triggers (with se-
quence length one) from UAT. We then feed these
triggers along with the history of the conversation h
to our dialogue model and generate different attack
utterances. Next, we pick the best suited attack ut-
terance amongst all the generated attack utterances
according to our selection criterion as demonstrated
in Figure 2. Since we are relying on the dialogue
model to generate the attack utterance given h, the
generated utterance should be relevant to the con-
versation. Furthermore, since we are using only
a unigram trigger from UAT, the fluency of the
utterance is not going to be sacrificed noticeably.
We quantify the toxicity of each candidate attack
utterance using either a single toxicity classifier or
an ensemble of such classifiers; see Section 2.2 and
Appendix A for more information. We use the av-
erage (for multiple classifiers) or raw (for a single
classifier) output probability scores obtained by the
toxicity classifiers, which we refer to as the toxicity
score xfor example i, and select the final attack ut-
terance amongst the ncandidate adversarial exam-
ples considering three selection criteria. Previous
work (Xu et al., 2020) has shown that toxic triggers
are more likely to provoke toxic responses. Thus, in
UTSC-1, we select the most toxic utterance among
all generated attack utterances according to toxicity
scores from toxicity classifiers as our final attack
utterance (i.e., arg max{x}). We experiment
with two additional criteria. For UTSC-2, we first
apply a threshold Tto toxicity scores of the candi-
date utterances and label the utterances above this
threshold as toxic. Next, from the pool of all toxic
utterances, we select the utterance with the low-
est toxicity score (i.e., arg min{x|x≥T}).
If no utterances fall above the threshold, then the
most toxic utterance is selected. Lastly, in UTSC-
3 we select the utterance with the lowest toxicity
score, i.e., arg min{x}. Details are provided
in Appendix A.
2.2 Experimental Setup
General Setup We use DialoGPT (Zhang et al.,
2020c) to generate 100 conversations around a spe-
cific topic. The topic is determined by the context
sentence that starts the conversation between the
adversary and the defender. Each conversation runs
for 10 turns. To measure the effectiveness of the
attack and defense mechanisms given the conversa-
tion history as well preservation of relevancy and
coherency, the adversary generates the attack utter-
ance on the third turn of each conversation.
Toxicity Detection Models To determine toxicity
of the candidate attack utterances by the adversary,
we utilize an ensemble of three different toxicity de-
tection models: Toxic-bert, Perspective API, and
Safety classifier (Xu et al., 2020). In short, Toxic-
bert is the least sensitive of the three, followed by
Perspective API, and the Safety classifier (details
in Appendix A). While using an ensemble of the
three models results in the most effective attacks, to
ensure that the adversary is not simply overfitting
the toxicity detection model but rather forcing the
defender to actually generate toxic language, we
also study the transferability of these attacks. We
allow the adversary to only use one of the toxicity
detection models to design its attack. We then quan-
tify toxicity using the other two toxicity detection
methods, not accessed by the adversary.
Data The context sentences around which bots
start their conversations come from two different2833datasets, Wizard of Wikipedia (Dinan et al., 2018)
and ConvoKit’s Reddit Corpus.We intend to con-
sider both controversial and neutral topics; thus, we
consider two different datasets in which the Wizard
of Wikipedia dataset contains context around neu-
tral topics and the Reddit corpus contains context
around more sensitive topics. We picked 50 random
context sentences from the Wizard of Wikipedia
and 50 from the Reddit datasets.
AMT Experiments To compare and verify the
quality of conversations generated during and af-
ter the attacks, we conduct human experiments in
which we ask workers on Amazon’s Mechanical
Turk (AMT) to rate the conversations in which
UAT, UAT-LM, and UTSC-1 attacks had happened.
We pick UTSC-1 attack as it is the strongest at-
tack amongst our proposed UTSC attacks. We
then asked AMT workers to rate the generated at-
tack’s fluency, relevancy of the attack to previous
utterances, and overall conversation coherency on
a likert scale of 1 to 3 representing poor, moder-
ate, and good qualities respectively. We also asked
AMT workers to rate if the utterance after the at-
tack is toxic or not to verify the effectiveness of the
attack according to human judgment. During this
study, AMT workers annotated 100 conversations
from each of the three attacks and each conversa-
tion was annotated by 3 AMT workers giving us
overall 900 annotated conversations 300 from each
attack. More details about this study along with the
survey can be found in Appendix A.
2.3 Results
We first discuss the results from our automatic eval-
uations demonstrating the efficacy of each attack.
We then discuss how well the attacks transfer to
other toxicity detection classifiers. Finally, we
present results from our human evaluation study.
Unless otherwise mentioned, for the UTSC attacks,
the adversary uses an equally weighted ensemble
of all three toxicity detection classifiers to chose
the final attack utterance.
Attack Effectiveness Here we report the “attack
effectiveness” by calculating the percentage of con-
versations in which the defender was provoked by
the adversary to generate a toxic response. We
first demonstrate the results comparing the UAT
baseline with UAT-LM andUTSC attacks. Results
in Figure 3 demonstrate that two of our proposed
attacks UAT-LM and UTSC-1 are performing thebest according to the Perspective API and Toxic-
bert classifiers. UAT baseline performs the best
according to Safety classifier. Overall results show
that UTSC-1 and UAT-LM attacks are competitive
attacks in terms of attack effectiveness. In addition,
UTSC-1 and UAT-LM attacks have the advantage
of being more fluent which makes the attack more
imperceptible. UAT attack tends to generate mean-
ingless phrases, e.g., “acist neighborhoodsJohnson
carry morals Ukrain” which can easily be detected
as an anomaly and make the conversation not flow
naturally. In our experiments, we observe that the
average perplexity score according to the GPT-2
language model for the attack phrases generated by
UAT is absurdly high ( ∼10) compared to ∼10
for UAT-LM, and ∼160for UTSC-1. The per-
plexity of the no attack case (unaltered DialoGPT
conversations) is ∼39. This automatically confirms
that our attacks are more fluent and natural, and
thus more imperceptible. This observation is fur-
ther confirmed by our human evaluations which we
discuss later.
Imposing the language model constraint on UAT
not only makes UAT-LM attack more fluent, but it
also causes UAT-LM to generate more toxic trig-
gers which results in more attack effectiveness. Our
results confirm the previous results (Xu et al., 2020)
in which authors show in a human adversary case
that more toxic attacks perform better in forcing the
model to generate toxic utterances. In our results,
we also show that UTSC-3 performs the worst
which is based on non-toxic utterances followed by
the UTSC-2 attack which is based on the least toxic
utterance attack constraint. However, UTSC-1 is
the strongest as it relies on most toxic utterances
followed by UAT-LM. Thus, results confirm that
the toxicity of the attack plays a significant role in
attack effectiveness.
In addition, we found that the adversary is able to
force the defender into generating toxic utterances2834regardless of the context sentence and whether or
not the conversation is around a sensitive topic (e.g.,
the Reddit corpus) or a more neutral one (e.g., the
Wizard of Wikipedia). Details are in Appendix B.1.
Note that even the smallest percentage of attack
effectiveness (e.g., 10%-20%) poses a major risk
for real-world conversational systems when those
systems are deployed at scale.
Attack Transferability Here, we discuss the trans-
ferability of our UTSC-1 attack toward different
toxicity detection classifiers. In Figure 4, we
demonstrate that even if the attacker only uses one
of the toxicity detection models (Toxic-bert), it still
can force the defender to generate toxic responses
according to Perspective API and Safety classifier
and have comparable performance to when it uses
all the toxicity classifiers. This confirms that the
attack is forcing the defender to generate actual
toxic language rather than fooling the toxicity clas-
sifier. The results for UTSC-1 using other toxicity
detection models can be found in Appendix B.1.
Human Evaluation Results from our human eval-
uation studies are in Figure 5. Our UTSC-1 at-
tack is rated to have the highest coherency. UTSC-
1 is rated to have more fluent attacks generated
with mostly moderate to good scores and a higher
average–shown by the black dotted lines–compared
to the UAT and UAT-LM baselines. UTSC-1 also
has better relevancy scores in terms of the attack
being more relevant to the conversation. However,
since UAT generates meaningless phrases, it is
rated very poorly for all the mentioned qualities.
With regards to toxicity scores, attacks are rated
to have competitive and comparable performances
at around 20% effectiveness close to automatic re-
sults from Perspective API classifier. Fleiss Kappa
(Fleiss, 1971) annotator agreement results from this
evaluation is reported in Table 1. Annotators have
reasonable overall agreement for all the qualities.
3 Defense Approaches
The defense against adversarial attacks has two
components (a) detecting the attack and (b) mit-
igating its effect by ensuring that the defender
does not generate a toxic response. The detection
problem is rather straightforward, as the defense
can simply run a toxicity classifier on the gener-
ated response. The mitigation is more challenging.
Xu et al. (2020) suggested a mitigating approach
which, when a toxic response is detected, simply
resets the dialogue and generates a (non-toxic) ut-
terance by randomly sampling from a predefined
set of topics (see Section 3.2.1). As we mentioned
before, we are interested in mitigation strategies
that avoid generating toxic utterances but at the
same time manage to keep the conversation flow in-
tact. We now discuss our approach in more details.
3.1 Methodology
Our defense is based on a two-stage mechanism in
which the defender first runs a toxicity detection
model on its generated utterance. If it finds that
the generated utterance is toxic, it then proceeds
with the second stage of the defense. The proposed
defense mechanism in the second stage utilizes
two layers of reasoning using two different inter-
pretability techniques. The first layer aims to detect
which tokens in the defender’s utterance is making2835
the toxicity detection model to label the utterance
as being toxic. We call these tokens the L1tokens.
The second layer aims to detect which tokens in
the adversary’s attack utterance are responsible for
generation of L1tokens form defender’s utterance.
We call these tokens identified in layer 2 as the
L2tokens. The defender then masks the L2tokens
from the adversary, which were responsible for trig-
gering the defender model to generate toxic tokens,
and generates a new utterance. We then apply a tox-
icity classifier on this new utterance. If it is deemed
safe, it is then going to replace the defender’s old
toxic utterance, otherwise we iteratively apply the
two-stage defense mechanism to mask more input
tokens until the generated output is deemed safe.
As we shall see, a single iteration of our defense is
sufficient in most of the experiments.
The defense framework is demonstrated in Fig-
ure 6. For the first layer, we use transformers inter-
pretwhich provides explanations and identifies the
L1 token according to Toxic-bert model. For the
second layer, we use LERG (Tuan et al., 2021) that
provides local explanations for dialogue response
generation and identifies the L2 token (given the
L1 token in the response utterance it identifies the
L2 token in the query utterance).
3.2 Experimental Setup
We use the aforementioned attacks, and apply our
defense against them. This follows the same experi-
mental setup, with the addition of baseline defenses
to compare our defense effectiveness against.3.2.1 Baselines
Two-stage Non Sequitur Baseline (Xu et al.,
2020) This baseline is also a two-stage approach
like ours in which the defender first uses a toxicity
classifier to detect if the utterance is toxic or not. It
then changes the topic of the conversation if the ut-
terance was detected to be toxic, e.g., “Hey do you
want to talk about something else? How about we
talk about X?” where X is a randomly chosen topic
from 1087 topics judged as safe from the Wizard
of Wikipedia conversational topic list (Dinan et al.,
2018). Xu et al. (2020) used this defense against ad-
versarial attacks performed by human adversaries
that force the model to generate toxic responses.
Notice that although this defense is using a tem-
plated sentence to change the topic into a non-toxic
topic and can be considered as the perfect solution
to avoid generating toxic responses, it can provide
the user with a non-plausible conversational ex-
perience given that the topic of the conversation
changes each time the defender detects a toxic ut-
terance. To this end, we expect this baseline to
do almost perfectly in terms of avoiding toxic re-
sponse generation given that the toxicity detection
classifier is a good detector; however, in terms of
conversational quality it will have worse relevancy
and coherency scores compared to our method as
shown in our human evaluations.
Trigger Masking (TM) Baseline In this baseline,
we consider masking the adversarial trigger tokens.
Note that the defender does not generally know
which tokens were the trigger-tokens used by the
adversary, so this approach is not applicable in real-
istic settings. However, we believe that considering2836
this type of oracle baseline can still give us interest-
ing insights, so we include it in our experiments.
3.2.2 AMT Experiments
We asked AMT workers to evaluate the defense
quality according to relevancy and fluency, the co-
herency of the overall conversation, and the toxic-
ity of the defense utterance. 27 conversations were
rated from each of the three defenses (TM, Two-
stage Non Sequitur, and our proposed defense). 3
AMT workers rated each conversation which gave
us 243 annotations 81 from each defense. More
details can be found in Appendix A.
3.3 Results
Defense Effectiveness We report “defense effec-
tiveness” as the percent decrease in a defender gen-
erating a toxic response after adversary’s attack
when the defense is applied compared to when it
isn’t. From our results, we observe that both our
proposed defense mechanism as well as the Non
Sequitur baseline achieve 100% defense effec-
tiveness according to Toxic-bert classifier. We also
noticed that for our proposed method for all the at-
tacks except UAT-LM, we were able to reach 100%
defense effectiveness by only masking one token.
For UAT-LM, almost 90% of cases were resolved
by masking one token and the rest were resolved
by the iterative approach that masked multiple to-
kens (up to 3). In addition, our defense is also
outperforming the oracle Trigger Masking which
shows that using model interpretability can give us
more valuable insights than blindly masking out
the triggers. In some cases tokens generated after
the trigger can themselves be more toxic and deci-
sive in forcing the defender into generating toxic
utterances (more details in Appendix B.1 Table 4.).
As expected, the Non Sequitur defense is always
effective as it replaces the toxic utterance with anon-toxic utterance by changing the topic; how-
ever, this approach is not necessarily creating the
best conversational experience as also verified by
our human experiments in terms of maintaining
relevancy and coherency of the conversation.
Defense Transferability We analyze transferabil-
ity of our defense mechanism with regards to three
different aspects as follows:
1. Transferability to other toxicity detection
classifiers: Results in Figure 7 demonstrate that
even if the defender is using the interpretability
results provided by the Toxic-bert classifier, it can
still be effective in reducing toxicity according to
Perspective API and Safety classifier on all attacks.
2. Transferability when UTSC attack uses dif-
ferent toxicity classifier than what the defender
uses in its defense: We also noticed that even if
the defender and the attacker do not use the same
toxicity detectors the defense can be effective. To
see the results of our defense on all the combina-
tion of toxicity detectors used by the attacker for
its selection criteria refer to Appendix B.1.
3. Transferability of the defense to human
generated attacks: Lastly, to make sure that our
defense also transfers to human generated attacks
and not just automatic attacks, we tried to generate
attacks against the DialoGPT model and converse
with it as the adversary. We managed to trigger
the system for 10% of the cases, in line with the
automatic attacks. We also saw 70% reduction in
toxic generation when we applied only one iteration
of our defense mechanism on these attacks.
Human Evaluation Results of our human evalu-
ations are demonstrated in Figure 8. Our defense
is rated to have the highest fluency and relevancy
scores. While our defense is mostly rated to have
moderate to good ratings for relevancy, the Non
Sequitur defense has poor relevancy scores. This is
because the Non Sequitur defense changes the topic
every-time a toxic utterance is generated which
lowers the quality of the conversational experience.
Thus, even if the Non Sequitur defense can be re-
ally effective in reducing the toxicity as it replaces
the toxic utterance with a non-toxic templated sen-
tence, it can create poor conversational experience
as also rated by human annotators. Human annota-
tor agreements were also reasonable for these tasks
(Table 2) according to Fleiss Kappa scores.2837
4 Beyond Conversational Agents
We show the generalizability of our defense method
against non-conversational generation tasks, by
conducting experiments with RealToxicityPrompts
dataset (Gehman et al., 2020). Previous work
showed that the prompts in RealToxicityPrompts
can force different generative models such as GPT-
2 (Radford et al., 2019) to generate toxic responses.
Thus, we used our defense to test whether it
can also be effective in reducing the number of
toxic responses given these prompts in RealToxic-
ityPrompts in the GPT-2 model. As evident from
the previous discussions, the Non Sequitur baseline
defense (Xu et al., 2020) that we considered in our
paper, only works for the conversational domain;
however, our method has the advantage of working
on any conditional generation task. We used the
100k prompts in RealToxicityPrompts and reported
the number of toxic generations before and after
applying our defense from the GPT-2 model.
Results in Figure 9 demonstrate that one itera-
tion of our defense reduces the number of generated
toxic responses by 81%, 31%, and 23%, according
to Toxic-bert, Perspective API, and Safety classi-
fier, respectively. Although the defense is based on
Toxic-bert, the results still transfer to Perspective
API and Safety classifier. These results show the
effectiveness of our defense in reducing toxic gen-
erations beyond conversational domain and a step
toward reducing toxic generation. Notice that the
setup of this experiment was not adversarial; how-
ever, prompts were causing the toxic generations.
5 Related Work
Crafting adversarial examples and using them in
training was previously shown to be an effective
technique in improving NLP and ML models (Nie
et al., 2020; Dinan et al., 2019; Kiela et al., 2021).
Not only that, but adversarial attacks can reveal im-
portant vulnerabilities in our systems (Zhang et al.,
2020a). Although previous work has studied adver-
sarial examples in NLP (Li et al., 2017; Zang et al.,
2020; Morris et al., 2020; Mozes et al., 2021) most
of them focused on accuracy as a metric of inter-
est. Among the ones that studied toxicity and other
ethical considerations (Wallace et al., 2019; Sheng
et al., 2020) they did not put the focus on either con-
versational agents or they did not consider attacks
being imperceptible. Cheng et al. (2019); Niu and
Bansal (2018) studied adversarial attacks on con-
versational agents; however, their focus was on task
oriented dialogue systems and also did not consider
toxicity but accuracy as a metric. Xu et al. (2020)
also considered conversational domains; however,2838they relied on human adversaries which can be
costly and non-scalable.
Beyond attacks, we discussed a possible defense
mechanism to improve robustness of generative
models against generating toxic responses using
interpretability methods. Using interpretability
mechanisms was also previously shown to be effec-
tive in reducing bias in ML applications (Mehrabi
et al., 2021a). In addition, there is a body of
work in detecting toxic behavior in conversational
agents (Zhang et al., 2018; Almerekhi et al., 2019;
Baheti et al., 2021) that can be utilized to design
ethically aligned systems.
6 Conclusion
We studied the possibility of generating impercepti-
ble attacks against conversational agents that, while
fluent and coherent, target the model into generat-
ing toxic responses. Through various automatic
and human experiments, we showed the effective-
ness of our attacks both in terms of being adver-
sarial as well as being able to maintain coherency,
relevancy, and fluency of the generated conversa-
tion (what we referred as the imperceptibility of the
attack). We then proposed a defense mechanism
that was shown to be effective through various auto-
matic and human evaluations as well as its transfer-
ability to human attacks, general generation tasks,
and different toxicity classifiers. Future work can
focus on improving our proposed attacks both in
terms of imperceptibility and effectiveness as well
as more advanced defense mechanisms.
Acknowledgments
We thank anonymous reviewers for providing in-
sightful feedback. We are also thankful to Emily
Dinan (Meta AI) for providing useful pointers
about safety classifiers. Ninareh Mehrabi’s re-
search was funded by USC + Amazon Center on
Secure and Trusted Machine Learning fellowship.
This work is partially sponsored by the Defense Ad-
vanced Research Projects Agency (DARPA) under
the contract HR00112290021.
Broader Impact
In this work, we proposed possible attacks and de-
fenses against conversational models that can help
improve robustness of conversational agents. We
also discussed the extension of our defense work
on any general generation task that can be an im-
portant contribution towards mitigating toxic gen-erations from our models. By proposing effective
imperceptible automatic attacks, we also eliminate
the need for human labor, reduce the cost, and make
this process more scalable.
Previous work has shown the importance of ad-
versarially crafted examples into improving NLP
systems (Nie et al., 2020; Dinan et al., 2019; Kiela
et al., 2021); thus, our automatically generated ex-
amples can be useful in not only improving robust-
ness of these systems and highlighting their vulner-
abilities, but also a step towards their improvement.
Not to mention our defense mechanism that can
directly mitigate the discussed issues.
However, we also acknowledge the negative im-
pacts that our work can have if used irresponsibly.
We acknowledge that our attack can be used by
unethical adversaries to force the models to gen-
erate toxic responses which is undesirable as also
previously observed in chatbots (Wolf et al., 2017;
Henderson et al., 2018; Dinan et al., 2021).
Since our defense mechanism relies on model
interpretability, some of the models may be black-
box or not-interpretable. In that case, we show
that the defender still can use proxy models which
are interpretable and as shown in the results of our
experiments the defense can still be transferable.
However, we acknowledge that in such cases the
defense might not be as effective, which can be
considered a limitation of our work. Another possi-
ble limitation of our defense mechanism can be the
token-level dependence of our defense approach
which can cause our defense mechanism to possi-
bly fail on more subtle cases where there is no clear
token that makes a sentence toxic.
In our studies, we also incorporated human an-
notators to annotate the quality of our generated
conversations. We made sure to provide the anno-
tators with appropriate and sufficient instructions
to complete the work along with a reasonable and
acceptable compensation for their labor. We also
made the annotators aware of possible toxic or in-
appropriate language in our generations ahead of
time. More details can be found in Appendix A.
We hope that our study can be used for the bene-
fit of the society and development of robust conver-
sational systems along with reduced toxic genera-
tions in our models. We release our code and data
in a public Github repository for the community to
be able to use and reproduce our results.2839References28402841Appendix
We discuss additional detailed experimental setups,
quantitative, and qualitative results that were left
out from the main paper due to space limitation.
A Details of Experimental Setup
General Setup For the UAT andUAT-LM attacks,
we used a 6-gram trigger to include in our baselines.
For “attack effectiveness”, we report the percentage
of conversations out of 100 conversations that we
study in this paper in which the defender was fooled
by the adversary after the performed attack into
generating a toxic response. This is done over one
round of analysis. For “defense effectiveness”, we
report the percent decrease in a defender generating
a toxic response after adversary’s attack when the
defense is applied compared to when it isn’t on
the same set of 100 conversations that we used
throughout the paper for different analysis. Similar
to attack effectiveness, this is done over one round
of analysis.
Computational Resources We used Nvidia
GeForce RTX 2080 to perform all our experiments
except the experiment using the GPT-2 model
which was ran on CPU for memory constraints.
Model details We used the small DialoGPT model
from Hugging face. The GPT-2and Unitary
Toxic-bertmodels were also adapted from Hug-
ging face. Safety classifier was adapted from Par-
lAI. We used all the default thresholds set by the
developers for all these toxicity detection classifiers
and a threshold value of 0.5 for the Perspective API
to detect whether an utterance is toxic or not. We
use the same threshold values both in our attacks
for the selection criteria and in defenses to deter-
mine if a generated utterance is toxic or not. Com-
paring the performance of three classifiers along
with analyzing qualitative results, we realized that
Toxic-bert is the least sensitive amongst the three
classifiers, followed by Perspective API that has the
closest agreement to humans, and Safety classifier.
Mechanical Turk Mechanical turk experiments
were performed on Amazon’s MTurk platform.
We tested the experiment carefully on the sandbox
platform before releasing it live. The turkers werechosen from the master workers pool with addi-
tional qualifications set (e.g., HIT approval rate
above 85, number of approved HITs above 1000)
to make sure workers are reliable workers. We
left a comment section to make sure we hear the
workers’ concerns about the task and the pay. We
received couple of comments about the task being
interesting with no complains on the pay. We made
sure to give reasonable and on time compensation
for the amount of work the workers put into and
made sure to hear their comments about the pay.
We paid 0.30 for each HIT to be completed. De-
tailed survey instruction forms of our attack and
defense are included in Figures 13 and 14.
Selection Criteria Details in UTSC Attack For se-
lection criteria, we used the average toxicity scores
from three different classifiers (Perspective API,
Toxic-bert, and Safety classifier) unless otherwise
stated in which we either used the score from one
toxicity classifier or the average score from two
classifiers. To determine whether an utterance is
toxic or not, we used the default thresholds set by
the developers for Toxic-bert and Safety classifiers
and a threshold value of 0.5 for Perspective API.
In addition to toxicity scores, we considered other
selection criteria, such as length of the generated
attack; however, we saw no significant signal in
using the length. Thus, we focused on using toxi-
city scores in the main text which as shown in the
results play a significant role in attack effective-
ness. Notice that other selection criteria can be
considered along with length and toxicity scores,
such as perplexity score for fluency or other met-
rics; however, for this study, we considered these
two cases. In our experiments the adversary gen-
erates 10 candidate attack utterances for each of
its attacks and the final attack utterance is selected
based on the selection criteria out of those 10 gen-
erated candidates. Additionally, we report some
statistics about toxicity scores of the adversary on
the attack utterance as well as defender’s toxicity
score after the attack for UTSC-1, UTSC-2, and
UTSC-3 attacks which can provide additional in-
tuition on how toxic each attack is. These results
are on the 100 conversations that are used in our
experiments and are reported in Table 3.
B Additional Results
B.1 Additional Quantitative Results
Data Sensitivity In Figure 15, we demonstrate
what proportion of the attack effectiveness comes2842
from which of the two Wizard of Wikipedia and
Reddit datasets. As also mentioned in the main text,
Reddit dataset contains context topics around more
sensitive issues, while the Wizard of Wikipedia
data is more neutral. We show in our results that
the topic context does not play a major role in
our attacks being effective and indeed our attack
can work as well or even better for the Wizard
of Wikipedia dataset that contains more neutral
context topics.
Attack Transferability In Figure 11, we demon-
strate that no matter what toxicity detection classi-
fier the attacker uses to chose its attack utterance,
the attack can still transfer to other toxicity detec-
tion classifiers. For instance, if the attacker only
uses Perspective API to perform its attack, results
show that the attack is still successful according
to Toxic-bert and Safety classifiers in addition to
Perspective API. Results for different combinations
is shown in Figure 11.
Defense Transferability In Figures 10 and 12, we
show two different types of defense transferabil-
ity. In Figure 10, we show that the defender and
the attacker do not need to use the same toxicity
detection classifiers for the defense to be effective.
We show that for instance, if the attacker is only
using Perspective API to perform its attack and the
defender is using Toxic-bert to perform the defense
the defense is still effective for 100% of the times.
We demonstrate different combinations of classi-
fiers used by the attacker against a defender that
uses Toxic-bert to perform the defenese. In all the
cases, we show that the defense is effective 100%
of the times for our defense mechanism.
In Figure 12, we show that the defense trans-fers to other toxicity detection classifiers as well
not only Toxic-bert for all the different combina-
tions of the attacker toxicity detection classifiers.
Thus, results show that even if the defender is us-
ing Toxic-bert to perform the defense, according
to both Perspective API and Safety classifiers the
amount of toxicity is still decreased after the attack
irrespective of what toxicity classifier the attacker
is using. Of course, the defense is the most effective
for Toxic-bert classifier; however, it is interesting
that the attack also transfers to other classifiers.
B.2 Additional Qualitative Results
Finally, we show some qualitative results from our
attacks and defenses in Figure 16. We show results
from our automatic attack strategy as well as our
defense mechanism on it (Figure 16 (a)) along with
our human experimental results in which a human
adversary tries to fool the system into generating
toxic utterances (Figure 16 (b)) and lastly the GPT-
2 experiments using the RealToxicityPromts and
how effective our proposed defense mechanism
works on these sets of prompts and model (Fig-
ure 16 (c-f)).
Notice that our human performed attacks did not
consider any contexts since the human adversary
was defining the context and starting the conver-
sation with the context in mind all in one shot.
This is slightly different than our automatically per-
formed attack setup in which we always start the
conversations given a context topic to force the bots
to converse around the given topic and not just a
random topic. The rest of the experimental setup,
however, is similar to the automatic attack/defense
setup.28432844284528462847