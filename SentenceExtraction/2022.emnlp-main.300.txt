
Zhaofeng WuRobert L. Logan IV
Pete WalshAkshita BhagiaDirk GroeneveldSameer Singh
Iz BeltagyMITDataminr Inc.Allen Institute for Artificial Intelligence
University of California, Irvine
zfw@csail.mit.edu rlogan@dataminr.com
{petew,akshitab,dirkg,beltagy}@allenai.org sameer@uci.edu
Abstract
Recently introduced language model prompt-
ing methods can achieve high accuracy in zero-
and few-shot settings while requiring few to
no learned task-specific parameters. Never-
theless, these methods still often trail behind
full model finetuning. In this work, we investi-
gate if a dedicated continued pretraining stage
could improve “promptability”, i.e., zero-shot
performance with natural language prompts or
few-shot performance with prompt tuning. We
reveal settings where existing continued pre-
training methods lack promptability. We also
identify current methodological gaps, which
we fill with thorough large-scale experiments.
We demonstrate that a simple recipe, contin-
ued pretraining that incorporates a trainable
prompt during multi-task learning, leads to im-
proved promptability in both zero- and few-
shot settings compared to existing methods, up
to 31% relative. On the other hand, we find that
continued pretraining using MAML-style meta-
learning, a method that directly optimizes few-
shot promptability, yields subpar performance.
We validate our findings with two prompt tun-
ing methods, and, based on our results, we
provide concrete recommendations to optimize
promptability for different use cases.
1 Introduction
Conditioning language models (LMs) on manually-
written or learned continuous prompts allows them
to solve tasks with high accuracy and minimal pa-
rameter overhead (Brown et al., 2020; Li and Liang,
2021; Lester et al., 2021, i.a.). However, prompt-
ing performance often still lags behind traditional
full finetuning. Natural language prompts usually
underperform trained models even when manually
curated (Brown et al., 2020; Sanh et al., 2022). Sim-
ilarly, while learned prompts yield higher accuracy,they do not work as well when the training data is
scarce (Gu et al., 2022), when the model is small
or moderately sized (Lester et al., 2021), and when
the tasks are difficult (He et al., 2022).
To reduce the gap between prompt and full
model tuning, past work has shown that continued
pretraining on data that resembles the downstream
prompting setup induces better “promptability”,
i.e., zero-shot performance with natural language
(NL) prompts and few-shot performance of prompt
tuning (Sanh et al., 2022; Gu et al., 2022). However,
in this paper, we identify several shortcomings of
these methods. First, continued pretraining on NL
prompts (Sanh et al., 2022) sometimes causes per-
formance degradation with prompt tuning. Second,
continued pretraining approaches that learn only a
universal prompt initialization (Gu et al., 2022; Vu
et al., 2022) bring only marginal improvement on
the P3 datasets (Bach et al., 2022).
To further improve zero-shot and few-shot
promptability, we investigate gaps in existing meth-
ods with different parameter configurations and
training procedures. First, we explore the effect
of incorporating a learned continuous prompt into
multi-task learning (MTL), and find it to signifi-
cantly improve zero- and few-shot promptability
across the board. In addition, we explore MAML-
style meta-learning (Finn et al., 2017; Nichol et al.,
2018) as an alternative to the standard continued
pretraining paradigm, but find that it underperforms
simple MTL, despite its previous success on few-
shot learning tasks (Li et al., 2017; Gu et al., 2018;
Qian and Yu, 2019, i.a.). We perform an analysis of
this phenomenon and present several explanations.
Through large-scale experiments, each involv-
ing continued pretraining on over 9B tokens (§A),
we make several contributions: (1) we thoroughly
evaluate continued pretraining methods, both ex-
isting and our proposed ones, in many setups; (2)
we demonstrate that a simple continued pretraining
recipe improves over existing methods by up to451731%; (3) we show that MAML-style meta-learning
underperforms multi-task learning and provide ex-
planations; (4) we provide concrete recommen-
dations to improve promptability in various use
cases.
2 Prompting
We review two types of prompting that we use: nat-
ural language (NL) prompting and prompt tuning.
Traditionally, NLP tasks are solved by task-
specific models that predict label y∈ Y from
input x∈ X . We can consider LMs as func-
tions that score any source and target text pair,
LM :V× V→Rwith vocabulary V.Past
work found that large LMs can be repurposed to
solve many tasks by casting x, yinto a text format
using a template function f:X ∪Y → Vand tak-
ing as prediction arg maxLM(f(x), f(y)).
NL prompts, or instructions, are manually
constructed f(·). Without task-specific training,
they have been successfully used to elicit predic-
tions from LMs to perform tasks with high accu-
racy (Brown et al., 2020; Logan IV et al., 2022).
Sharing the motivation, prompt tuning learns
a continuous prompt to condition the model. It
takes the source text embedded by the LM input
embeddings, s∈Rwith length Nand di-
mension d, and prepends learnable embeddings
E∈R, where Lis a hyperparameter, to ob-
tain a new (L+N)-lengthed embedded sequence.
We consider hybrid prompt tuning, where sis the
embedding of the templatized f(x), i.e., prompt
tuning is always performed in addition to NL tem-
plates. This has been widely adopted due to demon-
strated better performance (Gu et al., 2022; Min
et al., 2022). We also study a variant of prompt tun-
ing, sometimes called prefix tuning (Li and Liang,
2021), where the learnable vectors are added not
only to the input but all transformer layers. See
Lester et al. (2021) and Li and Liang (2021) for
more details on these methods. Following the ter-
minology of Liu et al. (2022b), we refer to the
input-level method as shallow prompt tuning and
the layer-specific method as deep prompt tuning.
3 Improving Promptability
In this section, we describe existing methods to
improve promptability and a new paradigm thatcombines their advantages.
While prompt tuning sometimes performs close
to full model finetuning (Lester et al., 2021; Liu
et al., 2022b), there is often still a substantial gap,
such as with limited training data (Gu et al., 2022),
non-gigantic models (Lester et al., 2021), or chal-
lenging tasks (He et al., 2022). We therefore study
ways to improve LMs’ “promptability.” We focus
on a low resource setup and consider zero-shot
NL prompts and few-shot learned prompts (which,
again, are in conjunction with NL prompts; §2).
For the former, better promptability increases the
performance when LMs face textual prompts of
new tasks. For the latter, it more effectively lever-
ages limited training examples for higher accuracy.
We investigate if promptability can improve with
a continued pretraining stage after LM pretraining
(or LM adaptation for LM-adapted T5 (Lester et al.,
2021)) and before task-specific finetuning. The
model is trained on a collection of tasks that have
NL prompts and evaluated on unseen tasks. The
methods that we explore below differ in how the
continued pretraining stage is performed. We use
the notation MTL-T_P_ to abbreviate those meth-
ods that are based on multi-task learning, where
the blanks _specify different configurations of the
transformer ( T) and the prompt ( P) components
during MTL. Architecturally, a method may con-
tinue to pretrain only the T5 model without prompt
parameters, in which case we use P ✗to denote
the lack of them; otherwise, both transformer and
prompt parameters exist during MTL. We useand to denote if the corresponding component is
trained or frozen in MTL, respectively. This nota-
tion describes the continued pretraining stage only:
in the final finetuning stage, all methods include
both the transformer and prompt components, but
only the latter is updated.
Continued pretraining has been studied in lim-
ited settings. Sanh et al. (2022) proposed T0 by
multi-task training a T5 model (Raffel et al., 2020)
as continued pretraining. They updated T5 pa-
rameters through learning on continued pretrain-
ing tasks, not including a prompt component, and
showed that this training improves zero-shot NL
promptability. Following our nomenclature, we re-
fer to this paradigm as MTL-T P✗. Additionally,
Gu et al. (2022) employed a similar stage, incor-
porating and multi-task training a shallow prompt
as continued pretraining, while freezing the trans-
former parameters in this stage. They showed that4518this strategy helps few-shot promptability during
finetuning. We refer to this paradigm as MTL-
TP.
In this work, we study the gains of the previous
two continued pretraining approaches, as well as a
model that synthesizes them, MTL-T P, which
we are the first to propose. For few-shot down-
stream tuning, the learned prompt can act as a good
initialization compared to MTL-T P✗. In the zero-
shot setup, prior work has discovered that includ-
ing certain text in a prompt, such as “Let’s think
step by step,” can adjust the reasoning of LMs to
yield substantially improved performance across
tasks (Kojima et al., 2022; Askell et al., 2021). The
learned prompt here could function analogously.
Compared to MTL-T P, on the other hand, the
additional capacity brought by more updatable pa-
rameters could further boost model performance.
MAML-style meta-learning (Finn et al., 2017)
directly optimizes for the downstream updates
and can outperform MTL for full model finetun-
ing (Dou et al., 2019; Bansal et al., 2020a). Yet,
it similarly remains unexplored for prompting.
We examine first-order MAML ( FOMAML ; Finn
et al., 2017), performing Tsteps of prompt tuning
in the inner loop and updating all parameters in
the outer loop. We also evaluate a version of Rep-
tile(Nichol et al., 2018) adapted for our setting that
performs Tsteps of prompt tuning followed by one
step of full model tuning, and use the resulting Rep-
tile gradient for model updates. They have the same
architecture as MTL-T Pand all parameters are
trainable too. We provide a detailed description
and theoretical discussion of these processes in §B.
See the original papers for more details.
4 Experimental Setup
We use P3, a collection of NL-templatized exam-
ples for a variety of datasets, for training and evalu-
ation using the standard splits in Sanh et al. (2022).
Not only is there no dataset overlap between train-
ing and evaluation, but no taskoverlap either (e.g.,
sentiment vs. QA), making it challenging. We re-
port dataset statistics in §A. We perform continued
pretraining for one epoch over all training datasets.
Each dataset has multiple templates, each evaluated
with accuracy. As different datasets have differ-
ent numbers of answer choices and hence different
baseline accuracy, we report Average Relative Gain
(ARG; Ye et al., 2021) as a single summary metric
by averaging across all templates the relative ac-curacy improvement over a random baseline. We
perform significance testing using bootstrap with
1,000 iterations, in each iteration randomly sam-
pling evaluation examples and comparing the two
models in question. §D reports per-dataset results.
Following Sanh et al. (2022), we initialize the
continued pretraining stage from T5 finetuned with
an LM objective (Lester et al., 2021), making it
more amenable to prompting. We experiment with
two sizes: T5-Large with 770M parameters and T5-
XL with 3B parameters. We retrain T0 (Sanh et al.,
2022), i.e. MTL-T P✗, to eliminate confounding
factors in the training procedure. We also repro-
duce Gu et al. (2022)’s experiment in our setup, i.e.
MTL-T P, pretraining a shallow prompt with
other parameters frozen. During few-shot finetun-
ing, we train on the same 16 examples for 100
epochs. §C reports additional hyperparameters.
5 Results
Table 1 reports our results. From No Cont. Pre-
training , we find that continued pretraining is cru-
cial for prompt tuning with low resources—without
it, only few-shot deep prompt tuning yields slightly
above-random performance. These results contra-
dict previous findings that few-shot prompt tuning
works well without this stage (Min et al., 2022). We
believe this is due to the challenging nature of the
P3 evaluation datasets, compared to the simple sen-
tence classification tasks previously investigated.
This is consistent with what He et al. (2022) ob-
served in the full-data setting where deep prompt
tuning performs sub-optimally on difficult tasks.
Existing methods for continued pretraining have
their drawbacks. In contrast to Gu et al. (2022),
we found that MTL-T Pwith a shallow prompt
does not substantially perform above random. We
attribute this to (1) their simpler evaluation tasks
which, unlike ours, have decent prompt-tuned per-
formance without continued pretraining; and (2)
their hand-designed pretraining tasks that match
their evaluation tasks, while P3 conversely avoids
training-evaluation task overlap, requiring general-
izability. Vu et al. (2022) also found MTL-T Pto be effective, though with high resources. We
also compare with T0, i.e. MTL-T P✗, where
both the official model and our reproduction suffer
from degraded performance when few-shot shallow
prompt tuned (compared to 0-shot), likely because
the prompt added during finetuning is intrusive, and
the limited gradient updates are not sufficient to re-4519T5-Large (770M) T5-XL (3B)
Shallow Deep Shallow Deep
0-shot 16-shot 0-shot 16-shot 0-shot 16-shot 0-shot 16-shot
No Cont. Pretraining –1.9–1.5 –1.92.3 –1.5–1.5 –1.53.7
Previous methods
MTL-T P 2.8 2.9 — — –1.7 –1.6 — —
MTL-T P✗ — — — — 25.219.1 25.233.8
MTL-T P✗ 26.723.1 26.732.6 32.430.0 32.442.9
Our methods
MTL-T P 30.2 30.2 28.7 37.4 33.5 33.5 33.3 43.2
FOMAML 21.8 21.8 20.8 32.8 27.7 27.7 24.5 40.0
Reptile 18.4 18.4 24.9 33.0 23.1 23.2 26.3 39.7
cover from it. We note that the official T0 model is
not well-optimized: even without hyperparameter
tuning, our implementation is significantly better
(p <0.001for all).
MTL-T P significantly outperforms MTL-
TP✗, the strongest existing method we examine,
across all settings ( p < 0.005for all) except for
few-shot deep prompt tuning on T5-XL ( p= 0.21).
For zero-shot NL promptability, the improvement
could be due to the extra model capacity, or the
multi-task trained prompt adjusting the reasoning
of the LM, analogous to the text-based “Let’s think
step by step” effect (Kojima et al., 2022). For few-
shot shallow prompt tuning, unlike MTL-T P✗,
MTL-T Pdoes not degrade in performance, re-
sulting in 31% higher ARG than MTL-T P✗on
T5-Large. This is likely because of the model’s
familiarity with the prompt, though the limited ca-
pacity of shallow prompt tuning does not yield
benefits either. Nevertheless, with deep prompt tun-
ing, which gives the model sufficient conditioning
capacity, few-shot tuning does lead to performance
increase, again outperforming MTL-T P✗. Here,
MTL-T Pprovides a good prompt initialization
and alleviates its intrusiveness. These results em-
phasize the importance of continued pretraining
being aware of the downstream finetuning process.
Interestingly, however, the gap between these two
models shrinks as the model size increases, no
longer significant at T5-XL ( p= 0.21). Also, no-
tably, pretraining with a shallow prompt has better
0-shot performance than a deep prompt. This high-lights that higher pretraining capacity is not always
beneficial, and matches our motivation from text-
based conditioning which also happens at the input
level.
FOMAML and Reptile surprisingly underper-
form MTL-T Pin few-shot prompt tuning, even
though they specifically optimize for this proce-
dure and have demonstrated success in NLP for
full model finetuning (Dou et al., 2019; Bansal
et al., 2020b, 2021, i.a.) and few-shot learning (Gu
et al., 2018; Qian and Yu, 2019; Mi et al., 2019,
i.a.). While Ye et al. (2021) also found FOMAML
to underperform MTL, they sub-optimally only per-
formed one inner loop update. Here, we show that
this comparison holds for more appropriate hyper-
parameters. This could be due to the fewer number
of gradient updates: to perform one gradient update,
MTL uses one training batch, while FOMAML
withTinner loop steps or Reptile with Tprompt
tuning steps use T+ 1batches. Not only might
this be an inefficient use of training examples, but
compute FLOPs too, since each inner loop/prompt
tuning step involves a full forward-backward pass.
We attempt using a T+ 1times smaller meta batch
size (see §B for more detail) to pretrain a deep
T5-Large-sized Reptile. When prompt-tuned, it
achieves 22.8 ARG, which is even lower, possibly
due to higher gradient estimation noise. Alterna-
tively, other factors could affect the performance of
meta-learning. It is, for example, well-known that
MAML-style meta-learning can be unstable and4520sensitive to architectures and hyperparameters (An-
toniou et al., 2019). This instability is likely am-
plified by our large heterogeneous multi-task setup
and our inability to afford hyperparameter search.
Furthermore, its theoretical foundation has mostly
only been examined through simple optimizers, pre-
dominantly SGD (Finn et al., 2017; Nichol et al.,
2018). How it interacts with optimizers more com-
mon in modern NLP, such as Adafactor (which we
use), remains to be explored.
Recommendations. Based on our findings, we
recommend practitioners to always incorporate
a prompt during continued pretraining and to
train the entire model. Without downstream task-
specific tuning, such as when there is no train-
ing data or sufficient compute, a shallow prompt
yields better accuracy. When few-shot task-specific
prompt tuning is affordable, continued pretraining
with a deep prompt enables the best performance.
6 Conclusion
We demonstrated that the simple recipe of con-
tinued pretraining with a prompt significantly
improves zero-shot NL promptability and few-
shot learned promptability. MAML-based meta-
learning, on the other hand, obtains worse perfor-
mance, for which we provided several explanations.
Nonetheless, we believe future efforts to leverage
their conceptual advantage could be fruitful, per-
haps aided by our observations. We also hope to
study the effect of continued pretraining with other
parameter injection methods (Houlsby et al., 2019;
Hu et al., 2022; Liu et al., 2022a).
Limitations
Due to the expensive nature of our experiments,
each involving continued pretraining on over 9B to-
kens (§A), we could not afford to perform hyperpa-
rameter tuning, and instead took hyperparameters
from prior work. It is, nevertheless, possible that
careful hyperparameter tuning might yield slightly
different trends from what we observed. Further-
more, because of computational constraints, we
were unable to perform experiments on the largest
released T5 model with 11B parameters. Though
we validated our findings on two model sizes,
it has been found that larger models sometimes
demonstrate qualitatively different results (Srivas-
tava et al., 2022; Lampinen et al., 2022; Wei et al.,
2022). We would be excited to see if our experi-
ments could be reproduced at a larger model scale.Acknowledgments
We appreciate Victor Sanh, Albert Webson, Colin
Raffel, Zaid Alyafeai, and other members of the
Bigscience project who answered many of our ques-
tions when we reimplemented T0, and Yuxian Gu
when we reimplemented Gu et al. (2022). We also
thank Sébastien M. R. Arnold whose help was cru-
cial for our meta-learning implementation. We are
also grateful for the members at AI2 and UC Irvine,
as well as the anonymous reviewers for their valu-
able feedback.
References4521452245234524Input: Number of inner loop steps T, meta
batch size B
Initialize LM-adapted T5 parameters ϕ
global_grad = 0
forb←1,···,Bdo
ϕ= clone( ϕ)
fort←1,···,Tdo
data = next_batch() // support
grad = forward_backward( ϕ, data)
update( ϕ, grad, prompt_only=True)
data = next_batch() // query
grad = forward_backward( ϕ, data)
global_grad += grad
global_grad = global_grad / B
update( ϕ, global_grad, prompt_only=False) The FOMAML algorithm for
prompt tuning.
A Dataset Details
We use P3 as our training and evaluation
datasets (Bach et al., 2022). It contains 35 datasets
grouped into 8 tasks: Multiple-Choice QA, Extrac-
tive QA, Closed-Book QA, Sentiment, Topic Clas-
sification, Structure-To-Text, Summarization, and
Paraphrase Identification. Examples in each dataset
are templatized using multiple human-written tem-
plates. Across the 35 datasets, there are a total of
313 templates. For continued pretraining, we fol-
low Sanh et al. (2022) and only use the training
split of each dataset. Four tasks are held out for
evaluation in P3: Sentence Completion, Natural
Language Inference, Coreference Resolution, and
Word Sense Disambiguation. They consist of 11
evaluation datasets (considering the three splits of
ANLI as separate datasets) and 116 templates in
total. We use the training split of each dataset for
few-shot experiments, and, following Sanh et al.
(2022), evaluate on the validation splits. The only
exception is StoryCloze which does not have a
training split, so we use its validation split for train-
ing and evaluate on its test split. Unlike T0, we do
not evaluate on the BIG-Bench datasets (Srivastava
et al., 2022) as they had not stabilized as a collec-
tion of datasets at the time of this work. All the
prompts in P3 are collected in English.
To make training more efficient, we right-
truncate all source sequences to 768 tokens and
target sequences to 192 tokens. For the continued
pretraining stage, this affects 2% of all trainingInput: Number of inner loop steps T, meta
batch size B, inner loop learning rate α
Initialize LM-adapted T5 parameters ϕ
global_grad = 0
forb←1,···,Bdo
ϕ= clone( ϕ)
fort←1,···,Tdo
data = next_batch() // support
grad = forward_backward( ϕ, data)
update( ϕ, grad, prompt_only=True)
data = next_batch() // query
grad = forward_backward( ϕ, data)
update( ϕ, grad, prompt_only=False)
global_grad –= ϕ
global_grad = global_grad / (αB)+ϕ
update( ϕ, global_grad, prompt_only=False) The Reptile algorithm for prompt
tuning.
examples, and among the 313 templates, 24 have
more than 1% examples truncated. Also, following
Sanh et al. (2022), we cap all datasets to have a
maximum of 500k examples. This results in 31.6M
training examples across all datasets and templates,
totaling 5.7B tokens on the source side and 3.5B
tokens on the target side.
B Meta-Learning Details
In this section, we elaborate on our meta-learning
training procedures. Algorithm 1 contains pseudo-
code for our first-order MAML (FOMAML) pro-
cedure. In the inner loop, we perform Tsteps of
prompt tuning on a cloned model using support
data. In the outer loop, we use query data to evalu-
ate the prompt-tuned model and compute gradients.
We use the first-order approximation where the gra-
dient is not taken with respect to the entire prompt
tuning process but only the forward pass with query
data because it is computationally more tractable,
and past work has shown that this first-order ap-
proximation does not hurt performance much, if at
all (Finn et al., 2017; Dou et al., 2019). Theoreti-
cally, to perfectly simulate the downstream prompt
tuning procedure, we should use the same batch
of support data for the Tsteps of update. Never-
theless, this would traverse the training data much
more slowly, so we use different support batches.
Our theoretical analysis through the perspective of
Reptile below also justifies this.
In preliminary experiments, we found a naïve4525adoption of Reptile (Nichol et al., 2018) to yield
subpar performance. As there is no inner- and
outer-loop distinction in Reptile, doing prompt tun-
ing leads to only the prompt parameter being up-
dated throughout the entire continued pretraining
stage, likely causing the performance degradation.
This effect is also seen in our multi-task learning
setup with the MTL-T Pmodel. Thus, we pro-
pose to adapt Reptile to better suit prompt tuning,
which we illustrate in Algorithm 2. It is similar
to FOMAML, but instead of considering the outer
loop’s gradient as the meta-learning gradient, it
uses/summationtext(ϕ−ϕ)where bis the cloned
model’s final parameter for meta-batch b.
Now we theoretically justify our proposed Rep-
tile version, mostly following the original proof
structure in Nichol et al. (2018), in the context of
prompt tuning. We can think of the downstream
finetuning stage as starting from the initial model
parameters ϕand performing Tsteps of prompt
tuning on the same batch of training data which
produces a loss function L(ϕ)for some model
parameters ϕ. Then this model is evaluated on
some test data that similarly produces a loss func-
tionL(ϕ)for the final trained model ϕ. Let
us first abbreviate some gradients and Hessians:
g=L(ϕ) =∂
∂ϕL(ϕ)
H=L(ϕ)
g=L(ϕ)
H=L(ϕ)
Then we can write each step of the prompt tuning
process as an update function:
U(ϕ) =ϕ−αm◦L(ϕ)
U(ϕ) =I−αM◦L(ϕ)
where αis the learning rate and mandMare
boolean masks that contain 1 for the prompt pa-
rameters. ◦indicates element-wise multiplication,
which we prescribe to take the highest precedence
in the equations below.
With Titerations of U, we have:
ϕ=ϕ−αm◦/summationdisplayg (1)Plugging in Equation 1, by Taylor’s theorem:
g=L(ϕ)
=L(ϕ) +L(ϕ)(ϕ−ϕ) +O(α)
=g+H(ϕ−ϕ) +O(α)
=g−αHm◦/summationdisplayg+O(α)
=g−αiHm◦g+O(α)
(2)
where the last step can be seen by induction, itera-
tively applying the second-to-last line.
With a similar process, we can derive:
H=H+O(α) (3)
The FOMAML gradient is the same as L(ϕ).
Plugging in Equation 2 but sweeping its non-
leading terms into O(α):
g =L(ϕ)
=L(ϕ) +L(ϕ)(ϕ−ϕ)
+O(α)
=g+H(ϕ−ϕ) +O(α)
=g−αHm◦/summationdisplayg+O(α)
=g−αTHm◦g+O(α)
The full MAML gradient takes the derivative
throughout the entire prompt tuning process. Plug-
ging in Equation 3 and g =L(ϕ)and
sweeping terms into O(α)when possible:4526
The Reptile gradient, in our adaptation, takes
the prompt’s gradient during the Tsteps and the
entire model’s gradient for one step. Taking the
Reptile gradient from Algorithm 2 and using ϕ
to represent the parameters after the outer loop full-
finetuning update:
g =1
α(ϕ−ϕ)
=m◦/summationdisplayg+g
=m◦/summationdisplay(g−αjHm◦g)
+g +O(α)
=Tm◦g−αT(T−1)
2m◦(Hm◦g)
+g +O(α)
=g+Tm◦g
−αT(T−1)
2m◦(Hm◦g)
−αTHm◦g+O(α)
We can see that all three meta-learning gradi-
ents have a similar effect: they only contain amixture of lone gradients terms ( g, g), which act
as a pure multi-task learning objective, and terms
that are Hessian times gradient, which Nichol et al.
(2018) termed “AvgGradInner” and showed to en-
courage the expected similarity between different
data batches, improving generalization.
Back to our use of different data batches in FO-
MAML’s inner loop and Reptile’s prompt tuning
steps. If the inner loop uses the same support (i.e.,
training) data, as in the derivation above, the “Avg-
GradInner” terms become somewhat degenerate,
with the same term scaled Tortimes. With
different inner loop batches, on the other hand,
there would be more diverse Hessian-gradient in-
teractions between different batches of data and
hence encouraging generalization between more
tasks.
C Training Details
Due to the expensiveness of our experiments, we
did not perform any hyperparameter tuning. For
all continued pretraining runs, we follow Raffel
et al. (2020) and Sanh et al. (2022) and use Adafac-
tor (Shazeer and Stern, 2018) with a 0.001 learning
rate. We use a batch size of 4,096 which we calcu-
lated to be close to what Sanh et al. (2022) used.
We clip gradients to unit norm. For shallow prompt
tuning, we follow Min et al. (2022) and use L= 20
prompt tokens, each with the same dimension as
the word embedding size, on the source side only.
For deep prompt tuning, we similarly use 20 hid-
den vectors that are prepended in every transformer
layer, on both the source and target side for added
capacity. For meta-learning, we use a batch size of
16, simulating our 16-shot evaluation (see below),
and a meta batch size of 128. We perform 7 steps
of inner loop updates (FOMAML) / prompt tun-
ing (Reptile), following Bansal et al. (2020b) and
Bansal et al. (2021), and similarly using Adafactor
with learning rate 0.001. All continued pretraining
experiments run for one epoch over the training
datasets with no checkpoint selection. In few-shot
finetuning, we train on one batch of 16 randomly
selected examples for 100 epochs (the same batch
throughout training), following Min et al. (2022).
Like Min et al. (2022), we do not manually balance
the label distribution in these examples, unlike in
prior work (Gao et al., 2021; Logan IV et al., 2022).4527We perform all experiments on 80GB A100
GPUs. Each continued pretraining run takes four
(sometimes eight) of them. The largest MTL model
takes 10 days to pretrain with four GPUs, while the
largest meta-learning model takes 14 days.
D Per-Dataset Results
In Figures 1 to 3, we compare the per-dataset ac-
curacy ofMTL-T P✗(our reproduction), MTL-
TP,FOMAML , and Reptile . We omit MTL-
TPdue to its near-random performance.4528452945304531