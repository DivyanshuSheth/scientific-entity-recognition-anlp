
Mehwish Fatima and Michael Strube
Heidelberg Institute for Theoretical Studies
(mehwish.fatima|michael.strube)@h-its.org
Abstract
Automating Cross-lingual Science Journalism
() aims to generate popular science sum-
maries from English scientific texts for non-
expert readers in their local language. We in-
troduceas a downstream task of text sim-
plification and cross-lingual scientific summa-
rization to facilitate science journalists’ work.
We analyze the performance of possible ex-
isting solutions as baselines for thetask.
Based on these findings, we propose to com-
bine the three components - ,
and () to produce cross-lingual
simplified science summaries for non-expert
readers. Our empirical evaluation on the dataset shows thatsignificantly
outperforms the baselines for thetask and
can serve as a strong baseline for future work.
We also perform an ablation study investigat-
ing the impact of individual components of.
Further, we analyze the performance ofon
a high-quality, real-worlddataset with hu-
man evaluation and in-depth analysis, demon-
strating the superior performance of for.
1 Introduction
Cross-lingual Science Journalism () produces
popular science storiesfrom English scientific
texts for non-expert readers in their local language.focuses on simultaneously reducing linguistic
complexity and length of the original text. Au-
tomatingcan facilitate science journalists in
their work for writing popular science stories. A
real-world example ofis Spektrum der Wis-
senschaft ( ). It is a popular monthly
science magazine, the German version of Scientific
American. The magazine is considered a bridge
between scientific literature and the non-expert pub-
lic.
Our work is initiated by a collaboration with - , where journalists have been writing pop-
ular science stories in German for decades. Ta-
ble 1 presents an example of a article-
summary pair, where the German summary is writ-
ten by a science journalist. Upon textual analysis
of the dataset, we find that
journalists’ stories are distinct from regular scien-
tific texts for the following properties. They are
popular science stories and are much more concise
than the original articles. The stories have less com-
plex words and technical terms while having local
collocations . These stories are cross-lingual.1843A few researchers have studied Monolingual Sci-
ence Journalism () (Louis and Nenkova, 2013b;
Dangovski et al., 2021) as a summarization task. In
summarization, some efforts have also been made
towards monolingual (Cohan et al., 2018; Dan-
govski et al., 2019; Cachola et al., 2020) and cross-
lingual (Ouyang et al., 2019; Fatima and Strube,
2021) scientific summarization. Our preliminary in-
vestigation also adopts existing cross-lingual sum-
marization () models to explorefollowing
the’s steps. Since these models focus only on
summary generation, these summaries still need to
be simplified for non-expert readers. Therefore, we
proposeas a downstream task of text simplifi-
cation and cross-lingual scientific summarization to
generate a coherent cross-lingual popular science
story.
We analyze the workflow of ’s journal-
ists to develop a solution for thetask. They
read complex English scientific articles and mark
the essential facts, make them straightforward for
non-expert readers, and then write a coherent story
in German. Influenced by this, we propose to com-
bine the three components - ,
and () for exploring. We follow
the divide-and-conquer approach to designso
that each component is responsible for only one
task. It makesmanageable, flexible and inno-
vative as we can train individual components and
modify/replace them without affecting the’s
information flow. Table 1 also presents the out-
put generated by fine-tuned m and. We
believe thatis the first step towards the automa-
tion of, and it can assist science journalists in
their work and open up further directions.
Contributions
1.We introduce Cross-lingual Science Journal-
ism () as a downstream task of cross-
lingual scientific summarization and text sim-
plification targeting non-expert readers.
2.To solve, we develop a pipeline compris-
ing the three components - ,- and () for producing
popular German summaries from English sci-
entific texts.
3.We empirically evaluate the performance ofagainst several existing models on
the dataset with various evalua-
tion metrics. We also analyze ablated
models to examine the significance of eachcomponent.
4.We evaluate’s performance on the - dataset with human judgments and var-
ious statistical features to analyze them lin-
guistically.
2 Related Work
2.1 Science Journalism
Louis and Nenkova (2013a,b) investigatefor
the writing quality of New York Times science sto-
ries by dividing them into three coarse levels of
writing quality: clear, interesting and beautiful or
well-structured. They also analyze general features
of discourse organization and sentence structure.
Barel-Ben David et al. (2020) examine the public’s
interactions with scientific news written by early-
career scientists by capturing various features. The
authors collect a dataset of 150 science news writ-
ten by 50 scientists from two websites: Mako and
Ynet. Dangovski et al. (2021) consideras ab-
stractive summarization and story generation. They
collect scientific papers and Science Daily press re-
leases and apply sequence-to-sequence () mod-
els for generating summaries. These studies are
limited in their scope and consider only monolin-
gual texts, thus cannot be used for.
2.2 Simplification
Mostly, simplification is explored on the word and
sentence level. Coster and Kauchak (2011) con-
struct a parallel dataset from Wikipedia and simple
Wikipedia for sentence-level simplification. Kim
et al. (2016b) develop a parallel corpus of scientific
publications and simple Wikipedia for lexical-level
simplification. Laban et al. (2021) build a system to
solve the simplification of multi-sentence text with-
out the need for parallel corpora. Their approach
is based on a reinforcement learning model to opti-
mize the rewards for simplicity, fluency, salience
and guardrails. Recently, Ermakova et al. (2022)
introduced the task of science simplification at
CLEF2022 to address these challenges.
2.3 Scientific Summarization
Monolingual. Many researchers have developed
scientific summarization datasets by collecting on-
line scientific resources such as ArXiv, PubMed
and Medline (Kim et al., 2016a; Nikolov et al.,
2018; Cohan et al., 2018), Science Daily (Dan-
govski et al., 2019), the ACL anthology net-
work (Yasunaga et al., 2019), scientific blogs (Vada-1844palli et al., 2018b,a), BBC (Narayan et al., 2018)
and Open Review (Cachola et al., 2020). These
datasets are further used for developing extrac-
tive (Parveen and Strube, 2015; Xiao and Carenini,
2019; Dong et al., 2021), abstractive (Zhang et al.,
2020a; Huang et al., 2021) and hybrid (Liu and
Lapata, 2019; Pilault et al., 2020) models. Unfortu-
nately, all these studies are limited to monolingual
summarization () and extreme summarization,
and we cannot adopt them for.
Cross-lingual. For scientific, most studies use
monolingual datasets with two popular pipelines:
Translate-then-Summarize ( -) (Ouyang
et al., 2019) and Summarize-then-Translate ( - ) (Zhu et al., 2019, 2020). These pipelines
adopt machine translation () andmodels
to get the cumulative effect of. Recently, a
multilingual dataset - WikiLingua is created from
WikiHow text (Ladhak et al., 2020). The authors
collect parallel data in different languages from
WikiHow, which describes the instructions for solv-
ing a task. The nature of this dataset makes it
unsuitable for science journalism or scientific sum-
marization. Aumiller and Gertz (2022) create a
German dataset for joint summarization and simpli-
fication tasks for children or dyslexic readers from
the German children’s encyclopedia “Klexikon”.
Unfortunately, this dataset does not fit in our con-
text. Takeshita et al. (2022) construct a synthetic
dataset for cross-lingual extreme summarization
of scientific papers. The extreme summarization
task maps the abstract/content of a scientific paper
to the one-line summary, which is quite different
from thetask. Fatima and Strube (2021) col-
lect adataset from Wikipedia Science Portal
for the English-German language pair and a small
high-quality science magazine dataset from - . To the best of our knowledge, these scien-
tific datasets (Fatima and Strube, 2021) are the best
suitable option for our task.
3 Select, Simplify and Rewrite ()
3.1 Overview
The architecture ofconsists of three compo-
nents, , and . Fig-
ure 1 illustrates’s information flow among the
components. accepts English source text
as input and selects the most salient sentences of
the given text from different sections.
receives these selected sentences as its input andgenerates a linguistically simplified version of the
given input in English. Then these selected and
simplified sentences are passed to at the
encoder as an input, and the target summary of the
source text is given at the decoder as a reference.
Finally, generates a German output sum-
mary.
Plug-and-Play. We apply a divide-and-conquer ap-
proach to break down the task into manageable
components. We divide cross-lingual scientific
summarization into two further components: mono-
lingual scientific summarization and cross-lingual
abstractive summarization. Here we discuss the ra-
tionale behind it before discussing its components.
(1) Scientific Discourse. For the scientific text,
summarization models should include the salient
information in summary from all sections because
the pivotal content is spread over the entire text,
following an “hourglass” structure (see Figure A.1
in Appendix A). The existing models accept only
lead tokens from the source while discarding the
rest. Initially, the models were built with mostly
news datasets, which follow an “inverted pyramid”
structure, so this conventional method is reliable
for news but ineffective for scientific texts.
(2) Text length. The average length of scientific
texts is 4900 words in the ArXiv dataset, 3000
words in the PubMed dataset and 2337 words in the
Spektrum dataset (Fatima and Strube, 2021). Even
recently, there has been a significant gap between
the average and accepted input lengths by tradi-
tional models (max. 500 tokens) and pre-trained
models (max. 2048 tokens) such as ,,
etc. Longer texts often lead to model degradation
resulting in hallucination and factual inconsisten-
cies (Maynez et al., 2020). So, the recent language
models are still struggling to handle sizable docu-
ments (Jin et al., 2020).
We aim to deal with all these challenges by develop-
ingfor. With thearchitecture, we can
say thatis a proficient, adaptable and conve-
nient plug-and-play application where components
can be modified or exchanged without affecting the
information flow.
3.2 Architecture
3.2.1 Select in is responsible for selecting the
salient sentences from sections. We define the sec-
tion based on the structure of the text, e.g., intro-
duction, materials and methods, results, discussion,1845
and conclusion. We apply () (Dong
et al., 2021) as , which is a hierarchical
discourse model for scientific summarization. Here
we discuss the details of ().
Graph-based Ranking. It takes a document as a
graph, whereis the set of sentences
andis the set of relations between sentences. A
directed edge efrom sentence vto sentence v
is weighted by a (cosine) similarity score:
where fis an additional weight function.
Hierarchical Connections. A hierarchical graph
is created upon sections and sentences for intra-
sectional (local) and inter-sectional (global) hierar-
chies. The asymmetric edge weights are calculated
on the hierarchical graph. The asymmetric edge
weighting works on boundary functions at sentence
and section levels to find important sentences.
Similarity of Pairs. Before calculating asym-
metric edge weights over boundaries, a sentence-
sentence pair similarity sim(v, v)and a section-
sentence pair similarity sim(v, v)are computed
with cosine similarity with various vector represen-
tations. However, these similarity scores cannot
capture salience well, so asymmetric edge weights
are calculated and injected over intra-section and
inter-section connections.
Asymmetric edge weighting over sentences. To
find important sentences near the boundaries, a sen-
tence boundary function ( s) computes scores over
sentences ( v) in a section:
where nis the number of sentences in section
andxrepresents sentence iposition in the sec-tion.αis a hyper-parameter that controls the rel-
ative importance of the start or end of a section or
document. The sentence boundary function allows
integration of directionality in edges and weighing
edges differently based upon their occurrence with
a more/less important sentence in the same section
(see Appendix B.1).
Asymmetric edge weighting over sections. A
section boundary function ( d) computes the im-
portance of a section ( v) to reflect that sections
near a document’s boundaries are more important:
whereis the number of sections in the document
andxrepresents sectionposition in the docu-
ment. The section boundary function enables inject-
ing asymmetric edge weighting wsection edges
(see Appendix B.1). The boundary functions (1)
and (2) naturally prevent redundancy because sim-
ilar sentences have different boundary positional
scores.
Overall Importance. It is computed as the
weighted sum of local and global centrality scores
(see Appendix B.1) where µis an inter-section cen-
trality weighting factor.
Generation. A summary is generated by greedy
extraction of sentences with the highest importance
scores. These extracted sentences are then for-
warded to the next component in.
3.2.2 Simplify
The next component in the pipeline is- that aims to reduce the linguistic complexity
of the given text from . We adopt --1846 () (Laban et al., 2021) as , a
reinforcement learning syntactic and lexical simpli-
fication model. It has four components: simplicity,
fluency, salience and guardrails that are trained
together for the reward maximization. Here, we
discuss the components of ().
Simplicity. It is computed at syntactic and lexi-
cal levels: S is calculated by Flesch Kincaid
Grade Level ( ) with linear approximation, and
L is computed with the input paragraph ( W)
and the output paragraph ( W) as follows:
where(see Appendix B.2) is the aver-
age Zipf frequency of inserted and deleted words,
clipped between 0 and 1 (denoted as [·]), and cis
a median value to target Zipf shift in the L.
Fluency. It consists of a-based Language
Model () generator and a -based dis-
criminator. The fluency score is computed with a
likelihood of the original paragraph ( LM(p)) and
the generated output ( LM(q)):
where λis a trainable hyperparameter (see Ap-
pendix B.2). As LM is static and determinis-
tic, a dynamic discriminator is trained jointly with
the generator for the dynamic adaption of the flu-
ency score. The -based discriminator is a
classifier with two labels: 1=authentic paragraphs
and0 =generator outputs. The discriminator is
trained on the training buffer. The discriminator
score is computed on the probability that a para-
graph ( q) is authentic:
where Xdenotes the input and Yis the output
probability.
Salience. It is based on a transformer-based cover-
age model trained to look at the generated text and
answer fill-in-the-blank questions about the origi-
nal text. Its score is based on the model’s accuracy:
the more filled results in relevant content and the
higher score. All non-stop words are masked, as
the task expects most of the original text should be
recoverable.
Guardrails. The two guardrails - brevity and inac-
curacy are pattern-based binary scores to improve
the generation. The brevity ensures the similar
lengths of the original paragraph ( L) and gener-ated paragraph ( L). The brevity is defined as com-
pression: C=L/Lwhere the passing range
ofCisC≤C≤C. The inaccuracy is
a Named Entity Recognition () model for ex-
tracting entities from the original paragraph ( E)
and the output paragraph ( E). It triggers if an en-
tity present in Eis not in E.
Training. It trains on a variation of Self-Critical Se-
quence Training ( ) named k- , so the loss
is redefined for conditional generation probability:
where kis the number of sampled candidates, andanddenote the candidate and sampled mean
rewards, Pis the input paragraph and Nis the
number of generated words. All these components
are jointly optimized by using the product of all
components as the total reward. accepts the input from and
generates simplified text of that as its output. This
simplified text is then given to the next component.
3.2.3 Rewrite
The last component of is , which
is a cross-lingual abstractive summarizer. It ac-
cepts the output of at the encoder as
an input and the reference summary at the decoder
as a target. aims to learn cross-lingual
mappings and compression patterns to produce a
cross-lingual summary of the given text. We adopt
m (Liu et al., 2020) as , which con-
sists of 12 stacked layers at the encoder and de-
coder. Here we discuss three main components of (m ).
Self-attention. Every layer of the encoder and de-
coder has its own self-attention, consisting of keys,
values, and queries from the same sequence.
whereis a query,is transposed(key) andis the value. All parallel attentions are concate-
nated to generate multi-head attention scaled with
a weight matrix.
Cross-attention. The cross-attention is the atten-
tion between the encoder and decoder, which gives
the decoder a weight distribution at each step, in-
dicating the importance of each input token in the
current context.
Conditional Generation. The model accepts an1847input text x= (x,···, x)and generates a sum-
mary y=(y,···, y). The generation probability
ofyis conditioned on xand trainable parameters
θ:
3.3 Training
We train all models with Pytorch, Hugging Face
and Apex libraries. is a readily available
model, while and are trained
independently. .For, we initialize the--
medium model with the Adam optimizer at a learn-
ing rate of 10, a batch size of 4 and k= 4. We
initialize -base with the Adam optimizer
at a learning rate of 10and a batch size of 4. Themodel takes 14 days for training. .We fine-tune m -large-50 for a
maximum of 30 epochs. We use a batch size of 4, a
learning rate () of5e, and 100 warm-up steps
to avoid over-fitting the fine-tuned model. We use
the Adam optimizer ( beta= 0.9,beta= 0.99,
ϵ= 1e) withlinearly decayedscheduler.
During decoding, we use the maximum length of
200 tokens with a beam size of 4. The encoder lan-
guage is set to English, and the decoder language
is German. m takes 6 days for fine-tuning.
4 Experiments
4.1 Datasets is collected from the Wikipedia Sci-
ence Portal for English-German science arti-
cles (Fatima and Strube, 2021). It consists of mono-
lingual and cross-lingual parts. We use only the
cross-lingual part of this dataset. It contains 50,132
English articles ( 1572 words) paired with German
summaries ( 100words). is a high-quality real-world dataset col-
lected from Spektrum der Wissenschaft (Fatima
and Strube, 2021). It covers various topics in di-
verse science fields: astronomy, biology, chemistry,
archaeology, mathematics, physics, etc. It has 1510
English articles ( 2337 words) and German sum-
maries ( 361words).
We use with a split of 80-10-10 for ex-
periments, while is used for zero-shot
adaptability as a case study.4.2 Baselines
We define extractive and abstractive baselines with
diverse experimental settings: (1)four-
models: , ( ) (Mihalcea
and Tarau, 2004), (Nallapati et al., 2017),with - ()(Dong et al.,
2021), (2)three scratch-trainedmodels:
& attention-based sequence-to-sequence (),
pointer generator network (), transformer-
based encoder-decoder () (Fatima and Strube,
2021), and (3)three fine-tuned models: m(Xue
et al., 2021), m (Liu et al., 2020) and
LongFormer-based encoder-decoder () (Belt-
agy et al., 2020). The training parameters of all
baselines are discussed in Appendix C.
4.3 Metrics
We evaluate all models with three metrics: (1) (Lin, 2004) -andcompute the uni-
and bi-gram overlaps to assess the relevance , andcomputes the longest common sub-sequence be-
tween reference and system summaries to find the
fluency .(2) -score () (Zhang et al., 2020b)
captures faraway dependencies using contextual
embeddings to compute the relevance .(3)Flesch
Kincaid Reading Ease () (Kincaid et al., 1975)
computes text readability with the average sentence
length and the average number of syllables.
We also perform a human evaluation to compare and m outputs. Human evaluation of
long cross-lingual scientific text is quite challeng-
ing because it requires bi-lingual annotators with
some scientific background.
5 Wikipedia Results
All the results are the average of five runs for each
model. We report the-score of and,
and of all models on in Table 2.
The first block includes the- baselines,
the second and third blocks present directand
fine-tuned models, and the last block includes the
different variations ofmodels.
From Table 2, we find that all- mod-
els perform quite similarly considering ,
and. The extractive models select the sen-
tences from the original given text, due to which
these summaries can have linguistically complex
text (hard readability) as confirmed by their1848
scores.
For direct models in Table 2, performs
better than andfor ,and.
Interestingly,scores are similar to-
models. One reason behind the low scores for
andis that these models use restricted size vo-
cabulary, due to which <> tokens are present
in the outputs. Moreover, the model heav-
ily relies on the coverage of the given text, due to
which thescore is low.
For fine-tuned models in Table 2, m performs
the best in this group, m’s performance is also
good, however, performs quite low. We also
runwith 2048 tokens for the encoder, resulting
in much worse performance. We infer that longer
inputs of lead tokens are not helpful for scientific
summarization. These models produce easier read-
ability outputs except. As these models are
pre-trained with large-size datasets, we infer that
these models have latent simplification properties.
Comparing the performance of the best baseline
with our model from Table 2, outperforms
m by a wide margin for ,and.
We infer that transforming input texts by
and components helpslearn better
contextual representations.
We compute the statistical significance of the re-
sults with the Mann-Whitney two-tailed test for a
p-value ( p < .001) against the fine-tuned models.
These results indicate a significant improvement in
performance.
5.1 Component Analysis
Table 2 also shows the performance of ablated
models.+denotes the model without- , resulting in a significant decrease in perfor-
mance for and andas compared to
but maintaining the performance for.+
refers to the model without , also result-
ing in a notable drop in performance andas compared to, while showing similar per-
formance for. Overall, the completemodel
(last row) demonstrates that all three components
are necessary to generate good-quality simplified
cross-lingual stories.
Component Replacement. We also explore the
behavior ofby component replacement with
their counterparts.
For , we replacewith and- to compare their performances. Interest-
ingly, shows slightly higher performance
as compared to. We manually analyzed the out-
puts ofand . We find that themodel
(in some examples) changes the order of sentences
according to the importance score calculation of
the section. We infer that it is the reason for the
slightly low performance of. Overall, these re-
sults indicate the importance of .
For , we could not find any comparable
paragraph-based simplification model as a replace-
ment for.
For , we replace m with mand to compare their performances. Overall, the
performance of all models improves as compared to
fine-tuned models. However,performs higher
than mand.
In summary, these replacements demonstrate the1849
resilience and robustness ofwith intact infor-
mation flow.
6 Spektrum Results
Table 3 presents the-score of and, andof baselines andon (average
of 5 runs). The model performs quite well
on the set. We find a similar perfor-
mance pattern among the models for the - dataset. However, these results are lower
than those on the test set because these
models are trained on the training and
validation sets.
Table 3 shows the dataset results.
m performs best among the baselines. How-
ever,outperforms all the baselines. We test the
statistical significance of the results with the Mann-
Whitney two-tailed test for a p-value ( p < .001)
against the fine-tuned models. These results in-
dicate a significant improvement in performance.
These results exhibit the superior performance of.
6.1 Human Evaluation
We hired five annotators and provide them with 25
randomly selected outputs (of each model) fromand m with their original texts and gold
references. We asked the annotators to evaluate
each document for three linguistic properties on a
Likert scale from 1 to 5. The judges were asked
to rank the overall summary compared to the gold
summary (see Appendix D for the guidelines). The
first five samples were used for resolving the anno-
tator’s conflicts, while the rest of the annotations
were done independently.
We compute the average scores and inter-rater re-
liability using Krippendorff’s αover 20 samples,
excluding the first five examples. Table 4 presents
the results of human evaluation. We find that the outputs are significantly higher ranked than
m forfluency ,relevance ,simplicity and over-
all ranking.6.2 Readability Analysis
We further extend the readability analysis (Blaneck
et al., 2022) to investigate the similarities and differ-
ences between the references and outputs. For all
graphs, Text represents English documents, Gold
is German references,is m andis
outputs.
6.2.1 Lexical Diversity
Hypergeometric Distribution Diversity () (Mc-
Carthy and Jarvis, 2007) and Measure of Textual
Lexical Diversity ( ) (McCarthy, 2005) calcu-
late lexical richness with no impact of text length.
Figure 2 shows that gold summaries have higher
lexical diversity, while both system summaries are
slightly lower. These results indicate that the sys-
tem summaries are not as lexically diverse as the
gold references and are similar to the text.
6.2.2 Readability Index
Coleman Liau Index () computes the score us-
ing sentences and letters (Coleman and Liau, 1975).does not consider syllables for computing the
score. Linsear Write Formula () takes a sample
of 100 words and computes easy ( ≤2 syllables) and
hard words ( ≥3 syllables) scores (Plavén-Sigray
et al., 2017). In Figure 3,indicates that gold
and output summaries are difficult to read com-1850
pared to texts, and m outputs are the most
difficult. However, demonstrates that gold
andoutputs are the easiest among all. The
difference in results with andis due to the
difference in features used for calculation. Cumu-
latively, both scores indicate thatsummaries
are easier to read than texts.
6.2.3 Density Distribution
Word density () and sentence density () mea-
sure how much information is carried in a word
and a sentence. Word and sentence densities are
correlated and can be a language function. Fig-
ure 4 shows that m produces dense sentences,
while word densities ofare slightly higher. Sur-
prisingly, English texts have higher word density,
even though German is famous for its inflections
and compound words, suggesting that English texts
are harder to read.
6.3 Summary
We summarize the overall performance ofon
the dataset. The results of ,
and show thatoutperforms all the base-
lines for. We further investigate it with in-
depth analysis based on the human evaluation and
readability analysis that indicate the good linguis-
tic properties of outputs. We present some
random example outputs of and m in
Appendix E.
7 Conclusions
We propose to study Cross-lingual Science Jour-
nalism () as a downstream task of text simplifi-
cation and cross-lingual scientific summarization.
Automatingaims to produce popular cross-
lingual summaries of English scientific texts for
non-expert readers. We develop a pipeline compris-
ing the three components - ,
and () as a benchmark for. Our
empirical evaluation shows thatoutperforms
all baselines by wide margins on and
achieves good performance on . We
further explore the ablated models with compo-
nent replacements, demonstrating the resilience
and robustness of the application. We con-
duct a human evaluation of the outputs,
indicating its good linguistic properties, further af-
firmed by readability analysis. We plan for joint
training of and models foras future work.
8 Limitations
We investigatedwith , and . We adopted as be-1851cause it is a lightweight, unsupervised model that
extracts a summary in a discourse-aware manner.
However, when we replaced it with other extractive
models during the component analysis, we found
no significant difference in overall performance.
We adopted -- for be-
cause it facilitates paragraph simplification. We
found the model is quite heavy, making it slow dur-
ing training. To the best of our knowledge, there is
no paragraph-based simplification model we could
explore in component replacement.
The choice among various pre-trained models for was quite challenging, as all these mod-
els are variations of transformer-based architec-
tures. So we adopted the latest three mod-
els, which are efficient and effective summariza-
tion models. We also trained the vanilla sequence-
to-sequence model, pointer-generator model and
transformer as our baselines to provide sufficient
variations of models. We found m is
more promising performance-wise in our experi-
ments. However, its training time is also slow for
our datasets due to longer inputs.
9 Ethical Consideration
Reproducibility. We discussed all relevant param-
eters, training details, and hardware information in
§ 3.3.
Performance Validity. We proposed an innovative
application, , and ,
for the Cross-lingual Science Journalism task and
verified its performance for and - data for the English-German language pair.
We believe this application is adaptable for other
domains and languages; however, we have not ver-
ified this experimentally and limit our results to
the English-German language pair for the scientific
domain.
Legal Consent. We explored the
dataset with their legal consent for our experi-
ments. We adopted the public implementations
with mostly recommended settings, wherever ap-
plicable.
Human Evaluation. We published a job on the
Heidelberg University Job Portal with the task
description, requirements, implications, working
hours, wage per hour and location. We hired five an-
notators from Heidelberg University who are native
Germans, fluent in English and master’s or bach-
elor’s science students. The selected students for
the evaluation task submitted their consent whileagreeing to the job. We compensated them at C15
per hour, while the minimum student wage ranges
between C9.5−12in 2022 according to German
law.
Acknowledgements
We would like to thank the former editor-in-chief
of , Carsten Könneker, for suggesting
us to work on. We thank for giv-
ing us access to their German summaries. We
thank the anonymous reviewers for their construc-
tive feedback and suggestions. We also thank
Carolin Robert, Caja Catherina, Pascal Timmann,
Samuel Scherer and Sophia Annweiler from Hei-
delberg University for their human judgments. This
work has been carried out at Heidelberg Institute
for Theoretical Studies (HITS) [supported by the
Klaus Tschira Foundation], Heidelberg, Germany,
under the collaborative Ph.D. scholarship scheme
between the Higher Education Commission of Pak-
istan (HEC) and Deutscher Akademischer Aus-
tausch Dienst (DAAD). The first author has been
supported by HITS and HEC-DAAD.
References185218531854
A Scientific and News Structure
Figure A.1 presents the difference between a scien-
tific text discourse and a news text discourse.
B Select, Simplify and Rewrite ()
B.1 Select
Asymmetric edge weighting over sentences. The
weight wfor intra-section edges (incoming edges
fori) is defined as:
wherefor an edgeoccurs withis
weighted more ifis closer to the text boundarythan. The sec-
tion boundary function enables injecting asymmet-
ric edge weightingsection edges:
wherefor an edgeoccurs tois
weighted more if sectionis closer to the text
boundary than section. It is computed as the weighted
sum of local and global centrality scores.
whereis the neighboring sentences set of,
is the neighboring sections set, andis an inter-
section centrality weighting factor.
B.2 Simplifyis computed as the aver-
age Zipf frequency of inserted words and deleted
words: If thebyor more, . If, then , otherwise it is a linear interpo-
lation.
C Baselines: Training
C.1-
We create the pipeline ( ) for
extractive baselines withfor translation wher-
ever required. There is no training required for
extractive models andfor these models.
C.2
We train three models -, andfrom
scratch without any pre-training (Fatima and
Strube, 2021). Forand models, we use
word embeddings with 128 dimensions and hidden
layers with 256 dimensions. The vocabulary size
is kept to 100and 50at the encoder and decoder
sides. We use the Adam optimizer with a learning
rate of 0.15 and a mini-batch of size 16. The mod-
els are trained for 30 epochs with early stopping
on the validation loss, and the validation loss is
calculated to determine the best-trained model.1855Themodel consists of 6 layers stacked encoder
and 8 multi-attention heads at the decoder. We use
word embeddings with 512 dimensions and hidden
layers with 786 dimensions. The vocabulary size is
kept the same as forand,., 100at the
encoder and 50at the decoder. We use the Adam
optimizer with a learning rate of 0.0001 and with a
residual dropout of 0.1.
For all these models, we use a fixed input length
of 400 (lead) tokens and an output length of 100
tokens, with a beam search of size 4 during the
inference as in Fatima and Strube (2021). We train
all these models on a single Tesla with . For training and inference, theandmodels take around 6 days, and themodel
takes 3 days.
C.3 -
We fine-tune three pre-trained models - m-base,
m -large-50 andon the dataset.
We train these models for a maximum of 30 epochs
with a batch size of 4. We use a learning rate ()
ofand 100 warm-up steps to avoid over-fitting
of the fine-tuned models. We use the Adam opti-
mizer with alinearly decayedscheduler. The
encoder language is set to English, and the decoder
language is German. The input to the encoder is
the first (lead) 1024 tokens of each document. Dur-
ing decoding, we use the maximum length of 200
tokens with a beam size of 4. Each model of m-
base takes 4 days, and m -large-50 takes 6 days
for fine-tuning on a single Teslawith
memory.
D Guidelines for Human Evaluation
D.1 Task Description
We present annotators with 25 examples of docu-
ments paired with a reference summary and two
system-generated summaries. The models’ iden-
tities are hidden. The annotators were asked to
evaluate each model summary for the following
linguistic features after reading the original En-
glish text. The annotators were given a Likert scale
from(1=worst, 2=bad, 3=neutral/ok, 4=good,
5=best). They were asked to use the first 5 exam-
ples to resolve the annotator’s conflict, while the
rest examples were to be evaluated independently.
D.2 Linguistic Features
We asked annotators to evaluate each summary for
the following features. A summary delivers adequate informa-
tion about the original text. Relevance determines
the content relevancy of the summary. The words and phrases fit together within
a sentence, and so do the sentences. Fluency deter-
mines the structural and grammatical properties of
a summary. Lexical (word) and syntactic (syntax)
simplicity of sentences. A simple summary should
have minimal use of complex words/phrases and
sentence structure. Compared with reference sum-
maries, how is the overall coherence of each
model’s summary?1856E Examples from the dataset
We mark wrong words or sentences with red and unfaithful information with blue.185718581859ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
8
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3, 4
/squareB1. Did you cite the creators of artifacts you used?
3, 4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
4
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
4
C/squareDid you run computational experiments?
4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
3, 41860/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Not applicable. Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
5, 6
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
3, 4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
6, 9
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
6,D
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
9
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.1861