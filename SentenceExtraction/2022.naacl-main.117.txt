
Xiaobing Sun and Wei Lu
StatNLP Research Group
Singapore University of Technology and Design
xiaobing_sun@mymail.sutd.edu.sg, luwei@sutd.edu.sg
Abstract
Although self-attention based models such as
Transformers have achieved remarkable suc-
cesses on natural language processing (NLP)
tasks, recent studies reveal that they have
limitations on modeling sequential transfor-
mations (Hahn, 2020), which may prompt
re-examinations of recurrent neural networks
(RNNs) that demonstrated impressive results
on handling sequential data. Despite many
prior attempts to interpret RNNs, their internal
mechanisms have not been fully understood,
and the question on how exactly they capture
sequential features remains largely unclear. In
this work, we present a study that shows there
actually exist some explainable components
that reside within the hidden states, which are
reminiscent of the classical n-grams features.
We evaluated such extracted explainable fea-
tures from trained RNNs on downstream sen-
timent analysis tasks and found they could be
used to model interesting linguistic phenomena
such as negation and intensification. Further-
more, we examined the efficacy of using such
n-gram components alone as encoders on tasks
such as sentiment analysis and language model-
ing, revealing they could be playing important
roles in contributing to the overall performance
of RNNs. We hope our findings could add in-
terpretability to RNN architectures, and also
provide inspirations for proposing new archi-
tectures for sequential data.
1 Introduction
Modern recurrent neural networks (RNNs), includ-
ing Long Short-Term Memory (LSTM) (Hochre-
iter and Schmidhuber, 1997) and Gated Recurrent
Units (GRU) (Cho et al., 2014), have demonstrated
impressive results on tasks involving sequential
data. They have proven to be capable of modeling
formal languages (Weiss et al., 2018; Merrill, 2019;
Merrill et al., 2020) and capturing structural fea-
tures (Li et al., 2015a,b, 2016; Linzen et al., 2016;
Belinkov et al., 2017; Liu et al., 2019) on NLP
tasks. Although Transformers (Vaswani et al.,
2017) have achieved remarkable performances onFigure 1: An RNN hidden state may encode a linear
combination of all the n-grams ending at the current
time step.
NLP tasks such as machine translation, it is argued
that they may have limitations on modeling hierar-
chical structure (Tran et al., 2018; Hahn, 2020) and
cannot handle functions requiring sequential pro-
cessing of input well (Dehghani et al., 2019; Hao
et al., 2019; Bhattamishra et al., 2020; Yao et al.,
2021). Furthermore, a recent work shows that com-
bining recurrence and attention (Lei, 2021) can
result in strong modeling capacity. Another recent
work incorporating recurrent cells into Transform-
ers (Hutchins et al., 2022) substantially improved
performance on language modeling involving very
long sequences, prompting re-investigations of
RNNs. On the other hand, it was observed in prior
work that RNNs were able to capture linguistic
phenomena such as negation and intensification
(Li et al., 2016), but the question why they could
achieve so still largely remains unanswered.
In this work, we focus on better understand-
ing RNNs from a more theoretical perspective.
We demonstrate that the recurrence mechanism of
RNNs may induce a linear combination of inter-
pretable components. These components reside
in their hidden states in the form of the iterated
matrix-vector multiplication that is based on the
representations of tokens in the (reverse) order they
appear in the sequence. Such components, solely
depending on inputs and learned parameters, can
be conveniently interpreted and are reminiscent of
those compositional features used in classical n-
gram models (Jurafsky and Martin, 2009). They
may also provide us with insights on how RNNs
compose semantics from basic linguistic units. Our
analysis further shows that, the hidden state at each
time step includes a weighted combination of com-
ponents that represent all the “ n-grams” ending
at that specific position in the sequence as shown1624in Figure 1. We gave specific representations for
then-gram components in Elman RNNs (Elman,
1990), GRUs and LSTMs.
We investigated the interpretability of those n-
gram components on trained RNN models, and
found they could explain phenomena such as nega-
tion and intensification and reflect the overall polar-
ity on downstream sentiment analysis tasks, where
such linguistic phenomena are prevalent. Our ex-
periments also revealed that the GRU and LSTM
models are able to yield better capabilities in mod-
eling such linguistic phenomena than the Elman
RNN model, partly attributed to the gating mech-
anisms they employed which resulted in more ex-
pressive n-gram components. We further show that
the linear combination of such components yields
effective context representations. We explored the
effectiveness of such n-gram components (along
with the corresponding context representations) as
alternatives to standard RNNs, and found they can
generally yield better results than the baseline com-
positional methods on several tasks, including senti-
ment analysis, relation classification, named entity
recognition, and language modeling.
We hope that our work could give inspirations
to our community, serving as a useful step towards
proposing new architectures for capturing contex-
tual information within sequences.
2 Related Work
Interpretability of RNNs: A line of work fo-
cuses on the relationship between RNNs and finite-
state machines (Weiss et al., 2018; Merrill, 2019;
Suzgun et al., 2019; Merrill et al., 2020; Eyraud
and Ayache, 2020; Rabusseau et al., 2019), pro-
viding explanation and prediction on the expres-
sive power and limitations of RNNs on formal lan-
guages both empirically and theoretically. Kanai
et al. (2017) investigated conditions that could pre-
vent gradient explosions for GRU based on dy-
namics. Maheswaranathan et al. (2019) and Ma-
heswaranathan and Sussillo (2020) linearized the
dynamics of RNNs around fixed points of hidden
states and elucidated contextual processing. Our
work focuses on studying a possible mechanism of
RNNs that handles exact linguistic features.
Another line of work aims to detect linguistic fea-
tures captured by RNNs. Visualization approaches
(Karpathy et al., 2015; Li et al., 2016) were ini-
tially used to examine compositional information
in RNN outputs. Linzen et al. (2016) assessed
LSTMs’ ability to learn syntactic structure andEmami et al. (2021) gave rigorous explanations on
the standard RNNs’ ability to capture long-range
dependencies. Decomposition methods (Murdoch
and Szlam, 2017; Murdoch et al., 2018; Singh et al.,
2019; Arras et al., 2017, 2019; Chen et al., 2020)
were proposed to produce importance scores for hi-
erarchical interactions in RNN outputs. Our work
can be viewed as an investigation on how those
interaction came about.
Compositional Models: A variety of compo-
sitional functions based on vector spaces have
been proposed in the literature to compose seman-
tic meanings of phrases, including simple com-
positions of adjective-noun phrases represented
as matrix-vector multiplication (Mitchell and La-
pata, 2008; Baroni and Zamparelli, 2010) and
a matrix-space model (Rudolph and Giesbrecht,
2010; Yessenalina and Cardie, 2011) based on ma-
trix multiplication. Socher et al. (2012, 2013) in-
troduced a recursive neural network model that
assigns every word and longer phrase in a parse
tree both a vector and a matrix, and represents com-
position of a non-terminal node with matrix-vector
multiplication. Kalchbrenner and Blunsom (2013)
employed convolutional and recurrent neural net-
works to model compositionality at the sentence
and discourse levels respectively. Those models are
designed in an intuitive manner based on the nature
of languages thus being interpretable. We can show
that RNNs may process contextual information in
a way bearing a resemblance to those early models.
3 A Theory on N-gram Representation
First, let us spend some time to discuss how to rep-
resent n-grams. Various approaches to represent-
ingn-grams have been proposed in the literature
(Mitchell and Lapata, 2008; Bengio et al., 2003;
Mitchell and Lapata, 2008; Mnih and Teh, 2012;
Ganguli et al., 2008; Orhan and Pitkow, 2020;
Emami et al., 2021; Rudolph and Giesbrecht, 2010;
Yessenalina and Cardie, 2011; Baroni and Zam-
parelli, 2010). We summarize in Table 1 different
approaches for representing n-grams.
Although empirically it has been shown that dif-
ferent approaches can lead to different levels of ef-
fectiveness, the rationales underlying many of the
design choices remain unclear. In this section, we
establish a small theory on representing n-grams,
which leads to a new formulation on capturing the
semantic information within n-grams.
Let us assume we have a vocabulary Vthat con-
sists of all possible word tokens. The set of n-
grams can be denoted as V(including the special1625
n-gram which is the empty string ϵ). Consider
three n-grams a,b, and cfromV, with their se-
mantic representations r(a),r(b), andr(c)respec-
tively. Similarly, we may have r(ab)which return
the semantic representations of the concatenated
n-grams ab. It is desirable for our representations
to be compositional in some sense. Specifically, a
longer n-gram may be semantically related to those
shorter n-grams it contains in some way.
Under some mild compositional assumptions re-
lated to the principle of compositionality (Frege,
1948), it is reasonable to expect that there exists
some sort of rule or operation that allows us to com-
pose semantics of longer n-grams out of shorter
ones. Let us use ⊗to denote such an operation.
We believe a good representation system for n-
grams shall satisfy several key properties. First, the
semantics of the n-gram abcshall be determined
through either combining the semantics of the two
n-grams aandbcor through combining the seman-
tics of abandc. The semantics of abcis unique,
regardless of which of these two ways we use. Sec-
ond, for the empty string ϵ, it should not convey any
semantics. Formally, we can write them as:
•Associativity :∀a, b, c∈V,(r(a)⊗r(b))⊗
r(c) =r(a)⊗(r(b)⊗r(c))
•Identity :∀a∈V,r(a)⊗r(ϵ) =r(a), and
r(ϵ)⊗r(a) =r(a)
This essentially shows that the representation
space for all n-grams under the operation ⊗, de-
noted as (V,⊗), forms a monoid , an impor-
tant concept in abstract algebra (Lallement, 1979),
with significance in theoretical computer science(Meseguer and Montanari, 1990; Rozenberg and
Salomaa, 2012).
On the other hand, it can be easily verified that
the space of all d×d(where dis an integer) real
square matrices under matrix multiplication, de-
noted as (R,·), also strictly forms a monoid
(i.e., it is associative and has an identity, but is
notcommutative). We can therefore establish a
homomorphism from VtoR, resulting in the
function r(·)∈V→R.
This essentially means that we may be able to
rely on a sub-space within Ras our mathemati-
cal object to represent the space of n-grams, where
the matrix multiplication operation can be used to
compose representations for longer n-grams from
shorter ones. Thus, for a unigram x(a single word
in the vocabulary), we have:
where A∈Ris the representation for the
word x(how to learn such a matrix is a separate
question to be discussed later). Note that the empty
string ϵcomes with a unique representation which
is the d×didentity matrix I.
We can either use matrix left-multiplication or
right-multiplication as our operator ⊗. Assume the
language under consideration employs the left-to-
right writing system. It is reasonable to believe that
a human reader processes the text left-to-right, and
the semantics of the text gets evolved each time
the reader sees a new word. We may use the ma-
trix left-multiplication as the preferred operator in
this case. The system will left-multiply (modify)
an existing n-gram representation with a matrix
associated with the new word that appears right af-
ter the existing n-gram, forming the representation
of the new n-gram. Such an operation essentially
performs a transform that simulates the process
of yielding new semantics when appending a new
word at the end of an existing phrase. With this, for1626a general n-gram x, x, . . . , x(i≤t), we have:
However, the conventional wisdom in NLP has
been to use vectors to represent basic linguistic
units such as words, phrases or sentences (Mikolov
et al., 2013a,b; Pennington et al., 2014; Kiros et al.,
2015). This can be achieved by a transform:
where u∈Ris a vector that maps the resulting
matrix representation into a vector representation.
Next, we will embark on our journey to examine
the internal representations of RNNs. As we will
see, interestingly, our developed n-gram represen-
tations can emerge within such models.
4 Interpretable Components in RNNs
An RNN is a parameterized function whose hidden
state can be written recursively as:
where xis the input token at time step tand
h∈Ris the previous hidden state. Assume
fis differentiable at any point, with the Taylor
expansion, hcan be rewritten as:
where ∇f(x,0) =|is the Jacobian
matrix, and ois the remainder of the Taylor series.
Letg(x) =f(x,0)andA(x) =∇f(x,0).
Note that g(x)∈RandA(x)∈Rare both
functions of x. Therefore, the equation above can
be written as:
If the hidden state has a sufficiently small norm,
it can be approximated by the first-order Taylor
expansion as follows:
Next we illustrate how this recurrence relation
can help us identify some salient components.4.1 Emergence of N-grams
Consider the simplified RNN with the following
recurrence relation,
whereh∈R, andg(x)∈RandA(x)∈R
are functions of x. This recurrence relation can be
expanded repeatedly as follows,
We can see that vbear some resemblance to
the term in Equation 3, which can be rewritten as:
With the definition A(x) :=Aandg(x) :=
A(x)u, we can see vcan be interpreted as an
“n-gram representation” that we developed in the
previous section. It is important to note that, how-
ever, the use of function g(x)in RNNs may lead
to greater expressive power than the original for-
mulation based on Au.
This interesting result shows that the hidden state
of a simple RNN (characterized by Equation 8) is
the sum of the representations of all the n-grams
ending at time step t. Such salient components
within RNN also show that the standard RNN may
actually have a mechanism that is able to capture
implicit n-gram information as described above.
This leads to the following definition:
Definition 1 ( N-gram Representation) For the
n-gram x, x, . . . , x, its representation is:
where A(x)∈Randg(x)∈R.
4.2 Context Representation
With the above definition, we may want to consider
how to perform learning. The learning task in-
volves identifying the functions Aandg– in other
words, learning representations for word tokens.
A typical learning setup that we may consider
here is the task of language modeling. Such a task1627can be defined as predicting the next word x
based on the representation of preceding words
x, x, . . . , xwhich serves as its left context. This
is an unsupervised learning task, where the un-
derlying assumption involved is the distributional
hypothesis (Harris, 1954). Specifically, the model
learns how to “reconstruct” the current word x
out of x, x, . . . , xwhich serves as its context.
Now the research question is how to define the
representation for this specific context. As this left
context is also an n-gram, it might be tempting
to directly use its n-gram representation defined
above to characterize such a left context. However,
we show such an approach is not desirable.
Then-gram representation for this context can
be written in the following alternative form:
This shows that the n-gram representation of
x, x, . . . , xcould be interpreted as a “weighted”
representation of the word x(where the weight
matrix is derived from the words between xand
x, measuring the strength of the connection be-
tween them). However, ideally, the context repre-
sentation shall not just take xbut other adjacent
words preceding xinto account, where each
word contributes towards the final context represen-
tation based on the connection between them. This
leads to the following way of defining the context:
In fact, such an idea of defining the context as
a weighted combination of surrounding words is
not new – it recurs in the literature of language
modeling (Bengio et al., 2003; Mnih and Teh,
2012), word embedding learning (Mikolov et al.,
2013a,b), and graph representation learning (Cao
et al., 2016).
Interestingly, the hidden states in the RNNs, as
shown in Equation 9, also suggest exactly the same
way of defining this left context. Indeed, when
using RNNs for language modeling, each hidden
state is exactly serving as the context representation
for predicting the next word in the sequence.
The above gives rise to the following definition:
Definition 2 (Context Representation) For the
n-gram x, x, . . . , x, its representation when
serving as the (left) context is:
where A(x)∈Randg(x)∈R.4.3 Model Parameterization
With the above understandings on such salient com-
ponents within RNNs, we can now look into how
different variants of RNNs parameterize the func-
tionsAandg. The definition of Elman RNN, GRU
and LSTM together with the corresponding Jaco-
bian matrix A(x)and vector function g(x)func-
tions are listed in Table 2. We discuss how such
different parameterizations may lead to different
expressive power when they are used in practice.
We can see the ways GRU or LSTM parameter-
izeA(x)andg(x)appear to be more complex
compared to Elman RNN. This can partially be
attributed to their gating mechanisms. Although
the original main motivation of introducing such
mechanisms may be to alleviate the exploding gra-
dient and vanishing gradient issues (Hochreiter and
Schmidhuber, 1997; Cho et al., 2014), we could
see such designs also result in terms describing
gates and intermediate representations. Aandg
are then independently derived based on certain
rich interactions between such terms. We believe
such interactions may likely increase the expressive
power of the resulting n-gram representations. We
will validate these points and discuss more in our
experiments.
5 Experiments
In our experiments, we focus on the following as-
pects: 1) understanding the effectiveness of the pro-
posed n-gram (and context) representations when
used in practice, as compared to baseline models;
2) examining the significance of the choice of con-
text representation; 3) interpreting the proposed
representations by examining how well they could
be used to capture certain linguistic phenomena.
We employ the sentiment analysis, relation clas-
sification, named entity recognition (NER) and lan-
guage modeling tasks as testbeds. The first task
is often used in investigating n-gram phenomena
(Yessenalina and Cardie, 2011; Li et al., 2016)
while the others are often used in examining how
capable an encoder is when extracting features
from texts (Grave et al., 2018; Zhou et al., 2016;
Lample et al., 2016).
Datasets For sentiment analysis, we considered
the Stanford Sentiment Treebank (SST) (Socher
et al., 2013), the IMDB (Maas et al., 2011), and
the AG-news topic classification(Zhang et al.,1628
2015) datasets. The first dataset has sufficient la-
bels for phrase-level analysis, the second dataset
has instances with relatively longer lengths, and the
third one is multi-class. For relation classification
and NER, we considered the SemEval 2010 Task 8
(Hendrickx et al., 2010) and CoNLL-2003 (Tjong
Kim Sang and De Meulder, 2003) datasets respec-
tively. For language modeling, we considered the
Penn Treebank (PTB) dataset (Marcus et al., 1993),
the Wikitext-2 (Wiki2) dataset and the Wikitext-
103 (Wiki103) dataset (Merity et al., 2016). PTB is
relatively small while Wiki103 is large. The statis-
tics are shown in Tables 6 and 7 in the appendix.
Baselines Then-gram representations (together
with their corresponding context representations)
discussed in the literature are considered as base-
lines, which are listed in Table 1 along with the
MVMA and MVM models. MVM(A)-G/L/E refers
to the MVM(A) model created with the Aandg
functions derived from GRU/LSTM/Elman, but are
trained directly from data. The Aandgfunctions
for GRU, LSTM and Elman are listed in Table 2.
Additionally, to understand whether the com-
plexity of Aaffects the expressive power, we
created a new model called MVMA-ME, which
comes with an Afunction that is slightly more
complex than that of MVMA-E but less complex
than those of MVMA-G and MVMA-L: A(x)=
0.25 diag[tanh( Wx)]M+ 0.5Iandg(x) =
tanh(Wx)(here,W,MandWare learnable
weight matrices). The gfunction is the same as
that of MVMA-E.
Setup For sentiment analysis, relation classifica-
tion and language modeling, models consist of one
embedding layer, one RNN layer, and one fully-
connected layer. The Adagrad optimizer (Duchi
et al., 2011) was used along with dropout (Srivas-
tava et al., 2014) for sentiment analysisand rela-
tion classification. For language modeling, models
were trained with the Adam optimizer (Kingma
and Ba, 2014). We ran word-level models with
truncated backpropagation through time (Williams
and Peng, 1990) where the truncated length was set
to 35. Adaptive softmax (Joulin et al., 2017) was
used for Wiki103. For NER, models consist of one
embedding layer, one bidirectional RNN layer, one
projection layer and one conditional random field
(CRF) layer. The SGD optimizer was used. Final
models were chosen based on the best validation
results. More implementation details can be found
in the appendix.
5.1 Comparison of Representation Models
We investigate how baseline n-gram representation
models, the MVM model, and the MVMA model
perform on the aforementioned testbeds. We also
compare with the standard RNN models.
Sentiment Analysis Apart from the GRU and
LSTM models, it can be observed that our MVMA-
G and MVMA-L models are also able to achieve
competitive results on three sentiment analysis
datasets, as we can see from Table 3, demonstrating
the efficacy of those recurrence-induced n-gram1629representations. Although Elman RNN and its cor-
responding MVMA-E and MVM-E models also
have a mechanism for capturing n-gram informa-
tion (similar to GRU and LSTM), they did not per-
form well, which may be attributed to a limited
expressive power of their Aandgfunctions when
used for defining n-grams as described previously.
Both MM and V A-EW fail to converge on AG-
news and IMDB, showing challenges for them to
handle long instances. This may be explained
by the lengthy matrix multiplication involved in
their representations, which may result in vanish-
ing/exploding gradient issues. Interestingly, MVM-
G and MVM-L, which solely rely on the longest n-
gram representation, are also able to achieve good
results on SST-2, indicating a reasonable expres-
sive power of such n-gram representations alone.
However, they fail to catch up with MVMA-G
and MVMA-L on IMDB which contains much
longer instances, confirming the significance of
the context representation, which captures n-grams
of varying lengths.
Unlike MVMA-E, the MVMA-ME model does
not suffer from loss stagnation on AG-news and
IMDB but the performance on IMDB obviously
falls behind MVMA-G and MVMA-L as shown
in Table 3. This indicates a sufficiently expressive
A(x)(such as the Jacobian matrices of GRU and
LSTM) may be needed to handle long instances.
Relation Classification & NER For relation clas-
sification, context representations (or final hidden
states) are used for classification. For NER, we use
the concatenated context representations (or hid-
den states) at each position of bidirectional mod-
els to predict entities and their types. Table 4
shows that MVMA-G and MVMA-L outperform
the MVM-G and MVM-L models respectively on
both tasks, again confirming the effectiveness of
the context representations. MVM(A)-E did not
perform as well as MVM(A)-G and MVM(A)-L,
which demonstrates the significance of expressive
power for the Aandgfunctions. Similar to the
results in sentiment analysis, MVMA-ME did not
perform as well as MVMA-G and MVMA-L. How-
ever, to our surprise, MVMA-ME did not outper-
form V A-EW on NER, suggesting that a delicate
choice of Acan be important for this task. The poor
performance of V A-W on NER might be explained
by a weak expressive power of its n-gram represen-
tations. MM fails to converge on the relation clas-
sification task, which implies it is not robust across
different datasets. Interestingly, it is remarkable
that MVMA-G, MVMA-L and MVMA-E could
yield competitive results compared to GRU, LSTM
and Elman on NER, implying such n-gram repre-
sentations could be crucial for our NER task.
Language Modeling For the language modeling
task, we choose MVMA-G, MVMA-L, MVM-G
and MVM-L for experiments. We also run MVMA-
ME. As we can see from Table 5, there are perfor-
mance gaps between the MVMA models and the
standard RNNs – though the gaps often do not ap-
pear to be particularly large. This indicates there
may be extra information within higher order terms
of the standard RNN functions useful for such a
task. Yet, such information cannot be captured by
the MVMA models that employ simplified func-
tions. The gaps between the MVM models and
MVMA models are remarkable, which again in-
dicates that the correct way of defining the left
context representation can be crucial for the task
of next word prediction. MVMA-ME did not per-
form well on the language modeling task, which
might be attributed to the less expressive power of
its functions Aandg.
5.2 Interpretation Analysis
We conduct some further analysis to examine the
interpretability of n-gram representations. Specif-
ically, we examine whether the models are able
to capture certain linguistic phenomena such as
negation, which is important for sentiment anal-
ysis (Ribeiro et al., 2020). We also additionally1630made comparisons with the vanilla Transformer
(Vaswani et al., 2017) heredespite the fact that it
remains largely unclear how it precisely captures
sequence features such as n-grams.
We could also obtain the n-gram representations
and the corresponding context representations from
the learned standard RNN models, based on their
learned parameters. We denote such n-gram repre-
sentations as RNN, and the context represen-
tations as RNN , where “RNN” can be GRU,
LSTM or Elman. As n-gram representations are
vectors, a common approach is to transform them
into scalars with learnable parameters (Murdoch
et al., 2018; Sun and Lu, 2020). We define the
n-gram polarity score to quantify the polarity in-
formation as captured by an n-gram representation
vfrom time step itot, which is calculated as:
where wis the learnable weight vector of the final
fully-connected layer. We also define the context
polarity score for the context as/summationtexts.
We trained RNNs and baseline models on SST-2
and automatically extracted 73 positive adjectives
(e.g., “ nice” and “ enjoyable ”) and 47 negative ad-
jectives (e.g., “ bad” and “ tedious ”) from the vocab-
ulary.N-gram polarity scores were calculated
for those adjective unigrams and their negation bi-
grams formed by prepending “ not” to them. For
V A-EW and V A-W, their n-gram representations
do not involve tokens other than the last token.
Such limitations prevent them from capturing any
negation information. We therefore calculate the
context polarity scores using their context represen-
tations instead (which in this case is a bigram). This
also applies to Transformer for the same reason.
We observed that, for the GRU and LSTM mod-
els along with their corresponding MVMA models,
then-gram representations are generally able to
learn the negation for both the adjective and their
negation bigrams as shown in Figures 2a and 2b,
prepending “ not” to an adjective will likely reverse
the polarity. This might be a reason why they could
achieve relatively higher accuracy on the sentiment
analysis tasks. Interestingly, MVM-G could also
capture negation as shown in Figure 2c, again sug-
gesting the impressive expressive power of such
n-gram representations alone.However, as shown in Figure 2, models such
as V A-W, MVMA-E, and MM are struggling to
capture negation for negative adjectives, again im-
plying a weaker expressive power of their n-gram
representations. Specifically, MVMA-E fails to
capture negation for negative adjectives, which may
be attributed to a relatively weaker Jacobian ma-
trix function A(as compared to those of GRU and
LSTM) preventing them from pursuing optimal
conditions.
Figure 2e shows that the MVMA-ME model,
which has a function Aless complex than the ones
from MVMA-G and MVMA-L but more complex
than the one from MVMA-E, still can generally
learn negation of negative adjectives better than the
MVMA-E model. This demonstrates the necessity
of choosing more expressive Aandgfunctions.
Interestingly, both V A-W and Transformer are
struggling with capturing the negation phenomenon
for negative adjectives in our experiments as shown
in Figures 2g and 2h, which suggests that they may
have a weaker capability in modeling sequential
features in our setup. However, we found they
could still achieve good performances on the AG-
news and IMDB datasets. We hypothesize this
is because the nature of SST-2 makes these two
models suffer more on this dataset – it has rich
linguistic phenomena such as negation cases while
the other two datasets do not.
Additionally, we examined the ability for GRU,
LSTM, MVMA-G and MVMA-L to capture both
the negation and intensification phenomena. For
such experiments, instead of using SST-2, we
trained the models on SST-5, which comes with
polarity intensity information. Polarity intensities
were mapped into values of {−2,−1,0,+1,+2},
ranging from extremely negative toextremely posi-
tive. We conducted some experiments based on the
same setup above for capturing negation on SST-2.
To our surprise, our preliminary results show that
all models were performing substantially worse in
terms of capturing intensification than capturing
negations. We hypothesize that this is caused by
the imbalance between negation phrases and inten-
sification phrases. Specifically, the intensification
word “very” (1,729 times) was exposed less than
the negation word “not” (4,601 times) in the train-
ing set of SST-5.
One approach proposed in the literature for sen-
tence classification is to consider all the hidden
states of an RNN in an instance (Bahdanau et al.,
2015). We believe this may actually be able to al-1631
leviate the above issue as it allows more n-grams
within an instance to be exposed to the label in-
formation. Thus, we followed their approach for
training our MVMA and MVM models.
We can see that the negation and intensification
phenomena can be explained by both the context
representations in Figure 3. Specifically, prepend-
ing either positive or negative adjectives with “ very”
will likely strengthen their polarity while adding
“not” will likely weaken their polarity. These results
suggest that RNNs are able to capture information
of linguistic significance within the sequence, and
our identified n-gram representations within their
hidden states appear to be playing a salient role.
5.3 Discussion
From the experiments above, we can see that our
introduced n-gram representations, coupled with
the corresponding context representations, are pow-
erful in practice in capturing n-gram information
better than the baseline compositional models intro-
duced in the literature. We also found that RNNs
can induce such representations due to their recur-
rence mechanism.
However, there can be several factors that af-
fect the efficacy of different representations. First,
through comparisons with different variants ofMVMA, we can see that the precise way of parame-
terizing the functions A(x)andg(x)matter. Sec-
ond, through the comparison between MVMA and
MVM, we can see that defining an appropriate con-
text representation that incorporates a correct set of
n-grams is also important. Third, for models which
do not capture such explicit n-gram features like
ours, interestingly, they may still be able to yield
good performances on certain tasks. For example,
though V A-W and Transformer did not perform
well on SST-2, they yielded results competitive to
GRU and LSTM on AG-news and IMDB. This ob-
servation indicates there could be other useful fea-
tures captured by such models that can contribute
towards their overall modeling power.
Although in this work we did not aim to propose
novel or more powerful architectures, we believe
our work can be a step towards better understand-
ing of RNN models. We also hope it can provide
inspiration for our community to design more in-
terpretable yet efficient architectures.
6 Conclusion
In this work, we focused on investigating the under-
lying mechanism of RNNs in terms of handling se-
quential information from a theoretical perspective.
Our analysis reveals that RNNs contain a mecha-
nism where each hidden state encodes a weighted
combination of salient components, each of which
can be interpreted as a representation of a classi-
caln-gram. Through a series of comprehensive
empirical studies on different tasks, we confirm
our understandings on such interpretations of these
components. With the analysis coupled with exper-
iments, we provide findings on how RNNs learn to
handle certain linguistic phenomena such as nega-
tion and intensification. Further investigations on
understanding how the identified mechanism may
capture a wider range of linguistic phenomena such
as multiword expressions (Schneider et al., 2014)
could an interesting future direction.1632Acknowledgements
We would like to thank the anonymous reviewers
and our ARR action editor for their constructive
comments. This research/project is supported by
the Ministry of Education, Singapore, under its
Tier 3 Programme (The Award No.: MOET32020-
0004). Any opinions, findings and conclusions or
recommendations expressed in this material are
those of the authors and do not reflect the views of
the Ministry of Education, Singapore.
References163316341635A Dataset Statistics
The statistics of the sentiment analysis, relation
classification and NER datasets are shown in Table
6. The language modeling datasets are obtained
from Einstein.ai and the statistics are shown in
Table 7.
We created the binary dataset SST-2 by extract-
ing instances (including phrases) with polarity from
the constituency parse trees in the original SST
dataset (Socher et al., 2013). We merged the labels
extremely positive andpositive aspositive and the
labels extremely negative andnegative asnegative .
We also extracted all the phrases in the constituency
parse trees from the original dataset and created
the 5-class dataset SST-5. The labels extremely
positive ,positive ,neutral ,negative andextremely
negative were mapped into +2, +1, 0, -1, and -2
respectively.
B More Result from the SST datasets
B.1 Negation and Intensification
Figure 4 shows that the n-gram representations
from the LSTM model together with its correspond-
ing MVMA-L and MVM-L models can also cap-
ture negation on the extracted adjectives from SST-
2. However, V A-EW fails to capture the negation
phenomenon for the negative adjectives, which may
be explained by that: the n-gram representation
of V A-EW solely involves the current token, thus
being less expressive compared to the one from
models such as MVMA-L and MVMA-G. More-
over, the MVMA-G model can also capture the
negation and intensification phenomena on SST-5
as shown in Figure 5. The intensification token
will generally strengthen the polarity of an adjec-
tive while the negation token will generally weaken
the polarity of it.
We also visualized the polarity score of each n-
gram within a sentence. Two examples are shown
in Figures 6a and 6b, where a warmer color indi-
cates a higher polarity score (i.e., the n-gram is
more positive). For example, “ never ” itself has
a remarkably negative polarity score while “ loses ”
has a remarkably positive one. Consequently, the n-
grams starting from “ never ” (while ending with an-
other word) generally have positive polarity scores.1636
Such visualization results show that our identified
representations defined over the linguistic units as
captured by RNNs can be highly interpretable.
B.2 First-order Approximation
To examine how well the recurrence relation in
Equation 7 can approximate the standard RNNs,
we followed the method in the work of Mah-
eswaranathan and Sussillo (2020) and compared
the hidden state of the standard RNNs ( h=
RNN (x,h)) at each time step to the corre-
sponding context representations ( ˆh=g(x) +
A(x)h). The error at each time step is defined
as
||h−ˆh||/||h||. (15)
We used the current standard hidden state to predict
the next hidden state and the context representa-
tions on the SST-2 test set.
We noticed that the weight decaying coefficient
has a remarkable impact on the error. Specifically, a
larger coefficient can result in smaller errors. When
the coefficient is 1e−5, the average errors on the El-
man, GRU, and LSTM models were 26.2%, 21.7%
and 46.6% and respectively. When the coefficient
is3e−4the the average errors dropped to 17.1%,
15.1%, and 33.3% respectively. Note that since
this is the single step error, the accumulated errors
across many times steps can be large, particularly
for LSTM, and thus the first-order approximation
cannot fully replace standard RNNs. Despite this,
the resulting context and n-gram representations
can help us understand how RNNs process contex-
tual information such as n-gram features.
C T-sne Visualization
We visualized the context representations from the
MVMA-G model using t-sne (van der Maaten and
Hinton, 2008), which provides us with an intuitive
understanding on the efficacy of our identified rep-
resentations. We automatically extracted 2,188
phrases with less than 30 tokens from AG-news
with 4 topicsand projected their context repre-
sentations to a two-dimension space. Figures 7a
and 7b show there exist four major clusters corre-
sponding to the four topics, indicating those repre-
sentations can generally learn the topic information
and explain the differences. Similar to the previous
analysis, the MVM-G model is able to learn the
topic information with the n-gram representations.
D Results on Transformer
We have also run the Transformer model on the
sentiment analysis datasets and the results are listed
in Table 8.1637
E Implementation Details
E.1 Sentiment Analysis
Settings For the SST-2, AG-news, and IMDB
datasets, we used the cross-entropy as the loss
function to train the models. Embeddings were
randomly initialized and trainable during training.
For the SST-5 dataset, we treated the classification
as a regression problem as the labels are polarity
intensity. The mean-squared error was used as the
loss function during training. Note that we initial-
ized embeddings with pre-trained GloVe (Penning-
ton et al., 2014) and fixed them during training on
SST-5 for the analysis of both the negation and
intensification phenomena.
Furthermore, for the MM model, each token was
represented as a matrix and the matrix size was set
as 32×32. For the other models, the embedding
and hidden sizes were both set as 300.
Polarity Adjectives We automatically extracted
adjectives with polarity (examples shown in Table
9) from SST-2 in two steps. In the first step, follow-
ing the method of Sun and Lu (2020), we calculated
a frequency ratio for each token (in the vocabulary)
between the frequencies of the token seen in the
positive and negative instances respectively. If a
token has a frequency ratio either larger than 3 or
less than 1/3, it will be extracted as an positive to-
kenor an negative token . In the second step, we
used the textblob packageto detect positive and
negative adjectives from those positive tokens and
negative tokens respectively.
E.2 Relation Classification
Following the work of Gupta and Schütze (2018),
we examined the RNN, baseline, MVMA and
MVM models on SemEval 2010 Task 8 (Hendrickx
et al., 2010) which has 9 directed relationships and
an undirected other type. We used the final hidden
states of the standard RNNs (or context representa-
tions of the MVMA, MVM and baseline models) as
the instance representations for classification. The
cross-entropy loss was employed during training.E.3 Named Entity Recognition
At each time step, we concatenated the context
representations (or hidden states) from both di-
rections in a bidirectional model, fed them to a
projection layer and then to a linear CRF layer.
More details about the architecture can be referred
to the biLSTM-CRF model in the work of Lam-
ple et al. (2016). We also referred to the code
at https://github.com/allanj/pytorch_neural_crf for
the implementation of the linear CRF layer.
CoNLL-2003 contains four types of entities:
persons (PER), organizations (ORG), locations
(LOC) and miscellaneous names (MISC). The orig-
inal dataset was labeled with the BIO (Beginning-
Inside-Outside) format. For example, “United Arab
Emirates” are labeled as “B-LOC I-LOC I-LOC”.
We transform the tags into the IOBES format where
two prefixes “E-” and “S-” are added. Specifically,
“E-” is used to label the last token of an entity span.
The “S-” prefix is used for a single-token span. For
example, “United Arab Emirates” are labeled as
“B-LOC I-LOC E-LOC” in this format. There are
20 categories of tags in total including the starting,
ending and padding tags. We trained the models to
predict each entity.
The embedding size and hidden size were set to
300 and 200 respectively. The SGD optimizer was
used to learn parameters.
E.4 Language Modeling
The embedding size and hidden size were both 512
for PTB and Wiki2, and 256 and 512 respectively
for Wiki103. The cross-entropy loss was used dur-
ing training. For PTB and Wiki2, the output of
the final fully-connected layer was fed to a soft-
max function while the Adaptive softmax (Joulin
et al., 2017) was used for Wiki103 (because of its
large vocabulary size). We only considered the
word-level models. We trained each model for 50
epochs, chose the model which had the best perfor-
mance on the development set as the final model
and evaluated the final model on the test set.
F Jacobian matrix of LSTM
Unlike GRU and Elman RNN, LSTM has a mem-
ory cell apart from a hidden state. Here, we de-
scribe how to get their Jacobian matrices. An1638
LSTM cell can be written as
i=σ(Wx+Wh),
f=σ(Wx+Wh),
o=σ(Wx+Wh), (16)
c= tanh( Wx+Wh),
c=f⊙c+i⊙c,h=o⊙tanh(c),
wherei,f,o∈Rare the input gate, forget gate
and output gate respectively. c∈Ris the new
memory, and cis the final memory.
Let us expand the memory state and hidden state
at time step tas
c=g(x) +B(x)c
+D(x)h+o(c,h),
h=g(x) +E(x)c
+F(x)h+o(c,h),(17)
where B,D,EandF∈Rare all Jacobian
matrices. o(h)ando(h)are remainder
terms of the Taylor expansion.
We concatenate the memory state and hidden
state and view the concatenation as an “extended
hidden state”. The context representation for the
“extended hidden state” at time step t(assuming of
zero vectors as initial states) will be written as:
/bracketleftbiggˆc
ˆh/bracketrightbigg
=/summationdisplay/bracketleftbiggv
v/bracketrightbigg
=/summationdisplay/bracketleftigg/productdisplayA(x)/bracketrightigg/bracketleftbigg
g(x)
g(x)/bracketrightbigg
,
(18)
where ˆcandˆhrefer to the context representations
corresponding to the memory state and hidden state
respectively. g,g∈R, and A∈Rare all
functions of inputs. A(x)contains many interac-
tion terms resulting from the gating mechanism,
which may result in a strong expressive power. As
the hidden state his commonly used for down-
stream tasks, we will only consider vas the n-
gram representation on our tasks, and the context
representation will be/summationtextv.1639