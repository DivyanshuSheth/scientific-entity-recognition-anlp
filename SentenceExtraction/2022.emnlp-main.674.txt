
Jinlan Fu
NUS
jinlan@nus.edu.sgSee-Kiong Ng
NUS
seekiong@nus.edu.sgPengfei Liu
CMU & Inspired Cognition
stefanpengfei@gmail.com
Abstract
This paper aims for a potential architectural im-
provement for multilingual learning and asks:
Can different tasks from different languages be
modeled in a monolithic framework, i.e. with-
out any task/language-specific module ? The
benefit of achieving this could open new doors
for future multilingual research, including al-
lowing systems trained on low resources to be
further assisted by other languages as well as
other tasks. We approach this goal by devel-
oping a learning framework named Polyglot
Prompting to exploit prompting methods for
learning a unified semantic space for different
languages and tasks with multilingual prompt
engineering. We performed a comprehensive
evaluation of 6tasks, namely topic classifi-
cation, sentiment classification, named entity
recognition, question answering, natural lan-
guage inference, and summarization, covering
24datasets and 49languages. The experimen-
tal results demonstrated the efficacy of multilin-
gual multitask prompt-based learning and led
to inspiring observations. We also present an
interpretable multilingual evaluation method-
ology and show how the proposed framework,
multilingual multitask prompt training, works.
We release all datasets prompted in the best
setting and code.
1 Introduction
The emergence of multilingual pre-trained lan-
guage models (Xue et al., 2021; Liu et al., 2020;
Conneau et al., 2020; Conneau and Lample, 2019)
enables different languages to be represented in a
unified semantic space. As a result, a fine-tuned
model of a data-rich language such as English
can achieve decent transfer (e.g., zero-shot) per-
formance in geographically, syntactically, or pho-
netically similar languages (Malaviya et al., 2017).
The insufficient features learned by languages un-
der lower-resource settings can thus be compen-Figure 1: Application of prompt technology in three dif-
ferent scenarios: Generic, Multitask (MT), Multilingual
Multitask (MLMT). QA,Sum, and NLIrepresent differ-
ent tasks, namely question answering, summarization,
and natural language inference here. PLM represents
pre-trained language model, and “zh”, “en”, “fr”, “ja”,
“de”,“es” denote different languages.
sated through the higher-resource languages shared
with them.
Despite the preliminary success in the low-
resource scenarios using shared knowledge across
languages in multilingual language models (Wang
et al., 2019b; Liu et al., 2019; K et al., 2020), the
cross-lingual transfers have mostly occurred only
within the boundary of the same task or similar
tasks. Can more conceptually-diverse tasks from
different languages communicate together? While
researchers have made some preliminary progress
towards this direction (Yang et al., 2016; Lin et al.,
2018; Hasan et al., 2021; Dong et al., 2015; Ma-
habadi et al., 2021), the scope of the “different”
tasks had remained relatively narrow, such as limit-
ing to tasks with the same sequence labeling form
(Yang et al., 2016) (e.g., named entity recognition,
chunking, and part-of-speech), or different domains
for the same task (Wang et al., 2020).
Unifying different tasks into one framework can
be challenging if we are to avoid introducing ad-
ditional task-specific parameterized modules. Re-
cently, the success of the prompting methods (Liu
et al., 2021b; Sanh et al., 2021) has provided us
with new clues on unifying different tasks in the
same framework without task-specific parameters
by formulating all tasks as a pre-training problem9919with various frameworks such as the mask language
model (Devlin et al., 2019; Conneau et al., 2020) or
the encoder-decoder model (Xue et al., 2021; Raf-
fel et al., 2020; Liu et al., 2020; Chi et al., 2021).
In this paper, we leverage prompt techniques
to cross the boundaries of different tasks and lan-
guages so that multiple tasks in different languages
can be placed in a monolithic framework (as shown
in Fig. 1-(c) as opposed to single task single lan-
guage (Fig. 1-(a)) as well as multiple task sin-
gle language learning (Fig. 1-(b))) to benefit from
one another without requiring any task/language-
specific modules.
We name this multilingual multitask training
model as Polyglot Prompting (PolyPrompt) . Differ-
ent tasks from different languages can then be seam-
lessly connected together by being reformulated as
pre-training tasks. Architecturally, we choose the
encoder-decoder pre-training framework so that
more NLP tasks could be unified, as compared
to other architectures such as the mask language
model that favors classification-based tasks. Our
explorations in this paper are driven by following
research questions:
Q1: Can different tasks from different languages
benefit from each other by a monolithic framework?
If the answer is “yes”, can the performance be fur-
ther improved by introducing more high-resource
datasets that are more readily available?We de-
velop PolyPrompt , a new multitask multilingual
learning framework, and study the performance in-
fluenced by the introduction of 17 high-resource
datasets. (Sec. 5.1)
Q2: Can PolyPrompt benefit all languages in
different datasets? If not, how do different charac-
teristics of datasets and languages affect the perfor-
mance of PolyPrompt? We try to give answers by
designing a multilingual interpretable evaluation
methodology (Fu et al., 2020; Liu et al., 2021a) to
analyze the strengths and weaknesses of the uni-
fied framework for different tasks, datasets, and
languages. (Sec. 5.2)
Q3: What makes a good prompt for multilingual
multitask prompt training? Applying the prompt-
ing method to a multilingual multitasking setting re-
quires considering various difficulties of prompt en-
gineering in the linguistic dimension. We study twoaspects of the prompt designs for PolyPrompt : the
language choice of prompt templates and the uni-
formity of prompt templates across tasks. (Sec. 5.3)
The main observations are listed in Sec. 6. Be-
low, we summarize the main contributions. (1)
To the best of our knowledge, this is the first ar-
chitectural exploration for the learning of multi-
ple conceptually-different tasks (e.g., classification,
question answering and text generation) and mul-
tiple diverse languages, which relies solely on a
monolithic model. (2) We introduce the concept
ofmultilingual prompt engineering and provide
empirical insights on what makes a good multi-
lingual prompt. (3) We have conducted extensive
experiments for in-language training ,cross-lingual
zero-shot transfer , and cross-task & cross-lingual
zero-shot transfer scenarios, and designed an in-
terpretable multilingual evaluation methodology
to understand how multitask multilingual prompt-
ing works, which leads to interesting observations
(Sec. 5.2).
2 Related Work
Multitask & Multilingual Learning The devel-
opments of neural networks have made it easier to
share information across tasks or languages. As
such, in the past few years, there has been much
work on multitask learning within the same lan-
guage (Liu et al., 2015, 2016; Søgaard and Gold-
berg, 2016; Kumar et al., 2016), or multilingual
learning in the same or similar types of tasks such
as sequence labeling (Yang et al., 2016; Lin et al.,
2018) and machine translation (Dong et al., 2015;
Mahabadi et al., 2021; Hu et al., 2021). However,
the task of learning different languages and tasks
simultaneously in a unified learning framework
without task- or language-dependent parameters
has remained unexplored.
Prompting Methods Prompting is a technique
that aims to make better use of pre-trained knowl-
edge by reformulating tasks at hand accordingly
(Liu et al., 2021b) and thus allowing us to do more
with one model by unifying signals cross tasks
(Sanh et al., 2021), languages (Zhao and Schütze,
2021), even modality (Zhao et al., 2021). In this
paper, we expand what a system can do by propos-
ing multilingual multitask learning with prompting
methods for connecting geographically diverse lan-
guages and linguistically different tasks, thereby
allowing them to leverage one another effectively.9920
3 Multitask Multilingual Prompt
Training
We unify different tasks from different languages
by reformulating each NLP task as a sequence-
to-sequence problem (Sutskever et al., 2014; Bah-
danau et al., 2015) so that they can be connected
by a multilingual pre-trained language model (e.g.,
mT5 (Xue et al., 2021)) that also adopts a sequence-
to-sequence training objective. Fig. 2 shows the
overview of our proposed framework. Each sam-
ple from different tasks will be re-formatted as a
(prompt ,answer ) pair using pre-defined templates
and then be fed into a multilingual pre-trained lan-
guage model.
Formally, given a task set T={T, T,⋯, T}
withntasks and corresponding prompt templates
K={K, K,⋯, K}(without loss of generality,
we shall assume that each task has one prompt for
easy understanding). First, we transform the sam-
ples in each task into a form that is understandable
by the language model based on the predefined tem-
platesK. Assume that (x,y)∈Zis the input
and output pair for the j-th sample of the i-th task,
whereZcontains all input and output sample pairs.
The input-output pair (x,y)for the j-th sam-
ple of the i-th task can be converted to (ˆx,ˆy)
through the predefined template K, which can be
formulated as:
(ˆx,ˆy)=K(x,y) (1)
We choose a sequence-to-sequence language model
to achieve multilingual multitask prompt training,
where samples from n tasks will be the input of
the chosen language model. The loss function is tomaximize the log-likelihood of the output text and
can be defined as:
L=∑log(
∏P(ˆy∣ˆy,ˆx;θ)),(2)
where(ˆx,ˆy)∈Zrepresents a sequence-to-
sequence text pair for any task. ∣ˆy∣is the number
of tokens in the decoded text, and ˆyis the target
tokens before the time step m.
4 Experiment Setup
4.1 Tasks & Datasets
The datasets, tasks, and evaluation metrics stud-
ied in this work are shown in Tab. 1. We call
those datasets that provide training and test sets
for multilingual multitask prompt training as tar-
get datasets . To explore the influence of introduc-
ing more high-resource English and multilingual
datasets to PolyPrompt , we present the expanding
datasets , which only provides training datasets for
multilingual multitask prompt training (we do not
evaluate PolyPrompt and its variants on expanding
datasets). Overall, we study 7 multilingual target
datasets covering 4 NLP tasks ( question answering,
sentiment classification, topic classification, and
sentence pair classification ), and 15 monolingual
(English) and 2 multilingual expanding datasets
covering 6 NLP tasks ( text summarization, named
entity recognition, and 4 tasks covered by target
datasets ). Further details of the target and expand-
ing datasets can be found in App. B. The languages
considered in this work can be seen in App. A9921
4.2 Experimental Settings
Model We list 5models explored in this work.
(1)Vanilla mT5 : In the cross-lingual zero-shot
transfer setting, mT5 is trained on the training set
in English of the specific task (e.g. XNLI), while
in the in-lingual training setting, mT5 is trained
on the training samples in all languages for the
particular task (e.g. XNLI).
(2)Polyglot Prompt (PolyPrompt) is a stan-
dard multilingual multitask prompt training
model，which is trained on 7 target datasets cover-
ing 4 NLP tasks (e.g., QA).
(3)PolyPrompt+Expand is the PolyPrompt model
trained on the 7 target datasets and 15 high-
resource (English) expanding datasets.
(4) PolyPrompt+Expand+PANX is the
PolyPrompt trained on the 7 target datasets,
15 high-resource datasets, and a multilingual NER
dataset (PANX).
(5) PolyPrompt+Expand+XLSum is the
PolyPrompt trained on the 7 target datasets,15 high-resource datasets, and a multilingual
summarization dataset (XL-Sum).
Parameters The PolyPrompt model is built
based on the mT5 (Xue et al., 2021), and our exper-
iments are designed based on the mT5-base (Wolf
et al., 2020) with 580 million parameters. We used
token limits of size 512and64for input and output
sequences, respectively. All models have a learning
rate of 1e−4, with the batch size set to 18, and
were trained for 20epochs. During training, check-
points were saved every 1,000steps. The model
with the best performance on the validation set was
selected.
Training Data Construction Some datasets
have a large number of training samples, for ex-
ample, XNLI has 4.5million training samples. To
reduce the expensive computational cost of our ex-
periments, we randomly sampled 3,000samples
from the training set for each language of the target
datasets, and 5,000samples from each expanding
dataset. These selected samples will serve as the
training set for multilingual multitask prompt train-
ing with different experiment scenarios.
Experimental Scenario We consider three ex-
perimental scenarios: (1) In-language training ,
fine-tuned on golden data in all target languages.
Like Hu et al. (2020), we use the translations from
English released by Hu et al. (2020) as the golden
training samples for the target language for the
XQuAD, MLQA, XNLI, and PAWS-X datasets,
which have only English training sets. (2) Cross-
lingual zero-shot transfer (Hu et al., 2020), where
the model is fine-tuned only on the training set in
English. (3) Cross-task & cross-lingual zero-shot
transfer , where a model is evaluated on tasks and
languages that did not appear in its training dataset.
5 Results & Analysis
5.1 Exp-I: Effect of Multitask Prompt
Training
The experiment in this section is designed to an-
swer the research question Q1in Sec.1, namely to
investigate whether multilingual multitask prompt
training ( PolyPrompt ) can achieve improvement,
and whether the performance can be further im-
proved by introducing more high-resource datasets.
5.1.1 Approach
Significance tests: To examine whether the
PolyPrompt and its variants are significantly better9922
than the vanilla mT5 , we perform the significance
test with Wilcoxon’s Signed-rank Test (Wilcoxon
et al., 1970) at p=0.05. The null hypothesis is
that the performance of PolyPrompt and its variants
is indistinguishable from that of vanilla mT5 .
5.1.2 Results
We detail main observations in Tab. 2 and Fig. 3:
(1)PolyPrompt can achieve improvement, es-
pecially with the introduction of high-resource
datasets. Compared to Vanilla mT5 , the averageperformance of PolyPrompt and its variants (e.g.
PolyPrompt+Expand ) was greatly improved on the
7 datasets of 4 tasks with both the in-language train-
ingandcross-lingual zero-shot transfer settings,
other than the PolyPrompt+Expand+XLSum with
thecross-lingual zero-shot transfer setting ( p=
0.18>0.05). Furthermore, the best systems for the
in-language training andcross-lingual zero-shot
transfer scenarios are PolyPrompt+Expand+PANX
(5 out of 7) and PolyPrompt+Expand (7 out of 7),
respectively, illustrating the effectiveness of intro-9923ducing high-resource expanding datasets.
(2)It is more beneficial for PolyPrompt to
introduce high-resource English datasets in
the cross-language zero-shot transfer. In
Fig. 3-(b), we observe that PolyPrompt achieved
performance gains in many languages belong-
ing to different datasets in the cross-language
zero-shot transfer scenarios. When the ex-
ternal English datasets ( PolyPrompt+Expand )
are introduced, more languages gained perfor-
mance improvements. However, when mul-
tilingual datasets (e.g. PANX and XLSum)
were introduced ( PolyPrompt+Expand+PANX and
PolyPrompt+Expand+XLSum ), the overall perfor-
mance dropped (observed from Tab. 2) and there
were more languages with negative relative perfor-
mance gains (compared to PolyPrompt+Expand ).
5.2 Exp-II: Multilingual Interpretable
Evaluation
This section aims at the research question Q2(How
do different characteristics of datasets and lan-
guages affect the performance of PolyPrompt? ) by
introducing a multilingual interpretable evaluation.
5.2.1 Approach
Interpretable evaluation (Liu et al., 2021a; Fu et al.,
2020; Ruder et al., 2021) aims to breakdown the
holistic performance (e.g., F1) to a more fine-
grained level based on predefined features (e.g.,
text length) to interpret the model’s behavior better.
Below, we list some features of each task ex-
plored in this work. XQuAD, TyDiQA, MLQA :
the length of context ( cLen ), question ( qLen ), and
answer ( aLen ). The BLUE score of the answer and
context ( BLUE_AC ).PAWS-X, XNLI : the length
ofsentence(t1Len ) and sentence(t2Len ). The
BLUE score of the sentence pair ( BLUE_t1t2 ). Fur-
ther detailed interpretable evaluation definition can
be found in App. C. We also measure the dataset-
level feature ϕ, the average feature value over a
dataset. For example, ϕdenotes the av-erage answer length of MLQA dataset. Further
details can be found in App. E
5.2.2 Results
Here are the main observations in Tab. 3 and Fig. 4:
(1) Dataset Perspective: the strengths of
PolyPrompt in the co-occurring languages on dif-
ferent datasets are inconsistent due to dataset
bias. For example, en,de, and frco-occur on
PAWS-X and XNLI. PolyPrompt was better in the
short sentence(t2Len:XS ) in PAWS-X, while ex-
celling in the long sentence(t2Len:XL ) of XNLI.
This inconsistency results from the dataset bias
shown in Fig. 4: the ϕ(XNLI-[en,de,fr] )< 12
while ϕ(PAWS-X-[en,de,fr] )> 20. Therefore,
ϕ(PAWS-X-[en,de,fr] )on bucket t2Len:XS
was close to ϕ(XNLI-[en,de,fr] )on bucket
t2Len:XL .
(2)Model Perspective: PolyPrompt achieves
overall performance improvements on the 7 target
datasets, but it cannot perform well on all sam-
ples (e.g., worse performance on long sentences).
PolyPrompt is better at short context samples for
MLQA (cLen:XS/S ), long context samples for
XQuAD (cLen:XL/L ), long sentenceforXNLI
(t2Len:XL/L ), which is valid for most languages.
Disadvantage analysis: PolyPrompt is worse
at handling long question samples ( qLen:XL/L )
forXQuAD ,TyDiQA , and MLQA , long sentence
samples for XNLI-es, and long sentenceand
sentencesamples in zh,ko,ja of PAWS-X.
(3)Language Perspective: it is difficult for
PolyPrompt to bring gains for languages that ap-
pear only once in the 7 target datasets unless high-
resource datasets are introduced. For example,
PolyPrompt showed a slight performance improve-
ment over vanilla mT5 in languages bn,fi,id,
andtethat only appeared in the TyDiQA dataset.
When introducing high-resource English datasets,
the performance of bnis significantly improved
especially for long context and short answers sam-
ples, while the performance improvement of fi,
id, and teis still limited until a high-resource mul-
tilingual training dataset PANX is introduced. The
reason may be that most of the languages in the 7
tasks belong to the IE language family (findings
from Fig. 3), and so does the bnlanguage. There-
fore, compared to fi,id, and te, it is easier for
bnto get knowledge from neighbor languages in
multitask training.9924
5.3 Exp-III: Effect of Prompt
In this section, we try to find out what prompts or
prompt combinations are suitable for multilingual
and multitask scenarios ( Q3).
5.3.1 Prompt Design
Although prompting methods have proven effective
in many NLP scenarios, its effectiveness comes at
the cost of prompt engineering (Liu et al., 2021b),as there are usually a multitude of factors that in-
fluence the prompt design process, and the situa-
tion is clearly more complicated in the multilin-
gual situation. Existing works have studied manual
prompt (Schick and Schütze, 2021), soft (trainable)
prompt (Lester et al., 2021), and mix prompt (mix-
ing the manual and soft prompt) (Gu et al., 2021;
Zhao and Schütze, 2021) design approaches. In
this work, we take particular care of language and9925
uniformity of prompt templates designed for mul-
tilingual multitask setting. The examples of the
considered prompt design can be seen in Tab. 4.
Language Choice: we consider both the in-
lingual and cross-lingual prompts. In-lingual
prompts are those in which the language of the
prompt is the same as the target language (Zhao
and Schütze, 2021). Cross-lingual prompts denote
those in which the language of the prompt template
is different from the target language. In this work,
we keep the language of the prompt template in En-
glish (en) (Lin et al., 2021) regardless of the target
language (e.g., zh).
Uniformity of Templates: Previous studies (Caru-
ana, 1997; Evgeniou and Pontil, 2004; Argyriou
et al., 2008) have shown that similar tasks would
benefit from multitask training. In this work, we
study unified prompts versus diversified prompts .
Unified prompts indicates that prompt templates
of different datasets have similar structures and
cooccurrences. Diversified prompts means that the
prompt templates for each task did not consider the
same structure and multiple co-occurrence words.
In practice, for each dataset, we designed 5differ-
ent prompts and then randomly selected one prompt
for each task to build a set of diverse prompts for
multitask prompt training. In total, we created 5
groups of diversified templates. The list of tem-
plates can be found in App. F.
5.3.2 Results
(1)Cross-lingual prompt can help better retrieve
knowledge encoded in language model. We can
observe from Fig. 5-(a) that the average overall
performance of the 5models equipped with CLprompts outperformed IL prompts, which holds
for all the seven datasets. We think they might
be because mT5 was pre-trained on a larger body
of English corpus, it can understand the English
template well. This makes it easier for downstream
NLP tasks to retrieve knowledge from mT5.
(2)The unified template outperforms the diver-
sified template In Fig. 5-(b), we observed that the
PolyPrompt with uniform templates outperforms
any diverse templates (e.g. PolyP-v1 ), especially
on the QA task. The reason may be that unified
prompts helped eliminate the boundaries between
tasks, thereby reducing the distance between tasks
and making the interaction between tasks easier.
5.4 Exp-IV: Cross-task Cross-lingual
zero-shot transfer
To investigate whether PolyPrompt is better at re-
trieving relevant knowledge from pre-trained lan-
guage models for tasks and languages unseen
in training stage, we investigate vanilla mT5 ,
PolyPrompt , and PolyPrompt+Expand fine-tuned
on the English datasets and evaluate these three
models on the PANX dataset, a named entity recog-
nition task with 40 languages. We then subtract the
performance of vanilla mT5 from PolyPrompt and
PolyPrompt+Expand in the same language, and the
results are shown in Fig. 6.
Results: (1)Almost all languages benefit from
both PolyPrompt and its variants. PolyPrompt
brings gains for 34 of the 40 languages, and
more languages will benefit when PolyPrompt
is enhanced with high-resource English training
datasets. Interestingly, PolyPrompt+Expand per-9926
formed much better than PolyPrompt in languages
belonging to IE: Germanic andIE: Romance lan-
guage families, which made up a large proportion
of samples in the pre-training corpus of mT5.
(2) PolyPrompt significantly improves perfor-
mance on languages that have never appeared in
the pre-training corpus of mT5.Both PolyPrompt
andPolyPrompt+Expand improve a lot over mT5
ontl, a language that never appeared in mT5’s pre-
training corpus. Furthermore, PolyPrompt+Expand
achieves the best performance gain on tl. The
reasons can be attributed to (1) we unify differ-
ent tasks into a monolithic framework (including
NER), which effectively shortens the distance be-
tween different tasks; (2) English (en) andtl
share the same semantic space, NER knowledge in
English (en) can be effectively transferred to tl.
6 Conclusions
We can provide the following preliminary empirical
answers to our research questions.
(1)Can different tasks from different languages
benefit from each other by a monolithic frame-
work? Yes. What’s more, introducing more high-
resource datasets can further improve the tasks’
performance involved in multitask prompt training.
(2)How do different characteristics of datasets
and languages affect the performance of
PolyPrompt? PolyPrompt cannot benefit all lan-
guages in all datasets. For example, (a) languages
that appear only once in target datasets have bene-
fits when PolyPrompt is enhanced by high-resource
datasets; (b) PolyPrompt is better in short con-
text samples for MLQA , long context samples for
XQuAD , while poor in long question samples for
XQuAD, TyDiQA, and MLQA.(3)What makes a good prompt for multilingual
multitask prompt training? The best performance
is achieved when the model is equipped with cross-
lingual prompts (i.e., using English as prompt tem-
plates regardless of what the language of training
samples is) and prompts with unified templates
across tasks.
7 Limitations
Although in this paper, we try to cover as many lan-
guages and tasks as possible, some tasks (e.g., se-
mantic parsing, machine translation) and languages
are still not considered. In addition, due to lim-
ited computational resources, we adopt a relatively
small pre-trained language model, and the results
on the larger pre-trained language models are also
worth expecting. In addition, there are a variety of
factors affecting the design of prompts in a multi-
lingual setting. This paper only considers two (lan-
guage choice and uniformity of prompt templates),
so more comprehensive studies in this direction
could be conducted.
Acknowledgements
We thank Graham Neubig and Junjie Hu for their
useful discussion and suggestions on this work.
This work was supported by the National Research
Foundation of Singapore under its Industry Align-
ment Fund – Pre-positioning (IAF-PP) Funding
Initiative. Any opinions, findings, conclusions, or
recommendations expressed in this material are
those of the authors and do not reflect the views of
the National Research Foundation of Singapore.9927References992899299930
A Languages
In this work, we studied 49languages that appear
in24datasets covering 6NLP tasks. For brevity,
the languages are shown in ISO 639-1 codesas
follows: af, am, ar, az, bg, bn, cy, de, el, en, es, et,
eu, fa, fi, fr, gu, ha, he, hi, hu, id, ig, it, ja, jv, ka, kk,
ko, ml, mr, ms, my, nl, np, pa, pt, ru, si, sw, ta, te,
th, tl, tr, ur, vi, yo, zh. Among them, zh, ja, th, te,
kmare languages that do not use space separation
for words.
B Datasets
B.1 Target Datasets
XQuAD (Artetxe et al., 2020) is a cross-lingual
question answering dataset, including 11 languages.
Its English dataset is a subset of the development
set of SQuAD v1.1. The other 10 languages are pro-
fessional translations of the English dataset. There-
fore, the dataset in 11 languages is completely par-
allel.
MLQA (Lewis et al., 2020) is another multi-
language extractive QA dataset, including 7 lan-
guages. Each QA instance is paralleled between 4
languages on average. Since MLQA and XQuAD
lack training sets, following (Hu et al., 2020), we
use the training data of SQuAD v1.1 as their train-
ing set.
TyDiQA-GoldP (TyDiQA) (Clark et al., 2020)
is the gold passage version of the TyDiQA bench-
mark, including 9 languages for training, develop-
ment, and testing. TyDiQA-GoldP is a simplified9931version of the primary task, discarding Thai and
Japanese languages and samples without answers.
Like XQuAD and MLQA, TyDiQA is evaluated
with SQuAD 1.1 (Rajpurkar et al., 2016) metrics.
XNLI (Conneau et al., 2018) is a cross-lingual
natural language inference dataset containing an-
notated development and test sets in 15 languages,
and an annotated English training set. The English
training set has been translated into the other 14
languages through a machine translator.
PA WS-X (Yang et al., 2019) is a cross-lingual
paraphrase adversary from a word scrambling
dataset with 7 languages. The goal of this task
is to predict whether two sentences are paraphrases.
The training set of PAWS-X is the PAWS’s train-
ing data, and the subset of PAWS’s development
and test sets are translated into 6 other non-English
datasets for evaluation.
MARC (multilingual Amazon Reviews Corpus)
(Keung et al., 2020) is a multilingual text classifi-
cation dataset with 6 different languages. Here, we
use the binarized classification task that is defined
by Keung et al. (2020).
MLDOC (Schwenk and Li, 2018) is a multilin-
gual document classification dataset with six topics.
B.2 Expanding Datasets
Expanding tasks simply provide training sets for
the multitask prompt training. In summary, we
studied 15 English and 2 multilingual datasets.
Extractive Question Answering is the task of
finding an answer to a given question from the con-
text. We adopt SQuAD 2.0 (Rajpurkar et al., 2016),
Quoref (Dasigi et al., 2019), NewsQA (Trischler
et al., 2017), and ROPES (Lin et al., 2019).
Multiple-choice Question Answering aims to
select an answer from candidate options based on
the context and question. In this work, we study
MCTest (Richardson et al., 2013) and Social IQa
(Sap et al., 2019).
Natural Language Inference aims to determine
the inference relation (e.g. entailment) between two
texts. The datasets used in this work are Quora ,
RTE (Wang et al., 2019a), and SNLI (Bowman
et al., 2015).Topic Classification is a task to predict a suitable
topic (e.g., health) for a given text. We use the fol-
lowing topic classification datasets: DBpedia-2014
(Zhang et al., 2015), AG_News (Zhang et al.,
2015), and YATC (Yahoo! Answers Topic Clas-
sification Dataset) (Zhang et al., 2015).
Sentiment Classification aims to identify the
sentiment polarity of a given text. We
studied datasets IMDB (Maas et al., 2011),
Amazon Review Polarity (ARP) (Zhang et al.,
2015), and SST2 (Socher et al., 2013).
XL-Sum (Hasan et al., 2021) is a multilingual
summarization dataset covering 45 low- to high-
resource languages. We randomly select 32 out of
45 languages for multitask prompt training. The
ISO-639-1 codes for the chosen languages are en,
ar, vi, ko, es, zh, ru, fr, tr, hi, id, fa, pt, mr, th, az, bn,
np, sr, sw, ta, te, ur, cy, am, my, gu, ha, ig, pa, si,
yo.
PANX (Pan et al., 2017) is a multilingual named
entity recognition dataset in 40 languages con-
structed based on Wikipedia corpus. Following
Hu et al. (2020), we use the version with balanced
train, development, and test splits from Rahimi
et al. (2019).
C Interpretable Multilingual Evaluation
For interpretable evaluation, the first step is at-
tribute definition, and the second is sample break-
down. Assume that ϕ(x)is a function to calcu-
late the number of tokens in the given text x, and
ϕ(x, x)is to compute the BLUE score of
two given texts xandx. The following are the
features tailored for the 7multilingual datasets in
this paper:
•XQuAD, TyDiQA, MLQA :cLen =ϕ(X),
qLen =ϕ(X), aLen =ϕ(X), and
BLUE_AC =ϕ(X, X), where X,X,
andXdenote the context, question, and answer
sequence, respectively.
•PA WS-X, XNLI :t1Len =ϕ(X),
t2Len =ϕ(X),t1Len/t2Len =
ϕ(X)/ϕ(X), and BLUE_t1t2 =
ϕ(X, X), where XandXdenote
the premise and hypothesis (sentence-1 and
sentence-2 for PAWS-X) sequence.
•MARC, MLDOC :t1Len =ϕ(X),t1basic
=ϕ(X), and t1eNum =ϕ(X), where9932Xdenotes a sequence of review (news for ML-
DOC). ϕ(x)andϕ(x)are functions to
calculate the proportion of words belonging to
the 1000 essential English wordsand entities,
respectively.
We then follow Fu et al. (2020) and breakdown
the samples into four buckets, XS (extra-small), S
(small), L (large), and XL (extra-large), according
to their feature values, and calculate the perfor-
mance for each bucket.
D Main Observations
Due to the space limitation, we summarize some
main observations here.
(1) Whether a language that appears in only
one task could gain improvement depends on
the difficulty of the task. In Fig. 3-(a), we
can observe that some languages in XNLI -[ur,bg ],
TyDiQA -[bn,fi,id,te ] and MLDOC -[it] were
not present in other tasks (e.g. itis only present
inMLDOC ). These languages that appeared only
once in multitask training have significant perfor-
mance gains on the XNLI dataset, while perfor-
mance dropsped significantly on the TyDiQA and
MLDOC datasets. The reason could be that XNLI
is a task that relies more on fundamental knowl-
edge (Yin et al., 2019), which is relatively easier
to acquire from other tasks. On the contrary, tasks
such as TyDiQA need to understand more, for ex-
ample, the semantics of sentences and the position
of the answer.
(2)PolyPrompt improves the performance of
non-Indo-European languages a lot in the in-
language training .From Fig. 3-(a), we can
observe that languages belonging to non-Indo-
European language families (e.g. Sino-Tibet and
Niger-Congo ) always have performance gains no
matter which datasets were employed. However,
in languages belonging to the Indo-European -
related language families, the relative performance
gains varied widely across datasets. For exam-
ple, languages belonging to the XNLI and XQuAD
datasets consistently achieved positive relative per-
formance, while languages belonging to the PAWS-
X and MLDOC datasets mainly achieved negative
relative performance. However, this problem was
found to be alleviated after introducing additional
high-resource datasets (e.g. PolyPrompt +Expand).(3) For low-resource languages, PolyPrompt
with in-language prompts will bring more
gains, while cross-lingual prompts bring
more gains when introducing high-resource
training datasets. From Fig. 7, we can ob-
serve that PolyPrompt with in-lingual prompts
outperform with cross-language prompts in
low-resource languages. However, when
the external English dataset was introduced
(PolyPrompt+Expand ), cross-language prompts
have more gains in both low and high resource
languages. With the introduction of multilingual
datasets ( PolyPrompt+Expand+XLSum ), the
relative advantages of cross-lingual prompts
increased.
E Dataset Bias
Dataset-level Features We also obtain the
dataset-level features. Given a dataset Dand a
feature p(e.g. qLen ), the dataset-level feature can
be defined as:
ϕ(D)=1
∣D∣∑ϕ(d), (3)
where dis a sample of the test set D∈D, and
ϕ(⋅)is a function that computes the feature value
for a given sample. For example, ϕ(MLQA)
denotes the average question length of the MLQA .
Dataset bias is measured by ϕ, the dataset-level
feature defined in Eq. 3. Tab. 8 shows five target
datasets explored in Sec. 5.2.
F Prompt Template
Tab. 5 presents the cross-lingual (English) prompt
templates explored in this work. We designed 5
templates for each of the 7tasks.993399349935