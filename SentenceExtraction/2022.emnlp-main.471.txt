
Peiyuan Zhang and Wei Lu
StatNLP Research Group
Singapore University of Technology and Design
peiyuan_zhang@sutd.edu.sg, luwei@sutd.edu.sg
Abstract
Few-shot relation extraction aims to learn to
identify the relation between two entities based
on very limited training examples. Recent ef-
forts found that textual labels (i.e., relation
names and relation descriptions) could be ex-
tremely useful for learning class representa-
tions, which will benefit the few-shot learn-
ing task. However, what is the best way to
leverage such label information in the learning
process is an important research question. Ex-
isting works largely assume such textual labels
are always present during both learning and
prediction. In this work, we argue that such ap-
proaches may not always lead to optimal results.
Instead, we present a novel approach called la-
bel prompt dropout , which randomly removes
label descriptions in the learning process. Our
experiments show that our approach is able to
lead to improved class representations, yield-
ing significantly better results on the few-shot
relation extraction task.
1 Introduction
Enabling machines to comprehend sentences and
extract relations between entities has been a cru-
cial task in Natural Language Processing (NLP).
Conventional methods frame this task as a multi-
class classification problem, trying to solve it
through large-scale supervised training with LSTM
(Hochreiter and Schmidhuber, 1997) or BERT (De-
vlin et al., 2019) as the backbone (Zhou et al., 2016;
Zhang et al., 2017; Yamada et al., 2020). Such an
approach has shown great effectiveness. However,
one problem left unsolved is to identify novel re-
lations with only a handful of training examples.
Therefore, recent studies (Han et al., 2018; Gao
et al., 2019b) introduce the task of few-shot rela-
tion extraction (FSRE) to study this data scarcity
problem.
Aligned with the success of few shot learn-
ing in Computer Vision (Sung et al., 2018; Sator-Figure 1: An example of 2-way-1-shot learning using
label prompt dropout (LPD). Top: Instead of assuming
textual labels are always present for support instances,
LPD randomly drops out such textual labels. Here the
textual label “ country of origin ” for the second instance
is droppoed out. Bottom: LPD directly concatenates the
textual label and the context sentence. The textual label
serves as a prompt to guide BERT to derive a better
class prototype. Note that for simplicity we use the
relation names here, while in our implementation we
use relation descriptions, which are lengthier and more
complex.
ras and Estrach, 2018), most attempts in FSRE
adopt a meta learning framework (Santoro et al.,
2016; Vinyals et al., 2016) that randomly samples
episodes with different label sets from the training
data to mimic the few shot scenario in the testing
phase. As a meta learning approach, prototypi-
cal network (Snell et al., 2017) aims to learn a
class-agnostic metric space. A query instance is
classified as the class that has the nearest prototype
during inference.
While the BERT-based prototypical networks
(Baldini Soares et al., 2019; Peng et al., 2020a)
have shown impressive performance on FSRE, the
class prototypes are only constructed through the
average representation of support instances of each
class, neglecting the textual labels that may provide
additional useful information. Therefore, recent ef-
forts try to modify the prototypical network such
that it can use the label information as well. Yang6996et al. (2020) insert both entity type information
and relation descriptions to the model. Dong et al.
(2021) use a relation encoder to generate relation
representation besides the sentence encoder. Han
et al. (2021a) propose a hybrid prototypical net-
work that can generate hybrid prototypes from con-
text sentences and relation descriptions. Nonethe-
less, these methods largely assume that every sup-
port instance is provided with a corresponding tex-
tual label in the support set during both learning
and prediction. We argue that injecting textual la-
bels to all support instances may render the training
task unchallenging, because the model can largely
rely on the textual labels during training, and thus
results in poor performance during testing when
faced with unseen relations and textual labels. Ide-
ally, textual labels should be treated as additional
source of information, such that the model can
work with or without the textual labels, as shown
in the top part in Figure 1.
In this work, we propose a novel approach called
Label Prompt Dropout (LPD). We directly con-
catenate the textual label and the context sentence,
and feed them together to the Transformer encoder
(Vaswani et al., 2017). The textual label serves as
alabel promptto guide and regularize the Trans-
former encoder to output a label-aware relation
representation through self-attention. During train-
ing, we randomly drop out the prompt tokens to
create a more challenging scenario, such that the
model has to learn to work with and without the
relation descriptions. Experiments show our ap-
proach achieves significant improvement on two
standard FSRE datasets. Extensive ablation stud-
ies are conducted to demonstrate the effectiveness
of our approach. Furthermore, we highlight a po-
tential issue with the evaluation setup of previous
research efforts, in which the pre-training data con-
tains relation types that actually overlap with those
in the test set. We argue that this may not be a de-
sirable setup for few-shot learning, and show that
the performance gain of existing efforts may be
partly due to this “knowledge leakage” issue. Wepropose to filter out all the overlapping relation
types in the pre-training data and conduct more rig-
orous few-shot evaluation. In summary, we make
the following contributions:
•We present LPD, a novel label prompt dropout
approach that makes better use of the textual
labels in FSRE. This simple design has signif-
icantly outperformed previous attempts that
fuse the textual label and the context sentence
using complex network structures.
•We identify the limitation of the previous ex-
perimental setup in the literature and propose
a stricter setup for evaluation in FSRE. For
both setups, we show strong improvements
over the previous state of the art.
2 Related Work
2.1 Few-Shot Relation Extraction
Few-shot relation extraction (FSRE) aims to train
a model that can classify instances into novel re-
lations with only a handful of training examples.
Han et al. (2018) are the first to introduce a large
scale benchmark for FSRE, in which they evalu-
ate a model in N-way- K-shot settings. Gao et al.
(2019a) propose a hybrid attention-based proto-
typical network to handle the diversity and noise
problem of text data. Qu et al. (2020) model the re-
lationship between different relations via Bayesian
meta-learning on relation graphs. Han et al. (2021a)
apply an adaptive focal loss and hybrid networks
to model the different difficulties of different rela-
tions.
Another line of work focuses on further training
pre-trained language models (PLMs) on the task of
relation extraction (RE). Based on the hypothesis
that sentences with the same entity pairs are likely
to express the same relation, Baldini Soares et al.
(2019) collect a large-scale pre-training dataset
and propose a “matching the blanks” pre-training
paradigm. Peng et al. (2020a) present an entity-
masked contrastive pre-training framework for re-
lation extraction. Dong et al. (2021) introduce a
semantic mapping approach to include relation de-
scriptions in the pre-training phase. Inspired by
these works, we propose a contrastive pre-training
with label prompt dropout approach to use relation
descriptions during pre-training while creating a
more difficult setup by dropping out the relation
descriptions.6997
2.2 Prompt-Based Fine-Tuning
Prompt-based models have shown promising per-
formance in few-shot and zero-shot learning in
many recent studies (Brown et al., 2020; Schick
and Schütze, 2021; Shin et al., 2020). Models in
this line of research try to align the downstream
fine-tuning task with the pre-training masked lan-
guage modeling objective (Devlin et al., 2019) to
better use the pre-trained language model’s latent
knowledge. Han et al. (2021b) use prompt tuning
with rules to perform relation classification. Liu
et al. (2022) introduce “Multi-Choice Matching
Networks” that construct prompts by concatenat-
ing multiple relation descriptions.
However, unlike many other tasks in NLP where
the label semantics are straightforward, such as
“positive/negative ” in binary sentiment analysis,
the relation types in relation extraction can be
quite complex, often requiring lengthy sentences as
their descriptions. For example, relation P2094 in
FewRel is described to be “ official classification by
a regulating body under which the subject (events,
teams, participants, or equipment) qualifies for in-
clusion ”. Prompt-based models struggle in this
case because they require the template to be fixed
(e.g., the number of [MASK] tokens in the prompt
template has to be fixed). Previous approaches had
to rely on manually designed prompt templates and
use relation names instead of relation descriptions.
To tackle this problem, we propose to directly usethe entire relation description as the prompt without
any mask tokens. While in conventional prompt-
based models, prompts are used to create natural
descriptions such that the model can perform bet-
ter prediction at the [MASK] positions, the label
prompt used in this work uses natural descriptions
to help regularize the model to output a better class
representation.
3 Task Definition
For an FSRE task, each instance (x, e, y )
is composed of a context sentence x=
{x, x, x, ..., x}, where xstands for the in-
put token of position i; entity positions e=
{e, e}, where e refers to the head entity
span and erefers to the tail entity span; and a
label y={y, y}, where yis the textual
label and y is the numerical label.
LetE,E,Ebe the training, validation,
and test dataset with mutually exclusive label sets.
Under the meta-learning paradigm, each dataset
consists of multiple episodes, each with a support
setSand query set Q. For N-way- K-shot learn-
ing, the support set S={s;n= 1, ..., N, k =
1, ..., K }contains Ndifferent classes. Inside each
class there are Kdifferent support instances. Our
job is to predict the correct label y∈ {y, ..., y}
for each query instance qin the query set. In this
work, we will follow the continued pre-training
setup (Peng et al., 2020a), so there is another6998dataset E . Note that this E is not
the dataset used for the masked language modeling
but for the domain-specific pre-training in RE.
4 Approach
4.1 Training with Label Prompt Dropout
For each support instance, we directly concatenate
the relation description and context sentence with
a “:” in between. For example, the sentence “ Bei-
jing held the 2022 winter Olympics ” will become
“location of event: Beijing held the 2022 winter
Olympics. ” The idea is to create a natural instance
where definition is given first, followed by exam-
ples. The relation description and colon serve as
a label prompt to guide the Transformer encoder
to output a label-aware relation representation. To
prevent the model from relying entirely on the label
prompt and overlooking the context sentence, the
label prompt is randomly dropped out with prob-
ability α. For example, the support instance
“Decimal number was first developed in India ” in
Figure 1 remains in its initial form because its la-
bel prompt is dropped out. For query instances,
we directly input the sentence without any label
prompt. This is becauase the query set is essen-
tially the same as the test set, where we should
not assume access to the ground truth knowledge.
Subsequently, the special entity markers are used to
mark the head and tail mentions (Zhang et al., 2019;
Baldini Soares et al., 2019), and we add the special
classification and separation token to the front and
the end of the sentences, such as “ [CLS] location
of event: [E1] Beijing [/E1] held the [E2] 2022
winter Olympics [/E2] .” The parsed sentence is
then fed to the Transformer encoder. We concate-
nate the final layer representations of the start entity
markers (i.e., [E1] and[E2] ), forming the relation
representation of each instance:
r= [Encoder( x); Encoder( x)] (1)
where hstands for the position of [E1] ,tstands for
the position of [E2] , andris the relation represen-
tation. For K-way- N-shot learning, we average the
relation representations of the Ksupport instances
within one class to obtain the class prototype. The
dot product between the query instance and each
class prototype is then calculated and used as the
logit in the cross entropy loss:
u=/summationtextr(2)
L=−/summationtextlog(3)where rstands for the relation representation of
thek-th support instance in class n,uis the class
prototype for class n, and ris the relation repre-
sentation of the query instance.
4.2 Testing with Prompt Guided Prototypes
Similar to the standard dropout operation (Srivas-
tava et al., 2014) in which neurons are randomly
dropped out during training and are restored during
testing, LPD does not drop out any label prompt of
the support instances during testing as well. By in-
putting the relation description together with each
support instance, we essentially obtain a prompt
guided prototype for every support class. We out-
put the prediction by finding the closest class pro-
totype to the query’s relation representation:
ˆy= arg maxru(4)
4.3 Contrastive Pre-training with Label
Prompt Dropout
LPD can also be added to the domain-specific pre-
training stage in relation extraction. In fact, pre-
training is a crucial step for LPD, because the large
dataset in the pre-training stage allows the model to
fit to the LPD input format and learn how to extract
useful information from the label prompts. We fol-
low the framework proposed by Peng et al. (2020b)
to sample positive and negative pairs used in con-
trastive pre-training. Given a knowledge graph
(KG)Kand two sentences with entity pairs ( h,t)
and (h,t), the two sentences will be labeled as a
positive pair if Kdefines a relation Rsuch that ( h,
t) and ( h,t) belongs to that relation. Sentences
that do not form a positive pair will be sampled
as negative pairs. In Figure 2, for instance, the
entity pairs ( Harry Potter ,United Kingdom ) and
(Obama ,Honolulu Hawaii ) form the same relation
“birthplace ” in the KG. Thus, the two sentences
containing these two entity pairs are sampled as a
positive pair. Under such an approach, the data is
noisily labeled, because two entities forming the
relation in the KG may not express such relation in
the sentence. For example, “ D.C. is a federal dis-
trict of the United States ” will be labeled as as an
instance of the “ captical of ” relation, even though
such relation is not expressed in this sentence. Fol-
lowing Baldini Soares et al. (2019), we randomly
mask entity spans with the special [BLANK] token
with probability ρ = 0.7to avoid relying on
the shallow cues of entity mentions.6999
Each instance in the pre-training stage under-
goes the same transformation as that of the support
instances in the training stage. A label prompt is
prepended to each sentence with dropout proba-
bility α , and special tokens are inserted to
the sentences. Contrastive loss is used to train the
model:
L=−logexp(rr)
exp(rr) +/summationtextexp(rr)
(5)
where (r, r)constitutes the positive pair and
(r, r),1≤i≤Nrepresents negative pairs. Fol-
lowing Peng et al. (2020b), the masked language
modeling objective ( L) is used to maintain the
model’s ability of language understanding. So the
final pre-training loss becomes:
L =L+L (6)
5 Experimental Setup
5.1 Datasets
Following Peng et al. (2020b) and Han et al.
(2021a), we use FewRel 1.0 (Han et al., 2018) and
FewRel 2.0 (the domain adaption portion) (Gao
et al., 2019b) to evaluate our model. FewRel 1.0 is a
large-scale FSRE dataset sampled from Wikipedia
articles and annotated by human annotators. It
contains 100 relations and 700 instances for each
relation. We follow the official split to use 64 re-
lations for training, 16 for validation, and 20 for
testing. In order to study the domain transferability
of LPD, we also evaluate our model on FewRel 2.0,
which is collected from the biomedical domain and
does not contain a training set. It has 25 relations
and 100 instances for each relation.
For contrastive pre-training, we use the same
dataset as in Peng et al. (2020b). This dataset is
collected using the method introduced in Section
4.3, with Wikipedia articles as the corpus and Wiki-
data (Vrande ˇci´c and Krötzsch, 2014) as the knowl-
edge graph. We notice that Peng et al. (2020b) had
excluded all entity pairs in test sets of FewRel 1.0
from the pre-training data, but they did not exclude
the relation types that appear in the train, validation
and test set of FewRel 1.0 from the pre-training
dataset. We argue that this may not be a desirable
setup for few-shot learning due to the potential
“knowledge leakage”: models can learn the distant
supervision signal in the pre-training dataset and
thus learn the relation types in FewRel 1.0 during
the pre-training stage, even though the pre-training
dataset is not manually annotated by human. Thus,
we propose a harder experimental setup by filtering
out all 100 relation types present in FewRel 1.0,
not just the entity pairs in the FewRel 1.0 test set,
from the pre-training dataset. We will refer to the
original dataset produced by Peng et al. (2020b) as
Wikipedia, and the filtered out dataset as Wikipedia
(filtered) in this paper. Table 1 shows statistics of
the original pre-training dataset and the new dataset
after filtering.
5.2 Implementation Details
We pre-train our model on top of BERT-base from
the Huggingface Transformer libraryusing 2 RTX
6000. The entire pre-training process took around
6 hours on the original Wikipedia dataset and 3
hours on the filtered one. It takes 5 hours to fine-
tune our model on FewRel 1.0 with a single RTX
6000. Table 2 shows the detailed hyperparameters.
The same set of hyperparameters, except α, are
used for both FewRel 1.0 and FewRel 2.0. We set
αto 0.4 for FewRel 1.0, and 0.8 for FewRel 2.0,
which we tuned based on the model’s accuracy on
the validation sets.7000
5.3 Evaluation
We evaluate our model by randomly sampling
10,000 episodes from the N-way- K-shot support
set and a query instance. We follow previous works
(Han et al., 2018; Gao et al., 2019b) to choose N
to be 5 and 10, and Kto be 1 and 5. For the main
comparison (i.e., Table 3 and Table 4), we report
the average accuracy together with the standard
deviation of 3 runs using different random seeds.
We report the accuracy of 1 run for all ablation
studies. To obtain the test set accuracy, we submit
our predictions to the FewRel leaderboard.
6 Results
6.1 Comparison with Baselines
We compare our model with the following baseline
methods: 1) Proto-BERT (Snell et al., 2017) is
a prototypical network with BERT-base (Devlin
et al., 2019) serving as the backbone. Note that ourproposed method will be reduced to Proto-BERT
if we discard pre-training and set αandα
to 1.0. 2) BERT-PAIR (Gao et al., 2019b) is a
method that measures similarity of a sentence pair.
3)REGRAB (Qu et al., 2020) is a label-aware
method that models the relationship between differ-
ent relation types via a Bayesian network. 3) MTB
(Baldini Soares et al., 2019) is a model pre-trained
with their proposed matching the blank objective
based on BERT. Note that we report the results
produced by Peng et al. (2020a) for a fair compari-
son with all BERT-base based models because Bal-
dini Soares et al. (2019)’s original work is based
on BERT-large. 4) CP(Peng et al., 2020a) pre-
trains Proto-BERT using a contrastive pre-training
approach that regards sentences with the same re-
lations as positive pairs and other instances in the
same batch as the negative pairs. 5) MapRE (Dong
et al., 2021) extends CP with a relation encoder to
consider relation type information. 6) HCRP (Han
et al., 2021a) equips Proto-BERT with a hybrid
attention module and a task adaptive focal loss.
Wikipedia (filtered) is more challenging. Com-
paring models that use the original Wikipedia to
pre-train (the middle part of Table 3) and those that
use the filtered version of Wikipedia (the bottom
part of Table 3), we observe that the accuracy drops
drastically across all models, substantiating our
speculation that the performance gain of existing
pre-training efforts is partly due to the “knowledge
leakage” between the pre-training dataset and the
FewRel 1.0 dataset. Therefore, we call for atten-
tion on this issue. We suggest that the community7001
should focus more on the new setup that we pro-
pose to perform more rigorous evaluations on the
FSRE task in the future.
LPD makes better use of textual labels. When
LPD is also used in the pre-training stage (see the
last row of the bottom two blocks of Table 3), we
find that our model significantly outperforms the
previous methods, no matter if the pre-training is
performed on the original Wikipedia dataset or
on the filtered version. Specifically, HCRP and
MapRE also use textual label in their model, but
their models lead to sub-optimal performance, prov-
ing the effectiveness of dropping out textual labels
during training. Remarkably, when compared with
the previous state-of-the art HCRP, LPD improves
the 10-way-1-shot task by 7.08 points on the val-
idation set and 2.69 points on the test set when
the original Wikipedia dataset is used in the pre-
training stage.
Pre-training with LPD is important. Interest-
ingly, we find that our model does not outperform
HCRP when it is not pre-trained with LPD (in other
words, when LPD is only used during training and
testing), as shown in the three LPD setups with
no LPD in pre-train in Table 3. We hypothesize
this is because our method introduces a new input
format (i.e., label prompt followed by the context
sentence). As a result, it would be more beneficial
for our approach to have access to abundant data
in the pre-training stage, in order to learn how to
acquire the relevant information within the data of
such a new format. We carry out a more detailed
analysis regarding this matter in Section 6.3.
Knowledge leakage leads to performance drop
in domain transfer. To evaluate LPD’s ability
to transfer knowledge to datasets in a different do-
main, we train our model on the FewRel 1.0 train-
ing set and evaluate its accuracy on the FewRel
2.0 test set. In Table 4, we show that LPD sub-
stantially improves the results over the previous
state-of-the-art models with or without pre-training.
An important finding is that when LPD is pre-
trained with the unfiltered Wikipedia, its accuracy
is even lower than the LPD pre-trained on the fil-tered counterpart, even though the former is more
than twice the size of the latter (see Table 1). We
also run CP (Peng et al., 2020a) pre-trained with
Wikipedia (filtered), and observe a similar trend
to that of LPD. This again confirms our specula-
tion that much of the previous work’s performance
gain on FewRel 1.0 comes from the overlapping
relation types between the pre-training dataset and
FewRel 1.0. When the model is pre-trained with
the unfiltered Wikipedia, it overfits to the overlap-
ping relations. While such overfitting increases the
accuracy on FewRel 1.0 because the FewRel 1.0
test set also contains those overlapping relations, it
fails to generalize to FewRel 2.0 where all relation
types are truly novel and come from a different
domain.
6.2 Ablation Study on the Dropout Rate
The dropout rate αis a crucial hyperparameter
in LPD. In this section, we first study the ef-
fect of α by pre-training our model on the
Wikipedia (filtered) dataset, and testing it on the
FewRel 1.0 validation set without any training pro-
cess. As shown in Table 5, LPD is not very sensi-
tive to α . We can only see significant drop
ifα is below 0.1 or above 0.9. When the la-
bel prompt is always fed together with the context
sentence (i.e., α = 0.0), the model accuracy
is rather sub-optimal. This substantiates our claim
that the model will be over-reliant on the textual la-
bels if textual labels are always fed to the model to-
gether with the context sentences. Setting dropout
rate to be larger than 0.0 are essentially making the
learning more challenging (as compared to α
= 0.0), which forces the model to be more robust.
When we set α to 1.0, our model essentially
reduces to the contrastive learning framework pro-
posed by Peng et al. (2020b), which also results
in a lower accuracy, because the model does not
have access to the helpful textual label information.
Figure 3 shows the accuracy under different α
when we fix α to be 0.6. We can observe a
similar trend as that of α . The model main-
tains a rather consistent performance when α
is between 0.1 and 0.6. The accuracy drops when
αis close to 0 or 1.7002
6.3 Ablation Study on the Number of Relation
Types
As discussed in Section 6.1, our method performs
comparatively to HCRP when it is not pre-trained
with LPD, while it outperforms all baselines when
we include LPD in the pre-training stage. We hy-
pothesized that this is because our method intro-
duces a new input format (i.e., label prompt fol-
lowed by the context sentence). The new format
requires our model to use the label prompt to guide
the Transformer self-attention to output a better
representation while still outputting high quality
representations when the label prompt is dropped
out. Thus, the amount of relation types in FewRel
1.0 may not be sufficient for the model to make full
use of LPD. To validate this hypothesis, we only
use part of the Wikipedia (filtered) dataset to pre-
train the models, as shown in Figure 4. LPD (by
instance) shows LPD pre-trained with X% of the
instances in the pre-training dataset for each rela-
tion type. We also considered another setup where
we use X% relation types, shown by LPD (by class)
andCP (by class) . Comparing the by instance and
by class setup for LPD, we find that the model is
able to achieve a high accuracy with very small
portion of the data when we keep the number of
relation types fixed (by instance), while the perfor-
mance remains sub-optimal if we only use a small
set of the relation types (by class). This shows that
instead of the absolute amount of pre-training in-
stances, large number of relation types will be more
beneficial to our model, confirming our hypothe-
sis above. Comparing the by-class setup between
CP and LPD, we find that the accuracy of CP satu-
rates when 50% relation types are used, while LPD
only saturates when 70% relation types are used
for pre-training. This shows that LPD indeed needs
more relation types to be properly trained, while it
also has the better potential to benefit from a large
amount of relation types. The accuracy increase
brought by the pre-training for LPD is 7.86%, while
for CP it is 4.37%.
6.4 Analogy to Dropout
In this section, we show that LPD shares similar
properties to that of the dropout method used in
neural networks (Srivastava et al., 2014). In Table
6, model 3 essentially reduces to Proto-BERT dur-
ing training, but it has access to the label prompt
during testing. Model 3’s lower performance as
compared with LPD shows that it fails to make
good use of the label prompt, as the model is not
specifically trained for such input during learning.
In model 4, we equipped the model with LPD dur-
ing training, with αset to 0.4, but did not insert
any label prompts during prediction. Again, the
performance reduces to the same level as that of
Proto-BERT. This means that LPD during train-
ing does not result in a better model to encode the
relation representation if there is no label prompt
prepended to the context sentence during inference.
In addition, model 5 shows a drop in accuracy from
79.61 to 77.44, demonstrating that it is crucial to
setαto 0.0 to enable the model to extract useful
information from the label prompt for all support in-7003stances during prediction. Using the same dropout
rate during both training and testing will lead to
a sub-optimal performance. These five different
setups show that, interestingly, our proposed LPD
approach really shares similar properties to that of
the standard dropout method, which drops out a
subset of but not all of the neurons during learning,
and use all the neurons during prediction.
6.5 Analogy to Prompt
To demonstrate that the label prompt really serves
as the prompt, we first try to corrupt the information
contained inside the label prompt, shown in Table
6 as model 6 and model 7. In model 6, we corrupt
the label prompt by randomly deleting 50% of the
tokens of each relation description.In model 7,
instead of assigning the correct relation description
to the context sentence, we shuffle the descriptions
and randomly assign them to the sentences.We ob-
served severe decline in performance in both cases,
showing that adding informative and correct label
prompts are crucial for guiding the model to out-
oput a better relation representation. This property
is similar to the prompt-based model, where the
prompt design should be coherent to the task and
guide the model to make prediction at the [MASK]
spans (Brown et al., 2020; Liu et al., 2021).
To provide a qualitative examination of the effec-
tiveness of LPD, we pre-train LPD on Wikipedia
(filtered), train it on the FewRel 1.0 training set, and
visualize the relation representations of two similar
relations child andmother from the FewRel 1.0 val-
idation set using t-sne (van der Maaten and Hinton,
2008). As shown in Figure 5, our method is able to
yield a large margin between the support instances
of these two relations while maintaining a reason-
able distance between query instances of these two
relations. On the other hand, using relation de-
scriptions without dropout will render the model
relying too much on the relation descriptions, re-
sulting in poor separation between query instances
of the two relations, even though it achieves perfect
separation for support instances. If we only use
label prompts during testing, or simply discard all
the label prompts such that the model is reduced
to CP (Peng et al., 2020a), we can observe that
some of the support instances of the two relations
still fall in close proximity. We argue that the rela-
tion description really serves as a prompt to guide
and regularize the support instance representations
within the same class, while the model still retains
the ability to work without the label prompt in the
query set. Label prompts are able to promote the
generation of discriminative class representations,
which will benefit the FSRE task.
7 Conclusion
This paper proposes a novel label prompt dropout
approach that directly concatenates the label
prompt with the context sentence for few-shot re-
lation extraction. The label prompt is randomly
dropped out during pre-training and training to cre-
ate a more challenging learning setup, leading to
better use of the relation descriptions. In the exper-
iments, we discover a “knowledge leakage” issue
in the previous works’ experimental setup. We pro-
pose a stricter setup for more rigorous evaluations
in FSRE by filtering out all overlapping relations.
Our method has demonstrated significant improve-
ments on both evaluation settings. Ablation studies
show that LPD shares some similar and interest-
ing properties to the neural dropout operation and
prompt based methods. One possible direction
of future work is to generalize this idea to other
text classification tasks such as intent classification
(Larson et al., 2019).7004Limitations
There are several limitations of this work. First,
LPD only works under the N-way- K-shot setup,
because it requires a support set in which the tex-
tual labels are given to construct the label prompts.
Second, its effectiveness is only examined on the
task of few-shot relation extraction, while whether
this method is able to generalize to other text clas-
sification tasks, such as intent classification and
news classification, is not yet explored in this pa-
per. Third, whether the model has the ability
to perform non-of-the-above detection (Gao et al.,
2019b), where a query instance may not belong to
any class in the support set, is not investigated in
this work.
Acknowledgements
We would like to thank the anonymous reviewers,
our meta-reviewer, and senior area chairs for their
constructive comments and support on our work.
We would also like to thank Vanessa Tan for her
help with this work. This research/project is sup-
ported by the National Research Foundation Singa-
pore and DSO National Laboratories under the AI
Singapore Program (AISG Award No: AISG2-RP-
2020-016).
References70057006