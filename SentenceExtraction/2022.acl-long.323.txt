
Ying Su, Hongming Zhang, Yangqiu Song, Tong ZhangHKUSTTencent AI lab, Seattle
ysuay@connect.ust.hk ,{hzhangal,yqsong}@cse.ust.hk ,
tongzhang@ust.hk
Abstract
Word sense disambiguation (WSD) is a cru-
cial problem in the natural language processing
(NLP) community. Current methods achieve
decent performance by utilizing supervised
learning and large pre-trained language mod-
els. However, the imbalanced training dataset
leads to poor performance on rare senses and
zero-shot senses. There are more training in-
stances and senses for words with top frequency
ranks than those with low frequency ranks in
the training dataset. We investigate the sta-
tistical relation between word frequency rank
and word sense number distribution. Based
on the relation, we propose a Z-reweighting
method on the word level to adjust the training
on the imbalanced dataset. The experiments
show that the Z-reweighting strategy achieves
performance gain on the standard English all
words WSD benchmark. Moreover, the strat-
egy can help models generalize better on rare
and zero-shot senses.
1 Introduction
Word sense disambiguation (WSD) has been a long-
standing problem in natural language processing
community. The task can benefit many downstream
applications (Navigli, 2009), including but not lim-
ited to machine translation (Vickrey et al., 2005;
Pu et al., 2018) and information retrieval (Stokoe
et al., 2003; Zhong and Ng, 2012).
The goal of the WSD task is to disambiguate
word senses given contexts. For example, the word
“lift” in the context “ Lifta load” and “The detective
carefully lifted some fingerprints from the table”
has different meanings. The former one means
“raise from a lower to a higher position” and the
latter one means “remove from a surface”. From
semantic recognition of human being, the former
sense is easier to disambiguate as it is the most
common sense of the word while the latter one is a
relatively rare one.A skewed distribution exists in SemCor (Miller
et al., 1993), a commonly used human-labeled
dataset for the WSD task, where most common
senses have many training examples while rare
senses have much fewer examples. A large cover-
age of senses are not accompanied with training
examples, which are called zero-shot senses. Many
deep neural-networks-based methods are affected
by this imbalanced training corpora (Luo et al.,
2018; Huang et al., 2019b).
Previous approaches attempt to address this prob-
lem by designing a new dataset or task specifically
for the rare senses and zero-shot senses (Holla et al.,
2020; Blevins et al., 2021; Barba et al., 2021) or en-
riching the sense embeddings by incorporating ex-
ternal lexical knowledge (Kumar et al., 2019; Scar-
lini et al., 2020; Blevins and Zettlemoyer, 2020).
Different from these methods, we address the un-
balanced training issue from the perspective of ad-
justing the learning process.
An interesting human language phenomenon is
that it follows a statistical distribution described by
Zipf’s law (Zipf, 1949), which also exists in many
corpora including SemCor. From the linguistic per-
spective, an explanation for Zipf’s law is that peo-
ple tend to use more common words to minimize
the communication effort (Zipf, 1949). Inspired by
this, we consider a word with top rank in frequency
should be assigned high training weight.
From the statistical perspective, two laws have
been proposed to explain Zipf’s law in word fre-
quency, namely the meaning-frequency law (Zipf,
1945) and Zipf’s law of abbreviation (Florence,
1950; Grzybek, 2006). The meaning-frequency
law proposes that more frequent words have larger
number of word senses, which we also denote as
larger word #sense. Based on this, we calculate
the word #sense distribution in SemCor and use a
mathematical function to fit the relation between
word rank and word #sense. Based on the relation,
we design the Z-reweighting strategy on the word4713level to help models generalize better to rare and
zero-shot senses.
To the best of our knowledge, we are the first to
leverage linguistic distribution to address the train-
ing bias on the WSD task. Our method improves
the generalization ability of deep neural models on
rare senses and zero-shot senses. Results on all
English words WSD evaluation benchmarks show
that our system achieves improvement on rare and
zero-shot senses by 2.1% and 3.6% on F1 score.
Furthermore, our strategy outperforms the system
without any reweighting strategy and achieves a
performance gain on the F1 score on all senses. We
open source our code.
2 Related works
2.1 Word Sense Disambiguation
Word sense disambiguation is to distinguish the
sense of a specific word given a context sentence.
Current methods can be broadly classified into two
streams, supervised-learning-based and knowledge-
based. Supervised-learning-based approaches view
the WSD task as a classification problem. For ex-
ample, Zhong and Ng (2010) learn classifiers inde-
pendently for each word. Knowledge-based meth-
ods, such as (Banerjee et al., 2003; Basile et al.,
2014), mainly exploit two kinds of knowledge: 1)
the gloss, usually in the form of a sentence defin-
ing the word sense; 2) graph structure of lexical
resources.
Recent researches integrate supervised learning
and knowledge into a unified system and achieve
better performance than systems relying on knowl-
edge only. For utilizing gloss, GlossBert (Huang
et al., 2019b) constructs context-gloss pairs and
conducts sentence-pair classification training. Bi-
encoder (Blevins and Zettlemoyer, 2020) proposes
an end-to-end learning system to train the embed-
ding space of context words and senses together.
For utilizing structure properties, EWISE (Kumar
et al., 2019) injects gloss and knowledge graph em-
bedding into sense embeddings. EWISER (Bevilac-
qua and Navigli, 2020) further injects relational
knowledge as additional supervision.
Different with the previous approaches, we focus
on addressing training bias caused by the imbal-
anced distribution in the training dataset. In this
paper, we analyze the formulation of the distri-
bution and propose the Z-reweighting method toimprove performance on rare and unseen senses.
2.2 Zipf’s Law in Word Frequency
Power law distribution widely exists in human lan-
guage, where the word frequency can be described
by Zipf’s law (Zipf, 1949). Previous works show
that the linguistic law exists in many corpora, in-
cluding SemCor (Miller et al., 1993), CHILDES
(MacWhinney, 2000), and Wikipedia (Grefenstette,
2016). SemCor is also one of the largest train-
ing datasets for the WSD task, which also in-
cludes Ontonotes (Marcus et al., 2011) and OMSTI
(Taghipour and Ng, 2015).
Manin (2008) argues from the semantic view and
proposes that the word semantics are influenced by
the expansion of word meanings and competition of
synonyms results in the law. Zipf (1945) proposes
that word frequency is related to its word #sense,
in which more frequent words have larger word
#sense. Recently, Casas et al. (2019) investigates
the law from the perspective of both word #sense
and word length. Similarly, our work takes consid-
eration of word #sense distribution and utilizes it
for balanced training on the WSD task.
2.3 Learning Imbalanced Dataset
There are many approaches to address the influ-
ence on learning brought by imbalanced training
data under a supervised setting. Most of the algo-
rithms belong to re-weighting (Huang et al., 2016,
2019a) or re-sampling (Buda et al., 2018; Cui et al.,
2019). Re-weighting methods adjust the weights
of different classes. Re-sampling methods balance
the learning by over-sampling minority classes or
under-sampling the frequent classes. Another line
of works incorporates the idea of angular margin,
aiming to enlarge the intra-class margin (Liu et al.,
2016; Wang et al., 2018; Cao et al., 2019).
Our work follows the line of re-weighting. We
take consideration of the word #sense distribution
and propose Z-reweighting method for the WSD
task, which is quite different with previous re-
weighting methods.
3 Distribution Analysis in SemCor
In this section, we first show the overall word and
sense distribution in SemCor(Miller et al., 1993).
Since we propose to utilize the word #sense distri-
bution as the basis for the Z-reweighting strategy,4714Type Total num MCS LCS
Instance 226,036 166,361 59,675
Sense 33,316 22,320 10,996
Avg. Ins. 6.78 7.45 5.43
Word num 22,436 22,320 5,495
Word #sense 54,203 53,795 26,217
Avg. #sense 2.41 2.41 4.77
we further look into the relationship between word
rank, frequency, and word #sense.
3.1 Imbalanced Data Distribution in SemCor
As mentioned in (Kilgarriff, 2004), a Zipfian distri-
bution exists in the word senses of human language.
In this part, we investigate the details of the distri-
bution in training data of SemCor on both the word
level and the sense level.
Senses in WordNet are generally ordered from
most to least frequently used. The most common
sense is ranked first, denoted as MCS . We denote
other senses of a word as least common senses
LCS . Following this definition, we calculate the
distribution of training data in the SemCor corpus,
and the resulting distribution is shown in Table 1.
SemCor contains 226,036 training instances, where
each instance is a sentence with a labeled sense of
one word. Among all the instances, 73.5% are train-
ing instances for MCS, belonging to 22,320 words
and the rest are for LCS. LCS has 5.43 training
instances for each sense, much lower than MCS
which has 7.45 instances on average.
We further investigate the word #sense distribu-
tion of training words labeled with MCS and LCS
respectively. The word #sense defined in WordNet
is utilized to calculate the distribution. The average
word #sense for training words labeled with LCS is
4.77, much greater than that of MCS. This shows
that words labeled with LCS have a larger cover-
age of senses to distinguish. The words with LCS
in training data SemCor has higher word #sense
while with fewer training instances. Therefore, we
can see that disambiguating LCS is much more
challenging than MCS in the WSD task.
3.2 Word Rank, Frequency, and Word #Sense
To investigate the details of the Zipfian distribution
in SemCor, we calculate the number of training
instances and word #sense for each word and sort
them by frequency in descending order.
We apply a binning technique to reduce noise
and get a better view of Zipf’s law on word dis-
tribution. Specifically, every adjacent 300 words
belong to a bin for clear analysis in this part. The
distribution of instance number with sorted word
rank by decreasing frequency is shown in Figure
1. As we can see, top ranked words have much
more training cases than low ranked words both for
training words labeled with MCS and LCS.
To get a deeper understanding of the statistical
law in word frequency, we further analyze the re-
lation between word #sense and sorted word rank.
Similar to training instances, we calculate the aver-
age #sense of every 300 words in a bin and get the
distribution of word #sense with the sorted word
rank by decreasing frequency. As shown in Figure
2, the words with top rank have larger #sense than4715words with low rank. This shows that words with
the top frequency rank have more senses to disam-
biguate. Moreover, words with LCS are mostly
with top ranks.
4 Algorithms
In this section, we first introduce the terminol-
ogy for the WSD task. Then we illustrate our
Z-reweighting strategy on adjusting the training
loss for the imbalanced training dataset.
4.1 Terminology
The WSD task is to disambiguate the meanings
of a set of words w={w, w, ..., w}given a
context sentence S. Each context word w, i∈
[1, n]in a sentence Shas several candidate senses
{s, s, ..., s}. Each sense is described by a
definition sentence, also called gloss in WordNet
(Miller, 1998). The candidate senses have a corre-
sponding gloss set {g, g, ..., g}.
4.2 Z-Reweighting Strategy
To alleviate the influence brought by the im-
balanced training dataset, we propose the Z-
reweighting strategy to balance the learning be-
tween MCS and LCS during training, resulting in a
stronger capability of the model in disambiguating
rare and zero-shot senses while maintaining com-
parable performance on MCS at the same time.
Training words in SemCor are denoted in form
W={W, W, ..., W}with descending order of
frequency. The #sense of a word represents the
number of senses belonging to the word. P=
{p, p, ..., p}is the #sense array of the words.
To facilitate the analysis of word #sense, we use
a bin parameter Kto group the words. Average
#sense array is calculated for every Kwords as:
p=p
K, o∈[1,N
K], d∈[1, N],(1)
P={p, ..., p}, (2)
As analyzed in (Casas et al., 2019), a power law
exists between word frequency and #sense in the
corpora CHILDES (MacWhinney, 2000). Simi-
larly, we utilize a function f(x) =aln(x+b) +c
to fit the relation between word #sense Pand word
ranko= [1,2, ...,]in SemCor mathematically,
where a, b, c are parameters. The fitting function
is monotonic decreasing with word rank. An ex-
ample of the fitting curve and original word #sensedistribution with word frequency at K= 300 is
shown in Figure 2.
With the same word ranks, a smoothed word
#sense array can be calculated from the fitting
curve as:
P={p, ..., p}, (3)
The discrete fitting word #sense array is normal-
ized for further processing:
P=P
max( P), (4)
Since the number of words is too large to as-
sign each word a weight, thebins of words are
further split into Mgroups. For word in k-th bin,
k∈[1,], belonging to group j∈[1, M], the
regularized #sense satisfies:
p≤p< p, (5)
where P={p, ..., p}is the threshold array to
split the groups. The words in group j∈[1, M]
are assigned weight:
α= (p), (6)
where ηis a power parameter.
Assume the predicted output probabilities
from a model for candidate sense set as z=
[z, z, ..., z], the standard cross entropy loss
given true word sense label y is:
loss(w, y) =−log(exp(z)exp(z)).(7)
In Z-reweighting strategy, the weight αis used
to adjust the training on word level. The new
weighted training loss is:
loss(w, j, y) =−αlog(exp(z)exp(z)),(8)
where i∈[1, N]andj∈[1, M], representing the
word with rank iin group jhas training weight α.
5 Experiments
In this section, we first introduce the training
dataset and evaluation metrics. Then we show dif-
ferent baseline methods. Finally, details of the
training process are presented.47165.1 Dataset
SemCor 3.0 is used as the training dataset. Five
standard WSD datasets from Senseval and Se-
mEval competitions are used as evaluation set.
Among them, semeval 2007 (Pradhan et al., 2007)
is used as development dataset for selecting the
best model. Other four datasets including senseval-
2 (Palmer et al., 2001), senseval-3 (Snyder and
Palmer, 2004), semeval2012 (Navigli et al., 2013)
and semeval2015 (Moro and Navigli, 2015) are
used as test datasets. We select F1 as the evaluation
metric. We also follow previous works (Raganato
et al., 2017) to report the overall performance on all
datasets. For further analysis, F1 scores on MCS,
LCS, and zero-shot senses are also calculated.
5.2 Baselines
BEM framework (Blevins and Zettlemoyer, 2020)
without any balanced strategy is a baseline system.
In addition, different balanced training methods
applied on BEM framework are used as three
more baseline systems. The balanced methods are
classified into two levels, namely, the sense-level
(balanced reweighting and margin based method
LDAW (Cao et al., 2019)), and the word-level
(balanced resampling).
Biencoder Model (BEM). The model utilizes the
gloss knowledge from WordNet. The two en-
coders are initialized with the same pre-trained
language model. The encoders take a context sen-
tence and glosses as input, generating represen-
tations for word wand corresponding gloss set
{g, g, ..., g}asEand{G, G, ..., G}sepa-
rately. Based on the representations, the similarity
score between words and glosses are calculated as:
z=E·G, j∈[1, m]
A standard cross-entropy loss is used in training as
Equation 7.
Balanced Reweighting Method (B-reweighting).
The B-reweighting strategy is applied on the sense
level. For each word, the weights of senses is pro-
portional to the inverse of training instances.
loss(w, y) =−βlogexp(z)exp(z),
where β=forl∈[1, ..., m ]andn
is the number of training instances on sense lofw.Balanced Resampling Method (B-resampling).
The B-resampling method is applied on the word
level. Firstly each word is sampled with the
same probability. Then the training cases of the
selected word are sampled randomly. Standard
cross-entropy loss is used in this method.
LDAM. The margin-based method adjusts the train-
ing on the sense level. The goal of LDAM is to
solve the class-imbalance problem by utilizing a
label-distribution-aware margin loss. We apply the
LDAW loss on the sense level as another baseline.
The smoothed relaxation of LDAM in the cross-
entropy loss with enhanced margins is as follows:
loss (w, y) =−loge
e+e,
where ∆=, forl∈ {1, . . . , m }andCis a
constant. nis the training instances number of
sense jfor word w.
Standard LDAW is trained in two stages. Firstly
the label-distribution-aware margin loss is ap-
plied to train the model for three epochs and B-
reweighting loss is used for further training. In
both stages, the learning rate is always 1e-5. For
first stage training, C is set as 0.5.
5.3 Implementation details
Baseline Systems. Each system is trained for
20 epochs. AdamW (Kingma and Ba, 2015)
is selected as the optimization algorithm. The
learning rate is fixed at 1e-5 during training. The
encoders in the biencoder framework both are
initialized with bert-base (110M parameters)
or bert-large (336M parameters) (Kenton and
Toutanova, 2019). The experiments in which
encoders initialized with bert-base are run on
RTX 2080 and the experiments in which encoders
initialized with Bert-large are run on RTX 3090.
Average running hours is 30 hours for Bert-base
and 40 hours for Bert-large.
Z-reweighting. To simplify the mathematical func-
tion fitting of word #sense distribution, we first
split the words into bins by setting a fixed group
number K. For the second grouping stage, to sim-
plify the reweighting strategy on word level, we
use thresholds to group the smoothed values cal-
culated by fitting curve given word rank. In the
experiments, we use weights of 1 decimal place
as thresholds. The defined threshold array is as4717Test Datasets
model SE07 SE2 SE3 SE13 SE15 ALL
WordNet S1 55.2 66.8 66.2 63.0 67.8 65.2
MFS 54.5 65.6 66.0 63.8 67.1 65.5
EWISE (Kumar et al., 2019) 67.3 73.8 71.1 69.4 74.5 71.8
BERT-base (Kenton and Toutanova, 2019) 68.6 75.9 74.4 70.6 75.2 73.7
GlossBERT (Huang et al., 2019b) 72.5 77.7 75.2 76.1 80.4 77.0
BEM (Blevins and Zettlemoyer, 2020) 72.878.877.277.881.478.1
B-resampling 60.471.568.872.874.770.9
B-reweighting 71.378.475.575.680.477.1
LDAW 71.378.675.476.680.377.1
Z-reweighting 71.979.676.578.982.578.6
Senses
model MCS LCS Zero-shot
BEM 93.451.7 67.2
B-Resampling 84.946.4 70.0
B-Reweighting 89.555.4 70.2
LDAW 92.152.8 68.6
Z-reweighting 92.953.8 70.8
P= [1.0,0.9, ...,0.1], where the gap between
thresholds is 0.1. For assigning weights, η= 1,2
is used to adjust the value of weight. For example,
words with regularized word #sense in [0.3,0.4)
are in a group, assigning weight 0.16 when η= 2.
The weight is further rounded with one decimal
number as 0.2. If the rounding weight is less
than 0.1, we use a weight of 0.1. For compari-
son with baselines, we set K= 300 , η= 2in the
Z-reweighting strategy.
6 Results
In this section, we first analyze the overall perfor-
mance on the test datasets using different training
strategies. Then the details of improvement on
MCS, LCS, and zero-shot senses for word groups
are presented. Finally, we analyze the influences
brought by hyper-parameters in Z-reweighting andinfluences brought by backbone models.
6.1 Overall Performance
The performance on test datasets by different sys-
tems are shown in Table 2. WordNet S1 uses the
most common sense in WordNet and MFS uses the
most frequent sense in the training dataset. Both the
baselines achieve much lower performances than
previous learning based systems, including BERT-
base (Kenton and Toutanova, 2019), GlossBERT
(Huang et al., 2019b) and BEM. The sense em-
beddings in EWISE is fixed during training, which
explains its much lower performance than Gloss-
BERT and BEM.
The F1 score on MCS, LCS and zero-shot senses
in ALL testset, with 4,603, 2,650, and 1,139 test in-
stances respectively, are reported in Table 3. Com-
paring BEM with systems with balancing strategies,
only Z-reweighting achieves performance gain on
LCS and zero-shot senses while maintaining com-
parable performance on overall performance at the
same time. Details show that though Z-reweighting
slightly drops on MCS, performance on LCS and
zero-shot senses increases 2.1% and 3.6% sepa-
rately.
Besides the Z-reweighting method, B-
reweighting and LDAW also show performance
improvement on LCS and zero-shot senses,
comparing with the BEM baseline. However, these
balanced strategies deteriorate the system ability in4718
disambiguating MCS, resulting in the drop of F1
score on the ALL dataset.
Among all the balanced training strategies, B-
resampling performs the worst. Equally sampling
the words leads to insufficient training of top-
ranked words, which results in poor performance.
Our Z-reweighting strategy outperforms all the
other balanced training strategies, indicating that
our method is effective in improving the general-
ization ability of the model.
6.2 LCS, Zero-shot Senses in Word Groups
Among all the balanced strategies, only Z-
reweighting outperforms baseline system BEM on
the F1 score of ALL dataset. To look into how
the Z-reweighting strategy works, we analyze the
details of performance in the word groups. Not-
ing that according to the Z-reweighting strategy,
words in each group are assigned the same weight.
The hyper-parameters are K= 300 , η= 2 for
our results. Under the setting, there are six groups
of words from training dataset. These six groups
of words are sorted by decreasing frequency or-
der. The left words belong to a oovgroup in which
words are not shown in the training dataset.
We calculate the F1 score of LCS and zero-
shot senses of ALL test set and plot the results
in Figure 3 and Figure 4 separately. In Figure 3,
our system outperforms BEM on group one, in
which the words are with highest frequency. The
Z-reweighting strategy assigns the largest weight
in this group and the F1 score improves 3.4%. For
group 4 to group 7, our algorithm also shows con-
sistent improvements. The performance gain drops
in group 2. The reason behind this is that we man-
age to improve the performance of all words on the
WSD task and use semeval2007 as development
set for model selection.
For zero-shot senses shown in Figure 4, our sys-
tem achieves improvement in five out of seven word
groups, at most 24.3% for group five. The results
show that the Z-reweighting strategy enables the
model to generalize better to unseen senses. For
words in group seven, the performance of zero-shot
senses also improves, which shows that our method
can further generalize better to senses of unseen
words.
6.3 Impact of Kandηin Z-reweighting
Each different bin number Kresults in a set of
distinct weights for training words. In our exper-
iments, we set K= 50 ,100,200,300,400 and
η= 1,2.
The performances of our system under differ-
ent hyper-parameter settings are shown in Figure
5 and Table 4. In Figure 5, we can see that η= 2
achieves higher performance than η= 1 in most
settings. This shows that the training weights with
larger disparity on top and low ranked words result
in higher performance on the overall score. The
best performance achieves at K= 300 , η= 2.
It is interesting to see that with different hyper-
parameters, the system has various overall scores.
When K= 400 , the overall score achieves low-
est both for η= 1 andη= 2. It indicates that
largeKeliminates the weight distinctness between
words during training, leading to drop on overall
performance.
To explore the details of effects brought by4719
hyper-parameters, we further show the F1 score
of MCS, LCS, and zero-shot senses in Table 4.
The accuracy of MCS varies from 92.8% to 93.3%
under different settings. For most of the groups,
MCS achieves higher performance with η= 1than
η= 2. When the gap between weights becomes
larger with η= 1 changing to η= 2, the influ-
ences on LCS and zero-shot senses are greater than
those on MCS. LCS achieves the best score 54.3%,
1.4% higher than the lowest score. Zero-shot senses
achieve the best score of 72.3%, 1.4% higher than
the lowest score. For all the combinations, we can
see improvements on LCS and zero-shot senses
compared to baseline BEM, demonstrating the ef-
fectiveness of our strategy.
6.4 Impact of Backbone Models
In this section, we show the influences brought by
backbone models in BEM. The encoders of BEM
are initialized by Bert-base and Bert-large models
respectively for comparison. For Z-reweighting
strategy, we use K= 300 , η= 2. Training param-
eter settings are the same with the two backbone
models. We experiment with different balanced
training strategies. The results are presented in Fig-
ure 6. From the figure we can see that Bert-large
achieves better performance on BEM and LDAM
systems. For B-reweighting and Z-reweighting sys-
tems, the overall scores remain almost the same.
However, for the B-resampling strategy, the per-
formance drop 1%. Since the performances on
Bert-large are nearly the same or even worse than
Bert-base, we use Bert-base as the backbone model
for training efficiency.Senses
Parameter MCS LCS Zero-shot
K=50,η=1 93.3 53.0 70.9
K=50,η=2 93.0 53.5 71.6
K=100, η=1 93.3 53.1 71.5
K=100, η=2 93.1 53.7 72.3
K=200, η=1 93.2 53.4 71.6
K=200, η=2 92.9 54.0 71.9
K=300, η=1 92.9 53.4 71.5
K=300, η=2 93.0 54.3 72.2
K=400, η=1 93.2 52.9 71.3
K=400, η=2 92.8 53.7 72.2
7 Conclusion
In this paper, we address the problem in learn-
ing imbalanced training dataset on the WSD task.
Words with top frequency rank have more senses to
disambiguate both for MCS and LCS. We assume
these words should be assigned larger weights dur-
ing training. Specifically, we use a mathematical
function to fit the relation between word rank and
word #sense, and utilize smoothed #sense to de-
sign the Z-reweighting strategy for all words En-
glish WSD task. The strategy leads to improvement
on the performance of LCS and zero-shot senses
on standard English WSD evaluation benchmarks.
Furthermore, our method achieves performance
gain on the F1 score for all senses. The results
demonstrate the effectiveness of our methods.4720Acknowledgements
This research was partially supported by the
NSFC Fund (U20B2053) from the NSFC of
China, the RIF (R6020-19 and R6021-20) and the
GRF (16211520) from RGC of Hong Kong, the
MHKJFS (MHP/001/19) from ITC of Hong Kong
with special thanks to HKMAAC and CUSBLT,
and the Jiangsu Province Science and Technology
Collaboration Fund (BZ2021065). We thank our
colleague Tianqing Fang for providing insightful
discussion and help in the research.
References472147224723