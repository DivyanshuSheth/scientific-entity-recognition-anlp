
Mitja Nikolaus
mitja.nikolaus@univ-amu.fr
Emmanuelle Salinand Stephane Ayacheand Abdellah Fourtassiand Benoit FavreAix Marseille Univ, Université de Toulon, CNRS, LIS, Marseille, FranceAix-Marseille Univ, CNRS, LPL, Aix-en-Provence, France
Abstract
Recent advances in vision-and-language model-
ing have seen the development of Transformer
architectures that achieve remarkable perfor-
mance on multimodal reasoning tasks. Yet, the
exact capabilities of these black-box models are
still poorly understood. While much of previ-
ous work has focused on studying their ability
to learn meaning at the word-level, their ability
to track syntactic dependencies between words
has received less attention.
We take a first step in closing this gap by cre-
ating a new multimodal task targeted at evalu-
ating understanding of predicate-noun depen-
dencies in a controlled setup. We evaluate a
range of state-of-the-art models and find that
their performance on the task varies consider-
ably, with some models performing relatively
well and others at chance level. In an effort to
explain this variability, our analyses indicate
that the quality (and not only sheer quantity) of
pretraining data is essential. Additionally, the
best performing models leverage fine-grained
multimodal pretraining objectives in addition
to the standard image-text matching objectives.
This study highlights that targeted and con-
trolled evaluations are a crucial step for a pre-
cise and rigorous test of the multimodal knowl-
edge of vision-and-language models.
1 Introduction
Vision-and-language (V&L) models have recently
shown substantial improvement on a range of mul-
timodal reasoning tasks. Taking inspiration from
successes in text-only Natural Language Process-
ing (Devlin et al., 2019; Brown et al., 2020), state-
of-the-art V&L models are usually composed of
a Transformer-based architecture pre-trained in a
self-supervised manner on large-scale data, and
then fine-tuned on downstream tasks.
While these models show remarkable perfor-
mance on a range of tasks, more controlled and
systematic analyses are necessary in order to obtainFigure 1: We evaluate V&L models on their ability to
track predicate-noun dependencies that require a joint
understanding of the linguistic and visual modalities.
The task is to find the correct sentence (choosing be-
tween the target and distractor) that corresponds to the
scene in the image. In this example, the models should
connect the predicate “is wearing a hat” to “man”. A
model that does not track dependencies would judge
the distractor sentence “A man is wearing glasses” as
equally likely, as there is a man is the image, as well as
a person that is wearing glasses.
a better understanding of their exact multimodal
knowledge.
A range of studies has investigated their abil-
ity to map words to their visual referents for
nouns (Kazemzadeh et al., 2014; Mao et al., 2016;
Shekhar et al., 2017) and verbs (Ronchi and Per-
ona, 2015; Yatskar et al., 2016; Pratt et al., 2020;
Hendricks and Nematzadeh, 2021), but there are
only a few studies on whether recent V&L mod-
els can capture multimodal syntactic dependencies
between words and concepts.
In this paper, we explore how well V&L models
learn predicate-noun dependencies across modal-
ities (see example in Figure 1). To this end, we
create an evaluation set that contains carefully se-
lected images and pairs of sentences with minimal1538differences. Given an image and two predicate-
noun sentences, the models need to find the correct
sentence corresponding to the image. Crucially,
they can only succeed by taking into account the
dependencies between the visual concepts in the
image corresponding to the noun and predicate in
the sentence.
As it has been shown that visual reasoning per-
formance in several tasks can be spuriously aug-
mented by capitalizing on textual biases in the
training data (Goyal et al., 2017; Agrawal et al.,
2018; Hendricks et al., 2018; Cao et al., 2020), we
counter-balance our evaluation dataset in a way that
controls for such exploitation of linguistic biases.
We evaluate pre-trained state-of-the-art V&L
models in a zero-shot setting and find that the
ability to track predicate-noun dependencies varies
considerably from model to model. Of all models
tested, UNITER (Chen et al., 2019) and LXMERT
(Tan and Bansal, 2019) show the highest scores, but
their performance is still far from optimal. Other
models such as ViLBERT (Lu et al., 2019) and
CLIP (Radford et al., 2021) perform at chance level.
We discuss how differences in the models could ex-
plain their performance variability, highlighting the
role of pretraining data quality and fine-grained
multimodal pretraining objectives.
Code to reproduce the analyses and run the
evaluation on new models is publicly avail-
able at https://github.com/mitjanikolaus/
multimodal-predicate-noun-dependencies .
2 Related Work
Targeted evaluation of V&L models Recently,
a growing number of tasks have been created for
targeted evaluation of V&L models’ abilities to
perform various multimodal reasoning.
Shekhar et al. (2017) create sets of distractor
captions to analyze whether V&L models are sen-
sitive to single word replacements (with a focus on
nouns). Similar targeted evaluation datasets have
also been proposed for referring expressions (Chen
et al., 2020), image-sentence matching (Hu et al.,
2019), and Visual Question Answering (VQA; Bo-
gin et al., 2021), with a focus on compositional
reasoning.
Tasks such as visual semantic role labeling or
situation recognition, typically involve classifying
the primary activity depicted in an image, as well
as the semantic roles of involved entities (Ronchi
and Perona, 2015; Lu et al., 2016; Chao et al., 2015;Gupta and Malik, 2015; Yatskar et al., 2016; Pratt
et al., 2020). While these studies demonstrate that
V&L models can learn semantic roles to some de-
gree in a supervised learning setup, such tasks do
not allow for a controlled evaluation of models in a
zero-shot setting.
In Hendricks and Nematzadeh (2021), the au-
thors evaluate state-of-the-art V&L models in a
controlled zero-shot setup and find that they still
have more trouble understanding verbs compared
to subjects or objects. They also observe that mod-
els trained on larger datasets with less descriptive
captions perform worse than models trained on
smaller, manually-annotated datasets.
Several works have also tried to shed more light
on the precise multimodal semantic capabilities of
V&L models using probing techniques. Salin et al.
(2022) show that although state-of-the-art V&L
models can grasp some multimodal concepts such
as color, they still do not fully understand more
difficult concepts such as object size and position
in the image. Parcalabescu et al. (2021) use prob-
ing to demonstrate that such models still lack the
capability to correctly count entities in an image.
Evaluation of grounded syntax Akula et al.
(2020) tests for sensitivity to word order in refer-
ring expressions. Similarly, Thrush et al. (2022)
studies the ability of V&L models to take word or-
der into account by designing adversarial examples
that require differentiating between similar image
and text pairs, while the text pairs only differ in
their word order. Their results suggest that state-
of-the art models still lack precise compositional
reasoning abilities.
Li et al. (2020a) studies so-called syntactic
grounding of VisualBERT. They show that certain
attention heads of the transformer architecture at-
tend to entities that are connected via syntactic
dependency relationships. However, such probing
experiments do not necessarily indicate to what
degree a model is actually using the encoded infor-
mation when making predictions.
In our work, we test a range of state-of-the-
art models specifically on their ability to track
predicate-noun dependencies. Crucially, we test
the models in a much more controlled setting com-
pared to previous work: Our setup involves visual
distractors as well as control task, disentangling the
challenge of understanding syntactic dependencies
from more simple object and predicate recognition.
Additionally, we strictly control for any possible1539linguistic bias by counter-balancing all evaluation
examples.
3 Methods
3.1 Evaluation Dataset
We construct an evaluation dataset that is suited
for evaluating the sensitivity to visually grounded
predicate-noun dependencies in a zero-shot setup.
The data consists of pairs of triplets, and each
triplet consists of an Image I, a target sentence S,
and a distractor sentence S. Target and distrac-
tor sentences are minimal pairs, i.e. one sentence
differs from the other only with regard to either
the noun (e.g., “A girl is sitting.” vs. “A man is
sitting.”, Figure 2) or the predicate (e.g., “A man
is wearing a hat.” vs. “A man is wearing glasses.”,
Figure 1).
Crucially, the images always contain visual dis-
tractors, meaning that both the noun and the pred-
icate of the distractor sentence are present in the
image, but they do not have a noun-predicate rela-
tionship (e.g., for the distractor sentence “A man
is wearing glasses”, there is a man in the image,
who is not wearing glasses, and a person wearing
glasses, who is not a man). Thus, it is necessary
to take into account the dependency between noun
and predicate to distinguish between the target and
distractor sentence (Figure 1).
Controlling for linguistic biases V&L models
have shown to rely sometimes on textual bias in-
stead of using visual information (Goyal et al.,
2017; Agrawal et al., 2018; Hendricks et al., 2018;
Cao et al., 2020). For example, if a training dataset
contains more often the phrase “a girl is sitting”
than “a man is sitting”, a model might prefer the
caption “a girl is sitting” during evaluation only
based on linguistic co-occurrence heuristics, irre-
spective of the visual content. In our evaluation
dataset, we control for potential linguistic biases in
the training datasets by pairing every triplet with
a corresponding counter-balanced example where
target and distractor sentence are flipped. More
specifically, for every triplet (I, S, S), there ex-
ists a corresponding triplet (I, S, S), as depicted
in Figure 2. In that way, a model that does not take
into account the visual modality cannot succeed in
the task (see also Nikolaus and Fourtassi, 2021).
Automatic pre-filtering Our evaluation dataset
is based on Open Images (Kuznetsova et al., 2020).
We pre-filter the images based on existing human-
annotated object and relationship labels and bound-
ing boxes. The objects refer to persons, animals,
as well as inanimate objects. The relationships can
either describe an action that an object is engaged
in (e.g., -), or an action linking two
objects (e.g., - - ). All nouns
in the selected relationships for our dataset refer to
persons, due to lack of sufficient annotations for
other kinds of agents.
We look for images that contain a target object-
relationship pair as well as a distractor object-
relationship pair for which either the target and
distractor object are the same, but the relationships
differ, or vice versa (as in the example in Figure 1).
Additional details on the pre-filtering can be found
in Appendix A.1.
Manual selection We manually select suitable
images after the automated pre-filtering, in order to
ensure high quality of each example and in particu-
lar to verify that the distractor sentences are indeed
incorrect given the images. This step is crucial,
because many of the annotations in Open Images
are incomplete, and an image may contain, for ex-
ample, a woman that is sitting but not annotated as
such (in this case, we disregard the image for our
evaluation set).
We select pairs of examples and counter-
examples and ensure that there are no duplicate
images within the set of images for each object-
relationship pair.
Sentence generation We generate target and dis-
tractor sentences based on the verified object and
relationship annotations from Open Images.
We construct English sentences using a template-
based approach. Given an object and a relationship,1540we add the indefinite article (a/an) in front of each
noun and use all verbs in present progressive tense
as this is most frequent in image-text datasets.For
example, from --we generate “a
woman is sitting.”; and from - - - “a man is holding a camera.”.
This template-based approach is necessary for
our controlled evaluation. As the choice of the ex-
act template for the construction of the sentences
may influence the results, we evaluate the mod-
els, additionally, using a slightly different template,
and we show that the overall result patterns remain
largely similar (see Appendix A.4.2).
Final evaluation set The final evaluation set con-
tains 2584 triplets. For 1486 of these triplets, the
distractor sentence contains an incorrect predicate
and for the other 1098 triplets, the distractor con-
tains an incorrect noun. More detailed statistics
regarding the number of triplets concerning spe-
cific concepts are provided in Appendix A.2.
A note on perceived gender annotations Our
evaluation dataset uses annotations from the Open
Images dataset, which rely on the physical appear-
ance of persons to annotate their perceived gender.
We use the provided annotations, and the resulting
biases are unfortunately reproduced in our evalua-
tion set. We discuss this issue in further detail in
the Ethics Statement (Section 8).
In Salminen et al. (2018) gender classification
from face pictures by human annotators shows
an inter annotator agreement greater than 95%.
True gender cannot be classified, and high inter-
annotator agreement does not imply a correct gen-
der choice, but we expect the gender annotations
of Open Images to be reliable enough to be used as
a basis for our analyses.
3.2 Metric
We evaluate pre-trained models on their image-text
matching performance in a zero-shot setting, i.e.
without any further training. For each triplet, we
test whether the models give a higher similarity
score for the correct sentence than for the distractorsentence. We calculate accuracy for each pair, i.e.
the model needs to succeed for both the example
and the counter-balanced example triplet.
For each pair of triplets (t, t) =
([I, S, S],[I, S, S]), we calculate the
following score:
f(t, t) =

1,ifs(I, S)> s(I, S)
ands(I, S)> s(I, S)
0,otherwise
where s(I, S)denotes the similarity between an
image Iand a sentence S. To obtain the similarity
score, we use the softmaxed output of the image-
text matching pretraining heads of the models.
The final accuracy is the average score over all
pairs in the evaluation set. Chance performance is
at 25%.
As the dataset was manually filtered and requires
only rather simple understanding of the images, we
assume human performance to be close to 100%.
To verify this claim, we had a one person annotate a
randomly sampled subset of 500 triplets. For each
triplet, the annotator was asked to judge which of
the two sentences describes the image better. The
resulting performance was at 100%.
A topline: the cropped task In order to explore
the effect of the visual distractors on this noun-
predicate dependency task, we additionally evalu-
ate all models in a cropped task: We reduce the
image to the bounding box of the target object.
Thus, the cropped image usuallyonly contains
the target object, and no more visual distractors
(i.e., the referent of the noun or the predicate in
the distractor sentence is no longer present in the
cropped image). To succeed at this (simpler) task,
the model no longer needs to capture the predicate-
noun dependency, it just needs to ground the single
words correctly. We use this task to estimate how
much the performance of the models is affected
by the ability to ground nouns and predicates in
our evaluation dataset, in comparison to the (more1541sophisticated) ability of understanding predicate-
noun dependencies .
3.3 Models
We consider a range of state-of-the-art V&L mod-
els that are pre-trained using text, image, and mul-
timodal pretraining objectives on corpora of paral-
lel image and text data. All models use the trans-
former architecture (Vaswani et al., 2017), but vary
in terms of pretraining data and objectives, image
encoders, and multimodal fusion approaches.
In addition to their image and text pretraining
objectives, the models commonly make use of an
image-text matching objective, where the models
are asked to predict whether a given sentence de-
scribes an image or not. We leverage the output of
the corresponding pretraining head for calculating
image-text similarities for our task.
We evaluate LXMERT (Tan and Bansal, 2019),
UNITER (Chen et al., 2019), ViLBERT (Lu et al.,
2019), Oscar (Li et al., 2020b), VinVL (Zhang
et al., 2021), ViLT (Kim et al., 2021), and CLIP
(Radford et al., 2021). We could not evaluate the
original VL-BERT (Su et al., 2020), because it was
not pre-trained using an image-text matching loss.
We also did not evaluate the original VisualBERT
(Li et al., 2019), as their implementation of the
image-text matching loss requires one correct cap-
tion (in addition to a possibly faulty caption), which
is not available for the Open Images dataset. Both
models were however evaluated in the controlled
conditions using VOLTA (see Section 4.2).
Pretraining datasets ViLBERT and VL-BERT
are pretrained on Conceptual Captions (Sharma
et al., 2018). UNITER, LXMERT, ViLT, and
VinVLmake use of additional publicly available
datasets such as COCO (Lin et al., 2014), SBU
captions (Ordonez et al., 2011), Flickr30K (Young
et al., 2014), VisualGenome (Krishna et al., 2017),and VQA datasets (Goyal et al., 2017; Zhu et al.,
2016; Hudson and Manning, 2019). The original
VisualBERT is only pre-trained on COCO. Ap-
pendix A.3 details the pretraining datasets for all
models as well as their sizes. The pretraining data
for CLIP has not been publicly released, the au-
thors state that it consists of 400M image-text pairs
from the internet (an order of magnitude more data
than for most of the other models, which do not
surpass 10M image-text pairs in size).
4 Results
4.1 Original Implementations
We test all models using the evaluation methods
and data described above. We make use of pre-
trained models made publicly available by the au-
thors.
Resulting accuracies are shown in Table 1.
We find that only some models perform substan-
tially above chance, notably ViLT, UNITER and
LXMERT. In the cropped task, performance is
much higher for all models, with VinVL and ViLT
reaching the highest performance. This gap in per-
formance between the full andcropped tasks in-
dicates that while those models can match nouns
and predicates in the image with the corresponding
words rather well, they struggle to take into account
the dependencies between them.
Accuracy
Model Full Cropped
LXMERT 0.57 0 .69
UNITER 0.54 0 .64
ViLBERT 0.28 0 .66
ViLT 0.40 0 .75
Oscar 0.32 0 .67
VinVL 0.30 0 .76
CLIP 0.20 0 .59
Chance 0.25 0 .25
4.2 Controlled Training Conditions
We additionally evaluate models that are trained in
controlled (and therefore more directly compara-
ble) conditions as proposed in the VOLTA frame-
work (Bugliarello et al., 2021). In this setup, all1542models are trained on Conceptual Captions using
the same pretraining objectives (masked language
modeling, masked object classification, and image-
text matching) and use the same image features,
extracted from a Faster R-CNN.
We evaluate all models for which pretrained
weights are available. Resulting accuracy scores
are presented in Table 2.
Accuracy
Model Full Cropped
CTRL_UNITER 0.24 0 .63
CTRL_LXMERT 0.20 0 .56
CTRL_ViLBERT 0.27 0 .66
CTRL_VL-BERT 0.24 0 .66
CTRL_VisualBERT 0.20 0 .64
Chance 0.25 0 .25
We find that under these controlled conditions,
all models perform comparably and generally
around chance level. It is therefore not straightfor-
ward to draw any conclusions regarding the effect
of model architecture from these results.
In the cropped task, performance is much
higher, with ViLBERT and VL-BERT reaching
the highest performance. The performance gap
between the two tasks (i.e., full vs.cropped )
is substantially larger than for the original imple-
mentations, suggesting that the models are even
less sensitive to predicate-noun dependencies un-
der these controlled training conditions.
5 Analyses and Discussion
5.1 Comparing Model Performances
The role of pretraining data Within the set of
the evaluated models, we do not find evidence for
a correlation between the size of the pretraining
dataset and the model’s ability to capture predicate-
noun dependencies (see also Appendix A.3). De-
spite being trained on comparable or even larger
amounts of data, ViLT, Oscar and VinVL perform
substantially worse than LXMERT and UNITER.
CLIP performs below chance level, despite having
by far the largest pretraining dataset.
The pretraining data of CLIP is not publicly
available, but as it was automatically scraped fromthe internet we believe the quality (i.e descriptive-
ness) of its captions to be comparable to that of
Conceptual Captions. In additional experiments
(see Appendix A.4.1), we study the performance
of CLIP models trained on different datasets using
a range of publicly available model checkpoints.
The performance of CLIP remains below chance
level for all tested checkpoints. This might be be-
cause all available checkpoints are all trained on
rather noisy data, or because the architecture and
pretraining objectives of CLIP don’t allow it to
learn grounded predicate-noun dependencies.
Datasets that are composed of highly descrip-
tive captions seem to be advantageous for the
learning of noun-predicate dependencies. Indeed,
for datasets such as COCO (Lin et al., 2014) or
VQA (Antol et al., 2015), the images are not
only strongly associated with the captions or ques-
tion–answer pairs (as they were crowdsourced
specifically for the tasks), but also precise and de-
tailed in nature. In contrast, Conceptual Captions
(Sharma et al., 2018) is composed of images with
captions that were automatically collected from
web pages, and therefore generally rather broad
descriptions of the image content.
ViLBERT and models trained in the controlled
conditions are only trained using Conceptual Cap-
tions, and the resulting performances are around
chance level. UNITER and LXMERT perform
much worse compared to their original training
setups. One main difference for these two mod-
els in their original implementation compared to
the controlled condition is that they are trained on
richer datasets with respect to the language modal-
ity, leveraging more descriptive captions.
This observation is coherent with what Hen-
dricks and Nematzadeh (2021) found when study-
ing verb understanding of V&L models: They com-
pare performance of the same model when trained
on Conceptual Captions or COCO, and find that the
model trained on COCO performs better, despite
Conceptual Captions being bigger and closer to the
task in terms of image and language distribution.
These results suggest that, when considering
multimodal dependencies, having a high quality
pretraining dataset with less noise and more de-1543scriptive textual data could be more important than
having a larger dataset. Highly descriptive textual
data is essential to learn precise predicate-noun
dependencies.
The role of pretraining objectives While mod-
els such as ViLT, Oscar, and VinVL are trained
on datasets that are comparable in size and qual-
ity to those of LXMERT and UNITER, they still
perform substantially worse on the task. One expla-
nation could be that contrary to the other models,
UNITER and LXMERT both have multimodal pre-
training objectives in addition to image-text match-
ing: Visual question answering for LXMERT and
word-region alignment for UNITER.This could
help the models to establish finer multimodal de-
pendencies. Indeed, ViLT and VinVL show bet-
ter results than UNITER and LXMERT in the
cropped task (indicating that their object/predicate
recognition performance even surpasses that of the
other models), but worse results in the full task.
Our hypothesis is that the pretraining objectives of
UNITER and LXMERT enable them to learn more
fine-grained multimodal dependencies than ViLT
and VinVL, even though their performance on the
cropped task is worse.
This gap in performance should not only be due
to the training data associated with the additional
pretraining objectives, as VinVL also uses data
from Visual Question Answering task, but without
training on the objective.
The impact of the multimodal pretraining ob-
jectives of UNITER and LXMERT can be an ad-
ditional explanation for the drop in performance
of CTRL_UNITER and CTRL_LXMERT, which
were only trained using image-text matching as a
multimodal pretraining objective. The gap in per-
formance between those controlled models and the
original models indicate that using more precise
multimodal pretraining objectives and better anno-
tated datasets can greatly improve the learning of
multimodal dependencies.
The lack of suitable multimodal pretraining ob-
jectives could also offer an explanation for the poor
performance of CLIP in our task.The role of image encoders The authors of ViLT
and VinVL motivate their work by suggesting that
improved image features are mandatory for im-
proved multimodal reasoning of V&L transform-
ers. Here, we observe that these improved features
only translate to better results in the cropped task
(where ViLT and VinVL perform best). We spec-
ulate that the improved image encoders allow for
a better understanding of visual entities, but not
necessarily of the dependencies between them. In
order to obtain more conclusive interpretations re-
garding the role of image features, we require more
targeted experiments which control for other con-
founding factors present here (such as different
pretraining objectives).
The role of model architecture In addition to
the lack of suitable pretraining objectives, the
worse performance of CLIP compared to the other
models could also be due to the fact that it does not
support any kind of inter-modal fusion of features
within the model (image and text are processed in
separate submodules that do not allow for inter-
modal interaction). This shortcoming of CLIP is
also discussed in Kim et al. (2021), where the au-
thors find representations from CLIP to be not use-
ful for the more advanced multimodal reasoning
task NLVR2 (Suhr et al., 2019).
However, there seems to be no major effect
of architecture with respect to multimodal fusion
in the case of single and dual stream transform-
ers: LXMERT and UNITER have comparable per-
formances, even though one is dual-stream trans-
former and the other a single-stream transformer.
5.2 Performance for Nouns vs. Predicates
Here, we compare performance for pairs in which
the sentences differ with respect to the noun, to
sentences with a different predicate. Detailed re-
sults for all models are reported in Appendix A.4.3.
Overall patterns show a slightly better performance
for cases in which the noun was switched, espe-
cially in the cropped task. This is in line with
findings that V&L models are better at ground-
ing nouns than verbs (Hendricks and Nematzadeh,
2021).
5.3 Analysis of individual nouns and
predicates
For a given concept (noun or predicate), we con-
sider all pairs that contain this concept in at least
one of the two sentences, i.e. cases in which a1544model’s understanding of a concept is instrumental
for making the correct decision.
Figure 3 shows the per-concept accuracies of the
best performing model, LXMERT. Appendix A.4.4
shows the per-concept accuracies for all models in
their original implementations.
We observe large variation in accuracy scores of
predicates, and less variation for nouns. We could
not find any simple reasons that explain the pred-
icates’ variability. For example, verbs can have
good or bad performances (e.g., “running” vs “talk-
ing”), and the same can be said for predicates that
are composed of both verb and noun (e.g., “hold-
ing a bottle” vs. “wearing a helmet”). That said,
factors that may influence model performance on
specific nouns or predicates are further discussed
in Section 5.4.
Additionally, we observe that for some concepts,
the models perform better if the concept is the tar-
get, and for others, performance is better if it is
the distractor. This is, e.g., the case for the pair
“sing” vs “stand”, where the models consistently
perform better if “sing” is the target predicate. Ap-
pendix A.4.5 shows the accuracy for each (target,
distractor) concept tuple.5.4 Confounding Factors
Here we discuss possible factors influencing the
models’ performances.
Object salience In most of the images in our eval-
uation set, the target and distractor persons in the
image are not of equal size, and not equally salient
(sometimes one is more in the foreground than the
other). We explore whether there is an effect on
the models’ decisions by correlating the models’
predictions with target and distractor bounding box
size and location.
More specifically, we measure the difference
in similarity for target and distractor sentence
s(I, S)−s(I, S)and correlate it with the dif-
ference in bounding box size of the target and dis-
tractor object. Further, we also correlate it with
the difference of distances from the center of the
image.
For LXMERT, we find no significant correla-
tion (Bounding box size: Pearson r=−0.03, p=
0.16, bounding box distance from center: Pearson
r= 0.03, p= 0.14). Correlation scores for other
models can be found in Appendix A.4.6. While
there are statistically significant correlations for
some models, these are small and of varying direc-
tion. The largest correlations are found for CLIP
(Bounding box size: Pearson r= 0.14, p < 0.01,
bounding box distance from center: Pearson r=
−0.24, p < 0.01), indicating that the performance
of CLIP could be affected, to some extent, by ob-
ject salience.
Concept recognizability We also correlate the
models’ similarity judgments differences to differ-
ences in concept recognizability, which we oper-
ationalize by taking the object or attribute confi-
dence score for a given concept in an image from a
Faster R-CNN (Ren et al., 2015) trained on Visu-
alGenome.
For most models, we find a small positive cor-
relation (see Appendix A.4.6), indicating that the
models’ similarity judgments are affected by the
varying degree to which the concepts are recogniz-
able in the image.
Linguistic biases Another aspect, already men-
tioned earlier, is that models’ performance could
be affected by linguistic biases in the training data,
such as the frequency and co-occurrence of words
and phrases.1545To explore this possible effect, we correlate the
difference in similarity for target and distractor
sentence with the difference of target and distractor
sentence perplexity. We calculate the perplexity for
each sentence using a single-modality BERT model
(bert-base-uncased ), that was fine-tuned for 3
epochs on the textual data of Conceptual Captions.
For LXMERT, we find no significant correlation
(Pearson r=−0.01, p= 0.48). For the other mod-
els, we find very small positive correlations (see
Appendix A.4.6). We conclude that the models do
not rely only on shallow heuristics of the training
data in the textual modality.
6 Conclusion
This work examines whether state-of-the-art V&L
models learn multimodal syntactic dependencies,
by focusing on a case study on simple predicate-
noun dependencies. Our controlled experiments
and analyses on a range of recent models reveal that
their capability track such dependencies is variable,
with some models (e.g., LXMERT and UNITER)
show performance above chance level and others
(e.g., CLIP) performing even below chance.
In contrast to the recent trend in the field fo-
cused on increasing pretraining data and using sim-
ple general-purpose pretraining objectives (Brown
et al., 2020; Devlin et al., 2019), here we observe
that best performance is achieved, rather, with high-
quality pretraining data, and more fine-grained pre-
training objectives.
More specifically, our results suggest that mul-
timodal pretraining objectives have a major im-
pact on the model’s learning of grounded predicate-
noun dependencies. Models that include more tar-
geted objectives such as visual question answering
and word region alignment in addition to the gen-
eral image-text matching objective show better per-
formance. In addition, having highly descriptive
pretraining datasets seems to help with learning
fine-grained multimodal dependencies. In compari-
son, models trained on larger, web-scraped datasets
do not perform well.
In the future, the proposed highly-controlled
evaluation protocol can be used to conduct more tar-
geted studies regarding the role of model architec-
ture, pretraining objectives, as well as training data
quality and quantity in order to build V&L models
that are better at learning grounded predicate-noun
dependencies, and possibly also other, move ad-
vanced multimodal reasoning tasks.7 Limitations
While our analyses revealed patterns that seems to
explain observed variability in the models’ perfor-
mance, the role of some architectural choices such
as image encoding techniques remains ambiguous.
A better understanding of all factors influencing
the learning of grounded predicate-noun dependen-
cies could be achieved by training sets of models
on comparable conditions and by varying only one
factor at a time (as done for example in Bugliarello
et al., 2021, regarding the role of model architec-
ture).
The range of concepts evaluated is rather small
and therefore not representative for the understand-
ing of grounded predicate-noun dependencies in
general. More targeted data collection will be nec-
essary in order to obtain more large-scale evalua-
tion datasets. Additionally, our zero-shot evalua-
tion paradigm introduces a possible mismatch be-
tween training and evaluation: Models are trained
using pairs of images and descriptions where the
descriptions often describe allsalient parts of the
image, whereas in our evaluation set the descrip-
tions focus on only oneaspect/person in the image.
In the cropped condition, the images are not are
not representative of the typical photographic fram-
ing of image-text corpora, which could deteriorate
our results. That said, random cropping is a fre-
quent data augmentation technique in computer
vision research, where it has been successfully
applied to improve generalization performance
(Krizhevsky et al., 2012).
Further, some scenes, actions and cultures are
disproportionally represented in our evaluation
dataset. As proposed in (Liu et al., 2021), it is
important to pursue further work on more diverse
datasets.15468 Ethics Statement
The proposed evaluation set relies on subjective an-
notations of perceived gender. Attempting to clas-
sify gender based on physical appearance is an ill-
posed problem (e.g., due to limitations of object de-
tectors, biases of the human annotators). Addition-
ally, the annotations only consider binary gender
classes (woman/man, girl/boy). Algorithms that
perform such classifications are neither ideal nor
desirable, as they perpetuate harmful stereotypes
and exclude non-binary gender identities (e.g., Dev
et al., 2021; Hamidi et al., 2018; Blodgett et al.,
2020; Bender et al., 2021). We explored whether
it would be possible to use other classes, but we
did not find many examples that would allow for
an evaluation of sensitivity to predicate-noun de-
pendencies in a controlled fashion. As our image
selection is very constrained (we require a visual
distractor, and a counter-example image with re-
verse properties), we found only sufficient exam-
ples for the categories used in the paper. We ini-
tially started a bottom-up data exploration in which
we considered all labels present in the Open Im-
ages dataset, but found only very few examples
for a few other categories (generally less than 5
examples after manual filtering). This might be
due to the focus of the annotations in Open Im-
ages, future work could explore the use of other
datasets that are focused on other types of annota-
tions, the main challenge being the requirement for
sufficiently large datasets in order to find match-
ing examples and counter-examples. Future efforts
should be dedicated to creating datasets that aim at
more inclusive annotations.
We acknowledge the severity of these issues,
and emphasize that our work does not promote
applications of gender classification in downstream
tasks, but only uses it as a basis for analysis of
existing models.
9 Acknowledgements
We thank the anonymous reviewers for their useful
comments and feedback.
Research supported by grants ANR-16-CONV-
0002 (ILCB), ANR-11-LABX-0036 (BLRI), ANR-
21-CE28-0005-01 (MACOMIC), AMX-19-IET-
009 (Archimedes Institute) and the Excellence Ini-
tiative of Aix-Marseille University (A*MIDEX).
This work was performed using HPC resources
from GENCI–IDRIS (Grant 2021-[101693]).References1547154815491550A Appendix
A.1 Image Pre-Filtering
We consider labels that occur at least 100 times in
the dataset.
As some labels are similar and sometimes
used interchangeably by the annotators, we cre-
ate groups of synonyms for some labels and treat
labels within a group as identical in the following.
The groups of synonyms can be found in Table 3.
Further, we verify that the bounding boxes of
the target and distractor objects are big enough (at
least 20% width and 20% height of the image) and
that the bounding box sizes of target and distractor
objects don’t differ by more than a factor of 2.
Finally, we ensure that there is at least one
counter-example for each triplet before starting the
manual image selection phase.
“Table”, “Desk”, “Coffee table”
“Mug”, “Coffee cup”
“Glasses”, “Sunglasses”, “Goggles”
“Sun hat”, “Fedora”, “Cowboy hat”, “Sombrero”
“Bicycle helmet”, “Football helmet”
“High heels”, “Sandal”, “Boot”
“Racket”, “Tennis racket”, “Table tennis racket”
“Crown”, “Tiara”
“Handbag”, “Briefcase”
“Cart”, “Golf cart”
“Tree”, “Palm tree”
“Football”, “V olleyball (Ball)”, “Rugby ball”,
“Cricket ball”, Tennis ball”
A.2 Dataset statistics
Figure 4 shows the number of triplets for each noun
and predicate. For a given noun or predicate, we
count all pairs that contain this concept in at least
one of the two sentences, i.e. cases in which correct
understanding of a concept is useful for making the
correct decisions.
A.3 Details on V&L Models
A.3.1 Pretraining Datasets
Table 4 details the multimodal datasets used for
V&L models. Dataset sizes as reported in the cor-
responding papers. Note that these sizes are also
affected by the fact that some models leverage vali-
dation sets for pretraining, while others constrain
the data to the training sets. Also, different ap-
proaches for dataset overlap detection have been
applied. The pretraining data size for CLIP is re-
portedly 400M image-text pairs.
A.3.2 Number of parameters
Table 5 compares the number of trainable parame-
ters for each model that was tested in their original
implementations.
A.4 Additional Analyses
A.4.1 Results for CLIP with varying
pretraining data
Table 6 presents the accuracy scores of multiple
publicly available checkpoints for CLIP trained on
different training data.1551Total size
Model CC COCO SBU VG QA F30K OI # images # image-text pairs
LXMERT ✓ ✓ ✓ 0.18M 9.18M
UNITER ✓ ✓ ✓ ✓ 4.16M 9.59M
ViLBERT ✓ 3.10M 3.10M
ViLT ✓ ✓ ✓ ✓ 4.05M 9.85M
Oscar ✓ ✓ ✓ ✓ ✓ 4.10M 6.50M
VinVL ✓ ✓ ✓ ✓ ✓ ✓ 5.65M 8.85M
Model # Parameters
LXMERT 228,051,752
UNITER 112,938,887
ViLBERT 250,044,029
ViLT 111,596,546
Oscar 111,062,018
VinVL 111,686,973
CLIP 151,277,313
Visual Encoder Dataset Accuracy
RN101 YFCC-15M 0.18
RN101 400M 0.21
RN50 cc12m 0.18
RN50 400M 0.20
RN50 YFCC-15M 0.17
ViT-B-32 aion2b_e16 0.21
ViT-B-32 laion400m_e31 0.20
ViT-B-32 laion400m_e32 0.19
ViT-B-32 400M 0.20
ViT-L-14 400M (336px) 0.20
A.4.2 Controlling for linguistic robustness
As the sentences used in our evaluation dataset are
built from a template, they do not vary in syntax.
We verify that results obtained do not depend on
the exact template chosen.
We vary the original templates by using the def-
inite article (“the”) at the beginning of sentences,
and using verbs in simple present instead of present
progressive tense (e.g., “the woman sits.” or “the
man holds a camera.”).Results with these alternative sentences are show
in Table 7. We find that overall result patterns are
highly similar to those with the original sentences
in Table 1.
Accuracy
Model Full Cropped
LXMERT 0.55 0 .70
UNITER 0.54 0 .66
ViLBERT 0.26 0 .67
ViLT 0.34 0 .72
Oscar 0.32 0 .65
VinVL 0.30 0 .74
CLIP 0.21 0 .58
A.4.3 Switching noun vs. switching predicate
Table 8 presents the accuracy for cases in which
target and distractor sentence differ with respect
to the predicate, or noun. We report scores for all
models in their original implementations.
A.4.4 Analysis of individual nouns and
predicates for all models
Figure 5 shows the accuracies for split up for the
different predicates and nouns for all models. For
more details, refer to Section 5.3.
A.4.5 Accuracies for (target, distractor) tuples
Figure 6 shows the accuracy for target-distractor
tuples for all models.
A.4.6 Confounding Factors
In Table 9 we show the correlation scores for sev-
eral confounding factors as described in Section
5.4.1552Full Cropped
Model Noun Predicate Noun Predicate
LXMERT 0.60 0 .55 0 .78 0 .62
UNITER 0.60 0 .50 0 .76 0 .56
ViLBERT 0.27 0 .28 0 .74 0 .59
ViLT 0.44 0 .37 0 .80 0 .72
Oscar 0.36 0 .30 0 .75 0 .62
VinVL 0.33 0 .28 0 .83 0 .71
CLIP 0.21 0 .19 0 .69 0 .52
Model Bounding box size Distance from center Perplexity Object detector confidence
LXMERT -0.03 (p=0.12) 0.03 (p=0.08) -0.01 (p=0.48) 0.30 (p=0.00)
UNITER -0.09 (p=0.00) 0.09 (p=0.00) 0.05 (p=0.01) 0.26 (p=0.00)
ViLBERT 0.11 (p=0.00) -0.16 (p=0.00) 0.05 (p=0.02) 0.22 (p=0.00)
ViLT -0.01 (p=0.73) 0.03 (p=0.13) 0.05 (p=0.02) 0.26 (p=0.00)
Oscar 0.12 (p=0.00) -0.17 (p=0.00) 0.06 (p=0.00) 0.15 (p=0.00)
VinVL 0.12 (p=0.00) -0.12 (p=0.00) 0.05 (p=0.01) 0.04 (p=0.05)
CLIP 0.14 (p=0.00) -0.24 (p=0.00) 0.08 (p=0.00) 0.17 (p=0.00)155315541555