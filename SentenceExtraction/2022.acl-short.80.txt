
Lisa Jin andDaniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
A common way to combat exposure bias is by
applying scores from evaluation metrics as re-
wards in reinforcement learning (RL). Metrics
leveraging contextualized embeddings appear
more flexible than those that match n-grams
and thus ideal as training rewards. Yet metrics
such as BERTS greedily align candidate
and reference tokens, which can give system
outputs excess credit relative to a reference.
Past systems using such semantic similarity re-
wards further suffer from repetitive outputs and
overfitting. To address these issues, we propose
metrics that replace the greedy alignments in
BERTS with optimized ones. Our model
optimizing discrete alignment metrics consis-
tently outperforms cross-entropy and BLEU
reward baselines on AMR-to-text generation.
Additionally, we find that this model enjoys
stable training relative to a non-RL setting.
1 Introduction
Automatic evaluation metrics often score natu-
ral language generation (NLG) system outputs
based on how well they lexically align to human-
annotated references. In tasks such as machine
translation and summarization, these metrics may
unfairly penalize outputs that express the correct
semantics despite a lower n-gram overlap with ref-
erence strings. As a result, models overfitting to
certain token-level patterns may dominate those
generating more creatively (e.g., through synonyms
or varied sentence structure).
NLG systems are typically trained to maximize
likelihood of a single set of references. Condition-
ing models on gold prefixes shields them from their
own predictions during training—an issue known
as exposure bias. Adding reinforcement learning
(RL) objectives (Ranzato et al., 2016; Edunov et al.,
2018) can aid exploration by giving a model feed-
back on sequences sampled from its own distri-
bution. However, it is common practice to use
automatic evaluation scores like BLEU (Papineniet al., 2002) and ROUGE (Lin and Hovy, 2002) as
sequence-level rewards. This results in the same
lack of semantic signal described earlier.
Instead of hinging evaluation on hard n-gram
overlap, recent metrics (Zhang et al., 2019; Zhao
et al., 2019) rely on vector similarity between con-
textualized subword embeddings to make more se-
mantically faithful judgments. BERTS , in
particular F , computes a token-level F1 score
based on greedy alignment of similar embeddings.
With their strength in offline evaluation, it is natu-
ral to ask how these embeddings-based metrics can
help provide more realistic training feedback.
Past approaches to train models with semantic
similarity scores include both non-differentiable
and differentiable objectives. Wieting et al. (2019)
separately train paraphrastic sentence embeddings
that provide semantic similarity rewards to a neu-
ral machine translation (NMT) system. Rewards
were included in a mixed minimum risk and maxi-
mum likelihood training phase. Besides an embed-
ding training overhead, the model needed a length
penalty term to limit repetitive outputs. Li et al.
(2019) adopt a similar fine-tuning approach using
an RL objective with F for abstractive sum-
marization. While their models were less repet-
itive, their news domain corpora may have been
a natural match for BERT embeddings. Finally,
Jauregi Unanue et al. (2021) also propose to opti-
mize F but with fully differentiable training
objectives in NMT. Yet their models overfit after
only a few epochs and scored lower in BLEU at
the cost of higher F . We hypothesize that
metrics employing external pretrained vectors may
suffer from domain mismatch with downstream
data. This can hurt the accuracy of semantic simi-
larity scores computed during training.
In this work, we focus on text generation from
Abstract Meaning Representations (AMRs, Ba-
narescu et al., 2013), sentence-level semantic
graphs that are rooted, directed, and acyclic. This710task’s models may especially benefit from an em-
phasis on semantic rather than lexical similarity.
It also provides a challenging setting to evaluate
overfitting given the relatively small corpus size.
In our analysis of F rewards, we note that
F could worsen repetition and incomplete out-
puts in NLG systems. Due to its greedy token align-
ment, F precision may assign extra credit to
a reference token ‘retrieved’ multiple times. In
response, we contribute the following.
•We introduce metrics that apply discrete and
continuous alignments to BERTS , miti-
gating the pitfalls of greedy alignment.
•For text generation from AMR, we are the first
to train on RL objectives with embeddings-
based evaluation metrics.
•As RL rewards, we compute BERTS -
based metrics on a model’s own token rep-
resentations rather than BERT embeddings.
This is more memory-efficient and does not
overfit relative to pure cross-entropy training.
2 Greedy Token Alignment
The main insight behind BERTS and related
metrics is to align hypothesis and reference to-
kens using their pairwise vector similarity scores.
These alignments are later used to weight the con-
tribution of token-level similarity scores towards
a final sequence-level score. Concretely, given
(ˆ y, . . . ,ˆ y)and(y, . . . ,y)hypothesis and ref-
erence token embeddings, precision in F is
P =1
mXmaxcos(ˆ y,y),
where cos(ˆ y,y) =ˆ yy/∥ˆ y∥∥y∥denotes cosine
similarity. Each hypothesis token ˆyis greedily
aligned to the reference token ywith the highest
corresponding embedding cosine similarity. Unlike
in BLEU, P does not clip the number of times
ˆycan align to a unique yby its count in y. As
such, a hypothesis will get excess credit by repeat-
ing a reference token beyond this count. While the
authors claim greedy alignments have little effect
onBERTS evaluation performance, they per-
form poorly relative to metrics based on optimized
alignments in our experiments.3 Optimized Token Alignment
Aligning tokens between hypothesis and reference
can be seen as an assignment problem, where a
token pair (ˆy, y)is highly weighted if it incurs
low cost (i.e., distance).
Here, we describe discrete token matching (one-
to-one) and soft alignment (one-to-many). For the
latter, we extract alignments from the earth mover’s
distance (EMD, Villani, 2009; Peyré and Cuturi,
2019) transport matrix. We weight pairwise token
similarities as in F using each of these two
alignments to provide metrics F andF .
3.1 Discrete word matching
To avoid the issues with greedy alignment in
P , we can extract one-to-one alignments be-
tween the two sequences. Let C∈Rde-
note the pairwise cosine distance matrix such that
C= 1−cos(ˆ y,y). For notational clarity, let
eC= 1−C. We wish to find alignments
T= arg minXXTC, (1)
such that no element in h=T1andr=T1
exceeds one. In other words, each ˆycan align to at
most one y(exactly one when m=k), and vice
versa. This linear sum assignment problem can
be solved in low-order polynomial time (Crouse,
2016), making it suitable for use during training.
Metric The updated precision is found as
P =1
mXXTeC. (2)
Recall R takes an analogous form and is com-
bined with P to produce an F1 score, F .
3.2 Continuous word alignment
We also experiment with soft alignments, where
weights in Tare continuous. In the case of P ,
one-to-many alignments between each hypothesis
token ˆyand those in {y}are permitted.
Inspired by work applying EMD to semantic text
similarity (Kusner et al., 2015; Clark et al., 2019),
we frame alignment as minimizing the transporta-
tion cost between token embeddings from the hy-
pothesis and reference distributions. The amount
of token-level mass to transport between the two
distributions is handr, respectively. Instead of711assigning IDF as the mass per token (Zhao et al.,
2019), we use the norm of its embedding (i.e., ∥y∥,
Yokoi et al., 2020) for simplicity.
The EMD, or optimal transport, problem is
T= arg minXXTC, (3)
s.t.h=T1,r=T1.
Intuitively, if we view Tas the joint probability
of aligning ˆywithy, the row and column sums
are marginals (Cuturi, 2013).
Metric To compute F , we normalize the
alignment weights such that the rows of Tsum to
one for precision, and the columns for recall.
P =1
mX1
hXTeC, (4)
R =1
kX1
rXTeC (5)
4 Semantic Similarity Rewards
We propose to fine-tune on our optimized F1 met-
rics, applying a weighted average of cross-entropy
and RL objectives. Given source sequence x(e.g.,
a linearized AMR), the former is computed as
L=−Xlogp(y|y, x).
To encourage close evaluation scores between sam-
pled¯yand reference y, the RL objective is
L= (∆(¯ y, y)−∆(¯y, y))Xlogp(¯y|¯y, x),
where ∆is the chosen evaluation metric and ¯y
is a greedily decoded baseline relative to ¯y. This
baseline helps reduce variance in REINFORCE
(Williams, 1992). The combined cross-entropy and
RL loss is
L=λL+ (1−λ)L,
where λis empirically set to 0.3.
5 Experiments
We examine the performance of our proposed met-
rics as RL rewards on AMR-to-text generation.BLEU METEORF BLEURT
XENT 36.37 39 .94 65 .68 56 .30
BL-R 37.06 40 .30 66 .19 56 .08
F 36.06 39 .85 65 .23 55 .45
F 36.91 40 .34 66 .07 55 .96
F 37.65 40 .61 66 .55 57 .01
5 10 15 20 25 3030323436
5.1 Setup
Dataset The LDC2017T10 dataset that we exper-
iment on contains ∼36K training and ∼1.4K each
of development and test AMR-sentence pairs. To
leverage strong pre-trained language models, the
AMRs are linearized as in Ribeiro et al. (2021).
Evaluation We report results in terms of BLEU
(Papineni et al., 2002), METEOR (Banerjee and
Lavie, 2005),F(Popovi ´c, 2015), and BLEURT
(Sellam et al., 2020). Only the latter metric makes
use of pre-trained contextualized embeddings.
Baselines For all experiments, we fine-tune the
small capacity T5 model (Raffel et al., 2020) from
Ribeiro et al. (2021). The model has 60M parame-
ters and features a Transformer-based encoder and
decoder. We compare our F andF met-
rics for RL-based training against three baseline
approaches. XENT is a pure cross-entropy objec-
tive. For RL-based approaches, we include a BLEU
reward (BL-R) and one with F —computed
on the lowest level token embeddings in T5.The
λscaling factor for the RL objective is set to 0.3
across all RL-based experiments.
Implementation details Adam (Kingma and Ba,
2015) is used to optimize the model with an initial712
learning rate of 1·10and a batch size of 16.
Following Ribeiro et al. (2021), we use a linearly
decreasing schedule for the learning rate and no
warm-up. Since Ribeiro et al. (2021) do not release
their training methodology, we train until valida-
tion BLEU does not increase for three epochs—an
approach found in previous work fine-tuning T5
for AMR-to-text generation (Hoyle et al., 2021).
We use SciPyand the Python Optimal Transport
libraryto solve Eqs. 1 and 3.
5.2 Results
Table 1 shows that F achieves the highest
scores on all metrics, surpassing F as well. It
scores higher than XENT by 1.28BLEU and 0.71
BLEURT points. Although BL-R was specially
trained to optimize BLEU, F still outperforms
it by over half a point on that metric.
There is a clear hierarchy among the approaches
based on F1 score, with F above F , fol-
lowed by F at the bottom. This dynamic sug-
gests that the optimized alignments may provide
higher quality reward signals during training.
We note that although F performed com-
parably to BL-R, it could exploit tensor operations
and was far faster to compute than BLEU. On the
other hand, F achieved significantly lower
scores than BL-R. As noted in §2, perhaps the
clipped precision counts in BLEU gave BL-R an
advantage over the greedy nature of F .
5.3 Analysis
Training stability As shown in Fig. 1, F
continues to improve on validation BLEU long
after XENT overfits at epoch 18. This runs counter
to the expectation of unstable RL-based training.It is also interesting that while F validation
performance looks fairly low relative to BL-R, it
achieves similar scores at test time. This may be
due to irrelevant differences between the validation
and test sets, however.
Manual inspection Table 2 lists a few examples
of model outputs for detailed analysis. In exam-
ple (1), both XENT and F make the error of
predicting “part” instead of “participating”. Only
F approaches the meaning of the reference.
This may be a side-effect of weighting lexical over
semantic similarity in the former two systems. In
(2),F repeats the word “bacterium”, while
XENT takes an anthropomorphic view of the bac-
terium. The repetition may be a result of F
rewarding multiple instances of the same token by
mistake during greedy alignment.
6 Conclusion
This paper proposes new F1 score metrics based
on optimized rather than greedy alignments be-
tween predicted and reference tokens. Instead of
letting hypotheses align to reference tokens with-
out regard to their frequencies (and vice versa), we
extract alignments as a constrained optimization
problem. In the discrete case, we treat alignment
as a matching problem between hypothesis and
reference tokens. In the continuous case, we find
alignments that minimize earth mover’s distance
between the two token embedding distributions.
We apply new metrics as rewards during RL-
based training for AMR-to-text generation, with
F outperforming both a cross-entropy baseline
and one optimizing BLEU rewards. Despite being
computed on a downstream model’s token embed-
dings, the metrics still provide informative rewards
during training without signs of overfitting.713Acknowledgments Research supported by NSF
awards IIS-1813823 and CCF-1934962.
References714715