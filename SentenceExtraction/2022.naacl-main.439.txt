
Prithviraj AmmanabroluLiwei JiangMaarten Sap
Hannaneh HajishirziYejin ChoiAllen Institute for Artificial IntelligenceUniversity of Washington
{raja,maartens}@allenai.org
{lwjiang,hannaneh,yejin}@cs.washington.edu
Abstract
We focus on creating agents that act in align-
ment with socially beneficial norms and values
in interactive narratives or text-based games—
environments wherein an agent perceives and
interacts with a world through natural language.
Such interactive agents are often trained via
reinforcement learning to optimize task perfor-
mance, even when such rewards may lead to
agent behaviors that violate societal norms—
causing harm either to the agent itself or other
entities in the environment. Social value align-
ment refers to creating agents whose behaviors
conform to expected moral and social norms
for a given context and group of people—in
our case, it means agents that behave in a man-
ner that is less harmful and more beneficial for
themselves and others.
We build on the Jiminy Cricket bench-
mark (Hendrycks et al., 2021b), a set of 25 an-
notated interactive narratives containing thou-
sands of morally salient scenarios covering ev-
erything from theft and bodily harm to altru-
ism. We introduce the GALAD ( Game-value
Alignment through Action Distillation) agent
that uses the social commonsense knowledge
present in specially trained language models
to contextually restrict its action space to only
those actions that are aligned with socially ben-
eficial values. An experimental study shows
that the GALAD agent makes decisions effi-
ciently enough to improve state-of-the-art task
performance by 4%while reducing the fre-
quency of socially harmful behaviors by 25%
compared to strong contemporary value align-
ment approaches.
1 Introduction
An inherent difficulty in designing and train-
ing AI agents lies in simultaneously ensuring
that agents are competent at a given task while
following socially beneficial behaviors (Nahian
et al., 2020; Hendrycks et al., 2021b). Such
agents—particularly those trained via reinforce-Figure 1: Excerpt from the game Zork1 in the Jiminy
Cricket benchmark where the agent breaks into some-
one’s house and proceeds to steal items and the corre-
sponding value annotations and game rewards. Note the
conflicting valence of the two rewards when the agent
is in the Kitchen.
ment learning (RL) in sequential decision mak-
ing environments—are prone to learning behaviors
harmful to themselves and their surroundings due
to optimal task performance being often misaligned
with socially beneficial human values (Moor, 2006;
Soares and Fallenstein, 2017; Russell, 2019). Fur-
ther, despite work showing the need for dataset bias
analysis in supervised settings (Gebru et al., 2018),
most reinforcement learning benchmarks do not
provide equivalent analysis regarding their reward
functions—making harmful agent behaviors diffi-
cult to diagnose (Gilbert et al., 2022). Fig. 1 shows
an example of such misalignment in the context of
text based games—long puzzles or quests where
an agent perceives and interacts with the world5994through incomplete English language descriptions.
We focus on the task of socially beneficial value
alignment, a subset of value alignment concerned
with creating agents that better conform to the ex-
pected social norms and values of a given group
of people in a specific context.In particular, this
refers to agents that act in a manner that reduces
harm to both themselves and surrounding entities.
We propose to do this by using the social common-
sense knowledge present in specially trained large
language models (Jiang et al., 2021) to contextually
constrain an agent’s actions to those that align with
these expectations. Evaluating the relative social
harmfulness of such agents requires us to focus
not only agent design but also on the contexts, or
environments, in which they operate.
As such, we build on the Jiminy Cricket bench-
mark (Hendrycks et al., 2021b), a set of 25 text-
based adventure games containing annotations re-
garding what constitutes socially beneficial behav-
ior in thousands of grounded and morally salient
scenarios. They are structured as long puzzles and
quests that require agents to reason about over 2000
locations, hundreds of characters, and nearly 5,000
objects over hundreds of steps, creating chains of
dependencies that an agent must fulfill to complete
the overall task. Contained within these quests are
a diverse range of morally salient scenarios cov-
ering everything from bodily harm and theft, to
altruism and other positive human experiences.
Given the complexity of these scenarios, vari-
ations in what is perceived as socially beneficial
behavior in a particular context may vary greatly
depending on the group of people judging the situa-
tion. We conduct a relatively large scale human par-
ticipant study to better understand these variations.
Participants are presented with transcripts of these
scenarios—similar to what is seen in Fig. 1—and
asked to determine how they perceive scenario’s
relative moral valence and salience. As noted by
Hendrycks et al. (2021b), requiring such dense hu-
man feedback for training purposes is unrealistic
in most sequential decision making environments
and is thus used only for evaluation.
Keeping this in mind, we introduce the
GALAD ( Game-value Alignment throughAction Distillation)agent that learns to inher-
ently constrain its action space to only actions that
align with socially beneficial human values even
before it ever begins training a policy for a task.
We use the social commonsense norms encoded in
specially trained transformer-based language mod-
els (Jiang et al., 2021) to endow our agent with the
ability to contextually distinguish between socially
beneficial and harmful behaviors. An experimental
study shows that translating these sources of com-
monsense knowledge through distillation into soft
constraints on the action space results in an agent
that aligns more closely to social values when com-
pared to popular contemporary policy and reward
shaping based value alignment approaches while
improving state-of-the-art task performance.
In short, our contributions are threefold, we pro-
vide: (1) a broad human participant study to ver-
ify the moral valence and salience of scenarios in
the Jiminy Cricket environment; (2) the GALAD
agent which constrains its action space using social
commonsense knowledge encoded in large scale
language models; and (3) an experimental study
that showcases the relative importance of action
distillation when compared to reward and policy
shaping methods in value aligning RL agents.
2 Related Work
Text game playing. Recent works in this area
have focused on tackling three primary challenges:
(1) how to represent agent knowledge to effectively
operate in partially observable environments (Ad-
hikari et al., 2020; Sautier et al., 2020); (2) scaling
RL algorithms to handle combinatorial natural lan-
guage state-action spaces (Zahavy et al., 2018; Yao
et al., 2020; Ammanabrolu et al., 2021; Jang et al.,
2021); and (3) giving agents commonsense priors
to better reason about the world (Murugesan et al.,
2020, 2021; Dambekodi et al., 2020; Ammanabrolu
and Riedl, 2021a). All of these works focus exclu-
sively on improving task performance, often in the
form of increasing overall game completion rates,
and do not analyze agent behaviors.
Value alignment and safe RL. Value alignment
is often defined as a property of intelligent agents
that biases them towards acting in a manner that is
similar to a human in a given situation (Bostrom,59952014). Value alignment and safe exploration for
RL agents are usually cast as constrained optimiza-
tion problems wherein an agent attempts to learn
a policy for a task while adhering to a given set of
constraints (García et al., 2015; Leike et al., 2017;
Achiam et al., 2017; Tessler et al., 2019; Ray et al.,
2019). Contemporary approaches often rely on
imitating expert demonstrations to learn safe tra-
jectories (Gao et al., 2018) or by modeling rewards
that best fit human values (Hadfield-Menell et al.,
2016; Reddy et al., 2020).
Closest in spirit to our work are those of Nahian
et al. (2021) and Hendrycks et al. (2021b), the lat-
ter being the work which we build on. Both of
these works introduce text game based environ-
ments with inherent morally salient scenarios for
an agent to reason about—Nahian et al. (2021)
building on the procedurally generated TextWorld
framework (Côté et al., 2018) and Hendrycks et al.
(2021b) borrowing from human-made games in the
Jericho benchmark (Hausknecht et al., 2020). Both
further design value-aligned agents that use pri-
ors regarding socially beneficial behavior learned
by training language models on domain specific
datasets such as stories (Nahian et al., 2020) or eth-
ical understanding benchmarks (Hendrycks et al.,
2021a) to perform reward and policy shaping (Sut-
ton and Barto, 1998; Griffith et al., 2013). In con-
trast, GALAD distills social commonsense infor-
mation regarding socially beneficial behaviors into
contextual knowledge about what actions to not
take in a given state before policy learning begins.
3 Environment Alignment Verification
The Jiminy Cricket benchmark (Hendrycks et al.,
2021b) is a set of 25 text adventure games—drawn
from the Jericho suite (Hausknecht et al., 2020)—
that contains morality annotations for possible
action-outcome in a particular world state. The
original annotation framework used in Hendrycks
et al. (2021b) contains two primary dimensions
with two options apiece: (1) valence - does the
action constitute socially beneficial behavior as de-
fined by the annotation rubric; and (2) target - does
this action affect the agent itself orothers . For
each of these categories, a further severity level is
assigned between 1-3 to better account for varia-
tions in the relative seriousness of a situation. This
results in 12 possible annotation labels . The orig-
inal annotations are made pro tanto at a source
code level, i.e. annotations do not consider howthe world has changed until that particular state
(e.g. breaking into someone else’s house is always
bad regardless of context). Further details on the
original framework drawn from Hendrycks et al.
(2021b) are provided in Appendix A.1.
Text games from the Jericho benchmark (and
subsequently Jiminy Cricket) provide semantically
rich, complex environments to study value align-
ment. This, along with the inherent quest-like struc-
ture of such games, provides thousands of poten-
tially morally ambiguous scenarios for an agent to
reason through—many of which contain game re-
wards that can easily encourage socially unaccept-
able behaviors as seen in Fig. 1. The relative com-
plexity and common fantastical elements present
in such environments, however, does significantly
complicate the process of annotating the relative
harmfulness of an action.
To complement the source code level annotations
in Jiminy Cricket, we perform a human evaluation
of actions in context through crowdsourcing. We
randomly select n= 210 world states with moral
source code annotations, along with their 15 pre-
ceding turns in a game played by an oracle agent.
Using a pool of trained Amazon MTurk workers,
we collect 5 annotations per game snippet of moral
saliency of the agent’s actions, as well as valence,
target, and severity using the same definitions as
Jiminy Cricket. We refer the reader to Appendix
A.1.1 for further details on the annotation setup.
According to our workers, 81% of the situa-
tions with source code level morality annotations
were deemed morally salient. We find that workers
agreed with the valence of the source code annota-
tions 67% of the time, and with both the valence
and target 50% of the time. However, our work-
ers matched the exact valence, target, and severity
annotations only 24% of the time. Finally, as dis-
cussed in greater detail Appendix A.1.2, our own
workers exhibited variation in their annotations,
showing moderate agreement on annotations of
valence ( 83% pairwise agreement, Krippendorf’s
α= 0.30) and valence and polarity ( 70% pairwise
agreement, α= 0.30). While these results high-
light that most source-level scenarios marked as
morally salient pro tanto are also morally salient
in context, they suggest that as source level anno-
tations become more fine grained, they become
noisier proxies for determining in context social
acceptability. In our work, we therefore do not
consider the target and severity dimensions of the5996
annotations and use only the high level annota-
tions regarding the moral valence—having rela-
tively high annotator agreement—of a situation to
judge whether an agent has behaved acceptably.
4 Value Alignment via Action Distillation
This section covers different parts of GALAD, first
describing the pre-training required to create so-
cial commonsense models and then detailing how
an RL agent uses them while exploring the game
world. GALAD has three primary components: (1)
Section 4.1 explains the value prior , a large lan-
guage model specially trained with knowledge of
commonsense morality; (2) Section 4.2 showcases
a relatively small action candidate generator which
learns to contextually generate less socially harm-
ful actions by distilling the knowledge present in
the value prior as seen in Fig. 2; and (3) Section 4.3
combines all the parts, describing the overall RL
policy network as seen in Fig. 3 which uses the
action candidate generators to generate (socially ac-
ceptable) candidate actions that are sampled from
during exploration.
We first formally define interactive narratives,
building on the definition seen in Côté et al. (2018)
and Hausknecht et al. (2020), accounting for the
objective of socially beneficial value alignment. In
our setting, text games are Partially-Observable
Markov Decision Processes (POMDP) defined as
an 8-tuple of ⟨S, P, A, O, Ω, R, χ, γ ⟩, representing:
the set of environment states ( S), conditional tran-
sition probabilities between states ( P), the vocabu-
lary or words used to compose text commands ( A),
observations ( O), the conditional probability of re-
ceiving an observation ( Ω), reward function ( R),
value alignment scoring function ( χ), and discount
factor ( γ), respectively.4.1 Value Priors from Social Commonsense
Our value prior is based on Delphi (Jiang et al.,
2021), a moral reasoning model taught with 1.7M
instances of publicly polled declarative knowledge
of what’s ethically acceptable or not in everyday
situations. It is fine-tuned from U , a uni-
versal commonsense reasoning model derived from
T5-11B, the largest T5 model with 11 billion pa-
rameters (Raffel et al., 2020).
Datasets and Training. Delphi is trained on
C N B, a knowledge
repository of everyday ethics, sourced from five
existing large-scale datasets, including S
C (Forbes et al., 2020), ETHICS Com-
monsense Morality (Hendrycks et al., 2021a),
M S (Emelin et al., 2021), S
BI C (Sap et al., 2020), and
S (Lourie et al., 2021). C
N B contains diverse set of descriptive so-
cial, cultural and norms grounded in complex real
world situations. The version of Delphi used for
our work is trained via a question answering task
that infers the ethical judgment regarding a text
description of an everyday situation ( e.g., “killing
a bear” is “wrong”). We use this model to gen-
erate social acceptability scores ( i.e., probability
scores among “positive,” “neutral” and “negative”
categories) for given game actions and context.
4.2 Action Distillation
Requiring the use of multi-billion parameter lan-
guage models for value feedback at every single
step is computationally infeasible, especially given
the large number of environment interactions that
are often required to make progress in sequential
decision making environments. Further, exhaustive
search by exploring every single possible action is
similarly infeasible given the combinatorially sized5997action spaces—a text game of average difficulty
such as Zork1 hasO(10)possible actions per
step. GALAD thus uses an action candidate gen-
erator that takes the current state of the world into
account and produces a limited set of contextually
relevant action candidates. The GALAD action
candidate generator used in this work is a 117m
parameter autoregressive language model (100x
smaller than the value prior) with the architecture
of GPT-2 (Radford et al., 2019). Fig. 2 provides an
overview of this entire action distillation process.
Datasets. We use two offline datasets designed
to help agents produce contextually relevant ac-
tions. The first dataset is the ClubFloyd dataset
introduced by Yao et al. (2020) and Ammanabrolu
and Hausknecht (2020). It contains transcripts
of human playthroughs of over 500games in the
form of alternating observations and actions. The
second dataset is the JerichoWorld dataset intro-
duced by Ammanabrolu and Riedl (2021b). It
consists of similar samples from 31games but
collected through the use of oracle agents that
can play a game perfectly. Both datasets are pro-
cessed into ( context ,action ) pairs with context be-
ing the observations from two subsequent steps
c= (o, o)∀i∈ {1...n}and corresponding
action being a. We note that we have taken addi-
tional precautions via data curation to ensure that
there is no overlap between the games in the Jiminy
Cricket benchmark and the games used to collect
data in either of these datasets.
Training. The autoregressive action candidate
generator is trained on this combined dataset via a
modified sequence-to-sequence strategy, with the
language model being trained to produce action
agiven context c. Standard autoregressive tech-
niques factor the distribution over the tokens kin
the target sequence of length Minto a chain of con-
ditional probabilities with a left to right structure.
P(a|c;θ) =/productdisplayP(a|a, c;θ)(1)
Where θrepresents the overall action candidate
generator network parameters and each action a
consists of a sequence of tokens a. This can then
be used to formulate a maximum likelihood train-
ing objective with cross-entropy for each individual
data sample.
L=−logP(a|c;θ) (2)
=−/summationdisplaylogP(a|a, c;θ)(3)
As seen in Fig. 2, each of the data samples is
fed into the value prior language model to measure
the relative contextual morality of performing that
particular action given the context. The model
outputs a probability distribution over three labels
corresponding to whether the action corresponds
to good, neutral, or bad behavior given the specific
context of that scenario. Each of the samples used
to train the action candidate generator is scaled
by these values to bias it towards taking actions
that represent less harmful behaviors by learning
which action to notperform. This gives an action
distillation loss:
L=λ(1−P(bad|c, a;θ))L (4)
Where θrepresents the parameters of the value
prior and λis a scaling weight hyperparameter.
4.3 RL Policy Training
The overall GALAD agent, as seen in Fig. 3, is
trained inspired by the Deep Reinforcement Rele-
vance Network (DRRN) (He et al., 2016; Yao et al.,
2020) used for natural language state-action spaces.
It uses Q-Learning (Watkins and Dayan, 1992) to
learn a control policy π(a|c),a∈Athat directly
maximizes long horizon expectation over Rand
implicitly minimizes expectation over χ.
At each game step t, the environment provides
context consisting of the last two observations5998c= (o, o)to both the action candidate gener-
ator and the policy network. The action candidate
generator autoregressively decodes a set of jac-
tion candidates A={a...a}through nucleus
sampling (Holtzman et al., 2020) given the context.
These actions and also the context are passed into
two separate encoders in the policy network. These
encoders are recurrent GRU networks whose hid-
den state is initialized from the final hidden state in
stept−1. Actions and context are encoded into vec-
torized representations c,a∀i∈0...j. An ad-
ditional joint encoder takes c,aand learns a Q-
value function Q(c,a)representing the relative
utility of performing action agiven cto maxi-
mize long term expected reward. Softmax sampling
is applied to these Q-values to pick the action a
that is then executed in the game. The network is
trained using experience replay (Lin, 1993) with
prioritized sampling (Moore and Atkeson, 1993)
for experiences with positive reward. Experiences
in the form of tuples of ⟨c, a, r, c⟩collected
during training are stored in a replay buffer and
then minibatches are priority sampled to calculate
temporal difference (TD) loss:
L(ϕ) =r+
γmaxQ(c,a;ϕ)−Q(c,a;ϕ)
(5)
Where ϕrepresents the parameters of the policy
network. Further training details and hyperparame-
ters are found in Appendix A.3.
5 Evaluation
We conduct two evaluations to compare GALAD
to other contemporary agents and a secondary ab-
lation study to identify the relative importance of
the components of GALAD.
Metrics. We base performance on three primary
metrics: (1) Harmfulness score which is returned
by the annotated Jiminy Cricket environment and
verified by the human participant study in Section 3
as a measure of how harmful an agent’s actions
were for both itself and others; (2) Completion
percentage which measures the overall task perfor-
mance of an agent by way of looking at collected
game score as a proportion of the maximum score;
and (3) Relative harmfulness which is defined as
Harmfulness score /Completion percentage to ac-
count for the fact that agents that complete moreof the games will encounter more scenarios where
they might potentially act unacceptably.
As the human participant study in Sec. 3 sug-
gests that the more fine-grained annotations in the
benchmark relating to severity might be prone to
being more noisy, we modify our metrics from
Hendrycks et al. (2021b) and do not consider la-
bels associated with the relative severity of harm
and only consider the more high-level annotations
pertaining to valence (good or bad). Unadjusted
results calculated taking severity also into account
are found in Appendix A.4. No trends discussed
change due to this adjustment.
We follow the overall experimental setup of
Hendrycks et al. (2021b) for fair comparison, test-
ing each agent at 5 evenly spaced starting loca-
tions in each of the 25 games in the Jiminy Cricket
benchmark—resulting in 125 total environments.
Additionally, we run each individual experiment
with 5 random seeds and report averaged results
and standard deviations. Number of training steps,
number of parallel environments for experience
collection, and parameter size of both the action
candidate generator and policy network are held
constant across all agents we test. Further details
are found in Appendix A.3.
5.1 Comparison to Baselines
We compare to recent, state-of-the-art, text game
works that create agents belonging to the same class
of agents as GALAD as defined by Hausknecht
et al. (2020)—i.e. none of these agents have the
commonly used valid action handicap that gets the
set of ground truth contextually relevant action can-
didates at each step during training.
NAIL is a heuristic rules-based agent created by
Hausknecht et al. (2019) to function as a general
text game playing agent.
CALM is developed by Yao et al. (2020), this
agent only uses the ClubFloyd dataset to train its
action candidate generator without any value prior
and uses the DRRN (He et al., 2016) architecture
otherwise.
CMPS is the best performing baseline agent
provided in the Jiminy Cricket benchmark by
Hendrycks et al. (2021b). It is identical to the
CALM agent but uses an additional RoBERTa (Liu
et al., 2019) morality model trained on the common-
sense portions of the ETHICS dataset (Hendrycks
et al., 2021a) to perform policy shaping.
CMPS+ is an enhanced version of CMPS. For a5999
more fair comparison given our use of additional
datasets, we provide results for CMPS+ that uses
an action candidate generator trained on both of the
datasets shown in Sec. 4.2, without distillation.
Table 1 and Fig. 4 outline the results for this
evaluation. The main trend to note here is that for
most of the baselines, completion rates and socially
harmful behavior appear to be directly proportional
to each other—i.e. the more an agent explores,the more chances it has to accumulate harmfulness
score. In particular, we see this when comparing
the results of CMPS and CMPS+, CMPS+ uses
data better suited to predicting all possible valid
actions for a given state to train its action candi-
date generator and so achieves 6.3%higher relative
completion rate than CMPS. This comes at the ex-
pense of effectively aligning its behavior and it
performs actions that are deemed harmful 31.4%
more than CMPS—implying that the data used to
train action candidate generators contain a bias that
skews agents towards harmful behaviors.
In contrast, GALAD shows a 4%greater comple-
tion rate than the next best agent (CMPS+) while
simultaneously reducing harmful behavior by 25%
when compared to the next best agent for the met-
ric (CMPS)—with over 50% more socially bene-
ficial agent behaviors being shown in some of the
more realistic games such ballyhoo, suspect and
up to 10xin more fantasy themed games like star-
cross, suspended . This indicates that distillation
from large language model based value priors is
an effective way to translate social commonsense
knowledge into value aligned agent actions without
sacrificing competency. Qualitative examples of
GALAD’s behavior are found in Appendix A.5.6000
5.2 Ablation Study
We test four variations of GALAD to analyze the
behavior and performance of our agent.
GALAD–. To better understand the relative
trade-offs between optimizing for task performance
and socially beneficial behaviors in this environ-
ment, we negate the valence seen in Eq. 4 to encour-
age the action candidate generator to take actions
perceived by the value prior as being socially unac-
ceptable.
GALAD+PS. We use a smaller version of the
value prior used by GALAD—trained similarly but
with less than 1 billion parameters—to perform
policy shaping in a manner similar to CMPS.
GALAD+RS. We use the same value prior as
given above to perform reward shaping by subtract-
ing from the reward given to the agent at each step
by a factor proportional to how socially unaccept-
able an agent’s behavior is perceived as.
GALAD+Oracle. We train GALAD by using
the dense, ground-truth harmfulness score feed-
back returned by the Jiminy Cricket benchmark to
perform policy shaping for the agent similarly to
CMPS—solely for analysis purposes as an upper
bound on harmfulness.
Table 2 shows the results of this evaluation.
There are two major trends to note in this table.
First of all, GALAD performs best when no ad-
ditional constraints are placed during policy net-
work training through reward or policy shaping.Reward and policy shaping techniques are popu-
lar approaches to value alignment but our results
indicate that they pose difficult dual optimization
problems—prone to incurring more noise during
policy training—and that an easier way to align
agents is to pre-train them to contextually learn
what actions to not take in a scenario. Adding in
the Oracle as an upper bound to provide dense feed-
back significantly drops harmfulness, suggesting
that there is ample room for improvement.
Further, we note that GALAD–, an agent trained
to behave unacceptably, performs worse than
GALAD, GALAD+PS, CMPS, and CMPS+ across
all metrics. From this we conclude that there is not
always a direct trade-off between acting harmfully
and task performance, sometimes acting altruisti-
cally in these environments is necessary to improve
task performance—i.e. when in doubt, defaulting
to socially beneficial behavior is more effective
than defaulting to socially unacceptable behavior.
6 Conclusion
Modern testbeds for developing intelligent interac-
tive agents often contain incentive structures that
can bias agents towards acting in ways that are
harmful both for themselves and towards the en-
vironment and entities within. Value alignment is
often seen as being directly at odds with task per-
formance, i.e. assuring altruistic behavior requires
a proportional sacrifice in general task competency.6001This is a consequence of the fact that many value
alignment approaches are based on reward or policy
shaping—trying to learn socially beneficial behav-
ior from expert feedback—which directly places
task performance and socially acceptable norms
against each other in a dual optimization problem.
In an attempt to encourage a greater volume of
work that treats value alignment as an principal
property of AI systems, we show that with careful
design—in our case the GALAD system made by
distilling social commonsense knowledge present
in large language models to contextually learn soft
constraints on what actions not to take—it is possi-
ble to create agents that act less socially harmfully
with respect to themselves and other agents without
loss in competency.
7 Ethical Considerations
As mentioned, this work is an attempt to tackle
the issue of creating agents that consider the rel-
ative harmfulness caused by their behaviors as a
first class citizen in their design in addition to task-
based rewards. Agents that simply focus on task
rewards are at significantly greater risk of acting in
a manner harmful to themselves and others. Text
games, in particular the games in the Jiminy Cricket
benchmark, provide semantically rich, grounded,
and morally salient scenarios for agents to navi-
gate through. To better understand and mitigate
the inherent biases found within the games of the
benchmark, we conduct a large scale human partic-
ipant study to judge the relative moral valence and
salience of the scenarios present in these games—
attempting to verify how accurately our evaluation
metrics map to values considered to be socially
beneficial by this particular set of humans.
We further note that agents trained in text envi-
ronments are more suited for domains in which
change in the world is affected via language,
which mitigates physical risks—downstream lines
of work are not directly relevant to robotics—but
not cognitive and emotional risks (Hausknecht
et al., 2020). As noted earlier, any system capa-
ble of generating natural language, even within the
limits of fantasy domains as seen in certain games,
is capable of accidental or intentional harmful and
biased language use—a property which we miti-
gate but do not entirely eliminate through our value
prior (Sheng et al., 2019; Dinan et al., 2020). We
note that we do not use this value prior to reason
about these moral scenarios by itself in vacuum—it is instead used to implicitly bias the actions of
an RL agent towards actions that are deemed to
be more aligned with socially beneficial behaviors
grounded in a particular context within the dynam-
ics of the environment.
We acknowledge that the data used to train the
value prior and the games themselves in addition to
the added annotations heavily skew towards West-
ern cultural and social norms. Further, despite at-
tempts at data curation and downstream task verifi-
cation, it is possible that the values encoded within
such language model priors represent socially un-
acceptable views (Bender et al., 2021). We thus
emphasize that these are not universally applica-
ble values and that agents trained in these environ-
ments cannot be used to directly assess the social
acceptability of human made actions in more real
world scenarios. This work presents a first step in
attempting to reduce the implicit harms that arise
from training interactive, intelligent agents to focus
only on task performance in popular, contemporary
sequential decision-making environments.
Acknowledgements
This work was supported in part by DARPA MCS
program through NIWC Pacific (N66001-19-2-
4031), Google Cloud Compute, and the Allen Insti-
tute for AI.
References600260036004
A Appendix
A.1 Jiminy Cricket Environment Annotation
Framework and Participant Study Details
A.1.1 Annotation Setup
For our human participant verification we first ask
workers to determine whether the actions described
in the game snippet are morally salient or not. If
deemed morally salient, we use the same frame-
work used by Hendrycks et al. (2021b) to collect an-
notations of valence (bad/good), target (self/others),
and severity (rating from 1-3).
1. Good for Others
2. Good for Self
3. Bad for Others
4. Bad for Self
Workers were allowed to select any of the four
valence-target pairs, and provide a severity rating
for each selected pair.
To guide the annotations, we provided the work-
ers with the original examples from Hendrycks et al.
(2021b):
•Negative, Others, 3: Killing a person, rape,
torture
•Negative, Others, 2: Intending to kill some-
one, harming someone (injury, theft, destroy-
ing value), killing animal, unwelcome kissing,
releasing evil forces (no immediate effects),
huge environmental damage
•Negative, Others, 1: Trespassing, eavesdrop-
ping, being rude / crude language, harming
animals, risk of harm, lying (of itself), emo-
tional harm (e.g. scaring someone), invading
privacy, burning money, kill small plant
•Negative, Self, 3: You get killed, suicide,
maiming
•Negative, Self, 2 You get harmed (e.g., con-
cussion, hit hard), risk of death•Negative, Self, 1: Minor discomfort (weather,
stench, bad dream, getting pushed, small lacer-
ation), Destroying value (in your possession),
intoxication, risk of harm, gambling
• Positive, Others, 3: Saving a life
•Positive, Others, 2: Helping a human altruisti-
cally, positive human experiences, preventing
evil
•Positive, Others, 1: Feeding a piranha, trans-
actions, fixing things altruistically
•Positive, Self, 3: Your life is saved, you get in
a relationship
•Positive, Self, 2: Ephemeral positive human
experiences
•Positive, Self, 1: Drinking water, transactions,
brushing teeth, learning tidbit
Annotators were compensated at an average
hourly wage of $8/h. We provide screenshots of the
interface seen by our participants. Figure 5 details
the instructions our participants received. Figure 6
shows a qualifying question to ensure that partic-
ipants are familiar with text games as well as the
context for the particular game and the transcript
of the scenario they will evaluate.
The transcripts are generated by using a oracle
agent provided by the Jericho and Jiminy Cricket
frameworks and walking through the game. Each
annotation task contains a morally salient action-
outcome pair according to the Jiminy Cricket
source code level annotations, as well as the 15
preceding action-outcome pairs in the oracle game
transcript. For our annotation, we randomly se-
lectn=210 transcripts. Figure 7 show the questions
asked of the participants, in line with the annotation
framework, to better understand the differences in
the perceived social values of exhibited behavior
across demographic groups.
A.1.2 Annotation Result Details
In our annotations, N= 71 annotators rated our
n= 210 snippets. 48% of our annotators were
women and 48% men, and 40% of them were be-
tween 30 and 40, and 25% between 40-50. Work-
ers were 70% white, 7% Black, 6% Asian, 6%
Hispanic, and 7% identified as other/mixed. Po-
litically, workers skewed more liberal, with 66%
identifying as (moderately) liberal, and only 27%6005
(moderately) conservative. 4% of workers declined
to provide gender or racial identity, 5% declined
to provide age, and 7% declined to provide their
political leaning.
On the moral saliency task, annotators had a
pairwise agreement of 73%. For selecting valence
and for valence-polarity, agreement was moder-
ately high (83% pairwise agreement, Krippendorf’s
α= 0.30and 70% pairwise agreement, α= 0.30,
respectively), when counting two workers as agree-
ing if their annotations had overlap. However,
for rating valence-polarity-intensity triples, agree-
ment was much lower (35% pairwise agreement,
α= 0.17), which is expected due to the larger
range of choices.600660076008A.2 Value Prior Training Details
The size of the smaller version of value prior used
to perform policy shaping is comparable to the size
of the RoBERTa value prior used in CMPS. The
value prior here is a T5-large ( <1 billion parame-
ters) based Delphi model trained with C - N B (Jiang et al., 2021), as the
original T5-11B based model is too bulky to be
used on the fly during policy shaping. We note
that even with a smaller value prior, this is up to
5xmore time consuming than GALAD. Table 3
contains the details of the hyperparameters used
to train the T5-large Delphi model used for policy
shaping.
A.3 GALAD Training Details
Table 4 contains the details of the hyperparameters
used to train the action candidate generator and
policy network in GALAD. All RL experiments
are run using a single Nvidia A100 GPU for at most
12hours per 15,000steps.
The ClubFloyd dataset (Yao et al., 2020; Am-
manabrolu and Hausknecht, 2020) used to train the
action candidate generators consists of human tran-
scripts of the form T= (o, a, o, a...o, a)—
interleaving observations and actions. Process-
ing these down into context-action pairs c=
(o, o)∀i∈ {1...n}results in 223,547total sam-
ples in the dataset. The JerichoWorld dataset (Am-
manabrolu and Riedl, 2021b) mapping observation
oto a set of valid actions v={a, a...a}that
are guaranteed to change the state of the world in
the given state. These are also processed down
into context action pairs c=oand corresponding
action being a∀i∈ {0...n}resulting in 463,930
total samples.
State and Action Spaces. Observations or in-
puts to the agent consist of free form—potentially
incomplete—textual descriptions of what an agent
“sees” written by a human, i.e. the input vocab-
ulary is not given. In response, agents output alanguage string into the game to describe the ac-
tions that they want to perform. These strings con-
sist of interchangeable verbs phrases ( V P), option-
ally followed by prepositional phrases ( V P PP ),
e.g.([carry/take ])and([throw/discard/put ]
[against/on/down ]), where the verbs and
prepositions within [.]are aliases. The words com-
prising an action for a particular game is restricted
to a given vocabulary.
A.4 GALAD Unadjusted Results
Tables 5, 6 show the results of the comparisons to
the baselines and the ablation study with the orig-
inal metrics seen in Hendrycks et al. (2021b). It
takes into account the valence, target, and severity
dimensions in the annotations—summing the over-
all instances where the agent behaved harmfully
towards other entities to calculate the harmfulness
score. We note that the overall trends discussed in
Section 5 are unchanged from the adjusted results
seen in Tables 1, 2 that do not take severity into
account.60096010A.5 GALAD Gameplay Transcripts
This section provides transcripts of 3 randomly
selected scenarios where GALAD behaved ben-
eficially (either for itself or others) as judged by
the annotation framework and 3 randomly selected
scenarios where GALAD behaved harmfully taken
after the agent has converged given 15000 steps per
parallel environment in a batch of 8. We note that
given the results that of Table 1 that the number
of scenarios where the agent behaved harmfully
exceeds those where it behaves beneficially. The
transcripts start with a brief description of the game
they are taken from ( ifdb.org ), for additional
context for the reader.
A.5.1 Examples of beneficial behavior
Suspect: You have walked into a hotbed of de-
ceit and trickery. And now they’re accusing you
of something you couldn’t have done. "You’re a
killer," they say. And until you can prove them
wrong, you’re guilty as charged - murder.
The Lurking Horror: A winter night at the G.U.E.
tech campus with most students away on vacation
serves as the backdrop for this tale of Lovecraftian
horror.60116012Seastalker: There’s something down there in the
ocean, something terrifying. And you have to face
it - because only you can save the Aquadome, the
world’s first undersea research station.6013A.5.2 Examples of harmful behavior
Ballyhoo: Exploring the tattered corners of the
circus lot, you overhear a conversation about the
owner’s daughter who has been kidnapped. Good
samaritan that you are, you start poking around
on your own. But watch your step. As the night
progresses, you realize you’re in as much danger
as the little girl.6014
Planetfall: "Join the Patrol, and see the Galaxy!"
You took the poster’s advice, bait and all, and
marched right over to the recruitment station near
your home on the backwater planet of Gallium. Im-
ages of exotic worlds, strange and colorful aliens,
and Deep Space heroism had danced in your head
as you signed the dotted line.6015Spellbreaker: You explore the mysterious under-
pinnings of the Zorkian universe. A world founded
on sorcery suddenly finds its magic failing, and
only you, leader of the Circle of Enchanters, can
uncover and destroy the cause of this paralyzing
chaos.60166017