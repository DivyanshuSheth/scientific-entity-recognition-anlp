
Zhecan Wang, Haoxuan You, Yicheng He, Wenhao Li, Kai-Wei Chang, Shih-Fu ChangColumbia University, New York,University of California, Los Angeles
{hy2612, rs4110, zw2627, sc250}@columbia.edu,kwchang@cs.ucla.edu
Abstract
Visual commonsense understanding requires
Vision Language (VL) models to not only
understand image and text but also cross-
reference in-between to fully integrate and
achieve comprehension of the visual scene de-
scribed. Recently, various approaches have
been developed and have achieved high per-
formance on visual commonsense benchmarks.
However, it is unclear whether the models re-
ally understand the visual scene and underly-
ing commonsense knowledge due to limited
evaluation data resources. To provide an in-
depth analysis, we present a Multimodal Eval-
uation (ME) pipeline to automatically gener-
ate question-answer pairs to test models’ un-
derstanding of the visual scene, text, and re-
lated knowledge. We then take a step further
to show that training with the ME data boosts
model’s performance in standard VCR evalu-
ation. Lastly, our in-depth analysis and com-
parison reveal interesting findings: (1) semanti-
cally low-level information can assist learning
of high-level information but not the opposite;
(2) visual information is generally under uti-
lization compared with text.
1 Introduction
Vision Language (VL) understanding is challeng-
ing because it requires VL models to identify
and integrate information from both modalities
to fully understand visual scenes. Numerous VL
benchmarks have been created such as CLEVER
(Johnson et al., 2017), GQA (Hudson and Man-
ning, 2019), VQA (Li et al., 2018), VCR (Zellers
et al., 2019) and SNLI-VE (Xie et al., 2018).
These benchmarks typically form VL evaluation
in question-answering format with images and test
models’ understanding of both VL modalities. De-
spite the high accuracy achieved by existing large
pretrained VL models, recent works have pointed
out that VL models tend to exploit data biases suchFigure 1: An example from VL benchmark, Visual Com-
monsense Reasoning (VCR) (Zellers et al., 2019). VL
models can answer the highly semantic VCR question
correctly but fail terribly in answering related visual
question (Q2), textual question (Q3), and background
knowledge question (Q4).
as shallow mappings with language priors and un-
balanced utilization of information between modal-
ities (Cao et al., 2020; Jimenez et al., 2022; Sel-
varaju et al., 2020).
As in Fig. 1, notwithstanding the model’s suc-
cess in answering Q1, the same model fails to an-
swer the related visual, textual, and background
questions. This example demonstrates that the VL
model does not fully understand the visual scene,
which leads to prediction inconsistency (when one
model makes conflicting(inconsistent) predictions
in two related questions). In our analysis, predic-
tion inconsistency is surprisingly common among
models in different modalities. Former works
have also pointed out that most VQA systems
achieve only middling self-consistency ( 60−80%)9212(Jimenez et al., 2022). Therefore, we pose doubts
to existing VL models’ ability to thoroughly com-
prehend visual commonsense despite their high
accuracy performance on the leaderboards.
In this work, we propose to evaluate models’ un-
derstanding and consistency in predictions across
modalities. For that intention, we propose a Mul-
timodal Evaluation (ME) evaluation schema that
can augment existing VL benchmarks like VCR.
For any given sample of the VL data, e.g.image-
question-answer pair, ME first retrieves and ex-
tracts related information of three modalities: vi-
sion, text, and background knowledge. After that,
it unifies the information across modalities via a
multimodal graph and further automatically gen-
erates related sub-questions corresponding to all
three modalities (as examples shown in Fig. 1).
The sub-questions would be semantically rele-
vant to the input image-question-answer pair, and
therefore, after answering the original input ques-
tion, we can further utilize them to evaluate existing
VL models’ understanding across the three modal-
ities and pinpoint their shortcoming and biases.
Under minimal human verification, with ME, we
create Multimodal Sub-question Evaluation Bench-
mark with 630k multiple choice sub-questions for
110k images from VCR (Zellers et al., 2019): 110k
of them are visual; 260k of them are about text; and
the rest 260k are related to background knowledge.
After in-depth evaluation and analysis with top-
performing VL models, we discover a few interest-
ing findings: (1) semantically low-level informa-
tion can assist learning of high-level information
but not the opposite; (2) visual information is gen-
erally under utilization compared with text. (3) VL
models may struggle to utilize related background
knowledge information.
Besides, we propose a Multimodal Coaching
(MC) framework to conditionally augment sub-
questions in training. Depending on VL mod-
els’ behavior, MC would conditionally decide if
it should augment to reinforce the understanding of
a particular modality. We show that by using MC,
we not only improve models’ consistency but also
the overall performance. For example, MC boosts
the performance of VL-BERT by more than 1%on
the original VCR Q2A metric and even more then
7%in sub-question evaluation metric.
Our contributions include:
1.We identify while existing VL models per-
form well in commonsense benchmark, theycannot answer related sub-questions.
2.Our proposed fine-grained automatic evalua-
tion approach allows the communities to bet-
ter evaluate VL models. The code/dataset will
be released upon acceptance.
3.Our in-depth evaluation and analysis with
top-performing VL models discover that: (1)
Training with semantically low-level informa-
tion may assist learning high-level concept
but not the opposite; (2) Visual information is
generally under utilized compared to textual
information.
2 Related Work
Biases occur if VL models cannot comprehensively
understand the contents of both images and texts.
They need to not only understand information from
the two modalities respectively but also integrate
these information by cross-referencing. (Cao et al.,
2020; Manjunatha et al., 2019) pointed out biases
like unbalanced utilization between visual and tex-
tual information. Based on these findings, previous
works proposed different methods for countering
them in VL benchmarks. For instance, (Agrawal
et al., 2018; Zhang et al., 2016; Dancette et al.,
2021) diversifies and shifts VQA’s answer distri-
bution (Goyal et al., 2017) to balance the dataset;
(Gokhale et al., 2020; Liang et al., 2020a; Gupta
et al., 2022; Liang et al., 2020b) augments images
or creates counterfactual images to train more ro-
bust models on VQA; (Niu and Zhang, 2021; Ra-
makrishnan et al., 2018; Niu et al., 2021; Wang
et al., 2022; Zhang et al., 2021) regularizes models’
training with prior knowledge to avoid learning
biases; (Ye and Kovashka, 2021) directly aligns
pronouns to demonstrate biases in VCR (Zellers
et al., 2019) , etc. However, none of them helps us
evaluate VL models’ understanding on each modal-
ity. Without understanding how much models un-
derstand the image, the text, or the background
knowledge, it is difficult to further regularize mod-
els in training.
Recently, large pretrained VL models, which
are mostly trained as implicit black boxes, have
been dominating VL benchmarks. It is difficult
to know if they understand the image and the tex-
tual information other than simply memorizing it.
Question-answering is the most general format for
evaluating a wide range of models while having
minimal requirements. (Ray et al., 2019; Ribeiro
et al., 2019; Selvaraju et al., 2020) annotated addi-9213tional questions on top of VQA questions to mea-
sure VL models’ consistency on prediction. How-
ever, these works only focus on semantically low-
level dataset like VQA and do not apply to highly
semantic dataset like VCR. Moreover, their data
fully relies on manual annotation and thus is hard
to scale. Furthermore, they also fail to evaluate
models’ understanding across modalities. To solve
these problems, we create a VL evaluation method
that generates data with minimal human efforts,
differentiates evaluation between modalities, and
applies to highly semantic VL benchmarks.
3 Multimodal-Eval (ME)
Given an input image-question(-answer) pair, ME
first analyzes information from the pair. Then it
would generate three follow-up fine-grained ques-
tions called sub-questions corresponding to three
modalities: vision, text, and background knowl-
edge. Following VCR’s format, each sub-question
also has four answer choices with one correct an-
swer. The VL models are expected to first answer
the VCR question and then answer the three re-
lated sub-questions. Through evaluating predic-
tions of the sub-questions, we can test models’ un-
derstanding across modalities. Overall, ME has
two parts: (1) Multimodal QA Generator that
generates related sub-questions of three modalities
and (2) Evaluation , which test VL models’ capa-
bilities with the sub-questions. We structure the
presentation as below: the method section explains
the QA generation process and the experiment sec-
tion discusses the evaluation process.
4 Multimodal QA Generator
We introduce Multimodal QA Generator through
the following steps in order: (1) Retrieving related
sentence statements of three modalities against the
input image-text, (2) Parsing the statements into
three unimodal graphs and then merging them into
a multimodal graph, (3) Converting triplets in a
multimodal graph into question and answer, (4)
Distractor Generation, (5) Adversarial Filtering.
4.1 Retrieving Statements
For producing relevant sub-questions, we need to
first analyze the input image-text pair and even ex-tract information from it. Therefore, it is intuitive
to have the input image and text information rep-
resented in the same level of complexity. Because
the input question-answer is already in text format,
we want to convert the image into text.
Visual Statement: Most of the existing highly-
semantic VL benchmarks build on top of im-
age/video captioning dataset, e.g. VQA (Goyal
et al., 2017) from COCO Captions (Chen et al.,
2015), SNLI-VE (Xie et al., 2018) from Flickr30K
(Young et al., 2014), VCR from LSMDC
(Rohrbach et al., 2017) , etc. Those captions are
visually descriptive and are not included in the
image-question-answer pair. Therefore, we can di-
rectly retrieve those already annotated captions as
visual statement.
Textual Statement: The input text prompt, e.g.the
question-answer pairs in VCR, can be converted
into statements with heuristic templates. For in-
stance, the QA in Fig. 2 can be converted to "Per-
son1 plays a trombone in front of everyone" + "be-
cause" + "he is performing a solo". We can also
regard the converted statement as textual statement,
as shown in Fig. 2 (Details in Appendix).
Background Knowledge Statement: In order to
obtain background knowledge relevant to the visual
scene, we apply keyword extractors (Campos et al.,
2020) to extract keywords from visual and textual
statements. Then we can regard those keywords
as query concepts (as illustrated in Fig. 2, query
concepts "trombone" and "solo" are extracted from
the visual and textual statements). Based on query
concepts, we can further browse external knowl-
edge database, i.e.ConceptNet (Speer et al., 2017)
to retrieve 1-hop related conceptsthrough a pool
of hand-selected relationships. As illustrated in
background knowledge graph in Fig. 2, different
triplets consisted of (Subject, Predicate, Object) are
retrieved and can be conveniently converted into
basic Subject-Verb-Object (SVO) sentences.
4.2 Generating Graph
For better integrating information, we leverage a
language parser to parse the statements so that we
can obtain semantic roles: Subject, Predicate and
Object (S, P, O). These roles help further unify fine-
grained information across three modalities. With9214
comparing them, we may find connections.
Domain-specific Graph: The background knowl-
edge we retrieved from ConceptNet is already in a
graph consisted of triplets (S, P, O). Therefore, we
only apply the Scene Graph Parser (Schuster et al.,
2015) to parse visual and textual statements into
graphs. As shown in Fig. 2, we then have three
domain-specific graphs corresponding to all three
modalities.
Multimodal Graphs: To merge two graphs, we
take turns to compare the similarity between each
pair of nodes from them. During comparison, we
not only measure their concepts’ similarity but also
their neighbors/context similarity. If they are sim-
ilar, they would be merged into one node. For
instance, given graph GandG, we compare ev-
ery node v, i∈[0, . . . , n ]inGwith every node
v, j∈[0, . . . , m ]inG. We calculate the se-
mantic similarity score between them, sim(v, v)
through an external tool (Zhu and Iglesias, 2017).
Subsequently, we also compare the neighbors of
vagainst the neighbors of v. Let’s assume vhas
p1-hop connections in Gandvhasq1-hop con-
nections in G. Every connection of vlinks two
concepts thus forming a triplet. It can be converted
into a SVO sentence containing vas either the Sub-
ject or Object. With that, we result in psentences
related to v. Similarly, we can also obtain qsen-
tences related to v. Following (Ni et al., 2022), we
inference a pretrained Sentence-T5 to extract pand
qsentence embeddings. Then, for every pair be-
tween S, l∈[0, . . . , p ]andS, o∈[0, . . . , q ], we
calculate the cosine distance sim(S, S). Lastly,
for every pair, (v, v), we sum both node concept
similarity and context similarity together by:Score(v, v) = sim(v, v) +/summationtext/summationtextsim(S, S)
p·q.(1)
IfScore(v, v)is larger than a threshold T
(Details in Appendix), we would consider v, vas
duplicates and only keep one in the graph.
Selecting Relevant Sub-graphs: After obtaining
the graph representation, we want to generate sub-
questions relevant to the input image and VCR
question of each sample u. Therefore, we filter
each triplet in the multimodal graph by its relevance
against the input image-question-answer pair.
Similar to above, we convert all rtriplets in
multimodal graph into sentences S, k∈[0, . . . , r ]
and measure their similarity to the textual statement
(a conversion of the input QA) via (Ni et al., 2022),
sim/parenleftig
S, S/parenrightig
.
Afterwards, we further utilize a pretrained CLIP
(Radford et al., 2021) to encode and then calculate
the cosine distance between every sentence against
the image I,rel(S, I). In conclusion, the
final score for every triplet would be:
/vextenddouble/vextenddoublesim/parenleftbig
S, S/parenrightbig
||+/vextenddouble/vextenddoublerel(S, I)||.(2)
After ranking, we select the top-1 ranked triplet
in every modality. If a triplet is selected in more
than one modality, we replace the duplicate with
the next following triplet in the same modality.
4.3 QA Templates
Given a triplet, we can ask questions about the
subject, the object, or the predicate. For instance,
in (boy, in front of, people), if asking about the
object, we could use templates like "What is the
[Subject] [Predicate]" (What is the boy in front
of?). In this case, the basic answer would be [Ob-
ject](People) or the converted full SVO sentence of9215the triplet, [Subject][Predicate][Object](The boy is
in front of people). When asking about the subject
or the predicate, similar procedure applies (Details
in Appendix)
4.4 Distractor Generation
The new evaluation task should have the same for-
mat as the original one (multiple-choice-format
(MCQ) in VCR) so that we can directly evaluate
existing models. For that purpose, it is necessary
to generate incorrect answer choices, distractors.
Simply rephrasing the correct answer may pro-
duce false negative that confuse the models. In a
sense, more non-trivial and meaningful disturbance
should be added to the answer distribution.
In practice, we choose to represent the answer
in SVO sentence format, e.g.“The boy is in front
of people”. We first parse the answer into (Subject,
Predicate, Object) e.g. (boy, in front of, people)
and regard this as the starting templates for creating
distractors. If the question is asking about the rela-
tionship, then we could regard relationship as the
“changeable part” in the template. We could replace
this “changeable part” with other words to create
new combinations for distractors e.g.(boy, behind,
people). In order to make meaningful replacement,
we use the original relationship concept, “in front
of”, as the query concepts to retrieve related con-
cepts from external resources like “behind”, “direc-
tion”, “location”. We apply the same procedure to
the subject and object.
Explicit Retrieval from External Knowledge:
We follow a similar procedure in retrieving back-
ground knowledge concepts from ConceptNet
(Speer et al., 2017), while only differing in our
selection of a different set of relationships (Details
in the Supple).
Implicit Retrieval from Language Models: We
also utilize pretrained language models to help re-
trieve related concepts in two perspectives. First, in
cases when the question is asking about the object
and the program fails to retrieve related concepts
from explicit resources, we leverage prompt engi-
neering to implicitly retrieve related concepts from
a pretrained language model, GPT2 (Radford et al.,
2019) alternatively. Using the same triplet as an
example, if the question asks about the object, then
we would design the prompt as “boy is in front
of [mask]”. After GPT2 fills in the [MASK], we
should be able to retrieve external concepts within
GPT2’s top predictions. We can further use as themas options for objects in distractors.
After successfully replacing concepts in the tem-
plate, we directly apply heuristic rules and convert
it into SVO sentences with heuristic rules to create
a distractor. Aiming for variety beyond rule-based
sentence construction, we also alternatively use an-
other language model to process the conversion.
We exploit a sentence generation model, T5 fine-
tuned on CommonGen (Lin et al., 2020) which is
built on top of ConceptNet. The training task in
CommonGen is to convert set of concpet words
into everyday sentences. For example, after replac-
ing concepts in the template, from (boy, in front of,
people) to(boy, back, people) and(boy, direction,
people) , we input them directly into T5, which
outputs a list of possible sentences e.g. “A boy
is running back to the people”, “A boy is facing
the same direction as the other people”. Differ-
ent from hard-coded templates used to generate
SVO sentences, T5 fills in context words around
the input concepts, thus also helps retrieving im-
plicit external concepts like (“running”, “facing”,
“same”, “other”). These additional concepts may
not be relevant to the visual scene which aligns
with the purpose of generating distractors.
4.5 Adversarial Filtering
High-quality distractors should be semantically re-
lated to the answer but also different enough for
humans to tell. Therefore, we design our own ad-
versarial filtering (Zellers et al., 2018, 2019) mech-
anism by using pretrained VL and language models
to filter data. We first correct all generated distrac-
tors by an off-shelf grammar checker. Then we
further filter them by a pretrained language model
to remove distractors that are too semantically close
to the correct answer to reduce potential false neg-
atives. Lastly, we apply a pretrained VL model
to measure their relevance against the image and
select the top three as final distractors (Details in
Appendix).
5 Dataset
Dataset Statistics Built on top of (Zellers et al.,
2019), Multimodal Sub-question Evaluation Bench-
mark has around 110k visual sub-questions corre-
sponding to the 110k images from (Zellers et al.,
2019), 260k text(prompt) sub-questions, and 260k
background knowledge sub-questions correspond-
ing to the 290k original questions from (Zellers9216
et al., 2019). Every question has four answer
choices and the answers have an average length
of 5.5 words. The ratio between training set and
validation set is 10 : 1 .
Quality Control To deliver a convincing eval-
uation method to existing VL models, we have
humans verify the full validation set. We designed
and deployed a user interface on Amazon Mechan-
ical Turk platform and hired experienced turkers
(with $12.6/hr)to help verify the correctness of
our questions and answers. Every image-question
pair was cross-verified and corrected by five turkers
(Details in Appendix).
Evaluation We randomly select 2 disjoint sets
each containing 100 image-question pairs from ME.
The first set consists generated QA data. The sec-
ond one consists QA data verified and corrected by
the turkers. We then hire an additional group of five
turkers to answer those 200 image-question pairs
without knowing the answer label. Next, we cal-
culate the predictions’ accuracy. As in Tab. 1, the
difference between generated and verified data is
very minimal which demonstrates the high-quality
of our generated data (More in Appendix).
6 Evaluation
In the following, we conduct experiments based on
the proposed dataset to demonstrate (1) The exist-
ing models that perform well on VL dataset often
cannot answer detailed vision, text, knowledge sub-
question correctly; (2) It is easier for VL models to
answer sub-questions originated from semantically
low-level VCR questions than high-level ones.Base Methods During our experiments, we use
three top-performing models, VL-BERT (Su et al.,
2019), UNITER (Chen et al., 2020) and VILLA
(Gan et al., 2020) on VCR leaderboard as our base
models.
Evaluation Metrics When calculating the orig-
inal Q2A accuracy of VCR (Zellers et al., 2019)
onntotal samples, let C be an indicator vari-
able for sample j, j∈[0, . . . , n ]. If the predic-
tion,P, is the same as the label, L, the
prediction is correct and C = 1; otherwise
C= 0.
Accu.=/summationtextC
n.
Similarly, in our new metrics, we have indicator
variables C for the correctness of prediction
on sub-questions related to modality x, which can
be vision, text, or background knowledge; simi-
larly, we use C to indicate the event that
both the VCR question and the sub-question corre-
sponding to modality xare correct. Lastly, C
indicates the event that all the sub-questions related
to sample jare predicted correctly.


C = 1, ifP =L ,else0
C = 1, if C= 1and C = 1
C= 1, if/summationtextC = 3, x∈ {V, T, BK }
6.1 Comparison across Modalities
We want to evaluate VL models’ capability in un-
derstanding fine-grained information from different
modalities. Tab. 2 shows the evaluation results of
the models. Looking at rows marked with "N"
under the column name "ME in Training", we dis-
cover that existing VL models all suffer around a
20% drop in accuracy on our sub-questions’ met-
rics. Among modalities, VL models generally per-
form the best in textual sub-questions. This is ex-
pected since the semantic contents of the textual
sub-questions are the closest one to the original
VCR questions. In contrast, these models often per-
form slightly worse in visual sub-questions. This
re-verifies previous works’ concerns that existing
VL models generally under-utilize visual informa-
tion. Lastly, they all suffer the most in answering
background knowledge sub-questions. We believe
that despite background knowledge may be useful
in humans’ perspective, VL models are still lack
of sufficient abilities to utilize them. In fact, they
seem to have the largest domain gap against the9217original VCR questions to VL models. These eval-
uation results help verify our previous hypothesis.
Consistency: When considering consistency, even
larger drops about 40% occur across models’ per-
formances. The general trend of performances on
Q2AS-x between modalities is similar to Q2S-x as
discussed before, but with lower overall values.
6.2 Comparison across Question Types
The first row in Fig. 3 (A) visualizes the number
of questions of each type in VCR validation set.
Observing it, we can see clear imbalanced distribu-
tion exists among question types in VCR validation
set. Hence, we on purpose, sample 2k questions of
each type from validation set to create a balanced
mini-validation set of 14k VCR image-question
pairs. We further evaluate the finetuned VL-BERT
on this mini-validation set. On the second row,
for each question type, we visualize the number of
Q2A questions that VL-BERT predicts correctly.
From the third to the fifth row, we visualize sub-
questions (originated from different types of Q2A
questions) that VL-BERT predicts correctly. As
we can see, for Q2S-V , explanation, activity and
scene questions have the most percentage. Apart
from the explanation and activity questions also
being the most dominant question types in the train-
ing set, the semantic relatedness with the activity
questions and the explanation questions also helps
models answer visual sub-questions. Additionally,
we realize that it is easier for the model to answer
sub-questions (from all three modalities) of seman-
tically low-level VCR questions like explanation,
activity types. However it is difficult for abstract
ones like mental questions.
7 Multimodal Coaching for Model
Improvement
Besides utilizing ME data for evaluating existing
VL models’ performance of fine-grained under-
standing across modalities and prediction consis-
tency, we also find that ME can further assist exist-
ing VL models’ training.
7.1 Multimodal Coaching
In order to better utilize ME data and allow VL
models to have a balanced learning over infor-
mation from different modalities, following (Ray
et al., 2019), we design the Multimodal Coach-
ing (MC) system. According to (B) in Fig. 3,
when iterating over every VCR sample in training,
Multimodal QA generator produces relevant sub-
questions. Then MC would take turns to test theQA model with those sub-questions across three
modalities. If the model fails on any of them, the
corresponding sub-question would be added to the
training pool otherwise it would be passed. There-
fore, we selectively augment ME data with VCR
data during the training.
7.2 Data Augmentation
We demonstrate the effectiveness of training with
ME data in Tab. 3. We keep VL-BERT as our base
model and cumulatively add sub-questions across
three modalities into the training set. VCR has 7
types of questions and some of them are highly
semantic not much related to visual compositions
like mental, hypothetical questions. With this prior
knowledge, when adding visual sub-questions, we
on purpose do not augment them to VCR questions
of these two types.
We observe in Tab. 3 that adding visual and
textual sub-questions both bring improvements
on Q2A, QA2R and sub-question metrics includ-
ing Q2S, Q2S-V , Q2S-T and Q2S-BK. However,
adding background knowledge sub-questions hurts
the performance. As mentioned before in Evalua-
tion section, the additional content from external
database like ConceptNet has a large domain gap
against VCR questions and thus may be too diffi-
cult for VL models to utilize. However, this result
further debunks existing VL models’ vulnerability
and confirms that it is important to include back-
ground knowledge sub-questions in VL evaluation
analysis.
Lastly, looking at the last row of Tab. 3, we
observe that MC could further boost VL-BERT’s
performance gain. In experiments, we also realize
that adding MC would allow training loss to be
more stable and converge faster.
7.3 Composite vs. Component Information
Comparing the first row against the third row in Tab.
4, we notice that VL-BERT performs better on Q2A
when having both VCR questions and visual sub-
questions in training set. Comparing the second
row against the third row, we also discover that VL-
BERT performs better on Q2S when the training set
only contains visual sub-questions. Adding VCR
questions would actually hurt its performance on
Q2S.
We observe similar results when comparing
other sets of rows like the (first, fourth, fifth) rows
for text sub-questions, and the (first, sixth, seventh)
rows for background knowledge sub-questions.9218
Even though, when having both background knowl-
edge sub-questions and VCR questions in training,
the Q2A performance drops slightly (due to po-
tential reasons explained above), the Q2S perfor-
mance drops even much more due to adding VCR
questions. Also, Q2A performance via training on
background knowledge sub-questions only is even
higher than the Q2S performance via training on
VCR questions only(Both questions share the same
MCQ format with four answer choices and random
guess is 25%).
If we regard VCR questions as composite infor-
mation since information from different modalities
are combined together in the questions, we can
then refer sub-questions as component information
"parsed from" the composite information. Based onthe comparison, we conclude that low-level com-
ponent information could potentially help models’
understanding of high-level composite information.
However, after learning with high-level composite
information, existing VL models may struggle to
utilize the high-level to help understand low-level
component information.
7.4 Comparison across Modalities
As in Tab. 2, after adding ME sub-question data
in training, VL models generally improve in accu-
racy across Q2A, sub-question metrics and consis-
tency metrics. Complementary to the findings in
theEvaluation section, we discover that (1)VL
models tend to have more consistent predictions in
answering textual sub-questions; (2)Adding tex-
tual sub-questions in training also brings more im-
provements on sub-questions metrics correspond-
ing to the other two modalities.
8 Conclusion
In this work, we propose ME to thoroughly probe
VL models’ understanding across and between
modalities. Our analysis brings new insights and
our experiments show that ME boosts models’ per-
formance when used in training.92199 Limitation
ME requires the given image to have paired cap-
tions so they can be easily converted into visual
statements. When absent, we can inference from
a pretrained caption generator at the expense of
accuracy. However, sometimes the visual caption
generator may not fully captures the most salient
activities in the image and thus produces trivial
captions with limited contents. Therefore, it would
be difficult for ME to extract related information
from the caption to further create the visual sub-
question.
Also, technically, for any VL dataset with image-
question-answer pairs, ME should be able to gener-
ate sub-questions from three modalities. However,
if the input question is very simple and focuses
on semantically low-level information. It woud
be challenge for ME to further extract and create
sub-questions from three modalities.
This study is solely based on English data and
leverages linguistic structures in English so it can-
not generalize to other languages.
10 Appendix
10.1 Generated vs. Verified
In Tab. 5, we evaluate VL-BERT with both gener-
ated ME data and data verified by human annota-
tors.
In Tab. 6, we finetune a VL-BERT with both
generated and verified data by humans.
Results from both tables demonstrate the high-
quality of our generated data.
10.2 Hyper-perameter
1. In practice, the semantic similarity between con-
cepts of two nodes would be first standardized via z-
score and then compared against a hyper-parameter
Tof 0.8.
10.3 Examples in other VL Benchmarks
Referring to Fig. 4, 5.
10.4 User Interface
Referring to Fig. 6
10.5 Adversarial Filtering
High-quality distractors should be semantically re-
lated to the answer but also different enough for
humans to tell. Therefore, we design our own ad-
versarial filtering (Zellers et al., 2018, 2019) mech-
anism by using pretrained VL and language modelsto filter data. We first correct all generated distrac-
tors by an off-shelf grammar checker. Then we
further filter them by a pretrained language model
to remove distractors that are too semantically close
to the correct answer to reduce potential false neg-
atives. Lastly, we apply a pretrained VL model
to measure their relevance against the image and
select the top three as final distractors.
Sentence-Similarity Modeling: Similar to pre-
vious procedures, across all znumber of dis-
tractors, we compare each of them S, w∈
[0, . . . , z ]against the textual (QA) statement S,
Score= sim/parenleftig
S, S/parenrightig
. By removing dis-
tractors whose Scoreis above a threshold, D
(0.7), we reduce potential false negatives that are
semantically close to the correct answer.
Image-Text Matching: After that, we also need
to ensure that the distractors are visually rele-
vant to the image. We load a pretrained CLIP
model (Radford et al., 2021) to measure the rel-
evance between each distractor against the image,
Rel= rel(S, I). We rank all the distrac-
tors by Reland select the top 3 distractors as
the final distractors.
10.6 Quality Control
To deliver a convincing evaluation method to ex-
isting VL models, we have humans verify the full
validation set. We designed and deployed a user in-
terface on Amazon Mechanical Turk platform and
hired experienced turkers (with $12.6/hr)to help
verify the correctness of our questions and answers.
Every image-question pair was cross-verified and
corrected by five turkers
Having the image on the side, every turker would
be first asked to verify the correctness of the ques-
tion in terms of grammar or understanding. If the
question is marked as incorrect or not understood,
we would ask the turkers to help re-correct the ques-
tion or skip it. Then the turkers would be provided
with 7 answer choices (1 correct answer choice
and 6 incorrect answer choices) and 2 additional
choices of "None of the above" and "I do not know
how to answer".
Avoiding causing any prior biases in the turkers
and resulting in false positives and false negatives,
we do not inform turkers the number of correct
answer choices and ask them to select all the ones9220Verified Evaluation
VCR (Q2A) Q2S Q2S-V Q2S-T Q2S-BK
N 75.67 54.23 54.81 54.95 54.87
Y 75.67 55.31 54.96 56.18 55.75
Verified in Training Evaluation
VCR (Q2A) Q2S Q2S-V Q2S-T Q2S-BK
N 76.13 59.34 60.77 61.74 58.8
Y 76.59 61.16 60.12 62.81 58.75
they think are correct. If they cannot understand
the visual scene or find a correct answer at all, they
can even select "I do not know how to answer" or
"None of the above". After selecting the answer
choices, we also give the turkers options to go over
every answer choice to re-correct it if there is any
grammatical issue. In the end, if the turkers have
selected "None of the above" before, they would be
asked to created their own correct answer choices.
To ensure the correctness of the annotation inter-
face, we first conduct many in-house experiments.
After that, we also randomly select several turk-ers’ annotations as pseudo groundtruths. We fur-
ther evaluate other turkers’ annotation against the
pseudo groundtruths to ensure the agreement rate
on selections.
For an image-question pair, if turkers have dif-
ferent selections on the correct answer choices, we
would avoid avoid using any answer choices se-
lected as correct by any of the turker as a distractor.
When filtering the annotations, we ensure that
every selected final distractor in ME cannot be se-
lected by any of the turkers as correct before to
avoid false negative. Further, when filtering every9221
sample’s annotations, among the five turkers, we
ensure that the selected final correct answer choice
should be selected by at least three of them to avoid
false positive. If more than one answer choice is
selected three times, we would compare and select
the one that has the most selections.
References922292239224