
Hao Zhang
Supportiv Inc, Berkeley, CA
haozhang@alumni.princeton.edu
Abstract
Pre-trained language models (LMs), such as
BERT (Devlin et al., 2018) and its variants,
have led to significant improvements on various
NLP tasks in past years. However, a theoretical
framework for studying their relationships is
still missing. In this paper, we fill this gap by
investigating the linear dependency between
pre-trained LMs. The linear dependency of
LMs is defined analogously to the linear de-
pendency of vectors. We propose Language
Model Decomposition (LMD) to represent a
LM using a linear combination of other LMs
as basis, and derive the closed-form solution.
A goodness-of-fit metric for LMD similar to
the coefficient of determination is defined and
used to measure the linear dependency of a set
of LMs. In experiments, we find that BERT
and eleven (11) BERT-like LMs are 91% lin-
early dependent. This observation suggests that
current state-of-the-art (SOTA) LMs are highly
“correlated”. To further advance SOTA we need
more diverse and novel LMs that are less de-
pendent on existing LMs.
1 Introduction
Large-scale pre-trained language models (LMs)
have pushed state-of-the-art (SOTA) of NLP re-
cently (Han et al., 2021; Qiu et al., 2020). Fol-
lowing BERT (Devlin et al., 2018), many variants
and improvements have been proposed since then.
They differ from BERT in various aspects, in terms
of training data (Liu et al., 2019), multilingual
support (Conneau et al., 2019), model size (Sanh
et al., 2019; Lan et al., 2019), pre-training objec-
tive (Yang et al., 2019), model architecture (Clark
et al., 2020), attention structure (He et al., 2020),
and sequence length (Beltagy et al., 2020). How-
ever, to the best of our knowledge, their relation-
ships have not been studied from a mathematical
perspective at the time of writing this paper.
In this work, we present a quantitative frame-work for studying LM dependency andcorrelation .
Conceptually, we view LM (e.g., encoders like
BERT (Devlin et al., 2018)) as a vector-valued func-
tion (or random vector) u(x) : Ω→R, where x
is text sequence from the sequence space Ω, andR
is the sequence embedding space. Sequence em-
bedding is defined as the mean pooling of the last
layer’s token embeddings. We define linear com-
bination and dependency of LMs analogously to
vectors/functions, and based upon these definitions
we propose the Language Model Decomposition
(LMD) algorithm to analyze the linear dependency
of LMs. A goodness-of-fit metric is defined for
LMD to quantify the linear dependency and corre-
lation of LMs. In experiments, we find BERT and
its successors are highly linearly dependent.
Theoretically, the linear dependency between LMs
implies redundancy, since they can be represented
by each other. Practically, it simply means building
more redundant LMs do not bring in new knowl-
edge. If LMs are more linearly independent, we
can potentially distill more knowledge (Alkhulaifi
et al., 2021) from multiple diverse models, and
combine them to create more powerful models.
The contributions of this paper are: (1) We formal-
ize the notion of linear dependency for LMs, and
propose Language Model Decomposition (LMD)
to study linear dependency; (2) We define an uni-
versal metric based on LMD to quantify the de-
pendency and correlation of LMs. (3) Our ex-
periments reveal that BERT and its variants are
91% “correlated”, suggesting current SOTA LMs
are highly redundant. The code is available at
https://github.com/haozhg/lmd .
2 Language Model Decomposition
2.1 Notations and Definitions
In this section, we formalize some necessary math-
ematical notations and definitions.2508Definition 2.1.1 (Linear Combination of LMs)
Given nLMs{u(x)}, a linear combination of
these LMs is/summationtextWu(x), where W∈R
are matrices.
Notice that here the coefficients are matrices, while
for vectors/functions they are scalars.
Definition 2.1.2 (Linear Dependent LMs) A set
of LMs {u(x)}is linearly dependent, if there
exists matrices {W}not all singular, such that/summationtextWu(x) =0,∀x∈Ω, where 0denotes the
zero vector.
Recall that a matrix Ais singular ⇐⇒Ais not
invertible ⇐⇒ det(A) = 0 . If the LMs are
not linearly dependent, they are said to be linearly
independent, that is, the above equation can only
be satisified by singular W,∀i.
Corollary 2.1.1 (Linear Dependency Condition)
A set of LMs is linearly dependent if and only if
one of them is zero or a linear combination (as in
Definition 2.1.2) of the others.
2.2 Language Model Decomposition
To quantify the degree of “linear dependency” of a
set of LMs, we propose Language Model Decom-
position (LMD), where a LM is approximated by
a linear combination of other LMs. LMD is moti-
vated by the Galerkin projection method (Reddy,
2010) that is widely used for model reduction and
reduced-order modeling. In particular, given a tar-
getLMu(x), andkbasis LMs{v(x)}, we fit
a model in the following form
u(x) =/summationdisplayWv(x) +e(x), (1)
where e(x)is the residual term. To simplify the
derivation, we treat LMs as random vectors from
now on. To minimize the residual, we solve the
following optimization problem
minL(W) = E∥e(x)∥, (2)
where E[·]is the expectation over x∈Ω, and∥ · ∥
is the Lnorm. L(W)is convex (not necessarily
strictly convex), so global optimum exists (not nec-
essarily unique). In the special case of d= 1, it
reduces to multivariate linear regression.
Closed-form Solution For simplicity, let
W= [W,W, . . . ,W]∈R,
z= [v,v, . . . ,v]∈R,we can rewrite Eq (2) as
L= E[∥u−Wz∥]
= E[(u−Wz)(u−Wz)]
= E[zWWz−2uWz+uu]
= E[tr( zzWW)]−2 E[tr( zuW)] +c
= tr(E[ zz]WW)−2 tr(E[ zu]W) +c,
where c= E[uu]is a constant, and we have used
cyclic property of trace, linearity of trace, and lin-
earity of expectation. Using the following matrix
calculus identity (Petersen et al., 2008),
∂tr(AXBXC)
∂X=CAXB +ACXB,
the gradient is
∂L
∂W= 2(WE[zz]−E[uz]). (3)
Setting gradient to zero, the solution is
W= E[uz](E[zz]), (4)
assuming E[zz]is full rank (in this case there is
an unique global optimal solution). E[zz]is the
covariance matrix (for simplicity we assume all
LMs are mean-subtracted), which is (symmetric)
positive semi-definite. Its eigenvalues are real and
non-negative. In practice, expectation is approxi-
mated with finite samples.
IfE[zz]is not full rank, Eq (2)is convex but
not strictly convex, and there are infinitely many
optimal solutions . The minimum-norm optimal
solution is
W= E[uz](E[zz]), (5)
where (E[zz])is the Moore–Penrose in-
verse (Moore, 1920) of E[zz]. Moore–Penrose
inverse exists even for non-invertible matrix or non-
square matrix. In the special case of a full rank
square matrix, Moore–Penrose inverse reduces the
“standard” matrix inverse.
Regularization A small regularization term
λ∥W∥can added to the loss function in Eq (2).
Mathematically, it ensures that Eq (2)is a strictly
convex hence the global optimum is unique regard-
less of the rank of E[zz]. Empirically, this in-
creases the numerical stability and accuracy when
computing matrix inverse. With regularization, the
unique global optimal solution is
W= E[uz](λI+ E[zz]), (6)
where λI+ E[zz]is positive definite and always
invertible.2509Bias Term In the above derivation, for simplicity
we assume all LMs are mean-subtracted. If we in-
clude a bias term in LMD equation (1), it becomes
u(x) =/summationdisplayWv(x) +b+e(x), (7)
where b∈Ris the bias term. In this case, the
solution is
W=cov(u,z)(cov(z,z)),
b= E[u]−WE[z],(8)
where cov(z,z) = E[ zz]−E[z] E[z]is the co-
variance matrix of z, and cov(u,z) = E[ uz]−
E[u] E[z]is the cross-covariance matrix of u,z.
2.3 Quantitative Measure of Dependency and
Correlation
Dependency Between Multiple Language Mod-
els To measure the goodness-of-fit for LMD, we
define R, analogous to the coefficient of determi-
nation used in linear regression (Draper and Smith,
1998). In particular,
R(u,{v}) = 1−SSR
SST, (9)
where SSR = E[∥e(x)∥]is the residual sum of
squares, and SST= E[∥u(x)−E[u(x)]∥]is the
total sum of squares. Here we view Ras a function
of the target LM u(x)and basis LMs {v(x)}.
Note that R∈[0,1].R= 1 implies that the
target model and the basis models are perfectly lin-
early dependent (by Corollary 2.1.1). A larger R
indicates that target model is well approximated
by basis models, and LMs are more linearly depen-
dent. Therefore, Ris a quantitative measure of the
target LM’s dependency on the basis LMs.
Correlation Between Multiple Language Models
The degree of correlation among a group of nLMs
{u(x)}is defined as
ρ({u}) =1
n/summationdisplayR(u,{u}), (10)
where {u}are the remaining n−1LMs after
removing the target LMu, which are used as the
basis LMs. Note that ρ({u})∈[0,1]. A value
of 1 implies that LMs are linear dependent (by
Corollary 2.1.1)) and furthermore each of them can
be represented by the rest.
Correlation Between Two Language Models
The measure of correlation between two LMs is of
particular interest, in this case it simplifies to
ρ(u,v) =1
2(R(u,v) +R(v,u)), (11)
where R(u,v)is the shorthand for R(u,{v})
when there is only one basis LM. Notice that
ρ(u,v)∈[0,1]. By definition, ρ(u,v)is sym-
metric, i.e, ρ(u,v) =ρ(v,u). However, R(u,v)
is asymmetric in general. ρ(u,u) = 1 , meaning
that a LM is perfectly “correlated” with itself.
In the special case of d= 1, n= 2, LMD reduces
to simple linear regression with a single feature.
Furthermore, both Eq (9)and Eq (11) reduce to the
“standard” coefficient of determination, which is
the square of the correlation coefficient.
3 Experiments and Results
3.1 Experiments
Language Models BERT (Devlin et al., 2018)
and many of its successors are pre-trained encoder
LMs, which take in text sequence and output se-
quence level embedding (defined as the mean pool-
ing of the last layer’s token embeddings). In this
work, we consider twelve (12) LMs, including
BERT (Devlin et al., 2018), distil-BERT (Sanh
et al., 2019), M-BERT (Devlin et al., 2018), distil-
M-BERT (Sanh et al., 2019), RoBERTa (Liu et al.,
2019), distil-RoBERTa (Sanh et al., 2019), XLM-
R (Conneau et al., 2019), XLNet (Yang et al., 2019),
ALBERT (Lan et al., 2019), ELECTRA (Clark
et al., 2020), DeBERTa (He et al., 2020), and Long-
former (Beltagy et al., 2020). The We apply LMD
to examine the linear dependency between these
LMs.2510Data We utilize the English Wikipedia (Devlin
et al., 2018)and BooksCorpus (Zhu et al., 2015),
which are used in pre-training BERT (Devlin et al.,
2018). The sequence length Tranges from 16 to
512 tokens, as determined by the BERT tokenizer.
When fed into other models, all sequences are trun-
cated at Ttokens as determined by their respec-
tive tokenizers. We randomly sample 512,000 and
51,200 sequences as the train data and test data
respectively. In our experiments, we find that fur-
ther increasing the size of the data does not affect
the results much. By central limit theorem, with
enough samples the estimation of expectations (see
Eq (4)) will become very accurate.
Implementation We download the pre-trained
checkpoints for the considered LMs from the pub-
lic huggingface model hub (Wolf et al., 2020), see
Table 1 for the full list of checkpoint names. We
ran experiments using PyTorch with a NVIDIA
V100 GPU. To improve numerical stability, a small
regularization term is added to ensure the mini-
mal eigenvalue of λI+ E[zz]is10. In our
experiments, the sequence embedding is the mean
pooling of the last layer’s token embeddings.
3.2 Results
The closed-form optimal solution (Eq (4)) depends
on expectations, which are approximated using
train data. The evaluation metrics R, ρare re-
ported on the test data. LMD Results on another
dataset are in Appendix B.
Pairwise Correlation of Language Models The
symmetric pairwise correlation ρ(u,u)between
LMs is visualized in Figure 1 (for T= 512 ), and
the asymmetric dependency measure R(u,u)
is shown in Figure 3 of Appendix A. First, no-
tice that distilled models (distil-M-BERT, distil-
RoBERTa, and distil-BERT) are highly “correlated”
with the orignal full models, because they distil
knowledge (Alkhulaifi et al., 2021) from the full
model. Second, multilingual LMs (XLM-R and
M-BERT) have lower correlation with other LMs.
Our hypthesis is that multilingual LMs are trained
to generalize to multiple languages, therefore they
contain knowledge beyond a single language.
Dependency and Correlation of Multiple Lan-
guage Models The dependency and correlation
of multiple LMs R(u,{u})is shown in Fig-
ure 2. For exact numbers, see Table 2 in Ap-
pendix A. First, note that Ris around 90% for all
models (for T= 512 ). This is consistent with the
fact that all models are variants of BERT. The group
correlation ρis91.25%, suggesting that these LMs
are highly linearly dependent, and there is some
redundancy in them. Second, Ris smaller for
shorter sequence length T. We suspect the reason
is that the sequence embedding is the mean pooling
of the last layer’s token embeddings, so by cen-
tral limit theory the variations in shorter sequence
embedding is larger. Therefore it is harder to ap-
proximate the target LM using others as basis LMs.
Third, similar to pairwise correlation, note that mul-
tilingual LMs have lower dependency compared
with others. Finally, for XLNet and ELECTRA,
Ris lower compared to other LMs (especially for
shorter sequences). Our hypothesis is that because
they have very different training objectives: Per-
mutation Language Modeling (Yang et al., 2019)
for XLNet and Replaced Token Detection (Clark
et al., 2020) for ELECTRA. Therefore they encode
text differently from the rest of the models which
mostly use Masked Language Modeling (Devlin
et al., 2018).
4 Conclusion and Future Work
In this work, we present a theoretical framework
to study the relationships between language mod-
els (LMs). We formalize the definitions of linear2511
combination and linear dependency of LMs. We
further propose the Language Model Decomposi-
tion (LMD) algorithm to represent one LM using
other LMs as basis. A LMD metric is developed to
quantify the linear dependency of LMs. Our exper-
iments show that BERT and its variants are 91%
“correlated". This suggests that there is redundancy
in SOTA pre-trained language models. Preliminary
results in this paper demonstrate the potential of
LMD as a framework to quantitatively analyze the
relationships among LMs. Finally, we leave some
open questions to motivate future research.
1.Can we leverage the linear dependency or in-
dependency of LMs to improve pre-training
and/or fine-tuning?
2.Is there any connection between Rand
LM performance in the pre-training and fine-
tuning stage? What does the “unexplained
variance” (i.e., 1−R) of each LM represent?
3.To further improve SOTA, how can we learn
complementary language representations that
are less linearly dependent on existing LMs?
4.Are LMs still highly linearly dependent after
fine-tuning on downstream tasks? How does
their linear dependency change during fine-
tuning?
5.Are multilingual LMs (e.g., M-BERT, XLM-
R) linearly dependent on monolingual LMs?
5 Limitations
In our preliminary experiments, we study encoder
LMs similar to BERT. It is worthwhile to inves-tigate other types of pre-trained LMs, including
encoder-decoder LMs (e.g, T5 (Raffel et al., 2020),
BART (Lewis et al., 2020)), decoder LMs (e.g,
GPT-2 (Radford et al., 2019)), pre-BERT models
(e.g, ELMo (Peters et al., 2018), ULMFit (Howard
and Ruder, 2018)), and even domain-specific LMs
(e.g, FinBERT (Araci, 2019)). The proposed “de-
pendency” and “correlation” measures quantify the
“similarity” between LMs, but there are still open
questions related to interpretation. Wikipedia and
BooksCorpus are used in our experiments, while
the results on a "neutral dataset" (such as a large
corpus that is not used in the pre-training of any
of the LMs) are also worth examination. We have
chosen the mean pooling of the last layers embed-
ding as the text sequence embedding because it is
commonly used for many downstream tasks. This
choice makes the LMD algorithm model agnos-
tic, meaning that it treats LM as a black box. It
only requires the final sequence embedding, but
not the intermediate representations. However, for
other task, the token level embedding is very im-
portant (e.g, determining the start and end location
of the answer for question-answering). Therefore,
layer-wise and token-level (including the [CLS]
and [MASK] token) “correlation” is of interest as
well, and further research is needed.
References25122513Acknowledgments
The author is grateful for the tremendous support
from his wife and Corgi.
A Details for Experiment Results
In this section, we show more details for experi-
ment results in Section 3.2.
Pairwise Dependency of Language Models Fig-
ure 3 shows the pairwise asymmetric dependency
R(u,u)between language models. Figure 1
shows the symmetric correlation ρ(u,u).
Dependency and Correlation of Multiple Lan-
guage Models Table 2 shows the exact numbers
for dependency R(u,{u})(defined in Eq (9))
between language models. The visualization is
shown in Figure 2.
B Additional Experiment Results
We also run LMD using the raw English
Wikicorpus dataset (Reese et al., 2010). This
corpus is not directly used in pre-training of afore-
mentioend LMs, though it is in the same domain as
the English Wikipedia.
The same group of language models are used (see
Section 3.1, Table 1), and the train/validation/testsample size is 128,000/12,800/12,800. The regu-
larization parameter is λ= 10. We fit the LMD
parameters using train set, validate on the valida-
tion set, and report dependency and correlation
measures on the test set.
Pairwise Dependency of Language Models The
pairwise dependency of language models are show
in Figure 4. Overall, the dependency R(u,u)(as
defined in Eq (9)) increases with sequence length.
This provides more evidence for our hypothesis
in Section 3.2. Because the sequence embedding
is the mean pooling of the last layer’s token em-
beddings, by central limit theory the variations in
shorter sequence embedding is larger. Therefore it
is harder to approximate the target LM using others
as basis LMs.
Dependency and Correlation of Multiple Lan-
guage Models The dependency R(u,{u})
(as defined in Eq (9)) and correlation ρ({u})
(as defined in Eq (10)) of multiple language models
are show in Figure 5, and the exact numbers are
in Table 3. The results are similar to the results on
English Wikipedia andBooksCorpus (see Fig-
ure 2, Table 2.).2514T=16 T=32 T=64 T=128 T=256 T=512
XLM-R 0.6415 0.7279 0.7866 0.8316 0.8430 0.8692
M-BERT 0.6861 0.7278 0.7541 0.7483 0.8139 0.8306
Longformer 0.8696 0.9329 0.9289 0.9015 0.9581 0.9481
DeBERTa 0.7492 0.7883 0.8503 0.8984 0.9105 0.9038
distil-M-BERT 0.7932 0.8190 0.8568 0.8830 0.8940 0.9035
RoBERTa 0.8322 0.9037 0.9512 0.9180 0.9566 0.9552
XLNet 0.3412 0.3925 0.6733 0.8341 0.8865 0.9154
BERT 0.7381 0.8293 0.8476 0.8803 0.8808 0.9124
ELECTRA 0.6190 0.7278 0.8029 0.8499 0.8852 0.9067
distil-RoBERTa 0.8722 0.8883 0.9221 0.9417 0.9469 0.9550
distil-BERT 0.8505 0.8827 0.9141 0.9323 0.9199 0.9472
ALBERT 0.7422 0.8093 0.8374 0.8696 0.9000 0.9027
Group Corr 0.7279 0.7858 0.8438 0.8741 0.8996 0.9125
T=16 T=32 T=64 T=128 T=256 T=512
XLM-R 0.6319 0.7179 0.7714 0.8215 0.8562 0.8808
M-BERT 0.6847 0.7234 0.7690 0.7753 0.8312 0.8741
Longformer 0.9240 0.9413 0.9542 0.9668 0.9753 0.9741
DeBERTa 0.7555 0.8260 0.8738 0.9022 0.9202 0.9311
distil-M-BERT 0.8048 0.8331 0.8659 0.8843 0.9035 0.9176
RoBERTa 0.9186 0.9401 0.9532 0.9667 0.9750 0.9761
XLNet 0.3255 0.3766 0.6868 0.8419 0.8959 0.9194
BERT 0.7829 0.8385 0.8784 0.9036 0.9189 0.9283
ELECTRA 0.6143 0.7173 0.8066 0.8618 0.8993 0.9228
distil-RoBERTa 0.8868 0.9077 0.9264 0.9418 0.9536 0.9612
distil-BERT 0.8593 0.8959 0.9230 0.9396 0.9477 0.9514
ALBERT 0.7240 0.7927 0.8435 0.8790 0.9044 0.9223
Group Corr 0.7427 0.7925 0.8543 0.8904 0.9151 0.9299251525162517