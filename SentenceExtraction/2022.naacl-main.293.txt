
Benjamin Minixhoferand Fabian Paischerand Navid RekabsazInstitute of Computational Perception, Johannes Kepler University LinzInstitute for Machine Learning, Johannes Kepler University LinzELLIS Unit Linz and LIT AI Lab
{benjamin.minixhofer, navid.rekabsaz}@jku.at
paischer@ml.jku.at
Abstract
Large pretrained language models (LMs) have
become the central building block of many
NLP applications. Training these models re-
quires ever more computational resources and
most of the existing models are trained on En-
glish text only. It is exceedingly expensive
to train these models in other languages. To
alleviate this problem, we introduce a novel
method – called WECHSEL – to efﬁciently
and effectively transfer pretrained LMs to new
languages. WECHSEL can be applied to any
model which uses subword-based tokenization
and learns an embedding for each subword.
The tokenizer of the source model (in English)
is replaced with a tokenizer in the target lan-
guage and token embeddings are initialized
such that they are semantically similar to the
English tokens by utilizing multilingual static
word embeddings covering English and the tar-
get language. We use WECHSEL to trans-
fer the English RoBERTa and GPT-2 models
to four languages (French, German, Chinese
and Swahili). We also study the beneﬁts of
our method on very low-resource languages.
WECHSEL improves over proposed methods
for cross-lingual parameter transfer and outper-
forms models of comparable size trained from
scratch with up to 64x less training effort. Our
method makes training large language models
for new languages more accessible and less
damaging to the environment. We make our
code and models publicly available.
1 Introduction
Large LMs based on the Transformer architec-
ture (Vaswani et al., 2017) have become increas-
ingly popular since GPT (Radford et al., 2018)
and BERT (Devlin et al., 2019) were introduced,
prompting the creation of many large LMs pre-
trained on English text (Yang et al., 2019; Clark
et al., 2020; Lewis et al., 2020; Ram et al., 2021).
There is a tendency towards training larger and
larger models (Brown et al., 2020; Fedus et al.,2021) while the main focus is on the English lan-
guage. Recent work has called attention to the costs
associated with training increasingly large LMs, in-
cluding environmental and ﬁnancial cost (Strubell
et al., 2019; Bender et al., 2021). If training large
LMs for English is already costly, it is prohibitively
expensive to train new, similarly powerful models
to cover other languages.
One approach to address this issue is creating
massively multilingual models (Devlin et al., 2019;
Conneau et al., 2020; Xue et al., 2021) trained on a
concatenation of texts in many different languages.
These models show strong natural language under-
standing capabilities in a wide variety of languages,
but suffer from what Conneau et al. (2020) call
thecurse of multilinguality : beyond a certain num-
ber of languages, overall performance decreases on
monolingual as well as cross-lingual tasks. Consis-
tent with this ﬁnding, Nozza et al. (2020) observe
that monolingual LMs often outperform massively
multilingual models. This might be attributed to
superior quality of monolingual tokenizers over
their multilingual counterparts (Rust et al., 2021).
It is thus desirable to train monolingual models
in more languages. Training monolingual models
in non-English languages is commonly done by
training a new model with randomly initialized pa-
rameters (Antoun et al., 2020; Louis, 2020; Martin
et al., 2020; Rekabsaz et al., 2019). However, to
train a model with capabilities comparable to that
of an English model in this way, presumably a sim-
ilar amount of compute to what was used to train
the English model would be required.
To address this issue, we introduce WECHSEL,
a novel method to transfer monolingual language
models to a new language. WECHSEL uses multi-
lingual static word embeddings between the source
language and the target language to initialize model
parameters. WECHSEL ﬁrst copies all inner (non-3992embedding) parameters of the English model, and
exchanges the tokenizer with a tokenizer for the tar-
get language. Next, in contrast to prior work doing
random initialization (de Vries and Nissim, 2021),
the token embeddings in the target language are
initialized such that they are close to semantically
similar English tokens by mapping multilingual
static word embeddings to subword embeddings.
The latter step is particularly important consider-
ing that token embeddings take up roughly 31%
of the parameters of RoBERTa (Liu et al., 2019)
and roughly 33% of the parameters of GPT2 (Rad-
ford et al., 2019). Intuitively, semantically transfer-
ring embeddings instead of randomly initializing
one third of the model should result in improved
performance. Our parameter transfer provides an
effective initialization in the target language, requir-
ing signiﬁcantly fewer training steps to reach high
performance than training from scratch. As mul-
tilingual static word embeddings are available for
many languages (Bojanowski et al., 2017), WECH-
SEL is widely applicable.
We conduct our experiments on RoBERTa and
GPT-2 as representative models of encoder and
decoder language models, respectively. We trans-
fer the English RoBERTa model to four languages
(French, German, Chinese and Swahili), and the
English GPT-2 model to the same four plus another
four very low-resource languages (Sundanese, Scot-
tish Gaelic, Uyghur and Malagasy). We evaluate
the transferred RoBERTa models on Named En-
tity Recognition (NER), and Natural Language In-
ference (NLI) tasks in the respective languages.
The transferred GPT-2 models are evaluated in
terms of Language Modelling Perplexity (PPL) on
a held-out set. We compare WECHSEL with ran-
domly initialized models (denoted as FullRand), as
well as the recently proposed TransInner method
which only transfers the inner (non-embedding)
parameters (de Vries and Nissim, 2021). All men-
tioned models are trained under the same condi-
tions (around 4 days on a TPUv3-8). We also
compare our model with models of comparable
size trained from scratch under signiﬁcantly larger
training regimes, in particular CamemBERT (Mar-
tin et al., 2020) (French), GBERT (Chan et al.,
2020) (German), and BERT-Chinese (Devlin
et al., 2019).
Results show that models initialized with
WECHSEL outperform randomly initialized mod-
els and models initialized with TransInner acrossall languages and all tasks, for both RoBERTa and
GPT-2. In addition, strong performance is reached
at a fraction of the training steps of other methods.
Our contribution is summarized as follows.
•We propose WECHSEL, a novel method for
transferring monolingual language models to
a new language by utilizing multilingual static
word embeddings between the source and the
target language.
•We show effective transfer of RoBERTa and
GPT-2 using WECHSEL to four and eight lan-
guages, respectively, achieved after minimal
training effort.
•We release more effective GPT-2 and
RoBERTa models than previously published
non-English models, achieved under our more
efﬁcient training setting. Our code and mod-
els are publicly available at github.com/
cpjku/wechsel .
In the following, we review related work in Sec-
tion 2. We introduce the WECHSEL method in
Section 3, followed by explaining the experiment
setup in Section 4. We show and discuss results in
Section 5.
2 Related Work
Large Language Models. Training Language
Models is usually done in a self-supervised manner
i. e. deriving labels from the training text instead
of needing explicit annotations. One optimization
objective is Masked Language Modelling (Devlin
et al., 2019, MLM), where randomly selected to-
kens in the input are replaced by a special [MASK]
token, and the task is to predict the original tokens.
Another common objective is Causal Language
Modelling (CLM), where the task is to predict the
next token. These two objectives highlight a funda-
mental distinction between language models: mod-
els can be trained as encoders (e.g. with MLM) or
as decoders (e.g. with CLM).
Instead of words, the vocabulary of recently pro-
posed language models commonly consists of sub-
words (Clark et al., 2020; Liu et al., 2019; Devlin
et al., 2019).
Multilingual representations. There has been a
signiﬁcant amount of work in creating multilin-
gual static word embeddings. A common method
is learning embeddings from scratch using data3993in multiple languages (Luong et al., 2015; Duong
et al., 2016). Alternatively, multilinguality can be
achieved by aligning existing monolingual word
embeddings using a bilingual dictionary, so that
the resulting embeddings share the same semantic
space (Xing et al., 2015; Joulin et al., 2018). Recent
studies improve on this by reducing (or completely
removing) the need for bilingual data (Artetxe et al.,
2017, 2018; Lample et al., 2018).
Beside static word embeddings, multilinguality
is also well studied in the area of contextualized
representations. One approach to learn multilingual
contextualized representations is through training
a model on a concatenation of corpora in differ-
ent languages. Some models created based on
this approach are mBERT (Devlin et al., 2019),
XLM-R (Conneau et al., 2020) and mT5 (Xue
et al., 2021), trained on text in 104, 100, and 101
languages, respectively. As shown by Pires et al.
(2019), a multilingual model such as mBERT can
enable cross-lingual transfer by using task-speciﬁc
annotations in one language to ﬁne-tune the model
for evaluation in another language. Despite the ben-
eﬁts, recent studies outline a number of limitations
of massively multilingual LMs. Wu and Dredze
(2020) empirically show that in mBERT “the 30%
languages with least pretraining resources perform
worse than using no pretrained language model at
all”. Conneau et al. (2020) report that beyond a
certain number of languages in the training data,
the overall performance decreases on monolingual
as well as cross-lingual tasks. These studies moti-
vate our work on introducing an efﬁcient approach
for creating effective monolingual LMs for more
languages.
Cross-lingual transfer of monolingual LMs.
Studies in this area can be divided into two cat-
egories:
•Bilingualization of a monolingual LM is
concerned with extending a model to a new
language while preserving its capabilities in
the original language. Artetxe et al. (2020)
approach this problem by replacing the to-
kenizer and relearning the subword embed-
dings, while freezing other (non-embedding)
parameters. Such a model becomes bilingual,
since the initial tokenizer and embeddings can
be used for tasks in the source language, while
the new tokenizer and embeddings can be used
for tasks in the target language. Thus, a model
can be ﬁnetuned on annotated task data inthe source language, and then zero-shot trans-
ferred to the target language. Tran (2020)
follow a similar approach, while instead of
randomly initializing embeddings, they utilize
static word embeddings to initialize embed-
dings in the target language close to semanti-
cally similar English tokens. They then con-
tinue training the model on an English text
corpus as well as on the target language in or-
der to preserve model capabilities in English.
•Creating a new monolingual LM in the tar-
get language is, in contrast, concerned with
transferring a model from a source to a tar-
get language without the necessity to preserve
its capabilities in the source language. Zoph
et al. (2016) and Nguyen and Chiang (2017)
show that cross-lingually transferring a ma-
chine translation model can improve perfor-
mance, especially for low-resource languages.
Zoph et al. (2016) use embeddings of random
tokens in the original vocabulary to initial-
ize token embeddings in the new vocabulary,
while Nguyen and Chiang (2017) utilize vo-
cabulary overlap between the source and tar-
get language. More recently, de Vries and Nis-
sim (2021) follow a similar approach to the
one of Artetxe et al. (2020) for transferring a
GPT-2 model to a new language. de Vries and
Nissim (2021) add an additional step, where
they train the entire model for some amount
of steps to allow adapting to the target lan-
guage beyond the lexical level. We refer to
the method of de Vries and Nissim (2021) as
TransInner and consider it as a baseline in our
experiments.
Our WECHSEL method belongs to the second
category. WECHSEL can be seen as an extension
to the method proposed by Tran (2020) with the
goal of creating a new monolingual LM instead
of bilingualizing the LM. This allows removing
the constraints imposed by the need to preserve
the model’s capabilities in the source language. In
addition, we generalize the semantic subword map-
ping done by Tran (2020) to consider an arbitrary
number of semantically similar subword with an
arbitrary temperature. We are the ﬁrst to show
that a cross-lingually transferred model can outper-
form monolingual models which have been trained
extensively from scratch in the target language,
while requiring substantially less computational
resources.39943 Methodology
To initialize the model in the target language, we
copy the inner (non-embedding) parameters from
the source model. Our goal, then, is given the tok-
enizerTin the source language with vocabulary
U, the corresponding token embeddings E, and a
tokenizerTin the target language with vocabulary
U, to ﬁnd a good initialization of the embeddings
Eby using E. To this end, we use existing bilin-
gual word embeddings enriched with subword in-
formation, containing a set of words and subword
n-grams in the source and target language and their
aligned vectors. We denote the set of words and
n-grams in the source and target language as V
andVrespectively, and the aligned static embed-
dings as WandW. In Appendix D we consider
an alternative method if no subword information is
available in the bilingual word embeddings.
First, independently for both languages, we com-
pute static subword embeddings for tokens in the
tokenizer vocabulary in the same semantic space
as the static word embeddings (Section 3.1). This
results in subword embeddings UandUfor the
source and target language, respectively. Next, we
useUandUto compute the semantic similar-
ity of every subword in Uto every subword in
U. Using these semantic similarities, we initial-
ize the embeddings in Ethrough a convex com-
bination of embeddings in E(Section 3.2). By
applying WECHSEL, the vectors of Eare in the
same semantic space as E, where a subword in
the target language is semantically similar to its
counterpart(s) in the source language. These steps
are summarized in Figure 1 and explained in more
detail in the following.
3.1 Subword Embedding Computation
The process of mapping word embeddings to sub-
word embeddings is done individually for the
source and the target language. Given a tokenizer
Twith vocabulary Uand embeddings W, the goal
is to ﬁnd subword embeddings Ufor subwords in
Uin the same semantic space as W. To this end,
we decompose subwords in Uinto n-grams and
compute the embedding by taking the sum of the
embeddings of all occuring n-grams, equivalent to
how embeddings for out-of-vocabulary words are
computed in fastText (Bojanowski et al., 2017).
u=/summationdisplayw
where Gis the set of n-grams occuring in the sub-
wordxandwis the embedding of the n-gram g.
Subwords in which no known n-gram occurs are
initialized to zero.
3.2 Subword similarity-based Transfer
Applying the previous step to both source and tar-
get language results in the subword embeddings
UandUover the subword vocabularies Uand
U, respectively. Our aim is to leverage these em-
beddings to ﬁnd an effective transformation from
EtoE. We ﬁrst compute the cosine similarity
of every subword x∈Uto every subword y∈U,
denoted ass.
s=uu
/bardblu/bardbl/bardblu/bardbl
We now exploit these similarities to initialize
embeddings in Eby a convex combination of
embeddings in E. In particular, each subword
embedding in Eis deﬁned as the weighted mean
of theknearest embeddings in Eaccording to
the similarity values. The weighting is done by a
softmax of the similarities with temperature τ.
e=/summationtextexp (s/τ)·e/summationtextexp (s/τ)
whereJis the set of kneighbouring subwords
in the source language. Subword embeddings for
which Uis zero are initialized from a random
normal distribution N(E[E],Var[E]).39954 Experiment Design
We evaluate our method by transferring the En-
glish RoBERTa (Liu et al., 2019) and the English
GPT-2 model (Radford et al., 2019) to French, Ger-
man, Chinese and Swahili. We refer to these lan-
guages as medium-resource languages . In addition,
we study the beneﬁts of our method on four low-
resource languages , namely Sundanese, Scottish
Gaelic, Uyghur and Malagasy.
We evaluate WECHSEL-RoBERTa by ﬁne-
tuning on XNLI (Conneau et al., 2018), and on the
balanced train-dev-test split of WikiANN (Rahimi
et al., 2019; Pan et al., 2017) to evaluate NLI and
NER performance, respectively. The hyperparame-
ters used for ﬁne-tuning are reported in Appendix B.
GPT-2 is evaluated by Perplexity (PPL) on a held-
out set from the same corpus on which the model
was trained on. Due to the difﬁculty of extrin-
sic evaluation on low-resource languages, we only
train GPT-2 models in these languages, and eval-
uate their performance intrinsically via Language
Modelling Perplexity on a held-out set. We use the
pretrained models RoBERTa with 125M pa-
rameters, and the small GPT-2 variant with 117M
parameters provided by HuggingFace’s Transform-
ers (Wolf et al., 2020) in all experiments.
Since under limited training regimes such as
ours, using a smaller corpus does not in general
degrade performance (Martin et al., 2020), we
use a subset of 4GiB from the OSCAR corpus
for German, French and Chinese. For the other
languages, we use data from the CC-100 corpus
(Conneau et al., 2020) which contains 1.6GiB,
0.1GiB, 0.1GiB, 0.4GiB and 0.2GiB for Swahili,
Sundanese, Scottish Gaelic, Uyghur and Malagasy,
respectively. To obtain aligned word embeddings
between the source and the target language we
use monolingual fastText word embeddings(Bo-
janowski et al., 2017). We align these embeddings
using the Orthogonal Procrustes method (Schöne-
mann, 1966; Artetxe et al., 2016) with bilingual
dictionaries from MUSE(Conneau et al., 2017)
for French, German and Chinese and a bilingual
dictionary from FreeDict(Ba´nski and Wójtowicz,
2009) for Swahili. For the low-resource languages,
we use bilingual dictionaries scraped from Wik-
tionary.
We choose temperature τ= 0.1and neighbors
k= 10 for WECHSEL by conducting a parameter
search over a grid with varying values for kand
τusing linear probes (Appendix A). We train tok-
enizers in the target languages using a vocabulary
size of 50k tokens and byte-level BPE (Radford
et al., 2019). After applying WECHSEL, we con-
tinue training RoBERTa on the MLM objective and
GPT-2 on the CLM objective. We compare against
two baseline methods.
•TransInner: Randomly initializing Ewhile
transferring all other parameters from the En-
glish model as in de Vries and Nissim (2021).
After training only embeddings for a ﬁxed
amount of steps while freezing other parame-
ters, the entire model is trained for the remain-
ing steps. In preliminary experiments reported
in Appendix E, we compare the method by
Zoph et al. (2016) with TransInner, observing
superior performance of TransInner, so we
choose TransInner as the baseline for cross-
lingual transfer in all our experiments.
•FullRand: Training from scratch in the target
language, as is commonly done when train-
ing BERT-like or GPT-like models in a new
language (Antoun et al., 2020; Louis, 2020;
Chan et al., 2020; Martin et al., 2020).
All models are trained for 250k steps with the
same hyperparameters across all languages (re-
ported in Appendix B). Training one model takes
around 4 days on a TPUv3-8. For WECHSEL and
FullRand we use a learning rate (LR) schedule with
linear warmup from zero to the peak LR for the ﬁrst
10% of steps, followed by a linear decay to zero.
For TransInner, we perform two warmup phases
from zero to peak LR, once for the ﬁrst 10% of
steps for training embeddings only, then again for
the remaining steps while training the entire model.
In addition to the mentioned baselines trained
under this setting, we compare the results of3996
RoBERTa models with existing comparable mod-
els trained from scratch with more training ef-
fort. We consider the total number of tokens the
model has encountered in the target language, com-
puted as the product of batch size ×sequence
length×train steps (shown in Table 1) as a proxy
for training effort. We evaluate the performance
of CamemBERT (Martin et al., 2020) (French),
GBERT (Chan et al., 2020) (German), and
BERT-Chinese (Devlin et al., 2019) as existing
monolingual LMs,as well as XLM-R(Artetxe
et al., 2020) as a high-performing multilingual LM.
5 Results
We present our results on transferring RoBERTa
and GPT-2 from English to other languages, fol-
lowed by analyzing training behavior. In Ap-
pendix C, we provide a qualitative assessment of
how well subword tokens are mapped between the
source and the target languages.
5.1 Transferring RoBERTa
Table 2 reports the evaluation results of RoBERTa.
As shown, models initialized with WECHSEL out-
perform models trained from scratch and models
initialized with TransInner across all languages.Surprisingly, close relatedness of the source and
target language is not necessary to achieve effective
transfer, as e. g. on NLI WECHSEL improves abso-
lute accuracy by 7.15%,6.31%,6.94% and4.71%
over models trained from scratch for French, Ger-
man, Chinese and Swahili, respectively.
We observe that our parameter transfer-based
model consistently outperforms the previously re-
leased LMs on both monolingual and multilingual
settings, while these models beneﬁt from much
larger training resources in terms of computation
time and corpus size. In particular, the results
show an improvement over XLM-Rby an av-
erage 3.54% accuracy for NLI and 1.14% micro
F1 score for NER. For NLI, we improve over the
prior monolingual models by 1.55%,3.15% and
1.77% absolute accuracy for French, German and
Chinese, respectively. For NER, we observe im-
provements over monolingual models with 0.62%
and0.26% absolute micro F1 score improvement
for French and German, respectively. For Chinese,
the monolingual model BERT-Chinese still out-
performs our method by 1.5%absolute micro F1
score. We suspect that the discrepancy between
NLI and NER is due to the limited training cor-
pus size (max. 4GiB), while a larger corpus can
potentially improve NER as more named entities
appear (Martin et al., 2020).3997
The ﬁrst two columns of Figure 2 show the
performance of RoBERTa models on downstream
tasks after each 12.5k training steps. Models ini-
tialized with WECHSEL reach high performance
in signiﬁcantly fewer steps than models initialized
with FullRand or TransInner.
We expect FullRand-RoBERTa to approach per-
formance of the respective prior monolingual mod-
els when trained on the same amount of tokens.3998For French, WECHSEL-RoBERTa outperforms
CamemBERT after 10% of training steps, reducing
training effort by 64x. For German, WECHSEL-
RoBERTa outperforms GBERT after 10% of
training steps, reducing training effort by 39x.
For Chinese, WECHSEL-RoBERTa outperforms
BERT-Chinese on NLI, but does not outper-
form BERT-Chinese on NER.
5.2 Transferring GPT-2
5.2.1 To Medium-Resource Languages
Results on medium-resource languages are shown
in Table 3. Similar to the results for WECHSEL-
RoBERTa, the GPT-2 models trained with WECH-
SEL consistently outperform the models trained
from scratch and with TransInner across all lan-
guages.
The rightmost column of Figure 2 depicts the
performance of GPT-2 models after each 12.5k
training steps. Comparing the results across all lan-
guages throughout training, we observe a stronger
dependence on similarity of the source to the tar-
get language than for downstream tasks such as
NLI or NER. In particular, for French and German,
WECHSEL is consistently better than TransInner
and FullRand throughout the entire training, while
for Chinese, a decrease in perplexity towards the
end of training causes WECHSEL to surpass train-
ing from scratch.
5.2.2 To Low-Resource Languages
Table 4 reports the perplexity of Language Mod-
elling on the low-resource languages. Again, we
observe consistent improvements using WECHSEL
on all languages. Furthermore, we ﬁnd that the
improvement from WECHSEL tends to increase
as the amount of training data decreases by con-
ducting a sensitivity analysis w. r. t. the amount of
available training data (Appendix F).
In Figure 3 we report the performance of the
low-resource LMs on the held-out set throughout
training. One difference of the low-resource mod-
els with the ones trained on medium-resource lan-
guages is that the low-resource LMs are prone to
overﬁtting, and require appropriate model selec-
tion even in the early steps of training. Notably,
TransInner-GPT2 takes more steps to overﬁt since
all non-embedding parameters are frozen for the
ﬁrst 25k steps (c. f. Section 4).
5.3 Is freezing necessary?
Previous work using the TransInner method freezes
non-embedding parameters for a ﬁxed amount of
steps before training the entire model (de Vries
and Nissim, 2021). This is done to prevent catas-
trophic forgetting at the beginning of training. To
evaluate if freezing non-embedding parameters is
still necessary with our method, we conduct an
additional experiment. We train a German GPT-2
model with WECHSEL and a model with TransIn-
ner without freezing any parameters, and the same
models with freezing of non-embedding parameters
for the ﬁrst 10% of steps. We match hyperparame-
ters of the main experiments except training for 75k
steps only. Based on the results shown in Figure 4,
we conclude that freezing is necessary when using
TransInner, but there is no need for freezing when
using WECHSEL.3999
6 Limitations and Potential Risks
6.1 Limitations
We conduct our experiments on up to eight lan-
guages, showing the beneﬁts of our parameter trans-
fer method to both medium- and low-resource lan-
guages. However, there are many more languages
with diverse linguistic characteristics on which our
WECHSEL method is not tested. This is a limi-
tation forced by computational constraints, as we
can not ascertain whether transfer to all other lan-
guages would result in similar improvements. In
addition, our extrinsic evaluation is limited to two
tasks (NLI and NER). While this choice is due
to the limitations on the available collections in
various languages, this evaluation does not neces-
sarily provide a comprehensive view of language
understanding tasks.
6.2 Risks
It is well-known that existing LMs trained on En-
glish text encode societal biases (Bolukbasi et al.,
2016; Caliskan et al., 2017; Rekabsaz et al., 2021b)
and stereotypes and using them in downstream
tasks might lead to unfair treatment of various so-
cial groups (Zerveas et al., 2022; Krieg et al., 2022;
Ganhör et al., 2022; Rekabsaz et al., 2021a; Mel-
chiorre et al., 2021; Rekabsaz and Schedl, 2020;
Elazar and Goldberg, 2018). Since we propose
a method to transfer the English LMs to new lan-
guages, it is highly probable that the existing biases
are also transferred to the target LMs. We therefore
advocate a conscious and responsible use of the
transferred LMs in practice.7 Conclusion
We introduce WECHSEL, an effective method to
transfer monolingual language models to new lan-
guages. WECHSEL exploits multilingual static
word embeddings to compute an effective initializa-
tion of subword embeddings in the target language.
We conduct experiments by transferring RoBERTa
and GPT-2 models from English to French, Ger-
man, Chinese and Swahili, as well as English GPT-
2 to four low-resource languages. The evaluation
results show that the transferred RoBERTa and
GPT-2 models are more efﬁcient and effective than
strong baselines, and consistently outperform prior
monolingual models that have been trained for a
signiﬁcantly longer time. WECHSEL facilitates
the creation of effective monolingual LMs for new
languages with medium to low resources, particu-
larly in computationally-limited settings. In addi-
tion, our work provides strong evidence towards
the hypothesis by Artetxe et al. (2020) that deep
monolingual language models learn abstractions
that generalize across languages.
8 Acknowledgments
Research supported with Cloud TPUs from
Google’s TPU Research Cloud (TRC). We thank
Andy Koh and Artus Krohn-Grimberghe for provid-
ing additional computational resources. The ELLIS
Unit Linz, the LIT AI Lab, the Institute for Ma-
chine Learning, are supported by the Federal State
Upper Austria. We thank the project INCONTROL-
RL (FFG-881064). Research also supported in part
by the NSF (IIS-1956221), the State of Upper Aus-
tria and the Austria’s Federal Ministry of Educa-
tion, Science, and Research through the project
FAIRFLOW (LIT-2021-YOU-215).
References400040014002
A Grid search over kandτ
To choose number of neighbors kand temperature
τfor WECHSEL we conduct a grid search over
linear probes of models with different initializa-
tion shown in Table 7. For RoBERTa, we compute
scores on NLI (using XNLI) and POS tagging (us-
ing the French, German and Chinese GSD corpora
in Universal Dependencies) using linear probes of
the last hidden state. We probe on NLI by taking
a concatenation of the mean of all token represen-
tations in the premise with the mean of all token
representations in the hypothesis. We probe on
POS tagging by taking the mean of all token rep-
resentations belonging to each word. For GPT2,
we compute Language Modelling Perplexity on the
held-out set also used to evaluate performance of
the trained models.
B Hyperparameters
Hyperparameters used to ﬁne-tune RoBERTa on
downstream tasks are shown in Table 5. Hyperpa-
rameters used to train models in our main experi-
ments are shown in Table 6.4003Parameter NLI NER
peak learning rate 2e-5 2e-5
batch size 128 32
sequence length 128 128
Adam/epsilon1 1e-8 1e-8
Adamβ 0.9 0.9
Adamβ 0.999 0.999
train epochs 2 10
warmup 10% of steps 10% of steps
warmup schedule linear linear
LR decay linear to zero linear to zero
Parameter RoBERTa GPT2
peak learning rate 1e-4 5e-4
batch size 512 512
sequence length 512 512
weight decay 0.01 0.01
Adam/epsilon1 1e-6 1e-6
Adamβ 0.9 0.9
Adamβ 0.98 0.98
train steps 250k 250k
C Qualitative subword correspondence
We show a small random sample of tokens in the
target language and their closest English token (ac-
cording to WECHSEL) in Table 8.
D Using Word Embeddings without
subword information
As an alternative to n-gram decomposition, we in-
troduce a method for mapping word embeddings to
subword embeddings without using any subword
information (shown in Figure 5). For this method,
we require word frequency information in addition
to the word embeddings. We apply the tokenizer T
to every word vinVresulting in a set of subwords
for each word. We deﬁne Vas the set of words
containing the subword xwhen tokenized. The
embedding uof the subword xis then deﬁned as
the average of the embeddings of words in V,
weighted by the word frequencies.
u=/summationtextw·f/summationtextf
where wis the embedding and fis the frequency
of wordv.
We call this variant of our method WECHSEL.
We evaluate WECHSELby training the same
models as for WECHSEL. Results are shown in
Table 9 for GPT2 and in Table 10 for RoBERTa.
We ﬁnd that, on average, performance is on par
with WECHSEL.
E Choosing a transfer baseline
We consider two baseline methods to transfer mod-
els to a new language without using any language-
speciﬁc information. One method copies non-
embedding parameters to the target language and
initalizes embeddings from a random normal distri-
bution as done by de Vries and Nissim (2021). We
refer to this method as TransInner. Another option
copies non-embedding parameters and assigns the
embedding of a random token in the source lan-
guage to each embedding in the target language
(effectively "shufﬂing" the embeddings) as done by
Zoph et al. (2016) and Nguyen and Chiang (2017).
We refer to this method as TransInnerShufﬂeEmb.
We evaluate these two methods using a setup equiv-
alent to the experiments in Section 5.3 and ﬁnd that
TransInner performs slightly better than TransIn-
nerShufﬂeEmb (Figure 6), so we use TransInner
for subsequent experiments.4004
F Sensitivity Analysis w. r. t. training
data size
Evaluating on languages with different amounts of
available data only indirectly measures the effect
of training data size on WECHSEL since other fac-
tors (e.g. language similarity to English) are also
involved. We conduct a sensitivity analysis to make
the relation to the amount of training data explicit
(Table 11). Due to computational constraints we
only do this for French. We ﬁnd that the improve-
ment from WECHSEL increases as the amount of
training data decreases. In addition, we ﬁnd that
using fastText embeddings trained on less data dete-
riorates performance, but still leaves a clear margin
to TransInner and FullRand.4005
Lang ModelScore@0 Score@25k Score@250k
NLI NER Avg NLI NER Avg NLI NER Avg
FrenchWECHSEL-RoBERTa 78.25 86.93 82.59 81.63 90.26 85.95 82.43 90.88 86.65
WECHSEL-RoBERTa 78.25 87.43 82.84 81.86 90.07 85.96 82.55 90.80 86.68
GermanWECHSEL-RoBERTa 75.64 84.53 80.08 81.11 89.05 85.08 81.79 89.72 85.76
WECHSEL-RoBERTa 77.00 84.70 80.85 80.71 89.09 84.90 82.04 89.72 85.88
ChineseWECHSEL-RoBERTa 63.23 72.79 68.01 77.19 79.07 78.13 78.32 80.55 79.44
WECHSEL-RoBERTa 62.75 72.87 67.81 77.07 78.03 77.55 77.99 80.65 79.32
SwahiliWECHSEL-RoBERTa 60.28 74.38 67.33 73.87 87.63 80.75 75.05 87.39 81.22
WECHSEL-RoBERTa 60.14 75.42 67.78 74.04 87.79 80.92 74.58 87.66 81.12
Best PPL
Model Subsample Size 16MiB 64MiB 256MiB 1024MiB
WECHSEL-GPT2 (original fastText embeddings) 78.33 44.75 31.63 24.66
WECHSEL-GPT2 (fastText embeddings trained on subsample) 97.42 49.50 32.88 24.75
FullRand-GPT2 281.46 83.43 43.08 27.09
TransInner-GPT2 216.37 77.71 35.27 25.154006