
Xuwu Wang, Junfeng Tian, Min Gui, Zhixu Li, Rui Wang,
Ming Yan, Lihan Chen, Yanghua XiaoSchool of Computer Science, Fudan University, ChinaAlibaba Group, ChinaShopee, SingaporeVipshop (China) Co., Ltd., ChinaFudan-Aishu Cognitive Intelligence Joint Research Center, China
{xwwang18,zhixuli,shawyh}@fudan.edu.cn,
{tjf141457, ym119608}@alibaba-inc.com,
min.gui@shopee.com, mars198356@hotmail.com, lhc825@gmail.com
Abstract
Multimodal Entity Linking (MEL) which aims
at linking mentions with multimodal con-
texts to the referent entities from a knowl-
edge base (e.g., Wikipedia), is an essen-
tial task for many multimodal applications.
Although much attention has been paid to
MEL, the shortcomings of existing MEL
datasets including limited contextual topics
and entity types, simpliÔ¨Åed mention ambigu-
ity, and restricted availability, have caused
great obstacles to the research and appli-
cation of MEL. In this paper, we present
WDiverse, a high-quality human-annotated
MEL dataset with diversiÔ¨Åed contextual topics
and entity types from Wikinews, which uses
Wikipedia as the corresponding knowledge
base. A well-tailored annotation procedure is
adopted to ensure the quality of the dataset.
Based on WDiverse, a sequence of well-
designed MEL models with intra-modality
and inter-modality attentions are implemented,
which utilize the visual information of im-
ages more adequately than existing MEL mod-
els do. Extensive experimental analyses are
conducted to investigate the contributions of
different modalities in terms of MEL, facili-
tating the future research on this task. The
dataset and baseline models are available at
https://github.com/wangxw5/wikiDiverse.
1 Introduction
Entity linking (EL) has attracted increasing atten-
tion in the natural language processing community,
which aims at linking ambiguous mentions to the
referent unambiguous entities in a given knowledge
base (KB) (Shen et al., 2014). It has been applied
to a lot of downstream tasks such as information
extraction (Yaghoobzadeh et al., 2016), questionFigure 1: Several MEL examples with mentions high-
lighted in the caption and the Ô¨Årst entity of each entity
listed as the gold label.
answering (Yih et al., 2015) and semantic search
(Blanco et al., 2015).
As named entities (i.e., mentions) with multi-
modal contexts such as texts and images are ubiq-
uitous in daily life, recent studies (Moon et al.,
2018; Adjali et al., 2020a) turn their focus towards
improving the performance of EL models through
utilizing visual information, i.e., Multimodal En-
tity linking (MEL). Several MEL examples are
depicted in Figure 1, where the images could effec-
tively help the disambiguation for entity mentions
of different types. Due to its importance to many
multimodal understanding tasks including VQA,
multimodal retrieval, and the construction of multi-
modal KBs, much effort has been dedicated to the
research of MEL. Moon et al. (2018) Ô¨Årst addressed
the MEL task under the zero-shot setting. Adjali
et al. (2020a) designed a model to combine the vi-4785
sual, textual and statistical information for MEL.
Zhang et al. (2021) designed a two-stage mecha-
nism that Ô¨Årst determines the relations between im-
ages and texts to remove negative impacts of noisy
images and then performs the disambiguation. Gan
et al. (2021) disambiguated visual mentions and
textual mentions respectively at Ô¨Årst, and then used
graph matching to explore possible relations among
inter-modal mentions.
Although much attention has been paid to MEL,
the existing MEL datasets as listed in the middle
rows of Table 1 have deÔ¨Åciencies in the following
aspects, which hinder the further advancement of
research and application for MEL.
Limited Contextual Topics. As shown in
Figure 2(a), the existing MEL datasets are
mainly collected from social media or movie
reviews, where there are only 5 topics in the
social media domain and 1 topic in the movie
review domain. But as we observed in the
news domain, there are more than 10 topics
including other popular topics like disasterand education. The lack of topics would limit
the generalization ability of the MEL model.
Limited Entity Types. Entities in the exist-
ing MEL datasets mainly belong to the types
of ‚Äúperson (PER)‚Äù and ‚Äúorganization (ORG)‚Äù.
This restricts the application of the MEL mod-
els over other entity types such as locations,
events, etc., which are also ubiquitous in com-
mon application scenarios.
SimpliÔ¨Åed Mention Ambiguity : Some
datasets such as Twitter (Adjali et al., 2020a)
create artiÔ¨Åcial ambiguous mentions by re-
placing the original entity names with the
surnames of persons or acronyms of organi-
zations. Besides, limited entity types also
lead to the limited mention ambiguity that
only occurs with PER and ORG. According
to our statistics of different domains as de-
picted in Figure 2(b), there are overall ten
kinds of mention ambiguities in news domain4786such as Wikinews, while existing datasets
collected from social media or movie reviews
only cover a small scope of ambiguity.
Restricted Availability. Most of the existing
MEL datasets are not publicly available.
To enable more detailed research of MEL, we
propose a manually-annotated MEL dataset named
WDiverse with multiple topics and multiple
entity types. It consists of 8K image-caption
pairs collected from WikiNews and is based on
the KB of Wikipedia with ~16M entities in to-
tal. Both the mentions and entities are charac-
terized by multimodal contexts. We design a
well-tailored annotation procedure to ensure the
quality of WDiverse and analyze the dataset
from multiple perspectives (Section 4). Based on
WDiverse, we propose a sequence of MEL mod-
els with intra-modality and inter-modality atten-
tions, which utilize the visual information of im-
ages more adequately than the existing MEL mod-
els (Section 5). Furthermore, extensive empirical
experiments are conducted to analyze the contri-
butions of different modalities for the MEL task
and visual clues provided by the visual contexts
(Section 6). In summary, the contributions of our
work are as follows:
We present a new manually annotated high-
quality MEL dataset that covers diversiÔ¨Åed
topics and entity types.
Multiple well-designed MEL models with
intra-modal attention and inter-modal atten-
tion are given which could utilize the visual
information of images more adequately than
the previous MEL models.
Extensive empirical results quantitatively
show the role of textual and visual modali-
ties for MEL, and detailed analyses point out
promising directions for the future research.
2 Related Work
Textual EL There is vast prior research on tex-
tual entity linking. Multiple datasets have been
proposed over the years including the manually-
annotated high-quality datasets like AIDA (Hoffart
et al., 2011), automatically-annotated large-scale
datasets like CWEB (Guo and Barbosa, 2018) and
zero-shot datasets like Zeshel (Logeswaran et al.,2019). To evaluate the EL models‚Äô performance, it
is usual to train on the AIDA-train dataset, and test
on the datasets of AIDA-test, MSNBC(Cucerzan,
2007), AQUAINT(Milne and Witten, 2008), etc.
However, as mentioned in (Cao et al., 2021), many
methods have achieved high and similar results
within recent three years. One possible explanation
is that it may simply be near the ceiling of what
can be achieved for these datasets, and it is difÔ¨Åcult
to conduct further research based on them.
Multimodal EL In recent years, the growing
trend towards multimodality requires to extend the
research of EL from monomodality to multimodal-
ity. Moon et al. (2018) Ô¨Årst address the MEL task
and build a zero-shot framework, which extracts
textual, visual and lexical information for EL in
social media posts. However, its proposed dataset
is unavailable due to GDPR rules. Adjali et al.
(2020a,b) propose a framework of automatically
building the MEL dataset from Twitter. The dataset
has limited entity types and ambiguity of mentions,
thus it is not challenging enough. Zhang et al.
(2021) study on a Chinese MEL dataset collected
from the Chinese social media platform Weibo,
which mainly focuses on the person entities. Gan
et al. (2021) release a MEL dataset collected from
movie reviews and propose to disambiguate both
visual and textual mentions. This dataset mainly
focuses on characters and persons of the movie do-
main. Peng (2021) propose three MEL datasets,
which are built from Weibo, Wikipedia, and Rich-
pedia information and use CNDBpedia, Wikidata
and Richpedia as the corresponding KBs. However,
using Wikipedia as the target dataset may lead to
the data leakage problem as many language models
are pretrained on it.
Our MEL dataset is also related to other named
entity-related multimodal datasets, including entity-
aware image caption datasets (Biten et al., 2019;
Tran et al., 2020; Liu et al., 2021), multimodal
NER datasets (Zhang et al., 2018; Lu et al., 2018),
etc. However, the entities in these datasets are not
linked to a uniÔ¨Åed KB. So our research of MEL
can enhance the understanding of named entities,
thereby enhancing the research in these areas.
3 Problem Formulation
Multimodal entity linking is deÔ¨Åned as mapping a
mention with multimodal contexts to its referent
entity in a pre-deÔ¨Åned multimodal KB. Since the
boundary and granularity of mentions may be con-4787troversial, the mention span is usually pre-speciÔ¨Åed.
Here we assume each mention has a correspond-
ing entity in the KB, which is the in-KB evaluation
problem.
Formally, let Erepresent the entity set of the KB,
which usually contains millions of entities. Each
mention mor entity e2Eis characterized by the
corresponding visual context V; Vand textual
context T; T. Here TandTrepresent the
textual spans around manderespectively. Vis
the image correlated with mandVis the image
ofein the KB. In real life, entities in KBs may
contain more than one image. To simplify it, we
select the Ô¨Årst image of easVand leave MEL
with multiple images per entity as the future work.
So the referent entity of mention mis predicted
through:
e(m) = arg max	 (m(T; V) ;e(T; V)):
where 	()represents the similarity score between
the mention and entity.
4 Dataset Construction
In this section, we present the dataset construction
procedure. Many factors including annotation qual-
ity, coverage of topics, diversity of entity types,
coverage of ambiguity are taken into consideration
to ensure the research value of WDiverse.
4.1 Data Collection
Data Source Selection 1) For the source of
image-text pairs, considering news articles are
widely-studied in traditional EL (Hoffart et al.,
2011; Cucerzan, 2007) and usually cover a wide
range of topics and entity types, we decide to use
news articles. Wikinews and BBC are two popular
sources of news articles. So we compared them
from two aspects. As shown in Table 2, Wikinews
has advantages in terms of alignment degree be-
tween image-text pairs and MEL difÔ¨Åculty. So we
select the image-caption pairs of Wikinews to build
the corpus. 2) For the source of KB, we use the
commonly-used Wikipedia (Hoffart et al., 2011;
Ratinov et al., 2011; Guo and Barbosa, 2018). We
also provide the annotation of the corresponding
Wikidata entity for Ô¨Çexible studies.
Data Acquisition 1) For the image-caption pairs,
we collect all the English news from the year 2007
to 2020 from Wikinews with multiple topics includ-
ing sports, politics, entertainment, disaster, tech-
nology, crime, economy, education, health and
weather. The data cover most of the common topics
in the real world. Finally, we obtain a raw corpus
with 14k image-caption pairs. 2) For the KB, we
use the Wikipedia. The entity set consists of all
the entities in the main namespace with the size of
~16M.
Data Cleaning For the image-caption pairs, we
remove the cases that 1) contain pornographic, pro-
fane, and violent content; 2) the text is shorter than
3 words. Finally, we get a corpus with 8K image-
caption pairs.
4.2 Annotation
Annotation Design The primary goal of
WDiverse is to link mentions with multimodal
contexts to the corresponding Wikipedia entity.
Therefore, given an image-text pair, annotators
need to 1) detect mentions from the text (Mention
Detection, MD) and 2) label each detected mention
with the corresponding entity in the form of
a Wikipedia URL (Entity Linking, EL). For
mentions that do not have corresponding entities
in Wikipedia, they are labeled with ‚ÄúNIL‚Äù. Seven
common entity types (i.e., Person, Organization,
Location, Country, Event, Works, Misc) are
required to be annotated. To avoid subjective
errors, we design detailed annotation guidelines
with multiple samples to avoid the controversy
of mention boundary, mention granularity, entity
URL, etc. Details can be found in the Appendix.
We also hold regular communications to discuss
some emerging annotations problems.
Annotation Procedure The annotators include
13 annotators and 2 experienced experts. All anno-
tators have linguistic knowledge and are instructed
with detailed annotation principles. Each image-
caption pair is independently annotated by two an-
notators. Then an experienced expert goes over4788
Train Dev. Test Total
# pairs 6377 796 796 7969
# ment. per pair 2.04 2.03 1.87 2.02
# words per pair 10.07 10.28 9.92 10.08
the controversial annotations, and makes the Ô¨Ånal
decision. Following Ding et al. (2021), we calcu-
late the Cohen‚Äôs Kappa to measure the agreements
between two annotators. The Kappa of MD and EL
are 88.98% and 83.75% respectively, indicating a
high degree of consistency.
4.3 Analysis of WDiverse
Size and Distribution of WDiverse We di-
vide WDiverse into training set, validation set,
and test set with the ratio of 8:1:1. The statistics
ofWDiverse are shown in Table 3. The col-
lected Wikipedia KB has ~16M entities in total
(i.e.jEj 16M). Besides, we report the entity
type distribution in Figure 4(a) and report the topic
distribution in Figure 2(a).
DifÔ¨Åculty Measure Firstly, we compare surface
form similarity of mentions and ground-truth enti-
ties. 51.31% of the mentions have different surface
forms compared with ground-truth entities. Specif-
ically, 16.05% of the mentions are totally different
from the ground-truth entities. The large difference
of the surface form brings challenges for MEL.
Secondly, we report the #candidate entities for
each mention in Figure 4(b). Intuitively, the more
entities a mention may refer to, the more am-
biguous the mention is, and the more difÔ¨Åcult the
EL/MEL is. SpeciÔ¨Åcally, we generate a m!e
hash list based on the ( m; e) co-occurrence statis-
tics from Wikipedia (See Section 5.1 for details).
As shown in Figure 4(b), we can see that 1) 48.63%
mentions have more than 10 candidate entities. 2)
15.26% mentions are not contained in the hash list,
which means their candidates are the entire entity
set of the KB.
Thirdly, we randomly sample 200 image-caption
pairs from WDiverse to evaluate the diversity of
ambiguity. As shown in Figure 2(b), WDiverse
covers a wide range of ambiguity.
5 Methods
It is challenging to directly predict the entity from a
large-scale KB because it consumes large amounts
of time and space resources. Therefore, following
previous work (Yamada et al., 2016; Ganea and
Hofmann, 2017; Cao et al., 2021), we split MEL
into two steps: 1) candidate retrieval (CR) is Ô¨Årst
used to guarantee the recall and obtain a candi-
date entity set consisting of the TopK entities that
are most similar to the mention; 2) entity disam-
biguation (ED) is then conducted to guarantee the
precision and predict the entity with the highest
matching score.
5.1 Candidate Retrieval
Existing methods (Yamada et al., 2016; Ganea and
Hofmann, 2017; Le and Titov, 2018) mainly utilize
two types of clues to generate the candidate entity
setE: (I) the m!ehash list recording prior
probabilities from mentions to entities: P(ejm).
(II) the similarity between the contexts of mention
mand entity e.
Following these works, we implement a se-
ries of baselines as follows: (I) P(ejm)(Ganea
and Hofmann, 2017): P(ejm)is calculated based
on 1) mention entity hyperlink count statistics
from Wikipedia; 2) Wikipedia redirect pages; 3)
Wikipedia disambiguation pages. (II) Baselines of
textual modality : we retrieve the TopK candidate
entities with the most similar textual context of the4789
mention based on BM25 (Robertson and Zaragoza,
2009), pretrained embeddings of words and enti-
ties obtained from (Yamada et al., 2020) (denoted
as WikiVec) and BLINK (Wu et al., 2020). (III)
Baseline of visual modality: we retrieve the TopK
candidate entities with the most similar visual con-
texts of the mention based on CLIP (Radford et al.,
2021).
5.2 Contrastive Entity Disambiguation
The interaction between multimodal contexts of
mentions and entities is complicated. It may bring
noises to the model without careful handling. So
we also introduce several baselines to explore the
fusion of multimodal information.
The key component of ED is to design the func-
tion	(m;e)that quantiÔ¨Åes the matching score
between the mention mand every entity e2E.
As shown in Figure 5, the backbone of 	(m;e)
includes different multimodal encoders of mande
respectively, followed by dot-production to evalu-
ate the matching degree between them. Specially, a
multi-layer perceptron (MLP) is then used to com-
bine the P(ejm). Formally, eofmis predicted
through:
m=Encoder(T; V);e=Encoder(T; V)
e= arg maxMLP (me; P(ejm))
(1)
So the multimodal encoders of mentions and en-
tities are the most signiÔ¨Åcant parts of MEL. They
use the same structure but training with different
parameters.
Multimodal Encoder Firstly, we get the textual
context‚Äôs embeddings. For the mention‚Äôs textual
context T=fw; : : : ; wg, we directly embed
it with the word embedding layer of BERT (Devlinet al., 2019). While for e, we embed it as the
pre-trained embeddings from Yamada et al. (2020),
which have compressed the semantics of e‚Äôs entire
contexts from Wikipedia.
f^w; :::;^wg=BERT (T) (2)
Secondly, we get the visual context embeddings.
Instead of the widely used region-based visual fea-
tures, we adopt grid features following (Huang
et al., 2020), which has the advantage of end-to-end.
SpeciÔ¨Åcally, the visual features are represented
with the grid features from :
f^v; :::;^vg=Flat(ResNet (V)) (3)
where Flat()represents Ô¨Çatting the feature along
the spatial dimension and Lindicates the number
of grid features.
Finally, taking the embeddings of the two modal-
ities as inputs, we capture the interaction between
them. We adopt several backbones to fuse multiple
modalities. 1) UNITER (Chen et al., 2020): the
two modalities are concatenated and then fed into
self-attention transformers to fuse them together.
2)UNITER* : we apply separate self-attention
transformers to the two modalities before UNITER
for better feature extraction of each modality. 3)
LXMERT (Tan and Bansal, 2019): the two modali-
ties are fed into separate self-attention transformers
at Ô¨Årst and then interact with cross-modal atten-
tion. The design of intra-modal and inter-modal
attention helps better alignment and interaction of
multiple modalities.
After multiple layers of the fusion operation:
Fuse(f^w; :::;^wg;f^v; :::;^vg), the hidden
states of the mention‚Äôs tokens fh; :::;hgare ob-
tained. Then we concatenate the hidden states
of the Ô¨Årst and the last tokens and feed them
into a MLP to get the mention‚Äôs embeddings:
MLP ([hjjh])
Contrastive Loss We introduce contrastive
learning (Karpukhin et al., 2020; Gao et al., 2021)
to learn a more robust representation of both men-
tions and entities. It is widely acknowledged that
selecting negative examples could be decisive for
learning a good model. To this end, we utilize both
hard negatives and in-batch negatives to improve
our model‚Äôs ability to distinguish between gold
entities and hard/general negatives. Let erepre-
sent the jcandidate entity of the imention in
a batch and let Pdenote the index of m‚Äôs gold4790ModalityMethod R@10R@50R@100
P P(ejm) 83.34 87.59 88.15
T BM25 38.37 48.78 53.34
T WikiVec 16.23 20.56 23.11
T BLINK 61.76 71.30 73.87
V CLIP 17.34 26.82 31.38
T+V* 61.51 74.80 79.66
P+T+V* 86.28 91.64 93.14
entity. The hard negatives are the other K 1can-
didate entities retrieved in CR step except for the
gold entity:feg. The in-batch negatives
are gold entities of other B 1mentions in the
mini-batch:feg, where Brepresents the
batch size. The optimization objective is deÔ¨Åned
as the negative log likelihood of the ground-truth
entity:
L(m;E) = loge
e+P
X=Xe
| {z }+Xe
| {z }
(4)
Besides the above baselines, we also compare
with the following classic baselines: 1) Baselines
of Textual Modality include REL (Le and Titov,
2018), BERT (Devlin et al., 2019), and BLINK
(Wu et al., 2020). 2) Baselines of Visual Modal-
ityinclude ResNet-50 and CLIP. 3) Multimodal
Baselines include MMEL18 (Moon et al., 2018),
MMEL20 (Adjali et al., 2020b). Details of the
baselines can be found in the Appendix.
6 Experimental Results
6.1 Candidate Retrieval Results
As shown in Table 4: 1) Our model achieves
93.14% of R@100 , which indicates most related
entities can be recalled from the large 16M KB.
For retrieval, each mention takes about 12ms ofModality Model F1 P R
T!TREL 59.52 60.77 58.34
BLINK 64.94 67.72 62.39
BERT 56.16 59.80 52.94
V!VResNet-50 26.80 28.46 25.32
CLIP 35.26 36.68 33.41
T+V!T MMEL18 51.22 53.27 48.78
T+V!T+VMMEL20 37.44 38.48 36.46
UNITER 68.09 70.63 65.72
UNITER* 68.76 73.27 64.80
LXMERT 68.91 73.04 65.22
UNITERy68.97 71.95 66.23
UNITER*y69.59 72.65 66.77
LXMERTy70.13 73.06 67.43
P(e|m), 40ms of BM25, 183ms of WikiVec and
CLIP, 60ms of BLINK; 2) As for ensemble of dif-
ferent modalities, T + V achieves better results
than V and T, which veriÔ¨Åes that the information
of different modalities are complementary;
In practice, we use grid search over the Dev. to
Ô¨Ånd the best combination of different modalities.
For example, when K= 10 , the best Eis gener-
ated with 80%P+ 10%T + 10%V .
6.2 Entity Disambiguation Results
Following previous work, we report micro F, pre-
cision, recall in Table 5. According to the experi-
mental results, we can see that: First, the proposed
multimodal methods outperform all the methods
with a single modality, which beneÔ¨Åt from multi-
modal contexts. Besides, contrastive learning can
even improve the performance. We reckon that con-
trastive learning improves the ability to distinguish
entities. Second, the textual baselines perform bet-
ter that the visual ones, which indicates the textual
context still plays a dominant role in MEL. Third,
the methods using transformers to model the in-
teraction between modalities perform better than
those with simple interaction (Moon et al., 2018;
Adjali et al., 2020a), which veriÔ¨Åes the importance
of fusing different modalities.4791
6.3 Multimodal Analysis
We also conduct some experiments on the ED tasks
as following.
Are the multiple modalities complementary?
We draw a Venn diagram of different modalities
in Figure 8. The circle of Method iis calculated
throughand the interaction of two circles
are calculated through. One can see
that the textual modality is dominant, while the
visual modality provides complementary informa-
tion. Specially, the multimodal method predicts
more new entities of 16.86%, which veriÔ¨Åes the
importance of fusing two modalities.
Is it better to have multimodal contexts of both
mentions and entities? We conduct an ablation
study and report the results in Table 6. We can see
that the model with multimodal contexts of both
mentions and entities achieves the best result. So
linking multimodal mentions to multimodal entities
is better than linking multimodal mentions to mono-
modal entities as done in (Moon et al., 2018).
What visual clues are provided by the vi-
sual contexts? We randomly select 800 image-
caption pairs from the test dataset, and then ask
annotators to label each mention with the types of
visual clues. The visual clues include 4 types: 1)
Object : the image contains the entity object. 2)
Scene : the image reveals the scene that the entity
belongs to (e.g. a basketball player of the ‚Äòbasket-
ball game‚Äô scene). 3) Property : the image contains
some properties of the entity (e.g. an American Ô¨Çag
reveals the property of a person‚Äôs nationality). 4)
Others : other important contexts. Note that the
four types of clues can be crossed and a sample
could have no clues. Examples of the visual clues
can be found in Figure 6. We Ô¨Ånd that visual con-
text is helpful for 60.54% mentions and 81.56%
image-caption pairs. We report the contribution
of different types of visual clues in Table 7. One
can see that: 1) For property clues and object clues,
the T+V is 11.20% and 8.48% higher than T. So
the multimodal model beneÔ¨Åts a lot from the infor-
mation of objects and properties in the images. 2)
For scene clues, the T+V is slightly worse than T,
which shows implicit visual clues are not used well
and indicates the direction of future research.
6.4 Case Study
We present several examples where multimodal
contexts inÔ¨Çuence MEL in Figure 7. Example (a)
and (b) verify the helpfulness of the multimodal
context. From the error cases, we can see that the4792
model still lacks such capabilities: 1) Eliminate the
inÔ¨Çuence of unhelpful images (e.g., Example (c));
2) Perform reasoning (e.g., inferring the ‚Äúwhite
house‚Äù from Example (d)‚Äôs image); 3) Alleviate
over-reliance on P(ejm)(e.g., Example (e)).
7 Conclusion and Future Work
We propose WDiverse, a manually-annotated
Wikipedia-based MEL dataset collected fromWikinews. To overcome the weaknesses of ex-
isting datasets, WDiverse covers a wide range
of topics, entity types and ambiguity. We imple-
ment a series of baselines and carry out multiple
experiments over the dataset. According to the ex-
perimental results, WDiverse is a challenging
dataset worth further exploration. Besides mul-
timodal entity linking, WDiverse can also be
applied to evaluate the pre-trained language model,
multimodal named entity typing/recognition, mul-
timodal topic classiÔ¨Åcation, etc. In the future, we
plan to 1) utilize more than one images of each en-
tity 2) adopt Ô¨Åner-grained multimodal interaction
models for this task and 3) transfer the model to
more general scenarios such as EL in articles.
Acknowledgement
We thank all the reviewers for their valuable sug-
gestions. This research was supported by the
National Key Research and Development Project
(No. 2020AAA0109302), National Natural Sci-
ence Foundation of China (No. 62072323), Shang-
hai Science and Technology Innovation Action
Plan (No. 19511120400), Shanghai Munici-
pal Science and Technology Major Project (No.
2021SHZDZX0103) and Alibaba Research Intern
Program.4793Ethical Considerations
We collected publicly available Wikinews image-
caption pairs without storing any personal data.
During data cleaning, we remove the cases that
contain pornographic, profane, and violent content.
We annotate the data using the crowdsourcing
platform of Alibaba. To ensure that the crowd
workers were fairly compensated, we paid them at
an hourly rate of 15 USD per hour, which is a fair
and reasonable rate of pay for crowdsourcing.
References47944795A Annotation Details
A.1 Annotation Guidelines
To avoid subjective errors, we designed detailed an-
notation guidelines with multiple samples to avoid
the controversy of mention boundary, mention gran-
ularity and Wikipedia URL. The entire annotation
guideline is summarized as follows.
1.Only label mentions with the entities that can
be inferred from the image-caption pairs in-
stead of the entities that can only be inferred
from the entire news.
2.Label mentions that do not have correspond-
ing entities in Wikipedia with ‚ÄòNIL‚Äô.
3.Mention types include persons, organizations,
locations, events, works, currency and others.
4.We assume that mentions are non-recursive
and non-overlapping. So if a mention is em-
bedded in another mention, only the top-level
mention is annotated.
5.The mention boundary is detected with the
smallest granularity while avoiding overlap-
ping boundaries. An example is labeling
"French President Nicolas Sarkozy" with both
"French" and "Nicolas Sarkozy" instead of
"French President Nicolas Sarkozy".
6.The title before mention is also part of the
mention span.
7.Metonymy is needed. Metonymy is a Ô¨Ågure of
speech that replaces the name of a thing with
the name of something else with which it is
closely associated. An example is using Eng-
land to represent the England national football
team.
A.2 Details of Image Data Cleaning
During the data cleaning, we have also done some
processing on the images:
To prevent image processing tools from being
unable to process certain types of images, we
normalize the images with less popular for-
mats (e.g. .svg, .tif, .gif) into the images with
popular formats (i.e., .png, .jpg);
As some ‚Äúimages‚Äù are videos actually, we
manually select a certain frame of the videos
as the image.B Other Details of Experimental Settings
B.1 Details about the Baselines
In the ED step, we also compare with the following
baselines:
Baselines of Textual Modality : 1)REL (Le
and Titov, 2018): it is a robust EL baseline
that incorporates latent relation variables into
the EL model for better understanding of the
text. 2) BERT (Devlin et al., 2019): it is
a widely acknowledged pre-trained language
model. 3) BLINK (Wu et al., 2020): it applies
cross-attention to the mention and entities for
MEL.
Baselines of Visual Modality : 1) ResNet-
50(He et al., 2016): it is a widely acknowl-
edge model with residual learning framework
trained on the ImageNet (Deng et al., 2009).
2)CLIP (Radford et al., 2021): it is a model
trained to predict the matching degree of texts
and images, which also achieves competitive
performance on visual tasks.
Multimodal Baselines : 1)MMEL18 (Moon
et al., 2018) uses modality attention to
fuse features from different modalities.
2)MMEL20 (Adjali et al., 2020b) uses
Sent2vec, BM25 and Inception-V3 to extract
features of different modalities, then inte-
grates different modalities together with the
concatenation operation followed by MLP.
B.2 Implementations Details
We train all the models on the same device for 20
iterations with the early stopping mechanism. The
learning rate is set as 1e-3. The batch size is set as
12. The model of UNITER consists of 12 layers of
transformers. The UNITER* consists of 8 layers
of textual transformers and visual transformers re-
spectively, followed by the concatenation operation
and 4 layers of transformers. The LXMERT con-
sists of 8 layers of textual transformers and visual
transformers respectively, followed by 4 layers of
cross-modality attention mechanism.
C Supplementary Experimental Results
C.1 Detailed Main Results
To verify the robustness of out method, we report
the model performance in Table 8.4796Modality Model F1 Precision Recall R@5
T!TREL 59.52(.16) 60.77(.70) 58.34(.53) 71.64(.19)
BLINK 64.94(.27) 67.72(.40) 62.39(.78) 71.21(.17)
BERT 56.28(.12) 59.80(.11) 52.94(.12) 68.19(.82)
V!VResNet-50 26.80(.83) 28.46(.85) 25.32(.71) 43.87(1.25)
CLIP 35.26(.72) 36.68(.50) 33.41(.14) 46.63(1.61)
T+V!T MMEL18 51.22(.88) 53.27(.70) 48.78(.27) 70.48(.50)
T+V!T+VMMEL20 37.44(1.20) 38.48(.56) 36.46(.85) 39.14(1.56)
UNITER 68.09(.17) 70.63(.39) 65.72(.16) 73.98(.55)
UNITER* 68.76(.15) 73.27(.83) 64.80(.82) 74.60(.89)
LXMERT 68.91(.21) 73.04(.21) 65.22(.46) 74.58(.43)
UNITERy 68.97(.28) 71.95(.08) 66.23(.53) 74.99(.56)
UNITER*y69.59(.10) 72.65(.44) 66.77(.40) 75.67(.64)
LXMERTy 70.13(.12) 73.06(.38) 67.43(.49) 75.18(.42)
Method First First&Last Average
F1 69.09 70.13 69.68
C.2 Comparison of Mention‚Äôs Pooling
Strategies
After multiple layers of multimodal fusion, we get
the hidden states of the mention‚Äôs token sequence.
Then a pooling operation is needed to get the rep-
resentation of the entire mention. Here we com-
pare three pooling methods: 1) the Ô¨Årst token of
the mention sequence (denoted as First); 2) the
concatenation of the Ô¨Årst and last of the mention
sequence (denoted as First&Last); 3) the average
of the entire mention sequence (denoted as Aver-
age). According to the result in Table 9, First&Last
has achieved the best performance, which is thus
selected in the Ô¨Ånal version of our model.
C.3 Analysis of the Contrastive Loss
To evaluate the inÔ¨Çuence of the contrastive loss, we
also performed a detailed analysis. SpeciÔ¨Åcally, we
conducted experiments with different numbers of
# hard negatives and # in-batch negatives. For ex-
pression convenience, we use KandBto represent
their numbers. To prevent other factors from affect-
ing the results, we do not change the batch size or
the candidate entity number, but only change the
number of negative instances.
Do hard negatives or in-batch negatives have a
greater impact on the results? By comparing Fig-
ure 9(a) and Figure 9(b), we can Ô¨Ånd out that even
without in-batch negatives, the model still achieves
relatively good results. However, the decrease of
hard negative leads to a sharp drop in model perfor-
mance. Therefore, the hard negatives inÔ¨Çuence the
model performance more.
Empirical analysis of the number of negative
samples. We can see that no matter hard negatives
or in-batch negatives, the more negative examples
are introduced, the effect will be improved. There-
fore, under the premise of sufÔ¨Åcient GPU memory,
negative examples should be increased as much as
possible, especially for the number of hard nega-
tives.4797