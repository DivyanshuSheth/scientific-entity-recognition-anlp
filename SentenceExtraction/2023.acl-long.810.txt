
Matthias Lindemannand Alexander Kollerand Ivan TitovILCC, University of Edinburgh,LST, Saarland University,ILLC, University of Amsterdam
Abstract
Seq2seq models have been shown to struggle
with compositional generalization in semantic
parsing, i.e. generalizing to unseen composi-
tions of phenomena that the model handles cor-
rectly in isolation. We phrase semantic parsing
as a two-step process: we first tag each input
token with a multiset of output tokens. Then
we arrange the tokens into an output sequence
using a new way of parameterizing and pre-
dicting permutations. We formulate predict-
ing a permutation as solving a regularized lin-
ear program and we backpropagate through the
solver. In contrast to prior work, our approach
does not place a priori restrictions on possible
permutations, making it very expressive. Our
model outperforms pretrained seq2seq models
and prior work on realistic semantic parsing
tasks that require generalization to longer exam-
ples. We also outperform non-tree-based mod-
els on structural generalization on the COGS
benchmark. For the first time, we show that a
model without an inductive bias provided by
trees achieves high accuracy on generalization
to deeper recursion depth.
1 Introduction
Sequence-to-sequence models have been very suc-
cessfully applied to many structural tasks in NLP
such as semantic parsing. However, they have also
been shown to struggle with compositional gener-
alization (Lake and Baroni, 2018; Finegan-Dollak
et al., 2018; Kim and Linzen, 2020; Hupkes et al.,
2020), i.e. the model fails on examples that contain
unseen compositions or deeper recursion of phe-
nomena that it handles correctly in isolation. For
example, a model which correctly parses ‘Mary
knew that Jim slept’ should also be able to parse
sentences with deeper recursion than it has seen
during training such as ‘Paul said that Mary knew
that Jim slept’. This sort of generalization is easy
for humans but challenging for neural models.Figure 1: We model seq2seq tasks by first predicting a
multiset (dashed boxes) for every input token, and then
permuting the tokens to put them into order. We realize
the permutation using the red edges: if there is an edge
from token itoj, then iis the predecessor of jin the
output. Every token is visited exactly once.
In order for a model to generalize composition-
ally in semantic parsing, it has to identify reusable
‘fragments’ and be able to systematically combine
them in novel ways. One way to make a model sen-
sitive to fragments is to make it rely on a tree that
makes the compositional structure explicit. How-
ever, this complicates the training because these
trees need to be obtained or induced. This is compu-
tationally demanding or at least requires structural
preprocessing informed by domain knowledge.
In this paper, we propose a two-step sequence-
based approach with a structural inductive bias that
does not rely on trees: the ‘fragments’ are multi-
sets of output tokens that we predict for every input
token in a first step. A second step then arranges
the tokens we predicted in the previous step into
a single sequence using a permutation model. In
contrast to prior work (Wang et al., 2021; Linde-
mann et al., 2023), there are no hard constraints
on the permutations that our model can predict.
We show that this enables structural generalization
on tasks that go beyond the class of synchronous
context-free languages.14488We overcome two key technical challenges in
this work: Firstly, we do not have supervision for
the correspondence between input tokens and out-
put tokens. Therefore, we induce the correspon-
dence during training. Secondly, predicting per-
mutations without restrictions is computationally
challenging. For this, we develop a differentiable
GPU-friendly algorithm.
On realistic semantic parsing tasks our approach
outperforms previous work on generalization to
longer examples than seen at training. We also out-
perform all other non-tree models on the structural
generalization tasks in semantic parsing on COGS
(Kim and Linzen, 2020). For the first time, we
also show that a model without an inductive bias
towards trees can obtain high accuracy on general-
ization to deeper recursion on COGS.
To summarize, our main contributions are:
•a flexible seq2seq model that performs well on
structural generalization in semantic parsing
without assuming that input and output are
related to each other via a tree structure.
•a differentiable algorithm to parameterize and
predict permutations without a priori restric-
tions on what permutations are possible.
2 Overview and Motivation
Our approach consists of two stages. In the first
stage (multiset tagging), we predict a multiset zof
tokens for every input token xfrom the contextual-
ized representation of x. This is motivated by the
observation that input tokens often systematically
correspond to a fragment of the output (like slept
corresponding to sleep andagent and a variable
in Fig. 1). Importantly, we expect this systematic
relationship to be largely invariant to phrases being
used in new contexts or deeper recursion. We refer
to the elements of the multisets as multiset tokens .
In the second stage (permutation), we order the
multiset tokens to arrive at a sequential output. Con-
ceptually, we do this by going from left to right over
theoutput and determining which multiset token
to put in every position. Consider the example in
Fig. 1. For the first output position, we simply
select a multiset token ( *in the example). All sub-
sequent tokens are put into position by ‘jumping’
from the token that was last placed into the output
to a new multiset token. In Fig. 1, we jump from *
togirl (shown by the outgoing red edge from *).
This indicates that girl is the successor of *in the
output and hence the second output token. Fromgirl we jump to one of the xtokens to determine
what the third output token is and so on. Since we
predict a permutation, we must visit every multiset
token exactly once in this process.
The jumps are inspired by reordering in phrase-
based machine translation (Koehn et al., 2007) and
methods from dependency parsing, where directly
modeling bi-lexical relationships on hidden states
has proven successful (Kiperwasser and Goldberg,
2016). Note also that anypermutation can be rep-
resented with jumps. In contrast, prior work (Wang
et al., 2021; Lindemann et al., 2023) has put strong
restrictions on the possible permutations that can
be predicted. Our approach is more flexible and
empirically it also scales better to longer inputs,
which opens up new applications and datasets.
Setup. We assume we are given a dataset
D={(x,y), . . .}of input utterances xand target
sequences y. If we had gold alignments, it would
be straightforward to train our model. Since we
do not have this supervision, we have to discover
during training which tokens of the output ybe-
long into which multiset z. We describe the model
and the training objective of the multiset tagging
in Section 3. After the model is trained, we can
annotate our training set with the most likely z, and
then train the permutation model.
For predicting a permutation, we associate a
score with each possible jump and search for the
highest-scoring sequence of jumps. We ensure that
the jumps correspond to a permutation by means of
constraints, which results in a combinatorial opti-
mization problem. The flexibility of our model and
its parametrization come with the challenge that
this problem is NP-hard. We approximate it with
a regularized linear program which also ensures
differentiability. Our permutation model and its
training are described in Section 4. In Section 5, we
discuss how to solve the regularized linear program
and how to backpropagate through the solution.
3 Learning Multisets
For the multiset tagging, we need to train a model
that predicts the multisets z, . . . , zof tokens by
conditioning on the input. We represent a multi-
setzas an integer-valued vector that contains for
every vocabulary item vthe multiplicity of vinz,
i.e.z=kmeans that input token icontributes k
occurrences of output type v. Ifvis not present in
multiset z, then z= 0. For example, in Fig. 1,
z = 1 andz= 2. As discussed in Sec-14489tion 2, we do not have supervision for z, . . . , z
and treat them as latent variables. To allow for
efficient exact training, we assume that all zare
independent of each other conditioned on the input:
P(z|x) =/productdisplayP(z|x) (1)
where vranges over the entire vocabulary.
Parametrization. We parameterize P(z|x)
as follows. We first pass the input xthrough a
pretrained RoBERTa encoder (Liu et al., 2019),
where (x)is the output of the final layer.
We then add RoBERTa’s static word embeddings
from the first, non-contextualized, layer to that:
h= (x)+ (x) (2)
We then pass hthrough a feedforward network
obtaining ˜h=(h)and define a distribution
over the multiplicity of vin the multiset z:
P(z=k|x) =exp/parenleftig
˜hw+b/parenrightig
/summationtextexp/parenleftig
˜hw+b/parenrightig(3)
where the weights wand biases bare specific to
vand the multiplicity k. In contrast to standard
sequence-to-sequence models, this softmax is not
normalized over the vocabulary but over the multi-
plicity, and we have distinct distributions for every
vocabulary item v. Despite the independence as-
sumptions in Eq. (1), the model can still be strong
because htakes the entire input xinto account.
Training. The probability of generating the
overall multiset mas the union of all zis the prob-
ability that for every vocabulary item v, the total
number of occurrences of vacross all input posi-
tionsisums to m:
P(m|x) =/productdisplayP(z+. . .+z=m|x)
This can be computed recursively:
P(z+. . .+z=m|x) =
/summationdisplayP(z=k|x)P(z+. . .z=m−k|x)
Letm(y)be the multiset of tokens in the gold se-
quence y. We train our model with gradient ascent
to maximize the marginal log-likelihood of m(y):
/summationdisplaylogP(m(y)|x) (4)Like Lindemann et al. (2023), we found it help-
ful to initialize the training of our model with
high-confidence alignments from an IBM-1 model
(Brown et al., 1993) (see Appendix C.3 for details).
Preparation for permutation. The scoring
function of the permutation model expects a se-
quence as input. There is no a priori obvious or-
der for the elements within the individual multi-
setsz. We handle this by imposing a canonical
order O (z)on the elements of zby sort-
ing the multiset tokens by their id in the vocabu-
lary. They are then concatenated to form the input
z=O (z). . .O (z)to the permuta-
tion model.
4 Relaxed Permutations
After predicting a multiset for every input token
and arranging the elements within each multiset to
form a sequence z, we predict a permutation of z.
We represent a permutation as a matrix Vthat
contains exactly one 1 in every row and column
and zeros otherwise. We write V= 1if position
iinzis mapped to position jin the output y. Let
Pbe the set of all permutation matrices.
Now we formalize the parametrization of permu-
tations as discussed in Section 2. We associate a
score predicted by our neural network with each
permutation Vand search for the permutation with
the highest score. The score of a permutation de-
composes into a sum of scores for binary ‘features’
ofV. We use two types of features.
The first type of feature is active if the permuta-
tion Vmaps input position ito output position j
(i.e.V= 1). We associate this feature with the
score sand use these scores only to model what
the first and the last token in the output should be.
That is, smodels the preference to map posi-
tioniin the input to the first position in the output,
and analogously smodels the preference to put
iinto the last position in the output. For all out-
put positions jthat are neither the first nor the last
position, we simply set s= 0.
The second type of feature models the jumps we
introduced in Section 2. We introduce a feature that
is1iffVcontains a jump from ktoi, and associate
this with a score s. In order for there to be
a jump from ktoi, the permutation Vmust map
input ito some output position j(V= 1)andit
must also map input position kto output position
j−1(V= 1). Hence, the product VV
is 1 iff there is a jump from ktoiat output position14490j. Based on this, the sum/summationtextVVequals 1
if there is anyoutput position jat which there is a
jump from ktoiand 0 otherwise. This constitutes
the second type of feature.
Multiplying the features with their respective
scores, we want to find the highest-scoring permu-
tation under the following overall scoring function:
arg max/summationdisplayVs+
/summationdisplays
/summationdisplayVV
 (5)
LetV(s)be the solution to Eq. (5) as a function
of the scores. Unfortunately, V(s)does not have
sensible derivatives because Pis discrete. This
makes V(s)unsuitable as a neural network layer.
In addition, Eq. (5) is NP-hard (see Appendix A.1).
We now formulate an optimization problem that
approximates Eq. (5) and which has useful deriva-
tives. Firstly, we relax the permutation matrix Vto
a bistochastic matrix U, i.e. Uhas non-negative el-
ements and every row and every column each sum
to 1. Secondly, note that Eq. (5) contains quadratic
terms. As we will discuss in the next section, our
solver assumes a linear objective, so we replace
VVwith an auxiliary variable W. The
variable Wis designed to take the value 1if and
only if U= 1andU= 1. We achieve this
by coupling WandUusing constraints. Then, the
optimization problem becomes:
arg max/summationdisplayUs+/summationdisplayWs (6)
subject to/summationdisplayU= 1 ∀j (6a)
/summationdisplayU= 1 ∀i (6b)
/summationdisplayW=U∀j >1, i (6c)
/summationdisplayW=U∀j >1, k(6d)
U,W≥0 (6e)
Finally, in combination with the linear objective,
the argmax operation still causes the solution U(s)
of Eq. (6) as a function of sto have no useful
derivatives. This is because an infinitesimal change
inshas no effect on the solution U(s)for almost
alls.To address this, we add an entropy regularization
termτ(H(U) +H(W))to the objective Eq. (6),
where H(U) =−/summationtextU(logU−1), andτ >0
determines the strength of the regularization. The
entropy regularization ‘smooths’ the solution U(s)
in an analogous way to softmax being a smoothed
version of argmax. The parameter τbehaves analo-
gously to the softmax temperature: the smaller τ,
the sharper U(s)will be. We discuss how to solve
the regularized linear program in Section 5.
Predicting permutations. At test time, we
want to find the highest scoring permutation, i.e.
we want to solve Eq. (5). We approximate this
by using U(s)instead, the solution to the entropy
regularized version of Eq. (6). Despite using a
low temperature τ, there is no guarantee that U(s)
can be trivially rounded to a permutation matrix.
Therefore, we solve the linear assignment problem
with U(s)as scores using the Hungarian Algo-
rithm (Kuhn, 1955). The linear assignment prob-
lem asks for the permutation matrix Vthat maxi-
mizes/summationtextVU(s).
4.1 Parametrization
We now describe how we parameterize the scores
sto permute the tokens into the right order. We
first encode the original input utterance xlike in
Eq. (2) to obtain a hidden representation hfor
input token x. Let abe the function that maps
a(i)∝⇕⊣√∫⊔≀→jif the token in position iinzcame from
the multiset that was generated by token x. For
example, in Fig. 1, a(6) = 3 since sleep was
predicted from input token slept . We then define
the hidden representation has the concatenation
ofhand an embedding of z:
h=/bracketleftbig
h; (z)/bracketrightbig
(7)
We parameterize the scores for starting the output
with token ias
s=w(h)
and analogously for ending it with token i:
s=w(h)
We set s= 0for all other i, j.
We parameterize the jump scores susing
Geometric Attention (Csordás et al., 2022) from
htoh. Intuitively, Geometric Attention favours
selecting the ‘matching’ element hthat is closest
tohin terms of distance |i−k|in the string. We
refer to Csordás et al. (2022) for details.14491
4.2 Learning Permutations
We now turn to training the permutation model. At
training time, we have access to the gold output y
and a sequence zfrom the output of the multiset
tagging (see the end of Section 3). We note that
whenever y(orz) contains one vocabulary item at
least twice, there are multiple permutations that can
be applied to zto yield y. Many of these permuta-
tions will give the right result for the wrong reasons
and the permutation that is desirable for generaliza-
tion is latent. For example, consider Fig. 2. The
token agent is followed by the entity who performs
the action, whereas theme is followed by the one
affected by the action. The permutation indicated
by dashed arrows generalizes poorly to a sentence
likeEmma knew that James slid since slidwill in-
troduce a theme role rather than an agent role (as
the sliding is happening to James). Thus, this per-
mutation would then lead to the incorrect output
know theme Emma ... slide agent James , in
which Emma is affected by the knowing event and
James is the one who slides something.
In order to train the permutation model in this
setting, we use a method that is similar to EM
for structured prediction.During training, the
model output U(s)andW(s)often represents
a soft permutation that does notpermute zinto
y. Our goal is to push the model output into the
space of (soft) permutations that lead to y. More
formally, let Q∈ Q(y,z)be the set of bistochastic
matrices such that Q= 0 iffz̸=y. That is,
any permutation included in Q(y,z)leads to the
gold output ywhen applied to z.
First, we project the current prediction U(s)
andW(s)intoQ(y,z)using the KL divergenceas a measure of distance (E-step):
ˆU,ˆW= arg minKL(U||U(s))+ (8)
KL(W||W(s))
subject to Eq. (6a) to Eq. (6e).
Similar to the M-step of EM, we then treat ˆU
andˆWas soft gold labels and train our model to
minimize the KL divergence between labels and
model:
KL(ˆU||U(s)) + KL(ˆW||W(s))
Eq. (8) can be solved in the same way as the entropy
regularized version of Eq. (6) because expanding
the definition of KL-divergence leads to a regular-
ized linear program with a similar feasible region
(see Appendix A.3 for details).
5 Inference for Relaxed Permutations
Now we describe how to solve the entropy regu-
larized form of Eq. (6) and how to backpropagate
through it. This section may be skipped on the
first reading as it is not required to understand the
experiments; we note that the resulting algorithm
(Algorithm 1) is conceptually relatively simple. Be-
fore describing our method, we explain the general
principle.
5.1 Bregman’s Method
Bregman’s method (Bregman, 1967) is a method
for constrained convex optimization. In particular,
it can be used to solve problems of the form
x= arg maxsx+τH(x)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright(9)
where C, . . . , Care linear equality constraints,
H(x) =−/summationtextx(logx−1)is a form of entropy
regularization, and τdetermines the strength of the
regularization. Note that our parameterization of
permutations (Eq. (6)) has this form.
Bregman’s method is a simple iterative process.
We start with the scores sand then cyclically iterate
over the constraints and project the current estimate
xonto the chosen constraint until convergence:
x= exps
τ
x= arg minKL(x||x)(10)
where KL(x|y) =/summationtextxlog−x+y
is the generalized KL divergence. We call14492Algorithm 1 Bregman’s method for Eq. (6) with
entropic regularization
function (s,τ)
U= exp( τs)
W= exp( τs)
while within budget and not converged do
U,W=KL project( U,W; 6a,6c) with Prop. 2
U,W=KL project( U,W; 6a,6d) with Prop. 2
U=KL project ( U, 6b) with Prop. 1
end while
return U ,W
end function
arg minKL(x|x)a KL projection. In order
to apply Bregman’s method, we need to be able to
compute the KL projection arg minKL(x|x)
forallC, . . . , Cin closed-form. We discuss how
to do this for Eq. (6) in the next section.
As an example, consider a problem of the form
Eq. (9) with a single linear constraint C={x|/summationtextx= 1}. In this case, Bregman’s method coin-
cides with the softmax function. This is because
the KL projection x= arg minKL(x||y)
fory>0has the closed-form solution x=.
If we have a closed-form expression for a KL
projection (such as normalizing a vector), we
can use automatic differentiation to backpropagate
through it. To backpropagate through the entire
solver, we apply automatic differentiation to the
composition of all projection steps.
5.2 Bregman’s Method for Eq. (6)
In order to apply Bregmans’ method to solve the
entropy regularized version of Eq. (6), we need to
decompose the constraints into sets which we can
efficiently project onto. We choose the following
three sets here: (i), containing Eq. (6a) and Eq. (6c),
and (ii), containing Eq. (6a) and Eq. (6d), and fi-
nally (iii), containing only Eq. (6b). We now need
to establish what the KL projections are for our
chosen sets. For (iii), the projection is simple:
Proposition 1. (Benamou et al. (2015),
Prop. 1) For A,m>0, the KL projection
arg minKL(U||A)subject to/summationtextU=mis
given by U=m.
Let us now turn to (i) and (ii). The constraints
Eq. (6d) and Eq. (6c) are structurally essentially
the same, meaning that we can project onto (ii) in
basically the same manner as onto (i). We project
onto (i), with the following proposition:Proposition 2. ForA,B>0, the KL projection
arg min
U,WKL(U||A) +KL(W||B)
subject to/summationdisplayU= 1∀j
/summationdisplayW=U∀j, i(11)
is given by:
U=T/summationtextT
W=UB/summationtextB
where T=/radicalbig
A·/summationtextB.
The proof can be found in Appendix A.2. We
present the overall algorithm in Algorithm 1, and
note that it is easy to implement for GPUs. In
practice, we implement all projections in log space
for numerical stability.
6 Evaluation
6.1 Doubling Task
Our permutation model is very expressive and is
not limited to synchronous context-free languages.
This is in contrast to the formalisms that other ap-
proaches rely on (Wang et al., 2021; Lindemann
et al., 2023). To evaluate if our model can struc-
turally generalize beyond the synchronous context-
free languages in practice, we consider the func-
tionF={(w, ww )|w∈Σ}. This function is
related to processing challenging natural language
phenomena such as reduplication and cross-serial
dependencies. We compare our model with an
LSTM-based seq2seq model with attention and a
Transformer in the style of Csordás et al. (2021)
that uses a relative positional encoding. Since the
input is a sequence of symbols rather than English,
we replace RoBERTa with a bidirectional LSTM
and use randomly initialized embeddings. The
models are trained on inputs of lengths 5 to 10
and evaluated on longer examples. The results can
be found in Fig. 3. All models get perfect or close
to perfect accuracy on inputs of length 11 but ac-
curacy quickly deteriorates for the LSTM and the
Transformer. In contrast, our model extrapolates
very well to longer sequences.
6.2 COGS
COGS is a synthetic benchmark for composi-
tional generalization introduced by Kim and Linzen14493
(2020). Models are tested for 21 different cases of
generalization, 18 of which focus on using a lex-
ical item in new contexts ( Lex). There are 1000
instances per generalization case. Seq2seq models
struggle in particular with the structural general-
ization tasks (Yao and Koller, 2022), and we focus
on those: (i) generalization to deeper PPrecursion
than seen during training ("Emma saw a hedge-
hog on a chair in the garden beside the tree..."), (ii)
deeper CPrecursion ("Olivia mentioned that James
saw that Emma admired that the dog slept"), and
(iii) PPs modifying subjects when PPs modified
only objects in the training data ( OS).
We follow previous work and use a lexicon
(Akyurek and Andreas, 2021) to map some input
tokens to output tokens (see Appendix C.2 for de-
tails). We also use this mechanism to handle the
variable symbols in COGS.
We report the means and standard deviations
for 10 random seeds in Table 1. Our approach ob-
tains high accuracy on CP and PP recursion but
exact match accuracy is low for OS. This is in part
because our model sometimes predicts semantic
representations for OS that are equivalent to the
gold standard but use a different order for the con-
juncts. Therefore, we report accuracy that accounts
for this in Table 2. In both tables, we also report the
impact of using a simple copy mechanism instead
of the more complex lexicon induction mechanism
(-Lex). Our model outperforms all other non-tree-
based models by a considerable margin.
Structural generalization without trees. All
previous methods that obtain high accuracy on re-
cursion generalization on COGS use trees. Some
approaches directly predict a tree over the input
(Liu et al., 2021; Weißenhorn et al., 2022), while
others use derivations from a grammar for data aug-
mentation (Qiu et al., 2022) or decompose the in-
put along a task-specific parse tree (Drozdov et al.,
2022). Our results show that trees are not as impor-
tant for compositional generalization as their suc-
cess in the literature may suggest, and that weaker
structural assumptions already reap some of the
benefits.
Logical forms with variables. COGS uses
logical forms with variables, which were removed
in conversion to variable-free formats for evalua-
tion of some approaches (Zheng and Lapata, 2021;
Qiu et al., 2022; Drozdov et al., 2022). Recently,
Wu et al. (2023) have argued for keeping the vari-
able symbols because they are important for some
semantic distinctions; we keep the variables.
6.3 ATIS
While COGS is a good benchmark for composi-
tional generalization, the data is synthetic and does
not contain many phenomena that are frequent in
semantic parsing on real data, such as paraphrases
that map to the same logical form. ATIS (Dahl
et al., 1994) is a realistic English semantic parsing
dataset with executable logical forms. We follow
the setup of our previous work (Lindemann et al.,
2023) (L’23) and use the variable-free FunQL rep-14494Model iid Length
LSTM seq2seq 75.984.95
Transformer 75.761.15
BART-base86.9619.03
L’2368.2629.91
L’23†74.1535.41
Ours76.6541.39
Ours 73.9338.79
resentation (Guo et al., 2020). Apart from the usual
iid split, we evaluate on a length split, where a
model is trained on examples with few conjuncts
and has to generalize to longer logical forms with
more conjuncts. For a fair comparison with pre-
vious work, we do not use the lexicon/copying.
We also evaluate a version of our model with-
out RoBERTa that uses a bidirectional LSTM and
GloVe embeddings instead. This mirrors the model
of L’23.
Table 3 shows mean accuracy and standard devi-
ations over 5 runs. Our model is competitive with
non-pretrained models in-distribution, and outper-
forms all other models on the length generaliza-
tion. The high standard deviation on the length
split stems from an outlier run with 18% accuracy –
the second worst-performing run achieved an accu-
racy of 44%. Even without pretraining, our model
performs very well. In particular, without grammar-
based decoding our model performs on par or out-
performs L’23 with grammar-based decoding.
The runtime of the model in L’23 is dominated
by the permutation model and it takes up to 12
hours to train on ATIS. Training the model pre-
sented here only takes around 2 hours for both
stages.
Performance breakdown. In order for our
approach to be accurate, both the multiset tagging
model and the permutation model have to be accu-
rate. Table 4 explores which model acts as the bot-
tleneck in terms of accuracy on ATIS and COGS.
The answer depends on the dataset: for the syn-
thetic COGS dataset, predicting the multisets cor-
rectly is easy except for OS, and the model strug-
gles more with getting the permutation right. In
contrast, for ATIS, the vast majority of errors can
be attributed to the first stage.
Model Calendar Doc Email
BART-base36.7 0.620.5
L’2357.236.143.9
L’23†69.542.455.6
Ours74.357.860.6
Ours 65.641.447.6
6.4 Okapi
Finally, we consider the recent Okapi (Hosseini
et al., 2021) semantic parsing dataset, in which an
English utterance from one of three domains (Cal-
endar, Document, Email) has to be mapped to an
API request. We again follow the setup of L’23 and
evaluate on their length split, where a model has to
generalize to longer logical forms. In contrast to
all other datasets we consider, Okapi is quite noisy
because it was collected with crowd workers. This
presents a realistic additional challenge on top of
the challenge of structural generalization.
The results of 5 runs can be found in Table 5. Our
model outperforms both BART (Lewis et al., 2020)
and the model of L’23. In the comparison without
pretraining, our model also consistently achieves
higher accuracy than the comparable model of L’23
without grammar-based decoding.
7 Related Work
Predicting permutations. Mena et al. (2018) and
Lyu and Titov (2018) use variational autoencoders
based on the Sinkhorn algorithm to learn latent
permutations. The Sinkhorn algorithm (Sinkhorn,
1964) is also an instance of Bregman’s method and
solves the entropy regularized version of Eq. (6)
without the W-term. This parameterization is con-14495siderably weaker than ours since it cannot capture
our notion of ‘jumps’.
Wang et al. (2021) compute soft permutations as
an expected value by marginalizing over the per-
mutations representable by ITGs (Wu, 1997). This
approach is exact but excludes some permutations.
In particular, it excludes permutations needed for
COGS.In addition, the algorithm they describe
takes a lot of resources as it is both O(n)in mem-
ory and compute. Devatine et al. (2022) investigate
sentence reordering methods. They use bigram
scores, which results in a similar computational
problem to ours. However, they deal with it by re-
stricting what permutations are possible to enable
tractable dynamic programs. Eisner and Tromble
(2006) propose local search methods for decoding
permutations for machine translation.
Outside of NLP, Kushinsky et al. (2019) have
applied Bregman’s method to the quadratic assign-
ment problem, which Eq. (5) is a special case of.
Since they solve a more general problem, using
their approach for Eq. (6) would require O(n)
rather than O(n)variables in the linear program.
Compositional generalization. Much research
on compositional generalization has focused on lex-
ical generalization with notable success (Andreas,
2020; Akyurek and Andreas, 2021; Conklin et al.,
2021; Csordás et al., 2021). Structural generaliza-
tion remains more challenging for seq2seq models
(Yao and Koller, 2022).
Zheng and Lapata (2022) modify the transformer
architecture and re-encode the input and partially
generated output for every decoding step to disen-
tangle the information in the representations. Struc-
ture has also been introduced in models by means
of grammars: Qiu et al. (2022) heuristically induce
a quasi-synchronous grammar (QCFG, Smith and
Eisner (2006)) and use it for data augmentation for
a seq2seq model. Kim (2021) introduces neural
QCFGs which perform well on compositional gen-
eralization tasks but are very compute-intensive.
Other works directly parse into trees or graphs in-
spired by methods from syntactic parsing (Liu et al.,
2021; Herzig and Berant, 2021; Weißenhorn et al.,
2022; Jambor and Bahdanau, 2022; Petit and Corro,
2023).
Several approaches, including ours, have decou-
pled the presence or absence of output tokens from
their order: Wang et al. (2021) train a model end-to-to-end to permute the input (as discussed above)
and then monotonically translate it into an output
sequence. Lindemann et al. (2023) also present
an end-to-end differentiable model that first ap-
plies a ‘fertility step’ which predicts for every word
how many copies to make of its representation, and
then uses the permutation method of Wang et al.
(2021) to reorder the representation before translat-
ing them. Cazzaro et al. (2023) first translate the in-
put monotonically and feed it into a second model.
They use alignments from an external aligner to
train the first model. The second model is a tag-
ger or a pretrained seq2seq model and predicts the
output as a permutation of its input. We compare
against such a baseline for permutations in Ap-
pendix B, finding that it does not work as well as
ours in the compositional generalization setups we
consider.
8 Conclusion
In this paper, we have presented a flexible new
seq2seq model for semantic parsing. Our approach
consists of two steps: We first tag each input token
with a multiset of output tokens. Then we arrange
those tokens into a sequence using a permutation
model. We introduce a new method to predict and
learn permutations based on a regularized linear
program that does not restrict what permutations
can be learned. The model we present has a strong
ability to generalize compositionally on synthetic
and natural semantic parsing datasets. Our results
also show that trees are not necessarily required
to generalize well to deeper recursion than seen at
training time.
Limitations
The conditional independence assumptions are a
limitation for the applicability of our multiset tag-
ging model. For example, the independence as-
sumptions are too strong to apply it to natural
language generation tasks such as summarization.
From a technical point of view, the independence
assumptions are important to be able to induce the
latent assignment of output tokens to multisets effi-
ciently. Future work may design multiset tagging
methods that make fewer independence assump-
tions.
While our method for predicting permutations
is comparatively fast and only has a memory re-
quirement of O(n), inference on long sequences,
e.g. with more than 100 tokens, remains somewhat14496slow. In future work, we plan to investigate other
approximate inference techniques like local search
and dual decomposition.
Regarding the importance of trees for compo-
sitional generalization, our model has no explicit
structural inductive bias towards trees. However,
we do not exclude that the pretrained RoBERTa
model that we use as a component implicitly cap-
tures trees or tree-like structures to a certain degree.
Acknowledgements
We thank Bailin Wang and Jonas Groschwitz for
technical discussions; we thank Hao Zheng for
discussions and for providing system outputs for
further analysis. We also say thank you to Christine
Schäfer and Agostina Calabrese for their comments
on this paper.
ML is supported by the UKRI Centre for Doc-
toral Training in Natural Language Processing,
funded by the UKRI (grant EP/S022481/1), the
University of Edinburgh, School of Informatics and
School of Philosophy, Psychology & Language Sci-
ences, and a grant from Huawei Technologies. IT
is supported by the Dutch National Science Foun-
dation (NWO Vici VI.C.212.053).
References1449714498
A Math Details
A.1 NP-hardness
We show that Eq. (5) can be used to decide the
Hamiltonian Path problem. Let G= (V, E)be a
graph with nodes V={1,2, . . . , n }. A Hamilto-
nian path P=v, v, . . . , vis a path in G(i.e.
(v, v)∈Efor all i) such that each node of G
appears exactly once. Deciding if a graph has a
Hamiltonian path is NP-complete.
Reduction of Hamiltonian path to Eq. (5).
Note that a necessary but not sufficient condition
forPto be a Hamiltonian path is that Pis a permu-
tation of V. This will be ensured by the constraints
on the solution in Eq. (5).
We construct a score function
s=/braceleftigg
1if(k, i)∈E
0else(12)
and let s= 0for all i, j. If we find the solution
of Eq. (5) for the score function Eq. (12), we obtain
a permutation PofV, which may or may not be
a path in G. In a path of nnodes, there are n−1
edges that are crossed. If the score of the solution
isn−1, then all node pairs (v, v)that are
adjacent in Pmust have had a score of 1, indicating
an edge (v, v)∈E. Therefore, Pmust be a
Hamiltonian path. If the score of the solution is
less than n−1, then there is no permutation of V
that is also a path, and hence Ghas no Hamiltonian
path.
A.2 Proof of Proposition 2
We now prove Prop. 2 using a very similar tech-
nique as Kushinsky et al. (2019). As the constraints14499KL(x||z) +KL(Y(x)||W)
=KL(x||z) +/summationdisplayxW/summationtextW(logW−1)
=KL(x||z) +/summationdisplayx/summationtextW /summationtextW(logx/summationtextW−1)
=/summationdisplayx(log(x
z)−1 + logx/summationtextW−1)
=/summationdisplayx(2 log x−logz−log
/summationdisplayW
−2)
= 2/summationdisplayx(logx−1−1
2(logz+ log
/summationdisplayW
)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright)
= 2·KL(x||q)
in Prop. 2 for any value of jdo not interact with
constraints for other values of j, we can assume
w.l.o.g. that jtakes a single value only and drop it
in the notation. We want to solve:
arg min
x,YKL(x||z) +KL(Y||W)
subject to/summationdisplayx= 1
/summationdisplayY=x∀i(13)
We can find Y= arg minKL(Y||W)subject
to/summationtextY=xbased on Prop. 1: Y=.
That is, we can express Yas a function of x(which
we write Y(x)), and therefore our overall problem
is now a problem in one variable ( x):
arg min
xKL(x||z) +KL(Y(x)||W)
subject to/summationdisplayx= 1(14)
We can now rewrite the objective function as
arg min
xKL(x||q)
subject to/summationdisplayx= 1(15)where q=/radicalig
z·/summationtextW. This step is justified
in detail in Fig. 4.
The rewritten optimization problem has the right
form to apply Prop. 1 a second time. We obtain:
x=q/summationtextq
By plugging this into Y(x), we obtain the solution
to the overall optimization problem.
A.3 Reduction of Eq. (8) to Eq. (6)
In this section, we show how Eq. (8) can be reduced
to a problem of the entropy regularized version of
Eq. (6). This is useful because it means we can use
Algorithm 1 to solve Eq. (8).
First, we show that computing a KL projection is
equivalent to solving an entropy-regularized linear
program. Let Cbe the feasible region of the linear
constraints.
arg maxsx−τ/summationdisplayx(logx−1)
= arg minτ/summationdisplayx(logx−s
τ−1)
= arg minKL(x||exp(s
τ))14500Due to this, Eq. (8) is equivalent to a linear pro-
gram that has the same feasible region as Eq. (6)
except for the additional constraint U∈ Q(y,z).
Note that U∈ Q(y,z)essentially rules out certain
correspondences. Therefore we can approximately
enforce U∈ Q(y,z)by masking U(s)such that
any forbidden correspondence receives a very low
score.
A.4 Derivation of loss function as ELBO
We now show how the training procedure we use
to train our permutation model can be derived from
a form of evidence lower bound (ELBO).
Ideally, our permutation model would be a dis-
tribution P(R|x,z)over permutation matrices R
and we would maximize the marginal likelihood,
i.e. marginalizing over all permutations:
P(y|x,z) =/summationdisplayP(R|x,z)P(y|z,R)(16)
where P(y|z,R) =/producttext/summationtextR·1(y=z)with
1being the indicator function. P(y|z,R)returns
1 iff applying the permutation Rtozresults in
y. Unfortunately, computing Eq. (16) exactly is
intractable in general due to the sum over permu-
tation matrices. We instead use techniques from
variational inference and consider the following
evidence lower bound (ELBO):
logP(y|x,z)≥maxElogP(y|z,R)
−KL(Q(R|x,z,y)||P(R|x,z))
(17)
where Q(R|x,z,y)is an approximate variational
posterior. We now relax the restriction that
P(R|x,z)places non-zero mass only on permu-
tation matrices and use the following definition of
P(R|x,z):
P(R= 1|x,z) =U(s)
where U(s)is the solution to Eq. (6) with added
entropy regularization.
It turns out, in our case, we can easily construct a
variational posterior Qthat has zero reconstruction
loss (the first term on the right side in Eq. (17)):
we can choose any Q(R|x,z,y)∈ Q(y,z)where
Q(y,z)is the set of bistochastic matrices such that
Q(R|x,z,y)= 0 iffz̸=y. To see that this
gives zero reconstruction error, consider position j
in the output: The probability mass is distributed
across precisely those positions iinzwhere the
right kind of token lives. In other words, any align-
ment with non-zero probability will reconstruct the
output token at position j.
Therefore we can use the following lower bound
to the log-likelihood:
logP(y|x,z)≥ (18)
−minKL(Q(R|x,z,y)||P(R|x,z))
During training, we need to compute the gradient of
Eq. (18). By Danskin’s theorem (Danskin, 1967),
this is:
−∇KL(Q||P(R|x,z)) (19)
where Q∈ Q(y,z)is the minimizer of Eq. (18).
Note that Qcan equivalently be characterised as
ˆU(Eq. (8)).
In practice, we also add −KL(ˆW|W(s))to our
objective in Eq. (18) to speed up convergence; this
does not change the fact that we use a lower bound.
B Additional results and analysis
Okapi. In Fig. 5 we show the accuracy of our
model on the document domain in comparison with
previous work by number of conjuncts in the logi-
cal form.
Permutation baseline. A simpler approach
for predicting a permutation of the output zfrom14501the multiset tagging is to use a seq2seq model. In
order to compare our approach to such a baseline,
we concatenate the original input xwith a separa-
tor token and z. We then feed this as input to a
BART-base model which is trained to predict the
output sequence y. At inference time, we use beam
search and enforce the output to be a permutation
of the input. As detailed in Table 6, this approach
works well in-distribution and it also shows a small
improvement over finetuning BART directly on the
length split of ATIS. However, it does not perform
as well as our approach. On COGS, our model
outperforms the permutation baseline by an even
bigger margin. Unseen variable symbols could be a
challenge for BART on COGS which might explain
part of the gap in performance.
This approach towards predicting permutations
is similar to that of Cazzaro et al. (2023) except that
they do not constrain the beam search to permuta-
tions. We found that not constraining the output
to be a permutation worked worse in the composi-
tional generalization setups.
C Further model details
C.1 Parametrization of permutation model
We do not share parameters between the multiset
tagging model and the permutation model.
Tokens that appear more than once in the same
multiset have the same representation hin Eq. (7).
In order to distinguish them, we concatenate an-
other embedding to h: if the token zis the k-th
instance of its type in its multiset, we concatenate
an embedding for ktoh. For example, in Fig. 1,
z=‘x’and it is the second instance of ‘x’in
its multiset, so we use the embedding for 2.
We found it helpful to make the temperature τ
of the scores for Algorithm 1 dependent on the
number of elements in the permutation, setting τ=
(logn), so that longer sequences have slightly
sharper distributions.
Since the permutation model is designed to
model exactly permutations, during training, zand
ymust have the same elements. This is not guar-
anteed because zis the prediction of the multiset
model which may not have perfect accuracy on the
training data. For simplicity, we disregard instances
where zandydo not have the same elements. In
practice, this leads to a very small loss in training
data for the permutation model.C.2 Lexicon mechanism
The lexicon Lis a lookup table that deterministi-
cally maps an input token xto an output token
L(x), and we modify the distribution for multiset
tagging as follows:
P(z=k|x) =/braceleftigg
P(z=k|x)ifv=L(x)
P(z=k|x) else
where P(z=k|x)is as defined in Eq. (3) and
Lis a special lexicon symbol in the vocabulary.
P(z|x)is a distribution over the multiplicity of
L(x), independent of the identity of L(x). We use
the ‘simple’ lexicon induction method by Akyurek
and Andreas (2021). Unless otherwise specified
during learning, L(x) =xlike in a copy mecha-
nism.
Handling of variables in COGS. For the
COGS dataset, a model has to predict variable sym-
bols. The variables are numbered (0-based) by
the input token that introduced it (e.g. in Fig. 1,
slept, the third token, introduces a variable symbol
x). In order to robustly predict variable symbols
for sentences with unseen length, we use a similar
mechanism as the lexicon look up table: we intro-
duce another special symbol in the vocabulary, Var.
IfVaris predicted with a multiplicity of kati-th
input token, it adds the token xto its multiset k
times.
C.3 Initialization of Multiset Tagging model
If there are lalignments with a posterior probability
of at least χthat an input token iproduces token
v, we add the term λlogP(z≥l|x)to Eq. (4).
λis the hyperparameter determining the strength.
This additional loss is only used during the first g
epochs.
D Datasets and Preprocessing
We show basic statistics about the data we use in Ta-
ble 7. Except for the doubling task, all our datasets
are in English. COGS uses a small fragment of En-
glish generated by a grammar, see Kim and Linzen
(2020) for details.
Doubling task. For the doubling task, we use an
alphabet of size |Σ|= 11 . To generate inputs with
a specific range of lengths, we first draw a length
from the range uniformly at random. The symbols
in the input are also drawn uniformly at random
and then concatenated into a sequence. Examples
of lengths 5 - 10 are used as training, examples of14502
length 11 are used as development data (e.g. for
hyperparameter selection), and examples of length
11 - 20 are used as test data.
D.1 Preprocessing
COGS. Unlike Zheng and Lapata (2022); Qiu
et al. (2022); Drozdov et al. (2022) we do not
apply structural preprocessing to the original
COGS meaning representation and keep the vari-
able symbols: all our preprocessing is local and
aimed at reducing the length of the logical form
(to keep runtimes low). We delete any token
in{",","_","(",")","x",".",";","AND" }as
these do not contribute to the semantics and can be
reconstructed easily in post-processing. The tokens
{"agent", "theme", "recipient", "ccomp",
"xcomp", "nmod", "in", "on", "beside" }are
always preceded by a "."and we merge "."and
any of those tokens into a single token.
Example:
* cookie ( x _ 3 ) ; * table ( x _ 6 )
; lend . agent ( x _ 1 , Dylan ) AND
lend . theme ( x _ 1 , x _ 3 ) AND lend .
recipient ( x _ 1 , x _ 9 ) AND cookie .
nmod . beside ( x _ 3 , x _ 6 ) AND girl
( x _ 9 )
Becomes
* cookie 3 * table 6 lend .agent 1 Dylan
lend .theme 1 3 lend .recipient 1 9 cookie
.nmod .beside 3 6 girl 9
ATIS. We follow the pre-procressing by Linde-
mann et al. (2023) and use the variable-free FunQL
representation as annotated by Guo et al. (2020).
We use spacy 3.0.5 (model en_core_web_sm ) to
tokenize the input.
Okapi. Again, we follow the preprocessing
of Lindemann et al. (2023). We use spacy 3.0.5
(model en_core_web_sm ) to tokenize both the in-
put utterances and the output logical forms.E Details on evaluation metrics
We provide code for all evaluation metrics in our
repository.
Doubling. We use exact match accuracy on the
string.
COGS. For COGS we use exact match accuracy
on the sequence in one evaluation setup. The other
evaluation setup disregards the order of conjuncts:
we first remove the ‘preamble’ (which contains
all the definite descriptions) from the conjunctions.
We count a prediction as correct if the set of definite
descriptions in the preamble matches the set of
definite descriptions in the gold logical form and
the set of clauses in the prediction match the set of
clauses in the gold logical form.
ATIS. We allow for different order of conjuncts
between system output and gold parse in computing
accuracy. We do this by sorting conjuncts before
comparing two trees node by node. This is the
same evaluation metric as used by Lindemann et al.
(2023).
Okapi. We follow Hosseini et al. (2021); Lin-
demann et al. (2023) and disregard the order of
the parameters for computing accuracy. We use a
case-insensitive string comparison.
F Hyperparameters
We use the same hyperparameters for all splits of
a dataset. For our model, we only tune the hy-
perparameters of the multiset tagging model; the
permutation model is fixed, and we use the same
configuration for all tasks where we use RoBERTa.
For model ablations where we use an LSTM in-
stead of RoBERTa, we use the same hyperparame-
ters for Okapi and ATIS, and a smaller model for
the doubling task. These configurations were de-
termined by hand without tuning. For BART, we
use the same hyperparameter as Lindemann et al.
(2023).
We follow the random hyperparameter search
procedure of Lindemann et al. (2023) for the multi-
set tagging models and the LSTM/transformer we
train from scratch: we sample 20 configurations
and evaluate them on the development set. We run
the two best-performing configurations again with
a different random seed and pick the one with the
highest accuracy (comparing the union of the pre-
dicted multisets with the gold multiset). We then
train and evaluate our model with entirely different
random seeds.14503
The chosen hyperparameters along with the
search space are provided in the github repository.
G Number of parameters, computing
infrastructure and runtime
We show the number of parameters in the models
we train in Table 9.
All experiments were run on GeForce GTX 1080
Ti or GeForce GTX 2080 Ti with 12GB RAM and
Intel Xeon Silver or Xeon E5 CPUs.
The runtime of one run contains the time for
training, evaluation on the devset after each epoch
and running the model on the test set. We show
runtimes of the model we train in Table 8. Since
we evaluate on 5 random seeds (10 for COGS due
to high variance of results), our experiments over-
all took around 64 hours of compute time on our
computing infrastructure.14504ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations, at the end of the paper
/squareA2. Did you discuss any potential risks of your work?
No apparent societal risks.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
6, Appendix D
/squareB1. Did you cite the creators of artifacts you used?
6
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The data we create programmatically is likely too simple to be protected by copyright.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
6
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
6, Appendix D
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
6, Appendix D
C/squareDid you run computational experiments?
6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix H14505/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
6, Appendix G, code submission.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
6, Appendix B
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix D
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.14506