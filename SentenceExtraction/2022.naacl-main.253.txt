
Luyao Shi Tanveer F. Syeda-Mahmood Tyler Baldwin
IBM Research, Almaden Research Center, San Jose, CA, USA
luyao.shi@ibm.com {stf, tbaldwin}@us.ibm.com
Abstract
Many clinical informatics tasks that are based
on electronic health records (EHR) need rel-
evant patient cohorts to be selected based on
ﬁndings, symptoms and diseases. Frequently,
these conditions are described in radiology re-
ports which can be retrieved using information
retrieval (IR) methods. The latest of these tech-
niques utilize neural IR models such as BERT
trained on clinical text. However, these meth-
ods still lack semantic understanding of the un-
derlying clinical conditions as well as ruled
out ﬁndings, resulting in poor precision dur-
ing retrieval. In this paper we combine clinical
ﬁnding detection with supervised query match
learning. Speciﬁcally, we use lexicon-driven
concept detection to detect relevant ﬁndings in
sentences. These ﬁndings are used as queries
to train a Sentence-BERT (SBERT) model us-
ing triplet loss on matched and unmatched
query-sentence pairs. We show that the pro-
posed supervised training task remarkably im-
proves the retrieval performance of SBERT.
The trained model generalizes well to unseen
queries and reports from different collections.
1 Introduction
Electronic health record (EHR) retrieval is impor-
tant for clinicians, staff and researchers. The tools
for performing clinically relevant searches could
aid in many use cases such as clinical decision sup-
port (Syeda-Mahmood, 2010), auditing, revenue
cycle management, and cohort selection for clin-
ical studies. Frequently, these searches involve
retrieval of patients based on clinical ﬁndings that
are often captured in unstructured textual reports
such as radiology reports, encounter notes, etc. Un-
like structured query-based lookup of EHR, re-
trieval of unstructured (free-text) EHRs is much
more challenging, requiring a semantic understand-
ing of the underlying clinical conditions present or
absent. Conventional exact or approximate term-
based retrieval methods such as BM25 (Robertsonand Zaragoza, 2009) often perform poorly in re-
sponse to ad-hoc queries (Chamberlin et al., 2020),
as these methods lack the ability of semantic under-
standing of the clinical as well as language context.
With the emergence of deep learning encoding mod-
els, new retrieval methods have emerged with stud-
ies showing BERT-based neural methods outper-
forming BM25 on multiple retrieval benchmarks
(Yilmaz et al., 2019a; Chang et al., 2020; Nogueira
and Cho, 2019; Yilmaz et al., 2019b; Qiao et al.,
2019). The BERT-based retrieval methods can be
classiﬁed into two categories: the cross-attention
(or interaction-based) models (Yilmaz et al., 2019a;
Nogueira and Cho, 2019; Yilmaz et al., 2019b)
and the embedding-based (or representation-based)
models (Chang et al., 2020; Reimers and Gurevych,
2019). While the BERT-style cross-attention mod-
els are very successful, they cannot be directly
applied to large-scale retrieval problems because
computing the similarity score for every possible
query-document pair during inference can be pro-
hibitively expensive. Therefore, they were often
used as a re-ranker after a initial candidate retrieval
round using BM25. The embedding-based meth-
ods can pre-encode the documents, and only the
queries need to be encoded upon retrieval. Re-
trieval can be achieved via approximate nearest-
neighbor search in the embedding space very ef-
ﬁciently (Johnson et al., 2021). In this study, we
focus on the embedding-based retrieval BERT mod-
els. Speciﬁcally, we adopted the sentence-level re-
trieval setting, as studies suggested that the "best"
sentence in a document provides a good proxy for
document relevance (Yilmaz et al., 2019a).
Different pre-training tasks were used to train
the BERT-based models for retrieval. The pre-
training tasks range from masked language mod-
elling (MLM) over unlabeled free-text to super-
vised training on labeled datasets such as STS (Cer
et al., 2017), MS MARCO (Nguyen et al., 2016)
or TREC Microblog track (Lin et al., 2014). How-3457ever, MLM is not tailored for the purpose of in-
formation retrieval (IR), and labeled datasets are
usually small and not easily accessible. Recently,
pre-trained models on biomedical corpora such
as BioClinicalBERT (Alsentzer et al., 2019) and
BioBERT (Lee et al., 2020) can obtain embeddings
with medical-domain-speciﬁc knowledge, but they
were still trained with MLM.
Early studies (Natarajan et al., 2010) showed
that most clinical queries are actually short queries
(e.g. a disease or a syndrome). We found that the
existing BERT models pre-trained with MLM per-
formed poorly on short queries as well as negative
queries (i.e. queries asking for lack of a ﬁnding).
Ideally, if a retrieval system could be trained by
matched and unmatched query-sentence pairs, in
both positive and negated instances, we can expect
a higher precision and recall in retrieval. How-
ever, manually labeling a large dataset is imprac-
tical, particularly for the medical domain where
the number of clinical ﬁndings is very large. Train-
ing neural IR models using weak supervision has
been previously investigated (Dehghani et al., 2017;
MacAvaney et al., 2019). These methods use unsu-
pervised methods (e.g. BM25) or article headings
to provide pseudo labels. However, these pseudo
labels usually are imprecise and do not consider
negative queries. Moreover, the article headings
are not always available.
Motivated by these challenges, we present a
hybrid approach where we combine automated
clinical ﬁnding detection with supervised query-
sentence pair learning. Speciﬁcally, we use an
automatic lexicon-driven concept detection method
to detect relevant positive or negative chest X-ray
(CXR) ﬁndings in sentences. These ﬁndings paired
with the sentences containing them serve as weakly
labeled training data for Sentence-BERT (SBERT)
(Reimers and Gurevych, 2019). The resulting ap-
proach avoids manual annotation and can be scaled
for training on a large number of query-sentence
pairs. We show that the proposed training task re-
markably improves the retrieval performance of
SBERT on datasets with automatic annotations and
human annotations.
2 Methods
2.1 Fine-grained concept extraction
The algorithm for extracting ﬁndings from sen-
tences in reports uses a vocabulary-driven approach.
Speciﬁcally, a domain-speciﬁc CXR ﬁnding lexi-con was used. This lexicon captures the name of
ﬁnding along with its potential variants and syn-
onyms mined from over 200,000 chest radiology
reports. To spot the occurrence of a ﬁnding lexicon
phrase within reports, a string matching algorithm
called the longest common subﬁx (LCF) algorithm
was used. To determine if a core ﬁnding is positive
or negative (e.g. "no pneumothorax"), a two-step
approach that combines language structuring and
vocabulary-based negation detection is used. The
method is reported to be highly accurate ( <3% er-
rors) compared with human labels. More details
are described in (Syeda-Mahmood et al., 2020).
2.2 Labeled data generation
In this paper, we focus on "anatomical ﬁndings"
as well as "disease concepts" as those are the
most commonly searched in EHR (Natarajan et al.,
2010). We use these ﬁnding modiﬁers as surrogates
for queries. For each sentence Sin our data col-
lection, we have a set with Klabeled data entries
I={(S,N,M)}. For each labeled
entry (S,N,M),Mis thei-th ﬁnding for
S, andN=yes|noindicates a positive or ruled
out ﬁnding. By using the ﬁndings as query surro-
gates, we can designate a query Q= (N,M)
paired with S: ifNequals toyes,Qis a
positive query, otherwise Qis a negative query.
For example, (yes, vascular congestion )and
(no, pulmonaryedema )are two queries for the
sentence "lungs: central vascular congestion with-
out overt edema." The actual queries can be more
properly phrased for data augmentation in training,
such as "presence of M" or "Mis observed" for a
positive query and "no evidence of M" or "absence
ofM" for a negative query, where Mis a ﬁnding.
In this study, however, we only consider the simple
form of "M" and "noM" as positive and negative
queries, in both training and evaluation.
Since we labeled all the sentences in our training
dataset extensively with all the ﬁnding types we
summarized, we can create a dictionary using each
unique query Q= (N,M )as the key and the list of
all the sentences that contain that query as the dic-
tionary value. Any sentence in the list is considered
as a matched sentence for that query, whereas other
sentences are considered as unmatched sentences.
2.3 Model
We used SBERT as our retrieval model. MEAN-
pooling was used to derive a ﬁxed size sentence
embedding (for either a query or an EHR sentence).3458We used the triplet objective function (Reimers and
Gurevych, 2019) to train our model. A diagram
of the training objective function is shown in Fig-
ure 1. Given a query q, a matched sentence m
and an unmatched sentence u, the triplet loss tunes
the network such that the distance between the em-
beddings of qandmis smaller than the distance
between the embeddings of qanduby a margin/epsilon1:
max (/bardble−e/bardbl−/bardble−e/bardbl+/epsilon1,0) (1)
wheree,eandeare the sentence embed-
dings forq,mandu, respectively./bardbl·/bardblis a distance
metric. We used the cosine distance and /epsilon1= 0.5.
To improve training, we further used hard-
sampling (HS) to mine the hardest unmatched sen-
tence for the triplet loss within a training batch. To
be speciﬁc, we performed inference within a batch
beforehand to ﬁnd the unmatched sentence with the
highest cosine similarity score (the most confusing
unmatched sentence) for each query. We further
applied mega-batching (MB) (Wieting and Gimpel,
2018) to encourage the model to learn to distin-
guish "harder" unmatched sentences by increasing
the batch size.
At inference, the cosine similarity between the
query embedding and the report sentence embed-
ding is used to determine the level of relevance.
3 Experiments and Results
3.1 Datasets
The experiments in Section 3.1-3.4 were carried
out on two public collections of radiology reports
provided by Indiana University (Demner-Fushman
et al., 2016) and NIH (Wang et al., 2017). Afterpruning for duplicates and applying our labeled
data generation algorithm described in Section 2.2,
a total of 21,612 labeled entries were generated for
the Indiana dataset, which include 10,363 unique
sentences, 200 positive queries and 75 negative
queries. For the NIH dataset, 17,047 labeled entries
were generated, including 9,091 unique sentences,
250 positive queries and 30 negative queries.
3.2 Sensitivity analysis and parameter tuning
We ﬁrst run a sensitivity analysis on the Indiana
dataset (IND) to investigate how much improve-
ment hard-sampling (HS) and mega-batching (MB)
can bring over random-sampling (RS, randomly
select unmatched sentence within a batch) and
normal-batching (NB, size 32). We randomly
split the IND dataset into two halves with non-
overlapping ﬁndings with the constraint that they
should roughly have equal number of labeled en-
tries. After the split, the two sets have 117/44 and
83/31 positive/negative queries, respectively. We
performed a 2-fold cross-validation and reported
the average of the two test results regarding mean
Average Precision (mAP). This allows us to evalu-
ate the model performance on unseen queries. The
evaluation was performed over positive queries
(Pos. Q.), negative queries (Neg. Q.) and all queries
(All Q.) separately.
The results in Table 1 shows that the combination
of HS and MB achieved the best results. Increas-
ing the mega-batching size to 128 resulted the best
performance, but further increasing the batch size
slightly degraded the performance. The remarkable
improvent of SBERT over the baseline BioClinical-
BERT also suggests that the proposed model can
generalize well to unseen queries.3459
3.3 Cross-dataset study
We also trained on the IND dataset and tested on
the unique sentences in the NIH dataset and vice
versa to investigate whether a trained model can
generalize well to a different dataset. The best
SBERT model from Table 1 was used here. We
further included Okapi BM25 ( k=1.5,b=0.75),
the pre-trained BERT (Huggingface "BERT-base-
uncased"), the ﬁne-tuned BERT (trained on the
EHR sentences using MLM, without using our
generated annotations), the BioClinicalBERT and
SBERT pre-trained on MS MARCO dataset for
comparison. More details about these models are
given in the appendix. In addition to mAP, mean
Recall (over all the queries) was also reported,
where Recall was deﬁned as the ratio of the number
of correctly retrieved sentences to the size of the
query’s ground truth list.
Table 2 shows that our ﬁne-tuned SBERT per-
forms very well on the dataset from another col-
lection regarding both mAP and mR, and out-
performed the other BERT/SBERT models by
large margins. The baseline BERT without pre-
training over medical texts obtained the worst re-
sults. The results for BERT (ﬁne-tuned) and Bio-
ClinicalBERT suggest that MLM training over
the texts from the same domain can lead to
some improvements but is still not ideal for di-
rect use of retrieval. SBERT pre-trained on MS
MARCO dataset showed signiﬁcant improvements
over BERT trained with MLM, but lacks domain-
speciﬁc knowledge and shows performance drop on
negative queries. BM25 performs well on positive
queries with performance degradation on negative
queries as well, because negation is not always
explicitly expressed in EHR.
3.4 Embedding separation analysis
Because we have the negation labels, we can also
create opposite-negation queries. For example, the
opposite-negation query for "no opacity" would be
"opacity". Ideally, with a high-precision retrieval
system, for a given sentence, the similarity score
between the matched query and sentence should
be higher than that between the opposite-negation
query and the sentence. We reported (Table 3) the
differences (mean±std) between these two scores
for all the entries in each dataset with all the BERT
embedding-based methods. Our trained SBERT
showed a clear separation in the embedding space.
The distances for the other BERT models are all
around zero with even negative distances, suggest-
ing poor negation awareness.
3.5 Evaluation on human-annotated data
We also evaluated our model on a separate human-
annotated dataset. The radiology reports used in
this section are private anonymized data obtained
from our collaborative partners. HIPPA was fully
enforced and all data were handled according to the
Declaration of Helsinki. All reports were written in
the English language. 206 CT reports and 120 chest
X-ray (CXR) reports were annotated for various
disease ﬁndings on the sentence level by 3 radiolo-
gists using the brat rapid annotation tool (available
athttps://brat.nlplab.org/ ). Majority
voting was used to handle disagreements. This
resulted in 2,990 unique sentences/8 queries for
CT reports and 1,810 unique sentences/18 queries
for CXR reports. Note that the candidate sen-
tences for retrieval also include those sentences
without any our interested disease ﬁndings. For3460
the CT reports, the annotation was based on the
presence or absence of 4 diseases (resulting in 8
queries): thoracic aneurysm, abdominal aneurysm,
lung nodule and pulmonary embolism. The aver-
age number of matched sentences for each query
is 42±33. For the CXR reports, the annotation
was based on 10 diseases (resulting in 18 queries,
as 2 negative queries do not have the correspond-
ing matched sentences): pulmonary embolism,
airspace opacity, lung nodule, emphysema, pneu-
mothorax, abdominal aortic aneurysm, thoracic
aortic aneurysm, rib fracture, scapula fracture and
spine fracture. The average number of matched
sentences for each query is 18 ±16. It it worth not-
ing that some of the diseases are not even used as
queries in the IND/NIH training data, including
thoracic aneurysm, abdominal aneurysm and spine
fracture.
Table 4 shows that our SBERT ﬁne-tuned on
either IND or NIH dataset outperforms the other
compared methods by large margins.
4 Discussion
In this paper we demonstrated that the proposed
supervised pre-training tasks with automated an-
notation can greatly improve the IR performance
of SBERT on short and negative queries. The pro-
posed labeled data generation method can also be
used to train the cross-attention BERT models for
further improvement when computation speed is
not the bottleneck.
We focused on short queries in this study, and
BM25 still performs well on positive queries. The
embedding-based BERT models are expected to
show more advantages over BM25 on complicated
queries that require semantic understanding. Hav-
ing the comprehensive negation and ﬁnding labels
for each sentence also allows us to assemble more
complicated queries that include more than one
ﬁnding, such as " AandB" or "AwithoutC"
whereA,BandCrepresent three different ﬁnd-
ings. These more challenging tasks can be explored
in the future work. The label generation tool canalso be extended to training IR models in domains
other than medical domain, such as ﬁnance, law, or
retail, provided with the corresponding lexicons.
In this study we did not evaluate retrieval on
the report-level because we have the sentence-level
annotations, which enable ﬁne-grained evaluation.
The report-level evaluation can be included in the
future work.
5 Conclusion
In this work we proposed to generate query-
sentence pairs automatically using a CXR lexicon
for training embedding-based BERT models on
the EHR retrieval problem. We showed that the
ﬁne-tuned SBERT obtained a substantial perfor-
mance gain over the other pre-trained models. The
trained model can also generalize well to unseen
queries and data from another source. The pro-
posed method can be especially helpful in training
and evaluating neural IR models in domains with
limited human-labeled data.
References34613462A Appendix: Model training details
Here we provide more details on the models used
in Section 3. We used the Huggingface "BERT-
base-uncased" model (pre-trained on BookCorpus
and English Wikipedia, availabel at: https://
huggingface.co/bert-base-uncased )
as our BERT model for comparison. The BERT
(ﬁne-tuned) model was ﬁne-tuned on the EHR text
(Indiana or NIH dataset) using MLM for 5 epochs
based on the "BERT-base-uncased" model. The
pre-trained BioClinicalBERT (Alsentzer et al.,
2019) (availabel at: https://github.com/
EmilyAlsentzer/clinicalBERT ) was
initialized with BioBERT (Lee et al., 2020) and
ﬁne-tuned on clinical notes.
Our SBERT model was initialized with the Bio-
ClinicalBERT. We ﬁne-tuned SBERT using the
triplet loss for 10 epochs for all datasets in this
study. We used AdamW optimizer with the learn-
ing rate 2e-5, weight decay 0.01 and a linear learn-
ing rate warm-up of 100 steps.
The SBERT model used as comparison was
pre-trained on 500K (query, answer) pairs from
the MS MARCO dataset. This pre-trained model
(msmarco-bert-base-dot-v5) was one of the recom-
mended sentence embedding models from the ofﬁ-
cial SBERT webpage ( https://www.sbert.
net/docs/pretrained_models.html ).
Among all the pre-trained models, we picked this
one because it is the only pre-trained model based
on "BERT-base" model, to be consistent with all
the other models (all based on "BERT-base") in
our experiments. Since this model was tuned to
be used with dot-product, we used dot-product
to calculate similarity scores only for this model
in the retrieval experiments in Table 2. For all
the other models, cosine-similarity was used to
calculate scores. However, for the embedding
separation analysis in Table 3, cosine-similarity
was used for SBERT (MS MARCO) as well so
that the scale of the similarity scores is comparable
to the others.3463