
Yongjie WangChuan WangRuobing LiHui LinLAIX Inc.Shanghai Key Laboratory of Artificial Intelligence in Learning and Cognitive Science
{yongjie.wang, chuan.wang, ruobing.li, hui.lin}@liulishuo.com
Abstract
In recent years, pre-trained models have be-
come dominant in most natural language pro-
cessing (NLP) tasks. However, in the area of
Automated Essay Scoring (AES), pre-trained
models such as BERT have not been properly
used to outperform other deep learning mod-
els such as LSTM. In this paper, we introduce
a novel multi-scale essay representation for
BERT that can be jointly learned. We also
employ multiple losses and transfer learning
from out-of-domain essays to further improve
the performance. Experiment results show that
our approach derives much benefit from joint
learning of multi-scale essay representation
and obtains almost the state-of-the-art result
among all deep learning models in the ASAP
task. Our multi-scale essay representation also
generalizes well to CommonLit Readability
Prize (CRP) data set, which suggests that the
novel text representation proposed in this paper
may be a new and effective choice for long-text
tasks.
1 Introduction
AES is a valuable task, which can promote the
development of automated assessment and help
teachers reduce the heavy burden of assessment.
With the rise of online education in recent years,
more and more researchers begin to pay attention
to this field.
AES systems typically consist of two modules,
which are essay representation and essay scoring
modules. The essay representation module extracts
features to represent an essay and the essay scoring
module rates the essay with the extracted features.
When a teacher rates an essay, the scores are
often affected by multiple signals from different
granularity levels, such as token level, sentence
level, paragraph level and etc. For example, thefeatures may include the numbers of words, the
essay structure, the master degree of vocabulary
and syntactic complexity, etc. These features come
from different scales of the essay. This inspires
us to extract multi-scale features from the essays
which represent multi-level characteristics of the
essays.
Most of the deep neural networks AES systems
use LSTM or CNN. Some researchers (Uto et al.,
2020; Rodriguez et al., 2019; Mayfield and Black,
2020) attempt to use BERT (Devlin et al., 2019)
in their AES systems but fail to outperform other
deep neural networks methods (Dong et al., 2017;
Tay et al., 2018). We believe previous approaches
using BERT for AES suffer from at least three lim-
itations. First, the pre-trained models are usually
trained on sentence-level, but fail to learn enough
knowledge of essays. Second, the AES training
data is usually quite limited for direct fine-tuning
of the pre-trained models in order to learn better
representation of essays. Last but not least, mean
squared error (MSE) is commonly used in the AES
task as the loss function. However, the distribution
of the sample population and the sorting proper-
ties between samples are also important issues to
be considered when designing the loss functions
as they imitate the psychological process of teach-
ers rating essays. Different optimizations can also
bring diversity to the final overall score distribu-
tion and contribute to the effectiveness of ensemble
learning.
To address the aforementioned issues and limi-
tations, we introduce joint learning of multi-scale
essay representation into the AES task with BERT,
which outperforms the state-of-the-art deep learn-
ing models based on LSTM (Dong et al., 2017;
Tay et al., 2018). We propose to explicitly model
more effective representations by extracting multi-
scale features as well as leveraging the knowledge
learned from numerous sentence data. As the train-
ing data is limited, we also employ transfer learn-3416ing from out-of-domain essays which is inspired
by (Song et al., 2020). To introduce the diversity of
essay scoring distribution, we combine two other
loss functions with MSE. When training our model
with multiple losses and transfer learning using R-
Drop (Liang et al., 2021), we almost achieve the
state-of-the-art result among all deep learning mod-
els. The source code of prediction module with
a trained model for ASAP’s prompt 8 is publicly
available.
In summary, the contribution of this work is as
follows:
•We propose a novel essay scoring approach
to jointly learn multi-scale essay representa-
tion with BERT, which significantly improve
the result compared to traditionally using pre-
trained language models.
•Our method shows significant advantages in
long text tasks and obtains almost the state-of-
the-art result among all deep learning models
in the ASAP task.
•We introduce two new loss functions which
are inspired by the mental process of teacher
rating essays, and employ transfer learn-
ing from out-of-domain essays with R-
Drop (Liang et al., 2021), which further im-
proves the performance for rating essays.
2 Related Work
The dominant approaches in AES can be grouped
into three categories: traditional AES, deep neural
networks AES and pre-training AES.
•Traditional AES usually uses regression
or ranking systems with complicated hand-
crafted features to rate an essay (Larkey, 1998;
Rudner and Liang, 2002; Attali and Burstein,
2006; Yannakoudakis et al., 2011; Chen and
He, 2013; Phandi et al., 2015; Cozma et al.,
2018). These handcrafted features are based
on the prior knowledge of linguists. Therefore
they can achieve good performance even with
small amounts of data.
•Deep Neural Networks AES has made great
progress and achieved comparable results with
traditional AES recently (Taghipour and Ng,
2016; Dong and Zhang, 2016; Dong et al.,2017; Alikaniotis et al., 2016; Wang et al.,
2018; Tay et al., 2018; Farag et al., 2018; Song
et al., 2020; Ridley et al., 2021; Muangkam-
muen and Fukumoto, 2020; Mathias et al.,
2020). While the handcrafted features are
complicated to implement and careful man-
ual design makes these features less portable,
deep neural networks such as LSTM or CNN
can automatically discover and learn com-
plex features of essays, which makes AES an
end-to-end task. Saving much time to design
features, deep neural networks can transfer
well among different AES tasks. By combin-
ing traditional and deep neural network ap-
proaches, AES can even obtain a better result,
which benefits from both representations (Jin
et al., 2018; Dasgupta et al., 2018; Uto et al.,
2020). However, ensemble way still needs
handcrafted features which cost numerous en-
ergy of researchers.
•Pre-training AES uses the pre-trained lan-
guage model as the initial essay representation
module and fine-tune the model on the essay
training set. Though the pre-trained methods
have achieved the state-of-the-art performance
in most NLP tasks, most of them (Uto et al.,
2020; Rodriguez et al., 2019; Mayfield and
Black, 2020) fail to show an advantage over
other deep learning methods (Dong et al.,
2017; Tay et al., 2018) in AES task. As far
as we know, the work from Cao et al. (2020)
and Yang et al. (2020) are the only two pre-
training approaches which surpass the other
deep learning methods. Their improvement
mainly comes from the training optimization.
Cao et al. (2020) employ two self-supervised
tasks and domain adversarial training, while
Yang et al. (2020) combine regression and
ranking to train their model.
3 Approach
3.1 Task Formulation
The AES task is defined as following:
Given an essay with nwords X={x}, we
need to output one score yas a result of measuring
the level of this essay.
Quadratic weighted Kappa (QWK) (Cohen,
1968) metric is commonly used to evaluate AES
systems by researchers, which measures the agree-
ment between the scoring results of two raters.34173.2 Multi-scale Essay Representation
We obtain the multi-scale essay representation
from three scales: token-scale, segment-scale and
document-scale.
Token-scale and Document-scale Input We
apply one pre-trained BERT (Devlin et al., 2019)
model for token-scale and document-scale essay
representations. The BERT tokenizer is used
to split the essay into a token sequence T=
[t, t, ......t], where tis the ith token and nis
the number of the tokens in the essay. The token
we mentioned in this paper all refer to WordPiece,
which is obtained by the subword tokenization algo-
rithm used for BERT. We construct a new sequence
Tfrom Tas following. Lis set to 510, which
is the max sequence length supported by BERT
except the token [CLS]and[SEP ].
The final input representation are the sum of the
token embeddings, the segmentation embeddings
and the position embeddings. A detailed descrip-
tion can be found in the work of BERT (Devlin
et al., 2019).
Document-scale The document-scale represen-
tation is obtained by the [CLS]output of the BERT
model. As the [CLS]output aggregates the whole
sequence representation, it attempts to extract the
essay information from the most global granularity.
Token-scale As the BERT model is pre-trained
by Masked Language Modeling (Devlin et al.,
2019), the sequence outputs can capture the con-
text information to represent each token. An essay
often consists of hundreds of tokens, thus RNN
is not the proper choice to combine all the token
information due to the gradients vanishing prob-
lem. Instead, we utilize a max-pooling operation to
all the sequence outputs and obtain the combined
token-scale essay representation. Specifically, the
max-pooling layer generates a d-dimensional vec-
torW= [w, w, ..., w, ..., w]and the element
wis computed as below:
w=max{h, h, ..., h}
where dis the hidden size of the BERT model.
As we use the pre-trained BERT model bert-base-
uncased, the hidden size dis 768. All the nse-
quence outputs of the BERT model are annotated as
[h, h, ..., h, ..., h], where his ad-dimensionalvector [h, h, ..., h]representing the ith se-
quence output, and his the jth element in h.
Segment-scale Assuming the segment-scale
value set is K= [k, k, ...k, ..., k], where S
is the number of segment scales we want to ex-
plore, and kis the ith segment-scale in K. Given
a token sequence T= [t, t, ......t]for an essay,
we obtain the segment-scale essay representation
corresponding to scale kas follows:
1.We define nas the maximum number of to-
kens corresponding to each essay prompt p.
We truncate the token sequence to ntokens if
the essay length is longer than n, otherwise
we pad [PAD ]to the sequence to reach the
length n.
2.Divide the token sequence into m=⌈n/k⌉
segments and each segment is of length k
except for the last segment, which is similar
to the work of (Mulyar et al., 2019).
3.Input each of the msegment tokens into the
BERT model, and get msegment representa-
tion vectors from the [CLS]output.
4.Use an LSTM model to process the sequence
ofmsegment representations, followed by at-
tention pooling operation on the hidden states
of the LSTM output to obtain the segment-
scale essay representation corresponding to
scale k.
The LSTM cell units process the sequence of
segment representations and generate the hidden
states as follows:
i=σ(Q·s+U·h+b)
f=σ(Q·s+U·h+b)
ˆc=tanh(Q·s+U·h+b)
c=i◦ˆc+f◦c
o=σ(Q·s+U·h+b)
h=o◦tanh(c)
where sis the tth segment representation from
BERT [CLS]output and his the tth hidden state
generated from LSTM. Q,Q,Q,Q,U,U,
UandUare weight matrices, and b,b,b, and
bare bias vectors.
The attention pooling operation we use is similar
to the work of (Dong et al., 2017), which is defined
as follows:
ˆα=tanh(Q·h+b)
α=
o=α·h3418ois the segment-scale essay representation corre-
sponding to the scale k.αis the attention weight
for hidden state h.Q,b,qare the weight ma-
trix, bias and weight vector respectively.
3.3 Model Architecture
The model architecture is depicted in Figure 1.
We apply one BERT model to obtain the
document-scale and token-scale essay representa-
tion. The concatenation of them is input into a
dense regression layer which predicts the score cor-
responding to the document-scale and token-scale.
For each segment-scale kwith number of segments
m, we apply another BERT model to get m CLS
outputs, and apply an LSTM model followed by
an attention layer to get the segment-scale represen-
tation. We input the segment-scale representation
into another dense regression layer to get the score
corresponding to segment-scale k. The final score
is obtained by adding the scores of all Ssegment-
scales and the score of the document-scale and
token-scale, which is illustrated as below:
y=y+y
y=ˆW·o+b
y =ˆW·H +b
H =wW
yis the predicted score corresponding to
segment-scale k.y is the predicted score
corresponding to the document-scale and token-
scale. ˆWandbare weight matrix and bias for
segment-scale respectively. W andb
are weight matrix and bias for document and token-
scales, ois the segment-scale essay representa-
tion with the scale k.wis the document-scale
essay representation. Wis the token-scale essay
representation. H is the concatenation of
document-scale and token-scale essay representa-
tions.
3.4 Loss Function
We use three loss functions to train the model.
MSE measures the average value of square er-
rors between predicted scores and labels, which is
defined as below:
MSE (y,ˆy) =(y−ˆy)
where yandˆyare the predicted score and the
label for the ith essay respectively, Nis the number
of the essays.
Similarity (SIM) measures whether two vectors
are similar or dissimilar by using cosine function.A teacher takes into account the overall level dis-
tribution of all the students when rating an essay.
Following such intuition, we introduce the SIM
loss to the AES task. In each training step, we take
the predicted scores of the essays in the batch as
the predicted vector y, and the labels as the label
vector ˆy. The SIM loss awards the similar vector
pairs to make the model think more about the cor-
relation among the batch of essays. The SIM loss
is defined as below:
SIM (y,ˆy) = 1−cos(y,ˆy)
y= [y, y, ..., y]
ˆy= [ˆy,ˆy, ...,ˆy]
where yandˆyare the predicted score and label
for the ith essay respectively, Nis the number of
the essays.
Margin Ranking (MR) measures the ranking or-
ders for each essay pair in the batch. We intuitively
introduce MR loss because the sorting property be-
tween essays is a key factor to scoring. For each
batch of essays, we first enumerate all the essay
pairs, and then compute the MR loss as follows.
The MR loss attempts to make the model penalize
wrong order.
MR(y,ˆy) =max(0,−r(y−y) +b)
r=1 ˆy>ˆy
-1 ˆy<ˆy
-sgn(y−y) ˆy= ˆy
yandˆyare the predicted score and label for the
ith essay respectively. ˆNis the number of the essay
pairs. bis a hyper parameter, which is set to 0 in
our experiment. For each sample pair (i, j), when
the label ˆyis larger than ˆy, the predicted result
yshould be larger than y, otherwise, the pair
contributes y−yto the loss. When ˆyis equal to
ˆy, the loss is actually |y−y|.
The combined loss is described as below:
Loss(y,ˆy) =αMSE (y,ˆy)+βMR (y,ˆy)+
γSIM (y,ˆy).
α,β,γare weight parameters which are tuned
according to the performance on develop set.
4 Experiment
4.1 Data and Evaluation
ASAP data set is widely used in the AES task,
which contains eight different prompts. A detailed
description can be seen in Table 1. For each prompt,
the WordPiece length indicates the smallest num-
ber which is bigger than the length of 90% of the3419
essays in terms of WordPiece number. We evalu-
ate the scoring performance using QWK on ASAP
data set, which is the official metric in the ASAP
competition. Following previous work, we adopt
5-fold cross validation with 60/20/20 split for train,
develop and test sets.
CRP data set provides 2834 excerpts from sev-
eral time periods and reading ease scores which
range from -3.68 to 1.72. The average length of the
excerpts is 175 and the WordPiece length is 252.
We also use 5-fold cross validation with 60/20/20
split for train, develop and test sets on CRP data
set. As the RMSE metric is used in the CRP com-
petition, we also use it to evaluate our system in
ease score prediction task.4.2 Baseline
The baseline models for comparison are described
as follows.
EASEis the best open-source system that par-
ticipated in the ASAP competition and ranked the
third place among 154 participants. EASE uses
regression techniques with handcrafted features.
Results of EASE with the settings of Support Vec-
tor Regression (SVR) and Bayesian Linear Ridge
Regression (BLRR) are reported in (Phandi et al.,
2015).
CNN+RNN Various deep neural networks
based on CNN and RNN for AES are studied
by (Taghipour and Ng, 2016). They combine CNN
ensembles and LSTM ensembles over 10 runs and
get the best result in their experiment.
Hierarchical LSTM-CNN-Attention (Dong
et al., 2017) builds a hierarchical sentence-
document model, which uses CNN to encode sen-
tences and LSTM to encode texts. The attention
mechanism is used to automatically determine the
relative weights of words and sentences in gener-
ating sentence representations and text represen-
tations respectively. They obtain the state-of-the-
art result among all neural models without pre-
training.3420SKIPFLOW (Tay et al., 2018) proposes to use
SKIPFLOW mechanism to model the relationships
between snapshots of the hidden representations
of an LSTM. The work of (Tay et al., 2018) also
obtains the state-of-the-art result among all neural
models without pre-training.
Dilated LSTM with Reinforcement Learn-
ing(Wang et al., 2018) proposes a method using a
dilated LSTM network in a reinforcement learning
framework. They attempt to directly optimize the
model using the QWK metric which considers the
rating schema.
HA-LSTM+SST+DAT and BERT+SST+DAT
(Cao et al., 2020) propose to use two self-
supervised tasks and a domain adversarial training
technique to optimize their training, which is the
first work to use pre-trained language model to out-
perform LSTM based methods. They experiment
with both hierarchical LSTM model and BERT in
their work, which are HA−LSTM +SST+DAT
andBERT +SST +DAT respectively.
BERT(Yang et al., 2020) combines regres-
sion and ranking to fine-tune BERT model which
also outperforms LSTM based methods and even
obtains the new state-of-the-art.
4.3 Settings
To compare with the baseline models and further
study the effectiveness of multi-scale essay repre-
sentations, losses and transfer learning, we conduct
the following experiments.
Multi-scale Models . These models are opti-
mized with MSE loss, and BERT-DOC repre-
sents essays with document-scale features based
on BERT. BERT-TOK represents essays with
token-scale features based on BERT. BERT-DOC-
TOK represents essays with both document-scale
and token-scale features based on BERT. BERT-
DOC-TOK-SEG represents essays with document-
scale, token-scale, and multiple segment-scale fea-
tures based on BERT. Longformer (Beltagy et al.,
2020) is an extension for transformers with an
attention mechanism that scales linearly with se-
quence length, making it easy to process long doc-
uments. We conduct experiments to show that our
multi-scale features also works with Longformer
and can further improve the performance in long
text tasks. Longformer-DOC-TOK-SEG uses
document-scale, token-scale, and multiple segment-
scale features to represent essays, but based on
Longformer instead of BERT. Longformer-DOCrepresents essays with document-scale features
based on Longformer.
Models with Transfer Learning . To transfer
learn from the out-of-domain essays, we addition-
ally employ a pre-training stage, which is similar
to the work of (Song et al., 2020). In this stage, we
scale all the labels of essays from out-of-domain
data into range 0-1 and pre-train the model on them
with MSE loss. After the pre-training stage, we
continue to fine-tune the model on in-domain es-
says. Tran-BERT-MS has the same modules as
BERT-DOC-TOK-SEG with pre-training on out-
of-domain data. MSmeans multiple scale features.
Models with Multiple Losses . Based on Tran-
BERT-MS model, we explore the performance of
adding multiple loss functions. Tran-BERT-MS-
ML additionally employs MR loss and SIM loss.
MLmeans multiple losses. Tran-BERT-MS-ML-
Rincorporates R-Drop strategy (Liang et al., 2021)
in training based on Tran-BERT-MS-ML model.
For the proposed model architecture which is de-
picted in Figure 1, the BERT model in the left part
are shared by the document-scale and token-scale
essay representations, and the other BERT model in
the right part are shared by all segment-scale essay
representations. We use the "bert-base-uncased"
which includes 12 transformer layers and the hid-
den size is 768. In the training stage, we freeze
all the layers in the BERT models except the last
layer, which is more task related than other lay-
ers. The Longformer model used in our work is
"longformer-base-4096". For the MR loss, we set b
to 0. The weights α,βandγare tuned according to
the performance on develop set. We use Adam op-
timizer (Kingma and Ba, 2015) to fine-tune model
parameters in an end-to-end fashion with learning
rate of 6e-5, β1=0.9, β2=0.999, L2weight decay
of 0.005. The coefficient weight αin R-Drop is
9. We set the batch size to 32. We use dropout
in the training stage and the drop rate is set to 0.1.
We train all the models for 80 epochs, and select
the best model according the performance on the
develop set. We use a greedy search method to find
the best combination of segment scales, which is
shown in detail in Appendix A. Following (Cao
et al., 2020), we perform the significance test for
our models.3421
4.4 Results
Table 2 shows the performance of baseline models
and our proposed models with joint learning of
multi-scale essay representation. Table 3 shows the
results of our model and the state-of-the-art models
on essays in prompt 1, 2 and 8, whose WordPiece
length are longer than 510. We summarize some
findings from the experiment results.
•Our model 12 almost obtains the published
state-of-the-art for neural approaches. For the
prompts 1,2 and 8, whose WordPiece length
are longer than 510, we improve the result
from 0.761 to 0.772. As Longformer is good
at encoding long text, we also use it to encode
essays of prompt 1, 2 and 8 directly but the
performance is poor compared to the meth-
ods in Table 3. The results demonstrate the
effectiveness of the proposed framework for
encoding and scoring essays. We further re-
implement BERTproposed by (Yang et al.,
2020), and our implementation of BERTis
not as well-performing as the published result.
Though (Uto et al., 2020) obtain a much bet-
ter result(QWK 0.801), our method performs
much better than their system with only neu-
ral features(QWK 0.730), which demonstrates
the strong essay encoding ability of our neural
approach.•Compared to the models 4 and 6, our model 11
uses multi-scale features to encode essays in-
stead of LSTM based models, and we use the
same regression loss to optimize the model.
Our model simply changes the representation
way and significantly improves the result from
0.764 to 0.782, which demonstrates the strong
encoding ability armed by multi-scale repre-
sentation for long text. Before that, the con-
ventional way of using BERT can not surpass
the performance of models 4 and 6.
4.5 Further analysis
Multi-scale Representation We further analyze
the effectiveness of employing each scale essay
representation to the joint learning process.
Table 4 and Table 5 show the performance of
our models to represent essays on different fea-
ture scales, which are trained with MSE loss and
without transfer learning. Table 4 shows the perfor-
mance on ASAP data set while Table 5 shows the
performance on CRP data set. The improvement of3422BERT-DOC-TOK-SEG over BERT-DOC, BERT-
TOK, BERT-DOC-TOK are significant ( p<0.0001)
on CRP data set, and are significant ( p<0.0001) in
most cases on ASAP data set. Results on both table
indicate the similar findings.
•Combining the features from document-scale
and token-scale, BERT-DOC-TOK outper-
forms the models BERT-DOC and BERT-
TOK, which only use one scale features. This
demonstrates that our proposed framework
can benefit from multi-scale essay representa-
tion even with only two scales.
•By additionally incorporating multiple
segment-scale features, BERT-DOC-
TOK-SEG performs much better than
BERT-DOC-TOK. This demonstrates the
effectiveness and generalization ability of our
multi-scale essay representation on multiple
tasks.
Reasons for Effectiveness of Multi-scale Rep-
resentation Though the experiment shows the ef-
fectiveness of multi-scale representation, we fur-
ther explore the reason. We could doubt that the ef-
fectiveness comes from supporting long sequences,
not the multi-scale itself. As Longformer is good
at dealing with long texts, we compare the re-
sults between Longformer-DOC and Longformer-
DOC-TOK-SEG. The results of the significance
test show that the improvement of Longformer-
DOC-TOK-SEG over Longformer-DOC are signif-
icant ( p<0.0001) in most cases. Performance of the
two models are shown in Table 6, and we get the
following findings.
•Though Longformer-DOC supports long se-
quences encoding, it performs poor, which
indicates us that supporting long sequence
ability is not enough for a good essay scor-
ing system.
•Longformer-DOC-TOK-SEG outperforms
Longformer-DOC significantly, which
indicates the effectiveness of our model
comes from encoding essays by multi-scalefeatures, not only comes from the ability to
deal with long texts.
These results are consistent with our intuition
that our approach takes into account different level
features of essays and predict the scores more ac-
curately. We consider it caused by that multi-scale
features are not effectively constructed in the rep-
resentation layer of pre-trained model due to the
lack of data for fine-tuning in the AES task. There-
fore, we need to explicitly model the multi-scale
information of the essay data and combine it with
the powerful linguistic knowledge of pre-trained
model.
Transfer Learning with Multiple Losses and
R-Drop We further explore the effectiveness of pre-
training with adding multiple loss functions and
employing R-Drop. As is shown in table 7, by in-
corporating the pre-training stage which learns the
knowledge from out-of-domain data, Tran-BERT-
MS model improves the result from 0.782 to 0.788
compared to BERT-DOC-TOK-SEG model. The
model Tran-BERT-MS-ML which jointly learns
with multiple loss functions further improves the
performance from 0.788 to 0.790. We consider it
due to the reason that MR brings ranking informa-
tion and SIM takes into account the overall score
distribution information. Diverse losses bring dif-
ferent but positive influence on the optimization
direction and act as an ensembler. By employing R-
Drop, Tran-BERT-MS-ML-R improves the QWK
slightly, which comes from the fact that R-Drop
plays a regularization role.
5 Conclusion and Future Work
In this paper, we propose a novel multi-scale es-
say representation approach based on pre-trained
language model, and employ multiple losses and
transfer learning for AES task. We almost obtain
the state-of-the-art result among deep learning mod-
els. In addition, we show multi-scale representation
has a significant advantage when dealing with long
texts.3423One of the future directions could be exploring
soft multi-scale representation. Introducing linguis-
tic knowledge to segment at a more reasonable
scale may bring further improvement.
References3424
A Appendix
All the segment-scales we explore range from 10 to
190. The interval between two neighbor scales is 20.
As the combination number of all segment-scales
is exponential, we use a greedy search method to
find the best combination.
1.Initialize the segment-scale value set Ras the
document-scale and token-scale.
2.Experiment the combination of each segment-
scale with the token-scale and document-scale
essay representation, and compute the average
QWK on develop set for all segment-scales,
which is denoted as QWK. The scale with
higher QWK compared to QWKis added
to the candidate scale list Land the scales in
Lare sorted according to their QWK values
from large to small.
3.For each ifrom 1 to |L|, we perform ex-
periments on the combination of the first i
segment-scales in Lwith the token-scale and
document-scale. The combination segment-
scales with the best performance on develop
set are added to the segment-scale value set R3425