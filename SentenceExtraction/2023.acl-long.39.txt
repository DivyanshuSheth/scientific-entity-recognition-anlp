Layer - wise Fusion with Modality Independence Modeling for Multi - modal Emotion Recognition Jun Sun1 , Shoukang Han , Yu - Ping Ruan , Xiaoning Zhang , Yulong Liu , Yuxin Huang , Shu - Kai Zheng , Taihao Li2∗ Institute of Artificial Intelligence , Zhejiang Lab , Hangzhou , China 1sunjun16sj@gmail.com , 2lith@zhejianglab.com Abstract a broad range of applications , including but not limited to emotional support ( Tu et al . , 2022 ; Liu et al . , 2021 ) , human - computer interaction ( Chowdary et al . , 2021 ) and healthcare surveillance ( Dhuheir et al . , 2021 ) . Henceforth , emotion recognition has attracted increasing attention from both research community and industry in recent years ( Hu et al . , 2021a ; Shen et al . , 2021 ) . Multi - modal emotion recognition has gained increasing attention in recent years due to its widespread applications and the advances in multi - modal learning approaches . However , previous studies primarily focus on developing models the unification of multiple modalities . In this paper , we propose that maintaining modality independence is beneficial for the model performance . According to this principle , we construct a dataset , and devise a multimodal transformer model . The new dataset , CHinese Emotion Recognition dataset with Modality - wise Annotations , abbreviated as CHERMA , provides uni - modal labels for each individual modality , and multi - modal labels for all modalities jointly observed . The model consists of uni - modal transformer modules that learn representations for each modality , and a multi - modal transformer module that fuses all modalities . All the modules are supervised by their corresponding labels separately , and the forward information flow is uni - directionally from the uni - modal modules to the multiThe supervision strategy modal module . and the model architecture guarantee each individual modality learns its representation independently , and meanwhile the multiinformation . modal module aggregates all Extensive empirical results demonstrate that our proposed scheme outperforms state - of - theart alternatives , corroborating the importance of modality independence in multi - modal emotion recognition . The dataset and codes are availabel at https://github.com/ sunjunaimer / LFMIM . that exploit The early works perform emotion recognition primarily with a single modality ( Mehendale , 2020 ; Alvarez - Gonzalez et al . , 2021 ; Schuller et al . , 2010 ) , e.g. , vision , text , audio and so on . Recent multi - modal approaches have showcased more appealing performance than their uni - modal counterparts ( Hu et al . , 2021b ; Zhao et al . , 2022 ) . However , most existing literature on multimodal learning overemphasizes the combination of different modalities without fully respecting modality independence , which might be harmful to the model . In the sequel , we illustrate this through the lens of datasets and model design . Datasets Current datasets for multi - modal emotion recognition are usually annotated with the joint observation of all modalities , resulting in shared labels for all modalities ( Zadeh et al . , 2016 , 2018 ; Busso et al . , 2008 ; Poria et al . , 2019 ; Li et al . , 2017b ) . This leads to the fact that all modalities in the multi - modal model are supervised by the same common labels , which reduces the modality diversity and might even mislead some In practice , it is modalities ( Yu et al . , 2020 ) . anticipated that inconsistent labels will be attained if we annotate different modalities separately . In this circumstance , in order to learn diverse and modality - specific representations , the modules for different modalities are expected to be trained with their own labels rather than the common labels . 1 Introduction The goal of human emotion recognition is to automatically detect or categorize the emotional states of human according to some inputs . Nowadays , emotion recognition can be found in Model design The emerging transformer has contributed to many success stories in natural language processing and computer vision ( Devlin et al . , 2019 ; Dosovitskiy et al . , 2020 ) . Naturally , it ∗Corresponding author is introduced to the field of multi - modal learning thanks to its versatility in dealing with sequences of different forms . Multi - modal transformer ( MulT ) is proposed in(Tsai et al . , 2019 ) , which adopts cross - modal attention to fuse any pair of modalities , and then incorporates all the information . The drawback of MulT is that it has a complexity of A2 n in terms of the number of cross - modal transformer blocks ( n is the number of modalities ) . To address the complexity issue , progressive modality reinforcement ( PMR ) and multimodal bottleneck transformer ( MBT ) which scale linearly with the number of modalities are proposed in ( Lv et al . , 2021 ) and ( Nagrani et al . , 2021 ) , respectively . PMR and MBT devise a message hub which draws information from the uni - modal blocks , performs fusion , and returns the fused common information to the uni - modal blocks . It can be concluded that , both MulT and the message hub based models reinforce each modality with the information from other modalities . This can lead to the problem that the model might rely heavily on some modalities , leaving other modalities under - trained . The reason is that the dominated modalities can cheat by peeping at the well - learned modalities , and hence becomes " lazy " in their own learning process . process their own information independently , and are supervised by the corresponding unimodal labels ; the multi - modal transformer fuses information from the uni - modal transformers layer by layer , and is supervised by the multimodal labels . The forward information flow in the model is uni - directionally from the unimodal modules to the multi - modal module . The supervision strategy and the uni - direction information flow promote modality independence , which reduces mutual information and increases complementary information across modalities ( as Figure 2(b ) in Section 4 illustrates ) . Therefore , the overall effective information for the final emotion recognition task aggregated by the multimodal module can be maximized . The proposed model features Layer - wise Fusion with Modality Independence Modeling , termed LFMIM . In summary , the contributions of this paper are mainly threefold . • A new dataset is built for multi - modal emotion recognition , of which the modalities are annotated separately . Apart from multi - modal emotion recognition , the dataset supports the research for the modality ( label ) inconsistency problem in multi - modal learning . • A model that encourages modality independence is proposed , and it is trained with uni - modal labels and multi - modal labels simultaneously . The model leads to more diverse representations , and therefore captures more complementary clues from different modalities . With the above observations of prior datasets and models for multi - modal emotion recognition , it is clear that existing studies primarily focus on establishing the dependency between modalities and capturing combined multi - modal information for the final task . Different modalities are coupled from both the labels and the model structure , and the resultant representations of different modalities share rich common information and lack diversity . However , it has been observed that more differentiated information from modalities facilitates to improve the complementarity between the modalities ( Yu et al . , 2020 ; Qu et al . , 2021 ) . • The proposed model demonstrates substantial improvement over existing competing models . The results shed light on the future research on the balance between modality dependence and independence in multi - modal learning . 2 Related Works In the light of the limitations of current datasets and fusion models , in this work , we construct a new dataset and propose a transformer model for multi - modal emotion recognition . Each sample in our dataset is annotated with three uni - modal labels corresponding to three modalities — text , audio and vision , and a multi - modal label for all modalities jointly observed . The proposed model employs three uni - modal transformer blocks as the backbones for the three individual modalities and one multi - modal transformer block for multi - modal information fusion . The uni - modal transformers There are a large volume of relevant works on multimodal emotion recognition , for which interested readers can refer to survey papers ( Siddiqui et al . , 2022 ; Ahmed et al . , 2023 ) and references therein . In this section , we only cover the most related works , corresponding to the datasets and multimodal fusion models in the following . 2.1 Datasets Popular datasets emotion recognition or sentiment analysis include CMUfor multi - modal ( Zadeh et al . , 2016 ) , CMU - MOSEI MOSI ( Zadeh et al . , 2018 ) , IEMOCAP ( Busso et al . , 2008 ) , MELD ( Poria et al . , 2019 ) , CHEAVD ( Li et al . , 2017b ) , CH - SIMS ( Yu et al . , 2020 ) , and CH - SIMS_v2 ( Liu et al . , 2022 ) . Most previous datasets annotate the samples with the same labels for all modalities . It is noteworthy that the two Chinese datasets , CH - SIMS and CH - SIMS_v2 , are currently the only datasets that conduct annotations for each modality independently . However , these two datasets are for sentiment analysis , and are labeled with polarized labels , ( weakly ) positive , ( weakly ) negative , and neutral . To the best of our knowledge , our dataset CHERMA is the first one that is targeted for multi - modal emotion recognition , and has modality - wise annotations . Item Number of utterances Average length of utterance ( second ) Minimum number of words per utterance Maximum number of words per utterance Male Female Preteen Teen Middle - age Old Quantity 28,717 4.6 4 63 11,207 17,510 288 18,500 8,522 1,407 Table 1 : Statistics of dataset CHERMA . across unaligned modalities . Due to its promising performance , this paper also leverages multi - modal transformer with layer - wise fusion for our emotion recognition task . 3 Dataset Description 2.2 Multi - modal fusion models In this section , we give a detailed introduction to the new dataset — CHERMA . We will present how the data is collected and annotated , the characteristics of the data , and the pre - processing of the data for model training . At the core of multi - modal emotion recognition is the modality fusion strategy . TFN ( Zadeh et al . , 2017 ) integrates the multi - modality information via calculating the outer product of modality embeddings . Unfortunately , the computation and memory required grow exponentially with the number of modalities , which is addressed by the work of LMF ( Liu and Shen , 2018 ) with low rank approximation . From the perspective of model structure , the previous fusion strategies are usually classified into early fusion and late fusion . Early fusion ( Lazaridou et al . , 2015 ; Williams et al . , 2018 ) simply concatenates the low - level features of all the modalities , and feeds the joint feature to the model . Early fusion can suffer from the problem of data sparseness ( Wu et al . , 2014 ) . Late fusion ( Liu et al . , 2014 ; Nguyen et al . , 2018 ; Yu et al . , 2020 ) concatenates the high - level features ( some studies also refer this to model - level fusion ( Chen and Jin , 2016 ) ) or decisions separately obtained from individual modalities , which is weak in establishing fine - grained correspondence across modalities . Before introducing the data , we give the definitions of some notations . Let t , a , v represent the three modalities — text , audio , and vision , respectively ; let m denote the joint of the three modalities . Denote by Xu ∈ RTu×du for u ∈ { t , a , v } , the feature sequence of the corresponding modality , where Tu and du are the sequence length and the feature dimension , respectively . Associated with each feature sequence is its unimodal labels and multi - modal label { yu|u ∈ { t , a , v , m } } . training dataset , we use ( { X n u } u∈{t , a , v , m } ) for n ∈ { 1 , 2 , · · · , N } to represent the n - th sample , where N denotes the total number of samples . In the rest of the paper , we sometimes drop the index n for brevity when no confusion occurs . For our u } u∈{t , a , v } , { yn 3.1 Data acquisition and annotations In order to cover as many scenarios as possible , our data is acquired from various sources , including 148 TV series , 7 variety shows , and 2 movies . The language of the video is Chinese , yet it can be translated to other languages for broader applications . The video is split into utterances with Adobe Premiere Pro . Only the utterances where there is a single person speaking and the speaker ’s face appears clearly are selected as our samples . Compared with the concatenation methods , multi - modal transformer is a more powerful tool that is capable of capturing the intra - modal and cross - modal dependency(Poria et al . , 2017 ; Lian et al . , 2021 ) . Recent transformer - based works ( Tsai et al . , 2019 ; Lv et al . , 2021 ; Nagrani et al . , 2021 ) can be regarded as layer - wise fusion , to differentiate them from early and late fusion approaches . Layer - wise fusion carries out feature fusion layer by layer from low level to high level , which can capture fine - grained correlation In total , 28 , 717 utterances are rounded up , of which the total length is 2 , 213.84 minutes . ( a ) ( b ) ( c ) Figure 1 : ( a ) The distributions of uni - modal labels and multi - modal labels . ( b ) Overall modality inconsistency . ( c ) Inconsistency between uni - modal labels and multi - modal labels . Table 1 reports the statistics of dataset CHERMA , including the information of the utterance samples , the gender and age distributions of the speakers in the video . The scenarios span household , hospital , restaurant , office , outdoor , telephone conversation , the acquired data is and so on . representative and close to real - world scenarios , and is therefore of practical value . meanwhile the samples of ambiguity are discarded . After the annotations , all the samples are shuffled , and are split into training , validation and test datasets with ratio 6:2:2 . 3.2 Label inconsistency In a word , Upon finishing the annotations , we explore the dataset by simple statistical analysis . Figure 1(a ) shows the distributions of the uni - modal labels and the multi - modal labels . We have two observations : 1 ) There are a large number of samples , of which the four labels are not identical to each other ; 2 ) With single modality , some emotions can not be identified and possibly be recognized as neutrality ; while using multi - modal information can infer more emotions . Following the convention , we categorize the samples into Ekman ’s six basic emotions ( Ekman , 1992 ) plus emotion neutrality , i.e. , happiness , sadness , fear , anger , surprise , disgust and neutrality . Each sample is annotated with three uni - modal labels and a multi - modal label . All the recruited annotators have experience in emotion annotations . Moreover , they are required to receive training for our annotation task and pass an examination before carrying out annotations . For the uni - modal annotations , the annotators are shown corresponding uni - modal information . While for multi - modal annotations , all the modalities are available ; that is , the videos are displayed in their original form . Each label is determined as a result of the following majority voting process . For each labeling , the feature , unimodal or multi - modal , is first assigned to three annotators . Each annotator gives it a unique label If the labeling result is 3 : 0 , independently . consensus is reached and the label is determined accordingly ; if the result is 1 : 1 : 1 , this sample will be discarded because of the disagreement ; otherwise , if the result is 2 : 1 , then one more annotator will join . In this case , if the final result is 3 : 1 , the label obtained ; otherwise , 2 : 2 or 2 : 1 : 1 means the sample will be discarded . Considering the limited labor , the above annotating scheme ensures the reliability of the labels in that 3 annotators out of 3 or 4 agree on each label , and To quantify the label inconsistency , we define the overall modality inconsistency between any two modalities u1 , u2 ∈ { t , a , v , m } as follows : ( cid:80)N n=1 1yn u1 N ̸=yn u2 Incon(u1 , u2 ) : = , where 1x = 1 , if x is true ; 1x = 0 , otherwise . of modality u Define with multi - modality m for any label y ∈ { happiness , sadness , fear , anger , surprise , disgust , and neutrality } as follows : the inconsistency ( cid:80)N n=1 1yn n=1 1yn u ̸=y Incon(u , m ; y ) : = . ( cid:80)N m = y reports which the overall modality Figure 1(b ) inconsistency , significant — the inconsistency between any pair of modalities exceeds 0.3 . The inconsistency between unimodality and multi - modality is less than that between any two uni - modalities . This is reasonable because the multi - modal label which is obtained is with all modality information can be regarded as a weighted average of three uni - modal labels . If the multi - modal labels are regarded as the ground - truth , a conclusion can be drawn that some modalities are from Figure 1(c ) better at inferring some emotions than other modalities . It is shown that audio performs well in identifying sadness , anger and neutrality . Vision is good at recognizing happiness , sadness and anger . In comparison , text shows more balanced performance among all emotions . Figure 2 : ( a ) The model architecture . The blocks with colors green , orange , pink and purple represent the text , audio , vision and multi - modal blocks , respectively ; symbols ⊕ and C⃝ denote summation and concatenation operations , ( b ) The overall useful information ( represented by the area covered by the three circles ) increases with modality dependence reduced ; each circle represents the information of the corresponding modality . 3.3 Data pre - processing In this subsection , we explain how the raw data is pre - processed for model training . The original data of the three - modalities will be converted to feature sequences with the following methods . Text : We pass the texts to pre - trained Chinese to obtain al . , 2021 ) BERT - base contextualized word embeddings . Since the maximum number of words in all the texts is 78 , all texts that have fewer words are padded to length 78 . With CLS and SEP tokens prepended and appended to each text , respectively , the input of BERT is of length 80 . Finally , each text modality information is represented by a sequence of length 80 and dimension 768 . Audio : The audio is sampled at frequency 16kHz with receptive field 25ms and frame shift 20ms . Then the extracted frame - level feature is input to pre - trained wav2vec ( Zhang et al . , 2022 ) , generating a feature sequence of dimension 768 . The length of the sequence corresponds to the number of the audio frames which depends on the length of the raw audio . Vision : The video is first processed with MTCNN ( Zhang et al . , 2016 ) to obtain aligned face and each frame is cropped to size of 224 × 224 . For each video utterance , we partition it evenly into 8 segmentations , and then randomly sample 8 frames from each segmentation , resulting in a 64 - frame vision sequence . Each frame is then fed to a pre - trained Resnet 18 ( trained with RAF - DB ( Li et al . , 2017a ) ) , which outputs a feature sequence of length 64 and dimension 512 . respectively . et ( Cui transformer . Each uni - modal transformer processes its corresponding modality independently ; while the multi - modal transformer relies on all the unimodal transformers . To be specific , the input of layer l + 1 of the multi - modal transformer comes from the output of its l - th layer and the outputs of l - th layer of all three uni - modal transformers . Each uni - modal module are independent from each other , and yields its own label prediction . 4.2 The uni - modal modules the modalities are The input features of all of The module for each individual modality adopts the same simple structure , mainly including a uni - modal transformer with L multi - head self attention layers . As Figure 2(a ) illustrates , the feature sequence , Xu , u ∈ { t , a , v } first goes through a 1D convolutional layer to unify the feature dimension for the following concatenation ; next , positional embedding ( PE ) is added , yielding the input sequence of the uni - modal transformer , Z1 u , u ∈ { t , a , v } . Then , the sequence is processed by the corresponding uni - modal transformer , and the input and output of the l - th transformer layer are Zl u , respectively , u ∈ { t , a , v } and l ∈ { 1 , 2 , · · · , L − 1 } . After the transformer block , a pooling layer reduces the output sequence into a feature vector . Subsequently , on the top is an MLP followed by a softmax layer , which gives the predicted label ˆyu , u ∈ { t , a , v } . It is obvious that each uni - modal module does not depend on the the same sequence form . u and Zl+1 4 The Proposed Model 4.1 Model overview As visualized in Figure 2 , the proposed model , LFMIM , consists of two main components , three transformers and one multi - modal uni - modal information from other modalities in the forward pass . It should be clarified that albeit we advocate modality independence , we do not oppose modality reinforcement for each other . In this work , we only investigate the independence side to unveil and highlight its importance . For more general multimodal learning , the two sides should be carefully balanced , which deserves further investigation . 4.3 The multi - modal module The multi - modal module is a feature extractor which draws three modalities from uni - modal transformers and fuses them layer by layer . Specifically , we define a learnable multi - modal FEature EXtraction token , FEX , to extract and summarize useful information from all modalities . The input of the l - th layer of the multi - modal m = [ FEXl ; ˙Zl transformer is Zl v ] , and m = [ FEXl+1 ; ¯Zl+1 the output is ¯Zl+1 ; ¯Zl+1 ] . v t ˙Zl+1 Furthermore , the modality independence is relative to existing layer - wise fusion approaches which couple the modalities with the same labels and modality interactions in the forward propagation . Actually , in our model , through backward propagation the multi - modal labels can also take effect in supervising uni - modal modules . To be more precise , our approach reduces modality dependence , but does not completely eliminate the indirect interactions across modalities . t ; ˙Zl a ; ˙Zl ; ¯Zl+1 a u , ∀u ∈ { t , a , v } are updated as follows : u Zl+1 where αl+1 u , u ∈ { t , a , v } and l ∈ { 0 , 1 , 2 , · · · , L − 1 } are learnable parameters ; ¯Z1 u , ∀u ∈ { t , a , v } are all - zero matrices with proper size . After the multi - modal transformer block , the following structure is the same as the uni - modal modules as introduced in last subsection . The final label prediction of the multi - modal module is ˆym . As shown in Figure 2(a ) , in the forward pass , the multi - modal module absorbs information from the uni - modal modules layer by layer , and does not return its information to the uni - modal modules . ˙Zl+1 ¯Zl+1 u , u = αl+1 and ¯αl+1 u + ¯αl+1 u u 5 Experiments and Analysis In this section , we first compare our proposed model with typical benchmark models to validate the effectiveness of our model . Then we perform ablation studies to analyze the proposed model , and demonstrate the differences between our model and its compared counterparts . 5.1 Comparisons with baseline models 5.1.1 Baseline models We compare our proposed model , LFMIM , with 6 typical baseline models : tensor fusion network ( TFN ) ( Zadeh et al . , 2017 ) , low - rank Multimodal ( Liu and Shen , 2018 ) , fusion ( LMF ) early fusion transformer ( EF - transformer ) , Late fusion transformer ( LF - transformer ) , multi - modal transformer ( MulT ) ( Tsai et al . , 2019 ) , and progressive modality reinforcement ( PMR ) ( Lv et al . , 2021 ) . Note that for early fusion and late fusion methods , we use more powerful transformer models instead of the models in ( Williams et al . , 2018 ) and ( Yu et al . , 2020 ) for the sake of fairness . We adapt the original PMR ( introduced in the introduction section ) to be trained with uni - modal labels and multi - modal labels as our model . 4.4 Optimization objective With the aforementioned model , our training task boils down to the optimization problem below . N ( cid:88 ) 1 N ( cid:88 ) βuL(yn u , ˆyn min u ) , n=1 u∈{t , a , v , m } where L ( · , · ) is the cross - entropy loss function ; βu , u ∈ { t , a , v , m } are the weight parameters that balance the loss among different modalities . To sum up , following the principle of maintaining modality independence , our approach for different utilizes modalities , and bans direct communications across individual modalities . In this way , it is expected that each modality can fully explore and exploit relying on other modalities . Hopefully , as illustrated in Figure 2(b ) , by aggregating more distinctive uni - modal representations with less mutual information and more complementary information , the overall useful information summarized by the multi - modal module can be maximized . separate supervisions Implementation details 5.1.2 To concatenate the features of the three modalities , we utilize 1D convolutional layers to convert them into 128 - dimensional feature sequence . For the audio feature which is of varying length , we fix the length to be 100 . If the original length is over 100 , we uniformly sample 100 feature vectors ; otherwise , we pad it with zero vectors . itself without ( a ) ( b ) Figure 3 : ( a ) The training loss and test loss of each modality during training . ( b ) The overall emotion recognition accuracy of each modality on training dataset and test dataset . ( c ) The test accuracy of different models . ( c ) Emotion TFN LMF EFT LFT MulT PMR LFMIM happiness 74.91 74.52 74.98 75.07 76.18 75.68 76.6 sadness 75.56 75.83 76.88 76.29 76.88 76.46 77.83 fear 66.15 66.73 67.32 66.80 67.36 67.97 69.44 anger 74.41 74.55 74.85 74.88 74.85 75.43 75.32 surprise 66.29 65.08 66.73 66.67 68.18 67.37 69.83 disgust 43.34 45.70 47.48 47.74 46.96 48.93 50.20 neutrality 65.60 65.64 64.60 65.97 65.26 66.59 68.24 overall 68.37 68.23 68.72 69.05 69.24 69.53 70.54 Table 2 : Performance ( in terms of F1 score ) comparison with baseline models . and vision modality the worst ( with test accuracy 54.60 % ) . Model MLF - DNN MLMF MTFN LFMIM Acc-2 Acc-3 Acc-5 38.03 69.06 82.28 37.33 67.70 82.32 37.20 69.02 82.45 48.36 71.33 83.37 F1 score 82.52 82.66 82.56 83.71 Figure 3(c ) compares the test accuracy curve of LFMIM and other baseline models — the accuracy of LFMIM surpasses that of all the others . It is noticed that PMR tends to overfit , which might be attributed to the fact that it employs a complicated model with 6 transformer blocks . Table 2 reports the detailed performance of the models , i.e. , the overall and emotion - wise F1 scores . It is shown that LFMIM outperforms the competing models by a significant margin in overall accuracy ( i.e. , the overall F1 score ) and in all emotions except emotion anger ( slightly outperformed by PMR ) . Table 3 : Results on dataset CH - SIMS . The transformer blocks in LFMIM are all comprised of 4 multi - head self attention ( MHSA ) layers , where each MHSA is with 8 heads . The optimizer utilized is SGD , and Lambda learning rate schedule is adopted . The initial learning rate is 0.005 , obtained with grid search . The weight coefficients in the objective are set as βu = 1 , ∀u ∈ { t , a , v , m } . The reported results in the following are the average of five repeated experiments with different seeds . 5.1.4 Results on dataset CH - SIMS In this subsection , we conduct experiments with the CH - SIMS dataset which is annotated for each modality with sentiment labels : negative , weakly negative , neutral , weakly positive , positively . We compare LFMIM with MLF - DNN , MLMF and MTFN , of which the results are from the reference ( Yu et al . , 2020 ) , as shown in Table 3 . Acc - k ( k ∈ { 2 , 3 , 5 } ) represents the accuracy for classification with k classes ( for binary classification , all labels reduce to negative and positive ; for 3 - class classification , labels are negative , neutral and positive ) , and the F1 - score pertains to binary classification . The results in Table 3 show that LFMIM significantly outperforms the previous models , and LFMIN achieves a remarkable 5.1.3 Performance comparisons As our model design philosophy advocates the independence of different modalities . Each modality module is associated with a training loss and a test loss that reflect how well this modality learns for the task . As illustrated in Figures 3(a ) and 3(b ) , all the losses and the accurac of LFMIM converge , yet reach different levels . Moreover , the gap between training loss ( resp . accuracy ) and test loss ( resp . accuracy ) exhibits significant variation with modalities . These observations mirror that the modality diversity does exist and have significant impact on emotion recognition ; that is , audio modality performs best ( with test accuracy 70.37 % ) ( a ) ( b ) Figure 4 : ( a ) The test loss of each modality during training . ( b ) The overall test accuracy of each modality . ( c ) The standard deviation of F1 score over three modalities . ( c ) Emotion happiness 66.14 63.08 78.91 75.68 75.36 66.10 61.65 74.11 76.60 sadness 67.94 79.05 70.03 76.46 76.77 63.60 77.59 53.55 77.83 fear 61.72 55.04 57.62 67.97 68.51 56.62 54.05 14.53 69.44 anger 72.06 77.24 73.15 75.43 75.10 68.85 76.04 56.98 75.32 surprise 68.06 45.15 16.30 67.37 68.76 65.90 40.41 13.68 69.83 disgust 37.31 36.22 16.59 48.93 49.59 33.42 34.31 12.81 50.20 neutrality 71.00 75.61 64.54 66.59 67.09 69.09 75.13 54.14 68.24 overall 67.39 71.52 67.08 69.53 69.79 64.61 70.37 54.60 70.54 t a v m LFMIM - ML m t a v m PMR LFMIM Table 4 : Performance ( in terms of F1 score ) comparison from the perspective of modality . improvement of 10 percentage points in terms of Acc-5 . modality m of LFMIM outperforms that of PMR ( except for emotion anger ) , reversely . Interestingly , the above results demonstrate information that although the uni - directional flow degrades the performance of each single modality , it does promote that of multi - modality . The reason is that bi - directional information flow in PMR allows each modality to draw information from other modalities , thus hindering the individual modality from fully exploiting itself . In contrast , uni - directional information flow encourages the modalities to learn more independent and distinctive representations , which can maximize the overall useful information attained by the multi - modal module . 5.2 Ablation studies LFMIM distinguishes from others mainly in 1 ) different modalities are trained with its own labels ; 2 ) the forward information flow in the model is uni - directionally from uni - modal modules to multimodal module . Therefore , in this subsection , we compare LFMIM with the model that is trained with only the multi - modal labels , and the model that allows bi - directional information flow between multi - modal and uni - modal modules . The former corresponds to LFMIM - ML ( LFMIM trained with multi - modal labels for all modules ) , and the later is exactly PMR in last subsection . Tabel 4 summarizes the F1 scores of different modalities for all the emotions . LFMIM has large standard deviation of F1 score over the three modalities u , ∀u ∈ { t , a , v } than PMR except for emotion happiness , which is more clearly displayed in Figure 4(c ) . This , to some degree , illuminates that uni - modal modules of LFMIM yield more distinctive representations , which contributes to the promising performance of our multi - modal module . We first compare the LFMIM with PMR in Figure 4 to demonstrate the impact of information In Figures 4(a ) and 4(b ) , flow in the model . comparing LFMIM and PMR in each modality , it is obvious that the uni - directional information lower ) uni - modal gives rise to 1 ) larger ( resp . losses ( resp . accuracy ) ; 2 ) smaller ( resp . higher ) multi - modal loss ( resp . accuracy ) ; and 3 ) larger modality gap in terms of loss gap and accuracy gap between different modalities . Table 4 shows that for each emotion , modalities t , a , and v of PMR respectively outperform the corresponding modalities of LFMIM in terms F1 score , but That modality m of LFMIM outperforms that of LFMIM - ML in Table 4 demonstrates the merit of uni - modal labels which also boost the diversity of the uni - modal representations . Comparing the 