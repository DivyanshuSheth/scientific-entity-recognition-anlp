
Xiaolei Huang
Department of Computer Science, University of Memphis
xiaolei.huang@memphis.edu
Abstract
Existing approaches to mitigate demographic
biases evaluate on monolingual data, however,
multilingual data has not been examined. In
this work, we treat the gender as domains (e.g.,
male vs. female) and present a standard do-
main adaptation model to reduce the gender
bias and improve performance of text classi-
fiers under multilingual settings. We evaluate
our approach on two text classification tasks,
hate speech detection and rating prediction, and
demonstrate the effectiveness of our approach
with three fair-aware baselines.
1 Introduction
Recent research raises concerns that document clas-
sification models can be discriminatory and can per-
petuate human biases (Dixon et al., 2018; Borkan
et al., 2019; Sun et al., 2019; Blodgett et al., 2020;
Liang et al., 2020). Building fairness -aware classi-
fiers is critical for the text classification task, such
as hate speech detection and online reviews due
to its rich demographic diversity of users. The
fairness-aware classifiers aim to provide fair and
non-discriminatory outcomes towards people or
groups of people based on their demographic at-
tributes, such as gender, age, or race. Fairness has
been defined in different ways (Hardt et al., 2016)
across downstream tasks; for mitigating biases in
the text classification, existing research (Dixon
et al., 2018; Heindorf et al., 2019; Han et al., 2021)
has focused on group fairness (Chouldechova and
Roth, 2018), under which document classifiers are
defined as biased if the classifiers perform better
for documents of some groups than for documents
of other groups .
Methods to mitigate demographic biases in text
classification task focus on four main directions,
data augmentation (Dixon et al., 2018; Park et al.,
2018; Garg et al., 2019), instance weighting (Zhang
et al., 2020; Pruksachatkun et al., 2021), debi-
ased pre-trained embeddings (Zhao et al., 2017;Pruksachatkun et al., 2021), and adversarial train-
ing (Zhang et al., 2018; Barrett et al., 2019; Han
et al., 2021; Liu et al., 2021). The existing studies
have been evaluated on English datasets contain-
ing rich demographic variations, such as Wikipedia
toxicity comments (Cabrera et al., 2018), senti-
ment analysis (Kiritchenko and Mohammad, 2018),
hate speech detection (Huang et al., 2020). How-
ever, the methods of reducing biases in text clas-
sifiers have not been evaluated under multilingual
settings.
In this study, we propose a domain adapta-
tion approach using the idea of “easy adapta-
tion” (Daumé III, 2007) and evaluate on the text
classification task of two multilingual datasets, hate
speech detection and rating prediction. We experi-
ment with non-debiased classifiers and three fair-
aware baselines on the gender attribute, due to its
wide applications and easily accessible resources.
The evaluation results of both non-debiased and de-
biased models establish important benchmarks of
group fairness on the multilingual settings. To our
best knowledge, this is the first study that proposes
the adaptation method and evaluates fair-aware text
classifiers on the multilingual settings.
2 Multilingual Data
We retrieved two public multilingual datasets that
have gender annotations for hate speech classifica-
tion (Huang et al., 2020) and rating reviews (Hovy
et al., 2015).The hate speech ( HS) data collects
online tweets from Twitter and covers four lan-
guages, including English (en), Italian (it), Por-
tuguese (pt), and Spanish (es). The rating review
(Review ) data collects user reviews from Trustpilot
website and covers four languages, including En-
glish, French (fr), German (de), and Danish (da).
The HS data is annotated with binary labels indi-
cating whether the tweet is related to hate speech717or not. The Review data has five ratings from 1 to
5. To keep consistent, we removed reviews with
the rating 3 and encoded the review scores into two
discrete categories: score > 3 as positive and < 3 as
negative. All the data has the same categories for
the gender/sex, male and female. We anonymized
tweets, lowercased all documents, and tokenized
each document by NLTK (Loper and Bird, 2002),
which supports processing English and the other
six languages.
We summarize the data statistics in Table 1. The
HS data is comparatively smaller than the review
data, and both datasets have a skewed label distribu-
tions. For example, most of the reviews have posi-
tive labels, and most of tweets are not hate speech.
Notice that the review data comes from a consumer
review website in Denmark, and therefore, Danish
reviews are more than the other languages of the
review data. We can find that all documents are
short, and the HS data from Twitter is compara-
tively shorter. For the gender ratio, most of the data
has a relatively lower female ratios.
Ethic and Privacy consideration. We only use
the text documents and gender information for eval-
uation purposes without any other user profile, such
as user IDs. All experimental information has been
anonymized before training text classifiers. Specif-
ically, we hash document IDs and replace any user
mentions and URLs by two generic symbols, “user”
and “url”, respectively. To preserve user privacy,
we will only release aggregated results presented
in this manuscript and will not release the data. In-
stead, we will provide experimental code and the
public access links of the datasets to replicate the
proposed methodology.3 Easy Adaptation Framework
Previous work has shown that applying domain
adaptation techniques, specifically the “Frustrat-
ingly Easy Domain Adaptation” ( FEDA ) ap-
proach (Daumé III, 2007), can improve document
classification when demographic groups are treated
as domains (V olkova et al., 2013; Lynn et al., 2017).
Based on these results, we investigate whether the
same technique can also improve the fairness of
classifiers, as shown in Figure 1. With this method,
the feature set is augmented such that each feature
has a domain-specific version for each domain, as
well as a domain-independent (general) version.
Specifically, the features values are set to the origi-
nal feature values for the domain-independent fea-
tures and the domain-specific features that apply to
the document, while domain-specific features for
documents that do not belong to that domain are set
to0. We implement this via a feature mask by the
element-wise matrix multiplication. For example,
a training document with a female author would
be encoded as [F , F ,0], while
a document with a male author would be encoded
as[F ,0, F ]. At test time we only
use the domain-independent features. While the
FEDA applies to non-neural classifiers, we treat
neural models as feature extractors and apply the
framework on neural classifiers (e.g., RNN). We
denote models with the easy adaptation with the
suffix -DA.
4 Experiments
Demographic variations root in documents, espe-
cially in social media data (V olkova et al., 2013;
Hovy, 2015). In this study, we present a standard
domain adaptation model on the gender factor, and
we treat each demographic group as a domain (e.g.,
male and female domains). We show the domain
adaptation method can effectively reduce the biases
of document classifiers on the two multilingual cor-
pora. Each corpus is randomly split into training
(80%), development (10%), and test (10%) sets.
We train the models on the training set and find
the optimal hyperparameters on the development
set. We randomly shuffle the training data at the
beginning of each training epoch.
4.1 Regular Baselines (B-Reg)
We experimented with three popular classifiers, Lo-
gistic Regression (LR), Recurrent Neural Network
(RNN), and BERT (Devlin et al., 2019). For the LR,718
we extract Tf-IDF-weighted features for uni-, bi-,
and tri-grams on the corpora with the most frequent
15K features with the minimum feature frequency
as 3. We then train a LogisticRegression
from scikit-learn (Pedregosa et al., 2011). We left
other hyperparameters as their defaults. For the
RNN classifier, we follow existing work (Park et al.,
2018) and build a bi-directional model with the
Gated Recurrent Unit (GRU) (Chung et al., 2014)
as the recurrent unit. We set the output dimen-
sion of RNN as 200 and apply a dropout on the
output with rate .2. We optimize the RNN with
RMSprop (Tieleman and Hinton, 2012). To encode
the multilingual tokens, we utilize the pre-trained
fastText multilingual embeddings (Mikolov et al.,
2018) to encode the top 15K frequent tokens. For
the BERT classifier, we build two new linear layers
upon on pretrained BERT models (Devlin et al.,
2019) including both English and multilingual ver-
sions. The multilingual version supports 104 lan-
guages that cover all languages in this work. The
first layer transforms the BERT-encoded represen-
tations into 200-dimension vectors and feeds the
vectors for the the final prediction layer. We opti-
mize the model parameters by the Adam (Kingma
and Ba, 2015). For the neural classifiers, we train
them with the batch size as 64, the max length
as 200, and the learning rate within the range of
[1e−4,1e−6]. The classifiers in the following
sections apply the same hyperparameter settings
for fair comparison.4.2 Fair-aware Baselines
Blind augments data by masking out tokens that
are associated with the demographic groups (Dixon
et al., 2018; Garg et al., 2019). We apply
the Blind strategy on the regular baselines and
denote the classifiers as LR-Blind, RNN-Blind,
and BERT-Blind respectively. We retrieved the
gender-sensitive tokens from the Conversation
AI project (ConversationAI, 2021), which con-
tains individual tokens. However, the existing
resource (Dixon et al., 2018; Garg et al., 2019)
only focused on English instead of the other lan-
guages. Therefore, we use the multilingual lexicon,
PanLex (Kamholz et al., 2014), to translate the
gender-sensitive English tokens into the other six
languages.
RNN-IW applies the instance weighting to re-
duce impacts of gender-biased documents (Zhang
et al., 2020) during training classifiers. The method
learns each training instance with a numerical
weightbased on explicit biases counted
by gender-sensitive tokens (ConversationAI, 2021).
Then the method utilizes a random forest classi-
fier to estimate the conditional distribution P(Y|Z)
and the marginal distribution P(Y). Finally, the
method applies the classifier on training instances
to obtain weight scores and assign the weights to
training instances during optimization loss calcula-
tion. The approach achieves the best results using
RNN models, and we keep the same settings. We
extend the approach to multilingual settings using
the translated resources.719
RNN-Adv utilizes adversarial training (Han
et al., 2021) to mitigate (Liu et al., 2021) gender
biases by two prediction tasks, document and gen-
der predictions. Instead of learning to better sep-
arate gender labels, the adversarial training aims
to confuse the gender predictions to reduce gender
sensitiveness. We adapt the RNN module which
achieved promising results (Han et al., 2021; Liu
et al., 2021).
4.3 Evaluation Metrics
We use F1-macro score (fit for skewed label distri-
bution) and area under the ROC curve (AUC) to
measure overall performance. To evaluate group
fairness, we measure the equality differences (ED)
of false positive/negative rates (Dixon et al., 2018)
for the fair evaluation. Existing study shows the
FP-/FN-ED is an ideal choice to evaluate fair-
ness in classification tasks (Czarnowska et al.,
2021). Taking the false positive rate (FPR) as an
example, we calculate the equality difference by
FPED =/summationtext|FPR−FPR|, where Gis the
gender and dis a gender group (e.g., female). We
report the sum of FP-/FN-ED scores and denote the
score as “Fair”. This metric sums the differencesbetween the rates within specific gender groups and
the overall rates.
4.4 Results
We present the averaged results after running eval-
uations three times of both baselines and our ap-
proach in Table 2. Fair-aware classifiers have sig-
nificantly reduced the gender bias over regular clas-
sifiers across the multilingual datasets, and our
approaches have better scores of the group fair-
ness by a range of 14% to 57.7% improvements
over the baselines. The data augmentation ap-
proach achieves better fair scores across multiple
languages, which indicates that the translated re-
sources of English gender-sensitive tokens can also
be effective on the evaluated languages. The neu-
ral fair-aware RNNs usually achieve worse per-
formance than the BERT-based models, but the
RNN-based models is more likely to achieve better
fairness scores. Note that the BERT and fastText
embeddings were pretrained on the same text cor-
pus, Wikipedia dumps, and the performance indi-
cates that fine-tuning the more complex models is
a practical approach to reduce gender bias under
the multilingual settings. Overall, our approach720appears promising to reduce the gender bias under
the multilingual setting.
Considering model performance, we can gener-
ally find that the fair-aware methods do not sig-
nificantly improve the model performance, which
aligns with findings in a previous study (Menon and
Williamson, 2018). For example, fair-aware classi-
fiers promote classification performance by around
1%, and fair-aware classifiers slightly decrease on
the English hate speech data. However, we also
find that all fair-aware models achieve better per-
formance on the Spanish, Italian, and Portuguese
hate speech data. We infer this due to the data size,
as for the three corpora are much smaller than the
corpora in other languages.
5 Conclusion
We present an easy adaptation method to reduce
gender bias on two downstream tasks (hate speech
detection and user rating prediction) under the mul-
tilingual setting. The experiments show that by
treating demographic groups as domains, we can
reduce biases while keeping relatively good perfor-
mance. Our future work will solve the limitations
of this study, including non-binary genders, multi-
ple demographic factors, embedding sources, and
label imbalance. Code and data instructions of our
work are available at https://github.com/
xiaoleihuang/DomainFairness .
5.1 Limitations
While we have proved the effectiveness of our pro-
posed framework, limitations must be acknowl-
edged in order to appropriately interpret our evalu-
ations. First, our experiments are based on coarse-
grained gender categories (binary gender groups)
and the multilingual datasets fail to provide fine-
grained information. Using coarse-grained at-
tributes would ignore people with non-binary gen-
der. Expanding evaluations of existing methods
may require enriching categories of demographic
attributes. In this study, we include two major data
sources and experiment the six languages aiming to
evaluate gender-bias-mitigation algorithms in a di-
verse and multilingual scenario. We keep the same
experimental settings with the baselines (Dixon
et al., 2018; Zhang et al., 2020; Han et al., 2021;
Liu et al., 2021) to ensure fair comparisons, such
as data sources and binary labels. Second, the
multilingual pretrained embeddings (fastText and
BERT), which were not trained on the social mediadata, may not achieve the best performance overall.
We may expect a performance boost if utilizing
in-domain pretrained embeddings. However, our
focus is on the augmentation framework to reduce
demographic (gender) biases.
Acknowledgements
The author wants to thank for reviewers’ valuable
comments. The experiments on the English hate
speech data was adopted from his Ph.D. thesis,
Section 5.6 (Huang, 2020). The author also wants
to thank Dr. Michael J. Paul for the initial idea
discussion and Dr. Shijie Wu for his suggestions
on selecting the multilingual lexicon.
References721722
A Implementation Details
While we have presented experimental and hyper-
parameter settings in the Section 4, we report im-
plementation tools in this section. We implement
neural models by PyTorch (Paszke et al., 2019)
and non-neural models by scikit-learn (Pedregosa
et al., 2011). For the BERT model, we use the
Hugging Face Transformers (Wolf et al., 2020).
The Keras (Chollet et al., 2015) helped prepro-
cess text documents for neural models, including
padding and tokenization. We trained models on
an NVIDIA RTX 3090 and evaluated the models
on CPUs.723