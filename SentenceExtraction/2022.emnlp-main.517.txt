
Johannes Mario Meissner, Saku Sugawara, Akiko AizawaThe University of Tokyo,National Institute of Informatics
{meissner,saku,aizawa}@nii.ac.jp
Abstract
Debiasing language models from unwanted be-
haviors in Natural Language Understanding
tasks is a topic with rapidly increasing inter-
est in the NLP community. Spurious statistical
correlations in the data allow models to perform
shortcuts and avoid uncovering more advanced
and desirable linguistic features. A multitude
of effective debiasing approaches has been pro-
posed, but flexibility remains a major issue. For
the most part, models must be retrained to find
a new set of weights with debiased behavior.
We propose a new debiasing method in which
we identify debiased pruning masks that can be
applied to a finetuned model. This enables the
selective and conditional application of debias-
ing behaviors. We assume that bias is caused by
a certain subset of weights in the network; our
method is, in essence, a mask search to iden-
tify and remove biased weights. Our masks
show equivalent or superior performance to the
standard counterparts, while offering important
benefits. Pruning masks can be stored with high
efficiency in memory, and it becomes possible
to switch among several debiasing behaviors
(or revert back to the original biased model) at
inference time. Finally, it opens the doors to
further research on how biases are acquired by
studying the generated masks. For example,
we observed that the early layers and attention
heads were pruned more aggressively, possibly
hinting towards the location in which biases
may be encoded.
1 Introduction
The issue of spurious correlations in natural lan-
guage understanding datasets has been extensively
studied in recent years (Gururangan et al., 2018;
McCoy et al., 2019; Gardner et al., 2021). In the
MNLI dataset (Williams et al., 2018), negation
words such as “not” are unintended hints for the
contradiction label (Gururangan et al., 2018), while
a high word overlap between the premise and the
hypothesis often correlates with entailment (Mc-
Coy et al., 2019).Figure 1: We find masks that remove bias from a fine-
tuned model by using frozen-weight movement pruning.
A common way to prevent the acquisition of bi-
ases (in this paper defined as unintended shortcut
behaviors) is to adopt a debiasing loss function,
such as product-of-experts (Hinton, 2002), to dis-
courage learning shortcuts through the use of the
annotated level of bias in each sample. Because
manual bias annotation is difficult and expensive,
the predictions of a biased model are used as bias
annotations (Clark et al., 2019). Commonly, mod-
els must be retrained to achieve debiased behav-
ior; this limits our ability to target different biases
separately, as well as choose varied levels of debi-
asing strength (which impacts the in- vs. out-of-
distribution trade-off).
We propose a new debiasing framework that fo-
cuses on removing bias from an existing model, in-
stead of the now-common approach of re-training
from scratch. Our approach is depicted in Figure 1.
We find a pruning mask that zeroes out the weights
that cause biased outputs, producing the desired
effect without altering the original model. This7607approach offers several clear advantages. First,
pruning masks can be stored very efficiently, only
occupying a fraction of the original model size.
This enables creating multiple masks for varied
debiasing behaviors. Secondly, masks can be ef-
fortlessly set or unset at inference time, as opposed
to replacing the entire model. This allows for flex-
ible application, as well as easy reversion to the
original model when needed. Finally, re-framing
the debiasing process as a mask search opens the
doors towards future analysis directions, helping
to more deeply understand how biases are learned,
and how they can be eliminated.
2 Related Work
2.1 Shortcuts in NLU
The study of shortcut solutions in machine learn-
ing (Geirhos et al., 2020; D’Amour et al., 2020)
has gained attention in recent years, including in
the field of Natural Language Understanding. In
SQuAD (Rajpurkar et al., 2016), Jia and Liang
(2017) show that distractor sentences can be in-
serted in such a way that the frequently used spuri-
ous features mislead the model’s answer. In MNLI,
Gardner et al. (2021) discuss the idea that all sim-
ple feature-label correlations should be regarded
as spurious. Their results go in line with Gururan-
gan et al. (2018), who show that models are able
to achieve strong performance just by training on
the hypothesis, a scenario where the desirable ad-
vanced features are not available. Instead, simple
word correlations are used to make the prediction.
Finally, other kinds of features such as lexical over-
lap have been pointed out as important shortcuts
(McCoy et al., 2019).
2.2 Debiasing
A wide range of debiasing approaches have been
proposed to alleviate shortcut behavior. For exam-
ple, perturbations in the model’s embedding space
can encourage robustness to shortcuts (Liu et al.,
2020), and training on adversarial data (Wang and
Bansal, 2018) has important benefits for general-
ization capabilities too. Removing a certain subset
in the training data (filtering) can help to avoid
learning spurious correlations. Bras et al. (2020)
devise an algorithm that selects samples to filter
out, reducing the training time and robustness of
the target model.
We will focus our efforts on a family of ap-
proaches that rely on a debiasing loss function todiscourage biased learning, instead of altering the
original training data. A wide range of debiasing
losses have been proposed to discourage shortcut
learning. They all rely on having a measure of the
level of bias for each training sample, which is com-
monly obtained via a biased model’s predictions.
Among the most common are product-of-experts
and focal loss (Schuster et al., 2019). Other ap-
proaches introduce a higher level of complexity,
such as confidence regularization (Utama et al.,
2020a) requiring a teacher model, or learned-mixin
(Clark et al., 2019) introducing an additional noise
parameter.
2.3 Pruning
Pruning consists in masking out or completely re-
moving weights in a neural network, such that they
no longer contribute to the output. Common goals
include reducing the model size or achieving an
inference speed-up.
Magnitude pruning is a basic pruning method
that removes weights based on their absolute value.
It has proven to be an effective method to reduce
model size without compromising on performance.
Gordon et al. (2020) apply this technique on trans-
former language models. Movement pruning, on
the other hand, was proposed by Sanh et al. (2020),
and involves training a score value alongside each
weight. Scores are updated as part of the optimiza-
tion process, and weights with an associated score
below the threshold are masked out.
Zhao et al. (2020) introduce an alternative to the
usual finetuning process by finding a mask for the
pretrained base model such that performance on
the target task increases. The same base model
can be used with several masks to perform multi-
ple tasks. They show that this approach achieves
similar performance to standard finetuning on the
GLUE tasks (Wang et al., 2018).
Our work is inspired by Zhao et al. (2020); but
we focus on removing biases from an already fine-
tuned model. We will refer to this method as a
mask search. It benefits from the same advantages:
masks can be easily applied and removed from
a shared base model, while additionally offering
storage improvements.
3 Masked Debiasing
Our proposed approach to debiasing takes a unique
point of view in the debiasing field by assuming
that biased behavior is encoded in specific weights7608
of the network and can be removed without altering
the remaining weights. Thus, we perform a mask
search to identify and remove those weights, expe-
riencing debiased behavior in the resulting model.
Our approach comes together by combining a de-
biasing loss, a biased model, and a score-based
pruning technique.
Debiasing Loss Among the debiasing losses
mentioned in Section 2.2, none appear to be clearly
superior, each offering certain strengths and weak-
nesses, and striking a balance between in- and out-
of-distribution performance. We err on the side of
simplicity and run our experiments with the focal
loss. We follow Clark et al. (2019) for the imple-
mentation.
Bias Model To utilize a debiasing loss, we must
obtain the predictions of a biased model on the
training samples. We utilize weak learners, fol-
lowing the nomenclature introduced by Sanh et al.
(2021); they do not require making any assump-
tions on the underlying biases. While our method
works with any debiasing loss and bias level source,
we choose this setting due to its flexibility and
adaptability to new scenarios.
To date, two major learners have been proposed:
undertrained (Utama et al., 2020b) and underparam-
eterized (Sanh et al., 2021) learners. We train and
extract the predictions of both of them for compari-
son. Undertrained learners are trained by selecting
a very small subset of the training data, but keeping
the target model’s architecture and hyperparame-
ters. This effectively translates into a model that
overfits on the selected data subset. Features un-
covered by the model in this manner are deemed
spurious, as it is assumed that more advanced fea-
tures require exploring larger quantities of data. Weused 2000 samples for MNLI, and 500 samples for
the other datasets. Underparameterized learners,
on the other hand, restrict the model complexity
by reducing layer size and count. The expectation
is that this underparameterization is restrictive and
limits the ability to find complex features. We use
BERT-Tiny (Turc et al., 2019), a BERT model with
2 layers and an inner size of 128.
Frozen Movement Pruning Our approach is
similar to Zhao et al. (2020); but we implement
it by leveraging the framework provided by Sanh
et al. (2020), with the addition of weight-freezing.
We use unstructured pruning, which means that
each individual weight is considered for exclu-
sion. Further, we utilize a threshold approach: in
each weight tensor we remove (mask out) those
weights with an associated score that is lower
than the threshold. A regularization term for the
scores helps avoid all scores growing larger than the
threshold. For further details, we refer the reader
to Sanh et al. (2020). As the starting point, we load
a model already finetuned on the target task.
4 Experimental Setup
We compare our approach against the standard ap-
proach of re-training the model with a debiasing
loss. Our experiments are carried out with BERT-
Base (Devlin et al., 2019).
4.1 Tasks and Evaluation Scenarios
We perform experiments on three tasks in NLU,
and consider several biases and evaluation datasets.
Natural Language Inference (NLI) This task
consists in classifying whether the relationship be-
tween a pair of sentences (premise and hypothesis)
is entailed, neutral or contradicted. We evaluate
on two well-known biases: lexical overlap bias
(McCoy et al., 2019) and negation words bias (Gu-
rurangan et al., 2018). We train on MNLI, and
evaluate on HANS (McCoy et al., 2019) and our
own negation-words subset. HANS can be used to
evaluate the lexical overlap bias. Additionally, we
create our own negation-words anti-bias set by se-
lecting entailed samples from the MNLI validation
set that contain negation words in the hypothesis.7609
Paraphrase Identification In this task the goal
is to identify whether a pair of sentences are para-
phrasing each other or not. We train on QQP
(Quora Question Pairs), and evaluate on PAWS
(Zhang et al., 2019), which tests the model’s relia-
bility on the lexical overlap bias.
Fact Verification We train on FEVER (Thorne
et al., 2018), and evaluate on FEVER Symmetric
(Schuster et al., 2019). The task setup is similar
to NLI; we classify a claim-evidence pair as ei-
ther support, refutes or not-enough-information.
FEVER Symmetric eliminates claim-only biases
(clues in the claim that allow to guess the label),
among which are negation words, in a similar fash-
ion to MNLI.
4.2 Hyperparameters and Reproducibility
We aim for complete reproducibility by providing
complete code and clear reproduction instructions
in our repository.In most cases, we follow the
configurations indicated by the respective original
papers. Appendix B is additionally provided for a
report of important hyperparameters.
We run all of our experiments on five seeds. We
not only seed the debiasing masks, but also the
accompanying weak models and base models too.
5 Results
We compare our approach against our reproduction
of the two weak model approaches, and compile
our results in Table 1.5.1 Masked Debiasing is Effective
First, we observe that our debiasing masks are very
effective, surpassing the performance of their stan-
dard debiasing counterparts in two out of the three
evaluated tasks, while keeping a competitive in-
distribution performance. In the FEVER task, our
masks provided a slight debiasing effect, but did
not beat the standard debiasing baseline. We do not
report undertrained debiasing results in the QQP
setting due to their failure to converge to adequate
results (in both standard and masked debiasing).
To confirm that the combination of masking with
a debiasing loss is necessary to achieve our results,
we provide an ablation experiment (Masking w/o
Debiasing) where the debiasing loss is replaced
with the standard cross-entropy loss, while using
the same pruning parameters. Results suggest that
pruning alone is not able to achieve the same debi-
asing effect.
5.2 Masks are Diverse
As a means to showcase the capacity of debiasing
masks to offer different debiasing behaviors, we
refer to each method’s capacity to mitigate the nega-
tion bias, evaluated by our negation-words subset.
As the standard debiasing results reveal, both
weak model approaches were unable to improve
baseline performance on our subset. Therefore,
we used the hypothesis-only bias model, as pro-
vided by Utama et al. (2020a). Negation words are
strongly correlated with the hypothesis-only bias;
accordingly, observe a performance improvement
on our subset when leveraging this debiasing target.76106 Mask Analysis
Our pruning approach uses a score threshold, which
allows for a dynamic pruning rate across modules.
To gain a better understanding on our masks, we
study the generated density distribution. Specifi-
cally, we study the rate at which weights were re-
moved in different network regions. In each layer,
we obtain the pruning rate of the attention layer’s
query, key, value, and output modules, as well as
the two linear modules that follow. We plot the
results in Figure 2.
We make two general observations. First, prun-
ing rates are relatively higher in the early layers.
It is known that the last few layers are very task
specific (Tenney et al., 2019), which likely implies
that their modification more directly impacts per-
formance. Thus, our masks may be targeting early
layers as a means to remove biases without causing
a noticeable performance decrease. Secondly, the
attention modules are more heavily pruned than the
linear layers, which could suggest that attention
heads play an important role in bias encoding.
7 Conclusion
We conclude that debiasing masks are an effective
approach to mitigating shortcuts. Alongside provid-
ing surprisingly well-performing debiased behav-
iors, masks allow to shift the way we think about
debiasing: no longer should biased and unbiased
models be treated as two separate models; rather,
it becomes possible to “remove” biases by simply
eliminating certain weights from the network.
Limitations
An important limitation of this approach is that we
found it necessary to follow Sanh et al. (2020) and
run the movement pruning technique for 12 epochs,
requiring longer training time. We hypothesize
that in future work, it could become possible to
drastically reduce training time for this approach.
Further, in this short paper we explored three
tasks in NLU with a fixed pruning configuration,
but it would be beneficial to explore its applicability
to other domains, combine it with other debiasing
approaches, or explore more varied configurations,
such as structured pruning methods for improved
inference efficiency.Acknowledgments
The education period leading to this project
has received funding from “la Caixa” Foun-
dation (ID 100010434), under agreement
LCF/BQ/AA19/11720042. This work was also
supported by JST PRESTO Grant Number
JPMJPR20C and JSPS KAKENHI Grant Number
21H03502.
References76117612
A Negation Words
Within the MNLI Validation Matched set, we select
those entailed samples where at least one of the fol-
lowing words is present in the hypothesis: no, not,
don’t, none, nothing, never, aren’t, isn’t, weren’t,
neither, don’t, didn’t, doesn’t, cannot, hasn’t won’t.
B Hyperparameters
B.1 Baseline and Debiased Models
Our baseline models are trained with a batch size
of 32, learning rate of 3e-5, weight decay of 0.1,
and 20% warmup steps.
B.2 Weak Learners
Undertrained models are trained with the standard
BERT architecture and 2000 samples (500 in the
case of QQP and FEVER) of the dataset for 5
epochs. Utama et al. (2021) mention 3 in their
paper, but train with 5 in their code repository. We
confirm that 5 yields better results. For underpa-
rameterized models, we follow (Sanh et al., 2021)
and use BERT-Tiny (Turc et al., 2019), a BERT
model with 2 layers and an inner size of 128. This
model is trained on the full training set for 3 epochs,
and slightly adjusted hyperparameters.
We use focal loss (sample reweighting) in all
mask debiasing experiments, with the exception
of the HypOnly MD model, which uses product-
of-experts (in an attempt to follow Utama et al.
(2020a) as closely as possible).B.3 Masked Debiasing Models
Our mask search is performed with a score learning
rate of 0.1, batch size of 128, and 12 epochs of
training.7613