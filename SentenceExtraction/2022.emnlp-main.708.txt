
Kai Shen, Yichong Leng, Xu Tan
Siliang Tang, Yuan Zhang, Wenjie Liu, Edward LinZhejiang University,University of Science and Technology of ChinaMicrosoft Research Asia,Microsoft Azure Speech{shenkai, siliang}@zju.edu.cn ,lyc123go@mail.ustc.edu.cn{xuta, yuanz, liwenjie, edlin}@microsoft.com
Abstract
Text error correction aims to correct the errors
in text sequences such as those typed by hu-
mans or generated by speech recognition mod-
els. Previous error correction methods usually
take the source (incorrect) sentence as encoder
input and generate the target (correct) sentence
through the decoder. Since the error rate of the
incorrect sentence is usually low (e.g., 10%),
the correction model can only learn to correct
on limited error tokens but trivially copy on
most tokens (correct tokens), which harms the
effective training of error correction. In this
paper, we argue that the correct tokens should
be better utilized to facilitate effective training
and then propose a simple yet effective mask-
ing strategy to achieve this goal. Specifically,
we randomly mask out a part of the correct to-
kens in the source sentence and let the model
learn to not only correct the original error to-
kens but also predict the masked tokens based
on their context information. Our method en-
joys several advantages: 1) it alleviates trivial
copy; 2) it leverages effective training signals
from correct tokens; 3) it is a plug-and-play
module and can be applied to different models
and tasks. Experiments on spelling error correc-
tion and speech recognition error correction on
Mandarin datasets and grammar error correc-
tion on English datasets with both autoregres-
sive and non-autoregressive generation models
show that our method improves the correction
accuracy consistently.
1 Introduction
Text error correction techniques have been widely
used to correct the errors in text, such as gram-
mar errors (Rothe et al., 2021; Grundkiewicz et al.,2019; Omelianchuk et al., 2020) or spelling er-
rors (Cheng et al., 2020; Zhang et al., 2020) and
automatic speech recognition (ASR) errors (Cucu
et al., 2013; D’Haro and Banchs, 2016; Anantaram
et al., 2018; Zhu et al., 2021), etc. Text error cor-
rection is a challenging task because in realistic
ASR transcripts or human-typed documents, it is
common that the errors are sparse (for example,
the error rate of an ASR system is usually below
10%) (Leng et al., 2021b). To handle the problem,
previous works aim to improve the accuracy of cor-
rection models by using error detector-corrector
architecture and focusing on two directions: 1)
promoting the accuracy of error detection (Zhang
et al., 2020), 2) enhancing the capability of cor-
rection (Cheng et al., 2020; Leng et al., 2021b,a).
Unfortunately, they all fall in the short that the con-
ventional correction models always copy the cor-
rect tokens directly, which is potentially harmful to
the learning of the error correction process.
The learning diagram that directly copies the cor-
rect tokens can be harmful in two aspects. Firstly,
it affects the learning of the correction procedure
of the error tokens. Conceptually, the correction
diagram can be divided into two main streams: 1)
copy the correct token, 2) correct the error token.
Intuitively, the former is much easier and contains
less language understanding knowledge. The unbal-
ance of trivial copying task (about 90% tokens) and
hard correction task (about 10% tokens) is poten-
tially harmful to the training of the error correction
model. Secondly, it severely underuses valuable
training data and consequently lead to low training
efficiency. To be more specific, since the errors
in input text are sparse, most of the tokens in in-
put text are correct and the correction model only
learns a trivial copy mapping from input to out-
put, which means that the model can hardly learns
knowledge such as language understanding from
correct tokens other than copying. To the best of
our knowledge, there is no previous work proposed10367to better leverage the correct tokens.
In this paper, we propose a simple but effec-
tive framework MaskCorrect to alleviate the heavy
copy phenomenon and improve the utilization of
correct tokens in training data. In detail, during
the training of the correction model, we randomly
perturb a proportion of correct tokens to a special
token <mask> in the source sentences and train the
correction model to predict the correct tokens from
<mask> token. With the help of our method, the
correction model can benefit from several aspects:
1) it learns the language understanding knowledge
on correct tokens instead of a trivial copy mapping,
and thus alleviating the trivial copy phenomenon;
2) since the <mask> token does not contain explicit
clues about its corresponding token, our method
will encourage the model to generate output tokens
based more on context information, and thus it al-
leviates the low training efficiency problem.
We conduct a series of experiments on two
tasks: 1) Spelling Error Correction and 2) ASR
Error Correction on Mandarin datasets with both
auto-regressive and non-autoregressive methods.
Furthermore, we also conduct experiments on the
Grammar Error Correction task on English datasets
in Appendix D.1. Experimental results show that
our MaskCorrect can consistently boost the conven-
tional correction methods in both tasks. Especially,
we outperform the state-of-the-art spelling error
correction method by a large margin.
In addition, it is worth noting that the method
proposed in this paper is a general framework and
can be potentially extended in situations where
there exists lots of trivial copy mapping in train-
ing data, such as text summarization and question
generation.
2 Related Works
In this section, we briefly review the spelling/ASR
error correction methods and then discuss the copy
phenomenon in closely related NLP tasks.
Spelling/ASR Error Correction The main-
stream model on spelling error correction leverage
a detector-corrector framework (Cheng et al., 2020;
Zhang et al., 2020; Guo et al., 2021), which consists
of an error detector to detect error tokens and an er-
ror corrector to correct the tokens detected as error
tokens. A series of works are proposed to improve
the correction accuracy with the help of pretrain-
ing methods such as BERT (Zhang et al., 2020) or
pronunciation of word (Zhang et al., 2021). As forthe ASR error correction, the early-stage correc-
tion model for ASR leverages the auto-regressive
translation model to correct the errors (Mani et al.,
2020; Liao et al., 2020; Wang et al., 2020; Zhu
et al., 2021). Recently, non-autoregressive correc-
tion models are proposed to correct errors in a fast
and accurate manner (Leng et al., 2021b,a; Du et al.,
2022). However, all the methods in both tasks fo-
cus on the learning of correcting error tokens, and
the models can only learn a trivial copy mapping
for tokens without error. Furthermore, they also
neglect the low training efficiency because of the
heavy copy phenomenon.
The Copy Phenomenon in NLP tasks The copy
phenomenon and the problems it causes have been
noticed in the related NLP tasks. In the abstrac-
tive text, the copy (pointer) mechanism has been
widely integrated to address the issue of frequently
copying words from the source context to the de-
sired summary (See et al., 2017; Wu et al., 2020).
However, a series of works raise potential shorts.
Wang et al. (2019); Ji et al. (2020) finds that merely
copying parts of the original text tends to plain and
uncreative summary. Wei et al. (2019) proposes a
novel regularization method to solve the dilemma
that traditional copy mechanisms cannot work prop-
erly when the target tokens are not consistent with
the source tokens.
When it comes to language pretraining, the
copy phenomenon attracts lots of attention from
researchers. In the original MLM training, it is a
common practice (Devlin et al., 2019; Liu et al.,
2019; He et al., 2020; Li et al., 2022b) to only mask
up to 15% of original tokens and recover them from
the corrupted context while the remaining other to-
kens are not trained. ELECTRA (Clark et al., 2020)
claims that the MLM training only learns from the
15% of the tokens, leading to the severe substantial
compute cost and low training efficiency. Instead
of MLM, they introduce a new pretraining diagram
that some tokens are firstly randomly replaced with
alternative tokens and the model is trained with
the replaced token detection task where it learns to
distinguish real input tokens with plausible gener-
ated tokens. Wettig et al. (2022) finds that masking
more than 15% of tokens can bring benefits. With
a large mask ratio, the model can learn from more
cases and thus boosting the training efficiency.
Although the copy phenomenon is still com-
mon in the correction, compared to the pretraining
which aims to learn the general and robust contex-10368tual knowledge, the correction model has a much
cleaner purpose that recovering the incorrect tokens
mistaken by humans. In this paper, we propose to
perturb correct tokens to utilize these correction
tokens to help the model learn knowledge about
context understanding.
3 Method
3.1 Preliminary Discussion
In this section, we first conduct some preliminary
experiments to qualitatively discuss the impact of
the copy phenomenon. In addition, we discuss a
simple solution to alleviate the proposed copy prob-
lem. Without loss of generality, we conduct pre-
liminary experiments on FastCorrect (Leng et al.,
2021b) which is a state of the art (SOTA) baseline
in ASR Error Correction (we will introduce the
task and FastCorrect in Sec. 4.2).
Add Additional "copy" Tokens. Based on the
assumption that heavy copy is harmful to the cor-
rection, one natural question is: what will happen
if we make the "copy" more frequently?
To answer this question, we gradually increase
the "copy" tokens in the training set while keeping
the number of the error tokens unchanged. In de-
tail, we first find out all the samples denoted as C
in the paired training data whose input sentence is
equal to the target sentence. Then we randomly
select q(denoted as the "copy" ratio q) samples
from collection Cand add them to the paired train-
ing set. Finally, we train the FastCorrect on the
synthetic training set. We vary the "copy" ratio
q∈ {0.1,0.2,0.3,0.4,0.5}. We report the word
error rate (WER) metric on the AISHELL-1 (Bu
et al., 2017)’s test set in Figure 1.From the results, we can find that as the "copy"
ratio increases, the performance of FastCorrect will
decrease gradually. Although FastCorrect is trained
with the same amount of error samples, the model
is paying more attention to the simply-copied cor-
rect samples. This phenomenon illustrates that the
heavy copy phenomenon is harmful to the correc-
tion, which proves the correctness of our motiva-
tion.
Mask on Target Loss. To alleviate the heavy
copy problem, one direct but natural idea is to mask
off the gradients of some correct tokens during
the gradient back-propagation. We implement this
idea by simply masking off a specific number of
correct tokens in the loss calculation. The results
are shown in Figure 2.
From the results, surprisingly, we can find that
the WER results increase when we gradually mask
more correct tokens. This experiment indicates
that although the correct tokens are copied as a
phenomenon, they cannot be easily dropped during
the loss calculation. We have several insights: 1)
we cannot alleviate the "copy" by preventing the
loss in back-propagation. Every token plays an
important role in the context and the overall dis-
tribution will be influenced if we simply prevent
back-propagating gradient of some tokens; 2) Al-
though the correct tokens are copied as a result,
they still contain information in some way and can-
not be ignored easily.
Motivated by this experiment, in the MaskCor-
rect framework, we choose to mask the correct
tokens in the source sentence and predict the cor-
responding token. There are several advantages:
1) every token participates in the correction loss10369
back-propagation, which does not affect the overall
distribution; 2) by masking off the correct tokens
in the source sentence, the model is encouraged to
predict the token only by the context.
3.2 MaskCorrect
In this subsection, we introduce our method for
addressing the heavy copy of correct tokens in the
training data of correction models.
As shown in Figure 3 (a), the conventional cor-
rection models focus on the correction of error to-
kens (i.e., token “F” in Figure 3) and simply learn
a copy mapping on the rest of the tokens that have
no error (i.e, token “A”, “B”, “D” and “E” in Fig-
ure 3). If the effective training samples in training
data are the tokens with error only, then the valu-
able training data is heavily underused since only
a small proportion of tokens in training data (typi-
cally about 10%) contains errors.
We encourage the model to leverage correct to-
kens by introducing perturbation, as shown in Fig-
ure 3 (b) in non-autoregressive and (c) in auto-
regressive way. Specifically, we random perturb
p% input tokens to a special token <mask> before
the input tokens are fed into the correction model
and keep the target tokens untouched. As a result,
the model needs to predict the masked token based
on its context information (i.e., predicting token “B”
based on its context information in Figure 1 (b) and
(c)). A better understanding of context information
will also benefit the correction of error tokens sincethe model needs to leverage context information to
detect which token has an error and how to correct
this error token.
We would like to highlight that the perturbation
is only conducted on the input tokens, and thus our
method can be used on any error correction tasks
with arbitrary model architecture or loss function
designed for certain error correction tasks. Our per-
turbation helps the model better utilize correction
tokens by a mask-and-predict task.
4 Experiments
In this section, we evaluate our method on two
tasks, namely ASR Error Correction andSpelling
Error Correction on Mandarin datasets. We first
report the accuracy of the baselines and our meth-
ods to demonstrate the effectiveness of our method
on each task. Then we conduct ablation experi-
ments to comprehensively analyze the effect of the
mask mechanism. Furthermore, we also conduct
experiments on Grammar Error Correction on En-
glish corpus in Appendix D.1 to demonstrate the
effectiveness.
4.1 Spelling Error Correction
In this section, we evaluate the effectiveness of our
method on spelling error correction task.
4.1.1 Datasets
We conduct the experiments on two publicly avail-
able SIGHAN datasets: SIGHAN 2014 (Yu and Li,10370Test Sets ModelDetection Level Correction Level
Acc. P. R. F1 Acc. P. R. F1
SIGHAN14FASPell - 61.00 53.50 57.00 - 59.40 52.00 55.40
SpellGCN 69.83 69.20 65.20 67.14 68.92 70.10 60.94 65.20
SoftMask 69.49 72.69 60.38 65.97 68.36 71.90 58.08 64.26
+ MaskCorrect 71.37 74.11 63.85 68.60 70.72 73.70 62.50 67.64
BERT 68.27 69.10 63.65 66.27 66.85 68.10 60.77 64.23
+ MaskCorrect 71.94 73.82 66.15 69.78 70.90 73.19 64.04 68.31
SIGHAN15FASPell - 67.60 60.00 63.50 - 66.60 59.10 62.60
SpellGCN 78.92 80.80 75.70 78.17 78.10 80.10 72.70 76.22
SoftMask 77.36 79.96 72.14 75.85 76.45 79.54 70.30 74.63
+ MaskCorrect 78.45 80.32 74.54 77.32 77.09 79.71 71.77 75.53
BERT 78.82 82.12 72.88 77.22 77.64 81.62 70.48 75.64
+ MaskCorrect 80.91 81.56 79.15 80.34 79.00 80.79 75.28 77.94
2014) (SIGHAN14) and SIGHAN 2015 (Tseng
et al., 2015) (SIGHAN15) from Chinese Spell
Check Bake-offs. Following Cheng et al. (2020);
Guo et al. (2021), we adopt the standard split of
the training and testing data of SIGHAN, as well
as the same data preprocessing which converts the
traditional Chinese to simple Chinese by OpenCC.
From the previous works (Cheng et al., 2020;
Guo et al., 2021), the SIGHAN is too small to
achieve acceptable results, we follow their common
practice and collect 5 million unpaired texts from
the internet to pretrain the model. The detailed
procedure to add noises can be found in Appendix
B.1. For the finetune split, we include the training
corpus of SIGHAN14 and SIGHAN15. In addition,
we also include the 271K samples generated by
automatic method (Wang et al., 2018) as the fine-
tune data. The detailed data statistics are shown
in Appendix A.1.
We use the pretrained Bert tokenizer provided
by HuggingFace (Wolf et al., 2020) to tokenize the
sentences. In addition, we filter out the sentences
whose length are larger than 100 for the paired and
unpaired training corpus.
4.1.2 Baseline Methods
FASPell The FASPell (Hong et al., 2019) pro-
poses a new correction paradigm that consists ofa denoising autoencoder and a decoder. Instead
of using an inflexible and insufficient confusion
dictionary, it utilizes the similarity metric to select
the candidates in the decoder.
SpellGCN the SpellGCN (Cheng et al., 2020)
incorporates the phonological and visual similarity
knowledge into language modeling. They propose
to build the similarity graph for each word token in
the view of the pronunciation and word shape.
BERT The BERT (Devlin et al., 2018) is a com-
monly used baseline for spelling error correction. It
simply employs the pretrained BERT as a sentence
encoder network. We take the encoded sentence
features and feed them to network with multiple
linear layers and ReLU activation to predict the
targets.
SoftMask BERT This method (Zhang et al.,
2020) is a BERT-based spelling error correction
baseline, which consists of a GRU-based error
detection module and a BERT-based error correc-
tion module. A soft-masking gate softly fuses the
<mask> token’s embedding with the initial word
embedding by the probability of being an error to-
ken predicted by the detection module. Then the
fused embeddings are fed to the correction module.
4.1.3 Evaluation Metrics
We report the sentence level accuracy, precision, re-
call, and F1 scores as the evaluation metrics, which
are commonly used in the spelling error correction10371Detection Level Correction Level
Mask Ratio Acc.(%) P.(%) R.(%) F1(%) Acc.(%) P.(%) R.(%) F1(%)
SIGHAN140% 68.27 69.10 63.65 66.27 66.85 68.10 60.77 64.23
10% 65.82 66.05 64.23 65.49 65.82 66.05 62.12 64.02
15% 71.85 73.87 65.77 69.58 70.80 73.29 63.85 68.24
20% 71.94 73.82 66.15 69.78 70.90 73.19 64.04 68.31
25% 72.60 76.08 64.23 69.66 71.56 75.47 62.12 68.14
30% 68.36 68.85 64.62 66.67 67.80 68.46 63.46 65.87
40% 66.29 66.01 64.23 65.11 64.78 64.90 61.15 62.97
50% 67.61 68.26 63.27 65.67 66.67 67.58 61.35 64.31
SIGHAN150% 78.82 82.12 72.88 77.22 77.64 81.62 70.48 75.64
10% 76.64 76.84 75.28 76.05 75.27 76.16 72.51 74.29
15% 78.27 78.32 77.31 77.81 77.36 77.90 75.46 76.66
20% 80.91 81.56 79.15 80.34 79.00 80.79 75.28 77.94
25% 78.36 79.92 74.91 77.33 77.09 79.35 72.32 75.68
30% 76.36 75.73 76.57 76.15 74.82 74.95 73.43 74.18
40% 77.82 76.90 78.60 77.74 76.55 76.30 76.01 76.16
50% 76.82 77.54 74.54 76.01 75.18 76.74 71.22 73.88
tasks. All the metrics are computed by the official
evaluation tools.
4.1.4 Implementation Details
We choose the BERT andSoftMask BERT as the
correction backbone and evaluate the effectiveness
of our method. The detailed training configuration
can be found in Appendix C.1. For both BERT
and SoftMask BERT, given the input sentence with
mtokens, we first find out all the correct tokens
by comparing the input sentence with the ground
truth sentence. Secondly, we randomly select the
tokens from the correct token set with ratio pwith-
out replacement. For each selected token, it will
be replaced with <mask> token with a probability
ofm, or will be replaced with a randomly chosen
other token with a probability of n, otherwise it will
not be changed with a probability of 1−m−n.
We apply the MaskCorrect on both unpaired and
paired training corpus with the same mask hyper-
parameter p, m, n . However, since it is inevitable
that there is no such <mask> token during the in-
ference stage, we have to finetune model with the
paired data additionally for 3 epochs to cope with
the training-inference mismatch problem to achieve
better performance.4.1.5 Results Analysis
In this section, we compare our method with the
baseline models on the two datasets for Spelling
Error Correction. We apply the proposed MaskCor-
rect method on two baselines, namely BERT +
MaskCorrect andSoftMask + MaskCorrect . The
results are shown in Table 1.
First, we compare all the methods and we
find the variant BERT + MaskCorrect outper-
forms all other baselines in both SIGHAN14 and
SIGHAN15 datasets. Specifically, we compare
two baselines BERT andSoftMask with the corre-
sponding MaskCorrect variants. With the MaskCor-
rect, 1) the SoftMask obtains 2.63% and 3.02% F1
score gains on detection and correction levels in
SIGHAN14 and obtains 1.47% and 0.9% gains
on SIGHAN15, respectively; 2) the BERT obtains
3.51% and 4.07% F1 gains and 3.12% and 2.30%,
respectively. It demonstrates that by randomly per-
turb a certain proportion of correct tokens into
<mask> tokens, the correction models can learn
the semantic knowledge from the correct tokens,
which enhances the ability of detection and correc-
tion the spelling errors.
4.1.6 Ablation Study
In this section, we further study the impact
of the masking ratio (i.e., the hyper-parameter10372p). Without loss of generality, we choose
the best baseline BERT + MaskCorrect as the
framework and conduct experiments on both
SIGHAN14 and SIGHAN15 with pvarying in
{0.1,0.15,0.2,0.25,0.3,0.4,0.5}. The results are
shown in Table 2.
From the results, we have several observations:
1) when the mask ratio is small, there are no gains
or even a slight performance drop. Because when
the mask ratio is small, there is no enough knowl-
edge learned from the correct tokens and the mask
tokens may hinder the correction of error tokens.
2) when the mask ratio is large, the MaskCorrect
cannot bring the expected benefits. Empirically, we
find that under a large mask ratio, the cross-entropy
loss is always large and cannot be well-optimized
even with many epochs. This is because when we
mask too much correct tokens, the language infor-
mation loses too much and the language model
cannot learn the correct knowledge from it. 3) the
best choice of mask ratio is 0.2 in this task for the
BERT correction model.
4.2 ASR Error Correction
In this section, we evaluate the effectiveness of our
method on the ASR Error Correction task.
4.2.1 Dataset and ASR Model
We conduct the experiments on two Chinese
datasets, the public dataset AISHELL-1 (Bu et al.,
2017) and the inhouse dataset. Following the previ-
ous works (Zhu et al., 2021), we employ the ASR
model to train on the training set. Then the ASR
model is used to transcribe the ASR training split
again to generate the transcripts. The transcripts
along with the corresponding ground truth texts
are used to train the ASR correction model. The
validation and test splits for ASR correction are
inferred by the above ASR model. The details of
dataset statistics and ASR model can be found in
Appendix A.2 and B.3, respectively.
However, for AISHELL-1 dataset, it’s too small
to achieve good correction performance (Leng
et al., 2021b,a). To address this issue, we follow
the previous works’ (Leng et al., 2021b,a) practice
to construct a much larger pseudo training corpus
to pretrain the correction model. Details can be
found in Appendix B.2. As for the inhouse dataset,
since it’s large enough to achieve good results, we
don’t apply unpaired data pretraining and directly
train the correction model from scratch.4.2.2 Baseline Methods and Evaluation
Metric
FELIX FELIX (Mallinson et al., 2020) is a NAR
algorithm for text edition. It aligns the mismatch
tokens between the source and target sentences
by two sub-tasks: 1) decide the subset of tokens
in source sentences and their order in the target
sentences by tagging operation and 2) insert tokens
to infill the missing tokens in the target sentences.
LevT Levenshtein Transformer (LevT) (Gu et al.,
2019) is a partially autoregressive seq2seq model
devised for flexible sequence generation tasks. It
implicitly predicts the insertion and deletion with
multiple iterations in the decoder.
Autoregressive Correction Model (AR Trans-
former) For the autoregressive sequence gen-
eration model, we follow the standard Trans-
former (Vaswani et al., 2017) which takes the sen-
tences with potential errors as input and outputs the
corrected sentences autoregressively.
FastCorrect FastCorrect (Leng et al., 2021b) is
a non-autoregressive ASR correction model based
on edit distance. It employs edit distance to find
the fine-grained mapping between error tokens and
the corresponding correct tokens based on edit op-
eration: insertion, deletion and substitution. We
follow the default setting and implement it based
on the official code.
As for the evaluation metrics, we follow the prac-
tice in previous works and adopt the word error rate
(WER) and Word Error Reduction Rate (WERR) as
the automatic evaluation metric. We use the official
code in ESPnet to calculate it.
4.2.3 Implementation Details
We evaluate our method on two SOTA ASR correc-
tion model in both AR and NAR, namely AR Trans-
former andFastCorrect . Similar to the Spelling
Error Correction 4.1.4, we first collect the correct
tokens from inputs and randomly mask the tokens
with ratio pand probability mandn. It is worth
noting that we apply the MaskCorrect on both un-
paired and paired training corpus for AISHELL-1
and only paired training corpus for inhouse dataset
(since we do not use unpaired training corpus for
the inhouse dataset as mentioned in Section 4.2.1).
In addition, since there is no <mask> token dur-
ing inference, we have to finetune on the original10373MethodsAISHELL-1 Inhouse Corpus
Test Set Dev Set Test Set
WER↓WERR ↑WER↓WERR ↑WER↓WERR ↑
No Correction 4.83 - 4.46 - 8.99 -
Non-autoregressive correction models
LevT(MIter=1) 4.73 2.07 4.37 2.02 8.86 1.45
LevT(MIter=3) 4.74 1.86 4.38 1.79 8.89 1.11
FELIX 4.63 4.14 4.26 4.48 8.74 2.78
FastCorrect 4.16 13.87 3.89 13.30 8.44 6.12
+ MaskCorrect 4.10 15.11 3.82 14.35 8.35 7.12
Autoregressive correction models
AR Transformer 4.08 15.53 3.80 14.80 8.32 7.45
+ MaskCorrect 4.03 16.56 3.75 15.92 8.25 8.23
data without mask for 3 epochs. Please refer to
Appendix C.2 for more details.
4.2.4 Results Analysis
In this section, we compare our method with the
baseline models on the two datasets for ASR Error
Correction. We apply the proposed MaskCorrect
framework on two baselines: FastCorrect andAR
Transformer , for NAR and AR correction diagrams,
respectively. Note that we only apply our method to
the strongest non-autoregressive correction model,
FastCorrect. The results are shown in Table 3.
From the table, we can have the following ob-
servations. Firstly, the AR + MaskCorrect model
achieves the best performance compared with all
other baselines, which demonstrates the effective-
ness of our method. Secondly, by comparing the
FastCorrect and AR Transformer with the corre-
sponding variants, we can find that the MaskCor-
rect can enhance both baselines on all datasets.
Concretely, with the MaskCorrect, the FastCorrect
can further reduce the WER (measured by WERR)
of ASR model by 1.24% and 1% on the test sets of
the AISHELL-1 and inhouse dataset, respectively.
Meanwhile the AR Transformer can reduce further
the WER by 1.03% and 0.78%, respectively. It
confirms that the MaskCorrect is useful for both
AR and NAR methods. By adding a perturb (i.e.,
masking off some correct tokens), the correction
model can learn from the correct tokens, which is
beneficial to the ASR error correction.4.2.5 Ablation Study
In this section, we conduct an ablation study to
verify the affect of the mask ratio p. We choose
the best NAR baseline FastCorrect + MaskCor-
rect as the framework and apply different value of
pvarying in {0.1,0.15,0.2,0.25,0.3,0.4,0.5}on
the training set. The results are shown in Table 4.
From the results, we have several observations:
1) when the mask ratio falls in [0, 0.2], the
MaskCorrect will bring a positive impact to the
FastCorrect, otherwise it will cause a slight perfor-
mance drop. We think when the mask ratio is too
large, the semantic information contained in the in-
complete text (with mask tokens and error tokens)
is missing too much. Thus, it is too difficult for
FastCorrect to recover it. When the mask ratio is
in a suitable range, FastCorrect will benefit from
recovering the mask tokens. 2) the best choice of
mask ratio is 0.15 for the FastCorrect.
AISHELL-1 Inhouse Corpus
Mask Ratio WER WERR WER WERR
0% 4.16 13.87 8.38 6.79
10% 4.14 15.75 8.35 7.12
15% 4.10 16.67 8.32 7.45
20% 4.16 15.30 8.34 7.23
25% 4.19 14.61 8.34 7.23
30% 4.2 14.38 8.38 6.79
40% 4.22 13.93 8.40 6.56
50% 4.24 13.47 8.41 6.45103745 Conclusion
In this work, we proposed a effective and plug-
and-play framework to address the trivial copy phe-
nomenon, in which we randomly perturb a propor-
tion of correct tokens into a special token <mask> .
The model aims to predict the correct tokens from
the special token. We apply this mechanism to
both AR and NAR models for Spelling Error Cor-
rection tasks and the ASR Correction tasks. And
experimental results show the effectiveness of the
proposed method. Furthermore, our method can
be easily extended to other tasks which suffer from
heavy copy phenomenon.
6 Acknowledgements
This work has been supported in part by the Zhe-
jiang NSF (LR21F020004), Chinese Knowledge
Center of Engineering Science and Technology
(CKCEST).
7 Limitation
In this work, we propose a framework MaskCorrect
to address the heavily copy phenomenon in the
spelling and ASR correction tasks. We partially
solve the problem by a simple mask strategy with
quantitative experiments. But, a more in-depth
theoretical analysis of how to prevent the negative
copy impact and how to adaptively mask the copied
tokens is worth studying. Furthermore, despite
the learning diagram we have explored, how to
adapt the conventional model structure to cope with
the heavily copied phenomenon is another future
direction.
References1037510376
A Dataset Statistics
A.1 The Dataset Statistics for Spelling Error
Correction
We provide the detailed dataset statistics for
spelling error correction in Table 5.
Traning data # Sent Avg. Len
Unpaired Corpus 5 million 51
Aug. 271329 44.4
SIGHAN 14 6526 49.7
SIGHAN 15 3174 30.0
Paired Corpus Sum. 281029 44.3
Test Data # Sent Avg. Len
SIGHAN 14 1062 50.1
SIGHAN 15 1100 30.510377A.2 The dataset statistics for ASR Error
Correction
The AISHELL-1 dataset is a Mandarin Speech cor-
pus containing 178 hours of training audios, 10
hours of validation audios and 5 hours of test au-
dios. And the inhouse dataset is a much larger
dataset for the industrial Mandarin ASR system. It
contains 75K hours of Mandarin speech data for
training and 200 hours of speech for both validation
and test.
B Dataset
B.1 Add Noise to unpaired data for Spelling
Error Correction
We take the official confusion dictionary released
by (Wu et al., 2013), which is a collection of map-
ping from one Chinese character to its’ widely mis-
taken characters. Following (Zhang et al., 2020),
we randomly replace 15% of the characters in them
with the mistaken characters by the confusion dic-
tionary to artificially generate corresponding sen-
tences with errors.
B.2 Add Noise to unpaired data for ASR
Error Correction
We first crawl 400M unpaired sentences from the
Internet. Secondly, we find out the insertion, dele-
tion, and substitution error rate in the training set.
Finally, for each unpaired sentence, we apply ran-
dom insertion, deletion, and substitution with a
homophone dictionary based on the error rate statis-
tics.
B.3 The ASR Model for ASR Error
Correction
As for the ASR model, we adopt a conformer with
12 layers encoder and 6 layers decoder, which is
trained with cross-entropy loss and an auxiliary
CTC loss on the encoder output. The hidden size of
the conformer’s encoder and decoder layers is 512.
We use ESPnetas the codebase. For the inhouse
dataset, the ASR model is a hybrid model consisted
of a latency-controlled BLSTM (Zhang et al., 2016)
acoustic model(am) and a 5-gram language model
which is trained on 436 billion tokens.C Implementation Details
C.1 Implementation Details of Spelling Error
Correction
For a fair comparison, we choose the same pre-
trained BERT model for the above two methods
provided by HuggingFace. For the BERT correc-
tion model, we use 2 layers of MLPs with softmax
to transform the bert embeddings to the target to-
ken probability distribution. Another 2 layers of
MLPs are used to predict the error probability as an
assistant loss. For the SoftMask BERT correction
model, we use a bidirectional GRU with hidden
dimension 768 as the error detection network. The
MLP layers’ dimension is set to 1024. The learning
rate is set to 1e-4. We pretrain on the unpaired data
with 9 epochs and finetune on the paired data with
4 epochs. The pretrain batch size is 1024 and the
finetune batch size is 128. The mask probability
ofmis 80%, and the randomly replace probabil-
ity of nis 10%. We mask the correct tokens with
probability p=20%.
C.2 Implementation Details of ASR Error
Correction
We use SentencePiece (Kudo and Richardson,
2018) to learn the subwords and apply it to all
the texts above. The dictionary size is set to 40K.
We train all correction models on 4 NVIDIA Tesla
V100 GPUs, with a batch size of 12000 tokens.
The mask probability of mis 80%, and the ran-
domly replace probability of nis 10%. We mask
the correct tokens with probability p=15%.
D Experiments
D.1 Grammar Error Correction
In this section, we evaluate the effectiveness of our
method on the Grammar Error Correction (GEC)
task.
D.1.1 Datasets
We conduct the experiments on the datasets of
BEA-2019 GEC shared task (Bryant et al., 2019),
restricted track. Specifically, the training set is
consisted by four parts: Lang-8 (Mizumoto et al.,
2011), the FCE training set (Yannakoudakis et al.,
2011), NUCLE (Dahlmeier et al., 2013), and
W&I+LOCNESS (Granger, 2014; Bryant et al.,
2019). We use the CoNLL-2013 (Ng et al., 2013)
test set as our dev set and the CoNLL-2014 (Ng10378et al., 2014) test set as our test set. We follow the
S2A’s (Li et al., 2022a) practice and preprocess
and tokenize all the sentences by 32K Sentence-
Piece (Kudo and Richardson, 2018) Byte Pair En-
coding (BPE) (Sennrich et al., 2015).
We use the official MaxMatch ( M)
scorer (Dahlmeier and Ng, 2012) to evalu-
ate the models in the English GEC tasks. Given
a source sentence, and a ground-truth sentence
sample, the Mscorer searches for the highest
overlap between them and reports the precision,
recall, and F.
D.1.2 Baseline Methods
Transformer-big The transformer (Vaswani
et al., 2017) architecture has shown its success in
the sequence-to-sequence fields. We simply cast
the GEC task as a sequence-to-sequence problem,
which takes the sentences with errors as inputs and
predicts the correct sentences autoregressively. We
follow the default setting of transformer-big and
implement it based on the official codes. .
Sequence-to-Action (S2A) The sequence-to-
action (Li et al., 2022a) is a strong baseline based
on sequence tagging, which is another diagram
compared with seq2seq methods for GEC. The S2A
method first predicts the token-level sequence ac-
tions and fuses the actions with the seq2seq frame-
work to generate the final sentences.
D.1.3 Implementation Details
We evaluate our method on the transformer-big
architecture. Similar to the previous tasks, we first
collect the correct tokens from inputs and randomly
mask the tokens with ratio 0.2and probability m=
0.8andn= 0.1. We finetune the model by 2
epochs.
D.1.4 Results Analysis
In this section, we compare our method with the
baseline models for the GEC task. The results are
shown in Table 6. From the results, we have sev-
eral observations. Firstly, our method improves the
transformer-big baseline for the English GEC task,
which demonstrates the effectiveness of MaskCor-
rect. Secondly, with MaskCorrect, the transformer-
big achieves state-of-art performance on all met-
rics.10379Model Precision Recall F
Transformer-big 64.9 26.6 50.4
Transformer-big + MaskCorrect 67.2 30.0 53.9
Sequence-to-Action 65.9 28.9 52.510380