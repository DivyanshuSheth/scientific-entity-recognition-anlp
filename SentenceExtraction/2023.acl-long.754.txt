
Yizhong WangYeganeh KordiSwaroop MishraAlisa Liu
Noah A. SmithDaniel KhashabiHannaneh HajishirziUniversity of WashingtonTehran PolytechnicArizona State UniversityJohns Hopkins UniversityAllen Institute for AI
yizhongw@cs.washington.edu
Abstract
Large “instruction-tuned” language models
(i.e., finetuned to respond to instructions) have
demonstrated a remarkable ability to general-
ize zero-shot to new tasks. Nevertheless, they
depend heavily on human-written instruction
data that is often limited in quantity, diversity,
and creativity, therefore hindering the general-
ity of the tuned model. We introduce S-
I , a framework for improving the
instruction-following capabilities of pretrained
language models by bootstrapping off their
own generations. Our pipeline generates in-
structions, input, and output samples from a
language model, then filters invalid or similar
ones before using them to finetune the original
model. Applying our method to the vanilla
GPT3 , we demonstrate a 33% absolute im-
provement over the original model on S -
N I , on par with the
performance of InstructGPT,which was
trained with private user data and human anno-
tations. For further evaluation, we curate a set
of expert-written instructions for novel tasks,
and show through human evaluation that tun-
ing GPT3 with S-I outperforms
using existing public instruction datasets by
a large margin, leaving only a 5% absolute
gap behind InstructGPT.S-I
provides an almost annotation-free method for
aligning pretrained language models with in-
structions, and we release our large synthetic
dataset to facilitate future studies on instruction
tuning.
1 Introduction
The recent NLP literature has witnessed a tremen-
dous amount of activity in building models that canFigure 1: Selected tasks from the generated instruction
data using vanilla GPT3 . Some texts are reformatted
for presentation. See Table 10 for more examples.
follow natural language instructions (Mishra et al.,
2022; Wei et al., 2022; Sanh et al., 2022; Wang
et al., 2022; Ouyang et al., 2022; Chung et al., 2022,
i.a.). These developments are powered by two
key components: large pretrained language mod-
els (LM) and human-written instruction data (e.g.,
P S (Bach et al., 2022) and S -
N I (Wang et al., 2022, S-NIfor short)). However, collecting such in-
struction data is costly and often suffers limited
diversity given that most human generations tend
to be popular NLP tasks, falling short of cover-
ing a true variety of tasks and different ways to13484
describe them. Continuing to improve the quality
and coverage of instruction-tuned models necessi-
tates the development of alternative approaches for
supervising the instruction tuning process.
In this work, we introduce S-I ,
a semi-automated process for instruction-tuning
a pretrained LM using instructional signals from
the model itself. The overall process is an itera-
tive bootstrapping algorithm (see Figure 2), which
starts off with a limited (e.g., 175 in our study)
seed set of manually-written tasks that are used to
guide the overall generation. In the first phase, the
model is prompted to generate instructions for new
tasks. This step leverages the existing collection
of instructions to create more broad-coverage in-
structions that define (often new) tasks. Given the
newly-generated set of instructions, the framework
also creates input-output instances for them, which
can be later used for supervising the instruction
tuning. Finally, various heuristics are used to auto-
matically filter low-quality or repeated instructions,
before adding the remaining valid tasks to the task
pool. This process can be repeated for many itera-
tions until reaching a large number of tasks.
To evaluate S-I empirically, we
run this framework on GPT3 (Brown et al., 2020),
which is a vanilla LM (§3). The iterative S-
I process on this model leads to about 52k
instructions, paired with about 82K instance inputs
and target outputs. We observe that the resulting
data provides a diverse range of creative tasks, asis demonstrated by examples in Figure 1. These
generated tasks deviate from the distribution of typ-
ical NLP tasks, and also have fairly small overlap
with the seed tasks (§3.2). On this resulting data,
we build GPT3by finetuning GPT3 (i.e.,
the same model used for generating the instruction
data). We evaluate GPT3in comparison to
various other models on both typical NLP tasks in-
cluded in S NI(Wang et al., 2022), and a set of
new instructions that are created for novel usage of
instruction-following models (§4). The results in-
dicate that GPT3outperforms GPT3 (the
original model) by a large margin (+33.1%) and
nearly matches the performance of InstructGPT.
Moreover, our human evaluation on the newly-
created instruction set shows that GPT3
demonstrates a broad range of instruction follow-
ing ability, outperforming models trained on other
publicly available instruction datasets and leaving
only a 5% gap behind InstructGPT.
In summary, our contributions are: (1) we in-
troduce S-I , a method for induc-
ing instruction following capabilities with mini-
mal human-labeled data; (2) we demonstrate its
effectiveness via extensive instruction-tuning ex-
periments; and (3) we release a large synthetic
dataset of 52K instructions and a set of manually-
written novel tasks for building and evaluating fu-
ture instruction-following models.134852 Method
Annotating large-scale instruction data can be chal-
lenging for humans because it requires 1) creativity
to come up with novel tasks and 2) expertise for
writing the solutions to each task. Here, we de-
tail our process for S-I , which refers
to the pipeline of generating tasks with a vanilla
pretrained language model itself, filtering the gen-
erated data, and then conducting instruction tuning
with this generated data in order to align the LM to
follow instructions better. This pipeline is depicted
in Figure 2.
2.1 Defining Instruction Data
The instruction data we want to generate con-
tains a set of instructions {I}, each of which
defines a task tin natural language. Task thas
n≥1input-output instances {(X, Y)}. A
model Mis expected to produce the output, given
the task instruction and the corresponding input:
M(I, X) =Y, fori∈ {1, . . . , n}. Note that
the instruction and instance input does not have a
strict boundary in many cases. For example, “write
an essay about school safety” can be a valid instruc-
tion that we expect models to respond to directly,
while it can also be formulated as “write an essay
about the following topic” as the instruction, and
“school safety” as an instance input. To encourage
the diversity of the data format, we allow such in-
structions that do not require additional input (i.e.,
Xis empty).
2.2 Automatic Instruction Data Generation
Our pipeline for data generation consists of four
steps: 1) generating task instructions, 2) determin-
ing if the instruction represents a classification task,
3) instance generation with either an input-first or
output-first approach, and 4) filtering low-quality
data.
Instruction Generation. At the first step, S-
I generates new instructions from a small
set of seed human-written instructions in a boot-
strapping fashion. We initiate the task pool with
175 tasks (1 instruction and 1 instance for each
task).For every step, we sample 8 task instruc-
tions from this pool as in-context examples. Ofthe 8 instructions, 6 are from the human-written
tasks, and 2 are from the model-generated tasks in
previous steps to promote diversity. The prompting
template is shown in Table 5.
Classification Task Identification. Because we
need two different approaches for classification and
non-classification tasks, we next identify whether
the generated instruction represents a classification
task or not.We prompt the LM in a few-shot way
to determine this, using 12 classification instruc-
tions and 19 non-classification instructions from
the seed tasks. The prompting template is shown
in Table 6.
Instance Generation. Given the instructions and
their task type, we generate instances for each in-
struction independently. This is challenging be-
cause it requires the model to understand what the
target task is, based on the instruction, figure out
what additional input fields are needed and gener-
ate them, and finally complete the task by produc-
ing the output. We found that pretrained LMs can
achieve this to a large extent when prompted with
instruction-input-output in-context examples from
other tasks. A natural way to do this is the Input-
first Approach , where we can ask an LM to come
up with the input fields first based on the instruc-
tion, and then produce the corresponding output.
This generation order is similar to how models are
used to respond to instruction and input, but here
with in-context examples from other tasks. The
prompting template is shown in Table 7.
However, we found that this approach can gen-
erate inputs biased toward one label, especially
for classification tasks (e.g., for grammar error de-
tection, it usually generates grammatical input).
Therefore, we additionally propose an Output-first
Approach for classification tasks, where we first
generate the possible class labels, and then condi-
tion the input generation on each class label. The
prompting template is shown in Table 8.We ap-
ply the output-first approach to the classification
tasks identified in the former step, and the input-
first approach to the remaining non-classification
tasks.13486Filtering and Postprocessing. To encourage di-
versity, a new instruction is added to the task pool
only when its ROUGE-L similarity with any exist-
ing instruction is less than 0.7. We also exclude
instructions that contain some specific keywords
(e.g., image, picture, graph) that usually can not be
processed by LMs. When generating new instances
for each instruction, we filter out instances that are
exactly the same or those with the same input but
different outputs. Invalid generations are identified
and filtered out based on heuristics (e.g., instruc-
tion is too long or too short, instance output is a
repetition of the input).
2.3 Finetuning the LM to Follow Instructions
After creating large-scale instruction data, we use it
to finetune the original LM (i.e., S-I ).
To do this, we concatenate the instruction and in-
stance input as a prompt and train the model to
generate the instance output in a standard super-
vised way. To make the model robust to different
formats, we use multiple templates to encode the
instruction and instance input together. For exam-
ple, the instruction can be prefixed with “Task:” or
not, the input can be prefixed with “Input:” or not,
“Output:” can be appended at the end of the prompt
or not, and different numbers of break lines can be
put in the middle, etc.
3 S-I Data from GPT3
In this section, we apply our method for inducing
instruction data to GPT3 as a case study. We use
the largest GPT3 LM (“davinci” engine) accessed
through the OpenAI API.The parameters for mak-
ing queries are described in Appendix A.2. Here
we present an overview of the generated data.
3.1 Statistics
Table 1 describes the basic statistics of the gener-
ated data. We generate a total of over 52K instruc-
tions and more than 82K instances corresponding
to these instructions after filtering.
3.2 Diversity
To study what types of instructions are generated
and how diverse they are, we identify the verb-noun
structure in the generated instructions. We use the
Berkeley Neural Parser(Kitaev and Klein, 2018;
Kitaev et al., 2019) to parse the instructions and
then extract the verb that is closest to the root as
well as its first direct noun object. 26,559 out of the
52,445 instructions contain such structure; other
instructions usually contain more complex clauses
(e.g., “Classify whether this tweet contains political
content or not.”) or are framed as questions (e.g.,
“Which of these statements are true?”). We plot
the top 20 most common root verbs and their top 4
direct noun objects in Figure 3, which account for
14% of the entire set. Overall, we see quite diverse
intents and textual formats in these instructions.
We further study how the generated instructions
differ from the seed instructions used to prompt
the generation. For each generated instruction, we
compute its highest ROUGE-L overlap with the
175 seed instructions. We plot the distribution of
these ROUGE-L scores in Figure 4. The results
indicate a decent number of new instructions were
generated, which do not have much overlap with
the seeds. We also demonstrate diversity in the
length of the instructions, instance inputs, and in-
stance outputs in Figure 5.
3.3 Quality
So far, we have shown the quantity and diversity
of the generated data, but its quality remains un-
certain. To investigate this, we randomly sample
200 instructions and randomly select 1 instance
per instruction. We asked an expert annotator (au-
thor of this work) to label whether each instance
is correct or not, in terms of the instruction, the
instance input, and the instance output. Evaluation
results in Table 2 show that most of the generated
instructions are meaningful, while the generated
instances may contain more noise (to a reasonable
extent). However, we found that even though the
generations may contain errors, most of them are
still in the correct format or partially correct, which
can provide useful guidance for training models to
follow instructions. We listed a number of good13487
examples and bad examples in Table 10 and 11,
respectively.
4 Experimental Results
We conduct experiments to measure and compare
the performance of models under various instruc-
tion tuning setups. We first describe our models
and other baselines, followed by our experiments.
4.1 GPT3: finetuning GPT3 on its
own instruction data
Given the instruction-generated instruction data,
we conduct instruction tuning with the GPT3model itself (“davinci” engine). As described in
§2.3, we use various templates to concatenate the
instruction and input, and train the model to gen-
erate the output. This finetuning is done through
the OpenAI finetuning API.We use the default
hyper-parameters, except that we set the prompt
loss weight to 0, and we train the model for 2
epochs. We refer the reader to Appendix A.3 for
additional finetuning details. The resulting model
is denoted by GPT3.
4.2 Baselines
Off-the-shelf LMs. We evaluate T5-LM (Lester
et al., 2021; Raffel et al., 2020) and GPT3 (Brown
et al., 2020) as the vanilla LM baselines (only pre-
training, no additional finetuning). These baselines
will indicate the extent to which off-the-shelf LMs
are capable of following instructions naturally im-
mediately after pretraining.
Publicly available instruction-tuned models.
T0andTk-I are two instruction-tuned
models proposed in Sanh et al. (2022) and Wang
et al. (2022), respectively, and are demonstrated
to be able to follow instructions for many NLP
tasks. Both of these models are finetuned from the13488T5 (Raffel et al., 2020) checkpoints and are pub-
licly available.For both of these models, we use
their largest version with 11B parameters.
Instruction-tuned GPT3 models. We evaluate
InstructGPT (Ouyang et al., 2022), which is de-
veloped by OpenAI based on GPT3 to follow hu-
man instructions better and has been found by
the community to have impressive zero-shot abil-
ities. There are various generations of these mod-
els, where newer ones use more expansive data
or algorithmic novelties.For our S NIex-
periments in §4.3, we only compare with their
text-davinci-001 engine, because their newer
engines are trained with the latest user data and are
likely to have already seen the S NItest set.
For our human evaluation on newly written instruc-
tions, we include their 001, 002 and 003 engines
for completeness.
Additionally, to compare S-I train-
ing with other publicly available instruction tuning
data, we further finetune GPT3 model with data
from P S andS NI, which are
used to train the T0andTk-I models.
We call them T0training and S NItraining
for short, respectively. To save the training bud-
get, we sampled 50K instances (but covering all
their instructions) for each dataset, which has a
comparable size to the instruction data we gener-
ated. Based on the findings from Wang et al. (2022)
and our early experiments, reducing the number of
instances per task does not degrade the model’s
generalization performance to unseen tasks.
4.3 Experiment 1: Zero-Shot Generalization
on S NI benchmark
We first evaluate the models’ ability to follow in-
structions on typical NLP tasks in a zero-shot fash-
ion. We use the evaluation set of S NI(Wang
et al., 2022), which consists of 119 tasks with 100
instances in each task. In this work, we mainly
focus on the zero-shot setup, i.e., the model is
prompted with the definition of the tasks only, with-
out in-context demonstration examples. For all our
requests to the GPT3 variants, we use the deter-
ministic generation mode (temperature as 0 and no
nucleus sampling) without specific stop sequences.
Results. We make the following observations
from the results in Table 3. S-I
boosts the instruction-following ability of GPT3 by
a large margin. The vanilla GPT3 model basically
cannot follow human instructions at all. Upon man-
ual analysis, we find that it usually generates irrele-
vant and repetitive text, and does not know when
to stop generation. Compared with other mod-
els that are not specifically trained for S NI,
GPT3achieves better performance than
T0or the GPT3 finetuned on the T0training set,
which takes tremendous human labeling efforts.
Notably, GPT3also nearly matches the
performance of InstructGPT, which is trained
with private user data and human-annotated labels.
Models trained on the S NItraining set still
achieve better performance on its evaluation set,
which we attribute to the similar instruction style
and formatting. However, we show that S-
I still brings in additional gains when
combined with the S NI training set, proving
its value as complementary data.
4.4 Experiment 2: Generalization to
User-oriented Instructions on Novel Tasks
Despite the comprehensiveness of S NIin col-
lecting existing NLP tasks, most of these NLP tasks
were proposed for research purposes and skewed
toward classification. To better access the practi-
cal value of instruction-following models, a sub-
set of the authors curate a new set of instructions
motivated by user-oriented applications. We first
brainstorm various domains where large LMs may
be useful (e.g., email writing, social media, pro-
ductivity tools, entertainment, programming), then13489
craft instructions related to each domain along with
an input-output instance (again, input is optional).
We aim to diversify the styles and formats of these
tasks (e.g., instructions may be long or short; in-
put/output may take the form of bullet points, ta-
bles, codes, equations, etc.). In total, we create 252
instructions with 1 instance per instruction. We
believe it can serve as a testbed for evaluating how
instruction-based models handle diverse and unfa-
miliar instructions. Table 9 presents a small portion
of them. The entire set is available in our GitHub
repository. We analyze the overlap between this set
set and the seed instructions in §A.1.
Human evaluation setup. Evaluating models’
performance on this evaluation set of diverse tasks
is extremely challenging because different tasks
require different expertise. Indeed, many of these
tasks cannot be measured by automatic metrics or
even be judged by normal crowdworkers (e.g., writ-
ing a program or converting first-order logic into
natural language). To get a more faithful evalua-
tion, we asked the authors of the instructions to
judge model predictions. Details on how we set up
this human evaluation are described in Appendix B.
The evaluators were asked to rate the output based
on whether it accurately and effectively completes
the task. We implemented a four-level rating sys-
tem for categorizing the quality of models’ outputs:
•R -A:The response is valid and satisfying.
•R -B:The response is acceptable but has
minor errors or imperfections.
•R -C: The response is relevant and re-sponds to the instruction, but it has significant
errors in the content. For example, GPT3 might
generate a valid output first, but continue to gen-
erate other irrelevant things.
•R -D:The response is irrelevant or com-
pletely invalid.
Results. Figure 6 shows the performance of
GPT3 model and its instruction-tuned counterparts
on this newly written instruction set (w. inter-
rater agreement κ= 0.57on the 4-class cate-
gorical scale, see Appendix B for details). As
anticipated, the vanilla GPT3 LM is largely un-
able to respond to instructions, and all instruction-
tuned models demonstrate comparatively higher
performance. Nonetheless, GPT3(i.e.,
GPT3 model finetuned with S-I )
outperforms those counterparts trained on T0or
S NIdata by a large margin, demonstrating
the value of the generated data despite the noise.
Compared with InstructGPT,GPT3is
quite close in performance—if we count accept-
able response with minor imperfections ( R -
B) as valid, GPT3is only 5% behind
InstructGPT. Lastly, our evaluation confirms
the impressive instruction-following ability of
InstructGPTand InstructGPT. Although
there are many factors behind this success, we con-
jecture that future work can largely benefit from
improving the quality of our generated data by us-
ing human annotators or training a reward model to
select better generations, similar to the algorithm
used by Ouyang et al. (2022).134904.5 Effect of Data Size and Quality
Data size. S-I provides a way to
grow instruction data at a low cost with almost no
human labeling; could more of this generated data
lead to better instruction-following ability? We an-
alyze the size of generated data by subsampling dif-
ferent numbers of instructions from the generated
dataset, finetuning GPT3 on the sampled subsets,
and evaluating how the resulting models perform
on the 252 user-oriented instruction set. We con-
duct the same human evaluation as in §4.4. Figure 7
presents the performance of GPT3mod-
els finetuned with different sizes of generated data.
Overall, we see consistent improvement as we grow
the data size. However, this improvement almost
plateaus after 16K. This is in-line with the data
scaling experiments in Wang et al. (2022, Fig. 5).
Interestingly, when evaluating on S NIwe
found the model’s performance gain plateaus ear-
lier at around hundreds of instructions. This may
be due to the fact that the new generated data is
distinct from typical NLP tasks in S NI, indi-
cating that future research may benefit from using a
combination of different instruction data for better
performance on various types of tasks.
Data quality. Another direction to improve the
model’s performance is to take our generated data
and get better supervision (with less noise). We
explore this idea by using InstructGPT(the best
available general-purpose model) to regenerate the
output field of all our instances given the instruc-
tion and input. We then use this improved ver-
sion of our data to finetune GPT3 . As is shown
in Figure 7, the resulting model outperforms the
counterpart trained with the original data by 10%,
which suggests big room for future work on using
our generation pipeline to get initial data and then
improving the data quality with human experts or
distillation from better models.
5 Related Work
Instruction-following LMs. A series of works
have found evidence that vanilla LMs can be effec-
tive at following general language instructions if
tuned with annotated “instructional” data—datasets
containing language instructional commands and
their desired outcomes based on human annota-
tion (Weller et al., 2020; Mishra et al., 2022; Wei
et al., 2022; Sanh et al., 2022, i.a.). Additionally,
they show a direct correlation between the size and
diversity of the “instructional” data and the general-
izability of resulting models to unseen tasks (Wang
et al., 2022; Chung et al., 2022). However, since
these developments largely focus on existing NLP
tasks and depend on human-annotated instructions,
this poses a bottleneck for progress toward more
generalizable models (e.g., see Fig. 5a in Wang
et al., 2022). Our work aims to move beyond classi-
cal NLP tasks and tackle the challenges of creating
diverse instruction data by employing pretrained
LMs. InstructGPT (Ouyang et al., 2022) shares
a similar goal as ours in building more general-
purpose LMs, and has demonstrated remarkable
performance in following diverse user instructions.
However, as a commercial system, their construc-
tion process still remains quite opaque. In partic-
ular, the role of data has remained understudied
due to limited transparency and the private user
data they used in their study. Addressing such chal-
lenges necessitates the creation of a large-scale,
public dataset covering a broad range of tasks.
Language models for data generation and aug-
mentation. A variety of works have proposed us-
ing LMs for data generation (Schick and Schütze,
2021; Wang et al., 2021; Liu et al., 2022; Meng
et al., 2023) or augmentation (Feng et al., 2021;
Yang et al., 2020; Mekala et al., 2022). Our work
differs from this line in that it is notspecific to a par-
ticular task (say, QA or NLI). In contrast, a distinct13491motivation for S-I is to bootstrap new
task definitions that may not have been defined be-
fore by NLP practitioners (though potentially still
important for real users). In parallel with our work,
Honovich et al. (2022a) also propose to generate
large-scale instruction data (so-called Unnatural
Instructions) with GPT3 models. The major differ-
ences are that 1) they use tasks in S NI(Wang
et al., 2022) as their seed tasks, resulting in a differ-
ent distribution of generated tasks; 2) they employ
InstructGPTfor generating the data, in which
sense they are distilling knowledge from an already
instruction-tuned model, while we solely rely on
the vanilla LM; 3) the detailed generation pipeline
and templates are different. Nevertheless, we be-
lieve that both efforts in expanding instruction data
are complementary, and the community will benefit
from these diverse datasets.
Instruction generation. A series of recent
works (Zhou et al., 2022b; Ye et al., 2022; Singh
et al., 2022; Honovich et al., 2022b) generate in-
structions of a task given a few examples. While
S-I also involves instruction gener-
ation, a major difference in our case is it is task-
agnostic; we generate new tasks (instructions along
with instances) from scratch.
Model self-training. A typical self-training
framework (He et al., 2019; Xie et al., 2020; Du
et al., 2021; Amini et al., 2022; Huang et al., 2022)
uses trained models to assign labels to unlabeled
data and then leverages the pseudo-labeled data to
improve the model. Zhou et al. (2022a) use mul-
tiple prompts to specify a single task and propose
to regularize via prompt consistency, encouraging
consistent predictions over the prompts. This al-
lows either finetuning the model with extra unla-
beled training data, or direct application at infer-
ence time. While S-I has similarities
with the self-training literature, most self-training
methods assume a specific target task as well as
unlabeled examples under it; in contrast, S-
I produces a variety of tasks from scratch.
Knowledge distillation. Knowledge distilla-
tion (Hinton et al., 2015; Sanh et al., 2019; West
et al., 2021; Magister et al., 2022) often involves
the transfer of knowledge from larger models to
smaller ones. S-I can also be viewed
as a form of “knowledge distillation", however, it
differs from this line in the following ways: (1) the
source and target of distillation are the same, i.e., amodel’s knowledge is distilled to itself; (2) the con-
tent of distillation is in the form of an instruction
task (i.e., instructions that define a task, and a set
of examples that instantiate it).
Bootstrapping with limited resources. A se-
ries of recent works use language models to boot-
strap some inferences using specialized methods.
NPPrompt (Zhao et al., 2022) provides a method
to generate predictions for semantic labels without
any finetuning. It uses a model’s own embeddings
to automatically find words relevant to the label of
the data sample and hence reduces the dependency
on manual mapping from model prediction to la-
bel (verbalizers). STAR (Zelikman et al., 2022)
iteratively leverages a small number of rationale
examples and a large dataset without rationales,
to bootstrap a model’s ability to perform reason-
ing. Self-Correction (Welleck et al., 2023) decou-
ples an imperfect base generator (model) from a
separate corrector that learns to iteratively correct
imperfect generations and demonstrates improve-
ment over the base generator. Our work instead
focuses on bootstrapping new tasks in the instruc-
tion paradigm.
Multi-modal instruction-following. Instruction-
following models have also been of interest in the
multi-modal learning literature (Fried et al., 2018;
Shridhar et al., 2020; Min et al., 2022; Weir et al.,
2022). S-I , as a general approach to
expanding data, can potentially also be helpful in
those settings, which we leave to future work.
6 Conclusion
We introduce S-I , a method to im-
prove the instruction-following ability of LMs via
their own generation of instruction data. On ex-
perimenting with vanilla GPT3 , we automatically
construct a large-scale dataset of 52K instructions
for diverse tasks, and finetuning GPT3 on this data
leads to a 33% absolute improvement on S NI
over the original GPT3 . Furthermore, we curate
a set of expert-written instructions for novel tasks.
Human evaluation on this set shows that tuning
GPT3 with S-I outperforms using ex-
isting public instruction datasets by a large margin
and performs closely to InstructGPT. We hope
S-I can serve as the first step to align
pretrained LMs to follow human instructions, and
future work can build on top of this data to improve
instruction-following models.134927 Broader Impact
Beyond the immediate focus of this paper, we
believe that S-I may help bring
more transparency to what happens “behind the
scenes” of widely-used instruction-tuned models
likeInstructGPT or ChatGPT. Unfortunately, such
industrial models remain behind API walls as their
datasets are not released, and hence there is lit-
tle understanding of their construction and why
they demonstrate impressive capabilities. The bur-
den now falls on academia to better understand the
source of success in these models and strive for
better—and more open—models. We believe our
findings in this paper demonstrate the importance
of diverse instruction data, and our large synthetic
dataset can be the first step toward higher-quality
data for building better instruction-following mod-
els. At this writing, the central idea of this paper
has been adopted in several follow-up works for
such endeavors (Taori et al., 2023; Xu et al., 2023;
Sun et al., 2023, i.a.).
8 Limitations
Here, we discuss some limitations of this work to
inspire future research in this direction.
Tail phenomena. S-I depends on
LMs, and it will inherit all the limitations that
carry over with LMs. As recent studies have
shown (Razeghi et al., 2022; Kandpal et al., 2022),
tail phenomena pose a serious challenge to the suc-
cess of LMs. In other words, LMs’ largest gains
correspond to the frequent uses of languages (head
of the language use distribution), and there might
be minimal gains in the low-frequency contexts.
Similarly, in the context of this work, it would not
be surprising if the majority of the gains by S-
I are skewed toward tasks or instructions
that present more frequently in the pretraining cor-
pus. As a consequence, the approach might show
brittleness with respect to uncommon and creative
instructions.
Dependence on large models. Because of S-
I ’s dependence on the inductive biases
extracted from LMs, it might work best for larger
models. If true, this may create barriers to ac-
cess for those who may not have large comput-
ing resources. We hope future studies will care-
fully study the gains as a function of model size
or various other parameters. It is worthwhile tonote that instruction-tuning with human annota-
tion also suffers from a similar limitation: gains of
instruction-tuning are higher for larger models (Wei
et al., 2022).
Reinforcing LM biases. A point of concern for
the authors is the unintended consequences of
this iterative algorithm, such as the amplification
of problematic social biases (stereotypes or slurs
about gender, race, etc.). Relatedly, one observed
challenge in this process is the algorithm’s diffi-
culty in producing balanced labels, which reflected
models’ prior biases. We hope future work will
lead to better understanding of the pros and cons
of the approach.
Acknowledgements
The authors would like to thank the anonymous
reviewers for their constructive feedback. We espe-
cially thank Sewon Min, Eric Wallace, Ofir Press,
and other members of UWNLP and AllenNLP for
their encouraging feedback and intellectual sup-
port. This work was supported in part by DARPA
MCS program through NIWC Pacific (N66001-19-
2-4031), ONR N00014-18-1-2826, ONR MURI
N00014-18-1-2670, and gifts from AI2 and an
Allen Investigator award.
References13493134941349513496
A Implementation Details
A.1 Writing the Seed Tasks
Our method relies on a set of seed tasks to bootstrap the generation. The seed tasks are important for both
encouraging the task diversity and demonstrating correct ways for solving the diverse tasks. For example,
with coding tasks to prompt the model, it has a larger chance to generate coding-related tasks; it’s also
better to have coding output to guide the model in writing code for new tasks. So, the more diverse the
seed tasks are, the more diverse and better quality the generated tasks will be.
Our seed tasks were written when we initiated this project, and targeted for the diverse and interesting
usages of LLMs. The tasks were written by the authors and our labmates at UWNLP, without explicit
reference to existing datasets or specific testing tasks. We further categorized the tasks into classification
and non-classification tasks, based on whether the task has a limited output label space. In total, there are
25 classification tasks and 150 non-classification tasks. We release this data in our GitHub repository.
To provide a sense of how much the model is generalizing beyond these seed tasks, we further quantify
the overlap between the instructions of these seed tasks and the instructions of our test sets, including both
S NItask instructions (§4.3) and the user-oriented instructions in our human evaluation(§4.4). We
compute ROUGE-L similarities between each seed instruction and its most similar instruction in the test
set. The distribution of the ROUGE-L scores are plotted in Figure 8, with the average ROUGE-L similarity
between the seed instructions and S NIas 0.21, and the average ROUGE-L similarity between the
seed instructions and user-oriented instructions as 0.34. We see a decent difference between the seed tasks
and both test sets. There is exactly one identical seed instruction occurring in the user-oriented instruction
test set, which is “answer the following question” and the following questions are actually very different.
A.2 Querying the GPT3 API
We use different sets of hyperparameters when querying GPT3 API for different purposes. These
hyperparameters are found to work well with the GPT3 model (“davinci” engine) and the other instruction-
tuned GPT3 variants. We listed them in Table 4. OpenAI charges $0.02 per 1000 tokens for making
completion request to the “davinci” engine as of December, 2022. The generation of our entire dataset
cost around $600.
A.3 Finetuning GPT3
GPT3and some of our baselines are finetuned from GPT3 model (“davinci” engine with 175B
parameters). We conduct this finetuning via OpenAI’s finetuning API.While the details of how the
model is finetuned with this API are not currently available (e.g., which parameters are updated, or what13497
the optimizer is), we tune all our models with the default hyperparameters of this API so that the results are
comparable. We only set the “prompt_loss_weight” to 0 since we find this works better in our case, and
every finetuning experiment is trained for two epochs to avoid overfitting the training tasks. Finetuning is
charged based on the number of tokens in the training file. In our case, finetuning GPT3from
the GPT3 model on the entire generated data cost $338.
A.4 Prompting Templates for Data Generation
S-I relies on a number of prompting templates in order to elicit the generation from language
models. Here we provide our four templates for generating the instruction (Table 5), classifying whether
an instruction represents a classification task or not (Table 6), generating non-classification instances with
the input-first approach (Table 7), and generating classification instances with the output-first approach
(Table 8).13498134991350013501B Human Evaluation Details for Following the User-oriented Instructions
B.1 Human Evaluation Setup
Here we provide more details for the human evaluation described in §4.4 for rating the models’ responses
to the 252 user-oriented instructions. To ensure faithful and reliable evaluation, we asked two authors
of these instructions (and of this paper) to judge model predictions. These two evaluators coordinated
the standards for the 4-level rating system before starting annotation and then each of them rated all
the instances independently. They were presented with the instruction, instance input, target output
(as a reference), and model responses. Model responses are listed in random order, with all the model
information anonymized. Figure 9 provides a screenshot of the annotation interface. The reported
performance in this paper is based on the results from one of the evaluators, and the trends from the other
evaluator’s results are the same.
B.2 Human Evaluation Agreement
To measure how reliable our human evaluation is, we calculate the inner-rater agreement between our two
evaluators.
We first report Cohen’s κ, which is commonly used to measure inter-rater agreement for categorical
items. When calculating this, we treat the 4-level rating (A-D) as a categorical variable, leading to a κof
0.58, which is a moderate agreement according to common practice.Furthermore, we also calculate the
agreement of our evaluators on classifying acceptable responses ((A or B) vs. (C or D)), with a final κof
0.75, indicating substantial agreement.13502We also compute the Spearman correlation coefficient ρbetween the ratings of our two evaluators by
treating the rating as an ordinal variable (A>B>C>D). The final coefficient is ρ= 0.81, indicating a high
correlation between the two evaluators.
B.3 Example Predictions from GPT3
We present a selection of user-oriented tasks, the corresponding GPT3-produced responses and
annotator ratings in Table 9. We see that even for responses rated as level C, the model demonstrates
extensive steps in solving the task, even though its final output is incorrect.1350313504C Task and Instance Examples from the Generated Instruction Data1350513506ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
7
/squareA2. Did you discuss any potential risks of your work?
7
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3, 4
/squareB1. Did you cite the creators of artifacts you used?
4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
1, 4
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
3, 4
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
The data released in this paper is synthetic data generated by models and cannot be easily veriﬁed.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
3
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
3
C/squareDid you run computational experiments?
3,4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
4, Appendix A13507/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix A
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
4
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix B
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Appendix B
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. The data collection was done by the authors themselves.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. The data collection was done by the authors themselves.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. The data collection was done by the authors themselves.13508