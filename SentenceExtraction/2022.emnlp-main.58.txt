
Alon EirewAvi CaciularuIdo DaganBar Ilan University, Ramat-Gan, IsraelIntel Labs, Israel
alon.eirew@intel.com
avi.c33@gmail.com
dagan@cs.biu.ac.il
Abstract
The task of Cross-document Coreference Reso-
lution has been traditionally formulated as re-
quiring to identify allcoreference links across a
given set of documents. We propose an appeal-
ing, and often more applicable, complementary
set up for the task – Cross-document Corefer-
ence Search , focusing in this paper on event
coreference. Concretely, given a mention in
context of an event of interest, considered as
a query, the task is to find all coreferring men-
tions for the query event in a large document
collection. To support research on this task, we
create a corresponding dataset, which is derived
from Wikipedia while leveraging annotations
in the available Wikipedia Event Coreference
dataset (WEC-Eng). Observing that the coref-
erence search setup is largely analogous to the
setting of Open Domain Question Answering,
we adapt the prominent Deep Passage Retrieval
(DPR) model to our setting, as an appealing
baseline. Finally, we present a novel model
that integrates a powerful coreference scoring
scheme into the DPR architecture, yielding im-
proved performance.
1 Introduction
Cross-Document Event Coreference (CDEC) res-
olution is the task of identifying clusters of text
mentions, across multiple texts, that refer to the
same event. For example, consider the following
two underlined event mentions from the WEC-Eng
CDEC dataset (Eirew et al., 2021):
1....On 14 April 2010 , an earthquake struck
the prefecture, registering a magnitude of 6.9
(USGS, EMSC) or 7.1 (Xinhua). It originated in
theYushu Tibetan Autonomous Prefecture...
2....a school mostly for Tibetan orphans in
Chindu County, Qinghai, after the 2010 Yushu
earthquake destroyed the old school...
Both event mentions refer to the same earth-
quake, as can be determined by the shared eventFigure 1: Example of Coreference Search. Provided
with a query passage containing a mention of interest, a
coreference search system retrieves from a large corpus
the best candidate passages containing mentions core-
ferring with the query.
arguments (2010, Yushu, Tibetan). In event coref-
erence resolution, the goal is to cluster event men-
tions that refer to the same event, whether within a
single document or across a document collection.
Currently, with the growing number of doc-
uments describing real-world events and event-
oriented information, the need for efficient meth-
ods for accessing such information is apparent.
Successful and efficient identification, clustering,
and access to event-related information, may be
beneficial for a broad range of applications at
the multi-text level, that need to match and inte-
grate information across documents, such as multi-
document summarization (Falke et al., 2017; Liao
et al., 2018), multi-hop question answering (Dhin-
gra et al., 2018; Wang et al., 2019) and Knowledge
Base Population (KBP) (Lin et al., 2020).
Currently, the CDEC task, as formed in corre-
sponding datasets, is intended at creating models
that exhaustively resolve all coreference links in900a given dataset. However, an applicable realistic
scenario may require to efficiently search and ex-
tract coreferring events of only specific events of
interest. A typical such use-case can be of a user
reading a text and encountering an event of interest
(for example, the plane crash event in Figure 1),
and then wishing to further explore and learn about
the event from a large document collection.
To address such needs, we propose an appealing,
and often more applicable, complementary set up
for the task – Cross-document Coreference Search
(Figure 1), focusing in this paper on event corefer-
ence. Concretely, given a mention in context of an
event of interest, considered as a query, the task is
to find all coreferring mentions for the query event
in a large corpus.
Such coreference resolution search use-case can-
not be addressed currently, for two main reasons:
(1) Existing CDEC datasets are relatively small for
the realistic representation of a search task; (2) Cur-
rent CDEC models, which are designed at linking
all coreference links in a given dataset, are inappli-
cable in terms of computation at the much larger
search space required by realistic coreference reso-
lution search scenarios.
To facilitate research on this setup, we present
a large dataset, derived from Wikipedia, by lever-
aging existing annotations in the Wikipedia Event
Coreference dataset (WEC) (Eirew et al., 2021).
Our curated dataset resembles in structure to an
Open-domain QA (ODQA) dataset (Berant et al.,
2013; Baudiš and Šedivý, 2015; Joshi et al., 2017;
Kwiatkowski et al., 2019; Rajpurkar et al., 2016),
containing a set of coreference queries and a large
passage collection for retrieval.
Observing that the coreference search setup is
largely analogous to the setting of Open Domain
Question Answering, we adapt the prominent Deep
Passage Retrieval (DPR) model to our setting, as an
appealing baseline. Further, motivated to integrate
coreference modeling into DPR, we adapted com-
ponents inspired by a prominent within-document
end-to-end coreference resolution model (Lee et al.,
2017), which was previously applied also to the
CDEC task (Cattan et al., 2020). Thus, we devel-
oped an integrated model that leverages compo-
nents from both DPR and the coreference model of
Lee et al. (2017). Our novel model yields substan-
tially improved performance on several important
evaluation metrics.Our datasetand codeare released for open
access.
2 Background
In this section, we first describe the Cross Doc-
ument Event Coreference (CDEC) task, datasets
and models (§2.1) and then review the common
open-domain QA model architecture (§2.2).
2.1 Cross-Document Event Coreference
Resolution
ECB+ (Cybulska and V ossen, 2014) is the most
commonly used dataset for training and testing
models for cross-document event coreference res-
olution. This corpus consists of documents par-
titioned into 43 clusters, each corresponding to
a certain news topic. ECB+ is relatively small,
where on average only 1.9 sentences per document
were selected for annotation, yielding only 722
non-singleton coreference clusters in total (that is,
clusters containing more than a single event men-
tion, while singleton clusters correspond to men-
tions that do not hold a coreference relation with
any other mention in the data).
Since annotating a CDEC dataset is a very chal-
lenging task, several annotation methods try to
semi-automatically create a CDEC dataset by tak-
ing advantage of available resources. The Gun Vio-
lence Corpus (GVC) (V ossen et al., 2018) leveraged
a structured database recording gun violence events
for creating an annotation scheme for gun violence
related events. In total GVC annotated 7,298 men-
tions distributed into 1,046 non-singleton clusters.
More recently, WEC-Eng (Eirew et al., 2021)
and HyperCoref (Bugert and Gurevych, 2021)
leveraged article hyperlinks pointing to the same
concept in order to create an automatic annota-
tion process. This annotation scheme helped Hy-
perCoref curate 2.7M event mentions distributed
among 0.8M event clusters, extracted from news ar-
ticles. The smaller WEC-Eng curates 43,672 event
mentions distributed among 7,597 non-singleton
clusters. Differently then HyperCoref, the WEC-
Eng development set (containing 1,250 mentions
and 233 clusters) and test set (contains 1,893 men-
tions and 322 clusters) have gone through a manual
validation process (see Table 1), ensuring their high
quality.901All the above mentioned datasets are targeted
for models which exhaustively resolve all corefer-
ence links within a given dataset (Barhom et al.,
2019; Meged et al., 2020; Cattan et al., 2020; Caci-
ularu et al., 2021; Yu et al., 2020; Held et al., 2021;
Allaway et al., 2021; Hsu and Horwood, 2022).
This setting resembles the within-document corefer-
ence resolution setting, where similarly all links are
exhaustively resolved in a given single-document.
However, while within-document coreference res-
olution is contained to a single document, CDCR
might relate to an unbounded multi-text search
space (e.g., news articles, Wikipedia articles, court
and police records and so on). To that end, we
aim at a task and dataset for modeling CDEC as a
search problem. To facilitate a large corpus for a
realistic representation of such a task, while ensur-
ing reliable development and test sets, we adopted
the WEC-Engas the basis for our dataset creation
(§3).
Within Document Coreference Resolution Re-
cent within-document coreference resolution mod-
els (Lee et al., 2018; Joshi et al., 2019; Kantor and
Globerson, 2019; Wu et al., 2020), were inspired
by the end-to-end model architecture introduced
by Lee et al. (2017). In particular, two distinct
components were adopted in those works, which
were shown to be effective in detecting mentions
and their coreference relations, both in the within-
document and cross-document (Cattan et al., 2020)
settings. In our proposed model, we similarly adopt
those two components to better represent corefer-
ence relations, in the coreference search settings.
2.2 Open-Domain Question Answering
Open-domain question answering (ODQA)
(V oorhees, 1999), is concerned with answering
factoid questions based on a large collection of
documents. Modern open-domain QA systems
have been restructured and simplified by com-
bining information retrieval (IR) techniques and
neural reading comprehension models (Chen et al.,
2017). In those approaches, a retriever component
finds documents that might contain an answer
from a large collection of documents, followed by
a reader component that finds a candidate answer
in a given document (Lee et al., 2019; Yang et al.,
2019; Karpukhin et al., 2020).
We observe that the Cross-Document Event
Coreference Search (CDES) setting resembles the
ODQA task. Specifically, given a passage contain-
ing a mention of interest, considered as a query ,
CDES is concerned with finding mentions core-
ferring with the query event in a large document
collection. To facilitate research in this task, we
created a dataset similar in structure to ODQA
datasets (Berant et al., 2013; Baudiš and Šedivý,
2015; Joshi et al., 2017; Kwiatkowski et al., 2019;
Rajpurkar et al., 2016), and established a suitable
model resembling in architecture to the recent two-
step (retriever/reader) systems, as described in the
following sections.
3 The CoreSearch Dataset
We formulated the Cross-Document Event Coref-
erence Search task following a similar approach
to open-domain question answering (illustrated in
Figure 1). Specifically, given a query containing a
marked target event mention, along with a passage
collection, the goal is to retrieve all the passages
from the passage collection that contain an event
mention coreferring with the query event, and ex-
tract the coreferring mention span of each retrieved
passage.
To facilitate research on this task, we present a
large dataset, derived from Wikipedia, termed Core-
Search . In this section we describe the CoreSearch
dataset structure (§3.1), following by describing
the structure of a single query instance (§3.2).9023.1 Dataset Structure
The CoreSearch dataset consists of two separate
passage collections: (1) a collection of passages
containing manually annotated coreferring event
mention, and (2) a collection of destructor pas-
sages.
Annotated Data The CoreSearch passage col-
lection which contains manually annotated event
mentions was created by importing the validated
portion of the WEC-Eng (Eirew et al., 2021) dataset
(§2.1 and Table 1).
Specifically, we merged the WEC-Eng validated
test and development set coreference clusters into
a single collection of 522 none-singleton clusters
(“Non-Singleton Culsters” in Table 1 and “Clusters”
in Table 2). We then split the clusters between
CoreSearch train, development and test sets. Each
cluster contains passages that form our annotated
passage collection.
Those passages will serve the roles of queries
and of positive retrieved coreferring passages.
Destructor Passages In order to collect a large
collection of passages for challenging and realistic
retrieval, we generate negative passages (i.e., de-
structing passages) using two resources: (1) The
entire WEC-Eng train set, which is not manually
validated, though quite reliable; (2) By extracting
the first paragraph of any Wikipedia article not
containing a hyperlink to any of the CoreSearch an-
notated passages, and hence are unlikely to corefer
with any of them (Table 2).
Cluster Types We observe that our annotated
data is characterized by two prominent types of
coreference clusters: Type-1 - clusters containing
only passages with event mention spans that in-
clude the event time or location (e.g., “2006 Dahab
bombings”, “2013’s BET Awards”), and Type-2 -
clusters that are comprised partly of passages as
in Type-1, as well as passages containing mention
spans without any event identifying participants
(e.g., “the deadliest earthquake on record”, “BET
Awards”, “plane crash”). Naturally, Type-2 clus-
ters will create queries/passage examples with a
higher degree of difficulty. Identifying coreference
for Type-2 clusters is indeed challenging in our
dataset, because WEC-Eng includes a multitude
of event mentions which are similar lexically but
do not corefer (e.g., different earthquakes) (Eirew
et al., 2021), requiring a model to identify event
coreference using the arguments in the surrounding
context.
To measure the distribution of cluster types
within CoreSearch, we randomly sampled 20 clus-
ters and found 90% are of type-2, demonstrating
the challenging nature of the CoreSearch data. Ta-
ble 3 illustrates examples of queries extracted ran-
domly from five type-2 clusters.
3.2 CoreSearch Instance Structure
An instance in the CoreSearch dataset is comprised
of: (1) a query passages pulled from the annotated
passage collection; (2) The collection of all other
passages, which are considered as the passage col-
lection for retrieval. Passages in the passage collec-
tion which belong to the same cluster as the pulled
query are considered positive passages, while all
the rest as negative passages.
Potential Language Adaptation The Core-
Search dataset is built on top of the English version
of WEC (WEC-Eng). Consequently, since WEC
is adoptable to other languages with relatively low
effort (Eirew et al., 2021), and the process for de-
riving CoreSearch from it is simple and fully auto-
matic, the CoreSearch dataset may be adopted to
other languages as well with very similar effort (as903for WEC).
4 Coreference-search Models
In this section, we aim to devise an effective base-
line for our event coreference search task to be
trained on our dataset. Following the observation
that coreference search formulation resembles the
open-domain QA (ODQA) (§2.2), we propose an
end-to-end neural architecture, comprised of a re-
triever and a reader models. Given a query passage,
the retriever selects the top- kmost relevant passage
candidates out of the entire passage corpus (§4.1).
Then, the reader is responsible for re-ranking the
retrieved passages and extracting the coreferring
event span, by using a reading comprehension mod-
ule (§4.2).
4.1 The Retriever Model
Given a query passage containing an event mention
of choice, the goal of the retriever is to select the
top-krelevant passage candidates out of a large col-
lection of passages. To that end, we build upon the
foundations of the Dense Passage Retriever model
(Karpukhin et al., 2020) and employ a similar re-
triever.
Similarly to DPR, we propose to encode the
query passage q= [CLS, q, . . . , q]and a can-
didate passage p= [CLS, p, . . . , p]using two
distinct neural encoders, E(·)andE(·),for
mapping their tokens into d-dimensional dense vec-
tors,[q,q, . . . , q]and[p,p, . . . , p]for
qandp, respectively. Here, both qandpde-
note the last hidden layer contextualized [CLS] to-
ken representations of qandprespectively, which
are then fed to a dot-product similarity scoring func-
tion, which determines candidate passage ranking:
sim(q, p) =q·p (1)
Event Mention Marking In order to accommo-
date our setup of mention-directed search and to
better signal the model to be aware of the query
event mention, we edit the query by marking the
span of the mention within the query passage by
using boundary tokens. Given the query event men-
tion span m= [q, q, . . . , q ], we append
the boundary tokens to obtain the final edited query
(mdenotes the sequence of the mention’s tokens):
q= [CLS, . . . , q,⟨S⟩, m,⟨\S⟩, q, . . . q].Improved Span Representation For implement-
ing the text encoders E(·)andE(·), we em-
ployed the SpanBERT(Joshi et al., 2020) model
as our query and passage encoders. SpanBERT is
an appealing encoder, as it was pre-trained for bet-
ter span representations, rather then the individual
tokens, and was also shown to be more effective
for coreference resolution tasks (Joshi et al., 2020;
Wu et al., 2020).
During our preliminary experiments, we ob-
served that both the additional event mention mark-
ing as well as replacing BERT with SpanBERT
contributed significantly to the performance over
our dataset.
Positive and Negative Training Examples We
construct our positive and negative examples by it-
erating sequentially through every training set event
coreference cluster C= [m, m, . . . , m],
where mdenotes an event mention surrounded
with its context (the entire passage). Given each
event mention macting as a query q, we con-
struct one positive coreference example for each
of the remaining |C| −1coreferring event men-
tions in the cluster. Then, for each such positive
example, we first construct one “challenging" neg-
ative example by selecting randomly one of the
top-20 passages returned by the BM25 retrieval
model for the corresponding query. In addition, for
each query in a training batch, we create additional
(“easier") in-batch negative examples by taking the
“challenging" passages of all other queries in the
current batch, similarly to Karpukhin et al. (2020).
Objective LetD=⟨q, p, p, . . . , p⟩be
the CoreSearch training set. Similarly to Karpukhin
et al. (2020), the goal is to optimize the negative
log likelihood loss of the positive passage, which
is based on the contrastive loss:
L/parenleftig
q, p, p, . . . , p/parenrightig
=−loge
e+/summationtexte.(2)
4.2 The Reader Model
Given a mention surrounded by its context as the
query, and its top- kretrieved passages, the reader
model is tasked to (1) re-rank the retrieved passages
according to a passage selection score and (2) ex-
tract the candidate mention span from each passage.904We implemented two flavours of readers, a DPR
baseline (§4.2.1), and a DPR reader enhanced with
event coreference scores (§4.2.2).
4.2.1 DPR Reader Baseline
We implemented a DPR-based passage selection
model that acts as re-ranker through cross-encoding
the query and the passage. Specifically, we append
a query q(including the event mention marker
tokens, see §4.1) and a passage p, and feed the
concatenated input sequence to the RoBERTa text
encoder E(·)(Liu et al., 2019). Similarly to
Karpukhin et al. (2020), we then use the output (last
hidden layer) token representations to predict three
probability distributions. We compute the span
score of the stottokens from the jpassage
asP(s)×P(t), and a passage selection
score of the jpassage as P(j):
P(s) =softmax (Pw) (3)
P(t) =softmax (Pw) (4)
P(j) =softmax (ˆPw), (5)
where [·]denotes column concatenation,,,kis
the number of the retrieved passages, and
w,w,w are learned vectors.
4.2.2 Integrating the Coreference Signal
While the above DPR-based reader yields appeal-
ing performance (§5.3), we conjecture that the pas-
sage selection (Eq. 5), which is based on the pas-
sages’ [CLS] token representations, is sub-optimal
for coreference resolution. These representations
learn high-quality sentence- or document-level fea-
tures (Devlin et al., 2019), however in our setting,
more fine-grained features are required in order to
capture information for better modeling corefer-
ence relations between mention spans. Motivated
by this hypothesis, we replaced the passage selec-
tion component (Eq. 5) with a method adapted
from recent neural within-document coreference
models (Lee et al., 2017, 2018; Joshi et al., 2019;
Kantor and Globerson, 2019; Wu et al., 2020).
Specifically, we aim to model the probability
of passage jto be selected by the likelihood it
contains an event mention mthat corefers to thequery’s event mention m:
P (j) =e
/summationtexte(6)
s(m, m) =s(m) +s(m, m) (7)
s(m) =w·FFNN/parenleftbig
g/parenrightbig
(8)
s(m, m) =w·FFNN/parenleftbig/bracketleftbig
g,g,g◦g/bracketrightbig/parenrightbig
(9)
Where s(m)is the mention scorer,
s(m, m)is the antecedent scorer that computes
coreference likelihood for the pair of mentions,
◦represents the element-wise product of gand
g,g= [m, m]is the concatenated vector
of the first and last token representations of the
mention in the passage x∈ {i, j}, ands(m, m)
is the final pairwise score. FFNN represents a feed
forward neural network with a single hidden layer.
Note that standard coreference resolution methods
compute also s(m), however since in our setup
the query mention is constant, it can be omitted.
During training, we extract the gold start/end
embeddings of the candidate passage, while at in-
ference time, we use the scores computed by Eq. 3
and Eq. 4 (see §4.2.1) in order to extract the most
probable plausible mention spans. Invalid spans,
who’s end precedes their start position point or are
longer than a threshold L, are filtered. For the query
event mention, we use the same mention marking
strategy used for the query encoder (§4.1). We fur-
ther show in §5.3 that this marking improves the
performance of the reader.
5 Experiments and Results
5.1 Implementation Details
Retriever We train the two separate encoders us-
ing a maximum query size of 64 tokens for the
query encoder. In order to cope with memory con-
strains, we limit the maximum passage size given
to the passage encoder to 180 tokens. Batch size
is set to 64. We train our model using four 12GB
Nvidia Titan-Xp GPUs.
Reader We train the single cross-encoder using
a maximum sequence size of 256 tokens, in order
to cope with memory constraints. We use up to 64
tokens from the surrounding query mention context
(which in many cases take less then 64 tokens) for
query representation, and concatenate the passage905context using the remaining available sequence. In
case the passage context length exceeds available
sequence size for passage representation, we seg-
ment the passage using overlapping strides of 128
tokens, creating additional passage instances with
the same query. The batch size is set to 24, and
both FFNN,FFNNuse a single hidden layer set to
128. We train the models using two 12GB Nvidia
Titan-Xp GPUs.
Hyperparameters All models parameters are
updated by the AdamW (Loshchilov and Hutter,
2019) optimizer, with a learning rate set to 10
and a wight-decay rate of 0.01. We also apply a
linear scheduling with warm-up (for 10% of opti-
mization steps) and dropout rate of 0.1. We train all
models for 5 epochs and consider the best perform-
ing ones over the development set. At inference,
we set the retriever top- kparameter to 500.
5.2 Evaluation Measures
In all our experiments, we followed the common
evaluation practices used for evaluating Informa-
tion Retrieval (IR) models (Khattab and Zaharia,
2020; Xiong et al., 2021; Hofstätter et al., 2021;
Thakur et al., 2021). Accordingly, we used the
following metrics:
Mean Reciprocal Rank (MRR@ k)Following
common evaluation practices, we set kto 10, ex-
pecting that the topmost correct result should ap-
pear amongst the top 10 results (that is, no credit is
given if the topmost correct result is ranked lower
than 10).
Recall (R@ k)We report recall at k∈ {10,50}
for the end-to-end model evaluation, assessing re-
call in two prototypical cases where the user might
choose to look at rather few or rather many re-
sults. For the retriever model we report recall at
k∈ {10,100,500}, illustrating the motivation for
thek= 500 cutoff point that we chose (beyond
which there were no substantial recall gains).
mean Average Precision (mAP@ k)The mAP
metric assesses the ranking quality of allcorrect
results within the top-k ones, measured for k∈
{10,50}, as measured for recall.
Reader Evaluation We use the above metrics
with the additional question answering (QA) mea-
surements of Exact Match (EM) and token level
F1 scorewith the reference answer after minor
normalization as in (Chen et al., 2017; Lee et al.,
2019; Karpukhin et al., 2020).
5.3 Results
Retriever Table 4 summarizes the retriever per-
formance results over the CoreSearch test set.
Our retriever model surpasses the BM25 method
(see further details in Appendix A.1) by a large
margin on every metric (Table 4, BM25 versus
Retriever-S). It should be noted that BM25 is
considered a strong information retrieval model
(Robertson and Zaragoza, 2009), also compared to
recent neural-based retrievers (Khattab and Zaharia,
2020; Izacard et al., 2021; Chen et al., 2021; Piktus906
et al., 2021). We observed this phenomenon during
our experiments, as the underlying DPR retriever
(i.e., BERT without boundary tokens), yielded poor
results on our settings, surpassed by the BM25
model on all measurements by a significant gap
(Table 4, BM25 versus Retriever-B).
End-to-end Table 5 presents our end-to-end sys-
tem results applied over the CoreSearch test set.
We found that both of the reader models ( E2E-
DPR andE2E-Integrated ) present appealing per-
formance given different measurement aspects we
now describe.
We conclude from the recall results (R@10 and
R@50) that the E2E-DPRmodel is an effective
re-ranking model, ranking almost all relevant pas-
sages extracted by the retriever within the top 50
results (86.62% out of maximum of 87.12% ranked
by the Retriever-Smodel at top-500). The EM
and F1 results indicate that the E2E-Integrated
model gains better mention extraction capabilities
compared to both E2E-DPR models (by 1.5% EM
and 1.2% F1 compared to E2E-DPR).
Finally, the MRR and mAP results indicate that
the E2E-Integrated model overall performs better
then both E2E-DPR models at ranking relevant
passages at higher ranks (indicated by MRR@10,mAP@10 and mAP@50 in Table 5). In particular,
we find that the MRR@10 results are especially
appealing (90.06%), showing the model predomi-
nately ranks a relevant passage at the first or second
position.
Finally, Table 6 illustrates a sample of the E2E-
Integrated top-2 model results, on a sample of
queries containing mention spans not including
event arguments, randomly sampled from five Type-
2CoreSearch clusters (§3.1). The table Illustrates
the model effectiveness in returning relevant pas-
sages and the coreferring mention within them.
False Negative Passages We observed that on
rare occasions the model returns a relevant passage
(and a coreferring mention) marked as negative in
the dataset. We sampled 15 queries and manually
validated their top-10 answers. We found that from
58 negative results, only 1 was a false negative,
indicating that indeed this phenomenon is rather
rare and insignificant. Such false negatives can
originate either from the WEC-Eng training set
(§2.1), or from our destructing passage generation
(§3). Notice that, such false negatives can only
have a deflating effect on results.9075.4 Ablation Study
To understand further how different model changes
affect the results, we conduct several experiments
and discuss our findings below. Table 4 presents
the retriever model results and Table 5 presents the
reader model results on the development set, for
some ablations.
Mention Span Boundaries In both our retriever
and reader experiments, we found that adding
the span boundary tokens around the query men-
tion, provides a strong signal to the model. In
our retriever experiments, while most of the gain
to performance was originated by replacing the
BERT model with SpanBERT (Retriever-B and
Retriever-Sin Table 4), applying boundary to-
kens significantly improved performance further
all across the board (Retriever-Sin Table 4).
However, in our end-to-end model experiments,
we observed that applying boundary tokens will
help the model mostly to improve at span detec-
tion, while less so at re-ranking ( E2E-DPRand
E2E-DPRin Table 5).
Modeling Coreference with QA Our main mo-
tivation for replacing the DPR reader passage se-
lection method (Eq. 5), with a coreference scoring
one, was to create a better passage selection mech-
anism for re-ranking. Indeed, this modeling prove
efficient both at re-ranking, as well as at mention
detection, as indicated by the E2E-Integrated model
results in Table 5.
5.5 Qualitative Error Analysis
To analyze prominent error types made by our E2E-
Integrated model we sampled 20 query results that
were incorrectly ranked at the first position (Table 7
in Appendix A.2 presents a few of these examples).
From those 20 results, 18 were indeed identified
as incorrect while 2 results were actually correct,
that is, including a mention that does corefer with
the query event but was missed in the annotation (a
false negative).
We observed two main errors types. The first
type involved event argument inconsistencies, iden-
tified in 10 out of the 18 erroneous results. In these
cases, the model identified an event of the same
type as the query event, but with non-matching ar-
guments (see examples 3, 4, 5 and 6 in Table 7).
This type of error suggests that there is room for im-
proving the model capability in within- and cross-
document argument matching. Some illustratingexamples in Table 7 for such argument mismatches
include “few days later”, “also that year”, “the
town” (examples 3, 4 and 5, respectively).
The second type of error, identified in 8 out of
the 18 erroneous results, corresponded to cases
where the two contexts of the query and result pas-
sages did not provide sufficient information for
determining coreference (see examples 1 and 2 in
Table 7). Manually analyzing these 8 cases, we
found that in 3 of them the coreference relation
could be excluded by examining other event men-
tions in the coreference cluster to which the query
belongs. In 7 cases, it was possible to exclude coref-
erence by consulting external knowledge, specifi-
cally Wikipedia, to obtain more information either
about the event itself or its arguments. Example 1
in the table illustrates a case where Wikipedia could
provide conflicting information about the event lo-
cation (the city of the Maxim restaurant vs. the city
of the query event). Example 2 illustrates a case
where Wikipedia provided conflicting information
about the event time (the time of the first Repub-
lican convention in the query vs. the time of the
convention discussed in the result). This error type
suggests the potential for incorporating external
knowledge in cross-document event coreference
models. Further, models may benefit from con-
sidering globally the information across an entire
coreference cluster, as was previously proposed in
some works (Raghunathan et al., 2010).
6 Conclusions
We introduced Cross-document Coreference
Search , a challenging task for accurate semantic
search for events. To support research on this
task, we created the Wikipedia-based CoreSearch
dataset, comprised of training, validation, and
test set queries, along with a large collection
of about 1M passages to retrieve from in each
set. Furthermore, our methodology for semi-
automatically converting a cross-document event
coreference dataset to a coreference search
dataset can be applied to other such datasets, for
example HyperCoref (Bugert et al., 2021) which
represents the news domain. Finally, we provide
several effective baseline models and encourage
future research on this promising and practically
applicable task, hoping that it will lead to a broad
set of novel applications and use-cases.9087 Limitations
In this work, we construct the CoreSearch dataset,
which relies on the existing Wikipedia Event Coref-
erence dataset (WEC-Eng) (Eirew et al., 2021).
This setup exposes potential limitations of the avail-
able annotations in WEC-Eng which might be par-
tially noisy in several manners.
By using Wikipedia as the knowledge source, we
assume that the corpus is comprised of high quality
documents. Yet, future work may further assess the
quality of the documents inside WEC-Eng, such as
checking for duplications.
Second, since the WEC-Eng train set was built
using automatic annotation, it might contain some
wrong coreference annotations. Wikipedia in-
structs authors to mark the first occurrence of a
mention in the article. However, for several rare
occasions, such distracting passages might contain
events which were not covered either due to an au-
thor not following the instructions or the existence
of more than one mentions of the same event within
the same passage (§5.3). While we observed that
false-negative retrievals are quite rare, this aspect
may be further investigated.
Finally, our dataset covers events which are “fa-
mous" to a certain extent, justifying a Wikipedia
entry, but does not cover anecdotal events that may
arise in various realistic use cases.
Acknowledgments
We thank the Deepset team for providing and sup-
porting the Haystack framework. This research
was supported in part by Intel Labs, the Israel Sci-
ence Foundation grant 2827/21, by a grant from the
Israel Ministry of Science and Technology and by
the PBC Fellowship for outstanding data science
students.
References909910911A Appendices
A.1 Sparse Passage Retriever
We created a BM25 baseline model following com-
mon practice of comparing a retriever model with
traditional sparse vector space methods such as
BM25 (Karpukhin et al., 2020; Khattab and Za-
haria, 2020). Additionally, our training procedure
depends on challenging negative examples pro-
vided by a BM25 model (§4.1).
In our task settings, a query is represented by
a context with mention, to that end, we experi-
ment using different query configurations in order
to maximize our BM25 results. This included; us-
ing the entire query context, the query sentence,
decontextualization (Choi et al., 2021) based on the
sentence containing the event mention, and using
the mention span followed by the Named Entities
from the surrounding context. We found the latter
to gave us the best BM25 results ( BM25 in Table 4).
A.2 A Sample of Erroneous Top Ranked
Results912913