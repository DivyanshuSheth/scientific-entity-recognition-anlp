
Andreas OpedalRan ZmigrodTim Vieira
Ryan CotterellJason EisnerETH ZürichMax Planck ETH Center for Learning SystemsUniversity of CambridgeJohns Hopkins University
andreas.opedal@inf.ethz.ch rz279@cam.ac.uk tim.f.vieira@gmail.com
ryan.cotterell@inf.ethz.ch jason@cs.jhu.edu
Abstract
This paper provides a reference description, in
the form of a deduction system, of Earley’s
(1970) context-free parsing algorithm with
various speed-ups. Our presentation includes a
known worst-case runtime improvement from
Earley’s O/parenleftbig
N|G||R|/parenrightbig
, which is unworkable
for the large grammars that arise in natural
language processing, to O/parenleftbig
N|G|/parenrightbig
, which
matches the runtime of CKY on a binarized
version of the grammar G. Here Nis the
length of the sentence, |R|is the number of
productions in G, and|G|is the total length of
those productions. We also provide a version
that achieves runtime of O/parenleftbig
N|M|/parenrightbig
with
|M| ≤ |G| when the grammar is represented
compactly as a single finite-state automaton
M(this is partly novel). We carefully treat the
generalization to semiring-weighted deduction,
preprocessing the grammar like Stolcke (1995)
to eliminate deduction cycles, and further
generalize Stolcke’s method to compute
the weights of sentence prefixes. We also
provide implementation details for efficient
execution, ensuring that on a preprocessed
grammar, the semiring-weighted versions of
our methods have the same asymptotic runtime
and space requirements as the unweighted
methods, including sub-cubic runtime on some
grammars.https://github.com/rycolab/
earleys-algo
1 Introduction
Earley (1970) was a landmark paper in computer
science.Its algorithm was the first to directly
parse under an unrestricted context-free grammar
in time O/parenleftbig
N/parenrightbig
, with Nbeing the length of the
input string. Furthermore, it is faster for certain
grammars because it uses left context to filter its
search at each position. It parses unambiguous
grammars in O/parenleftbig
N/parenrightbig
time and a class of “bounded-
state” grammars, which includes all deterministicgrammars, in O(N)time. Its artful combination
of top-down (goal-driven) and bottom-up (data-
driven) inference later inspired a general method
for executing logic programs, “Earley deduction”
(Pereira and Warren, 1983).
Earley’s algorithm parses a sentence incremen-
tally from left to right, optionally maintaining a
packed parse forest over the sentence prefix that
has been observed so far. This supports online sen-
tence processing—incremental computation of syn-
tactic features and semantic interpretations—and
also reveals for each prefix the set of grammatical
choices for the next word.
It can be attractively extended to compute the
probabilities of the possible next words (Jelinek
and Lafferty, 1991; Stolcke, 1995). This is a
standard way to compute autoregressive language
model probabilities under a PCFG to support cogni-
tive modeling (Hale, 2001) and speech recognition
(Roark, 2001). Such probabilities could further be
combined with those of a large autoregressive lan-
guage model to form a product-of-experts model.
Recent papers (as well as multiple github projects)
have made use of a restricted version of this, re-
stricting generation from the language model to
only extend the current prefix in ways that are gram-
matical under an unweighted CFG; then only gram-
matical text or code will be generated (Shin et al.,
2021; Roy et al., 2022; Fang et al., 2023).
It is somewhat tricky to implement Earley’s
algorithm so that it runs as fast as possible. Most
importantly, the worst-case runtime should be
linear in the size of the grammar, but this property
was not achieved by Earley (1970) himself nor
by textbook treatments of his algorithm (e.g.,
Jurafsky and Martin, 2009, §13.4). This is easy to
overlook when the grammar is taken to be fixed, so
that the grammar constant is absorbed into the O
operator, as in the opening paragraph of this paper.3687Yet reducing the grammar constant is critical in
practice, since natural language grammars can be
very large (Dunlop et al., 2010). For example, the
Berkeley grammar (Petrov et al., 2006), a learned
grammar for the Penn Treebank (PTB) (Marcus
et al., 1993), contains over one million productions.
In this reference paper, we attempt to collect the
key efficiency tricks and present them declaratively,
in the form of a unified deduction system that can
be executed with good asymptotic complexity.
We obtain further speedups by allowing the
grammar to be presented in the form of a weighted
finite-state automaton whose paths correspond to
the productions, which allows similar productions
to share structure and thus to share computation.
Previous versions of this trick use a different
automaton for each left-hand side nonterminal (Pur-
dom and Brown, 1981; Kochut, 1983; Leermakers,
1989; Perlin, 1991, inter alia ); we show how to use
a single automaton, which allows further sharing
among productions with different left-hand sides.
We carefully generalize our methods to han-
dle semiring-weighted grammars, where the parser
must compute the total weight of all trees that are
consistent with an observed sentence (Goodman,
1999)—or more generally, consistent with the pre-
fix that has been observed so far. Our goal is to
ensure that if the semiring operations run in con-
stant time, then semiring-weighted parsing runs
in the same time and space as unweighted pars-
ing (up to a constant factor), for every grammar
and sentence, including those where unweighted
parsing is faster than the worst case. Eisner (2023)
shows how to achieve this guarantee for any acyclic
deduction system, so we produce such a system
by preprocessing the grammar to eliminate cyclic
derivations.Intuitively, this means we do not have
to sum over infinitely many derivations at runtime
(as Goodman (1999) would). We also show how
to compute prefix weights, which is surprisingly
tricky and requires the semiring to be commuta-
tive. Our presentation of preprocessing and prefix
weights generalizes and corrects that of Stolcke
(1995), who relied on special properties of PCFGs.
Finally, we provide a reference implementationin Cythonand empirically demonstrate the value
of the speedups.
2 Weighted Context-Free Grammars
Acontext-free grammar (CFG )Gis a tuple
⟨N,Σ,R, S⟩where Σis a finite set of terminal
symbols, Nis a finite set of nonterminal symbols
withΣ∩ N =∅,Ris a set of productions from
a nonterminal to a sequence of terminals and non-
terminals (i.e., R ⊆ N × (Σ∪ N)), and S∈ N
is the start symbol. We use lowercase variable
names ( a, b, . . . ) and uppercase ones ( A, B, . . . )
for elements of ΣandN, respectively. We use
a Greek letter ( ρ, µ, orν) to denote a sequence
of terminals and nonterminals, i.e., an element of
(Σ∪ N). Therefore, a production has the form
A→ρ. Note that ρmay be the empty sequence ε.
We refer to |ρ| ≥0as the arity of the production,
|A→ρ|= 1 + |ρ|as the size of the production,
and|G|=/summationtext|A→ρ|for the total sizeof
the CFG. Therefore, if Kis the maximum arity of
a production, |G| ≤ |R| (1 +K). Productions of
arity 0, 1, and 2 are referred to as nullary ,unary ,
andbinary productions respectively.
For a given G, we write µ⇒νto mean that µ∈
(Σ∪ N)can be rewritten into ν∈(Σ∪ N)by a
single production of G. For example, A B⇒ρ B
expands Aintoρusing the production A→ρ. The
reflexive and transitive closure of this relation,⇒,
then denotes rewriting by any sequence of zero or
more productions: for example, A B⇒ρ µ ν . We
may additionally write A⇒ρiffA⇒ρ µ, and
refer to ρas aprefix ofρ µ.
Aderivation subtree ofGis a finite rooted or-
dered tree Tsuch that each node is labeled either
with a terminal a∈Σ, in which case it must be
a leaf, or with a nonterminal A∈ N , in which
caseRmust contain the production A→ρwhere
ρis the sequence of labels on the node’s 0 or more
children. For any A∈ N , we write Tfor the
set of derivation subtrees whose roots have label A,
and refer to the elements of Tasderivation trees .
Given a string x∈Σof length N, we write T
for the set of derivation subtrees with leaf sequence
x. For an input sentence x, its set of derivation
treesT=Tis countable and possibly infinite. It
is non-empty iff S⇒x, with each T∈ Tserving
as a witness that S⇒x, i.e., that Gcan generate x.
We will also consider weighted CFG s3688
(WCFG s), in which each production A→ρis ad-
ditionally equipped with a weight w(A→ρ)∈W
where Wis the set of values of a semiring
S=⟨W,⊕,⊗,,⟩. Semirings are defined
in App. A. We assume that ⊗is commutative,
deferring the trickier non-commutative case to
App. K. Any derivation tree TofGcan now be
given a weight
w(T)=/circlemultiplydisplayw(A→ρ) (1)
where A→ρranges over the productions associ-
ated with the nonterminal nodes of T. The goal of
aweighted recognizer is to find the total weight
of all derivation trees of a given input sentence x:
Z=w/parenleftig
S⇒x/parenrightig=/circleplusdisplayw(T) (2)
An ordinary unweighted recognizer is the special
case where Wis the boolean semiring, so Z=
true iffS⇒xiffT̸=∅. Aparser returns at least
one derivation tree from TiffT̸=∅.
As an extension to the weighted recognition
problem (2), one may wish to find the prefixweight of a string y∈Σ, which is the total weight
of all sentences x=yz∈Σhaving that prefix:
w/parenleftig
S⇒y/parenrightig=/circleplusdisplayw/parenleftig
S⇒yz/parenrightig
(3)
§1 discussed applications of prefix probabili-
ties—the special case of (3) for a probabilistic
CFG (PCFG ), in which the production weights
are rewrite probabilities: W=Rand
(∀A∈ N)/summationtextw(A→ρ) = 1 .
3 Parsing as Deduction
We will describe Earley’s algorithm using a deduc-
tion system , a formalism that is often employed in
the presentation of parsing algorithms (Pereira and
Shieber, 1987; Sikkel, 1997), as well as in math-
ematical logic and programming language theory
(Pierce, 2002). Much is known about how to exe-
cute (Goodman, 1999), transform (Eisner and Blatz,
2007), and neuralize (Mei et al., 2020) deduction
systems.
A deduction system proves items Vusing de-
duction rules . Items represent propositions; the
rules are used to prove all propositions that are true.
A deduction rule is of the form3689U U···V
where E is the name of the rule, the 0 or
more items above the bar are called antecedents ,
and the single item below the bar is called a conse-
quent . Antecedents may also be written to the side
of the bar; these are called side conditions and will
be handled differently for weighted deduction in
§6.Axioms (listed separately) are merely rules that
have no antecedents; as a shorthand, we omit the
bar in this case and simply write the consequent.
Aproof tree is a finite rooted ordered tree whose
nodes are labeled with items, and where every node
is licensed by the existence of a deduction rule
whose consequent Vmatches the label of the node
and whose antecedents U, U, . . .match the labels
of the node’s children. It follows that the leaves are
labeled with axioms. A proof of item Vis a proof
treedwhose root is labeled with V: this shows
howVcan be deduced from its children, which
can be deduced from their children, and so on until
axioms are encountered at the leaves. We say V
isprovable ifD, which denotes the set of all its
proofs, is nonempty.
Our unweighted recognizer determines whether
a certain goal item is provable by a certain set of
deduction rules from axioms that encode Gandx.
The deduction system is set up so that this is the
case iff S⇒x. The recognizer can employ a for-
ward chaining method (see e.g. Ceri et al., 1990;
Eisner, 2023) that iteratively deduces items by ap-
plying deduction rules whenever possible to an-
tecedent items that have already been proved; this
will eventually deduce all provable items. An un-
weighted parser extends the recognizer with some
extra bookkeeping that lets it return one or more
actual proofs of the goal item if it is provable.
4 Earley’s Algorithm
Earley’s algorithm can be presented as the specific
deduction system Earley shown in Table 1 (Sikkel,
1997; Shieber et al., 1995; Goodman, 1999), ex-
plained in more detail in App. B. Its proof trees
Dare in one-to-one correspondence with the
derivation trees T(a property that we will main-
tain for our improved deduction systems in §5 and
§7). The grammar Gis encoded by axioms A⇒ρ
that correspond to the productions of the grammar.
The input sentence xis encoded by axioms of theform [k−1, k, a]where a∈Σ; this axiom is true
iffx=x=a.The remaining items have
the form [i, j, A→µν], where 0≤i≤j≤N,
so that the span (i, j)refers to a substring x=
x···xof the input sentence x=xx. . . x.
The item [i, j, A→µν]is derivable only if the
grammar Ghas a production A→µ νsuch that
µ⇒x. Therefore,indicates the progress we
have made through the production. An item with
nothing to the right of, e.g., [i, j, A→ρ], is
called complete . The set of all items with a shared
right index jis called the item set ofj, denoted T.
While µ⇒xis a necessary condition for
[i, j, A→µν]to be provable, it is not sufficient.
For efficiency, the Earley deduction system is
cleverly constructed so that this item is provable
iffit can appear in a proof of the goal item for
some input string beginning with x, and thus
possibly for xitself.
Including [0,0, S→S]as an axiom in the sys-
tem effectively causes forward chaining to start
looking for a derivation at position 0. Forward
chaining will prove the goal item [0, N, S→S]
iffS⇒x. These two items conveniently pretend
that the grammar has been augmented with a new
start symbol S/∈ N that only rewrites according
to the single production S→S.
The Earley system employs three deduction
rules: P ,S, and C . We refer
the reader to App. B for a presentation and anal-
ysis of these rules, which reveals a total runtime
ofO/parenleftbig
N|G||R|/parenrightbig
. App. C outlines how past work
improved this runtime. In particular, Graham et al.
(1980) presented an unweighted recognizer that is a
variant of Earley’s, along with implementation de-
tails that enable it to run in time O/parenleftbig
N|G|/parenrightbig
. How-
ever, those details were lost in retelling their algo-
rithm as a deduction system (Sikkel, 1997, p. 113).
Our improved deduction system in the next section
does enable the O/parenleftbig
N|G|/parenrightbig
runtime, with execution
details of forward chaining spelled out in App. H.36905 An Improved Deduction System
OurEarleyFast deduction system, shown in the
right column of Table 1, shaves a factor of O(R)
from the runtime of Earley . It does so by effec-
tively applying a weighted fold transform (Tamaki
and Sato, 1984; Eisner and Blatz, 2007; John-
son, 2007) on P (§5.1) and C (§5.2),
introducing coarse-grained items of the forms
[i, j, A→⋆]and[i, j, A→⋆]. In these items,
the constant symbol ⋆can be regarded as a wild-
card that stands for “any sequence ρ.” We also
use these new items to replace the goal item and
the axiom that used S; the extra Ssymbol is
no longer needed. The proofs are essentially un-
changed (App. D).
We now describe our new deduction rules for
C andP. (S is unchanged.) We also
analyze their runtime, using the same techniques
as in App. B.
5.1 Predict
We split P into two rules: P1andP2.
The first rule, P1, creates an item that gathers
together all requests to look for a given nonterminal
Bstarting at a given position j: [i, j, A→µB ν][j, j, B →⋆]
There are three free choices in the rule: indices i
andj, and dotted production A→B ν. There-
fore, P1 has a total runtime of O/parenleftbig
N|G|/parenrightbig
.
The second rule, P2, expands the item into
commitments to look for each specific kind of B:
B→ρ [j, j, B →⋆][j, j, B →ρ]
P2has two free choices: index jand produc-
tionB→ρ. Therefore, P2has a runtime of
O(N|R|), which is dominated by O(N|G|)and so
the two rules together have a runtime of O/parenleftbig
N|G|/parenrightbig
.
5.2 Complete
We speed up C in a similar fashion to P.
We split C into two rules: C1andC2.
The first rule, C1, gathers all complete Bcon-
stituents over a given span into a single item:
[j, k, B →ρ][j, k, B →⋆]
We have three free choices: indices jandk, and
complete production B→ρwith domain size
|R|. Therefore, C 1has a total runtime of
O/parenleftbig
N|R|/parenrightbig
, orO/parenleftbig
N|G|/parenrightbig
.The second rule, C 2, attaches the result-
ing complete items to any incomplete items that
predicted them:
[i, j, A→µB ν] [j, k, B →⋆][i, k, A →µ Bν]
We have four free choices: indices i,j, and k,
and dotted production A→µB ν. Therefore,
C 2has a total runtime of O/parenleftbig
N|G|/parenrightbig
and so
the two rules together have a runtime of O/parenleftbig
N|G|/parenrightbig
.
6 Semiring-Weighted Parsing
We have so far presented Earley’s algorithm and our
improved deduction system in the unweighted case.
However, we are often interested in determining
not just whether a parse exists, but the total weight
of all parses as in equation (2), or the total weight
of all parses consistent with a given prefix as in
equation (3).
We first observe that by design, the derivation
trees of the CFG are in 1-1 correspondence with the
proof trees of our deduction system that are rooted
at the goal item. Furthermore, the weight of a
derivation subtree can be found as the weight of the
corresponding proof tree, if the weight w(d)of
any proof tree dis defined recursively as follows.
Base case: dmay be a single node, i.e., Vis an
axiom. If Vhas the form A→ρ, then w(d)is the
weight of the corresponding grammar production,
i.e.,w(A→ρ). All other axiomatic proof trees of
Earley andEarleyFast have weight.
Recursive case: If the root node of dhas child
subtrees d, d, . . ., then w(d) =w(d)⊗
w(d)⊗ ··· . However, the factors in this product
include only the antecedents written above the bar,
not the side conditions (see §3).
Following Goodman (1999), we may also asso-
ciate a weight with each item V, denoted ˙β(V),
which is the total weight of allits proofs d∈ D.
By the distributive property, we can obtain that
weight as an ⊕-sum over all one-step proofs of V
from antecedents. Specifically, each deduction rule
that deduces Vcontributes an ⊕-summand, given3691by the product ˙β(U)⊗˙β(U)⊗··· of the weights
of its antecedent items (other than side conditions).
Now our weighted recognizer can obtain Z(the
total weight of all derivations of x) as˙βof the goal
item (the total weight of all proofs of that item).
For an item Vof the form [i, j, A→µν], the
weight ˙β(V)will consider derivations of nontermi-
nals in µbut not those in ν. We therefore refer to
˙β(V)as an incomplete inside weight . However, ν
will come into play in the extension of §6.1.
The deduction systems work for any semiring-
weighted CFG. Unfortunately, the forward-
chaining algorithm for weighted deduction (Eis-
ner et al., 2005, Fig. 3) may not terminate if the
system permits cyclic proofs, where an item can
participate in one of its own proofs. In this case,
the algorithm will merely approach the correct
value of Zas it discovers deeper and deeper
proofs of the goal item. Cyclicity in our system
can arise from sets of unary productions such as
{A→B, B→A} ⊆ R , or equivalently, from
{A→E B E, B →A} ⊆ R where E⇒ε(which
is possible if Rcontains E→εor other nullary
productions). We take the approach of eliminating
problematic unary and nullary productions from
the weighted grammar without changing Zfor
anyx. We provide methods to do this in App. E
and App. F respectively. It is important to elimi-
nate nullary productions before eliminating unary
cycles, since nullary removal may create new unary
productions. The elimination of some productions
can increase |G|, but we explain how to limit this
effect.
6.1 Extension to Prefix Weights
Stolcke (1995) showed how to extend Earley’s algo-
rithm to compute prefix probabilities under PCFGs,
by associating a “forward probability” with each-
item.However, he relied on the property that all
nonterminals AhaveZ= 1, where Zdenotes
thefree weight
Z=/circleplusdisplay/circlemultiplydisplayw(B→ρ) (4)
As a result, his algorithm does not handle the case
of WCFGs or CRF-CFGs (Johnson et al., 1999;
Yusuke and Jun’ichi, 2002; Finkel et al., 2008), or
even non-tight PCFGs (Chi and Geman, 1998). It
also does not handle semiring-weighted grammars.
We generalize by associating with each-item, in-
stead of a “forward probability,” a “prefix outside
weight” from the same commutative semiring that
is used to weight the grammar productions. For-
mally, each w(V)will now be a pair (˙β(V),˙α(V)),
and we combine these pairs in specific ways.
Recall from §4 that the item V =
[i, j, A→µν]is provable iffit appears in
a proof of some sentence beginning with x.
For any such proof containing V, its steps can be
partitioned as shown in Fig. 1, factoring the proof
weight into three factors. Just as the incomplete
inside weight ˙β(V)is the total weight of all ways
to prove V, the future inside weight Zis the
total weight of all ways to prove [i, j, A→µ ν]
from Vand the prefix outside weight ˙α(V)is the
total weight of all ways to prove the goal item
from [i, j, A→µ ν]—in both cases allowing any
future words xas “free” axioms.
The future inside weight Z=/producttextZ
does not depend on the input sentence. To avoid
a slowdown at parsing time, we precompute this
product for each suffix νof each production in R,
after using methods in App. F to precompute the
free weights Zfor each nonterminal A.
Like ˙β(V),˙α(V)is obtained as an ⊕-sum over
all one-step proofs of V. Typically, each one-step
proof increments ˙α(V)by the prefix outside weight
of its-antecedent or-side condition (for C 2,
theleft-antecedent). As an important exception,
when V= [j, j, B →⋆], each of its one-step3692proofs via P1 instead increments ˙α(V)by
˙α([i, j, A→µB ν])
⊗˙β([i, j, A→µB ν])⊗Z (5)
combining the steps outside [i, j, A→µB ν]
with some steps inside the A(including its pro-
duction) to get all the steps outside the B. The base
case is the start axiom, ˙α([0,0, S→⋆]) =.
Unfortunately, this computation of ˙α(V)is only
correct if there is no left-recursion in the grammar.
We explain this issue in App. G.1 and fix it by
extending the solution of Stolcke (1995, §4.5.1).
The prefix weight of x(j >0)is computed
as an⊕-sum ˙α([j, j])over all one-step proofs of
the new item [j, j]via the following new deduction
rule that is triggered by the consequent of S: [i, j, A→µ aν][j, j]
Each such proof increments the prefix weight by
˙α([i, j, A→µ aν])
⊗˙β([i, j, A→µ aν])⊗Z (6)
7 Earley’s Algorithm Using an FSA
In this section, we present a generalization of
EarleyFast that can parse with any weighted
finite-state automaton (WFSA ) grammar Min
O/parenleftbig
N|M|/parenrightbig
. Here Mis a WFSA (Mohri, 2009)
that encodes the CFG productions as follows. For
anyρ∈(Σ∪N)and any A∈ N , forMto accept
the string ρ/hatwideAwith weight w∈Wis tantamount
to having the production A→ρin the CFG with
weight w. The grammar size |M| is the number of
WFSA arcs. See Fig. 2 for an example.
This presentation has three advantages over
a CFG. First, Mcan be compiled from an
extended CFG (Purdom and Brown, 1981),
which allows user-friendly specifications like
NP→Det?AdjNPPthat may specify in-
finitely many productions with unboundedly long
right-hand-sides ρ(although Mstill only describes
a context-free language). Second, productions
with similar right-hand-sides can be partially
merged to achieve a smaller grammar and a faster
runtime. They may share partial paths in M,
which means that a single item can efficiently
represent many dotted productions. Third, when
⊗is non-commutative, only the WFSA grammar
formalism allows elimination of nullary rules in
all cases (see App. F).
Our WFSA grammar is similar to a recursive
transition network orRTN grammar (Woods,
1970). Adapting Earley’s algorithm to RTNs was
discussed by Kochut (1983), Leermakers (1989),
and Perlin (1991). Klein and Manning (2001b)
used a weighted version for PTB parsing. None of
them spelled out a deduction system, however.
Also, an RTN is a collection of productions of
the form A→ M, where for Mto accept ρ
corresponds to having A→ρin the CFG. Thus an
RTN uses one FSA per nonterminal. Our innova-
tion is to use one WFSA for the entire grammar,
specifying the left-hand-side nonterminal as a final
symbol. Thus, to allow productions A→µ νand
B→µ ν, our single WFSA can have paths µ ν/hatwideA
andµ ν/hatwideBthat share the µprefix—as in Fig. 2.
This allows our EarleyFSA to match the µprefix
only once, in a way that could eventually result in
completing either an Aor aB(or both).
A traditional weighted CFG Gcan be easily en-
coded as an acyclic WFSA Mwith|M|=|G|, by
creating a weighted path of length kand weight
wfor each CFG production of size kand weight
w, terminating in a final state, and then merging the
initial states of these paths into a single state that
becomes the initial state of the resulting WFSA.
The paths are otherwise disjoint. Importantly, this
WFSA can then be determinized and minimized3693
(Mohri, 1997) to potentially reduce the number of
states and arcs (while preserving the total weight
of each sequence) and thus speed up parsing (Klein
and Manning, 2001b). Among other things, this
will merge common prefixes and common suffixes.
In general, however, the grammar can be speci-
fied by any WFSA M—not necessarily determin-
istic. This could be compiled from weighted reg-
ular expressions, or be an encoded Markov model
trained on observed productions (Collins, 1999), or
be obtained by merging states of another WFSA
grammar (Stolcke and Omohundro, 1994) in order
to smooth its weights and speed it up.
The WFSA has states Qand weighted arcs (or
edges) E, over an alphabet Aconsisting of Σ∪ N
together with hatted nonterminals like /hatwideA. Its initial
and final states are denoted by I ⊆ Q andF ⊆ Q ,
respectively.We denote an arc of the WFSA by
(q⇝q)∈ E where q, q∈ Q anda∈ A ∪ { ε}.
This corresponds to an axiom with the same weight
as the edge. q∈ Icorresponds to an axiom whose
weight is the initial-state weight of q. The item
q∈ F is true not only if qis a final state but more
generally if qhas an ε-path of length ≥0to a final
state; the item’s weight is the total weight of all
such ε-paths, where a path’s weight includes its
final-state weight.
For a state q∈ Q and symbol A∈ N , the pre-
computed side condition q⇝⋆is true iff there ex-
ists a state q∈ Q such that q⇝qexists in E. Ad-ditionally, the precomputed side condition q⇝⋆
is true if there exists a path starting from qthat
eventually reads /hatwideA. As these are only used as side
conditions, they may be given any non-weight.
The EarleyFSA deduction system is given in
Table 2. It can be run in time O/parenleftbig
N|M|/parenrightbig
. It is
similar to EarleyFast , where the dotted rules have
been replaced by WFSA states. However, unlike a
dotted rule, a state does not specify a P ed
left-hand-side nonterminal. As a result, when any
deduction rule “advances the dot” to a new state q,
it builds a provisional item [j, k, q ?]that is anno-
tated with a question mark. This mark represents
the fact that although qis compatible with several
left hand sides A(those for which q⇝⋆is true),
the left context xmight not call for any of those
nonterminals. If it calls for at least one such non-
terminal A, then the new F rule will remove
the question mark, allowing further progress.
One important practical advantage of this
scheme for natural language parsing is that it pre-
vents a large-vocabulary slowdown.InEarley ,
applying P to (say) [3,4,NP→DetN]
results in thousands of items of the form
[4,4,N→a]where aranges over all nouns in
the vocabulary. But EarleyFSA in the correspond-
ing situation will predict only [4,4, q]where qis
the initial state, without yet predicting the next
word. If the next input word is [4,5,happy ], then
EarleyFSA follows just the happy arcs from q,
yielding items of the form [4,5, q?](which will3694then be F ed away since happy is not a noun).
Note that S,C 1and C 2are
ternary, rather than binary as in EarleyFast . For
further speed-ups we can apply the fold transform
on these rules in a similar manner as before, re-
sulting in binary deduction rules. We present this
binarized version in App. I.
As before, we must eliminate unary and nullary
rules before parsing; App. J explains how to do
this with a WFSA grammar. In addition, although
Table 2 allows the WFSA to contain ε-arcs, App. J
explains how to eliminate ε-cycles in the WFSA,
which could prevent us from converging, for the
usual reason that an item [i, j, q ]could participate
in its own derivation. Afterwards, there is again a
nearly acyclic order in which the deduction engine
can prove items (as in App. H.1 or App. H.3).
As noted above, we can speed up EarleyFSA
by reducing the size of the WFSA. Unfortunately,
minimization of general FSAs is NP-hard. How-
ever, we can at least seek the minimal determin-
istic WFSA Msuch that |M| ≤ |M| , at least
in most semirings (Mohri, 2000; Eisner, 2003).
The determinization (Aho et al., 1986) and min-
imization (Aho and Hopcroft, 1974; Revuz, 1992)
algorithms for the boolean semiring are particu-
larly well-known. Minimization merges states,
which results in merging items, much as when
EarleyFast merged items that had different pre-
dot symbols (Leermakers, 1992; Nederhof and
Satta, 1997; Moore, 2000).
Another advantage of the WFSA presentation
of Earley’s is that it makes it simple to express a
tighter bound on the runtime. Much of the grammar
size|G|or|M| is due to terminal symbols that are
not used at most positions of the input. Suppose
the input is an ordinary sentence (one word at each
position, unlike the lattice case in footnote 7), and
suppose cis a constant such that no state qhas more
thancoutgoing arcs labeled with the same terminal
a∈Σ. Then when S tries to extend [i, j, q ], it
considers at most carcs. Thus, the O(|M|)factor
in our runtime (where |M|=|E|) can be replaced
withO(|Q| ·c+|E|), where E⊆ E is the set
of edges that are notlabeled with terminals.
8 Practical Runtime of Earley’s
We empirically measure the runtimes of Earley ,
EarleyFast , and EarleyFSA . We use the tropi-
cal semiring to find the highest-weighted deriva-
tion trees. We use two grammars that were ex-tracted from the PTB: Markov-order-2 (M2) and
Parent-annotated Markov-order-2 (PM2).For
each grammar, we ran our parsers (using the tropi-
cal semiring; Pin, 1998) on 100randomly selected
sentences of 5to40words from the PTB test-set
(mean 21.4, stdev 10.7), although we omitted sen-
tences of length >25from the Earley graph as it
was too slow ( >3minutes per sentence). The full
results are displayed in App. L. The graph shows
thatEarleyFast is roughly 20×faster at all sen-
tence lengths. We obtain a further speed-up of
2.5×by switching to EarleyFSA .
9 Conclusion
In this reference work, we have shown how the run-
time of Earley’s algorithm is reduced to O/parenleftbig
N|G|/parenrightbig
from the naive O/parenleftbig
N|G||R|/parenrightbig
. We presented this
dynamic programming algorithm as a deduction
system, which splits prediction and completion into
two steps each, in order to share work among re-
lated items. To further share work, we generalized
Earley’s algorithm to work with a grammar spec-
ified by a weighted FSA. We demonstrated that
these speed-ups are effective in practice. We also
provided details for efficient implementation of our
deduction system. We showed how to generalize
these methods to semiring-weighted grammars by
correctly transforming the grammars to eliminate
cyclic derivations. We further provided a method
to compute the total weight of all sentences with a
given prefix under a semiring-weighted CFG.
We intend this work to serve as a clean refer-
ence for those who wish to efficiently implement
an Earley-style parser or develop related incremen-
tal parsing methods. For example, our deduction
systems could be used as the starting point for
•neural models of incremental processing, in
which each derivation of an item contributes
not only to its weight but also to its represen-
tation in a vector space (cf. Drozdov et al.,
2019; Mei et al., 2020);
•biasing an autoregressive language model to-
ward high-weighted grammatical prefixes via
product-of-experts decoding (cf. Shin et al.,
2021; Roy et al., 2022; Fang et al., 2023);
•extensions to incremental parsing of more or
less powerful grammar formalisms.369510 Limitations
Orthogonal to the speed-ups discussed in this work,
Earley (1970) described an extension that we do not
include here, which filters deduction items using k
words of lookahead. (However, we do treat 1-word
lookahead and left-corner parsing in App. G.2.)
While our deduction system runs in time propor-
tional to the grammar size |G|, this size is measured
only after unary and nullary productions have been
eliminated from the grammar—which can increase
the grammar size as discussed in Apps. E and F.
We described how to compute prefix weights
only for EarleyFast , and we gave a prioritized
execution scheme (App. H.3) only for EarleyFast .
The versions for EarleyFSA should be similar.
Computing sentence weights (2) and prefix
weights (3) involves a sum over infinitely many
trees. In arbitrary semirings, there is no guaran-
tee that such sums can be computed. Comput-
ing them requires summing geometric series and—
more generally—finding minimal solutions to sys-
tems of polynomial equations. See discussion in
App. A and App. F. Non-commutative semirings
also present special challenges; see App. K.
Acknowledgments
We thank Mark-Jan Nederhof for useful references
and criticisms, and several anonymous reviewers
for their feedback. Any remaining errors are our
own.
Andreas Opedal is supported by the Max Planck
ETH Center for Learning Systems.
References369636973698A Semirings
As mentioned in §2, the definition of weighted
context-free grammars rests on the definition
of semirings. A semiring Sis a 5-tuple
⟨W,⊕,⊗,,⟩, where the set Wis equipped with
two operators: ⊕, which is associative and com-
mutative, and ⊗, which is associative and dis-
tributes over ⊕. The semiring contains values,∈Wsuch thatis an identity element for ⊕
(w⊕=⊕w=w,∀w∈W) and annihilator
for⊗(w⊗=⊗w=,∀w∈W) andis
an identity for ⊗(w⊗=⊗w=w,∀w∈W).
A semiring is commutative if additionally ⊗is
commutative. A closed semiring has an additional
operator ∗satisfying the axiom (∀w∈W)w=⊕w⊗w=⊕w⊗w. The interpretation
is that wreturns the infinite sum⊕w⊕(w⊗
w)⊕(w⊗w⊗w)⊕ ··· .
As an example that may be of particular inter-
est, Goodman (1999) shows how to construct a
(non-commutative) derivation semring, so that Z
in equation (2) gives the best derivation (parse tree)
along with its weight, or alternatively a representa-
tion of the forest of all weighted derivations. This
is how a weighted recognizer can be converted to a
parser.
B Earley’s Original Algorithm as a
Deduction System
§4 introduced the deduction system that corre-
sponds to Earley’s original algorithm. We explain
and analyze it here. Overall, the three rules of this
system, Earley (Table 1), correspond to possible
steps in a top-down recursive descent parser (Aho
et al., 1986):
•S consumes the next single input symbol
(the base case of recursive descent);
•P calls a subroutine to consume an
entire constituent of a given nonterminal type
by recursively consuming its subconstituents;
• C returns from that subroutine.
How then does it differ from recursive descent?
Rather like depth-first search, Earley’s algorithm
uses memoization to avoid redoing work, which
avoids exponential-time backtracking and infinite
recursion. But like breadth-first search, it pursues
possibilities in parallel rather than by backtracking.
The steps are invoked not by a backtracking call
stack but by a deduction engine, which can deduce3699new items in any convenient order. The effect on
the recursive descent parser is essentially to allow
co-routining (Knuth, 1997): execution of a recur-
sive descent subroutine can suspend until further
input becomes available or until an ancestor routine
has returned and memoized a result thanks to some
other nondeterministic execution path.
B.1 Predict
To look for constituents of type Bstarting at
position j, using the rule B→ρ, we need to
prove [j, j, B →ρ]. Earley’s algorithm imposes
[i, j, A→µB ν]as a side condition, so that we
only start looking if such a constituent Bcould be
combined with some item to its left.
B→ρ [i, j, A→µB ν][j, j, B →ρ]
Runtime analysis. How many ways are there
to jointly instantiate the two antecedents of P
with actual items? The pair of items is determined
by making four choices:indices iandjwith a do-
main size of N+1, dotted production A→µB ν
with domain size |G|, and production B→ρwith
a domain size of |R|. Therefore, the number of in-
stantiations of P isO/parenleftbig
N|G||R|/parenrightbig
. That is then
P’s contribution to the runtime of a suitable im-
plementation of forward chaining deduction, using
Theorem 1 of McAllester (2002).
B.2 Scan
If we have proved an incomplete item
[i, j, A→µa ν], we can advance the dot
if the next terminal symbol is a:
[i, j, A→µa ν] [j, k, a ][i, k, A →µ aν]This makes progress toward completing the A.
Note that S pushes the antecedent to a sub-
sequent item set T. Since terminal symbols have
a span width of 1, it follows that j=k−1.
Runtime analysis. S has three free choices:
indices iandjwith a domain size of N+ 1, and
dotted production A→µB νwith domain size
|G|. Therefore, S contributes O/parenleftbig
N|G|/parenrightbig
to the
overall runtime.
B.3 Complete
Recall that having [i, j, A→µB ν]allowed us
to start looking for a Bat position j(P).
Once we have found a complete Bby deriving
[j, k, B →ρ], we can advance the dot in the for-
mer rule:
[i, j, A→µB ν] [j, k, B →ρ][i, k, A →µ Bν]
Runtime analysis. C has five free choices:
indices i,j, and kwith a domain size of N+ 1,
dotted production A→µB νwith domain size
|G|, and the complete production B→ρwith a
domain size of |R|. Therefore, C contributes
O/parenleftbig
N|G||R|/parenrightbig
to the runtime.
B.4 Total Space and Runtime
By a similar analysis of free choices, the number
of items that the Earley deduction system will be
able to prove is O/parenleftbig
N|G|/parenrightbig
. This is a bound on the
space needed by the forward chaining implementa-
tion to store the items that have been proved so far
and index them for fast lookup (McAllester, 2002;
Eisner et al., 2005; Eisner, 2023).
Following Theorem 1 of McAllester (2002),
adding this count to the total number of rule instan-
tiations from the above sections yields a bound on
the total runtime of the Earley algorithm, namely
O/parenleftbig
N|G||R|/parenrightbig
as claimed.
C Previous Speed-ups
We briefly discuss past approaches used to improve
the asymptotic efficiency of Earley .
Leermakers (1992) noted that in an item of the
form [i, j, A→µν], the sequence µis irrelevant
to subsequent deductions. Therefore, he suggested
(in effect) replacing µwith a generic placeholder
⋆. This merges items that had only differed in their
µvalues, so the algorithm processes fewer items.
This technique can also be seen in Moore (2000)
and Klein and Manning (2001a,b). Importantly,3700this means that each nonterminal only has one
complete item, [j, k, B →⋆], for each span. This
effect alone is enough to improve the runtime of
Earley’s to O/parenleftbig
N|G|+N|G||R|/parenrightbig
. Our §5.2 will
give a version of the trick that only gets this effect,
by folding the C rule. The full version
of Leermakers (1992)’s trick is subsumed by our
generalized approach in §7.
While the GHR algorithm—a modified version
of Earley’s algorithm—is commonly known to be
O/parenleftbig
N|G||R|/parenrightbig
, Graham et al. (1980, §3) provide a
detailed exploration of the low-level implementa-
tion of their algorithm that enables it to be run in
O/parenleftbig
N|G|/parenrightbig
time. This explanation spans 20 pages
and includes techniques similar to those mentioned
in §5, as well as discussion of data structures. To
the best of our knowledge, these details have not
been carried forward in subsequent presentations
of GHR (Stolcke, 1995; Goodman, 1999). In the
deduction system view, we are able to achieve the
same runtime quite easily and transparently by fold-
ing both C (§5.2) and P (§5.1).In
both cases, this eliminates the pairwise interactions
between all |G|dotted productions and all |R|com-
plete productions, thereby reducing |G||R| to|G|.
D Correspondence Between Earley and
EarleyFast
The proofs of EarleyFast are in one-to-one corre-
spondence with the proofs of Earley .
We show the key steps in transforming between
the two styles of proof. Table 3 shows the corre-
spondence between an application of P and
an application of P1andP2, while Ta-
ble 4 shows the correspondence between an appli-
cation of C and an application of C 1and
C 2.
E Eliminating Unary Cycles
As mentioned in §6, our weighted deduction sys-
tem requires that we eliminate unary cycles from
the grammar. Stolcke (1995, §4.5) addresses the
problem of unary production cycles by modifying
the deduction rules.He assumes use of the prob-
ability semiring, where W= [0,1],⊕= + , and
⊗=×. In that case, inverting a single |N| × |N|
matrix suffices to compute the total weight of all
rewrite sequences A⇒B, known as unary chains ,for each ordered pair A, B∈ N.His modified
rules then ignore the original unary productions
and refer to these weights instead.
We take a very similar approach, but instead de-
scribe it as a transformation of the weighted gram-
mar, leaving the deduction system unchanged. We
generalize from the probability semiring to any
closed semiring—that is, any semiring that pro-
vides an operator ∗to compute geometric series
sums in closed form (see App. A). In addition, we
improve the construction: we do not collapse all
unary chains as Stolcke (1995) does, but only those
subchains that can appear on cycles. This prevents
the grammar size from blowing up more than neces-
sary (recall that the parser’s runtime is proportional
to grammar size). For example, if the unary produc-
tions are A→Afor all 1≤i < K , then there
is no cycle and our transformation leaves these
K−1productions unchanged, rather than replac-
ing them with K(K−1)/2new unary productions
that correspond to the possible chains A⇒Afor
1≤i≤j≤K.
Given a weighted CFG G=⟨N,Σ,R, S, w⟩,
consider the weighted graph whose vertices are N
and whose weighted edges A→Bare given by
the unary productions A→B. (This graph may
include self-loops such as A→A.) Its strongly
connected components (SCCs) will represent unary
production cycles and can be found in linear time
(and thus in O(|G|)time). For any AandBin
the same SCC, w(A⇒B)∈Wdenotes the total
weight of all rewrite sequences of the form A⇒B
(including the 0-length sequence with weight,
ifA=B). For an SCC of size K, there are
Ksuch weights and they can be found in total
timeO/parenleftbig
K/parenrightbig
by the Kleene–Floyd–Warshall algo-
rithm (Lehmann, 1977; Tarjan, 1981b,a). In the
real semiring, this algorithm corresponds to using
Gauss-Jordan elimination to invert I−E, where E
is the weighted adjacency matrix of the SCC (rather
than of the whole graph as in Stolcke (1995)). In
the general case, it computes the infinite matrix
sumI⊕E⊕(E⊗E)⊕ ··· in closed form, with
the help of theoperator of the closed semiring.
We now construct a new grammar G=
⟨N,Σ,R,S, w⟩that has no unary cycles, as fol-
lows. For each A∈ N , ourNcontains two non-
terminals, AandA. For each ordered pair of non-3701
terminals A, B∈ Nthat fall in the same SCC, R
contains a production A→Bwithw/parenleftbig
A→B/parenrightbig
=
w/parenleftig
A⇒B/parenrightig
. For every rule A→ρinRthat is not
of the form A→Bwhere AandBfall in the same
SCC,Ralso contains a production A→ρwith
w(A→ρ) =w(A→ρ), where ρis a version of
ρin which each nonterminal Bhas been replaced
byB. Finally, as a constant-factor optimization, A
andAmay be merged back together if Aformed a
trivial SCC with no self-loop: that is, remove the
weight-production A→AfromRand replace
all copies of AandAwithAthroughout G.
Of course, as Aycock and Horspool (2002)
noted, this grammar transformation does change
the derivations (parse trees) of a sentence, which is
also true for the grammar transformation in App. F
below. A derivation under the new grammar (with
weight w) may represent infinitely many deriva-
tions under the old grammar (with total weight
w). In principle, if the old weights were in the
derivation semiring (see App. A), then wwill be
a representation of this infinite set. This implies
that the ∗operator in this section, and the polyno-
mial system solver in App. F below, must be able
to return weights in the derivation semiring that
represent infinite context-free languages.
F Eliminating Nullary Productions
In addition to unary cycles (App. E) we must elim-
inate nullary productions in order to avoid cyclic
proofs, as mentioned in §6. This must be done
before eliminating unary cycles, since eliminating
nullary productions can create new unary produc-
tions. Hopcroft et al. (2007, §7.1.3) explain howto do this in the unweighted case. Stolcke (1995,
§4.7.4) sketches a generalization to the probability
semiring, but it also uses the non-semiring opera-
tions of division and subtraction (and is not clearly
correct). We therefore give an explicit general con-
struction.
While we provide a method that handles nullary
productions by modifying the grammar, it is also
possible to instead modify the algorithm to al-
low advancing the dot over nullable nonterminals,
i.e., nonterminals Asuch that the grammar allows
A⇒ε(Aycock and Horspool, 2002).
Our first step, like Stolcke’s, is to compute the
“null weight”
e=w/parenleftig
A⇒ε/parenrightig=/circleplusdisplayw(T) (7)
for each A∈ N . Although a closed semiring
does not provide an operator for this summation,
these values are a solution to the system of |N|
polynomial equations
e=/circleplusdisplayw(A→B···B)⊗/circlemultiplydisplaye(8)
In the same way, the free weights from equation (4)
in §6.1 are a solution to the system
Z=/circleplusdisplayw(A→ρ)⊗/circlemultiplydisplayZ (9)
which differs only in that ρis allowed to contain
terminal symbols. In both cases, the distributive3702property of semirings is being used to recursively
characterize a sum over what may be infinitely
many trees. A solution to system (8) must exist
for the sums in equation (2) to be well-defined
in the first place. (Similarly, a solution to sys-
tem (9) must exist for the sums in equations (3)
and (4) to be well-defined.) If there are multiple
solutions, the desired sum is given by the “mini-
mal” solution, in which as many variables as pos-
sible take on value. Often in practice the min-
imal solution can be found using fixed-point iter-
ation, which initializes all free weights toand
then iteratively recomputes them via system (8) (re-
spectively system (9)) until they no longer change
(e.g., at numerical convergence). For example,
this is guaranteed to work in the tropical semiring
(W,⊕,⊗,,) = (R,min,+,∞,0)and more
generally in ω-continuous semirings under condi-
tions given by Kuich (1997). Esparza et al. (2007)
and Etessami and Yannakakis (2009) examine a
faster approach based on Newton’s method. Neder-
hof and Satta (2008) review methods for the case
of the real weight semiring (W,⊕,⊗,,) =
(R,+,×,0,1).
Given the null weights e∈W, we now modify
the grammar as follows. We adopt the convention
that for a production A→ρthat is not yet in R,
we consider its weight to be w(A→ρ) =, and
increasing this weight by any non-amount adds
it toR. For each nonterminal Bsuch that e̸=,
let us assume the existence of an auxiliary nonter-
minal B/∈ N such that B̸⇒εbut∀x̸=ε,
w/parenleftig
B⇒x/parenrightig
=w/parenleftig
B⇒x/parenrightig
. We iterate this step:
as long as we can find a production A→µ B ν in
Rsuch that e̸=, we modify it to the more re-
stricted version A→µ Bν(keeping its weight),
but to preserve the possibility that B⇒ε, we also
increase the weight of the shortened production
A→µ νbye⊗w(A→µ B ν ).
A production A→ρwhere ρincludes knonter-
minals Bwithe̸=will be gradually split up by
the above procedure into 2productions, in which
eachBhas been either specialized to Bor re-
moved. The shortest of these productions is A→ε,
whose weight is w(A→ε) =eby equation (8).
So far we have preserved all weights w/parenleftig
A⇒x/parenrightig
,
provided that the auxiliary nonterminals behave as
assumed. For each Awe must now remove A→ε
fromR, and since Acan no longer rewrite as ε, we
rename all other rules A→ρtoA→ρ. This
closes the loop by defining the auxiliary nontermi-nals as desired.
Finally, since Sis the start symbol, we add back
S→ε(with weight e) as well as adding the new
ruleS→S(with weight). Thus (as in Chom-
sky Normal Form), the only nullary rule is now
S→ε, which may be needed to generate the 0-
length sentence. We now have a new grammar with
nonterminals N={S} ∪ { B:B∈ N} . To
simplify the names, we can rename the start sym-
bolStoSand then drop thesubscripts. Also,
any nonterminals that only rewrote as εin the orig-
inal grammar are no longer generating and can be
safely removed (see footnote 8).
G Working With Left Corners
G.1 Recursive Chains in Prefix Outside
Weights
As mentioned in §6.1, there is a subtle issue that
arises if the grammar has left-recursive produc-
tions. Consider the left-recursive rule B→B ρ.
Using equation (5), the prefix outside weight of
the predicted item [j, j, B →B ρ]will only in-
clude the weight corresponding to one rule appli-
cation of B→B ρ, but correctness demands that
we account for the possibility of recursively apply-
ingB→B ρas well. A well-known technique to
remove left-recursion is the left-corner transform
(Rosenkrantz and Lewis, 1970; Johnson and Roark,
2000). As that may lead to drastic increases in
grammar size, however, we instead provide a mod-
ification of P1that deals with this technical
complication (which adapts Stolcke (1995, §4.5.1)
to our improved deduction system and generalizes
it to closed semirings). Fig. 3 provides some further
intuition on the left-recursion issue.
We require some additional definitions: Bis
a left child of Aiff there exists a rule A→B ρ.
Thereflexive and transitive closure of the left-child
relation is⇒, which was already defined in §2.
A nonterminal Ais said to be left-recursive if A
is a nontrivial left corner of itself, i.e., if A⇒A
(meaning that A→B ρandB⇒Afor some B).
A grammar is left-recursive if at least one of its
nonterminals is left-recursive.
To deal with left-recursive grammars, we col-
lapse the weights of left-recursive paths similarly
as we did with unary cycles (see App. E), and ⊗-
multiply in at the P1 step.
We consider the left-corner multigraph: given
a weighted CFG G=⟨N,Σ,R, S, w⟩, its vertices
areNand its edges are given by the left-child re-3703
lations, with one edge for every production. Each
edge is associated with a weight equal to the weight
of the corresponding production ⊗-times the free
weights of the nonterminals on the right hand side
of the production that are not the left-child. For
instance, for a production A→B C D , the weight
of the corresponding edge in the graph will be
w(A→B C D )⊗Z⊗Z. This graph’s SCCs
represent the left-corner relations. For any AandB
in the same SCC w/parenleftig
A⇒B/parenrightig
∈Wdenotes the to-
tal weight of all left-corner rewrite sequences of the
form A⇒B, including the free weights needed
to compute the prefix outside weights. These can,
again, be found in O/parenleftbig
K/parenrightbig
time with the Kleene–
Floyd–Warshall algorithm (Lehmann, 1977; Tarjan,
1981b,a), where Kis the size of the SCC. These
weights can be precomputed and have no effect on
the runtime of the parsing algorithm. We replace
P1 with the following:[i, j, A→µC ν]
µ̸=ε C⇒B [j, j, B →⋆]
A one-step proof of P1contributes
˙α([i, j, A→µC ν])⊗w/parenleftig
C⇒B/parenrightig
⊗˙β([i, j, A→µC ν]) (10)
to the prefix outside weight ˙α([j, j, B →ρ]).
Note that the case B=Crecovers the standard
P1, and such rules will always be instantiatedsince⇒is reflexive. The P1rule has three
side conditions (whose visual layout here is not sig-
nificant). Its consequent will feed into P2; the
condition µ̸=εensures that the output of P2
cannot serve again as a side condition to P1,
since the recursion from Cwas already fully com-
puted by the C⇒Bitem. However, since this
condition prevents P1from predicting any-
thing at the start of the sentence, we must also
replace the start axiom [0,0, S→⋆]with a rule
that resembles P1and derives the start axiom
together with all its left corners: S⇒B[0,0, B→⋆]
The final formulas for aggregating the prefix
outside weights are spelled out explicitly in Table 5.
Note that we did not spell out a corresponding
prefix weight algorithm for EarleyFSA .
G.2 One-Word Lookahead
Orthogonally to App. G.1, we can optionally ex-
tend the left child relation to terminal symbols,
saying that ais a left child of Aif there exists a
ruleA→a ρ.
The resulting extended left-corner relation (in
its unweighted version) can be used to construct a
side condition on P1(orP1), so that at
position j, it does not predict all symbols that are
compatible with the left context, but only those that
arealso compatible with the next input terminal.3704
To be precise, P1(orP1) should only
predict Bat position jif[j, k, a ]andB⇒a(for
some a). This is in fact Earley (1970)’s k-word
lookahead scheme in the special case k= 1.
G.3 Left-Corner Parsing
Nederhof (1993) and Nederhof (1994b) describe
aleft-corner parsing technique that we could ap-
ply to further speed up Earley’s algorithm. This
subsumes the one-word lookahead technique of the
previous section. Eisner and Blatz (2007) sketched
how the technique could be derived automatically.
Normally, if Bis a deeply nested left corner
ofC, then the item A→µC νwill trigger a
long chain of P actions that culminates in
[j, j, B →⋆]. Unfortunately, it may not be pos-
sible for this B(or anything predicted from it) to
S its first terminal symbol, in which case the
work has been wasted.
But recall from App. G.1 that the P1rule
effectively summarizes this long chain of predic-
tions using a precomputed weighted item C⇒B.The left-corner parsing technique simply skips the
P steps and uses C⇒Bas a side condi-
tion to lazily check after the fact that the relevant
prediction of a-initial rule could have been made.
P1is removed, so the method never creates
dotted productions of the form A→µνwhere
µ=ε—except for the start item and the items
derived from it using P2.
InC2, a side condition µ̸=εis added. For
the special case µ=ε, a new version of C2is
used in which
–i=jis required,
–the first antecedent [i, i, A→B ν]is re-
placed by A→B ν (which ensures that
[i, i, A→B ν]is an item of EarleyFast ),
–the side conditions [h, i, D →µC ν]and
C⇒A(which ensures that EarleyFast
would have P ed that item). Note that
µ=εis possible in the case where Dis the
start symbol S.
TheS rule is split in exactly the same way into
µ̸=εandµ=εvariants.3705H Execution of Weighted EarleyFast
Eisner (2023) presents generic strategies for execut-
ing unweighted and weighted deduction systems.
We apply these here to solve the weighted recogni-
tion and prefix weight problems, by computing the
weights of all items that are provable from given
grammar and sentence axioms.
H.1 Execution via Multi-Pass Algorithms
The Earley andEarleyFast deduction systems
are nearly acyclic, thanks to our elimination of
unary rule cycles and nullary rules from the
grammar. However, cycles in the left-child re-
lation can still create deduction cycles, with
[k, k, A →B X]and[k, k, B →A Y]proving
each other via P or via P1 and P2.
Weighted deduction can be accomplished for
these systems using the generic method of Eisner
(2023, §7). This will detect the left-child cycles
at runtime (Tarjan, 1972) and solve the weights to
convergence within each strongly connected com-
ponent (SCC). While solving the SCCs can be ex-
pensive in general, it is trivial in our setting since
the weights of the items within an SCC do not ac-
tually depend on one another: these items serve
only as side conditions for one another. Thus, any
iterative method will converge immediately.
Alternatively, the deduction system becomes
fully acyclic when we eliminate prediction chains
as shown in App. G.1. In particular, this modi-
fied version of EarleyFast replaces P1with
P1.Using this acyclic deduction sys-
tem allows a simpler execution strategy: under
any acyclic deduction system, a reference-counting
strategy (Kahn, 1962) can be applied to find the
proved items and then compute their weights in
topologically sorted order (Eisner, 2023, §6).
In both cyclic and acyclic cases, the above
weighted recognition strategies consume only a
constant factor more time and space than their un-
weighted versions, across all deduction systems
and all inputs.ForEarleyFast and its acyclic
version, this means the runtimes are O(N|G|)for aclass of “bounded-state” grammars, O/parenleftbig
N|G|/parenrightbig
for
unambiguous grammars, and O/parenleftbig
N|G|/parenrightbig
for gen-
eral grammars (as previewed in the abstract and §1).
The space requirements are respectively O(N|G|),
O/parenleftbig
N|G|/parenrightbig
, andO/parenleftbig
N|G|/parenrightbig
. The same techniques
apply to EarleyFSA , replacing |G|with|M|.
H.2 One-Pass Execution via Prioritization
For the acyclic version of the deduction system
(App. G.1), an alternative strategy is to use a pri-
oritized agenda to visit the items of the acyclic
deduction system in some topologically sorted or-
der (Eisner, 2023, §5). This may be faster in prac-
tice than the generic reference-counting strategy
because it requires only one pass instead of two.
It also remains space-efficient. On the other hand,
it requires a priority queue, which adds a term to
the asymptotic runtime (worsening it in some cases
such as bounded-state grammars).
We must associate a priority π(V)with each
itemVsuch that if Uis an antecedent or side condi-
tion in some rule that proves V, then π(U)< π(V).
Below, we will present a nontrivial prioritization
scheme in which the priorities implicitly take the
form of lexicographically ordered tuples.
These priorities can easily be converted to inte-
gers in a way that preserves their ordering. Thus,
a bucket queue (Dial, 1969) or an integer prior-
ity queue (Thorup, 2000) can be used (see Eis-
ner (2023, §5) for details). The added runtime
overheadisO(M)for the bucket queue or
O(Mlog log M)for the integer priority queue,
where M=O/parenleftbig
N|G|/parenrightbig
is the number of distinct
priority levels in the set of possible items, and
M≤Mis the number of distinct priority lev-
els of the actually proved items, which depends on
the grammar and input sentence.
For EarleyFast with the modifications of
App. G.1, we assign the minimum priority to all of
the axioms. All other items have one of six forms:
1.[j, k, B →ρ](antecedent to C 1, P)
2.[j, k, B →⋆]
(rightmost antecedent to C 2)
3.[j, k, B →µν]where µ̸=ε, ν̸=ε(left-
most antecedent to P1, S, P)
4.[k, k](antecedent to nothing)
5.[k, k, B →⋆](antecedent to P2)
6.[k, k, B →ρ]
(leftmost antecedent to S, C 2)3706The relative priorities of these items are as follows:
•Items with smaller kare visited sooner (left-
to-right processing).
•Among items with the same k, items with
j < k are visited before items with j=k.
Thus, the leftmost antecedent of P1
precedes its consequent.
•Among items with the same kand with j < k ,
items with larger jare visited sooner. Thus,
the rightmost antecedent of C 2precedes
its consequent in the case i < j , where a
narrower item is used to build a wider one.
•Among items of the first two forms with the
same kand the same j < k ,Bis visited
sooner than AifA⇒B. This ensures that
the rightmost antecedent of C 2precedes
its consequent in the case i=jandν=ε,
which completes a unary constituent.
To facilitate this comparison, one may assign
integers to the nonterminals according to their
height in the unweighted graph whose vertices
areNand whose edges A→Bcorrespond
to the unary productions A→B. (This graph
is acyclic once unary cycles have been elimi-
nated by the method of App. E.)
•Remaining ties are broken in the order of the
numbered list above. This ensures that the
antecedents of C 1,P, and P2pre-
cede their consequents, and the rightmost an-
tecedent of C 2precedes its consequent
in the case i=jandν̸=ε, which starts a
non-unary constituent.
To understand the flow of information, notice that
the 6 specific items in the numbered list above
would be visited in the order shown.
H.3 Pseudocode for Prioritized Algorithms
For concreteness, we now give explicit pseudocode
that runs the rules to build all of the items in the
correct order. This may be easier to implement
than the above reductions to generic methods. It is
also slightly more efficient than App. H.2, due to
exploiting some properties of our particular system.
Furthermore, in this section we handle
EarleyFast as well as its acyclic modification.
When the flag pis set to true, we carry out
the acyclic version, which replaces P1withP1andS (App. G.1), and also includes
P(§6.1) to find prefix weights.
The algorithm pops (dequeues) and processes
items in the same order as App. H.2 (when pis
true), except that in this version, axioms of the
form B→ρand[k−1, k, a]are never pushed
(enqueued) or popped but are only looked up in
indices. Similarly, the [j, j]items (used to find pre-
fix weights) are never pushed or popped but only
proved. Thus, none of these items need priorities.
When an item Uis popped, our pseudocode
invokes only deduction rules for which Umight
match the rightmost antecedent (which could be
a side condition), or in the case of S or
P1, the leftmost antecedent. In all cases,
the other antecedents are either axioms or have
lower priorities. While we do not give pseudocode
for each rule, invoking a rule on Ualways checks
first whether Uactually does match the relevant
antecedent. If so, it looks up the possible matches
for its other antecedents from among the axioms
and the previously proved items. This may allow
the rule to prove consequents, which it adds to the
queues and indices as appropriate (see below).
The main routine is given as Alg. 1. A queue it-
eration such as “ fork∈ Q: . . . ” iterates over a col-
lection that may change during iteration; it is short-
hand for “ whileQ ̸=∅: {k=Q.pop(); . . . }.”
We maintain a dictionary (the chart ) that maps
items to their weights. Each time an item V
is proved by some rule, its weight w(V)is up-
dated accordingly, as explained in §6 and Table 5.
The weight is ˙β(V)or(˙β(V),˙α(V))according to
whether pisfalse ortrue.
Alg. 1 writes C(pattern )to denote the set of all
provable items (including axioms) that match pat-
tern. This set will have previously been computed
and stored in an index dedicated to the specific
invocation of C(pattern )in the pseudocode. The
index is another dictionary, with the previously
bound variables of the pattern serving as the key.
The pseudocode for individual rules also uses in-
dices, to look up antecedents.
When an item Visfirst proved by a rule and
added to the chart, it is also added to all of the ap-
propriate sets in the indices. Prioritization ensures
that we do not look up a set until it has converged.
Each dictionary may be implemented as a hash
table, in which case lookup takes expected O(1)
time under the Uniform Hashing Assumption. An
array may also be used for guaranteed O(1)access,3707although its sparsity may increase the algorithm’s
asymptotic space requirements.
What changes when pisfalse , other than a few
of the rules? Just one change is needed to the prior-
itization scheme of App. H.2. The EarleyFast de-
duction system is cyclic, as mentioned in App. H.1,
so in this case, we cannot enforce π(U)< π(V)
when UandVare an antecedent and consequent
of the same rule. We will only be able to guarantee
π(U)≤π(V), where the =case arises only for
P1andP2. To achieve this weaker priori-
tization, we modify our tiebreaking principle from
App. H.2 (when pisfalse) to say that for a given k,
all items of the last two forms have equal priority
and thus may be popped in any order.When a
rule proves a consequent that has the same priority
as one of its antecedents, it is possible that the con-
sequent had popped previously. In our case, this
happens only for the rule P1, so crucially, it
does not matter if the new proof changes the conse-
quent’s weight—this consequent is used only as a
side condition (to P2) so its weight is ignored.
However, to avoid duplicate work, we must take
care to avoid re-pushing the consequent now that it
has been proved again.
Rather than place all the items on a single queue
that is prioritized lexicographically as in App. H.2,
we use a collection of priority queues that are com-
bined in the pseudocode to have the same effect.
They are configured and maintained as follows.
•Qis a priority queue of distinct positions k∈
{0, . . . , N }, which pop in increasing order.
kis added to it upon proving an item of the
form [·, k,·]. Initially Q={0}due to the
start axiom [0,0, S→⋆].
•For each k∈ Q ,Pis a priority queue of
distinct positions j∈ {0, . . . , k }, which pop
in decreasing order except that kitself pops
last.jis added to it upon proving an item of
the form [j, k,·]. Initially P={0}due to
the start axiom.Algorithm 1 EarleyFast with priority queuesfunction E F(G,x, p) addG,xaxioms to dictionaries and queues ifp: S () ▷ fork∈ Q :▷ forj∈ P: forB∈ N: forU∈ C([j, k, B →ρ]):▷ C 1(U); P(U) forU∈ C([j, k, B →⋆]):▷ C 2(U) forU∈ C([j, k, B →µν], µ̸=ε̸=ν): S(U); P(U) ▷ ifp: P1(U)elseP1(U) ifpandk >0: ▷ noww([k, k]) = prefix weight of x ifp: forU∈ C([k, k, B →⋆]):▷ P2(U) forU∈ C([k, k, B →ρ]):▷ S(U) else forU∈ S: ▷ ▷ P1(U); P2(U); S(U) forU∈ C([0,|x|, S→⋆]): return w(U) ▷ return ▷
•For each j∈ Pwithj < k ,Nis a priority
queue of distinct nonterminals B∈ N , which
pop in the height order described in App. H.2
above. Bis added to it upon proving an item
of the form [j, k, B →ρ].
•Ifpisfalse, then for each k∈ Q,Sis a queue
of all proved items of the form [k, k, B →⋆]
or[k, k, B →ρ]. These items have equal
priority so may pop in any order (e.g., LIFO).
Initially Scontains just the start axiom.
Transitive consequents added later to a queue
always have ≥priority than their antecedents that
have already popped, so the minimum priority of
the queue increases monotonically over time. This
monotone property is what makes bucket queues
viable in our setting (see Eisner, 2023, §5). In
general, our priority queues are best implemented
as bucket queues if they are dense and binary heaps
or integer priority queues if they are sparse.3708I Binarized EarleyFSA
Table 6 gives a version of EarleyFSA in which
the ternary deduction rules S,C 1and
C 2have been binarized using the fold trans-
form, as promised in §7.
•TheS1andS2rules, which replace
S, introduce and consume new interme-
diate items of the form [i, j, q⇝⋆]. The
S1rule sums over possible start positions
jfor word a. This is only advantageous in the
case of lattice parsing (see footnote 7), since
for string parsing, the only possible choice of
jisk−1.
•In a similar vein, C 2andC 2in-
troduce and consume new intermediate items
[i, j, ⋆⇝q]. The C 2rule aggregates
different items from itojthat are looking
for a Bconstituent to their immediate right,
summing over their possible current states q.
•Similarly, C 1introduces new interme-
diate items that sum over possible final states
q.
•We did not bother to binarize the ternary rule
F , as there is no binarization that pro-
vides an asymptotic speed-up.
There are different ways to binarize inference
rules, and in Table 6 we have chosen to binarize
S andC 2in complementary ways. Our
binarization of S is optimized for the common
case of a dense WFSA and a sparse sentence, where
stateqallows many terminal symbols abut the in-
put allows only one (as in string parsing) or a few.
S1finds just the symbols aallowed by the in-
put and S2looks up only those out-arcs from
q. Conversely, our binarization of C 2is opti-
mized for the case of a sparse WFSA and a dense
parse table: C 2finds the small number of
incomplete constituents over [i, j]that are looking
for aB, and C 2looks those up when it finds
a complete Bconstituent, just like EarleyFast .
It is possible to change each of these bina-
rizations. In particular, binarizing S by first
combining [i, j, q ]with q⇝q(analogously to
C 2) would be useful when parsing a large or
infinite lattice—such as the trie implicit in a neu-
ral language model—with a constrained grammar
(Shin et al., 2021; Fang et al., 2023).J Handling Nullary and Unary
Productions in an FSA
As for EarleyFast ,EarleyFSA (§7) requires elim-
ination of nullary productions. We can handle
nullary productions by directly adapting the con-
struction of App. F to the WFSA case. Indeed,
the WFSA version is simpler to express. For each
arcq⇝qsuch that B∈ N ande̸=, we
replace the Blabel of that arc with B(preserv-
ing the arc’s weight), and add a new arc q⇝q
of weight e. We then define a new WFSA
M= (M ∩ ¬M)∪ M, where M
is an unweighted FSA that accepts exactly those
strings of the form /hatwideA(i.e., nullary productions), ¬
takes the unweighted complement, and M is
a WFSA that accepts exactly strings of the form /hatwideS
(with weight e) and S/hatwideS(with weight). As
this construction introduces new εarcs, it should
precede the elimination of ε-cycles.
Notice that in the example of App. F where a
production A→ρwas replaced with up to 2−1
variants, the WFSA construction efficiently shares
structure among these variants. It adds at most k
edges at the first step and at most doubles the total
number of states through intersection with ¬M.
Similarly, we can handle unary productions by
directly adapting the construction of App. E to the
WFSA case. We first extract all weighted unary
rules by intersecting Mwith the unweighted lan-
guage {B/hatwideA:A, B∈ N} (and determinizing the
result so as to combine duplicate rules). Exactly as
in App. E, we construct the unary rule graph and
compute its SCCs along with weights w/parenleftig
A⇒B/parenrightig
for all A, B in the same SCC. We modify the
WFSA by underlining all hatted nonterminals /hatwideA
and overlining all nonterminals B. Finally, we de-
fine our new WFSA grammar (M ∩ ¬M)∪
M. Here Mis an unweighted FSA that
accepts exactly those strings of the form B/hatwideAand
M is a WFSA that accepts exactly strings of
the form B/hatwideAsuch that A, B are in the same SCC,
with weight w/parenleftig
A⇒B/parenrightig
.
Following each construction, nonterminal names
can again be simplified as in Apps. E and F.
Finally, §7 mentioned that we must eliminate
ε-cycles from the FSA. The algorithm for doing
so (Mohri, 2002) is fundamentally the same as our
method for eliminating unary rule cycles from a
CFG (App. E), but now it operates on the graph3709
whose edges are ε-arcs of the FSA, rather than the
graph whose edges are unary rules of the CFG.
K Non-Commutative Semirings
We finally consider the case of non-commutative
weight semirings, where the order of multiplication
becomes significant.
In this case, in the product (1) that defines the
weight of a derivation tree T, the productions
should be multiplied in the order of a pre-order
traversal of T.
In §3, when we recursively defined the weight
w(d)of a proof, we took a product over the above-
the-bar antecedents of a proof rule. These should
be multiplied in the same left-to-right order that is
shown in the rule. Our deduction rules are care-
fully written so that under these conventions, the
resulting proof weight matches the weight (1) of
the corresponding CFG derivation.
For the same reason, the same left-to-right order
should be used in §3 when computing the inside
probability ˙β(V)of an item.
Eliminating nullary productions from a weighted
CFG (App. F) is not in general possible in non-commutative semirings. However, if the gram-
mar has no nullary productions or is converted
to an FSA before eliminating nullary productions
(App. J), then weighted parsing may remain possi-
ble.
What goes wrong? The construction in App. F
unfortunately reorders the weights in the product
(1). Specifically, in the production A→µ B ν ,
the product should include the weight eafter
the weights in the µsubtrees, but our construc-
tion made it part of the weight of the modified
production A→µ νand thus moved it before the
µsubtrees. This is incorrect when µ̸=εand⊗is
non-commutative.
The way to rescue the method is to switch to
using WFSA grammars (§7). The WFSA gram-
mar breaks each rule up into multiple arcs, whose
weights variously fall before, between, and after
the weights of its children. When defining the
weight of a derivation under the WFSA grammar,
we do not simply use a pre-order traveral as in
equation (1). The definition is easiest to convey in-
formally through an example. Suppose a derivation
tree for A⇒xuses a WFSA path at the root that3710accepts BC/hatwideAwith weight w. Recursively let w
andwbe the weights of the child subderivations,
rooted at BandC. Then the overall weight of the
derivation of Awill not be w⊗w⊗w(prefix or-
der), but rather w⊗w⊗w⊗w⊗w. Here we
have factored the path weight wintow⊗w⊗w,
which are respectively the weights of the subpath
up through B(including the initial-state weight),
the subpath from there up through C, and the sub-
path from there to the end (including the final-state
weight).
When converting a CFG to an equivalent WFSA
grammar (footnote 14), the rule weight always goes
at the start of the rule so that the weights are un-
changed. However, the nullary elimination pro-
cedure for the WFSA (App. J) is able to replace
unweighted nonterminals in the middle of a pro-
duction with weighted ε-arcs. This is the source
of its extra power, as well as its greater simplicity
compared to App. F.
It really is not possible to fully elimi-
nate nulls within the simpler weighted
CFG formalism. Consider an unambigu-
ous weighted CFG whose productions are
S→a S A, S →b S B, S →c, A→ε, B→ε,
with respective weights w, w, w, w, w. Then
a string x=abbc will have Zgiven by the mir-
rored product w⊗w⊗w⊗w⊗w⊗w⊗w.
Within our weighted CFG formalism, there is no
way to include the final weights w⊗w⊗w
if we are not allowed to have null constituents in
those positions.
Even with WFSAs, there is still a problem—in
the non-commutative case, we cannot eliminate
unary rule cycles (App. J). If we had built a binary
Aconstituent with weight w, then a unary CFG rule
A→Awith weight wrequired us to compute the
total weight of all derivations of A, by taking a
summation of the form w⊕(w⊗w)⊕(w⊗
w⊗w)⊕ ··· . This factors as (⊕w⊕(w⊗
w)⊕ ··· )⊗w, and unary rule cycle elimination
served to precompute the parenthesized sum, which
was denoted as w/parenleftig
A⇒A/parenrightig
, and record it as the
weight of a new rule A→A. However, in the non-
commutative case, the WFSA path corresponding
toA→Amight start with wand end with w. In
that case, the necessary summation has the form
w⊕(w⊗w⊗w)⊕(w⊗w⊗w⊗w⊗
w)⊕··· . Unfortunately this cannot be factored as
before, so we cannot precompute the infinite sums
as before.The construction in App. J assumed
that we could extract weighted unary rules from
the WFSA, with a single consolidated weight at the
start of each rule—but consolidating the weight in
that way required commutativity.
L Runtime Experiment Results
More details on the experiments of §8 appear in
Fig. 4.3711ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Sec 9
/squareA2. Did you discuss any potential risks of your work?
Not applicable. We do not foresee any direct risks of our work.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Sec 0 and sec 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
Sec 7 and app I
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Sec 73712/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Sec 7
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
App I
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.3713