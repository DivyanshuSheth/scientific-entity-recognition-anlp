
Orion Weller*, Marc Marone*,
Vladimir Braverman, Dawn Lawrie, Benjamin Van Durme
Johns Hopkins University
oweller@cs.jhu.edu,mmarone1@jhu.edu
Abstract
Since the advent of Federated Learning (FL),
research has applied these methods to natural
language processing (NLP) tasks. Despite a
plethora of papers in FL for NLP, no previous
works have studied how multilingual text im-
pacts FL algorithms. Furthermore, multilingual
text provides an interesting avenue to examine
the impact of non-IID text (e.g. different lan-
guages) on FL in naturally occurring data. We
explore three multilingual language tasks, lan-
guage modeling, machine translation, and text
classification using differing federated and non-
federated learning algorithms. Our results show
that using pretrained models reduces the nega-
tive effects of FL, helping them to perform near
or better than centralized (no privacy) learning,
even when using non-IID partitioning.
1 Introduction
Federated learning (FL) is a machine learning tech-
nique that trains a model across multiple distributed
clients holding local data samples, without ever
storing client data in a central location (Kone ˇcn`y
et al., 2016; McMahan et al., 2017). These tech-
niques are appealing for those who wish to learn
from data in a privacy-preserving way, without ever
transmitting the data off of a client device. FL be-
comes essential when data is especially sensitive,
as is the case at hospitals, legal firms, financial
institutions, or in countries that enact legislation
concerning data privacy (such as the EU’s GDPR
or the US’s HIPAA).
FL has been applied to problems in natural lan-
guage processing (NLP) since its inception, partic-
ularly in use of the language modeling task (Yang
et al., 2018; Hard et al., 2018; Ramaswamy et al.,
2019; Chen et al., 2019a; Ji et al., 2019; StremmelFigure 1: A depiction of different learning strategies
with Federated Learning (FL) and multilingual data,
with 4 clients and 16 instances from En, Fr, Ru, and Zh
in this toy example. Black lines indicate gradient flow.
Centralized learning is the standard training method (no
privacy), FL with IID data partitions the data into IID
data subsets for each client, while FL with non-IID data
has the languages separated across clients.
and Singh, 2020). Another large area of FL re-
search is focused on analyzing performance when
the data is non identically independently distributed
(non-IID). In these cases, many works have shown
that FL performance is sub-par with respect to cen-
tralized learning methods (Kone ˇcn`y et al., 2016;
Hard et al., 2018; Lin et al., 2021).
Despite the large amount of research in FL for
NLP, how different languages impact the FL train-
ing process has yet to be explored (Liu et al., 2021).
Furthermore, multilingual FL provides an interest-
ing and natural setting to explore non-IID data, of
which different languages are an obvious example.
In this work, we explore multilingual federated
learning across three multilingual language tasks
and different stages of model pretraining. Our re-
sults show that fine-tuning pretrained models with
FL can perform similarly to pretrained models fine-
tuned with the standard centralized method (the1413no privacy setting), despite having completely non-
IID language partitioned data. This finding shows
that pretrained models provide an effective way for
practitioners (and consumers) of multilingual data
to gain the privacy benefits of FL at little or no cost
to the final task performance.
2 Background and Related Work
The term Federated Learning was first proposed
in McMahan et al. (2017), who applied the
FederatedAveraging algorithm to the tasks
of language modeling and image classification.
Since then, much of the theoretical and applied
work in FL (e.g. Chen et al. (2019b); Wu et al.
(2020) among many others) has considered lan-
guage modeling as a key task or benchmark.
Concurrent with the growing interest in Feder-
ated Learning, NLP has rapidly shifted toward the
use of pretrained language models (PLMs) (e.g.,
BERT Devlin et al. 2019; T5 Raffel et al. 2019;
GPT-3 Brown et al. 2020). These PLMs are used
for both the core task of next word prediction and
as a starting point for learning other downstream
NLP tasks. This pretrain-and-fine-tune paradigm
has since become ubiquitous in modern NLP and
has inspired a large and active area of research in
model pretraining. Multilingual versions of these
pretrained models have since been developed and
are often used with transfer learning techniques
to increase performance for tasks where data is
limited (e.g. mBERT from Devlin et al. 2019).
The intersection of distributed learning from pri-
vate data partitions and PLMs is still a nascent area.
Several works have explored more efficient meth-
ods of federated communication with the purpose
of enabling these larger NLP models for produc-
tion situations (Sui et al., 2020; Wu et al., 2021).
Our work is orthogonal to these (and could be com-
bined in future work), as we explore the effects of
multilingual data on PLM FL, rather than creating
methods to enable their use. Other papers focus on
the gap between federated learning performance
and centralized performance, evaluating on a wide
variety of English NLP tasks (Liu and Miller, 2020;
Lin et al., 2021; Chen et al., 2021). Although they
focus on differential privacy (DP) rather than FL,
Li et al. (2021) find that direct PLM training is
difficult with standard DP methods, but that fine-
tuning PLMs on English data is possible with pri-
vate learning techniques. We differ from all these
works by studying private learning, specifically FL,for PLMs in the novel multilingual setting.
3 Experimental Design
3.1 Federated Learning Methods
We use FederatedAveraging as the pri-
mary learning algorithm (McMahan et al., 2017).
FederatedAveraging was introduced along-
side the term Federated Learning and has been stud-
ied in both learning theory research (Stich, 2019)
and applied work (Hard et al., 2018; Lin et al.,
2021). In this algorithm, each client runs stochas-
tic gradient descent (SGD) on its local data. After
a specified number of steps, the client transmits
its local model to the server, which averages these
updates into a single centralized set of parameters.
The server then broadcasts the centralized parame-
ters to each client and the process repeats.
3.2 Client Partitioning
We consider three different training settings: stan-
dard training with no FL (e.g. centralized orC), FL
with IID data ( FL IID orI), where the data for each
client is sampled randomly from all data, and FL
with non-IID data ( FL non-IID orN) where each
client only sees data for one language (or for MT,
one direction). See Figure 1 for a visual depiction
of these three client partitioning schemes.
3.3 Data
We study three multilingual language tasks, due
to their common use in the community: language
modeling (LM), machine translation (MT), and
text classification (TC). We note that the data we
use for training is relatively small; however, this
mirrors pratical FL, as each client will not have a
large amount of data. We measure scores using
perplexity (PPL) for LM, BLEU (Papineni et al.,
2002) for MT, and accuracy for TC.
Europarl We use the Europarl corpus (Koehn
et al., 2005) taken from transcripts of European
Union meetings. We sample data from eight lan-
guages: English, Spanish, Portuguese, French, Ger-
man, Finnish, Polish, Lithuanian, and Czech. We
sample 20k of each language for training and 5k
for validation/testing, and use it for the LM task.
MTNT We use the Machine Translation of Noisy
Text (MTNT) dataset (Michel and Neubig, 2018),
which was the testset for the 2019 WMT robust-
ness challenge. MTNT was gathered from user1414
comments on Reddit discussion threads and con-
tains noisy text including typos, casual language,
and niche terminology. The dataset contains two
non-English languages that we use: En →Fr and
En→Ja. This dataset has been used to test MT
systems for robustness to domain shift (Li et al.,
2019) and is suitable for our experiments since FL
deals with client data that is uniquely shifted from
centralized data. For more details on MTNT data
preprocessing for M2M-100, see Appendix C.
UN Corpus The UN Corpus (Ziemski et al.,
2016) consists of official records from the UN
proceedings over the years 1990 to 2014, in six
languages: English, French, Spanish, Russian, Chi-
nese, and Arabic. We use this data for LM (with
50k instances of training data per language and 5k
for validation/testing) as well as three MT direc-
tions covering 6 languages (En →Fr, Ar→Es, Ru
→Zh). Following previous work in MT adaption
(see MTNT above) we sample 10k in each direction
for training and 5k each for evaluation sets.
NC Corpus For text classification we use the
News Classification (NC) dataset from the XGLUEbenchmark for cross-lingual language understand-
ing (Liang et al., 2020). This is a classification
problem with 10 classes across 5 languages: En-
glish, Spanish, French, German, and Russian. We
predict the article category given the article title and
body (e.g. finance, sports, travel). Since only 10k
annotated examples are available for each language
(excluding the official test set), we sample 8k in-
stances for training and 1k for evaluation sets. Note
that although XGLUE is made for cross-lingual
evaluation, we use it for multilingual evaluation.
3.4 Modeling
For language modeling and text classification, we
examine two different initialization settings: (1)
fine-tuning from a pretrained multilingual model or
(2) training the same multilingual model architec-
ture but doing so with randomly initialized weights.
For the MT experiments, we omit the randomly-
initialized results as MT systems generally need
large amounts of data to produce good results (see
Appendix B for more details).
Our base model for the LM task is a distilled
version of the mBERT model (134M parameters),1415MTNT UN
Method En-Fr En-Ja Avg En-Fr Ar-Es Ru-Zh Avg
No Training 30.7 14.1 22.4 31.4 27.4 27.9 28.9
Centralized 31.8 *15.4 23.6 37.3 35.9 34.1 35.8
IID FL 33.1 15.6 24.4 38.6 36.9 *35.6 37.0
non-IID FL *32.9 15.6 24.3 37.9 *36.6 35.7 36.7
Method En Es Fr De Ru Avg
Centralized 86.6 ± 0.3 77.5 ± 1.2 74.9 ± 1.6 *82.3 ± 1.6 80.7 ± 0.7 80.4 ± 0.6
IID FL 88.0 ± 0.6 79.8 ± 0.5 76.4 ± 0.6 82.6 ± 0.6 82.5 ± 0.4 81.8 ± 0.3
non-IID FL 81.0 ± 0.9 69.3 ± 1.6 73.7 ± 1.0 76.0 ± 0.3 71.9 ± 1.1 74.4 ± 0.5
Centralized 93.5 ± 0.7 *86.3 ± 0.5 82.9 ± 0.3 89.6 ± 0.1 *88.5 ± 0.4 *88.1 ± 0.2
IID FL 94.0 ± 0.2 86.9 ± 1.1 82.1 ± 0.7 89.6 ± 0.2 89.1 ± 1.2 88.3 ± 0.3
non-IID FL 92.5 ± 0.1 *86.1 ± 0.6 81.4 ± 0.3 88.8 ± 0.1 84.5 ± 0.7 86.7 ± 0.1
shown to perform well across many languages
(Sanh et al., 2019; Devlin et al., 2019) while be-
ing smaller than the full mBERT.For MT, we use
the M2M-100 model (Fan et al., 2020) with 418M
parameters, a many-to-many MT model that can
translate between any pairing of 100 languages.
For text classification, we use the XLM-RoBERTa
base sized model (270M parameters). We note
that although there are other PLMs to consider, we
focus on testing a varied set of commonly used,
high-performing PLMs.
3.5 Training
We use the Flower framework (Beutel et al., 2020)
for federated training and evaluation due to its ease
of use and strong community support. We use Hug-
ging Face’s transformers library (Wolf et al., 2019)
for loading pretrained models and PyTorch as the
underlying differentiation framework (Paszke et al.,
2019). We train each LM model for 100 epochs
if pretrained or 200 epochs if randomly initialized.
For MT, we train for 25 epochs and for TC we trainfor 10 epochs if pretrained and 50 epochs if ran-
domly initialized. For other hyperparameters and
compute settings, see Appendix A.
4 Results
Language Modeling In Figure 2 we see the over-
all results of the language modeling task across the
two datasets. As expected, the randomly initialized
models perform much worse than the pretrained
models. The gap between between FL and cen-
tralized methods is smaller when using pretrained
models, indicating that pretrained models are an
effective initialization for federated learning.
In Table 1 we show results broken down by lan-
guage. Since the fine-tuning task is the same as the
pretraining objective (masked language modeling),
we can use the pretrained model as a baseline (top
row, B). In the randomly initialized category, the
centralized model is the same or better than the
FL methods in every single language, across both
datasets. In the pretrained section the results are
more mixed, with the centralized model winning or
tying in 5 of the 8 Europarl languages and obtaining
similar scores on the UN corpus. We also see that
the randomly initialized non-IID model appears to
diverge for some of the Europarl languages.1416Examining the difference between IID FL and
non-IID FL, we see that IID FL performs better
on average in three of the four settings. However,
when initializing with a pretrained model, the per-
formance gap narrows.
Machine Translation Table 2 exhibits results on
tuning a machine translation model on a domain
specific dataset. We see that on the MTNT dataset,
both FL algorithms actually outperform centralized
learning (24.4 avg. BLEU for IID FL vs 23.6 for
Centralized). The scores on Japanese are very sim-
ilar for all models, possibly reflecting the difficulty
of the task. On the UN corpus, we see again that
the IID FL model performs best.
Since the fine-tuning task matches the original
M2M-100 task, we can use the pretrained model di-
rectly as a baseline. In all cases, fine-tuning shows
an improvement (first row, No Training baseline).
Note that our scores are not directly comparable to
other work as we use a smaller training set.
Text Classification Table 3 shows results on text
classification. We see that when initialized ran-
domly, non-IID FL shows a large drop in perfor-
mance compared to the two other methods (i.e.
more than 5 points worse than the Centralized
method). Initializing with the pretrained model
yields a modest though consistent improvement for
all three models (80.4% accuracy vs 88.3% accu-
racy for Centralized).Furthermore, with a pre-
trained initialization the non-IID FL method scores
become significantly closer to the other two meth-
ods, with less than a two point difference between
them (86.7% non-IID FL vs 88.3% IID FL).
Discussion Our examination of multilingual FL
indicates that performance is similar when pre-
trained models are used. Despite the fact that local
models are averaged together, non-IID data parti-
tioning (where each client sees only one language)
has only a small impact on final multilingual per-
formance, when using pretrained models. These
findings suggest that, when possible, practitioners
who need multilingual federated learning should
employ pretrained models in order to gain the pri-
vacy benefits of federated learning, without taking
much (if any) of a performance loss to do so.
In several cases, we found that IID FL or non-
IID FL could even outperform centralized learn-ing. We leave investigation of this phenomena for
future work but note a couple of possible expla-
nations. First, FL with FederatedAveraging
may have similar implicit regularization effects to
checkpoint averaging, a common technique when
using transformer models (noted in Vaswani et al.
2017, Edunov et al. 2018, etc.). Furthermore, there
may be other regularization effects during feder-
ated fine-tuning, as transformer training is known
to be unstable and sensitive to optimization choices
(Mosbach et al. 2020, Nguyen and Salazar 2019).
Overall, our analysis shows that our conclusions
hold for different multilingual models, on disparate
NLP tasks, and across 13 different languages. We
acknowledge that the languages used in this study
are generally considered higher-resource, but ex-
pect that these conclusions will continue to hold
as long as the pretrained model is effective on the
target language (or language pairs, for MT).
5 Conclusion
In this work we provided the first analysis of mul-
tilingual language data on federated learning al-
gorithms. We found that fine-tuning a pretrained
model with FL methods can yield similar perfor-
mance to centralized learning, even when clients
are partitioned by language (non-IID FL). How-
ever, models trained from random initializations
still show a large gap between centralized and fed-
erated learning. Our results suggest that learning
on private partitioned data is possible without hav-
ing to incur a large performance penalty. We hope
that these results will aid practitioners in using FL
(and also downstream consumers) and inspire the
broader community to consider multilingual data
in future federated learning research for natural
language processing.
References14171418
A Hyperparameters
Each LM experiment ran for approximately a day
each on a 6 GPU cluster of RTX 6000 GPUs with
24GB of memory per GPU. The MT experiments
took approximately 12 hours each and the TC ex-
periments took around 3 hours each, all on the same
cluster.
We use the AdamW optimizer (Loshchilov and
Hutter, 2017; Kingma and Ba, 2014) for all exper-
iments (shown to be effective for FL in Lin et al.
2021). Each client goes through a full epoch of
local learning before synchronizing with the server.
For MT, we report results using the 5e-5 learning
rate, as we found in initial results (as have others
also, see Appendix B of Stickland et al. (2020) as
one example) that MT experiments are generally
consistent over learning rates when fine-tuning. For
language modeling and text classification, we use
three different learning rates (1e-4, 5e-5, 1e-5). All
models were selected using the best performing
version on the validation set, for the given model
and training setting. For both tasks, we use early
stopping (5 epochs of no improvement for MT and
TC, 10 epochs for LM).
We use the standard sacreBLEU settings:
nrefs:1, mixed case, eff:no, tok:13a, smooth:exp,
and version 2.0.0. For Ja and Zh we use their re-
spective tokenizers.
B Randomly Initialized MT
We do not report results for randomly initialized
training of MT systems, as large neural MT systems
generally need large amounts of data to be effective.
We ran experiments for the MTNT dataset from1419random initializations, running for twice as many
epochs. Resulting models appeared to converge by
loss but had extremely low BLEU scores. Thus, we
only include pretrained results in Table 2.
C MTNT Data Preprocessing for
M2M-100
M2M-100 was trained using scripts that removed
input with “excess punctuation." We follow this in
preparing MTNT training data. We use all En →
Ja data (consisting of approximately 6k instances)
and take the corresponding En →Fr instances, ran-
domly sampling additional instances until there are
the same number of instances in each direction. We
sample an equal number of training instances as we
are testing the effects of multilingual data, rather
than unequal dataset sizes. We then remove the
training instances with excess punctuation (or sen-
tences less than 3 characters) following the M2M-
100 script. This leaves 5605 instances in each di-
rection for training. We use the standard MTNT
dev and test sets, as-is, consisting of approximately
1k data points.
D Full LM Results
We show the full results of the LM experiments,
with standard deviations over five random seeds in
Tables 4 and 5.14201421