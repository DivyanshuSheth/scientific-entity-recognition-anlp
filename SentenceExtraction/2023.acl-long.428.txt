
Chenxi Whitehouse, Clara Vania, Alham Fikri Aji,
Christos Christodoulopoulos, Andrea PierleoniCity, University of LondonAmazon Alexa AI, Cambridge, UK
chenxi.whitehouse@city.ac.uk
{vaniclar, chrchrs, apierleoni}@amazon.co.uk
Abstract
Extracting structured and grounded fact triples
from raw text is a fundamental task in Informa-
tion Extraction (IE). Existing IE datasets are
typically collected from Wikipedia articles, us-
ing hyperlinks to link entities to the Wikidata
knowledge base. However, models trained only
on Wikipedia have limitations when applied to
web domains, which often contain noisy text
or text that does not have any factual informa-
tion. We present WIE, the first large-scale,
entity-linked closed IE dataset consisting of
1.6M sentences automatically collected from
the English Common Crawl corpus. WIE
also includes negative examples, i.e. sentences
without fact triples, to better reflect the data
on the web. We annotate21K triples from
WIEthrough crowdsourcing and introduce
mWIE, a translation of the annotated set
in four other languages: French, Spanish, Por-
tuguese, and Hindi. We evaluate the in-domain,
out-of-domain, and zero-shot cross-lingual per-
formance of generative IE models and find mod-
els trained on WIEshow better generalisabil-
ity. We also propose three training strategies
that use entity linking as an auxiliary task. Our
experiments show that adding Entity-Linking
objectives improves the faithfulness of our gen-
erative IE models.
1 Introduction
Information Extraction (IE) is the task of extract-
ing structured information from unstructured text,
usually in the form of triples <subject, relation,
object> . It is essential for many Natural Language
Processing applications such as knowledge base
population, question answering, faithful summari-
sation, and fake news detection (Trisedya et al.,
2019; Huguet Cabot and Navigli, 2021; Narayan
et al., 2021; Whitehouse et al., 2022).Typically, two pieces of information are needed
for training closed IEsystems: (i) the entities
mentioned in the text and (ii) the relations that ex-
ist between each pair of entities. Obtaining such
information requires expensive annotations, there-
fore most existing IE datasets, such as WikiNRE
(Trisedya et al., 2019) or REBEL (Huguet Cabot
and Navigli, 2021), are built using Wikipedia, as en-
tity information is available through hyperlinks and
relation information can be automatically extracted
via distant supervision (DS) approach (Mintz et al.,
2009) using a knowledge base (KB) such as Wiki-
data. The DS approach assumes that if two entities
are connected through a relation in a KB, then the
sentences that mention both entities together ex-
press the relation.
While models trained only on this fact-rich do-
mainhave shown to be useful for IE applications,
they have limited capacity when applied to ex-
tracting information in other web domains, which
often contains noisy text or text without any fac-
tual information. Take AllenAI’s C4 dataset, an
open-sourced version of Google’s C4 (Raffel et al.,
2020) dataset based on Common Crawl, as an ex-
ample. Our analysis using the DS approach re-
veals that less than 15% of the sentences contain
triples (§2.1), whereas we observe that a state-of-
the-art (SOTA) generative IE model, GenIE (Josi-
foski et al., 2022), which is trained on REBEL, the
largest IE dataset to date (which includes only pos-
itive examples), tends to generate triples for every
sentence, resulting in a high rate of false positives
and issues with hallucination.
To address these issues and facilitate future work
on IE on the web, we present WIE, the first
large-scale, entity-linked closed IE dataset col-
lected from web sources. The WIEdataset is7734
collected from the 200 most frequent URL domains
from the C4 dataset. First, we use ReFinED (Ay-
oola et al., 2022), a state-of-the-art Entity Linking
(EL) model to identify mention spans of the entities
and link them to Wikidata. We then apply the DS
approach to extract triples and use a Natural Lan-
guage Inference (NLI) model to filter out triples
not expressed by the sentence. We also include
negative examples, i.e., sentences without any fac-
tual information, to better reflect the data on the
web. Our final dataset consists of 1.6M sentences,
and we annotate a subset of21K triples through
crowdsourcing. The annotated set is exclusively
used as part of the test set to allow more reliable
evaluation. Finally, we introduce m WIE, which
contains human-corrected translations of the anno-
tated version of WIEin four languages: French,
Spanish, Portuguese, and Hindi.
Previous works have shown that compared to
discriminative pipelines which often suffer from
accumulative errors due to separate Entity Linking
and Relation Extraction (RE) steps (Mesquita et al.,
2019; Trisedya et al., 2019; Josifoski et al., 2022),
generative models achieve superior performance
in many closed IE tasks. Therefore we primarily
benchmark WIEwith generative, transformer-
based encoder-decoder models, BART (Lewis et al.,
2020) and mBART (Tang et al., 2021). The latter is
used to evaluate the zero-shot cross-lingual transfer
performance on mWIE.
We further propose three training strategies
(§3.2) that use entity linking as an auxiliary taskfor generative IE, namely joint generation with
the linked-entity prompt ( E -P ), multi-
task learning with distinguished artificial prompt
tokens ( A -P ), and training with
an additional task-specific language model (LM)
head ( 2LM-H ). We find that training with
EL as an auxiliary task overall leads to better and
more faithful IE results. An illustration of these
training strategies is provided in Figure 1.
Our experiments show that compared to mod-
els trained only on Wikipedia datasets, models
also trained on WIEare more robust and gen-
eralisable, achieving a new SOTA performance
on REBEL (§5) and competitive zero-shot perfor-
mance on WikiNRE. We demonstrate that W-IEserves as a complementary dataset to exist-
ing datasets based on Wikipedia, and show that
including negative examples is crucial for address-
ing false positives in generative IE.
Our main contributions are as follows: (1) We
present (m) WIE, the first large-scale, entity-
linked IE dataset on the web, where a subset is
further annotated by humans and translated into
four other languages; (2) We propose and study the
effectiveness of using entity linking as an auxiliary
task for generative IE with various training strate-
gies; (3) Our comprehensive experiments demon-
strate that models trained on WIEexhibit better
generalisability in Information Extraction on the
web domain, including competitive zero-shot per-
formance on IE tasks on Wikipedia.7735
2 (m)WIE
In this section, we provide a detailed explanation
of the dataset collection process for (m)WIE.
2.1 Collecting WIE
Data Preprocessing We start with the English
portion of the AllenAI’s C4 dataset and keep the
most frequent 200 URL domains. We randomly
sample 1M documents and use SpaCyfor sentence
segmentation. Sentences with fewer than 10 words
are removed, resulting in20M sentences.
Entity Linking and DS Dataset Next, we run
ReFinED (Ayoola et al., 2022), a state-of-the-art
EL model on the sentences to identify entity spans
and link them to their corresponding Wikidata ID.
Besides named entities, ReFinED also extracts nu-
merical entities that do not have Wikidata ID. In
this work, we only consider numerical entities that
express dates, and map them to the corresponding
year for simplicity. Some examples of ReFinED
processed output are included in Appendix B.
After obtaining the entity-linked sentences, we
apply the DS paradigm to retrieve the set of rela-
tions that exist between each pair of entities in each
sentence using Wikidata (September 2022 dump)
as our KB and build a DS dataset. After the above
steps, we obtain WIEDS dataset consisting of
21.2M entities and 4.8M triples.
Entailment Filtering One major drawback of
the DS approach is that the triples extracted
may or may not be expressed by the source sen-
tence (Riedel et al., 2010). Following previ-
ous work on obtaining a cleaner version of the
DS dataset (Huguet Cabot and Navigli, 2021;
Vania et al., 2022), we apply an NLI model,nli-deberta-v3-large, that is trained on
SNLI (Bowman et al., 2015) and MultiNLI
(Williams et al., 2018), to filter out triples that do
not entail the sentence.
Each source sentence is treated as the premise
and we use manually created templates (similar to
Vania et al. (2022)) to convert a DS triple to one
or more hypotheses . We then obtain the entailment
probability score for each premise-hypothesis pair
and take the maximum score for cases with mul-
tiple converted hypotheses. We set the threshold
to be 0.7, similar to Huguet Cabot and Navigli
(2021), and only keep triples with an entailment
score above the threshold. We retain 2.1M triples
(44% of the previous DS triples, see Table 1) after
this filtering process.
Negative Examples After the DS creation and
NLI filtering steps, only less than 10% of the origi-
nal sentences contain triples. To train models for
extracting facts from the web and alleviate false
positives, we include two kinds of negative exam-
ples in WIE: (i) sentences with one or zero en-
tities, and (ii) sentences with two or more entities,
but without any factual information (i.e., no rela-
tion between the entities). We randomly sample
negative instances covering both cases evenly and
add them to WIE. In the end, WIEconsists of
1.6M sentences, where 50% are negative examples.
A summary of the statistics of WIEwith a com-
parison with other datasets is shown in Table 1. The
dataset is randomly split into train/validation/test
sets using a 90/5/5 split.
2.2 Human Annotation
Existing IE datasets, such as REBEL, are often
automatically annotated using the DS approach,
hence the labels can be noisy. To allow more reli-
able evaluation of WIE, we randomly sample773621K triples from the most frequent 200 relations
and annotate them with MTurk. Given a sentence,
each HIT (Human Intelligence Task) is designed
to verify if a DS triple is correctly expressed in the
sentence. First, the annotators are asked to verify
if the head entity (subject) and tail entity (object)
are linked correctly. For each entity, we provide its
Wikipedia title and link to its Wikidata page as addi-
tional context. After that, the annotators are asked
to verify if the triple relation is correctly inferred
from the sentence. Here, we provide the relation
descriptions and example use cases of each relation.
We ask three MTurk workers to annotate each DS
triple and take the majority vote as the final label
for each triple. A triple is considered valid if both
entities are linked to the correct Wikidata entities
and the relation is inferredby the sentence. An
annotation interface is shown in Appendix C.
To ensure the annotation quality, we set quali-
fications with additional requirements for MTurk
workers (see Appendix C for details). The agree-
ment among the three annotators is high: 99.4%
for the head entities, 99.2% for the tail entities, and
76.1% for the relations have all three annotators
agreeing on the same label. After the majority vote,
92.1% of the triples are labelled as inferred and
therefore kept as valid triples.
2.3 Multilingual WIE
To enable zero-shot cross-lingual transfer evalua-
tion on WIE, we further extend the annotated
subset, with additional negative examples, to four
other languages: French, Spanish, Portuguese, and
Hindi. First, we use a neural machine translation
model, the distilled 1.3B variantof NLLB-200
(Costa-jussà et al., 2022), to translate the English
sentences into the target languages. We then use
MTurk to verify the translation and add entity span
information in the translated sentences. We pro-
vide the English sentence (with the entity spans
highlighted) and its translation, and first, ask the
annotators to correct the translation. After that,
MTurk workers are asked to mark the correspond-
ing entity spans in the target language. We ask
two annotators to complete the aforementioned
HIT, and an additional worker to select the bet-ter translation, which is used in our final dataset.
To obtain translations with higher quality, we re-
strict the region of the workers to countries where
the target language is the official language. The
final m WIEconsists of 9K instances in each
language, which corresponds to roughly 90% of
the 21K annotated triples.
3 Generative Information Extraction
This section describes the training strategies that
we use for benchmarking (m)WIE.
3.1 Sentence-to-Triples Generation
We use BART and mBART for all of our experi-
ments. Given a sentence sas input, we train the
model to autoregressively generate the linearised
triples tas an output. Following the practice from
Huguet Cabot and Navigli (2021) and Josifoski
et al. (2022), we linearise a triple tby convert-
ing it into “ <sub> head entity label <rel> relation
<obj> tail entity label <et> ”, where the tags in
brackets represent subject, relation, object, and
theend of triple, respectively. Head/tail entity la-
bel refers to the Wikipedia title that the mention
span in the sentence is mapped to, which also has a
one-to-one correspondence with the Wikidata ID.
For each sentence, we order its linearised triples
accordingly to the order in which they appear in the
input sentence; first by the order of the appearance
of the head entity, and then by the order of the
tail entity (for cases when the head entities are the
same). The conditional probability of generating
tis formulated as p(t|s) =/producttextp(t|t, s). We
use the standard cross-entropy loss and maximise
the output sequence likelihood with teacher forcing
(Sutskever et al., 2014). An example of input and
output can be seen in the top left of Figure 1.
3.2 Entity-Linking as an Auxiliary Task
The standard linearised triples output only contains
the label of the entity and not the span. As a result,
it may be difficult to trace back from which input
span an entity is generated, especially in the case
when the model hallucinates (e.g., by generating
an entity that is not mentioned in the sentence). To
encourage models to generate faithful and inter-
pretable output, we also experiment with models
that are jointly optimised for generating triples and7737EL. The goal of the EL task is to identify and ex-
tract entity spans from the input sentence and link
them to their corresponding KB entities. We posit
that adding the EL task as an additional training
objective will teach the model to put attention to
the input spans when generating the output. We
experiment with the following three approaches.
E -P Narayan et al. (2021, 2022)
have shown that generation with entity-chain plan-
ning, i.e. generating the desired entities first be-
fore the actual output, is effective in improving the
faithfulness and controlling hallucinations in text
generation tasks such as abstractive summarisation.
For generative IE tasks, EL can be used as an in-
termediate plan to ground the generation of the
linearised triples. We define the Entity-Linking tar-
get in the format of “ Mention Span# Entity Label
| Mention Span# Entity Label| ...”, where the
entity spans are ordered as they appear in the text.
We then prepend the EL target to the linearised
triples target, using special symbols as separators,
i.e., “ [ENTITY] Entity-Linking target [TRIPLE]
Linearised Triples Target ”, where “ [ENTITY] ” is
the start symbol before generating the EL output,
and “ [TRIPLE] ” is the start symbol before generat-
ing the linearised triples. Given an input sentence,
we essentially train the decoder to first generate the
EL chain and then generate the triples, conditioned
on both the input sentence and the EL output.
A -P Artificial Prompt tokens
are symbols placed in front of the input sequence,
which has previously been explored for neural ma-
chine translation to distinguish the language of the
target output translation (Johnson et al., 2017), and
visual question answering for joint answer and ex-
planation generation (Whitehouse et al., 2023). We
adapt this approach for jointly training our models
for EL and generative IE. Specifically, we use an
artificial prompt token <#el#> at the beginning
of the input sentence when training for the Entity-
Linking target, and use <#tri#>for linearised
output target. Training instances for both tasks are
mixed and randomly shuffled for training.
2LM-H Finally, inspired by Gontier et al.
(2022), the third approach that we experiment withis the addition of a second language model (LM)
head in the decoder, which is initialised with the
same weights as the first (standard) LM head. The
first LM head is optimised for generating the lin-
earised triples while the second LM head is opti-
mised for the EL task, thus each training instance
has two different target outputs. During training,
the input sentence is fed to the encoder once, and
different target outputs are given to the same de-
coder. Each task-specific LM head is then responsi-
ble for generating output targeted for it. The train-
ing loss is then formulated as a weighted sum of the
losses from both tasks: L=αL+ (1−α)L.
3.3 Inference with a Constraint Trie
In addition to standard beam search decoding, we
experiment with constraint decoding by restricting
the generated output to be valid Wikipedia titles
and Wikidata relations using a prefix Trie, follow-
ing the ideas proposed in GENRE (Cao et al., 2021)
and GenIE (Josifoski et al., 2022). We use two con-
straint Tries: an entity Trie and a relation Trie. The
entity Trie is built using all Wikipedia titles (as the
entity labels), and the relation Trie is built using
all Wikidata relation property labels. We refer the
readers to Cao et al. (2021) for more details on
constructing the Trie.
We use four special symbols, <sub> ,<rel> ,
<obj> and<et> to define the state of the gener-
ation. We apply both constraint Tries as follows.
We adopt the constraint Trie so that, in the very
first decoding state, the model is allowed to either
(i) return an empty string for a negative example,
or (ii) generate <sub> , which is the start symbol
for generating a triple. If the <sub> symbol is gen-
erated, then we generate the head entity using the
entity Trie, i.e., only valid entities will be consid-
ered. Once the generation of the head entity is com-
pleted, the model proceeds to generate <rel> (i.e.,
the start symbol for generating relation string) and
then subsequently generate allowed tokens from
the relation Trie which is built from the relations
in Wikidata. After that, the model generates <obj>
and the tail entity, in the same manner, using the
entity Trie. After generating the full triple (indi-
cated by <et> generated after the tail entity), the
decoder can either stop the generation or start a
new iteration for generating the next triple.
For the E -P models, since the en-
tity mention spans are text from the input sentences
and usually are not the same as the entity labels in7738
Wikidata, we propose a partial constraint genera-
tion approach. Specifically, we start the standard
beam search for the EL target output and only acti-
vate the Trie constraints after that when generating
the linearised triples.
4 Experiments
In this section, we explain the datasets used in the
experiments and the detailed modelling setup.
4.1 Dataset
In addition to our proposed WIEdataset, we
also use the following datasets for our experiments.
WikiNRE (Trisedya et al., 2019) is an IE dataset
based on Wikipedia which is automatically con-
structed by aligning Wikipedia sentences to Wiki-
data triples using the DS approach. The authors
apply a coreference resolution model (Clark and
Manning, 2016) to obtain sentences with implicit
entity names, and use a paraphrase detection model
(Ganitkevitch et al., 2013; Grycner and Weikum,
2016) to filter out sentences that do not express
the DS triples. In our experiments, we only use
WikiNRE for zero-shot evaluation.
REBEL (Huguet Cabot and Navigli, 2021) is
a large-scale IE dataset constructed automatically
from Wikipedia abstracts. Using the Wikipedia
hyperlinks in the abstracts, as well as numericalvalues and dates, they map the entity spans to their
corresponding Wikidata entities. They then use the
DS approach to identify triples in each sentence.
To filter out false positives, the authors use an NLI
model by concatenating the entities and the rela-
tion as the hypothesis. In our experiment, we use
the REBEL dataset that is sub-sampled by Josifoski
et al. (2022), where 857 relations are considered.
Both WikiNRE and REBEL do not contain nega-
tive examples and are not annotated by humans.
4.2 Models
We experiment with BART using two settings:
Bwith the pre-trained weights from Lewis
et al. (2020), and B, using the same con-
figuration and architecture but randomly initialised
weights. Across the two settings, Josifoski et al.
(2022) find that B generates better results
than Bon REBEL. For m WIE, we exper-
iment with the mBART-50model (for simplicity
we refer to it as mBART in this paper).
To compare models trained on different datasets,
we train both BandB on REBEL
(),WIE (), and both datasets together
(+). We evaluate the performance of the gener-
ated triples by parsing the linearised output to a list7739
of triples and comparing it to the gold label to cal-
culate precision, recall, and F1 scores. For WIE,
we also calculate the accuracy of the prediction
of negative instances, where a prediction is con-
sidered correct if the model accurately generates
empty strings rather than hallucinating triples.
For training with EL as an auxiliary task, we pri-
marily experiment with the B. We prepare
the training instances as described in §3.2, and train
separate models on REBEL and on WIE. For
the2LM-H , we conduct experiments with
different values of the αparameter in the combined
loss function, specifically, we set it to 0.5 and 0.75.
We use 8 GPUs, each with 32G VRAM, for all
experiments. We set the batch size to 8 and accu-
mulate gradient batches to 32. We follow the hyper-
parameters settings from Josifoski et al. (2022) and
set the learning rate to 3e, weight decay to 0.01,
and warmup steps to 5K. We train for up to 30
epochs with early stopping (patience 10), validate
twice per epoch, and take the last checkpoint for
evaluation. Training one epoch takes1.5 hours
for BART and2 hours for mBART.
5 Results and Analysis
We now present the main results of (m) WIEand
compare different training strategies.
5.1 Main Results
Table 2 shows our benchmarking results on W-IE. We report results with the constraint Trie in
decoding since it overall achieves better results.
Contrary to the findings from Josifoski et al. (2022),
we find that BART models with pre-trained weights
are better than initialised weights. Constraint Trie
decoding benefits REBEL, WikiNRE, and the re-
call performance of WIE, but may compromisethe precision since the models are also trained to
handle negative examples.
Models trained on both REBEL and WIE
(+)obtain overall better F1 scores on the
two datasets compared to models trained on each
dataset separately. Similar performance can also
be observed in the zero-shot performance on Wik-
iNRE. Models trained solely on the REBEL dataset
(Wikipedia-domain) show poor generalisability on
WIEand always generate false positives thus
resulting in 0% accuracy for negative instances in
WIE. This indicates that Wikipedia-domain data
only is not adequate for training robust models for
the web, and the absence of negative examples in
these datasets leads to a prominent issue of halluci-
nation when applied to the web.
B(+)also achieves a new state-of-
the-art F1 score of 71.87 on REBEL, surpassing
the performance of 68.93 from GenIE (Josifoski
et al., 2022) and 70.74 from KnowGL (Rossiello
et al., 2023), the latter of which trains with ad-
ditional information including entity type. The
results demonstrate the benefit of WIE, which
contributes to the generalisability of the models.
5.2 Cross-lingual Transfer with mBART
We train mBART on the training set of WIE
and evaluate the zero-shot cross-lingual transfer on
mWIE. Similar to prior experiments, results in
Table 3 show that constraint Trie decoding obtains
higher performance than standard decoding.
For English, mBART achieves higher overall
performance than B(see Table 2). The
zero-shot results reveal that Hindi has a significant
decline in performance compared to the other three
non-English languages, French, Spanish, and Por-
tuguese. Since these three languages utilise the7740Latin script as in English, which may result in an
overlap of entity surface forms. In contrast, the
transfer is more difficult for Hindi as it employs
a different writing system. Manual analysis indi-
cates that mBART tends to produce a high rate of
false negatives in Hindi examples, where the cor-
rect extraction mostly occurs when the entities in
the sentences share similar surface forms with the
English counterparts.
5.3 Results with Additional EL Training
Table 4 shows the results of training with Entity-
Linking as an auxiliary task. For REBEL, the
best results are achieved with the 2LM-H
approach, where the αparameter is set to 0.75.
ForWIEwith negative examples, all EL train-
ing models achieve better F1 performance than
B, with E -P particularly re-
sulting in better recall. This shows the benefit of
joint training with EL to improve the faithfulness of
web domain data. A -P achieves
the best precision in WIEbut does not show
significant differences in performance compared
toB. Nevertheless, all three approaches
provide better interpretability, i.e., the information
of the mention spans in the text that contributes to
the IE prediction.
E -P andA -P do
not require additional architectural adaptation over
the standard model. E -P also does
not introduce training overhead, whereas the other
two models may require twice the training time.
2LM-H offers the flexibility of adapting the
weighted combination of the main task and the aux-
iliary task by adjusting αin the joint loss formula,
which allows more emphasis on the main target.
6 Related Work
IE Datasets The term Information Extraction
has been used for different tasks in the litera-
ture. Most existing IE datasets are collected from
Wikipedia articles aligned with Wikidata, including
sentence-level IE datasets such as REBEL, Wik-
iNRE, FewRel (Han et al., 2018), T-REx (Elsahar
et al., 2018); document-level Relation Extraction
datasets, e.g., DocRED (Yao et al., 2019), CodRED
(Yao et al., 2021). SMiLER (Seganti et al., 2021)
is a multilingual sentence-level IE dataset that is
also based on Wikipedia, covering 14 languagesand 36 relations. These sentence-level IE datasets
typically do not contain negative examples.
Datasets such as TACRED (Zhang et al., 2017),
RE-TACRED (Stoica et al., 2021), and WebRED
(Ormandi et al., 2021) have negative relation ex-
amples but they are not linked to knowledge bases.
Our proposed dataset WIEis distinct from the
existing datasets in that it is on the web domain,
entity-linked, and with negative examples.
IE Approaches IE approaches can be classified
into two categories: pipeline systems with discrim-
inative models, and sequence-to-sequence systems
with generative models. Pipeline models typically
include separate modules for Named Entity Recog-
nition (NER), Entity Linking and Relation Extrac-
tion (Chaganty et al., 2017; Yamada et al., 2020).
Systems that jointly train NER, EL, and RE, have
also been explored, taking advantage of the infor-
mation shared between the tasks (Ji et al., 2020;
Eberts and Ulges, 2020).
In recent years, generative IE has gained a lot of
attention. Nayak and Ng (2020) utilise an LTSM
model and propose a pointer network-based decod-
ing. More recent approaches, e.g. as introduced
in REBEL and GenIE, train a transformer-based
encoder-decoder model with standard maximum-
likelihood objectives to convert sentences to lin-
earised output. KnowGL (Rossiello et al., 2023)
improves upon REBEL with additional entity type
information added to the linearised output. Our
work extends GenIE and experiments with three
different approaches where we incorporate explicit
EL information as an auxiliary task with adapted
constraint Trie decoding.
7 Conclusions
We present (m) WIE, the first large-scale, entity-
linked closed IE dataset on the web. A subset of the
dataset is further annotated by humans and trans-
lated into four other languages, French, Spanish,
Portuguese, and Hindi, via crowd-sourcing.
We benchmark WIEwith generative models
and compare the models trained on WIEand
REBEL (Wikipedia-domain). Our results show
that models trained on WIEhave competitive
zero-shot performance when applied to REBEL
and WikiNRE, whereas models trained only on
REBEL have 0% accuracy on the negative exam-
ples in WIE. This highlights the importance
of including negative examples for training more
robust models and reducing hallucination in genera-7741tive IE on the web. Models trained on both REBEL
andWIEachieve the best performance on both
datasets, as well as zero-shot results on WikiNRE,
showing that WIEserves as a complementary
dataset to existing Wikipedia-domain datasets.
Investigating the approaches with entity linking
as an auxiliary task, we find that adding an addi-
tional task-specific LM head achieves the overall
best performance. The E -P approach
shows the most significant improvement on W-IEwith the constraint Trie. We primarily bench-
mark transformer-based encoder-decoder models
onWIE, but future work could also explore
pipeline frameworks and larger language models
for few-shot performance.
Limitations
We identify several limitations in this work: (i)
False negatives : Our current automatic triple ex-
traction pipeline is built using the DS approach
followed by filtering using an NLI model. How-
ever, Wikidata is not complete (Tan et al., 2022).
While some triples may not be completely avail-
able in WIE, we expect models trained on this
dataset can still discover new triples that do not
exist in Wikidata. (ii) Limited relations in anno-
tation : the human annotation is only conducted
on the most frequent 200 relations. (iii) Limited
languages in m WIE: As discussed in §2.3 and
Appendix C, the languages in m WIEare lim-
ited to official languages from geographical regions
where there is a reasonable amount of MTurk work-
ers to accept the job. An alternative solution would
be to use professional translators, especially for
low-resource languages. (iv) Fixed dataset : Facts
might change in the world (and Wikidata). This
can lead to a degraded real-world performance if a
system relies exclusively on WebIE for evaluation
when the dataset is not updated accordingly.
Acknowledgements
We would like to thank Jens Lehmann for the help-
ful feedback on the paper draft, and Balkarn Hayre
for helping with the MTurk experiments. We also
thank the anonymous reviewers for their valuable
comments that improved the paper.
References77427743
A Additional Results
We show the full results in Table 5 for B
andBtrained on REBEL and WIE, us-
ing both beam search with and without constraint
Trie decoding.
We show in Table 6 the results for non-English
languages for m WIEwhen specifying the
source language and using the default (English)
for the mBART tokenizer. These results are from
beam search without constraint Trie. We can see
that specifying the source language mostly harms
the performance (except French), especially for
Portuguese. We hypothesise that due to the model
being trained solely on English as the source to-
ken, mBART may have difficulty handling other
languages.7744
B Examples of ReFinED Output
We show examples of the sentences processed
by ReFinED in Table 7. For each input
sentence, ReFinED identifies the set of en-
tities in that sentence, and outputs mention
span, Wikidata id, and Wikipedia title for
each entity. For our experiments, we use the
wikipedia_model_with_numbers model
withwikipedia entity set.
C MTurk Annotation Details
In this section, we describe the detailed settings for
annotating (m)WIEwith MTurk.
C.1 WIE
The first annotation task (HIT) is to verify the cor-
rectness of the triples automatically created from
the DS approach and filtered by the NLI model.
The guidance and the interface are shown in Fig-
ure 2 and Figure 3, respectively.
In each HIT, we provide a sentence with its enti-
ties highlighted (head entity in blue and tail entity
in green) and the URL of the web page which the
sentence is extracted from. For the first EL annota-
tion job, we provide both links to the Wikipedia andWikidata pages. Annotators are asked to choose
if the highlighted spans are linked correctly to the
KB. Next, the annotators are asked to verify if a re-
lation (highlighted in orange) can be inferred from
the sentence. We provide the description of the
relation and an example use case to facilitate the
annotation.
Each triple is annotated by three workers, and
we pay $0.2 per HIT. We hire MTurk workers with
Masters Qualification and set additional require-
ments including (i) having done 2,000 HITs and
(ii) having a job approval rate ≥99%.
C.2 mWIE
Figure 4 and Figure 5 illustrates the interface for
correcting machine-translated sentence and iden-
tifying corresponding entities in them. As it is
challenging to find qualified crowd workers for the
translation task, we set the geographical regions
for each language to the countries where the lan-
guage is one of the official languages. We find7745
that only India and countries in America have an
adequate number of MTurk workers, which highly
restricts the options for our target languages. In the
end, the countries we set for the target languages
are as follows: Portuguese: AO, BR, CV , ST, GW,
GQ, MZ; Spanish: ES, MX, CO, PE, CL; CA for
French, and IN for Hindi. It was also necessary to
remove the Masters Qualification requirement for
MTurk workers (except Hindi) to find adequate an-
notators. We then conduct pilot annotations, where
we deliberately introduce errors in the reference
machine translation to verify if the workers under
our requirement settings are able to correct them.
We provide the English sentence paired with
the original machine-translated sentence for the
actual HIT. The English sentence is highlighted
with its entity spans, and we instruct the workers
to correct the translation while ensuring that the
entities are correctly translated. After confirming
the translation, workers are then asked to highlight
the corresponding entities in the target language
(in green). For negative sentences without entity
spans, the longest noun phrases were highlighted
instead to prevent workers from simply copying the
reference translations. We pay $0.35 per HIT for
positive sentences and $0.25 for negative sentences
(since most sentences in negative examples have
only one highlighted entity/noun phrase and it isconsidered an easier task).
Two MTurk workers are asked for the translation
task, and an additional worker was asked to select
the better translation, for which $0.10 per HIT was
paid.
D Domains in WIE
The 200 URL domains included in WIEare
shown in Table 8.
E Relations in the Annotated Set
Table 9 shows the details of the 200 relations that
are covered in the human-annotated set of WIE.77467747774877497750775177527753ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Limitations
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
47754/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
2,4,5
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
2
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
2, appendix B
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Appendix B
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. we use MTurk platform
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. we use MTurk platform
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Appendix B7755