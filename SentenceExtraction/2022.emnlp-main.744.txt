
Younghoon JeongJuhyun Oh Jongwon Lee
Jaimeen AhnJihyung MoonSungjoon ParkAlice OhSchool of Computing, KAISTSoftlyAI Research, SoftlyAI , , , , ,
Abstract
Warning : this paper contains content that may
be offensive or upsetting.
Recent directions for offensive language detec-
tion are hierarchical modeling , identifying the
type and the target of offensive language, and
interpretability with offensive span annotation
and prediction.
These improvements are focused on English
and do not transfer well to other languages be-
cause of cultural and linguistic differences.
In this paper, we present the Korean Offensive
Language Dataset (KOLD) comprising 40,429
comments, which are annotated hierarchically
with the type and the target of offensive lan-
guage, accompanied by annotations of the cor-
responding text spans. We collect the comments
from NA VER news and YouTube platform and
provide the titles of the articles and videos as
the context information for the annotation pro-
cess.
We use these annotated comments as training
data for Korean BERT and RoBERTa models
and find that they are effective at offensive-
ness detection, target classification, and target
span detection while having room for improve-
ment for target group classification and offen-
sive span detection. We discover that the target
group distribution differs drastically from the
existing English datasets, and observe that pro-
viding the context information improves the
model performance in offensiveness detection
(+0.3), target classification (+1.5), and target
group classification (+13.1). We publicly re-
lease the dataset and baseline models.
1 Introduction
Online offensive language is a growing societal
problem. It propagates negative stereotypes about
the targeted social groups, causing representational
harm (Barocas et al., 2017). Among various re-
search directions for offensive language detection,Figure 1: An illustration of the annotation process of
KOLD. The title is given to provide context to the an-
notators. Along with the categorical labels, annotators
are asked to mark spans of a sentence that justifies their
decision.
Waseem et al. (2017) proposes a taxonomy to dis-
tinguish hate speech and cyberbullying based on
whether the offensive language is directed toward a
specific individual or entity, or toward a generalized
group. Zampieri et al. (2019) integrates this taxon-
omy into a hierarchical annotation process to cre-
ate the Offensive Language Identification Dataset
(OLID), which is adopted in other languages as
well (Zeinert et al., 2021; Zampieri et al., 2020).
Many problems remain in offensive language de-
tection such as failure to generalize (Gröndahl et al.,
2018; Karan and Šnajder, 2018), over-sensitivity to
commonly-attacked identities (Dixon et al., 2018;
Kennedy et al., 2020), and propagating bias in an-
notation (Sap et al., 2019; Davidson et al., 2019).
Among various attempts to solve those problems,
first is offensive language target detection to iden-10818
tify the individuals or groups who are the victims of
offensive language (Sap et al., 2020; Mathew et al.,
2021; Shvets et al., 2021). Second is offensive lan-
guage span detection to provide some explanation
for offensive language (Pavlopoulos et al., 2022;
Mathew et al., 2021). Third is to provide context
information of offensive language to improve of-
fensive language detection (Vidgen et al., 2021;
de Gibert et al., 2018; Gao and Huang, 2017).
However, these steps forward are limited to En-
glish because we lack comprehensive datasets in
other languages (Mubarak et al., 2017; Fortuna
et al., 2019; Chiril et al., 2020; Rizwan et al., 2020).
Language-specific datasets are essential for offen-
sive language which is very culture- and language-
dependent (Reichelmann et al., 2020; Albadi et al.,
2018; Ousidhoum et al., 2019). Although Korean
is a comparably high-resourced language (Joshi
et al., 2020), there are only a few publicly available
Korean offensive language corpora (Moon et al.,
2020), and they do not consider the type of the
target (e.g., group, individual) to differentiate hate
speech from cyberbullying, include context infor-
mation such as titles of articles, nor annotate text
spans for explainability.In this paper, we describe and publicly release
the Korean Offensive Language Dataset (KOLD),
40,429 comments collected from news articles and
videos.The unique characteristics of our dataset
are as follows:
•It is the first Korean dataset with a hierarchical
taxonomy of offensive language (see Figure
1). If the comment is group-targeted offensive
language, we additionally annotate among the
21 target group labels tailored to Korean cul-
ture.
•The specific spans of text that are offensive or
that reveal targeted communities are annotated
(see Table 1 for examples). KOLD is the first
publicly released dataset to provide both types
of spans for offensive language in Korean.
•The comments in our dataset are annotated
with the original context. We provide the titles
of the articles and videos during the anno-
tation process, which resembles the realistic
setting of actual usage.108192 Annotation Task Design
We use a hierarchical annotation framework based
on the multi-layer annotation schema in OLID
(Zampieri et al., 2019). Additionally, we identify
the specific target group of the offensive language.
We also annotate the spans that support the labeling
decision if the comment is offensive and/or con-
tains a target of offensiveness. Figure 1 illustrates
an overview of our annotation task, and Table 1
shows examples.
2.1 Level A: Offensive Language Detection
At level A, we determine whether the comment is
offensive (OFF) or not (NOT), and which part of
the comment makes it offensive (offensive span).
We consider a comment offensive if it contains any
form of untargeted profanity or targeted offense
such as insults and threats, which can be implicit
or explicit (Zampieri et al., 2019).
We define offensive span as a specific segment of
text that justifies why a comment is offensive, also
known as a rationale (Zaidan et al., 2007). In par-
allel with the definition of offensiveness, the span
includes not only explicit profanity but also im-
plicit offensive language (e.g., sarcasm or metaphor
(ElSherief et al., 2021)). If the offensiveness is con-
veyed across multiple sentences in the comment, all
of them are captured as the offensive span. Taking
into account the faithfulness of a rationale (DeY-
oung et al., 2020), the offensive span is the minimal
snippet of the text (i.e., sufficient) that includes all
forms of expressions that convey even the slightest
intensity of offense (i.e., comprehensive), such as
affixes and emojis.
2.2 Level B: Target Type Categorization of
Offensive Language
Level B categorizes the type of the target and high-
lights the supporting span of the target (target span).
There are four possible categories.
•Untargeted (UNT): An offensive comment
that does not contain a specific target.
•Individual (IND): An offensive comment that
is targeted at a specific individual. This in-
cludes a famous person or a named/unnamed
individual with specific reference in the text.
Comments targeted at an individual are cate-
gorized as cyberbullying (Chen et al., 2012).
•Group (GRP): An offensive comment tar-
geted at a group of people with shared pro-tected characteristics, such as gender or reli-
gion. Offensive language in this category is
generally considered as hate speech (Zhang
and Luo, 2019).
•Others (OTH): An offensive comment whose
target does not belong to the above two cate-
gories. Targeting an organization, a company,
or an event.
We define a target span as a span of characters in
the comment that indicates the target of the offen-
sive language. It is collected for all types of targeted
offensive speech, regardless of the target type (IND,
GRP, OTH). If the term used to indicate the target
is offensive, target span can overlap with the of-
fensive span (e.g., jjangkkae , which corresponds to
ching-chong in English).
2.3 Level C: Target Group Identification of
Group Targeted Offensive Language
Level C identifies the specific targets of offensive
language, which consists of two hierarchical levels:
target group attribute and target group. The target
group represents the specific social or demographic
groups that share the same identity (e.g., Women,
Muslim, Chinese), and the target group attribute is
a superclass for the target group. We allow multi-
group annotation if the target entity of the comment
belongs to more than one group. For instance, “페
미년(feminist bitch)” , a word that disparages a
feminist woman, targets two groups: Women and
Feminist . Table 10 in the Appendix contains the
full set of 21 target groups. To determine the set of
target groups, we begin with categorizing targets
in Sap et al. (2020) and add several categories to
better reflect the Korean language and culture. As
the result of analyzing the targets in 1,000 initial
samples, we add Chinese ,Korean-Chinese , and In-
dian to the Race, Ethnicity & Nationality attribute,
as they take up larger portions than the initial target
groups ( White ,Asian ). Group characteristics that
do not belong to the four target group attributes
(e.g., Disabled ,Feminist ) are grouped under Mis-
cellaneous . Note that we are aware that feminism is
a gender-related issue, but classified Feminist into
Miscellaneous because feminists embody a group
of people that share the same ideology rather than
being a subclass of gender. We show the distribu-
tion of the top two levels (A, B) and the target group
attributes (Level C) in Table 2 and the subsequent
target group categories in Table 3.108203 Collecting Annotations
3.1 Source Corpora Collection
We choose two social media platforms, NA VER
and YouTube, as our source of data, which are
two of the top three mobile apps used in Korea in
2021.In particular, we collect titles and comments
on NA VER news articles and YouTube videos dis-
tributed from March 2020 to March 2022.
Due to the scarcity of offensive comments, we
collect articles and posts by using predefined key-
words, which is a commonly used method in hate
speech dataset construction (Waseem and Hovy,
2016). Every keyword is potentially highly corre-
lated with articles or videos that may have abusive
comments. Keywords are listed in Appendix A.
To ensure we do not reveal users’ personally
identifiable information, we do not collect user ids.
We replace mentions of a username with <user>
tokens, URLs with <url> tokens, and emails with
<email> tokens to conceal private information.
3.2 Annotation Procedure
The steps we took for high-quality of annotations
include providing a detailed guideline, selecting
the annotators deliberately, and managing the an-
notation process carefully. In the guideline, we re-
solve predictable difficulties during the process.
For example, we provide rules for delimiting mor-
phological boundaries specific to Korean to collect
consistent text spans, and provide guidance of im-
plicit hate speech based on the taxonomy proposed
in ElSherief et al. (2021). To ensure overall anno-
tation quality, we only allow annotators who pass
a qualification test to participate in the main an-
notation process. We make it clear that annotators
refer to the title of the article or video as context
to reduce ambiguity. As hate speech annotation
can also be influenced by the bias of the annotators
(Davidson et al., 2019; Sap et al., 2022; Al Kuwatly
et al., 2020), we include annotators of diverse de-
mographic backgrounds by limiting the maximum
amount of annotation per worker to 1% of the data.
A total of 3,124 annotators participate in creating
the final dataset. To ensure each label is genuine,
we embed questions with clear known answers and
remove users who answer those wrong. We use
SelectStar, a crowdsourcing platform in Korea, to
collect annotations. The full task is shown in the
appendix (Figure 2).
To decide on the gold label, we apply majority
voting among the three annotations for the cate-
gorical labels, and take character offsets that more
than two out of three annotators highlighted for
text spans. When the gold label cannot be deter-
mined by majority voting as there are more than
two choices, inspectorsresolve the disagreement
to determine the gold label.
3.3 Annotation Result
Overall, the average Krippendorff’s αfor inter-
annotator agreement of each annotation level is
0.55. The label distribution of the collected data is
shown in Table 2.
Level A: Offensive Language Detection The
dataset contains 40,429 comments, of which 20,130
comments are classified as offensive language. Of-
fensive comments targeted at group characteristics
(also known as hate speech) comprise 30.7% of the
whole data. Specifically, Krippendorff’s αfor the
inter-annotator agreement is 0.55 for classifying
offensiveness.
Level B: Target Type of Categorization of Of-
fensive Language Among 20,130 offensive com-
ments, 2,596 comments are classified as untargeted
offense (UNT). These comments include comments
with non-targeted profanity and swearing. Group
(GRP) is the most common type of target, taking
up 70.1% of the three types of targeted offenses,
followed by individual (IND) (22.0%) and others10821
(OTH) (7.9%). Krippendorff’s αfor agreement on
deciding the type of target is 0.45 .
Level C: Target Group Identification of Group
Targeted Offensive Language Among 12,413
group-targeted offensive comments, the most com-
mon attribute is Gender & Sexual orientation tak-
ing up 7.4 of the whole dataset. Political Affiliation
andReligion both appear in less than 5% of the data.
Table 3 shows a breakdown of the target group at-
tributes within the top three most frequent as well
as the group Others , which is tagged at the target
group attribute level but does not belong to the tar-
get group choices we provide. In Race, Ethnicity &
Nationality ,Others take up the largest portion with
1,605 comments, since it includes small but various
origins ranging from Afghans and Americans to
North Koreans and North Korean defectors. The
three most frequently targeted group characteris-
tics in the whole dataset are Feminist ,LGBTQ+ ,
andWomen , which amount to 11.42%, 10.55%, and
8.7% of the group-targeted offensive language, re-
spectively. Krippendorff’s αis 0.65 for specifying
the target group of the offensive language.4 Dataset Analysis
4.1 Target Group Distribution
Our novel finding is that target groups are defined
based on the specific language and culture to em-
brace ongoing social phenomena and reflect them
to the dataset. Shown in Table 4, the distribution of
the target groups in KOLD largely differs from
the English HateXplain dataset (Mathew et al.,
2021). We observe that groups such as Jewish,
Arabs andHispanic which commonly appear in En-
glish datasets (e.g., Sap et al. (2020); Ousidhoum
et al. (2019)), do not frequently appear in KOLD.
Africans , the target group that appears most fre-
quently in HateXplain, is not included in the top
ten ranked groups in our dataset, and the reverse
is true for Feminist , the first-ranked target group
in our dataset. While Women, LGBTQ+ (which
includes Homosexual) and Men are common tar-
get groups in both datasets, other identity groups
such as Chinese, Korean-Chinese, Progressive and
Conservative only appear in our dataset.
Specifically, we observe that within the Ko-
rean language, the Asian race as a target of of-
fensive language should be more finely partitioned.
While Asians appear as a frequent target in English
datasets without race or ethnic division (An et al.,
2021; Ousidhoum et al., 2019; Hartvigsen et al.,
2022), in KOLD, it is further separated into fine-
grained targets grouped by nationality or ethnicity
(e.g., Chinese ,Indian ,Southeast Asian ). Moreover,
our dataset demonstrates that a single Asian race
should be divided into separate target groups. A
large portion of Chinese andKorean-Chinese tar-
geted offensive comments in KOLD highlights the
uniqueness of offensive language in Korean and
reflects the cultural differences between Korean
speakers and English speakers. This demonstrates
the prevalence of social bias among Asians even
though they share similar cultural values and phe-
notype (Lee et al., 2017).
4.2 The Role of Title for Target Group
Identification
In KOLD, 55.1% of group-identified offensive com-
ments have no target span marked in the comment,
which implies that in the majority of the cases,
titles contain information about the targets. For10822
example, given the title of the article “‘Islam’ in
Korea / Yonhap News” , it is easy to find a comment
without explicitly mentioning the target such as “I
don’t care what (they) believe, what matters is the
fact that (they) kill people” (penultimate row of
Table 1).
5 Experiments and Results
We experiment using three different model archi-
tectures: (1) sequence classification model for pre-
dicting offensiveness, target type, and target group
categories, (2) token classification model for pre-
dicting the offensive and target span, and (3) multi-
task model for predicting both category and span
at once. We report the score of single-task models
using various sizes of Korean BERT and RoBERTa
(Park et al., 2021), and compare the result against
the multi-task model.
We further conduct an ablation study by exclud-
ing the title from the input to discover how much it
impacts the prediction performance of the model.
We also compare the results of our model with
translated versions of English data, and a multilin-
gual span prediction model.
For the experiments, we use an 80-10-10 split for
each task, and report the best performances based
on the Fscore of the test set result with the tuned
hyperparameters. Training details are reported in
the Appendix B.
5.1 Category Prediction
For each level of annotation, we fine-tune the pre-
trained models to predict the label given the ti-
tle and comment, and then evaluate the model us-
ing precision, recall, and Fscores of the positive
class.
In Table 5, we observe that the more the cat-
egories are fine-grained, the task becomes more
difficult, and the larger model shows better perfor-
mance.
5.2 Span Prediction
We convert each span in the comment to BIO-tags
to formulate the span prediction task as a token clas-
sification task and fine-tune the pre-trained models
to predict BIO-tags assigned to each token. To eval-
uate the model, we follow the work of Da San Mar-
tino et al. (2019) by computing the Fscore of the
predicted character offsets with the ground truth. If
the ground truth is empty, a perfect score ( F= 1)
is assigned whereas if the predicted set of offsets is
empty, a score of zero ( F= 0) is assigned.
As demonstrated in Table 6, the best character-
level F1 score is 45.4 for offensive span and 62.5
for target span. The pattern of higher score with a10823
larger model is consistent with the results of the
category prediction.
5.3 Category and Span Prediction
We employ a multi-task learning approach to train
a model capable of classifying the category and
predicting the span at the same time. In the multi-
task model, the sequence classifier and the token
classifier share the neural representation of the pre-
trained model and only differ in the output layers
for each task. The representation of the first token
([CLS]) is fed into an output layer for sequence
classification, and the other representations are fed
into the layer for token classification. The model
jointly learns the global information of a given in-
put sequence and span information. We train two
types of multi-task models. First, we train a multi-
task model with the binary label of offensiveness
and the corresponding offensive span. Second, us-
ing the data labeled as offensive, we train a multi-
task model to predict the target type (Level B) and
the corresponding target span.
As shown in Table 7, multi-task models outper-
form single-task models in span prediction by 10%
in the F1 score, mainly due to joint learning of both
types of information. However, the performance of
sequence classification drops in both models.
Examples of model predictions and ground
truths are illustrated in Table 11 and Table 12 in
the Appendix.
5.4 Title Ablation on Classification Tasks
To find out how much the context information (ti-
tles of the articles and the videos) contributes to the
offensiveness and target classifications, we conduct
an ablation study by excluding the titles from the
input.
As can be seen in Table 8, if only the comments
are given, accuracy and the f1 scores drop signif-
icantly compared to the setting where titles and
comments are given together. This phenomenon
becomes more significant as the granularity of the
label increases. When predicting the fine-grained
target group, the f1 score dropped by more than
30%. We conclude that providing the context with
the comments helps the model predict the target
groups more precisely, as the comments may not
contain sufficient information.
5.5 Translated Data and Multilingual Models
To see how much translation and multilingual
model are effective at distinguishing offensiveness,
we compare our baselines against (1) sequence clas-
sification model trained on translated dataset, and
(2) multilingual offensive span detection model.
For the translation experiment, we translate the
OLID to Korean via google translate apiand use
the dataset for training the same sequence clas-
sifier described in Section 5.1. For the multilin-10824gual experiment, we adopt the multilingual token
classification model (MUDES) (Ranasinghe and
Zampieri, 2021) trained on English toxic span
dataset (Pavlopoulos et al., 2021). For all tasks,
evaluation is done with the KOLD test set. We re-
port the results in Table 9.
Overall, both translation and multilingual ap-
proaches are not more effective than our baselines.
For the offensive category prediction, our model is
13.7 higher, and for the span prediction, our model
is 27.8 higher. Although MUDES scores high on
English (61.6), the performance drops significantly
in Korean (12.8).
6 Related Work
6.1 Offensiveness & Hate Speech Detection
Most datasets created for the detection of offensive
language have dealt with the subtypes of offensive
language such as hate speech, cyberbullying, and
profanity as a flat multi-level classification task
(Waseem and Hovy, 2016; Davidson et al., 2017;
Wiegand et al., 2018; Mollas et al., 2022). Waseem
et al. (2017) and Zampieri et al. (2019) have pro-
posed a hierarchical taxonomy of offensive speech,
emphasizing the need for annotating specific di-
mensions of offensive language, such as the con-
tent’s explicitness and the type of targets. Rosenthal
et al. (2021) further expands the size of the dataset
using the OLID dataset proposed by Zampieri et al.
(2019) with semi-supervising method. The hierar-
chical annotation has also made possible systematic
expansion to subtypes of hate speech in the follow-
ing works, such as misogyny (Zeinert et al., 2021).
Our work also builds upon the taxonomy proposed
by Zampieri et al. (2019), further identifying the
targeted social group of offensive languages.
Recent papers focus on more diverse aspects,
such as interpretability and context information.
To train a human-interpretable classification mod-
els, Sap et al. (2020) collect social bias implicated
about the targeted group in a free-text format. In a
similar spirit, Pavlopoulos et al. (2022) and Mathew
et al. (2021) create datasets annotated with particu-
lar span of the text that makes the post toxic (Zaidan
et al., 2007). As most text in the real world appears
in context (Seaver, 2015), considering context is
important for the development of practical mod-
els. Recent work on offensive language detection
incorporates the context of the post (Vidgen et al.,
2021; de Gibert et al., 2018; Gao and Huang, 2017),
albeit the benefits of the context are controversial(Pavlopoulos et al., 2020; Xenos et al., 2021). Us-
ing hierarchical annotation, KOLD dataset system-
atically classifies multiple depths of contextualized
offensiveness, and collects textual spans to justify
such classification at the same time.
6.2 Non-English Datasets
There is relatively little work done on developing
offensive language datasets in languages other than
English. Simple translation of English datasets is
not enough as there are some well-known issues
in using automatically translated English datasets
in NLP, such as translationese (Koppel and Or-
dan, 2011) and over-representation of source lan-
guage’s culture (Hu et al., 2020). Several papers
have emphasized the need of high-quality monolin-
gual data (Hu et al., 2021; Park et al., 2021). This is
also true in offensive language datasets. The focus
of hatred differs by culture and country (Reichel-
mann et al., 2020). Ousidhoum et al. (2019) ob-
serve that there are significant differences in terms
of target attributes and target groups in the three
languages (English, French, Arabic) of which they
constructed hate speech datasets. Moreover, Nozza
(2021) shows that zero-shot, cross-lingual trans-
fer learning of English hate speech has limitations.
Some datasets for detection of toxicity or abuse
exist in other languages (e.g., Zeinert et al. (2021)
for Danish, Fortuna et al. (2019) for Portuguese,
Mubarak et al. (2021) for Arabic, and Çöltekin
(2020) for Turkish). For Korean, Moon et al. (2020)
have paved the way for hate speech detection, but
they are relatively small in size and lack focus on
the target of offensiveness. In comparison, KOLD
is built upon an extensive taxonomy that can handle
a broad range of offensive language with clearly
annotated target groups of 21 categories and textual
spans.
7 Conclusion
We present KOLD, a dataset of 40,429 comments
of news articles and video clips, annotated within
context. It is the first to introduce a hierarchical tax-
onomy of offensive language in Korean with textual
spans of the offensiveness and the targets. We es-
tablish baseline performance for multi-task model
that both detects the categories and the spans that
support the classification. Through analysis and ex-
periments, we show that target terms are often omit-
ted in offensive comments, and title information
helps models predict the target of the offense. This10825finding can be applied to other syntactically null-
subject languages other than Korean (e.g., Arabic,
Chinese, Modern Greek) as well. By comparing the
distribution of target groups with existing English
data and showing the inadequacy of multilingual
models, we demonstrate that offensive language
corpus customized for the language and its cor-
responding culture is necessary. We acknowledge
that our dataset does not cover all communities
of Korean social media whose offensive language
patterns may differ from each other. Despite this
limitation, KOLD will serve as a stepping stone to
developing more accurate and adaptive offensive
language detection models in Korean.
8 Ethical Considerations
This study has been approved by the KAIST In-
stitutional Review Board (#KH2021-177). During
the annotation process, we informed the annota-
tors that the content might be offensive or upset-
ting and limited the amount that each worker could
provide. Annotators were also paid above the mini-
mum wage. We are aware of the risk of releasing a
dataset containing offensive language. This dataset
must not be used as training data to automatically
generate and publish offensive language online, but
by publicly releasing it, we cannot prevent all ma-
licious use. We will explicitly state that we do not
condone any malicious use. We urge researchers
and practitioners to use it in beneficial ways (e.g.,
to filter out hate speech). Another consideration is
that the names of political figures and popular en-
tertainers mentioned in the comments remain in our
dataset. This is because offensive language detec-
tion becomes difficult without those mentions. This
is consistent with the common practice in other
offensive language datasets, and as a community,
we need to deliberate and discuss the potential im-
plications.
9 Limitations
We discuss two limitations of our work in this sec-
tion. First, our annotation method requires a high
annotation cost and a lot of time. Guiding the an-
notators to familiarize them with our annotation
process takes much time since the guideline is com-
plicated, and they are updated whenever ambiguous
comments are reported during the process, to give
annotators clear direction. Furthermore, as we col-
lect three annotations per comment, we need a large
number of annotators (3,124) and spent a signifi-cant amount of annotation cost to pay them above
the minimum wage.
In most cases, there is a trade-off between quan-
tity and quality. For example, if one plans to build
a large-scale dataset with limited amount of re-
sources, he/she should sacrifice the complexity of
the annotations by reducing the amount of work
for each annotator, or the accuracy of the labels by
decreasing the number of annotators for each sam-
ple. This is the reason why it is challenging to build
a large dataset with accurate and rich annotations.
Recently, there has been an approach to make a
large-scale machine-generated hate speech detec-
tion dataset (Hartvigsen et al., 2022). This might be
an alternative to overcome such limitations. By col-
laborating with such models, we can obtain large-
scale datasets with accurate labels while reducing
annotation costs and time.
Second, detecting patterns of offensive language
changing over time requires constant update. For
example, hateful comments related to COVID-19
emerged recently, and offensive language toward
political figures or celebrities also changes con-
stantly. It is difficult to train a model that captures
such changes well with a dataset within a limited
time period. A model trained on our dataset might
not perform so well in detecting hateful comments
that emerge in the future. To overcome this limi-
tation, a continuous update of datasets as well as
methods to efficiently update models (Qian et al.,
2021), is needed.
Acknowledgments
This project was funded by the KAIST-NA VER
Hypercreative AI Center. Alice Oh is funded by In-
stitute of Information & communications Technol-
ogy Planning & Evaluation (IITP) grant funded by
the Korea government (MSIT) (No. 2022-0-00184,
Development and Study of AI Technologies to In-
expensively Conform to Evolving Policy on Ethics).
SelectStar provided a crowdsourcing platform for
the annotation of the data.
References1082610827108281082910830Appendix
A Data Collection Keywords
•Gender :백래시(backlash), 여성단체(fe-
male organization), 여성혐오(misogyny),젠
더(gender), 페미니즘(feminism)
•Sexual orientation : 동성혼(homosexual mar-
riage),성소수자(sexual minority), 차별금
지법(anti-discrimination legislation), 퀴어
(queer),퀴어활동가(queer activist)
•Race & Ethnicity & Nationality : 난민
(refugee),동남아(Southeast Asia), 백인
(White),외국인근로자(foreign worker),
이민자(immigrant),인도(India),조선족
(Korean-Chinese), 중국동포(ethnic Korean
from China), 탈북민(North Korean defec-
tors),흑인(Black)
•Religion :이슬람(Islam),탈레반(Taliban),
기독교(Christianity), 카톨릭(Catholic),교
회(Church), 목사(minister)
B Training Details
All experiments are conducted using the Transform-
ers library via HuggingFace. For the experiments,
we searched for a learning rate out of {1e-5, 2e-5,
3e-5, 5e-5} and the number of epochs out of {1, 2,
3, 4, 5}. We kept the batch size fixed at 32 for train-
ing, 64 for validation. We report a mean score of 5
runs. The experiments are conducted on GeForce
RTX 2080 Ti 10GB, with 10.2 CUDA version. The
single experiment takes from an hour to 4 hours.
The number of parameters for BERT base model
is 110 million, 123 million for RoBERTa base and
354 million for RoBERTa large.108311083210833