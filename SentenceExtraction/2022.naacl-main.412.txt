
Youngjin JinEugene JangYongjae LeeSeungwon ShinJin-Woo ChungKAIST, Daejeon, South KoreaS2W Inc., Seongnam, South Korea{ijinjin,claude}@kaist.ac.kr{genesith,lee,jwchung}@s2w.inc
Abstract
The hidden nature and the limited accessibil-
ity of the Dark Web, combined with the lack
of public datasets in this domain, make it dif-
ﬁcult to study its inherent characteristics such
as linguistic properties. Previous works on text
classiﬁcation of Dark Web domain have sug-
gested that the use of deep neural models may
be ineffective, potentially due to the linguis-
tic differences between the Dark and Surface
Webs. However, not much work has been done
to uncover the linguistic characteristics of the
Dark Web. This paper introduces CoDA, a
publicly available Dark Web dataset consist-
ing of 10000 web documents tailored towards
text-based Dark Web analysis. By leveraging
CoDA, we conduct a thorough linguistic anal-
ysis of the Dark Web and examine the textual
differences between the Dark Web and the Sur-
face Web. We also assess the performance of
various methods of Dark Web page classiﬁca-
tion. Finally, we compare CoDA with an exist-
ing public Dark Web dataset and evaluate their
suitability for various use cases.
1 Introduction
The World Wide Web contains a vast, non-indexed
part of the Internet (known as the Deep Web) which
is hidden from traditional web search engines. The
Dark Web, which refers to the small portion of
the non-indexed pages that require speciﬁc routing
protocols such as Torfor access, has become a safe
haven for users wanting to conceal their identity
and preserve their anonymity.
A consequence of the properties of the Dark
Web (limited methods of access and the volatility
of its onion services) is that it is difﬁcult to grasp
the general topology and the overall content of the
Dark Web. A number of past academic studies have
tried to unravel the Dark Web through methods
such as page classiﬁcation (Al Nabki et al., 2017;Ghosh et al., 2017; He et al., 2019; Choshen et al.,
2019) and content analysis (Biryukov et al., 2014;
Avarikioti et al., 2018). However, not much work
has been done on the linguistic analysis of the Dark
Web (Choshen et al., 2019).
In addition, the Dark Web has been studied and
analyzed in the security research community to
uncover malicious activities including phishing
(Yoon et al., 2019), illicit online marketplace ac-
tivity (Soska and Christin, 2015), terrorism (Chen,
2011), cryptocurrency abuse (Lee et al., 2019), and
ransomware ecosystems (Meland et al., 2020). We
believe that the lack of a comprehensive work on
the language of the Dark Web from the NLP com-
munity mainly stems from the lack of Dark Web
datasets publicly available for research. Therefore,
a new dataset on the Dark Web may prove to be
very useful not only for the NLP community, but
also for other research communities devoted to cy-
bersecurity and cybercrime investigation through
methods such as page classiﬁcation and malicious
activity detection.
To the best our knowledge, the only currently
publicly available Dark Web dataset is DUTA
(Al Nabki et al., 2017), which has been extended
to hold over ten-thousand unique onion addresses
as DUTA-10K (Al-Nabki et al., 2019). DUTAhas
become a baseline dataset for many Dark Web re-
lated works such as Choshen et al. (2019), which
investigates the characteristics of language used in
various illegal and legal onion services.
Nevertheless, DUTA has its shortcomings. For
example, the category distribution of DUTA is
highly skewed, with some categories such as hu-
man trafﬁcking accounting for only 3 out of 10367
total onion services. In addition, the us e of DUTA
as a means of language analysis may not be ideal
as it contains many duplicate data, with only 51%
of the texts being unique (Al-Nabki et al., 2019).5621To provide a better understanding of the Dark
Web (and thus motivate more research on the
Dark Web), we introduce CoDA(Comprehensive
Darkweb Annotations), a text corpus of 10,000 web
documents from the Dark Web (primarily in En-
glish) which have been manually classiﬁed accord-
ing to their topic into ten categories. To ensure that
the quality of the classiﬁcation is not overlooked,
we develop detailed annotation / tagging guide-
lines (Section 3) to guide our annotators. Using
CoDA, we conduct a thorough text-based data anal-
ysis (Section 4) to uncover some of the linguistic
properties of the Dark Web, and gain insight into
differences in how language is used in the Surface
Web and the Dark Web. We build several text classi-
ﬁer models and train them using CoDA, and verify
which classiﬁcation methods perform particularly
well with the Dark Web (Section 5). Finally, to
evaluate the use of CoDA compared to DUTA, we
introduce use cases and compare the performances
of classiﬁers trained on each dataset (Section 6).
2 Related Work
The Dark Web is commonly crawled using Tor,
which relies on onion routing to enable encrypted
communications over a computer network (McCoy
et al., 2008). Several works use Dark Web search
engines such as Ahmiaand web directories such as
The Hidden Wiki to recursively search for content
on the Dark Web (Guitton, 2013; Al Nabki et al.,
2017; He et al., 2019). This method of crawling
works surprisingly well as the visible part of the
Dark Web is suggested to be well-connected via
hyperlinks (Sanchez-Rola et al., 2017; Avarikioti
et al., 2018).
To facilitate the research on Dark Web content
analysis, a text-based, manually labeled dataset
collected from the active domains in the Tor net-
work called DUTA was made publicly available
by Al Nabki et al. (2017). In a subsequent work,
the original DUTA dataset was extended to 10367
unique domains with minor changes to the labeling
procedure (Al-Nabki et al., 2019). To the best of
our knowledge, DUTA is the ﬁrst and only publicly
available Dark Web text dataset.
Past works have analyzed the Dark Web through
topical classiﬁcation of texts in onion services.
Many have approached text-based page classiﬁca-tion with machine learning methods such as SVM
(Support Vector Machine), NB (Naïve Bayes), and
LR (Logistic Regression) (Moore and Rid, 2016;
Al Nabki et al., 2017; Ghosh et al., 2017; Avarikioti
et al., 2018; He et al., 2019) using various infor-
mation retrieval weighting schemes like TF-IDF
(Term Frequency-Inverse Document Frequency)
and BOW (Bag-of-Words) (Al Nabki et al., 2017;
Ghosh et al., 2017; Choshen et al., 2019; He et al.,
2019).
Choshen et al. (2019) have suggested that deep
neural models may fare poorly with Dark Web clas-
siﬁcation as language in the Dark Web is lexically
and syntactically different compared to that of the
Surface Web. Their work demonstrated that rep-
resentation methods such as GloVe (Pennington
et al., 2014) and contextualized pre-trained lan-
guage representations such as ELMo (Peters et al.,
2018) resulted in a subpar performance compared
to traditional machine learning methods, suggest-
ing that the small size of training data and the spe-
cialized vocabulary in the Dark Web domain may
not be suitable with such methods. Nevertheless,
transformer-based pre-trained language models like
BERT (Devlin et al., 2019) showed promising re-
sults in text classiﬁcation tasks, although it is not
often the case that such models adapt with ease in
the Dark Web domain (Ranaldi et al., 2022).
3 The CoDA Corpus
In this section, we introduce our categorization
approach and the methods used to construct our
Dark Web dataset, CoDA.
3.1 CoDA Category Set
CoDA is comprised of ten categories (described
in detail in Section 3.5) as shown in Table 1. We
arrive at our ten categories through an extended
discussion with the dataset annotators (see Section
3.4) on a suitable method to categorize the var-
ious activities in the Dark Web and refer to the
high-level taxonomy from Moore and Rid (2016).
Unlike DUTA, we do not subdivide each category
intolegal andillegal (ornormal andsuspicious as
labeled in DUTA) activities because in many cases
it is difﬁcult to clearly distinguish between cate-
gories using only the surface information available
from websites. For example, while human anno-
tators easily agreed that most ‘counterfeit money’
services are highly likely to be illegal, they found it
difﬁcult to determine the legality of ‘bitcoin wallet’5622
services. This also applies to the drugs category,
as the sale of certain drugs may be illegal in some
countries, but can be legal in others.
Moreover, unlike DUTA, we exclude non-topical
categories such as forum andmarketplace as they
are orthogonal to topical categories; e.g., hacking
forums, which frequently appear in the Dark Web,
can be categorized as both forum andhacking . We
argue that such categories are more relevant to the
structure of webpages rather than topics, and thus
need to be annotated independently of topical cat-
egories. We leave this for future work, and do not
further split our ten categories into sub-categories.
Nonetheless, our category set still covers a wide
range of activities on the Dark Web.
Finally, we point out that about 30% of data is
categorized into others . During data collection, we
observed that many webpage documents contain
various pages not related to the categorized activi-
ties (such as blogs, news sites, search engines, wiki
pages, etc.). Since the content of such pages is not
necessarily attributed to a speciﬁc activity, we cate-
gorize them into others .
3.2 Data Collection
We collected onion addresses from Ahmia and
repositories of onion domain lists. Starting from
these seed addresses, we crawled the Dark Web
and extracted unseen onion addresses from crawled
webpage documents to gradually expand our onion
address list.
The Dark Web contains large amounts of nearly
identical websites since no expense is required for
maintenance due to free hosting services such as
“Freedom Hosting” (now defunct) (Al-Nabki et al.,
2019). These cloned website farms serve to pro-
vide stable services for illegal activities (Al-Nabki
et al., 2019), or to attract users and deceive them
into disclosing sensitive information (Yoon et al.,
2019). In order to construct a quality corpus, we
analyze the content of each document and refrain
from collecting redundant webpages (i.e., keeping
only one copy of such pages), using the document
similarity measure described in Section 4.2.3.
3.3 Data Size and Language Distribution
Using the crawled HTML webpage documents, we
compiled a Dark Web corpus consisting of exactly
10K web documents from a total of 7101 onion ser-
vices. The user statistics for Tor shows that clients
connect to the Dark Web from various countries,
which is reﬂected in the language distribution of
CoDA (as seen in Table 2). We observe that about
88% of documents in CoDA is in English. This is5623in line with the language distribution of DUTA in
which 84% of the samples are in English (Al-Nabki
et al., 2019). We argue that the dataset reﬂects the
various biases of the Dark Web, which should be
taken into account for future research.
3.4 Annotation
We recruited 10 annotators from a cyber threat an-
alytics company specializing in the Dark Web for
manual page-level annotation, i.e., assigning a sin-
gle category to each webpage document from one
of the ten categories achieving an inter-annotator
agreement Fleiss’ Kappa of 0.88. This is in con-
trast to DUTA, which concatenates multiple pages
from a single onion domain into one document to
assign a category (Al Nabki et al., 2017). Since
onion services such as wikis and forums usually
contain discussions on a wide range of topics across
different pages, page-level annotation was deemed
to be the most suitable choice for our category set.
We leveraged Prodigyfor an efﬁcient annotation
process.
3.5 Annotation Guidelines
A set of comprehensive annotation guidelines was
constructed for the annotators to consult when la-
beling each document to ensure the quality of la-
bels. While the annotation guidelines are extensive
with illustrative examples and methods to deal with
borderline cases, we present a brief summary of our
guidelines in Table 1. Each category is determined
based solely on the topic of page content, and not
by its type (marketplaces, services, forums, news,
blogs, wikis, search results, etc.).
Note that webpages sometimes cover more than
one speciﬁc topic on a single page (such as a mar-
ketplace selling drugs and weapons at the same
time). We exclude such multi-topic pages from our
corpus and leave multi-label datasets and classi-
ﬁcation for future work. Finally, we also exclude
webpages that contain malicious information on
personally identiﬁable individuals.
3.6 Additional Processing & Text Masking
As the Dark Web contains webpages in various lan-
guages, we label each of the documents in CoDA
with the language of its content using fastText
(Joulin et al., 2016a; Joulin et al., 2016b). To gener-
alize unnecessary details and anonymize sensitive
information in the Dark Web, we process each doc-
ument by masking appropriate information with
mask identiﬁers. A total of 18 types of mask iden-
tiﬁers are used to mask each document, as shown
in Table 3. We utilize simple keywords, regular
expressions, and a cryptocurrency address detec-
tion libraryto detect such phrases for masking.
This prevents personal information such as email
addresses from being included in our public dataset.
Finally, to ﬁlter out noisy data and optimize text
for linguistic analysis, we remove punctuations
and words that are over 50 letters long, lemmatize
words, and convert all text to lowercase.
4 Data Analysis
To assess the linguistic properties of the Dark Web,
illustrate the characteristics of textual content in
each category, and better understand the differences
in the use of language in the Dark / Surface Web,
we conduct an in-depth exploratory data analysis.
We analyze the text data of CoDA and compare
measurements with that of the other datasets (see
Section 4.1) as shown in Table 4.5624
4.1 Datasets for Comparison
We compare CoDA with DUTA to look for any
signiﬁcant differences between the two Dark Web
corpora, and use documents labeled as English for
our analysis. We also aggregate three existing text
datasets in English with Surface Web content (we
consider each of these datasets as a single sub-
category within the aggregate Surface Web data)
from here on to compare between the Dark / Sur-
face Web domains. These categories are chosen to
encompass the various topics and language styles
(formal / informal) used throughout the Surface
Web.
The aggregate Surface Web dataset consists of
the following categories: the IMDb Large Movie
Review Dataset (Maas et al., 2011), the Wikitext-2
Dataset (Merity et al., 2016), and the Reddit Corpus
(Chang et al., 2020), to represent review texts, wiki
articles, and online forum discussions, respectively.
To match the size of the dataset with its Dark Web
counterparts, the aggregate Surface Web dataset is
trimmed by randomly sampling a portion of doc-
uments from each category. The total raw word
count of each dataset is shown in Table 5.
It is worth noting that we use raw text data in-
stead of the masked data for some analyses to re-
duce bias; for example, the Surface Web aggregate
dataset is not masked, so we use the non-masked
versions of the Dark Web datasets for some com-
parisons.
4.2 Analysis Methods & Results
4.2.1 In-vocab / Out-of-vocab Word Analysis
It is known that Dark Web users intentionally
use obscure slangs and words to refer to speciﬁc
items (Harviainen et al., 2020; Zhang and Zou,
2020). To verify if this behavior affects the types
of words seen in the Dark Web, we analyze the in-
vocabulary andout-of-vocabulary words by build-
ing a list of unique words in each dataset and deter-
mining the presence of each word in a spellchecker
library. Words not listed in the library’s dictio-
nary are deﬁned to be out-of-vocabulary . Note out-
of-vocab words do not necessarily correspond to
incorrect or nonexistent words; for example, there
are many abbreviations that are out-of-vocab but
are widely used online.
The results are shown in Table 5. Due to the lim-
ited number of in-vocab words in the dictionary,
it follows that in-vocab word ratio decreases with
higher total word counts. Therefore, not much can
be said about the lexical characteristics of the Dark
Web from the ratios. To see which out-of-vocab
words frequently appear in each corpus, we rank
them by their frequency. In the Surface Web, com-
mon abbreviations, well-known companies, and
celebrity names rank high in the list, while ex-
plicit slangs, malicious activity-speciﬁc abbrevi-
ations, and misspellings (cann abalism, pedo filia,
shcool) manifest the Dark Web. A sample compar-
ison of abbreviations found in each web domain
is shown in Table 6. The Surface Web mainly ex-
hibits commonly used abbreviations such as mea-
surement units, household products, and colloquial
Internet language, while the Dark Web exhibits ab-
breviations related to ﬁnancial services, drugs, and
pornography.
4.2.2 Word Frequency Distribution
It is well known that large text corpora tend to
follow Zipf’s law (Piantadosi, 2014), which states
that the word frequency distribution is proportional
to a power law of the form:
f(r)∝r5625
forα≈1, whereris the frequency rank of the
word (most frequent word has r= 1 and so on)
andf(r)is the frequency of a word with rank r. To
verify whether the characteristics of language used
in the Dark Web affect the power law distribution
of words, we compare the word frequency distribu-
tion between Dark and Surface Web corpora. We
aggregate all texts in each category into a single
ﬁle, lemmatize each word using spaCy, and use
scikit-learn(Pedregosa et al., 2011) to retrieve
the word frequency per category.
We ﬁnd that, as far as word frequency distribu-
tion is concerned, there is no signiﬁcant difference
between the Dark Web and the Surface Web. As
the Dark Web contains many phishing sites which
are near identical copies of each other (Yoon et al.,
2019), we believed that some words may have ab-
normally high frequencies, which would affect the
overall distribution. However, the results suggest
that word frequency distribution is largely domain-
invariant.
4.2.3 Document Similarity
As mentioned in Section 1, about half of DUTA’s
documents contain duplicate data. CoDA addresses
this problem by crawling web pages whose textual
content is less similar to one another. To this end,
we measure the document similarity between the
two Dark Web corpora to validate the uniqueness
of documents in CoDA. Through manual inspec-
tion, we ﬁnd that some pages share the same exact
content but with slight variations in details such
as numbers. To prevent such differences from af-
fecting the document similarity, we mask and pre-
process documents in DUTA in the same manner
as CoDA and convert each document into a bag
of lowercase words. The similarity is measured by
taking the Jaccard distance on the bags of words,
with distance of 1 indicating complete similarity
between two documents.
To illustrate the amount of overlapping content
in CoDA and DUTA, we compare each document
with all other documents from the same corpus,
and denote the maximum Jaccard distance as its
maximum similarity. As shown in Figure 1, more
than half of the documents in DUTA share almost
completely overlapping content, whereas CoDA
exhibits much lower similarities overall. Although
DUTA consists of data from 10367 onion services
which is larger than the number of onion services
collected for use in CoDA, this shows that the data
in CoDA is more uniquely varied and thus has
higher information density.
4.2.4 Mask ID Distribution and TF-IDF
To gain insight into some of the lexical character-
istics of each category in the Dark Web, we eval-
uate the mask ID distribution and TF-IDF (term
frequency-inverse document frequency) for CoDA.
The mask ID distribution is calculated by dividing
the frequency of a particular mask ID (listed in Ta-
ble 3) in a document by the number of all mask IDs
in that document (we exclude ID_NUMBER in our
data for this analysis as it accounts for the majority
of all masks in every category). This is done for ev-
ery document, and we take the average distribution
by category. Similar methods are used for TF-IDF
using scikit-learn (Pedregosa et al., 2011). We ex-
clude English stopwords as deﬁned in NLTK (Bird
et al., 2009), but preserve the mask IDs to capture5626important mask IDs in each category.
We ﬁnd that some mask IDs are particularly
representative in some categories. For example,
thedrugs category has a high proportion of
ID_WEIGHT , which is reasonable as webpages
indrugs usually specify the weight of the drug
in their listings. The TF-IDF measurements also
show some interesting results; for example, the
majority of terms with the highest TF-IDF in the
electronics category are related to Apple products
(iPhones, iPads, MacBooks, etc.) which may sug-
gest a high popularity of these products in the Dark
Web.
5 Classiﬁcation Experiments
5.1 Setup
We build several classiﬁers to investigate the per-
formance of existing classiﬁcation models on Dark
Web text. Although deep neural network models
are widely used today (Minaee et al., 2021), simple
machine learning models such as SVM and naïve
Bayes (NB) have been reported to perform reason-
ably well on Dark Web texts, often outperforming
deep models (Choshen et al., 2019). Therefore, we
evaluate both types of models to see which is ad-
equate for Dark Web text classiﬁcation. We split
CoDA into training and test sets (7:3 ratio) after
stratiﬁed random shufﬂing with the same random
seed for all experiments. The preprocessing method
used for document similarity (Section 4.2.3) is ap-
plied here as it empirically works best across mod-
els.
Multi-class SVM: We train a multi-class SVM
classiﬁer with TF-IDF features, and tune its hy-
perparameters by grid search. We build our clas-
siﬁer using TfidfVectorizer ,LinearSVC ,
andGridSearchCV classes in scikit-learn.
CNN: Convolutional Neural Networks have been
established as one of the popular choices for text
classiﬁcation for the ability to recognize position-
invariant patterns such as text phrases (Minaee
et al., 2021). Using PyTorch, we build a CNN
model with a GloVe embedding layer (6B.300d),
2D convolution layers, and a fully-connected layer
(Pennington et al., 2014).
BERT: To beneﬁt from contextual representa-
tions and transfer learning, we use BERT (De-
vlin et al., 2019), a state-of-the-art language
model widely adopted across many NLP and
machine learning tasks. We use the pretrained
bert-base-uncased model in the PyTorch
version of the HuggingFace library (Wolf et al.,
2020) with a fully-connected classiﬁcation layer on
top of the [CLS] token.
5.2 Results
Table 7 summarizes the performance of the three
classiﬁers on CoDA. BERT exhibits the best results
possibly due to its capability to model unknown
words and utilize contextual information, despite
the lexical differences of the Dark Web as shown
in Section 4.2. SVM produces comparable results,
suggesting that the relatively simple bag-of-words
approach is still very effective at modeling topics of
such domain-speciﬁc text. In contrast, CNN fares
relatively worse, which is likely due to the spe-
cialized vocabulary of the Dark Web being poorly
covered by the pretrained word embedding as seen
in Choshen et al. (2019).
Table 8 shows the detailed results of classiﬁca-
tion using BERT. The classiﬁer works relatively
well for categories that exhibit a smaller special-
ized vocabulary such as arms ,electronics , and gam-
bling , whereas it performs worse for categories that
cover diverse subtopics such as cryptocurrency and
ﬁnancial . We also observe that the classiﬁer often
confuses hacking with cryptocurrency orﬁnancial
especially when documents contain phrases such as
“hacked PayPal” or “hacked Bitcoin wallets”, which
are not categorized in the hacking category by our
guidelines (the hacking category relates to hacking
services and professional hacking techniques).
6 Use Cases
In this section, we elaborate on the use cases of
our corpus and the classiﬁers trained on CoDA and
DUTA.
(1) Synonym Inference: Dark Web users tend to
use words differently from their original meaning
to conceal or disguise their intents. For example,
we observed that car company names (e.g., Tesla ,5627
Toyota ) are often used in drug-related documents
in the Dark Web to refer to synthetic drugs with
brand logos imprinted on each pill.
We test the above scenario by training two sim-
ple Word2vec models (Mikolov et al., 2013; Re-
hurek and Sojka, 2010), one using CoDA and an-
other using DUTA. For each model, we query Tesla
andToyota and retrieve the most similar words to
the queried terms. In this case, both models out-
put drug-related words such as methoxphenidine ,
testosterone , and alprazolam . We also query an-
other word, Wasabi , which originally refers to a
plant but is also used to refer to a Bitcoin wallet
service. In the Dark Web, Wasabi is more likely to
be used as a cryptocurrency term rather than the
plant itself. When Wasabi is queried, the model
trained on CoDA returns cryptocurrency-related
words, while the model trained on DUTA does not
have the word in its vocabulary. We list the top
10 most similar words to Wasabi as reported by
the model trained on CoDA, most of which are re-
lated to cryptocurrency services: mustard, electrum,
samourai, pools, trustless, mycelium, rpc, xapo, hi-
jacker, converter .
(2) Topic Classiﬁcation: We assess the efﬁcacy of
CoDA and DUTA in classifying document cate-
gories in the Dark Web. For this scenario, we man-
ually compile a list of 34 forum / marketplace web-
sites on the Dark Web across three different topics:
drugs ,weapons , and ﬁnance , and create an extra
benchmark dataset consisting of 2236 webpages
from the list. To remove possible overlap between
the CoDA / DUTA corpora and the benchmark
dataset, we exclude any documents crawled from
the same URL as those from the benchmark dataset,
or documents that mention any of the names from
the benchmark websites in their content. This ex-
cludes 246 and 220 documents from CoDA and
DUTA, respectively. We then train a BERT-based
classiﬁer for each Dark Web corpus on the remain-
ing documents with the same conﬁguration used in
the classiﬁcation experiments, and evaluate them
on the benchmark dataset.
Table 9 shows the classiﬁcation performance
measured on the benchmark dataset, in which the
CoDA-trained classiﬁer consistently outperforms
the DUTA-trained classiﬁer. We conjecture that
this is because CoDA contains less duplicate text
with more diverse domain-speciﬁc words in the
same number of documents, allowing the trained
classiﬁer to generalize better to unseen documents.
7 Conclusion
In this work, we introduced CoDA, a Dark Web
text corpus collected from various onion services
divided into topical categories. Using CoDA, we
conducted a thorough analysis of the linguistic
properties of the Dark Web and found that there
are clear lexical differences from the Surface Web
including abbreviations and lexical structure such
as PoS distribution. We also found lexical charac-
teristics of categories through mask ID distribution
and TF-IDF.
Our text classiﬁcation results showed that SVM
and BERT perform well in the Dark Web domain,
even with the language differences that the Dark
Web exhibits compared to that of the Surface Web.
Finally, we have demonstrated the practicality of
CoDA through two use cases with NLP methods.
We speculate that the lack of duplicate content in5628CoDA compared to DUTA may aid in the perfor-
mance of such applications.
We hope that our dataset and our work motivates
further research in the ﬁeld of language-based Dark
Web analysis.
Ethical Considerations
Masking Sensitive Information
Due to the nature of anonymous networks such as
Tor, raw data posted on the Dark Web may contain
private or illegal information. Such information
includes (but is not limited to) Bitcoin addresses,
credit card information, and social security num-
bers. Since CoDA was compiled by randomly se-
lecting web documents from the Dark Web, the
dataset may contain such information. Therefore, it
is important that a public Dark Web dataset such as
CoDA addresses ethical issues regarding sensitive
information.
To prevent the use of CoDA for malicious pur-
poses such as the extraction of sensitive informa-
tion, we identify types of potentially sensitive data
(such as email, IP, URL, crypto addresses, and
social security numbers) which are subsequently
masked (refer to Section 3.6 for the detailed de-
scription on mask identiﬁers used). These identi-
ﬁers are matched using regular expressions and
each page has been manually double-checked on
whether such content has been properly masked
by the authors. During this time, the authors did
not ﬁnd sensitive content outside of the masked
information.
As mentioned in Section 4, some data analysis
methods are conducted with unmasked versions of
the Dark Web to prevent bias. However, we only
use the unmasked version of CoDA for a fair anal-
ysis between the Dark and the Surface Web data,
and do not utilize or disclose information found in
the unmasked data in any way.
Handling Illegal Content
A signiﬁcant portion of the Dark Web deals with
explicit, pornographic content (violence, child
pornography, torture, etc.). The act of accessing
or viewing such media is illegal by law in many
parts of the world. To prevent the access of such
media, we collect crawled Dark Web pages in the
form of HTML and parse HTML tags to retrieve
only the text data. In addition, URL addresses and
onion addresses that may link to such illegal media
are also masked as previously mentioned (note thatall URL addresses and onion addresses have been
masked, regardless of their content). Consequently,
the authors and the annotators do not have access
to media that are illegal by law.
It is worth noting that CoDA still contains texts
of various activities that occur in the Dark Web,
some of which are illegal in nature (drug trade,
counterfeit products, etc.). However, the inclusion
of text in the dataset that describes potentially ille-
gal activities is not of ethical concern. Therefore,
we do not censor text data that correspond to illegal
activities.
Ethics on Annotation
Dark Web content often contains sensitive and il-
licit activities. Dealing with such content during the
annotation process may be unsettling for some peo-
ple, so we chose annotators who are experienced
with the Dark Web and has given consent to being
exposed to such content. The annotators recruited
for classifying CoDA were specialists who work
at a cyber threat data analytics & intelligence com-
pany specializing in Dark Web data. To ensure that
the annotation process is fair, each of the ten anno-
tators handled the same number of pages and were
given equal compensations.
Preventative Measures to Discourage
Non-Academic Use of CoDA
The content of CoDA includes text descriptions
of various Dark Web activities. A potential harm
of releasing this dataset is bringing increased at-
tention to these activities. We strongly believe
that our research should be used for scientiﬁc pur-
poses only and discourage non-academic use of
CoDA. We take a preventative approach by only
permitting access to CoDA to researchers with re-
search purposes that abide by the ACL Code of
Ethics. The terms of use agreement can be found
athttps://s2w.inc/resources/coda .
Acknowledgements
This research was supported by the Engineer-
ing Research Center Program through the Na-
tional Research Foundation of Korea (NRF)
funded by the Korean Government MSIT (NRF-
2018R1A5A1059921).
References562956305631A Inter-Annotator Agreement
To obtain high-quality annotations, we ran a training session with the ﬁrst 150 documents in three
50-document intervals. Each stage, we asked 10 annotators to annotate the same documents with the
guidelines, and measured inter-annotator agreements using Fleiss’ Kappa coefﬁcient (Landis and Koch,
1977; Artstein, 2017). After each interval, we held a discussion session to resolve disagreements and
revised the guidelines to accommodate feedback from the annotators. For each interval, the agreement
coefﬁcients were 0.67, 0.72, and 0.88. Note that the coefﬁcients greater than 0.60 and 0.80 can be
interpreted as “substantial” and “almost perfect” agreements, respectively (Landis and Koch, 1977).
This suggests that the training sessions helped the annotators gradually reach a common consensus and
familiarize themselves with the guidelines. We then assigned the remaining documents so that each
document is assigned to a single annotator to speed up the annotation. The whole process took about three
months, including one month of the training session.
B Experimental Details
The data analysis experiments were performed on a machine with Intel Xeon E5-2630 v4 CPU @ 2.2
GHz (with no GPU usage), and the classiﬁcation experiments were performed on a machine with Intel
Xeon Gold 6258R CPU @ 2.70 GHz and Nvidia GeForce RTX 3090.
SVM: To ﬁne-tune the parameters, we exhaustively generated all the candidate combinations of the
two parameter pairs: tolerance and regularization. From a grid of their values, we applied 10-fold cross
validation and found that the model with tolerance of 0.1 and regularization of 1.0 work best when we
used ‘ balanced_accuracy ’ as the scoring strategy. Since this model is a multi-class classiﬁer, we
used the OVR (one-versus-rest) multi-class strategy.
CNN: The model consists of one GloVe embedding layer (6B.300d), three 2-dimensional convolutions
(Conv2d), and one fully-connected layer. The kernel sizes of the three convolution layers are 3, 4, and
5, respectively. We applied the ReLU activation function and 1-dimensional max pooling after each
convolution layer. We also used the SGD optimizer and ran 10 training epochs with cross-entropy loss;
the learning rate was 1.5, and the batch size was set to 32.
BERT: We used the Adam optimizer and ran 10 training epochs with cross-entropy loss, a learning
rate of 2e-5, a linear schedule with no warmup step, a batch size of 32, and gradient norm clipping of 1.0.
We also limited the maximum sequence length to 256 tokens, assuming that the leading part of text is
indicative of topics. All the other settings are the same as those used in the original BERT paper.
C Language Distribution
We present the language distribution of CoDA in the following table:5632D Additional Data Analysis Methods & Results
Some additional details and ﬁgures of results collected from various data analyses methods in Section 4
are presented here.
D.1 PoS Distribution & Content Word / Function Word Ratio
Choshen et al. (2019) demonstrated that legal and illegal texts in the Dark Web can be distinguishable
through their lexical structure, that is, through part-of-speech (PoS) tags and distribution of content and
function words. We analyze the PoS distribution and the distribution of content and function words in each
dataset to see if there is a meaningful difference in the lexical structures of Dark / Surface Web contents.
To obtain the universal PoS of words in the dataset, we utilize the PoS tagger in spaCy. Following Choshen
et al. (2019), we deﬁne content words as words tagged by spaCy into to one of the following PoS tags:
{,, , , ,,}
and deﬁne all other words as function words. Since text length varies widely for each document in the
Dark Web, we analyze the mean PoS ratio and the mean content word / function word ratio (CF ratio) for
each category for both Dark and Surface Webs. The mean CF ratio ¯r(C)for some categoryCis given by
¯r(C) =1
|C|/summationdisplayN(d)
N(d)
where|C|denotes the number of documents (text ﬁles) in category C, andN(d),N(d)denote the total
number of content words and function words in some document d, respectively.
The results for the PoS distribution and CF ratio are shown in Figures 2 and 3. It is evident that Dark
Web categories generally have a much higher CF ratio compared to that of the Surface Web. From our PoS
distribution analysis, we ﬁnd that the Dark Web documents have a very high ratio of proper nouns ( PROPN )
and numerals ( NUM) compared to the Surface Web documents, both of which are PoS tags of content
words. Moreover, the Surface Web documents have a higher ratio of determiners(DET) compared to
that of the Dark Web. Since function words serve as critical components of sentence structures, this result
implies that language used in the Dark Web may contain a higher proportion of non-sentence structures.
For example, texts in the drugs category mostly consist of a list of drugs with their price and weight.
D.2 Discussion
The use of spaCy for the comparison of PoS distribution and CF ratios may raise questions as spaCy has
not been pretrained on Dark Web content, which may yield a higher error rate on the Dark Web results.
However, this is not an issue for our work as the main purpose of this analysis is to show that there are
linguistic differences between the Dark Web and the Surface Web. If the result of the analysis is heavily
skewed by the presence of Dark Web content and additional training is necessary for the pretrained spaCy
model, then this implies that there are meaningful lexical differences in the Dark Web. On the other
hand, if the result is not particularly affected by the Dark Web content, then it shows that there are clear
differences between the two domains. In either case, it is observable that the results suggest the existence
of lexical differences between the Dark Web and the Surface Web.
However, there is one possible limitation in our analysis in that the aggregate Surface Web dataset may
not encompass the complete representation of the Surface Web. This is evident from observing that a
signiﬁcant portion of the aggregate dataset (IMDb, Wikitext) consists of text content that is comprised of
complete sentences. For example, the inclusion of marketplace content such as eBay or Amazon could
affect the PoS distribution and the CF ratio of the aggregate Surface Web data. For future work, we may
attempt additional analysis through boilerplate removal of noisy, non-structural texts in CoDA and the
aggregate Surface Web data along with the augmentation of a broader diversity of Surface Web content
and examine if this approach signiﬁcantly affects the results obtained in Section D.1.563356345635E TF-IDF Measurements
A table of TF-IDF measurements (as mentioned in Section 4.2.4) showing the relevant words and phrases
of selected categories in CoDA is listed here.5636F Forum & Marketplace Benchmark Dataset5637