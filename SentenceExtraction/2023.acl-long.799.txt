
Haau-Sing Li, Mohsen Mesgar, André F. T. Martins, Iryna GurevychUbiquitous Knowledge Processing Lab (UKP Lab)
Department of Computer Science and Hessian Center for AI (hessian.AI), TU DarmstadtBosch Center for Artificial Intelligence, Renningen, GermanyInstituto Superior Técnico and LUMLIS (Lisbon ELLIS Unit)Instituto de Telecomunicações, Lisbon, PortugalUnbabel
hli@ukp.tu-darmstadt.de
Abstract
Code generation from text requires understand-
ing the user’s intent from a natural language
description and generating an executable code
snippet that satisfies this intent. While re-
cent pretrained language models demonstrate
remarkable performance for this task, these
models fail when the given natural language
description is under-specified. In this work,
we introduce a novel and more realistic setup
for this task. We hypothesize that the under-
specification of a natural language description
can be resolved by asking clarification ques-
tions. Therefore, we collect and introduce a
new dataset named CodeClarQA containing
pairs of natural language descriptions and code
with created synthetic clarification questions
and answers. The empirical results of our eval-
uation of pretrained language model perfor-
mance on code generation show that clarifi-
cations result in more precisely generated code,
as shown by the substantial improvement of
model performance in all evaluation metrics.
Alongside this, our task and dataset introduce
new challenges to the community, including
when and what clarification questions should
be asked. Our code and dataset are available on
GitHub.
1 Introduction
Text-to-code generation aims to understand a user’s
intention represented by a natural language descrip-
tion (NLD) to generate a code that satisfies the
user’s intention. Models for this task are a crucial
component of digital pair-programmers, which as-
sist data scientists (Agashe et al., 2019; Liu et al.,
2021), software developers (Chen et al., 2021; Xu
et al., 2022), and computer programming educators
(Li et al., 2022).
Recent work addresses this task using pretrained
language models (PLMs) fine-tuned on large-scale
Figure 1: (a) An example of NLD-code pair that requires
further clarification. We highlight operations that need
clarification. (b) The generated graph of the example.
Each node is an operation, with key operations marked
in red and the rest in gray. Edges show the data flow.
code data in general-purpose programming lan-
guages, such as Python and Java (Chen et al., 2021;
Li et al., 2022; Nijkamp et al., 2022; Chowdhery
et al., 2022; Xu et al., 2022; Lahiri et al., 2022).
Although these models are successful, they fail
to resolve the case of an NLD that lacks enough
specifications. Figure 1a depicts an example of
under-specified NLD (shown in yellow). The prob-
lem of missing specifications in NLDs not only
widely occurs in real-world use cases (Lahiri et al.,
2022; Chaurasia and Mooney, 2017) but is also
important for training text-to-code generation mod-
els. Although important, alleviating the under-
specification of NLDs is challenging for two rea-14287
sons. First, missing specifications can happen at
various levels, including individual operations, ar-
gument values of the operations, and sub-tasks con-
sisting of several operations decomposed from the
entire source code file as the task. Second, it is not
obvious how to identify if an NLD carries infor-
mation about specifications at any level mentioned
above or not.
In this paper, we introduce interactivity into text-
to-code generation for specifications on the level
of individual operation calls in Python. We hypoth-
esize that by gathering more specifications using
these interactions, we can alleviate the incomplete-
ness of NLD’s specifications and thus generate a
more precise code (Figure 1a). To train and eval-
uate such models, we introduce the CodeClarQA
dataset collected through a novel method to synthet-
ically generate clarification questions and answers
(CQAs) for an NLD-Code pair. To map the op-
erations to natural language, we retrieve the API
documentation. If there is a low similarity between
the NLD and the operation’s documentation, we
identify the operation as a missing specification and
generate a clarification question (CQ). The answers
to CQs are selected from the given code. Further-
more, we propose a pipeline to demonstrate the
use case of our dataset for developing NLD-Code
generation models. Our pipeline consists of three
modules – a clarification need predictor, a CQ gen-
erator, and a code generator. For each module, we
introduce models that can serve as baselines.
To evaluate the quality of our dataset, we con-
duct a human evaluation. We also evaluate the
models we proposed for each component of our
pipeline. Our empirical results show that by con-
ditioning PLM-based code generation models on
CQAs in our dataset, the performance of these
models increases, indicating the correctness of our
hypothesis and our collected dataset. Alongside,our experimental results show that advanced PLMs
(e.g., RoBERTa, BART, T5, and CodeT5) struggle
to achieve high performance under the interactive
code generation pipeline. This important observa-
tion demonstrates the difficulty of our dataset for
the recent PLMs, introducing new challenges for
these models.
2 Creating the CodeClarQA Dataset
We aim to make the process of code generation
interactive such that by asking CQs about a given
NLD, we resolve NLD under-specification before
generating any code. To do so, we design a novel
method to synthetically collect CQAs for a given
NLD-Code pair, leading to the new dataset, which
we call CodeClarQA. Figure 2 shows a general
view of our data creation method.
2.1 Identifying Key Operations
Key operations correspond to sub-tasks decom-
posed from the code snippet as the task. For
instance, the code in Figure 1 can be decom-
posed into three sub-tasks: call the logistic model,
use grid search to fit different logistic mod-
els, and save the best model. The correspond-
ing key operations are sklearn.LogisticRegression ,
sklearn.GridSearchCV , and joblib.dump . Ideally,
an NLD should provide enough information about
key operations in a code. If the NLD does not do
so, it lacks sufficient specifications. Thus, our first
step for generating CQAs for a given NLD-code
pair is to identify the key operations required to
generate the code from NLD.
To identify key operations, we represent the code
as a graph. Data flow graphs are an effective struc-
tural representation of code semantics in a code.
Given this fact, we use the graph defined by the
GraphGen4Code toolkit (Abdelaziz et al., 2021),
the state-of-the-art toolkit for generating a graph14288from a source code, including API-related opera-
tions and the data flow. This makes it easy for us
to identify key operations. Figure 1a shows the
graph representation. Non-leaf nodes represent key
operations. Edges indicate the data flow of the op-
erations. For each NLD-Code pair, we parse the
code to generate a graph.
Given a graph, we identify nodes with the fol-
lowing properties as key operations: (i) For op-
erations that are one object/function and its
methods/fields, we treat the object/function as
a key operation. This is coherent with one hy-
pothesis behind the design of GraphGen4Code,
where an object/function is first initiated before
fields/methods thereof are applied. For instance,
sklearn.GridSearchCV is the key operation among
all operations related to it, as other operations ap-
ply a method ( .fit) or read a field ( .best_estimator_ )
of it (Figure 1b). (ii) For a multiple-operation
line of code, the last operation on the data
flow path is a key operation. For instance,
sklearn.GridSearchCV andnumpy.linspace are in
the same line. sklearn.GridSearchCV is a key op-
eration since sklearn.GridSearchCV is the line’s
highest-level operation (Figure 1b). See Ap-
pendix A for details of the procedure of identifying
key operations.
2.2 Is a Key Operation Missing in NLD?
Given a set of key operations required to generate a
code, we should identify if the given NLD provides
any information about these operations. To do so,
for each key operation, we propose to align the
schema of textual documentation of a key operation
with the schema of a given NLD. A schema (Ma-
jumder et al., 2021) is defined as a set of important
elements of a document. Every schema element is
either in the form of ( verb,key-phrase ,relation ) or
(key-phrase ), where key-phrase is extracted using
YAKE (Campos et al., 2020), and verb andrela-
tionare obtained by searching through the closest
verb and its dependency relation using the depen-
dency tree (Qi et al., 2020). An example of ( verb,
key-phrase ,relation ) is (transforms, final estimator,
obl), and an example of ( key-phrase ) is (pipeline).
For each key operation required to generate a
code, we compute similarity scores for all schema
element tuples using elements from the NLD and
the documentation. For each pair of schema ele-
ments, we use a pretrained text encoder (Reimers
and Gurevych, 2019) to compute similarity scoresbetween these phrases as key information. Note
that we combine verb andkey-phrase if the schema
element is in the triplet form before computing the
similarity score. Eventually, we identify a key oper-
ation is missing in the NLD if the highest similarity
score of all schema element pairs is lower than the
threshold t. Each key operation is then labeled as
aligned ormissing . We perform a grid search to
find the best ton a validation set, maximizing the
F1 score. See Appendix B for an example.
2.3 Generating CQAs for Missing Key
Operations
We formulate CQs as multiple-choice questions
and yes/no questions. The former needs an answer
with yes/no following a choice of an API call. The
latter requires only an answer with yes/no.
Multiple-choice. We collect all extracted key op-
erations from the dataset, mentioned or missing,
that contain 1023 different API sub-modules, meth-
ods, and fields. We then extract the last tokens from
each operation name, filter out all stop words, and
keep operations that share the same last token of
their names. For instance, sklearn.partial_fit and
sklearn.fit share the same last token as fit. Note that
we hypothesize that for operations with the same
name but from a different library, e.g., keras.fit and
sklearn.fit , they refer to the same operation. We
generate multiple-choice questions for these key
operations if they are missing. To do so, we use the
template Do you want to call anything related to _ ? If yes, which one? .
Yes/No. For operations that do not belong to
multiple-choice questions, we generate a yes/no
question using the template Do you want to call _ documented as?. For in-
stance, a CQ about numpy.logspace is generated as
“Do you want to call numpy.logspace documented
asReturn numbers spaced evenly on a log scale? ”
2.4 Dataset
We use NLD-Code pairs in the notebookCDG
dataset (Liu et al., 2021) to create CQAs because
of the high code quality ensured by votes of the
Jupyter Notebooks and the high NLD quality en-
sured by the preprocessing method based on the
study of markdown documentation (Wang et al.,
2021a). We first identify key operations (§2.1) and
label them as either aligned ormissing (§2.2). Fi-
nally, we select NLD-Code pairs with at most five14289
missing key operations, duplicate missing key op-
erations, and create CQAs (§2.3). Table 1 shows
dataset statistics.
3 Pipeline for CQ-driven Code
Generation
Our system generates precise code by asking CQs
before generating. To do so, it uses an interactive
code generation pipeline that includes three mod-
ules: (i) a clarification need predictor, (ii) a CQ
ranker, and (iii) a code generator.
Given an NLD, the clarification need predictor
predicts the need to ask CQs, with labels Need and
No Need . If there is a need for asking CQs, the CQ
ranker selects nCQs. We set nas five to push these
models to choose CQs with the most information
gains. Given the NLD, CQs and corresponding
answers, the code generator generates a code.
4 Experiments
Having proposed our dataset (§2) and a pipeline for
interactive code generation (§3), we next evaluate
the quality of the dataset creation method by focus-
ing on §2.2 and results of use our dataset to evaluate
recent PLM-based models for each pipeline mod-
ule for interactive code generation, before assessing
the quality of the pipeline. The dataset evaluation
analyzes the effectiveness of identifying key oper-
ations, while experiments on the pipeline aim to
validate our hypothesis that interactiveness helps
code generation and evaluate task difficulty.
4.1 Dataset Evaluation
To evaluate our dataset creation method, we ran-
domly split our dataset into train/validation/test
sets. We asked two Ph.D. students in computer
science to annotate each NLD-Code pair in the val-
idation and test sets. The annotation for each NLD-
Code pair is a binary label, indicating if the NLDmisses any key operation from the code. These
annotations let us (i) study the properties of our
dataset and (ii) evaluate the quality of our method
for finding missing key operations using different
text encoders. See Appendix §D for more details.
Setting. The validation and test set consist of 100
NLD-Code pairs respectively. The Fleiss Kappa is
0.74 (0.83 for the validation and 0.66 for the test
set). We randomly chose one annotator’s annota-
tion as reference labels. See Appendix §E for more
analysis on annotation results.
4.2 Clarification Need Predictor
In order to label when CQs were needed, we
learned a binary classifier. This classifier predicts,
for an NLD, whether it needs further clarification.
The classifier was trained on the NLD-Code pairs
in the training portion of the CodeClarQA dataset.
Setting. We fine-tune baseline pretrained trans-
former classifiers, including BERT (Devlin et al.,
2019), RoBERTa (Liu et al., 2019), and the en-
coder of BART (Lewis et al., 2020). To include
models trained on NLD data, we also fine-tune the
encoder of PLBART (Ahmad et al., 2021). Models
are fine-tuned on the training set with NLDs as the
input. We fine-tune each model for 10 epochs with
learning rate 5×10and pick the best-performing
model on accuracy. We compare the models on the
test set using accuracy, precision, recall, and F1.
4.3 CQ Ranker
Given an NLD, a CQ ranker should recommend
potential key operations by asking CQs. We formu-
late this as a ranking task, where we select a subset
of CQs from a universal set of CQs. We use all
created CQs using our method mentioned in §2 as
the universal set.
Setting. We follow Aliannejadi et al. (2021) and
fine-tune cross-encoders on all NLD-CQ pairs and
experiment with models used in §4.2. Given an
NLD-CQ pair, each model is trained to do binary
classification. At inference time, all CQs in the uni-
versal set are paired with a given NLD and ranked
by model score. Given an NLD, positive samples
CQs created in the dataset. To create negative sam-
ples, we experiment with random negative sam-
pling and BM25 (Robertson et al., 1995). The
number of negative samples selected is the aver-
age number of positive samples. Each model is
trained for 10 epochs with learning rate 5×10.14290
We evaluate model performance with the test set
onR@k, k∈{1,3,5,10}.
4.4 Code Generator
The key hypothesis of our work is that inter-
active code generation systems outperform non-
interactive ones. In this experiment, we conduct a
proof-of-concept experiment to validate this hy-
pothesis, assuming a perfect interactive system
with perfectly asked CQs and answers. We fine-
tune models with and without oracle CQAs from
our dataset. Note that for both yes/no and multiple-
choice questions, we have only positive answers in
our dataset.
Setting. We experiment with models mentioned
by Zhou et al. (2022) for fine-tuning, including
GPT-Neo-{125M, 1.3B} (Black et al., 2021), T5
(Raffel et al., 2020), and CodeT5 (Wang et al.,
2021b). We include CodeParrot-{110M,1.5B}
(Tunstall et al., 2022). Note that for CodeParrot-
110M, we use the model fine-tuned on text-to-code
generation.Moreover, we finetune PLBART-base
(Ahmad et al., 2021). We train each model for 40
epochs with learning rate 5×10. Each experi-
ment takes up to 6 hours on a single A100 GPU. We
evaluate models on BLEU score (Papineni et al.,
2002), CodeBLEU score (Ren et al., 2020), and Ex-
act Match (EM). Note that we don’t include state-
of-the-art execution-based metrics (Huang et al.,
2022; Yin et al., 2022), since it requires us to in-
clude code context into the dataset, which leverages
the difficulty of dataset construction. As we don’t
include code context into the dataset, code predic-
tions are more likely to fail on e.g. variable naming,
which affects the execution results but does not nec-
essarily lead to poor code quality.
4.5 End-to-end Pipeline Evaluation
To assess the performance of the entire pipeline
(§3), we use the best-performing models for each
module. We pass an NLD to the clarification need
predictor. Given a positive prediction, we pass the
NLD to the CQ ranker. For each NLD, we select
the top- k(k∈{1,3,5}) ranked CQs by the CQ
ranker. We compare them to CQs created using
our approach and select overlapping CQs. Finally,
we concatenate the NLD and all selected CQs with
corresponding answers and feed them to the code
generator.
5 Results
5.1 Dataset Evaluation
We first evaluate the effect of different text en-
coders on the performance of our method for iden-
tifying missing operations. Table 2 shows the
results. We achieve the best performance using
MPNetqa-cos text encoder. We then use our an-
notations to analyze the predictions of this model.
Table 3 shows the results of this analysis in terms
of False Positive (FP) and False Negative (FN) er-
rors. For the sake of brevity, we report the full list
in Appendix §D.
The “Taxonomy” and “Element Pair” error types
take up to 7.32% and 8.57% of all operations pre-
dicted as aligned in the validation/test sets, respec-
tively.
The rare case of FP predictions suggests that our
approach to generating CQAs effectively creates
CQAs for missing key operations. The Taxonomy
error relates the differences related to the taxonomy
of terms that could not be identified, taking up to
about 8.57%. The Element Pair error relates to14291
the cases where non-relevant schema elements are
aligned, taking up to about 8.57%. The Argument
error represents the alignment between arguments,
taking up only 4.08%/4.35% of all negative predic-
tions from the validation/test set. Table 4 shows
examples of these errors.
For the taxonomy error, our method identifies a
schema element match of linear models but fails to
predict the difference between a lasso linear model
and a linear model in the taxonomy of machine
learning terms. This finding shows a potential di-
rection of future work, in which aligned operations
might require clarification to be distinguished from
operations with similar names. The example of
Argument error reflects the case where a complete
semantics of the operation needs both the documen-
tation and the argument values. As we proposed to
compare documentation and the NLD, we miss out
on arguments that can complement the semantics of
the operation. The corresponding example shows
that the operation .apply ’s semantics is incomplete
without the argument str. This is reflected in the
design of our method, as we use API documenta-
tion which reflects the semantics of the API call,
while argument values are not documented.
The Element Pair error example shows that
(make, index, obj) from the documentation’s
schema is aligned with (index) from NLD’s schema.
In contrast, the key operation from the documenta-
tion should be either drop ordeleted .
5.2 Clarification Need Predictor Evaluation
Table 5 summarizes the results of different clas-
sifiers. Most tested models obtain relatively high
performances except for RoBERTa, which over-
fits the imbalanced data where 63.71% samples
have positive labels, as shown by the high recall
but low precision. Moreover, BERThas the best
performance on both accuracy and F1 score.
5.3 CQ Ranker Evaluation
We report the results of our experiments on CQ
generation in Table 6. The results confirm that
our design of selecting CQs is reasonable, with the
best-performing model showing similar results to
the “Question Relevance” task designed by Alian-
nejadi et al. (2021). However, we hypothesize that
our task is more challenging, as the lexical over-
lap between the NLD and the correctly selected
CQs is low due to our design of dataset creation14292
which looks for key operations with documentation
that has no keyword matches to the NLD. This re-
quires the model to utilize the under-specified NLD
and infer the topic of the task and the user’s intent
before providing suggestions by asking CQs.
Our hypothesis is strongly supported by the low
recall of the BM25 ranker, which ranks CQs based
on their lexical similarities with NLD. Moreover,
we find that models trained with the BM25 negative
sampler always perform lower than the ones trained
with the random sampler, which also supports our
hypothesis because the BM25 negative sample is
expected not to select CQs that have high lexical
overlap with the NLD as negative samples, while
they have a higher chance of asking key operations
that are “mentioned”.
5.4 Code Generator Evaluation
We train recent models using only the NLD-
Code pairs or with NLD, Code, and CQAs in the
CodeClarQA dataset. The experimental setup
aims to test our hypothesis that interactiveness
helps code generation by running code generation
models with “perfect” clarifications. Note that this
only serves as proof of concept, as CQAs contain
operation names in the target source code, leading
to data leakage because the names of the API calls
exist in the CQs.
Table 7 shows that all models fine-tuned with
CQs have substantially better performance, with
the largest gap of 14.28 in BLEU, 10.5 in Code-
BLEU, and 6.03 in EM reached by PLBART,
which supports our hypothesis that interactions
help code generation. Moreover, all models pre-
trained on code data have better performances,
with CodeT5and PLBARTas the best-
performing models we tested.
5.5 Pipeline Evaluation
We use BERT clarification need predictor,
BARTCQ ranker with random negative sam-
pling, and PLBARTtrained with CQAs . Given
the question ranker’s predictions, we select CQAs
from the test sample with CQ included in the top- k
(k∈{1,3,5}) list yielded by the CQ ranker. Be-
sides concatenating selected CQs to NLDs, we also
concatenate CQs without selecting them, treating
them as “unanswered clarifications”.
We report the results of pipeline evaluation in Ta-
ble 8. We find that model performances on all eval-
uation metrics substantially increased with more
highly-ranked CQs being included and “answered”
by comparing highly-ranked CQs and the CQAs
in the dataset. Moreover, we also find the opposite
trend for “un-answered clarifications” where mod-
els perform worse with more highly-ranked CQs
included (but not “answered”). This aligns with the
challenge of asking CQs mentioned in §5.3.
Last but not least, we compare the pipeline infer-
ence results in Table 8 to the results in Table 7. No-
tably, our pipeline underperforms models trained
on data with only NLDs and code. This is expected,
as we use code generators that are fine-tuned on all
CQAs, and the results of ranking CQs suggest that
the task of asking CQs is challenging (§5.3).
6 Analysis
Intuitively, asking CQs helps code generation be-
cause it provides more specifications, thus aligning
model generations to desired and better-quality out-
puts. To test if this hypothesis stands under the con-
text of our proposed task and pipeline, we analyze14293
model generations quantitatively and qualitatively.
Recall of identified missing key operations. Ta-
ble 9 shows the recall of missing key operations
from predictions. We find that training with clari-
fications includes substantially more missing key
operations, while the pipeline still does not outper-
form models trained on data with only NLDs and
code, similar to Table 8. Furthermore, we report
Pearson correlation between the recall of missing
key operations and code generation results (See
Table 10), finding high and positive correlations
which support our hypothesis that asking CQs helps
code generation through clarified key operations.
Case study. We examine predictions and provide
an example in Table 11. We find that training
with oracle CQAs leads to predictions close to the
ground truth, especially on operations, with only
differences at argument-level specifications, which
is expected as we focus on clarifications on opera-
tions. However, the task is challenging as the top
5 ranked CQs do not include CQs in the reference
CQAs, leading to the pipeline prediction including
a call of confusion matrix but missing AdaBoost-
Classifier andcross_val_predict .
7 Related Work
CQ generation. Aliannejadi et al. (2019, 2021)
define CQs based on facets/aspects of the text in-
put’s topic, guiding annotators to write CQs based
on the facet information. Eberhart and McMil-
lan (2022) ask CQs for query refinement based on
facets/aspects from existing NLDs in a dataset. Our
work is distinguished from the above works as our
method does not require a predefined collection of
facets/aspects of the text inputs. The advantage of
our method is that we collect NLDs as specifica-
tions from code.
More generally, two main focuses of work on
CQ generation are (i) disambiguation of terms (Xu
et al., 2019; Guo et al., 2021) and (ii) providing
more information (Rao and Daumé III, 2018; Guo
et al., 2021; Majumder et al., 2021; Nakano et al.,
2022). With the goal of disambiguation of terms,
Xu et al. (2019) utilize the knowledge base to cre-
ate CQs that disambiguate different entities that
share the same entity names. Guo et al. (2021)
included CQs of coreference resolution that dis-
ambiguate pronouns. Rao and Daumé III (2018);
Guo et al. (2021) define CQs to gather information
missing from textual input. Majumder et al. (2021)
ask CQs on missing information from the item de-
scription but existing in similar items, defined as
missing schema. Nakano et al. (2022) construct
pseudo-CQs by eliminating a part of a sentence
and transforming it into a CQ and a corresponding
answer. Our work adopts the definition of CQs as
asking for new information and is distinguished
from these works by defining a new type of in-
formation as key operations for a code, which are
challenging to be defined and identified if they are
included in the original text query.
Text-to-Code generation. Text-to-code genera-
tion was first defined through learning on the par-
allel corpus of NLD-Code pairs (Allamanis et al.,
2015; Miceli Barone and Sennrich, 2017; Yin et al.,
2018). To study programming in practice with
dependency between different code snippets, Iyer
et al. (2018) introduced a more challenging task
that studies generation based on NLD and pro-
gramming context. Agashe et al. (2019) address
the task of generating code cells on Jupyter Note-
book given previous markdown and code cells.14294
Our work also sources NL-Code pairs collected
from Jupyter Notebooks (Liu et al., 2021). We
do not consider dependency between different
code/markdown cells when creating CQA, because
including previous cells will change the necessity
of asking some CQs and make our CQA creation
method less controllable.
Recent research also focuses on generating code
utilizing API knowledge or existing source code.
Xu et al. (2020) augment data with samples cre-
ated by documentation. Parvez et al. (2021) re-
trieve samples from the training set or an archived
code database. Zhou et al. (2022) use retrieval-
augmented generation approach by retrieving docu-
mentation from source code API usage. In contrast,
we design the task of retrieving CQs and consider
interactivity between the model and the user.
8 Conclusion and Future Work
In this paper, we introduced a new challenge of
asking clarification questions for code generation
for Python, along with a method to generate a
dataset to create clarification questions and answers
that do not require human annotations over the
whole dataset. We release our collected dataset
CodeClarQA, which consists of clarification ques-
tions and answers on API usage. We further pro-
posed a pipeline system implemented by recent textand code encoders to evaluate model performances
on this challenge. Our experimental results confirm
that clarification questions and answers are strong
information-gathering methods for better genera-
tion of code while deciding when to ask clarifica-
tion questions and what questions to ask remains
challenging. Future works include improving clari-
fication questions for higher user engagement and
question diversity; studying the lack of user intent
completeness beyond the level of operations, e.g.,
lack of user intent completeness in arguments; and
introducing conversational relations between clari-
fication questions.
Limitations
Our method primarily focuses on operation-level
specifications, while there are real-world use cases
with other specifications. Moreover, our method
of creating CQAs can only be scaled to all Python
codes that involve heavy API usage. However, if a
similar code knowledge graph generator of another
language is developed, our method can also be
scaled to the corresponding language. Our method
is also limited in identifying specifications missing
from the NLD, suggesting potential future work to
create CQs about specifications “mentioned but not
specified enough” in the NLD.14295Ethical Concerns
One concern about the data is the issue of copy-
right. Liu et al. (2021) have checked the data policy
of all 20 Kaggle competitions, in which none has
copyright issues. Furthermore, they have contacted
Kaggle’s administrator and have made sure that
the dataset collection procedure did not violate the
platform’s policy. We also check the license of
open-source APIs when collecting documentation
and make sure that there is no concern about copy-
right issues. Another concern about the data is that
it might include privacy data. Again, we think that
our data has a minimum risk of leakage of data
with privacy concerns since we only collect data
from the 20 Kaggle competitions where there is no
concern of privacy data. The API documentation
also has the minimum risk of containing data with
privacy concerns.
Acknowledgements
We thank Xuye Liu and Dakuo Wang for providing
the original dataset and source code for dataset
preprocessing. We thank Nico Daheim, Ben Peters,
Mert Tiftikci, Kexin Wang, Imbesat Hassan Rizvi,
Dominic Petrak for their valuable feedback and
suggestions on a draft of this paper. We thank
the anonymous reviewers for their detailed and
insightful comments.
This work has been funded by the LOEWE
Distinguished Chair “Ubiquitous Knowledge Pro-
cessing” (LOEWE initiative, Hesse, Germany), by
EU’s Horizon Europe Research and Innovation Ac-
tions (UTTER, contract 101070631), and by the
Fundação para a Ciência e Tecnologia through con-
tract UIDB/50008/2020.
References142961429714298Appendix
A Procedure of Identifying Key
Operation.
We present our procedure for identifying key oper-
ations in Algorithm 1 as a detailed description of
§2.1. Given an NLD-Code pair and all source codes
from its corresponding notebook, our method first
extracts operations for the entire notebook and se-
lects operations corresponding to the code from the
NLD-Code pair. We then identify key operations
by keeping (i) operations from the same API sub-
module that have the shortest data flow path and
(ii) operations that correspond to the last operation
within the same line. Note that we also filter out
operations that (i) are print functions, (ii) are nu-
merical operations, and (iii) have no corresponding
documentation.
B Preliminary Experiments on
Identifying Missing Key Operations
We also considered code/documentation-trained
models for computing similarities preliminarily.
We experimented with RFLHU-BERTOverflow
(Abdelaziz et al., 2022), which is trained on
documentation-StackOverflowPosts pairs and per-
forms similarly to the publicly unavailable RFLHU-
CodeBERT in Abdelaziz et al. (2022). We obtained
75.59, 57.14, 55.56, and 56.34 in accuracy, preci-
sion, recall, and F1. This is substantially lower
than all the results from Table 2.
C Example of Identifying if an Key
Operation is Missing
We present an example of identifying if a key oper-
ation is missing figure 3. Given the key operations
we have extracted (Figure 1b), we identify if a key
operation is missing by comparing all its schema
elements with schema elements of the NLD.
D Examples of Error Types
We analyzed predictions of MPNetqa-cos text
encoder using our annotations. Table 12 shows
examples of all types of FP and FN predictions
we categorize. We also present in Table 13 the
statistics of all FP and FN predictions.
E Annotation
We asked two Ph.D. students to annotate 200 NLD-
Code pairs, respectively. It takes a volunteer about14299Algorithm 1 Procedure of Extracting Key Operations
2 hours to annotate. We show the guide in figure 4
and an example of annotation figure 5.
Discrepancy of annotation between development
and test set. We noticed the discrepancy of Fleiss
Kappa between the development and test set. We
then asked annotators to provide reasons for differ-
ent annotations. As a result, subjectivity is the main
reason for differences between annotations. An ex-
ample is shown in figure 5, where fitting the model
is not directly mentioned yet can be inferred from
the NLD. We also find that the test set containsmore examples like this one, leading to a discrep-
ancy of Fleiss Kappa between the development and
the test set. We accept this difference as subjec-
tivity is part of deciding whether an operation is
mentioned .
F Examples of CodeClarQA Dataset
We present examples from our dataset in Table 14.1430014301143021430314304ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
No section number, "limitations" section (page 8 and 9)
/squareA2. Did you discuss any potential risks of your work?
Section 5.1
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
No section number, "abstract" (page 1) and "introduction" (page 1,2)
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 2.1, 2.4
/squareB1. Did you cite the creators of artifacts you used?
Section 2.1, 2.4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 2.1, 2.4, "Ethical Concerns" section (page 9)
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 2.1, 2.4
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No section number, "Ethical Concerns" section (page 9)
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 2.1, 2.4, "Ethical Concerns" section (page 9)
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 2.4
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 414305/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 5
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4.1
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix D
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 4.1
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Section 4.1
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. The only requirement for annotators is that they are experts in coding python.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
There is no need to do it. The only requirement is that they are experts in coding python.14306