
Yihong Liu, Haris JabbarandHinrich SchützeDepartment of Informatics, Technical University of MunichCenter for Information and Language Processing, LMU Munich
yihong.liu@tum.de
jabbar@cis.lmu.de
Abstract
In this work, we propose a flow-adapter ar-
chitecture for unsupervised NMT. It leverages
normalizing flows to explicitly model the distri-
butions of sentence-level latent representations,
which are subsequently used in conjunction
with the attention mechanism for the transla-
tion task. The primary novelties of our model
are: (a) capturing language-specific sentence
representations separately for each language
using normalizing flows and (b) using a simple
transformation of these latent representations
for translating from one language to another.
This architecture allows for unsupervised train-
ing of each language independently. While
there is prior work on latent variables for super-
vised MT, to the best of our knowledge, this is
the first work that uses latent variables and nor-
malizing flows for unsupervised MT. We obtain
competitive results on several unsupervised MT
benchmarks.
1 Introduction
Recent advances in deep learning have boosted the
development of neural machine translation (NMT).
Typical NMT models leverage an encoder-decoder
framework (Cho et al., 2014; Sutskever et al., 2014).
However, NMT models have been shown to be data-
hungry, as the number of parallel sentences signif-
icantly influences the performance (Zoph et al.,
2016). Unfortunately, large-scale bilingual corpora
are limited to a relatively small subset of languages
(Al-Onaizan et al., 2002). In contrast to bilingual
corpora, monolingual corpora are much easier to
obtain.
Unsupervised NMT, compared with supervised
NMT, aims to train a model without parallel data.
Some early works (Irvine and Callison-Burch,
2016; Sennrich et al., 2016b; Cheng et al., 2016)
used monolingual corpora to boost performance
when parallel data is not abundant. Lample et al.
(2018a) and Artetxe et al. (2018) explored the pos-
sibility of training a model relying only on mono-Figure 1: Inference pipeline of proposed flow-adapter
based model for source-to-target translation. The de-
coder also uses the attentional input (shown as the gray
arrow between the encoder and the decoder).
lingual corpora. They both leveraged a shared-
encoder architecture in order to generate universal
representations, trained with techniques such as
initial word-by-word translation through bilingual
dictionaries (Lample et al., 2018b; Artetxe et al.,
2017), denoising auto-encoding (DAE) (Vincent
et al., 2008) and iterative back-translation (BT)
(Hoang et al., 2018). However, Yang et al. (2018)
argued that it is a bottleneck in such shared-encoder
models to use a shared encoder that maps pairs of
sentences of different languages to the same shared
latent space. They proposed to use two indepen-
dent encoders sharing part of their weights and
achieved better results. But all of those aforemen-
tioned approaches trained the translation models
almost from scratch (with only some prior knowl-
edge in the pre-trained embeddings) and therefore
it is hard to further advance their performance.
More recently, with the advance in pre-trained1253models (Peters et al., 2018; Devlin et al., 2019),
researchers have begun to explore the possibil-
ity of using pre-trained models for unsupervised
NMT. Conneau and Lample (2019) extended the
pre-training from a single language to multiple lan-
guages, referred to as cross-lingual pre-training. By
using pre-trained cross-language models (XLMs)
to initialize encoder and decoder, they achieved
good unsupervised MT performance on multi-
ple language pairs. In related work, Song et al.
(2019) proposed masked sequence to sequence
pre-training (MASS), which directly pre-trains a
whole encoder-decoder model. Üstün et al. (2021)
proposed a language-specific denoising-adapter ar-
chitecture to increase the multilingual modeling
capacity of the pre-trained model mBART (Liu
et al., 2020) and used these adapters for multilin-
gual unsupervised NMT. Although these adapters
are trained with monolingual data only, the fine-
tuning step relies on parallel data.
Current NMT frameworks rely heavily on the
attention mechanism (Bahdanau et al., 2015;
Vaswani et al., 2017) to capture alignments. How-
ever, attention-based context vectors can fail to ex-
tract sufficiently accurate sentence-level semantics
and thus result in incorrect translations or transla-
tion ambiguity (Tu et al., 2016; Zhang et al., 2016).
To tackle this issue, several variational frameworks
for modeling the translation process have been pro-
posed (Zhang et al., 2016; Eikema and Aziz, 2019;
Setiawan et al., 2020). These approaches incorpo-
rate sentence-level latent representations into NMT.
A latent representation, in the context of this pa-
per, is a fixed-size continuous vector from an un-
known distribution that captures the semantics of a
source sentence. The target sentence is then gener-
ated from this latent representation using a simple
transformation along with the attention mechanism
commonly found in transformer architectures. In
this way, when the attention mechanism learns in-
correct alignments, the latent representation plays
a complementary role in guiding the translation.
Prior work in this vein has only been conducted
in supervised NMT. In this paper, we propose a
flow-adapter architecture for unsupervised NMT.
Similar to variational methods, we model the distri-
bution of sentence-level representations. However,
unlike variational methods, which model the dis-
tribution in an implicit way, we use a pair of nor-
malizing flows to explicitly model the distributions
of source and target languages. Secondly, differentfrom some previous unsupervised NMT models
that assume that the representations of source and
target sentences share a common semantic space,
we assume the representations are different because
of language-specific characteristics. Hence they
are modeled separately for each language. Subse-
quently a simple transformation converts source
representations into target representations. This
makes it possible to better capture sentence se-
mantics in a language-specific manner. Lastly, in-
stead of minimizing KL loss, the flows are directly
trained by maximum likelihood estimation (MLE)
of sentence-level latent representations. This gives
the latent representations more flexibility.
Our main contributions:
(1) We propose a novel flow-adapter architecture. It
uses normalizing flows to explicitly model the dis-
tributions of sentence-level representations and per-
forms a latent representation transformation from
source to target. To the best of our knowledge, this
is the first work that uses latent variables and nor-
malizing flows for unsupervised NMT.
(2) Experiments show the validity and effective-
ness of our flow-adapter architecture. It performs
very well in unsupervised NMT on several lan-
guage pairs on the Multi30K dataset. When addi-
tionally using pre-trained models, we achieve re-
sults competitive with the state of the art on WMT
datasets, especially for en-fr (WMT’14) and en-ro
(WMT’16).
2 Background
2.1 Normalizing Flows
Normalizing flows (NFs) are a special type of deep
generative model. Different from generative adver-
sarial networks (GAN) (Goodfellow et al., 2014)
and variational auto-encoding (V AE) (Kingma and
Welling, 2014), NFs allow for not only sampling
but also exact density estimation. Due to such de-
sirable properties, in recent years, they have been
successfully applied to fields such as image (Ho
et al., 2019; Kingma and Dhariwal, 2018), audio
(Esling et al., 2019; van den Oord et al., 2018) and
video generation (Kumar et al., 2019). In addition
to significant achievements in modeling continu-
ous data, NFs have also been used for modeling
discrete data, either by directly modeling the data
in discrete space (Tran et al., 2019; Hesselink and
Aziz, 2020) or by transforming the discrete data
into continuous space (Ziegler and Rush, 2019;
Tang et al., 2021).1254NFs transform between two distributions based
on the following change-of-variables formula (we
follow the introduction of (Dinh et al., 2015, 2017)):
logp(x) = log p(z)+logdet∂f(z)
∂z
(1)
where z∼p(z)andx∼p(x)denote two vec-
tors from a simple latent distribution p(z)and a
complex distribution of the observed data p(x),
fis an invertible and differentiable function (neu-
ral network with parameters θ),f(z) =xand
detdenotes the determinant of the Jacobian
matrix of f. The idea of NFs is to learn an fsuch
thatfandftransform between the latent space
p(z)and the observed space p(x).
Constructing a single arbitrarily complex invert-
ible and differentiable function is usually cumber-
some. Therefore, a generally adopted approach
is to stack multiple transformations ftogether,
i.e.,x=f(z) =f◦ ··· ◦ f(z). Similarly,
for the reverse direction we have z=f(x) =
f◦ ··· ◦ f(x), whose Jacobian matrix is effi-
cient to compute. Here Kdenotes the number of
sequential flows (e.g., K= 3in Table 1).
Normalizing flows are usually optimized by
MLE of the parameters θ, i.e., logp(D|θ) =Plogp(x|θ), where Nis the data size.
By applying a variant of the change-of-variable
formula in Equation (1), i.e., logp(x) =
logp(f(x)) + logdet, the MLE ob-
jective can be reformulated as follows:
logp(D|θ) =Xlogp(f(x)|θ)
+ logdet∂f(x)
∂x|θ(2)
2.2 Latent-variable (variational) NMT
Compared with standard encoder-decoder based
NMT models, latent-variable (variational) ap-
proaches (Zhang et al., 2016; Eikema and Aziz,
2019; Ma et al., 2019; Calixto et al., 2019; Seti-
awan et al., 2020; Shu et al., 2020) additionally
leverage latent random variables.
Letxbe a sentence from the source language
andybe its translation in the target language. Then,
the variational NMT framework introduces a con-
tinuous random latent variable zfor the translation
modeling, i.e., p(y|z,x). With the introduction ofz, the conditional probability p(y|x)can then be
reformulated as follows:
p(y|x) =Zp(y|z,x)p(z|x)dz (3)
In this way, zserves as a global semantic signal
that is helpful to counteract incorrect alignments
the model has learned and uses through attention.
However, the integration of zposes challenges for
inference. To address this problem, variational
NMT adopts techniques from V AE (Kingma and
Welling, 2014; Rezende et al., 2014), namely, neu-
ral approximation and the reparameterization trick.
Neural approximation leverages a neural net-
work to approximate the posterior distribution
p(z|x,y)withq(z|x,y), where ϕdenotes the
parameters of the neural network. In most works,
q(z|x,y)is designed as a diagonal Gaussian
N(µ,diag(σ)), where the mean µand the vari-
anceσare parameterized with neural networks.
Reparameterization means that the latent random
variable zis parameterized as a function of the
meanµand the variance σ. In this way, the gradi-
ent with respect to the parameters µandσcan be
computed. The reparameterization of zis often car-
ried out in a location-scale manner: z=µ+σ⊙ϵ
where ϵ∼ N(0,1)
With these two techniques, the learning objective
of variational NMT is the evidence lower-bound or
ELBO of the conditional probability p(y|x):
L(θ, ϕ;x,y) =−KL(q(z|x,y)||p(z|x))
+E[logp(y|z,x)](4)
where p(z|x)is the prior distribution modeled
by a neural network and p(y|z,x)is modeled
by the decoder given the input source sentence x
and the latent variable z. The KL term minimizes
the discrepancy between the prior p(z|x)and the
posterior q(z|x,y). In the inference step, zcan
therefore be sampled from the prior, which only
requires xinstead of the posterior that requires
bothxandy. Although this variational framework
leverages latent variables, which are helpful for
translation, it still has some flaws: 1)training a
variational NMT framework requires parallel cor-
pora to construct the posterior q(z|x,y) and such
parallel corpora are not available for unsupervised
MT; 2)the distribution family of the latent vari-
ables, e.g., p(z|x), is pre-defined, e.g., a Gaussian,
which might restricts the advantage of using a com-
plex posterior; 3)as variational NMT leverages z1255sampled from p(z|x)for inference, an underlying
assumption is that zshould be the same whether
onlyxis considered or both xandyare consid-
ered. In other words, this framework assumes zis
language-agnostic, which might not be true since
language-specific characteristics can influence the
generation of z.
3 Flow-Adapter Based Framework
In this work, we want to reap the benefits of in-
troducing latent variables into unsupervised MT
while at the same time avoiding the flaws of vari-
ational NMT we just discussed. Therefore, we
propose a flow-adapter based framework that uses
two NFs to explicitly model the distribution of the
sentence-level latent representations of the source
and target sentences. In this way, we can take ac-
count of multilinguality in unsupervised MT and
make use of language-specific sentence-level rep-
resentations. During the translation process, a la-
tent code transformation is performed to transform
the source-language representation into the target-
language representation so that the decoder can
leverage them to generate a better target-language
sentence. We will first introduce the sentence-level
representation as well as the latent code transforma-
tion in Section 3.1, followed by the description of
the flow-adapter based framework for unsupervised
MT in Section 3.2.
3.1 Modeling Representation by NFs &
Latent Code Transformation
As previously mentioned, variational methods such
as (Zhang et al., 2016; Setiawan et al., 2020) as-
sume that the semantics of the source sentence x
and target sentence yare the same and thus the
generated latent variable zis the same regardless
of whether we only consider xor consider both
xandy. Unsupervised NMT methods such as
(Lample et al., 2018a; Conneau and Lample, 2019)
similarly assume that a shared encoder maps source
and target sentences into a shared latent space.
In this work, however, we diverge from this as-
sumption and follow Yang et al. (2018) in adopt-
ing the desideratum that the unique and internal
characteristics of each language be respected. One
could think that the semantics of a pair of sentences
should theoretically be the same; but in reality, be-
cause of language-specific characteristics, the la-
tent representations zobtained by an encoder can
be different for source and target sentences. Differ-ences in vocabulary, pragmatics and other linguistic
properties all influence the generation of the latent
representations. Therefore, we consider the latent
representations from a different perspective as fol-
lows. We can view zandzas expressions of the
sentence-level representations in two distinct lan-
guages based on the same semantics ϵwhere ϵis
truly language-agnostic. zandzare obtained by
applying parameter-free techniques such as pooling
to the output of the encoder fed with source and
target languages (see Section 3.2 for details).
Modeling by NFs. For our unsupervised sce-
nario, we propose to explicitly model the distri-
butions of the sentence-level representations of
both source and target sentences – i.e., p(z)
andp(z)– using NFs with Ksequential flows:
p(z) =p(ϵ)Ydet∂f(z)
∂z
(5)
p(z) =p(ϵ)Ydet∂f(z)
∂z
(6)
where p(ϵ)is a base distribution, e.g., the stan-
dard normal distribution; fandfare the i
transformations for the source and target languages,
respectively; and zis the intermediate variable
where we define z=ϵandz=zorzfor
notational convenience. The base distribution can
be viewed as the “true” underlying semantic space,
abstracting away from language specifics.
Our transformation to the sentence-level repre-
sentations is similar to (Li et al., 2020). They ar-
gued that BERT induces a non-smooth anisotropic
semantic space of sentences, which can harm
its accurate representation of semantic similarity.
Therefore, they also used NFs to transform the
anisotropic BERT sentence-level distribution to a
standard Gaussian distribution that is smooth and
isotropic and reported better performance on some
sentence-level similarity tasks. By using this type
of sentence-level representation, the semantics of
sentences from different languages can therefore
be aligned in a simple common space in an un-
supervised way, which we show is effective for
unsupervised MT.
For simplicity, we denote the NFs for transform-
ing the distributions of source and target sentence-
level representations to the base distribution as
mappings GandG. Because of the1256
invertibility property of NFs, these mappings are
also invertible, and we have G=G
andG=G.
Latent Code Transformation. Inspired by
AlignFlow (Grover et al., 2020), we consider the
cross-domain transformation between zandz.
In this way, we can formulate a language-specific
latent code for the decoder. We formalize the
cross-language latent code transformation from the
source to the target language as follows:
G=G◦G (7)
The target-to-source latent code transformation is
then the composition of GandG. As
GandGare the inverse mappings of
GandG, we can easily obtain them
with normalizing flows, such as realNVP (Dinh
et al., 2017) and Glow (Kingma and Dhariwal,
2018). We also note that GandG
are both invertible since they are compositions of
two invertible mappings. Moreover, Gis
the inverse of Gand vice versa (see Ap-
pendix A.1 for details).
3.2 Flow-Adapter Based Unsupervised
Machine Translation
The general architecture is shown in Figure 1. The
transformer architecture (Vaswani et al., 2017) is
used for both encoder and decoder. We use source
encoder/decoder to denote the encoder/decoder
for encoding/generating the source-language sen-
tence. Similarly, target encoder/decoder refer to
the encoder/decoder encoding/generating the target-
language sentence. The decoders work in an autore-
gressive way. Source flow and target flow are NFsfor modeling the sentence-level latent representa-
tions of the source and target language, respectively,
as introduced in Section 3.1.
Encoding. The source encoder and the tar-
get encoder work in the same way; for brevity,
we only describe the procedure of encoding the
source sentence and how zis generated. The
source encoder takes the source sentence x=
{x,···, x}as input and generates the hidden
representations {h,···,h}. These hidden rep-
resentations will be used as encoder-decoder at-
tentional inputs. In addition, we use the hidden
representations to generate a sentence-level rep-
resentation for the source sentence by applying
max-pooling and mean-pooling to the token-level
representations. After that, we sum up the results
with the CLS representation h, which usually en-
codes some global information. Finally, we use a
projection matrix Wto project the resulting vec-
tor to a latent space. The output is referred to as
z, i.e., the sentence-level representation of the
source sentence (see Appendix A.2 for equation
and illustration).
Cross-lingual Translation. We hypothesize that
the decoder can better leverage language-specific
latent representations (i.e., zfor the source de-
coder and zfor the target decoder) than indis-
criminately using the same representational space
for source and target, e.g., zfor the target de-
coder. Therefore, we propose to perform a latent
code transformation for cross-language translation
as shown in Figure 1. If the model is perform-
ing the translation in the source-to-target direction,
the source flow first transforms the source latent
representation zintoϵ, which is a vector in the1257semantic base space. Then the target flow trans-
forms ϵback into z, which is in the target latent
representation space. Then zis used in the target
decoder for generating the target sentence.
Denoising Auto-Encoding (DAE) and Back
Translation (BT) Processes. The DAE recon-
structs a sentence from its noised version. For
inducing noise, we use the same strategy which is
used by (Lample et al., 2018a) (For more details,
please refer to Appendix A.3). Since we train the
DAEs separately for source and target languages,
hence we don’t need a latent code transformation
there. For BT, however, such a latent code transfor-
mation is performed twice; taking BT for the source
language as an example: first in the source-to-target
direction, then in the target-to-source direction as
shown in Figure 2.
Decoding. To enable the decoder to capture the
global semantics and mitigate improper alignments,
we use the procedure outlined in (Setiawan et al.,
2020), and incorporate the latent representation
zinto the output of the last layer of the decoder
{s,···,s}:
o= (1−g)⊙s+g⊙z (8)
where g=σ([s;z]),σ(·)is the sigmoid function,
⊙denotes Hadamard product between two vectors,
andois the logit vector used to generate a predic-
tion at the iposition. The values in gcontrol
the contribution of ztoo. In case the dimension
of the latent representation does not match the di-
mension of the decoder output, a linear projection
mapszto the desired dimension.
Training. Our flow-adapter framework has three
learning objectives: DAE, BT and MLE of the
sentence-level representations. The description of
DAE and BT is omitted here as they are well known
from related work (Lample et al., 2018a; Artetxe
et al., 2018). A single training iteration consists of a
DAE step followed by a BT step as shown in Figure
2. MLE computation is integrated into the DAE
step to calculate the likelihood of the sentence-level
representations. Our MLE learning objective for
the source monolingual dataset can be formulated
as follows (similar for the target dataset, omitted):
L(G) =E[logp(z)] (9)
where
p(z) =p(G(z))det∂G
∂z
(10)by definition of the source NFs in Equation
5.Eis approximated via mini-batches of
sentence-level latent representations generated by
the encoder in the training process. By training
the source flow and the target flow with this MLE
loss, the flows can therefore transform between
the language-specific latent space of the represen-
tations and the base semantic space. In this way,
the latent code transformations, i.e., Gand
Gcan be constructed.
4 Experiments
4.1 Datasets
Multi30K task1 dataset (Elliott et al., 2016,
2017).This is a multi-modal dataset that has
30,000 images annotated with captions in English,
German and French. Similar to (Lample et al.,
2018a), we only use the caption of each image.
The officially provided train, validation and test
sets are used. We use this dataset as a small-scale
test for validating the effectiveness of our methods.
WMT datasets.Our experiments are run with
the settings that were used for XLM (Conneau and
Lample, 2019). XLM uses the monolingual data
from the WMT News Crawl datasets. We report
results on newstest2014 en-fr ,newstest2016 en-de
andnewstest2016 en-ro .
4.2 Setups
Preprocessing. We tokenize the sentences with
the Moses script (Koehn et al., 2007). For the
Multi30K dataset, we process it similar to Lam-
ple et al. (2018a). Specifically, the sentences are
randomly divided into two parts. The source-
language monolingual dataset is built from the
source-language sentences in the first part and the
target-language dataset from the second part. In
this way, there will be no exact translations of any
sentences in the datasets. For the WMT datasets,
we use the preprocessing methods from (Conneau
and Lample, 2019). For the English-Romanian
dataset, we remove the diacritics as done by Sen-
nrich et al. (2016a) to avoid their inconsistent usage
in the Romanian part of the dataset.
Metric & Performance. We use BLEU as met-
ric (Papineni et al., 2002) for all our experiments.
Although Artetxe et al. (2020) recommended to use1258
unsupervised validation criteria for systematic tun-
ing, we follow the setting of (Conneau and Lample,
2019; Song et al., 2019) and use the provided paral-
lel validation sets for tuning hyperparameters. We
report the results on the test sets of the models that
achieve best performance on the validation sets.
Pre-trained Embeddings & Models. We use
the pre-trained MUSE(Lample et al., 2018b) em-
beddings for the multilingual unsupervised MT ex-
periment (Table 1). We also leverage pre-trained
cross-lingual models in the experiment of shared &
separate decoder(s) (Table 2). Specifically, XLM
models from HuggingFace(Wolf et al., 2020) are
used to initialize the encoder. Moreover, we also
incorporate our flow-adapter architecture directly
into the codebase of the original implementation
of XLMfor the WMT dataset experiment (Table
3). In this case, the encoder and decoder are both
initialized with pre-trained models. Details of these
models can be found in Appendix A.3.
4.3 Results of Multilingual Unsupervised
Machine Translation on Multi30K
As Multi30K provides parallel test data for English,
French and German, we first conduct experiments
to show the multilingual translation ability of our
flow-adapter models. The results are shown in Ta-
ble 1. The baseline model (without flow-adapter
architecture) is trained with only DAE loss, while
the flow-adapter based models (3-scf and 3-glow)
are additionally trained with MLE loss for the NFs.
3-scf (resp. 3-glow) is the baseline model with two
realNVP NF models (Dinh et al., 2017) (resp. Glow
NF models (Kingma and Dhariwal, 2018)) , each
of which consists of 3 sequential flows. Each NF
model is used to model the sentence-level represen-tations of one specific language, and two NF mod-
els then construct a flow-adapter for that translation
direction (as shown in Figure 1). The flow-adapter
based models additionally perform the latent code
transformation to generate a language-specific rep-
resentation while the baseline model does not per-
form such a transformation.
For this experiment, we use the pre-trained cross-
lingual word embeddings (MUSE embeddings) and
randomly initialize a shared encoder and a shared
decoder for all three languages. It is worth noting
that the training objective does not contain the iter-
ative back-translation. For further research where
there are far more languages accommodated, ran-
dom online back-translation (ROBT) proposed by
Zhang et al. (2020) could be considered.
Table 1 shows improvements over all six transla-
tion directions by using the flow-adapter architec-
ture. Notably, our 3-scf and 3-glow models achieve
19.83 and 20.14 BLEU scores, respectively, on de-
en, which is 0.52 and 0.83 higher than the baseline
model. Similar improvements can also be seen on
other translation directions. Our 3-scf model has
BLEU scores that are 0.46 to 0.88 higher than the
baselines while our 3-glow model has BLEU scores
that are 0.04 to 0.83 higher than the baselines. The
overall improvements show that the flow-adapter
can generate more suitable sentence-level represen-
tations by performing the latent code transforma-
tion, which is helpful for the decoder to capture the
semantics and generate more suitable translations.
We also find that the translation performance is
closely related to the language pair and the trans-
lation direction for both the baseline models and
flow-adapter models. Our models obtain very good
performance on en-fr , with performances in both
theen-fr orfr-en directions better by 16 BLEU
points. For other language pairs (including en-
fr), there is always one direction showing better
performance than the other. Specifically, de-en
achieves more than 19 BLEU points compared with
12 points for en-de , and de-fr achieves more than
11 BLEU points compared with 8.5 for fr-de .
4.4 Results of Shared-Decoder &
Separate-Decoder Models on Multi30K
We present the performance of our flow-adapter
models under the shared-decoder and separate-
decoder settings on Multi30K. For this experiment,
the encoder is initialized with the pre-trained XLM
model and fixed; the decoder parameters are ran-1259
domly initialized and then trained. We also report
the performance of a previous SOTA model, i.e.,
UNMT (Lample et al., 2018a).The results are
shown in Table 2. First, we notice that the shared-
decoder baseline model obtains very low BLEU
scores. By checking the translation generated, we
find the model only copies the input as translation.
This phenomenon shows that this baseline, which
does not perform the latent code transformation,
cannot model two languages simultaneously well,
and thus cannot generate translations in the desired
language domains. However, by incorporating the
flow-adapter, the models will no longer have this
limitation. Both shared-decoder models, i.e., 3-scf
and 3-glow, achieve very good performance on all
translation pairs. For example, the 3-scf model
obtains BLEU scores of 25.80, 28.92, 39.26 and
36.84 on en-de ,de-en ,en-fr andfr-en , which are
much higher than the baseline.
Compared with the shared-decoder scenario, the
models under the separate-decoder setting do not
suffer from the copying problem, because different
decoders are used to specifically model and gen-
erate sentences in distinct language domains. The
downside, however, is that using multiple decoders
at the same time can substantially increase the num-
ber of trainable parameters. Within the separate-
decoder models, the flow-adapter models generally
perform better than the baseline model, with about
1 BLEU increase on en-de andde-en directions and
relatively smaller improvements on en-fr andfr-en .
Those improvements demonstrate that the model
can benefit from the flow-adapter architectures as
language-specific latent representations are used,
thus advancing the translation quality.
We also observe that the separate-decoder mod-els generally perform better than the shared-
decoder models. The separate-decoder baseline
is much better than its counterpart as it avoids the
copying problem. For the 3-scf flow-adapter mod-
els, we find that the separate-decoder model out-
performs the shared-decoder model by 2.44, 1.71,
0.38 on en-de ,de-en anden-fr . However, on fr-en ,
the shared-decoder model achieves a BLEU socre
that is by 0.39 BLEU points better. A similar phe-
nomenon can also be seen for the 3-glow model.
We conjecture this is due to the similarity between
languages. As English and French share common
vocabulary, some common features can therefore
be captured by a shared decoder, thus improving
its generalization.
Lastly, when compared with UNMT, our mod-
els show superiority, improving performance by
more than 4 BLEU points in each direction. We
attribute the improvements to the usage of the
pre-trained model and incorporation of language-
specific sentence-level representations obtained by
our latent code transformation.
4.5 Results on WMT datasets
We further integrate our flow-adapter architecture
into the original implementation of XLM (Con-
neau and Lample, 2019) and conduct experiments
on the WMT datasets. To fully leverage the pre-
trained models, we initialize both the encoder and
decoder with XLM models and set them trainable.
In contrast to the experiment in Section 4.4, a single
shared decoder is used for this experiment, since
the decoder is also initialized with the pre-trained
model and has far more parameters compared with
the randomly initialized transformer decoder we
use in Section 4.4. We report the performance
of the flow-adapter based models (5-scf and 5-1260
glow) as well as the performance of the SOTA
models, namely XLM, MASS and CSP.The re-
sults are shown in Table 3. Noticeably, both of
our flow-adapter models achieve remarkable per-
formance on all language pairs. Compared with
the results of XLM (EMD + EMD), which uses
the pre-trained cross-lingual embeddings instead of
pre-trained models, both 5-scf and 5-glow achieve
overall better performance. For example, 3-scf
achieves BLEU scores higher by 5.20, 5.33, 6.61,
5.09, 6.37 and 4.32 on en-de ,de-en ,en-ro ,ro-en ,
en-fr andfr-en , respectively. While not being as
good as 5-scf, 5-glow still shows superiority over
XLM (EMD + EMD). These improvements can be
contributed to (1) the usage of pre-trained models
and (2) the introduction of the flow-adapter.
We further compare our flow-adapter based mod-
els with XLM (MLM + MLM), which is also ini-
tialized with pre-trained models. We find the per-
formance of x-en directions is consistently lower
than en-x directions for our models except for en-
de. This pattern is not limited to our architecture
but is consistently present in prior work. We, again,
speculate this is relating to the complexity of lan-
guages as well as similarity between languages.
We leave this finding for future investigation. Our
flow-adapter based models, though achieving simi-
lar or relatively worse BLEU scores on de-en and
ro-en compared with XLM (MLM + MLM), ob-
tain higher scores on other directions, i.e., en-de
anden-ro , suggesting that our models might be
more helpful on specific translation directions, as
the flow-adapter generates language-specific rep-resentations. Lastly, 5-scf achieves scores by 2.37
and 0.42 better than XLM (MLM + MLM) on en-fr
andfr-en . As in the other experiments, the improve-
ment due to flow adapters seems to be related to the
languages involved in that language pair and the
translation directions. We would like to investigate
this phenomenon in future research.
Finally, out models are competitve with MASS
and CSP, with only small differences in BLEU. In
general, the experiments shows the validity and
effectiveness of our flow-adapter architecture inte-
grated into pre-trained models.
5 Conclusion
In this work, we propose a novel flow-adapter ar-
chitecture for unsupervised NMT. The flow-adapter
employs a pair of NFs to explicitly model the dis-
tributions of the sentence-level representations. A
latent code transformation is performed in transla-
tion, which enables the decoder to better capture
the semantics of sentences in certain language do-
mains. Through extensive experiments, we show
the flow-adapter can improve multilingual transla-
tion ability. Moreover, it can alleviate the copying
problem. By integrating the flow-adapter into pre-
trained XLM models, we achieve results competi-
tive to state-of-the-art models on WMT datasets.
In the future, we would like to explore the pos-
sibility of pre-training the flow-adapter simultane-
ously when pre-training the language models so
that the flows can learn more information. More-
over, we would like to extend normalizing flows to
language generation. By using different flows for
different languages, multilingual language genera-
tion of the same semantics can be performed.
Acknowledgements
We are grateful to Alex Fraser and Alexandra
Chronopoulou for their insightful input. This work
was funded by the European Research Council
(ERC #740516).1261References126212631264
A Appendix
A.1 Proof of the Invertibility
The following proof is based on the proof by Grover
et al. (2020) and shows the source-to-target latent
code transformation is the inverse of the target-to-
source latent code transformation, and vice versa:
G= (G◦G)
=G◦G
=G◦G
=G(11)
A.2 Generation of Sentence-level
Representation
The following formula shows the process of how
the sentence-level representation is generated:
z=Linear (max-pool ([h,···,h])
+mean-pool ([h,···,h])
+h)(12)where the pooling operation generates a vector that
has the same dimension as h, so the three vectors
have the same shape and therefore are additive. An
illustration can be seen in Figure 3.
A.3 Details of the Experiments: Models &
Hyperparameters
A.3.1 Multi30K Experiment
For the multilingual machine translation tasks, we
use the cross-lingual MUSE (Lample et al., 2018b).
The embeddings were learned using fastText(Bo-
janowski et al., 2017) on Wikipedia data and then
aligned in a common space by the method pro-
posed by Lample et al. (2018b). The results shown
in Table 1 is the average over 10 runs on the test
sets. Denosing auto-encoding is used to train the
baseline model. The flow-adapter based (3-scf and
3-glow) models are additionally trained with MLE
loss. We follow the denoising auto-encoding hyper-
parameter settings used by Lample et al. (2018a).
Specifically, word drop and word shuffling are used.
For word drop, every word in a sentence (except
<bos> and <eos>) can be dropped with a proba-
bility p, which we set 0.1 in our experiments.
For word shuffling, a random permutation σis
applied to the input sentence, which satisfy the
condition: ∀i∈ {1, n},|σ(i)−i| ≤k, where i
is the index of a word in the sequence, nis the
length of the sequence and kis a hyperparameter
that controls the degree of the permutation which
we set 3 in our experiments. The dimension of
the pre-trained embedding is 300. The randomly
initialized shared encoder and decoder use trans-
former architecture with 512 hidden units, 4 heads
and 3 layers by default. We use separate embed-
ding layers for each language and tie their weights
with the output layers for each language. The size
of the sentence-level latent representation is set to
100. And the weight of the MLE loss for the flows
is set to 0.01. We use dropout (Srivastava et al.,
2014) probability of 0.2 for the transformers and
0 for the flows. The batch size is set to 32. The
whole model is trained in an end-to-end manner
with Adam optimizer (Kingma and Ba, 2015) with
an initial learning rate of 0.0001.
For the shared-decoder & separate-decoder ex-
periments, we use the pre-trained language models
xlm-mlm-enfr-1024 ,xlm-mlm-ende-1024 ,xlm-mlm-
enro-1024 from HuggingFace(Wolf et al., 2020)1265
to initialize a shared encoder and randomly initial-
ize the decoder(s).using pre-trained models. Denos-
ing auto-encoding and iterative back-translation
are used to train the baseline model. The flow-
adapter based (3-scf and 3-glow) models are addi-
tionally trained with MLE loss. The same denois-
ing auto-encoding hyperparameters as above are
used. For iterative back-translation, greedy decod-
ing is used to generate synthetic parallel sentences
as well as the reconstructions. A single embedding
layer (from the pre-trained model) is used for both
the source and target languages and its weight is
tied with the output layer. The parameters of the
encoder are fixed except for its embedding layer
which is also used by the decoder(s). The size
of the sentence-level latent representation is set
to 256. The pre-trained encoder uses 1024 as the
embedding size and GELU activations (Hendrycks
and Gimpel, 2016), and has 4096 hidden units, 8
heads and 6 layers. The randomly initialized de-
coder has 512 hidden units, 8 heads and 3 layers.
The models are firstly trained with DAE loss (and
MLE loss for flow-adapter models) for the first 3
epochs, then trained with all losses (including the
iterative back-translation) for the rest epochs. The
rest hyperparameters are the same as above.
A.3.2 WMT Experiment
We insert our implementation of flow-adapter archi-
tecture into the codebase of XLMand use the pre-
trained model of en-fr ,en-de anden-ro from them.
We also follow their recommended unsupervised
training settings. For the flow-related hyperparam-
eters, we use 256 as the size of the sentence-level
latent representation. The weight of the MLE loss
is set to 0.01.1266