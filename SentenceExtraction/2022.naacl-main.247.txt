
Hung Le, Nancy F. Chen, Steven C.H. HoiSalesforce Research AsiaSingapore Management UniversityAgency for Science, Technology and Research (A*STAR)
{hungle,shoi}@salesforce.com, nfychen@i2r.a-star.edu.sg
Abstract
Neural module networks (NMN) have
achieved success in image-grounded tasks
such as Visual Question Answering (VQA)
on synthetic images. However, very limited
work on NMN has been studied in the
video-grounded dialogue tasks. These tasks
extend the complexity of traditional visual
tasks with the additional visual temporal vari-
ance and language cross-turn dependencies.
Motivated by recent NMN approaches on
image-grounded tasks, we introduce Video-
grounded Neural Module Network (VGNMN)
to model the information retrieval process in
video-grounded language tasks as a pipeline
of neural modules. VGNMN ﬁrst decomposes
all language components in dialogues to
explicitly resolve any entity references and
detect corresponding action-based inputs from
the question. The detected entities and actions
are used as parameters to instantiate neural
module networks and extract visual cues
from the video. Our experiments show that
VGNMN can achieve promising performance
on a challenging video-grounded dialogue
benchmark as well as a video QA benchmark.
1 Introduction
Vision-language tasks have been studied to build
intelligent systems that can perceive information
from multiple modalities, such as images, videos,
and text. Extended from image-grounded tasks, e.g.
(Antol et al., 2015), recently Jang et al. (2017); Lei
et al. (2018) propose to use video as the ground-
ing features. This modiﬁcation poses a signiﬁcant
challenge to previous image-based models with the
additional temporal variance through video frames.
Recently Alamri et al. (2019) further develop video-
grounded language research into the dialogue do-
main. In the proposed task, video-grounded dia-
logues , the dialogue agent is required to answer
questions about a video over multiple dialogue
turns. Using Figure 1 as an example, to answerFigure 1: A sample video-grounded dialogue with a
demonstration of a reasoning process
questions correctly, a dialogue agent has to resolve
references in dialogue context, e.g. “he” and “it”,
and identify the original entity, e.g. “a boy" and “a
backpack". Besides, the agent also needs to iden-
tify the actions of these entities, e.g. “carrying a
backpack” to retrieve information from the video.
Current state-of-the-art approaches to video-
grounded dialogue tasks, e.g. (Le et al., 2019b;
Fan et al., 2019) have achieved remarkable perfor-
mance through the use of deep neural networks to
retrieve grounding video signals based on language
inputs. However, these approaches often assume
the reasoning structure, including resolving refer-
ences of entities and detecting the corresponding
actions to retrieve visual cues, is implicitly learned.
An explicit reasoning structure becomes more ben-
eﬁcial as the tasks complicate in two scenarios:
video with complex spatial and temporal dynamics,
and language inputs with sophisticated semantic
dependencies, e.g. questions positioned in a di-
alogue context. These scenarios often challenge
researchers to interpret model hidden layers, iden-
tify errors, and assess model reasoning capability.
Similar challenges have been observed in image-
grounded tasks in which deep neural networks ex-
hibit shallow understanding capability as they ex-
ploit superﬁcial visual cues (Agrawal et al., 2016;
Goyal et al., 2017; Feng et al., 2018; Serrano and3377Smith, 2019). Andreas et al. (2016b) propose neu-
ral module networks (NMNs) by decomposing a
question into sub-sequences called program and
assembling a network of neural operations. Moti-
vated by this line of research, we propose a new
approach, VGNMN, to video-grounded language
tasks. Our approach beneﬁts from integrating neu-
ral networks with a compositional reasoning struc-
ture to exploit low-level information signals in
video. An example of the reasoning structure can
be seen on the right side of Figure 1.
Video-grounded Neural Module Network
(VGNMN) tackles video understanding through
action and entity-paramterized NMNs to retrieve
video features. We ﬁrst decompose question
into a set of entities and extract video features
related to these entities. VGNMN then extracts
the temporal steps by focusing on relevant actions
that are associated with these entities. VGNMN is
analogous to how human processes information by
gradually retrieving signals from input modalities
using a set of discrete subjects and their actions.
To tackle dialogue understanding, VGNMN is
trained to resolve any co-reference in language in-
puts, e.g. questions in a dialogue context, to iden-
tify the unique entities in each dialogue. Previous
approaches to video-grounded dialogues often ob-
tain question global representations in relation to
dialogue context. These approaches might be suit-
able to represent general semantics in open-domain
dialogues (Serban et al., 2016). However, they are
not ideal to detect ﬁne-grained information in a
video-grounded dialogue which frequently entails
dependencies between questions and past dialogue
turns in the form of entity references.
In summary, our contributions include:
•VGNMN, a neural module network-based ap-
proach for video-grounded dialogues.
•The approach includes a modularized system
that creates a reasoning pipeline parameter-
ized by entity and action-based representa-
tions from both dialogue and video contexts.
•Our experiments are conducted on the chal-
lenging benchmark for video-grounded dia-
logues, Audio-visual Scene-Aware Dialogues
(A VSD) (Alamri et al., 2019) as well as TGIF-
QA (Jang et al., 2017) for video QA task.
•Our results indicate strong performance of
VGNMN as well as improved model inter-pretability and robustness to difﬁcult scenarios
of dialogues, videos, and question structures.
2 Related Work
2.1 Video-Language Understanding
The research of video-language understanding aims
to develop a model’s joint understanding capa-
bility of language, video, and their interactions.
Jang et al. (2017); Gao et al. (2018); Jiang et al.
(2020) propose to learn attention guided by ques-
tion global representation to retrieve spatial-level
and temporal-level visual features. Li et al. (2019);
Fan et al. (2019); Jiang and Han (2020) model
interaction between all pairs of question token-
level representations and temporal-level features of
the input video through similarity matrix, memory
networks, and graph networks respectively. Gao
et al. (2019); Le et al. (2019c, 2020b); Lei et al.
(2020); Huang et al. (2020) extends the previous
approach by dividing a video into equal segments,
sub-sampling video frames, or considering object-
level representations of input video. We propose to
replace token-level and global question represen-
tations with question representations composed of
speciﬁc entities and actions.
Recently, we have witnessed emerging tech-
niques in video-language systems that exploit deep
transformer-based architectures such as BERT (De-
vlin et al., 2019) for pretraining multimodal rep-
resentations (Li et al., 2020a; Yang et al., 2020;
Kim et al., 2021; Tang et al., 2021; Lei et al.,
2021; Zellers et al., 2021) in very large-scale video-
language datasets. While these systems can achieve
impressive performance, they are not straightfor-
ward to apply in domains with limited data such as
video-grounded dialogues. Moreover, as we shown
in our qualitative examples, our approach facili-
tates better interpretability through the output of
decoded functional programs.
2.2 Video-grounded Dialogues
Extended from video QA, video-grounded dialogue
is an emerging task that combines dialogue re-
sponse generation and video-language understand-
ing research. This task entails a novel requirement
for models to learn dialogue semantics and de-
code entity co-references in questions. Nguyen
et al. (2018); Hori et al. (2019); Hori et al. (2019);
Sanabria et al. (2019); Le et al. (2019a,b) extend
traditional QA models by adding dialogue his-
tory neural encoders. Kumar et al. (2019) en-3378hances dialogue features with topic-level represen-
tations to express the general topic in each dia-
logue. Schwartz et al. (2019) treats each dialogue
turn as an independent sequence and allows inter-
action between questions and each dialogue turn.
Le et al. (2019b) encodes dialogue history as a
sequence with embedding and positional represen-
tations. Different from prior work, we dissect the
question sequence and explicitly detect and decode
any entities and their references. Our approach also
enables insights on how models extract deductive
bias from dialogues to extract video information.
2.3 Neural Module Network
Neural Module Network (NMN) (Andreas et al.,
2016b,a) is introduced to address visual QA by de-
composing questions into linguistic sub-structures,
known as programs, to instantiate a network of neu-
ral modules. NMN models have achieved success
in synthetic image domains where a multi-step rea-
soning process is required (Johnson et al., 2017b;
Hu et al., 2018; Han et al., 2019). Yi et al. (2018);
Han et al. (2019); Mao et al. (2019) improve NMN
models by decoupling visual-language understand-
ing and visual concept learning. Our work is re-
lated to the recent work (Kottur et al., 2018; Jiang
and Bansal, 2019; Gupta et al., 2020) that extended
NMNs to image reasoning in dialogues and reading
comprehension reasoning. Our approach follows
the previous approaches that learn to generate pro-
gram structure and require no parser at evaluation
time. Compared to prior work, we use NMN to
learn dependencies between the composition in lan-
guage inputs and the spatio-temporal dynamics in
videos. Speciﬁcally, we propose to construct a rea-
soning structure from text, from which detected
entities are used to extract visual information in the
spatial space and detected actions are used to ﬁnd
visual information in the temporal space.
3 Method
In this section, we present the design of our model.
An overview of the model can be seen in Figure 2.
3.1 Task Deﬁnition
The input to the model consists of a dialogue D
which is grounded on a video V. The input com-
ponents include the question of current dialogue
turnQ, dialogue history H, and the features of
the input video, including visual and audio input.
The output is a dialogue response, denoted as R.
Each text input component is a sequence of words
w,...,w∈V, the input vocabulary. Simi-
larly, the output response Ris a sequence of tokens
w,...,w∈V, the output vocabulary. The ob-
jective of the task is the generation objective that
output answers of the current dialogue turn t:
ˆR= arg maxP(R|V,H,Q;θ)
= arg max/productdisplayP(w|R,V,H,Q;θ)
whereLis the length of the sequence R. In a
Video-QA task, the dialogue history His simply ab-
sent and the output response is typically collapsed
to a single-token response.
3.2 Encoders
Text Encoder. A text encoder is shared to encode
text inputs, including dialogue history, questions,
and captions. The text encoder converts each text
sequenceX=w,...,winto a sequence of em-
beddingsX∈R. We use a trainable embed-
ding matrix to map token indices to vector represen-
tations ofddimensions through a mapping function
φ. These vectors are then integrated with ordering
information of tokens through a positional encod-
ing function with layer normalization (Ba et al.,
2016; Vaswani et al., 2017). The embedding and
positional representations are combined through
element-wise summation. The encoded dialogue
history and question of the current turn are deﬁned
asH= Norm(φ(H) + PE(H))∈Rand
Q= Norm(φ(Q) + PE(Q))∈R.
Video Encoder. To encode video, we use pre-
trained models to extract visual and audio features.
We denoteFas the sampled video frames or video
clips. For object-level visual features, we denote O
as the maximum number of objects considered in
each frame. The resulting output from a pretrained
object detection model is Z∈R. We
concatenate each object representation with the3379
corresponding coordinates projected to ddimen-
sions. We also make use of a CNN-based pre-
trained model to obtain features of temporal di-
mensionZ∈R. The audio feature is
obtained through a pretrained audio model, Z∈
R. We passed all video features through a
linear transformation layer with ReLU activation
to the same embedding dimension d.
3.3 Neural Modules
We introduce neural modules that are used to as-
semble an executable program constructed by the
generated sequence from question parsers. We pro-
vide an overview of neural modules in Table 1 and
demonstrate dialogue understanding and video un-
derstanding modules in Figure 3 and 4 respectively.
Each module parameter, e.g. “a backpack”, is ex-
tracted from the parsed program (See Section 3.4).
For each parameter, we denote P∈Ras the aver-
age pooling of component token embeddings.
find(P,H)→H. This module handles en-
tity tracing by obtaining a distribution over to-
kens in the dialogue history. We use an entity-to-
dialogue-history attention mechanism applied from
an entityPto all tokens in the dialogue history.
Any neural network that learn to generate attention
between two tensors is applicable .e.g. (Bahdanau
et al., 2015; Vaswani et al., 2017). The attention
matrix normalized by softmax, A∈R, is
used to compute the weighted sum of dialogue his-
tory token representations. The output is combined
with entity embedding Pto obtain contextual en-
tity representation H∈R.
summarize(H,Q)→Q. For each con-
textual entity representation H,i= 1,...,N,
it is projected to Ldimensions and is combined
with question token embeddings through element-
wise summation to obtain entity-aware question
representation Q∈R. It is fed to a one-
dimensional CNN with max-pooling layer (Kim,
2014) to obtain a contextual entity-aware ques-
tion representation. We denote the ﬁnal outputasQ∈R.
While previous models usually focus on global
or token-level dependencies (Hori et al., 2019; Le
et al., 2019b) to encode question features, our
modules compress ﬁne-grained question represen-
tations at the entity level. Speciﬁcally, find
andsummarize modules can generate entity-
dependent local and global representations of ques-
tion semantics. We show that our modularized
approach can achieve better performance and trans-
parency than traditional approaches to encode dia-
logue context (Serban et al., 2016; Vaswani et al.,
2017) (Section 4).
where(P,V)→V. Similar to the find
module, this module handles entity-based atten-
tion to the video input. However, the entity rep-
resentationP, in this case, is parameterized by
the original entity in dialogue rather than in ques-
tion (See Section 3.4 for more description). Each
entityPis stacked to match the number of sam-
pled video frames/clips F. An attention network
is used to obtain entity-to-object attention matrix
A∈R. The attended feature are com-
pressed through weighted sum pooling along the
spatial dimension, resulting in V∈R,
i= 1,...,N.
when(P,V)→V . This module fol-
lows a similar architecture as the where mod-
ule. However, the action parameter Pis stacked
to matchNdimensions. The attention matrix
A∈Ris then used to compute the visual
entity-action representations through weighted sum
along the temporal dimension. We denote the out-
put for all actions PasV∈R
describe(P,V )→V. This module
is a linear transformation to compute V=
W[V ;P]∈Rwhere
W∈R,P is the stacked represen-
tations of parameter embedding PtoN×N
dimensions, and [; ]is the concatenation operation.
Note that the parameter Phere is extracted from
questions, often as the type of questions e.g. “what”3380
and “how”. This eliminates the need to have differ-
ent modules for different question types. However,
we noted the current design may be challenged in
rare cases in which an utterance contain numerous
questions (refer to Figure 5).
Theexist module is used when the questions
are “yes/no” questions. This module is a special
case of describe module where the parameter
Pis simply the average pooled question embed-
dings. The above where module is applied to
object-level features. For temporal-based features
such as CNN-based and audio features, the same
neural operation is applied along the temporal di-
mension. Each resulting entity-aware output is
then incorporated to frame-level features through
element-wise summation.
An advantage of our architecture is that it sepa-
rates dialogue and video understanding. We adopt
a transparent approach to solve linguistic entity ref-
erences during the dialogue understanding phase.
The resolved entities are fed to the video under-
standing phase to learn entity-action dynamics in
the video. We show that our approach is robust
when dialogue evolves to many turns and video
extends over time (Please refer to Section 4).
3.4 Question Parsers
To learn compositional programs, we follow
(Johnson et al., 2017a; Hu et al., 2017) and
consider program generation as a sequence-to-
sequence task. We adopt a simple template
“/angbracketleftparam/angbracketright/angbracketleftmodule/angbracketright/angbracketleftparam/angbracketright/angbracketleftmodule/angbracketright...” as
the target sequence. The resulting target sequences
for dialogue and video understanding programs are
sequencesPandPrespectively.
The parsers decompose questions into sub-sequences to construct compositional reasoning
programs for dialogue and video understanding.
Each parser is a vanilla Transformer decoder, in-
cluding multi-head attention layers on questions
and past dialogue turns (Please refer to Appendix
A.1 for more technical details).
3.5 Response Decoder
System response is decoded by incorporating the di-
alogue context and video context outputs from the
corresponding reasoning programs to target token
representations. We follows a vanilla Transformer
decoder architecture (Le et al., 2019b), which con-
sists of 3 attention layers: self-attention to attend
on existing tokens, attention to Qfrom dialogue
understanding program execution, and attention to
Vfrom video understanding program execution.
A= Attention( R|,R|,R|)∈R
A= Attention( A,Q,Q)∈R
A= Attention( A,V,V)∈R
Multimodal Fusion. For video features come
from multiple modalities, visual and audio, the con-
textual features, denoted V, is obtained through
a weighted sum of component modalities, e.g. con-
textual visual features Vand contextual audio
featuresV. The scores S to compute the
weighted sum is deﬁned as:
S = Softmax( W [Q;V;V])
whereQ is the mean pooling output of ques-
tion embeddings Qwhich is then stacked to N+
Ndimensions, and W∈Rare train-
able model parameters. The resulting S has a
dimension of∈R.3381Response Generation. To generate response se-
quences, a special token “ _sos” is concatenated as
the ﬁrst token w. The decoded token wis then ap-
pended towas input to decode wand so on. Sim-
ilarly to input source sequences, at decoding time
stepj, the input target sequence is encoded to ob-
tain representations of system response R|. We
combine vocabulary of input and output sequences
and share the embedding matrix E∈Rwhere
V=V∩V. During training time, we directly
use the ground-truth responses as input to the de-
coder and optimize VGNMN with a cross-entropy
loss to decode the next ground-truth tokens. During
test time, responses are generated auto-regressively
through beam search with beam size 5. Note that
we apply the same procedure to generate reasoning
programs from question parsers.
4 Experiments
Datasets. We use the A VSD benchmark from
the Dialogue System Technology Challenge 7
(DSTC7) (Hori et al., 2019). The benchmark con-
sists of dialogues grounded on the Charades videos
(Sigurdsson et al., 2016). Each dialogue contains
up to 10 dialogue turns, each turn consists of a ques-
tion and expected response about a given video.
For visual features, we use the 3D CNN-based fea-
tures from a pretrained I3D model (Carreira and
Zisserman, 2017) and object-level features from a
pretrained FasterRNN model (Ren et al., 2015b).
The audio features are obtained from a pretrained
VGGish model (Hershey et al., 2017). In the ex-
periments with A VSD, we consider two settings:
one with video summary and one without video
summary as input. In the setting with video sum-
mary, the summary is concatenated to the dialogue
history before the ﬁrst dialogue turn. We also adapt
VGNMN to the video QA benchmark TGIF-QA
(Jang et al., 2017). Different from A VSD, TGIF-
QA contains a diverse set of QA tasks:
•Count : an open-ended task which counts the
number of repetitions of an action
•Action : a multiple-choice (MC) task which
asks about a certain action occurring for a
ﬁxed number of times
•Transition : an MC task which emphasizes
temporal transition in video
•Frame : an open-ended QA about visual con-
tents of one of the video frames
For the TGIF-QA benchmark, we use the extracted
features from a pretrained ResNet model (He et al.,
2016). Table 2 shows a summary of the A VSD and
TGIF-QA benchmarks.
Training Details. We follow prior approaches
(Hu et al., 2017, 2018; Kottur et al., 2018) by ob-
taining the annotations of the programs through
a language parser (Hu et al., 2016) and a refer-
ence resolution model (Clark and Manning, 2016).
During training, we directly use these as ground-
truth labels of programs to train our models. The
ground-truth responses are augmented with label
smoothing technique (Szegedy et al., 2016). Dur-
ing inference time, we generate all programs and
responses from given dialogues and videos. We run
beam search to enumerate programs for dialogue
and video understanding and dialogue responses.
We use a training batch size of 32 and embed-
ding dimension d= 128 in all experiments. Where
Transformer attention is used, we ﬁx the number
of attention heads to 8 in all attention layers. In
neural modules with MLP layers, the MLP network
is ﬁxed to 2 linear layers with a ReLU activation in
between. In neural modules with CNN, we adopt
a vanilla CNN architecture for text classiﬁcation
(without the last MLP layer) where the number of
input channels is 1, the kernel sizes are {3,4,5},
and the number of output channels is d. We ini-
tialize models with uniform distribution (Glorot
and Bengio, 2010). During training, we adopt the
Adam optimizer (Kingma and Ba, 2015) and a de-
caying learning rate (Vaswani et al., 2017) where
we ﬁx the warm-up steps to 15K training steps. We
employ dropout (Srivastava et al., 2014) of 0.2 at
all networks except the last linear layers of ques-
tion parsers and response decoder. We train models
up to 50 epochs and select the best models based
on the average loss per epoch in the validation set.
All models are trained in a V100 GPU with a
capacity of 16GB. We approximated each training
epoch took about 20 minutes to run. For each3382model experiment with VGNMN, we obtained at
least 2 runs and reported the average results. We
implemented models in Pytorch and released the
code and model checkpoints.
Optimization. We optimize models by joint
training to minimize the cross-entropy losses to
generate responses and functional programs.
L=αL+βL+L
=α/summationdisplay−log(P(P))
+β/summationdisplay−log(P(P))
+/summationdisplay−log(P(R))
wherePis the probability distribution of an out-
put token. The probability is computed by passing
output representations from the parsers and decoder
to a linear layer W∈Rwith softmax activa-
tion. We share the parameters between Wand
embedding matrix E.
A VSD Results. We evaluate model performance
by the objective metrics, including BLEU (Pap-
ineni et al., 2002), METEOR (Banerjee and Lavie,
2005), ROUGE-L (Lin, 2004), and CIDEr (Vedan-
tam et al., 2015), between each generated response
and 6 reference gold responses. As seen in Table 3,
our models outperform most of existing approaches.
We observed that our approach did not outperform
the GPT-based baselines (Li et al., 2020b; Le and
Hoi, 2020) in the setting that allows video sum-
mary/caption input. However, the performance
of our model in the setting without video sum-
mary/caption input is on par with the GPT-based
baseline (Li et al., 2020b), even though our model
did not rely on deep pretrained representations on
large-scale text data. These observations imply
that GPT-based models can better capture video
context from video caption/summary through rich
pretrained representations. However, without ac-
cess to video caption/summary, these models may
fail to understand video from visual-only represen-
tations. In this setting, GPT-based models may be
inferior to VGNMN, which explicitly exploits the
compositional structures from textual inputs to inte-
grate visual features. We also found that VGNMN
applied to object-level features is competitive to
the model applied to CNN-based features. The
ﬂexibility of VGNMN neural programs show when
we treat the caption as an input equally to visual or
audio inputs and execute entity-action level neural
operations on the encoded caption sequence.
Robustness. To evaluate model robustness, we
report BLEU4 and CIDEr of model variants in var-
ious experimental settings. Speciﬁcally, we com-
pare against performance of output responses in the
ﬁrst dialogue turn position (i.e. 2-10turn vs.
the 1turn), or responses grounded on the shortest
video length range (video ranges are intervals of
0-10, 10-20percentile and so on). We report
results of the following model variants: (1) w/o
video NMN : VGNMN without using video-based
modules, e.g. when andwhere . Video features
are retrieved through a token-level representation
of questions (Le et al., 2019b). (2) no NMN : (1) +
without dialogue-based modules, e.g. find and
summarize . Dialogue history is encoded by a
hierarchical LSTM encoder (Hori et al., 2019).
Robustness to video length : In Table 4a, we
noted that the performance gap between VGNMN
and (1) is quite distinct, with 7/10 cases of video
ranges in which VGNMN outperforms. However,
in lower ranges (i.e. 1-23 seconds) and higher
ranges (37-75 seconds), VGNMN performs not3383
as well as model (1). We observed that related
factors might affect the discrepancy, such as the
complexity of the questions for these short and
long-range videos. Potentially, our question parser
for the video understanding program needs to be
improved (e.g. for tree-based programs) to retrieve
information in these ranges.
Robustness to dialogue turn : In Table 4b, we ob-
served that model (1) performs better than model
(2) overall, especially in higher turn positions, i.e.
from the 4turn to 8turn. Interestingly, we
noted some mixed results in very low turn position,
i.e. the 2and 3turn, and very high turn po-
sition, i.e. the 10turn. Potentially, with a large
dialogue turn position, the neural-based approach
such as hierarchical RNN can better capture the
global dependencies within dialogue context than
the entity-based compositional NMN method.
Robustness to question structure : Finally, we
compared performance of VGNMN with the no-
NMN variant (1) in different cases of question
structures: single-question vs. multiple-part struc-
ture. In single-question structures, we examined by
the question types (e.g. yes/no, wh-questions). In
multi-part structures, we further classiﬁed whether
there are sentences preceding the question (e.g.
“1Sent+Que”) or there are smaller (sub-)questions
(e.g. “2SubQue”) within the question. In Table
4c, we observed that VGNMN has clearer perfor-
mance gains in multi-part structures than single-
question structures. In multi-part structures, we
observed higher gaps between VGNMN and model
(1) in highly complex cases e.g. “2Sent+Que”
vs. “1Sent+Que”. These observations indicate
the robustness of VGNMN and the underlying
compositionality principle to deal with complex
question structures. We also noted that VGNMN
is still susceptible to extremely long questions
(“>2Sent+Que”) and future work is needed to ad-dress these scenarios.
Interpretability. In Figure 5, we show both suc-
cess and failure cases of generated responses and
corresponding generated functional programs. In
each example, we marked predicted outputs as in-
correct if they do not match the ground-truth com-
pletely (even though the outputs might be partially
correct). From Figure 5, we observe that in cases
where generated dialogue programs and video pro-
grams match or are close to the gold labels, the3384model can generate generally correct responses.
For cases where some module parameters do not
exactly match but are closed to the gold labels, the
model can still generate responses with the correct
visual information (e.g. the 4turn in example
B). In cases of wrong predicted responses, we can
further look at how the model understands the ques-
tions based on predicted programs. In the 3turn
of example A, the output response is missing a
minor detail as compared to the label response be-
cause the video program fails to capture “rooftop”
as awhere parameter. These subtle yet important
details can determine whether output responses can
fully address user queries. In the 3turn of exam-
ple B, the model wrongly identiﬁes “what room”
as awhere parameter and subsequently generates
a wrong response that it is “a living room”.
TGIF-QA Results. We report the result using
the L2 loss in Count task and accuracy in other
tasks. From Table 5, VGNMN outperforms the
majority of the baseline models in all tasks by a
large margin. Compared to A VSD experiments,
the TGIF-QA experiments emphasize the video un-
derstanding ability of the models, removing the
requirement for dialogue understanding and natu-
ral language generation. Since TGIF-QA questions
follow a very speciﬁc question type distribution
(count, action, transition, and frameQA), the ques-
tion structures are simpler and easier to learn than
A VSD. Using exact-match accuracy of parsed pro-
grams vs. label programs as a metric, our question
parser can achieve a performance 81% to 94% ac-
curacy in TGIF-QA vs. 41-45% in A VSD. The
higher accuracy in decoding a reasoning structure
translates to better adaptation between training and
test time, resulting in higher performance gains.Cascading Errors. Compared to prior ap-
proaches, we noted that VGNMN is a modular-
ized system which may result in cascading errors
to downstream modules. One major error is the
error of generated programs which is used as pa-
rameters in neural modules. To gauge this error,
we compare the performance of VGNMN between
2 cases: with generated programs and with ground-
truth programs. From Table 6, we noticed some
performance gaps between these cases. These ob-
servations imply that: (1) program generations
and response generations are positively correlated
and more accurate programs can lead to better re-
sponses; and (2) current question parsers are not
perfect, resulting in wrong parameters to instantiate
neural modules. Future work may focus on learn-
ing better question parsers or directly deploying a
better off-the-shelf parser tool.
For additional experiment results, qualitative
samples, and analysis between model variants, re-
fer to Appendix B and C.
5 Conclusion
In this work, we introduce Video-grounded Neural
Module Network (VGNMN). VGNMN consists of
dialogue and video understanding neural modules,
each of which performs entity and action-level op-
erations on language and video components. Our
comprehensive experiments on A VSD and TGIF-
QA benchmarks show that our models can achieve
competitive performance while promoting a com-
positional and interpretable learning approach.
6 Broader Impacts
During the duration of this work, there have been
no ethical concerns regarding the model implemen-
tation, training, and testing. The data used in this
work has been carefully reviewed and accordingly
to the description from the original authors, we did
not ﬁnd any concerns on any signiﬁcant biases. For3385any potential application or extension of this work,
we would like to highlight some speciﬁc concerns.
First, as the work is developed to build an intelli-
gent dialogue agents, models should not be used
with the intention to create fake human proﬁles for
any harmful purposes (e.g. ﬁshing or spreading
fake news). For wider use of dialogue systems,
the application of work might result in certain im-
pacts to some stakeholders whose jobs may be af-
fected by this application (e.g. customer service
call agents). We hope any application should be
carefully considered against these potential risks.
References338633873388
A Additional Model Details
A.1 Question Parsers
To learn compositional programs, we follow
(Johnson et al., 2017a; Hu et al., 2017) and
consider program generation as a sequence-to-
sequence task. We adopt a simple template
“/angbracketleftparam/angbracketright/angbracketleftmodule/angbracketright/angbracketleftparam/angbracketright/angbracketleftmodule/angbracketright...” as
the target sequence. The resulting target sequences
for dialogue and video understanding programs are
sequencesPandPrespectively.
The parsers decompose questions into sub-
sequences to construct compositional reasoning
programs for dialogue and video understanding.
Each parser is an attention-based Transformer de-
coder. The Transformer attention is a multi-head
attention on query q, keyk, and valuevtensors, de-
noted as Attention(q ,k,v). For each token in the
qsequence , the distribution over tokens in the k
sequence is used to obtain the weighted sum of the
corresponding representations in the vsequence.
Attention(q,k,v ) = softmax(qk
√d)v∈R3389Each attention is followed by a feed-forward net-
work applied to each position identically. We ex-
ploit the multi-head and feed-forward architecture,
which show good performance in NLP tasks such
as NMT and QA (Vaswani et al., 2017; Dehghani
et al., 2019), to efﬁciently incorporate contextual
cues from dialogue components to parse question
into reasoning programs. At decoding step 0, we
simply use a special token _sos as the input to
the parser. In each subsequent decoding step, we
concatenate the prior input sequence with the gener-
ated token to decode in an auto-regressive manner.
We share the vocabulary sets of input and output
components and thus, use the same embedding ma-
trix. Given the encoded question Q, to decode the
program for dialogue understanding, the contextual
signals are integrated through 2 attention layers:
one attention on previously generated tokens, and
the other on question tokens. At time step j, we
denote the output from an attention layer as A.
A= Attention( P|,P|,P|)
A= Attention( A,Q,Q )∈R
To generate programs for video understanding,
the contextual signals are learned and incorporated
in a similar manner. However, to exploit dialogue
contextual cues, the execution output of dialogue
understanding neural modules Qis incorporated
to each vector in Pthrough an additional atten-
tion layer. This layer integrates the resolved entity
information to decode the original entities for video
understanding. It is equivalent to a reasoning pro-
cess that converts the question from its original
multi-turn semantics to single-turn semantics.
A= Attention( P|,P|,P|)
A= Attention( A,Q,Q )∈R
A= Attention( A,Q,Q)∈R
A.2 How to locate entities?
Noted that in the neural modules described in
Section 3.3, during training, we simply feed the
ground-truth programs to optimize these modules.
For instance, the neural module where received
the ground truth entities Pwhich is then used to in-
stantiate the neural network and retrieve from video
V. During test time, we decode the programs token
by token through the question parsers, and feed the
predicted entities ˆPto neural modules. Note that
we do not assume, and hence not train model toretrieve ground-truth locations of visual entities in
videos. This strategy enables the applicability of
VGNMN as we consider these entity annotations
mostly unavailable in real-world systems.
B Additional Experimental Results
B.1 Non-NMN Models
We experiment with several Non-NMN based vari-
ants of our models. As can be seen in Table 7,
our approach to video and dialogue understand-
ing through compositional reasoning programs ex-
hibits better performance than non-compositional
approaches. Compared to the approaches that di-
rectly process frame-level features in videos (Row
B) or token-level features in dialogues (Row C,
D), our full VGNMN (Row A) considers entity-
level and action-level information extraction and
thus, avoids unnecessary and possibly noisy ex-
traction. Compared to the approaches that obtain
dialogue contextual cues through a hierarchical en-
coding architecture (Row E, F) such as (Serban
et al., 2016; Hori et al., 2019), VGNMN directly
addresses the challenge of entity references in di-
alogues. As mentioned, we hypothesize that the
hierarchical encoding architecture is more appro-
priate for less entity-sensitive dialogues such as
chit-chat and open-domain dialogues.
B.2 Dialogue context integration
Experimenting with different ways to integrate di-
alogue context representations, we observe that
adding an attention layer attending to question dur-
ing response decoding (Row G) is not necessary.
This can be explained as the representation Q
obtained from dialogue understanding program al-
ready contains contextual information of both dia-
logue history and question and question input is no
longer needed in the decoding phase. Furthermore,
we investigate the model sensitivity to natural lan-
guage generation through its ability to construct
linguistically correct programs and responses. To
generate responses that are linguistically appropri-
ate, VGNMN needs dialogue context representa-
tionQas input to the response decoder (Row
H). The model also needs encoded question Qas
input to the video understanding program parser to
be able to decompose this sequence to entity and
action module parameters (Row I).3390
C Interpretability
We extract the predicted programs and responses
for some example dialogues in Figure 6, 7, 8, and
9 and report our observations:
•We observe that when the predicted programs
are correct, the output responses generally
match the ground-truth (See the 1and 2
turn in Figure 6, and the 1and 4turn
in Figure 8) or close to the ground-truth re-
sponses (1turn in Figure 7).
•When the output responses do not match the
ground truth, we can understand the model
mistakes by interpreting the predicted pro-
grams. For example, in the 3turn in Figure
6, the output response describes a room be-
cause the predicted video program focuses on
the entity “what room” instead of the entity
“an object” in the question. Another exam-
ple is the 3turn in Figure 8 where the en-
tity “rooftop” is missing in the video program.
These mismatches can deviate the information
retrieved from the video during video program
execution, leading to wrong output responses
with wrong visual contents.
•We also note that in some cases, one or both
of the predicted programs are incorrect, but
the predicted responses still match the ground-
truth responses. This might be explained as
the predicted module parameters are still close
enough to the “gold” labels (e.g. 4turn in
Figure 6). Sometimes, our model predicted
programs that are more appropriate than the
ground truth. For example, in the 2turn in
Figure 7, the program is added with a where
module parameterized by the entity “the shop-
ping bag” which was solved from the refer-
ence “them” mentioned in the question.•We observe that for complex questions that
involve more than one queries (e.g. the 3
turn in Figure 8), it becomes more challenging
to decode an appropriate video understanding
program and generate responses that can ad-
dress all queries.
•In Figure 9, we demonstrate some output ex-
amples of VGNMN and compare with two
baselines: Baseline (Hori et al., 2019) and
MTN (Le et al., 2019b). We noted that
VGNMN can include important entities rele-
vant to the current dialogue turn to construct
output responses while other models might
miss some entity details, e.g. “them/dishes”
in example A and “the magazine” in example
B. These small yet important details can deter-
mine the correctness of dialogue responses.339133923393