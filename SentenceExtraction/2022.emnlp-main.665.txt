
Dongkyu LeeZhiliang TianYingxiu Zhao
Ka Chun CheungNevin L. ZhangDepartment of Computer Science and Engineering, HKUSTCollege of Computer, National University of Defense TechnologyNVIDIA AI Technology Center, NVIDIA{dleear, yzhaocx, lzhang}@cse.ust.hktianzhilianghit@gmail.comchcheung@nvidia.com
Abstract
In knowledge distillation, a student model is
trained with supervisions from both knowledge
from a teacher and observations drawn from
a training data distribution. Knowledge of a
teacher is considered a subject that holds inter-
class relations which send a meaningful su-
pervision to a student; hence, much effort has
been put to find such knowledge to be distilled.
In this paper, we explore a question that has
been given little attention: “ when to distill such
knowledge ." The question is answered in our
work with the concept of model calibration; we
view a teacher model not only as a source of
knowledge but also as a gauge to detect miscal-
ibration of a student. This simple and yet novel
view leads to a hard gate knowledge distillation
scheme that switches between learning from
a teacher model and training data. We verify
the gating mechanism in the context of natural
language generation at both the token-level and
the sentence-level. Empirical comparisons with
strong baselines show that hard gate knowledge
distillation not only improves model general-
ization, but also significantly lowers model cal-
ibration error.
1 Introduction
In recent years, the deep learning community has
achieved marked performance gains across a va-
riety of tasks (Brown et al., 2020; Devlin et al.,
2018). In the meantime, some deep learning mod-
els have become excessively large, limiting their
applicability in some scenarios. To cope with the
issue, Hinton et al. (2015) proposed knowledge
distillation (KD), in which knowledge of a large
network, called a teacher network, is transferred to
a relatively small model, called a student model.
The benefits of KD have been widely witnessed
across multiple domains (Romero et al., 2015; Jiaoet al., 2020). Recently, it has been observed that
KD can be used in both reducing model size and
improving model generalization (Tang et al., 2021;
Furlanello et al., 2018). Hinton et al. (2015) ar-
gue that a distribution, defined by a teacher, holds
inter-class relations, commonly referred to as the
dark knowledge , and that such distribution brings
a meaningful supervision to a student. There-
fore, a large body of research in KD has viewed a
teacher as a source of knowledge and has focused
onfinding a meaningful knowledge to be trans-
ferred (Romero et al., 2015; Bulò et al., 2016; Park
et al., 2019; Yuan et al., 2020; Kim et al., 2021).
In this work, we focus on when to distill knowl-
edge of a teacher . This is a central question to ask,
as a model can benefit from the adaptive control
of supervision between ground truth and a teacher;
When a model is trained to increase the predictive
score of a prediction, a one-hot encoded supervi-
sion, without incorporating teacher model, sends a
direct signal in increasing the score (Müller et al.,
2019). In another case, when a model is trained
to learn knowledge of a teacher, a teacher’s output
without fusing a ground truth sends more direct
signal in minimizing the knowledge gap between
the student and the teacher. However, the question
of “when" has not been answered. For this reason,
previous works choose to learn from both of the
supervisions.
We give an answer to the question from the
perspective of model calibration. Model calibra-
tion refers to how well a predicted probability of
a model reflects the true accuracy. Therefore, a
well-calibrated predictive score represents the like-
lihood of correctness of a prediction (Guo et al.,
2017). In this light, such score can be viewed as
a gauge to detect a miscalibration of a student in
training; when a student makes a prediction with
a probability mass that is higher than the expected
accuracy of the prediction (overconfidence), a stu-
dent model is trained with only supervision from a9793teacher. In the case of underconfidence, a student
is trained with only supervision from ground-truth.
Switching supervision is supported by two
widely accepted ideas: 1) the close link between
miscalibration and overfitting, and 2) the regular-
ization effect of KD. Guo et al. (2017) empirically
find that a model overfits to negative log likeli-
hood (NLL) training, leading to miscalibration, and
Mukhoti et al. (2020) further support the claim.
Therefore, we utilize the regularization effect held
in KD training (Yuan et al., 2020). Aside from
the inter-class relations held in knowledge, recent
findings suggest that KD is a form of adaptive reg-
ularization (Tang et al., 2021; Yuan et al., 2020),
where a teacher enforces a student to distribute
probability mass on output space more evenly.
Taking all these factors into account, we present
a simple, yet novel KD method, called Hard gate
Knowledge Distillation (HKD). Given a calibrated
teacher model, the teacher gates supervisions be-
tween knowledge and observation for each in-
stance/time step, selecting which objective the stu-
dent should be optimized to. We introduce two lev-
els of hard gates: the token-level and the sentence-
level which are instance-specific hard gates com-
puted on the fly during forward propagation. Our
work validates the proposed idea on a task in the
Natural Language Generation (NLG) domain, as
there is an inseparable relation between the qual-
ity of an output and model calibration (Kumar and
Sarawagi, 2019).
The contributions of the proposed method are as
follows:
•To the best of our knowledge, this work is
the first attempt to leverage knowledge and
observations in KD with a hard gate which is
instance-specific.
•Our work introduces a novel view and role
of a teacher model in student-teacher frame-
work which improve model generalization and
model calibration of a student by a significant
margin across multiple datasets.
2 Preliminaries & Related Work
2.1 Knowledge Distillation
The conventional logit-based KD (Hinton et al.,
2015) aims to minimize the distance between the
probability distribution mapped by a teacher and
that of a student, while another objective is to maxi-
mize the likelihood of predicting ground truth. Fol-lowing is the loss of an instance (x,y)∈ X × Y
at time-step t, where iindicates the index of the
sample.
L=−/summationdisplay(1−α)ylogP(y|c)
+αP(y|c;τ) logP(y|c;τ)(1)
Vandτdenote a set of vocabularies and a temper-
ature respectively. ϕandθdenote parameters of a
teacher and those of a student. αdenotes a balanc-
ing parameter which in this work is termed a gate,
andcis a context at time step t, hence made of
input xand preceding tokens y. The gate is set
to a value between 0 and 1, which indicates a soft
gate, and it is shared among instances and remains
fixed throughout training (Park et al., 2019; Hinton
et al., 2015; Yuan et al., 2020). Therefore, a student
model is trained with a soft target ˜y, a result of
linear interpolation between a ground truth and a
distribution mapped by a teacher.
Numerous studies have attempted to find mean-
ingful knowledge to be distilled. Starting with inter-
class relations on logit space (Park et al., 2019; Hin-
ton et al., 2015), the scope of knowledge expanded
to feature-level (Romero et al., 2015) to encourage
a student to maintain similar intermediate represen-
tations to those of a teacher. Recent studies find
that even a model with an identical model structure
to that of a student can suit the role as a teacher;
thus it is commonly referred to as Self-Knowledge
Distillation (Yuan et al., 2020; Kim et al., 2021; Liu
et al., 2021). (Yuan et al., 2020; Tang et al., 2021)
argue that the success is brought by KD’s close link
to label smoothing (Szegedy et al., 2016), with KD
holding a regularization effect. In this regard, there
have been attempts to explore the importance of
the soft gate. PS-KD (Kim et al., 2021) linearly
increases the value of the gate in the course of train-
ing. Similar to our work, Zhu and Wang (2021)
propose a hard gate mechanism in KD; however the
work utilizes an iteration-specific hard gate, and
the gates only apply to distillation loss of KD.
2.2 Calibration
A model is said to be well-calibrated when the
predictive confidence truly reflects true accuracy
(Guo et al., 2017).
P(ˆY=Y|P(ˆY|X) =p) =p,∀p∈[0,1](2)9794Therefore, when a model makes predictions with
probability of p, the accuracy of the predictions
is expected to be p. The quantity is commonly
approximated with Expected Calibration Error and
Maximum Calibration Error (Naeini et al., 2015).
There have been continuous efforts in lowering
the calibration error of a model, and one of the
simplest, yet effective methods is temperature scal-
ing (Guo et al., 2017). Temperature scaling is a
parametric post-hoc calibration method, where a
single parameter is learned; with model parameters
fixed, the single parameter is learned to lower the
negative log likelihood on validation dataset. This
simple calibration method has been widely appre-
ciated for its ability to improve the reliability of a
model (Müller et al., 2019).
3 Approach
In this section, we first discuss a new interpreta-
tion of a teacher under KD training and introduce
methods that switch supervision between knowl-
edge and observations with an instance-specific
hard gate.
3.1 A View on Teacher Model
When a teacher model is well-calibrated, via cali-
bration method such as temperature scaling (Guo
et al., 2017), the predictive score of a teacher can be
used to estimate the true likelihood of correctness.
In this light, a teacher can be used to evaluate if a
student model makes a miscalibrated prediction, ei-
ther resulting in underconfidence or overconfidence.
Furthermore, given a calibrated teacher, minimiz-
ing the knowledge gap provides a meaningful in-
sight which is more than learning the inter-class
relations, as the objective extends to improving cal-
ibration of a student. By minimizing the KL di-
vergence between the two probability distributions,
the prediction of a student is expected to reflect the
calibrated predictive score.
3.2 Hard Gate
From the novel view of a teacher, our work presents
two instance-specific hard gates: the token-level
and the sentence-level hard gate.
3.2.1 Token-Level Gate
When a predictive score of a prediction by a stu-
dent is high compared to an approximated likeli-
hood of the correctness of the prediction, a student
is supervised to distribute the probability mass to
other remaining classes, hence learning to output asmooth distribution. In another case, in which the
predictive score is less than the approximation, the
student is learned with supervision that increases
the probability, learning from a sample drawn from
the data distribution.
In every time step, instance-specific hard gates
are computed on the fly during forward propagation
as follows:
a=/braceleftigg
1,ifP(y|c)> f(y,c)
0,otherwise(3)
P(y|c)andf(y,c)are conditional prob-
ability of a ground truth index jmapped by a stu-
dent model and the true likelihood of yoccurring
in the given context. Since the true likelihood of
correctness cannot be obtained, we approximate
the quantity with a teacher network with enhanced
calibration ability f(y,c)≈P(y|c;τ).
Supervision from Observations ( α= 0)When
the hard gate is computed to be 0, it is an indication
ofunderfitting and underconfidence by a student on
the instance. The student needs further training so
that the likelihood of predicting the target index is
increased. Due to the normalizing activation layer,
softmax, a direct way of escalating the probability
mass on the ground truth index is to minimize the
KL divergence with one-hot encoded ground truth
(Müller et al., 2019), without incorporating knowl-
edge. That being the case, when the hard gate is
set to 0, supervision to a student solely comes from
ground truth.
Supervision from Knowledge ( α= 1)In an-
other case when the gate is set to 1, it is an indi-
cation of overconfidence evaluated by the approxi-
mated quantity mapped by a teacher. Therefore, a
student is trained to distribute the probability mass
on output space more evenly; the student learns to
close the gap between its probability distribution
and that of a teacher.
This gating mechanism can be viewed as smooth-
ing of labels, hence presenting a regularization ef-
fect. The entropy of supervisions by the proposed
method, conventional logit-based KD ( ˜y), and one-
hot encoded target (hard target) are as follows:
H(P(Y|c;τ))≥H(˜y)≥H(y)(4)
where ˜yis the soft target that is a linear interpola-
tion of a ground truth and a probability distribution
mapped by a teacher. The entropies illustrate how9795
the proposed method regularizes a student by pre-
senting high entropy supervision. Specifically, the
following proposition holds.
Proposition 1 (Opposite Gradient) When α= 1,
the sign of the expectation of the gradient by the
proposed KD method with respect to logit on “in-
correct" classes is guaranteed to be opposite to
that of the cross entropy with hard target.
E∂L
∂z<0≤E∂L
∂z(5)
The gradient of a sample with respect to a logit z
by the cross entropy is as follows:
∂L
∂z=P(y|c)−y (6)
When α= 1, the gradient of the proposed method
is
∂L
∂z=P(y|c)−P(y|c;τ)(7)
Then, it is straightforward to compute the sum of
the quantities except the target index j.
/summationdisplay∂L
∂z= 1−P(y|c) (8)
/summationdisplay∂L
∂z=P(y|c)−P(y|c) (9)As Equation 8 is guaranteed to be greater than
or equal to 0, Equation 9 must be smaller than 0,
since P(y|c)< P(y|c). The Proposition
1 is not guaranteed in conventional logit-based KD,
while it holds true within the proposed approach.
The cross entropy with hard target forces a stu-
dent to decrease the probability mass on the other
classes, while the proposed method sends gradi-
ents that have opposite direction to that of the cross
entropy. In return, the proposed method pushes
a student to increase the probability mass on the
other output space, regularizing the student.
In addition to the regularization effect, the inter-
class relations are given more directly to the stu-
dent than that of the conventional logit-based KD.
The conventional KD shrinks the dark knowledge
P(y|c;τ)byαas in Equation 1. This, how-
ever, is different in the proposed method as αis
set to 1, and hence the amount of dark knowledge
remains unchanged.
3.2.2 Sentence-Level Gate
A natural language generation task is a sequence
of classification. In this regard, in addition to the
token-level gate, we propose to compute hard gates
on the sentence-level. In particular, the gates are
determined by comparing the sentence probabilities
mapped by the two models in KD.
∀a=/braceleftigg
1,ifP(y|x)> f(y,x)
0,otherwise(10)9796where P(y|x)is a sentence probability computed
as the product of the conditional probabilities of
time steps/producttextP(y|c). As in the token-level
gate, the true likelihood of the sentence appear-
ing is approximated with a teacher f(y,x)≈/producttextP(y|c).
A probability of a sentence defined by a language
model is a reflection of how likely a model predicts
the sentence. If a student model defines a sentence
probability that is higher than that of a calibrated
teacher, this is a possible sign of overconfidence in
the sentence. Therefore, in such case, the student
only receives supervision from knowledge. In the
opposite case, as in the token-level gate, the student
is solely trained with ground truth.
Sentence-level computes hard gates in a more
cautious manner than token-level does. A sentence
probability is a product of probabilities of words
within the sentence; hence a miscalibrated proba-
bility of a word can cause much change in the final
probability. This aspect is depicted in Figure 3 in
which sentence-level gates and token-level gates
differ in the ratio of αin the course of training.
3.3 Final Loss
The final loss function is as follows:
L=−/summationdisplay(1−α)ylogP(y|x;θ)
+αP(y|x;ϕ) logP(y|x;θ)
(11)
Theαin both token and sentence-level is computed
during the forward propagation. Therefore, the
following propositions can be made.
Proposition 2 When expected αapproaches 0,
Lreduces to MLE with hard targets. In other
case, when the expected value approaches 1, L
reduces to minimizing KL divergence between prob-
ability distribution of a student and that of a
teacher.
limL=L(P(Y|X;θ), Y)
limL=L(P(Y|X;θ), P(Y|X;ϕ))
(12)
where Lindicates the cross-entropy loss. We
empirically observe that the former case is seen
in the early stages of training which is depicted in
Figure 3. In the other case in which the expected
value converges to 1, the loss reduces to solely min-
imizing the distance of the distributions mapped bythe models without any observation from empirical
training distribution.
One difference to notice is the temperature scal-
ing in Equation 11. The proposed KD solely ap-
plies temperature scaling on the logit of a teacher
for the purpose of calibrating the teacher’s out-
put. This is a marked difference from the existing
logit-based KD, where both a student and a teacher
logits are scaled with a pre-defined temperature as
in Equation 1. This distinction encourages a stu-
dent model to mimic a probability distribution of a
teacher which contains inter-class relations as well
as calibrated predictive scores.
4 Experiment
4.1 Dataset & Experiment Setup
We validate the proposed gating mechanisms on
three popular translation tasks: IWSLT14 German
to English (DE-EN), IWSLT15 English to Viet-
namese (EN-VI), and Multi30K DE-EN. There
are two core reasons for conducting experiments
on a NLG task. First, our method suits NLG tasks
by nature (token and sentence-level). Second, cali-
bration has inseparable relation with NLG, as pop-
ular generation schemes, such as top- k, top-p, and
beam search, are affected by the calibration abil-
ity of a language model. The generation schemes
start by assuming that a predictive score represents
likelihood of an event (Müller et al., 2019).
All of the experiments are conducted on a sin-
gle Telsa V100, and both a student and a teacher
model follow transformer architecture (Vaswani
et al., 2017). The proposed method is tested on
self-knowledge distillation environment, consider-
ing the efficiency of computation and general ap-
plicability. The hyperparameters are identical to
the specified configuration in fairseq (Ott et al.,
2019). The teacher network is trained with uni-
form label smoothing (Szegedy et al., 2016) in our
environment; nonetheless a teacher trained with
the regular cross-entropy training with hard targets
is a valid option, which we show in the ablation
study. For evaluation, we comprehensively validate
previous methods and the proposed methods with
popular translation evaluation metrics: BLEU (Pap-
ineni et al., 2002), METEOR (Banerjee and Lavie,9797
2005), Word Error Rate (WER), ROUGE-L (Lin,
2004), and NIST (Doddington, 2002). For quan-
tifying the level of model calibration, we report
Expected Calibration Error (ECE) and Maximum
Calibration Error (MCE) (Naeini et al., 2015).
4.2 Baselines
As this paper lies in a branch of KD, though is
closely linked to regularization methods, we com-
pare the proposed methods with baselines from
both of the domains.
Base Method The base method in this work is
the cross-entropy with hard targets (Base).Regularizers Although Label Smoothing (LS)
was first introduced to enhance model performance
in (Szegedy et al., 2016), it has been found to help
in model calibration as well. The prior label distri-
bution is commonly set with a uniform distribution
(LS-Uniform) (Vaswani et al., 2017; Lewis et al.,
2020), yet unigram distribution (LS-Unigram) is
another valid choice. Similar to label smoothing,
ConfPen (Pereyra et al., 2017) prevents a model
from outputting a peak distribution by penalizing
high confident predictions. Loras (Ghoshal et al.,
2021) theoretically find that the generalization er-
ror largely depends on prior label distribution; thus,
it jointly learns model parameters and prior label
distribution.9798
KD-Methods Yuan et al. (2020) empirically find
that even a weak teacher can improve a student.
Accordingly, the authors present TF-KD where a
pre-trained teacher with identical model structure
to that of a student is utilized in KD. PS-KD (Kim
et al., 2021) is similar, but the core difference is that
a teacher is the previous checkpoint of a student
in training. Zhang and Sabuncu (2020) introduce
instance-specific label smoothing methods, SD and
Beta, which make use of self-knowledge and en-
courage diversity in predictions.
4.3 Experimental Result
The automatic evaluation results are reported in
Table 1. Both of the proposed methods achieve
noticeable gains compared to the Base method and
the strong baseline methods. The improvements
are seen across every metric and corpus tested. Our
methods excel not just in n-gram matching (BLEU)
and harmonic mean of unigram precision and re-
call (METEOR), but also in having the longest
common subsequence with references (ROUGE-L)and outputting informative n-grams (NIST). More-
over, as clearly depicted with low WER, the outputs
of our systems require the least amount of modi-
fication to be converted into reference sequences.
Without adding any additional learnable parame-
ter compared to the base method, our token-level
HKD illustrates superior performance to that of the
base method, absolute gain of 3.32 BLEU score
and relative improvement of 8.16% on Multi30K.
Sentence-level hard gate method also illustrates
competitive results to those of the token-level hard
gate. On every corpus and metric tested, both to-
ken and sentence-level gates outperform the strong
baselines by a large margin.
Figure 3 depicts the change of ground truth su-
pervision ratio in the course of training. In each
corpus, the ratio of ground truth decays throughout
the training. In the early stage of training, most
of the supervisions come from ground truth as a
student is underfitted, leading to the low expected
α. The ratio decreases as the student model learns
to map the task distribution, increase in the ratio of
supervision from knowledge. The ratios converge
at a certain point in each dataset, illustrating how
the proposed methods hold self-regulating prop-
erty in switching the supervisions. A noteworthy
point is the correlation of the ratio and the corpus
size. Under Multi30K training, which is the small-
est in terms of training dataset size, the student
receives majority of supervision from the teacher
which the ratio is higher than 0.6 with token-level
and 0.8 with sentence-level hard gate. This em-
pirically shows that the proposed systems, in an
environment with the risk of overconfidence and
overfitting demonstrate strong regularization effect
in training. The methods enforce a student model9799
to learn from knowledge to avoid possible degrada-
tion in model generalization and calibration.
4.3.1 Model Calibration
Following (Müller et al., 2019), our work evaluates
model calibration by formulating the generation
process as the next token prediction task. Expected
Calibration Error (ECE) and Maximum Calibration
Error (MCE) of the models on the corpora tested
are reported in Table 1.
It is clearly demonstrated that the proposed meth-
ods, especially with sentence-level hard gate, lead
to a calibrated student. The ECE and MCE scores
of the base method are high as modern neural net-
works are found to be overconfident (Guo et al.,
2017). The amount of error is mitigated to some
extent with the introduction of regularizers and
KD methods. For instance, LS with uniform dis-
tribution lowers the ECE and MCE score to 6.43
and 9.98 on IWSLT14 corpus. The errors are re-
duced to around half of those of the base method.
Nevertheless, the most noticeable gain in calibra-
tion is seen across the proposed methods. HKD-S
achieves 1.27 in ECE and 3.22 in MCE, illustrat-
ing remarkable improvement in model calibration.
The absolute improvement compared to the base
method is approximately 11.7 score in ECE and
16 in MCE on IWSLT14 dataset. The proposed
methods enhance the model calibration of a stu-
dent by a large margin, with such gain observed
across the corpora. Reliability diagrams in Figure
2 further support the claim. Despite the baselines
improving model calibration, the methods tend to
show either underconfident or overconfident predic-
tions. LS-Uniform and PS-KD consistently make
overconfident predictions, while SD suffers from
underconfidence. On the other hand, the reliability
diagrams of our work display calibrated results that
mainly conform with the low ECE and MCE scores
in Table 1.
Furthermore, our methods make predictions
more evenly distributed as illustrated in Figure 4.
HKD-T and HKD-S do not make predictions solely
with low confidence; the number of predictions
with confidence scores between 0.0 to 0.9 by our
methods outnumbers those of the other methods,
demonstrating an ability to make decision with di-
verse confidence .
4.4 Ablation Study
In order to further validate the proposed method,
we conduct an ablation study with different teach-
ers and varying temperature values, and the results
are shown in Table 2. In a case where a teacher
is trained with the cross-entropy with hard targets,
both BLEU score and ECE score are enhanced
compared to those of Base method. However, with
a proper temperature control (enhanced calibra-
tion), both BLEU and ECE improve significantly.
The BLUE score is higher than that of a model
trained with LS-Uniform, and the ECE score is
also competitive. This empirically validates that
with a proper control of temperature, the proposed
KD systems are compatible with a wide choice of
teacher .
5 Conclusion
In this study, we present hard gate knowledge dis-
tillation, a mechanism that switches supervision
between knowledge and ground truth at either the
sequence-level or the token-level. This originates
from the novel view and role of a teacher in KD.
The proposed method is simple yet effective in
improving model generalization and calibration,
achieving superior performances compared to those
of the strong baselines.9800Limitations
As in previous KD methods, the proposed ap-
proaches utilize a teacher model, hence inevitably
causing the computation cost to increase. In addi-
tion, as two forward passes are needed, one by a
teacher and the other by a student, the training time
is longer than non-KD training methods. Lastly,
the proposed idea does not suit a natural language
understanding task due to the introduction of the
token-level and sentence-level gate.
Ethical Consideration
The proposed idea is a student-teacher framework;
hence, a teacher model can greatly affect a stu-
dent model. If a teacher model is trained with a
dataset with biased information or misinformation,
the student is likely to learn such features while
minimizing the knowledge gap. One can mitigate
the concern to some extent if fact checking system
or biased detection system is employed. This is not
the fundamental solution to the problem that KD
training faces, yet the level of danger is expected
to be mitigated to some extent.
Acknowledgement
Research on this paper was supported by Hong
Kong Research Grants Council (Grant No.
16204920).
References9801
A Dataset & Implementation Details
Multi30K dataset is the smallest in size among
the corpora tested, 28K sentence pairs for training,
1K for validation, and 1K for testing. IWSLT15
EN-VI comprises 133K pairs in the training set,
1.5K in the validation, and 1.3K in the testing set.
Lastly, IWSLT14 DE-EN corpus contains around
170K, 7K, and 7K pairs of sentences for training,
validation, and testing dataset respectively. Words
are processed into sequence of subword units with
subword-nmt (Sennrich et al., 2016).
The structure of models, both the student and the
teacher, in all of the experiments follow transformer
architecture (Vaswani et al., 2017). Specifically,
both the encoder and the decoder are composed of
6 transformer layers with 4 attention heads, and the
hidden dimension size is set to 512. The dropout9802probability is set to 0.3, and the maximum number
of tokens within a batch is 4096. For the tem-
perature in our experiments, we have tested {0.8,
1.0, 1.5, 2.0, 2.5}, and find that 1.0 works the best
with a self-teacher trained with label smoothing.
For a self-teacher trained with hard targets, 1.5
for temperature value illustrates the best perfor-
mance, as the temperature smoothed the output of
the teacher. We report random seeds used in our
work for reproducibility, the seeds being {0000,
3333, 5555} .9803