
Sathish IndurthiMohd Abbas Zaidi
Beomseok LeeNikhil Kumar LakumarapuSangha KimSamsung Research, South KoreaZoom AI Lab, Singapore
Abstract
The state-of-the-art adaptive policies for simul-
taneous neural machine translation (SNMT)
use monotonic attention to perform read/write
decisions based on the partial source and target
sequences. The lack of sufficient information
might cause the monotonic attention to take
poor read/write decisions, which in turn neg-
atively affects the performance of the SNMT
model. On the other hand, human translators
make better read/write decisions since they can
anticipate the immediate future words using
linguistic information and domain knowledge.
In this work, we propose a framework to aid
monotonic attention with an external language
model to improve its decisions. Experiments on
MuST-C English-German and English-French
speech-to-text translation tasks show the future
information from language model improves the
state-of-the-art monotonic multi-head attention
model further.
1 Introduction
A typical application of simultaneous neural ma-
chine translation (SNMT) is conversational speech
or live video caption translation. In order to achieve
live translation, an SNMT model alternates be-
tween performing read from source sequence and
write to target sequence. For a model to decide
whether to read orwrite at certain moment, either a
fixed or an adaptive read/write policy can be used.
Earlier approaches in simultaneous translation
such as Ma et al. (2019a) and Dalvi et al. (2018)
employ a fixed policy that alternate between read
andwrite after the waiting period of ktokens. To
alleviate possible long delay of fixed polices, re-
cent works such as monotonic infinite lookback
attention (MILk) (Arivazhagan et al., 2019), and
monotonic multihead attention (MMA) (Ma et al.,
2019c) developed flexible policies using monotonic
attention (Raffel et al., 2017).Figure 1: The finetuned XLM-RoBERTa language
model predicts German words using the prefix as in-
put.(Green: Correct, Red: Incorrect, Black: Neutral).
While these monotonic attention anticipates tar-
get words using only available prefix source and
target sequence, human translators anticipate the
target words using their language expertise (linguis-
tic anticipation) as well as contextual information
(extra-linguistic anticipation) (Vandepitte, 2001).
Inspired by human translation experts, we aim to
augment monotonic attention with future informa-
tion using language models (LM) (Devlin et al.,
2019; Conneau et al., 2019).
Integrating the external information effectively
into text-to-text machine translation (MT) systems
has been explored by several works (Khandelwal
et al., 2020; Gulcehre et al., 2015, 2017; Stahlberg
et al., 2018). Also, integrating future information
implicitly into SNMT system during training is ex-
plored in Wu et al. (2020) by simultaneously train-
ing different wait- kSNMT systems. However, no
previous works make use of explicit future informa-
tion both during training and inference. To utilize
explicit future information, we explored to inte-
grate future information from LM directly into the
output layer of the MMA model. However, it did
not provide any improvements (refer to Appendix
A), thus motivating us to explore a tighter integra-
tion of the LM information into SNMT model.
In this work, we explicitly use plausible future38
information from LM during training by transform-
ing the monotonic attention mechanism. As shown
in Figure 1, at each step, the LM takes the prefix
target (and source, for cross-lingual LM) sequence
and predicts the probable future information. We
hypothesize that aiding the monotonic attention
with this future information can improve MMA
model’s read/write policy, eventually leading to
better translation with less delay. Several experi-
ments on MuST-C (Di Gangi et al., 2019) English-
German and English-French speech-to-text transla-
tion tasks with our proposed approach show clear
improvements of latency-quality trade-offs over the
state-of-the-art MMA models.
2 Monotonic Attention with Future
Information Model
2.1 Monotonic Attention
In simultaneous machine translation (SNMT) mod-
els, the probability of predicting the target token
y∈ydepends on the partial source and target
sequences ( x∈x, y∈y). In sequence-to-
sequence based SNMT model, each target token y
is generated as follows:
h=E(x) (1)
s=D(y, c=A(s, h)) (2)
y=Output (s) (3)
where E(.)andD(.)are the encoder and decoder
layers, and cis a context vector. In monotonic
attention based SNMT, the context vector is com-
puted as follows:
e=MonotonicEnergy (s, h)(4)
p=Sigmoid (e) (5)
z∼Bernoulli (p) (6)When generating a target token y, the decoder
chooses whether to read/write based on Bernoulli
selection probability p. When z= 1 (write ),
model sets t=j,c=hand generates the target
token y. Forz= 0(read), it sets t=j+ 1and
repeats Eq. 4 to 6. Here trefers to the index of
the encoder when decoder needs to produce the i
target token. Instead of hard alignment of c=h,
Raffel et al. (2017) compute an expected alignment
in a recurrent manner and propose a closed-form
parallel solution. Arivazhagan et al. (2019) adopt
monotonic attention into SNMT and later, Ma et al.
(2019c) extend it to MMA to integrate it into the
Transformer model (Vaswani et al., 2017).
2.2 Monotonic Attention with Future
Information
The monotonic attention described in Section 2.1
performs anticipation based only on the currently
available source and target information. To aug-
ment this anticipation process using future informa-
tion extracted using LMs, we propose the following
modifications to the monotonic attention.
Future Representation Layer: At every de-
coding step i, the previous target token yis
equipped with a plausible future token ˆyas shown
in the Figure 2. Since the token ˆycomes from
an LM possibly with a different tokenizer and vo-
cabulary set, applying the model’s tokenizer and
vocabulary might split the token ˆyfurther into mul-
tiple sub-tokens {ˆy,ˆy,···,ˆy}. To get a single
future token representation ˜s∈ Rfrom all the
sub-tokens, we apply a sub-token summary layer:
˜s= Γ({ˆy,ˆy,···,ˆy}) (7)
TheΓrepresents a general sequence representation
layer such as a Transformer encoder layer or a sim-
ple normalized sum of sub-token representations.39We enrich ˜sat every layer lof the decoder block
by applying a residual feed-forward network.
˜s=FFN (˜y) (8)
Monotonic Energy Layer with Future Informa-
tion: Despite the fact that we can add the plau-
sible future information to the output layer (Ap-
pendix A) or append it to the target token represen-
tation y, the MMA read/write decisions happen
in Eq. 4. Therefore, we integrate ˜sinto the Eq. 4
instead.
The integration is carried out by modifying Eq.
4 - Eq. 5. We compute the monotonic energy for
future information using the enriched future token
representation ˜savailable at each layer:
˜e=MonotonicEnergy (˜s, h) (9)
We integrate the future monotonic energy function
into Eq. 5 as follows:
˜p=Sigmoid (e+ ˜e) (10)
After computing ˜p, we compute csimilar to
MMA model.
This way of integration of future information
allows the model to condition the LM output us-
age on the input sequence. The model can control
the relative weightage given to the LM output by
varying the ˜e. In case of insufficient source in-
formation in the low latency regime, we expect the
model’s decision policy to rely more on ˜e.
Inference: During inference, the start token does
not contain any plausible information. After pre-
dicting the first target token, for every subsequent
prediction of target token y, we invoke the LM to
predict the next plausible future token and integrate
this new information into Eq. 10.
3 Experiments and Results
3.1 Experimental Settings
Datasets and Metrics: We conduct our experi-
ments on the MuST-C English(En)-German(De)
and English(En)-French(Fr) speech-to-text (ST)
translation task. The speech sequence is repre-
sented using 80-dimensional log-mel filter bank
features. The target sequence is represented as sub-
words using a SentencePiece (Kudo and Richard-
son, 2018) model with a unigram vocabulary of
size 10,000. We evaluate the performance of the
models on both the latency and quality aspects. Weuse Average Lagging(AL) as our latency metric
and case-sensitive detokenized SacreBLEU (Post,
2018) to measure the translation quality, similar
to (Ma et al., 2020). The best models are chosen
based on the dev set results and reported results are
from the MuST-C test (tst-COMMON) sets.
Language Models We use two language mod-
els to train our proposed modified MMA model.
Firstly, we use the pretrained XLM-RoBERTa
(Conneau et al., 2019) model from Huggingface
Transformersmodel repository. Since the LM out-
put can be very open-ended and might not directly
suit/cater to our task and dataset, we finetune the
head of the model using the MuST-C target text
data for each task.
We also train a smaller language model (SLM),
which contains 6 Transformer decoder layers, 512
hidden-states and 24M parameters. We use the
MuST-C data along with additional data augmen-
tation to reduce overfitting. The SLM helps to
remove the issues related to vocabulary mismatch
as discussed in the Section 2.2.
Implementation Details: Our base model is
adopted from Ma et al. (2020). We use a pre-
decision ratio of 7 , which means that the simultane-
ousread/write decisions are made after every seven
encoder states. We use λorλ to refer to
the hyperparameter corresponding to the weighted
average( λ) in MMA. The values of this hyperpa-
rameter λare chosen from the set {0.01,0.05,0.1}.
TheΓlayer in Eq. 7 computes the normalized sum
of the sub-token representations. For SLM, it sim-
ply finds the embedding since it shares the same
vocabulary set. All the models are trained on a
NVIDIA v100 GPU with update _freq set to 8.
Simultaneous Translation Models: Even
though future information can be integrated
explicitly into the fixed policy approaches such as
Wait-K (Ma et al., 2019b), we choose monotonic
attention as our baseline due to its superior
performance (Arivazhagan et al., 2019; Ma et al.,
2019c). We train a baseline based on Ma et al.
(2020) work, called as MMA model. The MMA
model encoder and decoder embedding dimensions
are set to 392, whereas our proposed model’s
encoder and decoder embeddings are set to 256
to have similar parameters ( ≈39M) for a fair
comparison. We train two models using the40
modified MMA based on two LMs (XLM, SLM),
referred as MMA-XLM and MMA-SLM.
3.2 Results
We first analyze how the LM predictions are being
utilized by the our model. In order to measure the
relative weight given to model’s internal states ver-
sus the predictions from the LM, we compare the
norm of the monotonic energies corresponding to
the LM predictions e(Eq. 9) and the previous
output tokens e (Eq. 4). Let us define LM
prediction weight as follows:
LM=/parenleftbigg∥e∥
∥e∥/parenrightbigg
(11)
In Figure 3, we plot the variation of LM(av-
eraged) vs. λ. We use two additional values of
λ∈ {0.005,0.001}to obtain this plot. We can
observe that as the latency requirements become
more and more strict, the model starts to give more
weightage to the predictions coming from the LM.
This shows that the model learns to utilize the in-
formation coming from LM predictions based on
latency requirements.
Next, we discuss the performance improvements
obtained from our proposed approach. By vary-
ing the λ, we train separate models for different
latency regimes. Moreover, the quality and latency
for a particular model can also be varied by control-
ling the speech segment size during the inference.
Speech segment size or step size refers to the du-
ration of speech (in ms) processed corresponding
to each read decision. We vary these hyperparame-
ters for all the three models, namely MMA, MMA-
XLM and MMA-SLM.
The BLEU-AL curves for all the models have
been provided in Figure 4 and BLEU-AL num-
bers for all models are included in Appendix F
for reference. We vary the step sizes in intervals
of 80ms from 120 ms to 520 ms in order to get
performances corresponding to different latency
regimes. We can observe that the LM-based mod-
els using both XLM and SLM provide a significant
performance improvement over the baseline MMA
model. We observe improvements in the range of
1-2 BLEU scores consistently across all the latency
regimes ( λ= 0.1,0.05,0.01). The MMA using
SLM language model performs slightly better than
MMA using XLM language model. This is due to
SLM’s higher accuracy on the next token predic-
tion task as compared to XLM, 30.15% vs. 21.5%
for German & 31.65% vs. 18.45% for French. The
high accuracy of SLM is attributed to its training
on in-domain data.
4 Conclusion
In this work, we provide a generic framework to
integrate the linguistic and extra-linguistic infor-
mation into simultaneous models. We rely on lan-41guage models to extract this plausible future in-
formation and propose a new monotonic attention
mechanism to infuse this information. Several ex-
periments on speech-to-text translation tasks show
the effectiveness of proposed approach on obtain-
ing superior quality-latency trade-offs, compared to
the state-of-the-art monotonic multihead attention.
References42
A LM at MMA Output Layer
We explored a naive approach of integrating LM
information into the MMA. In this approach, we in-
tegrate the future information obtained from the
LM directly into the output layer of the MMA
model. We refer to this experiment as ‘LM Rescor-
ing(LMR)’, and the corresponding model is called
MMA-LMR.
As observed in Figure 5, MMA-LMR has infe-
rior performance compared to the MMA model.
Since the LM information integration is only done
at the output layer of the model, the MMA model
cannot easily discard the incorrect information
from LM. This motivates us to tightly integrate
the LM information into the simultaneous model.
B Language Models
As mentioned earlier, we train two different lan-
guage models (LMs) and use them to improve the
anticipation in monotonic attention based Simulta-
neous models.
B.1 XLM-Roberta(XLM-R)
XLM-R Large modelwas trained on the 100 lan-
guages CommonCrawl corpora total size of 2.5TB
with 550M parameters from 24 layers, 1024 hid-
den states, 4096 feed-forward hidden-states, and
16 heads. Total number of parameters is 558M. We
finetune the head of the XLM-R LM model using
the Masked Language Modeling objective which
accounts for 0.23% of the total model parameters,
i.e., 1.3M parameters.
B.2 Smaller Language Model
Since the LM predictions are computed serially
during inference, the time taken to compute theLM token serves as a bottleneck to the latency re-
quirements. To reduce the LM computation time,
we train a smaller Language Model (SLM) from
scratch using the Causal Language Modeling ob-
jective. SLM is composed of 6 Transformer de-
coder blocks, 512 hidden-states, 2048 feed-forward
hidden-states & 8 attention heads. It alleviates
the need for the sub-token summary layer since
it shares the vocabulary and tokenization with the
MMA models. The train examples are at the sen-
tence level, rather than forming a block out of multi-
ple sentences(which is the usual case for Language
Models).
Since the target texts contain lesser than 250k
examples, we use additional data augmentation
techniques to upsample the target data. We also use
additional data to avoid overfitting on the MuST-C
target text. Details have been provided in B.2.1.
B.2.1 Data Augmentation
Up-Sampling: To boost the LM performance
and mitigate overfitting, we use contextual data
augmentation (Kobayashi, 2018) to upsample the
MuST-C target text data by substituting and insert-
ing words based on LM predictions. We use the
NLPAUGpackage to get similar words based on
contextual embeddings. From the Hugging Face
Repository, we use two different pretrained BERT
(Devlin et al., 2019) models for German bert-base-
german-dbmdz-cased &bert-base-german-dbmdz-
uncased andbert-base-fr-cased for French. We
upsample German to 1.13M examples and French
to 1.38M examples.
Additional Data: We also use additional data
to avoid overfitting. For German we use the
Newscrawl(WMT 19) data which includes 58M
examples. For French, we use Common Crawl and
Europarl to augment 4M extra training examples.
We observe that both upsampling and data aug-
mentation help us to reduce the overfitting on the
MuST-C dev set.
B.3 Token Prediction
For each output token, the LM prediction is ob-
tained by feeding the prefix upto that token to the
LM model. These predictions are pre-computed
for training and validation sets. This ensures par-
allelization and avoids the overhead to run the LM
simultaneously during the training process. During43(a) EnDe Task (b) EnFr Task
inference, the LM model is called every time a new
output token is written.
C Dataset
The MuST-C dataset comprises of English TED
talks, the translations and transcriptions have been
aligned with the speech at sentence level. Dataset
statistics have been provided in the Table 1.
D Effect of LM Size on Latency-Quality
We train several SLM models with varying sizes in
our experiments and choose the best model based
on the top-1 accuracy. As we increase the number
of layers in the LM model from 2 to 4 to 6 layers,
the SLM and the proposed MMA with future infor-
mation models have shown performance improve-
ments. However, increasing the number of layers
greater than 6 does not yield any performance im-
provements. We also notice this degradation of
performance with the XLM model while varying
the number of hidden layers in the LM head.
E Training Details
We follow the training process similar to Ma et al.
(2020) training process. We train an English ASR
model using the source speech data. Next, we
train a simultaneous model without the latency loss
(setting λ = 0) after initializing the encoder
from the English ASR model. After this step, we
finetune the simultaneous model for different λs.
This training process is repeated for all the reportedmodels and for each task. The details regarding the
hyperparameters for the model have been provided
in Table 2.
F BLEU-AL Numbers
As mentioned in the results section of the main pa-
per, we vary the latency weight hyperparameter ( λ)
to train different models to obtain different latency
regimes. We also vary the step-size/speech seg-
ment size during inference. In total, we obtain 18
different data points corresponding to each model.
In Table 3, we compare the results obtained using
MMA, MMA-XLM and MMA-SLM under similar
hyperparameter settings. It will help the reader to
quantify the benefits obtained from our proposed
approach.44Task # Hours# Sentences# Talks# Words
Train Dev Test Source Target
English-German 408 225k 1,423 2,641 2,093 4.3M 4M
English-French 492 269k 1,412 2,632 2,510 5.2M 5.4M
MMA MMA-XLM/CLMHyperparameter
encoder layers 12 12
encoder embed dim 292 256
encoder ffn embed dim 2048 2048
encoder attention heads 4 4
decoder layers 6 6
decoder embed dim 292 256
decoder ffn embed dim 2048 2048
monotonic ffn embed dim – 2048
decoder attention heads 4 4
dropout 0.1 0.1
optimizer adam adam
adam- β (0.9, 0.999) (0.9, 0.999)
clip-norm 10.0 10.0
lr scheduler inverse sqrt inverse sqrt
learning rate 0.0001 0.0001
warmup-updates 4000 4000
label-smoothing 0.0 0.0
max tokens 40000 40000
conv layers 2 2
conv stride (2,2) (2,2)
#params ≈39M ≈39M45