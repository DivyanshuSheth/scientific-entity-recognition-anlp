
Jungsoo ParkGyuwan KimJaewoo KangClova AI, NA VER Corp.University of California, Santa BarbaraKorea University
{jungsoo_park,kangj}@korea.ac.kr
gyuwankim@ucsb.edu
Abstract
Consistency training regularizes a model by
enforcing predictions of original and perturbed
inputs to be similar. Previous studies have pro-
posed various augmentation methods for the
perturbation but are limited in that they are ag-
nostic to the training model. Thus, the per-
turbed samples may not aid in regularization
due to their ease of classiﬁcation from the
model. In this context, we propose an aug-
mentation method of adding a discrete noise
that would incur the highest divergence be-
tween predictions. This virtual adversarial dis-
crete noise obtained by replacing a small por-
tion of tokens while keeping original seman-
tics as much as possible efﬁciently pushes a
training model’s decision boundary. Experi-
mental results show that our proposed method
outperforms other consistency training base-
lines with text editing, paraphrasing, or a con-
tinuous noise on semi-supervised text classiﬁ-
cation tasks and a robustness benchmark.
1 Introduction
Building a natural language processing (NLP) sys-
tem often requires an expensive process to col-
lect a massive amount of labeled text data. Semi-
supervised learning (SSL) (Chapelle et al., 2009)
mitigates the requirement for such labeled data by
exploiting the structure of unlabeled data. Among
the SSL methods, the consistency training frame-
work (Laine and Aila, 2017; Sajjadi et al., 2016)
enforces a model to produce similar predictions
of original and perturbed inputs. This method has
several advantages over other training algorithms
such as naively adding augmented samples into the
training set (Wei and Zou, 2019; Ng et al., 2020)
in that it provides a richer training signal than a
one-hot label, and also applies to both labeled and
unlabeled data (Xie et al., 2020).
Figure 1: A simple illustration of the intuition behind
our method is visualized in the two-dimensional space,
where the augmented samples (triangle) would aid the
training given a limited number of data (circle).
For perturbing a text while preserving its seman-
tics, some approaches inject continuous noise to
embedding vectors (Xie et al., 2017; Miyato et al.,
2018), and others modify text itself in discrete fash-
ion by edit operations (Kobayashi, 2018; Wei and
Zou, 2019) or paraphrasing with back-translation
(Sennrich et al., 2016; Edunov et al., 2018; Xie
et al., 2020). However, adding continuous noise
might not strongly regularize the training model,
compared to diverse discrete noise-based augmen-
tation methods (Ebrahimi et al., 2017; Cheng et al.,
2019). Also, the augmentations with discrete noise
are mostly black-box approaches based on simple
rules or ﬁxed models without access to the training
model’s internal states, having no control over out-
put augmentations that would aid in the regulariza-
tion of the training model. As seen in Fig. 1 (d), the5646augmented samples with similar semantics but that
are outside the training model’s decision boundary
(i.e.adversarial) are the ones that would effectively
regularize the model to ﬁt into the complex real
data distribution.
To this end, we explore virtual adversarial train-
ing with discrete token replacements ( VAT-D ). Our
framework (1) ﬁrst perturbs a given input text by
replacing a small subset of tokens to maximize the
divergence between the original and the perturbed
samples’ model predictions (i.e., virtual adversar-
ial)while ﬁltering tokens to replace for constrain-
ing the semantic similarity, and (2) train a model
to minimize the divergence of the predictions of
original and perturbed inputs.
V AT-D shares the advantages of virtual adver-
sarial training (V AT) with continuous noise (Miy-
ato et al., 2018) in that the perturbation is model-
dependent, changing over the training time to ap-
proximate the augmented samples that would ef-
fectively push the decision boundary. On the other
hand, V AT-D differs from V AT in that the search
space is discrete rather than continuous, thus not
constrained by the pre-deﬁned norm on the em-
bedding space. Our method relies on the training
model’s predictions which do not require label in-
formation, hence being the ﬁrst work to success-
fully apply the adversarial training with perturba-
tion on discrete space to the SSL framework.
Our proposed method empirically outperforms
previous state-of-the-art methods on topic classiﬁ-
cation datasets (Chang et al., 2008; Mendes et al.,
2012; Zhang et al., 2015) under various SSL sce-
narios. We additionally conduct experiments on
ANLI robustness benchmark dataset (Nie et al.,
2020) for testing the robustness when only labeled
samples are given where the method improves over
the RoBERTa-Large (Liu et al., 2019) by 8 points.
2 Background
We explain the concept of consistency training and
V AT that our framework relies on.
Consistency Training Consistency train-
ing (Laine and Aila, 2017; Sajjadi et al., 2016)
enforces models’ predictions to be invariant when
the input is perturbed. This regularization pushes
the decision boundary to traverse a low-density
region (Verma et al., 2019). The consistency loss
is formally deﬁned as
L(x,x) =D[p(·|x),p(·|x)] (1)whereDis a non-negative divergence metric
between two probability distributions ( e.g., KL-
divergence), xis a perturbed sample from an input
xby any transformation.
Virtual Adversarial Training V AT (Miyato
et al., 2017, 2018) is a consistency training method,
which perturbs a given input with continuous noise
to maximize the divergence from the model’s pre-
diction of the original input. Such virtually ad-
versarial examples effectively smooth the deci-
sion boundary compared to the random pertur-
bation (Miyato et al., 2018). The formal def-
inition of virtual adversarial samples is ˆ x=
argmaxL(x,x)where the training objec-
tive is to minimize the L(x,ˆ x). Miyato et al.
(2017) perturbs input by injecting noise to the em-
bedding space, where the constraint of the per-
turbation is /epsilon1-ball inLnorm centered at x,i.e.
Neighbor (x) ={x|/bardblx−x/bardbl≤/epsilon1}.
3 Method
We aim to generate a perturbed sample by adding
discrete noise that incurs the highest divergence
of the model’s prediction logits from the original
one without signiﬁcant changes in its semantics.
Our augmentation is made on-the-ﬂy depending on
the current model to push the decision boundary
during training effectively.
Virtual Adversarial Discrete Noise We de-
velop the consistency training framework by per-
turbing inputs with virtual adversarial discrete
noise, called V AT-D. We want to perturb a given
sentence x= (x,...,x)∈Vof se-
quence length Minto a new sentence x=
(x,...,x)∈Vof the same length, where V
is the word vocabulary. In contrast with the continu-
ous case, we constrain that xdiffers from xin only
small portion of positions changing their surface
forms, i.e.Neighbor (x) ={/bardblx−x/bardbl/M≤τ}
whereHdenotes hamming distance in the token-
level andτis the replacement ratio. In this work,
we only focus on the replacement for simplicity.
Gradient Information The white-box ap-
proaches having an access to the training model’s
internal states, mostly rely on the gradient vectors
of the loss function with respect to the input
embeddings for ﬁnding adversarial discrete noise
(Ebrahimi et al., 2017). However, for acquiring
such gradient information under the framework of5647consistency training as in Eq. 1, naively resorting
to the linear approximation of the loss function
with respect to the input embeddings like in
previous works (Ebrahimi et al., 2017; Michel
et al., 2019; Cheng et al., 2019) does not hold
since the ﬁrst-order term from Taylor expansion is
zero when the label information is substituted to
model’s predictions (Miyato et al., 2018).
We bypass the obstacle by sharpening the distri-
bution of original examples’ predictions to enable
the linear approximation. Sharpening the distri-
bution makes high probabilities higher and lower
probabilities lower while not changing their rela-
tive order. By sharpening the distribution of the
original inputs’ predictions, the ﬁrst-order term
does not result in zero, hence can be utilized for
the approximation. This is because the modiﬁed
divergence loss is not zero when x=xindicating
the non-negative divergence is not necessarily min-
imum atr=x−x= 0(Note that the derivative
off(x)is zero when the f(x)is minimum at x).
The optimizing objective of Eq. 1 is modiﬁed to
˜L(x,x) =D[p(·|x)),p(·|x)] (2)
by sharpening the predicted distribution given an
original input by the pre-deﬁned temperature Tas
p(·|x) =p(·|x)//vextenddouble/vextenddouble/vextenddoublep(·|x)/vextenddouble/vextenddouble/vextenddouble.
Virtual Adversarial Token Replacement Con-
sequently, the optimization problem to ﬁnd a virtual
adversarial discrete perturbation changes to
ˆ x= argmax˜L(x,x).
Finally, we train the modiﬁed consistency loss
function from Eq. 2 with obtained discrete per-
turbation. The replacement operation of m-th to-
kenxto the arbitrary token xcan be written as
δ(x,x) :=e(x)−e(x), wheree(·)denotes
embedding look-up. We induce a virtual adversar-
ial token by the following criteria (Ebrahimi et al.,
2017; Michel et al., 2019; Cheng et al., 2019; Wal-
lace et al., 2019; Park et al., 2020):
ˆx= argmaxδ(x,x)·g (3)
whereg=∇˜L(x,x)|
gis the gradient vector of the sharpened con-
sistency loss from Eq. 2 with respect to the m-th
token. In brief, we replace the m-th original token
xwith one of the candidates xthat approximatelymaximizes the consistency loss. We randomly se-
lect token indexes to perturb and replace them si-
multaneously. To bound the semantics similarity
between the original sentence and the perturbed
one, we use a masked language model (MLM) (De-
vlin et al., 2019; Liu et al., 2019) to restrict a
set of possible candidates to replace x. We ﬁl-
ter top-k candidates (Cheng et al., 2019), denoted
astop_k(x,V), from the vocabulary having the
highest MLM probability at position mwhen an
original sentence xis given to the MLM. More
training details are in Appendix A.
4 Experimental Setup
4.1 Dataset
We experiment on three topic classiﬁcation datasets
and Adversarial NLI (ANLI) (Nie et al., 2020). The
former evaluate our method’s effectiveness in SSL
and the latter is for evaluating the robustness of
the models under the standard supervised training
framework. The three topic classiﬁcation bench-
marks consist of AG News (Zhang et al., 2015),
DBpedia (Mendes et al., 2012), and YAHOO! An-
swers (Chang et al., 2008). We follow the exper-
imental setting from Chen et al. (2020a), where
we train with a limited number of labeled data in
diverse settings, namely, 10, 200, 2500 per class.
We randomly sample the labeled, unlabeled, and
development set and report the performance on the
ofﬁcial test set. For producing the conﬁdent re-
sults, we report the average of ﬁve different seeds’
distinct runs.
As for ANLI, we train the model with two
different settings, training with only the ANLI
dataset or additionally training with other NLI
datasets, including SNLI (Bowman et al., 2015),
MNLI (Williams et al., 2017), and FEVER (Thorne
et al., 2018) following the original work (Nie et al.,
2020). Further details are in Appendix B.
4.2 Baseline
We compare our method with various baselines of
the perturbation methods including EDA (Wei and
Zou, 2019), UDA (Xie et al., 2020), V AT (Miyato
et al., 2017, 2018) for the topic classiﬁcation SSL
task. For the ANLI dataset, we compare with the
baselines (Devlin et al., 2019; Yang et al., 2019;
Liu et al., 2019; Jiang et al., 2020) that have re-
ported numbers on the ofﬁcial validation and test
set. More details are in Appendix C.5648
4.3 Training Details
We exploit the unlabeled data from the topic clas-
siﬁcation datasets and the labeled data from the
ANLI for consistency loss. Throughout the experi-
ments, we set the replacement ratio τas 0.25 and
top-k as 10. We sharpen the predictions with T
as 0.5 for topic classiﬁcation datasets (including
baselines) and 0.75 for the ANLI.
5 Experimental Results
5.1 Semi Supervised Text Classiﬁcation
Table 1 shows the experimental results on topic
classiﬁcation datasets under SSL setup. Our
method outperforms the baselines by up to 7.1
points from the BERT model ﬁnetuned with stan-
dard cross-entropy loss and 2.9 points from other
methods utilizing the consistency regularization
loss. The accuracy gained from the proposed
method from the baselines, especially when the
number of labeled samples is limited. However,
since all the methods have already achieved high
accuracy in the DBpedia, the difference among
methods is not signiﬁcant.
Among the baselines, V AT (Miyato et al., 2017,
2018) performs reasonably well. The ﬁnding sup-
ports the claim that a transformation during con-sistency training should be done with regard to the
training model.
5.2 Adversarial Natural Language Inference
Table 2 shows the experimental results on the ANLI
dataset with different training settings: training
with all the NLI datasets, or training with only the
ANLI dataset. Our method improves over base-
lines, including RoBERTa-Large (Liu et al., 2019)
and SMART (Jiang et al., 2020) in both settings.
Speciﬁcally, our method improves on an average
of 8.0 points in the test set from training with cross-
entropy loss only. Compared to SMART, which
combines smoothness regularization, i.e., a vari-
ation of V AT, and Bregman proximal point opti-
mization for ﬁnetuning, our method outperforms it
on an average of 1 point from the test set without
using other techniques such as Bregman proximal
point optimization.
6 Effectiveness of the White-box Search
Our central intuition behind the proposed method
is to generate the augmented samples concerning
the model, i.e., vulnerable to the model. This sec-
tion further conducts an ablation study on whether
such virtual adversarial search is crucial in discrete5649
space. We select the token among the top-K candi-
dates that would incur the highest divergence from
the model’s prediction. We compare with other
sampling strategies among the top-K candidates,
namely, uniform sampling (Uniform), selecting the
token with maximum MLM probability (Argmax),
and sampling from MLM probabilities (Sampling).
We match other training details except for the sam-
pling strategy for a fair comparison.
Table 3 illustrates the result of comparisons. The
virtual adversarial search among candidates out-
performs other search strategies in discrete space,
especially when the number of labeled samples is
limited. The result demonstrates that the virtual
adversarial search is indeed the crucial component
during perturbation. Furthermore, Fig. 2 shows the
indexes that our method selected from top-k dis-
tribution (sampling from YAHOO! dataset) during
training. The distribution of indexes selected from
our method resembles the uniform distribution;
however, as in the loss plot from Fig. 2, our method
searches for the diverse yet adversarial candidates
to the model, i.e., incurring high divergence.
7 Related Works
Consistency Regularization Consistency regu-
larization (Laine and Aila, 2017; Sajjadi et al.,
2016) has been mainly explored in the context of
SSL (Chapelle et al., 2009; Oliver et al., 2018).
A line of research in text-domain (Miyato et al.,
2017; Clark et al., 2018; Xie et al., 2020; Miy-ato et al., 2018; Jiang et al., 2020; Asai and Ha-
jishirzi, 2020) explored the idea. Existing studies
explored varying perturbation methods. Injecting
norm-constrained continuous noise to the embed-
ding space (Miyato et al., 2017; Jiang et al., 2020;
Liu et al., 2020; Chen et al., 2020b; Sato et al.,
2019) and directly perturbing the text (Clark et al.,
2018; Minervini and Riedel, 2019; Li et al., 2019;
Xie et al., 2020; Asai and Hajishirzi, 2020) via
discrete noise are the primary approaches for the
perturbation. Our method perturbs the sentence
by the discrete noise, yet the noise is generated
concerning the training model.
Adversarial Training Our method extends
the white-box-based adversarial training frame-
work (Goodfellow et al., 2014; Madry et al.,
2018), which has recently been explored widely
in NLP (Miyato et al., 2017; Ebrahimi et al., 2017;
Michel et al., 2019; Wang et al., 2019; Zhu et al.,
2020; Jiang et al., 2020; Liu et al., 2020). Cheng
et al. (2019) use adversarial training on machine
translation by discrete word replacements relying
on the label information, so not applicable to SSL
different from ours. There are also black-box ap-
proaches for generating the adversarial attacks or
test sets (Jia and Liang, 2017; Alzantot et al., 2018;
Ribeiro et al., 2018, 2019; Gardner et al., 2020)
to evaluate the vulnerability of the NLP models,
unlike our method, which utilizes gradient infor-
mation during training. Li et al. (2020); Garg and
Ramakrishnan (2020); Li et al. (2021) perturb input
using MLMs similar to ours but designed for an
attack so inefﬁcient for adversarial training.
Data Augmentation Synthetically generated
training examples are utilized to augment an ex-
isting dataset (Feng et al., 2021). Existing word-
level augmentation methods (Zhang et al., 2015;
Xie et al., 2017; Wei and Zou, 2019) are based on
heuristics. Mixup-based methods (Zhang et al.,
2018) interpolate input texts in hidden embed-
dings (Chen et al., 2020a; Guo et al., 2019) or input-
level (Yoon et al., 2021; Kim et al., 2021). Other
methods include utilizing back-translation mod-
els (Sennrich et al., 2016; Xie et al., 2020), contex-
tual language models (Kobayashi, 2018; Wu et al.,
2019), or generative models (Anaby-Tavor et al.,
2020; Yang et al., 2020). Unlike previous works,
our method is subject to the training model, thus
approximating the augmented points, efﬁciently
ﬁlling in gaps from the training data.5650Acknowledgements
We thank Jinhyuk Lee, Jaewook Kang, and Sung-
dong Kim for the discussion and feedback on the
paper. We also thank the members of the Conver-
sation team in Naver CLOV A for active discussion.
This research was supported by National Research
Foundation of Korea (NRF-2020R1A2C3010638)
and the Ministry of Science and ICT, Korea, under
the ICT Creative Consilience program (IITP-2022-
2020-0-01819).
References565156525653Algorithm 1: V AT_D Module
Input : input sentence x,index to perturb I
Output : perturbed sentence ˆ x
Function VAT_D( x,I):
ˆ x←x
form∈Ido
g←∇˜L(x,x)|
ˆx← argmaxδ(x,x)·g
Replacem-th token of ˆ xtoˆx
return ˆ x
A Training Details
Alg. 1 illustrates the procedure (V AT_D) to ac-
quire virtual adversarial tokens with the modi-
ﬁed consistency loss. We randomly select to-
ken indexes to perturb I, subject to the length of
the sentence. Considering multiple substitutions,
an exhaustive search over all possible combina-
tions to ﬁnd the optimal one is computationally in-
tractable. For efﬁcient generation during each train-
ing step, we replace multiple tokens simultaneously
instead of greedy search or beam search, which
has shown to work considerably well in previous
works (Ebrahimi et al., 2017; Cheng et al., 2019).
During training, the models are optimized with
standard cross-entropy and consistency loss with
an equal weight where we utilize KL-Divergence
as the divergence D. Our method takes approx-
imately 2.5 times the standard training whereas
other baselines (e.g., EDA, Back-translation) take
about 1.7 times the standard training. We utilize
P40 for training the SSL experiments and V100 for
the ANLI task.
In our preliminary experiment, utilizing the
MLM with masking was worse than that without
masking, similar to Li et al. (2020). While utilizing
the MLM for ﬁltering top-k candidates, we empiri-
cally veriﬁed that not applying masking operations
to the sentence achieved better performance than
doing so. We conjecture that the loss of informa-
tion when applying masking operation has evoked
the perturbed samples to signiﬁcantly deviate from
the original ones, resulting in a degradation in per-
formance. The ﬁnding matches that of Li et al.
(2020). Thus we do not apply masking operations
throughout the experiments. Moreover, we do not
ﬁne-tune the off-the-shelf MLM on the training cor-
pus but only the classiﬁcation model, which is to
ensure a fair comparison with other augmentation
baselines.
B Further Details on Data
The dataset statistics and split information regard-
ing topic classiﬁcation tasks and ANLI is presented
in Table B.1 and Table B.2.
ANLI (Nie et al., 2020) is an NLI testbed re-
cently introduced for evaluating the robustness of
the models in natural language understanding. The
dataset consists of three rounds (A1-A3), each con-
sisting of a train-dev-test set with increasing difﬁ-
culty, where the data is generated by human-and-
model-in-the-loop fashion to fool the strong pre-
trained models (Devlin et al., 2019; Yang et al.,
2019; Liu et al., 2019).
C Further Details on Baselines
For the SSL setup, we use the following baselines:
BERT (Devlin et al., 2019) We use the pre-
trained BERT-base-uncased model and ﬁnetune it
for the classiﬁcation dataset using only standard
cross-entropy loss.
EDA (Wei and Zou, 2019) EDA is a simple data
augmentation strategy based on word unit opera-
tions such as synonym replacement or deletion. We
perturb the unlabeled samples using EDAand ex-
ploit them for consistency training.
UDA (Xie et al., 2020) UDA paraphrases the sen-
tence using the back-translation. We employ the
WMT-19 DE↔EN model from fairseq(Ott et al.,56542019) to do the back-translation on unlabeled sam-
ples, and exploit them for consistency training.
V AT (Miyato et al., 2017, 2018) We re-
implement V AT where we apply the consistency
loss to the unlabeled samples.
D Augmentation Quality
We present some augmentation samples in Ta-
ble D.1 from three topic-classiﬁcation datasets. As
presented in the table, the augmentation samples
moderately modify some tokens from the original
sentence following the original context.
However, since we are decoding multiple tokens
at a same time, some samples are shown to be un-
grammatical (e.g., is→will instead of will be ).
Moreover, if the chosen token to be modiﬁed are
entities, the augmentation sample can sometimes
change the information presented in the sentence
(e.g., Patryk Dominik→Patryk Deinik ). However,
since we are solving the task of the closed-domain
topic classiﬁcation task, the problems didn’t mat-
ter much in this setting. If we are to solve the
knowledge-intensive task, we would have to con-
sider other ﬁltering modules for not changing the
entities.56555656