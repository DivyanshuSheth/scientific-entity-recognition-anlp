
Jiaan Wang, Fandong Meng, Duo Zheng, Yunlong Liang
Zhixu Li, Jianfeng Quand Jie Zhou
Abstract
To adapt text summarization to the multilingual
world, previous work proposes multi-lingual
summarization (MLS) and cross-lingual sum-
marization (CLS). However, these two tasks
have been studied separately due to the differ-
ent definitions, which limits the compatible and
systematic research on both of them. In this pa-
per, we aim to unify MLS and CLS into a more
general setting, i.e., many-to-many summariza-
tion (M2MS), where a single model could pro-
cess documents in any language and generate
their summaries also in any language. As the
first step towards M2MS, we conduct prelim-
inary studies to show that M2MS can better
transfer task knowledge across different lan-
guages than MLS and CLS. Furthermore, we
propose P , a pre-trained M2MS model
that learns language modeling, cross-lingual
ability and summarization ability via three-
stage pre-training. Experimental results indi-
cate that our P significantly outperforms
the state-of-the-art baselines, especially in the
zero-shot directions, where there is no training
data from the source-language documents to
the target-language summaries.
1 Introduction
The world we live in is multi-lingual. With global-
ization, text resources in various languages flood
the Internet, where global users can easily access
their desired information. Under this background,
the text summarization community presents multi-
lingual summarization (MLS) and cross-lingual
summarization (CLS), respectively. As shown in
Figure 1, MLS aims at building a unified model to
process documents in multiple languages and gen-
erate summaries in the corresponding language (Gi-
annakopoulos et al., 2015; Cao et al., 2020b; Hasan
et al., 2021b; Wang et al., 2021; Varab and Schluter,Figure 1: Illustration of (a) multi-lingual summarization,
(b) cross-lingual summarization and (c) many-to-many
summarization. XandYdenote the input document
and output summary in language i, respectively. En:
English; De: German; Zh: Chinese.
2021), while CLS generates a summary in the tar-
get language from the given document in a different
source language (Leuski et al., 2003a; Wan et al.,
2010; Wan, 2011; Yao et al., 2015; Zhu et al., 2019;
Ladhak et al., 2020; Perez-Beltrachini and Lap-
ata, 2021; Wang et al., 2022b,d,c, 2023). Despite
the close relationship between MLS and CLS ( e.g.,
both tasks involve more than one language and re-
quire models to distill the key information from
documents), previous work studies each task sep-
arately, hindering the systematic exploration for
both of them.
In this paper, we aim to unify MLS and CLS into
a more general setting named many-to-many sum-
marization (M2MS). As its name implies, the goal
of M2MS is to build a single summarization model
to process a document in any source language and
generate the corresponding summary in any given
target language. In this manner, one M2MS model
could perform more directions than MLS and CLS,
thus reducing the used parameters. For example,
one M2MS model involving nlanguages could re-
place one MLS model and n×(n−1)CLS models.
To provide a deeper understanding of M2MS, we
also conduct preliminary studies to systematically
compare M2MS with MLS and CLS, respectively.
In detail, following recent CLS work (Ladhak et al.,
2020; Perez-Beltrachini and Lapata, 2021), we use15127mBART-50 (Tang et al., 2021) as the summariza-
tion model, and train the model in the settings of
MLS, CLS and M2MS, respectively. After com-
paring the model performances, we find that the
model trained in M2MS setting can better trans-
fer task knowledge across different languages and
combine the advantages of those trained in MLS
and CLS settings. Therefore, we argue that it is
promising to unify MLS and CLS into M2MS.
Furthermore, we propose P, a pre-trained
M2MS model that learns language modeling, cross-
lingual ability and summarization ability via three
pre-training stages: (1) meta pre-training learns
the general language modeling knowledge from
multi-lingual unlabeled corpora; (2) cross-lingual
pre-training makes the model aware of the transfor-
mation between different languages based on par-
allel corpora; (3) task-specific pre-training utilizes
M2MS objective to simultaneously improve the
cross-lingual ability and the summarization abil-
ities of the model. Considering the high-quality
M2MS samples are non-trivial to collect, we lever-
age a simple strategy to construct pseudo M2MS
samples from multi-lingual unlabeled corpora. Dur-
ing the three-stage pre-training, P gradually
shifts from learning language modeling to the abil-
ities required by M2MS. Among them, the learned
cross-lingual ability plays a key role in enhancing
the knowledge transferability of the downstream
task ( i.e., summarization) from high-resource lan-
guages to low/zero-resource languages. Lastly, the
pre-trained P could be simply fine-tuned on
M2MS with input source-language documents and
output target-language summaries.
We evaluate P on the WikiLingua (Ladhak
et al., 2020) and CrossSum (Hasan et al., 2021a)
datasets. Experimental results show that P
achieves promising results compared with the state-
of-the-art baselines ( i.e., mBART-50 and mT5), es-
pecially in the zero-shot directions. Moreover, we
find that P is even able to generate summaries
for documents whose language never occurs in the
fine-tuning stage.
Our contributions are concluded as follows:
•To our knowledge, we are the first to unify MLS
and CLS into a more general setting (M2MS).
We also conduct preliminary studies to provide
deeper analyses among MLS, CLS and M2MS.
•We propose P , a pre-trained M2MS modelthat learns language modeling, cross-lingual abil-
ity and summarization ability through a carefully
designed three-stage pre-training.
•We conduct extensive experiments and show that
ourP achieves new state-of-the-art perfor-
mance on the large-scale benchmark datasets. Be-
sides, the effectiveness of P in low/zero-
resource languages is also demonstrated.
2 Related Work
Multi-Lingual Summarization. Multi-lingual
summarization (MLS) aims to process documents
in multiple languages and generate their summaries
in the corresponding language. Giannakopoulos
et al. (2015) present MultiLing-2015 dataset. Later,
this task receives increasing attention (Vanetik and
Litvak, 2015; Litvak et al., 2016). Recently, large-
scale MLS datasets (Scialom et al., 2020; Varab
and Schluter, 2021; Hasan et al., 2021b; Feng et al.,
2022; Liang et al., 2022a) together with sophisti-
cated methods (Cao et al., 2020b; Chi et al., 2020;
Wang et al., 2021; Li et al., 2023) are proposed
one after another. Considering the close relation
between MLS and CLS, Cao et al. (2020b); Feng
et al. (2022) also evaluate the MLS models on CLS
to show their zero-shot CLS ability.
Cross-Lingual Summarization. Given documents
in one language, cross-lingual summarization
(CLS) generates summaries in another language.
Early work typically focuses on pipeline meth-
ods (Leuski et al., 2003b; Or ˘asan and Chiorean,
2008; Wan et al., 2010; Wan, 2011; Yao et al.,
2015), i.e., translation and then summarization or
summarization and then translation. Recently, with
the availability of large-scale CLS datasets (Zhu
et al., 2019; Ladhak et al., 2020; Perez-Beltrachini
and Lapata, 2021; Wang et al., 2022b; Chen et al.,
2022; Zheng et al., 2023), many researchers shift
the research attention to end-to-end CLS models,
including multi-task learning (Cao et al., 2020a;
Bai et al., 2021; Liang et al., 2022b), knowledge
distillation (Nguyen and Tuan, 2022), resource-
enhanced (Zhu et al., 2020) and pre-training (Xu
et al., 2020; Chi et al., 2021) approaches. Among
them, most CLS work separately builds CLS mod-
els in each cross-lingual direction except for Hasan
et al. (2021a), who jointly train mT5 (Xue et al.,
2021) in multiple directions.
Different from previous MLS and CLS, we unify
them into a more general setting (M2MS) starting
from the training stage. Besides, we are the first to15128
systematically investigate the capabilities of mod-
els trained with MLS, CLS and M2MS settings.
Pre-Trained Models for Summarization. Pre-
trained models have shown their superiority in sum-
marization task, e.g., BART (Lewis et al., 2020)
and T5 (Raffel et al., 2020). To enhance the sum-
marization ability during the pre-training stage, P- (Zhang et al., 2020a) introduces the gap
sentence generation (GSG) objective to enable the
model to generate key sentences in an article from
the remaining ones. Further, P (Xiao et al.,
2022) extends GSG from single-document to multi-
document summarization. In dialogue scenarios,
Wang et al. (2022b) present m DBART for cross-
lingual dialogue summarization.
Among these pre-trained summarization models,
P andP only focus on monolin-
gual summarization. Though m DBART aims
at CLS, the model is merely built for a single cross-
lingual direction ( i.e., English ⇒German/Chinese)
and a specific scenario ( i.e., dialogue). Our P
is the first multi-lingual pre-trained model for gen-
eral summarization.
3Does Unifying All Directions in a Single
Model Help Each Other?
As discussed previously, M2MS unifies all summa-
rization directions in a single model. Therefore, we
wonder can such a setting help the model better
transfer task knowledge across different languages
compared with the settings of MLS and CLS? To an-
swer the question, we conduct preliminary studiesto investigate the influence of different settings.
3.1 Setup
Data. The preliminary studies are conducted on
WikiLingua (Ladhak et al., 2020), one of the largest
CLS datasets. We focus on six languages, i.e., En-
glish (En), French (Fr), Hindi (Hi), Chinese (Zh),
Thai (Th) and Turkish (Tr). Among them, Tr serves
as a zero-resource language, whose documents and
summaries only appear in the validation and test
sets. More details are given in Section 5.1.
Summarization Model. Following recent CLS
literature (Ladhak et al., 2020; Perez-Beltrachini
and Lapata, 2021), we use mBART-50 (Tang et al.,
2021) as the summarization model, and train the
model in the following four settings:
•mBART ( ONE): We separately train several mod-
els, each of which is built and evaluated in one
single direction. When the direction is cross-
lingual (or monolingual), the corresonding model
is a CLS (or monolingual summarization) model.
•mBART ( U-CLS ): We train a unified model with
all cross-lingual samples, and test the model in
all directions.
•mBART ( MLS): We train one unified model with
monolingual samples in all languages. Then, the
trained model is evaluated in all directions.
•mBART ( M2MS ): It is a new setting introduced
by this work, where the model is both trained and
evaluated in all directions.15129
3.2 Analytic Results
Table 1 shows the results in terms of R (Lin,
2004) and BS (Zhang et al., 2020b).
mBART (M2MS )vs. mBART (CLS).The results
in all directions show that mBART ( M2MS ) out-
performs mBART ( CLS) in all metrics, illustrating
that unifying all directions in a single model could
transfer task knowledge across different languages.
mBART (M2MS )vs. mBART (MLS).Comparing
mBART ( M2MS ) and mBART ( MLS), it is apparent
to find that mBART ( M2MS ) significantly outper-
forms mBART ( MLS) in cross-lingual directions
(e.g., 26.9 vs. 11.7 R -1in average), while
achieving competitive results in monolingual direc-
tions ( e.g., 33.9 vs. 34.2 R -1 in average).
To give a deeper understanding of why mBART
(MLS) performs poorly in cross-lingual directions,
we analyze its generated summaries and find that
most of them are not in the language we expected.
Table 2 shows the rate of the generated summaries
in the correct language.The languages of the
generated summaries are detected by fastlangid.
Compared with mBART ( M2MS ), mBART ( MLS)
struggles to generate summaries in the target lan-
guage. We conjecture this is because that mBART
(MLS) is only trained with monolingual data from
multiple languages without any cross-lingual sig-
nals, resulting in limited cross-lingual ability.
Based on the above analyses, we argue that the
summarization signals from cross-lingual direc-
tions could help mBART ( M2MS ) perform CLS and
transfer the task knowledge to zero-shot directions,
while mBART ( MLS) does not own such abilities.
mBART (M2MS )vs. mBART (U-CLS ).The only
difference between mBART ( M2MS ) and mBART
(U-CLS ) is that the training data of mBART
(M2MS ) contains all monolingual samples, while
mBART ( U-CLS ) does not. We find that the
performance gap between mBART ( M2MS ) and
mBART ( U-CLS ) is extremely smaller than thatbetween mBART ( M2MS ) and mBART ( CLS) /
mBART ( MLS). In detail, mBART ( M2MS ) outper-
forms mBART ( U-CLS ) in most directions when
the source and the target languages have been seen
during the fine-tuning stage, i.e., the source and
the target languages are from {En, Fr, Hi, Zh,
Th}. However, when the source or target language
is unseen ( i.e., Tr), the performance of mBART
(M2MS ) is slightly worse than mBART ( CLS). This
is because the monolingual training data used in
mBART ( M2MS ) makes the word embeddings of
the unseen languagedrift away from those of other
languages (see details in Appendix A). Addition-
ally, the cross-lingual signal between the unseen
language and other languages never occurs in the
fine-tuning stage, making it difficult to summarize
from or to the unseen language.
3.3 Preliminary Conclusion
The preliminary studies comparing mBART trained
in different settings indicate that (1) the multi-
lingual model trained in M2MS setting can better
transfer task knowledge across different languages
than those trained in the settings of MLS, CLS
and unified CLS. (2) Compared with unified CLS,
M2MS helps the model achieve better transferabil-
ity across visible languages, but sacrifices the trans-
ferability to unseen languages.
Grounding the above analyses, we argue that it is
valuable to unify previous MLS and CLS to M2MS.
Meanwhile, how to improve the transferability to
unseen languages becomes a keypoint in M2MS.
4 P
In this section, we propose P , a pre-trained
multi-lingual model for M2MS with the backbone
of transformer (Vaswani et al., 2017).
Figure 2 shows the overview of P , which
contains three pre-training stages. Specifically, the
meta pre-training (§ 4.1) lets the pre-trained model
learn general language modeling via monolingual
denoising objective in multiple languages. Then,
to improve the transferability across different lan-
guages, the cross-lingual pre-training (§ 4.2) adds
noises to the source-language sentences, and en-
courages the model to translate them into parallel
sentences in the target language. Note that the par-
allel sentences used in this stage might involve the
languages which are not seen in downstream tasks,15130
and it is the key to improving the transferability
to these languages. Finally, to narrow the gap be-
tween the pre-training and fine-tuning stages, the
task-specific pre-training (§ 4.3) trains the model
with pseudo M2MS samples, which are constructed
from the multi-lingual unlabeled corpora via gap
sentences selection and machine translation. Dur-
ing the three-stage pre-training process, the model
gradually learns the ability of language modeling,
then the cross-lingual ability, and finally the adap-
tation to the specific task.
4.1 Meta Pre-Training
The goal of meta pre-training is to provide good
initialization for the subsequent pre-training stages.
Here, we directly utilize mBART-50 (Tang et al.,
2021) as the meta pre-trained model.
mBART-50 is a multi-lingual BART (Lewis
et al., 2020) with the transformer encoder-decoder
architecture. The model is pre-trained on large-
scale multi-lingual unlabeled corpora to learn the
multi-lingual language modeling. Specifically, fol-
lowing BART, the denoising task is used as the
pre-training objective, and there are two types of
noise: (1) text infilling randomly masks text spans
in text sequences, and (2) sentence permutation ran-
domly shuffles sentences in documents. The model
is required to comprehend the noisy text sequences
and recover them. To indicate the input and out-
put languages, the language tags ( e.g.,<En> and
<Zh> ) are appended at the inputs of encoder and
decoder sides, respectively.4.2 Cross-Lingual Pre-Training
Despite the effectiveness of mBART-50, the input
and output sequences in its pre-training stage are
always in the same language, resulting in the under-
explored cross-lingual ability. However, such abil-
ity is indispensable for M2MS. Therefore, cross-
lingual pre-training is designed to improve the
cross-lingual transferability.
In detail, we propose a simple yet effective pre-
training task, i.e., cross-lingual denoising, which
lets the model generate sentences in the target lan-
guage based on their noisy parallel sentences in a
different source language. The noise used in this
stage is text infilling . In this way, the pre-trained
model is required to not only understand the text in
the source language but also learn the transforma-
tion between different languages.
4.3 Task-Specific Pre-Training
Task-specific pre-training aims to narrow the gap
between the pre-training and fune-tuning stages.
We directly adopt M2MS as its pre-training task.
Grounding the truth that high-quality M2MS sam-
ples are difficult to collect, we construct the pseudo
samples from multi-lingual unlabeled corpora.
In detail, for a source-language document D=
{s}, where sdenotes the i-th sentence in
D. Following previous monolingual pre-trained
summarization methods (Zhang et al., 2020a; Xiao
et al., 2022), we calculate the importance of each
sentence as S(s) = R -1(s, D/s),
where D/sindicates the rest of the document
aftersis removed. The sentences with high im-
portance are selected as the gap sentences S=15131
{s} (g∈ {1,2, ...,|D|}), which are fur-
ther translated to a different target language S=
{s} via Google Translation. In this man-
ner, the source-language document Dpaired with
source/target-language gap sentences S/S
could constitute a pseudo pre-training sample.
Quality Controlling. Since machine translation re-
sults might contain flaws, we further employ round-
trip translation strategy as suggested by Zhu et al.
(2019) and Feng et al. (2022). For each gap sen-
tence sinD, the translated counterpart sis
translated back to the source language, which we
denote as s. If the R -1score between s
andsis less than the pre-defined threshold λ,
the corresponding pseudo sample will be discarded.
Input Format. To help the model trade off be-
tween (1) generating new sentences instead of trans-
lating part of input sentences, and (2) learning
the translation pattern(Zhu et al., 2020), half of
source-language gap sentences in Dare randomly
masked with a special token <mask-sent> .
5 Experiments
5.1 Benchmark Datasets
In order to evaluate M2MS models, two require-
ments should be met in datasets, i.e., (1) involving
multiple languages and summarization directions,
and (2) having abundant samples in each direction.
Thus, we choose WikiLingua (Ladhak et al., 2020)
and CrossSum (Hasan et al., 2021a).The original WikiLingua dataset, which involves
18 languages, is designed for CLS task. The 18 lan-
guages constitute 306 (18 ×17) cross-lingual direc-
tions, each of which contains about 18k CLS sam-
ples in average. For each document, WikiLingua
also contains its summary in the original language.
Therefore, the dataset could be used to evaluate
M2MS models. However, the original splitting is
for CLS. Thus, we re-split WikiLingua with the spe-
cial consideration for M2MS: for each document in
the test (or validation) set of one direction, the doc-
ument and its parallel documentsare not allowed
to appear in the training and validation (or test) sets
of other directions. This rule reduces the likelihood
that learning shortcuts. We also intentionally create
several zero-shot directions.
We focus on six languages in this work: En-
glish (En), Chinese (Zh), French (Fr), Hindi (Hi),
Turkish (Tr) and Thai (Th). After re-splitting, the
statistics are shown in Table 3. There are 9 high-
resource directions each of which contains more
than 10k training samples. The other 8 directions
with less than 10k training samples are consid-
ered as low-resource directions . The remaining
19 zero-shot directions have no training sample.
According to whether both the source and target
languages appear in the whole training set , we
further divide them into 11 non-trivial and 8 con-
ventional zero-shot directions . Note that Tr never
appears in the training set of any direction, thus,
in other words, the non-trivial zero-shot directions
involve Tr while the conventional counterparts do
not. We call Tr an unseen language . Though there
is no training data in a conventional zero-shot di-
rection, both its source and target languages might15132
have training data with a pivot language, making
it less challenging than the non-trivial ones. Tak-
ing the conventional zero-shot direction Hi ⇒Zh
as an example, the training data in Hi ⇒En and
En⇒Zh could bridge the gap between Hi and Zh.
For statistics of the CrossSum dataset used in our
experiments, please refer to Appendix C.1.
5.2 Experimental Setup
Baselines. We use mBART-50 (Tang et al., 2021)
and mT5 (Xue et al., 2021) as baselines, which have
achieved state-of-the-art performances on many
CLS/MLS datasets (Perez-Beltrachini and Lapata,
2021; Hasan et al., 2021a; Feng et al., 2022).
Metrics. We adopt R -1/2/(Lin, 2004) and
BS (Zhang et al., 2020b) in our exper-
iments. The R scores measure the lexical
overlap between the generated summaries and cor-
responding references, while the BS mea-
sures the semantic similarity. These metrics are cal-
culated by multi-lingual rougeandbert-score
toolkits, respectively. The BS is based on
bert-base-multilingual-cased model. The statistical
significance test (Koehn, 2004) is also employed
for a fair comparison.
Implementation Details. The implementation de-
tails of the pre-training objectives, pre-training cor-
pora and fine-tuning hyper-parameters are given in
Appendix B.5.3 Quantitative Results
Table 4 shows the results on WikiLingua in terms
of average R score ( R) and BS
(B). Full results on R -1/2/are given in
Appendix D. The experimental results on Cross-
Sum also verify the superiority of P , which
are provided in Appendix C.2.
P vs. Baselines. OurP outperforms
mBART-50 and mT5 in all directions, indicating its
superiority. Specifically, P achieves an aver-
age increase of 7.9 Rand 5.4 Bover mBART-50
in non-trivial zero-shot directions when the target
language is not Tr. Compared with mBART-50,
the average improvement in conventional zero-shot
directions is 2.2 R/ 1.3 B, while the counterpart
in low-resource directions is 1.4 R/ 0.8 B. As
for high-resource directions, P outperforms
mBART-50 by 0.7 Rand 0.3 Bin average. It is
not difficult to find that the fewer resources in a di-
rection, the greater the improvement brought by our
P . This finding also indicates the potentiality
of our model when faced with the real-world sce-
nario, since there are thousands of languages in the
world and most directions are low-resource or zero-
shot. Through the cross-lingual and task-specific
pre-training stages, P facilitates the transfer
of task knowledge from high-resource directions to
the low-resource and zero-shot ones.
Non-Trivial Zero-Shot Direction. As shown in Ta-
ble 4, we divide the non-trivial zero-shot directions
into two categories ( i.e., Tr⇒Others and Any ⇒Tr)
according to whether Tr is the target language. We15133
discover that the results in Any ⇒Tr directions
are significantly worse than the Tr ⇒Others coun-
terparts. This finding suggests that generating
summaries in unseen languages is more difficult
than understanding documents in unseen languages .
This is because the encoder could partly understand
theunseen languages through the shared vocabu-
lary and the similar syntax constituent with other
languages. But for the decoder, we only change its
language tag to expect it can generate summaries in
unseen languages . This requires the decoder to si-
multaneously (1) capture the relationships between
the unseen language tag and the unseen language to-
kens and (2) summarize documents. However, the
pre-trained model only meets the requirement (1)
in the pre-training stage, while requirement (2)
in the fine-tuning stage, making it hard to simulta-
neously meet both requirements, and consequently,
cannot generate summaries in unseen languages.
We reserve this challenge for future work.
Ablations. We conduct ablation studies to investi-
gate the effect of the cross-lingual and task-specific
pre-training stages. We run the following ablations:
•P w/o TS . To demonstrate the effective-
ness of the task-specific pre-training, we also
pre-train a variant P model which does not
include the task-specific pre-training stage.
•P w/o CL . To measure the effectiveness
of the cross-lingual pre-training, we remove this
stage in the whole pre-training process, resulting
in another variant P .
•P w/o TS & CL removes both the cross-
lingual and task-specific pre-training stages,
which is the same as mBART-50.
As shown in Table 5, we conduct ablation studies
in several conventional zero-shot directions (results
in more directions are provided in Appendix E). In
each case, the RandBare lower than vanilla
P . In addition, both P w/o TS and
P w/o CL outperform P w/o TS & CL.
Thus, the effectiveness of both stages is proved.
5.4 Qualitative Results
Human Evaluation. Following Zhu et al. (2020);
Liang et al. (2022b), we conduct the human evalua-
tion on 50 random samples extracted from WikiLin-
gua (En ⇒Zh, Zh ⇒En and En ⇒En, respectively).
Three graduate students are invited to assess the
generated summaries from three aspects: informa-
tiveness (IF), conciseness (CC) and grammaticality
(GM). The scoring adopts a 5-point scale from 1
(worst) to 5 (best). Table 6 shows the average re-
sults. The IF, CC and GM scores of P are
significantly better than those of mT5 or mBART-
50, demonstrating the effectiveness of our model.
Case Study. Table 7 shows an example Turkish
document, the generated summary and the ground
truth summary. Though the summary generated by
P contains a repeated sentence, it has good
overlaps with the ground truth. But for mBART-50,
the generated summary is not relevant to the core
idea of the document. This observation indicates
that, through the cross-lingual and task-specific
pre-training, P could better transfer the task
knowledge from high-resource directions to zero-
shot ones, and even has the ability to generate sum-
maries for the documents whose language does not
occur in the fine-tuning stage.15134Error Analysis. To further study how future re-
search could advance M2MS, we take a closer look
at the generation errors of P and analyze them
in Appendix F.
6 Conclusion
In this paper, we unify MLS and CLS to M2MS.
Through carefully-designed preliminary studies,
we discuss that unifying MLS and CLS to M2MS is
valuable. In addition, we propose P , the first
pre-trained M2MS model, which contains three pre-
training stages to enable the model learn the multi-
lingual language modeling, cross-lingual ability
and summarization ability. Extensive experiments
show its superiority compared with the state-of-
the-art baselines (mBART-50 and mT5). The case
study further demonstrates that our model could
even generate summaries for the documents whose
language does not occur in the fine-tuning stage.
Ethical Considerations
In this section, we consider potential ethical issues
of our model. In this paper, we propose P
which utilizes mBART-50 (Tang et al., 2021) as the
meta pre-trained model and further suffers from
the cross-lingual pre-training and task-specific pre-
training stages. The pre-training samples are con-
structed from OPUS (Tiedemann and Thottingal,
2020) and mC4 (Xue et al., 2021) corpora. To
construct the pseudo M2MS samples in the task-
specific pre-training stage, Google Translation is
also adopted to translate gap sentences. Therefore,
P might involve the same biases and toxic be-
haviors exhibited by language models, pre-training
corpora and Google Translation.
Limitations
While we show that P outperforms mBART-
50 on WikiLingua (Ladhak et al., 2020), there are
some limitations worth considering in future work:
(1)P still struggles to generate summaries in
unseen languages (Section 5.3); (2) In this work,
we focus on six languages in total, and future work
could extend our method to more languages.
Acknowledgements
This work is supported by the National Natu-
ral Science Foundation of China (No.62072323,
62102276), Shanghai Science and Technology In-
novation Action Plan (No. 22511104700), the Nat-
ural Science Foundation of Jiangsu Province (GrantNo. BK20210705), the Natural Science Foundation
of Educational Commission of Jiangsu Province,
China (Grant No. 21KJD520005) and the Priority
Academic Program Development of Jiangsu Higher
Education Institutions.
References151351513615137
A Word Embeddings of the Unseen
Language and Other Languages
To verify the word embeddings of the unseen lan-
guage drift away from those of other languages af-
ter adding the monolingual training data, based on
MUSE dictionary, we choose top frequent 1000 En-
glish words and the words with the same meaning
in other five languages ( i.e., Fr, Hi, Zh, Th and Tr).
Then, we calculate the embeddings of these words
based on mBART ( M2MS ) and mBART ( U-CLS ),
respectively. For the word that consists of multiple
tokens, the word embedding is the average of em-
beddings of those tokens. As shown in Figure 3, we
utilize Principal Component Analysis (PCA) to vi-
sualize the word embeddings from mBART ( M2MS )
and mBART ( U-CLS ). In the PCA space, we fur-
ther calculate the central point of each language
by averaging the word embeddings in the language.
Then, we find the average distance between the cen-
tral point of Tr and other languages is 0.426 / 0.407
for mBART ( M2MS ) / mBART ( U-CLS ). This dis-
tance in vanilla mBART-50 (Tang et al., 2021) is
0.398. Therefore, the monolingual training data
used in mBART ( M2MS ) makes the word embed-
dings of the unseen language drift away from those
of other languages.
B Implementation Details
Pre-Training Details. We use mBART-50 (Tang
et al., 2021) as the meta pre-trained model, and
futher pre-train it via cross-lingual and task-
specific pre-training stages. The implementation
of mBART-50 is based on the Transformers (Wolf
et al., 2020) library with default settings (12 en-
coder layers, 12 decoder layers and 1024 hidden
states). In cross-lingual pre-training, we dynami-
cally mask 0-15% tokens in the source-language
sentences, and construct 20.6M samples from
OPUS parallel corpora (Tiedemann and Thottingal,
2020). In task-specific pre-training, we construct
3.1M training samples from mC4 corpus (Xue et al.,
2021). We set the total length of gap sentences to
k% of the document length, and kis dynamically15138
selected from [5,10,15]. The pre-defined λin the
round-trip translation is 0.7. All experimental re-
sults listed in this paper are the average of 3 runs.
Table 8 and Table 9 show the statistics of the con-
structed samples in the cross-lingual pre-training
and task-specific pre-training stages, respectively.
The cross-lingual pre-training and task-specific pre-
training stages are conducted on 8 NVIDIA Tesla
V100 GPUs with 32GB memory. In the cross-
lingual pre-training stage, we pre-train the model
for 150K steps, with early stopping, 32 batch size,
3e-5 learning rate following Xiao et al. (2022) and
10K warmup steps. In the task-specific pre-training
stage, we pre-train the model for 100K steps, with
early stopping, 4 batch size, 3e-5 learning rate and
10K warmup steps.
Fine-Tuning and Testing Details. In the fine-
tuning stage, we fine-tune the P model on 8
NVIDIA Tesla V100 GPUs (32G) with 4 batch size,
10 epochs, 2K warmup steps, 3e-5 learning rate,
and set the maximum number of tokens for input se-
quences to 1024. To balance the high-resource and
low-resource language data, following Xue et al.
(2021), we sample the training examples according
to the probability p(D)∝ |D|, where p(D)is
the probability of sampling training examples from
a give direction during fine-tuning and |D|is the
number of original examples in the direction. We
set the hyperparameter αto 0.5. To fine-tune mT5
baseline on M2MS, the language tags ( e.g.,<En>and<Zh> ) are appended at the inputs of both en-
coder and decoder sides. In the test process, we set
the beam size and the maximum decoded length to
5 and 128, respectively.
C Experiments on CrossSum
C.1 Data Statistics.
Table 10 lists the data statistics of the CrossSum
dataset (Hasan et al., 2021a) used in our experi-
ments. The data splitting mainly inherits from the
original CrossSum except for zero-shot directions
and monolingual directions: (1) If the number of
samples in a direction ( e.g., Fr⇒Hi) is less than
1k, we will regard the direction as a zero-shot di-
rection and evenly split its samples into validation
and test sets. (2) Considering the number of sam-
ples in cross-lingual directions is hundred-level or
thousand-level, we truncate the number of samples
in each monolingual direction ( e.g., En⇒En) to
10k to make a balance. The corresponding splitting
follows 8:1:1. If the number of samples in a mono-
lingual direction ( e.g., Th⇒Th) is less than 10k, its
splitting follows the original CrossSum.
C.2 Experimental Results.
Table 11 shows the experimental results on Cross-
Sum. Our P outperforms mBART-50 by 2.3
R -1, 2.0 R -2, 2.0 R -and 1.3
BS in the average of all directions, which15139
verifies the effectiveness of P . For the av-
erage results in all zero-shot directions, mBART-
50 achieves 33.8, 15.7, 28.1 and 67.1 in terms of
R -1/2/andBS . The counterparts
ofP are 37.9, 19.6, 31.8 and 69.3, showing
its superiority in the zero-shot directions.
D Full Results on WikiLingua
Table 12 shows the experimental results in terms of
R -1, R -2 and R -, respectively.
E Ablations in Conventional Zero-Shot
Directions
Table 13 shows the ablation results in all conven-
tional zero-shot directions.
F Error Analysis
We first randomly select 100 summaries generated
byP on WikiLingua (En ⇒Zh). After manu-
ally examining the generated summaries, we find
the following major error types:
•Missing Information : part of the information
in the ground truth summary is not mentioned inthe generated summary. This is the most frequent
error type, and accounts for 39% of the generated
summaries.
•Faithfulness : the generated summary involves
information that is inconsistent with (or not pre-
sented in) the source document. We find 32% of
the summaries have this error.
•Redundancy : the generated summary contains
additional information beyond the ground truth
summary. 17% of the generated summaries con-
tain this error.
•Foreign Words : the generated summary involves
words in another language. 9% of the generated
Chinese summaries involve some (typically one
or two) words in another language.
Redundancy and missing information are two
major flaws caused by the limited summarization
ability (Johner et al., 2021). Faithfulness error is an-
other error type that has been noticed in the summa-
rization research field recently (Huang et al., 2021).
The neural generative summarization models are
highly prone to generate factual inconsistency er-
rors (Huang et al., 2021). Some studies (Kryscinski
et al., 2020; Wang et al., 2022a) show that over15140
30% of the summaries generated by neural mod-
els contain this error. We confirm that CLS also
involves the faithfulness error. Future work could
give deeper and more fine-grained analyses of this
error type.
The issue of foreign words could also refer to the
code-switching phenomenon (Pfaff, 1979). Note
that the generated foreign words are not limited in
the source language. In several cases, the generated
Chinese summaries of the given English documents
even involve Thai words. We also find the seman-
tics of these foreign words are typically coherent
with their context. This error type might be caused
by the cross-lingual pre-training (which bridges the
representation gap of parallel words in different
languages) in P .15141ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.15142/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.15143