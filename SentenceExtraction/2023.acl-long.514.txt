
Siyu Yuan, Deqing Yang,
Jinxi Liu, Shuyu Tian, Jiaqing Liang, Yanghua Xiao, Rui XieSchool of Data Science, Fudan University, Shanghai, ChinaShanghai Key Laboratory of Data Science, School of Computer Science, Fudan UniversityFudan-Aishu Cognitive Intelligence Joint Research Center,Meituan, Beijing, China
{syyuan21,jxliu22, sytian21}@m.fudan.edu.cn ,
{yangdeqing,liangjiaqing,shawyh}@fudan.edu.cn ,rui.xie@meituan.com
Abstract
Concepts benefit natural language understand-
ing but are far from complete in existing knowl-
edge graphs (KGs). Recently, pre-trained lan-
guage models (PLMs) have been widely used
in text-based concept extraction (CE). How-
ever, PLMs tend to mine the co-occurrence as-
sociations from massive corpus as pre-trained
knowledge rather than the real causal effect
between tokens. As a result, the pre-trained
knowledge confounds PLMs to extract biased
concepts based on spurious co-occurrence cor-
relations, inevitably resulting in low precision.
In this paper, through the lens of a Structural
Causal Model (SCM), we propose equipping
the PLM-based extractor with a knowledge-
guided prompt as an intervention to alleviate
concept bias. The prompt adopts the topic of
the given entity from the existing knowledge
in KGs to mitigate the spurious co-occurrence
correlations between entities and biased con-
cepts. Our extensive experiments on repre-
sentative multilingual KG datasets justify that
our proposed prompt can effectively allevi-
ate concept bias and improve the performance
of PLM-based CE models. The code has
been released on https://github.com/
siyuyuan/KPCE .
1 Introduction
The concepts in knowledge graphs (KGs) enable
machines to understand natural languages better,
and thus benefit many downstream tasks, such as
question answering (Han et al., 2020), common-
sense reasoning (Zhong et al., 2021) and entity typ-
ing (Yuan et al., 2022). However, the concepts, es-
pecially the fine-grained ones, in existing KGs still
need to be completed. For example, in the widely
used Chinese KG CN-DBpedia (Xu et al., 2017),
there are nearly 17 million entities but only 0.27
million concepts in total, and more than 20% enti-
ties even have no concepts. Although Probase (WuFigure 1: The example of concept bias. The PLM-based
CE models are biased to extract novel mistakenly as the
concept of Louisa May Alcott from the text.
et al., 2012) is a large-scale English KG, the fine-
grained concepts with two or more modifiers in it
only account for 30% (Li et al., 2021). We focus
on extracting multi-grained concepts from texts to
complete existing KGs.
Most of the existing text-based concept acquisi-
tion approaches adopt the extraction scheme, which
can be divided into two categories: 1)pattern-
matching approaches (Auer et al., 2007; Wu
et al., 2012; Xu et al., 2017), which can obtain
high-quality concepts but only have low recall
due to poor generalization; 2)learning-based ap-
proaches (Luo et al., 2020; Ji et al., 2020; Yuan
et al., 2021a), which employ pre-trained language
models (PLMs) fine-tuned with labeled data to ex-
tract concepts.
However, an unignorable drawback of these
learning-based approaches based on PLMs is con-
cept bias . Concept bias means the concepts are
extracted based on their contextual (co-occurrence)
associations rather than the real causal effect be-
tween the entities and concepts, resulting in low
extraction precision. For example, in Figure 1,
PLMs tend to extract novel andwriter together as
concepts for the entity Louisa May Alcott even if
we explicitly input the entity Louisa May Alcott to
the model. Previous work demonstrates that causal
inference is a promising technique for bias analy-9255sis (Lu et al., 2022). To analyze the reasons behind
concept bias, we devise a Structural Causal Model
(SCM) (Pearl, 2009) to investigate the causal effect
in the PLM-based concept extraction (CE) system,
and show that pre-trained knowledge in PLMs con-
founds PLMs to extract biased concepts. During
the pre-training, the entities and biased concepts
(e.g.,Louisa May Alcott andnovel ) often co-occur
in many texts. Thus, PLMs tend to mine statistical
associations from a massive corpus rather than the
real causal effect between them (Li et al., 2022),
which induces spurious co-occurrence correlations
between entities ( i.e.,Louisa May Alcott ) and bi-
ased concepts ( i.e.,novel ). Since we cannot directly
observe the prior distribution of pre-trained knowl-
edge, the backdoor adjustment is intractable for
our problem (Pearl, 2009). Alternatively, the front-
door adjustment (Peters et al., 2017) can apply a
mediator as an intervention to mitigate bias.
In this paper, we adopt language prompting (Gao
et al., 2021; Li and Liang, 2021) as a mediator for
the frontdoor adjustment to handle concept bias.
We propose a novel Concept Extraction framework
with Knowledge-guided Prompt, namely KPCE to
extract concepts for given entities from text. Specif-
ically, we construct a knowledge-guided prompt by
obtaining the topic of the given entity ( e.g.,per-
sonforLouisa May Alcott ) from the knowledge in
the existing KGs. Our proposed knowledge-guided
prompt is independent of pre-trained knowledge
and fulfills the frontdoor criterion. Thus, it can
be used as a mediator to guide PLMs to focus on
the right cause and alleviate spurious correlations.
Although adopting our knowledge-guided prompt
to construct the mediator is straightforward, it has
been proven effective in addressing concept bias
and improving the extraction performance of PLM-
based extractors in the CE task.
In summary, our contributions include: 1)To the
best of our knowledge, we are the first to identify
the concept bias problem in the PLM-based CE
system. 2)We define a Structural Causal Model to
analyze the concept bias from a causal perspective
and propose adopting a knowledge-guided prompt
as a mediator to alleviate the bias via frontdoor ad-
justment. 3)Experimental results demonstrate the
effectiveness of the proposed knowledge-guided
prompt, which significantly mitigates the bias and
achieves a new state-of-the-art for CE task.2 Related Work
Concept Acquisition Most of the existing text-
based concept acquisition approaches adopt the
extraction scheme, which can be divided into two
categories: 1) Pattern-matching Approaches : ex-
tract concepts from free texts with hand-crafted
patterns (Auer et al., 2007; Wu et al., 2012; Xu
et al., 2017). Although they can obtain high-quality
concepts, they have low recall due to their poor gen-
eralization ability; 2) Learning-based Approaches :
mostly employ the PLM-based extraction models
from other extraction tasks, such as the Named En-
tity Recognition (NER) models (Li et al., 2020; Luo
et al., 2021; Lange et al., 2022) and Information
Extraction models (Fang et al., 2021; Yuan et al.,
2021a) in the CE task. Although they can extract
many concepts from a large corpus, the concept
bias cannot be well handled.
Causality for Language Processing Several re-
cent work studies causal inference combined with
language models for natural language processing
(NLP) (Schölkopf, 2022), such as controllable text
generation (Hu and Li, 2021; Goyal et al., 2022)
and counterfactual reasoning (Chen et al., 2022;
Paranjape et al., 2022). In addition, causal in-
ference can recognize spurious correlations via
Structural Causal Model (SCM) (Pearl, 2009) for
bias analysis and eliminate biases using causal
intervention techniques (Weber et al., 2020; Lu
et al., 2022). Therefore, there are also studies
showing that causal inference is a promising tech-
nique to identify undesirable biases in the NLP
dataset (Feder et al., 2022) pre-trained language
models (PLMs) (Li et al., 2022). In this paper, we
adopt causal inference to identify, understand, and
alleviate concept bias in concept extraction.
Language Prompting Language prompting can
distill knowledge from PLMs to improve the model
performance in the downstream task. Language
prompt construction methods can be divided into
two categories (Liu et al., 2021a): 1) Hand-crafted
Prompts , which are created manually based on hu-
man insights into the tasks (Brown et al., 2020;
Schick and Schütze, 2021; Schick and Schutze,
2021). Although they obtain high-quality results,
how to construct optimal prompts for a certain
downstream task is an intractable challenge; 2) Au-
tomated Constructed Prompts , which are generated
automatically from natural language phrases (Jiang
et al., 2020; Yuan et al., 2021b) or vector space (Li9256
and Liang, 2021; Liu et al., 2021b). Although pre-
vious work analyzes the prompt from a causal per-
spective (Cao et al., 2022), relatively little attention
has been paid to adopting the prompt to alleviate
the bias in the downstream task.
3 Concept Bias Analysis
In this section, we first formally define our task.
Then we investigate the concept bias issued by
PLMs in empirical studies. Finally, we devise a
Structural Causal Model (SCM) to analyze the bias
and alleviate it via causal inference.
3.1 Preliminary
Task Definition Our CE task addressed in this
paper can be formulated as follows. Given an en-
tityE={e, e,···, e}and its relevant text
T={t, t,···, t}where e(ort) is a word to-
ken, our framework aims to extract one or multiple
spans from Tas the concept(s) of E.
Data Selection It must guarantee that the given
text contains concepts. The abstract text of an en-
tity expresses the concepts of the entity explicitly,
which can be obtained from online encyclopedias
or knowledge bases. In this paper, we take the ab-
stract text of an entity as its relevant text T. The
details of dataset construction will be introduced in
§5.1. Since we aim to extract concepts from Tfor
E, it is reasonable to concatenate EandTto form
the input text X={E, T}.
3.2 Empirical Studies on Concept Bias
To demonstrate the presence of concept bias, we
conduct empirical studies on the CN-DBpedia
dataset (Xu et al., 2017). First, we randomly sam-
ple 1 million entities with their concepts from CN-
DBpedia, and select the top 100 concepts with the
most entities as the typical concept set. Then we
randomly select 100 entities with their abstracts
for each typical concept to construct the input texts
and run a BERT-based extractor to extract concepts.
Details of the extraction process will be introduced
in§4.2. We invite volunteers to assess whether
the extracted concepts are biased. To quantify the
degree of concept bias, we calculate the bias rate
of concept A to another concept B. The bias rate is
defined as the number of entities of A for which B
or the sub-concepts of B are mistakenly extracted
by the extractor, divided by the total number of
entities of A.
The bias rates among 26 typical concepts are
shown in Figure 2, where the concepts (dots) of
the same topic are clustered in one rectangle. The
construction of concept topics will be introduced
in§4.1. From the figure, we can conclude that
concept bias is widespread in the PLM-based CE
system and negatively affects the quality of the
results. Previous studies have proven that causal
inference can analyze bias via SCM and eliminate
bias with causal intervention techniques (Cao et al.,
2022). Next, we will analyze concept bias from a
causal perspective.
3.3 The Causal Framework for Concept Bias
Analysis
The Structural Causal Model We devise a
Structural Causal Model (SCM) to identify the
causal effect between the input text Xof a given
entity Eand the concept span Sthat can be ex-9257tracted from X. As shown in Figure 3, our CE task
aims to extract one or multiple spans SfromXas
the concept(s) of Ewhere the causal effect can be
denoted as X→S.
During the pre-training, the contextual embed-
ding of one token depends on the ones that fre-
quently appear nearby in the corpus. We extrapo-
late that the high co-occurrence between the en-
tities of true concepts ( e.g.,writer ) and biased
concepts ( e.g.,novel ) in the pre-trained knowl-
edge induces spurious correlations between enti-
ties ( e.g.,Louisa May Alcott ) and biased concepts
(e.g.,novel ). Therefore, the PLM-based CE models
can mistakenly extract biased concepts even if the
entity is explicitly mentioned in X. The experi-
ments in §5.4 also prove our rationale. Based on
the foregoing analysis, we define the pre-trained
knowledge Kfrom PLM-based extraction models
as a confounder.
We cannot directly observe the latent space of
the PLMs, and thus the backdoor adjustment (Pearl,
2009) is not applicable in our case. Alternatively,
we adopt the frontdoor adjustment (Peters et al.,
2017) and design a mediator to mitigate the concept
bias.
Causal Intervention To mitigate the concept
bias, we construct a prompt Pas a mediator for
X→S, and then the frontdoor adjustment can
apply do-operation.
Specifically, to make the PLMs attend to the
right cause and alleviate spurious co-occurrence
correlation ( e.g.,novel andLouisa May Alcott ), we
assign a topic as a knowledge-guided prompt P
(i.e.,person ) to the input text X(The detailed op-
eration is elaborated in §4.1). The topics obtained
from KGs are independent of pre-trained knowl-
edge, and thus Pfulfills the frontdoor criterion.
For the causal effect X→P, we can observe
thatX→P→S←Kis a collider that blocks
the association between PandK, and no backdoor
path is available for X→P. Therefore, we can
directly rely on the conditional probability after
applying the do-operator for X:
P(P=p|do(X=x)) =P(P=p|X=x).(1)
Next, for the causal effect P→S,P←X←
K→Sis a backdoor path from PtoS, which we
need to cut off. Since Kis an unobserved variable,we can block the backdoor path through X:
P(S|do(P)) =/summationdisplayP(S|P, X =x)P(X=x).
(2)
Therefore, the underlying causal mechanism of our
CE task is a combination of Eq.1 and Eq.2, which
can be formulated as:
P(S|do(X))
=/summationdisplayP(S|p, do(X))P(p|do(X))
=/summationdisplayP(S|do(P), do(X))P(p|do(X))
=/summationdisplayP(S|do(P))P(p|do(X)). (3)
The theoretical details of the frontdoor adjustment
are introduced in Appendix A.
We make the assumption of strong ignorability,
i.e., there is only one confounder Kbetween X
andS. One assumption of the frontdoor criterion
is that the only way the input text Xinfluences S
is through the mediator P. Thus, X→P→S
must be the only path. Otherwise, the front-door
adjustment cannot stand. Notice that Kalready rep-
resents all the knowledge from pre-trained data in
PLMs. Therefore, it is reasonable to use the strong
ignorability assumption that it already includes all
possible confounders.
Through the frontdoor adjustment, we can block
the backdoor path from input text to concepts and
alleviate spurious correlation caused by the con-
founder, i.e., pre-trained knowledge. In practice,
we can train a topic classifier to estimate Eq.1
(§4.1) and train a concept extractor on our train-
ing data to estimate Eq.2 ( §4.2). Next, we will
introduce the implementation of the frontdoor ad-
justment in detail.
4 Methodology
In this section, we present our CE framework
KPCE and discuss how to perform prompting to
alleviate concept bias. The overall framework of
KPCE is illustrated in Figure 4, which consists
of two major modules: 1) Prompt Constructor :
assigns the topic obtained from KGs for entities
as a knowledge-guided prompt to estimate Eq.1;
2) Concept Extractor : trains a BERT-based extrac-
tor with the constructed prompt to estimate Eq.2
and extract multi-grained concepts from the input
text. Next, we will introduce the two modules of
KPCE.9258
4.1 Prompt Constructor
Knowledge-guided Prompt Construction To
reduce the concept bias, we use the topic of a
given entity as a knowledge-guided prompt, which
is identified based on the external knowledge of
the existing KGs. Take CN-DBpedia (Xu et al.,
2017) as an example. We randomly sample one
million entities from this KG and obtain their exist-
ing concepts. Then, we select the top 100 con-
cepts having the most entities to constitute the
typical concept set, which can cover more than
99.80% entities in the KG. Next, we use spectral
clustering (V on Luxburg, 2007) with the adaptive
K-means (Bhatia et al., 2004) algorithm to cluster
these typical concepts into several groups, each
of which corresponds to a topic. To achieve the
spectral clustering, we use the following overlap
coefficient (Vijaymeena and Kavitha, 2016) to mea-
sure the similarity between two concepts,
Overlap (c, c) =|ent(c)∩ent(c)|
min(|ent(c)|,|ent(c)|) +δ
(4)
where ent(c)andent(c)are the entity sets of
concept cand concept c, respectively. We then
construct a similarity matrix of typical concepts to
achieve spectral clustering. To determine the best
number of clusters, we calculate the Silhouette Co-
efficient (SC) (Aranganayagi and Thangavel, 2007)
and the Calinski Harabaz Index (CHI) (Maulik and
Bandyopadhyay, 2002) from 3 to 30 clusters. The
scores are shown in Figure 5, from which we find
that the best number of clusters is 17. As a result,
we cluster the typical concepts into 17 groups and
define a topic name for each group. The 17 typical
topics and their corresponding concepts are listed
in Appendix B.1
Identifying Topic Prompt for Each Entity We
adopt a topic classifier to assign the topic prompt
to the input text X, which is one of the 17 typ-
ical topics in Table 6. To construct the training
data, we randomly fetch 40,000 entities together
with their abstract texts and existing concepts in
the KG. According to the concept clustering re-
sults, we can assign each topic to the entities. We
adopt transformer encoder (Vaswani et al., 2017)
followed by a two-layer perception (MLP) (Gard-
ner and Dorling, 1998) activated by ReLU, as our
topic classifier. We train the topic classifier to
predict the topic prompt P={p, p,···, p}
forX, which is calculated as:
P= arg max/parenleftbig
P(P|X)/parenrightbig
,1≤i≤17,(5)
where Pis the i-th topic among the 17 typical
topics.
In our experiments, the topic classifier achieves
more than 97.8% accuracy in 500 samples by hu-
man assessment. Through training the topic clas-
sifier, we can estimate Eq.1 to identify the causal
effect X→P.
4.2 Concept Extractor
Prompt-based BERT The concept extractor is
a BERT equipped with our proposed prompt fol-
lowed by a pointer network (Vinyals et al., 2015).
The pointer network is adopted for extracting multi-
grained concepts.
We first concatenate the token sequence with
the tokens of PandXto constitute the input, i.e.,
{[CLS] P[SEP] X[SEP] }, where [CLS] and
[SEP] are the special tokens in BERT. With multi-
headed self-attention operations over the above in-9259put, the BERT outputs the final hidden state (ma-
trix), i.e.,H∈Rwhere dis
the vector dimension and Nis the total number
of layers. Then the pointer network predicts the
probability of a token being the start position and
the end position of the extracted span. We use
p,p∈Rto denote the vectors
storing the probabilities of all tokens to be the start
position and end position, which are calculated as
[p;p] =softmax (HW+B)(6)
where B∈RandW∈Rand
are both trainable parameters. We only consider
the probabilities of the tokens in the abstract T.
Given a span with xandxas the start token and
the end token, its confidence score cs∈Rcan be
calculated as
cs=p+p. (7)
Accordingly, the model outputs a ranking list of
candidate concepts (spans) with their confidence
scores. We only reserve the concepts with confi-
dence scores bigger than the selection threshold.
An example to illustrate how to perform the pointer
network is provided in Appendix B.2.
During training, the concept extractor is fed with
the input texts with topic prompts and outputs the
probability (confidence scores) of the spans, and
thus can estimate the causal effect P→Sin Eq.2.
Model Training We adopt the cross-entropy
function CE(·)as the loss function of our model.
Specifically, suppose that y∈N(or
y∈N) contains the real label (0 or 1)
of each input token being the start (or end) position
of a concept. Then, we have the following two
training losses for the predictions:
L=CE(p,y), (8)
L=CE(p,y). (9)
Then, the overall training loss is
L=αL+ (1−α)L (10)
where α∈(0,1)is the control parameter. We use
Adam (Kingma and Ba, 2015) to optimize L.
5 Experiments
5.1 Datasets
CN-DBpedia From the latest version of Chinese
KG CN-DBpedia (Xu et al., 2017) and Wikipedia,we randomly sample 100,000 instances to construct
our sample pool. Each instance in the sample pool
consists of an entity with its concept and abstract
text. Then, we sample 500 instances from the
pool as our test set and divide the rest of the in-
stances into the training set and validation set ac-
cording to 9:1.
Probase We obtain the English sample pool of
50,000 instances from Probase (Wu et al., 2012)
and Wikipedia. The training, validation and test set
construction are the same as the Chinese dataset.
5.2 Evaluation Metrics
We compare KPCE with seven baselines, includ-
ing a pattern-matching approach i.e., Hearst pattern.
Detailed information on baselines and some exper-
iment settings is shown in Appendix C.1 and C.2.
Some extracted concepts do not exist in the KG,
and cannot be assessed automatically. Therefore,
we invite the annotators to assess whether the ex-
tracted concepts are correct. The annotation detail
is shown in Appendix C.3.
Please note that the extracted concepts may al-
ready have existed in the KG for the given entity,
which we denote as ECs (existing concepts). How-
ever, our work expects to extract correct but new
concepts (that do not exist in the KG) to complete
the KGs, which we denote as NCs (new concepts).
Therefore, we record the number of new concepts
(NC #) and display the ratio of correct concepts
(ECs and NCs) as precision (Prec.). Since it is dif-
ficult to know all the correct concepts in the input
text, we report the relative recall (Recall). Specif-
ically, suppose NCs # is the total number of new
concepts extracted by all models. Then, the relative
recall is calculated as NC # divided by NCs #. Ac-
cordingly, the relative F1 (F1) can be calculated
with Prec. and Recall. In addition, we also record
the average length of new concepts (Len) to
investigate the effectiveness of the pointer network.
5.3 Overall Performance
We present the main results in Table 1. Generally,
we have the following findings:
Our method outperforms previous baselines by
large margins, including previous state-of-the-art
(MRC-CE, Yuan et al., 2021a). However, the9260pattern-based approach still beats the learning-
based ones in precision, envisioning a room for
improvement. We find that KPCE achieves a
more significant improvement in extracting new
concepts, indicating that KPCE can be applied to
achieve KG completion ( §5.5). We also compare
KPCE with its ablated variant and the results show
that adding a knowledge-guided prompt can guide
BERT to achieve accurate CE results.
We notice that almost all models have higher
extraction precision on the Chinese dataset than
that on the English dataset. This is because the
modifiers are usually placed before nouns in Chi-
nese syntactic structure, and thus it is easier to
identify these modifiers and extract them with the
coarse-grained concepts together to form the fine-
grained ones. However, for the English dataset, not
only adjectives but also subordinate clauses modify
coarse-grained concepts, and thus identifying these
modifiers is more difficult.
Compared with learning-based baselines, KPCE
can extract more fine-grained concepts. Although
the Hearst pattern can also extract fine-grained
concepts, it cannot simultaneously extract multi-
grained concepts when a coarse-grained concept
term is the subsequence of another fine-grained
concept term. For example, in Figure 4, if Hearst
Pattern extracts American novelist as a concept,
it cannot extract novelist simultaneously. KPCE
solves this problem well with the aid of the pointer
network and achieves a much higher recall.
5.4 Analysis
In response to the motivations of KPCE , we con-
duct detailed analyses to further understand KPCE
and why it works.
How does KPCE alleviate the concept bias?
As mentioned in §3.2, the concept bias occurs
primarily among 26 concepts in CN-DBpedia. To
justify that KPCE can alleviate concept bias with
the aid of prompts, we randomly select five con-
cepts and run KPCE with its ablated variant to
extract concepts for 100 entities randomly selected
from each of the five concepts. Then we calculate
the bias rates of each concept, and the results in
Table 2 show that KPCE has a much lower bias
rate than the vanilla BERT-based concept extractor.
Thus, the knowledge-guided prompt can signifi-
cantly mitigate the concept bias.
Furthermore, a case study for an entity Korean
alphabet is shown in Table 3. We find that the
proposed prompts can mitigate the spurious co-
occurrence correlation between entities and biased
concepts by decreasing the confidence scores of bi-
ased concepts ( i.e.,language andalphabet ) and in-
creasing the scores of correct concepts ( i.e.,system
andwriting system ). Thus, the knowledge-guided
prompt can significantly alleviate the concept bias
and result in more accurate CE results.
How does the prompt affect the spurious co-
occurrence correlations? To explore the ratio-
nale behind the prompt-based mediator, we focus
on the attention distribution for the special token
[CLS] , since it is an aggregate representation of
the sequence and can capture the sentence-level se-
mantic meaning (Devlin et al., 2019; Chang et al.,
2022). Following previous work (Clark et al.,
2019), we calculate the attention probabilities of9261
[CLS] to other tokens by averaging and normal-
izing the attention value in 12 attention heads in
the last layers. The attention distributions of the
KPCE and its ablation variant are visualized in Fig-
ure 6. We find that the tokens of writer andnovel
both have high attentions in the vanilla BERT-based
concept extractor. However, after adopting our
knowledge-guided prompt, the attention probabili-
ties of novel is lower than before, and thus can help
the model to reduce the spurious co-occurrence
correlations derived from pre-trained knowledge.
What if other knowledge injection methods are
adopted? We claim that the topics obtained from
external KGs are better than the keyword-based
topics from the text on guiding BERT to achieve
our CE task. To justify it, we compare KPCE with
another variant, namely KPCE, where the
topics are the keywords obtained by running La-
tent Dirichlet Allocation (LDA) (Blei et al., 2001)
over the abstracts of all entities. Besides, we also
compare KPCE with ERNIE (Zhang et al., 2019),
which implicitly learns the knowledge of entities
during pre-training. The detail about LDA and
ERNIE is shown in Appendix C.4. The comparison
results are listed in Table 4. It shows that our design
of the knowledge-guided prompt in KPCE exploits
the value of external knowledge more thoroughly
than the two remaining schemes, thus achieving
better CE performance.
5.5 Applications
KG Completion We run KPCE for all entities
existing in CN-DBpedia to complement new con-
cepts. KPCE extracts 7,623,111 new concepts for
6 million entities. Thus, our framework can achieve
a large-scale concept completion for existing KGs.
Domain Concept Acquisition We collect
117,489 Food & Delight entities with their
descriptive texts from Meituan, and explore two
application approaches. The first is to directly
apply KPCE , and the second is to randomly select
300 samples as a small training set to fine-tune
KPCE . The results in Table 5 show that: 1)The
transfer ability of KPCE is greatly improved
with the aid of prompts; 2)KPCE can extract
high-quality concepts in the new domain only with
a small portion of training samples. Furthermore,
after running directly, KPCE extracts 81,800
new concepts with 82.66% precision. Thus,
our knowledge-guided prompt can significantly
improve the transfer ability of PLMs on the domain
CE task.
6 Conclusion
In this paper, we identify the concept bias in the
PLM-based CE system and devise a Structural9262Causal Model to analyze the bias. To alleviate
concept bias, we propose a novel CE framework
with knowledge-guided prompting to alleviate spu-
rious co-occurrence correlation between entities
and biased concepts. We conduct extensive exper-
iments to justify that our prompt-based learning
framework can significantly mitigate bias and has
an excellent performance in concept acquisition.
7 Limitations
Although we have proven that our work can sig-
nificantly alleviate concept bias and extract high-
quality and new concepts, it also has some limita-
tions. In this section, we analyze three limitations
and hope to advance future work.
Model Novelty Although KPCE can effectively
mitigate the spurious co-occurrence correlations
between entities and biased concepts, the proposed
framework is not entirely novel. The novelty of our
work is to conduct the first thorough causal anal-
ysis that shows the spurious correlations between
entities and biased concepts in the concept extrac-
tion task. After defining the problem and SCM of
concept extraction in §3.1, we propose a prompt-
based approach to implement the interventions to-
ward the SCM to elicit the unbiased knowledge
from PLMs. Previous work in language prompt-
ing mostly guides the PLMs with prompts but is
unaware of the cause-effect relations in its task,
which may hinder the effectiveness of prompts. We
hope our work can inspire future work to utilize
language prompting from a causal perspective.
Topic Classification Although the topics ob-
tained by clustering are mostly mutually exclusive,
there are still cases where an entity can be classi-
fied into multiple topics. Therefore, considering
only one topic for the entity excludes the correct
concepts.
Threshold Selection We only reserve concepts
with confidence scores bigger than the selection
threshold ( §4.2), which can hardly achieve a satis-
factory balance of precision and recall. If we select
a relatively big threshold, we can get more accurate
concepts but may lose some correct ones. If the
recall is preferred, precision might be hurt.
We suggest that future work consider these three
limitations to achieve better performance in the CE
task.Acknowledgement
We would like to thank the anonymous review-
ers for their valuable comments and suggestions
for this work. This work is supported by the Chi-
nese NSF Major Research Plan (No.92270121),
Shanghai Science and Technology Innovation Ac-
tion Plan (No.21511100401) and the Science and
Technology Commission of Shanghai Municipality
Grant (No. 22511105902).
References9263926492659266A Theoretical Details of Causal
Framework
A.1 Preliminaries
SCM The Structural Causal Model (SCM) is as-
sociated with a graphical causal model to describe
the relevant variables in a system and how they
interact with each other. An SCM G={V, f}
consists of a set of nodes representing variables
V, and a set of edges between the nodes as func-
tions fto describe the causal relations. Figure 3
shows the SCM that describes the PLM-based CE
system. Here the input text Xserves as the treat-
ment , and the extracted concept span Sis the out-
come . In our SCM, pre-trained knowledge Kis
a cause of both XandS, and thus Kis acon-
founder . A confounder can open backdoor paths
(i.e.,X←K→S) and cause a spurious correla-
tion between XandS. To control the confounding
bias, intervention techniques with the do-operator
can be applied to cut off backdoor paths.
Causal Intervention To identify the true causal
effects of X→S, we can adopt the causal inter-
vention to fix the input X=xand removes the
correlation between Xand its precedents, denoted
asdo(X=x). In this way, the true causal effects
ofX→Scan be represented as P(S=s|do(X=
x)). The backdoor adjustment and the frontdoor
adjustment are two operations to implement inter-
ventions and obtain P(S=s|do(X=x)).
Next, we will elaborate on the details of the two
operations.
A.2 The Backdoor Adjustment
The backdoor adjustment is an essential tool for
causal intervention. For our SCM, the pre-trained
knowledge blocks the backdoor path between X
andS, then the causal effect of X=xonScan
be calculated by:
P(S=s|do(X=x))
=P(S=s|X=x)
=/summationdisplayP(S=s|X=x, K =k)P(K=k)
=/summationdisplayP(S=s|X=x, K =k)P(K=k),
(11)
where Pis the probability after applying the do-
operator, and P(K=k)needs to be estimated
from data or priorly given. However, it is in-
tractable to observe the pre-training data and obtainthe prior distribution of the pre-trained knowledge.
Therefore, the back adjustment is not applicable in
our case.
A.3 The Frontdoor Adjustment
The frontdoor adjustment is a complementary ap-
proach to applying the intervention when we cannot
identify any set of variables that obey the backdoor
adjustment.
In our SCM, we aim to estimate the direct effect
ofXonS, while being unable to directly measure
pre-trained knowledge K. Thus, we introduce a
topic prompt Pas a mediator, and then the front-
door adjustment can adopt a two-step do-operation
to mitigate bias.
Step 1 As illustrated in §3.3, we first analyze the
causal effect X→P. Since the collider, i.e.,X→
P→S←Kblocks the association between P
andK, there is no backdoor path from XtoP.
Thus we can obtain the conditional probability as
(same as Eq.1):
P(P=p|do(X=x)) =P(P=p|X=x).
(12)
To explain Step 1, we take an entity Louisa May
Alcott with her abstract as an example. We can
assign the topic person as a prompt to make the
PLM-based extractor alleviate spurious correlation
between Louisa May Alcott andnovel , and concen-
trate on extracting person-type concepts.
Step 2 In this step, we investigate the causal ef-
fectP→S.P←X←K→Scontains a
backdoor from PtoS. Since the data distribution
ofXcan be observed, we can block the backdoor
path through X:
P(S=s|do(P=p))
=/summationdisplayP(S=s|X=x, P =p)P(X=x),
(13)
where P(X=x)can be obtained from the distribu-
tion of the input data, and P(S=s|X=x, P =
p)is the conditional probability of the extracted
span given the abstract with a topic prompt, which
can be estimated P(S=s|X=x, P =p)by the
concept extractor.
Now we can chain the two steps to obtain the9267causal effect X→S:
P(S|do(X))
=/summationdisplayP(S|p, do(X))P(p|do(X))
=/summationdisplayP(S|do(P), do(X))P(p|do(X))
=/summationdisplayP(S|do(P))P(p|do(X)). (14)
B Detailed Information about KPCE
B.1 Identifying Topic for Each Entity
The 17 typical topics and their corresponding con-
cepts are listed in Table 6. We predict the topic
of the entity as one of the 17 typical topics using
a transformer encoder-based topic classifier. We
randomly fetch 40,000 entities together with their
existing concepts in the KG. According to the con-
cept clustering results, we can assign each topic to
the entities. Specifically, we concatenate EandX
as input to the classifier. With multi-headed self-
attention operation over the input token sequence,
the classifier takes the final hidden state (vector) of
a token [CLS] , i.e.,h∈R, to compute the
topic probability distribution P(T|E, X )∈R,
where Nis the total number of layers and dis
the vector dimension. Then, we identify the topic
with the highest probability Tas the topic of X,
which is calculated as follows,
H=EW+B, (15)
H=encoder (H),1≤l≤N,(16)
P(T) =softmax (hW), (17)
T= arg max/parenleftbig
P(T)/parenrightbig
,1≤i≤17 (18)
where E∈Ris the random initial em-
bedding matrix of all input tokens and dis the em-
bedding size. H∈Ris the hidden ma-
trix of the l-th layer. h is obtained from H.
Furthermore, W∈R,B∈R
andW∈Rare both training parameters.
B.2 An Example for Point Network
As mentioned in §4.2, we adopt a point network
to achieve multi-grained concept extraction (Yuan
et al., 2021a). The model generates a ranking list
of candidate concepts (spans) along with their con-
fidence scores, and outputs the concepts with con-
fidence scores bigger than the selection threshold.
Note that one span may be output repeatedly as the
same subsequence of multiple extracted concepts
through an appropriate selection threshold.
For example, as shown in Figure 7, writer is ex-
tracted multiple times as the subsequence of three
different granular concepts when the confidence
score threshold is set to 0.30. Therefore, the point
network enables our framework to extract multi-
grained concepts.9268C Experiment Detail
C.1 Baselines
We compare KPCE with seven baselines. Most of
the compared models are the extraction models fea-
sible for extraction tasks, including Named Entity
Recognition (NER), Relation Extraction (RE), and
Open Information Extraction (Open IE). In addi-
tion, we also compare the pattern-based approach.
However, we do not compare ontology extension
models and generation models, since both do not
meet our scenario. Since entity typing models can-
not find new concepts, they are also excluded from
our comparison. Please note that, except MRC-
CE, other baselines applied in concept extraction
cannot extract multi-grained concepts.
•Hearst (Jiang et al., 2017): With specific
handwritten rules, this baseline can extract
concepts from free texts. We design 5 Hearst
patterns listed in Table 7 where we translate
the Chinese patterns for the Chinese dataset
into English.
•FLAIR (Akbik et al., 2019): It is a novel NLP
framework that combines different words and
document embeddings to achieve excellent
results. FLAIR can also be employed for con-
cept extraction since it can extract spans from
the text.
•XLNet (Yang et al., 2020): With the ca-
pability of modeling bi-directional contexts,
this model can extract clinical concepts effec-
tively.
•KVMN (Nie et al., 2020): As a sequence
labeling model, KVMN is proposed to handle
NER by leveraging different types of syntactic
information through the attentive ensemble.
•XLM-R (Conneau et al., 2020; Lange et al.,
2022): It is a Transformer-based multilin-
gual masked language model incorporating
XLM (Conneau and Lample, 2019) and
RoBERTa (Liu et al., 2019), which has proven
to be effective in extracting concepts.
•BBF (Luo et al., 2021): It is an advanced ver-
sion of BERT built with Bi-LSTM and CRF.
With optimal token embeddings, it can extract
high-quality medical and clinical concepts.•GACEN (Fang et al., 2021): The model in-
corporates topic information into feature rep-
resentations and adopts a neural network to
pre-train a soft matching module to capture
semantically similar tokens.
•MRC-CE (Yuan et al., 2021a): MRC-CE han-
dles the concept extraction problem as a Ma-
chine Reading Comprehension (MRC) task
built with an MRC model based on BERT. It
can find abundant new concepts and handle
the problem of concept overlap well with a
pointer network.
C.2 Experiment Settings
Our experiments are conducted on a workstation of
dual GeForce GTX 1080 Ti with 32G memory and
the environment of torch 1.7.1. We adopt a BERT-
base with 12 layers and 12 self-attention heads as
the topic classifier and concept extractor in KPCE .
The training settings of our topic classifier are: d =
768, batch size = 16, learning rate = 3e-5, dropout
rate = 0.1 and training epoch = 2. The training
settings of our concept extractor are: d = 768, m
= 30, batch size = 4, learning rate = 3e-5, dropout
rate = 0.1 and training epoch = 2. The αin Eq.8 is
set to 0.3 and the selection threshold of candidate
spans in the concept extractor is set to 0.12 based
on our parameter tuning.
C.3 Human Assessment
Some extracted concepts do not exist in the KG,
which cannot be automatically assessed. Therefore,
we invite some volunteers to assess whether the
extracted concepts are correct for the given entities.
We denote an extracted concept as an EC(existing
concept) that has already existed in the KG for the
given entity. We denote an extracted concept as an
NC(new concept) represents a correct concept not9269existing in the KG for the given entity. We employ
four annotators in total to ensure the quality of the
assessment. All annotators are native Chinese and
proficient in English. Each concept is labeled with
0, 1 or 2 by three annotators, where 0 means a
wrong concept for the given entity, while 1 and 2
represent EC and NC, respectively. If the results
from the three annotators are different, the fourth
annotator will be hired for a final check. We protect
the privacy rights of the annotators and pay the
annotators above the local minimum wage.
C.4 Other Knowledge Injection Methods
As we mentioned before, the topics of the
knowledge-guided prompt come from external
KGs, which are better than the keyword-based top-
ics from the text on guiding BERT to achieve accu-
rate concept extraction.
To justify it, we compared KPCE with another
variant, namely KPCE, where the topics are
the keywords obtained by running Latent Dirichlet
Allocation (LDA) (Blei et al., 2001) over all en-
tities’ abstracts. Specifically, the optimal number
of LDA topic classes was also determined as 17
through our tuning study. For a given entity, its
topic is identified as the keyword with the highest
probability of its topic class. Besides, we also com-
pared KPCE with ERNIE. ERNIE (Zhang et al.,
2019) adopts entity-level masking and phrase-level
masking to learn language representation. During
pre-training of ERNIE, all words of the same en-
tity mentioned or phrase are masked. In this way,
ERNIE can implicitly learn the prior knowledge of
phrases and entities, such as relationships between
entities and types of entities, and thus have better
generalization and adaptability.
The comparison results are listed in Table 4,
which shows that our design of the knowledge-
guided prompt in KPCE exploits external knowl-
edge’s value more thoroughly than the rest two
schemes.9270ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Limitations
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Check grammar for the whole paper
B/squareDid you use or create scientiﬁc artifacts?
Section 5.1
/squareB1. Did you cite the creators of artifacts you used?
Sections 5.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 5.1
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 5.1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Our data is collected from the existing KGs, and there is no offensive content.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 5.1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 5.1
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix C.29271/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix C.2
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5.2
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
We do not use existing packages
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Appendix C.3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix C.3
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Appendix C.3
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Appendix C.3
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. We do not have any human subjects research.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Appendix C.39272