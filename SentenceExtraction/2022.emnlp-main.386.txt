
Yanzhu Guo, Chloé Clavel, Moussa Kamal Eddine, Michalis VazirgiannisLIX, École Polytechnique, Institut Polytechnique de Paris, FranceLTCI, Télécom-Paris, Institut Polytechnique de Paris, France
{yanzhu.guo, moussa.kamal-eddine}@polytechnique.edu
chloe.clavel@telecom-paris.fr ,mvazirg@lix.polytechnique.fr
Abstract
The topicof summarization evaluation has
recentlyattractedasurgeofattentiondueto
the rapiddevelopment ofabstractive sum-
marizationsystems. However,theformu-
lation of the task is rather ambiguous, nei-
therthelinguisticnorthenaturallanguage
processing community has succeeded in
giving a mutually agreed-upon definition.
Due to this lack of well-defined formula-
tion, a large number of popular abstractive
summarization datasets are constructed in
amannerthatneitherguaranteesvalidity
nor meets one of the most essential criteria
ofsummarization: factualconsistency. In
thispaper,weaddressthisissuebycombin-
ingstate-of-the-artfactualconsistencymod-
els to identify the problematic instances
presentinpopularsummarizationdatasets.
WereleaseSummFC,afilteredsummariza-
tiondatasetwithimprovedfactualconsis-
tency,anddemonstratethatmodelstrained
on this dataset achieve improved perfor-
mance in nearly all quality aspects. We
argue that our dataset should become a
valid benchmark for developing and evalu-
ating summarization systems.
1 Introduction
WhiletherevolutionarysuccessoftheTrans-
former(Vaswanietal.,2017)architecturehas
drawnasurgeofattentiontoautomaticsum-
marization, most research has been focused
on improving performance metrics of summa-
rizationmodelsonasetofpopulardatasets. It
is often taken for granted that these datasets
provide representative examples of high qual-
ity summaries, and that models capable of
producing summaries that are similar to the
onesofthedatasetaresuperiorinthetaskof
automatic summarization. But is this really the
case?
Numerousworks(Fabbrietal.,2021;Huang
et al., 2020) have already pointed out deficien-cies in the most widely employed summariza-
tion datasets (e.g. CNN/DailyMail (Hermann
et al., 2015; Nallapati et al., 2016) and XSUM
(Narayan et al., 2018)). These datasets are
typicallycomposedofnewsarticlesautomat-
ically extracted from news websites, paired
togetherwithahighlightorintroductionsen-
tence which serves as the summary. However,
the qualities we seek in highlights and intro-
ductions are fundamentally different from the
ones we seek in summaries. It is thus time
for the NLP community to take a step back,
revisittheformulationofthesummarization
task and reconsider the appropriateness of
currently employed datasets.
Thegoalofautomaticsummarizationisto
take an information source, extract content
from it and present the most important con-
tent to the user in a concise form and in a
manner sensitive to the user’s or application’s
needs (Mani, 2001). In the scope of this pa-
per, we solely focus on generic summariza-
tion(NenkovaandMcKeown,2011),wherewe
makefewassumptionsabouttheaudienceor
the goal for generating the summary. While
the importance of the selected content and
the conciseness of its form are rather subjec-
tive,thereisonecriterionforsummarization
that is certain: the summary content should
be extracted from the information source. In
other words, the summary should be factually
consistent with the source document. This
is however not the case with summaries con-
structed from highlights or introductions. It
is common for highlights and introductions to
contain information that are not mentioned in
themainarticleorevenexaggeratecertainfacts
toachieve thegoalofclickbaiting. Therefore,
themodelstrainedonthesedatasetsperform
thetaskof“pitchgeneration”ratherthan the
intendedsummarygeneration. Anexampleof5716such a reference summary is given in Table 1.
In this paper, we aim to identify these er-
roneous data samples by scoring them with
factual consistency models. We push towards
answering two main research questions :
Q1What kind of factual inconsistency can differ-
ent factuality models capture and how can they be
combinedforbetterdetectionoferroneoussamples?
ThisquestionisansweredinSections3.2and
5.1 .
Q2Doeshavingmorereliabledatasetswithmore
factually consistent reference summaries lead to
better performing summarization models? And do
theresultsonlyimproveinfactualconsistencyor
also in other quality aspects such as informative-
ness andsaliency? We addressthis question in
Section 5.4.
Ourresearchaimstoanswerthesequestions
byfilteringoutsampleswithproblematicrefer-
encesummaries. Weachievethisbyleveraging
state-of-the-artfactualconsistencymodels. We
testtheperformanceofdifferentmodelsondif-
ferentcategoriesoffactualityerrors(Pagnoni
etal.,2021)andeventuallycombinethemfor
an optimized filtration methodology.
Ourcontributions are summarized below:
1.Weprovetheeffectivenessofthreestate-
of-the-art factual consistency models in
detectingfactualityerrors. Weusethese
models to affirm the factual consistency
issue present in three of the most popu-
larsummmarizationdatasets. Wedevise
a filtration methodology that combines
differentstate-of-the-artfactualitymodels
andachievesbetterdetectionofmislead-
ing reference summary samples.
2.WereleaseSummFC 1,aSummarization
dataset with improved Factual
Consistency. We prove that fine-
tuning summarization models on this
dataset leads to better performance
on not only factuality but also other
quality aspects. We believe that regularly
amending issues in widely employed
benchmark datasets should become a
common practice in NLP.Source Document:
Archibald, 22, dominated the event, with Dutch
riderKirstenWildsecondandBelgium’sLotte
Kopecky third. That came 24 hours after she
wonherthirdconsecutivewomen’sindividual
pursuittitle,havinggainedsilverinThursday’s
elimination race. The Scottish cyclist won gold
in the pursuit quartet at the Olympics in Rio.
Factually Inconsistent Reference Summary:
british olympic champion katie archibald won
omnium gold at the european track champi-
onships, her second title in two nights in paris.
2 Related Work
Summarization Benchmark Datasets Sum-
marization benchmark datasets are typically
composed of a large number of news docu-
ments paired together with “gold-standard”
human reference summaries. Unfortunately,
the human reference summaries in these
datasets are often constructed in a subopti-
mal way. Gehrmann et al. (2022) analyze a
sample of 20 papers proposing summariza-
tion approachespublished in2021; they find
27datasetsthatmodelswerebeingevaluated
on. The most popular ones, CNN/DM (Nal-
lapati et al., 2016) and XSum (Narayan et al.,
2018), were used five and four times respec-
tively. However, both of these datasets have
multiple issues. Taking the CNN/DM dataset
forexample,theconstructionisdonebypair-
inganarticlewiththebulletpointswrittenfor
itontheCNNandDailyMailwebsites. This
design works well for its initial use as a Ques-
tionAnsweringdataset(Hermannetal.,2015),
butdoesnotfunctionatthesamelevelafterits
adaptation for summarization. Regarding the
XSUM dataset, the main issue concerns its fac-
tuality. The reference summaries were never
meant to be a real summary, there is thus no
requirementforittobefaithfultothesource
article. An analysis of XSum finds that over
70% of reference summaries contain factual
inconsistencies (Maynez et al., 2020).5717FactualConsistencyModels Thankstothe
development of reference-free factual consis-
tencymetricsforsummarization(Huangetal.,
2021),theevaluationoffactualconsistencycan
now be performed automatically on a large
scale. Sincesuchmetricsaregenerallymodel-
based, we refer to them as factual consistency
models, in order to distinguish them from
the evaluation metrics in Section 5.3. Popu-
lar factual consistency models can generally
be categorized into two different paradigms:
Question Answering (Durmus et al., 2020;
Wang et al., 2020) and Entailment Classifica-
tion(Kryscinskietal.,2020;GoyalandDurrett,
2020). Question Answering models are based
ontheintuitionthatifweaskthesameques-
tiontoasummaryanditssourcearticle,they
shouldprovidesimilaranswersifthesummary
isfactual. Entailmentclassificationmodelsrely
ontheideathatafactuallyconsistentsummary
should be semantically entailed by the source
article. Theyareusuallytrainedmodelsfine-
tunedoneithersyntheticorhuman-annotated
datasets. AccordingtotheFRANKbenchmark
(Pagnoni et al., 2021), Entailment classification
models perform significantly better than the
QuestionAnsweringones. Inaddition,some
textgenerationevaluationmetrics(Zhangetal.,
2019; Yuan et al., 2021) falling out of these two
paradigms are also proven to perform well
asfactualconsistencymodels(Pagnonietal.,
2021).
ImprovingFactualConsistency Theideade-
fended in this paper is that factual consistency
modelsallowustodiscoverissuesincurrent
datasets and eventually release improved ver-
sions. Dataset quality in summarization has
only started receiving attention recently and
onlyafewpapershaveattemptedtomakeef-
fortsinthisdirection. Gehrmannetal.(2021)
releaseanimprovedversion ofXSUMthatfil-
ters the dataset with a BERT-based classifier
fine-tunedon500document-summarypairs,
manually annotated for faithfulness (Maynez
et al., 2020). However, the classifier is fairly
naive with only a single classification layer
added after the BERT model and they did not
compare the performance of models trained
respectively on the original and improved
datasets. Along similar lines, Matsumaru
et al. (2020) build a binary classifier for de-tecting untruthful article-headline pairs and
filteraheadlinegenerationdataset. Nanetal.
(2021) also perform filtration for summariza-
tion datasets but both the filtration and evalu-
ationmethodologiesarelimitedtotheentity
level. Goyal and Durrett (2021) identify non
factual tokens in the XSUM training data with
the Dependency Arc Entailment (DAE) model.
Theymask thesetokens correspondingtoun-
supported facts and ignore them during the
training step. Whilethis approach doesallow
for improved summarization quality, we see
potential issues arising when isolated ignored
tokensareconfrontedwiththecontextualna-
ture of the Transformer architecture. Filtering
outentiresampleswithdetectedinconsistency
is thus the preferred solution.
We present the first work on a systematic
investigation of an optimal methodology for
improvingfactualconsistencyinsummariza-
tiondatasets. Wearealsothefirsttocompre-
hensively analyze the effect of training data
qualityonsummarizationsystemoutputs. We
furthermore extend the range of analysis com-
pared to the previous papers, also performing
experiments on the recently released XL-SUM
dataset (Hasan et al., 2021).
3 Analyzing Factual Consistency
Models
In this section, we present the factual consis-
tency models we employ for detecting factual
errors in the reference summaries. We intro-
duce three state-of-the-art models that each
relyonadistinctivemechanism. Werelyonhu-
manannotationsfromtheFRANKbenchmark
(Pagnonietal.,2021)tovalidatetheeffective-
nessofthesemodelsandanalyzethevariations
intheirperformancewhenshiftingdatasetdo-
main or error category. The FRANK bench-
markisthemostrecentbenchmarkproposed
for evaluating factual consistency models, it
istheonlybenchmarktodatethatprovidesa
categorization of factual errors.
3.1 Factual Consistency Models
In order to be comprehensive in our choice
offactualconsistencymodels,werunexperi-
mentsonthetopperformingmodelfromthe
FRANK benchmark as well as two other re-
cently proposed models not included in the5718FRANK benchmark. Each of the three models
we choose depends on a different mechanism,
thus they are expected to complement each
other in detecting different types of errors.
BERTScore_Art Zhang et al. (2019) intro-
duce BERTScore as an automatic evaluation
metric for text generation. It computes a
similarity score for each token in the candi-
date summary with each token in the refer-
encesummaryleveragingBERTembeddings.
It obtains the final scores by performing a
greedy matching between tokens from both
texts. Here we employ a slightly modified ver-
sion BERTScore_Art, which directly compares
a summary to the source article instead of the
referencesummary,forthesakeofmodeling
factuality. We use the precision score which
matcheseachtokeninthesummarytoatoken
in the article, instead of the recall score which
does the opposite. This is the factuality model
thatobtainedthehighestcorrelationwithhu-
man annotations in the FRANK benchmark
(Pagnoni et al., 2021).
BARTScore Yuan et al. (2021) formulate the
evaluation of generated text as a text genera-
tion task from pre-trained language models.
The basic idea is that a high-quality summary
shouldbeeasilygeneratedbasedonthesource
article. The factuality score of a summary is
calculated asits generation probability condi-
tionedonthesourcedocument. Theideaisop-
erationalizedusingtheencoder-decoderbased
model BART and thus named BARTScore. It
buildsaconnectionbetweenthepre-training
objectives and evaluation of text generation
models. BARTScoreisshowntoproducestate-
of-the-art results for factuality evaluation on
the SummEval Dataset (Fabbri et al., 2021).
DAE (Dependency Arc Entailment) Goyal
andDurrett(2020)proposetodecomposesum-
mariesintodependencyarcsandtrainanen-
tailment model that makes independent fac-
tualityjudgmentsforeachdependencyarcof
the summary. The judgements made by the
model are binary and we use the probability
for the factual class in order to obtain a con-
tinuous score. The arc-level judgements are
thenaggregatedintosummary-levelbycom-
putingthemeanscoreforallarcspresentinthe
summary. The initial work proposes to train
the entailment model on synthetic datasets.
However, they later extend their method by
training it on human annotations and achieve
improved performance (Goyal and Durrett,
2021). ThisextendedversionofDAEisproven
to be the best performing entailment-based
model tested on the factuality dataset intro-
duced in Falke et al. (2019).
3.2 Model Validation Using the FRANK
Benchmark
The FRANK benchmark proposes a linguis-
tically motivated typology of factual errors
forfine-grainedanalysisoffactualityinsum-
marization systems: semantic frame errors,
discourse errors and content verifiability er-
rors. Semantic frames refer to the schematic
representation of an event, relation, or state
andasemanticframeerror isanerrorthatonly
involves participants of a single frame. Dis-
course errors extend beyond single semantic
frames and consist of erroneous relations be-
tween different discourse segments. Content
verifiabilityerrors arisewheninformationinthe
summarycannotbeverifiedagainstthesource
document.
TheFRANK benchmarkconsistsof human-
annotated categorical error scores for 2250
modelsummaries. Weusetheseannotationsto
computepartialcorrelationscoreswithhuman
judgements for different models on different
datasets and error types. We now examine
these correlations.
Partial Pearson correlation on different
datasets. Figure 1 shows the partial pearson
correlationbetweendifferentmodelsandhu-5719
man judgments on document-summary pairs
from different datasets. The FRANK bench-
markincludes1250pairsfromtheCNN/DM
dataset and 1000 from the XSUM dataset. We
observe that the three models achieve com-
parable performance across all datasets with
BARTScore obtaining the best performance
on CNN/DM, DAE obtaining the best perfor-
mance on XSUM and BERTScore_Art on both
of them combined.
Partial Pearson correlation on different error
types.Figure2showsthevariationinpartial
Pearson correlation when flipping the labels
ofaspecificerrortype. Ahigherpositivebar
indicates that the given type of error has high
influence in the overall correlation. In other
words,thefactualconsistencymodelperforms
wellindetectinganerrortypeifithasahigh
positive bar. We observe that DAE performs
the best for content verifiability errors on both
of the datasets, but gives unsatisfactory re-
sults for the other two error types. This is
expectedasDAEscoreseachdependancyarc
independantly and thus cannot detect erros
spanningacrossdifferentarcs. BERTScore_Art
and BartScore are both able to achieve posi-
tive results for discourse errors on XSUM, and
for semantic frame errors on CNN/DM. How-
ever, it is worth pointing out that all of the
threemodelsgenerallyperformtheworstfor
discourse errors,indicating thatthis isa chal-
lenging future direction for work on factuality
models.
Duetotherelativestrengthofeachmodelin
identifyingdifferenttypesoferrorsondifferent
datasets, we choose to use all three of them
in analyzing the summarization datasets inSection 4. We further combine these three
models in our filtration process in Section 5.1.
4 Examining Summarization
Benchmark Datasets
To question the validity of current summariza-
tion benchmark datasets, we first provide an
overview of the context in which each dataset
wascreatedandthemethodologyemployedin
theconstructionprocedures. Inaddition,we
make use of the previously mentioned factual
consistencymodelstoevaluatethefactuality
of human reference summaries collected in
each dataset. Our selection of summariza-
tion datasets is based on their popularity in
recently published papers introducing new
summarizationsystems. Webelievethatitis
crucial to examine their validity as they play
a fundamental role in both the development
and evaluation of new systems. The most
popularbenchmark datasetsaccording toour
criteriaare CNN/DM(Hermann etal., 2015)
and XSUM (Narayan et al., 2018). We also
include theveryrecently released dataset XL-
Summ (Hasan et al., 2021). It is not yet widely
employed but including it alongside the other
two can help us inspect the most recent ad-
vances made in the field of summarization
datasetcreation. Doesthemostrecentbenchmark
dataset exhibit any improvement on factuality in
comparisontothepreviousoneswithwell-known
issues?
4.1 Summarization Benchmarks
We present the three summarization bench-
mark datasets in the order of their time of
release. We observe an evolution in the con-5720struction methodology over time.
CNN/DM TheCNN/DMdataset(Hermann
etal.,2015)wasinitiallyconstructedasaQues-
tionAnsweringdatasetcomposedofnewswire
articles in English and their corresponding
highlights from the two platforms CNN and
DailyMail. ChengandLapata(2016)latercon-
verted it into a summarization dataset by sim-
ply concatenating these highlights into sum-
maries. It has now become the most broadly
employed summarization dataset for the En-
glish language. However, the summaries
formedfromtheconcatenationofbulletpoints
exhibitlowdegreesofabstractionandcoher-
ence,whicharebothhighlydesirablequalities
for abstractive summarization systems. The
datasetconsistsof311,971document-summary
pairs.
XSUM Narayan et al. (2018) create another
large-scale dataset for abstractive summariza-
tionbycrawlingonlinearticlesfromtheBBC
platform. Theytakethefirstlineofanarticleas
the summary and the rest of the article as the
source document. This method for annotating
summaries guarantees high levels of abstrac-
tion but creates other issues as the first line of
anarticleisoftennotwrittentobethesummary.
It might either include meta-information such
as the author and publication date or serve as
background introduction thus containing in-
formationnevermentionedagainintherestof
the article. XSUM contains 226,711 document-
summary pairs.
XL-Sum XL-Sum (Hasan et al., 2021) is a re-
centlyintroducedabstractivesummarization
datasetcontainingdocument-summarypairs
also extracted from the BBC platform. The
automatic annotation strategy of summaries
is similar to that of XSUM. However, they
find the first line of many articles to contain
meta-informationandthusannotatethebold
paragraphs instead as the summaries. The
datasetcovers44languages,formanyofwhich
no public summarization datasets were pre-
viously available. Summaries contained in
XL-Sum are also highly abstractive. The En-
glish subset of this dataset contains 329,592
document-summary pairs.4.2 Factual Consistency of Summarization
Benchmarks
We focus on evaluating the factual consistency
of human reference summaries contained in
eachofthesummarizationbenchmarkdatasets.
We often assume that human references are
“gold standards” but there is a lot to question
intheirvalidityconsideringhowtheyarean-
notated.
Here, we perform an analysis on the hu-
manreferencesusingthethreefactualitymod-
els introducedin Section3.1: BERTScore_Art,
BARTScore and DAE. The results are shown
in Figure 3. We remark that factuality scores
produced by all three models rank CNN/DM
asthemostfactualdatasetbyalargemargin.
This is due to its low level of abstraction and
thus do not indicate its superiority as an ab-
stractivesummarizationdataset. However,itis
also worth pointing out that the improvement
achieved betweeen XSUM and the recently
created XL-SUM is not significant. By qualita-
tivelyanalyzingthelowestrankingsummaries
scoredbyeachmodelfromeachdataset,one
can confirm their non factuality. A representa-
tivenonfactualexampleforXSUMisshown
inTable1. Examplesforthetwootherdatasets
are shown in Appendix Section A.
5 Introducing the SummFC Dataset
The approach we choose to address the factu-
alityprobleminthesesummarizationdatasets
is to filter the samples by their factual con-
sistency scores obtained in Section 4.2. We
applythisideatoconstruct SummFC ,thenew
Summarization benchmark dataset with im-
provedFactualConsistency. In the remain-
der of this section, we first present our fil-
trationmethodology,followedbystatisticsof
theSummFCdataset. Wethenintroducethe
evaluationmetricsusedinthecomparisonof
summaries generated by models trained on
the original benchmarks and models trained
on SummFC. Finally, we perform experiments
on the three datasets introduced in Section
4.1 and observe advantages of the SummFC
dataset.
5.1 Filtration Methodology
In Figure 4, we illustrate our pipeline for filtra-
tionandevaluation ofthethreedatasets. The5721
filtration methodology is based on the factu-
alitymodelsBERTScore_Art, BARTScoreand
DAE. For each of the three datasets, we filter
out the bottom 25% of document-summary
pairs scored by each of the three models. In
other words, we only keep the intersection
between the top 75% of samples scored by
each factuality model. This choice is made
because the three factuality models are shown
tocomplementeachotherinthedetectionof
erroneous samples (see Section 3.2). Experi-
mentsinSection5.4showthatdatasetsfiltered
bycombinedmodelsoutperformsinglemodel
filtration with the same threshold.5.2 The SummFC Dataset
In Table 2, we show statistics for the SummFC
datasets. Forallthreedatasets,theselectionra-
tioofsamplestoretainisbetween50%and60%.
In the context of our filtration methodology
removingthelowestscored25%ofsamplesfor
each factuality model, this final selection ratio
provesthatthereisahighdegreeofoverlapbe-
tween the samples different factuality models
choosetofilterout. Wealsoreporttheaverage
length of documents and summaries in each
dataset. We compute length as the number of
words (instead of tokens) included in the text.
We might expect to achieve higher factual con-
sistency scores for shortersummaries, as they5722
naturally contain less information. However,
we show that our filtration methodology does
not particularly favor samples with shorter
reference summaries.
5.3 Evaluation Metrics
To compare the quality of summaries pro-
duced by models fine-tuned on the the orig-
inal datasets and the SummFC selection, we
employthefollowingevaluationmetricscap-
turing different quality aspects.
Factual Consistency Models For testing the
improvement in factual consistency of the
produced summaries, we employ the same
factuality models as we use for filtration in
Section 3.1: BERTScore_Art, BARTScore and
DAE.
BLANC BLANC (Vasilyev et al., 2020) is a
reference-free evaluation metric for summa-
rization. Itisdefinedasameasureofhowwell
a summary helps an independent, pre-trained
languagemodelwhileitperformsitslanguage
understandingtask(maskedtokentask)onadocument. Inotherwords,BLANCmeasures
theinformativeness of a summary in the con-
textofdocumentunderstanding. Here,weuse
the BLANC-help version, which uses the sum-
mary by concatenating it to each document
sentence during inference.
ROUGE-2 ROUGE-2 (Lin, 2004) is a
reference-based text generation metric com-
putingthebigramoverlapbetweenreference
and candidate summaries. While reference-
free evaluationmetrics areadvantageous due
to the issues in reference summaries, there are
still quality aspects it cannot cover. There is
currentlynoreference-freemetricthatisable
tomeasureasummary’s saliency (i.e.coverage
of salient information). Thus we still need to
comparethegeneratedsummariestotherefer-
enceoneswithROUGE-2,assumingthatthe
referencesummariesexhibita highdegreeof
saliency. We also process the samples in the
test set with our filtration methodology when
usingROUGE-2. Foralltheotherevaluation
metrics, we do not perform filtration for the5723test set because they are reference-free and
thus not influenced by misleading reference
summaries.
5.4 Experiments and Results
Wefine-tunethepre-trainedBART-basemodel
(Lewis et al., 2020) from the Transformers li-
brary(Wolfetal.,2020)onthedifferentsum-
marization datasets. Since our goal is not to
advance summarization systems but to com-
paredifferentbenchmarkdatasets,wedonot
experiment with hyperparameter tuning. For
all parameters except batch size, we use the
default settings in the Transformers library.
Boththetrainingandpredictionaredoneon
a NVIDIA TITAN VGPU with a batch size of
8. We present results for models fine-tuned
on the original benchmarks and SummFC in
Table 3. We also create a random selection
baseline for each dataset by uniformly sam-
plingarandomsubsetofsampleswithequal
size to the SummFC selection.
We observe that models trained on the
SummFC selection of XSUM and XLSUM
achievethehighestscoresforallreference-free
evaluation metrics. Although the selection
procedureisonlybasedonfactualconsistency,
we see that the summarization systems have
also improved in the informativeness aspect
as measured by BLANC. For the reference-
based metric ROUGE-2, models trained on
SummFC obtain comparable scores to those
trained on the original dataset and beat the
random baseline by a large margin. It is also
interestingtoremarkthatforsomeofthefactu-
alityscores,eventherandombaselineachieves
betterperformancethanthefulltrainset. This
further confirms the conclusion that too much
erroneous training data hinders factual consis-
tency in summarization models. The results
onCNN/DMareconsistentwiththeothertwo
datasets, except for BARTScore and BLANC.
We believe that our filtration methodology
is slightly less effective on CNN/DM due to
thefactthatthisdatasetmanifeststhelowest
degree of factual inconsistency. Another im-
portant reason is that the BART model used
in BARTScore is also fine-tuned on CNN/DM,
which makes it biased on this dataset. How-
ever,itisworthnotingthatSummFCisconsid-
erablysmallerinsizecomparedtotheoriginalones. This means that using SummFC, we can
achievebetterresultsonnearlyallqualityaspects
whilereducingtrainingtimeandloweringtheneed
for computational resources .
Figure5showsmetricscoresobtainedwhen
fine-tuningonfilteredXLSUMdatasetswith
singlefactualitymodelsatdifferentthresholds.
We show scores for BLANC and ROUGE-2
becausethesetwometricswerenotuseddur-
ing filtration. We also create a baseline by
randomly selecting samples with proportions
equal to the tested thresholds. As the filtra-
tion criteria becomes stricter, BLANC scores
increase while ROUGE-2 scores decrease. Our
finalfiltrationthresholdischosenasacompro-
mise between the optimization of these two
scores. We also observe that there is no single
factualitymodelthatperformsthebestforboth
scores.Ingeneral,theSummFCcombinedfiltra-
tionstrategyoutperformssinglefactualitymodels
at the same threshold.
6 Conclusion
In this paper, we demonstrate that popular
summarization datasets suffer from the lack
of factual consistency and that summarization
modelstrainedonthesedatasetsarenotade-
quateforthetaskofabstractivesummarization.
We show that this problem can be solved by
filtering the benchmark datasets with scores
from factual consistency models. We propose
afiltrationmethodologycombiningthreestate-
of-the-art factual consistency models and in-
troduce the SummFC dataset. SummFC is
aunified Summarizationbenchmarkconsist-
ing ofFactually Consistent samples chosen
fromCNN/DM,XSUMandXLSUM.Experi-
mentsindicatethat,ingeneral,modelstrained
onthesmallerSummFCgeneratesummaries
withhigherqualitythanmodelstrainedonthe
larger original datasets. Our findings suggest
that more deliberate considerations should
be made in the construction of benchmark
datasets and that continuous revisions for the
already existing ones are particularly neces-
sary. However,constructingadequatebench-
markdatasetswithtextualcontentsandlabels
matching the initial formulation of NLP tasks
remainsan openquestion, majorobstacle, and
unresolved issue for the whole community.5724Limitations
Inthiswork,werestrictourselvestothemost
popular summarization datasets. The three
analyzeddatasetssharemanysimilaritieswith
each other and thus cannot account for di-
versity. They are all single-document single-
reference English language summarization
datasetsinthenewsdomain. Duetothelim-
ited sequence length accepted by Transformer-
based factual consistency models, our filtra-
tion methodology cannot be generalized for
longer document-summary pairs which are
frequently present in other domains such as
scientific articles and creative writing. The
methodology also cannot be generalized for
otherlanguages. Factualconsistencymodels
suchasDAEaretrainedonannotateddatasets
whichonlyexistfortheEnglishlanguage. This
providesmotivationforimprovingthegener-
alization of factual consistency models.
Another limitation of our work is the lack
ofhumanevaluation. Weprovethesuperior-
ityoftheSummFCdatasetbyevaluatingthe
generated summaries with automatic metrics.
Although these metrics achieve state-of-the-
art correlations with human annotated scores,
theyarestillknowntosignificantlydifferfrom
humanjudgements. Whilethefactualconsis-
tency models that we employ also represent
the current state-of-the art, it is far from guar-
anteed that they are able to identify all the
factualityerrors. Itisonlywithhumanevalu-
ationthatwecanprovideacompletepicture
of where SummFC falls on the full dataset
quality continuum. There is undoubtedly still
moreworktobedoneincontinuingtorefine
datasets to actually measure summarization.
Ethics Statement
We believe that research on improving the
factual consistency of summarization systems
can create positive social impact. We live in
an era of information explosion and everyone,
to some degree, relies on summarization to
process the information overload. It is our
responsibilitytoguaranteethegreaterpublic’s
access to truthful information.
Our research also brings positive impact on
the environmental aspect. Training on smaller
and higher quality datasets significantly re-
duces the consumption of computational en-ergy while also boosting performance.
Acknowledgements
Wethanktheanonymousreviewersfortheir
helpful feedback and insightful comments.
The first and last authors were supported by
the ANR HELAS chair (ANR-19-CHIA-0020-
01). The second author was supported by the
TélécomParisresearchchaironDataScience
and Artificial Intelligence for Digitalized In-
dustry and Services (DSAIDIS).
References57255726A Examples of Factually Inconsistent
Reference Summaries
Source Document:
By . Leah Simpson . PUBLISHED: . 16:46
EST, 19 July 2012 . | . UPDATED: . 02:31
EST,20July2012. Withtheseasonfinaleairing
on Sunday night, The Bachelorette star Emily
Maynard is already making arrangements to
extendher15minutesoffame-withamoveto
Hollywood on the cards. ...... But she’s really
excited to get the date, location and all of the
detailssetsothatshecanmarryJefandhavingit
aironTVfitsinperfectlywithherplans.’ Happy
couple: Emily is engaged to get married to Jef
Holm apparently .
Factually Inconsistent Reference Summary:
Bachelorette spoiler alert .
Source Document:
Bosses said the move was part of an efficiency
drive, with 10 posts set to go in Haverford-
west and 20 at its plant in Aspatria, Cumbria.
"Werecognisethattheimpactoftheseproposed
changes is significant for the people affected and
we are committed to treating people with respect
and consideration," said a spokesman. A staff
consultation starts this week.
Factually Inconsistent Reference Summary:
A total of 30 jobs are under threat at two First
MilkcreameriesinPembrokeshireandtheLake
District.5727