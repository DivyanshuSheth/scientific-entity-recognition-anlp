
Zhuo ZhangXiangjing HuJingyuan ZhangYating Zhang
Hui WangLizhen QuZenglin XuHarbin Institute of Technology, Shenzhen, ChinaPeng Cheng Lab, Shenzhen, ChinaMonash University, Melbourne, AustraliaIndependent Researcher
{iezhuo17, starry.hxj, zhangjingyuan1994, yatingz89}@gmail.com
wanghu06@pcl.ac.cn Lizhen.Qu@monash.edu.cn xuzenglin@hit.edu.cn
Abstract
The inevitable private information in legal data
necessitates legal artificial intelligence to study
privacy-preserving and decentralized learning
methods. Federated learning (FL) has merged
as a promising technique for multiple partici-
pants to collaboratively train a shared model
while efficiently protecting the sensitive data of
participants. However, to the best of our knowl-
edge, there is no work on applying FL to legal
NLP. To fill this gap, this paper presents the
first real-world FL benchmark for legal NLP,
coined FL , which comprises five legal
NLP tasks and one privacy task based on the
data from Chinese courts. Based on the exten-
sive experiments on these datasets, our results
show that FL faces new challenges in terms of
real-world non-IID data. The benchmark also
encourages researchers to investigate privacy
protection using real-world data in the FL set-
ting, as well as deploying models in resource-
constrained scenarios. The code and datasets
of FL are available here.
1 Introduction
It has been noticed that learning, comprehending
and properly using an ever-increasing huge amount
of legal data is way beyond human capability of
legal practitioners (Gomes et al., 2022). Since the
majority of the data is text, such an “information
crisis in law” is encouraging the research and de-
velopment of legal Natural Language Processing
(NLP) techniques, to provide affordable legal ser-
vices to both legal professionals and the general
public (Sun et al., 2020a). As the majority of those
techniques are based on machine learning, they re-
quire training on centralized datasets. However,
such approaches raise increasing privacy concernsFigure 1: The overview of FL .
of the public and impose risks of breaching data
protection laws, such as the General Data Protec-
tion Regulation (GDPR).
To address the above concerns, federated learn-
ing (FL) is widely considered as a family of train-
ing algorithms to achieve a promising trade-off
between information utility and privacy preserva-
tion, without sharing sensitive data of data own-
ers (McMahan et al., 2017). As depicted in Fig-
ure 1, those algorithms permit local machines of
participants to coordinate with one or multiple
servers to train a model in a decentralized and col-
laborative way while preserving data privacy. De-
spite its rosy future, FL still faces open challenges
due to the needs of coping with data heterogene-
ity (Ge et al., 2020), privacy attacks (Gupta et al.,
2022), and system inefficiency (Liu et al., 2022).
In particular, differences between local data dis-
tributions of participants impose a special chal-
lenge when they are not Independently and Iden-
tically Distributed (non-IID) (Zhao et al., 2018).
Although this phenomenon is broadly observed in
practice, almost all studies in this area rely on artifi-
cially partitioned non-IID datasets using heuris-3492
tic sampling methods (Ji et al., 2020; Morafah
et al., 2022), due to the lack of real-world non-IID
datasets. However, the FL datasets resulted from
those sampling methods are significantly less chal-
lenging for FL algorithms than non-IID local data
in real-world applications. As shown in Figure 2
(c), FL algorithms applied on the datasets using
heuristic sampling achieve significantly higher F1
scores than those on the natural non-IID data.
To facilitate FL research in the legal domain,
we build the first FL benchmark for legal NLP,
coined FL . It includes five legal NLP
tasks on real-world legal texts collected from Chi-
nese courts: Legal Cause Prediction ( FLCP ),
Legal Argumentation Mining ( FLAM ), Legal
Entity Recognition ( FLER ), Legal Relation Ex-
traction ( FLRE ), and Legal Judgment Predic-
tion ( FLJP). In addition, we introduce a privacy
attack task, coined FLPA , to evaluate risks of
privacy leakage. To preserve the naturalness of
local distributions, we partition datasets based on
either cities or case categories such that the data in
a different partition comes from a court in a differ-
ent city or belongs to a different case category. Due
to the varying socio-economic status of differentcities, we observe that the data distributions from
the courts in different cities are clearly non-IID. As
illustrated in Figure 2 (b), the data volumes and
label distributions differ dramatically across dif-
ferent cities. The local distributions between case
categories exhibit even higher divergence.
On those natural partitions of our datasets, we
conduct the firstempirical study to investigate the
model performance, privacy risks, and resource
consumption for each legal NLP task with varying
federated learning algorithms. In order to preserve
the key characteristics of sensitive data (shown in
Figure 2 (a)) without privacy leakage, we man-
ually substitute various types of personally iden-
tifiable information (PII) and values of sensitive
attributes, such as person names and addresses, for
non-existing fake information in the same data for-
mats. For example, replacing a real personal ID
with a randomly picked non-existing personal ID
in the same format. In addition, we provide a fully
modularized and easy-to-extend codebase to facil-
itate FL research in the legal domain. Through
extensive experiments on those legal NLP tasks,
we obtain the following interesting findings not
reported in prior FL studies.
•On the natural non-IID data of most of the
legal NLP tasks, there is still a large perfor-
mance gap between FL algorithms and super-
vised algorithms on centralized data.
•For FL algorithms, it is more challenging to
achieve high performance on the natural non-
IID local distributions of almost all legal NLP
tasks than that on the distributions sampled
by heuristic sampling algorithms. Heuristi-
cally splitted data exhibit different research
problems than naturally partitioned data.
•The natural non-IID data partitions pose more
challenges to small and shallow transformer
models (Liu et al., 2019) than their large and
deep counterparts.
2 Preliminaries
This section starts with reviewing the concepts,
problem formulations, and challenges of federated
learning, followed by providing an overview of the
lifecycle of the lawsuit in the Chinese court system.
2.1 Federated Learning
FL is a distributed learning technology that collabo-
ratively learns a shared global model from multiple3493
isolated participants (or silos), while preserving pri-
vacy (McMahan et al., 2017; Li et al., 2020, 2021b).
In a typical FL cross-silo setup, there is a server that
coordinates the FL process and aggregates model
information (e.g., model gradients) collected from
scattered participants.
FedAvg (McMahan et al., 2017) is the first and
one of the most widely used FL algorithms, whose
details are outlined in Algorithm 1. At the be-
ginning of each communication round, the server
sends model parameters Wto each participating
silo. Then, the silo trains on local private data D
(SiloLocalTraining ) and subsequently uploads the
updated model parameters. The server monitors
and collects the updated model parameters from the
silo. After collecting the model parameters from all
the silos, the server aggregates all model updates
according to Eq. (1). The above process is repeated
until the global model converges.
As elaborated in Algorithm 1, we identify three
main challenges in FL as follows. (1) Training
models with FL algorithms on the non-IID lo-
cal data Dbetween silos often leads to infe-
rior performance than that with centralized train-
ing, as demonstrated in previous work (McMahan
et al., 2017; Weller et al., 2022). (2) Although
FL aims to protect the participants’ private data,
prior studies (Zhu et al., 2019; Sun et al., 2020b;
Boenisch et al., 2021) show that the local train-
ing data can be partially reconstructed from thegradients uploaded by participants, resulting in
privacy leakage . (3) Resource-constrained FL
requires high-frequency communication between
the server and participants to accelerate model con-
vergence. However, these participantsoften have
limited computing resources and communication
bandwidth (Pfeiffer et al., 2023), which prevent
them from training large-scale pre-trained models.
2.2 The Lifecycle of Lawsuit
The procedure for legal cases can be broadly di-
vided into three phases in chronological order: (1)
AtPre-trial stage, plaintiffs submit the claims and
evidence to the court, and judges conduct a desk
review of the case and read through the files to
get a rough picture; During this stage, Legal AI
techniques can be applied to assist both plaintiffs
and judges with process work or paperwork. (2)
InTrial stage, two or more parties get chances to
cross-examine in the court; During this stage, the
judge needs to summarize the dispute focusing on
the views of different parties and inquire about their
concerns. This part of the work can be assisted with
Legal AI system by providing some suggestions
through the analysis over past cases. (3) In many
cases, the judge may not directly pronounce sen-
tence in court at the end of trial, instead several
weeks/months should be spent at After-trial stage
to let the judge further review the information ob-
tained during trial and then make the final decision.
In addition, the prosecutor’s office and the court
are responsible for supervising the quality of judg-
ments or even analyzing criminal clues or patterns
with some structural data.
3 FL
To facilitate the research on the incorporation of FL
and LegalAI, we present the legal FL benchmark
FL with natural non-IID partitions and
practical private information. FL consists
of six critical legal tasks which covers a broad range
of task types, federated participant numbers, and
natural non-IID data as shown in Table 1. Examples
for each task can be found in Appendix C.
3.1 Tasks
FLCP The task of Legal Cause Prediction
aims to automatically predict causes, namely case
categories (e.g., private lending disputes), of civil3494
cases. A system tackling this task is commonly
used to assist plaintiffs with limited legal knowl-
edge to choose the correct category of a case in the
filing process at the pre-trial stage.
FLJP Legal Judgment Prediction is a regres-
sion task that automatically predicts the duration
of a sentence given the facts identified by a judge.
Noteworthy, the goal of this task is to provide pre-
dicted judgements as references to users. Based on
estimated judgements, lawyers can tailor their ar-
guments, assess legal risks and provide appropriate
advice to litigants. Similarly, judges may double
check their judgements if there are discrepancies.
FLER The task of Legal Entity Recognition
aims to extract crime-related entities (e.g. instru-
ments of crime, stolen amount and alcohol level
in blood) from case documents. In practice, the
extracted entities contribute to sorting out the gist
of a case and characterization of a crime.
FLRE Based on the outputs of FLER , this
task detects relations among entities and classifies
entity pairs into specific types, such as a certain
drug and its weight. These relations are then uti-
lized to organize massive entities and avoid mis-
placed relations for subsequent analysis.
FLAM Legal Argument Mining seeks to
identify arguments and dispute focuses between
a plaintiff and a defendant from court transcripts
and estimate their argument types. To well un-
derstand a case, judges are required to summarize
those arguments and investigate them during a trial.
Before analyzing arguments and dispute focuses,
cases are divided into different categories and are
assigned to the corresponding courts. Law firmsare usually specialized in only one or a handful of
case categories. As cases are organized by case cat-
egories before analyzing arguments, we partition
data by case categories in this benchmark.
FLPA Legal Privacy Attack aims to eval-
uate privacy leaks in federated learning. Con-
cretely, FL provides a well-designed pri-
vacy attack dataset FLPA containing 80 privacy-
sensitive examples extracted from FLJP. As
shown in Figure 5, such attack data includes
privacy-sensitive attributes (e.g., age and gender)
with various types, such as numbers and characters.
Note that this is the firstreal-world privacy attack
dataset for FL. We hope that FLPA can facilitate
studies of FL in terms of privacy protection.
3.2 Dataset
The source data for all tasks are collected from
the public legal judgements that are anonymized
and released by the Supreme Court of China. The
FLCP dataset is collected from the results of
a rigorous charge determination process, and the
FLJP dataset directly uses the official court
decisions. Regarding the datasets for FLAM ,
FLER andFLRE tasks, we establish a data
schema and the corresponding annotation guide-
lines, and recruit a team of five law school students
for annotation. A legal professional oversees the
process, answering questions about annotation stan-
dards and performing quality checks. On average,
annotating a sample takes about three minutes per
person. The Kappa scores (McHugh, 2012) among
five annotators are 92%, 96%, 96% for each re-
spective task. The sentences provided for FLPA3495are manually created by the annotators to simulate
real-world cases.
Practitioners and researchers aim to improve FL
algorithms that customize models to perform well
on each distinct local dataset and build a global
model to perform well on all partitions without cus-
tomization. The above two goals in FL are often
difficult to achieve altogether, especially on signifi-
cantly heterogeneous data partitions (Kairouz et al.,
2021). Unfortunately, the existing FL benchmarks
only focus on one of the two goals but rarely take
both into consideration (Chen et al., 2022). Thus,
accurately evaluating the pros and cons of differ-
ent FL algorithms for both goals is difficult with
existing FL benchmarks. For example, an optimal
model personalized for a single data partition does
not necessarily perform well on all partitions.
In light of above analysis, we build a local and a
global evaluation set for each task in FL .
For the local one, we divide each local partition
into the local train/valid/test sets by 8:1:1. For the
global evaluation set, we collect the training data
of all partitions and divide the union into the global
train/valid/test sets with the ratios of 8:1:1. During
the global FL training, the global train set is par-
titioned for each participant w.r.t. either courts or
case categories for respective tasks. Table 1 shows
the basic statistics of each dataset in FL .
3.3 Framework Design
To facilitate research on FL in the legal domain, we
build a general FL framework for legal tasks. Fig-
ure 3 shows the overview of our framework. Our
framework is based on FedLab (Zeng et al., 2023),
a lightweight open-source framework for FL sim-
ulation. However, FedLab contains only basic FL
framework components (e.g., communication con-
figurations and FL algorithms), which lack APIs
for downstream tasks. Therefore, on top of Fed-
Lab, we further establish the training pipelines for
various legal tasks. Meanwhile, our framework in-
tegrates HuggingFace, which is widely recognized
for its rich pre-trained models for NLP applications.
Thus this framework is suitable for practitioners to
study Legal NLP problems in FL settings using the
state-of-the-art pre-trained language models.
4 Experiment
In this section, we first show the performance of dif-
ferent FL algorithms on FL (see Section
4.2). To obtain a clear understanding of the prac-
tical challenges of FL in real-world applications,
we conduct an in-depth investigation on FL-, covering privacy leakage analysis (see Sec-
tion 4.3) and resource-constrained FL scenario (see
Section 4.4).
4.1 Experiment Setup
Baseline Algorithms Our experiment adopts the
four typical FL algorithms for each legal task. The
first two are classic and global FL algorithms: Fe-
dAvg (McMahan et al., 2017) is the oft-cited FL
algorithm that collaboratively trains a global FL
model across participants, and FedProx (Li et al.,
2020) addresses statistical heterogeneity in FL by
introducing Lproximal term during the local train-
ing process. The last is the personalized FL method
FedOPT (Reddi et al., 2021) is an extended ver-
sion of FedAvg, which respectively uses two gra-
dient based optimizers in participants and servers.
Ditto (Li et al., 2021b), which excels at tackling
the competing constraints of accuracy, fairness, and
robustness in FL. Besides the FL family, we also
include the local training algorithm: Standalone
refers to the training model only using local data
on each participant without collaborations between
participants, and Centralized refers to the ideal
centralized training setting where the server could
collect all participants’ data. Since pre-trained lan-
guage models (PLMs) have been de facto base
model architecture in NLP research nowadays, we
adopt RoBERTa-WWM (Cui et al., 2019) released
by HggingFacefor all tasks. More implementa-3496tion details on each baseline algorithm can be found
in Appendix B.
Evaluation Strategies As described in Section
3.2, for a comprehensive evaluation, our exper-
iments test all algorithms using two evaluation
strategies: 1) Global test performance ( G )
is evaluated on the global test set and used to deter-
mine whether the model has learned global knowl-
edge. The better results of G indicate that
the model is closer to the centralized training. 2)
Local test performance ( L ) is evaluated on
each local test set and averaged by all participants.
TheL is more practical in real-world applica-
tions than G because it shows performance
improvement without centralizing all local data.
Training Details The number of silos involved in
federated training for each task are listed in Table 1.
Our experiments mainly focus on the cross-silo FL
scenario, where all silos participate in training at
each communication round. In silo local training,
we adopt AdamW optimizer for RoBERTa-WWM.
Considering the trade-off between computation and
communication, we set the local training epoch to
1 and the communication rounds to 20 throughout
experiments except for FLAM . Since FLAM
is a highly non-IID task, we set the communication
round to 50 on this task to ensure that the federated
model can be fully trained.
4.2 Utility Experiment
We first conduct experiments to investigate
different baseline algorithms’ utility on FL-. The experimental results demonstrate that
federated learning iscrucial and efficient for
privacy-sensitive downstream tasks (compared
with Standalone), while there isstill significant
room forperformance improvement using the
real-world data partitions (compared with
Centralized).
The G and L performances are
shown in Table 2 and 3 respectively. FL algo-
rithms outperform Standalone training on G
andL in the majority of FL tasks.
This can be attributed to FL’s privacy-preserving
training manner which enables the model to har-
ness knowledge from all participants, leading to
a significant performance boost. We also observe
that Standalone exhibits either superior or accept-
able L performance in FLCP andF-
LAM . Compared with other tasks, each participant
inFLCP has enough local data, which allowsthe local model to be fully trained and achieves
better performance in local test. As shown in Ta-
ble 4, when there is only a small amount of data
locally, Standalone’s L performance drops
precipitously while the FL algorithm still performs
well. This emphasizes the advantages of FL for
collaborative model training in situations where
local data is limited and centralized collection of
data is prohibited. As for FLAM , we presume
that its strong non-IID features lead to the L
performance better than federated algorithms.
Upon comparing various FL algorithms, we find
that they possess unique pros and cons, specific to
different tasks. While FedAvg may not attain the
best performance in all tasks, its margin of differ-
ence from the best-performing algorithm is min-
imal. FedProx can achieve similar performances
as FedAvg, consistent with the finding of Lin et al.
(2022). FedOPT, an advanced federation algorithm,
attains superior performance in most tasks, which
aligns with prior research (Lin et al., 2022). As a
personalized FL algorithm, Ditto can achieve bet-
ter performance results on L but struggles on
G .FL exhibits the clear trade-off
between global and personalized models, providing
a more comprehensive evaluation of different FL
algorithms. Comparing the FL algorithm with cen-
tralized training, we found a sharp performance gap
between the FL algorithm on G andL
due to the complex real-world data heterogeneity in
FL . In this sense, we believe FL
can facilitate the FL community to develop more
robust FL algorithms.
We further scrutinize the contrast between nat-
ural partitioning and commonly employed artifi-
cially split methods in non-IID settings. For this
analysis, we utilize oft-cited FedAvg and the appli-
cable artificially split methods in each task, refer-
enced in Appendix B. As shown in Table 5, com-
pared with artificially splitted datasets, we find
thatthenatural non-IID isnotably more arduous
toaddress infederated scenarios across alltasks.
Moreover, we uncover that artificially split methods
may fail to accurately reflect the attendant non-IID
complexities, such as those exhibited in FLJP
withαvaluesof 1.0 and 10.0 and FLAM with
αvalues of 0.1 and 1.0. These experimental find-
ings provide further justification for our motivation
to develop our FL .3497
4.3 Privacy Experiment
In FL systems, the server updates the global model
by aggregating participant-uploaded model gradi-
ents, maintaining privacy by not directly accessing
local data. However, prior work (Zhu et al., 2019;
Deng et al., 2021) has demonstrated the potential
privacy breaches in which participants’ training
data can be partially reconstructed from gradients.
To analyze the privacy leakage of FL, we adopt
two gradient-based privacy attack methods: DLG
(Deep Leakage from Gradients) (Zhu et al., 2019)
and TAG (Gradient Attack on Transformer-based
Models) (Deng et al., 2021) in our privacy attack
dataset FLPA . Both attack methods can effec-
tively recover the original data from the participant-
uploaded gradients. For the evaluation metrics, we
follow Song and Raghunathan (2020) and use pre-
cision (the average percentage of recovered words
in the target texts), recall (the average percentage
of words in the target texts are predicted), and F1
score (the harmonic mean between precision and
recall).
Figure 4 shows privacy attack results of
DLG and TAG on FLPA under differ-3498
ent local training batch sizes, we find that
attackers canstillefficiently reconstruct thedata
from theparticipant-uploaded gradients even in
privacy-preserving FL.Figure 4 also shows that
data is more likely to leak when the local batch
size is small. To attain a clearer understanding of
gradient attacks, we show the recovery progress
of gradient attacks on an example of FLPA in
Figure 5. Although the existing gradient attack
can effectively recover every token in the sentence,
itishard fortheattacker torecover theorder of
tokens. This outcome also reveals the potential pri-
vacy risks arising from the unordered bag of words
even though it may be challenging for an attacker
to obtain the exact original training data from the
gradient. Overall, FLPA provides an available
privacy attack dataset, which researchers can use to
simulate privacy attacks and study privacy defenses
in the FL setting.
4.4 Resource Cost
This section analyzes resource-intensive situations
in real-world federated systems, including commu-
nication overhead in federated training and compu-
tational resources of local participants.
The effect of communication We investigate
the performance versus communication budgets on
FLJP andFLAM , which is illustrated in fig-
ure 6. Although FLcanmake themodel attain the
desired performance bymultiple communications
(e.g., more than 80% performance ofcentralized
training), italso requires anextremely heavy
communication cost. For example, the local model
has to upload about 6 GB communication over-
head cumulatively when FL algorithms achieve the
desired performance on FLJP. Such cumber-
some communication overhead is unacceptable in
a real-world federation system, especially when
the local client has limited transmission bandwidth.
With the increasing scale of PLMs, communica-
tion overhead becomes a significant bottleneck
for landing PLMs in real-world FL scenarios. In
this sense, developing communication-friendly and
PLMs-empowered FL algorithms is necessary. Be-
sides, we find that vanilla FedAvg and FedProx al-
gorithms show better performance and robustness
inG performance under extremely non-IID
task FLAM.
The resource-constrained computation Partici-
pants in the FL system typically have limited com-3499putation resources, thereby it is practical to con-
sider small federated models to reduce the com-
putation costs. Figure 7 shows the performances
of different sizes of models in federated and lo-
cal training settings for FLER andFLAM
tasks. We find that smaller models suffer drastic
performance degradation inFL, despite reducing
the training cost of local clients. Note that, the
performance of FL is still weaker than the results
of Centralized setting. This result is contrary to
that in Lin et al. (2022), where they experimen-
tally demonstrate that a small-scale model can still
achieve competitive performance. We speculate
that this result may be due to the real-world data
heterogeneity in FL , and Lin et al. (2022)
uses a heuristic partitioning method. Based on this,
FL could be better to reflect the trade-off
between local computational resources and perfor-
mance.
5 Related Work
Legal Artificial Intelligence Legal Artificial In-
telligence (LegalAI) provides intelligent assistance
for legal practitioners in judicial domain. It pro-
motes the efficiency of lawyers and judges and pro-
vides afford-service for the public. Commendable
progress has been achieved for LegalAI applica-
tions, such as legal judgment prediction (Chalkidis
et al., 2019a; Ma et al., 2021), legal information
extraction(Cardellino et al., 2017; Angelidis et al.,
2018a; Cardellino et al., 2017), legal text classifica-
tion(Chalkidis et al., 2019b), legal text summariza-
tion(Aletras et al., 2016; Duan et al., 2019), and
legal question answering(Khazaeli et al., 2021).
Unfortunately, in practical situations, legal data
of limited size is usually distributed over multiple
regions/courts, and meanwhile different courts may
devote to various scenes of a same task. Due to pri-
vacy and strategic concerns, it is unattainable to put
all these data together (especially for non-public
files) to satisfy the demands of those data-driven
algorithms. The ways to effectively consume these
data in the justice sector remain under-explored.
Federated Learning Federated learning (McMa-
han et al., 2017) (FL) is a prevalent decentralized
machine learning technique in privacy-sensitive
tasks. To facilitate FL research, researchers have
proposed numerous FL benchmarks and made
successful progress in FL standardized evalua-
tion, such as LEAF (Caldas et al., 2018), Fed-
Scale (Lai et al., 2022), pFL-Bench (Chen et al.,2022), FedCV(He et al., 2021), and FedNLP (Lin
et al., 2022). To simulate the non-IID challenge
in FL, these benchmarks generally employ differ-
ent heuristic sampling methods (Ji et al., 2020; Li
et al., 2021a; Morafah et al., 2022) to build het-
erogeneous data partitions from an existing public
dataset and assign them to hypothetical participants,
which may bury the complexity of natural data het-
erogeneity in realistic applications (du Terrail et al.,
2022). Unlike these benchmarks, the datasets in
FL are collected from real-world applica-
tions and preserve the natural non-IID partitioning.
Recently, some benchmarks specifically de-
signed for FL have been proposed. du Terrail et al.
(2022) proposed FLamby, a realistic healthcare
cross-silo FL benchmark. Jain and Jerripothula
(2023) presented the first real-world FL image clas-
sification dataset. These benchmarks are all image
task datasets and either lack task scale or task diver-
sity. Compared to these benchmarks, FL
covers a broad range of NLP task types. To facili-
tate FL’s research on privacy attacks, FL
includes the first practical privacy attack dataset
FLPA.
6 Conclusion
This paper proposes the firstreal-world federated
learning benchmark for legal NLP ( FL ),
which contains five NLP tasks and one privacy task.
The benchmark features a large number of FL par-
ticipants and natural non-IID data partitions. On
this dataset, we conduct the extensive empirical
study, including performance comparisons, privacy
leakage, and resource-constrained analysis. The
experimental results reveal that FL algorithms are
effective for real-world applications but our bench-
mark poses new challenges on natural non-IID par-
titions. In addition, we build a lightweight and
easy-to-extend codebase to facilitate FL research in
the legal domain. We hope that FL would
facilitate the development of novel and practical
FL algorithms for real-world legal applications.
Limitations
We summarized the limitations of FL as
follows: (1) Although FL includes a va-
riety of legal tasks with natural language under-
standing, more useful legal generation tasks should
be included, such as legal court debate, legal case
summary, etc. However, the tasks in FL
are more commonly used in the legal domain com-3500pared to these tasks. On the other hand, the manual
annotation cost is also a limited factor. We will
expand more useful legal tasks and also welcome
contributions of new datasets to keep FL
up-to-date. (2) We do not analyze the FL algo-
rithm’s robustness attacks (i.e., poisoning attacks).
We argue that it is impractical to have malicious
court participants when multiple official courts per-
form federal learning. Therefore that discussion
is beyond the scope of our study in this paper. As
robustness attacks pose significant threats to FL,
FL containing natural non-IID will also be
more suitable for studying powerful FL algorithms
for resisting robustness attacks.
Ethics Statement
All proposed tasks aim at increasing the efficiency
of judges instead of helping the judges make de-
cisions. Extracted or classified information will
be further checked by judges and we only provide
techniques to serve as an auxiliary tool. All source
files of our datasets are from the official legal docu-
ment website and are properly anonymized. We do
not analyze the content of the case or the litigants
in any way other than provide tool for judges.
Acknowledgements
We’d like to thank all the anonymous review-
ers for their careful readings and valuable com-
ments. This work was partially supported by
the National Key Research and Development
Program of China (No. 2018AAA0100204), a
key program of fundamental research from Shen-
zhen Science and Technology Innovation Com-
mission (No. JCYJ20200109113403826), the Ma-
jor Key Project of PCL (No. 2022ZD0115301),
and an Open Research Project of Zhejiang Lab
(NO.2022RC0AB04).
References35013502A The Data Distribution of FL
Figure 8 plots the train/validation/test number of
samples per client for each task in FL .
More details about the example of FL can
be found in the released code.
B Implementation Details
Baseline Algorithms The implementations of
all baseline algorithms are from FedLab, which
is a lightweight open-source framework (Zeng
et al., 2023) for FL simulations. For FedProx, we
search its hyper-parameter λfrom { 0.001, 0.01,
0.05, 0.1, 1.0 }. For Ditto, we tune the hyper-
parameters aandafrom { 0.001, 0.01, 0.1, 1.0,
10.0, 100.0 }. For FedOPT, we design AdamW
as clients’ optimizer while adopting an SGD al-
gorithm with momentum for server optimizer fol-
lowed by FedNLP(Lin et al., 2022), with server’s
momentum hyper-parameter β∈{ 0.1, 0.3, 0.5,
0.7, 0.9, 0.92, 0.95, 0.98, 0.99, 0.999 } and fixed
server learning rate τ=1.0 . To make fair compar-
isons, the total number of local training epochs in
the Standalone algorithms will be greater than that
of FL algorithms. We set local training epochs as
20. All experiments are done on a server with 8
Nvidia Tesla V100 GPUs with 32GB RAM.
Base Models Pre-trained language models
(PLMs) have been de facto base model architec-
ture in NLP research nowadays, so our experiments3503choose PLMs as the base federated model through-
out baseline algorithms. We adopt the RoBERTa-
WWM (Cui et al., 2019) released by Hggingface
for all tasks. The reasons are (1) the corpus of
FL is in Chinese, (2) RoBERTa-WWM is
prevalent in Chinese version PLMs, which achieves
remarkable performance in various downstream
Chinese tasks.
Dir. Partition Methods Details For fair com-
parison, we follow Lin et al. (2022) to generate
artificial local data partitions in comparison with
the natural partitions. Specifically, we generate the
non-IID partitions sampled by Dirichlet (Dir.) dis-
tributions with hyper-parameter α∈{0.1, 1.0, 10},
and compare the performance of FedAvg under
different partitions.
In the context of FLCP andFLAM classi-
fication tasks, we employ the label-level Dirichlet
partition approach, which allocates each client a
specific proportion of samples for each label based
on a Dirichlet distribution. Specifically, for label i,
we sample q∼Dir(α)forNclients, where q
represents the proportion of instances with label i
assigned to client j. For FLJP andFLRE
tasks, we utilize quantity-level Dirichlet partition
to determine each client’s quantity of instances
based on Dirichlet distribution, simulating quan-
tity skew. We use FedLab’s data partition tool to
simulate these two non-IID partition methods. In
theFLER task, we utilize the clustering-level
Dirichlet partition, where sentence embeddings are
generated using Roberta-WWM (Cui et al., 2019),
and K-Means clustering is performed to obtain la-
tent labels. Subsequently, these latent labels are
used to perform label-level Dirichlet partition for
label skew simulation.
Metrics We utilize common metrics Micro-F1
and Macro-F1 to evaluate model performance of
classification tasks (Zhong et al., 2020), includ-
ingFLCP ,FLER ,FLER ,FLAM .
Micro-F1 treats all instances and categories equally,
whereas Macro-F1 computes an F1 score individ-
ually for each category and then averages them.
Precision and recall metrics are employed addition-
ally (Angelidis et al., 2018b) for FLER task.
ForFLJP task, we utilize the S-score metric and
Acc@0.2 metrics used in (Zhong et al., 2018) to as-
sess the judgment score for each case’s prison term.
We denote the ground-truth prison term for the i-thcase as ˆtand the predicted result as t. The differ-
encedis defined as d=|log(ˆt+1)−log(t+1)|.
Based on difference, we calculate prediction score
from the score function f(v)as:
And the final score is determined by taking the
average score of all case instances:
The Acc@0.2 metric calculates the average ac-
curacy of predictions that fall within a 20% interval
around the corresponding ground-truth values.
C FL examples
C.1 FLCP
•Claims (input): Li××submitted a lawsuit
request to the court: 1. Ordered the defendant
Yu××to repay the plaintiff 4000 yuan; 2. The
costs of the case shall be borne by the defen-
dant. Facts and reasons: On April 19, 2015,
because the defendant owed me 4,000 yuan in
wages, the defendant refused to pay me after I
urged him for many times. On November 21,
2017, the defendant issued an IOU to me at
his home, saying that he owed me 4,000 yuan
for his 2015 salary and paid off the IOU in
March 2018. After my repeated urging, the
defendant refused to pay for various reasons.
•Case Cause (ground truth): labor contract
dispute
C.2 FLJP
•Facts (input) : After the trial, it was found
that: 1. On March 29, 2019, at No. ×××,
Chaoyang District, Beijing, the defendant
Song ××defrauded the victim Shao (female,
28 years old, from Beijing) of RMB 16,500
in the name of an overseas purchasing agent.3504Yuan. 2. On March 6, 2019, Song ××, the de-
fendant, defrauded the victim Wang (female,
28 years old, from Beijing) of 8,500 yuan in
the name of an overseas purchasing agent at
No. ×××, Chaoyang District, Beijing.
•Defendants and charges (input) : Song ××;
crime of fraud
•Punishment (ground truth) : 12 Months
C.3 FLER
•Claim tokens (input and ground truth) : The
public prosecution accused: At about 14:00
on March 27, 2018, the defendant Chen ××
stole a Jinli brand F100S mobile phone of
thevictim Liu in Room ×××, Unit ×××, No.
121 Ding Road, ××District, ××District, this
city ( worth RMB 651) and cash RMB 140 .
The next day, the defendant Chen ×× was
arrested by the investigators and brought to
justice, and the above-mentioned cash was
seized, and the cash has been returned. On
April 16 of the same year, Chen ××’s fam-
ily members refunded the victim’s loss and
obtained an understanding.
Criminal suspect ; Victim ; Stolen items
C.4 FLRE
•Claim (input) : The public prosecution ac-
cused: At about 22 o’clock in the evening on
November 20, 2015, the defendant Li ××stole
an iPhone 6 mobile phone from the bag on the
right side of the victim Tang when she was
not prepared by the victim Tang near the ××
Shopping Center on ××Road, ××City. And
the iPhone 6 mobile phone is appraised value
is RMB 4288. Later, Li ××sold the mobile
phone to passers-by at a price of 1,200 yuan,
and the proceeds were squandered. At around
21:00 on November 21, 2015, the police ar-
rested Li near the ××Palace in ××District, ××
City.
•Subject and object (input) : Li ××and an
iPhone 6
•Relationship (ground truth) : Stealing (item)
relationship
C.5 FLAM
•Claim from the plaintiff (input) : The plain-
tiff, Tang ××, sued, claiming that there wasa relationship between the plaintiff and the
defendant in the sale of rough air pump
crankshafts. On January 26, 2013, after the
settlement between the two parties, the defen-
dant Liu still owed the plaintiff RMB 157,160
for the goods, and the defendant issued an
IOU. Afterwards, the defendant only paid
103,800 yuan for the goods, and the balance
of 53,360 yuan has not been paid so far. The
plaintiff has repeatedly demanded but failed.
The defendant Liu is now required to pay
RMB 53,360 for the goods.
•Argumentation from the defendant (input) :
The defendant, Liu ××, argued that the arrears
were true, but the plaintiff’s products had qual-
ity problems, and there were still defective
products worth more than 30,000 yuan that
had not been returned, and they were willing
to pay off the remaining money immediately
after returning the products.
•Disputes (ground truth) : Return goods dis-
pute; Payment dispute; Goods defect dispute3505ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
7
/squareA2. Did you discuss any potential risks of your work?
8
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
43506/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
8
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
3 and 8
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
8
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
8
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
83507