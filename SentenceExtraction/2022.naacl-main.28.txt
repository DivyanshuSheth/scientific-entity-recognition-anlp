
Hao Huang, Xiubo Geng, Guodong Long, Daxin JiangAustralian AI Institute, School of CS, FEIT, University of Technology SydneySTCA NLP Group, Microsoft
{hao.huang-4,guodong.long}@{student.uts,uts}.edu.au
{xigeng,djiang}@microsoft.com
Abstract
This work studies temporal reading comprehen-
sion (TRC), which reads a free-text passage and
answers temporal ordering questions. Precise
question understanding is critical for tempo-
ral reading comprehension. For example, the
question “ What happened before the victory ”
and “ What happened after the victory ” share
almost all words except one, while their an-
swers are totally different. Moreover, even if
two questions query about similar temporal re-
lations, different varieties might also lead to
various answers. For example, although both
the question “ What usually happened during
the press release? ” and “ What might happen
during the press release ” query events which
happen after the press release , they convey di-
vergent semantics. To this end, we propose a
novel reading comprehension approach with
precise question understanding. Specifically, a
temporal ordering question is embedded into
two vectors to capture the referred event and the
temporal relation. Then we evaluate the tempo-
ral relation between candidate events and the
referred event based on that. Such fine-grained
representations offer two benefits. First, it en-
ables a better understanding of the question
by focusing on different elements of a ques-
tion. Second, it provides good interpretabil-
ity when evaluating temporal relations. Fur-
thermore, we also harness an auxiliary con-
trastive loss for representation learning of tem-
poral relations, which aims to distinguish re-
lations with subtle but critical changes. The
proposed approach outperforms strong base-
lines and achieves state-of-the-art performance
on the TORQUE dataset. It also increases the
accuracy of four pre-trained language models
(BERT base, BERT large, RoBERTa base, and
RoBETRa large), demonstrating its generic ef-
fectiveness on divergent models.Figure 1: Examples of temporal reading comprehension.
Temporal relations are diverse: Q1-Q5 list examples of
possible varieties of temporal relations. Small changes
in the question might lead to substantially divergent
semantics: replacing usually in Q4 with might in Q5
leads to different answers. Related events are underlined
in the passage.
1 Introduction
Understanding temporal relationships between
events in a passage is essential for natural lan-
guage understanding (Wang et al., 2019; Dong
et al., 2019). Temporal reading comprehension
(TRC) (Ning et al., 2020) is a natural way to study
temporal relations since natural language questions
are flexible to capture divergent temporal relations
(Zhou et al., 2021). Figure 1 shows several exam-
ples of temporal reading comprehension, where
given a free-text passage, a system is required to
answer temporal questions like “ What usually hap-
pened during the press release? ”.
A natural solution for temporal ordering un-
derstanding is to compare each candidate answer
and the referred event in the question and classify
their temporal relation into several pre-defined cate-385gories, e.g., UzZaman et al. (2013) defines 13 possi-
ble relations such as after, ends, equal to . Nonethe-
less, since temporal relationships vary greatly, it is
almost impossible to enumerate all possible rela-
tionships. Figure 1 shows several divergent vari-
eties of temporal relations: one might query about
plain after in Q1, negated after in Q2, constrained
after in Q3, etc. Similarly, a question might query
about usually happen in Q4, might happen , or
other relations. Moreover, creating sufficient la-
bels for all such relations is costly and poses great
challenges for real-world applications. Therefore,
the classification-based approach is incompetent
to handle the flexible relations in temporal reading
comprehension.
Another paradigm is to formulate it as a read-
ing comprehension problem and directly predict
the answer to a question. With the help of large
pre-trained language models (e.g., BERT (Devlin
et al., 2019) and RoBERTa (Liu et al., 2019)), such
approaches have achieved relatively good perfor-
mance. However, they still struggle for the tempo-
ral reading comprehension task due to the lack of
precise question understanding. For example, given
the same passage, the BERT model fine-tuned on
SQuAD (Rajpurkar et al., 2016) predicts the same
answer to the two questions (Ning et al., 2020),
“What happened before a woman was trapped ” and
“What happened after a woman was trapped ”. In
this case, although the two questions share almost
the same words, the only different one between
before andafter leads to completely opposite inten-
tions. Moreover, even if two questions query about
similar relations, different varieties might also lead
to various answers. Take the question Q4 “ What
usually happened during the press release? ” and
“What might happen during the press release? ” in
Figure 1 as an example. Although they both query
about events occurring after the press release , the
slight difference conveys divergent semantics and
leads to different answers.
To tackle these challenges, we propose a novel
question answering approach with precise ques-
tion understanding. Intuitively, temporal ordering
questions consist of two elements, referred events,
and concerned temporal relations. For example,
the question “ What usually happened during the
press release? ” can be decomposed into the re-
ferred event the press release and the concerned
relation usually happen during . Inspired by this ob-
servation, we first encode such questions into tworepresentations, the event vector hand the relation
vector h. Then we evaluate how well each candi-
date answer matches the relation hcompared to
hwith a separate MLP module. Such fine-grained
representations enable a better understanding of
questions by focusing on different elements with
different vectors and further provides good inter-
pretability about the reasoning process. More im-
portantly, it empowers the model to capture the se-
mantics of divergent variants of temporal relations.
Specifically, we harness an auxiliary contrastive
loss that aims to distinguish relations with subtle
but critical changes.
We evaluate the proposed approach on the
TORQUE dataset and achieve state-of-the-art per-
formance compared to strong baselines. We further
testify its effectiveness based on four different mod-
els (i.e., BERT base, BERT large, RoBERTa base,
RoBERTa large) and demonstrate that precise ques-
tion understanding can improve the QA accuracy
for all models. Ablation study shows that both
question representation learning and contrastive
loss play a critical role in the approach.
2 Related Work
Temporal machine comprehension is closely re-
lated to two areas of works, i.e., machine reading
comprehension and temporal ordering reasoning.
2.1 Machine Reading Comprehension
Machine reading comprehension (MRC) (Ra-
jpurkar et al., 2016, 2018) has attracted much
attention in recent years. Traditional solutions
to MRC tasks focus on utilizing the interaction
information between questions and passages via
attention-based structures (Kadlec et al., 2016;
Dhingra et al., 2017). Later on, pre-trained lan-
guage models (PLMs), e.g., BERT (Devlin et al.,
2019), RoBERTa (Liu et al., 2019), and XLNet
(Yang et al., 2019), have been widely used for MRC
tasks. With the sheer scale of parameters and the
pretraining strategies, PLMs capture more knowl-
edge from the context and have shown outstanding
performance on traditional MRC benchmarks.
For more challenging MRC tasks which intro-
duce multi-hop reasoning (Yang et al., 2018), nu-
merical reasoning (Dua et al., 2019), etc., the
generic PLMs become not applicable. Recent ef-
forts use graph-based reasoning approaches (Chen
et al., 2020) or define specific pretraining training
techniques (Raffel et al., 2020) to solve the above
challenges. However, existing MRC approaches386still struggle for the temporal reading comprehen-
sion task due to the lack of temporal relation un-
derstanding (Ning et al., 2020). Hence, we propose
a novel question answering approach with precise
question understanding to tackle this challenge.
2.2 Temporal Ordering Reasoning
Traditional temporal order reasoning tasks (UzZa-
man et al., 2013; Cassidy et al., 2014; Ning et al.,
2018), are often formulated as relation extraction
tasks. Given the context passage, the target is
to classify the relation between every two events
from a predefined relation set, e.g., UzZaman et al.
(2013) defines 13 possible relations such as after,
ends, equal to . Existing solutions can be roughly
classified into two categories. The first category
focuses on developing the structure of the encoder
to capture more temporal information. For exam-
ple, Cheng et al. (2020) add up a GRU-based dy-
namically updating structure upon the outputs of
the common BERT sentence encoder. The second
category focuses on joint learning with external
knowledge or some specific constraints. For in-
stance, Ning et al. (2019) significantly improve the
extraction performance by joint training temporal
and causal relations.
However, the success of the existing approaches
is limited to the formulation of the traditional tem-
poral order reasoning tasks, where the events and
the candidate temporal relation set are fixed. How-
ever, the fixed candidate relation set cannot cover
all temporal relations in our daily uses. The most re-
cent released dataset, TORQUE (Ning et al., 2020),
formulates temporal ordering reasoning as a ma-
chine reading comprehension task. Given a context
passage, we need to answer a free-text question
about the temporal relations in the context passage.
The task is much analogous to our real-world tasks
and is more challenging – we need to automatically
identify the events and the relations in the free-text
question to retrieve the answers from the context
passage. To the best of our knowledge, we are the
very first to address this challenge.
3 Our Approach
We first introduce the definition of temporal reading
comprehension (TRC) and then describe the model
architecture consisting of contextual encoder, ques-
tion understanding, and event relation assessment.
Finally, we provide details for the learning and
inference process.3.1 Task Definition
The Temporal Reading Comprehension (TRC) task
is defined as follows. Given a passage Pwhich
describes a set of events, a system is required
to answer a temporal ordering question Q. Here
events refer to verbs or nouns which define actions
or states. A temporal ordering question usually
queries events satisfying some concerned temporal
relations considering one or more referred events.
For example, the first passage in Figure 1 describes
events about Hamas goverment , and question Q1
queries which events have the temporal relation
happen after with the referred event the victory .
The answer set Ato a question Qcould be empty
when no events meet the requirement.
3.2 Model Architecture
Figure 2 depicts the proposed model architecture.
Specifically, the passage Pand question Qare
first encoded by a contextual-aware encoder , after
which the representations of the question are passed
to aquestion understanding module. Finally, each
candidate answer is evaluated considering whether
it satisfies the concerned relation to the referred
event by an event relation assessment module.
Contextual Encoder We first encode the
passage-question pairs with a pre-trained language
model encoder, and here we take BERT as an ex-
ample. Given a question Q= [q]and a passage
P= [p], where mandnare token numbers,
we concatenate them into a sequence with the for-
mat of [cls] question [sep] passage [sep] , which
is then fed into the contextual encoder to generate
the embeddings,
[h, ...,h,h, ...,h] =
BERT([ q, ..., q, p, ..., p]),(1)
where h,h∈ Rare embeddings for question
token qand passage token p, and dis the embed-
ding size.387
Question Understanding As discussed in Sec-
tion 1, precise question understanding plays an
essential role in TRC task. Therefore, we propose
a question understanding module to achieve that.
Intuitively, a temporal ordering question consists of
two elements, referred events, and concerned tem-
poral relation. For example, the question “ What
usually happened during the press release ” queries
the temporal relation usually happen to the event
the press release . A straightforward solution is to
decompose the question into two segments directly.
However, natural language questions vary a lot, and
hard decomposition is risky and might propagate
errors to downstream modules, which is verified by
experimental analysis in Section 4.5,
Therefore, we design an attention-based extrac-
tor to decompose the question implicitly, and ob-
tain two hidden representations, hfor the referred
event and hfor the concerned temporal relation
as follows,
s= tanh( Wh+b), z∈ {c, r}(2)
α= Softmax( Ws+b), z∈ {c, r}
(3)
h=/summationdisplayαh, z∈ {c, r} (4)
where W,W∈ R, and b, b∈ R are
learn-able weights for the extractor, h∈ Ris
the embedding for the i-th question token. To effec-
tively learn handh, we employ several auxiliary
losses in the training phase, which will be described
in section 3.3.
Event Relation Assessment Given the question
representations handh, the event relation as-
sessment module evaluates how a candidate an-
swer satisfy the relation hwith respect to h.
Lete=p. . . pdenotes the candidate answer,
which consists of ltokens in the passage P. We
first get the representation of eby pooling over
according token vectors,
h= Pool( h, . . . , h). (5)
Then we concatenate the representations of the
candidate event h, question relation h, and the
question event h, and feed it into a two-layer MLP,
followed by a softmax function to get the final
probability,
o= tanh( W[h;h;h] +b), (6)
p= Softmax( Wo+b), (7)
where W∈ R,W∈ R,b∈ R,
b∈ Rare model parameters, and ;indicates
concatenation. p∈ Ris the probability whether
the candidate esatisfies the temporal relation h
with respect to event h.
3.3 Learning Objectives
We employ three learning objectives for model
training, including a classification loss Lfunction
for final answer prediction, and an attention loss
Land a contrastive loss Lfor precise ques-
tion understanding. The overall loss is a weighted
combination of all the objectives,
L=wL+wL+wL.(8)388Answer Prediction Loss The training objective
for final answer prediction is defined as,
L=−/summationdisplaywˆ plogp, (9)
where Cis the candidate event set, wis the weight
for candidate e,p∈ Ris the predicted probabil-
ity from Eq. (7), and ˆ p∈ {0,1}is the golden
label indicating whether the candidate ebelongs to
the final answer of the question.
Usually, the candidate set Cis derived by pre-
liminary filtering all unigrams in the passage P.
However, some candidates are easy to be classified
while others are not. For example, it is easy to clas-
sify the word government in Figure 1 as a negative
answer since it is not an event. In contrast, predict-
ing whether the word frozen is the answer for Q1
in Figure 1 is more challenging. Inspired by this
observation, we assign weights wfor candidates
in the learning objective, w= 1.5ifeis an event,
and otherwise w= 1.0. The label of whether
a word is an event can be derived when labeling
the final answer with little effort, so we can safely
assume that we always have such annotation.
Attention Loss Besides the answer prediction
loss, we also leverage an auxiliary loss to guide
the learning of the attention score αandα
defined in Eq. (3). We first derive silver annota-
tion for referred events and concerned relation in
a passage using a rule-based approach, which will
be detailed in Section 4.2. Let Q, Qbe the set
of event and relation tokens according to the silver
annotation. Then we have ˆα(z∈ {c, r})as the
derived attention label,
ˆα=/braceleftigg,ifq∈Q,
0, otherwise .(10)
The attention loss is defined as,
L=L+L, (11)
where
L=−/summationdisplayˆαlogα, z∈ {c, r}.(12)
Contrastive Loss As shown in Figure 1, a small
change of a question might lead to substantially di-
vergent temporal relations. To this end, we propose
to leverage a contrastive loss for precise learning
of question relation representations.
For the relation representation hof a question
Q, we derive a positive vector hand a set of
negative ones {h}). The positive sample h
is obtained in two ways. First, we search ques-
tions with the same temporal relations but different
events, from which we randomly sample one and
take its relation representation as h. Note we can
get the silver annotation of events and relations in
a question by a rule-based approach. Please re-
fer to section 4.2 for more details. Second, if no
such questions can be found, we take the similar
approach as in SimCSE (Gao et al., 2021), which
applies a different dropout on hand gets a variant
ofhash. We search questions that contain the
same events by different temporal relations with
respect to Q, and take their relation representations
as the negative set {h}).
Given the triple (h,h,{h})for the question
Q, its loss is defined as,
L(Q) =
−loge
e+/summationtexte,(13)
where cos()indicates cosine similarity.
3.4 Inference
The inference phase takes three steps. First, we gen-
erate a candidate set Cfor each passage P. Gener-
ally speaking, one can take any n-gram in Pas a
candidate. In temporal relation understanding, we
usually take a triggering word as an event candidate.
Therefore, Cis the set of all unigrams in P. Then,
we filter Caccording to part-of-speech (POS) tag-
ging. Specifically, we use an off-the-shelf POS tag-
ger to tag all words in P, and then keep only verbs
and nouns in C. Finally, each candidate e∈ C389together with the passage Pand the question Q
is fed into our proposed model, and eis evaluated
according to Eq. (7) and gets its score p, where
prepresents the probability that the candidate
matches the question Q. Then we can get the final
answer set AasA={e:e∈ Candp> τ},
where τis a predefined threshold.
4 Experiments
This section describes an empirical evaluation of
our proposed approach. We also provide analysis,
ablation studies, and case analysis to demonstrate
its effectiveness.
4.1 Settings
Dataset We evaluate the proposed approach on
the TORQUE dataset. TORQUE is a temporal
reading comprehension benchmark. Each training
sample contains a passage and a question requiring
understanding temporal relation between events in
the passage. Figure 1 shows several examples of
training data. The answer to a question consists
of an event set A, and Acould be empty if no
event in the passage satisfies the requirement of the
question. In TORQUE, events are defined as event
triggers, usually verbs or nouns describing actions
or states. There are 3.16k passages with 30.7k ques-
tions in total and 2 events for an answer on average.
We follow the official splitwith 80%/5%/15% of
data in training/validation/test.
Evaluation Metrics Following Ning et al.
(2020), we report three metrics in our exper-
iment, including standard macro F1 and Exact
Match (EM) for question answering and consis-
tency score(C). There are multiple annotations for
each passage-question pair, which might not al-
ways be consistent with each other. We follow the
official implementation. Specifically, for each sam-
ple, a model’s prediction is evaluated according to
all annotations, where the largest score is selected
and aggregated as the final result.
4.2 Implementation Details
We experiment four pre-trained language mod-
els as our contextual encoder, i.e., the base and
large model of BERT (Devlin et al., 2019) and
RoBERTa (Liu et al., 2019). The embedding size
dis set to 64, din Eq (6) and Eq (7) is set
to 64. The threshold τfor inference is set to
0.5. In model training, the batch size is set to16, the dropout rate is set to 0.5. The combina-
tion weight w,wandwin Eq. (8) is set
to 1.0, 0.3, and 1.0, respectively. We search the
learning rate lr, with grid searching within 3 tri-
als in lr∈ {0.9×10,1.0×10,1.1×10}
for the base and large model of RoBERTa, and
lr∈ {4.0×10,5.0×10,6.0×10}for the
base and large model of BERT. The implementa-
tion is based on Python and trained on a Tesla V100
GPU with Adam optimizer for approximately three
hours (base model with approximately 110M pa-
rameters) and ten hours (large model with approx-
imately 340M parameters). We get the averaged
result of three trials for each setting, choose the
model with the highest F1 score on the develop-
ment set, and report the performance on the test set
derived from the official online test.
Deriving Attention Annotation The relation an-
notation Qfor question Qis derived as follows.
First, we compile a dictionary for temporal rela-
tions, such as before ,after , etc. Please refer to
Appendix A.1 for the complete list. Then Qis
constructed with those words in Qthat hit the dic-
tionary. The event annotation Qis mainly derived
according to the passage P. Particularly, we as-
sume the mentioned event list EinPis known. If
a word of Qmatches an event in E, it is included in
Q. Otherwise, if no words of QhitE, we rely on
the relation annotation. Suppose the last relation
word is in position k, then Qis set as Q.
4.3 Main Results
We compare our approach with the baseline (Ning
et al., 2020), which takes a passage and the corre-
sponding question as input and applies a one-layer
perception on the embedding of each token to pre-
dict whether it is the answer of the question or not.
The comparison results with four different contex-
tual encoders are shown in Table 1. The table shows
that our proposed approach outperforms the base-
line on nearly all evaluation metrics. Our model
achieves state-of-the-art results with the RoBERTa-
large encoder, increasing the F1 score by 1.8% and
0.9% for the dev and test set, respectively. We can
see a huge increase for the consistency score (C) on
the test set from 34.5% to 38.1%. Using other pre-
train language models like BERT-base, our model
also improves the performance compared to the
baseline approach, by 2.6%, 3.2%, 2.5% in terms
of F1, EM, and C score, respectively. Although390
there is still a large gap towards the human perfor-
mance, our model takes a large step compared to
the baseline approach, verifying the effectiveness
of the proposed approach.
4.4 Ablation Study
We conduct an ablation study to illustrate the ef-
fectiveness of each loss in our approach. As shown
in Table 2, removing the contrastive loss will lead
to a 1.1% drop on consistency value. When we
remove both the contrastive and attention loss for
question understanding and use mean pooling over
the contextual embedding of the whole question
token sequence, the macro F1 score and the con-
sistency score decrease by 0.5% and 1.5%, respec-
tively, showing that precise question understanding
plays a critical role for TRC. Also, we remove
weight win the answer prediction loss in Eq. (9),
which results in a 0.3% drop in terms of the F1
score. When all auxiliary loss is removed, whichis basically the same as the baseline model with
our own implementation, it leads to a huge gap
of 1.3%, 1.3%, 4.1% on macro F1, exactly match
and Consistency score, respectively. The results
of the ablation study indicate that each element of
our proposed model is critical for temporal relation
understanding.
4.5 Question Representation Analysis
As discussed in Section 3.2, a straightforward
solution for question understanding is to decom-
pose a temporal ordering question into two parts
directly. This section compares our attention-based
approach with the hard question decomposition,
which obtains the two question vectors handh
by conducting mean pooling over embeddings of
tokens in QandQrespectively. The comparison
results are shown in Table 3. We can see that al-
though the rule-based approach achieves relatively
good accuracy, it still underperforms our attention-
based approach. For example, when no contrastive
loss is employed, the EM score drops by 0.9%
when replacing the attention-based representation
with the rule-based one. The possible reason is
that the rule-based decomposition cannot handle
all questions perfectly, and errors in the decompo-
sition will be propagated to downstream modules.
For example, “ What could have happened while
the announcement was made but didn’t? ”. “but
didn’t ” is a crucial negate in the temporal relation,
but the rule-based method might miss it.
4.6 Case Study
Figure 6 shows predicted answers of our model
and the baseline for several questions. For the first
passage, Questions 1, 2, and 3 inquire about the
“happened after ” temporal relation, but with subtle
differences. Q1 is the most common form, which
can be answered correctly by both the baseline and
our proposed approach. Meanwhile, the baseline
model can not capture the negation information391
in Q2 and fails to predict the correct answer. In
Q3 “ happened after ” is constrained by the word
begin , which confuses the baseline model and leads
to partially correct answers. In contrast, our pro-
posed approach can capture these subtle but critical
differences and thus makes correct predictions.
For the second passage, our proposed model
performs better for all three questions of differ-
ent temporal types. Q1 and Q2 are variants of
uncertain relations, which query about two oppo-
site temporal relations “ started after ” and “ started
before ”. The word “ might ” brings uncertainty for
the concerned temporal relation, which confuses
the baseline model, leading to the wrong predic-
tion for the candidate answer “ turbulence ” for both
questions. Q3 queries about a popular temporal
relation, and our model can precisely capture the
difference between it and two other ones and pre-
dict that the candidate event “ increase ” does not
meet its requirement since it comes from a contro-
versial report.
4.7 Error Analysis
We randomly sample 100 wrongly predicted
question-passage pairs from the validation set,
which can be summarized into three categories.
Multi-round Reasoning Sometimes one needs
to perform multi-round reasoning to infer the re-
lation between two events, for example, given the
passage “ Roughly 40 minutes after the operation
began, jubilant soldiers appeared on the rooftop of
the residence, flashing the V victory sign. Then Fu-
jimori, who ordered the operation, arrived to tour
the residence and embraced the freed hostages. ”,the temporal ordering between “ ordered ” and “ the
jubilant soldiers appeared on the rooftop ” is in-
ferred by multi-step reasoning. That is, “ ordered ”
happened before “ operation began ”, and “ opera-
tion began ” happened before “ solder appeared ”,
and thus “ ordered ” happened before “ appeared ”.
An advanced reasoning framework is necessary to
handle such cases, and we leave it as future work.
Commonsense Knowledge Required The given
passage might not provide sufficient information.
For example, in the passage “ He was preparing the
paperwork for the move, following the course of an
absolutely standard transfer. Sadly he killed him-
self at home in the meantime. ”, although it states
that “ preparing the paperwork ” and ““ he killed
himself ” happened “ in the meantime ”, common-
sense knowledge indicates that one cannot kill him-
self and prepare the paperwork at the same time .
So we can infer that “ preparing ” happened before
“killed ”. Incorporating external knowledge is a po-
tential solution for such cases.
Ambiguous Labeling Since the concept of event
is not well-defined, it might lead to ambiguous
labeling. Considering a passage contains a span
“decision is made ”, some annotators might label
decision as a candidate event, while others does
not. This causes inconsistent labeling, and thus
makes it difficult to learn a good predictor.
5 Conclusion and Future Work
Temporal reading comprehension plays a critical
role in natural language understanding. In this pa-
per, we propose a precise question understanding
method to tackle the TRC problem. Specifically,
we encode temporal ordering questions into repre-392sentations of referred events and concerned tempo-
ral relations, based on which candidate answers are
evaluated in terms of their temporal relations to the
referred events. In addition, a contrastive loss is em-
ployed to empower the model to capture essential
differences among temporal relations. Experimen-
tal results based on four pre-trained models verify
the effectiveness of our proposed approach. In the
future, we will investigate general approaches to
handle more diverse temporal relation understand-
ing problems and improve the passage understand-
ing capability for temporal reading comprehension.
References393
A Supplement Information for
Experiments
A.1 Dictionary for Temporal Relations
[’before’, ’after’, ’while’, ’not’, ’future’, ’might’,
’happen’, ’will’, ’may’, ’have’, ’begin’, ’but’, ’fin-ish’, ’don’t’, ’continue’, ’do’, ’start’, ’eventually’,
’during’, ’likely’, ’needs’, ’occur’, ’take’, ’place’,
’lead’, ’when’, ’prior’, ’same’, ’time’, ’end’, ’on-
going’, ’now’, ’past’, ’since’, ’already’, ’expect’,
’go’, ’fail’, ’around’, ’once’, ’be’]394