
Michalis Korakakis
University of Cambridge
mk2008@cam.ac.ukAndreas Vlachos
University of Cambridge
av308@cam.ac.uk
Abstract
Natural language inference (NLI) models are
susceptible to learning shortcuts, i.e. decision
rules that spuriously correlate with the label.
As a result, they achieve high in-distribution
performance, but fail to generalize to out-of-
distribution samples where such correlations
do not hold. In this paper, we present a training
method to reduce the reliance of NLI models on
shortcuts and improve their out-of-distribution
performance without assuming prior knowl-
edge of the shortcuts being targeted. To this
end, we propose a minimax objective between
a learner model being trained for the NLI task,
and an auxiliary model aiming to maximize the
learner’s loss by up-weighting examples from
regions of the input space where the learner
incurs high losses. This process incentivizes
the learner to focus on under-represented “hard”
examples with patterns that contradict the short-
cuts learned from the prevailing “easy” ex-
amples. Experimental results on three NLI
datasets demonstrate that our method consis-
tently outperforms other robustness enhancing
techniques on out-of-distribution adversarial
test sets, while maintaining high in-distribution
accuracy.
1 Introduction
Natural language inference (NLI)models have
achieved state-of-the-art results on many bench-
marks (Wang et al., 2019). However, recent work
has demonstrated that their success is partly due
to learning and using shortcuts (Gururangan et al.,
2018; Poliak et al., 2018; Geirhos et al., 2020),
i.e. spurious correlations between input attributes
and labels introduced during dataset creation.For
example, high word-overlap between the premise
and the hypothesis in the MNLI (Williams et al.,
Figure 1: Illustration of the proposed minimax train-
ing objective. The learner optimizes for the NLI task,
whereas the auxiliary tries to maximize the learner’s
loss by generating an example weight distribution that
encourages the learner to focus on “hard” examples with
patterns that contradict the shortcuts. At inference time
the learner can make predictions without the auxiliary.
2018) dataset is strongly correlated with the entail-
ment label (McCoy et al., 2019). Consequently,
NLI models that exploit shortcuts perform well on
in-distribution samples, but are brittle when tested
on out-of-distribution adversarial test sets that ex-
plicitly target such phenomena (Naik et al., 2018;
Glockner et al., 2018).
Thus, numerous methods have been proposed
to prevent models from learning shortcuts present
in NLI datasets (Belinkov et al., 2019; Schuster
et al., 2019; Zhou and Bansal, 2020; Stacey et al.,
2020; Du et al., 2021; Modarressi et al., 2023, inter
alia). Most approaches typically assume access to
an auxiliary model designed to rely on shortcuts
for predictions. The output of the auxiliary is then
used to re-weight training instances for the learner
model via ensembling (He et al., 2019; Clark et al.,
2019; Karimi Mahabadi et al., 2020). However,
knowing the shortcuts in advance assumes domain-
and dataset-specific knowledge, which is not al-
ways available and limits the potential of shortcut
mitigation (Rajaee et al., 2022).
A separate line of work overcomes this issue by14322forcing the auxiliary model to learn and exploit
shortcuts either by exposing it to only a small num-
ber of training examples (Utama et al., 2020b),
or by leveraging an auxiliary with reduced learn-
ing capabilities (Clark et al., 2020a; Sanh et al.,
2021). Another approach is to fine-tune an al-
ready trained NLI model on examples that were
frequently misclassified during the initial training
stage (Yaghoobzadeh et al., 2021). While these
works show promising results, they assume that
the learner will naturally exploit the same types of
shortcuts as the auxiliary. In practise, the behavior
of the learner diverges from that of the auxiliary.
For instance, Amirkhani and Pilehvar (2021) em-
pirically demonstrate that commonly used auxil-
iaries often down-weight examples that are useful
for training the learner, while Xiong et al. (2021)
show that inaccurate uncertainty estimations by
the auxiliary model can hinder the learner’s out-of-
distribution generalization capabilities.
In this paper, we propose a training method to
reduce the reliance of NLI models on shortcuts
in order to improve their out-of-distribution perfor-
mance. To this end, we frame training as a minimax
objective between a learner and an auxiliary (Fig-
ure 1). The learner optimizes for the NLI task,
whereas the auxiliary tries to maximize the loss of
the learner by up-weighting “hard” examples. The
key insight behind our training method is that NLI
models suffer from poor performance on under-
represented “hard” training instances with patterns
that contradict the shortcuts found in the dominant
“easy” examples (Tu et al., 2020). Therefore, by
encouraging the learner to perform well on these
examples and rely less on the “easy” examples with
shortcuts, we can obtain better out-of-distribution
generalization.
Compared to existing robustness enhancing tech-
niques, our training method (i) does not assume
knowledge of the shortcuts being targeted, (ii) de-
tects and up-weights examples in a data-driven way,
i.e. the auxiliary is a parameterized neural network
that predicts weights for each training instance
at every training iteration, and (iii) uses a small
feed-forward network rather than a large-scale pre–
trained language model (PLM) for the auxiliary,
thus incurring a small computational overhead.
We evaluate our proposed method in
three commonly-used NLI datasets, namely,
MNLI (Williams et al., 2018), FEVER (Thorne
et al., 2018), and QQP (Iyer et al., 2017), andtheir corresponding out-of-distribution adversarial
test sets, HANS (McCoy et al., 2019), Symmet-
ric (Schuster et al., 2019), and PAWS (Zhang
et al., 2019). We observe that compared to other
state-of-the-art robustness enhancing methods, the
minimax training objective consistently improves
out-of-distribution performance. We further verify
the effectiveness of the minimax objective using
a synthetic shortcut experimental setup, and then
show that the performance gains generalize to a
range of large-scale PLMs, out-of-domain test
sets, and a question answering dataset. Finally,
we empirically analyze the minimax objective
to obtain further insights in the workings of the
proposed training method.
2 Minimax Training for Shortcut
Mitigation
Suppose we have a dataset D={(x, y)}
comprising the input data x∈ X and the labels
y∈ Y. Our goal is to learn a model f:X → Y
parameterized by θ. The standard training objec-
tive for achieving this is empirical risk minimiza-
tion (ERM) that minimizes the average training
loss:
J(θ) = min1
N/summationdisplayℓ(f(x), y), (1)
where ℓ(f(x), y)is the cross-entropy loss. When
shortcuts are present in the “easy” examples that
are well-represented in the training data, ERM-
trained models will exploit them to achieve low
training loss. Consequently, this will lead to poor
performance on under-represented “hard” exam-
ples where such shortcuts do not hold. These
examples are pivotal for ensuring good general-
ization performance on out-of-distribution sam-
ples (Yaghoobzadeh et al., 2021). Crucially, the
loss of “hard” examples decreases considerably
more slowly than the average loss throughout train-
ing (Tu et al., 2020). Therefore, our aim is to obtain
a weight distribution that places emphasis on the
under-represented “hard” examples, where we min-
imize the weighted training loss:
J(θ) = min/summationdisplaywℓ(f(x), y), (2)
where wis the weight associated with the i-th ex-
ample x. Intuitively, the example weights should14323Algorithm 1: Minimax Training.
Input: Dataset D, learner f, auxiliary g,
mini-batch size n, # of iterations T
Output: optimized learner f
pre-train fonDusing ERM
forτ= 1, . . . , T do
sample mini-batch {x, y}fromD
generate weights via g
generate predictions via f(x, y)
update θto
min/summationdisplayg(x, y)ℓ(f(x), y)
update ϕto
max/summationdisplayg(x, y)ℓ(f(x), y)
end
have high values for the under-represented “hard”
instances, and low values for the prevailing “easy”
instances with shortcuts.
To compute the example weight distribution, we
propose a minimax training objective between a
learner fand an auxiliary g:X × Y → [0,1]
parameterized by ϕ. Both models are optimized
in an alternating fashion. The learner ftries to
minimize the loss for the classification task (NLI in
this paper). The task of the auxiliary gis to maxi-
mize the learner’s loss by generating a weight for
each training example at every training iteration,
such that the learner is incentivized to concentrate
on regions of the input space where it incurs high
losses. Thus, the learner will prioritize learning
from under-represented “hard” examples that coun-
teract the use of shortcuts present in the dominant
“easy” examples. Formally, the minimax objective
can be written as:
J(θ, ϕ) = minmax/summationdisplayg(x, y)ℓ(f(x), y).
(3)
Both θandϕcan be optimized using any stan-
dard optimization algorithm, such as stochastic gra-
dient descent. In order to ensure that the example
weights lie in the range [0,1], the output of the
auxiliary model is passed through a sigmoid func-
tion. At test time the learner can make predictions
without relying on the auxiliary. Algorithm 1 sum-
marizes the overall training procedure.3 Experimental Setup
3.1 Data
We conduct experiments using three English NLI
datasets, MNLI, FEVER, and QQP. For each
dataset, we evaluate performance on an out-of-
distribution adversarial test set constructed to ex-
amine model reliance on specific shortcuts for pre-
dictions.
MNLI The MNLI (Williams et al., 2018) dataset
contains approximately 430k premise-hypothesis
pairs labelled as entailment if the premise entails
the hypothesis, contradiction if it contradicts the
hypothesis, or neutral otherwise. We evaluate in-
distribution performance on MNLI-matched and
out-of-distribution performance on HANS (McCoy
et al., 2019), an adversarial test set designed to
investigate whether a model exploits the high-word
overlap shortcut to predict entailment.
FEVER We conduct experiments on
FEVER (Thorne et al., 2018), a fact verifi-
cation dataset containing around 180k pairs of
claim–evidence pairs. The goal in FEVER is to
predict whether the retrieved evidence supports
a claim, refutes a claim, or there is not enough
information. As we are interested in the NLI part
of the task, we assume the gold evidence given.
We further evaluate on Symmetric (Schuster et al.,
2019), which is designed to address the claim-only
shortcut, whereby a model learns to use only the
claim for predictions while ignoring the evidence.
QQP The QQP (Iyer et al., 2017) dataset con-
tains over 400k question pairs annotated as either
paraphrase or non-paraphrase. We evaluate out-of-
distribution performance on PAWS (Zhang et al.,
2019), an adversarial test set constructed to pe-
nalize the high-word overlap shortcut that models
exploit to predict the paraphrase label.
3.2 Models
Following previous work (Sanh et al., 2021; Utama
et al., 2020b; Yaghoobzadeh et al., 2021), we use
BERT (Devlin et al., 2019) and conduct experi-
ments with BERT-base as the learner model. We
use a 3-layer multiple-layer perceptron (MLP) for
the auxiliary with tanh as the activation function
for the middle layer. Furthermore, we normal-
ize the weights of the auxiliary to have a mean
weight of 1 across the batch, and add a constant
value to every example weight to ensure that all14324
examples contribute to the loss in order to avoid
filtering useful “easy” examples and hurting in-
distribution performance, i.e. w+c> 0, with c=
1. We obtain word representations for the auxiliary
using 300-dimensional Glove (Pennington et al.,
2014) embeddings and averaging them. We use
Adam (Kingma and Ba, 2015) to train both the aux-
iliary and the learner model. For the learner model
we use default architectures and hyperparameters
from the Hugging Face Transformers library (Wolf
et al., 2020). Finally, we pre-train the learner model
for 3 epochs to ensure that it will initially priori-
tize learning the shortcuts. We train models for
10 epochs and report the mean and standard de-
viation over 5 runs with different random seeds.
Finally, we conduct statistical significance using a
two-tailed t-test (with p< 0.05).
3.3 Baselines
We compare our method with representative tech-
niques from two robustness enhancing categories.
The first category assumes that the shortcut being
targeted for mitigation is known in advance. We use
the method of Karimi Mahabadi et al. (2020) (PoE)
which ensembles the auxiliary and the learner via
the product-of-experts (Hinton, 2002), so that the
learner will focus on examples that the auxiliary
cannot predict well. We also consider confidence
regularization (Utama et al., 2020a) (Regularized-
conf) which relies on a self-knowledge distillation
objective to down-weight examples for which the
auxiliary provides over-confident predictions.
The second robustness enhancing category in-
cludes approaches that do not assume any priorshortcut knowledge. We use the method of Utama
et al. (2020b) (Self-debias), who propose to ex-
ploit a “shallow” model trained on a small fraction
of the training data as the auxiliary model. Sanh
et al. (2021) (PoE + CE) use BERT-tiny (Turc
et al., 2019) as a “weak” (low-capacity) auxiliary
model, and train it on the full training dataset. Fi-
nally, Yaghoobzadeh et al. (2021) ( F) first
train the model on the entire dataset, and then fine-
tune it on the “forgettable examples” (Toneva et al.,
2019), i.e. samples that during the initial training
stage were either classified correctly and misclassi-
fied afterwards, or they were never properly classi-
fied.
4 Results
Main Results Table 1 presents the main exper-
imental results. In general, we see that in all set-
tings the minimax objective significantly improves
out-of-distribution performance on the adversar-
ial test sets compared to the ERM-trained BERT-
base baseline. In particular, it outperforms the
latter on HANS, Symmetric, and PAWS by 10.2,
7.4, and 17.7, respectively. However, we also ob-
serve that training using the minimax objective
results in a small reduction in the in-distribution
performance. Specifically, on MNLI, FEVER, and
QQP, the decrease in the in-distribution accuracy
is 0.8, 0.3, and 2.9, respectively. Compared to
other state-of-the-art robustness enhancing tech-
niques, our method improves in-distribution accu-
racy on MNLI and out-of-distribution performance
on HANS and Symmetric. Conversely, on QQP,
Fand Reguralized-conf outperform minimax143250.2 0.4 0.6 0.8405060708090
pERM
Minimax
training by 1.1 and 1.2, while on PAWS Self-debias
improves out-of-distribution performance by 3.5.
Notably, the improvement for Self-debias comes
at the expense of a considerable drop in the in-
distribution performance on QQP, i.e. 5.6 reduction
in accuracy for Self-debias compared to the ERM-
trained BERT-base model.
Synthetic Shortcut Following previous
work (He et al., 2019; Clark et al., 2019; Sanh
et al., 2021), we modify the MNLI training data
by adding a synthetic shortcut, i.e. a prefix in the
hypothesis containing the ground-truth label with
probability p or alternatively a random
label. Conversely, on the modified MNLI test set
the prefix is always a random label. If the model
exploits the synthetic shortcut for predictions, then
it will have low accuracy on the test set. Figure 2
shows that the performance of the ERM-trained
BERT-base model deteriorates rapidly on the test
set as the number of training examples containing
the synthetic shortcut increases, whereas the
reduction in performance for the model trained
with the minimax objective is much less drastic.
Out-of-Domain Generalization We further in-
vestigate the generalization capabilities of models
trained using the proposed minimax objective on
various out-of-domain NLI datasets. To this end,
following the experimental setup of Karimi Ma-
habadi et al. (2020) and Sanh et al. (2021), we
train models on SNLI (Bowman et al., 2015),
and evaluate performance on AddOneRTE (Ad-
dOne) (Pavlick and Callison-Burch, 2016), Defi-
nite Pronoun Resolution (DPR) (Rahman and Ng,
2012), Semantic Proto-Roles (SPR) (Reisinger
et al., 2015), FrameNet+ (FN+) (Pavlick et al.,
2015), SciTail (Khot et al., 2018), GLUE diagnos-
tic test set (Wang et al., 2018), and the SNLI-hard
test set (Gururangan et al., 2018). Table 2 presents
the results. In general, we observe that our method
consistenly outperforms the ERM-trained BERT-
base baseline, with the only exception being the
GLUE diagnostic test set, where the latter improves
accuracy by 0.24. Furthermore, we see that the min-
imax training outperforms PoE and PoE + CE in
five out of 7 out-of-domain test sets.
Large-scale Pre-trained Language Models We
examine whether the performance improvements
of training the BERT-base model using the min-
imax objective also transfer to large-scale PLMs.
In particular, we conduct experiments with BERT-
large (Devlin et al., 2019) (340M parameters),
RoBERTa-large (Liu et al., 2019) (340M parame-14326
ters), and XLNet-large (Yang et al., 2019) (355M
parameters). The experimental results in Table 3
demonstrate that our method yields substantial per-
formance gains over ERM for all three large-scale
PLMs. In particular, on HANS, Symmetric, and
PAWS, minimax training improves performance
compared to ERM for BERT-large by 5.7, 9.1, and
19.2, for RoBERTa-large by 4.2, 6, and 17.8, and
finally, for XLNet-large by 2.5, 7.7, and 18.2, re-
spectively.
Question Answering Following Sanh et al.
(2021), we also conduct experiments on a question
answering dataset. In particular, we train BERT-
base models on SQuAD (Rajpurkar et al., 2016),
and evaluate their out-of-distribution performance
on the Adversarial SQuAD dataset (Jia and Liang,
2017). Table 4 shows that minimax improves out-
of-distribution performance on the AddSent and
AddOneSent adversarial test sets compared to the
ERM-trained BERT-base baseline and PoE + CE.
5 Analysis
Using the loss to detect “hard” examples We
investigate whether the loss provides a robust sig-
nal for discovering “hard” examples that contradict
the shortcuts found in the “easy” examples. To this
end, we manually classify training instances from
MNLI into two categories, namely, “easy” entail-
ment instances with a large amount of words oc-
curring both in the hypothesis and the premise, and
under-represented “hard” non-entailment examples
with high word overlap, and study their average
losses during training. Figure 3 demonstrates that
the high-loss examples on MNLI are dominated by
the “hard” non-entailment category, whereas the
“easy” entailment examples incur predominantly
low-losses.
Removing “easy” examples and inverting the
weight distribution We evaluate whether we can
improve the overall performance of the minimax0.501.001.502.002.500.501.001.502.002.503.00
LossEasy
Hard
objective by discarding the “easy” examples (fil-
tering minimax), i.e. removing their contribution
to the loss by allowing the auxiliary to generate
example weights w≥0via setting c= 0. Fur-
thermore, we also examine whether the learnt ex-
ample weight distribution is meaningful, by keep-
ing the order of the examples fixed and inverting
their weights (inverse minimax), i.e. the examples
with the largest weights get the lowest weights and
vice versa. The experimental results in Table 5
show that using filtering minimax results in similar
out-of-distribution performance to that of standard
minimax, however, the drop in the in-distribution
performance for the former is much more consid-
erable. Conversely, the inverse minimax objective
leads to high in-distribution accuracy (similar to
that of ERM-trained models), at the expense of
out-of-distribution performance.
Effect of number of epochs for pre-training
the learner We investigate how performance
changes as we vary the number of epochs re-
quired for pre-training the learner model. Figure 4a
demonstrates that out-of-distribution performance
is high when we pre-train the learner for 2 and 3
epochs, but drops when the duration of the pre-
training stage is either too short or too long, which
consequently results in less informative losses for
the auxiliary to learn from.
Impact of size of the auxiliary We explore
whether the size of the auxiliary impacts the14327MethodMNLI FEVER QQP
Dev HANS Dev Sym. Dev PAWS
ERM 84.4 62.6 85.7 55.1 90.8 36.0
Minimax 83.6 72.8 85.4 62.5 87.9 53.7
Filtering Minimax 80.1 69.9 81.7 61.7 83.8 51.3
Inverse Minimax 84.3 59.6 85.5 53.2 90.6 31.2
0 1 2 3 4 560708090
Epochs
1 2 3 4 5708090
Auxiliary Size
learner’s in-distribution and out-of-distribution per-
formance. To this end, we train the learner using
several auxiliary models of varying sizes. Specifi-
cally, we make auxiliary models larger by increas-
ing the number of hidden layers while keeping the
other hyperparameters constant. We observe that
varying the capacity of the auxiliary model affects
the learner’s in-distribution and out-of-distribution
performance (Figure 4b). In particular, the out-
of-distribution performance of the learner model
increases as the auxiliary model becomes stronger
up to a certain point, while in-distribution perfor-
mance drops slightly at first and then more strongly.
Finally, we observe that increasing the size of the
auxiliary has the side effect of incentivizing it to
learn the trivial solution of maximising all example
weights.
Examining the weighted examples We use the
converged auxiliary model to present examples of
down-weighted and up-weighted training instances
on MNLI. Table 6 demonstrates that the auxiliary is
able to correctly identify, and subsequently, down-
weight “easy” examples, i.e. entailment with a largeamount of words occurring in the premise and hy-
pothesis, and up-weight “hard” examples with pat-
terns that directly contradict the shortcut, i.e. non-
entailment with high word overlap. Furthermore,
Figure 5 visualises the distribution of the MNLI
example weights learned at the end of training. We
observe that the minimax objective does not use the
trivial solution of setting all weights to 1 to max-
imize the learner’s loss. Conversely, the example
weights form two main clusters, at both ends of the
histogram.
6 Related Work
Distributionally Robust Optimization Train-
ing objectives for ensuring strong model perfor-
mance across all samples typically fall under
the framework of distributionally robust optimiza-
tion (DRO) (Ben-Tal et al., 2013; Duchi and
Namkoong, 2018). DRO seeks to minimize the
worst-case loss by placing emphasis on “hard” ex-
amples. Sagawa et al. (2020) extend the DRO
framework to the case where the training data
belongs to predefined groups (e.g. demographic14328
0.501.001.502.002.503.003.500.501.001.502.002.503.00
Weight
groups), and then focus on improving the worst-
group performance. Our proposed method is clos-
est to research works that assume that group an-
notations are not available during training. For in-
stance, Bao et al. (2021) develop group-specific
classifiers to discover groupings, Sohoni et al.
(2020) cluster the data, and Liu et al. (2021) pro-
pose a two-stage approach, by first training a model
for a few epochs, and then using a second model to
up-weight examples that the first model misclassi-
fied. However, these approaches require access to
a validation set with group annotations, and/or rely
on fixed example weights to determine groupings,
i.e. they do not dynamically monitor the learner’s
training dynamics.Example Weighting Our proposed training ob-
jective is also related to example weighting meth-
ods that are typically used to mitigate dataset-
related issues, such as label noise and class imbal-
ance. For example, approaches like focal loss (Lin
et al., 2017) encourage the model to focus on “hard”
instances. Conversely, in self-paced learning (Ku-
mar et al., 2010), the example weights empha-
size training using the “easy” examples first. Re-
cent works focus on learning a weighting scheme
with gradient-based (Fan et al., 2018; Raghu et al.,
2021; Wang et al., 2020) and meta-learning meth-
ods (Jiang et al., 2018; Ren et al., 2018; Shu et al.,
2019). While our proposed method also learns
example weights in a data-driven way, we do so
using a minimax training objective that places em-
phasis on up-weighting examples with patterns
that contradict the shortcuts. Finally, Zhou et al.
(2022) present a related shortcut-agnostic mitiga-
tion method by framing the task of generating an
example weight distribution as nested optimization,
where in the lower-level objective the model min-
imizes the weighted ERM loss, and on the upper-
level objective the example weights are updated
to minimize an out-distribution criterion. Our ap-
proach is different since we incorporate two models
into the training process, while Zhou et al. (2022)
use the same model in both the lower- and the
upper-level objectives.
Dataset Filtering Another line of related work
focuses on improving robustness by filtering the
training data instead of modifying the training
objective and/or the model architecture. Zellers
et al. (2018) and Zellers et al. (2019) use this ap-14329proach to mitigate the inclusion of shortcuts during
dataset creation. Sakaguchi et al. (2020) propose
AFLITE, an adversarial method for filtering exam-
ples with shortcuts. AFLITE works by training
several models over small partitions of the initial
dataset to discover “easy” examples that contain
shortcuts. Wu et al. (2022) fine-tune a PLM to
synthetically generate NLI examples, and then use
z-statistics (Gardner et al., 2021) to remove sam-
ples with shortcuts.However, dataset filtering meth-
ods may hinder in-distribution performance, due
to removing useful examples that contribute to-
wards learning the underlying task. Conversely, our
proposed minimax training objective assigns low
weights to “easy” examples instead of completely
eliminating them, thus preserving in-distribution
performance.
Generative Adversarial Networks The mini-
max objective we propose is reminiscent of the
training objective of generative adversarial net-
works (GANs) (Goodfellow et al., 2014). In NLP,
GANs are commonly used to address exposure
bias in text generation (de Masson d’Autume et al.,
2019). However, in practise, they perform worse
than simpler methods (Caccia et al., 2020). A
separate family of methods focuses on using the
training objective of GANS to improve the com-
putational efficiency of language modelling pre-
training (Clark et al., 2020b). Closer to our work,
adversarial training (Miyato et al., 2017) aims to
improve robustness in text classification, but this
method only operates at the level of word embed-
dings used in representing a single sentence, and
thus is not applicable to NLI.
7 Conclusion
In this work, we present a minimax training ob-
jective for reducing the reliance of NLI models
on shortcuts in order to improve their overall ro-
bustness without assuming prior knowledge about
the existence of specific shortcuts. Our proposed
method leverages an auxiliary model that tries to
maximize the learner’s loss by up-weighting under-
represented “hard” examples with patterns that
contradict the shortcuts present in the prevailing
“easy” examples. Experiments across three NLI
datasets demonstrate that our minimax objective
consistently improves performance on various out-
of-distribution adversarial test sets.Limitations
Since the minimax objective requires using two
separately trained models, i.e. the learner and the
auxiliary, the design of the latter plays a crucial
role in the overall stability of the training process.
In particular, while having a very capable auxil-
iary model will naturally result in a more accurate
and robust example weight distribution, it will also
potentially lead to overfitting to certain training
instances with high-losses. Another potential limi-
tation of minimax training is that the existence of
noise in the labels may cause the auxiliary to gen-
erate erroneous example weights due to high-loss
noisy instances co-existing with the “hard” exam-
ples containing meaningful patterns that contradict
the shortcuts. Furthermore, we explore shortcut
mitigation only for NLI in English, and thus our
method might not transfer to other tasks and/or
languages. Finally, the datasets we consider are
well-used and -discussed in the literature, and con-
sequently their shortcuts (and how they are adopted
by the models) are well-known. Further testing is
needed to establish whether our approach would
transfer to datasets containing different shortcuts.
Acknowledgements
The authors wish to thank Pasquale Minervini and
Tal Schuster for their helpful comments and feed-
back. Michalis Korakakis is supported by the Cam-
bridge Commonwealth, European and International
Trust and the ESRC Doctoral Training Partnership.
Andreas Vlachos is supported by the ERC grant
A VeriTeC (GA 865958).
References143301433114332143331433414335A Training Details
In this section, we detail the models and hyperpa-
rameters we use in our experiments. For all ex-
periments, the auxiliary model is optimized using
the Adam optimizer β= (0.9,0.999), ϵ= 1e−8,
with a learning rate of 1e−3. We use the Hugging-
Face implementation of BERT-base-uncased as
our learner model.
MNLI We use the following hyper-parameters
for the learner model: a learning rate of 5e−5and
a batch size of 32. The learning rate is linearly
increased for 2000 warming steps and linearly de-
creased to 0 afterward. We use an Adam optimizer
β= (0.9,0.999), ϵ= 1e−8, and add a weight
decay of 0.1.
FEVER We use the following hyper-parameters
for the learner model: a learning rate of 2e−5, and
a batch size of 32. The learning rate is linearly
increased for 1500 warming steps and linearly de-
creased to 0 afterward. We use an Adam optimizer
β= (0.9,0.999), ϵ= 1e−8, and add a weight
decay of 0.1.
QQP We use the following hyper-parameters for
the learner model: a learning rate of 5e−5, and
a batch size of 32. The learning rate is linearly
increased for 1000 warming steps and linearly de-
creased to 0 afterward. We use an Adam optimizer
β= (0.9,0.999), ϵ= 1e−8, and add a weight
decay of 0.1.
B Additional Experimental Results
Domains ERM Minimax Inv. Minimax
ADD1 86.54 87.25 57.48
DPR 49.92 50.16 38.13
SPR 58.71 61.86 39.63
FN+ 53.98 54.23 37.52
SCITAIL 70.14 75.19 54.68
GLUE 55.62 55.38 41.74
SNLI-hard 81.07 81.81 59.40
Inverse Minimax - Out of Domain Generaliza-
tion We train the inverse minimax model (whichis incentivized to up-weight the “easy” examples
with shortcuts) on SNLI, and evaluate performance
on several out-of-distribution test sets. From the
results in Table 7 we observe that the out-of-
distribution performance of the inverse minimax
model is considerably worse compared to the ERM-
trained baseline and the model trained using the
proposed minimax objective.
Additional Results for MNLI In Table 8,
we show the performance of our method
on MNLI-matched (MNLI-m) and MNLI-
mismatched (MNLI-mm), and their corresponding
hard sets.
Additional Results for HANS Table 9 shows
detailed accuracy scores on the three shortcut cate-
gories of HANS. Overall, compared to the ERM-
trained BERT-base model minimax training retains
satisfactory performance in the entailment class,
and provides considerable improvements for non-
entailment. Specifically, on the Lexical Overlap,
Constituent, and Subsequence shortcut categories,
the decrease in accuracy in entailment for minimax
training compared to the ERM-trained BERT-base
model is 7.1, 1.6, and 2.5, while for non-entailment
performance improves by 20.2, 9.4, and 36.7, re-
spectively.1433614337ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.14338/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.14339