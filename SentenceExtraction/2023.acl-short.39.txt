
Ujan Deb
IIT Bhilai
ujand@iitbhilai.ac.inRidayesh Parab
IIT Bombay
ridayesh@gmail.comPreethi Jyothi
IIT Bombay
pjyothi@cse.iitb.ac.in
Abstract
Adapters have emerged as a parameter-efficient
Transformer-based framework for cross-lingual
transfer by inserting lightweight language-
specific modules (language adapters) and task-
specific modules (task adapters) within pre-
trained multilingual models. Zero-shot transfer
is enabled by pairing the language adapter in
the target language with an appropriate task
adapter in a source language. If our target lan-
guages are known apriori, we explore how zero-
shot transfer can be further improved within the
adapter framework by utilizing unlabeled text
during task-specific finetuning. We construct
language-specific subspaces using standard lin-
ear algebra constructs and selectively project
source-language representations into the target
language subspace during task-specific finetun-
ing using two schemes. Our experiments on
three cross-lingual tasks, Named Entity Recog-
nition (NER), Question Answering (QA) and
Natural Language Inference (NLI) yield con-
sistent benefits compared to adapter baselines
over a wide variety of target languages with
up to 11% relative improvement in NER, 2%
relative improvement in QA and 5%relative
improvement in NLI.
1 Introduction
Zero-shot cross-lingual transfer refers to the trans-
fer of task-specific knowledge from a (high-
resource) source language to a (zero-resource) tar-
get language that has no labeled task-specific data
for training. A popular paradigm for cross-lingual
transfer learning is to finetune pretrained multilin-
gual models using labeled task-specific data in the
source language and directly evaluate these fine-
tuned models on target language test sets.
A parameter-efficient alternative to full finetun-
ing for cross-lingual transfer is MAD-X (Pfeiffer
et al., 2020b), an adapter-based framework thatscaffolds on multilingual pretrained models to com-
bine task-specific and language-specific modules in
a plug-and-play manner. Adapters (Houlsby et al.,
2019) are feedforward layer blocks inserted within
each Transformer layer to selectively learn task-
specific and language-specific capabilities via task
adapters and language adapters, respectively. Lan-
guage adapters are trained using self-supervised
objectives like masked language modeling (MLM)
and task adapters are trained using task-specific
objectives. To enable task transfer to a target lan-
guage, the relevant language and task adapters are
combined at test-time.
In the zero-shot setting, we assume access to
unlabeled text in the target languages. In MAD-
X, this text is only used to train target language
adapters and not further used during finetuning.
Given knowledge of which languages we want to
target, can we make effective use of unlabeled text
in the target languages even during task-specific
finetuning? This is the main question we tackle in
this work.
We propose a general adapter-based technique
to inject target language bias into task-specific fine-
tuning. Using the unlabeled text in each target lan-
guage, we construct an affine subspace from con-
textualized representations for every Transformer
layer in the multilingual model. These subspaces
are defined using singular value decomposition
(SVD) and only need to be computed once per
target language. During task-specific finetuning us-
ing labeled data in the source language, we project
the source representations onto the target language
subspaces. This projection can be invoked ran-
domly using a projection probability defined as a
hyperparameter. Projections can also be triggered
depending on whether the current source represen-
tations are closer to the mean embedding of the
source language subspace compared to the mean
embedding of the target language subspace. We
investigate both these projection policies and find449that they both improve performance across multiple
tasks in multiple languages compared to state-of-
the-art adapter baselines. We also release codeto
reproduce our experiments.
2 Methodology
Adapters and MAD-X. Adapters for language
models (Houlsby et al., 2019) are bottleneck feed-
forward modules, typically inserted in each Trans-
former layer of a multilingual model before layer
normalization. Instead of finetuning the entire
model, only adapters are tuned for a specific
task. Pfeiffer et al. (2020b) extended adapter-
based fine tuning to support cross-lingual trans-
fer. Their framework called MAD-X (Multiple
Adapters for Cross-lingual transfer) comprises of
language adapters and task adapters. Language
adapters are pretrained using masked language
modeling to learn language-specific features. Task
adapters are stacked on top of language adapters
during downstream task finetuning to learn task-
specific information. To achieve zero-shot transfer,
the model is trained with a frozen source-language
language adapter and a task adapter. During test
time, the source-language adapter is replaced with
the target-language adapter and evaluated on test
instances in the target language.
Overview of our technique. We are interested
in the setting where we have apriori knowledge of
which languages we want to target at test time. We
aim to bias cross-lingual transfer towards known
target languages during task-specific finetuning.
We start with MAD-X as our underlying frame-
work and adopt the following 3-step approach:
•We construct layer-specific subspaces for each
of the target languages. This is done by computing
SVD on contextualized token representations ex-
tracted from each layer. See §2.1 for more details.
•During task-specific training, we selectively
project output representations from the language
adapter of a chosen layer onto the target language
subspace. These projections are triggered based on
two policies: Random projection ( §2.2) and Mean
Cosine Distance ( §2.3). The projected representa-
tions are further passed through the task adapter
that is trained using labeled data in the source lan-
guage.
•Similar to MAD-X, we evaluate on the target
language by simply swapping the source languageadapter with the target language adapter while keep-
ing the task adapter fixed. No projection is done
during inference.
2.1 Language Subspaces and Projections
Our objective is to bias the model towards the tar-
get language while fine-tuning for a task. For
this, we need to extract language-specific infor-
mation from model representations that jointly ex-
hibit language-specific and language-independent
properties. Language-specific subspaces have been
typically used to analyze representations in mul-
tilingual language models. Choenni and Shutova
(2020) showed that individual representations can
be used to predict linguistic typological features
after projecting onto language-sensitive subspaces.
Chang et al. (2022) construct language subspaces
with SVD using language-specific contextualized
token embeddings. They analyze model perfor-
mance and other properties after computing layer-
wise projections of representations to various lan-
guage subspaces.
We construct subspaces for each of the target lan-
guages using SVD and contextualized token repre-
sentations for unlabeled text in the target language.
Consider a pretrained model like XLMR (Conneau
et al., 2020) that takes text sequences from the
target language as its input. d-dimensional embed-
dings from a particular layer for a given language
Acan be grouped into a matrix M∈R. SVD
ofM(after subtracting the mean representation
forA) can be written as: M=UΣV. The
right singular matrix Vis considered to be the
subspace for language A. These subspaces only
need to be computed once for each layer. Next, we
look at when projections should be invoked.
2.2 Random Projection
For a given target language, during finetuning us-
ing task-specific data in the source language, we
project the source representations onto the target
language subspace with a predetermined proba-
bility p. This projection is invoked right before
passing the representation through the task adapter,
having already passed through the language adapter.
To project onto a target subspace, we first shift the
target language subspace so that it passes through
the source language mean embedding and then
take the projection onto the target subspace (Chang
et al., 2022). Let Sbe the source language and Q
be the target language. Let subspaces and means of
representations from one of the Transformer layers450for the source language be Vandµ, respectively.
Projection of a representation xonSis given by:
Project(x) =VV(x−µ) +µ
The projection of xonto the target language sub-
space, that is shifted onto the source subspace, can
be computed as:
Project(x) =VV(x−µ) +µ
The main intuition here is that by probabilis-
tically projecting source representations onto the
target language subspace during task-specific fine-
tuning, the model can encode both source and target
language information in its representations. The
model cannot solely rely on source-language spe-
cific features during task-specific training.
2.3 Mean Cosine Distance (MCD)
We suggest another projection scheme, Mean Co-
sine Distance (MCD), that is more informed than
randomly projecting source representations based
on a projection probability p. Using MCD, we
project those embeddings that are deemed as be-
ing further away from the target language subspace
compared to the source language subspace. This is
quantified using a cosine distance between an em-
bedding from a layer and means of source and tar-
get language subspaces. If an embedding is closer
to the source language mean compared to the tar-
get language mean, we project it onto the target
language subspace so as to make it more similar to
target language embeddings. However, if an em-
bedding is closer to the target language mean, we
can possibly omit projection since it already con-
tains information relevant to the target language.
Consider a set of embeddings extracted from
one of the Transformer layers. Let the means of
all embeddings from this layer and the associated
subspace be denoted by µandV, respectively. µ
andµdenote the means for the source and target
language, respectively. Similarly, VandVrefer
to the respective subspaces. Let xdenote a token
embedding from the source language. The MCD
policy can be written as:
x=/braceleftigg
Project(x)ifc(x,µ)<c(x,µ)
x otherwise
where Project(x)is defined in Section 2.2 as
the projection of xonto the target subspace V
andc(x,y)refers to the cosine similarity between
two embeddings xandy.
Figure 1 provides an illustration of our proposed
technique within a single Transformer layer that in-
cludes language and task adapters (as in the MAD-
X framework).
3 Experimental Setup
Subspace construction. To construct language
specific subspaces, we adopt the settings used by
Chang et al. (2022). Text sequences of length 512
are taken from the OSCAR dataset (Ortiz Su’arez
et al., 2019) and passed through XLMR (Conneau
et al., 2020) to produce layer-wise contextualized
embeddings. We pick 262K contextualized rep-
resentations and subtract the representation mean
before computing SVD. For a low-dimensional sub-
space, we select the greatest ksingular values such
that their sum of squares is greater than or equal
to 90% of the total variance. (Total variance is
given by the sum of the squared singular values pro-
duced.) Finally, in order to compute the language-
specific subspaces, the corresponding right singular
vectors are taken as the basis.451NER
hi vi de id is ilo sw my jv avg
XQuAD
hi vi de avgNLI
qu gn avg
Datasets. We conduct cross-lingual transfer ex-
periments on three tasks, Named Entity Recogni-
tion (NER), Question Answering (QA) and Natu-
ral Language Inference (NLI), where the source
language is always English. For NER, we use
the WikiANN dataset (Rahimi et al., 2019), and
show results for nine languages Hindi, Vietnamese,
German, Indonesian, Icelandic, Ilocano, Swahili,
Burmese and Javanese with roughly 20K instances
in the English train set and between 1K and 10K
instances in the target dev and test sets. For QA,
we use XQuAD (Artetxe et al., 2019), a multilin-
gual extension of SQuAD (Rajpurkar et al., 2016)
and we report results for Hindi, Vietnamese and
German consisting of around 87K examples in the
English SQuAD train set and 1190 instances in the
three target dev sets. For NLI, we use the Amer-
icasNLI dataset (Ebrahimi et al., 2021) which is
an extension of the XNLI dataset (Conneau et al.,
2018) with low-resource American languages. We
report results on Quechua and Guarani, consist-
ing of 392k instances in the English train set and
2490 and 5010 instances in the dev and test sets,
respectively for each target language.
Training setup. We use transformer models from
the adapter-transformersfork of the HuggingFace
transformers library (Wolf et al., 2020). We use
pre-trained language adapters from AdapterHub
(Pfeiffer et al., 2020a) for our transfer experiments.
XQuAD and NLI fine-tuning experiments were
conducted on a single NVIDIA A100 80 GB gpu
for 15 epochs and 10 epochs, with learning rate
1e-4 and batch size 16. NER experiments were runfor 30 epochs on Nvidia 1080 Ti with 12 GB ram.
Further details can be found in Appendix A.
4 Results
NER, XQuAD and NLI results are shown in Ta-
ble 1, Table 2 and Table 3 respectively. All values
correspond to F1 scores averaged over 3 different
seeds. We use the target language validation set to
choose the best hyperparameter values for all exper-
iments. Both MCD and random projections show
consistent improvement over the MAD-X baseline
numbers. With MCD, we explicitly instruct the
model when to project. This removes a hyperpa-
rameter from the setup, compared to random pro-
jections, while maintaining consistent performance
gains over the baseline. To further analyze MCD,
we consider the fraction of embeddings being pro-
jected onto the target language subspace for NER.
Table 4 shows the fraction of embeddings projected
during training (averaged across all layers) for each
language. For languages dissimilar to en (such as hi
and id), it makes sense that the projection fractions452
hi vi de id is
Proj. Frac.
are high since the language subspace means are
closer to the source language mean (Chang et al.,
2022), compared to languages more similar to en
like de and is. Figure 2 shows how projection frac-
tions vary across layers averaged across training
epochs. We see high projection rates in early and
final layers across languages. This correlates with
these layers encoding a lot of English-specific in-
formation (Rogers et al., 2020) via training on the
task-specific English data, thus triggering projec-
tions via MCD often.
5 Related Work
Multilingual language models like mBERT (De-
vlin, 2018), XLM-R (Conneau et al., 2020) pos-
sess some zero-shot cross-lingual capabilities, even
without any explicit finetuning on the languages of
interest (Wu and Dredze, 2019; Pires et al., 2019).
Such transfer without any finetuning could lead
to degradation in performance across certain lan-
guage pairs (Hu et al., 2020). Nevertheless, multi-
lingual models are a good foundation to bootstrap
and further develop cross-lingual generalization.
While there is a rapidly growing body of work
on cross-lingual transfer, very few approaches uti-
lize language-specific subspaces for this purpose.
Both Choenni and Shutova (2020) and Chang et al.
(2022) construct language-specific subspaces in
multilingual models for an exploratory analysis of
the model’s representations. Yang et al. (2021)
use projections on language specific subspaces to
remove language specific information from the rep-
resentations. We note such removal of language
bias did not perform well on cross-lingual trans-
fer in our experiments. Parovi ´c et al. (2022) train
bilingual language adapters using both source and
target language text before task adapter training.
However, this requires training language adapters
using both source and target language unlabelled
text, for every language pair, in addition to training
task adapters. In contrast, our setup is a simple
architectural extension of MAD-X, requiring no ad-
ditional training once the subspaces are computed
for each language. To the best of our knowledge,
ours is the first work to exploit language-specific
subspaces for cross-lingual transfer.6 Conclusions
In this work, we present a new adapter-based cross-
lingual transfer technique for an apriori known
set of target languages. We construct language
subspaces using contextualized representations for
source and target languages. Representations dur-
ing task-specific training are projected onto the tar-
get subspace if they exceed a probability threshold
or if they are closer to a mean source embedding.
Both schemes consistently improve zero-shot trans-
fer for three natural language understanding tasks
across many languages.
Acknowledgements
The first author (Ujan) was supported by the Uplink
Internship Program of the India Chapter of ACM
SIGKDD. The authors are thankful to the anony-
mous reviewers for their constructive suggestions
that helped improve this submission.
Limitations
While our proposed projection techniques often
improve cross-lingual transfer, the choice of the
projection layer and the projection probability in
the case of random projection are hyperparameters
that vary across tasks and languages. Our ongoing
work involves identifying a mechanism via which
we can parameterize these quantities, enabling the
model to directly learn the optimal layer and prob-
ability values for projection.
References453
A Implementation Details
We use the xlm-roberta-base model from Hugging-
Face Transformers (Wolf et al., 2020) pretrained
on 2.5 TB of CommonCrawl data, for all our ex-
periments. NLI and XQuAD experiments were
conducted on a single NVIDIA A100 GPU (80 GB454
NER XQuAD NLI
hi vi de id is sw ilo jv my hi vi de qu gn
NER XQuAD NLI
hi vi de id is sw ilo jv my hi vi de qu gn
RAM) and the NER experiments ran on a single
Nvidia 1080Ti GPU (12 GB RAM). We used a
learning rate of 1e-4 with a batch size of 16. The
hyperparameter choices for layers and probabilities
for our experiments are given in Tables 5 and 6,
respectively.
All datasets used are taken from HuggingFace
Datasets (Lhoest et al., 2020). For evaluating mod-
els, we use the HuggingFace Evaluate libraryas
well as the seqeval python package455ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
7
/squareA2. Did you discuss any potential risks of your work?
No potential risks
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
3
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
3, appendix456/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
3, appendix
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
appendix
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.457