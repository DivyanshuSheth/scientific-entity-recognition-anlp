
Giwon HongJeonghwan KimJunmo KangSung-Hyon MyaengSchool of Computing, KAISTSchool of Interactive Computing, Georgia Institute of Technology
{giwon.hong, jeonghwankim123, myaeng}@kaist.ac.kr
junmo.kang@gatech.edu
Abstract
A graph is a suitable data structure to repre-
sent the structural information of text. Re-
cently, multi-hop question answering (MHQA)
tasks, which require inter-paragraph/sentence
linkages, have come to exploit such properties
of a graph. Previous approaches to MHQA re-
lied on leveraging the graph information along
with the pre-trained language model (PLM) en-
coders. However, this trend exhibits the follow-
ing drawbacks: (i) sample inefficiency while
training in a low-resource setting; (ii) lack of
reusability due to changes in the model struc-
ture or input. Our work proposes the Graph-
Induced Transformer (GIT) that applies graph-
derived attention patterns directly into a PLM,
without the need to employ external graph mod-
ules. GIT can leverage the useful inductive
bias of graphs while retaining the unperturbed
Transformer structure and parameters. Our ex-
periments on HotpotQA successfully demon-
strate both the sample efficient characteristic
of GIT and its capacity to replace the graph
modules while preserving model performance.
1 Introduction
Graphs are a widely employed data structure
from fake news detection in social media (Monti
et al., 2019) to molecular graph generation in bio-
medicine (Jin et al., 2018). More recently, graphs
have come to manifest themselves in many subdo-
mains of natural language processing (NLP) for
its topological properties that provide useful con-
nectivity inductive bias for model reasoning. Par-
ticularly, some models in the multi-hop question
answering (MHQA) tasks like HotpotQA (Yang
et al., 2018) and MuSiQue (Trivedi et al., 2022) that
require textual reasoning through multiple para-
graphs (i.e., multi-hop) make use of such graph
structures to model the internal structural infor-
mation within the given document. The external
graphs used in these studies have demonstrated theFigure 1: Existing graph-based MHQA model vs.
Graph-Induced Transformer (GIT).
effectiveness of harnessing the information from
textual units such as paragraphs and sentences to
perform multi-hop reasoning in the QA setting.
The graphs used in MHQA models like HGN
(Fang et al., 2020), SAE (Tu et al., 2020) and
DFGN (Qiu et al., 2019) are typically placed on
top of the pre-trained language models (PLM) like
BERT (Devlin et al., 2019) and RoBERTa (Liu
et al., 2019). These graphs use graph neural net-
work’s (GNN) message propagation mechanism to
update the node and edge parameters, learning to
extract useful inductive, structural bias from the
graph. These features are then fused with the con-
textualized representations from the PLM to lever-
age the information derived from this connectivity
(e.g., paragraph-to-sentence) of these graphs. De-
spite the merits of these graphs, such graph mod-
ules require numerous samples (i.e., sample inef-
ficiency ) to acquire the necessary inductive bias,
increasing the overall cost in training. This sig-
nificantly hinders the model’s use of graph con-
nectivity information, especially in a low-resource
scenario.
To address the sample inefficient training while
preserving the structural information of graphs,10288
we propose the Graph-Induced Transformer (GIT).
GIT is a simple, intuitive approach to leverage
graphs in MHQA tasks without changing the PLM
architecture or using additional graph modules. By
injecting attention patterns derived from text-based
graphs, GIT is able to take advantage of the graphs
without the extra modules to fuse graph represen-
tations with the text (Figure 1). This enables GIT
to: (i) conduct sample-efficient training under a
low-resource setting, and (ii) successfully replace
the graph module while preserving performance.
We conduct experiments to prove the advantages of
using GIT instead of separate graphs and provide a
range of empirical evidence to show the effective-
ness of GIT on the above two aspects.
2 Related Works
MHQA models that leverage the useful connectiv-
ity inductive bias of graphs have shown substantial
performance gains. With DFGN (Qiu et al., 2019)
using entity node graphs, SAE (Tu et al., 2020)
using sentence nodes and HGN (Fang et al., 2020)
leveraging the hierarchical, paragraph-sentence-
entity nodes for its graph, the use of instance-
specific graphs has led the development of graph-
based MHQA models. Nevertheless, this has led to
sample inefficient training that prevents the graph
modules from sufficiently acquiring the ability to
leverage the interconnected node and edge infor-
mation, especially in low-resource settings.
Meanwhile, some works have tried to modify
the Transformer architecture for graph modeling.
Graph-BERT (Zhang et al., 2020) added node-
specific positional embeddings and modified the
encoder to solve the suspended animation problem
of graph neural networks (GNN). Other studies
incorporated heterogeneous graphs by convertingthem into meta-paths (Yun et al., 2019) or using
relative temporal positional encodings (Hu et al.,
2020) in Transformers. Their modified architec-
tures, however, are specific to graph tasks only,
preventing their direct use in the MHQA tasks (and
other downstream NLP tasks in general) because
they preclude the use of text as input and do not
reuse the pre-trained parameters like those of BERT.
GIT, in contrast, is designed to handle the MHQA
tasks by integrating these graphs into PLMs while
preserving the PLM architecture for generalization.
3 Graph-Induced Transformer
The core idea behind GIT is to represent the node
connections in the form of masked self-attention
layers. Since the attention mask is an inherent part
of the Transformer architecture, we can simply alter
the attention mask pattern to inject the connective
inductive bias of graphs. The major benefit GIT
brings with its graph injected as attention masks
is the natural and explicit integration of its induc-
tive bias without the need to learn about the graph
structure, thereby ensuring a sample efficient uti-
lization of the graph information during training.
The graph-derived attention masks regulate the in-
formation flow among the intermediate representa-
tions, emulating the inter-node connections within
a graph (Figure 2).
In GIT, the nodes of the graph are represented
as a group (or chunks) of tokens that belong to
a homogeneous textual unit; if a group of tokens
constitute a sentence, then the group as a whole cor-
responds to a single sentence node. Edges of the
graph are represented as an attention connection
between these two groups of tokens. The dynam-
ically extracted attention mask selectively blocks
the connections in a self-attention layer, except for10289Algorithm 1 Graph-Induced Transformer
Input: Context C, Attention Mask M, Graph G
Number of Transformer layers N, Number of
GNN layers N, Number of Edge types N
Output: Final Representation HM←copy _and_zeros (M)foreach edge e = ( i, j)∈Gdo t←edge _type(e) ▷t∈ {1, ..., N} M[t, i, j]←1end forM←concat (M[:N+1], M[N+1 :])H←embeddings (C)forn=1, ..., N−1do ifn < N−Nthen H←layer(H, M) else H←layer(H, M) end ifend forH←layer(H, M)return H
the connections to the words that belong to the
nodes that constitute the edge.
In the above equation, H∈Rdenotes the hid-
den state of the i-th token at the n-th layer, where
n < N−N(refer to Algorithm 1 line 9), Q,
KandV∈Reach denote query, key and
value projection matrix, respectively. O∈R
is the projection matrix for the intermediate embed-
dings concatenated over Nheads, where the
k-th head is assigned an attention mask Mwhich
enforces the connection between one textual unit
(e.g., paragraph) to another (e.g., sentence) by ze-
roing out the other attentions and maintaining the
remaining attention coefficients for the connection.
Since there are multiple edge types that connect
one node type to another, we define an edge type as
a pair of head and tail node types (e.g., paragraph-
to-sentence), in addition to the edge types of the
original graph. The different edge types are then
applied to the Transformer encoder as a set of atten-
tion masks by assigning a head per edge type; this is
based on the previous studies (Rogers et al., 2020;Jo and Myaeng, 2020) that each head in a Trans-
former learns different linguistic concept. Further-
more, considering that nodes in the graph are up-
dated through ldifferent steps given lGNN layers,
we allocate llayers inside the PLM to emulate the
message propagation in GNNs. To promptly adjust
to different end tasks, we keep the last layer as an
original fully connected self-attention layer.
4 Experiments
4.1 Settings
Dataset We use HotpotQA(Yang et al., 2018),
an English multi-hop question answering (MHQA)
dataset for experiments. HotpotQA contains
90,564 training instances and 7,405 for develop-
ment and test. All the evaluations are conducted
using the development set (Distractor setting).
Models We use the following graph-based
MHQA models as our baselines for which the
codes are publicly available. For simplification,
we denote question (Q), paragraph (P), sentence
(S), and entity (E) accordingly (e.g., the edge con-
necting a paragraph to sentence is a P2S type).
DFGN (Qiu et al., 2019) uses an entity graph
that connects an entity to another in the text (E2E).
SAE (Tu et al., 2020) uses a sentence graph
that connects a sentence node with other sentence
nodes (S2S) with 3 different edge types: (i) within
document, (ii) across documents and (iii) when
both sentences share an entity with the question.
HGN (Fang et al., 2020) uses a hierarchical
graph with question, paragraph, sentence and entity
nodes, with 8 different edge types: 1) Q2P, 2) Q2E,
3) P2P, 4) P2S, 5) S2P (hyperlink), 6) S2S, 7) S2E,
and 8) a self-loop
Implementation We have retrained the models
in the original papers in our environment for a fair
comparison against GIT. For the details on imple-
mentation refer to Appendix A.
4.2 Graph Replacement with GIT
In this section, we first analyze whether GIT can
be applied to different baselines and replace their
graphs. Table 1 shows the result of comparing
baselines under two different settings: "Graph"
refers to using the original graph-based model and
"GIT" refers to removing the graph and applying
GIT.10290Ans EM Ans F1 Sup EM Sup F1 Joint EM Joint F1
DFGNGraph 55.40 69.13 49.26 80.25 31.56 58.27
GIT 55.91 70.01 49.17 80.77 31.75 59.17
SAEGraph 66.68 80.06 60.35 86.50 43.55 71.45
GIT 66.32 79.92 60.93 86.49 43.59 71.31
HGNGraph 68.08 81.63 63.34 88.45 46.19 73.70
GIT 67.91 81.61 62.11 88.33 45.31 73.61
SAE Joint EM Joint F1
Graph 43.55 71.45
w/o Graph 42.39 70.66
GIT 43.59 71.31
Graph + GIT 43.38 71.12
The results in Table 1 show that GIT achieves
performance that is on par with those of the base-
lines. This indicates that GIT is generalizable to
various graphs with different node and edge types,
and effectively replaces the external graph mod-
ules.
To evaluate whether the proposed GIT actually
emulates and compensates for the role of graphs,
we conduct a more thorough study as in Table 2.
Here, we seek to address the equivalence of GIT to
graphs from the information retention perspective.
For "w/o Graph," we ascertain that removing the
graph decreases model performance as the authors
of SAE claim. By applying GIT, this loss of perfor-
mance is successfully restored, proving the equiva-
lence of GIT and graphs in retaining information.
In addition, "Graph + GIT" does not outperform
the original setting, which implies that GIT and
graphs introduce features, while informative, of the
same nature to the model.
4.3 Low-resource Scenario
To test the sample efficiency of GIT in training, we
designed a low-resource scenario that uses only 1%,
2%, 5%, 10% and 50% of the HotpotQA training
dataset. Table 3 compares the results of SAE, a
MHQA graph model and its GIT counterpart; theData Portion SAE GIT
1% 9.57 15.68 (↑63.8%)
2% 17.79 24.88 (↑39.8%)
5% 28.05 29.68 (↑5.8%)
10% 31.41 33.02 (↑5.1%)
50% 40.61 41.67 (↑2.6%)
percentage of improvement is denoted in red.
The results in Table 3 demonstrate significant
performance gains by replacing the graphs with
GIT layers. The main takeaway here is that, unlike
the previous graph-based models, the GIT takes
the GNN module out of the picture, effectively
eliminating the need to train its additional param-
eters while taking the full advantages of already
pre-trained Transformer. We can clearly see that
GIT outperforms the graph-based model when the
data is scarce (i.e. low-resource), especially in the
1% (∼900 instances) case GIT improves the model
performance by 63.8% compared to its original set-
ting. Similar improvements are evidenced in 2%,
5%, 10%, and 50% cases. This result provides a
conclusive evidence that GIT is very effective when
dataset is scarce. On top of the GIT’s robustness
in data scarcity, it is apparent that even in the 50%
setting where the number of instances exceeds 45K,
GIT outperforms the graph-based MHQA model
by a substantial margin.
5 Conclusions
This work presented GIT, the graph-induced Trans-
former that drastically improved MHQA models’
sample efficiency and replaces graphs in the models
while retaining their performance. Our empirical
evidences demonstrated that models can enjoy the10291benefits of connective inductive bias of graphs with-
out additional graph modules in place. The design
of GIT also allowed us to reuse the parameters of
PLM while incorporating the graph information.
Future directions of our work may include using
GIT in downstream NLP applications where the
graph inductive bias is necessary and dataset is
scarce.
Limitations
Our proposed method, GIT, allocates each edge
type of the graph to the attention head of the Trans-
former. This means that if the number of edge types
exceeds that of attention heads, it is difficult to ap-
ply the GIT scheme. This may be problematic in
a situation 1) where the number of attention heads
of the Transformer is very small, for example, in
the tiny/mini/small setting suggested by Turc et al.
(2019) or 2) when we have a complex graph which
consists of numerous types of nodes and edges.
Also, we tested GIT only with MHQA, a QA task
where graphs are proven to be useful. In practice,
however, it should be generally applicable when
text and its connectivity are important.
Acknowledgements
This work was supported by Institute for Informa-
tion & communications Technology Planning &
Evaluation(IITP) grant funded by the Korea govern-
ment(MSIT) (No. 2013-2-00131, Development of
Knowledge Evolutionary WiseQA Platform Tech-
nology for Human Knowledge Augmented Ser-
vices) and (No. 2022-0-00369, (Part 4) Develop-
ment of AI Technology to support Expert Decision-
making that can Explain the Reasons/Grounds for
Judgment Results based on Expert Knowledge)
References10292
A Experiment Setting Details
We abide by the official HotpotQA evaluation met-
rics and the script provided.
Unless otherwise specified, we have used the
original hyperparameters of those three models.
DFGN (Qiu et al., 2019) We have changed batch
size (32 -> 8) and gradient accumulation step (1-
>4). For the encoder, BERT (Devlin et al., 2019)
pre-trained model ( bert-base-uncased ) which con-
tains 110M parameters, same as the original code.
Also, since there is no explicit query node in DFGN,
we generated connections between the query tokens
and the other nodes in the GIT layers.
SAE (Tu et al., 2020) As the published SAE
code is incomplete on the training side, we had to
implement some parts of the code on our own. We
have changed batch size (2->4) and gradient accu-
mulation step (1->8). Also we used Roberta (Liu
et al., 2019) model which contains 355M parame-
ters ( RoBERTa-large ) pre-trained on the SQuAD
dataset (Rajpurkar et al., 2018), similar to the orig-
inal code (As we was not able to get the pre-trained
model that the authors used). Also, since there is
no explicit query node in SAE, we generated con-
nections between the query tokens and the other
nodes in the GIT layers.
HGN (Fang et al., 2020) We have changed
batch size (8->4), gradient accumulation step (1-
>2), and "fp16" option (True -> False). Also,we used RoBERTa pre-trained model ( RoBERTa-
large ) which contains 355M parameters, same as
the original code.
B Computing Infrastructure
Our workstation contains two NVIDIA GeForce
RTX 3090s, AMD Ryzen Threadripper 3960X 24-
Core Processor, and 128GB RAM. Each of the
baseline used in this work was trained using a sin-
gle GeForce RTX 3090, except for DFGN which
requires two GPUs.
C Layer-wise Probing for GIT
To probe the layer-wise applicability of the atten-
tion masks constructed with our GIT scheme, we
probe chunks of Transformer layers within the
PLM (RoBERTa-large) and evaluate their perfor-
mance on HotpotQA development set in the distrac-
tor setting. Our results suggest that the preserva-
tion of lower and intermediate layers is important
in building a set of contextualized representations
that are rich in semantic and syntactic information,
while applying the attention masks in the upper-
most layers prove to be most effective. This im-
plies that the last layers can be harmlessly altered
to embrace the purposefully injected inductive bias.
To determine whether additional layers that in-
corporate our GIT masks positively affect the PLM
when scaling their parameters, we also experiment
with the additional layers setting. By comparing
the additional layers with and without GIT masks,
we observe that the PLM with extra layers that in-
clude the GIT masks outperform the vanilla extra
layers. This hints at the feasibility of scaling up the
PLMs with positive graph inductive bias.10293GIT Joint EM Joint F1
Layer 1-24 42.75 70.70
Layer 1-3 42.47 70.72
Layer 8-10 43.07 71.21
Layer 15-17 43.04 70.76
Layer 22-24 43.36 71.01
Layer 21-23 43.59 71.31
GIT Additional Layers
Layer 25-27 (No GIT) 43.13 71.07
Layer 25-27 44.09 71.6410294