
Neeraj Varshney and Chitta Baral
Arizona State University
Abstract
Despite remarkable progress made in natural
language processing, even the state-of-the-art
models often make incorrect predictions. Such
predictions hamper the reliability of systems
and limit their widespread adoption in real-
world applications. Selective prediction partly
addresses the above concern by enabling mod-
els to abstain from answering when their pre-
dictions are likely to be incorrect. While se-
lective prediction is advantageous, it leaves us
with a pertinent question ‘ what to do after ab-
stention ’. To this end, we present an explo-
rative study on ‘Post-Abstention’, a task that al-
lows re-attempting the abstained instances with
the aim of increasing coverage of the system
without significantly sacrificing its accuracy .
We first provide mathematical formulation of
this task and then explore several methods to
solve it. Comprehensive experiments on 11
QA datasets show that these methods lead to
considerable risk improvements –performance
metric of the Post-Abstention task– both in the
in-domain and the out-of-domain settings. We
also conduct a thorough analysis of these re-
sults which further leads to several interesting
findings. Finally, we believe that our work will
encourage and facilitate further research in this
important area of addressing the reliability of
NLP systems.
1 Introduction
Despite remarkable progress made in Natural Lan-
guage Processing (NLP), even the state-of-the-art
systems often make incorrect predictions. This
problem becomes worse when the inputs tend to
diverge from the training data distribution (Elsa-
har and Gallé, 2019; Miller et al., 2020; Koh et al.,
2021). Incorrect predictions hamper the reliability
of systems and limit their widespread adoption in
real-world applications.
Selective prediction partly addresses the above
concern by enabling models to abstain from answer-
ing when their predictions are likely to be incorrect.By avoiding potentially incorrect predictions, it
allows maintaining high task accuracy and thus
improves the system’s reliability. Selective predic-
tion has recently received considerable attention
from the NLP community leading to development
of several methods (Kamath et al., 2020; Garg and
Moschitti, 2021; Xin et al., 2021; Varshney et al.,
2022d). While these contributions are important,
selective prediction leaves us with a pertinent ques-
tion: what to do after abstention ?
In this work, we address the above question and
present an explorative study on ‘ Post-Abstention ’,
a task that allows re-attempting the abstained in-
stances with the aim of increasing coverage of the
given selective prediction system without signifi-
cantly sacrificing its accuracy . Figure 1 illustrates
the benefit of employing a post-abstention method;
a model that achieves an accuracy of 70% is first
enabled with the selective prediction ability that
increases the accuracy to 85% but answers only
71% instances. Then, a post-abstention method is
employed (for the 29% abstained instances) that
assists the system in answering 9%more instances
raising the coverage to 80% without considerably
dropping the overall accuracy. We note that this
task allows re-attempting all the abstained instances
but does not require the system to necessarily out-
put predictions for all of them i.e. the system
can abstain even after utilizing a post-abstention
method (when it is not sufficiently confident even
in its new prediction). This facet not only allows
the system to maintain its performance but also
provides opportunities of sequentially applying
stronger post-abstention methods to reliably and
optimally increase the coverage in stages.
We provide mathematical formulation of the
post-abstention task and explore several baseline
methods to solve it (Section 2). To evaluate the effi-
cacy of these methods, we conduct comprehensive
experiments with 11Question-Answering datasets
from MRQA shared task (Fisch et al., 2019) in967
both in-domain and out-of-domain settings (Sec-
tion 3). Our post-abstention methods lead to overall
risk improvements (performance metric of the pro-
posed task) of up to 21.81in the in-domain setting
and24.23in the out-of-domain setting. To further
analyze these results, we study several research
questions, such as ‘what is the extent of overlap
between the instances answered by different post-
abstention methods’, ‘what is the distribution of
model’s original confidence on instances that get
answered in the post-abstention stage’, and ‘how
often do the system’s predictions change after ap-
plying post-abstention methods’. In Section 4, we
show that these investigations lead to numerous
important and interesting findings.
In summary, our contributions are as follows:
1.We present an explorative study on ‘Post-
Abstention’ , a task that aims at increasing the
coverage of a given selective prediction system
without significantly sacrificing its accuracy .
2.Weexplore several baseline post-abstention
methods and evaluate them in an extensive ex-
perimental setup spanning 11QA datasets in
both in-domain and out-of-domain settings.
3.We show that the proposed post-abstention meth-
odsresult in overall risk value improvements
of up to 21.81and24.23in the in-domain and
out-of-domain settings respectively.
4.Ourthorough analysis leads to several interest-
ing findings, such as (a) instances answered by
different post-abstention methods are not mu-
tually exclusive i.e. there exist some overlap-
ping instances, (b) instances that get answered
in the post-abstention stage are not necessarily
the ones on which the given system was initiallymost confident, etc.
We believe our work will encourage further re-
search in Post-Abstention, an important step to-
wards improving the reliability of NLP systems.
2 Post-Abstention
In this section, we first provide background for
post-abstention (2.1) and then describe the task
(2.2) and its approaches (2.3).
2.1 Background
Post-abstention, as the name suggests, is applicable
for a system that abstains from answering i.e. a
selective prediction system. A system can typically
abstain when its prediction is likely to be incorrect.
This improves the reliability of the system. Such a
system typically consists of two functions: a pre-
dictor ( f) that gives the model’s prediction on an
input ( x) and a selector ( g) that determines if the
system should output the prediction made by f:
(f, g)(x) =/braceleftigg
f(x), if g(x) = 1
Abstain, if g(x) = 0
Typically, gcomprises of a prediction confidence
estimator ˜gand a threshold ththat controls the
level of abstention for the system:
g(x) =1[˜g(x))> th]
A selective prediction system makes trade-offs be-
tween coverage andrisk. Coverage at a threshold
this defined as the fraction of total instances an-
swered by the system (where ˜g > th ) and risk is
the error on the answered instances.
With decrease in threshold, coverage will in-
crease, but the risk will usually also increase. The968overall selective prediction performance is mea-
sured by the area under Risk-Coverage curve (El-
Yaniv et al., 2010) which plots risk against cover-
age for all confidence thresholds. Lower AUC is
better as it represents lower average risk across all
confidence thresholds.
In NLP, approaches such as Monte-Carlo
Dropout (Gal and Ghahramani, 2016), Calibra-
tion (Kamath et al., 2020; Varshney et al., 2022c,d;
Zhang et al., 2021), Error Regularization (Xin et al.,
2021) and Label Smoothing (Szegedy et al., 2016)
have been studied for selective prediction. In this
work, we consider MaxProb (Hendrycks and Gim-
pel, 2017), a technique that uses the maximum
softmax probability across all answer candidates
as the confidence estimator. We use this simple
technique because the focus of this work is on post-
abstention i.e. the next step of selective prediction.
However, we note that the task formulation and the
proposed methods are general and applicable to all
selective prediction approaches.
2.2 Task Formulation
We define the post-abstention task as follows:
Given a selective prediction system with an
abstention threshold, the post-abstention task
allows re-attempting the abstained instances
with the aim of improving the coverage with-
out considerably degrading the accuracy (or
increasing the risk) of the given system.
Next, we mathematically describe the task and its
performance evaluation methodology.
Let the coverage and risk of the given selective
prediction system at abstention threshold thbe
covandriskrespectively. A post-abstention
method re-attempts the originally abstained in-
stances (where ˜g < th ) and outputs the new pre-
diction for the ones where it is now sufficiently
confident. This typically leads to an increase in the
coverage of the system with some change in the
risk value; let the new coverage and risk be cov
andriskrespectively. From the risk-coverage
curve of the given system, we calculate its risk
at coverage covand compare it with riskto
measure the efficacy of the post-abstention method
(refer to Figure 2).
For a method to have a positive impact , its
risk (risk) should be lower than the risk of the
given system at coverage cov. We summarize
this performance evaluation methodology in Figure
2. To get an overall performance estimate of a post-
abstention method, we compile these differences
in risk values for all confidence thresholds and cal-
culate an aggregated value. The higher the overall
improvement value, the more effective the method
is. We note that this evaluation methodology is fair
and accurate as it conducts pair-wise comparisons
atequal coverage points. An alternative perfor-
mance metric could be AUC but it computes the
overall area ignoring the pair-wise comparisons
which are crucial for our task because the cover-
age points of the original system would be different
from those achieved by the post-abstention method.
2.3 Approaches
2.3.1 Ensembling using Question Paraphrases
It is well known that even state-of-the-art NLP
models are often brittle i.e. when small semantic-
preserving changes are made to the input, their
predictions tend to fluctuate greatly (Jia and Liang,
2017; Belinkov and Bisk, 2018; Iyyer et al., 2018;
Ribeiro et al., 2018; Wallace et al., 2019). En-
sembling the predictions of the model on multiple
semantically equivalent variants of the input is a
promising approach to address this issue (Anantha
et al., 2021; Vakulenko et al., 2021) as it can reduce
the spread or dispersion of the predictions.969
We leverage the above technique in re-
attempting the abstained questions i.e. we first
generate multiple paraphrases of the input instance
and then aggregate the model’s predictions on them.
We use BART-large (Lewis et al., 2019) model
fine-tuned on Quora Question Corpus (Iyer et al.,
2017), PAWS (Zhang et al., 2019), and Microsoft
Research Paraphrase Corpus (Dolan and Brockett,
2005) for paraphrasing and explore the following
strategies for aggregating the model predictions:
•Mean : In this strategy, we calculate the average
confidence assigned to each answer candidate
across all predictions. Then, we select the candi-
date with the highest average confidence as the
system’s prediction. Note that the system will
output this prediction only if its confidence sur-
passes the abstention threshold.
•Max : Here, like the mean strategy, we select
the answer candidate with the highest average
confidence but we use the maximum confidence
assigned to that candidate as its prediction con-
fidence. This is done to push the most confident
prediction above the abstention threshold.
2.3.2 Re-Examining Top N Predictions
(REToP)
State-of-the-art models have achieved impressive
performance on numerous NLP tasks. Even in
cases where they fail to make a correct prediction,
they are often able to rank the correct answer as
one of their top N predictions. This provides op-
portunities for re-examining the top N predictions
to identify the correct answer in case of abstention.
To this end, a model that can estimate the correct-
ness of a prediction can be leveraged. Following
this intuition, we develop an auxiliary model that
takes the context, question, and a prediction as in-
put and assigns a score indicating the likelihoodof that prediction to be correct. This model can be
used for each of the top N predictions given by the
QA model to select the one that is most likely to be
the correct answer.
Training Auxiliary Model: We first create data
instances by annotating (context, question, predic-
tion) triplets conditioned on the correctness of the
QA system’s predictions and then train a classifica-
tion model using this data. This model is specific
to the given QA system and essentially learns to
distinguish its correct and incorrect predictions.
•Annotate (context, question, prediction)
triplets : We utilize the trained QA model to get
its top N predictions for each training instance.
Then, we annotate each (context, question, pre-
diction) triplet based on the prediction’s correct-
ness i.e. a correct prediction is annotated as ‘1’
and an incorrect prediction is annotated as ‘0’.
Figure 3 illustrates this annotation step.
•Train a classification model : Then, a binary
classification model is trained using the anno-
tated dataset collected in the previous step. This
model specifically learns to distinguish the cor-
rect predictions of the QA model from the incor-
rect ones. Softmax probability assigned to the
label ‘1’ corresponds to the likelihood of correct-
ness for each prediction.
Note that we use the QA model’s top N predictions
to collect the ‘0’ annotations instead of randomly
selecting candidates because this procedure results
in highly informative negative instances (that are
probable predictions and yet incorrect) and not
easy/obvious negatives. This can help the auxil-
iary model in learning fine-grained representations
distinguishing correct and incorrect predictions.
Leveraging Auxiliary Model: For an abstained
instance, we compute the likelihood value for each970of the top Npredictions given by the QA model us-
ing our trained auxiliary model. Then, we calculate
the overall confidence ( c) of each prediction ( p) as
a weighted average of the QA model’s probability
(s) and the auxiliary model’s likelihood score ( s)
i.e.cis calculated as:
c=α∗s+ (1−α)∗s
where αis a weight parameter.
We incorporate QA model’s probability as it pro-
vides more flexibility to compute the overall confi-
dence. Finally, prediction with the highest overall
confidence is selected as the new prediction. We
differentiate this method from existing methods
such as calibration in Appendix C.
2.3.3 Human Intervention (HI)
In intolerant application domains such as biomedi-
cals where incorrect predictions can have serious
consequences, human intervention is the most re-
liable technique to answer the abstained instances.
Human intervention can be in various forms such
as providing relevant knowledge to the model, ask-
ing clarifying questions (Rao and Daumé III, 2018)
or simplifying the input question. In this work,
we explore a simple human intervention approach
in which the system provides multiple predictions
instead of only one prediction for the abstained
instances. The human can then select the most
suitable prediction from the provided predictions.
Performance of this method can be approximated
based on the presence of the correct answer in the
predictions provided to the human. Note that the
above approach would answer all the abstained in-
stances and hence the coverage would always be
100%. This implies that with the increase in ab-
stention threshold, the risk would monotonically
decrease as multiple predictions would be returned
for a larger number of instances.
In addition to the above approach, we also ex-
plore a REToP-centric HI approach in which the
system returns multiple predictions only when RE-
ToP surpasses the confidence threshold in the post-
abstention stage. Similar to REToP, it abstains
on the remaining instances. Finally, we note that
comparing the performance of HI approaches with
other post-abstention approaches would be unfair
as other approaches return only a single prediction.
Therefore, we present HI results separately.3 Experiments and Results
3.1 Experimental Setup
Datasets: We experiment with SQuAD 1.1 (Ra-
jpurkar et al., 2016) as the source dataset and the
following 10datasets as out-of-domain datasets:
NewsQA (Trischler et al., 2017), TriviaQA (Joshi
et al., 2017), SearchQA (Dunn et al., 2017), Hot-
potQA (Yang et al., 2018), and Natural Questions
(Kwiatkowski et al., 2019), DROP (Dua et al.,
2019), DuoRC (Saha et al., 2018), RACE (Lai et al.,
2017), RelationExtraction (Levy et al., 2017), and
TextbookQA (Kim et al., 2019). We use the pre-
processed data from the MRQA shared task (Fisch
et al., 2019) for our experiments.
Implementation Details: We run all our exper-
iments using the huggingface (Wolf et al., 2020)
implementation of transformers on Nvidia V100
16GB GPUs with a batch size of 32and learning
rate ranging in {1−5}e−5. We generate 10para-
phrases of the question in Ensembling method, re-
examine top 10predictions, vary αin the range
0.3−0.7for REToP method, and vary the number
of predictions in the range 2to5for HI methods.
Since the focus of this work is on post-abstention,
it’s crucial to experiment with models that leave
sufficient room for effectively evaluating the ability
of post-abstention methods. For that reason, we ex-
periment with a small size model (BERT-mini hav-
ing just 11.3M parameters) from Turc et al. (2019)
for our experiments. However, we note that our
methods are general and applicable for all models.
3.2 Results
3.2.1 REToP
Table 1 shows the post-abstention performance of
REToP for selected abstention thresholds. The
last column (‘ Total Risk Improvement ’) in this ta-
ble corresponds to the overall improvement aggre-
gated over all confidence thresholds. It can be
observed that REToP achieves considerable risk
improvements both in the in-domain setting ( 21.81
on SQuAD) and the out-of-domain settings ( 24.23
on TextbookQA, 21.54on HotpotQA, 20.42on RE,
etc). Next, we analyze these results in detail.
Higher improvement on moderate confidences:
In Figure 4, we plot risk improvements achieved
by REToP on SQuAD (in-domain) and HotpotQA
(out-of-domain) datasets for all confidence thresh-
olds. These plots reveal that the improvement is971
more on moderate thresholds as compared to low
thresholds. We attribute this to the high difficulty
of instances that remain to be re-attempted at low
thresholds i.e. only the instances on which the
given system was highly underconfident are left for
the post-abstention method. It has been shown that
model’s confidence is negatively correlated with
difficulty (Swayamdipta et al., 2020; Rodriguez
et al., 2021; Varshney et al., 2022b) implying that
the remaining instances are tough to be answered
correctly. This justifies the lesser improvement in
performance observed at low thresholds.
In-Domain vs Out-of-Domain Improvement:
REToP achieves higher performance improvement
on the in-domain dataset than the out-of-domain
datasets (on average). This is expected as the auxil-
iary model in REToP is trained using the in-domain
training data. However, it still has good perfor-
mance on out-of-domain datasets as the auxiliary
model learns fine-grained representations to dis-
tinguish between correct and incorrect predictions.
Furthermore, the improvement on out-of-domain972
data varies greatly across datasets (from 0.7 on
TriviaQA to 24.23 on TextbookQA).
3.2.2 Comparing Post-Abstention Approaches
We provide the performance tables for other post-
abstention approaches in Appendix. However,
we compare their total risk improvement values
in Table 2. In the in-domain setting, REToP
achieves higher improvement than Ensembling
method. This is because the auxiliary model in RE-
ToP has specifically learned to distinguish the cor-
rect and incorrect predictions from the training data
of this domain. However, in some out-of-domain
cases, Ensembling outperforms REToP (SearchQA,
TriviaQA, NewsQA). Overall, REToP leads to a
consistent and higher risk improvement on average.
Ensembling also leads to a minor degradation in
a few out-of-domain datasets (DuoRC and Text-
bookQA). Next, we analyze the performance of
human intervention (HI) methods.
3.2.3 Human Intervention (HI)
We study two variants of HI method. In the first
variant, multiple predictions (n=2) are returned for
all the abstained instances. This makes the cover-
age to be 100% for all the confidences; therefore,
we present only the risk values in Table 3. As ex-
pected, with increase in abstention threshold, the
risk decreases because multiple predictions get out-
putted for a larger number of instances. Selection
of operating threshold for an application depends
on the trade-off between risk that can be tolerated
and human effort required to select the most suit-
able prediction from a set of predictions returned
by the system. For example, a low threshold can
be selected for tolerant applications like movie rec-
ommendations and a high threshold for tolerant
applications like house robots.
In the second variant of HI method, we study
aREToP-centric approach in which the system
returns multiple predictions only when REToP
surpasses the confidence threshold in the post-
abstention stage. The last column in Table 2 shows
the risk improvements achieved by this approach
(n=2). Note that REToP re-examines the top N pre-
dictions and selects one while this method outputs
multiple predictions and requires a human to select
the most suitable one. These results indicate that
though REToP achieves good performance, there
is still some room for improvement.
3.2.4 Ensembling Using Paraphrases
Comparing the performance of Mean and Max En-
sembling strategies reveals that Max increases the
coverage more than the Mean strategy but it also in-
creases the risk considerably. Thus, pushing the in-
stance’s confidence to surpass the abstention thresh-
old fails to provide risk improvements. However,
such a technique could be employed in scenarios
where risk degradation can be tolerated.
4 Analysis
What is the distribution of model’s original con-
fidence on the instances that get answered after
applying post-abstention method? In Figure 5,
we show the distribution of model’s original con-
fidence on SQuAD instances that get answered by
REToP at abstention threshold 0.5. Green-colored
bars represent the number of instances answered
from each confidence bucket. We found that REToP
answers a large number of instances from the high973
confidence buckets; however, instances from even
low confidence buckets get answered. This can fur-
ther be controlled using the weight parameter ( α)
in the overall confidence computation.
How often do the system’s predictions change
after applying REToP and what is its impact?
REToP can either boost the confidence of the top
most prediction of the given model or can select
a different answer by re-examining its top N pre-
dictions. In Figure 6, we specifically analyze the
latter scenario i.e. the instances on which REToP’s
prediction differs from the original model’s pre-
diction. At a threshold of 0.5, the original system
abstains on 3411 SQuAD instances and after ap-
plying REToP, it answers 1110 of those instances.
Out of these 1110 instances, the REToP changes
the prediction on 186instances. The original pre-
diction is incorrect in more cases ( 99vs87) and
after applying REToP, the system gives 116correct
predictions and only 70incorrect. This implies that
by overriding the original system’s prediction, RE-
ToP improves the system’s accuracy. However, in
some cases, it also changed a correct prediction to
incorrect but such cases are lesser than the former.
To what extent do the instances answered by
different post-abstention methods overlap? In
Figure 7, we demonstrate the Venn diagram of
SQuAD instances answered by REToP and Ensem-
bling (Mean) approaches at abstention threshold
0.5. REToP answers 1110 instances while Ensem-
bling answers 277and there 127common instances
between the two approaches. This indicates that
the two sets are not mutually exclusive i.e. there
are some instances that get targeted by both the ap-
proaches; however, there are a significant number
of instances that are not in the intersection. This
result motivates studying composite or sequential
application of different post-abstention methods to
further improve the post-abstention performance.
5 Conclusion and Discussion
In this work, we formulated ‘Post-Abstention’, a
task that allows re-attempting the abstained in-
stances of the given selective prediction system
with the aim of increasing its coverage without
significantly sacrificing the accuracy . We also
explored several baseline methods for this task.
Through comprehensive experiments on 11QA
datasets, we showed that these methods lead to
considerable performance improvements in both
in-domain and out-of-domain settings. We further
performed a thorough analysis that resulted in sev-
eral interesting findings.
Looking forward, we believe that our work opens
up several avenues for new research, such as explor-
ingtest-time adaptation ,knowledge hunting , and
other human intervention techniques like asking
clarification questions as post-abstention methods
(discussed in Appendix D). Studying the impact
of composite or sequential application of multi-
ple post-abstention methods in another promising
direction. Furthermore, prior selective prediction974methods can also be repurposed and explored for
this task. We plan to pursue these crucial research
directions in our future work. Finally, we hope our
work will encourage further research in this impor-
tant area and facilitate the development of more
reliable NLP systems.
Limitations
The proposed post-abstention methods require ad-
ditional computation and storage. Despite this addi-
tional requirement, we note that this is not a serious
concern as current devices have high storage capac-
ity and computation hardware. Furthermore, addi-
tional computation for training auxiliary model in
REToP is required only once and just an inference
is required at evaluation time which has a much
lower computation cost. Moreover, the risk mitiga-
tion that comes with the post-abstention methods
weighs much more than the computational or stor-
age overhead in terms of importance. Secondly,
human-intervention techniques require a human to
be a participant and contribute in the answering
process. However, these approaches do not ex-
pect the participating human to be an expert in the
task. Like other empirical research, it is difficult
to exactly predict the magnitude of improvement
a post-abstention method can bring. Our idea of
exploring sequential application of multiple post-
abstention methods addresses this concern and can
be used based on the application requirements.
Acknowledgement
We thank the anonymous reviewers for their in-
sightful feedback. This research was supported by
DARPA SAIL-ON program.
References975976977Appendix
A Ensembling (Mean) Performance
Table 5 shows the performance of using Ensem-
bling (Mean) as a post-abstention method for a
few selected abstention threshold values. For
each dataset, we provide three rows: the first
row (‘ Given ’) shows the coverage and risk values
of the given selective prediction system at speci-
fied abstention thresholds, the second row (‘ Ens’)
shows the coverage and risk after applying the post-
abstention method on the abstained instances of the
given selective prediction system, and the final row
(‘G@Ens’) shows the risk of the given selective
system at the coverage achieved by Ensmethod.
For the post-abstention method to be effective the
risk in the second row should be less than that in
the third row and the magnitude of difference corre-
sponds to the improvement. The last column ‘ Total
Risk Improvement ’ shows the overall improvement
aggregated over all confidence thresholds ranging
between 0 and 1 at an interval of 0.02.
B Dataset Statistics
Table 4 shows the statistics of all evaluation
datasets used in this work. SQuAD corresponds
to the in-domain dataset while the remaining
10 datasets are out-of-domain. We use the pre-
processed data from the MRQA shared task (Fisch
et al., 2019).
C Differentiating REToP from
Calibration
REToP is different from calibration based tech-
niques presented in (Kamath et al., 2020; Varshney
et al., 2022c) in the following aspects:
(a) Firstly, REToP does not require a held-out
dataset unlike calibration based methods that in-
fer the model on the held-out dataset to gather in-
stances on which the model in incorrect.
(b) Secondly, the auxiliary model trained in REToP
predicts the likelihood of correctness of (context,
question, prediction) triplet i.e. it is used for each
of the top N prediction individually. This is in con-
trast to calibrators that predicts a single score for
an instance and ignores the top N predictions.
(c) Finally, we use the entire context, question,
and the prediction to predict its correctness like-
lihood score unlike feature-based calibrator models
in which a random-forest model is trained using
just syntax-level features such as length of question,978
semantic similarity of prediction with the question,
etc.
D Other Post-Abstention Techniques
Asking clarifying questions to the user in order to
get information about the question has started to
received considerable research attention in conver-
sational, web search, and information retrieval set-
tings (Aliannejadi et al., 2021, 2020; Zamani et al.,
2020a; Zhang et al., 2020; Zamani et al., 2020b).
These techniques can be leveraged/adapted for the
post-abstention task.
Test-time adaptation is another promising re-
search area in which the model is adapted at test-
time depending on the instance. This is being stud-
ied in both computer vision (Chen et al., 2022) and
language processing (Wang et al., 2021; Banerjee
et al., 2021).
Cascading systems in which stronger and
stronger models are conditionally used for infer-
ence is also an interesting avenue to explore with
respect to Post-Abstention (Varshney and Baral,
2022; Li et al., 2021; Varshney et al., 2022a).
E Coverage 100% for Human
Intervention Methods
We believe that the ability to identify situations
when there is no good answer in the top N returned
candidates is a very difficult task (for the humans
also) and it requires even more cognitive skills than
just selecting the best answer from the provided
answer candidates. Because of this reason, the
coverage is 100%.
F Comparison with Other Selective
Prediction Methods
In this work, we presented a new QA setting and
studied the performance of several baseline meth-
ods for this task. The focus of this work is on
studying the risk improvement that can be achieved
in this problem setup. We consciously do not pitchthe approaches for this task as competitors of the
existing selective prediction approaches. In fact,
these approaches are complimentary to the se-
lective prediction approaches. A post-abstention
method can be used with any selective prediction
method as the first step.979980ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
We have Limitations Section at the end of the paper after Conclusion
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
References
/squareB1. Did you cite the creators of artifacts you used?
We use the publicly available standard NLP datasets in this work with appropriate citations and
references.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We do not create any artifcats in this reserach. We use the publicly available standard NLP datasets
in this work with proper citations and references.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
We do not create any artifcats in this reserach. We use the publicly available standard NLP datasets
in this work with proper citations and references.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We do not collect any data for this research and use standard publicly available NLP datasets
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
We do not collect any data for this research
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4981C/squareDid you run computational experiments?
Sections 3 and 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Sections 3 and 4
/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Sections 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Sections 3 and 4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Sections 3 and 4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.982