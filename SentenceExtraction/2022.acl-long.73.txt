
Zeming Liu, Jun Xu, Zeyang Lei, Haifeng Wang, Zheng-Yu Niu, Hua WuResearch Center for Social Computing and Information Retrieval,
Harbin Institute of Technology, Harbin, ChinaBaidu Inc., Beijing, China
zmliu@ir.hit.edu.cn, {xujun03, leizeyang, wanghaifeng, niuzhengyu, wu_hua}@baidu.com
Abstract
Most dialog systems posit that users have ﬁg-
ured out clear and speciﬁc goals before start-
ing an interaction. For example, users have
determined the departure, the destination, and
the travel time for booking a ﬂight. However,
in many scenarios, limited by experience and
knowledge, users may know what they need,
but still struggle to ﬁgure out clear and speciﬁc
goals by determining all the necessary slots.
In this paper, we identify this challenge, and
make a step forward by collecting a new
human-to-human mixed-type dialog corpus. It
contains 5k dialog sessions and 168k utter-
ances for 4 dialog types and 5 domains. Within
each session, an agent ﬁrst provides user-goal-
related knowledge to help ﬁgure out clear and
speciﬁc goals, and then help achieve them.
Furthermore, we propose a mixed-type dia-
log model with a novel Prompt-based con-
tinual learning mechanism. Speciﬁcally, the
mechanism enables the model to continually
strengthen its ability on any speciﬁc type by
utilizing existing dialog corpora effectively.
1 Introduction
One of the overarching goals of Artiﬁcial Intel-
ligence is to build an intelligent agent that can
generate coherent multi-turn dialogs to meet user
needs/goals. Recently, multiple dialog agents have
been launched, such as Echo and Siri. These agents
usually position themselves as some kind of “do
engines” that act under users’ clear instructions.
Speciﬁcally, they posit users have ﬁgured out clear
and speciﬁc goals by determining all the necessary
aspects or slots of their goals. For example, be-
fore booking a ﬂight, a user has determined the
departure, the destination and the travel time.However, such assumption can not hold in many
real-world scenarios. For example, a user wants
to plan a trip to Beijing for relaxing, but he or she
only has limited knowledge about Beijing. Thus
it is difﬁcult for him or her to decide which slots
are needed to achieve this goal. Obviously, in this
scene, the user needs additional consultant services
from an agent to help ﬁgure out clear and speciﬁc
goals. However, the aforementioned assumption
hinders providing these services effectively.
In this paper, we make a step towards solving the
challenge. In order to facilitate the study of how
to help users clarify ing their goals, we construct a
new Dialog corpus at Bai du, denoted as DuClar-
ifyDial .As shown in Figure 1, a user chats
about “feels anxious” because of work pressure,
and wants to relax himself or herself but have no
clear idea about the trip. In the scenario, the agent
conducts knowledge-grounded dialogs and ques-
tion answering conversations to help the user learn
more about goal-related knowledge, which helps
ﬁgure out clear and speciﬁc goals. Finally, the user
determines to visit “Wangfujing Catholic Church”
and books a restaurant nearby. Speciﬁcally, in
DuClarifyDial, besides basic social chitchat, an
agent should help users ﬁgure out clear and speciﬁc
goals by providing goal-related knowledge through
coherent knowledge-grounded dialogs and question
answering (QA) conversations. Then, upon request,
it should also conduct task-oriented dialogs to help
achieve user goals.
To this end, we ﬁrst collect a human-to-human
mixed-type dialog dataset. It contains 5k dialog
sessions and 168k utterances for 4 dialog types and
5 domains. Speciﬁcally, each session contains at
least two of following four dialog types, i.e., social
chitchat, question answering, knowledge-grounded
dialog, and task-oriented dialog. Furthermore, in1024
order to seamlessly blend different types of dialogs,
we make efforts in both dataset collection and task
deﬁnition. For dataset collection , we ﬁrst collect
human-to-human dialogs within the Wizard-of-Oz
framework (Kelley, 1984). Then, we design a uni-
ﬁed dialog state schema and dialog act schema
for all types of dialogs. Here, the uniﬁcation can
(1) ease the dialog annotation procedures, (2) sim-
plify dialog model design, and (3) facilitate wiser
dialog management by bringing a shared dialog
semantic space for different types of dialogs. Fi-
nally, we annotate dialog states and dialog acts. For
task deﬁnition , we ﬁrst unify the dialog modelling
into three sub-procedures, which includes dialog
state tracking, dialog act planning and response
generation. Then, we deﬁne one sub-task for each
sub-procedure. Besides, in order to facilitate end-
to-end modelling, we also deﬁne an end-to-end
dialog generation sub-task.
To facilitate model comparison, we conduct
bench-marking experiments on DuClarifyDial for
the aforementioned four sub-tasks. Furthermore,
since DuClarifyDial is a mixed-type dialog corpus,
it is straightforward to explore effective methods
for utilizing existing single-type or mixed-types
dialog corpora in task modelling. Speciﬁcally, we
propose a novel Prompt-based continual learning
mechanism to strengthen the model ability, by con-
tinually utilizing existing different types of dia-log corpora. Here, we equip a pre-trained dialog
model (Bao et al., 2020) with (1) different prompt
texts as input and (2) type, task and domain rep-
resentation in embedding layer for different dia-
log types. Furthermore, we train our model by
two steps with continual learning mechanism: ﬁrst
Prompting on existing dialog corpora and then ﬁne-
tuning on DuClarifyDial.
This work makes the following contributions:
•We identify a new challenge that users have
difﬁculties to ﬁgure out all the aspects of their
goals in many real-world scenarios.
•We propose a large-scale Chinese mixed-type
corpus, where each session weaves together
multiple types of dialogs with natural cross-
type transitions. Speciﬁcally, we design a uni-
ﬁed dialog state (act) schema for all types
of dialogs. Here, the uniﬁed organization
ﬁrst brings a shared semantic space for task-
oriented and non-task-oriented dialogs. Then,
it enables a uniﬁed dialog modelling proce-
dures for all types of dialogs, which can facil-
itate more effective dialog management.
•We build benchmarking baselines on DuClari-
fyDial and propose a novel Prompt-based con-
tinual learning mechanism to utilize existing
dialog corpora effectively.10252 Related Work
2.1 Multi-Domain Task-Oriented Dialog
Datasets
Task-oriented dialog systems have continued an
active research area for decades and have been
consistently supported by the development of
new datasets. Recently, several large-scale
multi-domain task-oriented dialog datasets have
emerged (Budzianowski et al., 2018; Quan et al.,
2020; Rastogi et al., 2020; Zhu et al., 2020; Jin
et al., 2021; Chen et al., 2021). Speciﬁcally, Multi-
WOZ (Budzianowski et al., 2018) is a fully-labelled
collection of human-human written conversations
spanning over multiple domains and topics, which
contains a size of 10k dialogs. Schema (Rastogi
et al., 2020) proposes a schema-guided paradigm
for task-oriented dialog, which contains over
16k multi-domain conversations spanning 16 do-
mains. CrossWOZ (Zhu et al., 2020) and Ri-
SAWOZ (Quan et al., 2020) are Chinese cross-
domain task-oriented datasets, which contains 6K
and 11k dialogs respectively. ABCD (Chen et al.,
2021) includes over 10K dialogs that incorporate
procedural, dual-constrained actions.
Although achieved promising progress, these
datasets usually posit that users have ﬁgured out
clear and speciﬁc goals before staring an interac-
tion, which is not hold in many practical scenarios.
In this paper, we focus on providing additional con-
sultant services for users, to help ﬁgure out clear
and speciﬁc user goals.
2.2 Knowledge grounded Dialog Datasets
Open-domain dialog systems have attracted lots of
interests in recent years. To develop more human-
like dialog models, several knowledge-grounded
corpora have been proposed (Wu et al., 2019b;
Moon et al., 2019; Liu et al., 2020b; Zhou et al.,
2020; yang Wang et al., 2021; Komeili et al., 2021;
Feng et al., 2020; Yoshino and Kawahara, 2015;
Tanaka et al., 2021). The main purpose on these
datasets is to generate more knowledgeable dialogs.
In comparison, DuClarifyDial focuses on helping
ﬁgure out clear and speciﬁc user goals. Moreover,
DuClarifyDial is a mixed-type dialog dataset that
contains four types of dialogs.
2.3 Multi-tasking Dialogs
Recently, there are multiple efforts on developing
dialog systems that can multi-task on multiple types
of dialogs (Kim et al., 2020; Smith et al., 2020;Mosig et al., 2020; Madotto et al., 2020; Saha et al.,
2018; Sun et al., 2021; Young et al., 2021). Specif-
ically, Kim et al. (Kim et al., 2020) propose to
handle out-of-API requests, by accessing unstruc-
tured domain knowledge in task-oriented dialogs.
Sunet al. (Sun et al., 2021) and Yong et al. (Young
et al., 2021) propose to fuse task-oriented and open-
domain dialogs in conversational agents, in order
to generate more engaging and interactive dialogs.
The DuClarifyDial dataset differs from these
datasets in that we focus on helping ﬁgure out
clear and speciﬁc user goals, rather than target-
ing at the out-of-API problem (Kim et al., 2020)
or facilitating a more engaging and interactive dia-
log generation (Young et al., 2021). Furthermore,
DuClarifyDial contains more types of dialogs than
previous datasets. Moreover, in order to seamlessly
blend different types of dialogs for efﬁcient consult-
ing, DuClarifyDial utilizes the same dialog state
schema and dialog act schema for all types of di-
alogs, rather than utilizes different schema for dif-
ferent types of dialogs.
3 The DuClarifyDial Dataset
DuClarifyDial is designed to collect a high quality
mixed-type dialog dataset for helping ﬁgure out
clear and speciﬁc goals. In DuClarifyDial, one per-
son serves as the user and the other as the wizard
(agent). In order to help ﬁgure out clear and speciﬁc
goals, besides social chitchat, the agent provides
user-goal-related information through knowledge
grounded dialogs and QA conversations, and then
help achieve the goals through task-oriented di-
alogs.
Speciﬁcally, in order to effectively weave to-
gether multi types of dialogs for achieving this pur-
pose, it is essential for different types of dialogs to
share the same state space and action space. Thus,
in Section 3.4, we utilize a uniﬁed dialog state
schema and dialog act schema for the aforemen-
tioned four types of dialogs.
In the following, we will introduce the four steps
of DuClarifyDial collection: (1) building knowl-
edge base to provide goal-related information; (2)
constructing dialog templates to assist dialog col-
lection; (3) collecting conversation utterances by
crowdsourcing; (4) annotating dialog states and
dialog acts.1026
3.1 Knowledge Base Construction
In order to create a knowledge base that includes
ﬁve domains: hotel, attraction, restaurant, food,
and movie, we collect publicly available informa-
tion from the WEB. Speciﬁcally, for the hotel do-
main, we collect 1,133 entities and their related
knowledge from two famous online accommoda-
tion reservation websites, Qunar and Ctrip.For
the attraction domain, we collect 435 entities and
their related knowledge from the famous travelling
website, Mafengwo.For the restaurant domain,
we collect 122 entities and their related knowledge
from the famous shopping platform, Meituan.
For the food domain, we collect 1,971 entities and
their related knowledge from the famous online en-
cyclopedia, Baidu Baike.Finally, for the movie
domain, we collect 224 entities and their related
knowledge from two famous social networking
websites, Mtime and Douban.3.2 Dialog Template Construction
Based on the collected knowledge base, we
generate dialog templates to guide crowdsourc-
ing workers, which is in line with previous
work(Budzianowski et al., 2018; Liu et al., 2020b).
Here, each template consists of a sequence of dia-
log sub-scenarios, and each sub-scenario is deﬁned
by a dialog type, a dialog topic and a detailed de-
scription text. Table 1 shows an example dialog
template. Speciﬁcally, in order to better imitate
the real scenarios, dialog templates should intro-
duce different interaction behaviours. For example,
a user may ask for reserving a ticket during con-
ducting an in-depth knowledge-grounded dialogs
around a certain entity, e.g., an attraction. Further-
more, a user may interrupt a task-oriented dialog
by chatting about some instant content in mind, and
then continue the task-oriented dialog.
In order to construct dialog templates, we ﬁrst
utilize heuristic rules to automatically enumerate
candidate sub-scenarios sequences that have natu-
ral topic transitions. Then, we utilize pre-deﬁned
templates to generate detailed descriptions for these
sub-scenarios. Finally, to further ensure natural
topic transitions, we manually ﬁlter out a few inco-
herent dialog templates, such as descriptions that
contain inconsistent facts.
3.3 Dialog Collection
In order to collect high quality dialogs, we set a
strict annotation procedure to guide workers to
annotate dialogs based on the given templates.
Speciﬁcally, the collection procedure includes three
stages: (1) reliable crowdsourcing workers recruit-
ment, (2) dialog generation, and (3) quality veriﬁ-
cation.
In the worker recruitment stage , in order to
select reliable workers, we recruit 100 candidates
in a famous crowdsourcing platform.Then, we
ask each candidate to label 10 dialog sessions based
on given templates. Lastly, we employ the top-
40 candidates with the highest labelling quality to
serve as crowdsourcing workers.
In the dialog generation stage , we develop a
labelling interface for crowdsourcing workers to
converse synchronously. Then, we randomly pair
up two crowdsourcing workers and set each of them
a role of the user or the wizard (bot). Lastly, the
two crowdsourcing workers generate dialogs with1027
the help of the aforementioned knowledge base and
dialog templates.
User Side For a given dialog template, in order
to prevent information overload, we only provide
a sub-scenario to the user at a time. During dialog
collection, a user ﬁrst reads though the detailed de-
scription to understand the provided sub-scenario.
Then, based on the given sub-scenario, the user
communicates with the wizard turn by turn. Finally,
the user may require for another sub-scenario if he
or she believes the current sub-scenario has been
accomplished. Speciﬁcally, in order to diversify
the corpus, we encourage the users to follow their
own speaking style in communication.
Wizard Side A wizard is required to serve as a
consultant, who is responsible for helping users
ﬁgure out clear and speciﬁc goals. At each sub-
scenario, the wizard can get access to the associated
knowledge in the interface, which is extracted from
the knowledge base automatically. When receiving
an utterance from the user side, the wizard needs
to respond appropriately.
In the quality veriﬁcation stage , we manually
check the collected dialogs. Speciﬁcally, if a dialog
is considered as unqualiﬁed, we will ask the two
crowdsourcing workers to revise the dialog until itis qualiﬁed.
3.4 Dialog Annotation
After collecting the conversation data, we recruit
crowdsourcing workers to annotate dialog states
and dialog acts. Speciﬁcally, in order to seamlessly
blend multi types of dialogs for helping users ﬁgure
out clear and speciﬁc goals, we ﬁrst design a uniﬁed
dialog states schema and dialog act schema for
all types of dialogs, and then annotate the dialogs
based on the schema.
The uniﬁed dialog state consists of a list of
domain-states, as shown in Figure 2. Speciﬁcally,
we add a “general” domain to store user-proﬁle
related states, e.g., user mood. The “general” do-
main is important, since user-proﬁle may have a
signiﬁcant impact on his or her goal. For other
domains, we split domain-states into three parts:
(1)“_booked” for storing booked orders in this do-
main. Each booked order contains all the necessary
information for ﬁnishing the order; (2) “_semi” for
storing the important but not necessary information
for an order; (3) “_entities” for storing all the men-
tioned entities and the mentioned speciﬁc pieces
of information about these entities. Speciﬁcally,
we store an “_attitude” slot in each mentioned en-
tity to capture user interest directly. The values
of the “_attitude” slot contain two types: positive
and negative. Here, the “_booked” part is mainly
corresponding to the task-oriented dialog, the “_en-
tities” part is mainly corresponding to the knowl-
edge grounded dialog and question answering dia-
log, and the “_semi” part corresponding to all the
aforementioned three dialog types.
The uniﬁed dialog act schema consists of do-
mains, intents, slots and values. Speciﬁcally, we
add a “general” domain to store intents that are
not directly related to user goals. For other do-
mains, they usually contain four intents: “_re-
quest”, “_inform”, “_recommend” and “_no-offer”.
Speciﬁcally, the classical knowledge selection in
knowledge-ground dialog is treated as an “_inform”
action in this uniﬁed act schema.
Based on the uniﬁed schema, we recruit 10
crowdsourcing workers to annotate these dialog
states and dialog acts. Speciﬁcally, before formal
annotation, each worker must pass a labelling test.
Here, we ﬁrst annotate 10 dialogs manually. Then,
we ask workers to annotate these dialogs. Lastly,
a worker passes the test if his annotations are the
same as our annotations.1028
3.5 Overall Dataset
The overall collected data consists of 5,052 dialog
sessions in total, with 3,000 sessions in the training
set, and validation and test sets of 500 and 1,052
sessions, respectively. Overall statistics can be
found in Table 2.
We conduct human evaluations for data quality.
Speciﬁcally, if a dialog follows the instruction in
task templates and the utterances are ﬂuent and
grammatical, it will be rated “1”, otherwise “0”.
Then we ask three workers to judge the quality of
200 randomly sampled dialogs. Finally we obtain
an average score of 0.83 on this evaluation set.
4 The Mixed-Type Dialog Model with
Prompt-based Mechanism
Recently, large scale pre-trained dialog models
have achieved impressive performance, both in
task-oriented dialog (Heck et al., 2020; Yang et al.,
2021) and open-domain chitchat (Adiwardana et al.,
2020; Roller et al., 2021; Bao et al., 2020). Mean-
while, the methodologies for different types of di-
alogs have gradually shifted to generative and end-to-end modelling. Following these trends, we pro-
pose a pre-trained mixed-type dialog model based
on (Bao et al., 2020), denoted as PLATO-MT. Fur-
thermore, we equip our model with a novel Prompt-
based continual learning mechanism to strengthen
the model ability by continually utilizing external
existed different types of dialog corpora.
4.1 The Prompt-based Continual Learning
Mechanism
Figure 3 shows an overview of the proposed
PLATO-MT model. As shown in Figure 3 (1),
the model is a multi-layer transformer-based neural
network. Furthermore, the inputs and outputs of
all dialog sub-tasks are formalized as simple text
sequences.
In order to effectively blend the abilities of
mixed-type dialog in one model, we follow the
“Prompt + LM Fine-tuning” strategy (Liu et al.,
2021). Speciﬁcally, we design different Prompt
texts as input for different dialog types. For exam-
ple, for knowledge-based dialogs, the Prompt text
of input is “[Knowledge] context”. Here “[Knowl-
edge]” refers to knowledge sentences used for con-
text. Similarly, the Prompt text of QA is “[Ques-
tion|Answer] context” and the Prompt text of task-
oriented dialog is “[Domain|Slot|Value] context”.
Furthermore, we add type, task and domain embed-
ding representation in embedding layers to further
differentiate the characters of different dialog types.
Meanwhile, we train the PLATO-MT model with
continuous learning mechanism, as shown in Fig-
ure 3 (2). In particular, we ﬁrst carry on prompt-
ing on existing dialog corpora, such as Cross-
WOZ (Zhu et al., 2020), RiSAWoz (Quan et al.,
2020), BiToD (Lin et al., 2021), Kdconv (Zhou
et al., 2020) and DurecDial (Liu et al., 2020b).
Thus we strengthen our model ability by contin-1029ually utilizing external existed different types of
dialog corpora. Then we ﬁnetune the prompted
model on our proposed dialog corpus DuClarify-
Dial.
5 DuClarifyDial as a New Benchmark
We break down the mixed-type dialog modelling
task into three sub-tasks: dialog state tracking, di-
alog act planning, and dialog-act-to-text genera-
tion. Besides, in order to facilitate end-to-end di-
alog modelling, we deﬁne an end-to-end dialog-
context-to-text generation sub-task. For each of
the four sub-tasks, we report benchmark results on
the following dialog models, which have achieved
promising performance in the popular MultiWOZ
dataset (Budzianowski et al., 2018). Speciﬁcally,
we use the original codes released by the authors.
UBAR (Yang et al., 2021) UBAR is a fully end-
to-end task-oriented dialog model that takes a pre-
trained model as backbone. Here, since DuClari-
fyDial is a Chinese dataset, we utilize a Chinese
large-scale pre-trained model, ERNIE (Xiao et al.,
2020), to initialize UBAR.
MinTL (Lin et al., 2020) MinTL is a strong model
that utilizes effective transfer learning to plug-and-
play pre-trained models. Here, instead of utilizing
BART (Lewis et al., 2020) as in the original paper,
we utilize the multi-lingual version, mBART (Liu
et al., 2020a), for initialization.
PLATO (Bao et al., 2020) PLATO is the state-of-
the-art Chinese pre-trained dialog model. We use
the released parameters.
PLATO-MT It is the proposed uniﬁed mixed-type
dialog model with Prompt-based Continual Learn-
ing mechanism. Here, the Prompt-related parame-
ters are random initialized.
PLATO-MT w/o Prompt It is the PLATO-MT
model without Prompting. We ﬁrst ﬁne-tune it on
the same set of existing dialog corpus as in PLATO-
MT, and then ﬁne-tune it on DuClarifyDial.
5.1 Dialog State Tracking
For building a successful dialog system, a robust
dialog state tracking ( DST ) is considered as the
ﬁrst step. It takes previous dialog utterances and
the recent dialog state as input, and then outputs
the current dialog state.
To evaluate the performance on dialog state
tracking, we utilize both slot-level metric anddialog-level metrics. For slot-level metric, we mea-
sure the slot accuracy ( Slot Acc. ). Speciﬁcally, the
slot accuracy is measured by individually compar-
ing each (domain, slot, value) triplet to its ground
truth label. For dialog-level metric, besides dialog
type accuracy ( Type Acc. ) and dialog domain ac-
curacy ( Domain Acc. ), we also measure the joint
goal accuracy ( Joint Acc. ) (Wu et al., 2019a). It
compares the predicted dialog states to the ground
truth at each turn, and the output is considered cor-
rect if and only if all the predicted values exactly
match the ground truth.
Table 3 shows the evaluation results. We can see
all the models achieve promising results in terms
of “Type Acc.” and “Domain Acc.”. It indicates
the effectiveness of utilizing large-scale pre-trained
models as backbone. Furthermore, we notice that
PLATO-MT outperforms all the baselines, espe-
cially in terms of “Slot Acc.” and “Joint Acc.”.
It demonstrates that PLATO-MT can track dialog
states effectively.
5.2 Dialog Act Planning
The dialog act planning ( DAP ) sub-task takes di-
alog context, current dialog state and retrieved
coarse knowledge as input, and then outputs system
act. Speciﬁcally, for each dialog session, we ﬁrst
extract all the entities in it, and then retrieve all the
related knowledge about these entities to serve as
the retrieved coarse knowledge.
To evaluate the performance on dialog act plan-
ning, we measure the dialog act accuracy ( Act
Acc.) and the BLEU-1/2 (Papineni et al., 2002)
score.
Table 3 shows the evaluation results. We notice
that PLATO-MT outperforms all the baselines, es-
pecially in terms of “Act Acc.”. It demonstrates
that PLATO-MT can plan appropriate dialog acts
effectively.
5.3 Dialog-Act-to-Text Generation
The dialog act to text generation ( RG) sub-task
aims to transform a structured dialog act into a
response. It takes dialog context and delexicalized
dialog act as input, and then outputs a response.
To evaluate performance on generation, we uti-
lize both automatic metrics and manual metrics.
For automatic evaluation, we use several clas-
sical metrics, including BLEU (Papineni et al.,
2002), METEOR (Banerjee and Lavie, 2005),
CIDER (Vedantam et al., 2015) and Distinct
(Dist. ) (Li et al., 2016).1030MethodsSub-Task1: DST Sub-Task2: DAP
Type Acc. Domain Acc. Slot Acc. Joint Acc. Act Acc. BLEU-1/2
UBAR 0.96 0.95 0.77 0.39 0.85 0.84/0.83
MinTL 0.99 0.94 0.86 0.48 0.87 0.83/0.82
PLATO 0.99 0.97 0.85 0.48 0.91 0.89/0.88
PLATO-MT 0.99 0.97 0.88 0.51 0.93 0.90 /0.90
-w/o Prompt 0.99 0.97 0.87 0.49 0.92 0.90/0.89
For manual evaluation, we conduct evaluation on
randomly sampled 50 sessions at the level of both
turns and dialogs. For turn-level human evaluation,
the generated responses are evaluated by three an-
notators in terms of appropriateness ( Appr. ) and
informativeness ( Info. ). For dialog-level human
evaluation, we measure hallucination ( Hallu. ) that
measures information accuracy in generated re-
sponses, and dialog success ( Suc.) that measures
whether an agent helps users ﬁgure out clear goals.
Speciﬁcally, if a user has not completed any order
during a session, the success score is 0; Otherwise,
the success score equals to the information accu-
racy in a session.
Table 4 shows the evaluation results. We ﬁnd
PLATO-MT signiﬁcantly outperforms all the base-
lines in terms of all the metrics except “Dist-1/2”
(sign test, p-value < 0.01). It indicates that PLATO-
MT can generate dialogs with higher qualities.
5.4 End-to-End Dialog Generation
This end-to-end dialog generation sub-task ( E2E-
DG) takes dialog context as input, and then outputs
an utterance for responding. Speciﬁcally, in the
end-to-end settings, since the dialog domain and
type information are not available at each turn, we
do not use them as input information. Here, we
consider the same set of evaluation settings as in
Section5.3.
Table 5 shows the evaluation results. We ﬁnd
PLATO-MT signiﬁcantly outperforms all the base-lines in terms of all the metrics except “Dist-1/2”
(sign test, p-value < 0.01). Speciﬁcally, in terms of
“Hallu.” and “Suc.” in manual evaluation, PLATO-
MT outperforms other models by a large margin. It
indicates that PLATO-MT is much more competent
in helping users learn about correct goal-related
knowledge, which is essential for helping users
ﬁgure clear and speciﬁc goals.
5.5 Ablation Study
In order to evaluate the contribution of the proposed
Prompt-based continual learning mechanism, we
remove the mechanism from PLATO-MT, denoted
as “PLATO-MT-w/o Prompt”. Here, we ﬁrst ﬁne-
tune PLATO on the same set of existing dialog
corpus as in PLATO-MT, and then ﬁne-tune it on
DuClarifyDial. For evaluation, we consider the
same set of settings as in Section5.3.
As shown in Table 3, Table 4 and Table 5, its per-
formance drops in terms of most metrics in all the
four sub-tasks. Speciﬁcally, in manual evaluation
in Table 5, we notice a sharp performance degra-
dation in terms of “Hallu.” and “Suc.”. It demon-
strates the Prompt-based mechanism is essential for
effectively utilizing existing dialog corpora, which
enables PLATO-MT can continually strengthen its
ability on any speciﬁc dialog type.
Furthermore, we ﬁnd that, in terms of most met-
rics, the mechanism gains more in the end-to-end
conversation generation sub-task than in the other
three sub-tasks. This is because there are no avail-1031
able annotated information in the end-to-end con-
versation generation sub-task, which makes it a
more difﬁcult task. Thus, the effect of Prompt-
based continual mechanism appears relatively more
signiﬁcant.
6 Conclusion
In this paper, we ﬁrst identify the challenge that
users may struggle to ﬁgure out clear and speciﬁc
goals in many real scenarios. Then, we make a
step forward by collecting a new human-to-human
mixed-type dialog corpus, which contains 5k dia-
log sessions and 168k utterances for 4 dialog types
and 5 domains. Furthermore, we setup bench-
marks based on the corpus. Moreover, we pro-
pose a mixed-type dialog generation model with
a novel Prompt-based continual learning mecha-
nism. Finally, experimental results demonstrate the
effectiveness of the mechanism.
Ethical Considerations
We make sure that DuClarifyDial has been col-
lected in a manner that is consistent with the terms
of use of any sources and the intellectual property
and privacy rights of the original authors of the
texts. And crowd workers were treated fairly. This
includes, but is not limited to, compensating them
fairly and ensuring that they were able to give in-
formed consent, which includes, but is not limited
to, ensuring that they were voluntary participants
who were aware of any risks of harm associated
with their participation. Please see Section 3 for
more details characteristics and collection process
ofDuClarifyDial .
References103210331034