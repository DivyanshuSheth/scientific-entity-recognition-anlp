
Sarah WiegreffeJack HesselSwabha Swayamdipta
Mark RiedlYejin ChoiSchool of Interactive Computing, Georgia Institute of TechnologyAllen Institute for Artiﬁcial IntelligencePaul G. Allen School of Computer Science and Engineering, University of Washington
saw@gatech.edu, riedl@cc.gatech.edu, {jackh,swabhas,yejinc}@allenai.org
Abstract
Large language models are increasingly capa-
ble of generating ﬂuent-appearing text with rel-
atively little task-speciﬁc supervision. But can
these models accurately explain classiﬁcation
decisions? We consider the task of generating
free-text explanations using human-written ex-
amples in a few-shot manner. We ﬁnd that
(1) authoring higher quality prompts results
in higher quality generations; and (2) surpris-
ingly, in a head-to-head comparison, crowd-
workers often prefer explanations generated by
GPT-3 to crowdsourced explanations in exist-
ing datasets. Our human studies also show,
however, that while models often produce fac-
tual, grammatical, and sufﬁcient explanations,
they have room to improve along axes such
as providing novel information and supporting
the label. We create a pipeline that combines
GPT-3 with a supervised ﬁlter that incorpo-
rates binary acceptability judgments from hu-
mans in the loop. Despite the intrinsic subjec-
tivity of acceptability judgments, we demon-
strate that acceptability is partially correlated
with various ﬁne-grained attributes of explana-
tions. Our approach is able to consistently ﬁl-
ter GPT-3-generated explanations deemed ac-
ceptable by humans.
1 Introduction
As natural language understanding tasks have be-
come increasingly complex, the ﬁeld of explainable
NLP has embraced explanations written in free-
form natural language. In contrast to extractive
explanations that highlight tokens in the input, free-
text explanations provide a natural interface be-
tween machine computation and human end-users
(Hendricks et al., 2016; Camburu et al., 2018). The
dominant paradigm for producing free-text expla-
nations is via direct supervision, i.e., training an
autoregressive, generative language model to pre-
dict human-authored explanations directly (Kim
et al., 2018; Park et al., 2018; Ehsan et al., 2018;
Narang et al., 2020; Wiegreffe et al., 2021, i.a.).Figure 1: Illustration of our overgeneration + ﬁltra-
tion pipeline for producing human acceptable explana-
tions for CommonsenseQA and SNLI (see examples in
Table 1). Authors of this work write explanations to
prompt GPT-3, generating 5 explanations per instance.
An acceptability ﬁlter, trained on human binary accept-
ability judgments, determines which of these generated
explanations are plausible. Evaluation is performed at
both the explanation and the instance level.
However, collecting high-quality written expla-
nations to serve as supervision is difﬁcult and ex-
pensive. More than 70% of existing free-text ex-
planation datasets are crowdsourced (Wiegreffe
and Marasovi ´c, 2021), and even the most metic-
ulous crowdsourcing efforts frequently fail to elicit
logically consistent and grammatical explanations
(Narang et al., 2020). Furthermore, a lack of
standardized crowdsourcing design has resulted in
highly varied datasets, which are hard to compare
or combine (Tan, 2021).632Recent progress in prompting large language
models (LLMs) provides a potentially promising
alternate to large-scale crowdsourcing. The in-
context learning paradigm, wherein powerful lan-
guage models are prompted in a few-shot manner
with just a few examples, has proven surprisingly
effective across a range of NLP tasks (Radford
et al., 2019; Brown et al., 2020; Shin et al., 2020;
Schick and Schütze, 2021a, i.a.). In this work
we ask: can LLMs also generate reliable expla-
nations ? In human subjects studies, we ﬁnd that
GPT-3 (Brown et al., 2020) can be readily made
to generate explanations via prompting, and sur-
prisingly, humans often prefer GPT-3 generated
explanations to crowdsourced explanations in ex-
isting datasets (§2).
Two additional human subjects studies, however,
demonstrate that GPT-3-generated explanations
still have signiﬁcant room for improvement along
axes such as providing new information (i.e., avoid-
ing repetition) and supporting the label; human
subjects found less than half of greedily-decoded
GPT-3-generated explanations to be acceptable
with 100% agreement. To improve upon this, we
re-frame the role of crowd annotators: instead of
asking them to write explanations as in prior work,
we (1) repeatedly query GPT-3 to generate multi-
ple candidate explanations for each input instance,
and (2) ask crowdworkers to rate the acceptability
of each candidate generation. After showing that
GPT-3 can usually generate an explanation that hu-
mans unanimously ﬁnd acceptable within as few as
ﬁve queries (§3), we use a small number of these
binary crowdworker judgments to supervise an ac-
ceptability ﬁltering model, which can be applied
to select high quality candidates among GPT-3’s
outputs (Figure 1; §4).
Despite intrinsic subjectivity in acceptability rat-
ings, our supervised model improves upon the
already-competitive few-shot paradigm by consis-
tently selecting (human-identiﬁed) high quality ex-
planations better than strong baselines. Human
evaluations reveal that the ﬁltration model not only
improves acceptability, but also other axes like sup-
porting the label and providing novel information.
In summary, our main ﬁndings are:
i.few-shot prompting with GPT-3 produces sur-
prisingly competitive explanations, providing
a promising alternative to crowd-authored free-
text explanation corpora;
ii.binary human labeling can instead be leveragedto train a ﬁlter that selects high-quality machine-
generated explanations; and
iii.in areas where GPT-3 struggles, including infor-
mation content, supporting the label, and overall
acceptability, our proposed overgenerate-and-
ﬁlter pipeline improves generated explanations.
We publicly release our code and data.
2 GPT-3 is Competitive with
Crowdsourced Explanation Datasets
We investigate three research questions:
1.Are GPT-3-generated explanations preferable to
crowdsourced ones in existing datasets? (§2.1)
2.Can improving prompt quality improve GPT-3-
generated explanations? (§2.2)
3.Along what ﬁne-grained dimensions are GPT-3-
generated explanations preferred, and do these
correlate with overall acceptability? (§2.3)
Explanation tasks and datasets. We consider
two English tasks: CommonsenseQA and natural
language inference (NLI), shown in Table 1. Com-
monsenseQA (Talmor et al., 2019) is a multiple
choice task posed over commonsense questions.
Crowdsourced free-text explanations for instances
in CommonsenseQA are provided in the CoS-E
v1.11 (Rajani et al., 2019) and ECQA (Aggarwal
et al., 2021) datasets. ECQA explanations are coun-
terfactual, i.e., annotators were instructed to ex-
plain not only the correct answer choice but also
why the others are incorrect. ECQA was released to
address the quality issues of CoS-E (Narang et al.,
2020); for completeness, we experiment with both.
Our second task is NLI, which involves inferring
whether a given hypothesis sentence entails, contra-
dicts, or is neutral towards a premise. This task is
instantiated with the SNLI dataset (Bowman et al.,
2015) and crowdsourced explanations from e-SNLI
(Camburu et al., 2018). For each task, we report
results on a ﬁxed, randomly-sampled 250-instance
test set not observed during prompt design.
Few-shot prompting for explanations. We use
GPT-3 Davinci(Brown et al., 2020), an autore-
gressive language model with ~175B parameters
trained on a large dataset of text scraped from the
internet. We prompt the model with several (ques-
tion, gold answer, explanation) triplets followed by
an unexplained question-gold answer instance for633
which we expect the model to generate an explana-
tion.We use a total of 115 randomly sampled train
instances to create our prompts; each prompt con-
sists of 8-24 randomly selected examples from this
set. For each instance, we generate a single expla-
nation with greedy decoding. More details about
prompt construction are in Appendix A; example
prompts are given in Tables 2 and 11.
Crowdsourcing evaluation. Given that existing
automatic metrics often do not correlate well with
human judgements of explanation quality (Clinciu
et al., 2021; Kayser et al., 2021), we conduct human
studies for evaluation.We ensure each experiment
has a substantial number of distinct crowdworkers
to mitigate individual annotator bias (Table 17).
We present workers with a dataset instance, its
gold label, and two explanations for the instance
generated under different conditions (“head-to-
head”). We then ask them to make a preferential
selection, collecting 3 annotations per data point.
We report inter-annotator agreement (IAA) using
Krippendorff’s α(Krippendorff, 2011). We ﬁnd
low-to-moderate agreement across studies, indicat-
ing the subjective nature of the task; see Tables 3,
4, and 5. Appendix B contains further details on
quality control.634
2.1 Are GPT-3 explanations preferred over
crowdsourced ones?
We perform a head-to-head comparison of expla-
nations generated by GPT-3 with greedy decod-
ing vs. gold human-written explanations in the
original datasets. The crowdsourced explanations
serve as a reasonable upper bound for what a su-
pervised explanation generation model trained on
them could produce. Table 1 contains examples of
GPT-3-preferred explanations.
Results are shown in Table 3. GPT-3 greedily-
decoded explanations are frequently preferred or
comparable to crowdsourced explanations in CoS-
E, which is not too surprising given the dataset has
many ungrammatical explanations (Narang et al.,
2020). And, while ECQA and e-SNLI explanations
are strongly preferred to GPT-3, there are still a non-
trivial number of cases where GPT-3 explanations
are competitive (47.3% and 36.4%, respectively).
2.2 Can improving prompt quality improve
GPT-3-generated explanations?
Given that low-quality training instances may re-
sult in low-quality predictions (especially in a few
shot setting),we ask: can we improve GPT-3 gen-
erations simply by conditioning on higher-quality
instances? To this end, we replace the 115 crowd-
sourced explanations from the original datasets for
prompting GPT-3 with explanations carefully writ-
ten by the authors of this paper (see Table 12 for
examples). Our prompts are used to generate a dif-
ferent set of GPT-3 explanations on the same test
data.
We perform a head-to-head human evaluation
of the GPT-3 generations conditioned on the ex-
planations we wrote vs. those conditioned on the
crowdsourced explanations. Results in Table 4
show that, for all three corpora, generations condi-
tioned on our explanations outperform generations
conditioned on crowdsourced ones, illustrating the
importance of good-quality prompts for GPT-3.
We repeat the experiment of §2.1, but with our
prompts instead of dataset prompts. With this
change, GPT-3 generations are even more com-
petitive (Table 5). For all three datasets, more than
half the time, few-shot prompting results in an ex-
planation at least as good as a human-written ex-
planation. For subsequent experiments, we prompt
GPT-3 with the author-written explanations.
2.3 What types of explanations does GPT-3
generate?
Pairwise evaluations can only offer perspective on
the relative quality of generated explanations. Are
crowd annotators simply comparing explanations
on surface-level features like grammaticality?
To understand ﬁner-grained characteristics of ex-
planations, we design a second human study to col-
lect absolute Likert-scale judgments across seven
axes of quality (with each explanation judged by
3 annotators). The ﬁrst three axes capture surface-
level features: generality, grammaticality, and fac-
tuality. The next three capture richer aspects of
explanation quality: whether new information is
introduced (a requirement for non-vacuous explana-635
tions), whether explanations support the gold label,
and whether the amount of information given is
sufﬁcient. Finally, we ask for an overall judgement
of quality: is the explanation acceptable ? We ex-
plain our design process in Appendix B.3. Results
on the crowdsourced and GPT-3 explanations for
both tasks are given in Figure 2.
For both tasks, GPT-3 explanations do well in
all 3 surface-level categories, with statistically sig-
niﬁcantly greater ratings in generality and gram-
maticality (and factuality for CommonsenseQA)
compared to crowdsourced explanations, and dis-
tributional means close to 1. In these categories,
there is little room for improvement.
On the other hand, GPT-3 explanations do not
contain as much new information as ECQA and
e-SNLI explanations, indicating substantial room
for improvement (mean= 0.1for both tasks com-
pared to 0.6for ECQA and 0.2for SNLI; these
differences are statistically signiﬁcant at p≤0.01).
GPT-3 explanations are substantially more support-
ive of the label vs. CoS-E, but not as supportive
as ECQA or e-SNLI (all statistically signiﬁcant
atp≤0.1). Indeed, the mean rating of GPT-3
explanations for label support is 0.5for Common-senseQA and−0.1for NLI, demonstrating room
for improvement. These axes are crucial to ensur-
ing explanations are not vacuous and are on-topic.
Finally, GPT-3 explanations are judged as accept-
able at higher rates than CoS-E or ECQA explana-
tions, but not e-SNLI explanations. Mean scores of
0.5(CommonsenseQA) and 0.0(NLI) indicate that
GPT-3 explanations have room to improve overall.
Correlation between acceptability and other at-
tributes To understand what factors are impor-
tant for the overall “acceptability" judgement, we
compute Spearman correlation ( ρ; Spearman, 1987)
between acceptability and all other attributes (Ta-
ble 6). Each is positively correlated with accept-
ability, though with varying degrees of magnitude.
Acceptability is least correlated with “new infor-
mation," and most correlated with grammaticality,
generality, and the explanation’s support for the
label. Overall, the results indicate that human pref-
erence for explanations is not fully explained by
any one attribute, and is not limited to surface-level
features.636
3 Beyond Greedy Explanations
While GPT-3 explanations demonstrate strength
across surface-level features and are surprisingly
competitive in head-to-head settings, they can still
be improved. One might imagine a setting with
multiple end-users in which we wish to provide
the most unambiguously acceptable explanation
as output from a system. When considering the
data from §2.3, we ﬁnd that only 46.3%of the
greedily-decoded GPT-3-generated explanations
for CommonsenseQA and 31.5%for NLI are rated
acceptable by3/3annotators .
Inspired by work in other generation tasks
(Holtzman et al., 2020; Massarelli et al., 2020;
Holtzman et al., 2021), we hypothesize that equally
or more informative explanations can be generated
by sampling stochastically. We sample 4 additional
generations from GPT-3 for each instance to com-
plement the greedy generation. We crowdsource 3
acceptability annotations for each new explanation.
As expected, sampled explanations exhibit lower
3/3acceptability than greedy explanations ( 25.1%
for CommonsenseQA; 11.3%for SNLI). However,
this results in a surprisingly higher proportion of
instances that have at least one acceptable expla-
nation in the set of 5. The greedy explanation was
judged to be 3/3acceptable in 46.3%of instances
for CommonsenseQA and 31.5%for NLI; this in-
creases to 79.5%and 51.2%, respectively, when
sampled explanations are included.
4 Improving Explanation Generation
with Acceptability Filtering
The challenge of overgeneration is that GPT-3
alone cannot discern which of its stochastic samplesare acceptable. Inspired by West et al. (2021), we
explore training a supervised ﬁlter on the collected
labels. Our key intuition is that by re-framing the
role of annotators from explanation authors to bi-
nary judges, we can alleviate the need to collect
a large-scale explanations dataset—the result is a
simpler, cheaper, and easier crowdsourcing setup
to administer (§4.1). Namely, we can (1) aggregate
ratings over multiple annotators to produce more
reliable labels, (2) use numerical metrics of anno-
tator agreement to remove annotators contributing
noisy data, and (3) collect annotations more quickly
and cheaply than asking annotators to hand-write
explanations. Moreover, we ﬁnd that the ﬁlter can
be trained with a relatively small amount of binary
human judgments (§4.2). We demonstrate that the
trained model is not simply taking advantage of
surface-level features (§4.4). Figure 1 presents an
overview of our pipeline.
4.1 Acceptability Annotations
We generate train and validation sets by repeating
the procedure of generating 1 greedy and 4 sam-
pled explanations for 991 and 1K instances, respec-
tively, of the CommonsenseQA and SNLI training
sets. Combining these with the annotated test sets
from previous experiments results in a dataset of
1241/1250 instances in a 72/8/20% train/val/test ra-
tio for each task. We again collect 3 binary accept-
ability ratings for each instance, resulting in ~ 6200
instance-explanation pairs and ~19K individual an-
notations per task. Table 13 contains statistics. To
ensure that models trained on these corpora do not
overﬁt to speciﬁc annotators (Geva et al., 2019), we
collect an additional set of judgments for the test
set of SNLI from a group of annotators who did
not participate in any of our previous annotation
tasks (“Test2”). Figure 9 and Figure 10 show the
user interface.
While we evaluate at test-time with the schema
that only instances that 3/3annotators deem ac-
ceptable are considered acceptable, preliminary
experiments show that treating both 2/3and3/3
agreement instances as acceptable during training
performs best on the 3/3evaluation criterion at
test-time.We also train a variant where we ran-
domly select one annotation from the three as the
gold label (“without human agreement”).6374.2 Acceptability Filter
Concretely, given the problem instance, the gold la-
bel, and a generated explanation, the acceptability
ﬁlter predicts whether the explanation is acceptable.
We ﬁne-tune two sequence-to-sequence architec-
tures, T5-Large (Raffel et al., 2020) and T0-3B
(Sanh et al., 2022). Each model is trained 5x with
different random seeds. Further training details are
given in Appendix E.
Baselines. We train an explanation-only base-
line, which receives as input only the explanation;
similar baselines have been proposed for NLI (Po-
liak et al., 2018; Gururangan et al., 2018). These
models represent the hypothesis that annotator rat-
ings can be reconstructed with only surface features
of the explanation candidates, e.g., grammatical-
ity. We also consider a negative log-likelihood
(NLL) baseline, which uses GPT-3’s estimated
probability as the acceptability classiﬁcation score.
This is a slightly more competitive baseline than
greedy; greedy usually (but not always) produces
the highest-likelihood explanation.
4.3 Evaluation
We consider three evaluation settings. The ﬁrst
isinstance-level (“select-1”), where the system
returns 1 explanation selected from the set of 5
for each instance. We return the explanation with
the highest model-estimated probability and report
instance-level accuracy, i.e., the % of instances for
which a gold acceptable explanation is selected.
We also evaluate at the explanation-level ,
where we treat each explanation independently and
compute metrics over the full dataset. This aligns
with the binary classiﬁcation training of the mod-
els (cross-entropy on the explanation labels) and
is suited for the setting in which we want to return
allof the acceptable explanations per instance. In
this setting, we report average precision (AP) , an
estimate of area under the precision-recall curve.
Finally, we perform an absolute human evalu-
ation (§2.3) on the subset of instances where the
ﬁlter model does notselect the greedy explanation
as the best, i.e., comparing “select-1” performance
to a greedy baseline on the instances where it dif-
fers. We additionally re-perform the head-to-head
comparison of Table 5, replacing the greedy GPT-3
explanations with those selected by the ﬁlter.
4.4 Results
Classiﬁer performance is given in Tables 7-8.
Effect of model size. On CommonsenseQA, T0-
3B outperforms T5-Large by ~2-4% select-1 accu-
racy and ~5-6% explanation-level AP across splits.
We use T0-3B in subsequent experiments.
NLL baseline vs. full model. For both tasks
on both validation and test sets, T0-3B outper-
forms the NLL baseline substantially. On Common-
senseQA, we observe a 7-8% gain in instance-level
accuracy, and a gain of 18% explanation-level AP
on the test set. This provides strong evidence that
the supervised model is able to incorporate binary
human feedback to predict acceptable explanations
at a level much higher that GPT-3 achieves on its
own. We present examples where “select-1” pre-
dictions differ between NLL and our ﬁlter model
in Table 10 and Table 16.
Explanation only vs. full model. Our results
suggest that our models are leveraging feature in-
teractions between the instance and explanation to
make their predictions: without instance-level con-
text, the explanation-only baselines are on average
more than 5 points worse across metrics. Though638
they under-perform signiﬁcantly relative to the full
model, explanation-only baselines do fare surpris-
ingly well, indicating that shallow features like
factuality and grammaticality may represent latent
factors in human acceptability judgments.
The effect of multiple training annotations. In
some cases, performance improves if the training
instances are labeled with the consensus of three
annotators (vs. the singularly annotated case “w/o
HA"), though the effects are inconsistent. In most
cases, using consensus labels results in reduced
variance across random seeds. However, the gains
may not outweigh the 3x annotations required.
Our model doesn’t overﬁt to speciﬁc annota-
tors. The performance of our model when evalu-
ated on the NLI test set labeled by separate anno-
tators (“Test2”) is comparable to the original test
set (instance-level accuracy drops a few points, but
explanation-level AP slightly rises).
Our model improves generated explanations
along desirable traits. We present our absolute
human evaluation for greedy vs. ﬁltered expla-
nations from GPT-3 in Figure 3— for both tasks,
explanations ﬁltered by our model more readily
introduce new information, support the label, and
contain at least enough information for both tasks
(in addition to being more acceptable). Interest-
ingly, greedy explanations still prevail in surface-
level features (grammaticality and, in the case of
CommonsenseQA, factuality; differences are sta-
tistically signiﬁcant with low p, see Table 20).
We additionally ﬁnd in our head-to-head study639(Table 9) that, compared to Table 5, using ﬁltered
GPT-3 explanations instead of greedy increases the
preference for GPT-3 explanations by 15-24% for
both CommonsenseQA datasets. We do not see
an increase in the SNLI case, which may be due
to the fact that fewer GPT-3 explanations change
after ﬁltering (36.4%, compared to 62.4% for Com-
monsenseQA), and GPT-3 explanations for SNLI
tend to be less acceptable overall, resulting in a
lower upper-bound oracle of instances where an
acceptable explanation can be selected (§3).
In summary. We have demonstrated the effec-
tiveness of modeling binary crowd judgements of
acceptability as a means to select candidates from
GPT-3 which are deemed acceptable with unan-
imous agreement. For the method that does not
leverage human agreement, this is done with only
~5k binary annotations. We additionally demon-
strate that our ﬁltered explanations improve upon
greedy generations in ﬁne-grained categories that
probe their topical relevance and meaningful con-
tent. The gap between our best model and the
upper-bound oracle indicates that there is still sub-
stantial room for improvement in both task settings
(but especially for SNLI). Future work may investi-
gate sampling more explanations, or incorporating
other sources of supervision signal.
5 Related Work
Free-text explanation generation. The earliest
neural free-text explanation models did so for com-
puter vision applications (Hendricks et al., 2016;
Park et al., 2018; Kim et al., 2018) and NLI (Cam-
buru et al., 2018). These methods relied on super-
vised datasets to train the explanation generator.
Others have proposed to generate explanations or
clariﬁcations to improve task performance in a su-
pervised (Rajani et al., 2019; Lampinen et al., 2022)
or unsupervised (Shwartz et al., 2020) manner. Yor-
danov et al. (2021) study transfer learning between
datasets for few-shot generation.
Latcinnik and Berant (2020) proposed a method
to generate free-text explanations supervised only
on task signal, and Brahman et al. (2021) used
sources of weak supervision to generate expla-
nations for defeasible inference. Paranjape et al.
(2021) design hand-crafted templates which they
use with mask-inﬁlling to produce contrastive ex-
planations from pretrained language models.
Concurrent work (Marasovi ´c et al., 2021) stud-
ies the effect of prompt format and model size oncrowdworker judgements of prompted explanation
plausibility. They ﬁnd that GPT-3 Davinci out-
performs other smaller pretrained models, but that
crowdworkers ﬁnd these explanations less plausi-
ble than those from the datasets, aligning with our
ﬁrst experimental result (Table 3). We perform
a more in-depth study of the ﬁne-grained criteria
comprising human acceptability, and demonstrate
that with higher-quality prompts and ﬁltering, GPT-
3’s performance can be signiﬁcantly improved.
Supervising on human preferences. Prior and
concurrent work has used binary judgements from
crowdworkers to ﬁt models to human preferences
for non-XAI tasks such as summarization (Ziegler
et al., 2020; Stiennon et al., 2020), creating com-
monsense knowledge bases (West et al., 2021), and
building natural language inference datasets (Liu
et al., 2022). Unlike these works, we apply human
preference modeling to increase the human accept-
ability of model-generated free-text explanations.
West et al. (2021) demonstrate that GPT-3 + a super-
vised acceptability ﬁlter can generate a high-quality
causal knowledge graph: in addition to their work
being conducted in a different domain, our success
conditions and evaluation metrics differ because
wemust produce a prediction for each instance
(whereas they can simply discard bad generations).
6 Conclusion
We demonstrate GPT-3’s capacity to generate free-
text explanations for NLP task instances in a few-
shot setting. We further improve this capability via
an overgenerate + ﬁlter approach, where the ﬁlter
is trained on supervision from human acceptabil-
ity ratings. We hope our results can guide future
work on free-text explanations via neural or neuro-
symbolic systems (Brahman et al., 2021; Majumder
et al., 2021; Saha et al., 2021). Future work may
also further investigate the beneﬁts of counterfac-
tual explanations.
While human rationales for decision making are
not necessarily the same as model rationales, the
goal behind modeling human acceptability is often
to build trust with a human user. This trust may or
may not be warranted (Jacovi et al., 2021); future
work would be well-suited to further investigate
generated explanations for incorrect label predic-
tions such as in Appendix C, which could mislead
end users.640Acknowledgements
We thank Jena Hwang for helpful advice in de-
signing our human user studies and Peter West for
sharing GPT-3 prompting scripts. We thank other
members of the Mosaic team at the Allen Institute
for AI for valuable suggestions, as well as both our
anonymous annotators and reviewers. This work
was done while SW was an intern on Mosaic.
7 Ethics & Broader Impacts
All datasets used in this work are public, and we
plan to release the machine-generated explanations
and annotations we collected. We do not collect any
personal information from our human participants.
Models that produce explanations in the means
used in our experimental protocol (i.e., by condi-
tioning on the gold labels) have the possibility to
cause humans to place unwarranted trust in an AI
system. This line of research is complementary
to works investigating the faithfulness of model-
generated free-text explanations (Hase et al., 2020;
Wiegreffe et al., 2021). We demonstrate in Ap-
pendix C that GPT-3’s explanations lack reliability
because the model can explain answer choices that
were not its prediction equally well. This may be
due in part to the fact that decoding algorithms
for generating predictions from language models
are sub-optimal (e.g., Zhao et al., 2021; Holtzman
et al., 2021) and GPT-3 may have factual knowl-
edge stored in its parameters about other answer
choices that allow it to provide reasonably accept-
able explanations. Until this phenomenon is bet-
ter understood, we do not condone using GPT-3-
generated explanations in real-world deployment.
Lastly, our model of human acceptability is
based on the aggregate judgements of participants
from primarily Western, English-speaking coun-
tries working on crowdsourcing platforms. The
subjective judgements of explanation acceptability
may vary signiﬁcantly among different population
groups.
References641642643
A Prompt Construction
Following Perez et al. (2021), we avoid prompt tun-
ing on the full training and development sets of the
datasets studied, in order to ensure that our methods
represent a true few-shot setting. To develop the ini-
tial prompt design, we experimented with no more
than 10 different layouts in the GPT-3 Sandbox
platform using 15 training examples on the CoS-E
and e-SNLI datasets. For subsequent prompt de-
sign, we again used these 15 training examples for
each dataset from which we sampled 6 prompts,644
along with a ﬁxed 100-example “development set”
randomly drawn from the training set. We preserve
the “few-shot” approach by using a maximum of
these same 115 instances to develop our prompt-
ing methods. For these 115 examples, the authors
of this paper manually wrote high-quality expla-
nations to be used as prompt examples (Table 12).
As presented in Table 2, we found that structuring
SNLI as a question-answering task achieved the
best performance, similarly to Zhao et al. (2021).
We provide an example of our SNLI prompt in
Table 2 and CommonsenseQA in Table 11.
In-context learning methods have been shown to
have high variance based on hyperparameters in-
cluding example order, number of examples given,
and which examples are given (Jiang et al., 2020;
Zhao et al., 2021; Lu et al., 2022). While thesevalues have not been standardized, two prominent
papers, Schick and Schütze (2021b) and Brown
et al. (2020), use 32 and 64 prompt examples, re-
spectively. Due to the 2049-token limit of the Ope-
nAI GPT-3 API and the fact that the addition of
explanations elongates each prompt instance, we
ﬁnd the maximum number of examples the API
can accommodate is 24 for CoS-E, e-SNLI, and
our handwritten explanations and 16 for ECQA.
The focus of this work is not on ﬁnding the opti-
mal prompt, but on developing a general strategy
for few-shot explanation generation that could be
successful when no additional (large) validation
set for tuning is available. Therefore, to provide
as robust of an expected performance estimate as
possible, we do not tune the additional hyperpa-
rameters, instead sampling them to approximate
performance.Namely, while prior work uses one
ﬁxed prompt for all instances and varies the random
seed, we approximate the same expected perfor-
mance by sampling a new set of prompts for each
instance. We also sample the number of prompts
for each instance (and shufﬂe their order) from
the values{8,16,24}for CommonsenseQA exper-
iments,{8,16}for experiments using ECQA ex-
planations, and{12,18,24}for SNLI experiments
(to maintain label balance). To overcome label
bias in prompt ordering, for tasks with distinct an-
swer choices per instance (CommonsenseQA), we
shufﬂe the answer choices. For tasks with ﬁxed an-
swer choices (SNLI), we sample an equal number
of prompt instances for each label (so number of
prompt instances is a multiple of 3).
Table 12 shows a few non-cherry-picked ex-
amples of our handwritten explanations used as
prompts relative to the datasets.
B Crowdsourcing Details
We discuss shared details of the study designs in
§B.1. We discuss the head-to-head interface in
§B.2, the absolute interface in §B.3, and the ac-
ceptability interface in §B.4. Finally, we present
details on quality control and payment in §B.5 and
annotator statistics in §B.6.
B.1 Shared Interface Details
For all three human subjects study designs de-
signs, we show the user the input instance (e.g.,645
premise +hypothesis) and the gold label in addition
to the explanation(s). We explain our motivation
for using the gold label as a methodological control
in §2.
For a similar reason, we do not show the other
incorrect label choices to the user, which is particu-larly of note for the CommonsenseQA task which
has different answer choices per instance. Some
instances in CommonsenseQA have multiple cor-
rect or very similar answer choices, due to noise
in the dataset and the fact that the wrong answer
choices were deliberately collected to make the
task challenging. We (the authors) again found we
struggled to accurately judge explanation quality
when we disagreed with the selected answer choice
or found multiple answer choices to be correct. To
remove this possible confounder, we instruct par-
ticipants to pretend the gold label is correct even if
they disagree with it, and make this easier by hid-
ing the other answer choices. This may result in a
slight bias in judgements against the ECQA dataset
due to its unique counterfactual nature, though our
goal was not to study the beneﬁts and downsides
of counterfactual explanations in this work.
B.2 Head-to-Head Interface Details
We show the user the task input and gold label,
and ask them to select which of two explanations
best explains the answer. We instruct workers to
consider the gold label to be correct even if they
disagree with it (CommonsenseQA instances can
be subjective) and to ignore minor grammar and
spelling mistakes such as improper upper-casing.
Figures 5 and 6 show the evaluation interface.
B.3 Absolute Interface Details
Figures 7 and 8 show the absolute evaluation inter-
face (minus the acceptability attribute, which is col-
lected in a separate run of the study). Our interface
is inspired by prior work from psychology and the
social sciences (Leake, 1991; Gopnik, 1998; Lom-
brozo, 2007; Zemla et al., 2017; Chiyah Garcia
et al., 2018; Clinciu et al., 2021; Sulik et al., 2021).
We iterated over 3-4 versions of the questions and
UI design until we had optimized agreement rates
as much as possible. Our resulting two-part evalua-
tion consists of 7 questions:
Part 1: Context-Independent Evaluation We
ﬁrst assess the explanation in isolation, i.e., these
questions are presented to the user without reveal-
ing the question/context that the explanation is at-
tempting to address:
1.How factual is this statement? (generally false,
sometimes or partially true, generally true, or
need more information to judge). This question
is designed to test both generality (can the expla-
nation’s truthfulness be ascertained or is more in-646formation needed?) and factuality, which aligns
with “compatibility with receiver’s existing be-
liefs” and that the best explanation is the “most
likely” explanation from the receiver/user’s per-
spective (Lombrozo, 2007; Zemla et al., 2017;
Sulik et al., 2021). Generality is coded based
on whether a truthfulness answer is selected
(considered to be general) or whether the “need
more information to judge” choice is selected
(considered not to be general).
2.Is this statement grammatical? (yes or no) This
question is designed to test for clarity, aligning
with characteristics such as coherence (Lei et al.,
2016) and human-likeness and understandability
(Ehsan et al., 2019).
Part 2: Context-Dependent Evaluation We
next show the user the question (premise and hy-
pothesis for SNLI) and gold answer that the expla-
nation was conditioned on. We then ask:
1.Does the explanation provide new facts, infor-
mation or reasoning not stated in the question
and answer? (yes or no) In our preliminary
experiments, we found some explanations sim-
ply restate the question declaratively with the
answer ﬁlled in. This question addresses the dis-
tinction between “validity” and “utility” (Leake,
1991): an explanation can be valid (i.e., a restate-
ment of the question with the answer ﬁlled-in
might be correct), but not useful; utility is de-
ﬁned by whether an explanation “satisﬁes an
explainer’s need for information”. And while
utility is best understood in the context of real-
world applications (Lai et al., 2020), we nonethe-
less aim to identify vacuous explanations that
do not provide new information.
2.Is the new information relevant to justifying the
answer? (yes or no) New information, if pro-
vided, “should be compatible with our existing
beliefs, and consistent with the evidence and
with itself” (Zemla et al., 2017). This ques-
tion is designed to test whether the information
provided supports the label. The speciﬁc inter-
pretation of “relevance” is purposefully left to
the annotator.
3.How much information does the explanation
have to justify the answer? (not enough, enough,
or too much) This question is designed to testthe extent to which the provided novel informa-
tion is adequate orsufﬁcient (Kim et al., 2016;
Lei et al., 2016; Ehsan et al., 2019).
4.Is the explanation acceptable? (yes or no) The
ﬁnal question is designed to assess annotators’
overall judgement of the explanation as a whole.
We only ask Question 2 if the answer to Question 1
is “yes” and Question 3 if the answer to Question
2 is yes, because they regard the new facts, infor-
mation, or reasoning. We found that most prior
work tends to lump added-value, relevance, and
adequacy judgements into one “informativeness”
judgement (Clinciu et al., 2021), which we felt was
too course to allow for meaningful error analysis.
B.4 Acceptability Interface Details
Figures 9 and 10 show the binary acceptability
interface used to collect training and test data for
the overgeneration ﬁlter model.
Spearman’s rank-order correlation coefﬁcients
(Table 6) are computed using scipy (Virtanen et al.,
2020) on the 250 test explanations from the 5 data
sources in Figure 2. Each instance is annotated by
3 annotators for a total of 3750 datapoints (some
criteria are only evaluated conditionally, resulting
in less total annotations– see Appendix B.3). Sta-
tistical signiﬁcance is computed using the built-in
two-sided non-parametric test.
B.5 Quality Control and Payment
We use Amazon Mechanical Turk (AMT), and cal-
culate pay on a rate of $15/hour. Every few batches,
we check to ensure that the median time taken per-
annotator amounts to approximately this pay rate.
While annotators do tend to speed up the more
HITs we released, ﬁrst-round median times were
approximately 30 seconds per head-to-head evalu-
ation HIT (thus paid at $0.12 each), 1 minute per
absolute evaluation HIT (thus paid at $0.25 each),
and 35-39 seconds per acceptability HIT (5 expla-
nations; paid at $0.20 each).
We require annotators to be located in either Aus-
tralia, Canada, New Zealand, the United Kingdom,
or the United States, as a proxy for English compe-
tency.We require a past HIT approval rate of >64798% and>5000 HITs approved. We do not allow
annotators to participate who were previously on a
block list from our past AMT studies.
Annotators must complete a qualifying exam in
order to participate in the main annotation tasks.
The qualifying exam consists of 3 HITs in the same
format as the main absolute evaluation task for
CommonsenseQA We pay $2.25 for the qualify-
ing exam. There are 9-18 questions in total (3-6
questions per HIT), some of which are only answer-
able conditioned on previous answers. A user who
answers “no” to question 3, for example, will not
be asked to answer questions 4 and 5. Given the
challenging and sometimes ambiguous nature of
some of the questions, for the ﬁrst run of the quali-
ﬁcation exam, we manually awarded qualiﬁcations
by inspecting the annotators’ answers. Scores for
the ﬁrst run compared to our answers (out of 17
annotators attempting) ranged from 5 to 14 out of
18. The median accuracy was 11 out of 18, and we
ﬁnd that awarding the qualiﬁcation to those with
scores at or above the median aligns closely with
our manual inspection. We thus use this score to
assign qualiﬁcations in future iterations.
Because it is necessary that annotators under-
stand the task before they can evaluate explanation
quality (Wiegreffe and Marasovi ´c, 2021), for tasks
that are more difﬁcult, i.e., NLI, we additionally
require annotators to pass (score of 7 or above) a
task-speciﬁc qualiﬁcation exam with 8 questions,
paid at $1.25.
In order to track quality throughout evaluation,
we compute inter-annotator agreement using Krip-
pendorff’sαand use a hidden built-in Javascript
function to compute time per HIT spent. If any an-
notator completed the tasks in an unreasonably low
time, or removing their annotations substantially
improves Krippendorff’s α, we remove their anno-
tations and re-annotate their instances. We addition-
ally ensure that each experiment has a substantial
number of distinct crowdworkers to mitigate indi-
vidual annotator bias, reporting this as well as the
mean and median number of HITs completed by
each in Table 17.
B.6 Statistics
The number of distinct crowd annotators and the
median and mean number of HITs completed for
each experiment can be found in Table 17. More
detailed breakdowns of inter-annotator agreement
for some experiments are in Tables 14 and 15.
C Absolute Evaluation by Label
Accuracy
Can GPT-3 produce convincing explanations even
for instances it cannot predict correctly? This has
implications for model-generated explanations be-
ing “right for the right reasons”. To produce label
predictions, we follow the same prompt format as
in Tables 2 and 11, removing the ?token
and the gold explanations so that the model gen-
erates a label prediction instead. GPT-3 achieves
50.8% accuracy on CommonsenseQA compared
to a 20% random baseline, and 46% accuracy on
SNLI compared to a 33.33% random baseline.
Figure 4 presents absolute evaluation results bro-
ken down by whether GPT-3 correctly predicts the
instance label. The results show little variation
between the correctly-predicted and incorrectly-
predicted groups in most attributes. This indicates
that GPT-3 explanations are not faithful enough to
use in real-world applications in their current form.
D2/3Acceptability Statistics
When we treat explanations rated by at least 2/3
annotators as “acceptable”, for CommonsenseQA,
77.9%of greedily-decoded explanations are accept-
able; for SNLI, 51.0%.50.5%of sampled expla-
nations are acceptable; for SNLI, 23.5%. Out of
the set of 5 (1 greedy + 4 stochastic), 97.7%of
CommonsenseQA instances have at least one ac-
ceptable explanation, and 79.5%of SNLI.648
E Filter Model Details
We split the 4,955 distinct annotated explana-
tions for CommonsenseQA (5000 for SNLI) into
a train/dev set of 4500/455 (4500/500 for SNLI),
where all 5 explanations for a given instance are
placed in the same set to avoid leakage. We
present statistics on the label distribution in Ta-
ble 13. Along with the metric settings reported in
the paper (“select-1” and explanation-level), we
computed a metric that is instance-level but con-
siders all explanations by computing metrics over
the 5 explanations of an instance and then averag-
ing across instances, ﬁnding in practice that the
results are highly similar to the explanation-level
evaluation.
We use Huggingface Datasets (Lhoest et al.,
2021) and Huggingface Transformers (Wolf et al.,
2020) for implementation. We format inputs to the
models as follows, where expl is one of the ﬁve
explanations and the gold_label is either 0 (not
acceptable) or 1 (acceptable):
The T5-Large model is trained using a learning
rate of 1E−4with linear decay, a batch size of
64, and default values for Adam (Kingma and Ba,
2015), gradient clipping, and dropout. We train for
a maximum 200epochs, performing early stopping
on the validation loss with a patience of 10epochs.
For T0-3B, we train with a batch size of 50. We
use AdaFactor (Shazeer and Stern, 2018) with a
linear warmup of 500 steps. We conduct an initial
hyperparameter sweep over learning rate, consid-
ering 1E−5,5E−05,5E−06. The learning
rate that achieves the best validation loss for the
full-information model and the explanation-only
model is 1E−5, which we use for all training
experiments.
F Additional Filter Results
In the main experiments, at evaluation time, we
labelled an explanation as acceptable if 3/3anno-
tators agreed on it. Here, we report results if this
threshold is relaxed to 2/3. Overall, the results
are comparable: T0-3B outperforms the baselines
according to both select-1 accuracy and AP (see
Table 21 and Table 22).649650651652653654655656657658