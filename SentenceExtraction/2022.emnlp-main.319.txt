
Peng Xu, Mostofa Patwary, Shrimai Prabhumoye, Virginia Adams, Ryan J. Prenger,
Wei Ping, Nayeon Lee, Mohammad Shoeybi, Bryan CatanzaroThe Hong Kong University of Science and Technology,NVIDIA
pengx@nvidia.com
Abstract
Parameter efficient learning methods ( PERM s)
have recently gained significant attention as
they provide an efficient way for pre-trained
language models (PLMs) to adapt to a down-
stream task. However, these conclusions are
mostly drawn from in-domain evaluations over
the full training set. In this paper, we present
comparisons between PERM s and finetuning
from three new perspectives: (1) the effect of
sample and model size to in-domain evalua-
tions, (2) generalization to unseen domains and
new datasets, and (3) the faithfulness of gen-
erations. Our results show that for in-domain
settings (a) there is a cross point of sample
size for which PERM s will perform better than
finetuning when training with fewer samples,
and (b) larger PLMs have larger cross points.
For cross-domain and cross-dataset cases, we
show that (a) Adapter (Houlsby et al., 2019)
performs the best amongst all the PERM s stud-
ied here, and (b) it outperforms finetuning if
the task dataset is below a certain size. We
also compare the faithfulness of generations
and show that PERM s can achieve better faith-
fulness score than finetuning, especially for
small training set, by as much as 6%. Finally,
we apply Adapter to MT-NLG 530b (Smith
et al., 2022) and achieve new state-of-the-art
results on Xsum (Narayan et al., 2018) for all
ROUGE scores (ROUGE-1 49.17, ROUGE-2
27.20, ROUGE-L 40.98).
1 Introduction
Parameter efficient learning methods ( PERM s)
serve as potential alternatives to finetuning for
adapting and deploying language models in real
world scenarios (Ding et al., 2022). They allow
users to finetune only a small number of parame-
ters while freezing the rest of the shared parameters
of pre-trained language models (PLMs). This is
especially important for large language models (e.g.
GPT-3 (Brown et al., 2020) and MT-NLG (Smith
et al., 2022)) as finetuning the entire model willbe very expensive or infeasible due to their model
size.
Prefix tuning (Li and Liang, 2021), which is one
of the PERM s, draws inspiration from prompting
and introduces a small set of continuous vectors
as virtual prompts to allow subsequent tokens to
attend to, which obtains comparable performance
to finetuning in the full data setting. Prompt tun-
ing (Lester et al., 2021) shows the power of scal-
ing PLMs and that tuning only a few extra em-
beddings is sufficient to achieve similar perfor-
mance to finetuning the entire 11b T5-XXL (Raf-
fel et al., 2020) model. P-tuning v2 (Liu et al.,
2022a) further demonstrates that small PLMs can
also achieve comparable results to finetuning with
Prefix tuning. Different from adding new param-
eters through prompts, Adapter (Houlsby et al.,
2019) injects trainable parameters through low-
rank structure in a skip-connection way. Other
PERM s includes LoRA (Hu et al., 2021), Mix-
And-Match adapter (He et al., 2021a), Compactor
(Karimi Mahabadi et al., 2021), BitFit (Zaken et al.,
2022), diff-pruning (Guo et al., 2021) and etc.
Most conclusions about PERM s so far are drawn
from their in-domain evaluations over full training
samples. To the best of our knowledge, it is not
yet investigated (1) how these conclusions apply
to different training sizes and model sizes, and (2)
how PERM s generalize to unseen domains and
new datasets, which are both important aspects for
deploying PERMs in real-world applications.
In addition, faithfulness in natural language gen-
eration has become an important topic as it is vital
to real-world applications. Various efforts are made
to systematically measure and mitigate factual er-
rors in many generation tasks, including summa-
rization (Huang et al., 2021) and dialogue gener-
ations (Rashkin et al., 2021; Shuster et al., 2021;
Dziri et al., 2021; Wu et al., 2021). However, exist-
ing work on faithfulness only focuses on faithful-
ness of finetuning, and the impact of PERM s on4824the faithfulness of generation is not yet explored.
In this paper, we provide an in-depth study of
PERM s for generation tasks through three impor-
tant aspects when deploying PERM s in practical
applications: (1) in-domain evaluation by scaling
both training dataset size and model size of PLMs,
(2) cross-domain and cross-dataset generalization,
and (3) faithfulness assessment. Two generation
tasks are used for evaluation: summarization and
dialogue generation. We study four representative
methods: P-tuning, Prompt tuning, Prefix tuning,
and Adapter, but mainly focus on Prefix tuning
and Adapter as our preliminary results show that
they are better than the others. Our contributions
are summarized as follows: (1) To the best of our
knowledge, we present the first comparisons of
faithfulness for PERM s. Our experimental results
show that PERM s, especially prefix tuning can
achieve better faithfulness than finetuning by up to
6%. (2) For in-domain settings, there is always a
cross point of sample size for which PERM s will
be better than finetuning when training on fewer
samples. Larger PLMs have larger cross points.
Users need to choose which method to use based
on their own training sample size and model size.
(3) Compared to finetuning, not all PERM s can eas-
ily achieve better cross-domain and cross-dataset
scores than finetuning even with 8.3b PLM. Our
results show that Adapter is a better method than
Prefix tuning on 13 out of 15 comparison settings.
(4) New state-of-the-art results on Xsum (Narayan
et al., 2018) are obtained by applying Adapter to
MT-NLG 530b model.
2 Methodology
We compare the following four PERM s tofinetun-
ing(FT) using GPT-style models from Megatron-
LM (Shoeybi et al., 2019). (1) Adapter ( AP)adds
an extra layer with a bottleneck structure by first
projecting input hto a low dimension using train-
able weights W and then projecting up to the
original dimension using trainable weights W.
It is incorporated into backbone model in a skip-
connection way.
Adapter (h) =h+g(hW)W,
where gis the activation function. In our case, we
insert Adapter layer both after the multi-head atten-
tion (MHA) and feedforward layer (FFD) of Trans-
former (Vaswani et al., 2017). (2) Prefix Tuning
(PF)adds trainable prefix tokens at the beginningof each transformer block. We follow the imple-
mentation of Li and Liang (2021) to replace the
keysK, values Vof MHA with the concatenation
of the trainable prefix weights W,Wand the
K, V .
K←concat ([W;K])
V←concat ([W;V])
We also add reparameterization trick suggested by
Li and Liang (2021). (3) Prompt Tuning ( PT)
adds extra parameters to the embedding layer and
uses these trainable embeddings to prompt the in-
put.(4) P-tuning (Liu et al., 2021b) adds a prompt
encoder to encode pseudo prompts and the encoded
representation is used to prompt the input.
3 Experimental Setup
3.1 Datasets
Summarization We use Xsum (Narayan et al.,
2018), a widely used summarization dataset, to
train and evaluate different methods. It con-
sists of 204,017/11,327/11,333 pairs for the train-
ing/validation/test. As Xsum does not divide the
dataset based on topics, we follow Li and Liang
(2021) to split the Xsum dataset into news articles
for training and sports articles for testing. This
cross-domain version has 149,115/8,263/2,823
pairs for training/validation/test. For the cross-
dataset evaluation, we choose the test set from
CNN/Daily Mail (Nallapati et al., 2016). It con-
tains 11,490 samples.
Dialogue We use Wizard of Wazards (WoW) (Di-
nan et al., 2018) dataset for our dialogue generation
task. The modeling of the wizard response is usu-
ally composed of two steps: knowledge retrieval
and response generation. To simplify the prob-
lem, following Rashkin et al. (2021), we ignore the
knowledge retrieval step and take the golden knowl-
edge for the response generation. The response of
the wizard is then used to train the model. For the
cross-dataset evaluation, we use the CMU_DoG
(Zhou et al., 2018) dataset. We test our model over
all test set dialogue turns except the starting one.
3.2 Metrics
Quality Metrics We use ROUGE-1 (R-1),
ROUGE-2 (R-2), ROUGE-L (R-L) (Lin, 2004)
scores to evaluate the generations for summariza-
tion task as it is well adopted in all summarization
tasks. For the dialogue generation task, Dinan et al.4825
(2018) reports both PPL and unigram F1 scores and
better PPL in general gives better F1 scores. Adi-
wardana et al. (2020) also shows high correlation
between PPL and the quality of dialogue based on
human evaluations. We therefore choose to report
PPL as an indicator of the quality of generated dia-
logues. In the Results section, if we say “A is better
than B”, we mean A has a higher ROUGE score
for summarization task or/and a lower PPL score
for dialogue tasks.
Faithfulness Metrics Following Rashkin et al.
(2021), we use a state-of-the-art natural language
interference (NLI) model (Roberta trained on
MNLI (Liu et al., 2019)) to predict whether a
response can be entailed by the given evidence.
We evaluate the faithfulness of generated response
against the concatenation of dialogue history and
the golden knowledge. Entailment score is reported
as the ratio of the samples being predicted as entail-
ment from the NLI model. We use factCC (Kry ´s-
ci´nski et al., 2020) to evaluate the faithfulness for
the Xsum as it has the highest Spearman correlation
with human evaluations (Pagnoni et al., 2021).
4 Results
4.1 In-domain Results
In-domain evaluations are presented in Table 1.
Although Adapter( AP) and Prefix Tuning ( PF) are
better than prompt tuning and p-tuning, they are
still much worse than FT(3.7 lower for R-L). To
better understand when PERM s is better than FT,
we scale both the training sample size and model
size for summarization and dialogue generation
task. As PFandAPare much better than other
PERMs, we focus on those two methods.
The results on Xsum and WoW are shown in
Figure 1a and Figure 1b. Comparing APwith PF,
we find that APis better than PFon 26 out of 31
comparisons. It is also aligned with the conclusion
in Ding et al. (2022). This can be attributed to the
structural bias of Adapter. The skip-connection
structure allows Adapter to add a small deviation
to the activation, which makes the optimization
of the PLM checkpoint smooth. On the contrary,
PFintroduces deviations to the keys and values
of the self-attention module and therefore greatly
varies the activation of each layer. As a result, it
takes more efforts for PFto converge. Another
phenomenon we observed in Figure 1 is that if we
train a 8.3b model with enough training samples
(74k for WoW or 200k for Xsum), the performance
gap between PF,APandFTis quite marginal.
This suggests us to use PERM s instead of FTto
save the cost of deploying 8.3b PLMs.
Comparing FTwith AP, we find there is always
a cross point of sample size where FTis better than4826
AP. This shows that if we have large number of
samples in training set, FTwill work better. But if
the number of samples for the task are small, AP
will be better. Also, this cross point will be larger if
we use larger PLMs. For example, the cross point
for 1.3b model over Xsum is less than 10k samples,
whereas for the 8.3b model, it is 50k samples .
This phenomenon can be attributed to that FTcan
easily overfit when you have large models or few
training samples. It motivates us to use APwhen
you have small dataset or large model to achieve
better in-domain performances.
Interestingly, tuning APwith a 8.3b model
of only 32m extra parameters over 5k samples
achieves much better results than finetuning 357m
model over 100k samples. This means more than
90% task specific parameters can be saved for de-
ployment and more than 97% tasks specific sam-
ples can be reduced for training by sharing the
larger frozen PLMs.
Scaling up to 530b model Since APgets better
performances than other methods, we apply APto
one of the largest GPT model, MT-NLG (Smith
et al., 2022). Table 2 shows that by adding 103m
parameters to a frozen 530b model, we achieves a
new state-of-the-art result on Xsum for all ROUGE
scores (R-1 49.17, R-2 27.20, R-L 40.98). It out-
performs both strong encoder-decoder models (e.g.
T5 (Rothe et al., 2021)), as well as specialized
models (e.g. BRIO (Liu et al., 2022b)). This re-
sult shows that decoder-only model can still beat
encoder-decoder model, but it needs a much larger
model size.
We also study the effects of varying parameter
sizes for PERM s in Table 3. We found that the
score of AP increases for 1.3b model, which sug-
gests the model is under-fitting. He et al. (2021a)
observed a similar trend with a similar sized model
(700M). On the other hand, the score of PFdrops
for 1.3b model, which suggests it is overfitting.
This difference can be attributed to the way we
count the parameters in Table 3. Note that the
number of parameters for PFis counted as extra in-
ference parameters following Li and Liang (2021),
which is different from trainable parameters. For
example, PF1.3b model with 10m extra inference
parameters actually contains 80m extra trainable
parameters, which is much higher than the 10m
shown in the table. Such a large number of train-
able parameters will easily make the model overfit
forPFand thus leads to the performance drop with
more parameters. For the 8.3b model, the scores
of both APandPFdrops with more parameters
as both of them are overfitting and AP has a more
serious overfitting issue there. Table 3 suggests that
(1) more parameters do not always help PForAP,
(2) task specific parameters can be further reduced
by sacrificing little scores.
4.2 Cross-domain and Cross-dataset
Generalization
Table 4 shows cross-domain results over Xsum and
WoW and cross-dataset results can be found in
Table 5 and Table 6. We find that APachieves
in general better generalization than PFin cross-
domain and cross-dataset setting by 13 out of 154827
comparisons.
For Xsum, both APandPFare universally better
than FTfor cross-domain and cross-dataset setting
across different training sample sizes. For WoW,
we find the conclusion is a bit different. PFis
worse than FTwith 7 out of 9 comparison settings
in Table 4 and Table 6 while APhas only 3 out of
9. We conjecture this is due to the bias of differ-
ent tasks as we can see the cross points for Xsum
and WoW are also quite different. WoW reflects
a more general finding that APcan achieve bet-
ter cross-domain and cross-dataset scores than FT
when PLMs are large enough (e.g. 1.3b) or training
samples are small enough (e.g. 5k). In these two
cases, APonly tunes a relatively small amount of
parameters comparing to FTand therefore is less
likely to overfit. It is also aligned with conclusions
under in-domain scenario.
4.3 Faithfulness
Table 7 and Table 8 shows the faithfulness eval-
uation over WoW and Xsum dataset using entail-
ment score and factCC score. The faithfulness
score for Xsum is quite low as the dataset contains
many unfaithful training samples (Pagnoni et al.,
2021). Both of the tables show that PFachieves
the best faithfulness score across all model sizes
and sample size. However, when increasing the
PLM size from 357m to 8.3b, or training samples
from 5k to 74k, we see a constant drop of entail-
ment score or factCC score. This can be attributed
to (1) both WoW and Xsum have many responses
or summaries that contains information external to
the evidence (Rashkin et al., 2021; Pagnoni et al.,
2021) and (2) larger language models memorize
more world knowledge itself (Brown et al., 2020).
Therefore, our models will become more unfaithful
when they learn from those unfaithful examples or
use its embedded knowledge. In such case, PFpro-
vides an option for users to sacrifices a little PPL
to earn more faithfulness. How to further improve
the faithfulness of PERM s is still an open research
problem and we leave it for future work.
5 Conclusion
In this paper, we extensively compare PERM s with
finetuning over three main areas: (1) in-domain
evaluation by scaling both the sample size and
model size (2) cross-domain and cross-dataset gen-
eralization (3) faithfulness of generations. For
in-domain settings, we find (a) there is a cross
point of sample size for which parameter efficient
learning will be better than finetuning when train-
ing with fewer samples and (b) larger PLMs have
larger cross points. This suggests users to choose
the method based on their own sample size and
model size. Simply apply Adapter to MT-NLG, we
achieve new state-of-the-art results on Xsum for
all ROUGE scores (ROUGE-1 49.17, ROUGE-2
27.20, ROUGE-L 40.98). Compared to finetun-
ing, not all PERM s can easily achieve better cross-
domain and cross-dataset scores than finetuning
even with large PLM (e.g. 8.3b). Adapter is a bet-
ter choice than other PERM s in such cases. Lastly,
we provide the first comparison of PERM s over
faithfulness of generations and show that Prefix
tuning is the best method for faithfulness. We be-
lieve our findings will help users better choose and
deploy PERMs.
6 Limitations
Our paper have the following limitations. Firstly,
we are only able to qualitatively show the cross
point when FTis better than AP. We do not derive4828a quantitative estimation of the cross point given the
model size and the task name. Therefore, the cross
point of our paper can be served as a reference only
for summarization and dialogue generation when
choosing between these methods. (2) Even though
we show PFachieves better faithfulness than other
methods. We found that when the model is large
enough, (e.g. 8.3b) and the dataset is large too (e.g.
74k), PFachieves quite close scores to FT(0.721
vs 0.720). Therefore, it remains a question how to
achieve better faithfulness under such setting.
References482948304831A Example Appendix
A.1 Training Details
We use the pre-trained GPT checkpoint trained by
Megatron-LM (Shoeybi et al., 2019). When num-
ber of samples is less than 5000, we set batch size
as 8 and otherwise we set it as 64. We set learn-
ing rate as 1e-4 for Xsum dataset when running
Adapter ( AP) tuning or Prefix tuning ( PF). We set
it as 3e-4 for WoW dataset. For finetuning over
Xsum dataset, we use the learning rate of 3e-5 for
357m and 1e-5 for 1.3b and 8.3b model. When
finetuning WoW dataset, we set it for 8e-6 for all
model sizes. Xsum dataset is trained for 10 epochs
and WoW dataset is trained for 5 epochs. We used
ROUGE-L score at the validation set to select the
best model for Xsum and used PPL at the seen val-
idation set for WoW. To APtune 530b MT-NLG
model, we set the learning rate as 3e-5 and the
batch size as 32.
The prefix length is fixed as 100 and hidden di-
mension as 800 for PFexperiments (Li and Liang,
2021). For AP, the hidden size was set to 50 to
achieve a similar extra number of parameters for
inference. We summarize the extra task specific
parameters introduced by PFandAPin Table 9.
Note that we don’t intend to do extensive hyperpa-
rameter search for all the combinations for model
size, sample size and tuning methods. We instead
would like to draw conclusions that can general-
ize across model size, sample size and tasks. We
used NVIDIA V100 and A100 GPUs to run all
experiments.
For summarization, we simply give the
article as the input and the summary as
the output. For dialogue, We formu-
late the input with the following template:
“{TOPIC }\t{dialogue_history }\tKnowledge:
{knowledge }\t\tB:”. For dialogue history, we
addA: and B: in front of each utterance todistinguish different speakers. \tis used to
separate different dialogue turns. We use beam
search for the decoding step and we set beam size
as 5 for all settings.
A.2 More Related Work
More work about faithfulness include summariza-
tion (Cao et al., 2018; Dong et al., 2020; Cao
and Wang, 2021; Huang et al., 2021), dialogue
generations (Rashkin et al., 2021; Shuster et al.,
2021; Dziri et al., 2021; Wu et al., 2021), data-to-
text (Wiseman et al., 2017; Nie et al., 2019; Liu
et al., 2021a; Rebuffel et al., 2022), and transla-
tion (Lee et al., 2019; Wang and Sennrich, 2020).
However, still relatively little is known about faith-
fulness/hallucination problem. Pagnoni et al. con-
duct a good analysis of error types.
Other parameter efficient learning methods in-
cludes PPT (Gu et al., 2022), masking (Zhao et al.,
2020) with application in multitask learning (Stick-
land and Murray, 2019; Mahabadi et al., 2021),
transfer learning (Pfeiffer et al., 2021; Su et al.,
2022; Vu et al., 2022), improving robustness (Han
et al., 2021), low resources settings (He et al.,
2021b)
A.3 Additional Results
We present detailed ROUGE scores for Xsum in
the following tables.48324833