Rahul Kumar,Sandeep Mathias,Sriparna Saha,Pushpak BhattacharyyaDepartment of Computer Science and Engineering, IIT KanpurDepartment of Computer Science and Engineering, Presidency University, BangaloreDepartment of Computer Science and Engineering, IIT PatnaDepartment of Computer Science and Engineering, IIT Bombayrahul@iitk.ac.in,sandeepalbert@presidencyuniversity.in,sriparna@iitp.ac.in,pb@cse.iitb.ac.in
Abstract
Most research in the area of automatic essay
grading (AEG) is geared towards scoring the
essay holistically while there has also been lit-
tle work done on scoring individual essay traits.
In this paper, we describe a way to score essays
using a multi-task learning (MTL) approach,
where scoring the essay holistically is the pri-
mary task, and scoring the essay traits is the
auxiliary task. We compare our results with
a single-task learning (STL) approach, using
both LSTMs and BiLSTMs. To find out which
traits work best for different types of essays,
we conduct ablation tests for each of the essay
traits. We also report the runtime and number
of training parameters for each system. We find
that MTL-based BiLSTM system gives the best
results for scoring the essay holistically, as well
as performing well on scoring the essay traits.
The MTL systems also give a speed-up of be-
tween 2.30 to3.70 times the speed of the STL
system, when it comes to scoring the essay and
all the traits.
1 Introduction
Anessay is a piece of text that is written in re-
sponse to a topic, called a prompt (Mathias and
Bhattacharyya, 2020). Qualitative evaluation of the
essay consumes a lot of time and resources. Hence,
in 1966, Page proposed a method of automatically
scoring essays using computers (Page, 1966), giv-
ing rise to the domain of Automatic Essay Grading.
Essay traits are different aspects of the essay that
can aid in explaining the score assigned to the es-
say. Examples of essay traits include content (how
much information is present in the essay) (Page,
1966), organization (how well the essay is struc-
tured) (Persing et al., 2010), style (how well written
the essay is) (Page, 1966), prompt adherence (howmuch the essay stays on topic for the essay prompt)
(Persing and Ng, 2014), etc.
Most of the research work done in the field of
AEG is geared toward scoring the essay holistically,
rather than studying the importance of essay traits
in the overall essay score. In this paper, we ask the
question:
“Can we use information learnt from scoring
essay traits to score an essay holistically?”
In our paper, not only do we score essays holisti-
cally, but we also describe how to score essay traits
simultaneously in a multi-task learning framework.
Scoring essay traits is essential as it could help in
explaining why the essay was scored the way it was,
as well as providing valuable insights to the writer
about what aspects of the essay were well-written
and what the writer needs to improve.
Multi-task learning is a machine learning tech-
nique where we use information from multiple aux-
iliary tasks to perform a primary task (Caruana,
1997). In our experiments, scoring the individual
essay traits is the auxiliary task, and scoring the
essay holistically is the primary task. In addition to
this, we also study the impact of scoring an essay
trait as the primary task while the other traits and
overall essay score are auxiliary tasks.
Contributions. In this paper, we describe a way
to simultaneously score essay traits and the essay
itself using multi-task learning. We evaluate our
system against different types of essays and essay
traits. We also share our code and the data for
reproducibility and further research.14852 Motivation
Most of the work done in the area of automatic
essay grading is in the area of holistic AEG - where
we provide a single score for the entire essay based
on its quality. However, for writers of an essay, a
holistic score alone would not be enough. Provid-
ing trait-specific scores will tell the writer which
aspects of the essay need improvement.
In our dataset, we observe that writers of good
essays usually have a lot of content, appropriate
word choice, very few errors, etc. Essays that are
poorly written often lack one or more of these qual-
ities (i.e. they are either too short, have lots of
errors, etc.). We, therefore, observe a high correla-
tion between individual trait scores and the overall
essay score (Pearson correlation trait scores and
overall essay score >0.7across all essay sets in
our dataset). Hence, we believe that using essay
trait scores will benefit in scoring the essay holis-
tically, as their scores will provide more relevant
information to the AEG system.
3 Related Work
3.1 Holistic Essay Grading
Holistic essay grading involves assigning an over-
all score for an essay (Mathias and Bhattacharyya,
2020). The first AEG system was designed by Page
(1966). In the decade of the 2000s there were a lot
of AEG systems which were developed commer-
cially (see Shermis and Burstein (2013) for more
details).
After the release of Kaggle’s Automatic Student
Assessment Prize’s (ASAP) Automatic Essay Grad-
ing (AEG) dataset in 2012, there has been a lot
of research on holistic essay grading. Initial ap-
proaches, such as those of Phandi et al. (2015)
and Zesch et al. (2015) used feature engineering
techniques and domain adaptation in scoring the
essays. More recent papers look at using a num-
ber of deep learning approaches, such as LSTMs
(Taghipour and Ng, 2016; Tay et al., 2018) and
CNNs (Dong and Zhang, 2016) or both (Dong et al.,
2017; Zhang and Litman, 2018, 2020). Zhang and
Litman (2020) describe a way to extract impor-
tant information, called topical components, from
a source-dependent response.3.2 Trait-specific Essay Grading
In the last decade or so, there has been some work
done in scoring essay traits such as sentence fluency
(Chae and Nenkova, 2009), organization (Persing
et al., 2010; Taghipour, 2017; Mathias et al., 2018;
Song et al., 2020), thesis clarity (Persing and Ng,
2013; Ke et al., 2019) coherence (Somasundaran
et al., 2014; Mathias et al., 2018), prompt adher-
ence (Persing and Ng, 2014), argument strength
(Persing and Ng, 2015; Taghipour, 2017), stance
(Persing and Ng, 2016), style (Mathias and Bhat-
tacharyya, 2018b) and narrative quality (Somasun-
daran et al., 2018). None of the above work, how-
ever, uses trait information to score the essay holis-
tically.
There has also been work on scoring multi-
ple essay traits (Taghipour, 2017; Mathias and
Bhattacharyya, 2018a, 2020). (Rama and Vajjala,
2021) discuss solutions across multiple languages
(German, Czech and Italian). Mathias and Bhat-
tacharyya (2020) describes work on the use of neu-
ral networks for scoring essay traits. Our work
combines the scores of essay traits for holistic es-
say grading. We focus on using trait-specific essay
grading to improve the performance of an auto-
matic essay grading system. We also show how
using multi-task learning - simultaneously scoring
both the essay and its traits - we are able to speed
up the training of our system without too much of a
loss in scoring the essay traits. (Ridley et al., 2021)
describe a multi-task learning approach to grade
essays and their traits using a neural network. Our
system differs from theirs with respect to the shared
layers and trait-specific layers. While Ridley et al.
(2021) share the embedding and word-specific lay-
ers (to get sentence representations), we share only
the embedding layer.
3.3 Multi-task Learning
Multitask Learning was proposed by Caruana
(1997) where the argument was that training signals
from related tasks could help in a better generaliza-
tion of the model. Collobert et al. (2011) success-
fully demonstrated how tasks like Part-of-Speech
tagging, chunking and Named Entity Recognition
can help each other when trained jointly using deep
neural networks. Song et al. (2020) described a
multi-task learning approach to score organization
in essays, where the auxiliary tasks were classify-
ing the sentences and paragraphs, and the primary
task was scoring the essay’s organization. Cao et al.1486(2020) also use a domain adaptive MTL approach
to grade essays, where their auxiliary tasks are sen-
tence reordering, noise identification, as well as
domain adversarial training. However, they also
use all the other essay sets as part of their train-
ing, whereas we use only the essays present in the
respective essay set for training.
4 System Architecture
4.1 STL Essay Grading Stack
For scoring the essays, we use essay grading stacks.
Each stack is used for scoring a single essay trait.
The architecture of the stack is based on the archi-
tecture of the holistic essay grading system pro-
posed by Dong et al. (2017). The essay grading
stack takes the essay as input (split into tokens and
sentences) and returns the score of the essay / essay
trait as the output. Figure 1 shows the architecture
for the essay grading stack.
For each essay, we first split the essay into to-
kens and sentences. This is given as an input to
the essay grading stack. In the word embedding
layer, we look up the word embeddings of each
token. Just like Taghipour and Ng (2016), Dong
et al. (2017), Tay et al. (2018), Mathias and Bhat-
tacharyya (2020) and Mathias et al. (2020), we use
the most frequent 4000 words of the training data
as the vocabulary with all other words mapping to a
special unknown token. This is done mainly to cap-
ture out-of-vocabulary words, as well words that
generally don’t belong in the topic. If the vocabu-
lary size is too small, then a number of words will
be marked as spelling errors. On the other hand, if
the vocabulary size is too large, a lot of spurious
words would also be learnt as important ones.
This sequence of word embeddings is then sent
to the next layer - the 1 dimension CNN layer -
to get local information from nearby words. The
output of this layer is aggregated using attention
pooling to get the sentence representation of the
sentence. This is done for all sentences in the essay.
Each of the sentence representations are then
sent through a recurrent layer. We experiment on
two different types of recurrent layers - a unidirec-
tional LSTM (Hochreiter and Schmidhuber, 1997)
and bidirectional LSTM (BiLSTM) - as the recur-
rent layer. The outputs of the recurrent layer arepooled using attention pooling to get the represen-
tation for the essay. This essay representation is
then sent through a fully-connected Dense layer
with a sigmoid activation function to score the es-
say either holistically or a particular essay trait. For
our experiments, we minimize the mean squared
error loss .
Prior to input, we scale the scores to the range of
[0,1]using min-max normalization. The output of
the sigmoid function is a scalar in the range of [0,1]
which is rescaled back up to a score in the original
score range and rounded off to get the score for the
essay. This essay stack is used for the scoring of
the single-task learning (STL) models.
4.2 MTL Model
The architecture of our MTL model for an essay
ofMtraits is shown in Figure 2. Here, the word
embedding layer is shared across all the tasks . In
the multi-task learning framework, each stack is
used to learn an essay representation for each essay
trait. In a similar manner, the essay representation
for the overall score is learnt and it is concatenated
with the predicted trait scores before being sent
to a Dense layer with a sigmoid activation func-
tion to score the essay holistically. For calculat-
ing each score - both overall and trait scores - we
use the mean squared error loss function. We
experimented with multiple weights for the loss
function for the essay trait scoring task, but settled
on uniform weights for all the traits and the overall
scoring task. This is done because we want to get
accurate predictions of the traits scores which are
used for predicting the overall score.
5 Dataset Used
For our experiments, we use the Automated Stu-
dent’s Assessment Prize (ASAP) Automatic Essay
Grading (AEG) dataset. The dataset has a total of
8 essay sets - where each essay set has a number
of essays written in response to the same essay
prompt. In total, there are nearly 13,000 English
essays in the dataset, written by American high
school students from classes 7 to 10.
Table 1 gives the properties of each of the essay
sets in our dataset. It reports the overall essay
scoring range, traits scoring, average word count,
number of traits, number of essays and essay type.
We use the overall scores directly from the
ASAP AEG dataset. Since the original dataset
only provided trait-specific scores for Prompts 71487
& 8, we use the trait-specific scores provided by
Mathias and Bhattacharyya (2018a).
Depending on the type of prompt for the essay
set, each essay set has a different set of traits. Ar-
gumentative / Persuasive essays are essays which
the writer is prompted to take a stand on a topic
and argue for their stance. These essay sets have
traits like content, organization, word choice, sen-
tence fluency, and conventions. Source-dependent
responses (Zhang and Litman, 2018) are essays
where the writer reads a piece of text and answers
a question based on the text that they just read.
These essay sets have traits like content, prompt
adherence (Persing and Ng, 2014), language andnarrativity (Somasundaran et al., 2018). Narrative /
Descriptive essays are essays where the writer has
to narrate a story or incident or anecdote. They
have traits like content, organization, style, conven-
tions, voice, word choice, and sentence fluency.
Table 2 lists the different essay traits for each essay
set.
6 Experiments
6.1 Evaluation Metric
We use Cohen’s Kappa with quadratic weights (Co-
hen, 1968) (QWK) as the evaluation metric. This
is done for the following reasons. Firstly, the final
scores predicted by the system are distinct num-
bers/grades, rather than continuous values; so we
cannot use the Pearson Correlation Coefficient or
Mean Squared Error. Secondly, evaluation metrics1488
like F-Score and accuracy do not take into account
chance agreements. For example, if we are to grade
every essay with the mean score or most frequent
score, we would get F-Score and accuracy as high
as 60% or more, whereas the Kappa score will be 0!
Thirdly, the fact that the scores given are ordered
(i.e.0<1<2<3...) means that we need to use
weighted Kappa to capture the distance between
the actual and predicted scores. Between linear
weighted Kappa and QWK, we choose QWK be-
cause it rewards matches and punishes mismatches
more distinctly than linear weighted Kappa.
6.2 Evaluation Method
We evaluate our experiments using five-fold cross
validation . We use the same data splits as used by
Taghipour and Ng (2016). To avoid overfitting, we
choose the model which gives the best result on the
validation set for evaluating on the test set, and we
report the mean value of all 5 folds.
6.3 Experiment Configuration
Table 3 gives the different hyperparameters used
in our systems. For the sake of uniformity, we
use these hyperparameters irrespective of the net-
work configuration (STL vs MTL, or LSTM vs
BiLSTM).1489
To evaluate the performance of our systems in
scoring the essay overall, we use 4 different con-
figurations - STL-LSTM ,STL-BiLSTM ,MTL-
LSTM , and MTL-BiLSTM . In addition to the
above systems, we also compare our approach with
a state-of-the-art string kernel system designed by
Cozma et al. (2018), using the same splits for train-
ing, testing, and validation, as well as a baseline
transformer-based implementation (BERT-STL),
using the BERT-base-uncased model. We run this
baseline model for 100 epochs and a batch size of
30, all other hyperparameters remaining default.
We also study the effect of using our system to
grade an essay trait as the primary task, and score
the other traits and the essay overall as auxiliary
tasks (MTL*).
In the STL configurations, we train our system
to predict a single score at a time- either the overall
essay score or the score for any of the essay traits.
In the MTL configurations, our system learns to
score the essay and all its traits simultaneously.
The LSTM configurations use only a forward direc-
tion LSTM, while the BiLSTM configurations use
a bidirectional (i.e. forward and reverse) LSTM.
7 Results and Analysis
In this section, we report our results and analyze
them for different experiments.
7.1 Performance on Holistic Essay Scoring
Table 4 gives the QWK scores of each of our sys-
tems as they score each essay set holistically. The
different systems used are the Single Task Learning(STL) (only scoring the essay overall) and Multi-
task Learning (MTL) (scoring the essay and the
traits simultaneously). The first column lists out
the different essay sets (Prompts 1 to 8). The
next three columns report results for STL using
both LSTM and BiLSTM, as well as results using
the string kernel-based approach of Cozma et al.
(2018). The next two columns report results for the
MTL systems using both LSTM and BiLSTM. The
last column shows the results using the baseline
BERT-STL system.
From the table, we see that the MTL-BiLSTM
performs the best of all the non-transformer sys-
tems (almost as good as the results of our BERT-
STL system). In order to see if the improvements
observed are statistically significant, we run the
Paired T-Test for each of the essay sets and com-
pare the results using a p-value of p <0.05.
7.2 Performance on Scoring Essay Traits
We also look at how our system performs in the
auxiliary tasks - namely scoring the different essay
traits. Figure 3 gives the results of our experiments
in scoring the essay traits, using the String Kernel
(HISK) (Cozma et al., 2018), CNN-LSTM (STL)
(Dong et al., 2017), and Our Systems (MTL and
MTL*). We use the same evaluation method, which
we used for scoring essay traits, with the same data
splits. For the STL systems, we train them for every
essay trait individually. MTL* is the results of
using our system to score essay trait as the primary
task, and score the other traits and the essay overall
as auxiliary tasks.
We compare the results with that of our MTL-
BiLSTM system, which was trained to score the
essay traits as auxiliary tasks. Figure 3 gives the1490
results of our experiments. From the figure, we see
that, while the STL-LSTM system is able to outper-
form our MTL-BiLSTM system, the MTL* system
(where the traits are primary tasks) outperforms the
STL-LSTM system. While the STL system opti-
mizes for scoring only a trait, the MTL* system
learns information from other traits to score the
given essay trait.
One of the reasons for the different trait per-
formance depends on how easy (or difficult) it is
to score the individual trait, as well which all es-
say traits have the particular trait. For example,
prompt adherence has a higher average QWK than
the other traits because it is present mainly in the
source-dependent essays (which have a mean QWK
over 1% higher than the mean QWK across all es-
say sets). Similarly, V oice has the lowest QWK
mainly because it is present only in Prompt # 8,
which has a very low holistic QWK.
7.3 Scoring Traits as the Primary Task
An interesting question for analysis is “What if we
score the traits as the primary task ?” In order to do
that, we changed our system to make scoring one
of the trait as the primary task, and scoring the rest
of the traits as well as the essay overall, as auxiliary
tasks. The comparison of these results are shown
in Figure 3 (in the MTL* column). We see that our
MTL* system outperforms the STL-based system
on scoring individual traits, although it would take
a lot longer time to train (as it would be equivalent
to running the MTL system between 4 to 6 times).7.4 Ablation Tests
In order to know which trait is most important for
each essay set, we run a series of ablation tests.
For each essay set, we ablate one essay trait at a
time before scoring the essay. Table 5 reports the
results of the ablation test. The values in the table
correspond to the drop in performance in scoring
the essay holistically. We find that the Content is
the most important essay trait for 3 of the essay
sets. Prompt Adherence and Word Choice are the
most important traits for 2 of the essay sets where
they are scored.
7.5 Error Analysis
As we have seen, the MTL model generally helps
over the STL model when it comes to holistic essay
scoring, especially if there is no well-defined rule
(Example: Holistic Score = Sum of trait scores) for
scoring the essay holistically.
A possible scenario where STL could help over
MTL is if the holistic score is a well-defined func-
tion of the trait scores AND the STL system can
predict the trait scores with a good deal of accuracy.
The essay sets corresponding to Prompts 7 & 8 are
two such essay sets, where the overall score is a
function of the individual trait scores. To verify
this, we ran the experiments in a pipelined manner
- first scoring the essay traits, then calculating the
holistic score using the predicted trait scores and
comparing it with the gold standard holistic scores.
We found no difference in QWK for Prompt 7 (a
QWK of 0.796 vs. 0.795), but a much lesser perfor-
mance with Prompt 8 (a QWK of 0.684 vs. 0.699)
as compared to our MTL-based system. One of the
main reasons for this is due to the poor performance
in predicting the trait scores as single tasks.
7.6 Runtime Analysis
We also ran experiments to see how much resources
and time our approaches will take. Table 6 gives
the total training time (in hours). The total training
time is the total time taken to train our system to
score the essay holistically as well as all the traits
in that essay set for all 100 epochs. We also report
the speed-up when using the MTL approach as
compared to the STL approach. From our results,
we observe a 2.30 to3.70 speed-up in using the
MTL models as compared to using the STL models.
The BERT-STL experiments ran for about 5 days
(113 hours).
We also report the average number of training1491
System Average Range
STL-LSTM 326K 326K
STL-BiLSTM 436K 436K
MTL-LSTM 891K 829K - 1.08M
MTL-BiLSTM 1.5M 1.38M - 1.85M
parameters per system in Table 7. For the STL
systems, the number of trainable parameters is the
same irrespective of essay set. For the MTL sys-
tems, the models, the number of training parame-
ters varies based on the number of essay traits in
the essay set. Prompts 3 to 7, which have only 4
traits, have about 1.38 million training parameters.
On the other hand, Prompt 8, which has 6 essay
traits, has over 1.85 million training parameters.
All our experiments were run on an Nvidia
GeForce GTX 1080 Ti Graphics Card with 12GB
of GPU memory, using Python version 3.5.2, Keras
version 2.2.4 and Tensorflow version 1.14.
7.7 Comparison with Transformer Models
Most modern NLP systems have started to use
attention-based transformer networks and large pre-
trained language models. Yang et al. (2020), Cao
et al. (2020), and Uto et al. (2020) use the BERT-base-uncased (Devlin et al., 2019) pre-trained lan-
guage model to perofrm automatic essay grading
achieving QWKs in the range of 0.79 to 0.805.
However, BERT has about 110 million parameters
(compared to our largest model with just under 2
million parameters). Another limiting factor with
using BERT is the fact that we can only input 512
tokens. This is a problem, especially for Prompt 8,
where the average essay length is about 650 words.
Mayfield and Black (2020) describe some of the
other limitations of using BERT for scoring essays.
8 Conclusion and Future Work
In this paper, we described an approach to use
multi-task learning to automatically score essays
and their traits. We achieve this by concatenating
a representation of the essay with the trait scores
- predicted as an auxiliary task. We compared our
results with single-task learning models as well.
We found out that the MTL system with the Bi-
Directional LSTM outperforms the STL-based sys-
tems and has results comparable with a baseline
BERT-STL system. We then ran an ablation test
and found out which essay trait was important for
the corresponding essay sets. We also report our
system’s performance, which shows a 2.30 to3.70
speed-up of using the multi-task learning system,
compared to using a single task learning system.
An exciting avenue of future work is using trait
scoring to aid in providing text feedback to the
writer, like showing where the low score for the
trait originates, similar to Hellman et al. (2020) (for
content scoring), rather than a trait-specific score
only. We also plan to investigate using ALBERT
(Lan et al., 2020), in lieu of the essay stack, to
grade essays and their traits simultaneously. We
also plan to explore how to extend our approach1492in generalizing our system, training it on essays
written in response to one set of source prompts,
and tested it on essays written for another prompt.
Acknowledgements
The authors would like to thank the anonymous re-
viewers of the ACL Rolling Review for their com-
ments in helping improve this work.
The authors would also like to acknowledge the
different research grants in the various host institu-
tions, namely:
•The Young Faculty Research Fellowship
(YFRF) Award supported by the Visvesh-
varaya Ph.D. Scheme for Electronics and IT,
Ministry of Electronics and Information Tech-
nology (MeitY), Government of India, be-
ing implemented by Digital India Corporation
(formerly Media Lab Asia).
•The Presidency University Faculty Seed Grant
Award.
for funding us in carrying out our research.
References149314941495