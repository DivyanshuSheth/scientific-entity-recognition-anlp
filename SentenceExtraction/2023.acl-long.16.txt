
Zujie Liang, Feng Wei, Jie Yin, Yuxi Qian, Zhenghong Hao, Bing Han
MYbank, Ant Group
jokieleung@outlook.com
{huodeng.wf,yibo.yj,qianyuxi.qyx,haozhenghong.hzh,hanbing.hanbing}@mybank.cn
Abstract
Thanks to the recent success of Pre-trained Lan-
guage Models (PLMs), it has become a promis-
ing research direction to develop a universal
model (UIE) that can solve all typical infor-
mation extraction tasks within one generative
framework. Nonetheless, in real-world scenar-
ios of UIE applications, new data of different
IE tasks and domains usually come in a stream
over time. A desirable UIE system should be ca-
pable of continually learning new tasks without
forgetting old ones, thereby allowing knowl-
edge and functionalities expansion without re-
training the whole system. In this paper, we
study the UIE system under a more challeng-
ing yet practical scenario, i.e., “lifelong learn-
ing” settings, to evaluate its abilities in three
aspects, including knowledge sharing and ex-
pansion, catastrophic forgetting prevention, and
rapid generalization on few-shot and unseen
tasks. To achieve these three goals, we present
a novel parameter- and deployment-efficient
prompt tuning method namely Lottery Prompt
Tuning (LPT). LPT freezes the PLM’s param-
eters and sequentially learns compact pruned
prompt vectors for each task leveraging a bi-
nary prompt mask, while keeping the prompt
parameters selected by the previous tasks in-
susceptible. Furthermore, we use a simple yet
effective method to perform mask selection and
show the powerful transferability of Lottery
Prompts to novel tasks. Extensive experiments
demonstrate that LPT consistently sets state-of-
the-art performance on multiple lifelong learn-
ing settings of UIE, including task-incremental
setting on seen tasks, few-shot adaptation, and
zero-shot generalization on novel tasks.
1 Introduction
Information Extraction (IE) is one of the fundamen-
tal tasks in Natural Language Processing (NLP),
which aims to extract the desired structural infor-
mation from unstructured texts (Andersen et al.,Figure 1: Two different dimensions of lifelong learning
among IE tasks. During real-world scenarios, the data
from various IE tasks across varying domains come in a
stream.
1992; Surdeanu et al., 2003; Ma and Hovy, 2016;
Kolluru et al., 2020). Previous IE research mostly
focuses on one specific IE task (Miwa and Bansal,
2016; Wang et al., 2020; Lin et al., 2020; Zheng
et al., 2021) and designs different model architec-
tures (Lample et al., 2016; Sohrab and Miwa, 2018;
Li et al., 2020; Hsu et al., 2022) to tackle differ-
ent tasks. To facilitate knowledge sharing between
different tasks, various efforts have been paid for
unifying all IE tasks with one model structure (Wad-
den et al., 2019; Nguyen et al., 2021; Paolini et al.,
2021). Most recently, Lu et al. (2022); Fei et al.
(2022) unify general IE tasks in a generative way
with a text-to-structure framework (UIE), which
proves that universally modeling various IE tasks
can better learn general knowledge from varying
data sources.
Nonetheless, current work usually assumes the
accessibility of training data for every task. In
many real-world scenarios, as shown in Figure 1,
the training data are often streamed, and the IE277systems are required to identify new mention spans
or semantic relations to support new domains and
functionalities, which can be formulated as the
paradigm of lifelong learning. The ability to ac-
cumulate knowledge continually is crucial for the
quick deployment of UIE systems based on PLMs,
which allows the system to add new domains and
functionalities over time without incurring the high
cost of re-training the whole system each time. In
addition, considering that humans can acquire new
knowledge from a few examples (Montague, 1974),
it is expected for the models to generalize well on
novel tasks with few-shot data or even no data.
Motivated by this, our work aims to address
these more challenging yet practical issues by
proposing a lifelong learning setup for UIE. In
this setup, the system sequentially learns over mul-
tiple IE tasks (potentially of different task types
and varying domains) one by one. Then it will
be evaluated to preserve its performance on solv-
ing previously seen tasks, and generalize well to
novel tasks with few examples or even no exam-
ples. We cover two conventional properties of life-
long learning (Ke and Liu, 2022), i.e.,catastrophic
forgetting prevention (CF) and knowledge trans-
fer(KT), while in our setup, the evaluation of KT
extends to the novel tasks. In NLP community,
large Pre-trained Language Models (PLMs) have
been widely applied in many downstream tasks. In
order to lower computation and storage costs, re-
cent popular lifelong learning techniques (Madotto
et al., 2021; Ke et al., 2021a; Zhu et al., 2022;
Wang et al., 2022c) try to solve the CF and KT
leveraging parameter-efficient fine-tuning (PEFT)
methods (He et al., 2022a).
In this work, we inherit this wisdom and also
focus on parameter-efficient methods for lifelong
learning. Inspired by the lottery ticket hypothesis
and the efficiency of prompt tuning, we propose a
novel framework for lifelong UIE, named Lottery
Prompt Tuning (LPT). Specifically, we adopt an
encoder-decoder model architecture (Raffel et al.,
2020) and re-frame all types of IE tasks into a
text-to-structure format (Lu et al., 2022). First, we
prepend a sequence of continuous prompt vectors
to the input, which is shared across tasks. To con-
tinually learn a new IE task, we simultaneously
learn the prompt vectors together with a task-aware
binary prompt mask. The task-aware mask is de-
voted to pruning the shared prompt vectors and
producing an optimal task-specific pruned prompt,i.e., lottery prompt. To provide a pruning criterion
for finding the lottery prompt online, we introduce
a separate set of learnable parameters serving as the
importance scores, which have the same shapes as
the soft prompts. Hence, the lottery prompt can be
easily found by selecting the parameters with the
Top-k%importance scores online, without iterative
retraining and pruning procedure. To facilitate the
forward knowledge transfer when learning a new
task, the lottery prompt is permitted to selectively
reuse the learned prompt parameters for the former
tasks. Besides, the proposed LPT eliminates catas-
trophic forgetting and negative transfer by freezing
the prompt parameters for the previous tasks dur-
ing back-propagation. In the whole learning pro-
cess, the PLM is kept frozen to maintain general
knowledge. During inference, the same model can
handle different tasks by inputting different lottery
prompts, which is friendly for deployment.
We show that our proposed framework effec-
tively outperforms state-of-the-art baselines on life-
long learning for UIE in terms of catastrophic for-
getting prevention and knowledge transfer. More-
over, LPT closes the gap between continual learn-
ing and multi-task learning. The efficacy of the
proposed modules is thoroughly studied both em-
pirically and analytically. In summary, this work
makes three key contributions:
•A challenging yet practical benchmark is pro-
posed for lifelong UIE, where one UIE system
should not only keep its performance on solv-
ing seen IE tasks, but also generalize well on
novel IE tasks with few or even no examples.
•We proposed Lottery Prompt Tuning (LPT),
an extremely efficient prompt tuning frame-
work for lifelong UIE that directly learns
pruned prompts sequentially without an ex-
tra pruning stage.
•Extensive experiments on the benchmark
show that our approach outperformed base-
lines with higher parameter efficiency.
2 Related Work
Lifelong Learning Lifelong Learning, also
known as Continual Learning, aims to learn a se-
quence of tasks with one single model. Two main
goals are demanded: catastrophic forgetting (CF)
prevention and positive knowledge transfer (KT).
The research in this area can be categorized into
three folds: Regularization ,Rehearsal , and Archi-
tecture based methods. (a) Regularization-based278methods (Li and Hoiem, 2017; Kirkpatrick et al.,
2017; Ritter et al., 2018) ease the catastrophic for-
getting issue by regularizing important parameters
for learned tasks. These approaches usually need
a trade-off between learning new tasks and for-
getting the old tasks. In NLP, it is studied (Han
et al., 2020) to constrain the useful information
from the huge amount of knowledge inside the
PLMs. (b) Rehearsal-based methods methods re-
use old examples from the previously learned tasks
while learning new tasks. These examples are ei-
ther derived from real training data of previous
tasks (Rebuffi et al., 2017; Lopez-Paz and Ran-
zato, 2017; Mi et al., 2020), or generated by a
pseudo-data generator (Sun et al., 2019; Qin and
Joty, 2021; Zhao et al., 2022). Although these
methods work well, they are limited by data privacy
or the quality of generated data. (c) Architecture-
based methods tackle the continual learning prob-
lem by expanding new modules to the network over
time (Veniat et al., 2020; Douillard et al., 2022)
or isolating the network’s parameters for differ-
ent tasks (Serra et al., 2018; Mallya and Lazebnik,
2018; Mallya et al., 2018; Wortsman et al., 2020;
Geng et al., 2021; Kang et al., 2022). In NLP, in
order to better take advantage of the PLMs, these
methods usually are in conjunction with parameter-
efficient fine-tuning approaches, including adapter
tuning (Houlsby et al., 2019) and prompt tun-
ing (Lester et al., 2021a; Li and Liang, 2021; Liu
et al., 2022b). AdapterCL (Madotto et al., 2021)
trains a separate adapter for each task, leaving
knowledge transfer out of consideration. Ke et al.
(2021b,a); Ermis et al. (2022); Zhang et al. (2022)
overcome this drawback by introducing capsule net-
work (Sabour et al., 2017), distillation mechanism
and adaptive compositional modules, respectively.
For the latter, CPT (Zhu et al., 2022) learns a sepa-
rate prompt with continual prompt initialization for
each task. Wang et al. (2022c,b) propose to learn
a prompt pool and then select the useful prompts
to alleviate forgetting and potentially share knowl-
edge across tasks. Dai et al. (2022) extend the idea
to organize the prompt pools in a hierarchical way
to guide the pre-trained models in different granu-
larities. In contrast, we here share a single copy of
prompt parameters to instruct the PLMs, yet incre-
mentally learn a task-aware prompt mask for each
task whilst keeping the prompt parameters used
by the previous tasks unchanged. This not only
isolates the harmful prompt parameters that lead toforgetting but also shares useful prompt parameters
for knowledge transfer.
Lifelong learning in Information Extraction
In IE areas, some efforts are paid for building
IE systems to handle continual learning scenarios,
including continual NER (Monaikul et al., 2021;
Zheng et al., 2022), relation extraction (Cui et al.,
2021; Qin and Joty, 2022; Wang et al., 2022a), and
event detection (Yu et al., 2021; Liu et al., 2022a).
However, they merely study continual learning on
one single IE task. Very recently, UIE (Lu et al.,
2022; Fei et al., 2022) regards general IE tasks as a
text-to-structure generation task, thus unifies all IE
tasks with one model framwork. To a step further,
our work studies a more challenging yet practical
continual learning paradigm for UIE, where one
universal IE system needs to solve different types
of IE tasks across different domains incrementally.
Lottery Ticket Hypothesis Frankle and Carbin
(2018) propose the The Lottery Ticket Hypothe-
sis (LTH) that an over-parameterized network con-
tains a sub-network (lottery ticket) that, when ini-
tialized and trained in isolation, can match or ex-
ceed the test accuracy of the original network af-
ter training for at most the same number of itera-
tions. The LTH has been widely explored in many
fields of deep learning (Liu et al., 2018; Frankle
et al., 2019; Gong et al., 2022; Yu et al., 2019)
In NLP, researchers also explore the existence of
winning tickets under transfer learning regimes
for over-parametrized pre-trained language models
across various tasks (Morcos et al., 2019; Desai
et al., 2019). Chen et al. (2020); Prasanna et al.
(2020) show the existence of winning tickets when
fine-tuning BERT on downstream tasks. Liang
et al. (2021) shows the existence of super tick-
ets inside PLMs that can improve generalization.
Xprompt (Ma et al., 2022) is the pioneer to ex-
plore the LTH in the context of prompt tuning by
hierarchical structure pruning. However, Xprompt
needs iterative retraining, pruning and rewinding
to get the pruned prompts, which is impractical to
perform during continual learning settings since it
needs excessive computational time and costs. By
contrast, our LPT does not require an explicit prun-
ing stage and jointly learns prompt and task-related
masks together, which accelerates convergence dur-
ing continual learning. Moreover, our pruning is
performed at the parameter level while Xprompt’s
pruning is performed at the token and piece level.2793 Preliminary
3.1 Lifelong Learning Protocols
Conventional continual learning is defined as train-
ing machine learning models on a continuum of
data from a sequence of tasks. Here in our life-
long learning protocols for UIE, the incoming task
on the task sequence can be of different types
(e.g., entity extraction, relation extraction, event
extraction, and aspect-based sentiment analysis.),
or of the same type but potentially of different do-
mains. An intuitive demonstration can be found
in Figure 1. Formally, we define a sequence of
tasksD={D,···,D}, where the k-th task
D=/braceleftbig/parenleftbig
x,y/parenrightbig/bracerightbigcontains a set of data sam-
ples. For each data sample, the input xis con-
structed by the raw text tand a specific pre-
defined schema s, while the desirable output y
is structural information contained in the text x
indicated by the schema s. Note that our approach
is Rehearsal-free, meaning that data from the previ-
ous tasks can not be used anymore when training fu-
ture tasks. The goal of a lifelong UIE model should
perform well on all Ttasks after being trained with
the samples of these tasks sequentially. Further, in
the realistic scenario, it is usually expensive and
impractical to acquire plenty of labeled data for a
newly emerged task. To simulate this circumstance,
we adapt the sequentially trained model on a set of
n novel tasks individually {D}. Hence,
we can access the model’s ability to accumulate
previously learned knowledge for generalization
to new tasks by evaluating the few-shot/zero-shot
transferability of the lifelong model.
3.2 Generative UIE Framework
In this section, we cast all IE tasks as text genera-
tion and model the UIE system in a text-to-structure
framework (Lu et al., 2022). In this generative
framework, different IE structure generation is de-
composed into two atomic operations, i.e., spotting
and associating. Spotting indicates locating tar-
get information pieces from the sentence, e.g., the
entity and the trigger word in the event. Associ-
ating means connecting spans by assigning them
with specific semantic roles based on pre-defined
schemas, such as the relation between entity pairs
or the role between an event and its argument.
Input the input xfor the UIE model is formu-
lated as the concatenation of the raw sentence anda schema-based prompt in the form of:
x= [s;t] =/bracketleftbig
s, s, . . . , s, t, t, . . . , t/bracketrightbig
=[[spot],SPOT,[spot],SPOT. . . ,
[asso],ASSO,[asso],ASSO. . . ,
[text], t, t, . . . , t/bracketrightbig
(1)
SPOTrepresents the targeted spotting name in the
IE tasks, e.g., “organization" in the NER task; and
ASSOrepresents the targeted association name,
e.g., “work for” in the relation extraction task.
Output the output text yis a unified Structured
Extraction Language (SEL) that describes how the
structural elements organize into the target struc-
ture, which can be represented as “{Spot Name:
Info Span, (Asso Name: Info Span) (Asso Name:
Info Span)}" . The Spot Name andAsso Name are
the target structure from the pre-defined schemas,
while the Info Span refer to the text span mentioned
in the raw text.
Model We employ a Transformer-based encoder-
decoder language model i.e., T5 (Raffel et al.,
2020), as the model architecture for UIE. Given the
schema and the raw sentence as input sequences x
and the SEL as output sequences y, the model com-
putes the conditional language model distribution
of each token yusing the chain rule of probability
asp(y|y, x). It finishes prediction when out-
putting the end signal [EOS] . The predicted SEL
expression will be converted back into the extracted
information record for evaluation.
4 Method
4.1 Overview
In this section, we present a novel pruning-based
parameter-efficient tuning method for lifelong
learning, called Lottery Prompt Tuning (LPT). The
overall process of LPT is illustrated in Figure 2. To
continually learn a new IE task, we simultaneously
learn the prompt vectors together with a paired
task-aware binary prompt mask, while the mask is
devoted to producing a pruned prompt, i.e., Lottery
Prompt. During training for each incoming task,
LPT can selectively re-use the previously learned
prompt parameters to encourage knowledge trans-
fer, while the parameter updates only happen on
those soft prompt parameters that have not been
selected by the previous tasks. Finally, the model
shares the same set of soft prompts for all tasks
however uses the binary masks to isolate the shared280
parameters and get the lottery prompt for each task,
which solves catastrophic forgetting.
4.2 Lottery Prompt Tuning
Prompt tuning (Li and Liang, 2021; Liu et al.,
2022b) learns a set of continuous prompts and only
tunes the prompts while fixing the whole parame-
ters in PLM, which has been proven to be effective
in various downstream tasks. In this work, we com-
bine prompt tuning and the aforementioned gener-
ative UIE into one unified framework, where the
PLM takes the concatenation of continuous learn-
able soft prompts p, schema instruction sand the
raw text t, i.e., x= [p;s;t]. The training objective
is formalized as
L=/summationdisplay−logp(y|x;θ) (2)
Note that only the soft prompt parameters θare
trainable. Recently, Ma et al. (2022) show that
pruning prompts at token and piece level yields a
more parameter-efficient prompt yet with compet-
itive performance. Inspired by this, we propose
a novel Lottery Prompt Tuning (LPT) which ac-
quires high-performing pruned prompts for con-
tinual learning by assigning the prompt vectors θ
together with a task-aware binary mask m. The
mask selects the top- c%of soft prompts that lead to
good performance on the current task. To achieve
this, we introduce a set of learnable parameters s
that have the same shape as the soft prompts, which
indicates the importance scores of the prompt pa-
rameters. Once trained, these scores are thresh-olded to obtain the prompt mask, i.e.,m=h(s),
where h(.)is an indicator function that outputs "1"
for top- c%of the scores in the prompt parameters
or "0" otherwise. Therefore, the pruned prompt
parameters ˆθfor task k,i.e., lottery prompt, is
obtained by ˆθ=θ⊙m.
To get rid of the need for iterative retraining,
pruning and rewinding procedures during continual
learning, we perform online pruning by simultane-
ously optimizing the prompt parameters and the
importance scores together. To achieve this, we
use a straight-through gradient estimator (Bengio
et al., 2013) to ignore the derivatives of the indica-
tor function h(.)and directly update the scores as
follows:
minimizeL(θ⊙m;D) ;
s←s−η/parenleftbigg∂L
∂s/parenrightbigg(3)
where the ηis learning rate. While training on a
newly emerge task k, we use an extra binary mask
M=∨mto prevent updating the prompt
parameters allocated by previous tasks. Hence, the
prompt parameters θare updated as follows:
θ←θ−η/parenleftbigg∂L
∂θ⊙(1−M)/parenrightbigg
(4)
To summarize, LPT circumvents the forgetting is-
sue by isolating the prompt parameters for each
task. Meanwhile, taking the separate scores as the
pruning criterion allows sharing some of the param-
eters from previously chosen parameters θ⊙m281in solving the current task k, which contributes to
knowledge transfer.
4.3 Mask Selection for Novel Tasks
When generalizing to the use-case on novel tasks
where few or no labeled data for training, it is a
desired property to transfer knowledge learned by
the previous tasks to achieve better performance.
Hence, we provide two simple solutions to select
the binary masks in hands for initializing the lottery
prompt. The first way is to utilize the perplexity
(PPL) of each mask mover the input Xas a mea-
surement (Madotto et al., 2021), i.e.,PPL(X).
The mask with the lowest PPL will be chosen for
initialization. Another solution is to select the mask
by the gradient-based one-shot algorithm (Worts-
man et al., 2020). It first associates each of the
Tlearned masks mwith a proxy coefficient α,
initially set to 1/T. Then, infer the novel exam-
ple with the weighted mask ˆ m=/summationtextαmto
get the entropy. Further, the one-shot gradient cal-
culated by the entropy for each αindicates the
transferability of each mask. The mask with the
highest gradient will be chosen for initialization.
5 Experimental Settings
5.1 Datasets
To cover all four typical IE task types (including
NER, relation extraction, event extraction, and sen-
timent extraction), we formalize the lifelong UIE
benchmark by leveraging 13 IE datasets to con-
struct the task sequence. Specifically, NER tasks
include ACE04 (Mitchell et al., 2005), ACE05-Ent
(Walker et al., 2006), CoNLL03 (Tjong Kim Sang
and De Meulder, 2003); Relation extraction tasks
include CoNLL04 (Roth and Yih, 2004), ACE05-
Rel, SciERC (Luan et al., 2018), NYT (Riedel
et al., 2010); Event extraction tasks include CASIE
(Satyapanich et al., 2020), ACE05-Evt; Aspect-
Based Sentiment Analysis (ABSA) tasks include
SemEval-14 (Pontiki et al., 2014), SemEval-15
(Pontiki et al., 2015), SemEval-16 (Pontiki et al.,
2016). Refer to Appendix A for more detail about
the datset statistics. For dataset split, we follow the
same practice of the relevant prior works (Lu et al.,
2022) when using it. As the task order could in-
fluence the performance, we create 5 different task
orders by random permutation, which are listed in
Table 4.5.2 Evaluation Metrics
For the evaluation of IE performance, we use the
widely adopted span-based offset Micro-F1 as the
primary metric following previous work (Lu et al.,
2022). Given the generated text spans by our model,
we map spans to offsets by finding the first matched
offsets that are not already matched in the same
SEL hierarchical level. For the evaluation of life-
long learning ability, we denote aas the F1 on
the test set of task iafter training on task T. The av-
erage F1 on all tasks after training on the final task
is reported following the common protocol (Lopez-
Paz and Ranzato, 2017; Madotto et al., 2021):
Average =1
T/summationdisplaya (5)
To measure the forgetting during lifelong learning,
we use the BWT, which assesses the impact that
learning on subsequent tasks has on a previous
task. Negative BWT indicates that the model has
forgotten some previously acquired knowledge.
BWT =1
T−1/summationdisplaya−a (6)
Another metric is FWT (Ke et al., 2020), which
measures how much performance boost has hap-
pened to a new task after learning the task, repre-
senting the forward knowledge transfer.
FWT =1
T/summationdisplaya−a (7)
where arefers to the performance of training task
iindividually.
5.3 Baselines and Training Details
We adopt the following methods including recent
SOTA as our baselines, which covers both con-
tinual learning (CL) andNon-CL methods. (1)
continual learning methods: Naive Fine-tuning:
fine-tunes the whole model on new task data con-
tinually. EWC (Kirkpatrick et al., 2017) is a
Regularization-based method that regularizes the
change of important model parameters during train-
ing. ER(Chaudhry et al., 2019) is a Rehearsal-
based method that saves |M|(50 here) samples
randomly sampled from the training set of each
taskito memory Mand jointly trains the model on
new task data Dand memory M < k .Individual
saves a separate model for each task by fine-tuning282
the whole PLM, which clearly has neither forget-
ting nor knowledge transfer. AdapterCL (Madotto
et al., 2021) trains an adapter for each task sepa-
rately. Similarly, C-PT (Zhu et al., 2022) trains
a prompt for each task. L2P (Wang et al., 2022c)
trains a prompt pool to transfer task knowledge
and a distance-based prompt selection strategy to
select the task-specific prompt. (2) Non-CL meth-
ods:Multi-task Learning: Fine-tuning the whole
model in a multi-task manner using all tasks’ data
concurrently. Multi-task Prompt/Adapter Tun-
ing: Prompt/Adapter Tuning in a multi-task man-
ner instead of CL. These multi-task setups are
widely accepted as the upper bound of continual
learning. As for the LPT, we set the pruning ratio
top-c%of LPT as 0.7 in our experiments. For all
the prompt tuning methods mentioned above, theprompt length is set to 20. The parameters of PLM
are initialized from UIE-large checkpoints (Lu
et al., 2022). We keep all the same hyperparame-
ters for the UIE model reported in their paper. We
train the model for 30 epochs per task with batch
size 24 on 8 NVIDIA A100 GPUs. All the CL
and Non-CL baselines are implemented under the
same UIE framework. For the prompt tuning meth-
ods, we adopt the deep prompt tuning version (Li
and Liang, 2021; Liu et al., 2022b) to allow more
per-task capacity.
6 Results & Analysis
6.1 Results on Seen Tasks
The proposed LPT’s performance is compared with
current SOTAs w.r.t six measurements on the afore-
mentioned 13 IE tasks as shown in Table 1. Among283
all the continual learning methods, we highlight
that our method achieves the highest average F1
(improvements of up to 3% compared with L2P),
BWT and FWT with the lowest computation re-
source usage, which verifies the effectiveness of
LPT. While compared with the non-CL methods,
we can see the results of LPT are even comparable
with Multi-task prompt tuning , which is deemed as
the upper bound of prompt tuning methods for con-
tinual learning. That could be due to some negative
interference among tasks during multitask learn-
ing, however in our case, the parameter-isolation
mechanism solves that. Note that w.r.t computa-
tion resource usage, the parameter-efficient-based
methods generally require no memory and only
add a small number (around 0.29% to 5.6% ) of
additional parameters for each task, largely decreas-
ing the computational and storage overhead. Even
so, the LPT shows a remarkable superiority over
other methods (only 0.097% and 0.302% on "Tune
Param." and "+ Param." respectively). That’s be-
cause the saved binary masks for lottery prompts
only introduces an approximate overhead of 1/32 of
the prompt vectors, which are usually represented
by 32-bit float values. Detailed results on each IE
task refer to Table 5.
6.2 Results on Novel Tasks
We exclude 4 datasets in the task sequences (with
different IE task types) as novel tasks and con-
duct experiments on them in the few-shot/zero-shot
adaptation settings respectively. For the few-shot
setting, we conduct 10-shot learning where 10 sam-
ples per class are used for the training. While in the
zero-shot setting, the sequentially trained model
is directly used for testing. We perform the afore-
mentioned PPL-based mask selection method due
to its simplicity and effectiveness. Performances
are reported in Table 2 for the four evaluation tasks
individually and on average. We see LPT could
outperform all the CL baselines in few-shot and
zero-shot settings, which implies that the mask se-
lection module can make good use of upstreamtasks for novel task generalization. This points
to the fact that explicitly transferring knowledge
learned from a similar task is critical for systematic
adaptation to novel tasks.
6.3 Ablation Studies
6.3.1 Sparsity & capacity
We choose task order #1 to visualize the model
performance and the capacity of total prompts var-
ing with the prompt pruned ratio. As shown in
Figure 4, with the decrease of sparsity, the perfor-
mance of the model (blue bar) presents a trend of
first rising and then declining, while the prompt
parameter usage (orange line) keeps rising with the
decrease of sparsity. It is noteworthy that when the
model is trained on a very long sequence of tasks,
the prompt capacity could approach full. In this
case, our LPT framework is capable of expanding
the parameters by introducing new prompt tokens,
which shows great flexible.
6.3.2 Mask Correlations
To investigate how LPT reuses parameters over se-
quential tasks, we visualize all the task-wise binary
mask correlations trained from 5 different task se-
quences in Figure 3. We see LPT shares parameters
used for prior tasks with new ones, and is capable284of self-adaptively exploring not-yet-chosen param-
eters. This demonstrates the effectiveness of LPT
in both transferring positive knowledge from simi-
lar tasks and automatically exploring new patterns
for dissimilar tasks.
7 Conclusions
In this paper, we study a lifelong learning paradigm
for UIE systems, which we regard as an important
step towards general IE intelligence. We propose a
novel parameter-efficient framework, i.e., Lottery
Prompt Tuning (LPT), to achieve positive knowl-
edge transfer, catastrophic forgetting prevention,
and rapid generalization. Experimental results vali-
date the capability of our method on three settings.
Limitations
Though our method does not require iterative re-
training, pruning, and rewinding process, one ques-
tion still remains under-explored: how to self-
adaptively find the optimal sparsity instead of trial
training, which can boost the training efficiency.
Also, we plan to further investigate the effective-
ness of Lottery Prompt Tuning in other scenarios,
including the multi-task learning (He et al., 2022b),
prompt ensembling (Lester et al., 2021b), etc. Fur-
thermore, the proposed learning method should
be compatible with other parameter-efficient fine-
tuning methods, such as Adapter tuning (Houlsby
et al., 2019) and LoRA (Hu et al., 2021). We leave
these for future research.
References285286287288
A Dataset Statistics
B Detailed results of task-incremental
setting
Here we present detailed experimental results on
all 13 IE tasks across different task types including
NER, relation extraction, event extraction and senti-
ment extraction. As shown in Table 5, the proposed
LPT outperforms all competitive baselines.289290ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section Limitations
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section Abstract; Section1 Introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Not applicable. Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Section
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section Implementation Details291/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.292