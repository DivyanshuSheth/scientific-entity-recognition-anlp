
Iria de-Dios-Flores and Juan Pablo García-Amboage and Marcos Garcia
Centro Singular de Investigación en Tecnoloxías Intelixentes (CiTIUS)
Universidade de Santiago de Compostela
iria.dedios@usc.gal, juanpablo.garcia@rai.usc.gal
marcos.garcia.gonzalez@usc.gal
Abstract
Using psycholinguistic and computational ex-
periments we compare the ability of hu-
mans and several pre-trained masked language
models to correctly identify control depen-
dencies in Spanish sentences such as ‘José
le prometió/ordenó a María ser ordenado/a’
(‘Joseph promised/ordered Mary to be tidy’).
These structures underlie complex anaphoric
and agreement relations at the interface of
syntax and semantics, allowing us to study
lexically-guided antecedent retrieval processes.
Our results show that while humans correctly
identify the (un)acceptability of the strings, lan-
guage models often fail to identify the cor-
rect antecedent in non-adjacent dependencies,
showing their reliance on linearity. Additional
experiments on Galician reinforce these con-
clusions. Our findings are equally valuable for
the evaluation of language models’ ability to
capture linguistic generalizations, as well as for
psycholinguistic theories of anaphor resolution.
1 Introduction
Treating pre-trained language models (LMs) as psy-
cholinguistic subjects via the behavioral evalua-
tion of their probability distributions has proven to
be a very useful strategy to study to which extent
they are able to generalize grammatical informa-
tion from raw text (Linzen et al., 2016; Futrell et al.,
2019). A common method consists of comparing
model probabilities for grammatical and ungram-
matical sentences (e.g., “The key to the cabinets
is|*are on the table”). These experiments often
concentrate on syntactic phenomena that are instan-
tiated with surface strings that provide unequivo-
cal information about the elements that enter the
dependency, e.g. agreement morphology (Gulor-
dava et al., 2018; Kuncoro et al., 2018a). Yet, we
know less about the ability of LMs to coordinate
syntactic and semantic information during the res-
olution of dependencies whose elements are not
overtly signaled by morphosyntactic cues in theinput. Such is the case of control structures like
those in (1). These superficially simple construc-
tions underlie complex lexically-guided antecedent
retrieval processes, and they represent an interest-
ing candidate to study dependency resolution at the
syntax-semantics interface.
(1) a. Maríale prometió a Joséser ordenada.
María promised José to be tidy.
b. Joséle ordenó a Maríaser ordenada.
José ordered María to be tidy.
At the infinitive verb serin (1), it is crucial to
interpret its implicit subject. In other words, who
is tidy? The term control reflects the idea that the
interpretation of the implicit subject is controlled
by, or is determined by, another referent (Rosen-
baum, 1967; Chomsky, 1981). This type of con-
trol dependencies entail interpreting an anaphoric
relation between the implicit subject of the embed-
ded clause and one of the NPs in the main clause
(known as controller or antecedent). Crucially, this
interpretive relation is guided by specific lexico-
semantic properties of the main clause predicates
(Jackendoff and Culicover, 2003). In (1a), the cor-
rect antecedent is Juan (the main clause subject)
because promise has subject control properties. In
(1b), the correct antedecent is María (the main
clause object), because order has object control
properties. Retrieving the correct antecedent is es-
sential in order to build an accurate representation
of the message and to compute the agreement de-
pendency that is established between the controller
and the adjective tidy. Consequently, the resolution
of these dependencies entails coordinating informa-
tion about the lexico-semantic properties of control
predicates, co-reference, and agreement morphol-
ogy, and provides a great context for probing LMs’
grammatical abilities beyond morphosyntax.
In this work, we take advantage of the rich agree-
ment properties of two Romance languages (Span-
ish and Galician) in order to examine humans’ and203language models’ ability to correctly identify con-
trol dependencies. To do so, we have carefully
created an experimental design via the manipula-
tion of the gender of the NPs (feminine/masculine),
the type of control verb (subject/object control),
and the gender of the embedded adjective. This de-
sign will allow us to test whether humans and LMs
identify or produce agreement violations at the ad-
jective, which is used as a proxy for the accuracy
of antecedent retrieval processes. Furthermore, this
design will allow us to test for the presence of in-
terference effects of non-controlling NPs (referred
to as distractors) when they match or mismatch in
gender with the embedded adjective. We created
several datasets that have been used for a human
acceptability judgement task (Experiment 1), a LM
acceptability task (Experiment 2), and a LM pre-
diction task (Experiment 3). For Experiments 2
and 3, we tested the most prominent monolingual
and multilingual masked LMs based on transform-
ers for Spanish, and provide additional translated
datasets and results from the same computational
experiments carried out with Galician LMs in or-
der to confirm the cross-linguistic robustness of
our findings. Our results show that while humans
correctly identify the acceptability of the strings re-
gardless of the configuration of the NPs, language
models often fail to correctly identify the relevant
antecedent in subject control dependencies, show-
ing their reliance on linear relations rather than lin-
guistic information, something which is observed
in their below-chance accuracy for discontinuous
dependencies.
The main contributions of our paper are: (i)
the release of wide-covering and highly controlled
datasets to evaluate control structures in Spanish
and Galician, (ii) a psycholinguistic evaluation
of humans’ performance, a computational evalua-
tion of monolingual and multilingual LMs’ perfor-
mance, and a careful comparison between humans
and LMs; (iii) a demonstration of the limitations of
LMs to capture grammatical information thanks to
the adversarial example of control constructions.
2 Related work
Targeted evaluation of LMs: Targeted evalua-
tions of LMs focusing on different syntactic phe-
nomena have found evidence suggesting that these
models may generalize syntactic information from
raw text (Linzen et al., 2016; Goldberg, 2019;
Futrell et al., 2019; Mueller et al., 2020). In thisregard, the subject-verb (number) agreement task is
one of the most used adversarial examples for these
evaluations, although Marvin and Linzen (2018)
introduced further experiments dealing with other
syntactic phenomena in English (such as negative
polarity items or reflexive anaphora). These types
of datasets have been extended and adapted to dif-
ferent languages (Warstadt et al., 2020; Mueller
et al., 2020; Pérez-Mayos et al., 2021) and incor-
porated into online evaluation platforms (Gauthier
et al., 2020). In these experiments, the overall per-
formance of large pre-trained LMs is found to be
comparable to that of human subjects (Bernardy
and Lappin, 2017; Gulordava et al., 2018; Kuncoro
et al., 2018b), except for long-distance dependen-
cies with distracting nouns between the elements
of the target dependencies (Marvin and Linzen,
2018), where LMs often fail to identify the target
dependency relation. Besides, recent work found
that LMs’ perplexity is not always correlated to
their syntactic generalization abilities (Hu et al.,
2020), nor with human reading times (Eisape et al.,
2020). Other complex structures that seem diffi-
cult to interpret by LMs are nested constructions,
which may require recursive abilities to be solved.
Recent studies on Italian and English have found
that, although both recurrent and transformer neu-
ral networks achieve near-perfect performance on
short embedded dependencies, their performance
drops to below-chance levels on slightly longer de-
pendencies, unlike humans (Lakretz et al., 2021,
2022). Lampinen (2022), however, questions these
comparisons between humans and LMs, as the for-
mer receive guidance before the experiments, while
LMs are evaluated on zero-shot scenarios, and their
performance improves with few-shot prompts.
Despite the fact that most of the work evaluating
the linguistic capabilities of LMs has been carried
out in English, there exist some experiments that
have focused on Spanish and Galician LMs show-
ing that the LMs tested in this work perform very
well in the context of different linguistic depen-
dencies, including simple and complex agreement
dependencies with distractors. Recent studies in
both Spanish and Galician show that models’ per-
formance for these dependencies (which rely on
morphosyntactic information) are similar to those
in English (with expected variations across models).
For instance, Pérez-Mayos et al. (2021) found that
monolingual and multilingual models achieve even
better performance in agreement resolution in Span-204ish than BERT in English. For Galician, several
experiments showed that the monolingual BERT
models can generalize morphosyntactic agreement
(number and gender) on complex subject-verb and
subject-predicative adjective dependencies (Garcia
and Crespo-Otero, 2022), and that this information
is learned relatively early in the training process
(de Dios-Flores and Garcia, 2022). The syntactic
strengths observed in these models establish a base-
line performance against which we can examine
the results obtained for control dependencies.
Concerning control constructions, studies explor-
ing LMs’ abilities to solve these complex relations
are very scarce. In a recent paper, Kogkalidis and
Wijnholds (2022) trained supervised models that
take advantage of contextualized representations
extracted from BERT, and evaluate them at captur-
ing control verb nesting and verb raising in Dutch.
The results suggest that transformer LMs do not ad-
equately capture the target relations, although fine-
tuning the pre-trained models in one-shot learning
scenarios improves the performance of the probes.
More similar to our study, an initial approxima-
tion by Lee and Schuster (2022) evaluated GPT-2
on object and subject control verbs, using number
agreement with an embedded reflexive pronoun to
track dependency resolution. Their findings sug-
gest generative LMs are unable to differentiate be-
tween these two types of constructions. However,
their manipulations were very limited in scope, as
they only used 5 noun phrases, and 3 control verbs.
Pycholinguistics and control dependencies:
Even though control constructions have been at
the center of linguistic theorizing over the past
decades, their theoretical interest has not translated
into an equivalent amount of experimental research
in the psycholinguistics literature. The key ques-
tion, though, is whether (and how) control infor-
mation is used in parsing. Some early works have
argued that control information was not used dur-
ing initial parsing stages due to its lexico-semantic
nature (e.g., Frazier et al., 1983; Nicol and Swin-
ney, 1989). Nonetheless, these works barely looked
at the contrast between lexically induced subject
and object control relations. In this regard, more
recent eye-tracking investigations have produced
results that could be interpreted as evidence that
lexical control information is used from early pars-
ing stages (e.g. de Dios-Flores, 2021; Betancort
et al., 2006; Kwon and Sturt, 2016) while they also
suggest that object control dependencies seem to besolved faster due to their linear proximity. Yet, to
our knowledge, no previous work provided accept-
ability judgements contrasting subject and object
control dependencies with distractors, which is a
highly informative measurement to establish the
grammatical and psycholinguistic status of such
constructions.
3 The present work
The present work takes control dependencies as an
adversarial case to test LMs’ ability to generalize
grammatical information at the syntax-semantics
interface (Experiments 2 and 3). Given the com-
plexity of these constructions, and the lack of psy-
cholinguistic evidence, we go one step further and
start by evaluating humans’ grammaticality percep-
tion (Experiment 1), not only to obtain a grammati-
cal verification of the acceptability status of such
innovative experimental materials and to be able to
directly compare humans’ and LMs’ performance,
but also to contribute to the scarce psycholinguistic
evidence on the processing of control. The datasets,
code, and results from all the experiments are freely
available.
3.1 Experimental materials
For the main dataset, used in Experiments 1 and
2, the experimental materials consisted of 96
items that had 8 different versions (768 experi-
mental sentences). An example set is shown in
Table 1. The experimental conditions were cre-
ated by manipulating the type of control verb
and the gender of the main clause nouns, while
keeping the gender of the adjective constant. It
is a factorial design that fully crosses the fac-
tors control (subject/object), grammaticality
(grammatical/ungrammatical) and distractor
(match/mismatch). To create the control conditions,
we selected 12 subject and 12 object control verbs
whose control preferences (i.e. subject and object)
had been shown to be robust in a large-sample
cloze task conducted by de Dios-Flores (2021). A
sentence is ungrammatical when the adjective and
the target controller differ in gender. The term dis-
tractor is used to refer to the non-controller NP in
the sentence. A distractor was considered a match
when it matches in gender with the adjective, and a
mismatch when it mismatches in gender.
One of the key elements of our manipulation is
the difference in dependency length between sub-205Subject control
Gramm.Dist. match Maríale prometió a Carmenser más ordenadacon los apuntes.
Dist. mismatch Maríale prometió a Manuelser más ordenadacon los apuntes.
Ungramm.Dist. match Joséle prometió a Carmenser más ordenadacon los apuntes.
Dist. mismatch Joséle prometió a Manuelser más ordenadacon los apuntes.
Object control
Gramm.Dist. match Maríale ordenó a Carmenser más ordenadacon los apuntes.
Dist. mismatch Joséle ordenó a Carmenser más ordenadacon los apuntes.
Ungramm.Dist. match Maríale ordenó a Manuelser más ordenadacon los apuntes.
Dist. mismatch Joséle ordenó a Manuelser más ordenadacon los apuntes.
ject and object control. While subject control con-
structions engage in a discontinuous dependency
where the object NP (the distractor) is interven-
ing, object control dependencies engage in an adja-
cent dependency, where the subject NP (the distrac-
tor) precedes the dependency. Those conditions
in which the two NPs (controller and distractor)
have the same gender are respectively taken as
grammatical and ungrammatical baselines for both
subject and object control sentences. Hence, the
critical conditions are those in which only one of
the NPs agrees in gender with the adjective (i.e.
grammatical sentences with a matching distractor
and ungrammatical sentences with a mismatching
distractor). Humans’ and LMs’ behavior in these
conditions will be essential to ascertain whether
they can accurately implement control-determined
antecedent retrieval processes and whether they are
fallible to interference effects from gender match-
ing but structurally irrelevant antecedents, in a sim-
ilar vein as the attraction effects observed in agree-
ment dependencies (e.g. Bock and Miller, 1991).
While there are very few gender-ambiguous
names in Spanish, in order to maximize gender
transparency, the nouns used to create the materi-
als were carefully selected according to the most
frequent female-only and male-only names on the
official Spanish census. In addition, we created an
adaptation of the main dataset substituting proper
nouns with personal pronouns (e.g. ‘She promised
him to be tidier’), to avoid potential bias, ambigui-
ties or misrepresentations of proper nouns (Shwartz
et al., 2020). Both versions of the dataset (with
nouns and with pronouns) were translated into Gali-
cian by a native speaker linguist, to put Galician
LMs to the test and to check if our findings heldcross-linguistically. These materials were adapted
for the LM prediction task (see section 6.1).
3.2 Pre-trained models
We evaluate the following pre-trained models using
HuggingFace’s transformers library (Wolf et al.,
2020):
Multilingual : mBERT (12 layers) (Devlin et al.,
2019), and XLM-RoBERTa base and large (12 and
24 layers) (Conneau et al., 2020).
Spanish : BETO (12 layers) (Cañete et al., 2020),
and RoBERTa base and large (12 and 24 layers)
(Gutiérrez Fandiño et al., 2022).
Galician : Bertinho small and base (6 and 12
layers) (Vilares et al., 2021), and BERT small and
base (6 and 12 layers) (Garcia, 2021).
4 Experiment 1: human acceptability
The primary goal of this acceptability task is to
determine whether native speakers of Spanish are
able to detect agreement violations that do not con-
form with the control properties of main predicates.
This is, to our knowledge, the first experimental
investigation on control of its kind, and we be-
lieve it is essential to corroborate native speakers’
offline sensitivity to the different control manipula-
tions that will be then put to the test with artificial
LMs. It will be of particular importance to eluci-
date whether comprehenders are able to correctly
distinguish the acceptability of the strings regard-
less of the type of control (subject or object) and
the presence of a gender matching or mismatching
distractor.2064.1 Participants and procedure
40 native speakers of Spanish recruited at the Uni-
versidade de Santiago de Compostela participated
in this experiment. Their participation was vol-
untary and all of them provided informed consent.
Participants were presented with the entire sentence
in the middle of the screen along with a rating scale,
and they could only move to the next one once they
had emitted a rating. They were instructed to rate
the sentences in terms of whether they came across
as well-formed Spanish: 7 meaning totally accept-
able and 1 totally unacceptable. Experimental sen-
tences were intermixed with 96 filler sentences of
similar structure and complexity. The task was
completed by all participants in less than 30 min-
utes.
4.2 Results
The average rating for each condition is shown in
Figure 1. For this and the following experiments,
we carried out a statistical analysis of variance in
order to observe differences among the experimen-
tal conditions. For the sake of clarity and space, the
most relevant significant differences will be marked
with an asterisk in the figures. The statistical analy-
ses revealed a significant main effect of grammati-
cality, such that grammatical sentences (green bars)
received much higher ratings than ungrammatical
ones (red bars). Importantly, there was a signifi-
cant interaction between the factors grammaticality
and distractor. Planned comparisons showed thatthis interaction was driven by a significant effect of
distractor only in ungrammatical sentences. This is
shown in significant higher ratings for the distractor
match condition in ungrammatical sentences com-
pared to distractor mismatch ones. Such an effect
is not present in grammatical sentences. Critically,
no differences were observed between subject and
object control conditions.
In addition, we took the 1-7 ratings produced by
humans and converted them into a binary accuracy
measure by classifying their answers as correct or
incorrect depending on the grammaticality of the
sentence and whether the rating issued was above
or below the sample mean (3.79). As expected,
accuracy was above 85% for all conditions. This
value will allow us to have a more direct compari-
son with the results from Experiment 3.
4.3 Discussion
The results from this experiment clearly show that
native speakers are able to detect agreement viola-
tions that arise when the adjective did not match
in gender the appropriate antecedent, and hence,
that they are able to correctly use control infor-
mation to retrieve the antecedent. This finding
also provides a confirmation that the items display
unequivocal control readings. Crucially, subject
and object control sentences were rated similarly
across all four conditions. In addition to the clear
contrast between grammatical and ungrammatical
conditions, an important result from this experi-
ment is that there is evidence for interference ef-
fects in ungrammatical sentences. That is, ungram-
matical sentences with a matching distractor re-
ceived slightly higher ratings than ungrammatical
sentences with a mismatching distractor. This ef-
fect shows that the presence of a matching distrac-
tor leads them to accept ungrammatical sentences
more often than when the distractor does not match
in gender with the adjective. Crucially, this effect
appeared equally in subject and object control con-
ditions, that is, independently of the position of the
distractor. This represents evidence for a facilita-
tory interference effect, or an illusion of grammat-
icality (Phillips et al., 2011), a pattern akin to the
widely attested agreement attraction effect (Wagers
et al., 2009).
5 Experiment 2: LM acceptability
This experiment aims at observing whether the
probabilities of the language models are similar207to those of humans. That is, whether LMs assign
lower surprisal to grammatical than to ungrammati-
cal sentences regardless of the presence of a match-
ing or mismatching distractor. For this purpose, we
use the exact same dataset as in Experiment 1.
5.1 Procedure
The minicons library (Misra, 2022) was used to
compute the surprisal assigned by the LM to the
embedded adjectives, which function as a proxy
for antecedent retrieval.
5.2 Results
The Spanish models’ results for the different ex-
perimental conditions are shown in Figure 2. It
should be noted that, for ease of interpretation and
comparison with Experiment 1 (Figure 1), the sur-
prisal values were inverted such that higher val-
ues mean less surprisal (hence more acceptability)
while lower values mean more surprisal (hence less
acceptability).While we observe significant ef-
fects of grammaticality for all models (meaning
that, overall, grammatical sentences were more
acceptable than ungrammatical ones), the results
show a very different pattern of contrasts for sub-
ject and object control sentences. On the one hand,
in subject control sentences, all the models showed
higher acceptance for grammatical sentences with
a matching distractor (dark green bars) than for
grammatical sentences with a mismatching distrac-
tor (light green bars). Furthermore, also in subject
control sentences, ungrammatical sentences with
a matching distractor (light red bars) received un-
expectedly high acceptance levels, which, for most
models, are higher than those observed for gram-
matical sentences with a mismatching distractor
(light green bars). On the other hand, in object
control sentences, the pattern of contrasts is very
different. First, none of the models exhibited differ-
ences among the grammatical conditions regardless
of the gender of the distractor. Second, while for
all the models, the values observed for ungrammat-
ical sentences with a matching distractor (light red
bars) were higher than those for ungrammatical
sentences with a mismatching distractor (dark redbars), this difference was only statistically signifi-
cant for some models.
The same pattern of results is observed using
pronouns instead of names (see Figure 4) and for
the Galician models using names and pronouns
(Figures 5 and 6). This is also corroborated by
the very strong correlations ( ρ > 0.9) observed
for the adjective surprisal values using names and
pronouns, in both languages. Furthermore, we cal-
culated the Spearman ρcorrelations between the
acceptability values provided by the humans and
the models’ surprisal values. Overall, they revealed
weak to moderate correlations, while higher corre-
lations are found for object control sentences than
for subject control ones. The correlations for each
model at each experimental condition can be found
in Table 4.
5.3 Discussion
The results for Experiment 2 show that, unlike hu-
mans, all the LMs evaluated behave very differently
for subject than for object control dependencies,
being better at detecting the acceptability of the
strings in object control conditions. The key ques-
tion here is whether they are able to do so by lever-
aging the lexico-semantic information of control in
order to find the correct antecedent. The pattern of
results obtained suggests that, rather than control
information, the relevant cue being used is linear
proximity. It must be reminded that, in subject con-
trol dependencies, the correct antecedent is the NP
that is further away from the adjective, while the
distractor NP is closer to it. The presence of sig-
nificant differences between the two grammatical
conditions, and the two ungrammatical conditions,
found for subject control dependencies, points to
the fact that LMs are taking the closer (and wrong)
NP, the object, as the antecedent. This explains
why the acceptability is reduced for grammatical
sentences with a mismatching distractor, despite
being perfectly grammatical, and that it is dramati-
cally increased for ungrammatical sentences with a
matching distractor, despite being ungrammatical.
Reliance on linear proximity also explains why
LMs are better, and more akin to humans, on object
control dependencies. In these structures, the cor-
rect antecedent (i.e. the object) coincides with the
linearly closest NP. Interestingly, nonetheless, LMs
also exhibit evidence for interference effects from208
control-irrelevant but gender-matching distractors.
This is perhaps clearer in the case of object control
sentences, since linear proximity and interference
converge in the case of subject control ungram-
matical sentences with a matching distractor. In
object control sentences, some models also exhibit
higher acceptability for ungrammatical sentences
with a matching distractor even when in this case
the gender-matching NP is the farthest NP. These
issues are further explored in Experiment 3.
6 Experiment 3: LM masked prediction
This experiment aims at further exploring the be-
havior of LMs using the masked prediction task. In
contrast with Experiment 2, where we compute the
surprisal for the same adjective in a given (gram-
matical or ungrammatical) sentence, our objective
here is to test whether LMs predict grammatically
compatible adjectives in subject and object control
sentences regardless of the presence of a matching
or mismatching distractor. In Experiment 2 the
adjective’s gender was kept constant across experi-
mental conditions, and hence, we could not assess
LMs’ preferences for the masculine or feminine
version. By contrast, here we test if LMs predict
grammatically compatible adjectives in subject and
object control sentences by directly comparing the
probabilities of a given adjective in its masculine
or feminine form, something which provides uswith more comprehensive information in this re-
spect. Furthermore, evaluating model accuracy
rather than surprisal values will also allow us to as-
sess and compare the performance across models.
6.1 Experimental materials
The experimental materials used for Experiment 3
are an adaptation of the dataset described in sec-
tion 3.1 (including its variants with personal pro-
nouns and Galician translations) so that they could
be used in the masked prediction task. This al-
lows us to evaluate our dataset in the two possible
gender configurations, expanding it such that each
sentence has two possible outcomes: a grammat-
ical and an ungrammatical one. Therefore, the
manipulation is a 2x2 factorial design ( control x
distractor ), as shown in Table 2
6.2 Procedure
We rely on the standard approach for targeted
syntactic evaluation to obtain the accuracy of the
models on the minimal pairs (Linzen et al., 2016;
Warstadt et al., 2020). For each sentence, we
extract the probabilities of the grammatical and
ungrammatical target adjectives, and consider a
trial as correct if the model gives a higher prob-
ability to the grammatical target adjective. It is
worth noting that this method requires compatible
tokenization between both variants (grammatical
and ungrammatical). To make a fair evaluation,209Subject control
Dist. match Maríale prometió a Carmenser más [ordenada|*ordenado]con los apuntes.
Dist. mismatch Maríale prometió a Manuelser más [ordenada|*ordenado]con los apuntes.
Object control
Dist. match Maríale ordenó a Carmenser más [ordenada|*ordenado]con los apuntes.
Dist. mismatch Joséle ordenó a Carmenser más [ordenada|*ordenado]con los apuntes.
we check if both variants appear as single tokens
in the models’ vocabulary, or whether their last
subtokens (the ones that carry the morphosyntactic
information) are comparable so that we can use
their probabilities. For instance, the Spanish pair
afectuoso|afectuosa (‘affectionate’) is tokenized
by RoBERTa as afect+uoso|uosa , and hence, we
can use the last subtokens for comparison. How-
ever, desconfiado|desconfiada (‘skeptical’) is di-
vided as desconf+iado anddescon+fi+ada . We
discard these incompatible cases (19% of the items
for Spanish, and 16% for Galician, on average).
6.3 Results
Table 3 displays the global accuracy for all the mod-
els under evaluation in Experiment 3 (global accu-
racy values for all the datasets tested in Spanish and
Galician are in Table 5). RoBERTa large emerges
as the best performing model, closely followed by
XLM RoBERTa large, while mBERT base emerges
as the worst performing model. Nonetheless, in
order to analyze the impact of linear proximity on
model performance, it is essential to examine the
factors control anddistractor separately. Fig-
ure 3 shows the accuracy per condition for the tar-
get adjectives. Statistical analyses show a main ef-
fect of distractor, such that the accuracy was higher
for distractor match sentences (dark green bars,
when the two NPs had the same gender) than for
distractor mismatch ones (light green bars, when
the NPs differed in gender). However, this dif-
ference was much more acute for subject control
sentences, where significant differences arise for
all the models, than for object control sentences,
where significant differences are only found forRoBERTa-large and XLM-RoBERTa-base. The
same pattern of results is observed using pronouns
instead of names (see Figure 7) and for the Gali-
cian models using names and pronouns (Figures 8
and 9). This is also shown in the very strong cor-
relations ( ρ > 0.8) observed for the results using
names and pronouns in both languages.
Model Accuracy
BETO base 0.78
RoBERTa base 0.77
RoBERTa large 0.83
mBERT base 0.61
XLM RoBERTa base 0.78
XLM RoBERTa large 0.82
6.4 Discussion
The results from Experiment 3 reinforce and com-
plement the findings from Experiment 2 in several
respects. First, reliance on linear proximity is, if
anything, even clearer, as subject control sentences
with a mismatching distractor display clear inter-
ference effects, which are materialized in a dra-
matically below-chance accuracy. These are the
cases in which the distractor is the sentence ob-
ject, which is also the closer NP. In these cases,
LMs’ predict a target adjective that agrees in gen-
der with the object, rather than the subject (i.e. the
correct antecedent) and hence, demonstrating that
antecedent retrieval processes unfold disregarding
the lexico-semantic information on control. Im-
portantly, these effects are almost absent in object
control sentences, where only two models show
evidence for interference effects, these being much
less pronounced (only a few accuracy points). Even
though the results from this experiment cannot be
directly compared with those of humans (Experi-
ment 1), it should be noted that human accuracy
was above 80% for all conditions.210
7 General discussion and conclusions
The empirical evidence gathered in this work pro-
vides a very straightforward picture: whereas hu-
mans’ can coordinate lexico-semantic and syn-
tactic information in order to determine the
(un)acceptability of control structures, LMs resort
to a heuristic based on linear proximity, disregard-
ing control information. These findings are robust,
as they replicate across tasks (acceptability and
masked prediction), models (monolingual and mul-
tilingual), languages (Spanish and Galician LMs),
and type of antecedent (names and pronouns). Fur-
thermore, they go in line with evidence advanced
in Lee and Schuster (2022) for English with respect
to autoregressive language models.
These findings contrast with those obtained for
superficially similar dependencies like subject-verb
agreement, in which these models have been at-
tested to display accurate levels of performance
for Spanish and Galician (Pérez-Mayos et al.,
2021; de Dios-Flores and Garcia, 2022; Garcia and
Crespo-Otero, 2022). Crucially, however, agree-
ment and control dependencies engage different
types of linguistic information. While the former
rely on co-ocurring patterns containing overt mor-
phological cues which are pervasive in the training
data, control dependencies rely on abstract lexico-
semantic properties of verbs and verb meaning,
which these models are not able to generalize from
the training data at their disposal even when it pre-sumably contains control verbs (although a system-
atic examination of this issue is essential).
Control verbs and control structures have a high
frequency in natural language and, ideally, state-of-
the-art LMs should be able to capture their mean-
ing differences and the consequences they have
for phrase-structure relations (ultimately, who does
what to whom?). Some authors have suggested
that their performance on similar structures could
be improved in one-shot learning scenarios, or by
adding more control constructions in the training
data (Kogkalidis and Wijnholds, 2022; Lee and
Schuster, 2022). While this supports the idea that
these constructions are “learnable” with sufficiently
explicit input, adding examples on the infinite com-
binatorial possibilities of language does not seem
like a strategy that can be generalized. Further re-
search is needed on how LMs capture linguistic
generalizations and how these processes can be
enhanced.
One of the biggest challenges of working with
control constructions is the elaboration of appropri-
ate experimental materials. This is why the care-
fully curated Spanish and Galician datasets used
in this work, which are freely available, represent
a key contribution, as we hope they are valuable
for further computational and psycholinguistic re-
search beyond English, the dominant language in
these fields.211Limitations of the work
Given that the training data for most pre-trained
models has not been released, further investigation
of the frequency effects of control verbs in the cor-
pora, or for that matter, of any other critical word
in the sentence (names, adjectives, etc.) is not fea-
sible. This is a shortcoming of our work because
word frequency during training is known to be an
important factor for model performance (Wei et al.,
2021). Nonetheless, in order to approximate this
issue, we run preliminary comparisons of the mod-
els’ performance depending on whether the control
verb appears or not in the vocabulary (and there-
fore, assuming that it had enough frequency in the
training corpus). Very similar results were obtained
for both sentences with known and unknown verbs
in the main clause.
Besides, detailed comparisons between models
have been left out for reasons of space and scope,
since the objective of the research was not to com-
pare model performance, although it is a relevant
and interesting issue in itself (for instance, the fact
that the LMs based on the RoBERTa architecture
performed better across tasks, or that the high per-
formance of XLM-RoBERTa contrasts with that
of mBERT). In relation to this, the comparison of
models with different architectures and training ob-
jectives (e.g. generative models) was also left for
further research.
Finally, it is worth noting that the two languages
evaluated in this study (Spanish and Galician) are
very similar, so that it could be interesting to ex-
pand the research to non-romance languages.
Ethics Statement
Experiment 1 complied with the standards of re-
search involving human subjects. Their participa-
tion was voluntary, all of them were informed of
the nature of the task, and provided informed con-
sent before starting the experiment. With respect
of C02 consumption for the computational exper-
iments (Experiments 2 and 3), it should be noted
that we used pre-trained models and hence the im-
pact of the calculations obtained is expected to be
minimal. The experiments were run on a NVIDIA
A100 GPU, and the results were obtained in a few
minutes. Since this work is circumscribed within
basic research on artificial language modelling, no
applications or tools are to be directly derived by it
and hence, we do not think of any potential harms
or bias that can be derived from our work.Acknowledgements
We would like to thank the anonymous review-
ers for their valuable comments. This research
was funded by the Galician Government (ERDF
2014-2020: Call ED431G 2019/04, and ED431F
2021/01), by MCIN/AEI/10.13039/501100011033
(grants with references PID2021-128811OA-I00
and TED2021-130295B-C33, the former also
funded by “European Union Next Generation
EU/PRTR”), by a Ramón y Cajal grant (RYC2019-
028473-I), and by the project “Nós: Galician in
the society and economy of artificial intelligence”
(Xunta de Galicia/Universidade de Santiago de
Compostela).
References212213214Appendix
A Correlations between human acceptability judgements (Experiment 1) and LMs
surprisal measurements (Experiment 2)
BETO RoBERTa-b RoBERTa-l mBERT XLM-b XLM-l
Item Adj Sent Adj Sent Adj Sent Adj Sent Adj Sent Adj Sent
Avg 0.31 0.17 0.31 0.27 0.33 0.25 0.27 0.11 0.42 0.17 0.53 0.24
Subj 0.21 0.19 0.27 0.26 0.25 0.23 0.09 0.08 0.37 0.14 0.44 0.19
Obj 0.39 0.16 0.34 0.30 0.41 0.28 0.44 0.14 0.47 0.21 0.61 0.30Match 0.03 0.23 -0.01 0.30 -0.09 0.15 0.07 0.23 0.07 0.22 0.12 0.28
Mism 0.12 0.18 0.16 0.30 0.17 0.22 0.01 0.24 0.01 0.29 0.08 0.38Match -0.05 0.02 -0.07 -0.16 -0.04 -0.17 0.10 -0.01 0.09 -0.23 0.06 -0.34
Mism 0.07 0.26 -0.10 0.09 -0.05 0.02 0.05 0.03 0.06 0.05 0.09 0.08Match -0.04 0.10 -0.19 0.15 -0.14 0.14 -0.06 -0.01 -0.04 0.10 -0.01 0.18
Mism -0.01 -0.04 -0.03 0.22 -0.09 0.14 -0.01 -0.05 -0.03 0.16 0.01 0.23Match -0.04 0.06 0.09 0.08 -0.01 0.10 -0.25 -0.17 -0.12 -0.02 -0.20 0.03
Mism 0.23 -0.11 0.07 0.16 0.08 0.00 0.02 -0.00 -0.04 -0.00 0.03 0.04
B Additional figures for Experiment 2215216C Additional tables and figures for Experiment 3
Spanish datasets
LMsNames
target adjectiveNames
top NPronouns
target adjectivePronouns
top N
BETO base 0.78 0.80 0.72 0.73
RoBERTa base 0.77 0.78 0.74 0.75
RoBERTa large 0.83 0.84 0.81 0.81
mBERT base 0.61 0.68 0.59 0.66
XLM RoBERTa base 0.78 0.79 0.83 0.85
XLM RoBERTa large 0.82 0.78 0.86 0.76
Galician datasets
LMsNames
target adjectiveNames
top NPronouns
target adjectivePronouns
top N
Bertinho small 0.63 0.65 0.68 0.71
Bertinho base 0.61 0.63 0.66 0.69
BERT small 0.71 0.73 0.70 0.72
BERT base 0.74 0.79 0.73 0.75
mBERT base 0.60 0.69 0.59 0.68
XLM RoBERTa base 0.78 0.78 0.79 0.80
XLM RoBERTa large 0.82 0.78 0.84 0.74217218219220ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations section
/squareA2. Did you discuss any potential risks of your work?
Ethics statement
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Data, models or other artifacts used are properly cited in the relevant sections of the experiments.
Mainly 3.2. for pre-trained models and transformers library, or 5.1 for the procedure of Experiment 2
using the minicons library.
/squareB1. Did you cite the creators of artifacts you used?
In different sections where these are described. Mainly 3.2. for pre-trained models and transformers
library, or 5.1 for the procedure of Experiment 2 using the minicons library.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The artifacts used are freely available. We do not discuss their license terms in our contribution.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
For the artifacts we create, we specify they are freely available (and are added as supplementary
materials).
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We do not speciﬁcally discuss this on the paper because our data did not contain personal information
or offensive content. Some notes are added on the ethics statement.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
3.1. for the datasets created, and 4.1. for the demographics of the human sample.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
In 3.1. for the datasets.221C/squareDid you run computational experiments?
Sections 5 and 6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
We use pre-trained models. Some notes are added on the ethics statement.
/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Not applicable. Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Results section: 4.2, 5.2, and 6.3
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4.1
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
We reported a summary in section 4.1. The acceptability task is a wide-spread method and this is
why the full detailed instructions were not provided.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 4.1
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Section 4.1
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
It was not required by the institution at the time of data collection.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Section 4.1222