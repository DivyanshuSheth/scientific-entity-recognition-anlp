
Seungju HanBeomsu KimJin Yong YooSeokjun Seo
Sangbum Kim Enkhbayar Erdenee Buru Chang
Hyperconnect
{
Abstract
In this paper, we consider mimicking fictional
characters as a promising direction for building
engaging conversation models. To this end, we
present a new practical task where only a few ut-
terances of each fictional character are available
to generate responses mimicking them. Fur-
thermore, we propose a new method named
Pseudo Dialog Prompting (PDP) that gener-
ates responses by leveraging the power of large-
scale language models with prompts containing
the target character’s utterances. To better re-
flect the style of the character, PDP builds the
prompts in the form of dialog that includes the
character’s utterances as dialog history. Since
only utterances of the characters are available
in the proposed task, PDP matches each utter-
ance with an appropriate pseudo-context from
a predefined set of context candidates using
a retrieval model. Through human and auto-
matic evaluation, we show that PDP generates
responses that better reflect the style of fictional
characters than baseline methods.
1 Introduction
How would you feel if you could talk to your fa-
vorite character?
In recent years, open-domain conversation mod-
els (Adiwardana et al., 2020; Roller et al., 2021)
have achieved remarkable progress with the devel-
opment of large-scale language models (Radford
et al., 2019; Brown et al., 2020). Meanwhile, recent
studies have suggested several directions reflecting
desirable traits of real-life conversation to make
open-domain conversation models more engaging
beyond plain chit-chat. Style-controlling conversa-
tion models generate responses in the target styles
such as emotion (Zhou et al., 2018; Demszky et al.,
2020) and empathy (Rashkin et al., 2019). Persona-
grounded conversation models (Zhang et al., 2018a;
Kim et al., 2020; Majumder et al., 2020) produceFigure 1: Illustration of PDP. The retriever matches
pseudo-context for utterances from the character, and
utilizes them in a prompt while generating the response.
responses that preserve consistent personalities by
leveraging personal descriptions (e.g., "I have two
dogs"). In this paper, we consider mimicking fic-
tional characters as a promising direction for build-
ing engaging conversation models.
When it comes to building conversation models
that mimic fictional characters, two major chal-
lenges prevent us from directly applying previous
models designed for conditional response gener-
ation: (1) It is difficult to define fictional charac-
terswith only a few descriptions, as in persona-
grounded conversation models. Furthermore, it
is not expressive enough to represent characters’
styles with discrete labels (e.g., angry, happy), as
style-controlling conversation models do. (2) There
lacks sufficient dialog data of fictional characters
for training conversation models. It is inefficient
to manually create dialog datasets of characters for
training, especially considering that additional data
is needed for each new character.
To address these two challenges, we propose
a new task where only a few utterances of the fic-
tional characters are available to generate responses
mimicking the characters. Such setting is justified
by the two following reasons: (1) Utterances of
fictional characters provide useful clues for gen-5114erating responses mimicking the characters as the
personal traits or styles of speakers are inherent in
their utterances (Boyd et al., 2020; Li et al., 2020).
(2) Collecting only a few utterances of target char-
acters is a cost-effective scenario compared to con-
structing the full dialog data consisting of context
and utterance pairs; this allows us to extend our
method to a new character easily.
To perform the task, we introduce Pseudo Di-
alog Prompting (PDP), a method that builds
prompts using a few numbers of target characters’
utterances to leverage the power of pre-trained lan-
guage models. We claim that designing the prompt
in the form of dialog that includes the character’s
utterances as dialog history (as in Figure 1) is an ef-
fective method for reflecting the style of character.
However, since only utterances of the characters
are available in the proposed task, we match each
utterance with an appropriate pseudo-context by
using a retrieval model (Humeau et al., 2019) to
select the relevant context from a predefined set of
context candidates. Through human and automatic
evaluation, we show that PDP generates responses
that better reflect the style of fictional characters
than existing baseline models.
2 Method
We model a conversation agent that generates a re-
sponse rcorresponding to a given context xwhile
mimicking an arbitrary character with kutterances
{u, u,···, u}of the character. The simplest
way to design the prompt with the character’s ut-
terances is to concatenate utterances as Madotto
et al. (2021) does for PersonaChat (Zhang et al.,
2018a). However, in our preliminary experiments,
we observed that this method tends to generate
dull responses that do not reflect the styles of the
character (will be shown in Section 4). We hypoth-
esize that the language model fails to utilize the
utterances because such a format of the prompt is
unlikely to have appeared naturally in the training
set (Brown et al., 2020; Wei et al., 2021).
To address this issue, we propose PDP, which
builds a dialog format prompt where character utter-
ances are included in the dialog history, as depicted
in Figure 1. Since a speaker tends to maintain a
consistent style throughout the conversation, using
such a prompt will induce the language model to
generate responses that seamlessly reflect the style
from the character’s utterances. To build a dialog
when only given the utterances of the character, werequire a pseudo-context cmatching each utter-
anceuto get a context-utterance pair (c, u). We
use a retriever Rto acquire a pseudo-context c.
Particularly, we employ Bi-encoder (Humeau et al.,
2019) as our retriever R. We first define a fixed
set of single-turn context candidates Cobtained
from BST dataset (Smith et al., 2020b), which is
the largest open-domain conversation dataset re-
leased to date. We then select a candidate as the
pseudo-context cfor the given utterance uusing
R. Bi-encoder maps the context cand the response
rinto the embedding space as e(c)ande(r),
respectively. Bi-encoder is trained to represent the
relevance score between a context cand response
rwithe(c)·e(r). There are several variants
to select the pseudo-context cas follows:
•Static Match selects a pseudo-context cthat can
coherently precede the given utterance uusing
the retrieval model R. Given u,Rcalculates
a score sfor each c∈ C bys(c;u) =
e(c)·e(u). We set the pseudo-context c
ofuasc=argmaxs(c;u). We name this
variant static since the selected pseudo-context
cdepends only on the given utterance u.
•Dynamic Match selects a pseudo-context crel-
evant to the input context xin addition to u.
Given xandu,Rcalculates a score sfor
eachc∈ Cbys(c;x, u) =e(c)·e(x) +
s(c;u). We set the pseudo-context cofuas
c=argmaxs(c;x, u). Since language mod-
els quickly adapt to the context-response map-
ping of the given prompt via in-context learning,
we believe providing pseudo-contexts that are se-
mantically similar to the input context as in Dy-
namic Match facilitates the reflection of styles in
corresponding utterances. We name this variant
dynamic because the pseudo-context cdepends
on the varying input context x.
•Random Match selects a pseudo-context cran-
domly from the context candidates set Cwithout
using R. This variant is used as a baseline to
study the effect of the pseudo-context c.
Finally, all the kpairs (c, u)of the character are
sorted by e(x)·e(u)in ascending order and
are concatenated into a prompt in a dialog format.
3 Experiments
3.1 Evaluation
We employ the HLA-Chat (Li et al., 2020) dataset
to define the set of characters for evaluation. HLA-
Chat consists of single-turn dialogs of characters5115in various TV shows. We select ten characters
among all the characters and manually curate eight
utterances that best reveal each character’s unique
characteristics from their utterances in the dataset.
Note that we consider that creating eight utterances
is feasible even if new characters are given and
we also empirically observed that language models
adequately reflect each character’s unique charac-
teristics from the eight utterances.
In evaluating the performance of each method,
we focus on two criteria: (1) Does the model’s re-
sponse reflect the style of a given character? (2)
Does the model respond coherently to the given di-
alog context? To examine these two criteria, we run
the model on fixed dialog contexts and calculate
metrics that exhibit the style reflection and dialog
coherency. We use the utterances of the test split of
DailyDialog (Li et al., 2017) for dialog contexts.
Human Evaluation. We conduct a human evalua-
tion to assess the quality of the generated responses.
First, we select five characters which style can be
distinguished apparently. We then randomly sam-
ple 50 contexts from the full fixed-context set of the
characters. Using Amazon MTurk, we collect hu-
man annotations for the samples contexts. Human
evaluators are asked to rate from 0 to 2 scale score
how each model response (1) strongly reveals the
style of a given character ( Style Strength ) and (2)
whether a response is fluent and appropriate for a
given dialog context ( Appropriateness ). To reduce
annotator bias and inter-annotator variability, we
apply Bayesian Calibration (Kulikov et al., 2019)
to the human evaluation score.
Automatic Evaluation. Similar to the previous
works on text style transfer (Li et al., 2018a; Riley
et al., 2021; Smith et al., 2020a), we utilize a char-
acter classifier trained on the utterances in HLA-
Chat to measure the style strength of the generated
responses. We denote StyleProb as the classifier’s
average probability of predicting a target character.
We use StyleProb instead of Style Accuracy since
HLA-Chat has a class imbalance issue so that the
performance on infrequent classes are hard to be
measured by accuracy. For measuring coherency,
we use MaUdE (Sinha et al., 2020), an automated
dialog evaluation metric known to capture human
judgment on the coherency of response.
3.2 Pre-trained Language Model
For all the methods, we use a decoder-only trans-
former of 3.8B parameters, denoted as Base-LM ,as a base language model. To make Base-LM ac-
quire general language skills and better understand
conversations, we train Base-LM on The Pile (Gao
et al., 2020) and an additional corpus of public web
documents.
3.3 Baseline Methods
Only Utterances. Instead of utilizing pseudo-
context as suggested in our methods, we provide
the set of character utterances as the "quotes of
character during conversation" in the prompt. Com-
paring PDP with this method will verify the effect
of pseudo-contexts.
Zero-shot Prompting. In this method, we only in-
clude the name of the character and the show in the
prompt without using utterances of the character.
The format of the prompt is similar to the prompt
of Madotto et al. (2021) for controlled generation.
TextSETTR (Riley et al., 2021). We first construct
a dialog prompt similar to Zero-shot Prompting
(but without character information) and use it with
Base-LM to generate plain responses. Then, we use
TextSETTR, a few-shot text style transfer model
that can transfer arbitrary styles without additional
training, to transfer the style of plain responses to
the target character’s style.
GCC (Boyd et al., 2020). GCC is a method to
control a user persona by utilizing the user’s con-
versation history by concatenating users’ previous
utterances before input dialog context. Still, it has
the drawback that it requires further training on a
large-size character-conditioned dialog dataset.
3.4 Advantaged Methods
Unlike baseline methods that only have access to a
few utterances of characters, advantaged methods
also have access to additional data, which gives
them an unfair advantage over other methods.
HLA-Chat Full-dataset Fine-tuning. We fine-
tune Base-LM on the full HLA-chat dataset. In
this method, character information is injected by
concatenating the character’s name and the show’s
name at the front of the dialog input.
Gold Match. Instead of using pseudo-context, this
model uses the actual contexts corresponding to
character example utterances annotated in the HLA-
chat dataset.
Details for all methods and experiments are fur-
ther described in Appendix.5116
4 Results
Table 1 shows the experimental results. Overall,
our proposed PDP demonstrates far better style
reflection scores on both human evaluation and au-
tomated metrics than all baseline methods – and
even better than advantaged methods. In particu-
lar, PDP shows significantly higher style reflection
scores compared to Only Utterances . Consider-
ing that the core difference between the prompt
of PDP and that of Only Utterances is the pres-
ence of pseudo-contexts, this result demonstrates
that providing a dialog-formatted prompt is highly
effective at reflecting the styles of a character.
While PDP methods generally report better style
reflection scores than baseline methods, we ob-
serve that the performance on style reflection and
response coherency varies to some extent depend-
ing on how pseudo-context is selected. Static
Match shows the highest response coherency scores
among all variants of PDP while performing a little
bit worse than Dynamic Match in terms of style
reflection metrics. On the other hand, DynamicMatch shows the best performance on style re-
flection metrics, where it losses some coherency.
This observation confirms our hypothesis that us-
ing pseudo-context cthat is semantically similar
to the input context xis effective for utilizing styles
from the character’s utterances. Thus, the choice
between Static Match andDynamic Match depends
on which of the two qualities – style and coherency
– is more important. Lastly, Random Match , which
is considered a simple ablation baseline, also shows
reasonably high performance in terms of style re-
flection metrics. We plan to analyze the Random
Match method in a follow-up study since it is un-
expected that such a simple baseline shows high
performance.
Discussion. Gold Match shows worse perfor-
mance in style strength than PDP. We believe that
the gold context-response pairs in the HLA-Chat
are not always the most appropriate pairs for our
experiments. Since the HLA-Chat originated from
scripts of TV shows, there might be some addi-
tional contexts outside of a single-turn dialogue
(e.g., the background of characters, events that hap-
pened before the dialogue, audio-visual informa-
tion, etc.). Without understanding the context be-
hind the scripts, even gold context-response pairs
might seem irrelevant. Therefore, directly using
the context-response pairs in HLA-Chat as in Gold
Match could adversely affect the quality of subse-
quent responses in style strength and coherency.
PDP methods tend to have slightly lower re-
sponse coherency scores compared to other base-
lines. Our speculations for this phenomenon are
as follows. Pseudo-dialog pairs (c, u)created
by PDP methods might have some degree of in-
coherency, and it might incur adverse effects in
coherency via in-context learning in the language
model. The fact that the response coherency score5117
ofStatic Match is higher compared to Dynamic
Match , which finds a pseudo-context that is more
similar to the input context, or Random Match,
which finds a random pseudo context at all, sup-
ports this claim. Additionally, automated metrics
like MaUdE are tuned to work with texts in stan-
dard dialog style. Since responses that strongly
reflect character styles (e.g., "Yippie ki-yi-yay!" in
Figure 1) are out-of-domain examples when put
next to standard texts, there might be an unavoid-
able decrease in MaUdE scores. An interesting
future work would be finding a method that does
not reduce response coherency while also success-
fully reflecting the character styles.
Applicability of PDP to other language mod-
els. We further evaluate our method by leveraging
different language models instead of Base-LM to
verify that our method generally works well on any
language model. We use three pre-trained language
models, GPT-J 6B (Wang and Komatsuzaki, 2021),
GPT-Neo 2.7B (Black et al., 2021), and GPT2-
xl 1.5B (Radford et al., 2019), which are publicly
available. Similar to our main experiments, we con-
duct the automatic evaluation with these language
models.
The results are shown in Table 3. The overall
trend of the results is similar to the results using
Base-LM as a pre-trained language model (Table 1).
This common trend shows that mimicking charac-
ters through the PDP method can be generally used
not only with Base-LM but also with other pre-
trained language models.5 Conclusion
In this paper, we introduce the task of mimicking a
fictional character by using only a few utterances of
the character. We propose a new method, Pseudo
Dialog Prompting, which builds a prompt for a
language model to solve this task by creating a
pseudo dialog using the given utterance set with a
retrieval model. Extensive experiments show that
our method effectively generates responses that
reflect the style of a given character better than
baseline models and even advantaged models.
Ethical Considerations
Like any conversation or generation model, we note
that the quality of the models’ responses depends
on the quality of its training data. Our Base-LM
model was trained on The Pile dataset (Gao et al.,
2020) and Pushshift Reddit dataset (Baumgartner
et al., 2020). Since the contents in these datasets
were collected online, they may include underlying
biases or potentially offensive words. These biases
and toxicities can be projected into our models.
Therefore, we highly recommend that additional
steps are taken to filter out profanity and inappro-
priate responses when the model is deployed to the
real world.
Furthermore, while we intend our method to be
used to mimic fictional characters from movies,
shows and stories to build more engaging conversa-
tion models, we also recognize that it is possible to
use our method to mimic real-life individuals based
on their utterances. Some potential risks include
impersonating individuals, which can be harmful
to the targeted individuals, and mimicking figures
to generate content that can be harmful to groups of
individuals. We hope that our method is deployed
in a safe manner to avoid such malicious usage.
References5118511951205121Appendix
A Related Work
A.1 Text Style Transfer
There are various studies of text style transfer,
which are not bound for open-domain conversa-
tion. These studies utilize task-specific parallel data
for style transfer (Jhamtani et al., 2017; Rao and
Tetreault, 2018; Chawla and Yang, 2020). However,
since obtaining parallel data requires a substantial
amount of labor, many studies have been proposed
to address unsupervised text style transfer recently.
One line of the studies addresses unsupervised
text style transfer by constructing pseudo-paired
texts and training a model on those paired texts.
Subramanian et al. (2018); Zhang et al. (2018b)
create those parallel texts by back-translation and
Lai et al. (2021) construct pseudo-parallel paired
texts using generic resources and fine-tune two gen-
eration models on these pseudo parallel texts iter-
atively. However, these methods require a further
step to create parallel data by synthesizing or lever-
aging existing resources and train generation mod-
els on those pairs. Moreover, these methods are
not applicable for arbitrary text style transfer since
the methods target predefined style pairs only (e.g.,
British-American and Modern-Shakespeare).
Another line of studies solves unsupervised text
style transfer by disentangling content and style
from texts. Most of the studies (Shen et al., 2017;
Li et al., 2018a) assume that enough style-labeled
texts are available for training. Ma et al. (2021)
utilize a collaborative learning framework to disen-
tangle content and style from the texts, but it also
requires style-labeled texts while training gener-
ation models. Zhao et al. (2018) consider a sce-
nario where only target style labels are available.
Since our work considers the task where only a
few utterances of characters are available to gener-
ate responses, we do not consider these methods
requiring style-labeled texts as baseline methods
of evaluation. Instead, we select TextSETTR (Ri-
ley et al., 2021), which extracts style vectors from
generic texts without requiring style-labeled texts,
as a baseline method for a fair evaluation.
A.2 Stylized Response Generation
There are several studies that directly address styl-
ized response generation, which is a special case
of text style transfer. Similar to text style transfer,
stylized response generation can also be dividedinto supervised (Akama et al., 2017) and unsuper-
vised ways (Gao et al., 2019; Zheng et al., 2020).
In particular, Gao et al. (2019) utilize conversation
data with distinct style-labeled texts to models a
shared latent space. Zheng et al. (2020) utilize un-
paired texts that have distinct styles and convert
them into pseudo conversation pairs using inverse
model. Finally, these pseudo conversation pairs are
employed to train a generation model with a joint
training process. However, the above studies do not
meet our problem condition since they require a
considerable amount of style-labeled texts or need
further training procedure and target only specific
styles.
Several stylized response generation studies
could be applicable to our setting. Boyd et al.
(2020) introduce a method to reflect arbitrary user’s
style by utilizing the user’s conversation history
without requiring additional fine-tuning. Madotto
et al. (2021) utilize prompt-based few-shot learn-
ing to control style of generated responses. We ex-
tend Madotto et al. (2021)’s framework to stylized
response generation as a baseline method (Zero-
shot Prompt) by providing a proper prompt.
B Model Details
Pseudo Dialog Prompting Details. Like all other
baseline models, we also employ Base-LM to gen-
erate responses by conditioning it with a prompt
built by Pseudo Dialog Prompting method. For
the retrieval-based conversation model Rused for
Pseudo Dialog Prompting, we use a 256M param-
eter Bi-encoder (Humeau et al., 2019) retrieval
model trained with the method of Kim et al. (2021),
along with the utterances of Blended Skill Talk
training dataset as the fixed set of context candi-
datesC. Table 4 shows the prompt template and
an example for the character for Pseudo Dialog
Prompting.
Base-LM Training Details. The sizes of the
datasets are both 700G for the Pile and the
Pushshift Reddit comment dataset, respectively.
For the Pushshift Reddit comment dataset, we use
the comment created up to April 2020. For the
hyperparameters of the model, we use 32 as the
number of layers, 3072 as the number of units in
each bottleneck layer, and 32 as the number of at-
tention heads. For the tokenizer, we use the same
byte-level BPE tokenizer as in GPT-2 (Radford
et al., 2019). We use an initial learning rate of
1.6×10and batch size of 512 for the training5122
hyperparameters and follow other configurations
from Brown et al. (2020). The model is trained for
a total of 300 billion tokens, which takes approxi-
mately 21 days using 64 NVIDIA A100 GPUs.
GCC Training Details. We reproduce GCC with
three minor modifications: First, we train the model
with the HLA-chat dataset instead of the Reddit
comment dataset. Secondly, we do not include
a context (notated ’parent comment’ in the orig-
inal paper) of reference histories since only the
utterances of a character are available in our task
setup. Lastly, we do not utilize token-type em-
beddings since dialogs in HLA-chat only consist
of two speakers. The HLA-Chat dataset is divided
into an 8:1:1 split based on character, and each split
is used as train, validation, and test split, respec-
tively. While constructing a dataset, we omit ten
characters selected for our evaluation for fair com-
parison as a baseline. For reference contexts, we
randomly sample a maximum of eight utterances
of a character, excluding the gold response itself.
We fine-tune the model from Base-LM using the
data format of Table 5 with the hyperparameter of
input length 1024, initial learning rate 1.0×10with cosine decay schedule with 100 warmup steps,
10 training epochs, and the batch size 128. We use
the early-stopped model using the validation split
perplexity.
Full-dataset Fine-tuning Training Details. We
fine-tune Base-LM on full HLA-Chat dataset, us-
ing a data format of Table 6. Similar to GCC,
HLA-Chat data is divided into an 8:1:1 split, but
here ten characters selected for evaluation are con-
tained in the training set. We fine-tune the model
from Base-LM using the hyperparameter of input
length 1024, initial learning rate 1.0×10with
cosine decay schedule with 100 warmup steps, 10
training epochs, and the batch size 128. We also
early-stopped fine-tuning using the validation split
perplexity.
Prompts for Baseline Methods. Tables 7,
8, 9 show the prompt template and an example
for the character for each baseline methods. Here,
we assume we only have two utterances from the
character.5123
C Evaluation Details
Decoding Options When we generate samples,
we adopt a top-k decoding strategy which is
widely used for generating diverse and specific re-
sponses (Fan et al., 2018). We use k= 20 for
our top-k sampling. We choose a minimum beam
length and a beam size as 10 and 5, respectively,
and use 5-gram beam blocking.
Automatic Evaluation For the automatic evalu-
ation, we choose ten characters among all char-
acters included in HLA-Chat. We construct the
test set consisting of 5903 utterances by selecting
only utterances with a length of 30 or more from
among the utterances from DailyDialog test set. We
use the utterances of the test split of DailyDialog
dataset for fixed dialog contexts to construct dialog
contexts that are typical and not dependent on spe-
cific characters. For the StyleProb metric, we train
a character style classifier using the utterances from
ten selected characters in the HLA-chat dataset. We
collect the utterances of ten evaluation characters
from the dataset and train a 10-class classifier by
fine-tuning the RoBERTa-base model. We use Hug-
gingface transformers (Wolf et al., 2020) to train
the model, and use the learning rate 2.0×10,
batch size 128, the number of training epochs 3.The accuracy of the classifier on the validation split
is 0.5838. For calculating the MaUdE metric, we
use the code officially provided by the authors.
Human Evaluation For the human evaluation,
we select five characters which style can be dis-
tinguished apparently. Additionally, we use the
randomly selected subset of the full fixed-context
set consisting of 50 contexts. We use Amazon
MTurk for collecting assessments, and Figure 2
shows the instructions and the interface for the hu-
man evaluation. We mitigate the bias from the
annotator by setting a maximum number of annota-
tions per worker as 20 and randomly shuffling the
order of the model and the corresponding response.
To control the annotation quality, we only allow the
annotators who satisfy the following requirements:
(1) HITs approval rate greater than 95%, (2) Lo-
cation is one of Australia, Canada, New Zealand,
United Kingdom, and the United States, (3) Life-
time number of HITs approved greater than 1000,
following Li et al. (2018b). We estimated that each
HITs takes around 1.5 minutes on average (87 sec-
onds per each HIT estimated by the 85th percentile
of response times) and set the payment to USD 10
per hour. Therefore, annotators are paid USD 0.255124
per HITs.
Descriptive Statistics. We provide the 95% con-
fidence interval of human evaluation results in Ta-
ble 10. The 95% confidence interval of all the
MaUdE results reported in the Table 1 is ±0.002.
Dataset Details. We mainly used HLA-Chat
dataset for our evaluation. The HLA-Chat dataset
is an English single-turn dialogue dataset where the
dialogue is scraped from TV show scripts. Dataset
consists of dialogues from 327 characters in 38 TV
shows, resulting in a total of 1,042,647 dialogue
lines. We divided the split into 8:1:1 split based on
character, where each split is used as train, valida-
tion, and test split, respectively. For our main ex-
periments, we selected ten characters and selected
eight utterances that best reveal each character’s
unique characteristics. The set of utterances used
for describing the characters used for our experi-
ments is reported in our codebase.
Number of Experiments We perform the exper-
iment once rather than running it multiple times
with different seeds. Since our evaluation process
incorporates a human annotation, which requires
a payment to human annotators, we were not able
to perform multiple sets of experiments due to the
limitation on budget.D Additional Analysis
D.1 Lexical Overlap
In Table 11 we report an additional automated met-
ric,n-gram overlap (where n= 2), for analyzing
the style of generated responses. n-gram overlap
indicates the ratio of n-grams in the generated re-
sponse, which is contained in the target character
utterances. The trend of n-gram overlap metric
is similar to that of StyleProb metric. PDP-based
methods, especially a Dynamic Match, show higher
n-gram overlap values than other methods, indicat-
ing that PDP-based methods actively utilize the lex-
ical phrases appearing in the character utterances.
The high n-gram overlap values of PDP methods
indicate that PDP methods actively utilize the lex-
ical phrases appearing in the character utterances.
Using the unique vocabulary of the character will
help people to realize a better individualization of
the specific character. Nonetheless, this observa-
tion may imply that the model focuses on utilizing
lexical language habits and may not capture the
inherent characteristics of the character. Since ad-
dressing the inherent characteristics given only a
few utterances is a highly challenging task, we
think that extending our work to mimic characters’
intrinsic characteristics will be an intriguing future
direction.5125
D.2 More Examples
In Tables 12 we show more examples. We can see
that our Static Match and Dynamic Match methods
are able to generate responses that contain contents
that are highly specific to the character. For ex-
ample, for BMO (from the show Adventure Time)
response generated by our method mentions terms
such as "core system drivers" and "MO Factory"
that are relevant to the fact that BMO is an animated
video game console in the show. Furthermore, we
can see that our methods generate a response that
reflects the character’s style. For Spock (from Star
Trek), our response reflects Spocks’ stoic, highly
logical, and cold personality. For Sheldon (from
The Big Bang Theory), our response reflects Shel-
don’s excited speech style.
E Failure Modes of Dynamic Match
As in we discussed before, there exists a trade-
off between the style reflection and response co-
herency between Static Match and Dynamic Match.
In Tables 13 we show some failure modes of our
Dynamic Match method that reveal how DynamicMatch loses the response coherency. In the first
case, the model generates a response that exhibits a
strong character style but is incoherent to the input
context. In the second case, the model confuses the
identity of the speaker so that the model introduces
itself as Dr. Leonard Hofstadter. Last but not least,
when the given input context is highly specific, we
see that the generated responses do not reflect the
character’s style.
FExtending to General Style-Controlling
Conversation
In this section, we extend our methodology to more
general style-controlling conversation tasks such
as controlling sentiment, emotion, or writing styles,
not just mimicking a fictional character. We test
three style-controlling tasks – controlling senti-
ment (Positive, Negative), emotion (Anger, Joy),
and writing style (Modern, Shakespearean). For
each task, the utterances for defining a style and a
style classifier for the evaluation are obtained from5126
the Yelp restaurant review dataset, GoEmotions
dataset (Demszky et al., 2020), and Shakespearean
dataset (Xu et al., 2012), respectively. Style classi-
fier for each task is trained using the same codebase
and hyperparameters as in training the character
style classifier in the HLA-chat dataset. We used
Style Accuracy rather than StyleProb, following
previous literature on style transfer.
The experimental result of general style-
controlling conversation tasks is depicted in Ta-
ble 14. Similar to mimicking fictional characters,
PDP methods show significantly higher style re-
flection metrics than the baseline methods in gen-
eral style controlling tasks. Especially, Dynamic
Match shows the best style accuracy metric among
all the PDP methods, which is also a trend simi-
larly observed in character mimicking experiments.
These results demonstrate that our method is not
limited to the character mimicking task but has the
ability to be generally applicable to all kinds of
style-controlling conversation tasks. Although the
PDP methods have a lower MaUdE score than base-
line methods, we believe this tendency is becausethe MaUdE metric has difficulties evaluating a sen-
tence that strongly reflects a distinctive style, as
discussed in the main text. For instance, reflecting
the emotion "Anger" causes the model to generate
upper-cased responses (e.g., "I DO NOT WANT
TO EAT LUNCH"), which is an out-of-distribution
sample when training the MaUdE model.
G Multi-turn Chit-chat Examples
We show some multi-turn conversation examples
with the characters generated by our method in
Figure 3.
H Mimicking a New Character
To show that our method can be generally applied
to any fictional characters that do not appear in the
pre-training dataset nor the HLA-Chat dataset, we
report a conversation example of the PDP method
with an imaginary character generated by ourselves.
The character is called Pie the Duck , who is a duck
character that quacks all the time, likes to eat fish,
and enjoys swimming. We use the following utter-
ances to define the character:
• My name is Pie the Duck, Quack Quack!
•I really like swimming, Quack! And I am also
good at it, Quack!
• I like rainy day!! Quack Quack!!
•Salmon avocado salad is my favorite food!
But... anything made of fish is fine :)
•I’m looking at the sky... Will be fishes living
in the sky too? Quack.
• I’m so cute! Look at my beak!
•I’m recently on a diet to better float on water!
It’s necessary! Quack!5127•I majored sports, That’s why I’m a good swim-
mer! Quack Quack!
Figure 4 shows the example of a multi-turn conver-
sation with Pie the Duck. As shown in the example,
PDP successfully captures the unique style and
persona reflected on characters’ utterances, includ-
ing quacking habits, own name, identity as a duck,
favorite food, etc., while maintaining a dialog co-
herency.
I Scientific Artifacts
License. Table 15 denotes the license of the
datasets and pre-trained models that we used for
this paper. Unless for the case where the license
is not specified, all of the licenses allow the use of
resources for research purposes; therefore, the use
of these artifacts in this work is valid.
Intended Use. We want to clarify that the intended
useof pre-trained language models (when speci-
fied) is for text generation or fine-tuning to a down-
stream task; therefore, we are consistent with their
intended use.
Description of the Artifacts. Blended Skill Talk
(BST) dataset is an English open-domain, multi-
turn dialogue dataset built to enable conversational
agents to use multiple conversational skills (e.g.,
Using persona information, talk about knowledge,
empathetic conversation) in a single conversation.
DailyDialog dataset is an English open-domain,
multi-turn dialogue dataset that tries to reflect
our daily communication and cover various topics
about our daily lives. We describe the HLA-Chat
dataset in Section C. The Pile dataset is an 800GB
text corpus targeted at training large-scale language
models, mostly consisting of English texts and
constructed from 22 diverse text sources. The
Pushshift Reddit Comment dataset is a dump of
comments from the English website Reddit.
Privacy and Offensive Contents. We do not col-
lect any new data that can identify unique people /
contain offensive content. BST, Dailydailog, HLA-
Chat dataset is manually created using human anno-
tators or scraped from TV show scripts, therefore
having low risk on the issue of privacy or offensive
content. As discussed in their paper, the Pile dataset
explicitly used a profanity checker algorithm to re-
duce the pejorative content. While processing the
Pushshift dataset, we tried to exclude the training
offensive contents using blocklist keywords. Also,we did not include some subreddits that mostly
contain offensive content.51285129513051315132