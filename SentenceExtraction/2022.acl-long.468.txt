
Antoine Louis and Gerasimos Spanakis
Law & Tech Lab, Maastricht University
Abstract
Statutory article retrieval is the task of automat-
ically retrieving law articles relevant to a legal
question. While recent advances in natural lan-
guage processing have sparked considerable
interest in many legal tasks, statutory article
retrieval remains primarily untouched due to
the scarcity of large-scale and high-quality an-
notated datasets. To address this bottleneck,
we introduce the Belgian Statutory Article Re-
trieval Dataset (BSARD), which consists of
1,100+ French native legal questions labeled by
experienced jurists with relevant articles from a
corpus of 22,600+ Belgian law articles. Using
BSARD, we benchmark several state-of-the-
art retrieval approaches, including lexical and
dense architectures, both in zero-shot and su-
pervised setups. We find that fine-tuned dense
retrieval models significantly outperform other
systems. Our best performing baseline achieves
74.8% R @100, which is promising for the fea-
sibility of the task and indicates there is still
room for improvement. By the specificity of the
domain and addressed task, BSARD presents
a unique challenge problem for future research
on legal information retrieval. Our dataset and
source code are publicly available.
1 Introduction
Legal issues are an integral part of many people’s
lives (Ponce et al., 2019). However, the majority
of citizens have little to no knowledge about their
rights and fundamental legal processes (Balmer
et al., 2010). As the Internet has become the pri-
mary source of information in response to life prob-
lems (Estabrook et al., 2007), people increasingly
turn to search engines when faced with a legal is-
sue (Denvir, 2016). Nevertheless, the quality of
the search engine’s legal help results is currently
unsatisfactory, as top results mainly refer people
to commercial websites that provide basic informa-
tion as a way to advertise for-profit services (Hagan
and Li, 2020). On average, only one in five personsobtain help from the Internet to clarify or solve
their legal issue (Ponce et al., 2019). As a result,
many vulnerable citizens who cannot afford a le-
gal expert’s costly assistance are left unprotected
or even exploited. This barrier to accessing legal
information creates a clear imbalance within the
legal system, preventing the right to equal access
to justice for all.
People do not need legal services in and of
themselves; they need the ends that legal services
can provide. Recent advances in natural language
processing (NLP), combined with the increasing
amount of digitized textual data in the legal domain,
offer new possibilities to bridge the gap between
people and the law. For example, legal judgment
prediction (Aletras et al., 2016; Luo et al., 2017;
Zhong et al., 2018; Hu et al., 2018; Chen et al.,
2019) may assist citizens in finding insightful pat-
terns between their case and its outcome. Addition-
ally, legal text summarization (Hachey and Grover,
2006; Bhattacharya et al., 2019) and automated
contract review (Harkous et al., 2018; Lippi et al.,
2019) may help people clarify long, complex, and
ambiguous legal documents.
In this work, we focus on statutory article re-
trieval, which, given a legal question such as “ Is
it legal to contract a lifetime lease? ”, aims to re-
turn one or several relevant law articles from a
body of legal statutes (Kim et al., 2019; Nguyen
et al., 2020), as illustrated in Figure 1. A qualified
statutory article retrieval system could provide a
professional assisting service for unskilled humans
and help empower the weaker parties when used
for the public interest.
Finding relevant statutes to a legal question is a
challenging task. Unlike traditional ad-hoc infor-
mation retrieval (Craswell et al., 2020), statutory
article retrieval deals with two types of language:
common natural language for the questions and
complex legal language for the statutes. This differ-
ence in language distribution greatly complicates6789
the retrieval task as it indirectly requires an inher-
ent interpretation system that can translate a natural
question from a non-expert to a legal question to be
matched against statutes. For skilled legal experts,
these interpretations come from their knowledge of
a question’s domain and their understanding of the
legal concepts and processes involved. Neverthe-
less, an interpretation is rarely unique. Instead, it is
the interpreter’s subjective belief that gives mean-
ing to the question and, accordingly, an idea of the
domains in which the answer can be found. As a
result, the same question can yield different paths
to the desired outcome depending on its interpre-
tation, making statutory article retrieval a difficult
and time-consuming task.
Besides, statutory law is not a stack of indepen-
dent articles to be treated as complete sources of
information on their own – unlike news or recipes.
Instead, it is a structured and hierarchical collec-
tion of legal provisions that have whole meaning
only when considered in their overall context, i.e.,
together with the supplementary information from
their neighboring articles, the fields and sub-fields
they belong to, and their place in the hierarchy of
the law. For instance, the answer to the question
“Can I terminate an employment contract? ” will
most often be found in labor law. However, this is
not necessarily true if an employer is contracting a
self-employed worker to carry out a specific task,
in which case the answer probably lies at the higher
level of contract law. This example illustrates the
importance of considering the question’s context
and understanding the hierarchical structure of the
law when looking for relevant statutory articles.In order to study whether retrieval models can ap-
proximate the efficiency and reliability of legal ex-
perts, we need a suitable labeled dataset. However,
such datasets are difficult to obtain considering that,
although statutory provisions are generally publicly
accessible (yet often not in a machine-readable for-
mat), the questions posed by citizens are not.
This work presents a novel French native expert-
annotated statutory article retrieval dataset as its
main contribution. Our Belgian Statutory Article
Retrieval Dataset (BSARD) consists of more than
1,100 legal questions posed by Belgian citizens
and labeled by legal experts with references to
relevant articles from a corpus of around 22,600
Belgian law articles. As a second contribution,
we establish strong baselines on BSARD by com-
paring diverse state-of-the-art retrieval approaches
from lexical and dense architectures. Our results
show that fine-tuned dense retrieval models sig-
nificantly outperform other approaches yet sug-
gest ample opportunity for improvement. We pub-
licly release our dataset and source code at .
2 Related Work
Due to the increasing digitization of textual legal
data, the NLP community has recently introduced
more and more datasets to help researchers build
reliable models on several legal tasks. For instance,
Fawei et al. (2016) introduced a legal question
answering (LQA) dataset with 400 multi-choices
questions based on the US national bar exam. Sim-
ilarly, Zhong et al. (2020) released an LQA dataset
based on the Chinese bar exam consisting of 26,3656790multiple-choice questions, together with a database
of evidence that includes 3,382 Chinese legal pro-
visions and the content of the national examination
counseling book.
Furthermore, Duan et al. (2019) proposed a le-
gal reading comprehension dataset with 52,000
question-answer pairs crafted on the fact descrip-
tions of 10,000 cases from the Supreme People’s
Court of China. On a different note, Xiao et al.
(2018) presented a dataset for legal judgment pre-
diction (LJP) with around 2.68 million Chinese
criminal cases annotated with 183 law articles and
202 charges. Likewise, Chalkidis et al. (2019a)
introduced an LJP dataset consisting of 11,478 En-
glish cases from the European Court of Human
Rights labeled with the associated final decision.
Meanwhile, Xiao et al. (2019) introduced a
dataset for similar case matching with 8,964 triplets
of cases published by the Supreme People’s Court
of China, and Chalkidis et al. (2019b) released a
text classification dataset containing 57,000 En-
glish EU legislative documents tagged with 4,271
labels from the European V ocabulary. Addition-
ally, Manor and Li (2019) introduced a legal text
summarization dataset consisting of 446 sets of
contract sections and corresponding reference sum-
maries, and Holzenberger et al. (2020) presented a
statutory reasoning dataset based on US tax law.
Recently, Hendrycks et al. (2021) proposed a
dataset for legal contract review that includes 510
contracts annotated with 41 different clauses for
a total of 13,101 annotations. In the same vein,
Borchmann et al. (2020) introduced a semantic re-
trieval dataset for contract discovery with more
than 2,500 annotations in around 600 documents.
Lastly, the COLIEE Case Law Corpus (Rabelo
et al., 2020) is a case law retrieval and entailment
dataset that includes 650 base cases from the Fed-
eral Court of Canada, each with 200 candidate
cases to be identified as relevant to the base case.
Regarding statutory article retrieval, the only
other publicly available dataset is the COLIEE
Statute Law Corpus (Rabelo et al., 2020). It com-
prises 696 questions from the Japanese legal bar
exam labeled with references to relevant articles
from the Japanese Civil Code, where both the
questions and articles have been translated from
Japanese to English. However, this dataset focuses
on legal bar exam question answering, which is
quite different from legal questions posed by ordi-
nary citizens. While the latter tend to be vague andstraightforward, bar exam questions are meant for
aspiring lawyers and are thus specific and advanced.
Besides, the dataset only contains closed questions
(i.e., questions with “yes” or “no” answers) and
considers almost 30 times fewer law articles than
BSARD does. Also, unlike BSARD, the data are
not native sentences but instead translated from a
foreign language with a completely different legal
system.As a result, the translated dataset may
not accurately reflect the logic of the original legal
system and language. These limitations suggest the
need for a novel large-scale citizen-centric native
dataset for statutory article retrieval, which is the
core contribution of the present work.
3The Belgian Statutory Article Retrieval
Dataset
3.1 Dataset Collection
We create our dataset in four stages: (i) compiling
a large corpus of Belgian law articles, (ii) gath-
ering legal questions with references to relevant
law articles, (iii) refining these questions, and (iv)
matching the references to the corresponding arti-
cles from our corpus.
Law articles collection. In civil law jurisdictions,
a legal code is a type of legislation that purports
to exhaustively cover a whole area of law, such as
criminal law or tax law, by gathering and restat-
ing all the written laws in that area into a unique
book. Hence, these books constitute valuable re-
sources to collect many law articles on various
subjects. We consider 32 publicly available Bel-
gian codes, as presented in Table 3 of Appendix A.
Together with the legal articles, we extract the cor-
responding headings of the sections in which these
articles appear (i.e., book, part, act, chapter, sec-
tion, and subsection names). These headings pro-
vide an overview of each article’s subject. As pre-
processing, we use regular expressions to clean up
the articles of specific wording indicating a change
in part of the article by a past law (e.g., nested
brackets, superscripts, or footnotes). Additionally,
we identify and remove the articles repealed by
past laws but still present in the codes. Eventu-
ally, we end up with a corpus C={a,···, a}6791ofN= 22,633articles that we use as our basic
retrieval units.
Questions collection. We partner with Droits
Quotidiens (DQ),a Belgian organization whose
mission is to clarify the law for laypeople. Each
year, DQ receives and collects around 4,000 emails
from Belgian citizens asking for advice on a per-
sonal legal issue. Thanks to these emails, its team
of six experienced jurists keeps abreast of Bel-
gium’s most common legal issues and addresses
them as comprehensively as possible on its web-
site. Each jurist is an expert in a specific field (e.g.,
“family”, “housing”, or “work”) and is responsi-
ble for answering all questions related to that field.
Given their qualifications and years of experience
in providing legal advice in their respective fields,
the experts can be considered competent enough to
always (eventually) retrieve the correct articles to a
given question.
In practice, their legal clarification process con-
sists of four steps. First, they identify the most
frequently asked questions on a common legal is-
sue. Then, they define a new anonymized “model”
question on that issue expressed in natural language
terms, i.e., as close as possible as if a layperson had
asked it. Next, they search the Belgian law for arti-
cles that help answer the model question and refer-
ence them. Finally, they answer the question using
the retrieved relevant articles in a way a layperson
can understand. These model questions, legal refer-
ences, and answers are further categorized before
being posted on DQ’s website (e.g., the question
“What is the seizure of goods? ” is tagged under
the “ Money →Debt recovery ” category). With
their consent, we collect more than 3,200 model
questions together with their references to relevant
law articles and categorization tags.
Assuming it takes a jurist between 5 to 20 min-
utes to find the relevant articles to a given question
and categorize the latter. An estimate of the pe-
cuniary value of those labeled questions is over
C105,000 – 3,200 questions, each requiring 10
minutes to label, assuming a rate of C200 per hour.
Questions refinement. We find that around one-
third of the collected questions are duplicates. How-
ever, these duplicated questions come with differ-
ent categorization tags, some of which providing
additional context that can be used to refine the
questions. For example, the question “ Should Iinstall fire detectors? ” appears four times in total,
under the following tags: “ Housing →Rent→I am
a {tenant ,landlord }→In {Wallonia ,Brussels }”.
We distinguish between the tags with one or a few
words indicating a question subject (e.g., “ housing ”
and “ rent”) and those that provide context about a
personal situation or location as short descriptive
sentences (e.g., “ I am tenant in Brussels. ”). If any,
we append the contextual sentence tags in front of
the questions, which solves most of the duplicates
problem and improves the overall quality of the
questions by making them more specific.
Questions filtering. The questions collected are
annotated with plain text references to relevant law
articles (e.g., “ Article 8 of the Civil Code ”). We
use regular expressions to parse these references
and match them to the corresponding articles from
our corpus. First, we filter out questions whose
references are not articles (e.g., an entire decree
or order). Then, we remove questions with refer-
ences to legal acts other than codes of law (e.g.,
decrees, directives, or ordinances). Next, we ig-
nore questions with references to codes other than
those we initially considered. We eventually end
up with 1,108 questions, each carefully labeled
with the ids of the corresponding relevant law arti-
cles from our corpus. Finally, we split the dataset
into training/test sets with 886 and 222 questions,
respectively.
3.2 Dataset Analysis
To provide more insight, we describe quantitative
and qualitative observations about BSARD. Specif-
ically, we explore (i) the diversity in questions and
articles, (ii) the relationship between questions and
their relevant articles, and (iii) the type of reasoning
required to retrieve relevant articles.
Diversity. The 22,633 law articles that constitute
our corpus have been collected from 32 Belgian
codes covering a large number of legal topics, as
presented in Table 3 of Appendix A. The articles
have a median length of 77 words, but 142 articles
exceed 1,000 words (the lengthiest one being up
to 5,790 words), as illustrated in Figure 2b. These
long articles are mostly general provisions , i.e.,
articles that appear at the beginning of a code and
define many terms and concepts later mentioned
in the code. The questions are between 5 and 44
words long, with a median of 14 words, as shown
in Figure 2a. They cover a wide range of topics,
with around 85% of them being either about family,6792
housing, money, or justice, while the remaining
15% concern either social security, foreigners, or
work, as described in Table 1.
Question-article relationship. Questions might
have one or several relevant legal articles. Overall,
75% of the questions have less than five relevant ar-
ticles, 18% have between 5 and 20, and the remain-
ing 7% have more than 20 with a maximum of 109,
as seen in Figure 2c. The latter often have complex
and indirect answers that demand extensive reason-
ing over a whole code section, which explains these
large numbers of relevant articles. Furthermore, an
article deemed relevant to one question might also
be for others. Therefore, we calculate for each
unique article deemed relevant to at least one ques-
tion the total number of times it is cited as a legal
reference across all questions. As a result, we find
that the median number of citations for those arti-
cles is 2, and less than 25% of them are cited more
than five times, as illustrated in Figure 2d. Hence,
out of the 22,633 articles, only 1,612 are referred
to as relevant to at least one question in the dataset,
and around 80% of these 1,612 articles come from
either the Civil Code, Judicial Code, Criminal In-
vestigation Code, or Penal Code. Meanwhile, 18
out of the 32 codes have less than five articles men-tioned as relevant to at least one question, which
can be explained by the fact that those codes focus
less on individuals and their concerns.
4 Models
Formally speaking, a statutory article retrieval sys-
temR: (q,C)→ F is a function that takes as input
a question qalong with a corpus of law articles C,
and returns a much smaller filter set F ⊂ C of the
supposedly relevant articles, ranked by decreasing
order of relevance. For a fixed k=|F| ≪ |C| , the
retriever can be evaluated in isolation with multiple
rank-based metrics (see Section 5.1). The follow-
ing section describes the retrieval models we use
as a benchmark for the task.
4.1 Lexical Models
Traditionally, lexical approaches have been the de
facto standard for textual information retrieval due
to their robustness and efficiency. Given a query q
and an article a, a lexical model assigns to the pair
(q, a)a score s: (q, a)→Rby computing the
sum, over the query terms, of the weights of each
query term t∈qin the article, i.e.,
s(q, a) =Xw(t, a). (1)6793First, we use the TF-IDF weighting scheme, in
which
w(t, a) = tf( t, a)·log|C|
df(t), (2)
where the term frequency tfis the number of oc-
currences of term tin article a, and the document
frequency dfis the number of articles within the
corpus that contain term t. Then, we experiment
with the BM25 weighting formula (Robertson et al.,
1994), defined as
w(t, a) =tf(t, a)·(k+1)
tf(t, a)+k·
1−b+b·
·log|C| − df(t) + 0.5
df(t) + 0.5,(3)
where k∈Randb∈[0,1]are constant parame-
ters to be fixed, |a|is the article length, and avgal
is the average article length in the collection.
During inference, we compute a score for each
article in corpus Cand return the karticles with the
highest scores as the top- kmost relevant results to
the input query.
4.2 Dense Models
Lexical approaches suffer from the lexical gap prob-
lem (Berger et al., 2000) and can only retrieve arti-
cles containing keywords present in the query. To
overcome this limitation, recent work (Lee et al.,
2019; Karpukhin et al., 2020; Xiong et al., 2021)
relies on neural-based architectures to capture se-
mantic relationships between the query and doc-
uments. The most commonly used approach is
based on a bi-encoder model (Gillick et al., 2018)
that maps queries and documents into dense vector
representations. Formally, a dense retriever calcu-
lates a relevance score s: (q, a)→Rbetween
question qand article aby the similarity of their
respective embeddings h,h∈R, i.e.,
s(q, a) = sim ( h,h), (4)
where sim :R×R→Ris a similarity function
such as dot product or cosine similarity. Typically,
these embeddings result from a pooling operation
on the output representations of a word embedding
model:
h= pool ( f(q;θ)),and
h= pool ( f(a;θ)),(5)
where model f(·;θ) :W→Rwith parame-
tersθmaps an input text sequence of nterms fromvocabulary Wtod-dimensional real-valued word
vectors. The pooling operation pool : R→R
uses the output word embeddings to distill a global
representation for the text passage – using either
mean, max, or [CLS] pooling.
Note that the bi-encoder architecture comes with
two flavors: (i) siamese (Reimers and Gurevych,
2019; Xiong et al., 2021), which uses a unique
word embedding model (i.e., θ=θ) that maps
the query and article together in a shared dense
vector space, and (ii) two-tower (Yang et al., 2020;
Karpukhin et al., 2020), which use two independent
word embedding models that encode the query and
article separately into different embedding spaces.
During inference, the articles are pre-encoded
offline, and their representations are stored in an
index structure. Then, given an input query, an
exact search is performed by computing the sim-
ilarities between the query representation and all
pre-encoded article representations. The resulting
scores are used to rank the articles such that the k
articles that have the highest similarities with the
query are returned as the top- kresults.
4.2.1 Zero-Shot Evaluation
First, we study the effectiveness of siamese bi-
encoders in a zero-shot evaluation setup, i.e.,
pre-trained word embedding models are applied
out-of-the-box without any additional fine-tuning.
We experiment with two types of widely-used
word embedding models: (i) models that learned
context- independent word representations, namely
word2vec (Mikolov et al., 2013a,b) and fast-
Text (Bojanowski et al., 2017), and (ii) models
that learned context- dependent word embeddings,
namely RoBERTa (Liu et al., 2019).
RoBERTa can process texts up to a maximum
input length of 512 tokens. Although alternative
models exist to alleviate this limitation (Beltagy
et al., 2020; Ainslie et al., 2020), they have all been
trained on English text, and there are no French
equivalents available yet. Therefore, we use a sim-
ple workaround that splits the text into overlapping
chunks and passes each chunk in turn to the em-
bedding model. To form the chunks, we consider
contiguous text sequences of 200 tokens with an
overlap of 20 tokens between consecutive chunks.
For all zero-shot models, we use mean pooling
on all word embeddings of the passage to extract
a global representation for the latter and cosine
similarity to score passage representations.67944.2.2 Training
Thereafter, we train our own siamese and two-tower
RoBERTa-based bi-encoder models on BSARD.
LetD={⟨q, a⟩}be the training data where
each of the Ninstances consists of a query qasso-
ciated with a relevant (positive) article a. Us-
ing in-batch negatives (Chen et al., 2017; Hen-
derson et al., 2017), we can create a training set
T={⟨q, a,A⟩}whereAis a set of nega-
tive articles for question qconstructed by consider-
ing the articles paired with the other questions from
the same mini-batch. For each training instance, we
contrastively optimize the negative log-likelihood
of each positive article against their negative arti-
cles, i.e.,
L 
q, a,A
=−logexp 
s(q, a)/τ
Pexp (s(q, a)/τ),(6)
where τ >0is a temperature parameter to be set.
This contrastive loss allows learning embedding
functions such that relevant question-article pairs
will have a higher score than irrelevant ones.
To deal with articles longer than 512 tokens, we
use the same workaround as in the zero-shot evalu-
ation and split the long sequences into overlapping
chunks of 200 tokens with a window size of 20.
However, this time, we limit the size of the articles
to the first 1,000 words due to limited GPU memory.
Although not ideal, doing so remains reasonable
given that only 0.6% of the articles in our corpus
have more than 1,000 words, as mentioned in Sec-
tion 3.2. Each chunk is prefixed by the [CLS]
token, and we extract a global representation for
the whole article by averaging the output [CLS]
token embeddings of the different chunks. Here,
we use the dot product to compute similarities as it
gives slightly better results than cosine.
5 Experiments
We now describe the setup we use for experiments
and evaluate the performance of our models.
5.1 Experimental Setup
Metrics. We use three standard information re-
trieval metrics (Manning et al., 2008) to eval-
uate performance, namely the (macro-averaged)
recall @k(R@k), mean average precision @k
(MAP @k), and mean reciprocal rank @k(MRR @k).
Appendix B gives a detailed description of thesemetrics in the context of statutory article retrieval.
We deliberately omit to report the precision @k
given that questions have a variable number of
relevant articles (see Figure 2c), which makes it
senseless to report it at a fixed k– questions with r
relevant articles will always have P @k <1ifk > r .
For the same reason, kshould be large enough for
the recall @k. Hence, we use k∈ {100,200,500}
for our evaluation.
French word embedding models. Our focus is
on a non-English dataset, so we experiment with
French variants of the models mentioned above.
Specifically, we use a 500-dimensional skip-gram
word2vec model pre-trained on a crawled French
corpus (Fauconnier, 2015), a 300-dimensional
CBOW fastText model pre-trained on French Web
data (Grave et al., 2018), and a French RoBERTa
model, namely CamemBERT (Martin et al., 2020),
pre-trained on 147GB of French web pages filtered
from Common Crawl.
Hyper-parameters & schedule. For BM25, we
optimize the parameters on BSARD training set
and find k= 1.0andb= 0.6to perform best.
Regarding the bi-encoder models, we optimize the
contrastive loss using a batch size of 22 question-
article pairs and a temperature of 0.05 for 100
epochs, which is approximately 20,500 steps. We
use AdamW (Loshchilov and Hutter, 2019) with an
initial learning rate of 2e-5, β= 0.9,β= 0.999,
weight decay of 0.01, learning rate warm up over
the first 500 steps, and linear decay of the learn-
ing rate. Training is performed on a single Tesla
V100 GPU with 32 GBs of memory and evaluation
on a server with a dual 20 core Intel(R) Xeon(R)
E5-2698 v4 CPU @2.20GHz and 512 GBs of RAM.
5.2 Results
In Table 2, we report the retrieval performance of
our models on the BSARD test set. Overall, the
trained bi-encoder models significantly outperform
all the other baselines. The two-tower model im-
proves over its siamese variant on recall @100 but
performs similarly on the other metrics. Although
BM25 underperforms the trained bi-encoders sig-
nificantly, its performance indicates that it is still a
strong baseline for domain-specific retrieval. These
results are consistent with those obtained on other
in-domain datasets (Thakur et al., 2021).6795
Regarding the zero-shot evaluation of siamese
bi-encoder models, we find that directly using the
embeddings of a pre-trained CamemBERT model
without optimizing for the IR task gives poor re-
sults. Reimers and Gurevych (2019) noted similar
findings for the task of semantic textual similar-
ity. Furthermore, we observe that the word2vec-
based bi-encoder significantly outperforms the fast-
Text and BERT-based models, suggesting that pre-
trained word-level embeddings are more appropri-
ate for the task than character-level or subword-
level embeddings when used out of the box.
Although promising, these results suggest ample
opportunity for improvement compared to a skilled
legal expert who can eventually retrieve all relevant
articles to any question and thus get perfect scores.
6 Discussion
This section discusses the limitations and broader
impacts of our dataset.
6.1 Limitations
As our dataset aims to give researchers a well-
defined benchmark to evaluate existing and future
legal information retrieval models, certain limita-
tions need to be borne in mind to avoid drawing
erroneous conclusions.
First, the corpus of articles is limited to those
collected from the 32 Belgian codes described in
Table 3 of Appendix A, which does not cover the
entire Belgian law as thousands of articles from de-
crees, directives, and ordinances are missing. Dur-
ing the dataset construction, all references to these
uncollected articles are ignored, which causes some
questions to end up with only a fraction of their
initial number of relevant articles. This informa-
tion loss implies that the answer contained in the
remaining relevant articles might be incomplete,
although it is still appropriate.
Additionally, it is essential to note that not all
legal questions can be answered with statutes alone.For instance, the question “ Can I evict my tenants
if they make too much noise? ” might not have a
detailed answer within the statutory law that quan-
tifies a specific noise threshold at which eviction
is allowed. Instead, the landlord should probably
rely more on case law and find precedents similar
to their current situation (e.g., the tenant makes two
parties a week until 2 am). Hence, some questions
are better suited than others to the statutory article
retrieval task, and the domain of the less suitable
ones remains to be determined.
6.2 Broader Impacts
In addition to helping advance the state-of-the-
art in retrieving statutes relevant to a legal ques-
tion, BSARD-based models could improve the effi-
ciency of the legal information retrieval process in
the context of legal research, therefore enabling re-
searchers to devote themselves to more thoughtful
parts of their research.
Furthermore, BSARD can become a starting
point of new open-source legal information search
tools so that the socially weaker parties to disputes
can benefit from a free professional assisting ser-
vice. However, there are risks that the dataset will
not be used exclusively for the public interest but
perhaps also for profit as part of proprietary search
tools developed by companies. Since this would
reinforce rather than solve the problem of access
to legal information and justice for all, we decided
to distribute BSARD under a license with a non-
commercial clause.
Other potential negative societal impacts could
involve using models trained on BSARD to misuse
or find gaps within the governmental laws or use
the latter not to defend oneself but to deliberately
damage people or companies instead. Of course,
we discourage anyone from developing models that
aim to perform the latter actions.67967 Conclusion
In this paper, we present the Belgian Statutory Ar-
ticle Retrieval Dataset (BSARD), a citizen-centric
French native dataset for statutory article retrieval.
Within a larger effort to bridge the gap between peo-
ple and the law, BSARD provides a means of eval-
uating and developing models capable of retrieving
law articles relevant to a legal question posed by
a layperson. We benchmark several strong infor-
mation retrieval baselines that show promise for
the feasibility of the task yet indicate room for im-
provement. In the future, we plan to build retrieval
models that can handle lengthy statutory articles
and inherently exploit the hierarchy of the law. In
closing, we hope that our work sparks interest in
developing practical and reliable statutory article
retrieval models to help improve access to justice
for all.
Acknowledgments
This research is partially supported by the Sector
Plan Digital Legal Studies of the Dutch Ministry of
Education, Culture, and Science. In addition, this
research was made possible, in part, using the Data
Science Research Infrastructure (DSRI) hosted at
Maastricht University.
References679767986799
Appendix
A Legal Codes
Table 3 presents a detailed summary of the 32 pub-
licly available Belgian codes collected for BSARD.
B Evaluation Metrics
Letrel(a)∈ {0,1}be the binary relevance label
of article afor question q, and⟨i, a⟩ ∈ Fa result
tuple (article aat rank i) from the filter set F⊂ C
of ranked articles retrieved for question q.
Recall. Therecall Ris the fraction of relevant
articles retrieved for query qw.r.t. the total number
of relevant articles in the corpus C, i.e.,
R=Prel(a)
Prel(a). (7)
Reciprocal rank. Thereciprocal rank (RR) cal-
culates the reciprocal of the rank at which the first
relevant article is retrieved, i.e.,
RR= maxrel(a)
i. (8)
Average precision. Theaverage precision AP
is the mean of the precision value obtained after
each relevant article is retrieved, that is
AP=PP×rel(a)
Prel(a), (9)
where Pis the precision computed at rank j
for query q, i.e., the fraction of relevant articlesretrieved for query qw.r.t. the total number of
articles in the retrieved set {F}:
P=Prel(a)
{F}. (10)
We report the macro-averaged recall (R),mean
reciprocal rank (MRR), and mean average preci-
sion (MAP), which are the average values of the
corresponding metrics over a set of nqueries. Note
that as those metrics are computed for a filter set of
sizek=|F| ≪ |C| (and not on the entire list of
articles in C), we report them with the suffix “ @k”.
C Dataset Documentation
C.1 Dataset Nutrition Labels
As a first way to document our dataset, we provide
thedataset nutrition labels (Holland et al., 2018)
for BSARD in Table 4.
C.2 Data Statement
In addition to the data nutrition labels, we include
thedata statement (Bender and Friedman, 2018)
for BSARD, which provides detailed context on the
dataset so that researchers, developers, and users
can understand how models built upon it might gen-
eralize, be appropriately deployed, and potentially
reflect bias or exclusion.
Curation rationale. All law articles from the se-
lected Belgian codes were included in our dataset,
except those revoked (identifiable because men-
tioned before the article or empty content) and
those with a duplicate number within the same
code (namely, the articles from Act V , Book III
of the Civil Code; from Sections 2, 2bis, and 3 of
Chapter II, Act VIII, Book III of the Civil Code;
from Act XVIII, Book III of the Civil Code; from
the Preliminary Act of the Code of Criminal In-
struction; from the Appendix of the Judicial Code).
Not including the latter articles did not pose a vi-
tal concern because none of them were mentioned
as relevant to any of the questions in our dataset.
Regarding the questions, all those that referenced
at least one of the articles from our corpus were
included in the dataset.
Language variety. The questions and legal arti-
cles were collected in French (fr-BE) as spoken in
Wallonia and Brussels-Capital region.6800Authority Code #Articles #Relevant
Federal Judicial Code 2285 429
Code of Economic Law 2032 98
Civil Code 1961 568
Code of Workplace Welfare 1287 25
Code of Companies and Associations 1194 0
Code of Local Democracy and Decentralization 1159 3
Navigation Code 977 0
Code of Criminal Instruction 719 155
Penal Code 689 154
Social Penal Code 307 23
Forestry Code 261 0
Railway Code 260 0
Electoral Code 218 0
The Constitution 208 5
Code of Various Rights and Taxes 191 0
Code of Private International Law 135 4
Consular Code 100 0
Rural Code 87 12
Military Penal Code 66 1
Code of Belgian Nationality 31 8
Regional Walloon Code of Social Action and Health 3650 40
Walloon Code of the Environment 1270 22
Walloon Code of Territorial Development 796 0
Walloon Public Service Code 597 0
Walloon Code of Agriculture 461 0
Brussels Spatial Planning Code 401 1
Walloon Code of Basic and Secondary Education 310 0
Walloon Code of Sustainable Housing 286 20
Brussels Housing Code 279 44
Brussels Code of Air, Climate and Energy Management 208 0
Walloon Animal Welfare Code 108 0
Brussels Municipal Electoral Code 100 0
Total 22633 1612
Speaker demographic. Speakers were not di-
rectly approached for inclusion in this dataset and
thus could not be asked for demographic informa-
tion. Questions were collected, anonymized, and
reformulated by Droits Quotidiens. Therefore, no
direct information about the speakers’ age and gen-
der distribution or socioeconomic status is avail-
able. However, it is expected that most, but not
all, of the speakers are adults (18+ years), speak
French as a native language, and live in Wallonia
or Brussels-Capital region.
Annotator demographic. A total of six Belgian
jurists from Droits Quotidiens contributed to anno-tating the questions. All have a law degree from a
Belgian university and years of experience in pro-
viding legal advice and clarifications of the law.
They range in age from 30-60 years, including one
man and five women, gave their ethnicity as white
European, speak French as a native language, and
represent upper middle class based on income lev-
els.
Speech situation. The questions were written be-
tween 2018 and 2021 and collected in May 2021.
They represent informal, asynchronous, edited,
written language that does not exceed 44 words.
No question contains hateful, aggressive, or inap-6801
Metadata
ProvenanceVariables
propriate language as they were all reviewed and
reworded by Droits Quotidiens to be neutral, anony-
mous, and comprehensive. All the legal articles
were written between 1804 and 2021 and collected
in May 2021. They represent strong, formal, writ-
ten language containing up to 5,790 words.
Text characteristics. Many articles complement
or rely on other articles in the same or another
code and thus contain (sometimes lengthy) legal
references, which might be seen as noisy data.
Recording quality. N/A.
Other. N/A.
Provenance appendix. N/A.
C.3 Intended Uses
The dataset is intended to be used by researchers to
build and evaluate models on retrieving law articlesrelevant to an input legal question. Therefore, it
should not be regarded as a reliable source of legal
information at this point in time, as both the ques-
tions and articles correspond to an outdated version
of the Belgian law from May 2021 (time of dataset
collection). In the latter case, the user is advised to
consult daily updated official legal resources (e.g.,
the Belgian Official Gazette).
C.4 Hosting
We provide access to BSARD on Hugging
Face Datasets (Lhoest et al., 2021) at .
Additionally, the dataset is hosted on Zenodo at .
C.5 Data Format
The dataset is stored as CSV files and can be read
using standard libraries (e.g., the built-in csv mod-6802ule in Python) or the datasets library:
C.6 Reproducibility
We ensure the reproducibility of the experimental
results by releasing our code on Github at .
C.7 Licensing
The dataset is publicly distributed under a CC BY-
NC-SA 4.0 license, which allows sharing freely
(i.e., copy and redistribute) and adapt (i.e., remix,
transform, and build upon) the material on the con-
ditions that the latter is used for non-commercial
purposes only, proper attribution is given (i.e., ap-
propriate credit, link to the license, and an indica-
tion of changes), and the same license as the origi-
nal is used if one distributes an adapted version of
the material. In addition, the code to reproduce the
experimental results of the paper is released under
the MIT license.
C.8 Maintenance
The dataset will be supported and maintained by
the Law & Tech Lab at Maastricht University.
Any updates to the dataset will be communicated
via the Github repository. All questions and
comments about the dataset can be sent to Antoine
Louis: .
Other contacts can be found at .6803