
Yunxiang Zhang, Xiaojun Wan
Wangxuan Institute of Computer Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{yx.zhang,wanxiaojun}@pku.edu.cn
Abstract
Despite being a common figure of speech, hy-
perbole is under-researched in Figurative Lan-
guage Processing. In this paper, we tackle the
challenging task of hyperbole generation to
transfer a literal sentence into its hyperbolic
paraphrase. To address the lack of available
hyperbolic sentences, we construct HYPO-XL,
the first large-scale English hyperbole corpus
containing 17,862 hyperbolic sentences in a
non-trivial way. Based on our corpus, we pro-
pose an unsupervised method for hyperbole
generation that does not require parallel literal-
hyperbole pairs. During training, we fine-tune
BART (Lewis et al., 2020) to infill masked hy-
perbolic spans of sentences from HYPO-XL.
During inference, we mask part of an input
literal sentence and over-generate multiple pos-
sible hyperbolic versions. Then a BERT-based
ranker selects the best candidate by hyperbol-
icity and paraphrase quality. Automatic and
human evaluation results show that our model
is effective at generating hyperbolic paraphrase
sentences and outperforms several baseline sys-
tems.
1 Introduction
Hyperbole is a figure of speech that deliberately
exaggerates a claim or statement to show emphasis
or express emotions. If a referent has a feature X,
a hyperbole exceeds the credible limits of fact in
the given context and presents it as having more of
that X than warranted by reality (Claridge, 2010).
Take the following example, “ I won’t wait for you:
it took you centuries to get dressed. ” It over-blows
the time for someone to get dressed with a single
word “ centuries ” and thus creates a heightened
effect. From a syntactic point of view, Claridge
(2010) classifies hyperbole into word-level, phrase-
level and clause-level types, and conclude that the
former two types are more common in English.
Although hyperbole is considered as the second
most frequent figurative device (Kreuz and Roberts,1993), it has received less empirical attention in
the NLP community. Recently Tian et al. (2021)
addressed the generation of clause-level hyperbole.
In this paper, we instead focus on word-level and
phrase-level hyperbole, which can be unified as
span-level hyperbole.
To tackle the hyperbole generation problem we
need to address three main challenges:
•The lack of training data that either consists
of large-scale hyperbolic sentences or literal-
hyperbole pairs, which are necessary to train
an unsupervised or supervised model.
•The tendency of generative language models
to produce literal text rather than hyperboles.
•Trade-off between content preservation and
hyperbolic effect of the generated sentences.
In order to address the above challenges, we pro-
pose MOVER (Mask,OVE r-generate and Rank),
an unsupervised approach to generating hyperbolic
paraphrase from literal input. Our approach does
not require parallel data for training, thus allevi-
ating the issue of scarce data. Still, we need a
non-parallel corpus containing as much hyperbolic
sentences as possible. To this end, we first build a
large-scale English hyperbole corpus HYPO-XL in
a weakly supervised way.
Based on the intuition that the hyperbolic effect
of a sentence is realized by a single word or phrase
within it, we introduce a sub-task of hyperbolic
span extraction. We identify several possible n-
grams of a hyperbolic sentence that can cause the
hyperbolic bent with syntactic and semantic fea-
tures. We apply this masking approach to sentences
in HYPO-XL and teach a pretrained seq2seq trans-
former, BART (Lewis et al., 2020), to infill the
words in missing hyperbolic spans. This increases
the probability of generating hyperbolic texts in-
stead of literal ones. During inference, given a
single literal sentence, our system provides multi-
ple masked versions for inputs to BART and gen-6018erates potential hyperbolic sentences accordingly.
To select the best one for output, we leverage a
BERT-based ranker to achieve a satisfying trade-
off between hyperbolicity and paraphrase quality.
Our contributions are three-fold:
•We construct the first large-scale hyperbole
corpus HYPO-XL in a non-trivial way. The
corpus is publicly available,contributing to
the Figurative Language Processing (FLP)
community by facilitating the development
of computational study of hyperbole.
•We propose an unsupervised approach for
hyperbole generation that falls into the
“overgenerate-and-rank” paradigm (Heilman
and Smith, 2009).
•We benchmark our system against several
baselines and we compare their performances
by pair-wise manual evaluations to demon-
strate the effectiveness of our approach.
2 HYPO-XL: Hyperbole Corpus
Collection
The availability of large-scale corpora can facilitate
the development of figurative language generation
with pretrained models, as is shown by Chakrabarty
et al. (2020b) on simile generation and Chakrabarty
et al. (2021) on metaphor generation. However,
datasets for hyperbole are scarce. Troiano et al.
(2018) built an English corpus HYPO containing
709 triplets [hypo, para, non _hypo], where hypo
refers to a hyperbolic sentence, para denotes the
literal paraphrase of hypo andnon_hypo means
a non-hyperbolic sentence that contains the same
hyperbolic word or phrase as hypo but with a lit-
eral connotation. The size of this dataset is too
small to train a deep learning model for hyperbole
detection and generation. To tackle the lack of hy-
perbole data, we propose to enlarge the hyperbolic
sentences of HYPO in a weakly supervised way
and build a large-scale English corpus of 17,862 hy-
perbolic sentences, namely HYPO-XL. We would
like to point out that this is a non-parallel corpus
containing only hyperbolic sentences without their
paraphrase counterparts, because our hyperbole
generation approach (Section 3) does not require
parallel training data.
The creation of HYPO-XL consists of two steps:
1.We first train a BERT-based binary classifieron HYPO and retrieve possible hyperbolic
sentences from an online corpus.
2.We manually label a subset of the retrieved
sentences, denoted HYPO-L, and retrain our
hyperbole detection model to identify hyper-
bolic sentences from the same retrieval corpus
with higher confidence.
2.1 Automatic Hyperbole Detection
Hyperbole detection is a supervised binary classi-
fication problem where we predict whether a sen-
tence is hyperbolic or not (Kong et al., 2020). We
fine-tune a BERT-base model (Devlin et al., 2019)
on the hyperbole detection dataset HYPO (Troiano
et al., 2018). In experiment, we randomly split
the data into 567 (80%) hyperbolic sentences, with
their literal counterparts ( para andnon_hypo ) as
negative samples, in training set and 71 (10%) in
development set and 71 (10%) in test set. Our
model achieves an accuracy of 80% on the test set,
which is much better than the highest reported ac-
curacy (72%) of traditional algorithms in Troiano
et al. (2018).
Once we obtain this BERT-based hyperbole de-
tection model, the next step is to retrieve hyperbolic
sentences from a corpus. Following Chakrabarty
et al. (2020a), we use Sentencedict.com,an online
sentence dictionary as the retrieval corpus. We re-
move duplicate and incomplete sentences (without
initial capital) in the corpus, resulting in a collec-
tion of 767,531 sentences. Then we identify 93,297
(12.2 %) sentences predicted positive by our model
as pseudo-hyperbolic.
2.2 HYPO-L: Human Annotation of
Pseudo-labeled Data
Due to the small size of training set, pseudo-labeled
data tend to have lower confidence score (i.e., the
prediction probability). To improve the precision of
our model,we further fine-tune it with our human-
annotated data, namely HYPO-L. We randomly
sample 5,000 examples from the 93,297 positive
predictions and invite students with proficiency in
English to label them as hyperbolic or not. For each
sentence, two annotators provide their judgements.
We only keep items with unanimous judgments (i.e.
both of the two annotators mark the sentence as hy-
perbolic or non-hyperbolic) to ensure the reliability6019Dataset # Hypo. # Non. # Para. # Total
HYPO 709 698 709 2,116
HYPO-L 1,007 2,219 - 3,226
HYPO-XL 17,862 - - 17,862
Measurement Value
% Non-hypo 6%
# Avg hypo span tokens 2.23
% Long hypo spans ( >1 token) 37%
# Distinct hypo spans 85
# Distinct POS-ngrams of hypo spans 39
of annotated data. In this way, 3,226 (64.5%) out
of 5,000 annotations are left in HYPO-L. This per-
centage of unanimous judgments (i.e., raw agree-
ment, RA) is comparable to 58.5% in the creation
of HYPO (Troiano et al., 2018). To be specific,
HYPO-L consists of 1,007 (31.2%) hyperbolic sen-
tences (positive samples) and 2,219 (68.8%) literal
ones (negative samples).
We continue to train the previous HYPO-fine-
tuned BERT on HYPO-L and the test accuracy is
80%,which we consider as an acceptable met-
ric for hyperbole detection. Finally we apply the
BERT-based detection model to the retrieval corpus
again and retain sentences whose prediction prob-
abilities for positive class exceed a certain thresh-
old.This results in HYPO-XL, a large-scale cor-
pus of 17,862 (2.3%) hyperbolic sentences. We
provide a brief comparison of HYPO, HYPO-L
and HYPO-XL in Table 1 to further clarify the data
collection process.
2.3 Corpus Analysis
Since HYPO-XL is built in a weakly supervised
way with only a few human labeled data samples,
we conduct a quality analysis to investigate how
many sentences in the corpus are actually hyper-bolic. We randomly sample 100 instances from
HYPO-XL and manually label them as hyperbole
or non-hyperbole. Only six sentences are not hy-
perbole. This precision of 94% is on par with 92%
on another figurative language corpus of simile
(Zhang et al., 2021). Actually we can tolerate a
bit noise in the corpus since the primary goal of
HYPO-XL is to facilitate hyperbole generation in-
stead of detection , and a small proportion of non-
hyperbole sentences as input will not harm our
proposed method.Table 2 shows the statistics
of hyperbolic text spans (defined in Section 3.1)
for the rest 94 real hyperboles. We also provide
additional analyses in Appendix A.
3 Hyperbole Generation
We propose an unsupervised approach to generate
hyperbolic paraphrase from a literal sentence with
BART (Lewis et al., 2020) such that we do not re-
quire parallel literal-hyperbole pairs.An overview
of our hyperbole generation pipeline is shown in
Figure 1. It consists of two steps during training:
1.Mask. Given a hyperbolic sentence from
HYPO-XL, we identify multiple text spans
that can possibly produce the hyperbolic
meaning of a sentence, based on two features
(POS n-gram and unexpectedness score). For
each identified text span, we replace it with
the<mask> token to remove hyperbolic at-
tribute of the input. Ntext spans will result
inNmasked inputs, respectively.
2.Infill. We fine-tune BART to fill the masked
spans of input sentences. The model learns to
generate hyperbolic words or phrases that are
pertinent to the context.
During inference, there are three steps:
1.Mask. Given a literal sentence, we apply POS-
ngram-only masking to produce multiple in-
put sentences.
2.Over-generate. BART generates one sen-
tence from a masked input, resulting in multi-
ple candidates.
3.Rank. Candidates are ranked by their hyper-
bolicity and relevance to the source literal sen-
tence. The one with highest score is selected
as the final output.6020
We dub our hyperbole generation system
MOVER (Mask,OVE r-generate and Rank). We
apply masking technique to map both the hyper-
bolic (training input) and literal (test input) sen-
tences into a same “space” where the masked sen-
tence can be transformed into hyperbole by BART.
It falls into the “overgenerate-and-rank” paradigm
(Heilman and Smith, 2009) since many candidates
are available after the generation step. The remain-
der of this section details the three main modules:
hyperbolic span masking (Section 3.1), BART-
based span infilling (Section 3.2) and the hyperbole
ranker (Section 3.3).
3.1 Mask: Hyperbolic Span Masking
We make a simple observation that the hyperbolic
effect of a sentence is commonly localized to a sin-
gle word or a phrase, which is also supported by a
corpus-based linguistic study on hyperbole (Clar-
idge, 2010). For example, the word marathon in
“My evening jog with Bill turned into a marathon ”
overstates the jogging distance and causes the sen-
tence to be hyperbolic. This inspires us to lever-
age the “delete-and-generate” strategy (Li et al.,
2018) for hyperbole generation. Concretely, a lit-
eral sentence can be transformed into its hyperbolic
counterpart via hyperbolic span extraction and re-
placement. We propose to extract hyperbolic spans
based on POS n-gram (syntactic) and unexpected-
ness (semantic) features.POS N-gram We extract POS n-gram patterns of
hyperbole from the training set of HYPO dataset
and obtain 262 distinct POS n-grams. As a mo-
tivating example, the following three hyperbolic
spans, “faster than light”, “sweeter than honey”,
“whiter than snow” , share the same POS n-gram of
“JJR+IN+NN”.
Unexpectedness Hyperbolic spans are less co-
herent with the literal contexts and thus their vector
representations are distant from the context vectors.
Troiano et al. (2018) have verified this intuition
with the unexpectedness metric. They define the
unexpectedness score Uof a sentence swith the
token sequence {x, x, ..., x}as the average co-
sine distance among all of its word pairs.
U=average(cosine_distance (v, v))(1)
where vdenotes the word embedding vector of
token x. Similarly, we define the unexpectedness
score Uof an n-gram {x, x, ..., x}in
a sentence sas the average cosine distance among
word pairs that consist of one word inside the n-
gram and the other outside.
U= average(cosine_distance (v, v))
(2)6021
Text spans with higher unexpectedness scores tend
to be hyperbolic. Figure 2 illustrates the cosine
distance of the word pairs in the sentence “ I’ve
drowned myself trying to help you ”. The words in
the span “ drowned myself ” are distant from other
words in terms of word embedding similarity.
For the masking step during training, we extract
all text spans in the original input hyperbolic sen-
tences that match one of the hyperbolic POS n-
grams. Then we rank them by their unexpected-
ness scores and choose top-3 items as the masked
spans.For the masking step during inference, we
simply mask all the spans that match hyperbolic
POS n-grams, since the span unexpectedness score
is not applicable to a literal input. We evaluate the
accuracy of our hyperbolic span masking approach
on the development set of the HYPO dataset. The
proportion of exact match (EM) (Rajpurkar et al.,
2016) between our top-3 masked spans with the
human-labeled spans is 86%, which shows that
our simple method based on the above-mentioned
hand-crafted features is effective for the task of
hyperbolic span extraction.
3.2 Over-generate: Hyperbolic Text Infilling
with BART
In order to generate hyperbolic and coherent text
from the masked span, we leverage the text span
infilling ability of BART (Lewis et al., 2020), a pre-
trained sequence2sequence model with a denois-ing autoencoder and an autoregressive autodecoder.
During its pretraining, it learns to reconstruct the
corrupted noised text. One of the noising transfor-
mations is random span masking, which teaches
BART to predict the multiple tokens missing from
a span. During our training process, we fine-tune
BART by treating the masked hyperbolic sentence
as the encoder source and the original one as the
decoder target. This can change the probability
distribution when decoding tokens and increase the
chance of generating a hyperbolic, rather than lit-
eral, text span conditioned on the context. During
inference, BART fills the masked span of a literal
sentence with possible hyperbolic words.
Note that if the masked span of an input sentence
is actually not hyperbolic, then fine-tuning on this
example will just enhance the reconstruction ability
of BART, which will not exert negative effects on
hyperbole generation. This can give rise to our tol-
erance for non-hyperbolic sentences in the training
corpus (Section 2.3) and non-hyperbolic masked
span (Section 3.1).
3.3 Rank: Hyperbole Ranker
Recall that for each literal input during inference,
we apply POS-ngram-based masking, produce dif-
ferent masked versions of the sentence, and gen-
erate multiple output candidates. Obviously, not
all masking spans are suitable for infilling hyper-
bolic words due to the noise of masking. To select
the best candidate for final output, we introduce a
hyperbole ranker which sorts candidate sentences
by their degree of hyperbolicity and relevance to
the source inputs. For hyperbolicity evaluation,
we leverage the BERT-based hyperbole detection
model fine-tuned on HYPO and HYPO-L (Section
2.2) to assign a hyperbole score (i.e., prediction
probability) for each candidate. For the evaluation
of content preservation, we train a pairwise model
to predict whether hyperbolic sentence A is a para-
phrase of a literal sentence B. To this end, we use
the distilled RoBERTa-base model checkpoint
pretrained on large scale paraphrase data provided
by Sentence-Transformer (Reimers and Gurevych,
2019). It calculates the cosine similarity between
the literal input and the candidate as the paraphrase
score. We fine-tune the checkpoint on the train-
ing set of the HYPO dataset, where we treat the
pairs of hypo andpara as positive examples, and6022pairs of hypo andnon_hypo as negative examples
(Section 2). The accuracy on test set is 93%.
Now that we obtain the hyperbole score hypo(c)
and the paraphrase score para(c)for candidate c,
we propose an intuitive scoring function score(·)
as below:
score(s) =/braceleftigg
hypo(s)para(s)∈(γ,1−ϵ)
0 else
(3)
Here we filter out a candidate if its paraphrase score
is lower than a specific threshold γor it is almost
the same as the original input (i.e., the paraphrase
score is extremely close to 1). For diversity pur-
poses, we do not allow our system to simply copy
the literal input as its output. We then rank the
remaining candidates according to their hyperbole
score and select the best one as the final output.
4 Experiments
There are no existing models applied to the task
of word- or phrase-level hyperbole generation. To
compare the quality of the generated hyperboles,
we benchmark our MOVER system against three
baseline systems adapted from related tasks.
4.1 Baseline Systems
Retrieve (R1) Following Nikolov et al. (2020),
we implement a simple information retrieval base-
line, which retrieves the closest hyperbolic sen-
tence as the output (i.e., the highest cosine simi-
larity) from HYPO-XL, using the hyperbole para-
phrase detection model para(·)in Section 3.3. The
outputs of this baseline system should be hyper-
bolic yet have limited relevance to the input.
Retrieve, Replace and Rank (R3) We first re-
trieve the top-5 most similar sentences from HYPO-
XL like the R1 baseline. Then we apply hyperbolic
span extraction in Section 3.1 to find 3 text spans
for each retrieved sentence. We replace the text
spans in a literal input sentence with retrieved hy-
perbolic spans if two spans share the same POS
n-gram. Since this replacement method may result
in multiple modified sentences, we select the best
one with the hyperbole ranker in Section 3.3. If
there are no matched text spans, we fall back to R1
baseline and return the most similar retrieved sen-
tence verbatim. In fact, this baseline substitutes theBART generation model in MOVER system with a
simpler retrieval approach, which can demonstrate
the hyperbole generation ability of BART.
BART Inspired by Chakrabarty et al. (2020b),
we replace the text infilling model in Section 3.2
with a non-fintuned off-the-shelf BART,because
BART has already been pretrained to predict tokens
from a masked span.
4.2 Implementation Details
We use 16,075 (90%) samples in HYPO-XL for
training our MOVER system and the rest 1,787
sentences for validation. For POS Tagging in Sec-
tion 3.1, we use Stanford CoreNLP (Manning et al.,
2014). For the word embedding, we use 840B 300-
dimension version of GloVe vectors (Pennington
et al., 2014). For BART in Section 3.2, we use the
BART-base checkpoint instead of BART-large due
to limited computing resources and leverage the im-
plementation by Huggingface (Wolf et al., 2020).
We fine-tune pretrained BART for 16 epochs. For
the parameters of the hyperbole ranker in Section
3.3, we set γ= 0.8andϵ= 0.001by manual in-
spection of the ranking results on the development
set of the HYPO dataset.
4.3 Evaluation Criteria
Automatic Evaluation BLEU (Papineni et al.,
2002) reflects the lexical overlap between the gener-
ated and the ground-truth text. BERTScore (Zhang
et al., 2020) computes the similarity using contex-
tual embeddings. These are common metrics for
text generation. We use the 71 literal sentences
(para ) in the test set of HYPO dataset as test in-
puts and their corresponding hyperbolic sentences
(hypo ) as gold references. We report the BLEU
and BERTScore metrics for generated sentences
compared against human written hyperboles.
Human Evaluation Automated metrics are not
reliable on their own to evaluate methods to gener-
ate figurative language (Novikova et al., 2017) so
we also conduct pair-wise comparisons manually6023System BLEU BERTScore
R1 2.02 0.229
R3 33.25 0.520
BART 33.57 0.596
MOVER 39.43 0.624
w/o para score 39.22 0.604
w/o hypo ranker 34.83 0.610
COPY 51.69 0.711
(Shao et al., 2019). We evaluate the generation
results from the 71 testing literal sentences. Each
pair of texts (ours vs. a baseline / human reference)
is given preference (win, lose or tie) by five people
with proficiency in English. We use a set of four
criteria adapted from Chakrabarty et al. (2021) to
evaluate the generated outputs: 1) Fluency (Flu.) :
Which sentence is more fluent and grammatical? 2)
Hyperbolicity (Hypo.) : Which sentence is more
hyperbolic? 3) Creativity (Crea.) : Which sentence
is more creative? 4) Relevance (Rel.) : Which sen-
tence is more relevant to the input literal sentence?
4.4 Results
Automatic Evaluation Table 3 shows the auto-
matic evaluation results of our system compared to
different baselines. MOVER outperforms all three
baselines on these two metrics. However, BLEU
and BERTScore are far not comprehensive evalu-
ation measures for our hyperbole generation task,
since there are only a few modifications from lit-
eral to hyperbole and thus there is a lot of overlap
between the generated sentence and the source sen-
tence. Even a naive system (COPY in Table 3) that
simply returns the literal input verbatim as output
(Krishna et al., 2020) can achieve the highest per-
formance. As a result, automatic metrics are not
suitable for evaluating models that tend to copy
input as output.
Human Evaluation The inter-annotator agree-
ment of raw human evaluation results in terms of
Fleiss’ kappa (Fleiss, 1971) is 0.212, which indi-
cates fair agreement (Landis and Koch, 1977). We
take a conservative approach and only consider
items with an absolute majority label, i.e., at least
three of the five labelers choose the same prefer-
ence (win/lose/tie). On average there are 61 (86%)
items left for each baseline-criteria pair that satisfy
this requirement. On this subset of items, Fleiss’
Kappa increases to 0.278 (fair agreement). This de-
gree of agreement is acceptable compared to other
sentence revision tasks (e.g., 0.322 by Tan and
Lee (2014) and 0.263 by Afrin and Litman (2018))
since it is hard to discern the subtle changing effect
caused by local revision.
The annotation results in Table 4 are the absolute
majority vote (majority >=3) from the 5 annota-
tors for each item. The results show that our model
mostly outperforms (Win% >Lose%) other base-
lines in the four metrics, except for creativity on R1.
Because R1 directly retrieves human written hyper-
boles from HYPO-XL and is not strict about the rel-
evance, it has the advantage of being more creative
naturally. An example of this is shown in Table 5.
Our model achieves a balance between generating
hyperbolic output while preserving content, indicat-
ing the effectiveness of the “overgenerate-and-rank”
mechanism. It is also worth noting that in terms of
hyperbolicity, MOVER even performs better than
human for 16.7% of the test cases. Table 5 shows a
case where MOVER is rated higher than human.6024Generated Hyperbole s hypo(s)para(s)score(s)
You have ravished me away by a power I cannot resist. 0.962 0.954 0.962
You have ravished me away by a power I find unyielding to resist. 0.960 0.959 0.960
You have ravished me alive by a power I find difficult to resist. 0.954 0.931 0.954
You have driven me away by a power I find difficult to resist. 0.858 0.914 0.858
You have ravished me away with a beauty I find difficult to resist. 0.958 0.778 0.000
Case Study Table 5 shows a group of generated
examples from different systems. MOVER changes
the phrase “very bad” in the original input to an ex-
treme expression “sheer hell”, which captures the
sentiment polarity of the original sentence while
providing a hyperbolic effect. R1 retrieves a hyper-
bolic but irrelevant sentence. R3 replaces the word
“very” with “richly”, which is not coherent to the
context, although the word “richly” may introduce
some hyperbolic effects. BART just generates a
literal sentence, which seems to be a simple para-
phrase. Although human reference provides a valid
hyperbolic paraphrase, the annotators prefer our
version in terms of fluency, hyperbolicity and rele-
vance. Since our system makes fewer edits to the
input than to the human reference, we are more
likely to win in fluency and relevance. Also, the
generated hyperbolic span “sheer hell” presents a
more extreme exaggeration than “out of the world”
according to the human annotators.
Table 6 shows the over-generation results for a
literal input, with their hyperbole and paraphrase
scores. On the one hand, our system can generate
different hyperbolic versions, like the generated
words “cannot”, “unyielding”, and “alive”. This is
reasonable since there might be multiple hyperbolic
paraphrases for a single sentence. It is only for
comparison with other baselines that we have to use
the ranker to keep only one output, which inevitably
undermines the strength of our approach. On the
other hand, our ranker filters out the sentence if the
infilled text violates the original meaning, which
can be seen in the last row of Table 6. In this
way, we gain explicit control over hyperbolicity and
relevance through a scoring function, and endow
MOVER with more explainability.
Despite the interesting results, we also observe
the following types of errors in the generated out-
puts:•The output is a paraphrase instead of hyper-
bole: “ My aim is very certain ”→“My aim is
very clear ”.
•The degree of exaggeration is not enough:
“The news has been exaggerated ”→“The
news has been greatly exaggerated ”.
•The output is not meaningful: “ I’d love to
hang out every day ”→“I’d love to live
every day ”. We believe that incorporating
more commonsense knowledge and gener-
ating freeform hyperboles beyond word- or
phrase-level substitutions are promising for
future improvement.
Ablation Study We investigate the impact of re-
moving partial or all information during the ranking
stage. The results are shown in Table 3. Specif-
ically, if we rank multiple generated outputs by
only hyperbole score (w/o para score), or randomly
select one as the output (w/o hypo ranker), the per-
formance will become worse. Note that we do not
report the ablation result for ranking only by para-
phrase score (w/o hypo score), because it has the
same problem with COPY: a generated sentence
that directly copies the input will result in the high-
est paraphrase score and thus be selected as the
final output.
Furthermore, we note that the experiments on
R3 and BART also serve as ablation studies for the
text infilling model in Section 3.2 as they substitute
the fine-tuned BART with a retrieve-and-replace
method and a non-fine-tuned BART, respectively.
5 Related Work
Hyperbole Corpus Troiano et al. (2018) built
the HYPO dataset consisting of 709 hyperbolic
sentences with human-written paraphrases and
lexically overlapping non-hyperbolic counterparts.
Kong et al. (2020) also built a Chinese hyperbole6025dataset with 2680 hyperboles. Our HYPO-L and
HYPO-XL are substantially larger than HYPO and
we hope they can facilitate computational research
on hyperbole detection and generation.
Figurative Language Generation As a figure
of speech, hyperbole generation is related to the
general task of figurative language generation.
Previous studies have tackled the generation of
metaphor (Yu and Wan, 2019; Stowe et al., 2020;
Chakrabarty et al., 2021; Stowe et al., 2021), sim-
ile (Chakrabarty et al., 2020b; Zhang et al., 2021),
idiom (Zhou et al., 2021), pun (Yu et al., 2018;
Luo et al., 2019b; He et al., 2019; Yu et al., 2020),
and sarcasm (Chakrabarty et al., 2020a). HypoGen
(Tian et al., 2021) is a concurrent work with ours
on hyperbole generation. However, we share a dif-
ferent point of view and the two methods are not
directly comparable. They tackle the generation of
clause-level hyperboles and frame it as a sentence
completion task, while we focus on the word-level
orphrase-level ones and frame it as a sentence edit-
ingtask. In addition, their collected hyperboles and
generated outputs are limited to the “ so...that ” pat-
tern while we do not posit constraints on sentence
patterns.
Unsupervised Text Style Transfer Recent ad-
vances on unsupervised text style transfer (Hu et al.,
2017; Subramanian et al., 2018; Luo et al., 2019a;
Zeng et al., 2020) focus on transferring from one
text attribute to another without parallel data. Jin
et al. (2020) classify existing methods into three
main branches: disentanglement ,prototype editing ,
and pseudo-parallel corpus construction . We ar-
gue that hyperbole generation is different from text
style transfer. First, it is unclear whether “literal”
and “hyperbolic” can be treated as “styles”, espe-
cially the former one. Because “literal” sentences
do not have any specific characteristics at all, there
are no attribute markers (Li et al., 2018) in the input
sentences, and thus many text style transfer meth-
ods based on prototype editing cannot work. Sec-
ond, the hyperbolic span can be lexically separable
from, yet strongly dependent on, the context (Sec-
tion 3.1). On the contrary, disentanglement-based
approaches for text style transfer aim to separate
content and style via latent representation learning.
Third, MOVER could also be used for constructing
pseudo-parallel corpus of literal-hyperbole pairs
given enough literal sentences as input, which is
beyond the scope of this work.Unsupervised Paraphrase Generation Unsu-
pervised paraphrase generation models (Wieting
et al., 2017; Zhang et al., 2019a; Roy and Grangier,
2019; Huang and Chang, 2021) do not require para-
phrase pairs for training. Although hyperbole gen-
eration also needs content preservation and lacks
parallel training data, it is still different from para-
phrase generation because we need to create a bal-
ance between paraphrasing and exaggerating. We
further note that the task of metaphor generation
(Chakrabarty et al., 2021), which replaces a verb
(e.g., “ The scream filled the night ”→“The scream
pierced the night ”), is also independent of para-
phrase generation.
6 Conclusion
We tackle the challenging task of figurative lan-
guage generation: hyperbole generation from lit-
eral sentences. We build the first large-scale hy-
perbole corpus HYPO-XL and propose an unsuper-
vised approach MOVER for generating hyperbole
in a controllable way. The results of automatic and
human evaluation show that our model is successful
in generating hyperbole. The proposed generation
pipeline has better interpretability and flexibility
compared to the potential end-to-end methods. In
future, we plan to apply our “mask-overgenerate-
rank” approach to the generation of other figurative
languages, such as metaphor and irony.
7 Ethical Consideration
The HYPO-XL dataset is collected from a public
website Sentencedict.com, and we have asked the
website owners permission to use their data for
research purposes. There is no explicit detail that
leaks a user’s personal information including name,
health, racial or ethnic origin, religious affiliation
or beliefs, sexual orientation, etc.
Our proposed method MOVER utilizes the pre-
trained language model, which may inherit the bias
in the massive training data. It is possible that
MOVER is used for malicious purposes, since it
does not explicitly filter input sentences with tox-
icity, bias or offensiveness. Therefore, the output
generated by MOVER could potentially be harm-
ful to certain groups or individuals. It is important
that interested parties carefully address those biases
before applying the model to real-world situations.6026Acknowledgements
This work was supported by National Science Foun-
dation of China (No. 62161160339), National Key
R&D Program of China (No.2018YFB1005100),
State Key Laboratory of Media Convergence Pro-
duction Technology and Systems and Key Labo-
ratory of Science, Techonology and Standard in
Press Industry (Key Laboratory of Intelligent Press
Media Technology). We appreciate the anonymous
reviewers for their helpful comments. Xiaojun Wan
is the corresponding auhor.
References60276028
A Additional Dataset Statistics
We annotate the hyperbolic spans (Section 3.1) for
the 94 real hyperboles in Section 2.3 and show
some examples of the most common POS n-grams
of hyperbolic spans in Table 7. We further fol-
low Troiano et al. (2018) to annotate the types
of exaggeration along three dimensions: “measur-
able”, “possible” and “conventional”. A hyperbole
is “measurable” if it exaggerates something which
is objective and quantifiable. A hyperbole is rated
as “possible” if it denotes an extreme but conceiv-
able situation. A hyperbole is judged as “conven-
tional” if it does not express an idea in a creative
way. However, we note that there are no absolute
answers for these three questions and the annota-
tion results may be subjective. Each hyperbole
is either YES or NO for each dimension and the
reported numbers in Table 8 are for YES.
B More Generated Examples
Table 9 shows more examples of output generated
from different systems and human references.6029POS # Hyperbole Example
NN 19 His words confirmed everything .
RB 15 He descanted endlessly upon the wonders of his trip.
JJ 14 Youth means limitless possibilities.
Type # Hyperbole Example
Measurable 44 At any moment, I feared, the boys could snap my body in half
with just one concerted shove.
Possible 27 The words caused a shiver to run a fine sharp line through her.
Conventional 65 She is forever picking at the child.
System Sentence Flu. Hypo. Crea. Rel.
LITERAL At that point, the presidency was hard to recover. - - - -
MOVER At that point the presidency was virtually impossible to recover. - - - -
R1 The destruction of a President with its collapse of executive au-
thority was too staggering to contemplate .W W T W
R3 At that point the presidency was staggering to recover. W W L W
BART At that point the presidency was too fragile to recover T W T T
HUMAN At that point, the presidency was fatally wounded . T W W W
LITERAL His piano playing is very bad. - - - -
MOVER His piano playing is beyond bad. - - - -
R1 Her piano playing is absolute magic . T T L W
R3 His piano guitar is very bad. T T L L
BART His piano playing is very good . T W W W
HUMAN His piano playing is enough to make Beethoven turn in his grave . T L L W
LITERAL The professor humiliated me in front of the class. - - - -
MOVER The professor humiliated me in every conceivable way . - - - -
R1 She infected the whole class with her enthusiasm. W W W W
R3 That lecture humiliated me in front of the class. T W T T
BART The professor humiliated me and the rest of the class. W W W W
HUMAN The professor destroyed me in front of the class. T L W W
LITERAL It annoys me when you only drink half of the soda. - - - -
MOVER It kills me when you only drink half of the soda. - - - -
R1 That was the best ice-cream soda I ever tasted . T W W W
R3 It annoys me when you only drink boredom of the soda. T W W T
BART It annoys me when you only drink half of it. W W W W
HUMAN It drives me crazy when you only drink half of the soda. T W T T6030