
Haohai SunShangyi GengJialun ZhongHan HuKun HeSchool of Computer Science and Technology,
Huazhong University of Science and TechnologyMicrosoft Research Asia
{haohais, shangyigeng, zhongjl}@hust.edu.cn
hanhu@microsoft.com brooklet60@hust.edu.cn
Abstract
Temporal Knowledge Graph (TKG) reasoning
has attracted increasing attention due to its enor-
mous potential value, and the critical issue is
how to model the complex temporal structure
information effectively. Recent studies use the
method of encoding graph snapshots into hid-
den vector space and then performing heuristic
deductions, which perform well on the task of
entity prediction. However, these approaches
cannot predict when an event will occur, and
have the following limitations: 1) there are
many facts not related to the query that can
confuse the model; 2) there exists information
forgetting caused by long-term evolutionary
processes. To this end, we propose a Graph
Hawkes Transformer (GHT) for both TKG en-
tity prediction and time prediction tasks in the
future time. In GHT, there are two variants of
Transformer, which capture the instantaneous
structural information and temporal evolution
information, respectively, and a new relational
continuous-time encoding function to facili-
tate feature evolution with the Hawkes pro-
cess. Extensive experiments on four public
datasets demonstrate its superior performance,
especially on long-term evolutionary tasks.
1 Introduction
Knowledge Graph (KG), a multi-relational di-
rected graph database that stores human knowl-
edge and facts, is widely used in downstream ap-
plications such as recommendation systems (Guo
et al., 2020; Wang et al., 2018), web search (Paul-
heim, 2017) and question answering (Saxena et al.,
2021). Conventionally, KGs store each fact in the
form of a triplet(𝑠𝑢𝑏𝑗𝑒𝑐𝑡, 𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒, 𝑜𝑏𝑗𝑒𝑐𝑡 ).
However, many facts may change over time
and may contain event-based interaction data.
To encode the temporal information, Temporal
Knowledge Graph (TKG) is proposed so thateach temporal fact is stored as a quadruple
(𝑠𝑢𝑏𝑗𝑒𝑐𝑡, 𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒, 𝑜𝑏𝑗𝑒𝑐𝑡, 𝑡𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 ). Typ-
ically, we represent TKG as a sequence of static
KG snapshots associated with timestamps.
There are two types of reasoning for TKG. In-
terpolated reasoning is designed to complete the
missing facts on known historical snapshots, while
extrapolated reasoning predicts future events or
facts. This work focuses on extrapolated reason-
ing, a problematic but meaningful task, which can
be used for crisis warning, user behavior predic-
tion, supply chain management, etc. While most
existing works (Han et al., 2021a; Sun et al., 2021)
perform link predictions on snapshots at the next
timestamp, we consider predicting events that may
occur over a long period in the future, which are
more appropriate in the real-world setting.
The task requires the assistance of known histor-
ical facts. A key feature required by the model is
to retrieve key information from complex tempo-
ral structured data that can help answer the query
and make correct judgments. Recent methods
of RE-NET (Jin et al., 2020) and RE-GCN (Li
et al., 2021b) employ R-GCN (Schlichtkrull et al.,
2018) to capture structural information from his-
torical snapshots and then use Recurrent Neural
Networks (RNNs) to model the latent vector se-
quences. While the two methods work well on en-
tity prediction task, they also meet some limitations.
(1) Query information cannot be fully utilized by
R-GCN. (2) RNN-based models assume that se-
quences are equidistant, which is inconsistent with
real-life event sequences. (3) Step-by-step infer-
ence methods accumulate errors during training.
(4) They cannot predict the timestamp that an event
will occur in the future.
There are some early methods (Trivedi et al.,
2017, 2019) that could do both tasks simultane-
ously by using a temporal point process to repre-
sent the evolution of facts, with the key concept
of conditional intensity function denoting the con-7481ditional probability of an event occurring over a
period. Nevertheless, these methods could not per-
form well on the entity prediction task as they
did not capture the structural information from
the graph snapshots. On the other hand, Trans-
former (Vaswani et al., 2017) has recently been
widely used in various fields (Lin et al., 2021) ow-
ing to its powerful modeling capabilities. However,
to our knowledge, there exists no work that uses
Transformer or its variants to solve the TKG evolu-
tion problem.
In this work, we propose a new model termed
Graph Hawkes Transformer (GHT), which intro-
duces Transformer into the TKG evolution model-
ing and further integrates the neural temporal point
process. Specifically, we design two Transformer
variants to capture the structural and temporal infor-
mation in TKGs by building conditional intensity
function. One variant is used to aggregate multi-
relational graphs, capturing structural information
for each timestamp and generating feature vectors.
The model can learn which interactions are more
critical in query with the attention mechanism. The
other variant captures temporal information based
on a sequence of feature vectors and simultane-
ously outputs a hidden state for each timestamp in
the future. Finally, the model uses the hidden state
to calculate the conditional intensity and then gets
the candidate entity score or the time probability
density of the next event.
Our main contributions are as follows:
•We propose a new Transformer-based model
for TKG extrapolated reasoning, which cap-
tures both structural and temporal information.
Not only can it simultaneously predict events
of multiple timestamps in the future, but it can
also predict when an event will occur. To our
knowledge, this is the first Transformer-based
temporal point process model for the TKG
evolutionary representation learning.
•We design a new relational continuous-time
encoding function that can handle unseen
timestamps and provide personalized re-
sponses to different queries. It can be used to
construct conditional intensity in the Hawkes
process.
•State-of-the-art performance has been
achieved on four popular benchmarks,
indicating the effectiveness of our model,
especially on long-term evolution tasks.2 Related Work
2.1 Neural Hawkes Process
The Hawkes process is a self-exciting temporal
point process applied to model sequential dis-
crete events occurring in continuous time (Hawkes,
1971). It assumes that past events can temporarily
excite future events, characterized via an intensity
function. The intensity function 𝜆(𝑡)represents
the expected number of events happened in interval
(𝑡,𝑡+𝑑𝑡]defined as follows, where ∗is a shorthand
for conditioning on history H:
(1)
The traditional Hawkes process can only cap-
ture simple patterns of the events. In contrast, the
neural Hawkes process (Shchur et al., 2021) that
introduces neural networks to parameterize the in-
tensity function exhibits high model capacity in
complex real-life scenarios. Researchers have re-
cently started adapting neural networks, especially
RNN models, to the temporal point process to build
more flexible and efficient models. RMTPP (Zhou
et al., 2013) embeds the sequence data into RNN
and models the conditional intensity function con-
sidering the historical non-linear dependence. Mei
and Eisner (2016) develop a neural Hawkes pro-
cess based on LSTM to model the asynchronous
event sequence. Zhang et al. (2020) apply the at-
tention mechanism to the neural Hawkes process.
Moreover, Zuo et al. (2020) propose a Transformer
Hawkes process, which utilizes Transformer to cap-
ture the complicated short-term and long-term tem-
poral dependencies. However, these works do not
model structured information.
2.2 Temporal Knowledge Graph Reasoning
Embedding-based methods (Bordes et al., 2013;
Trouillon et al., 2016) achieve excellent results
on static KGs, and have been extended to tempo-
ral KGs (Leblay and Chekol, 2018; García-Durán
et al., 2018; Goel et al., 2020; Lacroix et al., 2020).
However, these methods cannot handle extrapo-
lated reasoning since the timestamps in the test
dataset do not exist in the training dataset. For
extrapolation, Know-Evolve (Trivedi et al., 2017)
and GHNN (Han et al., 2020) use temporal point
processes to estimate conditional probability. How-
ever, they fail to model the structural information
in historical graphs, leading to low performance.7482Recently, TITer (Sun et al., 2021) uses rein-
forcement learning to search answer in the his-
tory. xERTE (Han et al., 2021a) finds answers
on subgraphs through subgraph sampling and at-
tention flow. Both methods restrict the answer
domain to N-order neighbors and do not capture
evolutionary representations. CyGNet (Zhu et al.,
2021) uses a generate-copy mechanism to let the
model remember recurring historical events. RE-
NET (Jin et al., 2020) and RE-GCN (Li et al.,
2021b) use RGCN (Schlichtkrull et al., 2018) to
capture structural information and then use RNN
to perform representational evolution. Addition-
ally, TANGO (Han et al., 2021b) uses a neural
ordinary differential equation to model this task in
the continuous-time domain. CluSTeR (Li et al.,
2021a) uses a two-stage approach for clue search-
ing and candidate ranking. These methods achieve
good performance on the entity prediction task but
cannot predict time.
3 The Proposed Model
Our method first obtains concurrent structural in-
formation from subgraphs and generates feature
vectors for each source node. Then it captures the
temporal evolution information from the vector se-
quences and finally outputs the hidden state vectors
to participate in the construction of the conditional
strength function to complete the prediction task.
Compared with previous evolutionary representa-
tion learning methods, e.g., RE-NET (Jin et al.,
2020) and RE-GCN (Li et al., 2021b), we expect
the model to make up for the four limitations intro-
duced in Section 1. Figure 1 illustrates the overall
architecture of our model.
3.1 Notations and Task Definition
LetEandRdenote the sets of entities and relations,
respectively. LetFdenote the set of facts at time 𝑡.
A TKG can be represented as a sequence of static
KG snapshots, i.e., a known TKG from time 1to
time𝑡can be described as G={G,G,...,G},
whereG=(E,R,F)is the KG snapshot at time 𝑡,
a directed multi-relational graph. Each fact in Fis
described in the form of a quadruple (𝑒,𝑟,𝑒,𝑡),
where𝑒,𝑒∈Eand𝑟∈R. The quadruple can be
seen as an edge from 𝑒to𝑒of type𝑟at graphG.
Based on the historical TKG G, the model
needs to predict the facts in time 𝑡. We further
consider another task of predicting the time of an
event that will occur in the future. For a concretefact, the two tasks can be defined as follows.
Task 1. Entity Prediction. Given a query
(𝑒,𝑟,?,𝑡), the model needs to predict the miss-
ing entity.
Task 2. Time Prediction. Given a query
(𝑒,𝑟,𝑒,?), the model needs to predict the times-
tamp𝑡that this event will occur next time.
In the framework of the neural Hawkes pro-
cess (Shchur et al., 2021; Han et al., 2020), H=
(𝑒,𝑟,G)denotes the historical information
related to the query until time 𝑡. Let𝜆(𝑡|H)be
the conditional intensity function of a candidate ob-
ject entity𝑒, abbreviated as 𝜆, we can accomplish
both tasks with the conditional intensity function:
𝑒=𝑎𝑟𝑔𝑚𝑎𝑥{𝜆}, (2)(3)
𝑡=∫𝜏𝑝(𝜏|𝑒,H)𝑑𝜏. (4)
Since all candidate entities share the same sur-
vival term (Han et al., 2020), we can directly com-
pare the intensity value to get the answer entity, as
shown in Eq. 2. Eq. 3 is the corresponding condi-
tional time density function, according to which we
can estimate the time through the integral formula
of Eq. 4. Then, we introduce how to model the
intensity function.
3.2 Relational Graph Transformer
Capturing the structural information on historical
snapshots is the first key to answer the query. Pre-
vious works (Jin et al., 2020; Li et al., 2021b) use
R-GCN (Schlichtkrull et al., 2018) for informa-
tion aggregation to store interaction information
between nodes in the form of hidden vectors.
However, in a historical snapshot, many events
relate to an entity simultaneously, yet only a tiny
fraction of the relational information is helpful to
answer the query. R-GCN cannot handle this issue
as it treats every message equally important. There-
fore, we design a Relational Graph Transformer
(RGT) to let our model know which concurrent
events are more critical in a snapshot. There are
two forms of graph representation: global recep-
tive field (Ying et al., 2021) and local receptive
field (Velickovic et al., 2018; Dwivedi and Bresson,7483
2020; Hu et al., 2020). The global form cannot
cope with graphs with too many nodes. Thus, to
maximize the preservation of structural informa-
tion of the graph, RGT operates in local graph
neighborhoods.
LetE∈RandR∈Rbe the initial
embedding matrices of the entities and relations,
respectively, where 𝑑represents the dimension of
the embedding.e=E[𝑖]is the embedding of
entity𝑒, andr=R[𝑖]is the embedding of relation
𝑟. RGT aggregates structural information from
each incoming edge for each entity in the snapshots
sequence.
Specifically, forG, we initialize the hidden
states of the nodes as the initial embeddings of the
corresponding entities, and each node will update
its hidden state via the message-passing framework.
E.g., for an entity 𝑒, we concatenate the hidden
state of the source node and the corresponding rela-
tion embedding as the message for each incoming
edge𝑒. Then we pack them together as the key ma-
trix𝐾∈Rand value matrix 𝑉∈R,
where|𝑀|is the number of messages. For each
message, we use ras the query, and the query
matrix is𝑄∈R. Finally, the hidden state of
an entity at layer 𝑙∈[1,𝑁]is defined as follows:
h=𝐿𝑁(𝐹𝐹𝑁(𝑀𝐻𝐴(𝑄,𝐾,𝑉)+h),(5)where𝐿𝑁is the layer normalization, 𝐹𝐹𝑁 is the
feed-forward blocks, and 𝑀𝐻𝐴 is Multi-Head At-
tention (Vaswani et al., 2017). Finally, we out-
put the hidden state sequence of the query entity
𝐻={h,...,h}. We do not introduce query
entity information ein the query matrix 𝑄, be-
cause as a TKG develops, new entities could appear
on the graph, and the model has not learned their
initial embeddings. In contrast, the semantics of
relations are stable and context-independent , so all
layers share the relational embedding matrix.
3.3 Temporal Transformer
We design the Temporal Transformer (TT) to model
the temporal evolution of entity representations in
the continuous-time domain. Transformer learns
location information through a position encoding
function. Although researchers have designed a
variety of position encoding methods (Devlin et al.,
2019; Shaw et al., 2018; Dai et al., 2019; Ke et al.,
2021), most of them do not apply to our scenario
due to the following two reasons: (1) These posi-
tion encoding methods are performed in the dis-
crete domain and are not suitable for the Hawkes
process. (2) Different from discrete tokens, our
input is a continuous vector after the graph infor-
mation aggregation, and the distribution shift be-
tween the training set and the test set will lead
them (Vaswani et al., 2017; Devlin et al., 2019) to
have poor generalization performance.7484Moreover, the temporal distribution of attention
could be different for different query event types.
For example, diarrhea is more likely to be caused
by eating spoiled food the day before, while long-
term eating habits may cause obesity. Thus, we re-
design a relational continuous-time encoding func-
tion to assist the attention calculation. We need
to ensure the inductive ability of the function be-
cause future timestamps cannot be seen during the
model training. Based on the calculation princi-
ple of Transformer attention, we need to ensure
that (Xu et al., 2020), ∀𝑐∈R:
⟨𝑇𝐸(𝑡+𝑐),𝑇𝐸(𝑡+𝑐)⟩=⟨𝑇𝐸(𝑡),𝑇𝐸(𝑡)⟩,
where𝑇𝐸denotes the time encoding function, ⟨·,·⟩
indicates inner product, 𝑡and𝑡represent the time
of key and time of query in the attention calculation,
respectively. We can use absolute time encoding to
represent relative time information.
We design a learnable sinusoid function that
meets the above condition, and use the query rela-
tionrto control the amplitude of the function:
𝑇𝐸(𝑡)=[𝛼𝑐𝑜𝑠(𝑤𝑡),𝛼𝑠𝑖𝑛(𝑤𝑡),...,
𝛼𝑐𝑜𝑠(𝑤𝑡),𝛼𝑠𝑖𝑛(𝑤𝑡)],(6)
where[𝛼,...,𝛼]is the linear projection of the
relation embedding r, and[𝑤,...,𝑤]is a𝑑-
dimensional learnable vector. We use the time
information to calculate a bias term for the atten-
tion matrix𝐴. Let𝑇=[𝑇𝐸(𝑡);...;𝑇𝐸(𝑡)]
denote the queries’ time encoding matrix, and
𝑇=[𝑇𝐸(𝑡);...;𝑇𝐸(𝑡)]denotes the keys’
time encoding matrix, we have the attention matrix
𝐴as follows:
𝐴=(𝐻𝑊)(𝐻𝑊)+(𝑇)(𝑇)
√
2𝑑,(7)
where𝑊,𝑊are weight matrices. 𝐻is the em-
bedding matrix of the query relation and 𝐻is the
packed matrix of the query entity embedding se-
quence obtained from RGT.
3.4 Conditional Intensity
After the encoding described in Section 3.3, we
can generate a hidden representation at any time
in the future. Then, we can use it to construct a
continuous-time conditional intensity function 𝜆
for all candidate entities:
𝜆=𝑓(⟨[e,r,h]𝑊,e⟩), (8)where𝑊∈Ris the projection matrix, h
is the hidden state at time 𝑡obtained from TT,
𝑓(𝑥)=𝛽·𝑙𝑜𝑔(1+𝑒𝑥𝑝())is softplus function
with parameter 𝛽that guarantees a positive inten-
sity, and e,r, and eare the embeddings of the
query entity, query relation and candidate entity,
respectively.
Finally, we can predict the entity or time based
on Eq. 2, Eq. 3 and Eq. 4. For the integral oper-
ation, we use the trapezoidal rule (Stoer and Bu-
lirsch, 2013) for approximation:
𝑡=∑︁𝑡−𝑡
2/√︁a√︂enleftbig𝑡𝑝(𝑡|𝑒,H)+
𝑡𝑝(𝑡|𝑒,H)/√︁a√︂en√︂ightbig,(9)
where𝑡∈[𝑡,+∞), and𝐿is the number of sam-
ples. Other estimation methods, such as Simpson’s
rule (Stoer and Bulirsch, 2013) and Monte Carlo
integration (Stoer and Bulirsch, 2013) can also be
used.
3.5 Training
We view the entity prediction task as a multi-class
classification task and the time prediction task as
a regression task. Then, we use the cross-entropy
loss for the entity prediction task and the mean
square error (MSE) loss for the time prediction
task. LetD denote the training set, 𝐿be the
loss of the entity prediction and 𝐿be the loss of
time prediction. Then,(10)
𝐿=∑︁(𝑡−ˆ𝑡). (11)
Here ˆ𝑡is the estimated time. We jointly train the
two tasks, and the final loss 𝐿=𝐿+𝜇𝐿, where
𝜇is a hyperparameter.
4 Experiments
4.1 Experimental Setup
Datasets We evaluate our model on four public
TKG datasets, ICEWS14, ICEWS18 ICEWS05-
15 and GDELT. Integrated Crisis Early Warning
System (ICEWS) (Boschee et al., 2015) is an inter-
national event dataset. Its three subsets, ICEWS14,
ICEWS18 and ICEWS05-15, are usually used to
evaluate the performance of TKG reasoning mod-
els, which contains events occurring in 2014, 2018,7485and 2005 to 2015 respectively. Global Database
of Events, Language and Tone (GDELT) is a large
comprehensive event dataset that records data ev-
ery 15 minutes. Following the previous work (Jin
et al., 2020), we split the dataset into train/valid/test
by timestamps, and 𝑡𝑟𝑎𝑖𝑛 𝑡𝑖𝑚𝑒 < 𝑣𝑎𝑙𝑖𝑑 𝑡𝑖𝑚𝑒 <
𝑡𝑒𝑠𝑡𝑡𝑖𝑚𝑒 . Statistics of these datasets are shown in
Appendix A.1.
Evaluation Metrics The MRR and Hits@ 𝑘(𝑘∈
{1,3,10}) are standard metrics for the entity pre-
diction task. MRR is the average reciprocal of the
correct query answer rank. Hits@ 𝑘indicates the
proportion of correct answers among the top 𝑘can-
didates. As mentioned in (Han et al., 2020; Sun
et al., 2021), the static filtered setting (Jin et al.,
2020; Zhu et al., 2021), which removes entities
from candidates according to the triples without
considering the time, is unsuitable for TKG rea-
soning. Thus, we adopt the time-aware filtered
setting (Han et al., 2020; Sun et al., 2021; Han
et al., 2021a).
Moreover, most previous works only evaluate
the performance of their models for the entity pre-
diction at the next timestamp. Such evaluation
cannot adequately reflect the model’s performance
for future predictions. Therefore, we evaluate the
model’s short-term and long-term evolution by set-
ting different forecasting time window size of Δ𝑡.
For the time prediction task, we use the mean
absolute error (MAE) between the predicted time
and ground truth time as the metric.
Baselines For the entity prediction task, we
compare our model with three types of KG rea-
soning models: (1) static KG reasoning mod-
els, including TransE (Bordes et al., 2013), Dist-
Mult (Yang et al., 2014) and ComplEx (Trouillon
et al., 2016). Ignoring the time information, we
can get a static knowledge graph, and then ap-
ply these model. (2) TKG interpolated reasoning
models, including TTransE (Leblay and Chekol,
2018), TA-DistMult (García-Durán et al., 2018),
DE-SimplE (Goel et al., 2020), and TNTCom-
plEx (Lacroix et al., 2020); (3) TKG extrapolated
reasoning models, including RE-NET (Jin et al.,
2020), CyGNet (Zhu et al., 2021), RE-GCN (Li
et al., 2021b) and TITer (Sun et al., 2021). It is
worth noting that RE-NET and RE-GCN are the
most relevant model, which also learn evolutionary
representation.
For the time prediction task, we compare ourmodel with the previous temporal point process
model for TKG, GHNN (Han et al., 2020). As for
Know-evolve (Trivedi et al., 2017), it has a prob-
lematic formulation that has already been discussed
in (Jin et al., 2020; Han et al., 2020). Thus, we do
not choose it as a baseline.
4.2 Implementation Details
Baseline details During the inference, static rea-
soning models, interpolated reasoning models, and
CyGNet do not encode the historical information.
Therefore, they perform the same in the short-
term as well as the long-term evolution task, so
we directly use the results reported in previous
works (Sun et al., 2021; Han et al., 2021a). For
TITer, RE-NET, RE-GCN, and GHNN, using
their released source code with default hyperparam-
eters, we rerun these models on four benchmarks.
To ensure fairness, for RE-GCN, we do not use the
module that encodes the node type information.
GHT details We implement our model in Py-
Torch. We set the dimension of all embeddings and
hidden states to 100. The history length is limited
to 6. The layer number of the Relational Graph
Transformer is set to 2, and the attention head num-
ber is set to 1. For Temporal Transformer, the layer
number is 2, and the attention head number is 2.
We apply dropout with dropout rate of 50% to each
layer and use label smoothing to suppress overfit-
ting. The hyperparameter 𝜇used for loss function
is set to 0.2. We use Adam to optimize the param-
eters, with a learning rate of 0.003 and a weight
decay of 0.0001. The batch size is set to 256. All
experiments were done on Tesla P100.
4.3 Results and Discussion
4.3.1 Entity Prediction
Table 1 and Figure 2 report the experimental results
on the entity prediction task. We report results for
Δ𝑡=10in details, other cases can be found in
Appendix A.2. To explore the effect of evolution
time on our model’s performance, we report MRR
over four datasets as the evolution timespan grows,
where𝑑𝑡indicates that the model predicts events
after𝑑𝑡interval. Due to layout limitations, the
experimental results for GDELT are in Appendix
A.2.7486
The extrapolated reasoning methods can deal
with unseen timestamps. GHT outperforms all
these methods on metrics MRR, Hits@3, and
Hits@10. As for Hits@1, GHT also achieves the
state-of-the-art (SOTA) performance. We further
analyze and compare these models through the
curve diagrams in Figure 2. As can be seen, al-
though TITer outperforms the other models when
𝑑𝑡=1, it decays faster on long-term future pre-
dictions. We think this is because it searches the
answer in local subgraphs, focusing more on the
explicit cues. As the evolution timespan grows, the
explicit cues will gradually decrease, and the im-
plicit cues will become more important. Moreover,
retrieving answers from neighbor subgraphs limits
the candidate answer space, which is more pro-
nounced on long-term evolution tasks. Our model
outperforms RE-NET and RE-GCN in predicting
long-term events because they use a heuristic evolu-
tionary method to predict long-term events through
gradual evolution, which forgets previous knowl-
edge during the evolution. By contrast, we intro-
duce an attention mechanism when encoding struc-
tural information, which helps the model capture
more helpful information.
Overall, our model has excellent performance,
providing more accurate answers when perform-
ing entity prediction tasks, especially over long
intervals.
4.3.2 Time Prediction
Figure 3 shows the results of the time prediction
task. The result indicates the superiority of our
model utilizing Transformer for conditional in-
tensity function construction. GHNN aggregates7487
Model MRR H@1 H@3 H@10
GHT 37.40 27.77 41.66 56.19
GHT w.o. TE 36.21 26.52 40.31 55.69
GHT w. APE 36.42 26.91 40.46 55.20
GHT w. RPE 36.49 26.88 40.73 55.24
GHT w. RGCN 33.86 24.25 37.87 52.75
neighbor information through simple mean pooling
and only focuses on the most relevant first-order
neighbors. In comparison, our RGT has a better
structural information extraction ability. Moreover,
GHNN uses continuous-time LSTM (Mei and Eis-
ner, 2016) to estimate the intensity function, which
is not as good as Transformer in capturing complex
long-term and short-term dependencies(Zuo et al.,
2020). Therefore, GHT outpeforms GHNN on the
time prediction task.
During the experiment, our model also shows
good efficiency. The discussion on efficiency anal-
ysis can be found in Appendix A.3.
4.4 Ablation Study
In this subsection, we study the effect of each mod-
ule separately, and the results are reported in Table
2 and Figure 4. We do all ablation experiments on
the ICEWS14 dataset. Specially, we try various
position encoding functions to show the effective-
ness of the relational time encoding function, and
use other relational GNNs instead of RGT to verify
the performance of RGT. We report the entity pre-
diction results in Table 2, and the time prediction
results in Figure 4. We also study the hyperpa-
rameter’s sensitivity of GHT, which can be seen in
Appendix A.4
4.4.1 Different Graph Aggregator
R-GCN, the most common relational graph aggre-
gator, is used in RE-NET and RE-GCN. We replace
RGT with R-GCN and denote the model as GHTw. RGCN . Results in Table 2 and Figure 4 demon-
strate that GHT significantly outperforms GHT w.
RGCN on both entity and time prediction tasks. It
indicates that RGT is more suitable for TKG ex-
trapolated reasoning than R-GCN. Compared with
R-GCN, RGT can extract more useful information
for query answering from complex structural infor-
mation.
4.4.2 Different Position Encoding Functions
To verify whether our proposed relational
continuous-time encoding function is effective, we
compare GHT with several variants, which use
different position encoding methods: (1) Do not
use the relational continuous-time encoding func-
tion, denoted as GHT w.o. TE ; (2) Replace it with
absolute position encoding (Devlin et al., 2019),
denoted as GHT w. APE ; (3) Replace it with rela-
tive position encoding (Ke et al., 2021), denoted as
GHT w. RPE . In Table 2 and Figure 4, we observe
that GHT outperforms all other variants on both
entity and time prediction tasks. Note that GHT w.
APE andGHT w. RPE perform almost the same
as not using the position encoding function. This
indicates that they did not learn the location infor-
mation. By contrast, relational continuous-time
encoding can effectively capture temporal informa-
tion and help the model achieve good performance.
5 Conclusion
We propose a new TKG reasoning model, called
Graph Hawkes Transformer (GHT), a neural tempo-
ral point process model based on Transformer. We
first analyze four limitations of the previous state-
of-the-art methods, RE-GCN (Li et al., 2021b) and
RE-NET (Jin et al., 2020). Then we design two
Transformer blocks, which are used to capture the
structural and temporal information, respectively.
Based on Hawkes process, the model can learn a
conditional intensity function to solve the above
issues with the attention mechanism and the pro-
posed relational continuous-time encoding func-
tion. Moreover, most previous works only evalu-
ate their methods for entity prediction at the next
future timestamp, while we evaluate the models
comprehensively by setting different forecasting
time window sizes. Experimental results on four
popular datasets demonstrate the superior perfor-
mance of our model on both entity prediction and
time prediction tasks. Notably, our model performs
much better under long-term evolution scenarios.74886 Limitations
The inference of GHT relies entirely on the learned
entity representation, and new entities that emerge
as TKG evolves will get a random initialization,
which limits the model’s inductive reasoning abil-
ity.
When making time predictions, the integral op-
eration is implemented by approximate estimation.
The more accurate the estimation, the more calcu-
lations are required. The attention computation in
Transformer is also of quadratic complexity, which
requires much calculation. All of these factors limit
the capability of GHT to handle large-scale graphs.
Acknowledgements
This work is supported by National Natural Sci-
ence Foundation (62076105) and International Co-
operation Foundation of Hubei Province, China
(2021EHB011).
References748974907491A Appendix
A.1 Dataset Statistics
Table 3 details the statistics of ICEWS14,
ICEWS18, ICEWS05-15 and GDELT.
A.2 Supplementary Results
In order to illustrate the performance of our model
more comprehensively, we conduct experiments
under different Δ𝑡. Table 1 reports the experimental
results when Δ𝑡=5. Since the experiments on the
GDELT dataset are very time-consuming, Table 1
only provides the experimental results on the three
ICEWS datasets.
Figure 2 presents the results of MRR for the
GDELT dataset as 𝑑𝑡changes. Since the training
of RE-NET and TITer on the GDELT dataset are
too time-consuming, we use CyGNet instead for
comparison. Because the time span of GDELT is
relatively small, the change is not obvious.
A.3 Efficiency Analysis
We analyzed the computational complexity of GHT
for each module. The computational complexity of
RGT is𝑂(𝑁𝑀𝑑), where𝑁is the layer number,
𝑀is the message number(in-degree of the graph),
and𝑑is the dimension. For TT, the computational
complexity is 𝑂(ˆ𝑁𝐿𝑑), and𝐿is the sequence
length. Thus, the computational complexity of
GHT is𝑂(𝑁𝑀𝑑+ˆ𝑁𝐿𝑑).
Figure 6 illustrates the inference time of RE-
NET, RE-GCN, and GHT under the setting of Δ𝑡=
5on ICEWS14. We can see that RE-NET inference
is the slowest because it processes the query for
each timestamp separately and can only predict the
event that occurs at the next timestamp. RE-GCN
is also a heuristic model, which only predicts the
next timestamp step by step. In contrast, our model
can parallelly predict events that occur at multiple
different timestamps. Therefore, compared with
RE-NET and RE-GCN, the longer the evolution
Dataset # entity # relation # train # valid # test # timestamp Time granularity
ICEWS14 7128 230 63685 13823 13222 365 24 hours
ICEWS18 23033 256 373018 45995 49545 304 24 hours
ICEWS0515 10488 251 322958 69224 69147 4017 24 hours
GDELT 7691 240 1734399 238765 305241 2751 15 mins7492
process is, the less time our model inference takes
relatively.
A.4 Sensitivity Analysis
We also study the hyperparameter’s sensitivity of
GHT, including the layer number and the attention
head number of RGT, the layer number and the
attention head number of TT, and the history length.
We report the MRR results of entity prediction on
ICEWS14 (𝑑𝑡=1) in Figure 7.
Results in Table 1 demonstrate that GHT out-
performs all static KG reasoning models and TKG
interpolated reasoning models because these base-
lines fail to utilize the time information. For static
methods, it is effortless to understand that it has
no ability to model the time. The interpolated rea-
soning methods learn an embedding for each times-
tamp. However, in our experimental setting, the
timestamps of inference are not seen during train-
ing, causing the models to use randomly initialized
timestamp embeddings.
RGT Analysis The layer number of RGT corre-
sponds to the aggregated neighbor order. In Figure
7e, the model performs best when the layer number
is 2, aggregating more structural information than
1-layer. However, the model’s performance willdeteriorate as the number of layers increases. This
may be because, in ICEWS, neighbors above the
third order do not provide more information but
instead introduce noise. Figure 7c shows that the
number of attention heads has a small impact.
TT Analysis Figure 7b and 7d show the perfor-
mance of GHT with different number of TT layers
and attention heads. We notice that 2-layer TT and
2-head TT perform better than others. Compared
with the number of RGT layers, the model is less
sensitive to the number of TT’s layers.
History Length Analysis GHT needs to model
the historical sequence information, and we fix a
hyperparameter to limit the maximum sequence
length. Figure 7a shows the performance with dif-
ferent history length. If the length is no greater
than 6, the longer the history, the better the GHT
performance. However, continuing to extend the
sequence will make the MRR fall. It shows that the
effective information density is inversely propor-
tional to the history length.7493