
Hoyun Song Jisu Shin Huije Lee Jong C. Park
School of Computing
Korea Advanced Institute of Science and Technology
{hysong1991,jisu.shin,angiquer,jongpark}@kaist.ac.kr
Abstract
Social media is one of the most highly sought
resources for analyzing characteristics of the
language by its users. In particular, many re-
searchers utilized various linguistic features
of mental health problems from social media.
However, existing approaches to detecting men-
tal disorders face critical challenges, such as the
scarcity of high-quality data or the trade-off be-
tween addressing the complexity of models and
presenting interpretable results grounded in ex-
pert domain knowledge. To address these chal-
lenges, we design a simple but flexible model
that preserves domain-based interpretability.
We propose a novel approach that captures the
semantic meanings directly from the text and
compares them to symptom-related descrip-
tions. Experimental results demonstrate that
our model outperforms relevant baselines on
various mental disorder detection tasks. Our de-
tailed analysis shows that the proposed model
is effective at leveraging domain knowledge,
transferable to other mental disorders, and pro-
viding interpretable detection results.
1 Introduction
Mental health problems, a significant challenge
in public healthcare, are usually accompanied by
distinct symptoms, such as loss of interest or ap-
petite, depressed moods, or excessive anxiety. As
these symptoms can often be expressed over social
media, detecting mental health conditions using so-
cial media text has been studied extensively (Yates
et al., 2017; Coppersmith et al., 2018; Matero et al.,
2019; Murarka et al., 2021; Harrigian et al., 2021;
Jiang et al., 2021; Nguyen et al., 2022). Such ap-
proaches could give rise to a monitoring system that
provides clinical experts with information about
possible mental crises.
To automatically identify mental health prob-
lems, traditional approaches focus on finding lin-
guistic patterns and styles from the language ofpsychiatric patients. Utilizing these features, sta-
tistical models can explain the correlation between
linguistic factors and mental illnesses. However,
these approaches suffer from increased complexity
of models, necessitating pipelines of steps, from
engineering features to producing results. By con-
trast, more recent works have employed strong pre-
trained models, which allow a direct use of raw data
and simplify model development (Matero et al.,
2019; Jiang et al., 2020). While such end-to-end
approaches may be effective at achieving higher
performance, they often lack domain-based inter-
pretation, which is essential for decision-support
systems (Mullenbach et al., 2018). Hence, there is
a trade-off between providing interpretable predic-
tions based on domain knowledge and the simplic-
ity of the models.
The lack of a sufficient sample size for high-
quality data is another challenge in the clinical do-
main (De Choudhury et al., 2017; Harrigian et al.,
2020). Despite the availability of diverse datasets
and methods for detecting mental disorders, most
of them aim primarily at identifying only clini-
cal depression. To tackle such a problem, recent
studies have focused on developing transferable
linguistic features that can be used for the detection
of various mental disorders (Aich and Parde, 2022;
Uban et al., 2022). However, the linguistic features
that are trained on a particular dataset may not be
fully transferable to a different task (Ernala et al.,
2019; Harrigian et al., 2020).
Others utilized symptom-related features that
are more common properties of psychiatric pa-
tients, resulting in generalizability of depression
detection (Nguyen et al., 2022). Despite this im-
provement, however, their approach still faces chal-
lenges because they rely on pipelined methods
using manually-defined symptom patterns. Such
symptom patterns for depression detection lack
flexibility as they cannot be easily adapted to other
mental disorders. In addition, the pipeline approach12190with symptom extraction is quite complex to imple-
ment. It involves multiple steps, designing symp-
tom patterns, training a symptom identification
model, and detecting depression using the iden-
tified symptom patterns.
To address these challenges, we propose to de-
sign a simple and more flexible approach that also
preserves interpretability. We are motivated by the
process that humans use to quickly learn related
features, often by reading just a single explanation.
For example, when people are reading depression
questionnaires, they readily understand the ques-
tions and learn about symptoms that are related to
depression, allowing them to self-diagnose their
levels of depression.
To this end, we employ the siamese network
(Koch et al., 2015), which captures the semantic
meaning of the text inputs and compares them di-
rectly to symptom-related descriptions. This pro-
cess is simple since they find symptom-related
clues directly from the input, rather than relying on
hand-engineered features or intermediate models.
Our proposed model, Multi-Head Siamese network
(MHS), can be easily adapted to other mental ill-
ness domains by simply replacing the symptom-
related descriptions. In addition, our model is
designed to capture the distinct features of each
symptom using multiple heads. By examining the
learned weights of each symptom head, our model
gives rise to human-understandable interpretations.
We evaluate the performance of our model, de-
tecting texts containing mental health problems on
four mental disorders. Furthermore, the detailed
analysis of the proposed model shows its efficiency
in utilizing symptom-related knowledge, its ability
to be applied to different mental disorders, and its
interpretable reasoning for detected results.
2 Related Work
Social media are commonly used for mental health
research because of the ease of access to various
aspects of human behavior studies. Similarly to
other NLP domains, pre-trained language models,
such as BERT (Devlin et al., 2019), are widely used
for identifying mental health problems (Matero
et al., 2019; Jiang et al., 2020; Murarka et al., 2021;
Dinu and Moldovan, 2021).
Others have presented interpretable detection
methods for the mental health domain based on
linguistic features (Song et al., 2018; Uban et al.,
2021). Various efforts have also been made to studysuch linguistic features accompanying mental ill-
ness, such as differences in word usage (Tadesse
et al., 2019; Jiang et al., 2020; Dinu and Moldovan,
2021), or in syntactic features (Kayi et al., 2017;
Ireland and Iserman, 2018; Yang et al., 2020).
Some studies address the differences between sen-
timents or emotional aspects (Preo¸ tiuc-Pietro et al.,
2015; Kirinde Gamaarachchige and Inkpen, 2019;
Allen et al., 2019; Wang et al., 2021), or differ-
ences in topics (Tadesse et al., 2019; Kulkarni et al.,
2021).
The linguistic features are also used for transfer-
able methods across other mental disorders (Aich
and Parde, 2022; Uban et al., 2022), focusing on
the fact that a large number of studies have been
done primarily on depression (De Choudhury et al.,
2013; Yates et al., 2017; Eichstaedt et al., 2018;
Song et al., 2018; Tadesse et al., 2019; Yang et al.,
2020; Nguyen et al., 2022), compared to other
disorders, such as anxiety disorder (Ireland and
Iserman, 2018), anorexia (Uban et al., 2021), or
schizophrenia (Kayi et al., 2017). However, such
linguistic features do not generalize well to new
user groups. For example, De Choudhury et al.
(2017), Loveys et al. (2018), and Pendse et al.
(2019) found that the linguistic styles may vary
to their backgrounds. In addition, Harrigian et al.
(2020) found that a model trained on a particu-
lar dataset does not always generalize to others.
To handle such a generalization problem, Nguyen
et al. (2022) and Zhang et al. (2022) focused on
the shared and general properties (i.e., symptoms)
of a mental health problem. However, unlike ours,
which captures the symptom features directly from
raw data, these methods require additional steps for
learning symptom-related features.
In this paper, we use the siamese network (Koch
et al., 2015), based on one-shot learning, exploited
recently for simple networks (Chen and He, 2021;
Zhu et al., 2021). We utilize the symptom de-
scriptions sourced from DSM-5 (American Psychi-
atric Association, 2013) to make our model learn
symptom-related knowledge.
3 Methodology
In this section, we introduce our simple but flexi-
ble modeling for leveraging clinical questionnaires.
Our model aims to detect texts with mental illness
episodes based on the presence of symptom-related
features just by a single component.
An overview of our network is shown in Figure 1.12191
We designed our model based on the siamese net-
work (Koch et al., 2015). As with the original
siamese neural network, our model also contains
a single feature extractor with shared parameters.
The extractor directly obtains features from con-
textualized embeddings generated by sentence en-
coders. Then, employing the similarity function,
we compare the similarity to see the presence of
symptom-related features from the target text. In
addition, we apply multi-headed learning to the
original siamese network, repeating the compari-
son process for each distinct symptom. We describe
the detailed structure in the following subsections.
3.1 Model Structure
Our model, the Multi-Head Siamese network
(MHS), is an end-to-end model that takes raw input
texts and produces the final result without the need
for manual feature engineering. MHS is designed
to take two types of inputs, the target text to be
classified and descriptions of symptoms. The de-
scriptions are grouped for each symptom, and each
symptom group is the input for the corresponding
symptom head. For example, assuming that we
havensymptoms for discriminating against men-
tal disorder, we build a set of nheads ( H) from S
toSfor the detection model as follows:
H={S, S, ..., S} (1)Each head Srepresents discrete symptoms, con-
taining a number of descriptions and questions re-
garding the corresponding symptom. For example,
ifShasmsentences describing the symptom, we
have a set Sof questions:
S={s, s, ..., s} (2)
With a given input of the target sentence, our model
obtains embedding vectors ( E ) by employing
pre-trained sentence encoders, such as BERT or
RoBERTa. We also get symptom embeddings by
encoding all sentences from all heads ( H).
Our siamese network employs a multi-channel
convolutional neural network (CNN) for feature
learning. We apply three channels for convolution
layers, whose kernel sizes are 2, 3, and 5. Thus, our
model is designed to capture informative clues with
the window sizes of 2, 3, and 5 from texts. Each
channel contains two convolutional layers and two
max-pooling layers. The final convolutional layer
is flattened into a single embedding vector. As a
result, we obtain three feature embedding vectors
(F ) with k= 2,3,5from the target text:
F =Conv 1d(E) (3)
Through the same process, we also obtain feature
embedding vectors from symptom texts from the
ihead and jsentence as follows:
F=Conv 1d(E) (4)12192We compute the distances ( d) between the target
feature vector ( F ) and a symptom-sentence
vector ( F) using cosine similarity, ranging
from [−1,1]. We calculate a single distance value
by taking the average of Kdistance values, where
Krepresents the number of channels:
sim(x,y) =xy
∥x∥∥y∥(5)
d=1
K/summationdisplaysim(F , F)(6)
Finally, when there are distance values for all sen-
tences, they are averaged to yield the distance value
of the ihead ( d):
d=1
m/summationdisplayd (7)
To regularize the results, we choose to use aver-
aging as an aggregation function for the distance
values.
We iterate this process over the number of heads
(n). After the siamese network step, all distance
values ( d) are stacked into a 1×nvector ( D).
By applying the fully connected layer, the distance
vector is reduced into a two-dimensional vector o,
which is an output probability of classifying mental
illness:
f:R→R(8)
o=f(D) =W·D+b (9)
By analyzing the weights ( W) and distance values
(D) of the fully connected layer, we can examine
which symptoms are activated as important infor-
mation when classifying the related mental disorder.
Further details are discussed in Section 5.4. The
implementation code and symptom-sentences are
made publicly available.
3.2 Symptom Descriptions
In the present study, we focus on four mental dis-
orders: major depressive disorder (MDD), bipolar
disorder, generalized anxiety disorder (GAD), and
borderline personality disorder (BPD). As summa-
rized in Table 1, we compiled the diagnostic criteria
for each mental disorder, sourced from DSM-5. We
constructed heads based on the list of symptoms.
For example, in the case of MDD, there are a total
of 9 symptoms (D0-D8), so when constructing a
model detecting depressive symptoms, there will be
a total of 9 heads ( n(H) = 9 ). As for bipolar
disorder, symptoms can be divided into depressive
episodes (D0-D8) and manic episodes (M0-M7),
with a total of 17 heads. The depressive episodes
of bipolar disorder are the same as those of MDD.
Each head includes a description of diagnostic
criteria and questions from self-tests corresponding
to each symptom. As a result, each head contains
two or more sentences ( n(S)≥2). In the case
of more than two related questions for a symptom,
the corresponding head contains more than two
sentences.
We collected the questions from the publicly
available self-tests. The process was conducted
under the guidance of a psychology researcher. The
complete list of collected sentences for each head
is shown in Appendix C. Our model can easily12193
transfer to other mental disorders by just replacing
symptom descriptions, as evidenced by the findings
in Section 5.3.
4 Experiments
4.1 Dataset and Evaluation
In order to evaluate our model, we constructed four
datasets to detect possible mental disorder episodes.
We sampled posts from Reddit, which is one of the
largest online communities. Each sample is a con-
catenation of a title and a body from a post. Each
dataset contains two groups of Reddit posts. One
includes the posts collected from mental disorder-
related subreddits as a text containing the mental
illness contents, and the other is from random sub-
reddits as a clean text. The detailed statistics of
each group is shown in Table 2. We performed pre-
processing by discarding posts containing URLs
or individually identifiable information, and posts
shorter than ten words (i.e., tokens). We only re-
tained posts in English; otherwise, they are dis-
carded.
We conducted four tasks, employing these col-
lected datasets, discriminating texts sourced from
mental disorder-related subreddits out of non-
mental illness texts. The details of each task are as
follows: MDD detection ( r/depression +random),
Bipolar disorder detection ( r/bipolar +random),
GAD detection ( r/anxiety +random), and BPD de-
tection ( r/bpd +random).
To compare our model with baseline models with
respect to classification performance, we report
results using standard metrics, Accuracy (Acc.), F1
score (F1) for the mental illness group, and Area
Under the Curve (AUC). The performance measure
is reported by five-fold cross-validation, and each
repetition is trained on six different seeds. We
averaged after 30 runs (5×6) to get the final result.
4.2 Baselines and Experimental Setup
In this subsection, we describe models and imple-
mentation details for experiments. More experi-mental details are shown in Appendix A.
1) Traditional Models We implemented two
feature-based classifiers, a support vector ma-
chine (SVM) and a random forest (RF), with two
versions: BoW , employing lexical features only
(Tadesse et al., 2019; Jiang et al., 2020), and Fea-
ture, adding sentimental and syntactic features
(Allen et al., 2019; Yang et al., 2020; Wang et al.,
2021). 2) BERT (Devlin et al., 2019) is one of
the most well-known baseline models using con-
textualized embeddings (Jiang et al., 2020; Matero
et al., 2019). 3) XLNet (Yang et al., 2019) is an-
other strong baseline with a pre-trained language
model (Dinu and Moldovan, 2021). 4) RoBERTa
(Liu et al., 2019) is a robustly optimized BERT and
one of the most solid baselines in natural language
classification (Dinu and Moldovan, 2021; Murarka
et al., 2021). 5) GPT-2 (Radford et al., 2019) is a
strong few-shot learner with a large Transformer-
based language model. 6) PHQ9 (Nguyen et al.,
2022) is a depression detection model constrained
by the presence of PHQ9 symptoms.
We implemented our models using PyTorch and
fine-tuned our models on one 24GB Nvidia RTX-
3090 GPU, taking about 13 minutes for each epoch.
The batch size and embedding size of all models
are 8 and 512, respectively, and are fine-tuned over
five epochs. We truncated each post at 512 tokens
for all models. For each model, we manually fine-
tuned the learning rates, choosing one out of {1e-5,
2e-5, 1e-6, 2e-6} that shows the best F1 score. We
report the average results over 30 runs (five-fold
cross-validations are trained on six different seeds)
for the same pre-trained checkpoint.
4.3 Experimental Results
Table 3 shows the overall performance of our pro-
posed model (MHS) and strong baselines on four
tasks. Each task is about detecting texts with corre-
sponding mental illness episodes on social media.
We see that our model outperforms all competing
approaches, including linguistic feature-based mod-
els, end-to-end pre-trained models, and a method
that uses symptom-related knowledge.
Linguistic feature-based models exhibit signifi-
cant performance variations based on the level of
detail in their feature design. By contrast, MHS can
simply find the features directly from the contex-
tualized representation, giving better performance
improvements. Pre-trained models with contextu-
alized embeddings have the benefits that can be12194
easily fine-tuned for a wide range of tasks. How-
ever, compared to MHS, they lack a specific focus
on domain-based features, while MHS is tailored
to identify such features, leading to better perfor-
mance.
We implemented our model and PHQ9 model
with two different encoders, BERT and RoBERTa,
and the tendency for performance improvement is
the same on both encoders. Both PHQ9 and MHS
leverage symptom-related information but differ in
their architecture, specifically whether it is a multi-
step pipeline or an end-to-end model. The end-to-
end design of MHS allows for direct learning of
complex relationships, reducing the potential for
error propagation, and resulting in enhanced perfor-
mance compared to the pipeline model. Moreover,
for this pipeline model to apply to other mental dis-
orders, a symptom pattern must be created for each
mental disorder, which is challenging to achieve
without expert-level knowledge. On the other hand,
our proposed model overcomes these challenges by
simply replacing symptom descriptions. A detailed
analysis of the performance improvement is shown
in Section 5.
4.4 Model Parameters
Table 4 shows the number of parameters for each
model. Compared to the baseline models, the ad-
ditional number of parameters for our siamese net-
work is about 655K. It is a much smaller number
than that of the additional parameters for RoBERTa
and BERT (about 16M), but the performance of
MHS (w/bert) is slightly better or shows little dif-
ference. It suggests that our proposed model, learn-
ing domain knowledge, achieves much efficient
performance improvement by adding just a small
number of parameters.
5 Model Analysis and Discussions
5.1 Ablation Study
We conducted an ablation study to investigate the
effectiveness of each part in our proposed model.
We removed the siamese network from our pro-
posed methods, resulting in just convolutional neu-
ral networks (CNNs). We implemented a single-
head siamese network in which all sentences from
all heads are put together into just one head. We
also implemented two versions of a multi-head
siamese network employing just one description or
multiple descriptions, respectively.
The experimental results are shown in Table 5.
The result shows that our proposed model gives
the best performance when all of the modules are
combined. Compared to CNN models, the per-
formances are improved when the siamese net-
work is added. Note that the siamese network con-
tributes to accurate detection, since it captures the12195
symptom-related features by comparing target texts
with symptom descriptions. In addition, the per-
formances are also improved when employing a
multi-head rather than a single-head. It implies
that individually training each symptom yields bet-
ter results than training all symptoms together, as
each symptom has unique features. Compared to
learning from only one description per head, the
performance of learning from multiple descriptions
is improved. It may be due to each head learning
further about the symptom through various sen-
tences, covering distinct aspects of each symptom.
5.2 Contribution of Symptom Descriptions
To assess the effectiveness of symptom descrip-
tions in detecting the presence of symptoms, we
measure their performance by replacing the de-
scriptions of symptoms with those of other mental
disorders. The results are shown in Table 6. We car-
ried out four mental disorder detection tasks using
four models, each utilizing symptom descriptions
of four distinct mental disorders as inputs.
The models exhibit optimal performance when
the input symptom description corresponds to the
target mental disorder. It suggests that, by pro-
viding the model with accurate and appropriate
symptom descriptions, MHS can learn effectively
to identify the specific features associated with a
particular mental disorder. This also implies that
MHS can identify and utilize the nuanced distinc-
tions in the characteristics of each symptom, lead-
ing to enhanced performance in detection.
5.3 Cross-domain Test
In order to investigate the flexibility of MHS, we
evaluated its performance across datasets and other
mental disorders.
Dataset Transferability Given that the ability to
generalize to new and unseen data platforms is
a crucial aspect of mental illness detection mod-
els (Harrigian et al., 2020), we evaluate their
performance across different datasets. We se-
lected two datasets, RSDD (Yates et al., 2017) and
eRisk2018 (Losada et al., 2019), to evaluate cross-
dataset transfer. Unlike our Reddit dataset (Sub-
section 4.1), sourced from communities specific to
certain mental illnesses, RSDD and eRisk2018 data
are based on user self-reports, resulting in data that
is different from and potentially unseen by the Red-
dit dataset. We trained each model using the Reddit
train dataset and evaluated its performance on the
test sets of RSDD and eRisk2018, respectively.
As shown in Table 7, MHS outperforms all
strong baselines over all datasets. The improved
performance of MHS compared to GPT-2, a strong
few-shot learner, is likely due to its ability to lever-
age domain-specific knowledge. The higher gen-
eralizability of MHS compared to PHQ9 is likely
attributed to its end-to-end architecture, which al-
lows for direct learning of symptom features from
data, as opposed to PHQ9’s reliance on pre-defined
symptom patterns.
Domain Transferability As suggested by some re-
searchers (Aich and Parde, 2022; Uban et al., 2022),
we evaluated the transferability of MHS across
other mental disorders by training the models on
a depression dataset and testing on other mental
disorder datasets (see Table 8). The results of the
experiments indicate that MHS significantly out-
performs all relevant baselines, particularly when
it utilizes symptoms that match the target mental
disorder. This suggests that the transferability of12196
the model can be significantly enhanced by simply
replacing symptom descriptions. This also implies
that it may be feasible to develop a model that
can classify texts related to various other mental
disorders if the symptoms of those disorders are
provided appropriately.
5.4 Interpretation
Using our model, we can interpret the detected re-
sults by analyzing their representations of learned
weights and distance values. In order to see if our
model properly learned symptom-related knowl-
edge from a few descriptions and identified sim-
ilar stories from the target texts, we looked into
the learned weights produced by the last step of
our model, the fully connected layer. To show
the effectiveness of MHS, we visualize the exam-
ples of learned weights from training steps in Fig-
ure 2. The color scale represents the strength of the
learned weights (i.e., the distance values of each
head). Each row represents heads, indicating each
symptom referring to Table 1, and each column rep-
resents the labels. We observe a clearly contrasting
pattern in the distance weights for each task.
We could also identify which symptoms are
mainly activated or not by investigating the learned
weights during the training process. For exam-
ple, in detecting MDD-related texts, most of the
symptoms have higher weights than depression. It
suggests that most of the symptoms give rise to a
major role during the detection process.
An important criterion in diagnosing a mental
illness by experts is the number of expressed symp-
toms. The number of symptoms must exceed a
certain number to be diagnosed as a corresponding
mental illness. In order to see if the human-level
diagnostic process works in our model as well, we
looked into the number of salient symptoms in
true-positive samples. We calculated percentiles
from the similarity scores for each symptom in the
true-positive samples from test sets, and set the
threshold by 70% of the percentile. Then, when
exceeding the threshold set by the criterion, the
symptom was selected as a prominent feature in
the text. We present the distribution of the numbers
of salient symptoms and their averaged probabili-
ties of the final output from test sets of detecting
MDD-related texts in Figure 3.
In our model, the average probability is relatively
low when there are fewer than three symptoms, but
for three symptoms or more, our model makes a
decision with high confidence at a similar level.
It suggests that MHS also detects mental disorder-
related texts with high confidence when the number
of symptoms exceeds a specific number, the same
as when humans diagnose. The criterion number
being smaller in MHS may be due to the shorter
length of social media texts, which may not fully
convey the user’s background and lifestyle.
5.5 Case Study
For the case study, we made an example based on
the samples corresponding to each mental disorder
in the psychology major textbook. We present ex-
ample sentences for MDD and GAD (Table 9), and
the model’s predictions were correct in both cases.
We set the same threshold as shown in Figure 3.
The dominant symptoms predicted by the model
are D0 ( depressed mood ), D1 ( diminished interest ),
and D8 ( suicidal ideation ), for MDD, and A1 ( dif-
ficult to control the worry ), A2 ( irritability ), and
A3 (easily fatigued ), for GAD. In the case of D012197
and D1 in MDD, our model captures the feature
related to the symptom, despite the absence of the
term ‘ depress ’ or ‘ interest ’. These cases support
the assumption that our model can detect and inter-
pret when symptoms of a particular mental illness
are prominent in text.
6 Conclusion
In this paper, we proposed a simple but flexible
model for detecting texts containing contents of
mental health problems. Our model outperformed
the state-of-the-art models and achieved human-
interpretable results over symptoms regarding men-
tal disorders. The proposed model demonstrates
an exceptional ability to utilize domain knowledge
as it is designed to capture relevant features from
texts directly. Experimental results also indicate
that MHS can quickly adapt to other mental dis-
order domains by simply replacing symptom de-
scriptions. The scope of this paper was limited to
the investigation of four mental disorder detection
tasks. Nevertheless, this approach can be extended
to other mental health conditions as long as the
symptom-relevant questionnaires are provided ac-
cordingly.
Limitations
It should be noted that, as our model and the base-
line models in this study were trained using texts
from social media and the experiments were con-
ducted on online text, the results may not accu-
rately reflect the performance in a clinical setting.
A proper diagnosis by clinical experts necessitates
a comprehensive analysis of various factors, includ-
ing the number of manifested symptoms, the on-
set and history of symptoms, developmental back-
ground, lifestyle, and recent life changes, in order
to gain a comprehensive understanding of the pa-
tient’s condition. However, it is still challenging
to capture detailed information such as personal
secrets through online text, as these texts are of-
ten composed of fragments of daily life, episodic
experiences, and emotive expressions rather thanproviding a comprehensive view of an individual’s
life. Despite the domain-specific limitations im-
posed by the fragmentary text, we hope that our
model may still serve as a valuable aid for clinical
experts in their decision-making process. Further-
more, future research should aim to move beyond
predicting psychological symptoms and disorders
solely based on linguistic styles and expressions,
and instead seek to uncover the underlying features
that contribute to these expressions as our model
does.
Ethics Statement
Since privacy concerns and the risk to the individu-
als should always be considered, especially using
social media data, we have employed mechanisms
to avoid any harmful and negative consequences
of releasing our model. To this end, we removed
individually identifiable information such as user
names, user IDs, or e-mail addresses. We also re-
moved any URLs from our data not to be trained
on such personal information in our model. As for
the use of open datasets in this work, we used them
in accordance with guidelines that allow their use
within the established usage policy. Especially we
ensure that no attempts can be made to establish
contact with specific individuals or deanonymize
users in the datasets.
Our paper may contain direct references to spe-
cific disorders or diseases (such as psychiatric pa-
tients, Siamese, or names of mental disorders) and
expressions that could be considered offensive to
particular individuals. We want to emphasize that
these expressions are used solely for the purpose
of academic discourse and are not intended to be
disrespectful or offend anyone.
In addition, our proposed model is not intended
to label or stigmatize individuals online but rather
to serve as a warning system for potential threats
to personal well-being and public health. It is im-
portant to note that even if this model identifies
potential mental illnesses and symptoms, it should
not be considered a definitive diagnosis. Still, the12198model provides an indication of the likelihood of a
disorder; it should be used as a reference for self-
diagnose and in consultation with a mental health
expert for an official diagnosis. An official diag-
nosis and results require consultation with medical
and psychological experts, and this system aims
at serving as an aid in the diagnostic process. We
make our implementation code publicly available
for research purposes, and we hope it will be used
to improve the lives of individuals suffering from
mental illnesses.
Acknowledgements
This work was supported by the National Re-
search Foundation of Korea (NRF) (No. RS-2023-
00208054, A multi-modal abusive language detec-
tion system and automatic feedback with correc-
tion) grant funded by the Korean government.
References121991220012201A Experimental Setups
We implemented two feature-based models, sup-
port vector machine (SVM) and random forest (RF).
We fine-tuned SVM with Gaussian kernel and set
Cto 100, and RF set max depth to 100. We em-
ployed BERT’s vocabulary to train BoW models.
ForFeature models, we used a pre-trained senti-
ment classification model, and a Part-of-Speech
Tagging model from the Huggingface library (Wolf
et al., 2019). We fine-tuned the transformer base-
line models employing the default settings from
the Huggingface library: BERT (bert-base-cased ),
XLNet (xlnet-base-cased ),RoBERTa (roberta-
base),GPT-2 (gpt2). For all experiments, we set
the batch size as 8 and fine-tuned all models on
a single 24GB GeForce RTX 3090 GPU. For the
implementation of the PHQ9 model, we follow
the structure of the questionnaire-depression pair
models by using the publicly available code from
PHQ9(Nguyen et al., 2022). We utilized the
symptom patterns which are provided by Nguyen
et al. (2022). We trained each of the models using
all six randomly selected seeds, and all the models
were trained for 3 epochs. We optimize the model
parameters of all models with the Adam optimizer
(Kingma and Ba, 2014). The learning rates for
BERT, XLNet, and RoBERTa models were manu-
ally fine-tuned, choosing one out of {1e-05, 2e-05,
1e-06, 2e-06} that shows the best F1 score. The
learning rate for GPT-2 was selected from {1e-05,
2e-05}, and for PHQ9, the learning rate was set to
1e-03, which was provided as an optimized hyper-
parameter.
B Comparison with Large Language
Model
Recent developments in large language models
(LLMs), such as GPT-3 (Brown et al., 2020), have
demonstrated strong zero-shot performance across
various NLP tasks. LLMs have the ability to
achieve high performance without fine-tuning for
downstream tasks, even with only zero or few ex-
amples, due to their large number of pre-trained
parameters.
We experimented with obtaining results for the
examples referred to in Table 9 by using GPT-3, a
widely recognized LLM. To this end, we utilized
instructional prompts by listing symptom descrip-
tions for a specific mental illness. The examplesof prompt input and the result are shown in Ta-
ble 10. The experimental results show that the
model successfully outputs the classification results
in a sentence when given instructional prompts for
a specific mental illness. However, the process of
selecting symptoms appears to focus on identify-
ing multiple symptoms rather than pinpointing a
specific symptom with precision.
These examples are presented for demonstration
purposes only, and the results may vary depending
on the utilization of different prompt optimizations
(Liu et al., 2021; Qin and Eisner, 2021). This as-
pect of research is beyond the scope of our current
study; thus, there is room for further research to be
conducted in future work.
C Details of Symptom Descriptions
In this section, we present the symptom descrip-
tions that were utilized in our current study. Ta-
ble 11 shows the complete list of symptom descrip-
tions. We used Diagnostic and Statistical Man-
ual of Mental Disorders (DSM-5) (American Psy-
chiatric Association, 2013) as a reference for the
symptom descriptions, as it provides comprehen-
sive guidelines for identifying symptoms of vari-
ous mental disorders. We also incorporated pub-
licly available clinical questionnaires from online
sources. Subsequently, under the guidance of a
psychology researcher, we conducted a mapping
process of the questions in the self-test to the cor-
responding diagnostic criteria, as depicted in Fig-
ure 4.122021220312204ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Yes, in the "Limitation" section
/squareA2. Did you discuss any potential risks of your work?
Yes, in the "Ethics statement" section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Yes, the paper’s main claims are provided in the 1. Introduction section.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Yes, in 3. Methodology.
/squareB1. Did you cite the creators of artifacts you used?
Yes, in 2. Related work
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No, the codes will be publicly available after the reviewing process.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Yes, we discuss about the possible problems in the "Ethics statement" section.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Yes, it is also discussed in "Ethics statement"
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Yes, in section 4.1 datasets
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Yes, in section 4.1 datasets
C/squareDid you run computational experiments?
Yes, in Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Yes, in section 4, and Appendix12205/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Yes, in section 4, and Appendix
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Yes, in section 4 and 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Yes, in section 4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.12206