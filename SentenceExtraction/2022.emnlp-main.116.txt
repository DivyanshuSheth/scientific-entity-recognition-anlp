
Hongxin Zhang, Yanzhe Zhang, Ruiyi Zhang, Diyi YangShanghai Jiao Tong University,Georgia Institute of TechnologyAdobe Research,Stanford Universityicefox@sjtu.edu.cn,z_yanzhe@gatech.eduruizhang@adobe.com,diyiy@cs.stanford.edu
Abstract
Demonstration-based learning has shown great
potential in stimulating pretrained language
models’ ability under limited data scenario.
Simply augmenting the input with some demon-
strations can significantly improve performance
on few-shot NER. However, why such demon-
strations are beneficial remains unclear since
there is no explicit alignment between the
demonstrations and the predictions. In this pa-
per, we design pathological demonstrations by
gradually removing intuitively useful informa-
tion from the standard ones to take a deep dive
of the robustness of demonstration-based se-
quence labeling and show that (1) demonstra-
tions composed of random tokens still make the
model a better few-shot learner; (2) the length
of random demonstrations and the relevance of
random tokens are the main factors affecting
the performance; (3) demonstrations increase
the confidence of model predictions on cap-
tured superficial patterns. We have publicly
released our code at https://github.com/
SALT-NLP/RobustDemo .
1 Introduction
Current large pretrained language models (PLMs)
struggle to learn NLP tasks under limited data sce-
narios (Devlin et al., 2019; Liu et al., 2019; Lewis
et al., 2020; Xie et al., 2020; Huang et al., 2021). In
contrast, humans can solve natural language tasks
with only a few illustrative examples(Lake et al.,
2015). Motivated by this, demonstration-based
learning has been introduced to augment the in-
put with a few examples and labels. For instance,
Brown et al. (2020) simply picked up to 32 ran-
domly sampled instances and directly concatenated
them with the input to perform in-context learning
with the model frozen and significantly boosted the
performance. Lee et al. (2022) concatenated the
input with task demonstrations to create augmented
input and fed them into PLMs to obtain improved
token representations to do sequence labeling in aFigure 1: Performance for different demonstrations
on CoNLL03 5-shot support set. Here Xdenotes the
input sentence, such as "Jobs was born in America.",
and we show the PER part of the whole demonstration
in applegreen for visualization. Surprisingly, random
tokens can be good demonstrations too.
classifier-based fine-tuning way.
However, how and why such demonstrations
help remains unclear. As such, there has been a
growing amount of work investigating the robust-
ness and interpretability of demonstration-based
learning. For instance, Lu et al. (2021) reported
that few-shot text classification is very sensitive to
the ordering of the demonstrations in in-context
learning. On a wide range of low-resource Nat-
ural Language Understanding (NLU) tasks, Min
et al. (2022b) investigated why demonstrations in
in-context learning can bring performance gains
over zero-shot inference and found that correct
input-label mapping matters very little.
Building on these prior works, we take a deeper
dive into the robustness of demonstration-based
learning (Lee et al., 2022), especially for struc-
tured prediction tasks like Named Entity Recogni-
tion (NER). Demonstrations might not be robust
for more structured prediction settings since these
limited amounts of examples might not include
much inductive bias. Also, using classifier-based
fine-tuning demonstrations could be even more un-
reliable since there is no alignment between the
demonstrations and the prediction space.
Concretely, we investigate the robustness of
demonstration-based sequence labeling by design-1769ing pathological demonstrations: gradually ruling
out the helpful information from the demonstra-
tions. We surprisingly find that a working demon-
stration does not need to contain correct examples
to observe improved performance. Furthermore,
randomly replacing every token in the demonstra-
tion can still make a better few-shot learner even
it is no longer a meaningful sentence and does not
make any sense (Figure 1). This observation con-
flicts with some existing hypotheses (Gao et al.,
2021; Lee et al., 2022) that models are learning
meaningful knowledge from these demonstrations.
We also find that the length of the pathological
demonstration and the relevance of its random to-
kens drastically affect the performance. Empiri-
cal results on Name Regularity Bias (NRB) diag-
nose dataset (Ghaddar et al., 2021) shows that the
demonstrations rarely help the performance when
there is no easy patterns. Additionally, we show the
pathological demonstrations can obtain similar or
better performance on NLU tasks such as classifi-
cation and natural language inference. In summary,
our empirical results encourage the rethinking on
how the demonstration helps the model obtain bet-
ter few-shot capability and provides some insights.
2 Related Work
2.1 Demonstration-based Learning
Demonstrations are first introduced by the GPT
series (Radford et al., 2019; Brown et al.,
2020), where a few examples are sampled from
training data and transformed with templates
into appropriately-filled prompts. The existing
demonstration-based learning research can be
broadly divided into three categories based on the
task reformulation and whether there is an update
on the model parameters:
In-context Learning reformulates the task as a
language modeling problem, and the model makes
predictions by filling in the blank without us-
ing classifiers. In in-context learning, the model
learns by only conditioning on the demonstration
in a tuning-free way, while an enormous language
model is often needed for this method (Brown et al.,
2020; Zhao et al., 2021; Min et al., 2022a; Wei
et al., 2022).
Prompt-based Fine-tuning also reformulates
the task as a masked language modeling prob-
lem, and the demonstrations are incorporated asadditional contexts (Gao et al., 2021). The model
learns by fine-tuning on a small set of training data
and moderately-sized PLMs are often used. Vir-
tual demonstrations such as trainable embeddings
(Liang et al., 2022) belong to this setting.
Classifier-based Fine-tuning requires no refor-
mulation of the task and simply augments the orig-
inal input with demonstrations (Lee et al., 2022).
One advantage of this method is that it can benefit
tasks such as sequence labeling when it is hard to
be reformulated as (masked) language modeling.
Much work has been conducted to examine
how to make a good sample selection (Liu et al.,
2021a; Mishra et al., 2021) and ordering (Lu et al.,
2021) as informative demonstrations are crucial for
model performance. Our work focuses on the third
demonstration-based learning method—classifier-
based fine-tuning under the traditional token classi-
fication framework on sequence labeling tasks.
2.2 Analyses of Prompts and Demonstrations
With the recent prevalence of using prompts and
demonstrations to stimulate the ability of PLMs
under limited data scenarios (Schick and Schütze,
2021a,b; Liu et al., 2021b), a growing amount of
works look at how prompting and demonstrations
work. For instance, Webson and Pavlick (2021)
studied different prompt templates and target words
on NLI tasks mainly with the prompt-based fine-
tuning method and found evidence that prompt-
based models still perform well when irrelevant or
even misleading prompts are used. Similarly, Min
et al. (2022b) showed that in-context learning is
not taking advantage of correct label mappings but
more surface form in the demonstrations, like the
distribution of the input text and the overall for-
mat of the sequence, while later work (Kim et al.,
2022) argued the impact of correct mapping de-
pends on different configurations. Logan IV et al.
(2021) demonstrated fine-tuning language models
on a few data can considerably reduce the need
for prompt engineering, while Utama et al. (2021)
showed that prompt-based fine-tuning improves
the in-distribution performance while gradually
increases models’ reliance on surface heuristics.
Garg et al. (2022) trained transformers to directly
learn function classes provided in demonstrations
and showed such learning is possible even under
distribution shifts.
Different from them, we focus on the analysis of
demonstration-based learning under the classifier-1770
based fine-tuning framework on sequence labeling
tasks where we do not reformulate the task into
(masked) language model problems and therefore
rule out the impact of target word selection. Fur-
thermore, we create more effective and adversarial
demonstrations consisting of only random tokens,
showing that it is not only the in-context learn-
ing or prompt-based fine-tuning but also the tradi-
tional ways of utilizing PLMs that may trigger this
counter-intuitive performance gain.
3 Problem Definition
This section focuses on demonstration-based learn-
ing under limited data scenario, by introducing
concepts of limited data sequence labeling tasks
in Section 3.1, as well as describing traditional
token classification methods in Section 3.2 and
demonstration-based learning in Section 3.3.
3.1 Limited Data Sequence Labeling Tasks
Given an input sentence x= [x, x,···, x]com-
posed of ntokens, the sequence labeling task is to
predict a tag y∈Y∪ {O}for each token x,
where Yis a predefined set of tags such as {LOC,
PER, ...} for Named Entity Recognition (NER) and
{NP, VP, ...} for chunking, and Odenotes outsidea tagged span. Under limited data scenario, we
only have K-shot support set D for training
which contains Kexamples for each tag type.
3.2 Traditional Token Classification Methods
As shown in Figure 2(a), traditional methods for se-
quence labeling use the encoders from PLMs such
as BERT to encode the input x= [x, x,···, x]
to get contextualized representations for each token
h= [h, h,···, h], and then use a linear classi-
fier or CRF layer to get the label estimation lfor
each token. The model is trained to minimize the
cross entropy loss between landy.
3.3 Demonstration-based Learning
Constructing Task Demonstration As shown
in Figure 2(b), for each tag type t, we sample
one tag example e, along with its original con-
textsfrom support set D . We use tem-
plate Tto convert them into type demonstration
d=T(s, e, t), and then construct task
demonstration ˜xby concatenating all type demon-
strations together:
˜x=d⊕d⊕ ··· ⊕ d
Here⊕denotes the concatenation of input se-
quences. We further concatenate the original input1771xwith demonstration ˜xto obtain demonstration-
augmented input [x;˜x]. Prior work (Lee et al.,
2022) have studied various example sampling
strategies and templates to construct the demonstra-
tion. We adopt their popular strategy to choose
ethat occurs most frequently among the corre-
sponding examples and context template of " s.
eist.", given their strong performances in Lee
et al. (2022). Here, we refer the " eist." part in
the template as labeling part of the demonstration.
Learning with Demonstration As shown in Fig-
ure 2(c), like traditional token classification meth-
ods, we feed the demonstration-augmented input
[x;˜x]into the encoder, and get the token represen-
tation [h;˜h]. We then feed hinto the classification
layer to get the label estimation lfor each token in
original input and train the model to minimize the
cross entropy loss between landy. Note that we
use identical demonstrations during training and
testing, which is crucial for demonstration-based
learning to work (Lee et al., 2022).
4 Pathological Demonstrations
We refer to the demonstration constructed in Sec-
tion 3.3 as STandard ( ST) demonstration. Suppose
the model leverages the demonstrations in a human-
analogous way and understands the meaning of
them, there will be no more performance gains if
we no longer provide correct example-label pairs
or actual examples. To this end, we design three
pathological demonstrations by gradually remov-
ing such intuitively helpful information from ST
demonstrations:
1.SW(Standard demonstration with Wrong la-
bels): Intuitively, the most helpful information
in demonstrations is the correlation between
provided examples and tag types, thus the first
kind of pathological demonstrations provides
wrong examples for each tag type on purpose.
2.SN(Standard demonstration with No label):
Furthermore, the existence of examples or
tags in labeling part of the demonstration
might give away hints, so we remove the la-
beling part to create the second pathological
demonstration that consists of only contexts
from the support set.
3.TR(Totally Random demonstration): Finally,
we test a seemingly useless demonstration by
using random token strings as demonstrations.
Specifically, we replace every token in the
demonstration SNwith random tokens sam-
pled from the vocabulary.
We show templates and examples for these
pathological demonstrations modified from stan-
dard demonstration for NER in Table 1.
5 Experiments
5.1 Few-Shot Datasets
Datasets We conduct experiments on two se-
quence labeling tasks: named entity recognition
and chunking. For NER task, we use dataset
CoNLL03 (Tjong Kim Sang and De Meulder,
2003), and OntoNotes 5.0 (Weischedel et al.,
2013). Since we primarily focus on named en-
tities, we omit the 7 value types in OntoNotes
following Ma et al. (2021). In addition, we use
CoNLL00 (Tjong Kim Sang and Buchholz, 2000)
for the chunking task. Since the number of some
phrase types is very limited, we only consider 6
most frequent types (which are NP , VP , PP , ADVP ,
SBAR andADJP , accounting for 99% of the labeled
chunks).
Few-shot data sampling Different from
sentence-level few-shot tasks, in sequence labeling,
one sample for a class refers to a span in the
sentence, and one sentence may contain multiple
samples of different types. We follow the greedy
sampling strategy proposed by Yang and Katiyar
(2020) to sample Kshots for each type in an
increasing order with respect to their frequencies,
the detailed algorithm can be found at Appendix C.
The detailed dataset statistics are shown in Table 2.1772Dataset |Y|L|D| |D|
CoNLL03 4 18 8.0 3453
OntoNotes 5.0 11 21 26.6 12217
CoNLL00 6 36 8.6 2012
5.2 Implementation Details
We use bert-base-cased model from Hugging-
Face (Wolf et al., 2020) as our backbone for all
the experiments and set the batch size and learning
rate to 4 and 2e-5, respectively, following Lee et al.
(2022). We use NVIDIA GeForce RTX 3080 Ti
to conduct all experiments. For each variant, we
run 50 epochs over 5 different sub-samples and 3
random seeds with early-stopping 20 and report its
micro-F1 scores along with its recall and precision.
5.3 Results
We show the detailed results for demonstration-
based learning with standard demonstrations and
pathological demonstrations in Section 4, as well
as traditional token classification methods with no
demonstration in Table 3.
Demonstration is effective! Comparing results
between no demonstration method ( NO) and
demonstration-based learning ( ST), we found
that demonstrations improve the few-shot perfor-
mance significantly (e.g., from 28.71 to 46.25 on
CoNLL03, and from 63.17 to 70.55 on CoNLL00).
A closer look at reveals that the performance gains
are mainly from a much higher recall for NER task,
indicating demonstrations are mainly helping rec-
ognize more entities.
No need for labels? As shown in Table 3, there
is no significant difference between the F1 scores
of standard demonstration ( ST) and its patholog-
ical variation SW. This suggests that a working
demonstration even does not need to have the cor-
rect labels. Moreover, even if we remove the entire
labeling part (often perceived as the most important
factor for demonstrations), the pathological demon-
stration SNcan still achieve as impressive results
asSTdemonstration. Our results are consistent
with a recent work (Min et al., 2022b) that cor-
rect label mapping may not be needed for demon-
strations to work, though we approach the robust-
ness of demonstration based learning in a different
classifier-based fine-tuning setting.
Random demonstration also works. Surpris-
ingly, there is significant performance gain when
using demonstration TRover NO(e.g., from 28.71
to 41.33 on CoNLL03, and from 63.17 to 69.28 on
CoNLL00), though the gap between TRandST
demonstration still exists. Note that TRdemon-
stration is no longer a real sentence and may not
provide any meaningful or useful information.
5.4 Analysis
This section provides a deeper understanding of
why and how demonstration-based learning works
given the counter-intuitive results in Section 5.3.
Relevance of the tokens counts! Looking at the
performance difference between demonstration SN
andTR, we hypothesize that a key factor might
be the random tokens’ relevance to the input sen-
tence. To test this, we construct a demonstration
SR(Support set sampled Random demonstration)
as shown in Table 1, by first creating a relevant
vocabulary consisting of only tokens appear in
the support set D , and then sampling tokens
from this relevant vocabulary to replace tokens in
demonstration. The result is shown in the last row
of Table 3. We found that the performance of SRis
superior to TR(45.99 v.s. 41.33 on CoNLL03) and
comparable to STdemonstration (45.99 v.s. 46.25
on CoNLL03). This implies that the relevance of
tokens of the demonstration is very essential to
demonstration-based learning.
Length of demonstrations matters. Since there
is not much semantic meaning included in the
demonstration SR, another crucial difference be-
tween SRandNOis the length of random demon-1773
strations. Thus, we conducted ablation study by
varying the length of demonstration SR. We evalu-
ated the performance with demonstration consist-
ing of α%tokens of original SRdemonstration. As
shown in Figure 3, the performance of demonstra-
tion improves from 28.71 to 45.99 on CoNLL03
and from 37.37 to 41.60 on OntoNotes 5.0 with
the longer length of αfrom 0 to 100; it saturates
(achieving 98% of original SRdemonstration’s per-
formance) at a relatively short length of 50% of the
original length. This suggests that a fair number
of tokens is needed for demonstration SRto be
working, and it seems the length of demonstrations
matters much more than their content. Our find-
ing here is consistent with the finding in Xie et al.
(2022).
The magic vanishes with more data. We fur-
ther examine whether the performance gain of
demonstration-based learning changes over differ-
ent level of data scarcity, namely K-shots support
set. We show results for the aforementioned (no)
demonstrations under K= 5,10,20shots and full
data in Figure 4. The F1 score gain from demon-
stration is 17.54 (from 28.71 to 46.25) for 5-shot
support set, 6.2 in the 10-shot setting, and neg-
ligible for the 20-shots support set and full data.
Consistent with Lee et al. (2022); Gao et al. (2021),
the performance gain (no matter standard or patho-
logical) vanishes with more data. This indicates
demonstrations have a strong boost on performance
especially in extremely limited scenario, where
there is no enough data for the model to fit well.
Similar findings on Roberta. To see whether
this counter-intuitive finding holds on other PLMs
as well, we experimented on Roberta with
roberta-base model from HuggingFace and show
the results in Tabel 4. Similar to the results on
BERT, standard demonstration improves the perfor-
mance by 1.87 F1-score while pathological demon-
strations with no intuitively meaningful informa-
tion work as well. It implies that the cause behind
this counter-intuitive finding is not only specific to
BERT, but may aslo be prevalent with other PLMs.17746 Understanding the Demonstrations
We take a closer look at the surprising perfor-
mance of (pathological) demonstrations to examine
whether such strong performance has any connec-
tions with spurious patterns or dataset bias, which
the deep learning models are constantly being ac-
cused of leveraging (Wang et al., 2021). As a case
study, we use a carefully designed testbed NRB
(Ghaddar et al., 2021) to diagnose Name Regularity
Bias in the NER models learned with demonstra-
tions (Section 6.1). We also conduct experiments
on the more popular way of utilizing demonstra-
tions with prompts in Section 6.2 to show the effec-
tiveness of pathological demonstrations.
6.1 Name Regularity Bias
Name Regularity Bias (Ghaddar et al., 2021; Lin
et al., 2020) in NER occurs when a model relies on
a signal from the entity name to make predictions
and disregards evidence from the local context. For
example, given the input sentence " Obama is lo-
cated in far southwestern Fukui Prefecture. ", mod-
ern NER models may wrongly predict the entity
Obama asPER, while the context clearly signals
it is a LOC . Therefore, Ghaddar et al. (2021) care-
fully designed a testbed utilizing the Wikipedia
disambiguation page to diagnose Name Regularity
Bias of NER models. The NRB dataset contains
examples whose labels can be easily inferred from
the local context but hard to be tagged by a popular
NER system. The WTS dataset is a domain con-
trol set that contains the same query terms covered
by NRB, but can be correctly labeled by both the
popular NER tagger and local context-only tagger.
6.1.1 Results
We use both the NRB and WTS datasets to evaluate
the model trained with different modes of demon-
strations on CoNLL03 under 5,10,20-shots support
set and full data. The result is shown in Figure 5.
As we can see, demonstration-based learning on
the control set WTS consistently brings impressive
performance gains under all low-resource settings
(e.g. from 27.00 to 68.07 with 5-shots support
set, and from 69.76 to 82.57 with 20-shots support
set). On challenging dataset NRB, it only shows
a performance gain from 15.82 to 29.44 with 5-
shots support set, with no performance gain with
10-shots support set and even decreased F1 scores
with the 20-shots support set. This suggests that
demonstration-based learning leverages the name
regularity bias to recognize entities rather than the
context information.
6.1.2 Analysis
Demonstrations bring no robust improvements
To have a better understanding on how demon-
strations affect the performance, we show the de-
tailed prediction flips for all entities after adding
STdemonstrations to NOin Figure 6, where each
cell shows the number of predictions that flip from
the original prediction (row) to the new prediction
with STdemonstrations (column), while the di-
agonal represents the number of predictions that
remain unchanged. The left figure contains the
overall number of such prediction flips and the
right figure contains the number of such predic-
tion flips that are correct. As we can see, though
there is a similar pattern of prediction flip on both
NRB and WTS, the correctness of these predic-
tion flips are different. For the challenging dataset
NRB, the prediction flip with the highest number,
namely O →PER and ORG →PER, have a rela-
tively low correct ratio of only 29%(223 /756) and
22%(118 /548), compared with 95%(1124 /1188)
and84%(331 /394) respectively on WTS. The sec-
ond row in Table 5 shows an example of the wrong
prediction flip for token " Clinton " from OtoPER,
while the true label should be LOC . Demonstra-
tions can better recognize entities, but can not dis-
tinguish well among the entity types and misguide
the model to make more false positive predictions
on NRB. This further supports that demonstrations
are not making robust improvements and simply
leverage the name regularity bias.1775
Demonstrations increase the confidence of
model predictions To take a closer look at how
these prediction flips work, we show the detailed
confidence score ( a.k.a the probability) for mod-
els’ predictions with some illustrative examples in
Table 5. Notably, in the first and third row, the con-
fidence for token " post " to be Oand " Clinton "
to be PER increase from 0.88 to 1.0 and 0.43 to
0.94 respectively, therefore we hypothesize demon-
strations increase the confidence of model’s final
prediction. We show empirical cumulative distribu-
tion function (ecdf) of the model’s confidence for
the final prediction with different modes of demon-
strations on both NRB and WTS benchmarks in
Figure 7. We found that, with demonstrations (ei-
ther standard or random), the model tends to be
more confident.
6.2 Demonstrations with prompt-tuning
Our construction of pathological demonstrations
can be easily generalized to other types of
demonstration-based learning, such as the prompt-
based fine-tuning used in LM-BFF (Gao et al.,
2021). Following their settings, we conduct ex-
periments with roberta-large model on single-
sentence task SST-5 (Socher et al., 2013) and
sentence-pair task MNLI (Williams et al., 2018)
with 16-shots support set, as shown in Figure 8. We
found that the performance of pathological demon-
strations is competitive with standard demonstra-
tions, consistent with our findings on sequence la-
beling tasks with classifier-based fine-tuning.17767 Discussion and Conclusion
In this paper, we study the robustness of
demonstration-based learning by designing patho-
logical demonstrations. We found that, replacing
demonstrations with random tokens still makes the
model a better few-shot learner; the length of ran-
dom token strings and the sampling space for ran-
dom tokens are the main factors affecting the per-
formance; and demonstrations increase the confi-
dence of model predictions on captured superficial
patterns such as the name regularity bias. Below
we discuss the broader impacts of our findings.
Our findings imply that natural language is not
sufficiently understood by the PLMs when being
utilized together with demonstrations, since ran-
dom token strings also lead to similar strong per-
formances. Similar to our work, recent studies (Ri
and Tsuruoka, 2022; Chiang and Lee, 2022) also
found models pretrained on random token strings
with a nesting dependency structure still provide
transferable knowledge to downstream fine-tuning,
suggesting the insufficient utilization of natural lan-
guage during pretraining. We urge future work on
demonstration based learning to think twice about
their model performance gains by designing more
robustness tests and ablation studies.
Our work also calls for a better design of demon-
strations that are free of spurious patterns, as exist-
ing demonstrations are less effective while there is
no spurious patterns to leverage (see Section 6.1).
Future work should not only optimize the perfor-
mances of demonstration based methods on the val-
idation set, but also pay attention to whether these
models suffer from spurious patterns and how to
increase their generalization abilities.
Limitations
This work is subject to several limitations. First,
we primarily look at sequence labeling tasks in this
work, and have not applied similar techniques for
other tasks such as text classification. Second, we
followed Lee et al. (2022) to use the widely used
bert-base-cased as our backbone for most of our
experiments. A thorough examination of other lan-
guage models such as T5 is needed, which we leave
as future work. Finally, the present work focuses
on understanding the robustness of demonstration-
based learning, and we have not developed any
practical guidelines on how to use our findings to
design more effective demonstrations.Acknowledgements
The authors would like to thank reviewers for their
helpful insights and feedback. This work is funded
in part by grants from Adobe and Amazon.
References177717781779A Additional Experimental Details
We use bert-base-cased model from Hugging-
Face (Wolf et al., 2020) as our backbone and use
NVIDIA GeForce RTX 3080 Ti to conduct all ex-
periments. The model have roughly 110M parame-
ters and takes 1 hour for each mode of demonstra-
tion on average to train under 5-shot scenario.
B Example Demonstrations
We show a list of real demonstrations we con-
structed and used in the experiments for CoNLL03
and CoNLL00 in Table 6 and Table 7.
C Sampling Algorithm
We follow the greedy sampling strategy proposed
by Yang and Katiyar (2020) to sample Kshots
for each tag in an increasing order with respect to
their frequencies, the detailed algorithm is shown
in Algorithm 1.
Algorithm 1: Greedy sampling
Require: # of shot K, labeled set Xwith tag set
CSort classes in Cbased on their freq. in XS←ϕ//Initialize the support set{Count←0}//Initialize counts of entity
classes in Swhile i <|C|do while Count< K do Sample (x,y)∈Xs.t.C∈y, w/o
replacement S←S∪ {(x,y)} Update {Count} ∀C∈y end whileend whilereturn S1780ST [SEP] 9/16 - Luo Yigang ( China ) beat Jason Wong ( Malaysia ) 15-5 15-6 China is LOC .
[SEP] Fox said the British government wanted an end to the alleged harassment of its nationals
at Dhaka airport by customs officials . British is MISC . [SEP] One dealer said positive stances
from Merrill Lynch and SBC Warburg were the key factors behind the gains . Merrill Lynch is
ORG . [SEP] +2 D.A. Weibring through 12 D.A. Weibring is PER .
SW [SEP] 9/16 - Luo Yigang ( China ) beat Jason Wong ( Malaysia ) 15-5 15-6 China is MISC .
[SEP] Fox said the British government wanted an end to the alleged harassment of its nationals
at Dhaka airport by customs officials . British is ORG . [SEP] One dealer said positive stances
from Merrill Lynch and SBC Warburg were the key factors behind the gains . Merrill Lynch is
PER . [SEP] +2 D.A. Weibring through 12 D.A. Weibring is LOC .
SN [SEP] 9/16 - Luo Yigang ( China ) beat Jason Wong ( Malaysia ) 15-5 15-6 [SEP] Fox said the
British government wanted an end to the alleged harassment of its nationals at Dhaka airport
by customs officials . [SEP] One dealer said positive stances from Merrill Lynch and SBC
Warburg were the key factors behind the gains . [SEP] +2 D.A. Weibring through 12
TR [SEP] ##llan costs similar Requiem tracking Michelle seeds 15th HM influenced ##OH ##inia
canyon visited USB punished ##ter hadn mom ##BA ##rrow ##hetto ##loss idea [SEP]
carriages ##uk Mellon inconsistent archaeologists Server quartet Low Downs ##izations Bears
##titis again falsely sprawling Dennis hey plural exam goalkeeper kingdom Argentine [SEP]
befriended ##ndi accept 1926 symbolic Colonel reviewer sketch rabbi Tampa ##orra tour Jul
minorities ##iary closing Beta Sunday Jai counts quasi ##uminous [SEP] ambitious Funk Got
##orm types Another Elements growled ##aris evaluation resulted
SR [SEP] Everton Merrill gains ##s One Moldova Ho beauty British qualifier S Lynch Dhaka
through said 1995 Merrill ##ull beauty opening 12 working 9 . [SEP] Ta qualifier of 9 Russian
through harassment Ho Dhaka up England airport its key ##burg republic ##man ##nch called
Malaysia wounds ) [SEP] by ##ron China ##burg dealer ( Malaysia said Glenn up 9 in customs
##tars officials at + factors Jason Tale ##nife ##s [SEP] ##tars taken up behind husband 12 end
Yi dealer S government17811782