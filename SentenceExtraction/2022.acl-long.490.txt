
Enwei Zhuand Jinpeng LiHwaMei Hospital, University of Chinese Academy of SciencesNingbo Institute of Life and Health Industry, University of Chinese Academy of Sciences
{zhuenwei,lijinpeng}@ucas.ac.cn
Abstract
Neural named entity recognition (NER) mod-
els may easily encounter the over-conﬁdence
issue, which degrades the performance and
calibration. Inspired by label smoothing and
driven by the ambiguity of boundary annota-
tion in NER engineering, we propose bound-
ary smoothing as a regularization technique for
span-based neural NER models. It re-assigns
entity probabilities from annotated spans to
the surrounding ones. Built on a simple but
strong baseline, our model achieves results bet-
ter than or competitive with previous state-
of-the-art systems on eight well-known NER
benchmarks.Further empirical analysis sug-
gests that boundary smoothing effectively mit-
igates over-conﬁdence, improves model cali-
bration, and brings ﬂatter neural minima and
more smoothed loss landscapes.
1 Introduction
Named entity recognition (NER) is one of the fun-
damental natural language processing (NLP) tasks
with extensive investigations. As a common setting,
an entity is regarded as correctly recognized only
if its type and two boundaries exactly match the
ground truth.
The annotation of boundaries is more ambigu-
ous, error-prone, and raises more inconsistencies
than entity types. For example, the CoNLL 2003
task contains four entity types (i.e., person, loca-
tion, organization, miscellaneous), which are easy
to distinguish between. However, the boundaries
of a entity mention could be ambiguous, because
of the “boundary words” (e.g., articles or modi-
ﬁers). Considerable efforts are required to specify
the “gold standard practice” case by case. Table 1
presents some examples from CoNLL 2003 An-
Table 1: Examples of CoNLL 2003 Annotation Guide-
lines and potential alternatives. The gold annotations
are marked in blue [*], whereas the alternative annota-
tions are in red [*].
notation Guidelines.In addition, some studies
have also reported that incorrect boundary is a ma-
jor source of entity recognition error (Wang et al.,
2019; Eberts and Ulges, 2020).
Recently, span-based models have gained much
popularity in NER studies, and achieved state-of-
the-art (SOTA) results (Eberts and Ulges, 2020;
Yu et al., 2020; Li et al., 2021). This approach
typically enumerates all candidate spans and classi-
ﬁes them into entity types (including a “non-entity”
type); the annotated spans are scarce and assigned
with full probability to be an entity, whereas all
other spans are assigned with zero probability. This
creates noticeable sharpness between the classiﬁca-
tion targets of adjacent spans, and may thus plague
the trainability of neural networks. In addition,
empirical evidence shows that these models easily
encounter the over-conﬁdence issue, i.e., the conﬁ-
dence of a predicted entity is much higher than its
correctness probability. This is a manifestation of
miscalibration (Guo et al., 2017).
Inspired by label smoothing (Szegedy et al.,
2016; Müller et al., 2019), we propose bound-
ary smoothing as a regularization technique for
span-based neural NER models. By explicitly re-
allocating entity probabilities from annotated spans7096to the surrounding ones, boundary smoothing can
effectively mitigate over-conﬁdence, and result in
consistently better performance.
Speciﬁcally, our baseline employs the contextu-
alized embeddings from a pretrained Transformer
of abase size (768 hidden size, 12 layers), and the
biafﬁne decoder proposed by Yu et al. (2020). With
boundary smoothing, our model outperforms previ-
ous SOTA on four English NER datasets (CoNLL
2003, OntoNotes 5, ACE 2004 and ACE 2005) and
two Chinese datasets (Weibo NER and Resume
NER), and achieves competitive results on other
two Chinese datasets (OntoNotes 4 and MSRA).
Such extensive experiments support the effective-
ness and robustness of our proposed technique.
In addition, we show that boundary smoothing
can help the trained NER models to preserve cal-
ibration, such that the produced conﬁdences can
better represent the precision rate of a predicted en-
tity. This corresponds to the effect of label smooth-
ing on the image classiﬁcation task (Müller et al.,
2019). Further, visualization results qualitatively
suggest that boundary smoothing can lead to ﬂat-
ter solutions and more smoothed loss landscapes,
which are typically associated with better general-
ization and trainability (Hochreiter and Schmidhu-
ber, 1997; Li et al., 2018).
2 Related Work
Named Entity Recognition The mainstream
NER systems are designed to recognize ﬂat entities
and based on a sequence tagging framework. Col-
lobert et al. (2011) introduced the linear-chain con-
ditional random ﬁeld (CRF) into neural network-
based sequence tagging models, which can explic-
itly encode the transition likelihoods between adja-
cent tags. Many researchers followed this work,
and employed LSTM as the encoder. In addi-
tion, character-level representations are typically
used for English tasks (Huang et al., 2015; Lample
et al., 2016; Ma and Hovy, 2016; Chiu and Nichols,
2016), whereas lexicon information is helpful for
Chinese NER (Zhang and Yang, 2018; Ma et al.,
2020; Li et al., 2020a).
Nested NER allows a token to belong to multi-
ple entities, which conﬂicts with the plain sequence
tagging framework. Ju et al. (2018) proposed to
use stacked LSTM-CRFs to predict from inner to
outer entities. Straková et al. (2019) concatenated
the BILOU tags for each token inside the nested en-
tities, which allows the LSTM-CRF to work as forﬂat entities. Li et al. (2020b) reformulated nested
NER as a machine reading comprehension task.
Shen et al. (2021) proposed to recognize nested
entities by the two-stage object detection method
widely used in computer vision.
Recent years, a body of literature emerged on
span-based models, which were compatible with
both ﬂat and nested entities, and achieved SOTA
performance (Eberts and Ulges, 2020; Yu et al.,
2020; Li et al., 2021). These models typically enu-
merate all possible candidate text spans and then
classify each span into entity types. In this work,
the biafﬁne model (Yu et al., 2020) is chosen and
re-implemented with slight modiﬁcations as our
baseline, because of its high performance and com-
patibility with boundary smoothing.
In addition, pretrained language models, also
known as contextualized embeddings, were also
widely introduced to NER models, and signiﬁcantly
boosted the model performance (Peters et al., 2018;
Devlin et al., 2019). They are used in our baseline
by default.
Label Smoothing Szegedy et al. (2016) pro-
posed the label smoothing as a regularization tech-
nique to improve the accuracy of the Inception
networks on ImageNet. By explicitly assigning a
small probability to non-ground-truth labels, label
smoothing can prevent the models from becom-
ing too conﬁdent about the predictions, and thus
improve generalization. It turned out to be a use-
ful alternative to the standard cross entropy loss,
and has been widely adopted to ﬁght against the
over-conﬁdence (Zoph et al., 2018; Chorowski and
Jaitly, 2017; Vaswani et al., 2017), improve the
model calibration (Müller et al., 2019), and de-
noise incorrect labels (Lukasik et al., 2020).
Our proposed boundary smoothing applies the
smoothing technique to entity boundaries, rather
than labels. This is driven by the observation that
entity boundaries are more ambiguous and incon-
sistent to annotate in NER engineering.To the
best of our knowledge, this study is the ﬁrst that
focuses on the effect of smoothing regularization
on NER models.70973 Methods
3.1 Biafﬁne Decoder
A neural network-based NER model typically en-
codes the input tokens to a sequence of represen-
tationsx=x;x;:::;xof lengthT, and then
decodes these representations to task outputs, i.e.,
a list of entities speciﬁed by types and boundaries.
We follow Yu et al. (2020) and use the biafﬁne
decoder. Speciﬁcally, the representations xare
separately afﬁned by two feedforward networks,
resulting in two representations h2Rand
h2R, which correspond to the start and
end positions of spans. For centity types (a “non-
entity” type included), given a span starting at the
i-th token and ending at the j-th token, a scoring
vectorr2Rcan be computed as:
r= (h)Uh+W(hhw) +b;(1)
wherew2Ris the (j i)-th width em-
bedding from a dedicated learnable matrix; U2
R,W2Randb2Rare learn-
able parameters. ris then fed into a softmax layer:
^y= softmax( r); (2)
which yields the predicted probabilities over all
entity types.
The ground truth y2Ris an one-hot encoded
vector, with value being 1 if the index corresponds
with the annotated entity type, and 0 otherwise.
Thus, the model can be optimized by the standard
cross entropy loss for all candidate spans:
L= Xylog(^y): (3)
In the inference time, the spans predicted to be
“non-entity” are ﬁrst discarded, and the remaining
ones are ranked by their predictive conﬁdences.
Spans with lower conﬁdences would also be dis-
carded if they clash with the boundaries of spans
with higher conﬁdences. Refer to Yu et al. (2020)
for more details.
3.2 Boundary Smoothing
Figure 1a visualizes the ground truth yfor an ex-
ample sentence with two annotated entities. The
valid candidate spans cover the upper triangular
area of the matrix. In existing NER models, the an-
notated boundaries are considered to be absolutely
reliable. Hence, each annotated span is assigned
with the full probability to be an entity, whereas all
unannotated spans are assigned with zero probabil-
ity. We refer to this probability allocation as hard
boundary , which is, however, probably not the best
choice.
As aforementioned, the entity boundaries may be
ambiguous and inconsistent, so the spans surround-
ing an annotated one deserve a small probability to
be an entity. Figure 1b visualizes ~y, the boundary
smoothing version of y. Speciﬁcally, given an
annotated entity, a portion of probability is as-
signed to its surrounding spans, and the remaining
probability 1 is assigned to the originally anno-
tated span. With smoothing size D, all the spans
with Manhattan distance d(dD)to the anno-
tated entity equally share probability =D. After7098such entity probability re-allocation, any remaining
probability of a span is assigned to be “non-entity”.
We refer to this as smoothed boundary .
Thus, the biafﬁne model can be optimized by
the boundary-smoothing regularized cross entropy
loss:
L= X~ylog(^y): (4)
Empirically, the positive samples (i.e., ground-
truth entities) are sparsely distributed over the
candidate spans. For example, the CoNLL 2003
dataset has about 35 thousand entities, which rep-
resent only 0.93% in the 3.78 million candidate
spans. By explicitly assigning probability to sur-
rounding spans, boundary smoothing prevents the
model from concentrating all probability mass on
the scarce positive samples. This intuitively helps
alleviate over-conﬁdence.
In addition, hard boundary presents noticeable
sharpness between the classiﬁcation targets of
positive spans and surrounding ones, although
they share similar contextualized representations.
Smoothed boundary provides more continuous tar-
gets across spans, which are conceptually more
compatible with the inductive bias of neural net-
works that prefers continuous solutions (Hornik
et al., 1989).
4 Experiments
4.1 Experimental Settings
Datasets We use four English NER datasets:
CoNLL 2003 (Tjong Kim Sang and Veenstra,
1999), OntoNotes 5, ACE 2004and ACE 2005;
and four Chinese NER datasets: OntoNotes 4,
MSRA (Levow, 2006), Weibo NER (Peng and
Dredze, 2015) and Resume NER (Zhang and Yang,
2018). Among them, ACE 2004 and ACE 2005 are
nested NER tasks, and the others are ﬂat tasks.
Hyperparameters For English corpora, we use
RoBERTa (Liu et al., 2019) followed by a BiL-
STM layer to produce the contextualized represen-
tations. For Chinese, we choose the BERT pre-
trained with whole word masking (Cui et al., 2019).The BiLSTM has one layer and 200 hidden size
with dropout rate of 0.5. The biafﬁne decoder fol-
lows Yu et al. (2020), with the afﬁne layers of
hidden size 150 and dropout rate 0.2. We addi-
tionally introduce a span width embedding of size
25. Note that the pretrained language models are
all of the base size (768 hidden size, 12 layers),
and the model is free of any additional auxiliary
embeddings; this conﬁguration is relatively simple,
compared with those in related work.
The boundary smoothing parameter is selected
inf0:1;0:2;0:3g; smoothing size Dis selected in
f1;2g.
All the models are trained by the AdamW op-
timizer (Loshchilov and Hutter, 2018) with a gra-
dient clipping at L2-norm of 5.0 (Pascanu et al.,
2013). The models are trained for 50 epochs with
batch size of 48. The learning rate is searched be-
tween 1e-3 and 3e-3 on the randomly initialized
weights, and between 8e-6 and 3e-5 on the pre-
trained weights; a scheduler of linear warmup in
the ﬁrst 20% steps followed by linear decay is ap-
plied.
Evaluation A predicted entity is considered cor-
rect if its type and boundaries exactly match the
ground truth. Hyperparameters are tuned according
to theFscores on the development set, and the
evaluation metrics (precision, recall, Fscore) are
reported on the testing set.
4.2 Main Results
Table 2 presents the evaluation results on four
English datasets, in which CoNLL 2003 and
OntoNotes 5 are ﬂat NER corpora, whereas ACE
2004 and ACE 2005 contains a high proportion of
nested entities. Compared with previous SOTA
systems, our simple baseline (RoBERTa-base +
BiLSTM + Biafﬁne) achieves on-par or slightly
inferior performance. Provided the strong baseline,
our experiments show that boundary smoothing can
effectively and consistently boost the Fscore of
entity recognition across different datasets. With
the help of boundary smoothing, our model outper-
forms the best of the previous SOTA systems by a
magnitude from 0.2 to 0.5 percentages.
Table 3 presents the results on four Chinese
datasets, which are all ﬂat NER corpora. Again,
boundary smoothing consistently improves model
performance against the baseline (BERT-base-
wwm + BiLSTM + Biafﬁne) across all datasets.
In addition, our model outperforms previous SOTA7099
by 2.16 and 0.55 percentages on Weibo and Re-
sume NER datasets, and achieves comparable F
scores on OntoNotes 4 and MSRA. Note that al-
most all previous systems solve these tasks within
a sequence tagging framework; this work adds to
the literature by introducing a span-based approach
and establishing SOTA results on multiple Chinese
NER benchmarks.
In ﬁve out of the above eight datasets, integrat-
ing boundary smoothing signiﬁcantly increases the
precision rate with a slight drop in the recall, result-
ing in a better overall Fscore. This is consistent
with our expectation, because boundary smoothing
discourages over-conﬁdence when recognizing en-
tities, which implicitly leads the model to establish
a more critical threshold to admit entities.
Given the use of well pretrained language mod-
els, most of the performance gains are relatively
marginal. However, boundary smoothing can work
effectively and consistently for different languages
and datasets. In addition, it is easy to implement
and integrate into any span-based neural NER mod-
els, with almost no side effects.
4.3 Ablation Studies
We perform ablation studies on CoNLL 2003,
ACE 2005 and Resume NER datasets (covering7100
ﬂat/nested and English/Chinese datasets), to eval-
uate the effects of boundary smoothing parameter
andD, as well as other components of our NER
system.
Boundary Smoothing Parameters We train the
model with inf0:1;0:2;0:3gandDinf1;2g;
the corresponding results are reported in Table 4.
Most combinations of the two hyperparameters can
achieve higher Fscores than the baseline, which
suggests the robustness of boundary smoothing.
On the other hand, the best smoothing parameters
are different across datasets, which are probably
related to the languages/domains of the text, the
entity types, and the annotation scheme (e.g., ﬂat
or nested NER). Hence, if the best performance is
desired for a new NER task in practice, hyperpa-
rameter tuning would be necessary.
Label Smoothing We replace boundary smooth-
ing with label smoothing in the span classiﬁer. La-
bel smoothing cannot improve, or may even impair
the performance of the model, compared with the
baseline (see Table 4). As aforementioned, we hy-
pothesize that the semantic differences between the
typical entity types are quite clear, so it is ineffec-
tive to smooth between them.
Pretrained Language Models We test if the per-
formance gain by boundary smoothing is robust
to different baselines. For English datasets, we
use BERT (Devlin et al., 2019) of the base and
large sizes, and RoBERTa (Liu et al., 2019) of
thelarge size (1024 hidden size, 24 layers). It
shows that boundary smoothing can consistently
increase the Fscores by 0.1–0.2 and 0.4–0.6 per-
centages for CoNLL 2003 and ACE 2005, respec-
tively. For Chinese, we use MacBERT (Cui et al.,
2020) of the base andlarge sizes, and bound-
ary smoothing still performs positively and consis-
tently, with an improvement of 0.2–0.3 percentage
Fscores on Resume NER (see Table 5).
It is noteworthy that boundary smoothing
achieves performance gains roughly comparable
to the gains by switching the pretrained language
model from the base size to the large size. This
suggests that the effect of boundary smoothing is
quite considerable, although the performance im-
provements seem small in magnitude.
In addition, our results show that RoBERTa
substantially outperforms the original BERT on
English NER. This is probably because that (1)
RoBERTa is trained on much more data; and
(2) RoBERTa focuses on the token-level task
(i.e., masked language modeling) by removing the
sequence-level objective (i.e., next sentence predic-
tion), hence, it is particularly suitable for within-
sequence downstream tasks, e.g., NER. This is also
the reason why we choose RoBERTa for our base-
line.
BiLSTM Layer We remove the BiLSTM layer,
directly feeding the output of pretrained language
model into the biafﬁne decoder. The results show
that this does not change the positive effect of
boundary smoothing (see Table 5). In addition,
absence of the BiLSTM layer will result in drops
of theFscores by about 0.3, 0.5 and 0.1 percent-
ages on the three datasets.71015 Further In-Depth Analysis
5.1 Over-Conﬁdence and Entity Calibration
The model performance (evaluated by, e.g., accu-
racy orFscore) is certainly important. However,
theconﬁdences of model predictions are also of
interest in many applications. For example, when it
requires the predicted entities to be highly reliable
(i.e., precision is of more priority than recall), we
may ﬁlter out the entities with conﬁdences lower
than a speciﬁc threshold.
However, Guo et al. (2017) have indicated that
modern neural networks are poorly calibrated, and
typically over-conﬁdent with their predictions. By
calibration, they mean the extent to which the pre-
diction conﬁdences produced by a model can rep-
resent the true correctness probability. We ﬁnd
neural NER models also easy to become miscali-
brated and over-conﬁdent. We observe that, with
the standard cross entropy loss, both the develop-
ment loss and Fscore increase in the later training
stage, which goes against the common perception
that the loss and Fscore should change in the
opposite directions. This phenomenon is similar
to the disconnect between negative likelihood and
accuracy in image classiﬁcation described by Guo
et al. (2017). We suppose that the model becomes
over-conﬁdent with its predictions, including the
incorrect ones, which contributes to the increase of
loss (see Appendix A for more details).
To formally investigate the over-conﬁdence is-
sue, we plot the reliability diagrams and calculate
expected calibration error (ECE). In brief, for an
NER model, we group all the predicted entities
by the associated conﬁdences into ten bins, and
then calculate the precision rate for each bin. If the
model is well calibrated, the precision rate should
be close to the conﬁdence level for each bin (see
Appendix B for more details).
Figure 2 compares the reliability diagrams and
ECEs between models with different smoothness 
on CoNLL 2003 and OntoNotes 5. For the baseline
model (= 0), the precision rates are much lower
than corresponding conﬁdence levels, suggesting
signiﬁcant over-conﬁdence. By introducing bound-
ary smoothing and increasing the smoothness , the
over-conﬁdence is gradually mitigated, and shifted
to under-conﬁdence ( = 0.3). In general, the model
presents best reliability diagrams when is 0.1 or
0.2. In addition, the ECEs of the baseline model are
0.072 and 0.063 on CoNLL 2003 and OntoNotes 5,
respectively; with of 0.1, the ECEs are reduced
to 0.013 and 0.034.
In conclusion, boundary smoothing can prevent
the model from becoming over-conﬁdent with the
predicted entities, and result in better calibration.
In addition, as mentioned previously, spans with
lower conﬁdences are discarded if they clash with
those of higher conﬁdences when decoding. With
the better calibration, the model can obtain a very
marginal but consistent increase in the Fscore.
5.2 Loss Landscape Visualization
How does boundary smoothing improve the model
performance? We originally conjectured that
boundary smoothing can de-noise the inconsis-
tently annotated entity boundaries (Lukasik et al.,
2020), but failed to ﬁnd enough evidence – the7102
performance improvement did not signiﬁcantly in-
crease when we injected boundary noises into the
training data.
As aforementioned, positive samples are very
sparse among the candidate spans. Without bound-
ary smoothing, the annotated spans are regarded to
be entities with full probability, whereas all other
spans are assigned with zero probability. This cre-
ates noticeable sharpness between the targets of the
annotated spans and surrounding ones, although
their neural representations are similar. Bound-
ary smoothing re-allocates the entity probabilities
across contiguous spans, which mitigates the sharp-
ness and results in more continuous targets. Con-
ceptually, such targets are more compatible with
the inductive bias of neural networks that prefers
continuous solutions (Hornik et al., 1989).
Li et al. (2018) have shown that residual connec-
tions and well-tuned hyperparameters (e.g., learn-
ing rate, batch size) can produce ﬂatter minima and
less chaotic loss landscapes, which account for the
better generalization and trainability. Their ﬁnd-
ings provide important insights into the geometricproperties of non-convex neural loss functions.
Figure 3 visualizes the loss landscapes for mod-
els with different smoothness on CoNLL 2003
and OntoNotes 5, following Li et al. (2018). In
short, for a trained model, a direction of the param-
eters is randomly sampled, normalized and ﬁxed,
and the loss landscape is computed by sampling
over this direction (refer to Appendix C for more
details).
The visualization results qualitatively show that,
the solutions found by the standard cross entropy
are relatively sharp, whereas boundary smoothing
can help arrive at ﬂatter minima. As many theo-
retical studies regard the ﬂatness as a promising
predictor for model generalization (Hochreiter and
Schmidhuber, 1997; Jiang et al., 2019), this result
may explain why boundary smoothing can improve
the model performance. In addition, boundary
smoothing is associated with more smoothed land-
scapes – the surrounding local minima are small,
shallow, and thus easy for the optimizer to escape.
Intuitively, such geometric property suggests that
the underlying loss functions are easier to train (Li
et al., 2018).
We believe that the sharpness in the span-based
NER targets is probably the reason for the sharp7103and chaotic loss landscape. Boundary smoothing
can effectively mitigate the sharpness, and result in
loss landscapes of better generalization and train-
ability.
6 Conclusion
In this study, we propose boundary smoothing as
a regularization technique for span-based neural
NER models. Boundary smoothing re-assigns en-
tity probabilities from annotated spans to the sur-
rounding ones. It can be easily integrated into any
span-based neural NER systems, but consistently
bring improved performance. Built on a simple but
strong baseline (a base -sized pretrained language
model followed by a BiLSTM layer, and the bi-
afﬁne decoder), our model achieves SOTA results
on eight well-known NER benchmarks, covering
English and Chinese, ﬂat and nested NER tasks.
In addition, experimental results show that
boundary smoothing leads to less over-conﬁdence,
better model calibration, ﬂatter neural minima and
more smoothed loss landscapes. These properties
plausibly explain the performance improvement.
Our ﬁndings shed light on the effects of smoothing
regularization technique in the NER task.
As discussed, boundary smoothing typically in-
creases the overall Fscore at the risk of a slight
drop in the recall rate; hence, one may be careful to
use it for recall-sensitive applications. Future work
will apply boundary smoothing to more variants of
span-based NER models, and investigate its effect
in a broader range of information extraction tasks.
Acknowledgements
We thank Yiyang Liu for his efforts in data pro-
cessing, and the anonymous reviewers for their
insightful comments and feedback. This work is
supported by the National Natural Science Foun-
dation of China (No. 62106248), Ningbo Science
and Technology Service Industry Demonstration
Project (No. 2020F041), and Ningbo Public Ser-
vice Technology Foundation (No. 2021S152).
References710471057106A Disconnect between Development Loss
andFScore
For most machine learning tasks, the desired metric
(e.g., accuracy or Fscore) is non-differentiable
and thus cannot be optimized via back-propagation.
The loss, on the other hand, is a designed differen-
tiable proxy such that minimizing it can increase
the original metric.
However, as illustrated in Figure 4a, when train-
ing an NER model by the standard cross entropy
loss, although the development Fscore keeps in-
creasing throughout, the development loss also in-
creases in the later stage (e.g., after ten epochs) of
the training process. Guo et al. (2017) describe
this phenomenon as a disconnect – the neural net-
work overﬁts to the loss without overﬁtting to the
metric. They regard this as indirect evidence for
miscalibration.
One plausible explanation is that in the later
training stage, the model becomes too conﬁdent
with its predicted outcomes, including both the
correct and incorrect ones. Therefore, although
slightly more spans are correctly classiﬁed on the
development set (as the Fscore increases), a small
portion of incorrectly classiﬁed spans is assigned
with much more conﬁdence and contributes to the
increase of loss.
Figure 4b presents the curves for boundary
smoothing loss. The development loss decreases
throughout the training process, opposite to the in-
creasingFscore. This result suggests that bound-
ary smoothing can help mitigate over-conﬁdence.
B Reliability Diagrams and Expected
Calibration Error
We generally follow Guo et al. (2017)’s approach
to plot reliability diagrams and calculate expected
calibration error (ECE).
Given an NER dataset and a model trained on it,
denote the gold and predicted entity sets as Eand^E,
respectively; the model produces a conﬁdence ^p
for each entity e2^E. WithKconﬁdence interval
bins, the predicted entities are grouped such that
those with conﬁdences falling into the k-th bin
constitute a subset:
^E=
eje2^E;^p2k 1
K;k
K
:
The precision rate (equivalent to the accuracy
with regard to a predicted set) of k-th group ^Eis:
Prec=j^E\Ej
j^Ej;
and the corresponding average conﬁdence is:
Conf=P^p
j^Ej:
The reliability diagrams plot Precagainst
Conffork= 1;2;:::;K . ECE is estimated by
the weighted average of absolute difference be-
tween PrecandConf:
ECE =Xj^Ej
j^EjPrec Conf
By deﬁnition, a perfectly calibrated model will
have Prec= Conffork= 1;2;:::;K . In this7107case, the reliability diagrams should lie along the
identity line, and ECE equals to 0.
C Loss Landscape Visualization
We generally follow Li et al. (2018)’s approach to
visualize the loss landscape.
Given a trained model of parameters , we sam-
ple a random direction from a normal distribution,
and rescale it by:
 kk
kk;
whereis thei-th weight of .On a data
set/splitD, the loss landscape plots the function:
f() =L(D;+);
whereL(D;)is the average loss value (in the
evaluation mode) on Dif the model takes parame-
ters of. In practice, we evenly sample 51 points in
the interval [ 1;1]for, and plot the loss values
against.7108