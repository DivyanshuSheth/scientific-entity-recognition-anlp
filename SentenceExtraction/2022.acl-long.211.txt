
Ekaterina Svikhnushina, Iuliana Voinea, Anuradha Welivita and Pearl Pu
School of Computer and Communication Sciences
EPFL, Lausanne, Switzerland
{ekaterina.svikhnushina,iuliana.voinea,
kalpani.welivita,pearl.pu}@epfl.ch
Abstract
Effective question-asking is a crucial compo-
nent of a successful conversational chatbot. It
could help the bots manifest empathy and ren-
der the interaction more engaging by demon-
strating attention to the speaker’s emotions.
However, current dialog generation approaches
do not model this subtle emotion regulation
technique due to the lack of a taxonomy of
questions and their purpose in social chitchat.
To address this gap, we have developed an
empathetic question taxonomy (EQT), with
special attention paid to questions’ ability to
capture communicative acts and their emotion-
regulation intents. We further design a crowd-
sourcing task to annotate a large subset of the
EmpatheticDialogues dataset with the estab-
lished labels. We use the crowd-annotated data
to develop automatic labeling tools and pro-
duce labels for the whole dataset. Finally, we
employ information visualization techniques
to summarize co-occurrences of question acts
and intents and their role in regulating inter-
locutor’s emotion. These results reveal impor-
tant question-asking strategies in social dialogs.
The EQT classification scheme can facilitate
computational analysis of questions in datasets.
More importantly, it can inform future efforts
in empathetic question generation using neural
or hybrid methods.
1 Introduction
Questions constitute a considerable part of casual
conversations and play many important social func-
tions (Huang et al., 2017; Enfield et al., 2010).
Asking follow-up questions about the speaker’s
statement indicates responsiveness, attention, and
care for the partner (Bregman, 2020; Huang et al.,
2017). Listeners who manifest such an empathetic
and curious attitude are more likely to establish the
common ground for meaningful communication(McEvoy and Plant, 2014) and appear more likable
to the speakers (Huang et al., 2017).
The vital role of questions in social interaction
makes question-asking a desirable property for
open-domain chatbots. These chatbots aim to en-
gage in a natural conversation with the users while
practicing active listening to deliver understanding
and recognition of users’ feelings (Rashkin et al.,
2019). In fact, generating meaningful questions is
so important that this has become one of the central
objectives of such agents (Xiao et al., 2020).
However, asking questions effectively is chal-
lenging as not all questions can achieve a particular
social goal, such as demonstrating attentiveness
or empathy (Huang et al., 2017; Robinson and
Heritage, 2006; Paukert et al., 2004). Given the
task complexity, automatic conversational question
generation is still gaining momentum, with only
few results reported so far. See et al. (2019) sug-
gested a way to control the number of questions
produced by the model with conditional training.
Wang et al. (2019) proposed a question-generation
method to increase their semantic coherence with
the answer, employing reinforcement learning fol-
lowed by the adversarial training procedure. Wang
et al. (2018) devised a model generating appropri-
ate questions for a variety of topics by modeling the
types of words used in a question (interrogatives,
topic words, and ordinary words). These works
presented approaches to produce contextually ap-
propriate and diverse questions, but none of them
considered the effect of questions on the interlocu-
tor’s emotional state. We attribute the deficiency in
this research to the lack of resources allowing to an-
alyze and model various question-asking strategies
in affect-rich social exchanges.
To address this gap, we present a categorization
and analysis of questions in social dialogs, with
four main contributions. First, we develop an Em-
pathetic Question Taxonomy, EQT, by manually an-
notating a subset of the EmpatheticDialogues (ED)2952dataset (Rashkin et al., 2019) (§4). EQT delineates
the acts and intents of questions. Question acts
capture semantic-driven communicative actions of
questions, while question intents describe the emo-
tional effect the question should have on the dialog
partner. For example, a listener may request in-
formation (question act) about the age of speaker’s
daughter by asking “How old is she?” after learning
about her success with the aim to amplify speaker’s
pride of his child (question intent). Second, we de-
sign and launch a crowd-sourcing annotation task
to grow the original labeled seed subset tenfold
(§5). Third, we devise an automatic classification
model, QBERT, to generate labels for the rest of
the ED dataset to demonstrate one important appli-
cation of the taxonomy (§6). QBERT can facilitate
the development of chatbots that offer engaging
and empathetic conversations by raising meaning-
ful questions. Finally, we inspect co-occurrences of
acts and intents and their effect on the interlocutor’s
emotion using visualization techniques (§7). The
analysis illustrates the most prominent question-
asking strategies in human emotional dialogs. To
conclude, we discuss the implications of these re-
sults for future question generation approaches.
2 Related Work
Previously proposed taxonomies of dialog acts fre-
quently differ in types of assisted natural language
tasks. The Dialog Act Markup in Several Lay-
ers (DAMSL) tag set was designed to enable com-
putational modeling of conversational speech us-
ing statistical methods (Jurafsky et al., 1997; Core
and Allen, 1997). It consists of 42 communicative
acts derived from a Switchboard corpus. Eight of
these labels describe different question types ac-
cording to their semantic role, e.g., Wh-question
orRhetorical-Question . Several works proposed
hierarchical taxonomies of dialog acts, targeted at
modeling users’ intents in human-machine conver-
sations. Montenegro et al. (2019) introduced their
annotation scheme for a symbolic dialog system in-
tended to improve the lives of the elderly, while Yu
and Yu (2021) designed a scheme for facilitating
general human-machine chit-chat. In both works
the logs of human-machine interactions were used
for producing the taxonomies. Each of them fea-
tures labels devoted to questions, characterizing
them either by a question word, e.g., How orWhat ,
or the form of expected answer, e.g., Open-ended
orYes/No question . Finally, Welivita and Pu (2020)suggested a taxonomy of empathetic response in-
tents in dialogs from the ED dataset with the pur-
pose of improving controllability in neural dialog
generation approaches. It further stated that Ques-
tioning is one of the most frequent intents of the
empathetic listeners. However, none of these works
focused on the fine-grained analysis of questions
and their role in empathetic dialogs.
Meanwhile, several linguistic studies closely ex-
amined the pragmatics of questions and offered a
number of classification schemes. Graesser et al.
(1994) developed a scheme of 18 tags based on the
information sought by the question. Their taxon-
omy applies well for transactional exchanges, but
does not capture the social dimension. Freed (1994)
studied the correspondence between the social func-
tion of questions and their syntactic form. She es-
tablished 16 social question functions occurring
in dyadic spoken conversations between friends.
In another research effort, a group of linguists ex-
plored the range of social actions performed by
questions across 10 languages (Enfield et al., 2010).
The authors developed a coding scheme compris-
ing 3 semantic question types and 7 social actions
and applied it to questions in spontaneous spoken
conversations (Stivers and Enfield, 2010). Finally,
Huang et al. (2017) developed a taxonomy of 6
question types to describe questions occurring in
their dataset of chat-based conversations between
strangers instructed to get to know each other.
The described works provide an insightful ba-
sis for studying questions in social conversations.
However, they do not consider the effect of ques-
tions on their addressee’s emotional states, neither
do they describe specific mechanisms to handle
computational modeling. Moreover, most of them
apply to spoken dialogs, impeding the extension
of their results to chat-based exchanges due to the
inherent differences in these modalities. Lastly,
they relied mainly on manual annotation, yielding
comparatively smaller datasets. In our study, we
extended the derived taxonomy to a large corpus
using crowd-sourcing and automatic methods and
analyzed the emerging patterns on a large scale.
We summarize the comparison of our question tax-
onomy with the existing schemes in Table 1.
3 Dataset
For taxonomy derivation, we sought a dataset that
contains social dialogs with diverse emotional ex-
pressions and could be applicable to train a chat-2953
bot with advanced question-generating abilities.
We avoided datasets featuring multi-modal dialogs
(IEMOCAP (Busso et al., 2008), MELD (Poria
et al., 2019)) as well as transcribed spoken conver-
sations (Emotionlines (Hsu et al., 2018), Switch-
board (Jurafsky et al., 1997)). Such dialogs contain
back-channel communication and other sensory
signals that are not present in chat-based conver-
sations and, therefore, are not well-suited for the
modeling task. Similarly, we rejected datasets that
assist other tasks than social conversation modeling,
such as SQuAD (Rajpurkar et al., 2016) (reading
comprehension) or QoQA (Reddy et al., 2019) (in-
formation gathering). Finally, we did not consider
datasets from social media as they can contain toxic
and aggressive responses (Zhang et al., 2018).
We opted for the EmpatheticDialogues (ED)
dataset (Rashkin et al., 2019), a benchmark dataset
for empathetic dialog generation containing 24,850
conversations grounded in emotional contexts.
Each dialog is initiated by a speaker describing
a feeling or experience and continued by a listener
who was instructed to respond empathetically. The
dialogs are evenly distributed over the 32 emotional
contexts, covering various speaker sentiments (e.g.,
sad,joyful ,proud ). We found the ED dataset to be
a rich source of question-asking as over 60% of all
dialogs contain a question in one of the listeners’
turns, resulting in a total of 20K listener questions.
Basic statistics of the dataset are given in Table 2.
Descriptor Value
# dialogs in total 24,850
# turns per dialog on avg. 4.31
# dialogs with at least one
question from listener15,253
(61.4%)
# questions from listeners 20,2014 Defining Empathetic Question
Taxonomy
Given the community’s interest in question-asking
functionality for chatbots and its significance for
empathetic response generation, we aimed at de-
veloping a taxonomy of listeners’ questions asked
in response to speakers’ emotional inputs. For this
purpose, being guided by prior literature review,
we employed a qualitative coding method, which is
an established approach for such tasks (Stivers and
Enfield, 2010; Huang et al., 2017; Zeinert et al.,
2021). Qualitative coding is a process of grouping
and labeling similar types of data and iteratively
validating the labels.
To cover a diverse range of speakers’ emotions,
we sampled several hundred dialogs uniformly
from the 32 emotional contexts in the ED corpus.
The sample size was chosen to balance the need for
the diversity of questions with researchers’ ability
to consider each question carefully and was consis-
tent with prior practice. The coding process was in-
formed by previous question classification schemes
(Table 1) and knowledge about general principles
of emotional regulation (Gross, 2013). Iterative ad-
justments were applied resulting from discussions
of the concrete data. Specifically, the first author
made several iterations of coding trials to develop
an initial set of labels. Throughout the process,
a number of review sessions were held with the
last author to merge the labels into more focused
classes. As a result, we developed the Empathetic
Question Taxonomy (EQT) with two distinguished
branches: question acts describe semantic-driven
features of questions (e.g., ask for confirmation ,
positive rhetoric ), whereas question intents charac-
terize their emotion-regulation functions targeted at
the interlocutor’s emotional state (e.g., sympathize ,
amplify excitement ). As it will be revealed further
(§7), an empathetic listener can use different ques-
tion acts to deliver the same intent, justifying the
proposed branching.
Overall, more than 310 questions were annotated.
EQT consists of 9 labels for question acts and 12
labels for question intents. The granularity of the
taxonomy was driven by earlier linguistic findings
and empirical observations about the interplay of
the labels in two branches. For example, question
actsrequest information (Enfield et al., 2010), ask
about consequence (Graesser et al., 1994), and ask
about antecedent (Graesser et al., 1994) are related
and could possibly be grouped. However, we de-2954cided to keep them separately as listeners use them
with unequal frequencies in positive and negative
emotional contexts and combine them with differ-
ent question intents (§7). Similarly, the initial set
of labels for question intents was created based on
the variety of emotions present in the dataset. We
further reduced it to a manageable size to make
it more applicable for an annotation task, while
still preserving sufficient expressiveness of labels
to represent subtleties of the data (Zeinert et al.,
2021). We present the labels with their definitions
below and provide several examples in Figure 1.
Examples for each act and intent label are given
correspondingly in Tables 4 and 5 from Appendix
A.
Question acts
Request information (38.7%): Ask for new fac-
tual information.
Ask about consequence (21.0%): Ask about the
result of the described action or situation.
Ask about antecedent (17.1%): Ask about the
reason or cause of the described state or event.
Suggest a solution (8.7%): Provide a specific so-
lution to a problem in a form of a question.
Ask for confirmation (5.8%): Ask a question to
confirm or verify the listener’s understanding of
something that has been described by the speaker.
Suggest a reason (5.2%): Suggest a specific rea-
son or cause of the event or state described by the
speaker in a form of a question.
Irony (1.3%): Ask a question that suggests the
opposite of what the speaker may expect, usually
to be humorous or pass judgement.
Negative rhetoric (1.3%): Ask a question to ex-
press a critical opinion or validate a speaker’s neg-
ative point without expecting an answer.
Positive rhetoric (1.0%): Ask a question to make
an encouraging statement or demonstrate agree-
ment with the speaker about a positive point with-
out expecting an answer.
Question intents
Express interest (57.1%): Express the willingness
to learn or hear more about the subject brought up
by the speaker; demonstrate curiosity.
Express concern (20.3%): Express anxiety or
worry about the subject brought up by the speaker.
Offer relief (4.8%): Reassure the speaker who is
anxious or distressed.
Sympathize (3.9%): Express feelings of pity and
sorrow for the speaker’s misfortune.Support (2.6%): Offer approval, comfort, or en-
couragement to the speaker, demonstrate an interest
in and concern for the speaker’s success.
Amplify pride (2.6%): Reinforce the speaker’s
feeling of pride.
Amplify excitement (1.9%): Reinforce the
speaker’s feeling of excitement.
Amplify joy (1.6%): Reinforce the speaker’s glad
feeling such as pleasure, enjoyment, or happiness.
De-escalate (1.6%): Calm down the speaker who
is agitated, angry, or temporarily out of control.
Pass judgement (1.6%): Express a (critical) opin-
ion about the subject brought up by the speaker.
Motivate (1.0%): Encourage the speaker to move
onward.
Moralize speaker (1.0%): Judge the speaker.
To validate the interpretability of the labels and
efficacy of the instructions for the crowd-sourcing
task, we invited two other members from our re-
search group and asked them to annotate questions
in 20 randomly selected dialogs, containing 25
questions. The annotators were instructed to con-
sider the preceding dialog turns while assigning the
labels as the same question might fall into differ-
ent categories based on the context. For example,
the question “What happened!?” can be classified
asExpress interest orExpress concern , depend-
ing on the valence of the speaker’s emotion. We
computed both the Fleiss kappa (Fleiss, 1971) and
the observed agreement among the first author and
two annotators. The observed agreement was cal-
culated as a percentage of questions with at least
two agreed labels (Endriss and Fernández, 2013).
We considered it as a reliable measure of inter-rater
–
–
–
–
–
–2955agreement as the number of coding categories was
large (9 for acts and 12 for intents), yielding rel-
atively low chance agreement (11.1% and 8.3%
respectively). The agreement resulted in 92% for
acts ( κ= 0.52) and 80% for intents ( κ= 0.31),
supporting the satisfactory interpretability of EQT.
5 Crowd-Sourced Annotation
For further analysis, we annotated a larger sub-
sample of the ED dataset with the EQT labels by
designing and launching a crowd-sourcing task on
Amazon Mechanical Turk (Mturk). The design was
refined based on three pilot studies: one internal
and two Mturk-based. For the annotation, we sam-
pled about 40% of dialogs from each of the orig-
inal 32 emotional contexts. We only sampled the
dialogs with at least one question in one of the lis-
tener’s turns. The dialogs were then pre-processed
so that each dialog ended with a question requir-
ing a label. Further, we distributed the dialogs
into individual human intelligent tasks (HITs) and
launched them on Mturk in a sequence of batches.
For each HIT we collected the annotations from
three workers. The incentive for one HIT varied
from $0.4 to $0.9 depending on the worker’s per-
formance and task configuration. We describe the
details about the task design and the annotation
procedure below; exhaustive explanations about
dialog pre-processing and the task user interface
are provided in Appendix B.
5.1 Task design
The interface consisted of four main components:
instructions, terminology, terminology quiz, and
the annotation task. The instructions informed the
workers about the purposes of the task. Next, the
terminology page outlined the description of the
EQT, listing the definition of each label with exam-
ples. The terminology quiz contained six dialogs
from the terminology page and invited the worker
to select correct labels for questions in each dialog.
Finally, the annotation task included 25 dialogs,
each ending with a listener turn with one or mul-
tiple questions. Under each question, labels from
two EQT branches were presented, and the worker
had to select one most suitable label within each of
the sets.Twenty out of the 25 dialogs were treated
as points for annotation, and the other 5 were bonusdialogs. For the bonus questions, we identified the
gold labels during the manual annotation phase and
used them to control workers’ quality: a worker
had to select the correct labels to score the points
counting towards additional incentive ($0.2).
We required all workers who accepted one of our
tasks for the first time to take the terminology quiz.
Workers who assigned the correct labels to at least
three questions could proceed to the annotation
task and were granted bonus payment for passing
the quiz ($0.1). The quiz was not required for the
workers who had successfully passed it once.
5.2 Quality control
In addition to the terminology quiz, we used sev-
eral mechanisms to control the annotation quality.
First, following Mturk recommendations, we only
allowed the workers with a 98% approval rate to
access our tasks. Second, we rejected assignments
whose completion time significantly deviated from
the expected average. Further, we ran additional
checks for the workers who accepted several of our
assignments simultaneously. Lastly, we computed
the inter-rater agreement for each batch and dis-
carded the submissions that harmed the agreement.
5.3 Results
Overall, we launched 556 HITs and 465 of them
were completed. The rejection rate after the qual-
ity control was 4.7%. Upon obtaining the results,
we first computed the Fleiss kappa scores for acts
(κ= 0.34) and for intents ( κ= 0.27) to validate
that the agreement between the workers is accept-
able. Then, we identified the final labels using the
majority vote: if at least two workers agreed on
a label, we chose it as a final label. This resulted
in an 83.6% observed agreement score for acts
and 75.8% observed agreement for intents. The
majority vote approach was shown to be able to
filter noisy judgments of amateurs, producing the
labeled set of comparable quality to the annotations
of experts (Nowak and Rüger, 2010). As a final
check, we computed the kappa agreement between
the crowd-sourced labels and the first author an-
notations for the subset of 450 randomly sampled
questions. The scores equaled 0.57 for acts (71.6%
observed agreement) and 0.50 for intents (68.0%
observed agreement), indicating moderate agree-
ment, which we treat as satisfactory for this type of
task. As a result, an act label was assigned to 6,433
questions and an intent label – to 5,826 questions,
with an intersection of 4,962 questions.29566 Automatic Labeling
To show how EQT can be operationalized, we
demonstrate the use of the taxonomy for annotating
the reminder of the ED dataset. We first formulate
the question act and intent prediction problems
and then build two classification models to address
them. Before training, we augmented the labeled
set using k-Nearest-Neighbors ( k-NN) method. We
also tried training the classifiers without data aug-
mentation, but their performance was weaker (see
Appendix D for details).
6.1 Data Augmentation
We employed the Sentence-BERT (SBERT) frame-
work (Reimers and Gurevych, 2019) to obtain em-
beddings for all questions with their contexts. Then
we used the cosine similarity measure to find kla-
beled NNs for each question in the unlabeled set
and assign the same labels to them. For the first
step, we computed the embeddings of each dialog
turn using the roberta-base-nli-stsb-mean-tokens
SBERT model and then combined them into a sin-
gle embedding per question with the weighted av-
erage. We opted for weighed average instead of
concatenation to keep manageable size of the em-
bedding vector. We used a half-decaying weighting
scheme, providing the highest weight to the final
question to indicate its importance. The usage of
this weighting scheme is guided by our previous ex-
periments of similar nature, where we observed that
the models with decaying weights performed better
than the ones without them (Welivita et al., 2021).
Next, we tested several approaches for identifying
semantically similar dialogs to propagate the la-
bels. One strategy was to take the same label as
the top-1 NN, given that the similarity was higher
than a predefined threshold. The other strategy was
to use the label identified with the majority vote
from the top-3 NNs. We did not experiment with
higher values of kdue to resource considerations.
We ran several cross-validation experiments on the
labeled set with grid search over various cosine-
similarity thresholds. Top-3 majority vote strategy
was shown to produce higher accuracy with a 0.825
cosine similarity threshold value resulting in the
acceptable trade-off between the accuracy ( ∼76%
for both label sets) and the number of labeled ques-
tions. Therefore, we applied this strategy for the
whole dataset, which produced additional 1,911 la-
bels for question acts and 1,886 labels for question
intents. More details are provided in Appendix C.6.2 Classifier Models
Using the human-annotated and augmented labels,
we trained two classifiers, which we collectively
call QBERT. QBERT models have identical archi-
tecture and vary only in the number of output cat-
egories in the final layer. Each model consists of
a BERT-based representation network, an atten-
tion layer, one hidden layer, and a softmax layer.
For the representation network, we used the archi-
tecture with 12 layers, 768 dimensions, 12 heads,
and 110M parameters. We initialized it with the
weights of RoBERTa language model pre-trained
by Liu et al. (2019) and for training used the same
hyper-parameters as the authors. As input, we fed a
listener question and preceding dialog turns in the
reverse order. To prioritize the question, the half-
decaying weighting scheme as described above was
applied to the token embeddings of each turn.
Before training, we took out a stratified random
sample of 20% of the questions (1,500) as a test set.
The test set contained respectively 1156 human-
and 344 SBERT-annotated questions. We sepa-
rately trained each model on 80% of the remaining
datapoints (5,475 acts, 4,969 intents), keeping the
rest as a validation set (1,369 acts, 1,243 intents).
We trained each model for 15 epochs and for pre-
diction retained the ones with the lowest validation
loss (see Appendix D for details). The classifiers
achieved 74.7% accuracy for intents and 79.1%
accuracy for acts on the test set. Further break-
down accuracies for human- and SBERT-annotated
test samples are given in Table 3. According to
previous work, human-human agreement can be
used as a proxy for human accuracy (Kumar, 2014;
Somasundaran and Chodorow, 2014). Given the
agreement in our Mturk experiment ( ∼75-85%),
QBERT exhibited reasonable predictive accuracy
and validated applicability and usefulness of EQT
for language modeling tasks.
Label source Question intents Question acts
human 71.0% 77.1%
SBERT 86.9% 87.5%
both 74.7% 79.1%29577 Analysis of Questioning Strategies
In this section we present the analysis of question-
ing strategies adopted by the empathetic listeners.
We base our examination on human-annotated ques-
tions instead of the whole ED dataset to avoid any
potential noise which might have been introduced
by automatic classification. Visualizations for the
whole dataset are included in Appendix E. Here,
by a questioning strategy , we imply a combination
of act and intent labels assigned to each question.
We first analyzed which labels from the two EQT
branches form such strategies by plotting the co-
occurrences of each pair (Figure 2). Larger circles
represent more frequent strategies, while an empty
cell indicates that people do not use the given act
to deliver the corresponding intent. For example,
to amplify partner’s joy, one may request informa-
tion for more details or ask about consequences of
the event, but will unlikely raise a negative rhetor-
ical question. Several strategies are much more
frequent than others. Act Request Information and
intent Express interest dominate in our dataset, oc-
curring together for 39% of questions. They define
the most general type of questions, which are prob-
ably easy to ask, providing a reason why listeners
use them often. At the same time, dialogs in the
ED dataset are relatively short, and it can be diffi-
cult for listeners to fully understand the ideas andfeelings of speakers in a couple of turns. In this
case, requesting information and expressing inter-
est demonstrates listener’s attentive curiosity about
the situation. Once listeners feel more confident
about the speakers’ sentiments and contexts, they
employ more specific question-asking strategies.
We further analyzed this phenomenon tempo-
rally across dialog turns (Figure 3). Primarily, we
studied how listeners’ questioning strategies affect
speakers’ emotions by visualizing the mappings
between them. For this visualization, we used 41
emotion and intent labels describing each turn in
the ED dataset produced by Welivita and Pu (2020).
To avoid clutter, we mapped the original 41 labels
to 3 coarser categories: positive, negative, and neu-
tral using our best judgement (see Appendix E for
details). Then, for the dialogs containing a ques-
tion in the second turn, we plotted how speakers’
emotions and listeners’ questioning strategies shift
over the first three turns. We computed the frequen-
cies of all questioning strategies and, for the ones
occurring in more than 0.5% of cases, we plotted
the flow patterns. We restricted our analysis to the
first three turns because over 70% of dialogs in the
ED dataset have only four of them, excluding the
possibility to study the influence of questioning
strategies on further speakers’ turns. In order to
still get an intuition how listeners’ question-asking
behavior changes in the consecutive turns, we plot-
ted the dynamics of the ratios of question act and
intent labels across the dialog depth.
Figure 3a shows the flow rates between speakers’
emotions and listeners’ questioning strategies. As
observed before, listeners most likely use follow-up
questions to elicit more details about the situation
by expressing interest and requesting information.
In most of such cases, the speaker’s emotion re-
mains preserved in their consecutive utterance as
the speaker elaborates on the first turn, maintaining
the sentiment. When speakers explain themselves
with sufficient clarity already in the first turn, lis-
teners raise more precise questions, adapting the
strategy to the affective context. If speakers share
a positive experience, listeners try to amplify their
emotions by requesting more information or asking
about the consequences of the situation. On the
contrary, when speakers disclose a negative sen-
timent, listeners try to validate and alleviate their
feelings. They typically intend to express concern,
sympathize, offer relief, or de-escalate the issue,
and achieve it by asking about what preceded or fol-2958
lowed the situation and politely suggesting possible
solutions or potential reasons for the issue. These
specific strategies demonstrate their effectiveness
as almost a half of negative speakers’ emotions gets
mitigated after the question intervention, while two
thirds of positive emotions keep up in the following
speaker’s turn. The examples of dialogs showing
how listeners use questions to treat both positive
and negative speakers’ sentiments are given in Fig-
ure 1. Additional examples are also available in
Figure 9 of Appendix D.
Figures 3b and 3c demonstrate how ratios of dif-
ferent acts and intents evolve over two successive
listeners’ responses. Even though the horizon of
four dialog turns might be too short to trace all the
patterns, a few observations can be made. With
increasing depth of the dialog, the overall number
of questions decreases, while two types get more
prominent: general questions ( Request Information ,
Express interest ) and questions aiming at suppress-
ing speakers’ negative emotions (e.g., Suggest a
solution ,Offer relief ). It may indicate that listeners
employ specific strategies to react to positive speak-
ers’ emotions immediately after their disclosure,
but in case of negative contexts they tend to askfor extra clarifications in the first place and deliver
targeted emotional treatment only in the next turn.
As dialogs converge to more neutral exchanges, re-
ducing the need to manage speakers’ feelings, the
ratio of questions demonstrating listeners’ general
curiously about the subject increases.
Finally, we reflected on the scarcely represented
labels. Among acts, Positive andNegative rhetoric
andIrony appear least frequently. These labels can
be broadly classified as rhetorical questions. They
typically serve for self-expression than conversa-
tional engagement and, therefore, are less common
than other forms of questions (Huang et al., 2017).
Moreover, negative rhetorical prompts may harm
the conversation quality (Zhang et al., 2018), which
could also explain why listeners avoided them in
empathetic dialogs. The same reasoning applies to
the two infrequent intents, Pass judgement andMor-
alize speaker . Another surprisingly rare intent is
Motivate . We believe that motivation might be diffi-
cult to express in the form of a question. Moreover,
for people who did not undergo special training,
expressing motivation might be more challenging
than other intents as it suggests a more thorough
approach to solving one’s problems.29598 Limitations and Future Work
Due to the nature of the ED dataset, some EQT la-
bels are less represented than others. We kept them
under consideration as we observed their distinc-
tive role in managing speaker’s emotions. Their
further analysis is crucial for further identifying
and designing effective questioning strategies for
empathetic conversations, such as promoting moti-
vational questions and avoiding judgmental ones.
Eliciting additional samples for these categories
could be possible by applying QBERT classifiers
to other datasets capturing social dialogs.
Our taxonomy does not cover the phatic role of
questions typically occurring during greetings, e.g.,
“What’s up?” or “How’s it going?” Such questions
were very rare in the ED dataset. We chose not to
analyze them, since these routine questions are the
most superficial (Huang et al., 2017) and unlikely
to serve any emotion-regulation function.
In the design of our annotation task, we opted for
asking the crowd workers to choose a single most
specific label from each of the two EQT branches.
This was done with the aim of facilitating further
analysis of questioning strategies withing the scope
of this study. Nevertheless, according to Graesser
et al. (1994), most adequate classification schemes
in the social sciences allow assigning an observa-
tion to multiple rather than only one category. This
also applies to our case. For example, for the ques-
tion “Did you go through a breakup recently?” both
Suggest a reason andRequest information can be
relevant. Future work can explore the possibilities
of using multiple applicable labels in addition to
the most specific one. Additional labels can be
obtained either by tagging the samples manually or
by taking top-N most confident predictions from
the classifiers.
The results of this paper can facilitate the de-
velopment of question-asking mechanisms of con-
versational chatbots. One can employ conditional
training (See et al., 2019) to train an end-to-end
neural model on a subset of most effective question-
ing strategies as defined by the co-occurrences of
the EQT labels and their mappings with speakers’
emotions (cf. Figure 3). To achieve even greater
interpretability and controllability, researchers can
devise architectures that dynamically model the se-
lection of appropriate questioning strategy before
generating a question. The strategy can be selected
based on the conversational history and speaker’s
emotion and further passed into the question gener-ation module. The main purpose of such modeling
approaches is to lead an engaging empathetic con-
versation by raising meaningful questions, which
deliver desirable effect on user’s emotional state.
Moreover, EQT along with QBERT models can be
used to label questions originating from other cor-
pora or chat logs and evaluate their effectiveness for
regulating speaker’s emotions, as described above.
9 Conclusion
In this paper we introduced EQT, an Empathetic
Question Taxonomy depicting acts and intents
of questions in social dialogs. We used crowd-
sourcing and automatic methods to tag all listeners’
questions from the ED dataset with the EQT labels,
which validated their interpretability and produced
useful annotations for future research. Further
analysis of the dataset with the visualization tech-
niques shed light on various question-asking strate-
gies employed by listeners in response to speakers’
emotionally-ridden inputs. We identified several
useful question-asking behaviors for favorable emo-
tional regulation. We expect that our findings will
enable the development of more controllable and
effective question-generation models.
10 Ethical Considerations
In this work, we used Mturk platform to collect
annotations for the dataset. Crowd workers on
Mturk are known to be underpaid according to
western standards, earning a median hourly wage
of only ∼$2/h (Kaufmann et al., 2011). At the
same time, monetary remuneration is not the only
factor defining people’s motivation to work on such
crowdsourcing platforms (Hara et al., 2018). For
example, workers might also engage with HITs to
learn new or train existing skills, pass free time,
or meet new people. Taking these factors into ac-
count, we designed our annotation experiments so
that workers received ∼$6/h on average to achieve
reasonable trade-off between the number of HITs
we could launch with the available budget and the
offered payment. While being slightly lower than
the US minimum wage ($7.25), it was deemed a
fair compensation given that it is three times higher
than the reported median wage and workers could
have other reasons to complete the tasks than purely
monetary reward. Nevertheless, we encourage fu-
ture works of similar nature to offer higher com-
pensation to the workers if possible.2960References2961
A Examples from Empathetic Question
Taxonomy
Tables 4 (acts) and 5 (intents) present the two
EQT branches with examples for each label. Ex-
amples are selected from the initial manually
annotated subset. For each label we include
its frequency for the three corresponding sets:
manually-labeled, Mturk-labeled, and overall (both
manually- ,Mturk-, and automatically-labeled).
The frequencies are approximately the same across
each label, which validates that our annotation
methods produced credible results. Examples of au-
tomatically assigned labels are given in Appendix
D.296229632964B Details about Mturk Annotation Task
B.1 Dialog Pre-processing
Throughout our study, we only used those ED di-
alogs that contained questions in at least one lis-
tener turn. Since one dialog could contain several
listener questions, for all downstream annotation
tasks each such dialog was split into several sepa-
rated dialogs, equal to the number of listener ques-
tions. The resulting sub-dialogs were truncated
such that they would end with the particular ques-
tion to which they corresponded to allow labeling
every question in each dialog, without losing the
previous conversational context. Figure 4 shows an
example of a dialog from the original ED dataset
and the resulting dialogs after the split.
In the Mturk interface, if the given listener turn
contained multiple questions, we showed the result-
ing sub-dialogs in the same page one after another
for contextual consistency. But if the original di-
alog contained listener questions in several turns,
we showed the resulting dialogs in the two separate
pages. Using the example from Figure 4, we would
show the first resulting dialog in one page and the
last two resulting dialogs together in another page.
B.2 Task User Interface
The user interface for the annotation task is illus-
trated in Figure 5.Original dialog
Speaker: – You are never going to believe what
I did!
Listener: – What did you do?
Speaker: – Well, I normally do not feel com-
fortable lending things to my friends,
but recently I mustered up the trust to
loan my friend my vehicle.
Listener: – Ouch... Is it just for a day? Is your
friend a safe driver?
Resulting dialogs
Speaker: – You are never going to believe what
I did!
Listener: – What did you do?
Speaker: – You are never going to believe what
I did!
Listener: – What did you do?
Speaker: – Well, I normally do not feel com-
fortable lending things to my friends,
but recently I mustered up the trust to
loan my friend my vehicle.
Listener: – Ouch... Is it just for a day?
Speaker: – You are never going to believe what
I did!
Listener: – What did you do?
Speaker: – Well, I normally do not feel com-
fortable lending things to my friends,
but recently I mustered up the trust to
loan my friend my vehicle.
Listener: – Ouch... Is it just for a day? Is your
friend a safe driver?2965C Details about Data Augmentation with
Lexical Similarity
C.1 Setup and Results
We used a half-decaying weighting scheme to en-
code questions with preceding context for the data
augmentation process. The highest weight was
always assigned to the final question to give it a
higher preference. For example, if the dialog con-
text consisted of three turns with embeddings e,
e,eand the fourth turn was a listener’s question
with embedding e, the final dialog embedding was
(8/15)e+ (4/15)e+ (2/15)e+ (1/15)e.
Figures 6 and 7 demonstrate the results of cross-
validation runs for question acts and question in-
tents for the Nearest-Neighbor label propagation
approach. For each label set, we experimented with
two similarity strategies: taking the same label as
the top-1 most similar dialog according to the co-
sine similarity ( Max, included in sub-figures 6a
and 7a) and identifying the label with the major-
ity vote from the top-3 most similar dialogs ( Vote,
included in sub-figures 6b and 7b). For each cross-
validation launch we conducted a grid-search over
cosine-similarity thresholds in a range between 0.7
and1.
We also tried concatenating one-hot-encoded
emotional context vectors with the dialog embed-
dings before running the cross-validation, but it did
not result in any improvement in the accuracy and
the resulting plots were almost identical to Figures
6 and 7, so we decided not to proceed with this
approach.
C.2 Examples of Annotated Questions
Table 6 presents several examples of propagated la-
bels obtained using the outlined data augmentation
process to give a better idea on the accuracy of this
approach.2966Head of Table 6
Annotated question Top-1 NN Top-2 NN Top-3 NN
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–2967Continuation of Table 6
Annotated question Top-1 NN Top-2 NN Top-3 NN
–
––
––
––
–
–
–
–
––
––
–
–
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–
–
––
––
––
–2968
D Details about training automatic
classifiers
For our automatic classifiers, we used GELU as
a hidden activation function and applied a 0.1
dropout to all layers and attention weights. For
training, we used Adam optimizer with β= 0.9,
β= 0.98,ϵ= 1×10, and the peak learning
rate of 2×10. The maximum number of input
tokens was set to 100, and we used the batch size
of 50. The evolution of train and validation losses
over the course of 15 training epochs is shown in
Figure 8. We used Google Colab environment for
the training.
The performance of classifiers trained only on a
human-annotated subset was several percent lower
than training on augmented data (see Section 6.2),
resulting in 75% accuracy for acts and 70% for
intents on the same (human-annotated) test set.
Therefore, in this paper, we focus on the results
obtained with the augmented data.Figure 9 demonstrates several examples of auto-
matically labeled questions in the ED dialogs. We
specify both the predicted act and intent labels for
each listeners’ question and emotions expressed
by speakers in each turn to observe how they are
influenced by listeners’ questions. Here we com-
bine the pre-processed dialogs (cf. Section B.1)
back to their original format, which explains why
some labeled questions appear in the middle of the
dialogs.
E Extended Analysis of Questioning
Strategies
E.1 Mapping of Emotions and Empathetic
Intents
Table 7 presents the mapping of 32 emotions
(Rashkin et al., 2019) and 9 empathetic intents
(Welivita and Pu, 2020) to three coarser emotion
categories of different valence, which we used to
produce visualizations for the analysis.
E.2 Additional plots for Human-Labeled
Subset
Figures 10 and 11 show the breakdown of flow rates
between speakers’ emotions and listeners’ question-
ing strategies (Figure 3) into separate mappings for
acts and for intents, respectively.
E.3 Analysis of Questioning Strategies on the
whole Dataset
For completeness, we include the same analyti-
cal visualizations as presented in Section 7 for the
whole ED dataset (Figures 12, 13, 14, and 15).
From these Figures, one can observe higher pres-
ence of more “general” categories ( Request infor-
mation ,Express interest ), which presumably orig-
inates from the fact that QBERT classifiers are
slightly biased towards these classes due to the
class imbalance in the training data.Neverthe-
less, despite this remark, other major patterns re-
vealed by the analysis of human-annotated subset
(cf. Section 7), preserve in the Figures produced
for the whole ED dataset (including automatically-
annotated questions).29692970–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–
–Category Mapped emotions and intents
Positive: trusting, surprised, caring, content,
joyful, excited, anticipating, hopeful,
prepared, nostalgic, impressed, faith-
ful, confident, proud, grateful
Neutral: neutral, encouraging, agreeing, sug-
gesting, acknowledging, sympathiz-
ing, wishing, consoling, questioning
Negative: devastated, afraid, apprehensive,
terrified, disappointed, disgusted,
lonely, anxious, sad, embarrassed, an-
noyed, furious, ashamed, angry, sen-
timental, guilty, jealous297129722973