
Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein,
Bela Gipp, Georg RehmDFKI GmbH,University of Göttingen,University of Copenhagenfirst.lastname@dfki.de ,lastname@uni-goettingen.de ,lastname@di.ku.dk
Abstract
Learning scientiﬁc document representations
can be substantially improved through con-
trastive learning objectives, where the chal-
lenge lies in creating positive and negative
training samples that encode the desired sim-
ilarity semantics. Prior work relies on discrete
citation relations to generate contrast samples.
However, discrete citations enforce a hard cut-
off to similarity. This is counter-intuitive to
similarity-based learning and ignores that sci-
entiﬁc papers can be very similar despite lack-
ing a direct citation – a core problem of ﬁnding
related research. Instead, we use controlled
nearest neighbor sampling over citation graph
embeddings for contrastive learning. This con-
trol allows us to learn continuous similarity, to
sample hard-to-learn negatives and positives ,
and also to avoid collisions between negative
and positive samples by controlling the sam-
pling margin between them. The resulting
method SciNCL outperforms the state-of-the-
art on the SciDocs benchmark. Furthermore,
we demonstrate that it can train (or tune) lan-
guage models sample-efﬁciently and that it
can be combined with recent training-efﬁcient
methods. Perhaps surprisingly, even training a
general-domain language model this way out-
performs baselines pretrained in-domain.
1 Introduction
Large pretrained language models (LLMs) achieve
state-of-the-art results through ﬁne-tuning on many
NLP tasks (Rogers et al., 2020). However, the sen-
tence or document embeddings derived from LLMs
are of lesser quality compared to simple baselines
like GloVe (Reimers and Gurevych, 2019), as their
embedding space suffers from being anisotropic,
i.e. poorly deﬁned in some areas (Li et al., 2020).
One approach that has recently gained attention
is the combination of LLMs with contrastive ﬁne-
tuning to improve the semantic textual similarity
between document representations (Wu et al., 2020;
Gao et al., 2021). These contrastive methods learnFigure 1: Starting from a query paper in a citation
graph embedding space. Hard positives are cita-
tion graph embeddings that are sampled from a similar
(close) context of , but are not so close that their gra-
dients collapse easily. Hard (to classify) negatives
(red band) are close to positives (green band) up to a
sampling induced margin . Easy negatives are very
dissimilar (distant) from the query paper .
to distinguish between pairs of similar and dissimi-
lar texts (positive and negative samples). As recent
works show (Tian et al., 2020b; Rethmeier and
Augenstein, 2022b,a; Shorten et al., 2021), the se-
lection of these positive and negative samples is
crucial for efﬁcient contrastive learning.
This paper focusses on learning scientiﬁc doc-
ument representations (SDRs). The core distin-
guishing feature of this domain is the presence of
citation information that complement the textual in-
formation. The current state-of-the-art SPECTER
by Cohan et al. (2020) uses citation information
to generate positive and negative samples for con-
trastive ﬁne-tuning of a SciBERT language model
(Beltagy et al., 2019). SPECTER relies on ‘cita-
tions by the query paper’ as a discrete signal for
similarity, i.e., positive samples are cited by the
query while negative ones are not cited.
However, SPECTER’s use of citations has its11670pitfalls. Considering only one citation direction
may cause positive and negative samples to collide
since a paper pair could be treated as a positive and
negative instance simultaneously. Also, relying
on a single citation as a discrete similarity signal
is subject to noise, e.g., citations may reﬂect po-
liteness and policy rather than semantic similarity
(Pasternack, 1969) or related papers lack a direct
citation (Gipp and Beel, 2009). This discrete cut-
off to similarity is counter-intuitive to (continuous)
similarity-based learning.
Instead, the generation of non-colliding con-
trastive samples should be based on a continuous
similarity function that allows us to ﬁnd semanti-
cally similar papers, even without direct citations.
With SciNCL, we address these issues by generat-
ing contrastive samples based on citation embed-
dings. The citation embeddings, which incorporate
the full citation graph, provide a continuous, undi-
rected, and less noisy similarity signal that allows
the generations of arbitrary difﬁcult-to-learn posi-
tive and negative samples.
Contributions:
•We propose neighborhood contrastive learn-
ing for scientiﬁc document representations
with citation graph embeddings (SciNCL)
based on contrastive learning theory insights.
•We sample positive (similar) and negative (dis-
similar) papers from the knearest neighbors
in the citation graph embedding space, such
that positives and negatives do not collide but
are also hard to learn.
•We compare against the state-of-the-art ap-
proach SPECTER (Cohan et al., 2020) and
other strong methods on the SD bench-
mark and ﬁnd that SciNCL outperforms
SPECTER on average and on 9 of 12 metrics.
•Finally, we demonstrate that with SciNCL,
using only 1% of the triplets for training, start-
ing with a general-domain language model, or
training only the bias terms of the model is
sufﬁcient to outperform the baselines.
• Our code and models are publicly available.
2 Related Work
Contrastive Learning pulls representations of
similar data points (positives) closer together, while
representations of dissimilar documents (negatives)
are pushed apart. A common contrastive objectiveis the triplet loss (Schroff et al., 2015) that Cohan
et al. (2020) used for scientiﬁc document represen-
tation learning, as we describe below. However,
as Musgrave et al. (2020) and Rethmeier and Au-
genstein (2022b) point out, contrastive objectives
work best when speciﬁc requirements are respected.
(Req. 1) Views of the same data should intro-
duce new information, i.e. the mutual information
between views should be minimized (Tian et al.,
2020b). We use citation graph embeddings to gen-
erate contrast label information that supplements
text-based similarity. (Req. 2) For training time
and sample efﬁciency, negative samples should be
hard to classify, but should also not collide with
positives (Saunshi et al., 2019). (Req. 3) Recent
works like Musgrave et al. (2020) and Khosla et al.
(2020) use multiple positives. However, positives
need to be consistently close to each other (Wang
and Isola, 2020), since positives and negatives may
otherwise collide, e.g., Cohan et al. (2020) consider
only ‘citations by the query’ as similarity signal
and not ‘citations to the query’. Such unidirectional
similarity does not guarantee that a negative paper
(not cited by the query) may cite the query paper
and thus could cause collisions, the more we sam-
ple (Appendix F.10). Our method treats both citing
and being cited as positives (Req. 2), while it also
generates hard negatives and hard positives (Req.
2+3). Hard negatives are close to but do not overlap
positives (red band in Fig. 1). Hard positives are
close, but not trivially close to the query document
(green band in Fig. 1). The sample induced mar-
gin (space between red and green band in Fig. 1)
ensures that contrast samples do not collide.
Triplet Mining remains a challenge in NLP due
to the discrete nature of language which makes
data augmentation less trivial as compared to com-
puter vision (Gao et al., 2021). Examples for aug-
mentation strategies are translation, word deletion,
or word reordering (Fang et al., 2020; Wu et al.,
2020). Positives and negatives can be sampled
based on the sentence position within a document
(Giorgi et al., 2021). Gao et al. (2021) utilize super-
vised entailment datasets for the triplet generation.
Language- and text-independent approaches are
also applied. Kim et al. (2021) use intermediate
BERT hidden state for positive sampling and Wu
et al. (2021) add noise to representations to obtain
negative samples. Xiong et al. (2020) present an
approach similar to SciNCL where they sample
hard negatives from the k nearest neighbors in the11671embedding space derived from the previous model
checkpoint. While Xiong et al. rely only on textual
data, SciNCL integrates also citation information
which are especially valuable in the scientiﬁc con-
text as Cohan et al. (2020) have shown.
Scientiﬁc Document Representations based on
Transformers (Vaswani et al., 2017) and pretrained
on domain-speciﬁc text dominate today’s scientiﬁc
document processing. There are SciBERT (Belt-
agy et al., 2019), BioBERT (Lee et al., 2019) and
SciGPT2 (Luu et al., 2021), to name a few. Re-
cent works modify these domain LLMs to support
cite-worthiness detection (Wright and Augenstein,
2021), document similarity (Ostendorff et al., 2020)
or fact checking (Wadden et al., 2020).
Aside from text, citations are a valuable signal
for the similarity of research papers. Paper (node)
representations can be learned using the citation
graph (Wu et al., 2019; Perozzi et al., 2014; Grover
and Leskovec, 2016). Especially for recommenda-
tions of papers or citations, hybrid combinations
of text and citation features are often employed
(Han et al., 2018; Jeong et al., 2020; Brochier et al.,
2019; Yang et al., 2015; Holm et al., 2022).
Closest to SciNCL are Citeomatic (Bhagavatula
et al., 2018) and SPECTER (Cohan et al., 2020).
While Citeomatic relies on bag-of-words for its
textual features, SPECTER is based on SciBERT.
Both leverage citations to learn a triplet-based docu-
ment embedding model, whereby positive samples
are papers cited in the query. Easy negatives are
random papers not cited by the query. Hard nega-
tives are citations of citations – papers referenced
in positive citations of the query, but are not cited
directly by it. Citeomatic also uses a second type
of hard negatives, which are the nearest neighbors
of a query that are not cited by it.
Unlike our approach, Citeomatic does not use the
neighborhood of citation embeddings, but instead
relies on the actual document embeddings from the
previous epoch. Despite being related to SciNCL,
the sampling approaches employed in Citeomatic
and SPECTER do not account for the pitfalls of us-
ing discrete citations as signal for paper similarity.
Our work addresses this issue.
Cross-Modal Transfer. SciNCL transfers
knowledge across modalities, i.e., from citations
into a language model. According to Cohan
et al. (2020), SciNCL can be considered as a
“citation-informed Transformer ”. This cross-modaltransfer learning is applied for various modalities
(see Kaur et al. (2021) for an overview): text-to-
image (Socher et al., 2013), RGB-to-depth image
(Tian et al., 2020a), or graph-to-image (Wang
et al., 2018). While the aforementioned methods
incorporate cross-modal knowledge through joint
loss functions or latent representations, SciNCL
transfers knowledge through the contrastive sample
selection, which we found superior to the direct
transfer approach (Appendix F.9).
3 Methodology
Our goal is to learn citation-informed represen-
tations for scientiﬁc documents. To do so we
sample three document representation vectors and
learn their similarity. For a given query paper
vector d, we sample a positive (similar) paper
vector dand a negative (dissimilar) paper vec-
tord. This produces a ‘query, positive, negative’
triplet (d,d,d)– represented by ( ,,)
in Fig. 1. To learn paper similarity, we need to
deﬁne three components: (§3.1) how to calculate
document vectors dfor the loss over triplets L;
(§3.2) how citations provide similarity between pa-
pers; and (§3.3) how negative and positive papers
(d,d)are sampled as (dis-)similar documents
from the neighborhood of a query paper d.
3.1 Contrastive Learning Objective
Given the textual content of a document d(paper),
the goal is to derive a dense vector representation
dthat best encodes the document information and
can be used in downstream tasks. A Transformer
language model f(SciBERT; Beltagy et al. (2019))
encodes documents dinto vector representations
f(d) =d. The input to the language model is the
title and abstract separated by the [SEP] token.
The ﬁnal layer hidden state of the [CLS] token is
then used as a document representation f(d) =d.
Training with a masked language modeling ob-
jectives alone has been shown to produce sub-
optimal document representations (Li et al., 2020;
Gao et al., 2021). Thus, similar to the SDR state-
of-the-art method SPECTER (Cohan et al., 2020),
we continue training the SciBERT model (Beltagy
et al., 2019) using a self-supervised triplet margin
loss (Schroff et al., 2015):
L= max/braceleftBig
/bardbld−d/bardbl−/bardbld−d/bardbl+ξ,0/bracerightBig11672Here,ξis a slack term ( ξ= 1 as in SPECTER)
and/bardbl∆d/bardblis theLnorm, used as a distance func-
tion. However, the SPECTER sampling method
has signiﬁcant drawbacks. We will describe these
issues and our contrastive learning theory guided
improvements in detail below in §3.2.
3.2 Citation Neighborhood Sampling
Compared to the textual content of a paper, cita-
tions provide an outside view on a paper and its
relation to the scientiﬁc literature (Elkiss et al.,
2008), which is why citations are traditionally used
as a similarity measure in library science (Kessler,
1963; Small, 1973). However, using citations as a
discrete similarity signal, as done in Cohan et al.
(2020), has its pitfalls. Their method deﬁnes pa-
pers cited by the query as positives, while paper
citing the query could be treated as negatives. This
means that positive and negative learning infor-
mation collides between citation directions, which
Saunshi et al. (2019) have shown to deteriorate per-
formance. Furthermore, a cited paper can have a
low similarity with the citing paper given the many
motivations a citation can have (Teufel et al., 2006).
Likewise, a similar paper might not be cited.
To overcome these limitations, we learn citation
embeddings ﬁrst and then use the citation neighbor-
hood around a given query paper dto construct
similar (positive) and dissimilar (negative) sam-
ples for contrast by using the knearest neighbors.
This builds on the intuition that nodes connected
by edges should be close to each other in the em-
bedding space (Perozzi et al., 2014). Using citation
embeddings allows us to: (1) sample paper similar-
ity on a continuous scale, which makes it possible
to: (2) deﬁne hard to learn positives, as well as (3)
hard or easy to learn negatives. Points (2-3) are
important in making contrastive learning efﬁcient
as will describe below in §3.3.
3.3 Positives and Negatives Sampling
Positive samples: dshould be semantically
similar to the query paper d, i.e. sampled close to
the query embedding d. Additionally, as Wang
and Isola (2020) ﬁnd, positives should be sam-
pled from comparable locations (distances from
the query) in embedding space and be dissimilar
enough from the query embedding, to avoid gradi-
ent collapse (zero gradients). Therefore, we sample
cpositive (similar) papers from a close neighbor-
hood around query embedding d(k−c,k],
i.e. the green band in Fig. 1. When sampling withNNsearch, we use a small kto ﬁnd positives
and later analyze the impact of kin Fig. 2.
Negative samples: can be divided into easy
and hard negative samples (light and dark red in
Fig. 1). Sampling more hard negatives is known to
improve contrastive learning (Bucher et al., 2016;
Wu et al., 2017). However, we make sure to sam-
ple hard negatives (red band in Fig. 1) such that
they are close to potential positives but do not col-
lide with positives (green band), by using a tun-
able ‘sampling induced margin’. We do so, since
Saunshi et al. (2019) showed that sampling a larger
number of hard negatives only improves perfor-
mance if the negatives do not collide with positive
samples , since collisions make the learning sig-
nal noisy. That is, in the margin between hard
negatives and positives we expect positives and
negatives to collide, thus we avoid sampling from
this region. To generate a diverse self-supervised
citation similarity signal for contrastive SDR learn-
ing, we also sample easy negatives that are farther
from the query than hard negatives. For negatives,
thekshould be large when sampling viaNNto
ensure samples are dissimilar from the query paper.
3.4 Sampling Strategies
As described in §3.2 and §3.3, our approach im-
proves upon the method by Cohan et al. (2020).
Therefore, we reuse their sampling parameters
(5 triplets per query paper) and then further op-
timize our methods’ hyperparameters. For exam-
ple, to train the triplet loss, we generate the same
amount of (d,d,d)triplets per query paper as
SPECTER (Cohan et al., 2020). To be precise, this
means we generate c=5positives (as explained
in §3.3). We also generate 5 negatives, three easy
negativesc=3and two hard negatives c=2,
as described in §3.3.
Below, we describe three strategies (I-III) for
sampling triplets. These either sample neighboring
papers from citation embeddings (I), by random
sampling (II), or using both strategies (III). For
each strategy, let cbe the number of samples for
either positives c, easy negatives c, or hard
negativesc.
Citation Graph Embeddings: We train a graph
embedding model fon citations extracted from the
Semantic Scholar Open Research Corpus (S2ORC;
Lo et al., 2020) to get citation embeddings C.
We utilize PyTorch BigGraph (Lerer et al., 2019),11673which allows for training on large graphs with mod-
est hardware requirements. The resulting graph
embeddings perform well using the default training
settings from Lerer et al. (2019), but given more
computational resources, careful tuning may pro-
duce even better-performing embeddings. Nonethe-
less, we conducted a narrow parameter search
based on link prediction – see Appendix D.
(I) K-nearest neighbors (NN): Assuming a
given citation embedding model fand a search in-
dex (e.g., FAISS §4.3), we run KNN (f(d),C)
and takecsamples from a range of the (k−c,k]
nearest neighbors around the query paper dwith
its neighbors N={n,n,n,...}, whereby neigh-
bornis thei-th nearest neighbor in the cita-
tion embedding space. For instance, for c=3
andk=10 the corresponding samples would be
the three neighbors descending from the tenth
neighbor:n,n, andn. To reduce comput-
ing effort, we sample the neighbors Nonly once
via[0; max(k,k)], and then generate triplets
by range-selection in N; i.e. positives = (k−
c;k], and hard negatives = (k−c;k].
(II) Random sampling: Sample any cpapers
without replacement from the corpus.
(III) Filtered random: Like (II) but excluding
the papers that are retrieved byNN, i.e., all neigh-
bors within the largest kare excluded. This is ana-
log to SPECTER’s approach of selecting random
candidates that are not cited by the query.
TheNNsampling introduces the hyperparame-
terkthat allows for the controlled sampling of pos-
itives or negatives with different difﬁculty (from
easy to hard depending on k). Speciﬁcally, in Fig. 1
the hyperparameter kdeﬁnes the tunable sample
induced margin between positives and negatives, as
well as the width and position of the positive sam-
ple band (green) and negative sample band (red)
around the query sample. Besides the strategies
above, we experiment with similarity threshold,
k-means clustering and sorted random sampling,
neither of which performs well (Appendix F).
4 Experiments
In the following, we introduce our experiments
including the data sets and implementation details.4.1 Evaluation Dataset
We evaluate on the SD benchmark (Cohan
et al., 2020). A key difference to other benchmarks
is that embeddings are the input to the individual
tasks without explicit ﬁne-tuning. The SD
benchmark consists of the following four tasks:
Document classiﬁcation (CLS) with Medical
Subject Headings (MeSH) (Lipscomb, 2000) and
Microsoft Academic Graph labels (MAG) (Sinha
et al., 2015). Co-views and co-reads (USR) pre-
diction based on the L2 distance between embed-
dings. Direct and co-citation (CITE) prediction
based on the L2 distance between the embeddings.
Recommendations (REC) generation based on
embeddings and paper metadata.
4.2 Training Datasets
The experiments mainly compare SciNCL against
SPECTER on the SD benchmark. However,
we found 40.5% of SD’s papers leaking into
SPECTER’s training data (the leakage affects only
the unsupervised paper data but not the gold labels
– see Appendix B). To be transparent about this
leakage, we train SciNCL on two datasets:
SPECTER replication (w/ leakage): We repli-
cate SPECTER’s training data including its leakage.
Unfortunately, SPECTER provides neither citation
data nor a mapping to S2ORC, which our citation
embeddings are based on. We successfully map
96.2% of SPECTER’s query papers and 83.3% of
the corpus from which positives and negatives are
sampled to S2ORC. To account for the missing
papers, we randomly sample papers from S2ORC
(without the SD papers) such that the abso-
lute number of papers is identical with SPECTER.
S2ORC subset (w/o leakage): We select a ran-
dom subset from S2ORC that does not contain
any of the mapped SD papers. This avoids
SPECTER’s leakage, but also makes the scores re-
ported in Cohan et al. (2020) less comparable. We
successfully map 98.6% of the SD papers
to S2ORC. Thus, only the remaining 1.4% of the
SD papers could leak into this training set.
The details of the dataset creation are described
in Appendix A and C. Both training sets yield 684K
triplets (same count as SPECTER). Also, the ra-
tio of training triplets per query remains the same
(§3.4). Our citation embedding model is trained
on the S2ORC citation graph. In w/ leakage , we
include all SPECTER papers even if they are part11674ofSD, the remaining SD papers are
excluded (52.5 nodes and 463M edges). In w/o
leakage , all mapped SD papers are excluded
(52.4M nodes and 447M edges) such that we avoid
leakage also for the citation embedding model.
4.3 Model Training and Implementation
We replicate the training setup from SPECTER as
closely as possible. We implement SciNCL using
Huggingface Transformers (Wolf et al., 2020), ini-
tialize the model with SciBERT’s weights (Beltagy
et al., 2019), and train via the triplet loss (Equa-
tion 3.1). The optimizer is Adam with weight de-
cay (Kingma and Ba, 2015; Loshchilov and Hutter,
2019) and learning rate λ=2. To explore the
effect of computing efﬁcient ﬁne-tuning we also
train a BitFit model (Ben Zaken et al., 2022) with
λ=1(§7.2). We train SciNCL on two NVIDIA
GeForce RTX 6000 (24G) for 2 epochs (approx.
24 hours of training time) with batch size 8 and
gradient accumulation for an effective batch size
of 32 (same as SPECTER). The graph embedding
training is performed on an Intel Xeon Gold 6230
CPU with 60 cores and takes approx. 6 hours. TheNNstrategy is implemented with FAISS (Johnson
et al., 2021) using a ﬂat index (exhaustive search)
and takes less than 30min for indexing and retrieval
of the triplets.
4.4 Baseline Methods
We compare against the following baselines (details
in Appendix E): USE (Cer et al., 2018), BERT
(Devlin et al., 2019), BioBERT (Lee et al., 2019),
SciBERT (Beltagy et al., 2019), CiteBERT (Wright
and Augenstein, 2021), DeCLUTR (Giorgi et al.,
2021), the graph-convolution approach SGC (Wu
et al., 2019), Citeomatic (Bhagavatula et al., 2018),
and SPECTER (Cohan et al., 2020).
Also, we compare against Oracle SciDocs which
is identical to SciNCL except that its triplets are
generated based on SD’s validation and test
set using their gold labels. For example, papers
with the same MAG labels are positives and pa-
pers with different labels are negatives. Similarly,
the ground truth of the other tasks is used, i.e.,
clicked recommendations are considered as pos-
itives etc. In total, this procedure creates 106K
training triplets for Oracle SciDocs . Moreover, we
under-sample triplets from the classiﬁcation tasks
to ensure a balanced triplet distribution over the
tasks. Accordingly, Oracle SciDocs represents an
estimate for the performance upper bound that canbe achieved with the current setting (triplet margin
loss and SciBERT encoder).
5 Overall Results
Tab. 1 shows the results, comparing SciNCL with
the best validation performance against the base-
lines. With replicated SPECTER training data (w/
leakage), SciNCL achieves an average performance
of 81.8 across all metrics, which is a 1.8 point ab-
solute improvement over SPECTER (the next-best
baseline). When trained without leakage, the im-
provement of SciNCL over SPECTER is consis-
tent with 1.7 points but generally lower (79.4 avg.
score). In the following, we refer to the results ob-
tained through training on the replicated SPECTER
data (w/ leakage) if not otherwise mentioned.
We ﬁnd the best validation performance based
on SPECTER’s data when positives and hard nega-
tive are sampled withNN, whereby positives are
k=25, and hard negatives are k=4000 (§6).
Easy negatives are generated through ﬁltered ran-
dom sampling. SciNCL’s scores are reported as
mean over ten random seeds (seed ∈[0,9]).
For MAG classiﬁcation, SPECTER achieves the
best result with 82.0 F1 followed by SciNCL with
81.4 F1 (-0.6 points). For MeSH classiﬁcation,
SciNCL yields the highest score with 88.7 F1 (+2.3
compared to SPECTER). Both classiﬁcation tasks
have in common that the chosen training settings
lead to over-ﬁtting. Changing the training by us-
ing only 1% training data, SciNCL yields 82.2
F1@MAG (Tab. 2). In all user activity and ci-
tation tasks, SciNCL yields higher scores than all
baselines. Moreover, SciNCL outperforms SGC on
direct citation prediction, where SGC outperforms
SPECTER in terms of n. On the recommender
task, SPECTER yields the best P@1 with 20.0,
whereas SciNCL achieves 19.3 P@1 (in terms of
n SciNCL and SPECTER are on par).
When training SPECTER and SciNCL without
leakage, SciNCL outperforms SPECTER even in
11 of 12 metrics and is on par in the other met-
ric. This suggests that SciNCL’s hyperparameters
have a low corpus dependency since they were only
optimized on the corpus with leakage.
Regarding the LLM baselines, we observe that
the general-domain BERT, with a score of 63.4,
outperforms the domain-speciﬁc BERT variants,
namely SciBERT (59.6) and BioBERT (58.8).
LLMs without citations or contrastive objectives
yield generally poor results. This emphasizes the11675
anisotropy problem of embeddings directly ex-
tracted from current LLMs and highlights the ad-
vantage of combining text and citation information.
In summary, we show that SciNCL’s triplet se-
lection leads on average to a performance improve-
ment on SD, with most gains being observed
for user activity and citation tasks. The gain from
80.0 to 81.8 is particularly notable given that even
Oracle SciDocs yields with 83.0 an only marginally
higher avg. score despite using test and validation
data from SD for the triplet selection. Ap-
pendix H shows examples of paper triplets.
6 Impact of Sample Difﬁculty
In this section, we present the optimization of
SciNCL’s sampling strategy (§3.3). We optimize
the sampling for positives and hard or easy nega-
tives with partial grid search on a random sample of
10% of the replicated SPECTER training data (sam-
pling based on queries). Our experiments show that
optimizations on this subset correlate with the en-
tire dataset. The validation scores in Fig. 2 and 3
are reported as the mean over three random seeds.
6.1 Positive Samples
Fig. 2 shows the avg. scores on the SD
validation set depending on the selection of posi-
tives with theNNstrategy. We only change k,
while negative sampling remains ﬁxed to its best
setting (§6.2). The performance is relatively sta-
ble fork<100with peak at k=25, fork>100
the performance declines as kincreases. Wang
and Isola (2020) state that positive samples should
be semantically similar to each other, but not too11676
similar to the query. For example, at k=5, pos-
itives may be a bit “too easy” to learn, such that
they produce less informative gradients than the
optimal setting k=25. Similarly, making ktoo
large leads to the sampling induced margin being
too small, such that positives collide with negative
samples , which creates contrastive label noise that
degrades performance (Saunshi et al., 2019).
Another observation is the standard deviation σ:
One would expect σto be independent of ksince
random seeds affect only the negatives. However,
positives and negatives interact with each other
through the triplet margin loss. Therefore, σis also
affected byk. To account for the interaction of
positives and negatives, one could sample simulta-
neously based on the distance to the query and the
distance of positives and negatives to each other.
6.2 Hard Negative Samples
Fig. 3 presents the validation results for different
kgiven the best setting for positives ( k=25).
The performance increases with increasing k
until a plateau between 2000<k<4000 with
a peak atk=4000 . This plateau can also
be observed in the test set, where k=3000
yields a marginally lower score of 81.7 (Tab. 2).
Fork>4000 , the performance starts to decline
again. This suggests that for large kthe samples
are not “hard enough” which conﬁrms the ﬁndings
of Cohan et al. (2020).
6.3 Easy Negative Samples
Filtered random sampling of easy negatives yields
the best validation performance compared pure ran-
dom sampling (Tab. 2). However, the performance
difference is marginal. When rounded to one dec-
imal, their average test scores are identical. The
marginal difference is caused by the large corpus
size and the resulting small probability of randomly
sampling one paper from theNN results. But
without ﬁltering, the effect of random seeds in-
creases, since we ﬁnd a higher standard deviation
compared to the one with ﬁltering.
As a potential way to decrease randomness, we
experiment with other approaches like k-means
clustering but ﬁnd that they decrease the perfor-
mance (Appendix F).
6.4 Collisions
Similar to SPECTER, SciNCL’s sampling based on
graph embeddings could cause collisions when se-
lecting positives and negatives from regions close
to each other. To avoid this, we rely on a sample
induced margin that is deﬁned by the hyperparam-
eterkandk(distance between red and green
band in Fig. 1). When the margin gets too small,
positives and negatives are more likely to collide.
A collision occurs when the paper pair (d,d)is
contained in the training data as positive and as neg-
ative sample at the same time. Fig. 4 demonstrates
the relation between the number of collisions and
the size of the sample induced margin. The number
of collisions increases when the sample induced
margin gets smaller. The opposite is the case when
the margin is large enough ( k>1000 ), i.e.,
then the number of collisions goes to zero. This
relation also affects the evaluation performance as
Fig. 2 and Fig. 3 show. Namely, for large kor
smallkSciNCL’s performance declines and ap-
proaches SPECTER’s performance.
7 Ablation Analysis
Next, we evaluate the impact of language model
initialization and number of parameters and triples.11677
7.1 Initial Language Models
Tab. 2 shows the effect of initializing the model
weights not with SciBERT but with general-domain
LLMs (BERT-Base and BERT-Large) or with
BioBERT. The initialization with other LLMs de-
creases the performance. However, the decline
is marginal (BERT-Base -0.6, BERT-Large -0.4,
BioBERT -0.4) and all LLMs outperform the
SPECTER baseline. For the recommendation
task, in which SPECTER is superior over SciNCL,
BioBERT outperforms SPECTER. This indicates
that the improved triplet mining of SciNCL has a
greater domain adaption effect than pretraining on
domain-speciﬁc literature. Given that pretraining
of LLMs requires a magnitude more resources than
the ﬁne-tuning with SciNCL, our approach can be
a solution for resource-limited use cases.
7.2 Data and Computing Efﬁciency
The last three rows of Tab. 2 show the results regard-
ing data and computing efﬁciency. When keeping
the citation graph unchanged but training the lan-
guage model with only 10% of the original triplets,
SciNCL still yields a score of 81.1 (-0.6). Even
with only 1% (6840 triplets), SciNCL achieves a
score of 80.8 that is 1.0 points less than with 100%
but still 0.8 points more than the SPECTER base-
line. With this textual sample efﬁciency, one could
manually create triplets or use existing supervised
datasets as in Gao et al. (2021).
Lastly, we evaluate BitFit training (Ben Zaken
et al., 2022), which only trains the bias terms of
the model while freezing all other parameters. This
corresponds to training only 0.1% of the originalparameters. With BitFit, SciNCL yields a con-
siderable score of 81.2 (-0.5 points). As a result,
SciNCL could be trained on the same hardware
with even larger (general-domain) language mod-
els (§7.1).
8 Conclusion
We present a novel approach for contrastive learn-
ing of scientiﬁc document embeddings that ad-
dresses the challenge of selecting informative posi-
tive and negative samples. By leveraging citation
graph embeddings for sample generation, SciNCL
achieves a score of 81.8 on the SD bench-
mark, a 1.8 point improvement over the previous
best method SPECTER. This is purely achieved
by introducing tunable sample difﬁculty and avoid-
ing collisions between positive and negative sam-
ples, while existing LLM and data setups can be
reused. This improvement over SPECTER can be
also observed when excluding the SD pa-
pers during training (see w/o leakage in Tab. 1).
Furthermore, SciNCL’s improvement from 80.0
to 81.8 is particularly notable given that even ora-
cle triplets , which are generated with SD’s
test and validation data, yield with 83.0 only a
marginally higher score.
Our work highlights the importance of sam-
ple generation in a contrastive learning setting.
We show that language model training with 1%
of triplets is sufﬁcient to outperform SPECTER,
whereas the remaining 99% provide only 1.0 addi-
tional points (80.8 to 81.8). This sample efﬁciency
is achieved by adding reasonable effort for sam-
ple generation, i.e., graph embedding training andNNsearch. We also demonstrate that in-domain
LLM pretraining (like SciBERT) is beneﬁcial,
while general-domain LLMs can achieve compara-
ble performance and even outperform SPECTER.
This indicates that controlling sample difﬁculty
and avoiding collisions is more effective than in-
domain pretraining, especially in scenarios where
training an LLM from scratch is infeasible.
9 Limitations
SciNCL’s strategy of selecting positive and neg-
ative samples requires additional computational
resources for training the graph embedding model,
performing theNNsearch, and optimizing the
hyperparameters k,k(§4.3). While some of
the compute resources are offset by the sample-
efﬁcient language model training (§7.2), we still11678consider the increased compute effort as the major
limitation of the SciNCL method.
Especially the training of the graph embedding
model accounts for most of the additional compute
effort. This is also the reason for us providing only
a shallow of evaluation of the graph embeddings
(Appendix D). For example, we did not evaluate
the effect of different graph embeddings on the
actual SD performance. Moreover, evalua-
tions with smaller subsets of the S2ORC citation
graph are missing. Such evaluations could indicate
whether also less citation data can be sufﬁcient,
which would lower the compute requirements but
would make SciNCL also applicable in domains
where less graph data is available.
Acknowledgements
We would like to thank Christian Schulze and
his team for providing the compute infrastruc-
ture that made our experiments possible. The re-
search presented in this article is partially funded
by the German Federal Ministry of Education and
Research (BMBF) through the projects QURA-
TOR (Rehm et al., 2020) (Unternehmen Region,
Wachstumskern, no. 03WKDA1A) and PANQURA
(no. 03COV03E).
References11679116801168111682A Mapping to S2ORC
Neither the SPECTER training data nor the Sci-
Docs test data comes with a mapping to the S2ORC
dataset, which we use for the training of the ci-
tation embedding model. However, to replicate
SPECTER’s training data and to avoid leakage of
SciDocs test data such a mapping is needed. There-
fore, we try to map the papers to S2ORC based on
PDF hashes and exact title matches. The remaining
paper metadata is collected through the Semantic
Scholar API. Tab. 3 summarizes the outcome of
mapping procedure. Failed mappings can be at-
tributed to papers being unavailable through the
Semantic Scholar API (e.g., retracted papers) or
papers not being part of S2ORC citation graph.
B SPECTER-SciDocs Leakage
When replicating SPECTER (Cohan et al., 2020),
we found a substantial overlap between the pa-
persused during the model training and the papers
from their SD benchmark. In both datasets,
papers are associated with Semantic Scholar IDs.
Thus, no custom ID mapping as in Appendix A is
required to identify papers that leak from training
to test data. From the 311,860 unique papers used
in SPECTER’s training data, we ﬁnd 79,201 papers
(25.4%) in the test set of SD and 79,609
papers (25.5%) in its validation set. When combin-
ing test and validation set, there is a total overlap
of 126,176 papers (40.5%). However, this over-
lap affects only the ‘unsupervised’ paper metadata
(title, abstract, citations, etc.) and not the gold la-
bels used in SD (e.g., MAG labels or clicked
recommendations).
C Dataset Creation
As describe in §4.2, we conduct our experiments
on two datasets. Both datasets rely on the cita-tion graph of S2ORC (Lo et al., 2020). More
speciﬁcally, S2ORC with the version identiﬁer
20200705v1 is used. The full citation graph con-
sists of 52.6M nodes (papers) and 467M edges
(citations). Tab. 4 presents statistics on the datasets
and their overlap with SPECTER and SD.
The steps to reproduce both datasets are:
Replicated SPECTER (w/ leakage) In order to
replicate SPECTER’s training data and do not in-
crease the leakage, we exclude all SD pa-
pers which are not used by SPECTER from the
S2ORC citation graph. This means that apart from
the 110,538 SPECTER papers not a single other
SD paper is included. The resulting citation
graph has 52.5M nodes and 463M edges and is
used for training the citation graph embeddings.
For the SciNCL triplet selection, we also repli-
cate SPECTER’s query papers and its corpus from
which positive and negatives are sampled. Our map-
ping and the underlying citation graph allows us
to use 227,869 of 248,007 SPECTER’s papers for
training. Regarding query papers, we use 131,644
of 136,820 SPECTER’s query papers. To align
the number training triplets with the one from
SPECTER, additional papers are randomly sam-
pled from the ﬁltered citation graph.
Random S2ORC subset (w/o leakage) To
avoid leakage, we exclude all successfully mapped
SD papers from the S2ORC citation graph.
After ﬁltering the graph has 52.3 nodes and 447M
edges. The citation graph embedding model is
trained on this graph.
Next, we reproduce triplet selection from
SPECTER. Any random 136,820 query papers are
selected from the ﬁltered graph. For each query,
we generate ﬁve positives (cited by the query), two
hard negatives (citation of citation), and three ran-
dom nodes from the ﬁltered S2ORC citation graphs.
This sampling produces 684,100 training triplets
with 680,967 unique papers IDs (more compared
to the replicated SPECTER dataset). Based on
these triplets the SPECTER model for this dataset
is trained with the same model settings and hyper-
parameters as SciNCL (second last row in Tab. 1).
Lastly, the SciNCL triplets are generated based
on the citation graph embeddings of the same
680,967 unique papers IDs, i.e, the FAISS index
contains only these papers and not the remaining
S2ORC papers. Also, the same 136,820 query pa-
pers are used.11683
D Graph Embedding Evaluation
To evaluate the underlying citation graph embed-
dings, we experiment with a few of BigGraph’s
hyperparameters. We trained embeddings with dif-
ferent dimensions d={128,512,768}and different
distance measures (cosine similarity and dot prod-
uct) on 99% of the data and test the remaining
1% on the link prediction task. An evaluation of
the graph embeddings with SD is not pos-
sible since we could not map the papers used in
SD to the S2ORC corpus. All variations
are trained for 20 epochs, margin m=0.15, and
learning rate λ=0.1(based on the recommended
settings by Lerer et al. (2019)).
Tab. 5 shows the link prediction performance
measured in MRR, Hits@1, Hits@10, and AUC.
Dot product is substantially better than cosine simi-
larity as distance measure. Also, there is a positive
correlation between the performance and the size
of the embeddings. The larger the embedding size
the better link prediction performance. Graph em-
beddings with d=768 were the largest possible size
given our compute resources (available disk spacewas the limiting factor).
E Baseline Details
If not otherwise mentioned, all BERT variations
are used in their base-uncased versions.
The weights for BERT ( bert-base-uncased ),
BioBERT ( biobert-base-cased-v1.2 ), CiteBERT
(citebert ), DeCLUTR ( declutr-sci-base ) are taken
from Huggingface Hub. We use Universal Sen-
tence Encoder (USE) from Tensorﬂow Hub. For
Oracle SciDocs , we use the SciNCL implementa-
tion and under-sample the triplets from the classiﬁ-
cation tasks to ensure a balanced triplet distribution
over the tasks. The SPECTER version for the ran-
dom S2ORC training data (w/o leakage) is also
trained with the SciNCL implementation. Please
see Cohan et al. (2020) for additional baseline meth-
ods and their implementation details.
F Negative Results
We investigated additional sampling strategies and
model modiﬁcation of which none led to a signiﬁ-
cant performance improvement.
F.1 Undirected Citations
Our graph embedding model considers citations as
directed edges by default. We also train a SciNCL
model with undirected citations by ﬁrst converting
a single edge (a,b)into the two edges (a,b)and
(b,a). This approach yields a slightly worse perfor-
mance (81.7 avg. score; -0.1 points) and, therefore,
was discarded for the ﬁnal experiments.
F.2NN with interval large than c
Our best results are achieved withNNwhere the
size of the neighbor interval (k−c;k]is equal to
the number of samples cthat the strategy should
generate. In addition to this, we also experimented
with large intervals, e.g., (1000; 2000] , from which
cpapers are randomly sampled. This approach
yields comparable results but suffers from a larger
effect of randomness and is therefore more difﬁcult
to optimize.
F.3 K-Means Cluster for Easy Negatives
Easy negatives are supposed to be far away from
the query. Random sampling from a large corpus
ensures this as our results show. As an alternative11684approach, we tried k-means clustering whereby we
selected easy negatives from the centroid that has
a given distance to the query’s centroid. However,
this decreased the performance.
F.4 Sampling with Similarity Threshold
As alternative toNN, we select samples based
on cosine similarity in the citation embedding
space. Take cpapers that are within the simi-
larity threshold tof a query paper dsuch that
s(f(d),f(d))<t, wheresis the cosine simi-
larity function.
For example, given the similarity scores
S={0.9,0.8,0.7,0.1}(ascending order, the higher
the similarity is the closer the candidate embed-
ding to the query embedding is) with c=2and
t=0.5, the two candidates with the largest simi-
larity scores and larger than the threshold would
be0.8and0.7. The corresponding papers would
be selected as samples. While the positive thresh-
oldtshould close to 1, the negative threshold t
should be small to ensure samples are dissimilar
fromd. However, the empirical results suggest
that this strategy is inferior compared toNN.
F.5 Hard Negatives with Similarity
Threshold
Selecting hard negatives based on the similarity
threshold yields a test score of 81.7 (-0.1 points).
Fig. 5 show the validation results for different simi-
larity thresholds. A similar pattern as in Fig. 3 can
be seen. When the negatives are closer to the query
paper (larger similarity threshold t), the validation
score decreases.F.6 Positives with Similarity Threshold
Positive sampling with Sperforms poorly since
even for small t<0.5many query papers do not
have any neighbors within this similarity threshold
(more than 40%). Solving this issue would require
changing the set of query papers which we omit for
comparability to SPECTER.
F.7 Sorted Random
Simple random sampling does not ensure if a sam-
ple is far or close to the query. To integrate a dis-
tance measure in the random sampling, we ﬁrst
samplencandidates, then order the candidates ac-
cording to their distance to the query, and lastly
select theccandidates that are the closest or fur-
thest to the query as samples.
F.8 Mask Language Modeling
Giorgi et al. (2021) show that combining a con-
trastive loss with a mask language modeling loss
can improve text representation learning. However,
in our experiments a combined function decreases
the performance on SD, probably due to the
effects found by (Li et al., 2020).
F.9 Student-Teacher Learning
Student-teacher learning is effective in related work
on cross-modal knowledge transfer (Kaur et al.,
2021; Tian et al., 2020a). We also try to adopt
this approach for our experiments, whereby the
Transformer language model is the student, and the
citation graph embedding model is the teacher. By
directly learning from the citation embeddings, we
could circumvent the positive and negative sam-
pling needed for triplet loss learning, which in-
troduces unwanted issues like collisions. Given a
batch of document representations derived from
textD (through the language model) and the
citation graph representations for the same docu-
mentsD , we compute the pairwise cosine
similarity for both sets S andS . To trans-
fer the knowledge from the citation embeddings
into the language model, we devise the student-
teacher lossLbased on a mean-squared-error
loss (MSE) such that the difference between the
cosine similarities is minimized:
L=MSE(S,S ) (1)
Despite the promising results from Tian et al.
(2020a), the student-teacher approach performs
poorly in our experiments. We attribute this the11685overﬁtting to the citation data (the training loss
approaches zero after a few steps while the val-
idation loss remains high). The model trained
withLyields only a SD average score
of 64.7, slightly better than SciBERT but substan-
tially worse than SciNCL with triplet loss.
Additionally, we experiment with a joint loss
that is the sum of triplet margin loss L (see
§3.1) and the student-teacher loss L:
L =L +L (2)
Training with the joint loss L achieves an
average score of 80.5. Even though the joint loss
is not subject to overﬁtting, its SD perfor-
mance is slightly worse than the triplet loss L
alone. Given this outcome and that the computation
of the cosine similarities adds additional complex-
ity, we discard the student-teacher approach for the
ﬁnal experiments.
F.10 SPECTER & Bidirectional Citations
SPECTER (Cohan et al., 2020) relies on unidirec-
tional citations for their sampling strategy. While
papers cited by the query paper are considered as
positives samples, those citing the query paper (op-
posite citation direction) could be negative samples.
We see this use of citations as a conceptional ﬂaw
in their sampling strategy.
To test the actual effect on the resulting docu-
ment representation, we ﬁrst replicate the original
unidirectional sampling strategy from SPECTER
with our training data (see w/ leakage in §4.2). The
resulting SPECTER model achieves an average
score of 79.0 on SD.When changing the
sampling strategy from unidirectional to bidirec-
tional (‘citations to the query’ are also treated as a
signal for similarity), we observe an improvement
of +0.4 points to 79.4. Consequently, the use of
unidirectional citations is not only a conceptional
issue but also degrades learning performance.
G Task-speciﬁc Results
Fig. 6 and 7 present the validation performance like
in §6 but on a task-level and not as an average over
all tasks. The plots show that the optimal kand
kvalues are partially task dependent.H Examples
Tab. 6 lists three examples of query papers with
their corresponding positive and negative samples.
The complete set of triplets that we use during
training is available in our code repository.116861168711688