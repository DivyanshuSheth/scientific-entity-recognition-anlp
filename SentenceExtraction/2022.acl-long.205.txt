
Guanglin Niu, Bo Li, Yongfei Zhang, Shiliang PuBeijing Key Laboratory of Digital Media, Beihang University, Beijing, ChinaInstitute of Artificial Intelligence, Beihang University, Beijing, ChinaHangzhou Innovation Institute, Beihang University, Hangzhou, ChinaState Key Laboratory of Virtual Reality Technology and Systems, Beihang University,
Beijing, ChinaHikvision Research Institute, Hangzhou, China
{beihangngl, boli, yfzhang}@buaa.edu.cn, pushiliang.hri@hikvision.com
Abstract
Knowledge graphs store a large number of fac-
tual triples while they are still incomplete, in-
evitably. The previous knowledge graph com-
pletion (KGC) models predict missing links be-
tween entities merely relying on fact-view data,
ignoring the valuable commonsense knowl-
edge. The previous knowledge graph embed-
ding (KGE) techniques suffer from invalid neg-
ative sampling and the uncertainty of fact-view
link prediction, limiting KGC’s performance.
To address the above challenges, we propose
a novel and scalable Commonsense- Aware
Knowledge Embedding (CAKE ) framework
to automatically extract commonsense from
factual triples with entity concepts. The gen-
erated commonsense augments effective self-
supervision to facilitate both high-quality nega-
tive sampling (NS) and joint commonsense and
fact-view link prediction. Experimental results
on the KGC task demonstrate that assembling
our framework could enhance the performance
of the original KGE models, and the proposed
commonsense-aware NS module is superior to
other NS techniques. Besides, our proposed
framework could be easily adaptive to various
KGE models and explain the predicted results.
1 Introduction
In recent years, knowledge graphs (KGs) such
as Freebase (Bollacker et al., 2008), DBpe-
dia (Lehmann et al., 2015) and NELL (Mitchell
et al., 2018) have been widely used in many
knowledge-intensive applications, including ques-
tion answering (Sun et al., 2020; Saxena et al.,
2020), dialogue systems (Yang et al., 2020; Zhou
et al., 2018) and recommender systems (Wang et al.,
2021, 2019a). However, the KGs constructed man-
ually or automatically are inevitably incomplete,
requiring KGC to infer new facts.Figure 1: Two examples exhibit the challenges that
needed to be addressed. Challenge 1 : Given a positive
triple, some generated negative triples are false-negative
or low-quality. Challenge 2 : For link prediction, the
entity California ranks higher than the correct entity
U.S.A. due to the uncertainty of KG embeddings. How-
ever, the correct answer entity should belong to the
concept Country in the view of commonsense.
The previous KGC models can be classified
into three main streams: (1) Rule learning-based
models mine logic rules for induction reason-
ing, such as AMIE+ (Galárraga et al., 2015),
DRUM (Sadeghian et al., 2019) and AnyBurl (Meil-
icke et al., 2019). (2) Path-based models (Liu et al.,
2020; Xiong et al., 2017; Lin et al., 2018) search
paths for multi-hop reasoning. (3) KGE models
such as TransE (Bordes et al., 2013) and its vari-
ants (Sun et al., 2019; Zhang et al., 2019a, 2020)
learn the embeddings of entities and relations to
score the plausibility of triples for link prediction.
Among all the existing KGC models, KGE
approaches achieve higher efficiency and better
performance. Specifically, the KGE-based KGC
pipeline can be divided into two stages: learning
knowledge graph (KG) embeddings at the training
and link prediction at the inference. Learning KG
embeddings relies on a basic procedure of negative
sampling (Li et al., 2021). Link prediction aims to2867infer the missing entity or relation in a triple via
ranking the candidate triples’ scores in virtue of the
learned KG embeddings.
However, the two separate stages both have draw-
backs: (1) Invalid negative sampling : all the
previous NS (Wang et al., 2014; Cai and Wang,
2018; Sun et al., 2019; Zhang et al., 2019b; De-
nis et al., 2015) cannot avoid sampling the false-
negative triples and low-quality negative triples,
simultaneously. For instance, given the positive
triple (Los Angeles, LocatedIn, California )
as shown in Figure 1, the existing NS strate-
gies might sample the corrupted triples such
as(San Francisco, LocatedIn, California ),
which is actually a missing correct triple namely
false-negative triple. On the other hand, the qual-
ity of some generated negative triples such as
(San Francisco, LocatedIn, Apple Pie )is too
poor to make little sense for training the KGE
models. (2) Uncertainty of fact-view link pre-
diction : performing link prediction solely based
on facts in a data-driven fashion suffers from un-
certainty due to the deviation of KG embeddings
compared to the symbolic representations, limit-
ing the accuracy of KGC. Take the tail entity pre-
diction (David, Nationality, ?)in Figure 1 as an
instance. The correct tail entity should belong to
the concept Country in the view of commonsense.
Whereas the entity California that is inconsistent
with commonsense even ranks highest via scoring
the candidate triples with KG embeddings.
Last but not least, although some KGE ap-
proaches exploit external information, including
entity types (Xie et al., 2016b), textual descrip-
tions (Xie et al., 2016a) and images of entities (Xie
et al., 2017). Such auxiliary information is hard to
access and enhances the single representation of
entities rather than providing the semantics of com-
monsense. However, the valuable commonsense
is always acquired by the expensive hand anno-
tation (Rajani et al., 2019), so its high cost leads
to relatively low coverage. Besides, the existing
large-scale commonsense KGs such as Concept-
Net (Speer et al., 2017) only contain the concepts
without the links to the corresponding entities, caus-
ing them unavailable to the KGC task.
To address the above challenges, we pro-
pose a novel and scalable Commonsense- Aware
Knowledge Embedding ( CAKE ) framework to im-
prove the NS in the training of KGE and boost
the performance of KGC benefited from the self-supervision of commonsense . In specific, we at-
tempt to automatically construct explicit common-
sense via an instance abstraction technique from
KGs. Then, contrary to random sampling, we pur-
posefully generate the high-quality negative triples
by taking advantage of the commonsense together
with the characteristics of complex relations. Fur-
thermore, a multi-view link prediction is conducted
to determine the entity candidates that belong to
the correct concepts in the commonsense view and
predict the answer entities with the learned KG em-
beddings from the perspective of fact. In summary,
the contributions of our work are three-fold:
•We propose a scalable KGC framework
with an automatic commonsense generation
mechanism to extract valuable commonsense
from factual triples and entity concepts.
•We develop a commonsense-aware negative
sampling strategy for generating valid and
high-quality negative triples. Meanwhile, a
multi-view link prediction mechanism is pro-
posed to improve the accuracy of KGC.
•Extensive experiments on four benchmark
datasets illustrate the effectiveness and the
scalability of our whole framework and each
module. We promise to release all the codes
and datasets when this paper is published.
2 Related Work
2.1 KGC Models
The existing KGC models can be classified into
three main categories: (1) Rule learning-based al-
gorithms such as AMIE+ (Galárraga et al., 2015),
DRUM (Sadeghian et al., 2019) and AnyBurl (Meil-
icke et al., 2019) automatically mine logic rules
from KGs and apply these rules for inductive link
prediction. However, these models are inefficient
due to the time-consuming rule searching and eval-
uation. (2) Path-based models search paths linking
head and tail entities, including path ranking ap-
proaches (Lao et al., 2011; Liu et al., 2020) and
reinforcement learning-based models (Xiong et al.,
2017; Lin et al., 2018). Whereas, multi-hop path-
based models also spend much time in path search-
ing. (3) KG embedding (KGE) models such as
TransE (Bordes et al., 2013), RESCAL (Nickel
et al., 2011), ComplEx (Trouillon et al., 2016), Ro-
tatE (Sun et al., 2019) and HAKE (Zhang et al.,28682020) learn the embeddings of entities and rela-
tions to score the plausibility of triples for predict-
ing the missing triples efficiently. KGE approaches
achieve higher efficiency and better performance on
KGC compared with the others. However, the nat-
ural uncertainty of embeddings limits the precision
of KGC relying solely on facts. More specifically,
the KGE models generally need a primary negative
sampling (NS) procedure to randomly or purposely
sample some triples that are not observed in the
KG as negative triples for training (Li et al., 2021).
2.2 Negative Sampling of KGE
Following the local closed-world assumption
(Dong et al., 2014), the existing NS techniques for
KGE can be classified into five categories: (1) Ran-
domly and uniformly sampling: the majority of the
KGE models generate negative triples via randomly
replacing an entity or relation in a positive triple
from a uniform distribution (Wang et al., 2014).
(2) Adversarial-based sampling: KBGAN (Cai and
Wang, 2018) integrates the KGE model with soft-
max probabilities to select the high-quality negative
triples in an adversarial training framework. Self-
adversarial sampling (Sun et al., 2019) performs
similar to KBGAN, but it utilizes a self-scoring
function without a generator. (3) Domain-based
sampling: domain-based NS (Wang et al., 2019b)
and type-constrained NS (Denis et al., 2015) both
leverage domain or type constraints on sampling
the corrupted entities that belong to the correct do-
main. (4) Efficient sampling: NSCaching (Zhang
et al., 2019b) employs cache containing candidates
of negative triples to improve the efficiency of sam-
pling. (5) None-sampling: NS-KGE (Li et al.,
2021) eliminates the NS procedure by converting
loss functions of KGE into a unified square loss.
However, all the previous NS algorithms can-
not address the issue of false-negative triples since
these NS techniques, except for none sampling,
would attempt to sample the corrupted triples with
higher probability while they might be correct and
just missing in the KG. Domain-based NS relies
heavily on the constraint of the single type rather
than the commonsense, limiting the diversity of
negative triples. KBGAN introduces generative
adversarial networks (GAN) in the NS framework,
making the original model more complex and hard
to train. None sampling has to convert each orig-
inal KGE model into square loss, which weakens
the performance of KGE models. These drawbacksof the NS strategies degrade the training of KGE
and further limit the performance of KGC.
2.3 Commonsense Knowledge Graph
Different from the factual triples, commonsense
could inject rich abstract knowledge into KGs.
However, the valuable commonsense is hard to
access due to the costly hand annotation. In recent
years, many researches attempt to construct general
commonsense graphs such as ConceptNet (Speer
et al., 2017), Microsoft Concept Graph (Ji et al.,
2019) and ATOMIC (Sap et al., 2019). However,
these commonsense graphs only contain the con-
cepts without the links to the corresponding enti-
ties, causing them inapplicable to the KGC task.
On the other hand, although some KGE models
such as JOIE (Hao et al., 2019) employ the ontol-
ogy built-in most of the KGs, i.e., NELL (Mitchell
et al., 2018) and DBpedia (Lehmann et al., 2015),
the relations in ontology such as isA,partOf and
relatedTo mainly represent the type hierarchy but
not the explicit commonsense. Such relations are
useless for KGC because there are few overlaps
between the ontological and the factual relations.
3 Methodology
In this section, we introduce our novel and scalable
CAKE framework. As shown in Figure 2, the entire
pipeline consists of three developed modules: the
automatic commonsense generation (ACG) mod-
ule, the commonsense-aware negative sampling
(CANS) module and the multi-view link predic-
tion (MVLP) module. Firstly, the ACG module
extracts the commonsense from the factual triples
with the entity concepts via an instance abstrac-
tion mechanism (§ 3.2). Then, the CANS module
employs the generated commonsense to produce
the high-quality negative triples, which takes the
characteristics of complex relations into account
(§ 3.3). Afterwards, our approach feeds the posi-
tive and the weighted negative triples into the KGE
model for learning entity and relation embeddings
(§ 3.4). Finally, the MVLP module conducts link
prediction in a coarse-to-fine fashion by filtering
the candidates in the view of commonsense and
predicting the answer entities with KG embeddings
from the candidates in the view of fact (§ 3.5).
3.1 Notations and Problem Formalization
Commonsense. Commonsense has gained
widespread attraction from its successful use in2869
understanding high-level semantics, which is gener-
ally represented as the concepts with their ontolog-
ical relations in some well-known commonsense
graphs such as ConceptNet (Speer et al., 2017) and
Microsoft Concept Graph (Ji et al., 2019). Notably,
we extend the commonsense in two forms: the in-
dividual form Cand the set form C. Both Cand
Care the sets of triples while each triple in Cis
constituted of a head entity’s concept cand a tail
entity’s concept cassociated with their instance-
level relation r, which can be written as follows:
C={(c, r, c)} (1)
On the contrary, each triple in Cconsists of a
relation rlinking the corresponding head concept
setCand tail concept set C, which is shown as:
C={(C, r, C)} (2)
The detailed description of commonsense gener-
ation is introduced in section 3.2.
KGE Score Function. We could leverage any
KGE model to learn the entity and relation embed-
dings owing to our scalable framework indepen-
dent of the KGE model. Thus, we define a uniform
symbol E(h, r, t )to represent the score function of
any KEG model for evaluating the plausibility of
a triple (h, r, t ). More specifically, the three most
typical score function patterns are given as follows:
(1) The translation -based score function, such
as TransE (Bordes et al., 2013):
E(h, r, t ) =∥h+r−t∥ (3)
where h,randtdenote the embeddings of head
entity h, relation rand tail entity t, respectively.
(2) The rotation -based score function, such as
RotatE (Sun et al., 2019):
E(h, r, t ) =∥h◦r−t∥ (4)where ◦indicates the hardmard product.
(3) The tensor decomposition -based score func-
tion, such as DistMult (Yang et al., 2015):
E(h, r, t ) =hdiag(M)t (5)
where diag(M)represents the diagonal matrix of
the relation r.
Link Prediction. Following most of the previous
KGC models, we regard link prediction as an entity
prediction task. Given a triple query with an entity
missing (h, r,?)or(?, r, t), link prediction takes
every entity as a candidate. It calculates the score
of each candidate triple by employing the learned
KG embeddings and the score function. Then, we
rank the candidate entities in light of their scores
and output the top n entities as results.
3.2 Automatic Commonsense Generation
In terms of the representation of commonsense
defined in section 3.1, our approach could theo-
retically generate commonsense from any KG au-
tomatically as long as there exist some concepts
linked to the entities in the KG. Specifically, we
develop an entity-to-concept converter to replace
the entities in each factual triple with correspond-
ing concepts. Meanwhile, the relations in com-
monsense entail the instance-level relations in fac-
tual KGs. Take an instance in Figure 2, the fac-
tual triple (David, Nationality, U.S.A. )can be
transformed to a concept-level triple (Person,
Nationality, Country ). Particularly, the com-
monsense in the individual form Cis achieved by
wiping out the reduplicated concept-level triples.
Afterwards, we merge the concept-level triples that
contain the same relation into a set to construct the
commonsense in the set form C.2870
3.3 Commonsense-Aware Negative Sampling
Intuitively, the negative triples satisfying common-
sense are more challenging to distinguish from pos-
itive triples, contributing to more effective training
signals. Therefore, we try to sample the negative
triples that conform to the commonsense.
To reduce the false-negative triples, we exploit
the characteristics of complex relations, namely
1-1, 1-N, N-1, and N-N defined in TransH (Wang
et al., 2014) for negative sampling, where 1implies
that the entity is unique when given the relation and
another entity, on the contrary, Ndenotes that there
might be multiple entities in this case (non-unique
entity). Based on this observation, two specific
sampling strategies are proposed: (1) uniqueness
sampling : in terms of corrupting a unique entity
such as the tail entity of the N-1 relation, the cor-
rupted triples except for the original positive one
are definitely actual negative triples. Furthermore,
the corrupted entities that share at least one concept
with the correct entity are regarded as high-quality
negative triples, contributing to a more consistent
training signal. (2) None-unique sampling : for
corrupting a non-unique entity such as a head entity
linked by the N-1 relation, the entities belonging
to the same concept(s) with the correct entity are
more likely to be false-negative due to the non-
uniqueness of the head entity. Thus, the weights of
these negative triples being false-negative should
be as low as possible in training. Meanwhile, wetry to sample the triples conforming to the com-
monsense Cfor high quality.
For a better understanding, an example of gener-
ating high-quality negative triples with an N-1 rela-
tion is shown in Figure 3. The whole NS procedure
can be divided into two steps. Step 1 : selecting the
candidate concepts with commonsense C. The can-
didate head concepts city,county andisland are
determined according to commonsense Cand non-
unique sampling. Besides, based on the uniqueness
sampling strategy, the candidate tail concept is se-
lected as the same concept stateprovince as that
ofGeorgia .Step 2 : attentive concept-to-entity
converting. To reduce false-negative while ensur-
ing the high quality of the negative triples, the cor-
rupted entities belonging to the candidate concepts
are sampled from the following distribution:
w(h, r, t) = 1−p((h, r, t)|{(h, r, t)})
= 1−expαE(h, r, t)PexpαE(h, r, t)(6)
w(h, r, t) =p((h, r, t)|{(h, r, t)})
=expαE(h, r, t)PexpαE(h, r, t)(7)
where handtare the corrupted head and tail en-
tities obtained by non-unique sampling and unique-
ness sampling. wandpdenote the weight and the
probability of the negative triple, respectively. αis
the temperature of sampling motivated by the self-
adversarial sampling (Sun et al., 2019). Remark-
ably, considering that a triple with a higher proba-
bility is more likely to be a positive one, the weight
of a negative triple containing the corrupted head
entity such is defined as Eq. 6 to prevent the issue
of false-negative. Besides, the negative triples con-
taining the corrupted tail entities with higher proba-
bility are endowed with higher-quality weight since
there is no false-negative issue. Thus, both the cor-
rupted head entity greenland and the corrupted
tail entity tennessee with the high weights are
selected to generate high-quality negative triples.
Similarly, we can generate the high-quality neg-
ative triples with an 1-N relation via uniqueness
sampling for head entity and none-uniqueness sam-
pling for tail entity. Besides, the negative triples
with a 1-1 relation can be obtained by only con-
ducting the uniqueness sampling for both head and
tail entities. On the contrary, only none-uniqueness
sampling is required for generating negative triples
with an N-N relation.28713.4 Traning the KGE Model
Based on the negative triples obtained by CANS,
we train the KGE model to learn the entity and
relation embeddings for enlarging the gap between
the scores of the positive and high-quality negative
triples. In this work, we employ the following loss
function as our optimization objective:
in which γis the margin. σis the sigmoid function.
3.5 Multi-View Link Prediction
Benefiting from the same relations among com-
monsense and facts, commonsense could directly
provide a definite range for link prediction results.
Hence we develop a novel multi-view link pre-
diction (MVLK) mechanism in a coarse-to-fine
paradigm to facilitate more likely predicted results.
Firstly, at the coarse prediction stage, we pick out
the candidate entities in the view of commonsense.
Specifically, take a query (h, r,?)for an example,
commonsense Cis employed for filtering the rea-
sonable concepts of the tail entity. The candidate
concept set of tail entity is defined as
C={c|(c, r, c)∈ C} (9)
where cis the i-th concept of h, and cdenotes
the tail concept in the commonsense (c, r, c).
Then, the entities belonging to the concept set C
can be determined as the candidate entities since
they satisfy commonsense and are more likely to
be the correct tail entities from the perspective of
commonsense compared with other entities.
Then, at the fine prediction stage, we score each
candidate entity ederived from the coarse predic-
tion stage in the view of fact as following
score (e) =E(h, r, e) (10)
where E(h, r, e)denotes the score function em-
ployed for training the KGE model. Subsequently,
the prediction results will rank the scores of can-
didate entities in ascending order and output the
entities with higher ranks.
4 Experiments and Results
In this section, we perform extensive experiments
of KGC on four widely-used KG datasets contain-
ing concepts. We firstly describe datasets, base-
line models, implementation details and evaluation
protocol. Then, the effectiveness of our proposed
framework CAKE and each module is demon-
strated by compared with several baselines. Fur-
thermore, we conduct extensive experiments, in-
cluding the ablation study and the case study.
4.1 Experiment Settings
Datasets. Four real-world datasets contain-
ing ontological concepts are utilized for exper-
iments, including FB15K (Bordes et al., 2013),
FB15K237 (Toutanova and Chen, 2015), NELL-
995 (Xiong et al., 2017) and DBpedia-242. Par-
ticularly, DBpedia-242 is extracted from DBpe-
dia (Lehmann et al., 2015) which contains totally
242 concepts. The statistics of the datasets are
summarized in Table 1. Notably, the entities in
FB15K and FB15K237 always belong to more than
one concept while each entity in NELL-995 and
DBpedia-242 has only one concept.
Baselines. We compare our CAKE model
with three state-of-the-art KGE models, including
TransE (Bordes et al., 2013), RotatE (Sun et al.,
2019) and HAKE (Zhang et al., 2020). Meanwhile,
these baselines are also the basic models integrated
with our framework. It is unnecessary to use many
baselines since the focus of this work is to observe
the impact of applying our CAKE framework to
original KGE models instead of defeating all the
SOTA models. We provide the results of baselines
by running their source codeswith the suggested
parameters. Note that all the existing type-based
and ontology-based models are not chosen as base-
lines since they are specific to a few KGs and can-
not work on most of the datasets in our experiment.
Implementation Details. Each complex relation
is labelled in the same way as in TransH (Wang
et al., 2014). We use Adam optimizer for the train-
ing and tune the hyper-parameters of our model by
grid search on the validation sets. Specifically, the2872
embedding size and the batch size are the same as
those of each basic model for a fair comparison.
The negative sampling size is set as 16 for all the
models considering the memory limitation and ef-
ficiency. The learning rate is chosen from 0.0001
to 0.01. The margin is tuned in {9, 12, 18, 24, 30}.
The sampling temperature is adjusted in {0.5, 1.0}.
The entity and relation embeddings are initialized
randomly. All the experiments are conducted in
Pytorch and on GeForce GTX 2080Ti GPUs.
Evaluation Protocol. Following the procedure
of MVLP in Section 3.5, we can obtain the rank
of the correct entity for each test example. Then,
the performance of link prediction is evaluated by
three commonly-used metrics: mean rank (MR),
mean reciprocal rank (MRR), and proportion of the
correct entities ranked in the top n (Hits@N). All
the metrics are in the filtered setting by wiping out
the candidate triples already exist in the datasets.
4.2 Experimental Results
Table 2 exhibits the evaluation results of link pre-
diction on the four datasets. We can observe that
both CANS and MVLP modules effectively im-
prove the performance of each basic model on each
dataset. Moreover, the entire CAKE framework fur-
ther facilitates more performance gains than each
separate module and outperforms all the baselines
consistently and significantly. Compared with the
performance average of the three baseline models,
our CAKE model improves MRR by 7.2%,11.5%,
16.2% and 16.7% on FB15K, FB15K237, DBpedia-
242 and NELL-995. These results demonstrate the
superiority and effectiveness of integrating com-
monsense with the original KGE models.
We compare our CANS module with various
types of NS techniques, including uniform sam-
pling (Bordes et al., 2013), none sampling (Li
et al., 2021), NSCaching (Zhang et al., 2019b),
domain-based sampling (Wang et al., 2019b) and2873
self-adversarial sampling (Sun et al., 2019). The
comparison results are obtained by combining
these NS techniqueswith the most classical KGE
model TransE(Bordes et al., 2013). From the re-
sults shown in Table 3, our CANS module signifi-
cantly outperforms all the other NS techniques on
all the datasets. Specifically, domain-based NS,
self-adversarial sampling and our CANS module
consistently outperform the others due to the con-
sideration of the quality of negative triples. Fur-
thermore, our CANS module performs better than
domain-based NS and self-adversarial sampling
since CANS could reduce false-negative. These re-
sults illustrate the superior ability of our CANS
module to generate more high-quality negative
triples for enhancing the performance of any KGE
model.
4.3 Ablation Study
We verify the effectiveness of each contribution
via integrating the whole framework CAKE and
the following ablated models into the basic model
HAKE: (1) neglecting the characteristics of com-
plex relations in CANS (-CRNS), (2) removing the
commonsense in CANS while retaining the char-
acteristics of complex relations (-CSNS), and (3)
omitting the commonsense-view prediction from
MVLP (-MVLP). The results in Table 4 demon-
strate that our whole model CAKE performs better
than all the ablated models on each dataset. It illus-
trates that introducing commonsense and the char-
acteristics of complex relations both make sense
in the NS process for generating more effective
negative triples. Besides, MVLP facilitates link
prediction performance benefited from determining
the reasonable candidate entities by prior common-
sense. In general, each contribution plays a pivotal
role in our approach.
4.4 Case Study
We provide the case study of explainable link
prediction with commonsense as shown in Fig-
ure 4. Given a query with the tail entity miss-
ing(rockets, teamplaysinleague, ?)on NELL-
995, our model could output the answer enti-
ties and provide the corresponding entity con-
cepts together with the commonsense specific
to the query. We can observe that all the top-28745 entities including the correct entity nba be-
long to the concept sportsleague which satisfies
the commonsense (rockets, teamplaysinleague,
sportsleague ). More interestingly, the common-
sense and the entity concepts could explain the ra-
tionality of the predicted answer entities to enhance
the users’ credibility of the answers.
5 Conclusion
In this paper, we propose a novel and scalable
commonsense-aware knowledge embedding frame-
work, which could automatically generate common-
sense from KGs with entity concepts for the KGC
task. We exploit the generated commonsense to
produce effective and high-quality negative triples.
On the other hand, we design a multi-view link
prediction technique in a coarse-to-fine paradigm
to filter the candidate entities in the view of com-
monsense and output the predicted results from
the perspective of fact. The experiments on four
datasets demonstrate the effectiveness and the scal-
ability of our proposed framework and each module
compared with the state-of-the-art baselines. Fur-
thermore, our framework could explain link pre-
diction results and potentially assemble new KGE
models to improve their performance.
Acknowledgements
This work was partially supported by Zhe-
jiang Science and Technology Plan Project (No.
2022C01082), the National Natural Science Foun-
dation of China (No. 62072022, 61772054) and
the Fundamental Research Funds for the Central
Universities.
References287528762877