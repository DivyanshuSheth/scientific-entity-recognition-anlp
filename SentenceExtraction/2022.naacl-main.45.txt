
Minghao Zhu and Junli Wang and Chungang Yan
Key Lab of Embedded System and Service Computing (Tongji University),
Ministry of Education, Shanghai 201804, China.
National (Province-Ministry Joint) Collaborative Innovation Center for
Financial Network Security, Tongji University, Shanghai 201804, China.
{mzhu, junliwang, yanchungang}@tongji.edu.cn
Abstract
Variational Autoencoder (V AE) is an effective
framework to model the interdependency for
non-autoregressive neural machine translation
(NAT). One of the prominent V AE-based NAT
frameworks, LaNMT, achieves great improve-
ments to vanilla models, but still suffers from
two main issues which lower down the transla-
tion quality: (1) mismatch between training and
inference circumstances and (2) inadequacy of
latent representations. In this work, we target
on addressing these issues by proposing pos-
terior consistency regularization. Specifically,
we first perform stochastic data augmentation
on the input samples to better adapt the model
for inference circumstance, and then conduct
consistency training on posterior latent vari-
ables to construct a more robust latent repre-
sentations without any expansion on latent size.
Experiments on En<->De and En<->Ro bench-
marks confirm the effectiveness of our methods
with about 1.5/0.7 and 0.8/0.3 BLEU points
improvement to the baseline model with about
12.6×faster than autoregressive Transformer.
1 Introduction
Neural Machine Translation (NMT) achieves great
success in recent years, and typical sequence-to-
sequence frameworks like Transformer (Vaswani
et al., 2017) achieved state-of-the-art performance
on the task of NMT. In this framework, source
sentences are translated in an autoregressive (AT)
manner where each token is generated depend-
ing on previously generated tokens. Inevitably,
such sequential decoding strategy results in a high
inference latency. To alleviate this issue, Non-
autoregressive translation (NAT; Gu et al., 2018)
was proposed to speed-up decoding procedure by
generating target tokens in parallel. However,
the translation quality of vanilla NAT is compro-
mised, one of the most significant problems is
multi-modality and it usually results in multiple
translation results, duplicate or missing words intarget sentences of NAT models (Gu et al., 2018).
This situation results from the conditional indepen-
dence proposed by NAT, since models are trained
to maximize the log-probability of target tokens at
each position while the interdependency is omitted.
The key to alleviate the multi-modality issue is
to model the dependency information of targets
implicitly or explicitly so decoder can easily learn
and capture the information between target tokens
and generate more accurate translations. For exam-
ple, Ghazvininejad et al. (2019) and (2020b) model
the target dependency by providing observed tar-
get tokens in training and performing iterative in-
ference. Ran et al. (2021) generates intermediate
representations by permuting the source sentences
in the target order. Libovick `y and Helcl (2018),
Shao et al. (2020) and Ghazvininejad et al. (2020a)
model the target dependency by introducing objec-
tive functions with alignment. Guo et al. (2020) and
Wang et al. (2019) improve the translation quality
by proposing additional regularizations. Zhou and
Keung (2020) and Liu et al. (2021) utilize the exter-
nal information like monolingual data or semantic
structure to help the training.
Previous studies have validated the effective-
ness of applying V AE on AT (Zhang et al. 2016;
McCarthy et al. 2019; Su et al. 2018) and NAT
(Kaiser et al. 2018; Shu et al. 2020) frameworks.
A prominent NAT model is LaNMT(Shu et al.,
2020) which encodes the source and target tokens
into intermediate Gaussian distribution latent vari-
ables and outperforms vanilla NAT with about 5.0
BLEU points on WMT14 En-De task with 12.5×
speedup to base Transformer. However, there exists
a slight lag behind the state-of-the-art fully NAT
models. It may be attributed to two reasons: (1)
The inadequate representations of latent variables.
Figure 1 shows the effect of latent size to translation
qualities. Obviously, the optimal capacity of latent
variables is significantly lower than the model’s hid-607
den size (512) while high-capacity latent variables
conversely deteriorate the performance because the
minimization between prior and posterior becomes
difficult (Shu et al., 2020). (2) The mismatch be-
tween training and inference circumstances that the
posterior module receives the gold sentence as in-
puts during training but imperfect initial translation
instead during inference. Thus, in this paper, we
aim to improve the robustness of the latent repre-
sentation and move the training circumstance close
to inference circumstance.
To this end, we propose a consistency regular-
ized posterior network for better latent represen-
tations and closer training-inference circumstance
with no extended latent size. Specifically, it can be
split into two main steps: we first apply stochas-
tic data augmentation methods to inject stochastic
noises in posterior inputs xandyto get two dif-
ferent views (x, y)and(x, y), which are then
transformed to corresponding latent variables z,
zby the posterior network. Secondly, the con-
sistency regularization step tries to minimise the
gap between zandzsince they are derived from
the same pair of input xandy. These two steps
enable the sequences better represented by the size-
limited latent variables which is trained to be more
robust to the noises in input samples. Meanwhile,
posterior module receives noisy views instead of
gold samples during training, it is more adaptive to
the inference time which receives imperfect initial
translations as inputs.
We verified the performance and effective-
ness of our methods on WMT14 En<->De and
WMT16 En<->Ro benchmarks. Our methods out-
perform the latent variable baseline with about1.5/0.7 and 0.8/0.3 BLEU points improvement on
four benchmarks. With these improvements, we
achieve the comparable performance to the state-
of-the-art fully NAT approaches: 25.65/30.23 and
31.56/31.20 of BLEU scores with similar decoding
speed, and it can be improved further with latent
search. The contributions of our work can be sum-
marized as follows:
•For better latent representations, we propose
consistency regularization optimized posterior
network, which improves the translation qual-
ity by training more robust latent variables to
noises.
•We apply four data augmentation methods to
cooperate with posterior consistency regular-
ization, where all of them are also benefit to
the translation quality by alleviating the mis-
match between training and inference circum-
stances.
•We show our strategy is capable of improv-
ing the translation quality of the base latent-
variable NAT model to be comparable with
the state-of-the-art fully NAT frameworks.
2 Background
2.1 Non-Autoregressive Translation
Traditional sequence-to-sequence NMT models
generate target sentences in an autoregressive man-
ner. Specifically, given a source sentence x, AT
frameworks model the conditional probability of
y={y, y,···, y}by the following form:
logp(y|x) =/summationdisplaylogp(y|y, x) (1)
where yindicates the target tokens already gener-
ated before y. Hence, the target tokens are gener-
ated sequentially which results in a high decoding
latency. To alleviate this issue, vanilla NAT (Gu
et al., 2018) breaks the conditional dependency by
conditional independence assumption so that all
tokens can be generated independently. Following
its probability form:
logp(y|x) =/summationdisplaylogp(y|x) (2)
where each target token ynow only depends on
the source sentence x. Benefit from the parallel608computing capability of hardware accelerators like
GPU or TPU, all tokens can be generated with one
iteration in an ideal circumstance.
2.2 Latent-Variable Model
We mainly focus on performing optimization on the
variational NAT framework proposed by Shu et al.
(2020). The network architecture is constructed
by four main components. An encoder p(z|x)
encodes the source representation of input xand
computes the prior latent variable. An approxi-
mate posterior network q(z|x, y)accepts both the
source sentence xand target sentence yas the in-
put and computes the posterior latent variable. A
length predictor p(l|z)predicts the length of tar-
get sentence y, and finally a decoder p(y|x, z, l)
with a length transform module to transform the
latent variables zto the target length lat first and
reconstruct yfrom zwith the source representa-
tions of x. Note that the lhere is the gold length
in training. Hence, the training objective is aiming
to maximize the evidence lowerbound (ELBO):
L(x, y) =E[logp(y|x, z)]
−KL [q(z|x, y)||p(z|x)]
p(y|x, z) =p(y|x, z, l)p(l|z)(3)
where the latent variables zis constrained with
the same length as xand the value is modeled
as spherical Gaussian distribution. KLdenotes
Kullback-Leibler divergence.
2.3 Consistency Regularization
Consistency regularization is considered as an ef-
fective method on semi-supervised learning to cap-
ture the potential features from unlabeled samples
(Sajjadi et al., 2016; Laine and Aila, 2017; Tar-
vainen and Valpola, 2017; Xie et al., 2020). It
is also utilized as a complementary regularization
tool with other regularization methods to prevent
model from overfitting (Liang et al., 2021). In a
nutshell, consistency regularization assumes a well
trained model should be robust enough to any small
changes in the input samples or hidden states and
generate invariant outputs (Xie et al., 2020). To
this end, it regularizes model’s final outputs to be
invariant to input samples with small stochastic
noises injected by minimizing the gap between two
augmented views of one sample.
In this paper, we focus on the posterior module
of the variational framework and apply consistency
regularization on it instead of the whole network.
Along with data augmentation for noise injection,
consistency regularization is capable to improve the
representation of this module and result in better
translation quality.
3 Approach
The posterior module is considered to train with
consistency regularization and data augmentation
for better translation quality. In this section, we
will introduce the details of our method, including
the overall network architecture, the objective and
procedure of training with consistency regulariza-
tion, four data augmentation methods and three
decoding strategies applied for inference.
3.1 Model Architecture
We follow the variational model architecture pro-
posed by Shu et al. (2020) with four main com-
ponents: encoder, posterior, length predictor and
decoder module. Since we apply consistency reg-
ularization on the posterior, an additional stochas-
tic data augmentation module is added for noise
injection on posterior input samples. With two aug-
mented views derived from one sample, each sam-609ple thus appears twice in a training batch. Figure
2 shows the brief model architecture and training
pipeline of our work. The part in the dashed box is
the major difference to the base model.
3.2 Posterior Consistency Regularization
As discussed above, consistency regularization is
applied on the posterior module to improve its ro-
bustness. Given a training sample with a pair of
source sentence xwithltokens and target sen-
tence ywith ltokens, we first apply data aug-
mentation on both xandytwice to inject stochas-
tic noises and obtain two different views (denoted
asx, yandx, y). Both views are forwarded
to the posterior network q(z|x, y)to predict the
mean and variance vectors of two latent variables
zandz. Since the latent variables derive from the
same input sample, the consistency regularization
method tries to minimize the difference between
these two latent variables by measuring bidirec-
tional KL-divergence as follows:
L=1
2(KL(z||z) + KL( z||z)),
z=q(Z|X=x, Y=y),
z=q(Z|X=x, Y=y)(4)
Combining with the basic negative log-likelihood
(NLL) objective on the decoder, since there are two
different zfor the same sample, it is evaluated by
averaging them:
L=−1
2(logp(y|x, z, l) + log p(y|x, z, l))
(5)
Note that the gold length lof target sentence y is
used which is known during training. Similarly, the
objective of the length predictor is calculated by:
L=−1
2(logp(l|z) + log p(l|z)) (6)
To back propagate the gradient information from
the decoder and length predictor to posterior, repa-
rameterization trick is applied to sample zfrom
qwhere z=µ+θ∗ N(0,1)in Eq. (6)and(5).
Here, µandθindicate mean and variance vector.
For encoder, it not only generates representations
of source sentence xbut also computes the prior
latent variables. Thus, we close the KL-divergence
between prior and two posterior latent variables by:
L =1
2(KL(z||z) + KL( z||z)) (7)where z=p(Z|X=x),zandzare obtained
from (4). Finally, to achieve the similar goal of
maximizing (3), we minimize the loss function by
combining (4), (5), (6) and (7) as follows:
L=L+L+L+αL (8)
where αhere is the only hyperparameter to weight
the consistency regularization loss.
3.3 Data Augmentation Methods
Given an embedding matrix RwithLtokens
embedded into d-dimensions vectors, to generate
different views for the posterior network inputs and
perform posterior consistency regularization, as
well as to close the gap between training and infer-
ence circumstances, we explore four data augmen-
tation methods for this purpose including dropout,
feature cutoff, token cutoff and replacement as pre-
sented in Figure 3.
Dropout Dropout (Srivastava et al., 2014) is
widely used as a regularization method to prevent
neural networks from overfitting. But in this paper,
we found that it is also an effective data augmen-
tation method for noise injection. Specifically, we
randomly choose values on token embeddings by a
specific proportion and force them to zero.
Cutoff This is a simple but effective augmenta-
tion method proposed by Shen et al. (2020). The
cutoff methods we adopt include token cutoff and
feature cutoff. For token cutoff, a specific propor-
tion of tokens are chosen from the token dimension
Land dropped by setting the vectors to zero. For
feature cutoff, the dropped values are chosen from
feature dimension dinstead.
Replacement This is similar to the token replace-
ment adopted by BERT pre-training (Devlin et al.,
2019) where the chosen token vectors are replaced
by the embedding of new tokens that randomly se-
lected from the vocabulary instead of setting them
to zero or any special tokens directly.
3.4 Decoding Strategies
Non-refinement For this strategy, we completely
follow the original design (Shu et al., 2020) where
the posterior network is discarded since the target
sentence yis unknown during inference. The fore-
most step is to obtain the representations of xand
the prior latent variable zfrom encoder with source
input x. The latent variable is then used to deter-
mine the target length and generate target sentence.610
Note that to avoid randomness during inference, z
is set to its mean value µinstead of reparameteriza-
tion sampling. This can be summarized as follows:
µ=E[z],
l= arg maxp(l|z=µ),
y= arg maxp(y|x, l, z=µ)(9)
Deterministic Refinement The posterior net-
work qcan be reused to take refinement on the
initial output yabove. According to Shu et al.
(2020), its original design allows multi-step itera-
tive refinement for more precise translations, but
sacrifices huge cost in decoding speed for a tiny
quality improvement. Thus, we consider one-step
only refinement in this paper:
µ=E[z],
l= arg maxp(l|z=µ),
y= arg maxp(y|x, l, z=µ)(10)
Here the yis the final output after refinement.
Latent Search Since reparameterization is dis-
abled in above two strategies to generate determin-
istic results, it is also able to search the best latent
variable from Gaussian distribution. Specifically,
mprior latent variables are sampled by reparam-
eterization and decoded in parallel, result in m
target candidates for each source sentence. To get
the best result, we select the candidate with the
highest score by averaging the log-probability of
tokens as the final output. This is different from
Shu et al. (2020) or Noisy Parallel Decoding (NPD;Gu et al. 2018) which rescore the candidates by
autoregressive teacher and at least cuts the decod-
ing speed by half, our no-rescoring strategy is still
effective and much faster.
4 Experiments
In this section, we will introduce the settings of
our experiments, report the main results and com-
pare our model to the representative NAT frame-
works. Our experiments mainly focus on (1) the
improvement benefit from our optimization to for-
mer V AE-based NAT model. (2) The effectiveness
of consistency regularization and different data aug-
mentation methods.
4.1 Experimental Setup
Dataset Four of the commonly used machine
translation benchmarks are adopted to evaluate our
proposed method: WMT14 English<->German
(En-De and De-En, 4.5M) and WMT16 English<-
>Romanian(En-Ro and Ro-En, 610K). We follow
previous works’ data preprocessing configurations
to preprocess the data (En-De: Shu et al., 2020, En-
Ro: Ghazvininejad et al. 2019). To learn the sub-
word vocabulary, we apply SentencePiece (Kudo
and Richardson, 2018) to generate joint subword
vocabulary of 32K tokens for each dataset respec-
tively.
Knowledge Distillation Following previous stud-
ies on NAT that models are trained on distilled data
generated by autoregressive teacher, we also ap-
ply sentence-level knowledge distillation for all
datasets to obtain less noisy and more deterministic
data. In this work, Transformer (Vaswani et al.,611Models Iter.WMT14
En-DeWMT14
De-EnWMT16
En-RoWMT16
Ro-EnSpeed
Transformer (Vaswani et al., 2017) N 27.30 / / / /
Transformer (ours) N 27.7431.2833.7334.381.0×
NAT-IR (Lee et al., 2018) 10 21.61 25.48 29.32 30.19 1.5×
CMLM (Ghazvininejad et al., 2019) 10 27.03 30.53 33.08 33.31 1.7×
LevT (Gu et al., 2019) Adv. 27.27 / / 33.26 4.0×
JM-NAT (Guo et al., 2020) 10 27.69 32.24 33.52 33.72 5.7×
Vanilla-NAT (Gu et al., 2018) 1 17.69 21.47 27.29 29.06 15.6×
Imitate-NAT (Wei et al., 2019) 1 22.44 25.67 28.61 28.90 18.6×
FlowSeq (Ma et al., 2019) 1 23.72 28.39 29.73 30.72 1.1×
NAT-DCRF (Sun et al., 2019) 1 23.44 27.22 / / 10.4×
NAT-REG (Wang et al., 2019) 1 20.65 24.77 / / 27.6×
BoN (Shao et al., 2020) 1 20.90 24.61 28.31 29.29 10.7×
AXE (Ghazvininejad et al., 2020a) 1 23.53 27.90 30.75 31.54 /
GLAT (Qian et al., 2021) 1 25.21 29.84 31.19 32.04 15.3×
Reorder-NAT (Ran et al., 2021) 1 22.79 27.28 29.30 29.50 16.1×
SNAT (Liu et al., 2021) 1 24.64 28.42 32.87 32.21 22.6×
LT (Kaiser et al., 2018) / 19.80 / / / 3.8×
LaNMT (Shu et al., 2020) 1 22.20 26.7629.2128.8922.2×
+ refinement 2 24.10 29.4730.7630.8612.5×
+ latent search w/ rescoring 2 25.10 / / / 6.8×
Ours, decode w/o refinement 1 23.92 27.39 29.90 29.04 25.6×
+ latent search (m=9) w/o rescoring 1 25.59 30.11 31.40 31.63 21.1×
decode w/ refinement 2 25.65 30.23 31.56 31.20 12.6×
+ latent search (m=9) w/o rescoring 2 26.23 31.23 32.50 32.14 11.0×
2017) with base settings is adopted and reproduced
as the teacher model for data distillation.
Implementation Details The model is trained
by the objective function illustrated on Eq. (8). To
avoid posterior collapse, freebits annealing (Chen
et al., 2017) is applied on KL terms in Eq. (7)to
keep a distance between prior and posterior. Its
threshold is fixed to 1 for the first half training
steps, and linearly decay to 0 on the second half.
For both dataset, we train the model with a batch
of approximate 40K tokens for overall 100K steps
on four Tesla V100 GPUs and conduct fine-tuning
for additional 20K steps with annealing disabled.
For network settings, we use 6 layers en-
coder and decoder with d/d =
512/2048 . Following Shu et al. (2020), the pos-
terior network contains 3 transformer layers and
the dimension of latent variable is set to 8. Weset L2 weight decay of 0.01 and dropout between
attention layers with rate of 0.1/0.3 for WMT14
En<->De and WMT16 En<->Ro respectively as
well as label smoothing rate ϵ= 0.1on target to-
kens. Models are trained by AdamW (Loshchilov
and Hutter, 2017) with settings of β= (0.9,0.98)
andϵ= 1e−4. The learning rate is warmed up for
first 4000 steps to 1.4e-3 and decayed by inverse-
square-root scheduler. To obtain the final model,
we average 5 best checkpoints chosen by valida-
tion BLEU score. For the rate of noise injection,
all four augmentation strategies are evaluated with
rates from 0.1 to 0.3 on WMT14 En-De bench-
mark and the settings for other experiments are
determined according to these results.
Evaluation For all benchmarks, we use sacre-
BLEU(Post, 2018) to evaluate BLEU score of612translation results. Following Lee et al. (2018) and
Shu et al. (2020), repetition tokens are removed
before generating the final outputs for evaluation.
The results of latent search is obtained by the mean
score of 5 independent runs on the test set of each
benchmark to get more precise measures since repa-
rameterization causes randomness in decoding.
To evaluate the decoding speed, following pre-
vious works (e.g. Gu et al. 2018, Lee et al. 2018),
models are run on WMT14 En-De test set with
batch size of 1 under the environment with one
GPU only. The mean value of decoding latency
among all samples is collected and represent as the
decoding speed. Meanwhile, base Transformer is
reproduced and evaluated on the same machine to
obtain the speed up rates.
Baselines We set former V AE based NAT frame-
works proposed by Kaiser et al. (2018) and Shu
et al. (2020) as the main baselines to present the
improvement of our method. We also compare
our model with base Transformer and other repre-
sentative NAT frameworks including iterative and
fully approaches. Due to the lack of computing re-
sources, experiments and comparison with connec-
tionist temporal classification (CTC) based frame-
works like Libovick `y and Helcl (2018) or Gu and
Kong (2021) are not considered in this paper since
the decoder output is many times the length of the
target sequence, which extremely consumes extra
GPU storage in training. The performance mea-
sures including BLEU score and speedup rate of
other models are directly obtained from the fig-
ures reported on their original paper, while some
unreported measures are obtained by our imple-
mentation.
4.2 Results and Analysis
The main results on the benchmarks are illus-
trated on Table 1, we report the best scores of
our experiments among different tested combina-
tions of data augmentation methods with consis-
tency regularization. As the performance measure
shown in Table 1, our methods significantly outper-
form former V AE-based baselines, with about 5.8
BLEU points improvement to the discrete latent
variable model (Kaiser et al., 2018) and 1.7/1.5,
0.6/0.7, 0.7/0.8, 0.1/0.3 points improvement on
non-refinement/refinement decoding to continu-
ous latent variable baseline (Shu et al., 2020) on
WMT14 En<->De, and WMT16 En<->Ro bench-
marks without latent search. All measures indi-cate that our posterior consistency regularization
method greatly enhances the robustness of the la-
tent representations and results in an improved
translation quality.
Comparing to other representative AT and NAT
models, our method shows the superiority of decod-
ing speed to AT and iterative NAT models while
there are less than 2 BLEU points lag behind. With
the refinement decoding, our model also achieves
a comparable translation quality to the state-of-the-
art fully-NAT approaches with similar decoding
latency.
The results of latent search is encouraging. Ben-
efit from the parallel computing capability of GPU,
latent search sacrifices very small decoding speed
to achieve about 0.5/1.0/0.9/0.9 BLEU improve-
ments for refinement decoding and 1.6/2.7/1.5/2.6
BLEU improvements for non-refinement decoding
on four benchmarks with m= 9candidates.
Method En-De De-En En-RoBaseline 24.10 29.4730.76
Dropout 25.39 29.74 30.85
Token Cutoff 25.58 30.05 31.34
Feat. Cutoff 25.44 29.58 30.95
Token Repl. 25.65 30.23 31.56Baseline 22.20 26.7629.21
Dropout 23.53 26.93 29.40
Token Cutoff 23.87 27.18 29.55
Feat. Cutoff 23.79 26.92 29.90
Token Repl. 23.92 27.39 29.68
Effectiveness of Data Augmentation Methods
In this work, we adopt four different data augmen-
tation strategies as the stochastic noise injection
method to cooperate with consistency regulariza-
tion. To evaluate their effectiveness and the impact
for translation quality, all data augmentation meth-
ods are tested on the benchmarks. The results are
reported on Table 2. The method we adopt combin-
ing posterior consistency regularization with data
augmentation is effective and capable to achieve
higher BLEU scores than the baseline. Specifically,
token replacement achieves the highest score on all
of benchmarks with refinement decoding since the
posterior network is trained on sentences with in-613correct tokens, this is more similar to the inference
circumstance. With the non-refinement decoding,
even the posterior module is disabled, the improve-
ment is still exist for all strategies.
Effectiveness of Consistency Regularization
Consistency regularization should work together
with stochastic data augmentation which is widely
known as a trick to train robust neural networks
(Shorten and Khoshgoftaar 2019; Shen et al. 2020).
Thus, to confirm that the model is not just benefit
from data augmentation but the contribution of pos-
terior consistency regularization, we perform the
experiments by disabling (setting α= 0in eq. (8))
consistency training module and train the model
with four data augmentation methods respectively
on WMT14 En-De dataset. The results illustrate
on Table 3. Without consistency regularization,
the data augmentation methods still result in im-
provement to baseline, but a slight lag exists behind
the model with consistency regularization enabled.
In other words, consistency regularization can im-
prove the translation quality further. Thus, it is
confirmed that our posterior consistency regulariza-
tion method is effective and capable to train better
latent representations in this work.
Method w/o Reg. w/ Reg.
Baseline 24.10
Dropout 24.85 25.39
Token Cutoff 25.38 25.58
Feature Cutoff 25.02 25.44
Token Replacement 25.36 25.65
Effect of Augmentation Rate To investigate the
impact of augmentation rate and choose the op-
timal hyperparameter, we train and evaluate the
models by different augmentation rates from 0.1
to 0.3 on WMT14 En-De dataset. Results are il-
lustrated on Figure 4. The best augmentation rate
is different for each augmentation methods, alone
with the increase of noise injection level, the per-
formance of each method increases firstly and then
gradually drops down. Token cutoff and replace-
ment achieves the outstanding peak performance to
others, it could be attribute to the mechanism that
model can potentially learn from the incomplete
sentences with incorrect or missing tokens and re-
vise them, which mostly benefits to the refinement
decoding at inference where there are massive in-
correct tokens from initial translations.
Tradeoff between Speed and Quality The trade-
off between the speedup rate and translation quality
on WMT14 En-De dataset is shown in Figure 5. We
draw the scatter points by evaluating the proposed
model on various number of candidates from 9 to
59 sampled for latent search. It can be observed
that both decoding with or without refinement can
benefit from latent search. Since the excellent par-
allel computation capability of GPU, and no autore-
gressive scorer is required in our framework, the
decoding speed remains acceptable. Specifically,
the non-refinement decoding with more latent can-
didates can reach the similar quality level of refine-
ment approach. However, refinement decoding can
achieve further improvements and reaches the peak
of about 26.5 BLEU points.614Summary Summarizing from the experiments
and corresponding results illustrated on Table 2,
3 and Figure 4, the mechanism of data augmenta-
tion and consistency regularization in this paper
can be explained in two ways: firstly, data augmen-
tation, especially token-level strategies, help the
posterior network learn the capability of encoding
correct latent variables from incomplete, incorrect
or noisy sentences, which narrows the gap between
training and inference circumstances. Thus, our
posterior network can do better refinement on the
initial translation yfrom Eq. (9)which is relatively
noisy and imperfect. Secondly, consistency regular-
ization helps the posterior network learn to be more
consistent on latent variables under the impact of
noises in input samples, this potentially improves
the robustness of latent representations which result
in further improvements. In addition to the abil-
ity of sample multiple translations in parallel from
latent distribution, all of these cooperate together
and maximize the overall translation quality.
Conclusion
In this work, we introduce posterior consistency
regularization along with a series of data augmen-
tation methods on the posterior module of a vari-
ational NAT model to improve its performance of
translation quality. This method trains the poste-
rior network to be consistent to stochastic noises
in inputs and potentially improves its representa-
tions. Meanwhile, data augmentation closes the
gap between training and inference circumstances.
Both are highly benefit to decoding and refine-
ment step. Experiments on WMT14 En<->De and
WMT16 En<->Ro benchmarks show that our ap-
proach achieves a significant improvement to the
baseline model and a comparable translation qual-
ity to other state-of-the-art fully NAT models with
fast decoding speed. As the effectiveness of con-
sistency regularization and data augmentation is
verified by our experiments, it is promising to be
applied on other models and tasks in the future.
Acknowledgements
This work was supported by the National Key Re-
search and Development Program of China (Grant
No. 2018YFB2100801). Authors wish to thank
anonymous reviewers for their critiques and con-
structive comments which helped improve this
manuscript.References615616617