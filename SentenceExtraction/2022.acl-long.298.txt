DIBIMT : A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation Niccolò Campolungo∗ Sapienza University of Rome campolungo@di.uniroma1.it Federico Martelli∗ Sapienza University of Rome martelli@di.uniroma1.it Francesco Saina SSML Carlo Bo , Rome f.saina@ssmlcarlobo.it Roberto Navigli Sapienza University of Rome navigli@diag.uniroma1.it Abstract di whiskey in which the noun sparo means gunshot . This is one of many examples that seem to encourage a deeper performance analysis in scenarios in which MT systems are required to deal with polysemous words and , speciﬁcally , with infrequent meanings of polysemous words . Although state - of - the - art MT systems , both commercial and non - commercial ones , achieve impressive BLEU scores on standard benchmarks , in our work we demonstrate that they still present signiﬁcant limitations when dealing with infrequent word senses , which standard metrics fail to recognize . Lexical ambiguity poses one of the greatest challenges in the ﬁeld of Machine Translation . Over the last few decades , multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words . Within this body of research , some studies have posited that models pick up semantic biases existing in the training data , thus producing translation errors . In this paper , we present DIBIMT , the ﬁrst entirely manuallycurated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in ﬁve different language combinations , namely , English and one or other of the following languages : Chinese , German , Italian , Russian and Spanish . Furthermore , we test state - of - the - art Machine Translation systems , both commercial and non - commercial ones , against our new test bed and provide a thorough statistical and linguistic analysis of the results . We release DIBIMT at https:// nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard . In the last few decades , attempts have been made to investigate the aforementioned phenomena . In fact , recent studies have observed a direct correlation between semantic biases in the training data and semantic errors in translation . However , their ﬁndings are limited by the following shortcomings : i ) they are not based on entirely manually - curated benchmarks ; ii ) they rely heavily on automaticallygenerated resources to determine the correctness of a translation ; and iii ) they do not cover multiple language combinations . In this work , we address the aforementioned drawbacks and present DIBIMT , to the best of our knowledge the ﬁrst fully manually - curated evaluation benchmark aimed at investigating the impact of semantic biases in MT in ﬁve language combinations , covering both nouns and verbs . This benchmark allows the community not only to better explore the described phenomena , but also to devise innovative MT systems which better deal with lexical ambiguity . Speciﬁcally , the contributions of the present work are threefold : 1 Introduction The polysemous nature of words poses a longstanding challenge in a wide range of Natural Language Processing ( NLP ) tasks such as Word Sense Disambiguation ( Navigli , 2009 ; Bevilacqua et al . , 2021 ) ( WSD ) , Information Retrieval ( Krovetz and Croft , 1992 ) ( IR ) and Machine Translation ( Emelin et al . , 2020 ) ( MT ) . In MT , some research works have addressed the ability of systems to disambiguate polysemous words . For instance , given the sentence He poured a shot of whiskey , the polysemous target word shot unequivocally means a small quantity and therefore a possible translation into Italian could be : Versò un goccio di whiskey . However , some MT systems propose the following translation : Versò uno sparo • We present DIBIMT , a novel gold - quality test bed for semantic biases in MT that goes beyond a simple accuracy score , covering ﬁve language combinations , namely English and one or other of the following languages : Chinese , German , Italian , Russian and Spanish ; ∗ Equal contribution . sentences contain one ambiguous target word . As target words , the authors considered only words in German whose translation into English does not cover multiple senses , thus making the evaluation more straightforward . Despite their effectiveness , such benchmarks do not allow systems to be tested in multiple language combinations , and only cover a very limited number of words and senses . To address these limitations , Raganato et al . ( 2019 ) proposed MuCoW , an automatically - created test suite covering 16 language pairs , with more than 200,000 sentence pairs derived from word - aligned parallel corpora . He poured a shot of whiskey . iniezione   sparo 枪手 本垒打   Injektion   Schlag pistolero   tiro стрелок   выстрел   杯   小杯 goccio   bicchierino шот   рюмкa   trago   chupito Schlückchen   Schuss ITALIAN SPANISH   RUSSIAN   CHINESE   GERMAN Figure 1 : Example of an annotated dataset item . Target word is shot , in its meaning of a “ small drink of liquor ” . We expect translations to contain , for example in Italian , goccio ( lit . a drop ) , but not , for example in Spanish , pistolero ( a person who shoots ) . • We deﬁne four novel metrics that better clarify Other research studies investigated the disambiguation capabilities of MT systems by exploring their internal representations ( Marvin and Koehn , 2018 ; Michel et al . , 2019 ) , or improving them via context - aware word embeddings ( Liu et al . , 2018 ) . More recently , Emelin et al . ( 2020 ) introduced a statistical method for the identiﬁcation of disambiguation errors in neural MT ( NMT ) and demonstrated that models capture data biases within the training corpora , which leads these models to produce incorrect translations . Although the authors expected their approach to be transferable to other language combinations , they only focused on German → English . the semantic biases within MT models ; • We provide a thorough statistical and linguistic analysis in which we compare 7 state - ofthe - art MT systems , including both commercial and non - commercial ones , against our new benchmark . Furthermore , we extensively discuss the results . To enable further research , we release DIBIMT as a closed benchmark with a public leaderboard at https://nlp.uniroma1.it/dibimt . 2 Related Work Over the course of the last few decades , several approaches to the evaluation of the lexical choice in MT have been proposed . To this end , cross - lingual benchmarks were created in which systems were required to provide the translation or a substitute for a given target word in context in a target language ( Vickrey et al . , 2005 ; Mihalcea et al . , 2010 ; Lefever and Hoste , 2013 ) . Based on the ﬁndings and open research questions raised in the aforementioned works , the present paper aims at investigating not only the presence , but also , most importantly , the nature and properties of semantic biases in MT in multiple language combinations , via a novel entirely manually - curated benchmark called DIBIMT and a thorough performance analysis . More recently , Gonzales et al . ( 2017 ) put forward ContraWSD , a dataset which includes 7,200 instances of lexical ambiguity for German → English , and 6,700 for German → French . This dataset pairs every reference translation with a set of contrastive examples which contain incorrect translations of a polysemous target word . For each instance , the answer provided by systems is considered correct if the reference translation is scored higher . Based on a denoised version of the ContraWSD dataset and focusing on the language combination German → English , Gonzales et al . ( 2018 ) present the Word Sense Disambiguation Test Suite which , unlike ContraWSD , evaluates MT output directly rather than by scoring translations . The suite consists of a collection of 3,249 sentence pairs in which the German source 3 Building DIBIMT The DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT , i.e. , biases of certain words towards some of their more frequent meanings . The creation of such a dataset requires i ) a set of unambiguous and grammaticallycorrect sentences containing a polysemous target word ; ii ) a set of correct and incorrect translations of each target word into the languages to be covered . Figure 1 depicts an example of a dataset item . 3.1 Preliminaries BabelNet Similarly to previous studies , we rely on BabelNet1 ( Navigli et al . , 2021 ) , a large multilin1https://babelnet.org an English sentence s , a target word wi and its associated synset σ ; this instance can be annotated for candidate translations of wi in some language L. We also denote λX P as the ( lemma , POS ) pair of wi . gual encyclopedic dictionary whose nodes are concepts represented by synsets , i.e. , sets of synonyms , containing lexicalizations in multiple languages and coming from various heterogeneous resources , including , inter alia , WordNet ( Miller et al . , 1990 ) and Wiktionary.2 Let us deﬁne B as an abstraction used to query the subset of synsets in BabelNet that contain at least one sense3 from WordNet and one or more senses in languages other than English,4 while only considering senses coming from highquality sources , i.e. , language - speciﬁc wordnets . 3.2.1 Starting Sentence Pool We collect our initial items from two main sources : WordNet and Wiktionary.6 Speciﬁcally , we use the examples from WordNet Tagged Glosses ( Langone et al . , 2004 ) , where each sentence ’s target word was manually associated with its synset7 , thereby readily providing the ﬁrst batch of initial items . Formal Notation Given an arbitrary synset σ , we deﬁne ΛL(σ ) as the set of lexicalizations of σ in language L contained within B. As an example , let us consider the synset ˜σ corresponding to the drink meaning of the word shot . ˜σ contains lexicalizations in different languages , including : ShotDE , shotEN , nipEN , chupitoES , tragoES , bicchierinoIT and goccioIT . Hence , ΛEN(˜σ ) = { shot , nip } , while ΛES(˜σ ) = { chupito , trago } . As for Wiktionary , instead , we start by obtaining every usage example s and its associated deﬁnition d ( ﬁltering out archaic usages and slang ) , then , we automatically extract the target words from the corresponding example.8 Now , the only step that remains in order to construct an initial item is to associate a synset σ with the word wi used in the example s. We perform this association in two phases : ﬁrst , we try to map the deﬁnition d related to the example s to a BabelNet synset by relying on the automatic mappings available in BabelNet 5 between WordNet and Wiktionary , discarding examples for which this association can not be found ; second , we manually validate and correct these successful associations to ensure that our initial items are of high quality . Furthermore , let λP represent a ( lemma , part of speech ) pair , where P is the part of speech . We denote ΩL(λP ) = { σ1 , . . . , σn } as the set of synsets which contain λP as a lexicalization in language L according to B. Additionally , we deﬁne δL(λP ) = |ΩL(λP ) | as the polysemy degree , i.e. , the number of senses , of λP in language L. For example , given λP = shotN OU N , ΩEN ( λP ) would be the set of synsets associated with the nominal term shot ( e.g. , the act of ﬁring , a photograph and a drink , among others ) . 3.2.2 Sentence Filtering We apply a ﬁltering step to the original sentences in order to select examples that are likely to be more challenging for the models to translate : i ) we discard every initial item X for which δEN(λX P ) < 3 , i.e. , we retain only sentences whose associated ( lemma , POS ) pair has a polysemy degree of at least 3 in BEN ; ii ) we retain at most only one sentence per sense per source9 ; iii ) differently from previous works , which impose a strict requirement on synsets that are monosemous in the target language , we retain sentences satisfying the following requirement . Let us consider the nominal senses of the word bank : among them , one represents a speciﬁc aviation maneuver . In Italian , this synset 3.2 Sentence Selection Process In this section , we detail the creation process of our dataset , i.e. , the selection of our sentences as well as the construction and ﬁltering of our items . Item Structure and Notation Before we proceed , let us formally state how each item in the dataset is structured : given a source sentence s = [ w1 , . . . , wn ] as a sequence of words , and given a target word5 wi in s tagged with some synset σ , we consider X = ( s , wi , σ ) as an initial item of the dataset , i.e. , an instance composed of 2https://www.wiktionary.org/ 3A “ sense ” is a lexicalization of a speciﬁc synset in some language . Henceforth , we will refer to lexicalizations and senses interchangeably . 6We use the dump of September 2021 . 7Which we convert from WordNet to BabelNet . 8In Wiktionary , target words are marked in bold inside the example sentence . 4Speciﬁcally , we consider synsets that have lexicalizations 9The reasoning for this choice is twofold : on the one hand , oftentimes Wiktionary has multiple examples for the same synset , that differ in only one or two words , thus we skip them to avoid repetitions ; on the other hand , we obtain an increase in sense coverage without worsening the annotator load . in English , Italian , German , Russian , Spanish and Chinese . 5For simplicity , we use the term word here , but our work focuses on multi - word expressions as well ( both in source and target sentences ) . includes one lexicalization , avvitamento ; although this is not monosemous in Italian ( e.g. , avvitamento might also refer to a screw thread ) , neither of the other possible senses of avvitamento has bank as an English lexicalization , which , for Italian , satisﬁes our third condition . If the same holds true for all languages , the synset passes the test and thus the sentence is retained . All Nouns Verbs # items # lemmas # synsets 597 305 471 314 186 254 283 147 217 Table 1 : General statistics of our annotated dataset . POS - speciﬁc lemmas do not sum to “ All ” as they can overlap across POS tags ( e.g. , run ) . 3.3 Annotating the Dataset % OG % RG % SL Once the set of initial items is ready , we can proceed with the annotation phase , which will produce our annotated items . 50.9 49.6 49.1 67.4 55.2 25.0 19.5 38.2 57.3 69.0 59.7 47.7 67.1 54.4 46.3 DE ES IT RU ZH Speciﬁcally , given a language L and an initial item X = ( s , wi , σ ) , we associate a set of good ( GL ) and bad ( BL ) translation candidates with X , which represent words that , respectively , we do , and do not , expect to see in a translation of sentence s in language L. Finally , we refer to XL as an annotated item , i.e. , the tuple ( s , wi , σ , GL , BL ) . 54.4 41.8 55.0 Mean Table 2 : Annotation Statistics : % OG represents the average percentage of Good lemmas that are Original , i.e. , were added by our annotators ; % RG represents the average percentage of Good lemmas that were Removed , i.e. , lemmas that came from BabelNet and that our annotators deemed incorrect in the context of the given example ; % SL represents the average percentage of times two senses Share the same set of Lexicalizations for two different example sentences . 3.3.1 Pre - annotation Item Creation Before moving forward with the annotation phase , we pre - populate the sets of good ( GL ) and bad ( BL ) lexicalizations for a given initial item X in language L extracting them from B. Formally , we assign GL = ΛL(σ ) , i.e. , the set of lemmas in language L of the BabelNet synset associated with σ ; P ) \{σ } ΛL(ˆσ ) , furthermore , we set BL = Sˆσ∈ΩL(λX i.e. , the set of all lemmas in language L of BabelNet synsets associated with any ˆσ excluding σ . With this step , we produce an automatically populated version of our annotated items . following requirements : they are native speakers or hold C2 - level certiﬁcations and work as professional translators in the given language combinations . The full instructions provided to the annotators can be found in Appendix C. 3.3.2 Annotation Guidelines 3.3.3 Resulting Dataset We instruct annotators to update the set of good ( GL ) and bad ( BL ) lexicalizations of wi ∈ s such that each lexicalization contained in the respective set can be considered a good or a bad translation equivalent for the target word in the provided sentential context.10 Our annotators analyzed around 800 sentences , discarding 200 of them , ﬁnally obtaining approximately 600 annotated items in 5 languages . Due to a coverage issue of the Russian language in BabelNet , we retain only sentences tagged with nominal or verbal synsets . Dataset statistics are reported in Table 1 . We also instruct annotators to discard sentences in which i ) the target word wi is an idiomatic expression or a proper noun , and ii ) the semantic context is not sufﬁcient to properly disambiguate wi . As expected , we note that the lexicalizations found in B have been substantially reﬁned by our annotators in all languages , as reported in Table 2 . Indeed , across languages , on average , 54 % of the good lexicalizations have been added by our annotators , while 42 % of the pre - existing lexicalizations have been removed . More importantly , given a language and two sentences containing words referring to the same synset , on average only in 55 % of cases do they also share those words ’ good Given the expertise required to carry out this task , we rely on three highly qualiﬁed translators : one for Italian , German and Russian ; one for Spanish and one for Chinese . Our annotators satisfy the 10Any lexicalization of σ in L that is removed from GL is automatically placed in BL . lexicalizations , conﬁrming that the assumption that all synonyms of a word are valid replacements can lead to incorrect results . of semantic bias ; and iv ) offer some insights into the causes of such biases . In Appendix D we include a model - speciﬁc breakdown of the various scores and metrics reported throughout this section . These statistics lead us to a straightforward , but important , conclusion : only in a limited number of cases is a lexicalization belonging to a given synset to be considered as a suitable translation equivalent for the provided target word and its context . Examined jointly , these metrics suggest that relying on synset lexicalizations from BabelNet alone is prone to producing errors , either due to BabelNet ’s intrinsic noise , or due to the lack of different granularity of synsets and contextualized words . 4.1 Comparison Systems We test a wide range of models , both commercial and non - commercial ones , and report their performances on DIBIMT ’s evaluation metrics : • DeepL Translator12 , a state - of - the - art commercial NMT system . • Google Translate13 , arguably the most popular commercial NMT system . Sentences ’ Properties Description As we stated in Section 3.2.1 , the sentences we annotate are all usage examples of speciﬁc concepts obtained from WordNet or Wiktionary . Such examples are typically short main clauses with no subordinates , featuring on average 9 words ( around 50 characters per sentence ) . All selected sentences include a semantic context which allows the meaning of the target word to be properly identiﬁed . • OPUS ( Tiedemann and Thottingal , 2020 ) , the smallest state - of - the - art NMT model available to date , a base Transformer ( each model has approximately 74 M parameters ) trained on a single language pair on large amounts of data . • MBart50 ( Tang et al . , 2021 ) , multilingual BART ﬁne - tuned on the translation task for 50 languages ( 610 M parameters ) . We refer to MBart50 as the English - to - many model , and to MBart50MTM as the many - to - many model . 3.4 Analysis Procedure DIBIMT ’s analysis procedure is fairly simple : given an annotated item XL = ( s , wi , σ , GL , BL ) and a translation model M , we compute tL = ML(s ) , i.e. , the translation of s in language L according to M. Then , we use Stanza ( Qi et al . , 2020 ) to perform tokenization , part - of - speech tagging and lemmatization of tL and , ﬁnally , we check if there is any match11 between the lemmas of the translated sentence and those contained in GL or BL . In case there is no match , we mark the translation as a MISS ; otherwise , we mark it as GOOD or BAD depending on which set matched the lemma . This produces an analyzed item , which for simplicity we denote as XM L = ( XL , tL , R , ωL ) , where R is one of GOOD , BAD or MISS and ωL represents the matched lemma in case there was a match ( GOOD or BAD ) , ǫ otherwise . • M2M100 ( Fan et al . , 2021 ) , a multilingual model able to translate from / to 100 languages . We test both versions of the model , the 418 M parameter one ( which we dub M2M100 ) and the 1.2B parameter one ( dubbed M2M100LG ) . 4.2 Discussion of MISS Figure 2 reports general results of the analysis per ( model , language ) pair . Given the high percentage of analyzed items classiﬁed as MISS , we asked our annotators to perform an inspection on a random sample of 70 items per language in order to unearth the reasons , with varying results . We identiﬁed multiple causes , namely : i ) word omission in the translation ( around 19 % of items , mostly in Chinese and Italian ) ; ii ) issues with Stanza ’s tokenization ( around 11 % , mostly Chinese and Russian ) and lemmatization ( around 12 % , mostly Italian and German ) ; iii ) words translated as themselves ( approximately 5 % , often in multilingual neural models ) ; iv ) translations which have nothing to 4 Results and Discussion We now : i ) use DIBIMT to carry out an evaluation of 7 different machine translation systems ; ii ) report the obtained results , including a thorough statistical and linguistic evaluation ; iii ) extensively discuss our ﬁndings , providing multiple measures 12https://deepl.com/ 13We used the = GOOGLETRANSLATE function available 11A more detailed description of the analysis procedure is provided in Appendix A. in Google Sheets . DeepL Google M2M100 M2M100LG MBart50 MBart50MTM OPUS Mean DE ES IT RU ZH 74.60 57.87 53.49 71.58 46.00 21.90 22.54 18.04 22.89 15.04 22.19 25.51 21.83 26.22 16.99 26.96 30.00 25.14 35.19 22.35 28.73 33.89 29.34 36.06 31.21 28.65 32.66 30.54 33.33 34.15 27.99 36.66 29.95 41.07 27.75 33.00 34.16 29.76 38.05 27.64 Mean 60.71 20.08 22.55 27.93 31.85 31.87 32.68 32.52 Table 3 : General results : accuracy on DIBIMT across models and languages . Higher is better . DeepL Google M2M100 M2M100LG MBart50 MBart50MTM OPUS Mean SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI DE ES IT RU ZH 34.78 56.04 57.71 41.97 64.97 28.30 46.14 49.01 33.64 59.58 86.61 83.84 85.47 84.01 91.97 79.54 78.41 80.62 83.49 87.98 82.00 83.08 80.22 79.85 91.81 76.15 77.95 76.58 78.34 87.18 78.90 79.87 78.69 74.72 88.79 74.71 73.84 76.10 69.69 82.17 84.10 77.13 78.67 73.86 80.39 73.86 71.06 71.51 70.11 73.14 84.95 79.06 79.41 78.58 76.59 74.24 71.57 69.48 72.87 71.50 79.85 74.85 80.59 68.49 79.96 76.25 69.12 72.02 69.27 75.66 75.89 76.27 77.25 71.64 82.07 69.00 69.73 70.76 68.20 76.75 Mean 51.10 43.33 86.38 82.01 83.39 79.24 80.19 75.30 78.83 71.94 79.72 71.93 76.75 72.46 76.62 70.89 Table 4 : Semantic Biases : SFII , i.e. , Sense Frequency Index Inﬂuence , represents the average percentage of errors at varying levels of µλP ( σ ) . SPDI , i.e. , Sense Polysemy Degree Importance , instead , represents the average percentage of errors at varying level of δL(λP ) . Lower is better . do with the source text14 ( around 23 % ) ; and v ) missing terms from either BL ( around 18 % ) or GL ( around 11 % ) . We intend to thoroughly investigate and tackle these issues and translation phenomena as future work . dex k means that synset σ is the k - th most frequent meaning for λP . In Figure 3(a ) , we plot the number and percentage of errors made on average by the models , group(σX ) , where X is a non - MISS ing items by µλX P analyzed item . As expected , the less frequent a meaning for a given word is , the harder it is for the model to correctly disambiguate it . 4.3 General Results Table 3 reports accuracy for non - MISS analyzed items ( i.e. , # GOOD+#BAD ) . With the sole exception of DeepL , which greatly outperforms every other competitor , models achieve extremely low scores , in the range of 20%-33 % . Surprisingly , Google Translate performs worst across languages . # GOOD Finally , given a ( model , language ) pair , we deﬁne the Sense Frequency Index Inﬂuence ( SFII ) as the average percentage of errors , for each group , that we detected . Values are reported in Table 4 . Interestingly , DeepL proves once again to be the best , obtaining a score of 51 % , far below the average 80 % achieved by the other models , with most non - commercial models performing ≤ 80 % . 4.4 Analyzing the Semantic Biases In addition to accuracy , DIBIMT analyzes the semantic biases of a translation model via four novel metrics , which we deﬁne in detail in what follows . Sense Polysemy Degree Importance ( SPDI ) Similarly to SFII , we also study the extent to which the polysemy degree , i.e. , how many senses a given word can have , impacts the models ’ disambiguation capabilities . This experiment mirrors SFII , but groups items by their lemma ’s polysemy degree δEN(λX P ) instead of µ. Figure 3(b ) reports the results on all items . Unsurprisingly , similarly to the frequency index , we observe that higher polysemy leads to more errors , conﬁrming that models still struggle with very polysemous words . Similarly to SFII , SPDI is deﬁned as the average percentage of Sense Frequency Index Inﬂuence ( SFII ) We study the sensitivity of models to disambiguating senses with respect to their frequency . To do this , we deﬁne µλP ( σ ) as the index of synset σ in ΩEN(λP ) ordered according to WordNet ’s sense frequency , as computed from SemCor . That is , in14An example is the sentence he is a crack shot , where the word shot is translated by MBart50 into Italian as “ schianto ” , which can be interpreted in this case as “ someone very good looking ” . DeepL Google M2M100 M2M100LG MBart50 MBart50MTM OPUS Mean MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ DE ES IT RU ZH 53.68 59.89 68.08 50.00 49.07 84.21 87.91 86.38 83.33 88.89 56.76 61.96 61.96 48.12 56.05 86.82 89.05 87.23 83.28 88.20 61.28 61.81 60.75 47.87 59.06 87.23 89.37 86.79 83.41 91.34 59.13 61.78 62.82 45.25 59.35 87.30 88.03 88.81 84.16 92.45 58.89 60.17 62.90 47.39 50.66 89.72 91.10 87.50 87.20 89.87 55.82 63.09 68.97 44.91 54.17 89.56 91.85 91.81 87.96 90.28 56.98 64.47 64.48 48.40 51.71 87.92 91.21 89.66 84.04 87.45 57.51 61.88 64.28 47.42 54.30 87.54 89.79 88.31 84.77 89.78 Mean 56.14 86.15 56.97 86.92 58.15 87.63 57.66 88.15 56.00 89.08 57.39 90.29 57.21 88.06 57.08 88.04 Table 5 : Frequency Analysis : MFS represents the average percentage of times the model mistakenly translates the target word into a lexicalization belonging to the Most Frequent Sense associated with λP . MFS+ , instead , checks whether the wrong translation belongs to any synset that is more frequent than the target one . Lower is better . DE ES IT RU ZH ALL NOUN VERB 0 0 0.4 D e e p L 0 0 . . 4 8 0 0 . . 0 4 6 0 4 3 4 3 0 0 0 . . . . 3 3 7 0.2 3 6 . . 0 2 5 3 5 . 3 2 3 2 3 1 7 0.2 . 0.19 Accuracy % MISS MFS MFS+ SFII SPDI 32.11 38.03 57.86 88.68 76.98 70.80 34.15 29.36 60.13 87.57 69.16 66.86 30.02 47.57 52.60 88.74 76.90 72.87 0.16 0 0 0 0 . . 0 G o o g e 0 0 0.4 5 6 5 5 . . 0 5 1 . . 4 8 0.4 0 0 4 6 4 6 . 4 1 . . 3 7 0.2 3 6 0.11 0.09 l 0.08 0.08 0.08 0 0.5 0 0 0.4 O P U S 0 0 . . . . 0 0 4 7 4 6 4 5 4 5 0 . . 0 3 8 3 8 0 2 7 0 2 6 0.2 . . 3 2 . 0.22 . 2 9 0.21 0.17 0.17 0 M B a r t 0.4 0 0 0 0 0.4 0 0 0.4 0.4 . . . 0 . 4 4 4 4 4 3 . 4 2 . . 3 9 3 8 0.2 3 6 5 0 0.21 0.2 0.18 0.17 0.17 0 Table 6 : Results by PoS tag . Numbers represent the mean value of each score introduced in the paper . The column ALL summarizes the results reported in the other tables . M B a r t 0.4 0 0 0 0 0 0 0 . . 0.4 0 . 5 0 M T M 0 . 4 5 4 5 . . 4 3 . 4 2 4 1 4 1 . . 3 9 3 7 0.2 3 6 0.19 0.19 0.18 0.17 0.17 0 0 M 2 M 1 0 0 0 0 0.4 . 0 0 0 . . 0 0 5 2 4 9 4 9 . 0.4 . . 0 . . 4 5 4 3 4 3 4 2 0.13 4 2 0.12 0.15 . 0.2 3 6 0.11 0.09 0 M 2 M 1 0 0 L G 0 0 0.4 0 0 0 0 . . dub More Frequent Sense ( MFS+ ) ; additionally , if µλX ( ˆσ ) = 1 , then the model disambiguated the P source word wi to the Most Frequent Sense ( MFS ) of the associated lemma λX P . The results of both these analyses are reported in Table 5 . 0 7 0.4 . 0 0 4 7 4 . . . 4 4 4 2 4 2 4 2 0.13 . . . 3 8 3 7 3 7 0.2 0.2 0.19 0.16 0.16 0 GOOD BAD MISS Figure 2 : General results of the analysis . Numbers represent percentages of the whole dataset ( 600 items ) . A full - page version of this image , for readability purposes , is available in the Appendix ( Figure 16 ) . We can observe a few interesting results : ﬁrst , on average , almost 60 % of the time a mistake reﬂects the Most Frequent Sense of the target word ( secondlast column ) ; second , almost 90 % of the errors concern translations towards more frequent senses of the target word ( last column ) . Importantly , these results are consistent across systems , whether commercial or not . Although it might seem straightforward , NMT models are still strongly biased towards senses that are more likely to be encountered during training ; while this could be related to the patternmatching nature of neural networks , it also depends heavily on the training data the model was trained upon , and this needs to be further investigated in future research . errors at varying polysemy degrees , and its values are reported in Table 4 : once again , DeepL outperforms all other systems by a large margin , conﬁrming that it is the least biased across the board . Most and More Frequent Senses To further corroborate our ﬁndings about semantic biases , we study how often models predict senses that are more frequent than the target one . Given a BAD analyzed item XM L , we denote ˆσ as the synset associated with the wrongly translated lemma ωL.15 Then , we check the frequency of σ and ˆσ with respect to λX ( σ ) , then the system ’s disambiguation steered towards a sense that is more frequent than the target one , which we ( ˆσ ) < µλX P P : if µλX P 4.5 Are verbs harder than nouns ? The existing literature in WSD points to the fact that verbs are generally harder than nouns , mostly due to their highly polysemous nature ( Barba et al . , 2021b ) . We try to analyze whether MT models 15In the case in which there are multiple possible synsets , we take the most frequent according to µλX , as we need to P rely on the assumption that the surface form represents the intrinsic disambiguation performed by the NMT system . 1 1 1500 1500 0.8 0.8 s r o r r e % s m e t i # 1000 0.6 0.6 1000 0.4 0.4 500 500 0.2 0.2 0 0 0 0 0 10 20 30 40 0 10 20 30 40 ( a ) Sense Frequency Index ( b ) Sense Polysemy Degree Figure 3 : Overall distribution of errors , summed across all models and languages , with respect to ( a ) Sense Frequency Index ( µλP ( σ ) ) and ( b ) Sense Polysemy Degree ( δEN(λP ) ) . Red bars represent the number of errors ( i.e. , BAD items ) for a given group , grey bars represent the number of correct ( i.e. , GOOD items ) items . Orange lines # GOOD+#BAD ) for a given group . represent the percentage of errors ( i.e. , # BAD are affected by the same phenomenon : in Table 6 , we report the average results obtained by running DIBIMT on all its sentences ( column ALL ) and the subset of sentences whose target word was either a NOUN or a VERB . In general , we observe an average drop of accuracy of 4 points , as well as an astounding difference of 18 percentage points in MISS handling , which we will investigate more thoroughly in future work . Interestingly , MT models are much more inclined to translate nouns into their most frequent sense ; we attribute this difference to the generally higher polysemy of verbs compared to nouns , which increases the size of the space of possible translations for a given verb , thus decreasing the chance that it gets translated into the MFS . Aside from this , we draw the same conclusion as that drawn by previous works in the ﬁeld of WSD , with nouns being generally easier to translate than verbs . DE ES IT RU ZH DE ES IT RU ZH DE    DE    1.0 0.68 0.68 0.65 0.58 1.0 0.66 0.69 0.65 0.56 ES    ES    0.68 1.0 0.69 0.67 0.60 0.66 1.0 0.73 0.72 0.60 IT    IT    0.68 0.69 1.0 0.73 0.61 0.69 0.73 1.0 0.76 0.61 RU    RU    0.65 0.67 0.73 1.0 0.52 0.65 0.72 0.76 1.0 0.54 ZH    ZH    0.58 0.60 0.61 0.52 1.0 0.56 0.60 0.61 0.54 1.0 ( a ) MBart50 ( b ) MBart50MTM DE ES IT RU ZH DE ES IT RU ZH DE    DE    1.0 0.73 0.75 0.75 0.67 1.0 0.72 0.78 0.71 0.68 ES    ES    0.73 1.0 0.79 0.73 0.70 0.72 1.0 0.77 0.72 0.64 IT    IT    0.75 0.79 1.0 0.77 0.72 0.78 0.77 1.0 0.73 0.73 RU    RU    0.75 0.73 0.77 1.0 0.61 0.71 0.72 0.73 1.0 0.59 ZH    ZH    0.67 0.70 0.72 0.61 1.0 0.68 0.64 0.73 0.59 1.0 ( c ) M2M100 ( d ) M2M100LG 4.6 Is the encoder disambiguating ? Figure 4 : Language Frequency Correlation : percentage of times that an item translates to the same synset . We try to assess to what extent , in a multilingual encoder - decoder architecture , the encoder is determining the implicit disambiguation of the source sentence before generating the translation . For instance , we ask ourselves this question : given an ambiguous word wi in the source sentence s , how often does the model translate it into a lexicalization representing the same sense , if prompted to translate s into different languages ? Intuitively , if the encoder was the sole contributor to the implicit disambiguation performed by the model , we would expect to see the meaning to always be the same , regardless of the target language . and L2 and an initial item X , we take M ’s ana17 and check if translalyzed items XM L1 and XM L2 tions in L1 and L2 have a synset in common , i.e. , |ΩL1(ωL1 ) ∩ ΩL2(ωL2)| > 0 . The results of this experiment are reported in Figure 4 . We observe that , on average , this phenomenon occurs around 70 % of the time . Hence , it is safe to assume that , while the encoder certainly plays an important role in the disambiguation of the input sentence , the decoder is also contributing signiﬁcantly . Another interesting observation is that the alphabet of the target language does not seem To measure this , we perform the following experiment : given a model M,16 two languages L1 languages . We also disregard DeepL and Google Translate as their architecture is proprietary . 16We disregard OPUS here as it is a set of bilingual models , rather than a single model capable of translating into multiple 17We skip item X if either XM L1 or XM L2 is a MISS . DeepL Google M2 M M2MLG MB MBMTM OPUS Mean M2M100 M2M100LG MBart50 MBart50MTM OPUS Mean DE ES IT RU ZH 66.86 67.89 66.67 66.76 68.42 71.04 72.76 72.58 69.55 71.89 65.85 66.77 66.35 66.42 69.26 67.18 66.86 68.50 67.69 69.82 66.87 65.37 64.33 66.35 68.93 67.77 67.18 65.81 64.29 69.58 66.95 66.83 65.82 69.21 69.88 67.50 67.67 67.15 67.18 69.68 DE ES IT RU ZH 98.00 100.00 94.00 94.00 96.00 98.00 98.00 90.00 90.00 98.00 92.00 88.00 86.00 98.00 94.00 94.00 90.00 100.00 92.00 98.00 84.00 94.00 88.00 88.00 92.00 93.20 94.00 91.60 92.40 95.60 Mean 67.32 71.56 66.93 68.01 66.37 66.93 67.74 67.84 Mean 96.40 94.80 91.60 94.80 89.20 93.36 Table 7 : WSD Results : ESCHER ’s accuracy on the set of English sentences of non - MISS analyzed samples for each ( model , language ) pair . Higher is better . Table 8 : Model Errors : percentage of times a model thought its BAD translation was better than a GOOD one . possible ) . For each ( model M , language L ) pair , we sample a BAD translation ( tBAD ) , pair it with a GOOD translation ( tGOOD ) produced by another model ( prioritizing DeepL ) , and ask annotators to check their correctness and apply corrections where needed,19 then compute the perplexities according to M with the corresponding English sentence , and call them pGOOD and pBAD respectively . We repeat this sampling 50 times per ( M , L ) pair and check how often pBAD < pGOOD . Table 8 shows that , on average , this happens in 93 % of cases , thus conﬁrming that most semantic biases are embedded within models and are not caused by the decoding strategy . to have any inﬂuence , as language pairs involving Russian display scores that are very similar to those of the other three European languages . We attribute lower scores in Chinese to coverage issues in BabelNet , which would hinder a correct fulﬁllment of the condition deﬁned for this experiment . 4.7 How challenging is DIBIMT ? Given the low performances achieved by MT models , we test a WSD system on the English sentences within DIBIMT , both to assess the toughness of our system and to establish an additional baseline . We use ESCHER18 ( Barba et al . , 2021a ) , a stateof - the - art model on English WSD . Interestingly , ESCHER achieves an overall accuracy score of 66.33 , almost 15 points lower than the results on the standard WSD benchmark ( 80.7 on ALL , Raganato et al . , 2017 ) , therefore conﬁrming the challenging nature of DIBIMT . Furthermore , in order to estimate the difference in disambiguation capability between NMT models and a dedicated WSD system , we compute ESCHER ’s performances on the set of English sentences of non - MISS analyzed items for each ( model , language ) pair . We report these results in Table 7 , whose accuracy scores can be directly compared to those in Table 3 . 5 Conclusions In this work , we presented DIBIMT , a novel benchmark for measuring and understanding semantic biases in NMT , which goes beyond simple accuracy and provides novel metrics that summarize how biased NMT models are . We tested DIBIMT on 7 widely adopted NMT systems , extensively discussing their performances and providing novel insights into the possible causes and relations of semantic biases within NMT models . Furthermore , statistics of our annotations suggest that , when dealing with translations , synsets ’ lexicalizations can not be used interchangeably , as their choice depends heavily on the context . As expected , the average MT accuracy is signiﬁcantly lower than ESCHER ’s , with the sole exception of DeepL , which manages to surpass it on German and Russian . These results clearly demonstrate that current NMT models are still not on par with dedicated WSD systems , and thus that they might beneﬁt from the inclusion of such WSD systems within the NMT ecosystem . In the future , we plan to improve DIBIMT by introducing better heuristics to recognize and handle MISS cases , especially covering the linguistic phenomena we described ( see Section 4.2 ) ; we also aim at widening language coverage and increasing the number of sentences in the benchmark , consequently improving word and sense coverage . To enable further research , we release DIBIMT as a closed benchmark with a public leaderboard at : https://nlp.uniroma1.it/dibimt . 4.8 Is this a decoding issue ? As a ﬁnal experiment , we assess whether the semantic biases are caused by search errors ( i.e. , failures of the decoding algorithm ) , or model errors ( i.e. , the models deemed their translations the best 19We do this to make the translations more grammatically ﬂuent , and not to correct the disambiguation of the target term , which was never detected as being wrong in the sampled cases . 18The publicly available version trained on SemCor data only . 