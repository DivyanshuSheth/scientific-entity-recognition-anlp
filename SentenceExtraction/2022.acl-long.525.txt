
Suixin Ou, Yongmei Liu
Dept. of Computer Science, Sun Yat-sen University, Guangzhou 510006, China
ousx@mail2.sysu.edu.cn ,ymliu@mail.sysu.edu.cn
Abstract
Table fact verification aims to check the cor-
rectness of textual statements based on given
semi-structured data. Most existing methods
are devoted to better comprehending logical op-
erations and tables, but they hardly study gen-
erating latent programs from statements, with
which we can not only retrieve evidences ef-
ficiently but also explain reasons behind veri-
fications naturally. However, it is challenging
to get correct programs with existing weakly
supervised semantic parsers due to the huge
search space with lots of spurious programs. In
this paper, we address the challenge by leverag-
ing both lexical features and structure features
for program generation. Through analyzing the
connection between the program tree and the
dependency tree, we define a unified concept,
operation-oriented tree, to mine structure fea-
tures, and introduce Structure-Aware Seman-
tic Parsing to integrate structure features into
program generation. Moreover, we design a
refined objective function with lexical features
and violation punishments to further avoid spu-
rious programs. Experimental results show that
our proposed method generates programs more
accurately than existing semantic parsers, and
achieves comparable performance to the SOTA
on the large-scale benchmark TABFACT.
1 Introduction
With the rise of misleading information on the Inter-
net, such as fake news, rumors and political deceit,
fact-checking has been developed as a means of
detecting and filtering false information. Table fact
verification (TFV) is a specific fact-checking task
that requires performing logical operations such as
comparison, superlative and aggregation over given
tables to verify textual statements.
Programs play an important role in TFV . On one
hand, correct programs can provide rationales for
model decisions, which make reasoning analysisFigure 1: The pipeline of ProgVGAT (Yang et al., 2020)
on TFV . Here the task is, given a table and a statement,
to predict whether the table entails the statement or
refutes it. Verbalized evidences are verbal descriptions
of the program execution procedure.
and failure diagnosis feasible (Zhou et al., 2018).
On the other hand, they can be used to fetch the key
evidences for verification. Figure 1 gives an exam-
ple of mainstream methods (Zhong et al., 2020a;
Shi et al., 2020b; Yang et al., 2020; Shi et al.,
2021) for TFV . It first generates latent programs
from statements, then collects evidences from ta-
bles by executing the programs over the tables, and
finally leverages all information for final predic-
tions. Compared with naive methods(Chen et al.,
2020; Zhang et al., 2020a) which simply put state-
ments and linearized tables into language models
for verification, the mainstream methods addition-
ally introduce programs to reveal the evidences
(e.g., verbalized evidence V1 ) covered by logical
operations (e.g., max([row1, row2]], podiums) ) and
to fetch the key information from the table (e.g.,
8). But an incorrect or spurious program may intro-
duce irrelevant or even contradictory evidences. So
it is crucial to get correct programs that properly ex-
tract evidences from tables, especially when tables
are too large to be encoded by neural networks.
Despite being important, program generation re-
mains underexplored for TFV . To the best of our
knowledge, only LPA (Chen et al., 2020) works
on program generation. It first searches programs7624with human-designed features, then ranks them
with a neural network, and finally uses the exe-
cution result of the top program as the prediction.
However, it exhibits an unacceptable performance
which means it generates incorrect programs. The
remaining approaches just predict the correctness
of statements but never concern about generating
correct programs. In TFV , there is still a need to
find better solutions for program generation.
Intuitively, we can resort to weakly supervised
semantic parsing (Liang et al., 2011) for the pro-
gram generation, but existing semantic parsers may
fail in TFV for the amplified spurious program
problem caused by the binary label. Due to the lack
of program labels, existing methods will sample la-
bel consistent programs for model training. In TFV ,
any sampled program that outputs a Boolean value
has a 50% chance of hitting the correct label; hence
there are many label consistent programs, while
only a small part of label consistent programs are
correct, implying that the rest are all spurious.
In this paper, we carefully examine the syntax
structures of statements and find that task-related
structure features are the key to address the issue
mentioned above. We propose a unified operation-
oriented tree constructed in three steps. Firstly, we
link entities between the table, trigger dictionary
and statement. Secondly, we obtain the original
tree using a dependency parser with the linked state-
ment as input. Thirdly, the original tree is pruned
and merged to a simplified tree that contains only
information related to operations. Such a unified
tree can provide distant supervision, assisting our
model in generating single operations correctly and
generating all operations in the correct order. As
a result, we have a higher probability of getting
correct programs and evading spurious ones. Then
we introduce Structure-Aware Semantic Parsing
(SASP) by designing a scoring function based on
the proposed tree and fusing the sample distribu-
tions computed by the scoring function and neural
network. At last, we design a refined objective
function with lexical features and violation punish-
ments to avoid spurious programs further.
Experimental results on Tabfact and Logic2Text
show that SASP improves the performance of the
baseline model significantly, and achieves compa-
rable performance to the State-Of-The-Art method.
Our contributions are as follows:
•We propose an operation-oriented tree to pro-
vide distant supervision for semantic parsing.•We propose SASP which leverages both lexi-
cal features and structure features for the se-
rious spurious problem in weakly supervised
semantic parsing for TFV .
•With the proposed method, we can generate
more accurate programs which can not only
boost existing mainstream methods for TFV ,
but also provide explanation for verification.
2 Related Work
Fact Verification Fact verification aims at identi-
fying the truthfulness of online textual statements
given different sources of evidences, including doc-
ument sets (Thorne et al., 2018; Nie et al., 2019;
Zhong et al., 2020b; Wan et al., 2021), images
(Suhr et al., 2019; Li et al., 2020) and structured
tables (Chen et al., 2020; Zhong et al., 2020a; Shi
et al., 2020b; Zhang et al., 2020a; Yang et al., 2020;
Shi et al., 2021). Despite the sources of evidences
used to support the verification vary, the methods
for different tasks appear to have the same idea.
They first locate the key evidences that will aid in
their verification, then fuse the collected key ev-
idences with the original statement to make the
final prediction. In this paper, we focus on gener-
ating better programs that allow existing methods
to get key evidences from tables efficiently, hence
benefiting existing methods for TFV .
There are also many explainable fact verification
works(Kotonya and Toni, 2020a). Attention based
methods(Popat et al., 2018; Lu and Li, 2020; Wu
et al., 2020) highlight key evidences according to
attention weights. Atanasova et al. (2020); Kotonya
and Toni (2020b) generate explanations in natural
language with text summarization technology. Gad-
Elrab et al. (2019); Ahmadi et al. (2020) use horn
rules and knowledge graphs to mine explanations.
Our work is similar to the third line of works from
the perspective of explainability.
Semantic Parsing Due to the expensive cost of
annotated programs, weakly supervised semantic
parsing (Liang et al., 2011; Berant et al., 2013;
Artzi and Zettlemoyer, 2013) has been proposed to
learn program generation from sentence-label pairs.
Compared with full supervision, weak supervision
brings spurious problems: there may be spurious
programs that accidentally reach the right answer
for the wrong reason, and they will provide wrong
supervision for model training. Previous work (Pa-
supat and Liang, 2016) uses crowd-sourced deno-7625tations to prune spurious programs. Liang et al.
(2018) use both programs inside and outside the
memory buffer to compute the expected return ob-
jective in case the neural model is misled by spuri-
ous programs inside memory. Dasigi et al. (2019);
Misra et al. (2018); Agarwal et al. (2019) rely on
lexical features to differentiate between spurious
and correct programs. Most recently, Cao et al.
(2019); Ye et al. (2019); Shao et al. (2021) exploit
the semantic correlations between sentences and
programs to rule out spurious programs via jointly
learning semantic parser and sentence generator. In
this paper, we focus on a more complex problem,
learning program generation with (sentence, binary
label) pairs, in this field, and take the above ap-
proaches a step further by leveraging both lexical
features and structure features.
There already exist many works utilizing the
structural correlations between a sentence and its
programs. Previous works(Reddy et al., 2016; Hu
et al., 2018) directly transform the dependency
structure of a sentence into a program, which is
not satisfactory on complex sentences. In recent
years, some works(Wang et al., 2019; Herzig and
Berant, 2021; Li et al., 2021) treat structural con-
straints as latent variables, then parse a sentence
into a program under the constraints. However, it is
difficult to learn latent variables in a noisy environ-
ment. Simultaneously, modeling structural corre-
lations explicitly requres human annotations.(Sun
et al., 2020; Shi et al., 2020a). In this paper, we
propose a concise and robust method to integrate
the structural correlations into semantic parsing.
3 Model
Structure-Aware Semantic Parsing (SASP) centers
around the operation-oriented tree to deconstruct
some compositionality of statement and generate
program correctly. Figure 3 gives an overview of
our proposed SASP. In this section, we will first
introduce the task formulation, then describe how
to construct the operation-oriented tree, and give
the way to generate programs following the well-
designed tree at last.
3.1 Problem Formulation and Notations
Given a table T={cell|i≤R, j≤C}with
the table header H={col|j≤C}as evidence,
a statement S={w|i≤W}withWwords and
a true label y∈Y={True, False }where True
means Tentails SandFalse means Trefutes S,
we aim to train a model to do explainable veri-
fication. More specifically, we train a model to
translate Sinto an executable program z, then pre-
dict a label ˆy∈Yby accessing the table Twith
program zsuch that ˆy=y. Different from most
existing methods, which just pay attention to pre-
dicting a label ˆy∈Ysuch that ˆy=y, our model
also generates a program as accurate as possible to
explain and support the verification.
Program A program zcan be seen as a set of
executable operations{op|i≤M}. Consider-
ing the program example in figure 1, there are
six operations in total, and each operation op=
{op.func, ..., op.arg, ..., op.out}has one oper-
atorop.func (e.g., filter_eq in the figure), mul-
tiple operands op.arg,0< j≤νrelevant to
the table T(e.g., all_rows ,season and1981 ) and
one output v=op.out which may be selected
as an operand by subsequent operations. When
the whole program is executed by an interpreter,
it will be parsed into a tree as shown in figure 1
and executed from bottom to up. According to the
execution correctness and the semantic consistency,
we divide programs from the executable program
setZinto three categories, as shown in figure 2.
3.2 Operation-Oriented Dependency Tree
In this part, we first reveal the connection between
the program tree and the dependency tree. Then,
we design a unified operation-oriented dependency
tree for making full use of the connection.
Syntactic structures, the organization of tokens
in a sentence and how the contexts among them are
interrelated, can be revealed by a dependency tree
whose nodes and edges correspond to words and
grammatical relations in the sentence. We observe
that: (1) the operations related to descendants tend
to be executed before those related to ancestors in
the dependency tree; (2) the operator and operands7626
within one operation tend to have shorter distances
in the dependency tree; in the correct program com-
pared with the incorrect or spurious one. Use the
dependency tree in figure 3 and the program in fig-
ure 1 as an example. The operation filter_eq related
to the child node is executed before the operation
eq(v1, 8) corresponding to the father node. What’s
more, the distance of operands in the incorrect oper-
ation filter_eq(all_rows, podiums, 1981) is 6, while
that in filter_eq(all_rows, season, 1981) , a correct
operation, is just 1.
The observations above suggest that there exist
some structural correlations between a statement
and its programs. We will present how to make
use of them in the next section. Before that, we
propose an operation-oriented dependency tree to
strengthen the above rules in two steps. First, we
prune the original dependency tree to focus exclu-
sively on the operation-related structure. Then, we
merge the information around every operation to
make information in a single operation more com-
pact. What’s more, it is more convenient to define
and calculate the distance in a simplified tree.
The left part of figure 3 illustrates how to con-
struct the proposed tree. First of all, we do rule-
based entity linking to find potential operators and
operands from the statement. For operators de-
tection, we match strings between the statement
and the pre-defined trigger words, and give the
matched entities a function type. As for operands,
we divide them into two types, cellandcolumn , as
they are linked to table cells and the table header re-
spectively (e.g., 1981 has a celltype and season hasAlgorithm 1 Operation-oriented tree construction
Input: Dependency tree τwith root ρ, where ev-
ery node has a child list children , a type list
type and a value list val.
Output: Operation-oriented tree ˆτwith root ˆρ.
acolumn type). Then we pass tokens and linked en-
tities with types into a general dependency parser to
get a dependency tree τ. Every linked entity node
n={n.children, n.type, n.val }, n∈τhas a list
type with one type and a list valwith one entity.
For every token node, its type list and vallist are
both empty. After that, for every entity node with a
celltype value cell, we will add column andcol
into its type list and vallist respectively. At last,7627we call PRUNE in algorithm 1 using τas input
and get output ˆτ. The nodes left in the tree may
contain function info corresponding to the logical
operations, cell info and column info from tables.
3.3 Structure-Aware Semantic Parsing
In this section, we will introduce SASP, which
unifies both structural features and lexical features
with one operation-oriented dependency tree.
As shown in the right part of figure 3, we first
employ BERT (Devlin et al., 2019) to encode the
statement Sand the table Tfollowing TABERT
(Yin et al., 2020). Then we get representations
for the statement and entities with different types,
which will be fed into the decoder. During de-
coding, the logits are computed by an LSTM with
attention mechanism(Luong et al., 2015):
h=LSTM (h, x)
a=MLP ([h;Attention (h, S)])
l=MatMul (X, a)(1)
where his the hidden state, xis the token gen-
erated previously, Xis the candidate token list
selected from the vocabulary according to the to-
ken type at timestep t(e.g., the type for the second
token in the program being predicted is column ),
andlare the logits for the tth token over X.
However, in TFV , it is difficult to find the correct
optimization direction with only attention mecha-
nism, especially at the beginning of the training, be-
cause of the serious spurious problem. So we bias
the logits with our proposed tree additionally. As
a result, our model can give the correct program a
higher probability, therefore exploring search space
efficiently and evading spurious programs.
More specifically, we design two scoring mecha-
nisms in line with the two rules found in the previ-
ous section. As shown in algorithm 2, given λ <1,
score =λmeans the closer distances, the
higher scores. For operator selection, we calculate
the average distance from the candidate x∈X
to its leaves in the tree ˆτ, and set the distance to
be+∞if it is not in the tree. For example, the
candidate operator max (triggered by highest ) has a
score of λ. In this way, we give operators closer to
leaves higher scores, which leads to operations re-
lated to descendants being generated before those
related to ancestors. For operand selection, we
compute the average distance from the candidate
x∈Xto tokens in the operation op. Use the op-
eration in figure 3 as an example, the score of theAlgorithm 2 Scoring function with candidate to-
ken list X, operation-oriented tree ˆτand operation
being predicted opas input, where λ < 1is a
hyper-parameter.
candidate 1981 isλwhen the timestep t= 3. In
this way, we prioritize the tokens closed to existing
information in the operation being generated, so
that the distances inside one operation tend to be
shorter in the dependency tree. At last, we combine
the scores ζgiven by algorithm 2 and the logits
lcomputed by Equation 1 to get the final sample
distribution:
ζ=Score (X,ˆτ, op)
P(X|S, T, x) =Softmax (l+αζ)(2)
where αis a hyper-parameter, ˆτis the operation-
oriented tree and opis the operation being pre-
dicted. After we sample x∼P(X|S, T, x), it
will be used to update h,ˆτandop. We give more
details in Appendix A.3.
Previous works(Agarwal et al., 2019; Dasigi
et al., 2019) measure the relevance between a sen-
tence and a program by their coverage, and use that
lexical coverage to augment the reward function.
In a similar spirit, we design the reward based on
our proposed tree. Our intuition is that different
types of tokens play different roles in the operation-
oriented tree, and therefore should be treated under
varying degrees. And our reward is defined below.
R(z) =

Xσr, ˆy=y
0, otherwise(3)
where Type ={“function ”,“cell”,“column ”},
{r|κ∈Type}are relevances, {σ|κ∈Type}7628are hyper-parameters, and ˆyis the label predicted
by accessing the table Twith the program z. Since
all operation-related tokens of a statement are re-
served in the operation-oriented tree, we can cal-
culate the relevance between a statement and a
program by
r=P 1{∃i, n.type [i] =κ∧n.val [i]∈z}P 1{∃i, n.type [i] =κ}
(4)
where {n|n∈ˆτ}are nodes of our proposed tree.
For further improvement, we modify the general-
ized update equation in PolicyShaping (Misra et al.,
2018) to get Maximum Likelihood Most Violation
Reward. The final objective function is:
J=XXR(z)π(z|S, T)
−γmax(π(z|S, T))(5)
where Dcontains all S-Tpairs, Zis the set of
sampled executable programs, Z⊆Zis the
set of incorrect programs, πis the sample policy, γ
is a hyper-parameter and θcontains all the trainable
parameters. We think such an update equation more
robust than REINFORCE helps the model learn
better with many spurious programs in Z.
4 Experiments
4.1 Experimental Settings
Dataset and Evaluation Metrics We conduct
experiments on the large-scale dataset TABFACT
(Chen et al., 2020), which aims to study fact ver-
ification given semi-structured data as evidence.
TABFACT contains 16,573 tables and 118,275
statements which are divided into training (80%),
validation (10%) and testing (10%) sets. The test-
ing set is further partitioned into simple and com-
plex sets. The statements in the complex set are
more complicated in semantic compositionality
than those in the simple set. Because there is no
program ground-truth provided in TABFACT, we
just use the label accuracy as metric for comparison,
which is also called execution accuracy (Ex.Acc).
We also conduct experiments on WikiTableQues-
tion (WTQ) (Pasupat and Liang, 2015), a com-
monly used weakly supervised semantic parsing
dataset, for further evaluation. And we use the
same setting as previous works.
To test our performance on program generation,
we use Logic2Text, a dataset that contains around10,000 correct statement-table-program tuples, to
evaluate parse tree matching accuracy (PT.Match)
(Kim et al., 2020) for programs generated by our
method and other methods that also provide pro-
grams. Because there are only "ENTAILED" state-
ments in Logic2Text, we use the model trained on
TABFACT to predict programs without tuning.
Implementation Details We use CRF2o (Zhang
et al., 2020b) for dependency parsing. For semantic
parsing, we use pytorch neural symbolic machine
(Liang et al., 2017, 2018; Yin et al., 2020) as our
baseline and improve it with the operation-oriented
tree. Further, to bootstrap SASP, we use ζin Equa-
tion 2 to sample around 10 label consistent pro-
grams per example, and load them into memory
buffer before training. For BERT parameters, we
set the hidden size to 768, and use Adam optimizer
with lr 5e-5, warmup step 30k, dropout 0.2. For
LSTM parameters, we set hidden size to 200, and
use Adam optimizer with lr 3e-3, train step 150k,
dropout 0.2. As for hyper-parameters λ,α,σ,
σ,σ andγ, we set them to 0.7, 2, 0.2, 0.4,
0.4 and 0.2 respectively. All experiments were con-
ducted on a workstation with 128 GB of RAM and
2 RTX 3090 GPUs. Our source code is available at:
https://github.com/ousuixin/SASP .
Compared Systems We compare our model with
the following baselines, including six that focus on
label prediction and two that pay extra attention
to program generation. Among the former five
methods, Table-BERT (Chen et al., 2020) and SAT
(Zhang et al., 2020a) focus on table linearization,
so they use different ways to change 2-dimensional
tables into 1-dimensional sequences composed of
tokens, and then feed them into BERT for label
prediction. LFC (Zhong et al., 2020a), HeterTFV
(Shi et al., 2020b), ProgVGAT (Yang et al., 2020)
and LERGV(Shi et al., 2021) pay attention to com-
prehending tables and programs. They use differ-
ent ways to encode programs (generated by LPA-
ranking) and tables for verification, although the
programs they use are not precise at all. The latter
two methods will generate programs and use pro-
gram execution results as final predictions, includ-
ing LPA-ranking (Chen et al., 2020) and MAPO
(Liang et al., 2018) with BERT.
4.2 Experimental Results
Performance on TABFACT Table 1 gives the
overall performance of all eight baselines and our
proposed SASP, from which we can observe that:7629Model Val Test Test(Simple) Test(Complex)
Table-BERT 66.1 65.1 79.1 58.2
SAT 73.3 73.2 85.4 67.2
Tapas78.6 78.5 90.5 72.5
LFC 71.8 71.7 85.4 65.1
HeterTFV 72.5 72.3 85.9 65.7
ProgVGAT 74.9 74.4 88.3 67.6
LERGV 75.6 75.5 87.9 69.5
MAPO w/ BERT refined-reward 56.6 57.2 60.2 55.8
LPA-Ranking 65.2 65.0 78.4 58.5
SASP 75.0 74.9 87.6 68.8
(1) As a semantic parsing method, our method
achieves performance comparable to the State-Of-
The-Art method LERGV while maintaining ex-
plainability. This is what previous semantic parsers
can not do, and shows our superiority in TFV .
(2) Our proposed method works better than
Table-BERT and SAT, demonstrating the power of
the content snapshot proposed by Tabert in catch-
ing key information from a table.
(3) SASP has a lead of 1.2% on the the complex
set compared with ProgVGAT, but falls behind on
the simple set. There are two reasons for that. On
one hand, mainstream methods like ProgVGAT
can fix some errors caused by the symbolic inter-
preter (e.g., executing eq("USA", "America") to
False ). While SASP uses the execution result of
the generated program as prediction. Due to the
limited expression ability, our interpreter can not
cover every statement with a correct program, lead-
ing to a lower probability of predicting a correct
answer. On the other hand, ProgVGAT can not
deal with structural mistakes (e.g., replacing max
with minoperation) in programs generated by LPA.
As a result, ProgVGAT performs worse in com-
plicated semantic environment where LPA has a
higher probability of making a structural mistake.
(4) Our method outperforms MAPO and LPA
by significant margins, suggesting that SASP can
generate programs more accurately.
Performance on WTQ Table 2 shows the ex-
perimental results on WTQ. Our model just has
comparable performance with our baseline, MAPO
w/ BERT. We give two possible reasons below:Model Dev Test
Pasupat and Liang (2015) 37.0 37.1
Dasigi et al. (2019) 43.1 44.3
Agarwal et al. (2019) 43.2 44.1
Wang et al. (2019) 43.7 44.5
MAPO w/ BERT (Yin et al., 2020) 49.6 49.4
SASP 49.3 49.5
(1) As can be seen in figure 1, the program has
more than three operations, which is quite common
in TFV , while they use at most three operations to
answer a question in previous works (Pasupat and
Liang, 2015; Zhong et al., 2017; Liang et al., 2018).
Because the compositionality of WTQ is lower than
TABFACT, our proposed operation-oriented tree
can only provide very limited help.
(2) The spurious program problem is further
amplified by the binary label in TABFACT. Any
program that outputs a Boolean value has a 50%
chance of hitting the correct label; hence there are
many label consistent programs. While in WTQ,
it is not that easy to hit the correct label. Suppose
that the vocabulary list has Ntokens, but only one
token corresponds to the answer. Every executable
program in WTQ will output an answer with the
string type, so it only has aprobability of hitting
the correct label. WTQ has much fewer spurious
programs, so lexical features are enough to rule out7630Model PT.Match Ex.Acc
MAPO w/ BERT 13.4 70.1
LPA 15.6 56.7
SASP 47.9 75.9
spurious programs in WTQ in many cases.
Performance on Logic2Text Results of differ-
ent semantic parsing methods are shown in table 3.
Our model outperforms other methods with a con-
siderable margin on PT.Match metric. This means
SASP can generate more correct programs, which
makes it behave well in table fact verification.
In program generation for TFV , the search space
is too large to be explored completely. To tackle
this problem, MAPO w/ refined reward performs
systematic search space exploration guided by
lexical features in the advanced reward function.
It only obtains PT.Match accuracy of 13.4% on
Logic2Text. The high Ex.Acc score shows that it
just predicts spurious programs executed to "True".
For LPA, it first collects all programs under the
search space restricted by a lexical feature based
algorithm, then ranks these programs with a neural
network (BERT). And LPA also has poor behavior
in program generation here.
The big gaps (more than 40% in MAPO and
LPA) between PT.Match and Ex.Acc accuracy sug-
gest that with only lexical features, there are still
many spurious programs being explored. Use the
spurious program in figure 2 as an example, it con-
forms to lexical features by making full use of
sentence tokens, and would be a promising candi-
date in MAPO and LPA. However, such kind of
programs will differ from the correct ones in the
order of operators or the position of operands, so
they can be distinguished from correct programs by
structure features. Our method captures both lexi-
cal and structure features, therefore evading such
spurious programs and biasing generated programs
from label consistent towards semantic consistent.
The smaller gap (28% in SASP) between PT.Match
and Ex.Acc accuracy confirms our analysis above.
4.3 Ablation Study
Effect of Structural Info We further conduct an
ablation study to evaluate the necessity of leverag-
ing structure information through rules (1) and (2).Model Val Test
SASP w/o proposed tree 56.6 57.2
SASP w/o function type 59.3 60.1
SASP w/o column type 60.5 61.5
SASP w/o cell type 70.2 71.1
SASP 75.0 74.9
Model Val Test
SASP w/ binary-reward 60.1 60.2
SASP w/o violation 73.5 73.1
SASP 75.0 74.9
For rule (1), which defines the operator selection
mechanism, we just drop types and values related
tofunction in our proposed tree to see how it in-
fluence. For rule (2), which defines the operand
selection mechanism, we drop types and values re-
lated to cellorcolumn . If we drop all types from
the tree, the algorithm degenerates into MAPO w/
BERT refined-reward violation. The experimental
results are given in Table 4. We can see that func-
tionis the most important type, then is column type,
followed by celltype. And all of the types make
significant contributions to the final performance.
The results above show that both mechanisms asso-
ciated with the rule (1) and the rule (2) are crucial
for our model because both operator and operand
selections are crucial for program generation.
Effect of Objective Function To evaluate the im-
pact of the refined objective function in Equation 5,
we conduct another ablation study, and the results
are shown in table 5.
We change the reward function in Equation 3
with a binary reward function for comparison. The
result shows that refined feedback taking lexical
features into account plays an essential role in our
model. Without the refined reward, some opera-
tions may be omitted because the partial programs
are already executed to the right label, resulting in
a much worse performance.
We also remove the violation punishment to in-
vestigate the necessity of a conservative update pol-
icy. The result shows that the robust update policy7631
makes around 1% improvement. The reward func-
tion we designed just prioritizes programs that use
tokens related to logical operators or tables as much
as possible, leading to label inconsistent programs
that meet the condition. Giving such programs a
punishment complements the refined reward.
4.4 Case Study
In figure 4, we provide two cases to demonstrate
the effectiveness of our method for program gener-
ation. In both cases, our method generates correct
programs that are semantic consistent with the state-
ment, while LPA screws them all up. In the first
case, max is the descendant compared with minsk
in the dependency tree, so our method uses max be-
foreminsk , while LPA gets the wrong order. This
confirms that our method generates programs in
the correct order with the operator selection mecha-
nism. In the second case, devin has a more close re-
lation to notin the dependency tree, so our method
chooses devin as an operand of filter_not_eq , while
LPA selects an incorrect operand milwaukee for
filter_not_eq . This confirms that our method gen-
erates single operations correctly with the operand
selection mechanism.
4.5 Error Analysis
To check the generalizability and limitations of our
proposed method, we randomly sampled 200 exam-
ples from the validation set of TABFACT, and man-
ually inspected the top one program of the beam
search using SASP. We found that SASP generated
correct programs for 99 examples, spurious pro-
grams for 57 examples and incorrect programs for
44 examples. The proportion of correct programs
(49.5%) and spurious programs (28.5%) is similar
to that in table 3 (47.9% and 28%). This shows
the generalizability of SASP and the rationality of
using Logic2Text for PT.Match evaluation. What’s
more, we classified the causes of 101 spurious orincorrect programs into four main categories.
Unsupported operations cause 30 error examples.
For instance, in "the new york rangers beat the at-
lanta flames by 2 points" , the minus operation in
a single table cell "4 - 2" is not supported by our
interpreter. The second category of errors occur
when the functions or entities can not be detected
and added to dependency tree nodes correctly. Use
"the maroon played 3 teams located in the united
states" as an example, "the united states" can not
be linked to "America" in the given table; hence
it will not be added to the operation tree. 31 er-
ror examples are caused by this reason. The first
two categories can not be handled by our proposed
method, and we leave the development of powerful
interpreter and robust entity linker for future work.
The third category is structure error, causing
13 error examples. In other words, the order of
operators or the position of operands in the pre-
dicted program differs from the correct one. The
wrong programs in figure 2 are all this kind of
error cases. Underutilized information causes 23
error examples. For the statement in figure 1, "fil-
ter_eq(all_rows, season, 1981); max(v0, podiums),
eq(v1, 8)" causes this kind of error.
5 Conclusion
In this paper, we have proposed a novel approach
to do explainable verification by structure-aware
semantic parsing. Firstly, we define a unified
operation-oriented tree by entity linking, depen-
dency parsing and tree pruning. Then, we demon-
strate how to integrate our proposed tree into se-
mantic parsing with the operator-related and the
operand-related principles. At last, we introduce
the refined objective function which could reduce
the influence of spurious programs. Experimen-
tal results confirm that our proposed method can
bias program generation from label consistent to-
wards semantic consistent and achieve acceptable
performance on the benchmark dataset TABFACT.
Future work will collect evidences that are more
precise and get better verification performance by
replacing LPA with SASP in the first stage of main-
stream methods.
Acknowledgement
We appreciate the discussion with Weilin Luo,
Weinan He and Yeliang Xiu. We acknowledge
support from the Natural Science Foundation of
China under Grant No. 62076261.7632References76337634A Appendix
A.1 Pre-difined API
As shown in figure 3 and algorithm 1, when we gen-
erate an operation, we first select an operand, then
check the pre-defined API to get the type informa-
tion, and finally select operands under the specific
type (according to the type information). Here we
list detailed descriptions for the pre-defined API in
table 6.
Actually, there are seven different types, includ-
ingFunction ,Cell-String ,Cell-Number ,Bool,Sub-
Table ,Column-String andColumn-Number . In fig-
ure 3 and algorithm 1, we divide them into three
types for a clearer illustration:
function cell column
FunctionBool,
Sub-Table,
Cell-String,
Cell-NumberColumn-String,
Column-Number
In practice, we will select operands according to
more detailed type information given by our prede-
fined API.
In addition, we will update cell values and rep-
resentations by adding the execution result of op
and the LSTM hidden state htoCell andC, re-
spectively (Line 12). In practice, we will maintain
more detailed symbol lists and representation lists.
For example, when the last token of the operation
max(v0, podiums) is generated, the hidden state of
LSTM will be added into the C-Number list, while
the execution result of max(v0, podiums) ,v=8, will
be put into the Cell-Number list.
A.2 Pre-difined Trigger Words
In the first step of the operation-oriented tree con-
struction, we match strings between the statement
and the pre-defined trigger words to find underly-
ing operators. Here we give details about the pre-
defined trigger words in table 7, partly following
LPA (Chen et al., 2020).
A.3 Implement Details for Decoding Module
Algorithm 3 gives the complete process of our de-
coding module. We initialize the program as an
empty list with no operations, then enlarge it with
operations generated progressively until the neural
network outputs a "stop" token (Line 13-15). As
for operation generation, we first sample an opera-
tor from the operator list Func , then get the type7635information of its operands through the pre-defined
API , with which we can choose representations
under the correct type. After that, we will sam-
ple operands from Cell if the operand type is cell,
and sample them from Header otherwise. Once
the generation is finished, the whole expression is
added to the program (Line 3-11). All these above
are similar to what they do in NSM (Liang et al.,
2017). But we redesign the SAMPLE function
according to equation 2.
Besides, to maintain our proposed tree ˆτ, we
will update information by dropping out the used
operators and celltype operands in op. What’s
more, we will update C, the cell representation list,
by adding current hidden state into into C(Line
12). At the same time, opandCell are updated by
adding the execution result of op(v=op.out ).Algorithm 3 Program sampling with statement
representation V, table cell representation list
C={V|c∈Cell}, table column representation
listH={V|h∈Header }, operator representa-
tion list F={V|f∈Func}, special token list
E={V|e∈ {continue, stop }}, the pre-defined
API , neural network LSTM and the operation-
oriented tree ˆτas input.76367637Operator (function) Trigger word list
filter _str_contain _not_any,
filter_not_eq["other than", "not", "no", "never", "n’t"]
is_none ["none", "neither", "not", "no", "never", "n’t"]
is_not ["not", "no", "never", "n’t"]
filter _less_eq, row _less_eq,
less_eq,["at most"]
filter _greater _eq,
row_greater_eq, greater["at least"]
filter_less, less, row_less ["less", "sooner", "faster", "closer", "earlier", "lesser", "smaller",
"younger", "worse", "shorter", "fewer", "lower", "behind", "below",
"before", "under"]
filter _greater, row _greater,
greater["longer", "taller", "older", "more", "greater", "larger", "slower", "big-
ger", "better", "higher", "faster", "later", "above", "over", "after"]
same ["same"]
diff ["difference", "gap"]
sum ["total", "sum", "summation"]
avg ["average", "avg", "mean"]
argmax, max ["greatest", "biggest", "tallest", "strongest", "highest", "longest",
"largest", "oldest", "most", "fastest", "best", "latest", "top", "first",
"max", "maximum"]
argmin, min ["fewest", "closest", "earliest", "smallest", "lowest", "shortest", "poor-
est", "youngest", "nearest", "least", "slowest", "worst", "latest", "bot-
tom", "last", "minimum"]
mode ["most", "majority", "main", "usually"]
only ["only"]
all ["always", "all", "every", "each"]7638