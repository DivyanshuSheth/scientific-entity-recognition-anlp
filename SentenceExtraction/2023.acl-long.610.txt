
Zhilei Hu, Zixuan Li, Xiaolong Jin, Long Bai, Saiping Guan,
Jiafeng Guo and Xueqi Cheng
School of Computer Science and Technology, University of Chinese Academy of Sciences;
CAS Key Laboratory of Network Data Science and Technology,
Institute of Computing Technology, Chinese Academy of Sciences.
{huzhilei19b, lizixuan, jinxiaolong, bailong18b, guansaiping}@ict.ac.cn
{guojiafeng, cxq}@ict.ac.cn
Abstract
Event Causality Identification (ECI) aims to
identify causal relations between events in un-
structured texts. This is a very challenging task,
because causal relations are usually expressed
by implicit associations between events. Exist-
ing methods usually capture such associations
by directly modeling the texts with pre-trained
language models, which underestimate two
kinds of semantic structures vital to the ECI
task, namely, event-centric structure and event-
associated structure. The former includes im-
portant semantic elements related to the events
to describe them more precisely, while the lat-
ter contains semantic paths between two events
to provide possible supports for ECI. In this
paper, we study the implicit associations be-
tween events by modeling the above explicit
semantic structures, and propose a Semantic
Structure Integration model ( SemSIn ). It uti-
lizes a GNN-based event aggregator to integrate
the event-centric structure information, and em-
ploys an LSTM-based path aggregator to cap-
ture the event-associated structure information
between two events. Experimental results on
three widely used datasets show that SemSIn
achieves significant improvements over base-
line methods.
1 Introduction
Event Causality Identification (ECI) is an impor-
tant task in natural language processing that seeks
to predict causal relations between events in texts.
As shown in the top of Figure 1, given the unstruc-
tured text and event pair ( shot,protect ), an ECI
model needs to identify that there exists a causal re-
lation between two events, i.e., protect−→ shot.
ECI is an important way to construct wide causal
connections among events, which supports a va-
riety of practical applications, such as event pre-
diction (Hashimoto, 2019), reading comprehen-
sion (Berant et al., 2014), and question answer-
ing (Oh et al., 2013, 2017).Figure 1: An example of the ECI task, as well as the se-
mantic graph and semantic structures corresponding to
the unstructured text. The orange nodes denote events.
ECI is challenging because most causal relations
are expressed by texts implicitly, which requires the
model to understand the associations between two
events adequately. Existing methods directly model
the texts with the Pre-trained Language Model
(PLM) (Liu et al., 2020; Cao et al., 2021; Zuo et al.,
2020, 2021b). They mainly rely on the ability of
PLMs, which cannot capture associations between
events comprehensively. To enrich the associa-
tions between events, some methods (Liu et al.,
2020; Cao et al., 2021) introduce external knowl-
edge, such as events in ConceptNet (Speer et al.,
2017) that are related to focused events. Other
methods (Zuo et al., 2020, 2021b) utilize the data
augment framework to generate more training data
for the model. However, the above methods are far
from fully modeling the associations among events
in the texts.
Actually, texts contain rich semantic elements
and their associations, which form graph-like se-
mantic structures, i.e., semantic graphs. Figure 110901shows the semantic graph generated by the Abstract
Meaning Representation (AMR) (Banarescu et al.,
2013) parser for the corresponding text, where the
nodes indicate events, entities, concepts, and other
semantic elements, while edges with semantic roles
describe the associations among semantic elements.
For example, “protect-01” indicates the specific
sense of the verb “protect” in the PropBank (Palmer
et al., 2005). “ARG0”, “ARG1” and “ARG2” in-
dicate different semantic roles. In this semantic
graph, we exploit two kinds of structures vital to the
ECI task, namely, event-centric structure and event-
associated structure. As shown in the bottom left
of Figure 1, the event-centric structure consists of
events and their neighbors, which describes events
more precisely by considering their arguments and
corresponding roles. For example, besides event
“protect-01”, “person (Horton)” and “person (stu-
dent)” are also important semantic elements, and
their corresponding semantic roles can supply the
extra information for the event. As shown in the
bottom right of Figure 1, the event-associated struc-
ture contains semantic paths between two events,
and each path contains the core semantic elements.
The composition of these elements indicates the
possible semantic relations between events and
provides supports for ECI. For example, the path
“protect-01− − − − → person
− − − − − → shoot-02 ” in-
dicates that “person (Horton)” protects somebody
first and then was shot. Events “protect-01” and
“shoot-02” share the same participant, and there
may exist a causal relation between them.
To make use of the above semantic structures
in texts to carry out the ECI task, we propose a
new Semantic Structure Integration model (Sem-
SIn). It first employs an AMR parser to convert
each unstructured text into a semantic graph and
obtains the above two kinds of semantic structures
in that graph. For the event-centric structure, Sem-
SIn adopts an event aggregator based on Graph
Neural Networks (GNN). It aggregates the infor-
mation of the neighboring nodes to the event nodes
to obtain more precise representations of the events.
For the event-associated structure, SemSIn utilizes
a path aggregator based on Long Short-Term Mem-
ory (LSTM) network. It encodes the compositional
semantic information in the paths and then inte-
grates the information of multiple paths with an at-
tention mechanism. With the above representations
of the events and paths as input, SemSIn conductsECI with a Multi-Layer Perceptron (MLP).
In general, the main contributions of this paper
can be summarized as follows:
•We exploit two kinds of critical semantic struc-
tures for the ECI task, namely, event-centric
structure and event-associated structure. They
can explicitly consider the associations be-
tween events and their arguments, as well as
the associations between event pairs.
•We propose a novel Semantic Structure In-
tegration (SemSIn) model, which utilizes an
event aggregator and a path aggregator to in-
tegrate the above two kinds of semantic struc-
ture information.
•According to experimental results on three
widely used datasets, SemSIn achieves 3.5%
improvements of the F1 score compared to the
state-of-the-art baselines.
2 Related Work
Identifying causal relations between events has at-
tracted extensive attention in the past few years.
Early methods mainly rely on the causal association
rule (Beamer and Girju, 2009; Do et al., 2011) and
causal patterns (Hashimoto et al., 2014; Riaz and
Girju, 2010, 2014a; Hidey and McKeown, 2016).
Some following methods exploit lexical and syn-
tactic features to improve performance (Riaz and
Girju, 2013, 2014b).
Recently, most of works apply PLM to conduct
ECI (Liu et al., 2020; Cao et al., 2021; Zuo et al.,
2021b). Although PLM has a strong ability for cap-
turing the associations among tokens in the texts,
they are not capable of this task because the asso-
ciations between events are implicit. To enhance
PLM, recent works try to introduce external knowl-
edge. Liu et al. (2020) proposed a method to enrich
the representations of events using commonsense
knowledge related to events from the knowledge
graph ConceptNet (Speer et al., 2017). Cao et al.
(2021) further proposed a model to exploit knowl-
edge connecting events in the ConceptNet for rea-
soning. Zuo et al. (2021b) proposed a data aug-
mented method to generate more training samples.
Instead of introducing external knowledge to en-
hance the abilities of the ECI model, we attempt to
dive deep into the useful semantic structure infor-
mation in the texts.10902
3 The SemSIn Model
In this section, we introduce the proposed SemSIn
model. Figure 2 illustrates the overall architecture
of the SemSIn model. Given an input text, SemSIn
first uses a pre-trained AMR parser to obtain the
corresponding semantic graph of the text. Then,
the event-centric structure and the event-associated
structure constructed from the semantic graph, as
well as the original text, are fed into the following
three components respectively: (1) Event aggre-
gator aggregates the event-centric structure infor-
mation into the representation of the event pair.
(2) Path aggregator captures the event-associated
structure information between two events into the
path representation. (3) Context encoder encodes
the text and obtains the contextual representation
of the event pair. With the above representations
as input, SemSin conducts binary classification to
get the final results with an MLP layer. Next, we
will first introduce the construction process of the
semantic graph and then present these three main
components in detail.
3.1 Semantic Graph Construction
The core motivation of SemSIn is to model the im-
plicit associations between events by introducing
explicit semantic structures. To get explicit seman-
tic structures from texts, SemSIn employs an AMR
parser to convert the original text into an AMR
graph, which contains fine-grained node and edge
types (Zhang and Ji, 2021).
In the AMR graph, the nodes indicate specificSemantic Roles Types
ARG0, ARG1, ARG2, ··· Core Roles
op1, op2, op3, op4 Operators
manner, instrument, topic, ··· Means
time, year, weekday, ··· Temporal
Other semantic roles Others
semantic elements and the edges indicate the se-
mantic roles among them. Table 1 lists the used
semantic roles in AMR graph. We then add inverse
edges to all the edges in the AMR graph to form the
final semantic graph, making it reachable between
any two nodes. Formally, a semantic graph is de-
fined as G= (V, E, R ), where V,EandRare the
sets of nodes, edges and role types, respectively.
3.2 Event Aggregator
Identifying the causal relation between two events
requires the model to comprehensively understand
what each event describes. Existing methods use
the event mentions in the text to represent the
events, which cannot highlight the semantic ele-
ments related to the events. Besides the event men-
tions, events usually have associations with their
arguments mentioned in the text. Similarly, event
arguments also have associations with some related
semantic elements. Therefore, to model this kind
of association, SemSIn obtains the event-centric
structure from the constructed semantic graph by
simply using the L-hop subgraph of the focused10903event, where Lis a hyperparameter.
Node Representation Initialization: To initial-
ize the representations of nodes in the event-centric
structure, a rule-based alignment toolis first em-
ployed to align AMR nodes to the tokens in the text.
For the AMR nodes that have the corresponding to-
kens in the text, their initialized representations are
obtained by averaging the representation vectors
of all tokens aligned to the nodes. For example,
given a node, the start and end positions of its cor-
responding tokens are aandb, respectively. Its
representation vector is calculated by:
h=1
|b−a+ 1|/summationdisplayx, (1)
where xis the representation of the token k. A
PLM, BERT (Devlin et al., 2019), is applied to
encode the sequence of tokens. For those nodes
without corresponding tokens in the original text
(i.e., auxiliary nodes added by the AMR parser,
such as “name” and “cause-01” in Figure 1), their
representations are randomly initialized.
Semantic Information Aggregation : The graph
convolutional network has the property of ag-
gregating the information of neighbor nodes to
the specific node, which is suitable to model the
event-centric structure. In addition, the types
of edges in the semantic graph also contain spe-
cial information that can be used to distinguish
the relations between nodes. Therefore, we ap-
ply a Relational Graph Convolutional Network
(RGCN) (Schlichtkrull et al., 2018) to aggregate
semantic information from L-hop neighbors of the
focused events. Specifically, the message passing
at layer l∈[0, L−1]is conducted as follows:
h=σ
/summationdisplay/summationdisplay1
cWh+Wh
,
(2)
where Rdenotes the set of the role types; Nde-
notes the set of the neighbors of node iunder rela-
tionr∈R;cis a normalization constant equal to
|N|;handhdenote the llayer representations
of the nodes iandj, respectively; WandWare
the weight matrices for aggregating features from
different relations and self-loop in the llayer; σ
is an activation function (e.g., ReLU); handh
are the initialized representations of the nodes intro-
duced above. After aggregating the event-centricstructure information, the representations of eand
eare denoted as handh. In addition, to elim-
inate the effect of the relative position of the two
events, we sum up the representations of the two
events to obtain F, the representation of the
event pair,
F=h+h. (3)
3.3 Path Aggregator
Besides the associations between events and their
arguments, identifying the causal relation requires
the model to discover the association between two
events. The paths in the semantic graph between
two events can reflect this kind of association. Sem-
SIn thus first finds paths between two events in
the semantic graph to form the event-associated
structure. Then, SemSIn encodes it via BILSTM
and path attention to get the representations of the
paths.
With the intuition that the fewer hops in the path,
the stronger information it contains to reflect the
association between two events, we choose the
shortest path between two event nodes in the se-
mantic graph to form the event-associated struc-
ture. This operation can avoid introducing re-
dundant information and improve efficiency. Be-
sides, we add the reverse path for each seman-
tic path. Formally, if there is a path denoted as
(v, r, v,···, v, r, v), the corresponding
reverse path is (v, r, v,···, v, r, v).
Path Encoding: The compositional semantic
information of the semantic elements and roles in
paths can provide possible supports to the causal
relation. Recently, recurrent neural networks have
been widely used in processing sequence data such
as path information (Wang et al., 2019; Huang
et al., 2021). Therefore, we apply a BiLSTM to bet-
ter encode each path in the event-associated struc-
ture and output its representations. Here, the ini-
tialized representations of all nodes are obtained
by applying the RGCN to the whole semantic
graph, while the representations of relations are
randomly initialized and updated during the train-
ing process. To convert multi-hop paths into a
sequence of vectors, we concatenate node and re-
lation representation vectors as the input at each
state. For example, the sequence is organized as
[(v,r); (v,r);···; (v,r)], where vde-
notes the representation of the node i;rdenotes
the representation of the relation i;rdenotes the
representation of the special PAD relation added to10904the last state. Then, the representation Pof this
path can be obtained:
P= BiLSTM [( v,r);···; (v,r)].(4)
Path Attention: There may exist multiple paths
with the same shortest length. Different paths re-
flect different semantic information. Thus, to dis-
tinguish the importances of different paths, Sem-
SIn adopts an attention mechanism to integrate the
information of multiple paths. The query for atten-
tion is the representation of the event pair F,
which is obtained from the event aggregator. Both
key and value are the representation Pof the path:
α=(FW)(PW)
√d, (5)
F=/summationdisplaySoftmax( α)(PW),(6)
where W,WandWare parameter weights;
αdenotes the salient score for path ito event pair
F;Fis the integrated representation
of multiple paths.
3.4 Context Encoder
Besides the above semantic structure information,
the contextual semantic information is proved to be
useful for ECI (Cao et al., 2021). Thus, we adopt
an extra context encoder to encode the tokens of the
text and obtain the contextual semantic representa-
tion of the event pair. Specifically, we first add two
pairs of special markers <e1></e1> and <e2></e2>
to indicate the boundaries of the two event men-
tions. Two special tokens [CLS] and [SEP] are
also added to indicate the beginning and end of the
whole text, respectively. To model the representa-
tions of the tokens in the context encoder and event
aggregator separately, here we adopt another BERT
model to encode the context. Following Liu et al.
(2020), we use the representations of the tokens
<e1> and <e2> as the representations of the two
events, i.e., eande. And the representation of
the token [CLS] is adopted as that of the whole text.
In order to achieve sufficient interaction between
the events and their corresponding contexts, we
apply a linear layer and an activation function to
obtain more accurate representations of the events:
˜u= tanh( W[u||u] +b), (7)
where ||represents the concatenation operation.
uanduare the representations of the wholetext and e,i∈(1,2), respectively. Wandb
are the weight matrix and the bias, respectively.
We again sum up the representations of the two
events as the representation of the event pair:
F=˜u+˜u, (8)
where Fis the contextual representation of
the event pair and will be used for further computa-
tion.
3.5 Model Prediction
We concatenate the representations obtained from
the above three components as the final representa-
tion of each event pair:
F=F||F||F.(9)
Then, Fis fed into the softmax layer for
classification,
p= softmax( WF+b),(10)
where pis the probability indicating whether there
is a causal relation between two events; Wand
bare trainable parameters.
3.6 Parameter Learning
For the classification task, the model generally
adopts the cross-entropy loss function and treats
all samples equally. However, most of the samples
without causality are easily predicted and these
samples will dominate the total loss. In order to
pay more attention to samples that are difficult to
predict, we adopt focal loss (Lin et al., 2017) as the
loss function of our model:
J(Θ) = −/summationdisplayβ(1−p)log(p),(11)
where Θdenotes the model parameters; (e, e)
denotes the sample in the training set E. Besides,
to balance the importance of positive and negative
samples, we add the loss weighting factor β∈
[0,1]for the class “positive” and 1−βfor the class
“negative”.
4 Experiments
4.1 Datasets and Metrics
We evaluate the proposed SemSIn on two datasets
from EventStoryLine Corpus v0.9 (ESC) (Caselli
and V ossen, 2017) and one dataset from Causal-
TimeBank (Causal-TB) (Mirza et al., 2014),
namely, ESC, ESCand Causal-TB.
ESCcontains 22 topics, 258 documents, and109055334 event mentions. The dataset is processed
following Gao et al. (2019), excluding aspectual,
causative, perception, and reporting event men-
tions, most of which are not annotated with any
causality. After processing, there are 7805 intra-
sentence event mention pairs in the corpus, of
which 1770 (22.7%) are annotated with a causal
relation. The same as previous methods (Gao et al.,
2019; Zuo et al., 2021b), we use the documents in
the last two topics as the development set, and re-
port the experimental results by conducting 5-fold
cross-validation on the remaining 20 topics. The
dataset used in the cross-validation evaluation is
partitioned as follows: documents are sorted ac-
cording to their topic IDs, which means that the
training and test sets are cross-topic. Under this set-
ting, the data distributions of the training and test
sets are inconsistent, and the generalization ability
of the model is mainly evaluated.
ESCis another data partition setting for the
ESC dataset, which is used in Man et al. (2022).
In this dataset, documents are randomly shuffled
based on their document names without sorting ac-
cording to their topic IDs. Thus, the training and
test sets have data on all topics. Under this setting,
the data distributions of the training and test sets
are more consistent, and it can better reflect the
performance of the model under the same distribu-
tion of data. In real data, some causal event pairs
are mostly appeared in topic-specific documents,
because the event type is related to the topic of the
document. This phenomenon inspires us to split
the dataset in two different ways, i.e., cross-topic
partition (ESC) and random partition (ESC*).
Causal-TBcontains 183 documents and 6811
event mentions. There are 9721 intra-sentence
event mention pairs in the corpus, of which 298
(3.1%) are annotated with a causal relation. Simi-
lar to Liu et al. (2020), we conduct 10-fold cross-
validation for Causal-TB.
Evaluation Metrics. For evaluation, we adopt
widely used Precision (P), Recall (R), and F1-score
(F1) as evaluation metrics.
4.2 Expeimental Setup
Implementation Details. In the experi-
ments, we use the pre-trained AMR parser
parse_xfm_bart_large v0.1.0. The PLM used in
this paper is BERT-base (Devlin et al., 2019) and
it is fine-tuned during the training process. The
representation dimension of nodes and relations
is set to 768, the same as the representation
dimension of tokens. The NetwokX toolkitis
adopted to obtain the shortest path between two
events. The learning rate of the model is set to 1e-5
and the dropout rate is set to 0.5. We perform grid
search on the number of the RGCN layers, and it
is experimentally set to 3. γin focal loss is set to 2.
βis set to 0.5 and 0.75 for ESC and Causal-TB,
respectively. The batch size is set to 20 for all the
three datasets. The AdamW gradient strategy is
used to optimize all parameters. Due to the sparsity
of causality in the Causal-TB dataset, we use
both positive and negative sampling strategies for
training. The positive sampling rate and negative
sampling rate are set to 5 and 0.3, respectively.
Baseline Methods. We compare the proposed
SemSIn method with two types of existing state-
of-the-art (SOTA) methods, namely, feature-based
ones and PLM-based ones. For the ESC dataset, the10906following baselines are adopted: LSTM (Cheng
and Miyao, 2017) is a sequential model based
on dependency paths; Seq(Choubey and Huang,
2017) is a sequential model that explores context
word sequences; LR+ andILP (Gao et al., 2019),
they consider the document-level causal structure.
For Causal-TB, the following baselines are se-
lected: RB(Mirza and Tonelli, 2014) is a rule-
based method; DD(Mirza and Tonelli, 2014) is a
data-driven machine learning based method; VR-
C(Mirza, 2014) is a verb rule-based model with
lexical information and causal signals.
In addition, we also compare SemSIn with typi-
cal methods based on PLMs. KnowDis (Zuo et al.,
2020) is a knowledge enhanced distant data aug-
mentation framework; MM (Liu et al., 2020) is a
knowledge enhanced method with mention mask-
ing generalization; CauSeRL (Zuo et al., 2021a) is
a self-supervised method; LSIN (Cao et al., 2021)
is a method that constructs a descriptive graph
to explore external knowledge; LearnDA (Zuo
et al., 2021b) is a learnable knowledge-guided
data augmentation framework; T5 Classify and
GenECI (Man et al., 2022) are the methods that
formulate ECI as a generation problem.
4.3 Experimental Results
Tables 2 and 3 present the experimental results
on the ESC and Causal-TB datasets, respectively.
Overall, our method outperforms all baselines in
terms of the F1-score on both datasets. Compared
with the SOTA methods, SemSIn achieves more
than 3.5% and 1.8% improvement on the ESC and
Causal-TB datasets, respectively. Note that, al-
though our method does not utilize external knowl-
edge, it still achieves better results than the SOTA
methods. The reason is that our method makes bet-
ter use of the semantic structure information in the
texts. The results indicate that the texts still contain
a considerable amount of useful information for
the ECI task that can be mined and exploited.
Compared with the SOTA method LearnDA in
Table 2, SemSIn achieves a significant improve-
ment of 8.3% in precision on the ESC dataset. This
suggests that SemSIn can better model the implicit
associations between two events. It can be ob-
served that LearnDA has a higher recall score than
SemSIn. The possible reason is that LearnDA can
generate event pairs out of the training set. Ex-
tra training samples make the model recall more
samples and get a higher recall score.Method P R F1 ∆
SemSIn 49.8 49.0 49.4 -
SemSIn 49.3 52.6 50.9 +1.5
SemSIn 44.5 63.6 52.4 +3.0
SemSIn 50.5 63.0 56.1 +6.7
To verify the effectiveness of the model on the
ESCdataset, we compare the proposed method
with the SOTA T5 Classify and GenECI methods.
The results are in the bottom of Table 2. Sem-
SIn achieves 4.7%, 8.6%, and 6.1% improvements
of the precision, recall and F1-score, respectively.
This again justifies that using semantic structures
is beneficial for ECI.
Comparing the results of SemSIn and SemSIn
in Table 2, the experimental results under different
settings have a large gap. The results on ESC are
significantly higher than those on ESC. This is
because the training and test data for ESC are cross-
topic, and data on different topics usually involve
diverse events. Dealing with unseen event pairs is
difficult, thus it is more challenging to conduct the
ECI task on ESC than ESC.
4.4 Ablation Studies
To illustrate the effect of two kinds of semantic
structures, we conduct ablation experiments on
the ESC dataset. The results are presented in Ta-
ble 4. w/o.stru indicates the model predicts event
causality without two kinds of semantic structures.
w/o.path andw/o.cent indicate without the event-
associated structure and without the event-centric
structure, respectively.
Impact of the Event-centric Structure. Com-
pared with SemSIn, SemSIn has a 6.0%
decrease of the precision score. By consid-
ering the event-centric structure information,
the model can describe events more accurately.
Thus SemSIn is worse than SemSIn.
Comparing SemSIn with SemSIn ,
SemSIn achieves 1.5% improvements of
the F1 score. It proves that the associations be-
tween events and their arguments are vital for the
ECI task. The event-centric Structure information
can enhance BERT with the ability to capture these
associations.10907Method P R F1
CompGCN 46.8 62.0 53.3
GCN 50.3 56.8 53.4
RGCN 50.5 63.0 56.1
Impact of the Event-associated Structure.
Compared with SemSIn, SemSIn has a
10.4% decrease of the recall score. This indicates
that the event-associated structure information can
help the model discover more causal clues be-
tween two events. Comparing SemSIn with
SemSIn , SemSIn achieves 3.0% im-
provements of the F1 score. It proves that the asso-
ciations between events are vital for this task.
4.5 Sub-module Analysis
Impact of Relations in the Path. In the phase of
acquiring semantic paths between two events, we
keep only the nodes in the paths and neglect the
edges. This method achieves an F1 score of 53.3%
on ESC, which is a 2.8% reduction compared to
the model that considers both nodes and edges. It
suggests that the relations between elements are
also useful for identifying causality.
Impact of the Path Attention. In the multi-path
information integration phase, we replace the atten-
tion mechanism with a method that averages the
representations of multiple paths. This approach
obtains an F1 score of 54.0% on ESC, which is a
2.1% reduction compared to the model utilizing
the attention mechanism. This shows that the “Path
Attention” sub-module can effectively aggregate
information from multiple paths.
4.6 Graph Encoder Analysis
Impact of the Graph Encoder. To analyze
the effect of graph encoders on experimen-
tal results, we utilized three different graph
encoders, namely, GCN (Kipf and Welling,
2017), CompGCN (Vashishth et al., 2020), and
RGCN (Schlichtkrull et al., 2018). The results are
shown in Table 5. From the results, we can observe
that the best result is achieved with the model us-
ing the RGCN graph encoder. This suggests that
RGCN has the capability to utilize the edge-type
information in the semantic graph more effectively,
enabling more accurate aggregation of information
from surrounding nodes.
Impact of the Number of the RGCN Layers.
The number of the RGCN layers Lis an important
parameter of the model, which means that nodes
can aggregate information from their L-hop neigh-
bors through message passing. We evaluate per-
formance of the model with different numbers of
the RGCN layers on ESC. The results are shown
in Figure 3. A larger Lcan get better results when
L <= 3. This is because that events usually have
associations with their arguments mentioned in the
text and event arguments also have associations
with some related semantic elements. Thus intro-
ducing a relative large Lcan describe the events
more precisely. It can be observed that the model
performance decreases significantly when L >3.
The reason may be that the larger Lmay introduce
some noisy nodes or the RGCN encounters the
over-smoothing problem (Kipf and Welling, 2016).
4.7 Case Studies
To well demonstrate how semantic structures can
help improve performance, a few cases are stud-
ied. Figure 4 shows two cases where causal re-
lations between events are implicit. Here, BERT
predicts the wrong answers and SemSIn predicts
the correct ones, which demonstrates that leverag-
ing the semantic structure information can effec-
tively enhance ECI. In Case 1, the meaning of the
“charge-05” is that “make an allegation or crim-
inal charge against someone”. Its event-centric
structure information includes “someone is facing
a charge”, “a person is charged” and “the charge
is for causing something to happen”, which are
the elements directly related to the event. By ag-
gregating the information of these elements, the
event is semantically represented more precisely. In
Case 2, the causal relation between the two events10908
“tremor” and “kill” is expressed indirectly through
the “tsunami” event. Specifically, it can be deduced
using “tremor sparked a tsunami” and “the tsunami
killed tens of thousands of people”. The model
effectively utilizes the event-associated structure
information to capture the associations between
events.
5 Conclusions
In this paper, we proposed a new semantic structure
integration model (SemSIn) for ECI, which lever-
aged two kinds of semantic structures, i.e., event-
centric structure and event-associated structure. An
event aggregator was utilized to aggregate event-
centric structure information and a path aggregator
was proposed to capture event-associated structure
information between two events. Experimental re-
sults on three widely used datasets demonstrate that
introducing semantic structure information helps
improve the performance of the ECI task.
Limitations
The limitations of this work can be concluded into
two points: (1) To obtain the associations between
semantic elements, SemSIn needs to transform the
texts into the corresponding semantic graphs. Ex-
isting methods can only transform single sentences
into semantic graphs, and cannot parse texts con-
taining multiple sentences. Therefore, this method
is not suitable for identifying causal relations be-
tween events in different sentences. (2) SemSIn
only exploits the semantic structures of the texts
and does not utilize external knowledge. External
knowledge is also important for the ECI task, and
simultaneously exploiting semantic structures andexternal knowledge is a good direction for future
studies.
Acknowledgements
The work is supported by the National Natural Sci-
ence Foundation of China under grant U1911401,
the National Key Research and Development
Project of China, the JCJQ Project of China, Bei-
jing Academy of Artificial Intelligence under grant
BAAI2019ZD0306, and the Lenovo-CAS Joint
Lab Youth Scientist Project. We thank anonymous
reviewers for their insightful comments and sug-
gestions.
References109091091010911ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
In the limitations section.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and introduction section.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
Section 4.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.10912/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4.2.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.1.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4.2.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.10913