
Yacine Gaci, Boualem Benatallah, Fabio Casati, Khalid BenabdeslemLIRIS - University of Lyon 1, FranceDublin City University, IrelandUNSW, Sydney, AustraliaServiceNow, USA
{yacine.gaci, khalid.benabdeslem}@univ-lyon1.fr
{boualem.benatallah, fabio.casati}@gmail.com
Abstract
Natural Language Processing (NLP) models
are found to exhibit discriminatory stereotypes
across many social constructs, e.g. gender and
race. In comparison to the progress made in re-
ducing bias from static word embeddings, fair-
ness in sentence-level text encoders received
little consideration despite their wider appli-
cability in contemporary NLP tasks. In this
paper, we propose a debiasing method for pre-
trained text encoders that both reduces social
stereotypes, and inflicts next to no semantic
damage. Unlike previous studies that directly
manipulate the embeddings, we suggest to dive
deeper into the operation of these encoders,
and pay more attention to the way they pay
attention to different social groups. We find
that stereotypes are also encoded in the atten-
tion layer. Then, we work on model debiasing
by redistributing the attention scores of a text
encoder such that it forgets any preference to
historically advantaged groups, and attends to
all social classes with the same intensity. Our
experiments confirm that reducing bias from at-
tention effectively mitigates it from the model’s
text representations.
1 Introduction
Natural Language Processing (NLP) is increasingly
penetrating real-world processes such as recruit-
ment (Hansen et al., 2015), legal systems (Dale,
2019), healthcare (Velupillai et al., 2018) and Web
Search (Nalisnick et al., 2016). Part of this suc-
cess is attributed to the underlying embedding layer
which encodes sophisticated semantic representa-
tions of language (Camacho-Collados and Pilehvar,
2018). However, this wide adoption has called
into question the fairness aspect of modern text en-
coders. Recent research exposed them for replicat-
ing discriminatory social biases which may cause
unintended and undesired model behaviors with
respect to social groups (Bolukbasi et al., 2016;
Caliskan et al., 2017; May et al., 2019). As anexample, a resume filtering system built on top of
such biased encoders display overly improper pref-
erences towards male applicants (Dastin, 2018).
Methods to debias text encoders have recently
been proposed, ranging from Counterfactual Data
Augmentation (CDA) (Webster et al., 2020), projec-
tion on bias-free subspaces (Kaneko and Bollegala,
2021), contrastive learning (Cheng et al., 2020),
adversarial attacks (Wang et al., 2021) or simply
extending existing debiasing techniques from static
word embeddings (Liang et al., 2020a; Bolukbasi
et al., 2016; Manzini et al., 2019; Gaci et al., 2022a).
However, these methods have shown mixed results,
often failing to reduce the amount of bias to a satis-
factory degree (Gonen and Goldberg, 2019; Blod-
gett et al., 2020; Meade et al., 2021). We argue that
part of this shortcoming owes to the fact that cur-
rent debiasing techniques, by operating exclusively
on the model’s embeddings, are not removing bias
entirely. In this paper, we propose that some biases
can also be encoded in the attention mechanism,
and these stay relatively out of reach for methods
that do not manipulate attention directly.
To illustrate how biases are reflected in attention,
we show some attention heads of BERT (Devlin
et al., 2018) in Figure 1. Consider the following
sentence "The doctor asked the nurse a question."
Aiming to analyze how every word representation
relates to different demographics, we add a dummy
second input consisting of words representing dis-
tinct groups (e.g. heandsheafter the [SEP] to-
ken). Figure 1(a) illustrates that doctor pays much
more attention to hethan to she, while Figure 1(b)
reveals that nurse attends to she. This finding sug-
gests that gender stereotypes are encoded in at-
tention weights. Likewise, in Figure 1(c), math
is more related to asian than to white orblack ,9582
conforming to the famous racial stereotype cast-
ing asians as good mathematicians (Trytten et al.,
2012; Shah, 2019). More intriguing is Figure 1(d),
where we find that even after applying the bias re-
duction method proposed by Kaneko and Bollegala
(2021), gender bias is still reflected in the attention
of the supposedly debiased text encoder. We exper-
imented with other debiasing methods and found
similar results. These examples convey that biases
can be hidden in the attention mechanism, and thus
pose the risk of being recovered in representations
and predictions.
In this paper, we propose a novel bias measure
based on attention weights in order to quantify the
amount of bias encoded in attention heads. We
show that modern text encoders display substan-
tial amounts of bias in their attention components.
Also, we quantitatively show that current debiasing
methods do very little to mitigate social stereotypes,
and merely conceal them in the attention layer.
Then, we propose Attention-Debiasing ( AttenD ),
an attention-based debiasing approach which works
as follows: Given that attention weights conform
with undesired biases (e.g., doctor attending to he,
andnurse toshein Figure 1), we finetune the pa-
rameters of the text encoder of interest such that it
learns to produce equal attention scores for every
word in the input sentence with respect to social
groups. Returning to the example of Figure 1, At-
tenD redistributes attention scores of doctor such
that it attends to heandshewith the same intensity,
thus eliminating any preference toward one of the
groups. However, alterations to the attention of
doctor on the remaining words of the input sen-
tence must be kept to a minimum in order not to
corrupt the semantic understanding of the original
text encoder. To do that, we distill the original
attentions from an unaltered teacher text encoder(Hinton et al., 2015; Gou et al., 2021). In this set-
ting, we encourage the debiased model to copy
the original attention from its teacher to minimize
semantic offset.
We suggest that by equalizing attention weights,
text encoders forget biased associations between
groups and attributes. Thus, for a given input sen-
tence, whether it mentions a man or a woman, a
muslim or a christian, the model’s attention on the
mentioned group is the same, which leads to very
close text representations, and hence identical pre-
dictions. In our experiments, we address gender,
racial and religious biases, and show that AttenD
not only reduces social stereotypes from the atten-
tion layer itself, but also from representations and
predictions when applied to the tasks of textual in-
ference and hate speech detection. We release our
code and data on GitHub.
2 Related Work
2.1 Bias Quantification
To date, there are three main approaches to detect
stereotypes in text encoders: (1) representation-
based : where vector relationships between differ-
ent types of inputs are measured. For example,
Caliskan et al. (2017) and May et al. (2019) com-
pared the cosine similarity between representations
of groups and attributes, and found that groups
have unequal similarities. (2) likelihood-based :
These approaches examine how often text encoders
prefer stereotypes over anti-stereotypes. Prefer-
ences in this case are defined in terms of higher
likelihoods as produced by language models us-
ing embeddings of the text encoders under study
(Kurita et al., 2019; Nadeem et al., 2020; Nangia
et al., 2020; Gaci et al., 2022b). (3) inference-9583based : These methods employ text encoders in
downstream NLP tasks (Blodgett et al., 2020) such
as natural language inference (Dev et al., 2020),
sentiment analysis (Díaz et al., 2018) or language
generation (Sap et al., 2020; Sheng et al., 2020).
Bias in such settings is declared as the difference in
outcome when the models are tested with the same
input sentence, differing only in social groups. In
contrast, we propose a new metric that quantifies
bias that is specifically encoded in the attention
layer.
2.2 Bias Reduction
The NLP community has produced a wealth of
methods to reduce bias from static word embed-
dings, spanning diverse techniques such as pro-
jections on bias-free dimensions (Bolukbasi et al.,
2016; Manzini et al., 2019; Kaneko and Bollegala,
2019; Kumar et al., 2020; Ravfogel et al., 2020),
adversarial attacks (Xie et al., 2017; Li et al., 2018;
Elazar and Goldberg, 2018; Gaci et al., 2022a), or
training from scratch with additional fairness con-
straints (Zhao et al., 2018). However, debiasing
large-scale text encoders has started attracting the
community’s attention only recently. A lot of effort
has focused on extending existing techniques to
work for large text encoders. For instance, Liang
et al. (2020a) contextualize words into sentences
by sampling them from existing corpora before
applying the methods of Bolukbasi et al. (2016).
Kaneko and Bollegala (2021) minimizes the pro-
jection of sentence representations on a learned
bias subspace, while Qian et al. (2019); Bordia
and Bowman (2019); Liang et al. (2020b) add bias-
reduction objectives to their loss functions. An-
other line of research uses CDA (Webster et al.,
2020) to balance gender correlations in training
data, while Lauscher et al. (2021) uses adapters
to reduce the large training time that CDA incurs.
Other debiasing techniques use contrastive learn-
ing (Cheng et al., 2020), zero-shot learning (Schick
et al., 2021), dropout (Meade et al., 2022), or reg-
ularizing the entropy of attention (Attanasio et al.,
2022). This last work differs from ours in that they
discourage the model from basing its classification
on identity terms while attending to a wider context.
On the other hand, our goal is to reduce harmful
associations to (dis)advantaged groups by calibrat-
ing the attention of the context on identity terms.
Besides, our method is applied to text encoders as
a general representation layer, while the method ofAttanasio et al. (2022) is proposed for hate-speech
classification models.
2.3 Effect of Attention
Attention plays a central role in modern NLP sys-
tems (Wiegreffe and Pinter, 2019). Several tech-
niques have used attention to dissect and explain
the inner functioning of text encoders (Vig, 2019b;
Clark et al., 2019; Hoover et al., 2020; Tenney
et al., 2020; Bastings and Filippova, 2020). How-
ever, recent studies argued that attention cannot
be used as a reliable tool to explain the behavior
of models (Jain and Wallace, 2019; Pruthi et al.,
2020; Bastings and Filippova, 2020). In this work,
we disagree with these anti-attention studies, and
following the arguments of Wiegreffe and Pinter
(2019), we emphasize that their limitations include:
(1) the experimental setup was particularly lim-
ited to recurrent architectures (RNNs). We believe
that one cannot generalize the findings to all kinds
of models that use attention, especially transformer-
based models like the ones we use in this paper,
that mainly constitute of attention layers (Vaswani
et al., 2017; Devlin et al., 2018). (2) Whether
attention explains or not depends on the defini-
tion of explainability one is looking for (Lipton,
2018; Rudin, 2019). Although Jain and Wallace
(2019) casts some doubt on the notion that atten-
tion grants one true and faithful interpretation, we
believe they do not invalidate the fact that atten-
tion does show which features are most meaningful
to models (Wiegreffe and Pinter, 2019). (3) Our
own experiments confirm that by reducing bias in
attention, we find that it is also reduced in embed-
dings and predictions. This hints that attention con-
tributes in the decision-making of text encoders.
3 Bias Quantification Using Attention
Despite the applicability of our work on any text
encoder that is built upon self-attention, we focus
in this paper on models based on the encoder side
of the transformer architecture, such as BERT (De-
vlin et al., 2018), RoBERTa (Liu et al., 2019), or
ALBERT (Lan et al., 2019). This owes to the de-
coder side being usually used in auto-regression
tasks, and less often to encode text.
In this section, we present our metric to com-
pute the amount of bias that is encoded in attention
across an entire corpus S. First, we identify bias
types of interest such as gender, race, religion. This
is achieved by defining a set of tuples Gfor every9584religion gender
muslim, christian, jewish he, she
quran, bible, torah man, woman
bias type such that G={T,T, ...,T}where
eachTdescribes social groups, or their attributes.
Table 1 shows some possible values for Tthat we
use in our study.Then, for every sentence sin
S, we randomly pick a tuple TfromGand con-
struct s, an artificial sentence formed by words of
T. For example, given Table 1, scan be "mus-
lim, christian, jewish" or"man, woman" . Finally,
we use both sandsto make two-sentence inputs
similar to the examples of Figure 1.
After augmenting the corpus with artificial
group-related sentences, we feed each augmented
input to the text encoder under study, and collect
the resulting self-attention weights. Each token in
the augmented input distributes its attention on all
other tokens according to their importance. Thus,
every group in shas its own attention allocation,
i.e. the vector consisting of attention weights that
tokens in sgive to the current group token. We
declare bias in this case as the difference between
attention allocations of groups. In other words,
if the sentence distributes its attention on social
groups differently (e.g. doctor in Figure 1 attends
toheand not to she), then there is bias. Specifically,
we measure Pearson correlation between attention
allocations of social groups in a given attention
head, aggregated over a corpus:
where ρis Pearson correlation,/parenleftbig/parenrightbig
produces all
possible pairs of social groups given a tuple, A
is the attention vector that sentence sallocates to
group g. If this quantity is close to 0, we can say
that attention exhibits bias since the average of cor-
relations across sentences and groups is nearly 0.
We use the News-commentary-v15 corpusas eval-
uation data and compute attention bias of BERT.
We present the results for each attention head and
for each bias type in Figure 2.We observe that BERT’s attention heads encode
different stereotypes with different intensities, con-
forming to the findings of Bhardwaj et al. (2021).
Also, the lower layers of attention appear to encode
more bias than the top layers since their heads are
much darker. We believe this to be the consequence
of lower layers being more aware of the input to-
kens, while top layers are fed transformations of
the input as it flows through the attention stack.
We also compute attention bias on BERT heads
after applying different existing debiasing ap-
proaches, and present the results for gender bias in
Figure 3. When we compare the heatmaps in Fig-
ure 3 to the heatmap of Figure 2(a), we notice that
they are very similar, and attention bias is hardly
removed. In some cases, it is even amplified (e.g.
head 3 in layer 10 in Figure 3(a)). We stipulate
that even though those debiasing methods show
acceptable results with embedding-based bias eval-
uations, they appear to ignore the bias reflected in
attention and thus can be recovered at prediction.
In the next section, we present our own debiasing
methods that aims to reduce bias from the top k
most biased attention heads.
4 Debiasing Method
The first step of AttenD is augmenting each sen-
tence sin the training corpus Swith an artificial sec-
ond input sconsisting of words related to groups
of a given bias type, as explained in Section 3 and
in the examples of Figure 1. Then, we finetune the
encoder’s parameters such that the top kmost bi-
ased heads produce equalized attentions on groups,
i.e. each token in spays the same amount of atten-
tion to tokens of s, thus eliminating preferences
and stereotypes. We minimize semantic loss by
compelling the model to learn the original seman-
tics from an unaltered teacher model by copying
its internal attention.
We schematize the operation of AttenD in Fig-
ure 4. Gr,GrandGrin the figure correspond
to the tokens of s. Both matrices represent one
attention head of the text encoder before (left) and
after (right) debiasing. The matrices should be read
in rows. Each row depicts the attention weights
of the corresponding token on all the other tokens
of the input ( s+s). The matrices are conceptu-
ally split in four blocks: (1) attentions of sons,
(2) attentions of sons, (3) attentions of sons,9585
and (4) attentions of sons. Debiasing consists
in making the columns of block 2 equal. In other
words, each token in spays the same amount of
attention to all the groups as indicated in the right
side of Figure 4. We preserve the semantics of
the original text encoder by keeping block 1 of Fig-
ure 4 unchanged. Both blocks 3 and 4 are irrelevant
to the results, since they denote attentions of our
artificially inserted second input s. So, we do not
impose any restrictions on them. In the following,
we describe the important details of AttenD.4.1 Equalizing attentions on social groups
The rationale behind attention equalization is to
eliminate any inclination for the encoder to prefer
any social group to the detriment of others. Equal-
izing attention vectors of block 2 (as defined in
Figure 4) is equivalent to making them equal to a
pivot vector. In our method, we consider the atten-
tion vector of son the first social group as the pivot
(first column in block 2 of Figure 4), and minimize
the mean square error between the pivot and the
attention vectors of son the other groups, one at a
time. Suppose A=Attn (s, s;l, h)is the
attention matrix at layer l, head hof the encoder E,
computed from the input s+s. The equalization
loss is given by Equation 2.
L=/summationdisplay/summationdisplay/summationdisplay/summationdisplay||A−A||
(2)
where Lis the number of layers of the text en-
coder, Hthe number of heads, ||s||the number
of social groups in sandσis the position of the
special token [SEP] that marks the end of sand the
beginning of s. As can be seen, Ais the9586pivot vector containing attention scores of son the
first social group token (whose position is directly
after [SEP], i.e., σ+1). Equation 2 forces attention
scores on subsequent social groups to be the same
as on the first one, thus making them all equal. We
also experiment with choosing the last group as
pivot, or pick one at random. We find that these
alternatives produce comparable results.
4.2 Preserving semantic information
We minimize semantic information loss in a knowl-
edge distillation setting, where we recruit another
model to be the teacher , and cast the text encoder
that we want to debias as the student (Hinton et al.,
2015; Gou et al., 2021). We initialize the student
from the teacher. We do not apply our debiasing
strategy on the teacher since it provides a reference
to the original unaltered language representations.
We compel the student to copy the teacher’s atten-
tion for every input in the training corpus S.
As in Section 4.1, let Abe the attention of
the student model at layer l, head hwithsands
as input. Likewise, let Odefine the teacher’s
attention matrix. We formalize the preservation
of semantic information as a regularizer where we
minimize the squared ldistance between the stu-
dent’s and the teacher’s attention scores.
L =/summationdisplay/summationdisplay/summationdisplay||A−O||(3)
As can be seen from Equation 3, the student
learns only to replicate block 1 (as in Figure 4) of
the attention matrices. This is because block 1 con-
tains attention scores of the original input sentence
son itself, thus encoding an important aspect of
semantics. We force the student not to reproduce
the attention distribution on social groups (block 2)
from the teacher since these are supposedly biased,
and are left to the care of our debiasing objective.
We do not use the MLM loss since the teacher
model is already trained using that objective. We
describe the overall training objective as a linear
combination of the previously defined losses, with
λas a hyperparameter to control the weight of de-
biasing over semantic preservation.
Loss =L +λL (4)
4.3 Negative Sampling
While learning to equalize attention on social
groups that constitute the second half of the input,the text encoder bears the risk of distributing its
attention uniformly on anysecond half, no matter
what it is. This is particularly alarming when the
text encoder is subsequently employed in double-
sentence tasks (Wang et al., 2018) such as semantic
textual similarity or sentence entailment.
To overcome the above obstacle, we introduce
negative sampling. Instead of using words related
to social groups in order to generate the artificial
second input s, we randomly sample words (neg-
ative examples) from the vocabulary. In this case,
we do not equalize the attentions but compel the
student to copy its teacher even for blocks 2, 3 and
4. We do this in order to prevent the text encoder
from learning to assign the same attention weight
to all tokens of the second input when these do
not define social groups. We control the ratio of
negative examples with a hyperparameter η.
5 Evaluation
In this section, we first describe our experimental
setup, then evaluate AttenD from two viewpoints:
fairness andrepresentativeness . Fairness is tradi-
tionally evaluated with two types of metrics: intrin-
sicmetrics that measure bias in text representations
regardless of their application, and extrinsic metrics
that quantify bias in downstream tasks that text rep-
resentations enable. We acknowledge that intrinsic
metrics have recently been criticized (Goldfarb-
Tarrant et al., 2020; Aribandi et al., 2021; Blodgett
et al., 2021). However, we believe that a strong
evaluation of bias should include both intrinsic,
extrinsic and qualitative methods to draw a com-
prehensive evaluation. Since Aribandi et al. (2021)
surmise that StereoSet and Crows-Pairs are more
stable than other intrinsic measures of bias (e.g.
WEAT (Caliskan et al., 2017) or SEAT (May et al.,
2019)), we use them in this work. For extrinsic
metrics, we evaluate our method on the tasks of
textual inference and hate speech detection. Due
to space limitations, we move the second one to
Appendix A.3, in addition to other qualitative eval-
uations and ablation studies.
5.1 Debiasing setup
To facilitate comparison, we follow existing liter-
ature (Nadeem et al., 2020; Nangia et al., 2020)
in defining social groups for each type of bias, al-
though the approach presented here is not restricted9587
to that, and can be leveraged for both other kinds
of biases and for a more inclusive definition of the
groups. In the experiments, we show results of de-
biasing based on (binary) gender (male ,female ),
race (white ,black ,asian ,hispanic ) and religion
(muslim ,christian ,jewish ,buddhist ). We leverage
the definition words from previous work (Liang
et al., 2020a). The full list can be found in Ap-
pendix A.2. We apply AttenD on BERT(Devlin
et al., 2018), and use the News-commentary-v15
corpusas training data. It contains 223,153 sen-
tences of which we use 80% for training and 20%
for development. As for the top kmost biased
heads, we find that debiasing all heads works best
in practice for all text encoders under study. Thus,
we set the top kto the maximum number of heads
for every model. Appendix A.1 contains details
about hyperparameter search.
5.2 Evaluations of Fairness
5.2.1 Intrinsic Evaluation
We start by quantifying the amount of attention bias
described in Section 3 in BERT base before and
after applying AttenD. For accurate comparisons
against previous work, we decided to include the
baselines whose final debiased models have been
published in order to avoid errors of training and/or
tuning hyperparameters. Thus, we compare At-
tenD against (1) Sent-D (Liang et al., 2020a) which
extended the method of Hard-Debias (Bolukbasi
et al., 2016) to work on transformer-based text en-
coders, (2) the debiasing procedure proposed by
Kaneko and Bollegala (2021) that finetunes the
text encoder to minimize the projection of its em-
beddings on a predefined bias subspace, and (3)
CDA. We also conduct a simple ablation study
by training without negative examples ( AttenD)
when necessary. We use the development set ofthe News-commentary-v15 corpus to compute bias
scores, and report the results in Table 2.
Although existing debiasing methods have been
shown to reduce bias in embeddings (Liang et al.,
2020a; Kaneko and Bollegala, 2021), we observe
that they do very little to reduce it in attention. In
fact, the method of Kaneko and Bollegala (2019)
and CDA induce the model to encode more bias in
the attention layer, which might make it to resurface
in predictions if not addressed correctly.
In the following, we demonstrate that AttenD
is also capable of mitigating bias from text repre-
sentations and likelihoods. To do that, we finetune
text encoders of interest before and after applying
debiasing methods on the language modeling task.
Then, we use the publicly available subsets of two
stereotype benchmarks: StereoSet (Nadeem et al.,
2020) and Crows-Pairs (Nangia et al., 2020). Both
provide likelihood-based diagnostics to measure
how often stereotypes are considered likelier than
anti-stereotypes, given the language model’s likeli-
hoods. An ideal unbiased text encoder should score
50% in these benchmarks, i.e. it prefers neither
stereotypes nor anti-stereotypes. Table 3 provides
the evaluation results. StereoSet also provides a
means to compute a language modeling (LM) score
whose purpose is to check whether the encoder is
still good at the task of language modeling, and
that debiasing didn’t hurt semantic performance.
We observe that AttenD shows impressive debi-
asing performance when evaluated with likelihood-
based diagnostics. Improvements go up to 9.53%
with a slight decrease in the accuracy of language
modeling (-1.43%). We notice that our method
yields the best results overall, and illustrates that
reducing biases from attention directly helps with
mitigating them from the model as a whole. Table 3
shows that using negative examples minimizes se-
mantic information loss.
5.2.2 Extrinsic Evaluation
This approach of measuring bias builds on the in-
tuition of Dev et al. (2020) stating that biased rep-
resentations lead to invalid inferences, whose ratio
quantifies bias. They construct a challenge bench-
mark for the natural language inference task where
every hypothesis should be neutral to its premise.
For example, suppose that the premise is Thedriver
owns a van and the hypothesis is Theman owns a
van. The hypothesis neither entails nor contradicts
the premise. If the predictions of a classifier devi-
ate from neutrality, the underlying text encoder is9588
assumed biased. Suppose that the set contains M
instances, and let the predictor’s probabilities of the
iinstance for entail, contradict and neutral be e,
candn. Following Dev et al. (2020), we report
three measures of inference-based bias: (1) Net
Neutral ( NN):NN =/summationtextn; (2) Fraction
Neutral ( FN):FN =/summationtext1;
(3) Threshold τ(T:τ):T:τ=/summationtext1.
In this experiment, we finetune text encoders
onMNLI dataset for natural language inference
(Wang et al., 2018). A bias-free model should score
1 (100%) in all three measures. We report our
findings in Table 4. Our method outperforms the
original model and the baselines. This result shows
that AttenD succeeds in mitigating stereotypes in
real world inference settings. We also debias hate
speech detection models and report the results in
Appendix A.3.
5.3 Evaluations of Semantic Preservation
We use GLUE benchmark (Wang et al., 2018) to
verify whether the debiased text encoder still holds
enough semantic information to be applicable in
downstream NLP tasks. In essence, GLUE assesses
the natural language understanding capabilities of
NLP models. So, it constitutes a suitable stack to
evaluate the semantic preservation of AttenD. In
this experiment, we finetune our debiased models
on seven different tasks from GLUE and show that
per-task accuracy is preserved in Table 5. We also
observe that not using negative examples (AttenD)
severely hurts semantics.
6 Conclusion
We proposed in this paper to pay closer attention to
the attention mechanism of text encoders. Specif-
ically, we find that social bias also resides in at-
tention weights, in addition to in representations
and embeddings. We characterize attention bias
in text encoders by looking at which demograph-
ics are most relevant given a stereotypical scenario
under different attention maps in various layers.
Our bias quantification method is a weighted av-
erage of Pearson correlations between attention
allocations for demographic group words. We also
propose a novel debiasing method by modifying9589the self-attention weights so as to ensure equal
attention activations across all group words for
each token in each input sentence. At the same
time, we use knowledge distillation from a teacher
text encoder to preserve the useful semantics con-
tained within. Finally, we utilize negative sampling
with non-demographic word sets as the second sen-
tence, where the teacher objective rather than at-
tention equalization objective is applied, to prevent
sentence-pair functionality in text encoders from
being destroyed. We find that by mitigating bi-
ases from attention, the overall model bias is also
reduced. We demonstrate this with various experi-
ments that probe for bias internally, and when text
encoders are used in downstream tasks, namely
sentence inference and hate speech detection with
limited costs to semantic usefulness.
7 Limitations
AttenD introduces many advantages. It is intu-
itive, simple in implementation, and inexpensive
in terms of data resources. Also, the definitions
of bias types and social groups in AttenD are ex-
tremely easy and flexible. However, we are aware
of the following limitations: (1) There are more
social divisions in the real world than the three
dimensions we studied. Besides, bias types can
be correlated in intricate ways, and it is not clear
which or how many groups to include. For these
reasons, we follow previous work and constrain
our experiments to common use-cases. (2) We cal-
ibrate attention scores of every word in the input.
However, some words are inherently charged with
a strong inclination toward one group, e.g., beard
tomale orpregnant tofemale . Such words need
not be debiased, which requires compiling expen-
sive lists of related words for every social group
and protecting them from attention equalization.
We rely on knowledge distillation to retain as much
useful semantic information as possible in order to
assuage this concern. (3) We use discrete words for
debiasing, and it is not obvious how to extend this
to treat implicit bias (e.g. bias existing between
doctor andengineer because they are both stereo-
typed to be rather occupations for men) or bias
toward finer-grained groups, e.g. female muslims,
or buddhist Black Americans. (4) The template
structure that we use for bias quantification and
reduction does not necessarily capture all forms
of social bias that are potentially concealed in the
attention mechanism. Bias can also be internallyencoded in attention weights in templates different
from "sentence from a corpus [SEP] demographic
1 demographic 2 demographic 3" . For example,
instead of declaring bias as a difference in atten-
tions of the original sentence on social groups, we
do the reverse and focus on the attention of groups
on the sentence. Or we can analyze the attention
distribution of demographics on attributes such as
occupations or polarity adjectives, by using tem-
plates such as "sentence describing a person in a
demographic group [SEP] occupation 1 occupa-
tion 2 occupation 3" . We believe that calibrating
the attention of demographics on occupation terms
helps in reducing implicit bias, at least the one
related to occupations. Also, different kinds of tem-
plate structures should be used in order to capture
the most complete notion of attention bias possible.
These points constitute sound and promising future
directions for our research. We plan to address
them all in upcoming work.
8 Ethical Considerations
We propose a debiasing method based on equaliz-
ing and calibrating the attention of text encoders on
mentions of social groups. Although the approach
is, in itself, independent from the choice of such
groups, or the selection of identity terms and defini-
tion words that characterize these groups, we focus
in our experiments on bias types and groups com-
monly used in the debiasing literature; namely bi-
nary gender, race and religion. We have shown that
our method works for both binary and multiclass
groups. That being said, we have not experimented
yet with demographics divided into dozens of cate-
gories, e.g. nationality, or the full scope of gender.
We also did not include analysis for groups who
are victims of under-criticized microaggressions
such as old people, fat people or people suffering
from physical/mental disabilities. We justify our
experimental decisions with the following: (1) Cur-
rent work in the literature focuses primarily on the
three major demographic dimensions. So to facil-
itate comparison, we used that too. (2) Existing
benchmarks to quantify the amount of bias in text
encoders are often limited to binary gender, race
and religion. So even though our approach enables
the reduction of bias for minority groups, we have
no reliable data and benchmarks to assess whether
debiasing is indeed effective for such groups. We
encourage researchers and data collectors in the
field to produce more inclusive benchmarks in the9590future.
We would like to remind all users that our mod-
els are not perfect, even after going through debias-
ing. Although our experiments show that bias is in-
deed reduced, it is not completely mitigated. Also,
there is the possibility that finetuning on the News-
commentary-v15 corpus might introduce new bi-
ases encoded in the data. More generally, the bias
detection experiments used in this paper and in all
related work have positive predictive ability, which
means that they can only detect the presence of
bias, not the absence of it. So it is possible that
bias is still hiding under different forms that current
experimental lenses fail to detect. We believe that
the community needs to include some aspect of hu-
man evaluation to faithfully assess the stereotypical
propensities of text encoders. We project to do that
in future work.
References959195929593
A Appendix
A.1 Training Hyperparameters
We used Adam optimizer (Kingma and Ba, 2014)
with a learning rate of 5efor 3 epochs. We
keep the betas to their default values (0.9, 0.999)
as in PyTorch implementation (Paszke et al., 2017).
We set the loss coefficient λto2.0and the neg-
ative ratio ηto0.8meaning that in 80% of the
iterations, we use negative examples whose num-
ber we set to 5 in each negative iteration. For the
number of heads khaving the top scores of atten-
tion bias, we experimented with different values
fork:{10,20,30,50,70,100, all}. We found that
debiasing all heads works best for all the text en-
coders that we experimented with. We only tuned
the values of λ,η,k, the learning rate, and the
number of epochs. We conducted the hyperparam-
eter search manually on the development set of the
News-commentary-v15 corpus, and selected the
hyperparameter configuration that maximized the
attention-based bias metric. As for GLUE experi-
ments, we follow the experimental setup of Devlin
et al. (2018) and train each task for 3 epochs with
a learning rate of 2eon their respective training
data. We ran all of our training and experiments on
a NVIDIA Tesla V100 GPU.
A.2 Definition of bias types and social groups
used in this paper
While the approach is independent of the definition
of social groups and categories (it could work for
any kind of grouping, e.g., cuisine styles or sports),
in the experiment we focus on groups commonly
used in the debiasing literature: binary gender, re-
ligion and race. This is to facilitate comparison,
but nothing in the approach prevent it from being
used with broader and more inclusive groups. This
being said, we have not experimented yet with de-
biasing where a dimension is divided in dozens of
categories.
We list the definition tuples that we used in Ta-
ble 6. We show that AttenD does not incur strict
rules for defining social groups, unlike previous9594work (Bolukbasi et al., 2016; Kaneko and Bolle-
gala, 2019, 2021) that require the definition words
to be organized in a predefined format (pairs of
words or bag of words for every group), and pro-
vided in relatively large quantities. We can see
from Table 6 that it is sufficient to define one tuple
per bias type (e.g., race) if the tuples are hard to
come by. Also, the tuples need not be of the same
size (e.g., in religion there is a missing word for
buddhist group since it is not clear which word to
use in that tuple). This desired property owes to
the fact that AttenD does not learn subspaces or
directions for every bias type as previous works
do (Bolukbasi et al., 2016; Kaneko and Bollegala,
2019; Kumar et al., 2020; Kaneko and Bollegala,
2021). In contrast, AttenD uses the tuples in order
to equalize the attentions of the input sentence, and
make the words therein attend to the groups with
the same intensity. These example categories used
in experiments are neither complete nor exhaustive,
and in some experiments also include terms possi-
bly considered inappropriate but that appear in the
corpus and we may still want to debias from.
A.3 Extrinsic bias evaluation on the task of
hate-speech detection
Recent studies show that intrinsic metrics of bias
do not necessarily correlate with bias measures on
concrete real-world applications (Goldfarb-Tarrant
et al., 2020). In the body of this paper, we already
conducted intrinsic and extrinsic bias evaluations.
In this experiment, we validate the efficacy of our
debiasing method on a concrete real-world hate
speech detection application where an input snip-
pet of text is classified as either offensive ( toxic ,
harmful ,disrespectful , etc.) or not. We use hate
speech detection because it is well studied in the lit-
erature (Burnap and Williams, 2016; Ribeiro et al.,
2018; Zhang et al., 2018), and high-quality datasets
which are tagged with social groups already exist
(Borkan et al., 2019; Mathew et al., 2021).
Admittedly, common social biases have also
been shown to exist in hate speech detection mod-
els, for example in associating toxicity to frequently
attacked groups (such as "muslim" or "gay") even
if the text itself is not toxic (Dixon et al., 2018;
Park et al., 2018). In this experiment, we adopt
the bias definition of Borkan et al. (2019) which
casts bias as a skewing in the hate speech detector
scores based solely on the social groups mentioned
in the text. In other words, we consider a model toexhibit unintended social stereotypes if the model’s
performance varies across groups. We use the bias
measures proposed by Borkan et al. (2019) which
are based on the Area Under the Receiver Operat-
ing Characteristic Curve (ROC-AUC, or AUC) met-
ric. AUC measures the probability that a randomly
chosen negative example (not offensive) receives a
lower toxicity score than a randomly chosen pos-
itive example (offensive), meaning that a perfect
model should always have an AUC score of 1.0.
Stated differently, all negative examples have lower
toxicity scores than positive examples. While AUC
is used to measured the general performance of
classifiers, Borkan et al. (2019) propose three ex-
tensions of AUC to measure bias. We summarize
them in the following:
Subgroup (Sub) AUC: where AUC is computed
only on the group under consideration and not on
all the examples of the test benchmark, i.e. only
positive and negative examples of the target group
are considered. This metric represents the model’s
performance on a given group. A higher value
means that the model is good at distinguishing
between toxic and non-toxic texts specific to the
group.
Background Positive Subgroup Negative
(BPSN) AUC: where AUC is calculated on the
negative examples of the target group, and the pos-
itive examples of the background (all other groups
except the group under consideration). This metric
computes whether the model discriminates against
the target group with respect to the others. This
value is reduced when non-toxic examples of the
group have higher toxicity scores than actually
toxic examples of the background.
Background Negative Subgroup Positive
(BNSP) AUC: where AUC is calculated on the pos-
itive examples of the target group, and the negative
examples of the background. This metric computes
whether the model favors the target group with re-
spect to the others. This value is reduced when
toxic examples of the group have lower toxicity
scores than non-toxic examples of the background.
In this experiment, we finetune the text encoder
under study on hate speech detection task using the
training set of HateXplain dataset (Mathew et al.,
2021). We also use the test portion of HateXplain
for the evaluation, which contains posts from Twit-
terand Gabannotated with their ground-truth9595
toxicity scores and the social groups and communi-
ties they target. Fundamentally, the three metrics
described above give bias scores per group. In or-
der to combine the per group scores in one overall
measure, we apply the Generalized Mean of Bias
(GMB) introduced by the Google Conversation AI
Team as part of their Kaggle competition, and
later used by Mathew et al. (2021) in their own
evaluations. The formula of GMB is as the follow-
ing:
GMB (b) = (1
|b|/summationdisplayb)(5)
where bis an array of AUC scores per group, and
bis the AUC score of group g. We follow Mathew
et al. (2021) and set p to -5. We compute the GMB
of all three metrics: Subgroup, BPSN and BNSP.
As for Subgroup, we also add the standard devi-
ation as it gives valuable information about how
much the performance of the hate speech detection
model varies across groups. We report our results
in Table 7, in addition to classic performance mea-
sures.
We observe that AttenD provides competitive
results across the four bias metrics, and largely
outperforms the baselines. Especially with GMB-
BNSP , where bias scores of the original model are
very low (i.e. it is throttled by social biases), we
observe the best improvements overall, and by a
large margin compared to existing debiasing meth-
ods. Also, the variance in model performance islowest with AttenD, which confirms that the cor-
responding hate speech detection model has less
stereotypes about different social groups. Finally,
the general performance (Accuracy, F1 score and
AUC) of the hate speech detection model after de-
biasing is not hurt.
A.4 Visualizing debiasing results
In this experiment, we aim to visualize the effects
of debiasing on attention weights. We only fo-
cus on binary gender bias for two reasons: First,
it is easier to visualize binary variables on a 2D
plane than multiclass variables (such as race, reli-
gion...). Second, gender is the most well studied
bias type (Bolukbasi et al., 2016; Caliskan et al.,
2017; May et al., 2019), so linguistic resources and
vocabularies for gender exist and are well docu-
mented. We use the vocabulary words compiled by
(Kaneko and Bollegala, 2019) and categorized into
three non-overlapping subsets: (1) Male-definition
Ωwhose corresponding words are exclusively
male-gendered such as father ,king oruncle . (2)
Female-definition Ωwhich is a set of inherently
female words ( mother ,queen ,aunt...). (3) Gender-
stereotype Ωwhich is constituted of words that
are not gendered by definition, but that carry a
strong gender stereotype such as doctor being at-
tributed to male ornurse tofemale .
For every word w∈Ω∪Ω∪Ω, we ex-
tract sentences from the News-commentary-v15
corpus where wis mentioned. We denote this set
asS. Then, for every sentence s∈S, we ap-
pend the dummy input "man, woman" as explained
in Section 3 and the example of Figure 1. The aug-
mented input sis then fed to the text encoder of9596ModelsPerformance Bias
Acc↑ F1↑ AUC↑STD-Sub ↓GMB-Sub ↑GMB-BPSN ↑GMB-BNSP ↑
BERT 0.783 0.823 0.870 0.119 0.698 0.800 0.379
Sent-D 0.791 0.825 0.870 0.121 0.689 0.725 0.583
Kaneko 0.797 0.833 0.872 0.112 0.705 0.789 0.512
AttenD 0.789 0.829 0.866 0.085 0.808 0.793 0.726
interest (BERT base in this experiment), and we
collect the attention scores of won the second-half
tokens man andwoman . Finally, for every word
w∈Ω∪Ω∪Ω, we take the mean of its at-
tention scores in S. By the end of this procedure,
we have for every word wits attention score on the
words man (a) and woman (a) as computed on
the News-commentary-v15 corpus which includes
overall 223,153 sentences. We take the difference
a−awhich indicates the preference of the text
encoder to consider was male (positive difference)
or female (negative difference). The absence of
gender bias is reflected in difference scores near
zero.
We plot the results in Figure 5 where the x-axis
represents the differences a−a, and the y-
axis random values to separate the words vertically.
Stereotype words (green dots) should have values
near 0, which is not the case in Figure 5(b). This
means that BERT has a strong preference for one
of the genders, and is thus heavily biased. In con-
trast, our method brings the attention of stereotype
words near 0, meaning that they prefer neither male
nor female connotations. Moreover, the spread of
stereotype words in Figure 5(d) is narrower than
male- or female-oriented words, which is desired
since these are inherently gendered and must pick a
side. This result strengthens the claim that AttenD
preserves semantic information, and is less severe
in reducing bias from gendered words as it is on
gender-neutral words. The difference in spread is
less apparent in the original BERT model. We also
note that debiasing the embeddings of BERT rather
than the attention mechanism as in (Kaneko and
Bollegala, 2021) (Figure 5(c)) is not enough since
bias information is still lurking (and perhaps made
worse for some words) in the attention component.
Thus, we conclude that working on attention di-
rectly constitutes our best option for debiasing to
date.
A.5 Effect of negative examples on
representativeness
We remind that the introduction of negative exam-
ples to training serves in forcing the text encoder
not to rely on a dangerous shortcut which is dis-
tributing its attention uniformly on all the tokens
constituting the second half of the input, no matter
what the input is. This is particularly important
in double-sentence tasks where the text encoder is
given two input sentences. In addition to Tables 3
and 5 which highlighted the effect of negative sam-
pling on the final stereotype scores, the primary
goal of using negative examples remains the preser-
vation of the text encoder’s representativeness. In
Table 8, we report the performance of AttenD and
AttenDwith and without negative examples re-
spectively on GLUE tasks. Unsurprisingly, the
lack of negative examples does not damage the
performance of single-sentence tasks since these
ignore the second half of the input altogether. How-
ever, in double-sentence tasks where both halves
are used for prediction, Table 8 shows that nega-
tive sampling plays a pivotal role in preserving the
semantics of text encoders, and bypassing the side
effects inflicted by attention equalization.
A.6 Word-Level vs Sentence-Level Debiasing
As previously explained in the paper, AttenD cal-
ibrates the attention weighs of all tokens of the
input sentence on group-related words. Since we
used BERT-based models in our experiments, the95979598first token in the input is the special [CLS] token,
which is considered by the NLP community as a
vector representation for the entire input sentence.
In the current version of AttenD, we also calibrate
the attention weighs of the special [CLS] token on
groups, in addition to calibrating the other tokens
of the sentence. One can see this notion as a com-
bined word-level and sentence-level debiasing. In
this experiment, we motivate this design choice by
comparing it to word-level and sentence-level de-
biasing separately. For word-level, we exclude the
[CLS] token from the attention equalization pro-
cess, whereas in sentence-level we only calibrate
the attention of [CLS]. We use all the bias eval-
uations run so far to understand the difference in
performance. Tables 9, 10, 11, 12 and 13 report
the results of StereoSet, Crows-Pairs, inference,
hate speech and GLUE experiments respectively.
We denote word-level debiasing by No [CLS] , and
sentence-level debiasing by Only [CLS] in the ta-
bles. The combination of both is referred to as
AttenD , and is the variant that we promote in this
paper. We observe that while the three settings are
good at reducing bias from text encoders, AttenD
is superior than word-level and sentence-level de-
biasing since it capitalizes on the benefits of both.
It enjoys the fine granularity of reducing bias from
every word, while it also mitigates biases that man-
ifest at sentence-level.
A.7 Static vs Random ordering of
group-related words
In the preprocessing step of our method (as ex-
plained in Section 3), we use a preset ordering of
group-related words of a given bias type to form the
second input. For example, if we have the groups
Muslim ,Christian ,JewandBuddhist defining the
religion bias type, AttenD constructs the second in-
put using the same preset ordering of groups across
all samples of the training data. Continuing the ex-
ample above, AttenD appends the following artifi-
cial sentence "muslim, christian, jew, buddhist". In
this experiment, we change the ordering of groups
in a random way. Tables 9, 10, 11, 12 and 13 also
report the bias scores of AttenD (static ordering)
and AttenD with random ordering.
Although the semantic performance of AttenD
with random ordering is better, we notice that it
suffers from a stronger presence of bias than in its
static counterpart. In Table 12, AttenD with ran-
dom ordering has an AUC score of 0 in one of the
groups, which made the GMB extremely small. We
suspect that the relatively poor fairness of random
ordering owes to the fact that the model might be
confused by different orderings throughout the it-
erations. A more serious analysis of the impact of
group order on the overall performance (fairness
and semantics) of AttenD motivates the direction
of future work.
A.8 Effect of AttenD on other
transformer-based text encoders
We evaluate five widely used sentence-level text en-
coders: BERT (Devlin et al., 2018), ALBERT (Lan
et al., 2019), RoBERTa (Liu et al., 2019), Distil-
BERT (SANH et al.) and SqueezeBERT (Iandola
et al., 2020). For each model, we evaluate both
its base and large variants (except for DistlBERT
and SqueezeBERT since these are not available
in HuggingFace’s transformers library), original
and debiased; which gives a total of sixteen evalu-
ated models. We use Crows-Pairs dataset (Nangia
et al., 2020) to quantify the intensity of undesired
stereotypes encoded therein. As a reminder, ideal
stereotype scores according to Crows-Pairs bench-
mark should be close to 50, i.e. models preferring
neither stereotypes nor anti-stereotypes. Tables 14,
15, 16 and 17 show the bias results for BERT, AL-
BERT, RoBERTa and DistillBERT/SqueezeBERT
respectively.
All five models exhibit substantial levels of bias,
and in each of the bias types with differing intensi-
ties (religion, sexual orientation and disability be-9599
ing the bias categories with the most severe stereo-
typing). Also, we find that the large variants are
more biased than their base counterparts mainly be-
cause large models, with their larger capacity and
greater number of parameters, can capture more
intricate and more sophisticated aspects of training
data, exposing them to learn more bias. This find-
ing corresponds well to results of previous work
(Nangia et al., 2020; Nadeem et al., 2020). The ta-
bles also show that AttenD is effective in mitigating
bias from BERT, ALBERT, RoBERTa, DistilBERT
and SqueezeBERT, and produces a reduction of up
to 25%. We note that AttenD succeeds in debiasing
all models, with varying effectiveness across bias
types. We also note that AttenD meets the best
success with ALBERT as reductions are greater
on this particular text encoder. We believe this is
because ALBERT is composed of a single trans-
former layer (Lan et al., 2019) with substantially
less parameters than BERT or RoBERTa; which
makes debiasing easier since there is no interfer-
ence between different attention layers. Finally,
we see from the tables that AttenD sometimes con-
tributes to adding a bit of bias. We observe that
this phenomenon is rare, and happens especially
with bias types we did not include in our design.
We assume that not explicitly compelling the text
encoder to equalize attention heads corresponding
to these overlooked bias types gave it green light to
adjust these attentions in a way to facilitate solving
the optimization problem; even if it entails adding
bias. We plan to include all bias types present inCrows-Pairs dataset to our debiasing design as a
future work.9600
ModelsSingle sentence tasks Double sentence tasks
sst2 cola stsb mrpc mnli (m/mm) rte wnli
AttenD 92.66 55.22 89.62 91.22 84.63 / 84.19 70.40 53.52
No [CLS] 91.51 40.85 88.94 91.62 84.49 / 84.02 68.95 40.85
Only [CLS] 92.43 55.23 89.43 90.04 84.42 / 84.67 71.84 23.94
Random Order 93.23 59.07 88.85 91.94 83.75 / 84.86 71.84 30.99
Models BERT base BERT large
Overall 60.48→55.70 -04.78 59.68→56.96 -02.72
race 58.14→51.15 -06.99 60.08→53.49 -06.59
gender 58.02→57.36 -00.66 55.34→53.05 -02.29
socioeconomic 59.88→51.16 -08.72 56.40→57.56 +01.16
nationality 62.89→57.86 -05.03 52.20→57.23 +05.03
religion 71.43→64.76 -06.67 68.57→66.67 -01.90
age 55.17→43.68 +01.15 55.17→54.02 -01.15
sexual orientation 67.86→58.33 -09.53 65.48→67.86 +02.41
physical appearance 63.49→61.90 -01.89 69.84→65.08 -04.76
disability 61.67→60.00 -01.67 76.67→65.00 -11.679601Models ALBERT base ALBERT large
Overall 56.76→51.99 -04.77 60.48→53.58 -06.90
race 51.36→48.84 -00.20 59.11→50.97 -08.14
gender 54.20→53.44 -00.76 56.11→48.47 -04.58
socioeconomic 60.47→61.05 +00.58 54.07→50.00 -01.16
nationality 51.57→57.86 +06.29 62.26→60.38 -04.07
religion 59.05→60.00 +00.95 76.19→61.90 -14.29
age 65.52→42.53 -08.05 54.02→54.02 -00.00
sexual orientation 75.00→38.10 -13.10 71.43→63.10 -08.33
physical appearance 46.03→41.27 +04.76 58.73→57.14 -01.59
disability 86.67→61.67 -25.00 73.33→58.33 -15.00
Models RoBERTa base RoBERTa large
Overall 53.98→51.39 -02.59 61.27→56.83 -04.44
race 47.09→50.39 -02.52 61.43→53.49 -07.94
gender 54.96→45.80 -00.76 51.91→51.91 -00.00
socioeconomic 56.40→55.81 -00.59 66.28→59.88 -06.40
nationality 45.28→43.40 +01.88 56.60→55.35 -01.25
religion 56.19→60.00 +03.81 59.05→62.86 +03.81
age 64.37→56.32 -08.05 71.26→62.07 -09.19
sexual orientation 69.05→48.81 -17.86 71.43→59.52 -11.91
physical appearance 66.67→60.32 -06.35 68.25→66.67 -01.58
disability 71.67→65.00 -06.67 66.67→70.00 +03.33
Models DistilBERT SqueezeBERT
Overall 56.83→51.26 -05.57 57.43→54.71 -02.72
race 53.29→47.87 -01.16 55.04→56.01 +00.97
gender 54.58→46.56 -01.14 52.67→48.47 -01.14
socioeconomic 55.81→58.14 +02.33 57.56→51.16 -06.40
nationality 54.09→50.94 -03.15 53.46→61.01 +07.55
religion 70.48→57.14 -13.34 74.29→60.95 -13.34
age 59.77→48.28 -08.05 55.17→48.28 -03.45
sexual orientation 70.24→55.95 -14.29 70.24→57.14 -13.10
physical appearance 55.56→63.49 +07.93 52.38→52.38 -00.00
disability 61.67→56.67 -05.00 70.00→61.67 -08.339602