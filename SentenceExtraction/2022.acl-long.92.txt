
Kashif Khan
Computer Science
University of Waterloo
Ontario, Canada
k2@iwrk.comRuizhe Wang
Computer Science
University of Waterloo
Ontario, Canada
r322wang@uwaterloo.caPascal Poupart
Computer Science
University of Waterloo
Vector Institute
Ontario, Canada
ppoupart@uwaterloo.ca
Abstract
We contribute a new datasetfor the task of
automated fact checking and an evaluation of
state of the art algorithms. The dataset in-
cludes claims (from speeches, interviews, so-
cial media and news articles), review articles
published by professional fact checkers and
premise articles used by those professional
fact checkers to support their review and verify
the veracity of the claims. An important chal-
lenge in the use of premise articles is the iden-
tiﬁcation of relevant passages that will help to
infer the veracity of a claim. We show that
transferring a dense passage retrieval model
trained with review articles improves the re-
trieval quality of passages in premise articles.
We report results for the prediction of claim
veracity by inference from premise articles.
1 Introduction
The rise of social media has led to a democratiza-
tion of news, but it has also ampliﬁed issues related
to fake news and misinformation. To that effect,
many fact checking organizations (e.g., Politifact,
Snopes, AFP Fact Check, Alt News, FactCheck.org,
Africa Check, etc.) have emerged around the globe.
They investigate debatable claims made by author-
ities, politicians, celebrities and the public. For
each claim, they publish a review article with links
to sources that support a verdict (e.g., true, partly
true/false, false) about the veracity of the claim.
Those reviews debunk false claims and mitigate the
spread of misinformation. We consider a key NLP
challenge in the context of automated fact check-
ing: claim inference from premise articles. Note
that determining the veracity of a claim without
additional information is nearly impossible since
claims are selected by professional fact checkersin part because their veracity is far from obvious
and also because of their degree of controversy. To
that effect, professional fact checkers invest a fair
amount of time to research each claim by ﬁnding
relevant sources and publishing a review article that
explains their verdict of the claim. Hence there is a
natural entailment problem, whereby anyone who
reads a review article should be able to arrive at
the same verdict as the professional fact checker
regarding the claim. Unlike many entailment tasks
that consist of short text (e.g., pairs of utterances)
that may be artiﬁcially generated or extracted, this
is a natural and challenging entailment task that
involves an entire document (review article) with
an utterance (claim) that requires a certain degree
of reading comprehension. We note that this entail-
ment problem has been tackled in some previous
work (Augenstein et al., 2019; Shu et al., 2018;
Nakov et al., 2021) and although it is a challenging
NLP problem, it does not correspond to the prob-
lem that professional fact checkers need to solve.
In this paper, we focus on the harder problem
of claim inference from premise articles. This is
part of the challenge that professional fact checkers
face. They ﬁnd premise articles that contain rele-
vant facts and then infer the veracity of the claim
based on those facts. Unlike many existing infer-
ence tasks where it is sufﬁcient to use one or a
few facts in a few sentences (Storks et al., 2019;
Schlegel et al., 2020), information from a set of
premise articles must be distilled and combined in
non trivial ways to infer the veracity of a claim.
We assembled a dataset of 33,697 claims made
between December 1996 and July 2021 with asso-
ciated review articles, premise articles and claim
verdicts. Many other datasets for claim veriﬁca-
tion are listed in Table 1. However, most of them
do not include premise articles needed for the in-
ference task described above. We note two ex-
ceptions: PubHealth (Kotonya and Toni, 2020b),
which is restricted to health claims and UKP1293Snopes (Hanselowski et al., 2019), which is re-
stricted to claims investigated by one fact check-
ing organization (Snopes). In contrast, WatClaim-
Check includes claims investigated by 8 fact check-
ing organizations on any topic.
Since there are several premise articles for a
given claim and each premise article may be long,
a simple two-stage approach to identify relevant
passages would consist of a lightweight retrieval
technique in a ﬁrst stage, followed by a heavy-
weight inference technique applied to those pas-
sages. When the ﬁrst stage fails to retrieve key
passages, then the inferred verdict will be nega-
tively affected regardless of how good the second
stage is. To that effect, several supervised dense
passage retrieval techniques have been proposed
for question-answering (Karpukhin et al., 2020; Qu
et al., 2021; Ren et al., 2021). Unfortunately, we
cannot directly apply those techniques since we do
not have labels for the relevant passages. Instead,
we show how to use the review articles to train a
supervised dense retrieval technique that is then
transferred to premise articles. The contributions
of the paper can be summarized as follows:
•New dataset of claims with review and
premise articles for claim inference in auto-
mated fact checking;
•Novel use of review articles to transfer a dense
retrieval technique to premise articles;
•Experiments establishing the state of the art
for claim inference.
The paper is organized as follows. Sect. 2 re-
views previous work related to automated fact
checking and claim veriﬁcation. Sect. 3 de-
scribes the new dataset and summarizes the differ-
ences with previous datasets for claim veriﬁcation.
Sect. 4 describes a two-stage process to i) extract
evidence sentences from premise articles and ii)
infer the veracity of claims. This section also ex-
plains how to transfer a dense passage retrieval
technique trained with review articles to premise
articles. Sect. 5 reports the results for the claim
veracity inference task. Finally, Sect. 6 concludes
and discusses possible future work.
2 Related Work
There is an important line of work that focuses
on claim veriﬁcation (Kotonya and Toni, 2020a;
Guo et al., 2022). This includes techniques thatpredict the veracity of a claim based on the text
of the claim only (Rashkin et al., 2017), linguis-
tic features (Popat et al., 2017), meta information
about the claimant (e.g., name, job, party afﬁliation,
veracity history) (Wang, 2017), review articles (Au-
genstein et al., 2019; Shu et al., 2018; Nakov et al.,
2021), relevant articles returned by a search en-
gine (Popat et al., 2018; Augenstein et al., 2019;
Mishra and Setty, 2019) as well as premise arti-
cles (Aly et al., 2021; Kotonya and Toni, 2020b).
There is an important distinction between articles
returned by a search engine and premise articles.
The techniques that use a search engine to ﬁnd arti-
cles related to a claim query the search engine after
a fact checking website has published a review ar-
ticle and therefore end up retrieving articles that
include the review article as well as other articles
that summarize and/or discuss the verdict of the
fact checking website. Hence they are tackling an
entailment problem. In contrast, the premise arti-
cles that we consider are the source articles used
by a fact checker before publishing a review article.
Those articles contain relevant facts, but not a sum-
mary or discussion of the review article since they
are published before the review article and in fact
serve as premises for the review article.
Closely related to claim veriﬁcation is the prob-
lem of fake news detection. In this problem, the
credibility of an entire news article is evaluated.
The credibility can be estimated based on lin-
guistic and textual features (Conroy et al., 2015;
Reis et al., 2019; Li et al., 2019), discourse level
structure (Karimi and Tang, 2019), network anal-
ysis (Conroy et al., 2015), knowledge graphs (Cui
et al., 2020), inter-user behaviour dynamics (Gan-
gireddy et al., 2020) or a combination of multiple
modalities (Wang et al., 2020). Some techniques
reorder the articles returned by a search engine
based on their degree of credibility (Olteanu et al.,
2013; Beylunioglu, 2020). An important task that
can help the detection of fake news is stance de-
tection (Borges et al., 2019; Jwa et al., 2019), i.e.,
does the content of an article agree or disagree with
the title of the article? The following surveys sum-
marize advances in fake news detection: (Kumar
and Shah, 2018; Bondielli and Marcelloni, 2019).
3 Fact Checking Dataset
3.1 Data Collection
We collect claims, along with a review article,
premise articles, and metadata from the follow-1294ing eight fact checking services: Politifact, Snopes,
AFP Fact Check, Alt News, FactCheck.org, Africa
Check, USA Today, and Full Fact. We utilize
Google’s fact check tool APIsto collect the claims’
metadata for all fact checking services except Poli-
tifact and Snopes. The claims’ metadata collected
from Google’s fact check tool APIs include the
claim review article URL, which is used to retrieve
the claim review article. The claim review articles
published by some of the fact checking services
provide the premise article URLs in a separate sec-
tion while others provide the URLs as inline links
in the review article body. We parse the article
body, retrieving the premise article URLs used in
the review article to justify the claim veracity. Fi-
nally, the premise URLs are used to retrieve the
premise articles. We try to directly retrieve the
article where possible, but also use archive.org’s
API in case a premise article is no longer available
online. We follow the same general procedure for
data collection from Politifact and Snopes except
that we directly crawl the respective websites in-
stead of using Google’s fact check tool APIs for
collecting claims and associated metadata.
We perform some basic cleanup to the col-
lected data before inclusion in the dataset. This
includes removing articles behind paywalls, remov-
ing claims with less than two premise articles, and
removing non-textual premise sources. We ob-
tain premise article text from their HTML pages
by loading the HTML ﬁles into a text based web
browser (Links browser) and then dumping the web
page text into a text ﬁle. This allows us to bypass
the CSS styling and JavaScript code included in the
HTML pages and obtain only the text displayed to
end users. Admittedly, this does not eliminate aux-
iliary text such as navigation links, footer text, rec-
ommended links, etc. The premise articles include
the source document of the claim when available as
well as evidence articles used by fact checkers. We
map the numerous claim veracity labels used by
the fact checking websites into three broad labels:
True, Partially True/False, and False.
3.2 WatClaimCheck Dataset
The contributed dataset contains a total of 33,721
claims. We split those claims into the following
three sets: training set containing 26,976 claims,
validation set containing 3,372 claims, and test
set containing 3,373 claims. For each claim in
the dataset, we provide the following data: ID,
Claimant, Claim, Claim Date, Reviewer Name,
Reviewer Site, Review Article URL, Review Arti-
cle Date, Review Article, Rating, Original Rating,
Premise Articles andPremise Article URLs . Here
Original Rating refers to the rating assigned by a
fact checking organization and Rating corresponds
to our mapping of the original rating to true, partly
true/false and false (see the dataset for the precise
mapping). We provide the extracted text ﬁles for
the review and premise articles.
Fig. 1 shows the number of claims per fact check-
ing services. Fig. 2 shows the claim rating distribu-
tion. Claims in the Partially True/False and False
categories signiﬁcantly outnumber the claims in
the True category. In reality, the number of true
claims is much larger than the number of partially
true/false and false claims, but fact checking ser-
vices focus on debunking controversial claims and
therefore the majority of the claims they investi-
gate are false or partially true/false. This imbalance
poses an important challenge.
3.3 Comparison with existing Datasets
We compare our proposed dataset with other pub-
licly available fact checking related datasets in Ta-
ble 1. We can broadly classify the fact checking
datasets into two different categories: (1) verac-
ity detection datasets based only on claim text and
some metadata, but without supporting evidence
documents and (2) datasets that provide claim text
along with supporting evidence and/or context doc-1295uments. The datasets that provide some evidence or
context documents can be further subcategorized:
(1) datasets that provide social media posts and
comments related to the claim (Mitra and Gilbert,
2015; Nakamura et al., 2020; Shu et al., 2018), (2)
datasets that retrieve supporting evidence for the
claims by performing a web search using queries
obtained from lexical and semantic features of the
claim text (Baly et al., 2018; Augenstein et al.,
2019; Gupta and Srikumar, 2021), (3) datasets that
provide Wikipedia pages as supporting evidence
(Thorne et al., 2018; Fan et al., 2020; Aly et al.,
2021), and (4) datasets that include premise articles
used by professional fact checkers (Hanselowski
et al., 2019; Kotonya and Toni, 2020b). Our pro-
posed dataset provides the documents cited by the
professional fact checkers in the claim review arti-
cle to justify their claim rating. This reﬂects the real
world task of automated veracity detection more
truthfully due to the availability of the premise arti-
cles cited by the professional fact checkers in claim
review articles. Although, social media posts and
comments can sometimes be helpful in claim verac-
ity detection they are rarely treated as authoritative
sources of information. Using a web search to re-
trieve evidence documents after a fact checking
service has veriﬁed a claim is problematic since
multiple news agencies often publish articles ref-
erencing the original fact checking review article.
Top-k web search results typically contain those ar-
ticles which may indirectly leak the veracity label.
4 Models
We develop a two-stage system to perform evidence
based veracity detection. The ﬁrst stage selects rel-
evant sentence level evidence from the premise
articles associated with a claim and the second
stage performs claim veracity inference using the
claim text and selected evidence sentences. For the
ﬁrst stage, we evaluate two different approaches.
The ﬁrst approach is term frequency inverse docu-
ment frequency (TF-IDF), which is typically used
by fact checking methods for sentence based re-
trieval (Aly et al., 2021). For the second approach,
we propose a novel way to adapt dense passage
retrieval techniques using the review articles for ev-
idence sentence selection. In our experiments, the
aforementioned dense passage retrieval technique
outperforms TF-IDF text retrieval and leads to over-
all system performance improvements. The second
stage consists of training deep learning models toperform claim veracity inference using the claim
text and selected evidence. We utilize multiple
deep learning models to perform claim veracity in-
ference ranging from basic bi-directional recurrent
networks to state of the art transformers.
4.1 Problem Formulation
We represent a claim containing ltokens as C=
fc; c; : : : ; cg, where n2[1; N]andNis
the size of the dataset. Each claim is associ-
ated with multiple premise articles, we repre-
sent the k-thpremise article associated with the
n-thclaim containing msentences as A=
fs; s; : : : ; sgwhere srepresents the
i-thsentence. Similarly, we represent the review
article associated with the claim Ccontaining m
sentences by R=fs; s; : : : ; sg. For a
given claim C, we represent its ground truth ve-
racity label by y. We cast the problem as a textual
inference problem. Given a claim Cand a set of
associated premise articles A, our goal is to predict
the ground truth veracity yof the claim.
4.2 Stage-1: Evidence Sentence Extraction
A key step performed by professional fact checkers
is examining the premise articles associated with
a claim and extracting useful evidence from them
to establish claim veracity. Our ﬁrst stage seeks to
perform a similar task. Each claim in our dataset
has multiple associated premise articles with each
article containing a large amount of text. Our goal
in the ﬁrst stage is to rank the evidence available
in the associated premise articles at the sentence
level and extract the ones which are most useful
and impactful for veracity detection in the second
stage. Our experiments show that an improvement
in this stage directly contributes to an overall im-
provement in the veracity detection performance.
4.2.1 TF-IDF
We measure TF-IDF similarity between the claim
text and the premise article sentences to rank the
sentence level evidence. Top ranked sentences are
used in the second stage to perform veracity detec-
tion. This approach is similar to the one used by
Thorne et al. (2018) to extract evidence sentences
from Wikipedia articles for fact checking.
4.2.2 Dense Passage Retrieval
We propose a novel way of adapting the dense pas-
sage retrieval method proposed by Karpukhin et al.
(2020) for open domain question answering to the1296
task of retrieving evidence sentences from premise
articles. Karpukhin et al.’s method uses a dual en-
coder architecture. Each encoder is implemented
using BERT (Devlin et al., 2018). The question
encoder Eand the passage encoder Eembed
question qand passage pintod-dimensional vec-
tors. The similarity between the question and pas-
sage is deﬁned as the dot product of their vectors:
sim(q; p) =E(q)E(p) (1)
The model is then trained to learn embeddings such
that the similarity score between relevant question-
passage pairs will be higher than irrelevant ones.
We adapt this method for our ﬁrst stage by tak-
ing advantage of the fact that the review article
published by fact checking websites (along with a
claim) typically contains key evidence taken from
the premise articles. The evidence is usually para-
phrased in order to form a coherent argument in
support of the claim veracity verdict.
To train the dense passage retrieval model for
stage-1, we use the claims and the associated re-
view articles in the training set of our dataset. We
form positive pairs using the claim and the sen-
tences from the associated review article. The
negative pairs are formed using that same claim
and sentences from review articles associated with
other claims. This corresponds to the “gold” nega-
tive sampling technique in (Karpukhin et al., 2020).
LetD=fhC; s; s; s; : : : ; sig
be the training data containingPjRjinstances
where Nis the number of claims in the training set,jRjis the number of sentences in the review article
associated with the i-th claim. Each instance is
made up of a claim Cwith one positive sentence
from the associated review article sandn 1
randomly chosen negative sentences s. These
negative sentences are positive sentences for other
claims within the same batch. We train the model
by optimizing the negative log likelihood of the
positive sentences:
L(C; s; s; s; : : : ; s)
= loge
e+Pe(2)
For model evaluation, we use the top-k recall rate
for retrieving the review article sentences corre-
sponding to the claims in the validation and test
set using the similarity score. The review article
sentences are retrieved from the corpus formed by
all the sentences from every review article in the
corresponding set.
After training, we use the encoders to encode
the claim text and the sentences of the associated
premise articles. We compute the similarity score
using the dot product between the encoded claim
vector and the premise article sentences. We use
the top scoring sentences as evidence sentences in
the next stage to perform claim veracity inference.
4.3 Stage-2: Claim Veracity Inference
In this section, we describe how several popular
sequence models are used to classify a claim as1297true, partly true/false or false based on the text of
the claim, the claimant and the evidence sentences
extracted in stage 1.
4.3.1 Bi-LSTM and Bi-GRU
We ﬁrst consider bi-directional long short term
memory (Bi-LSTM) networks and bi-directional
gated recurrent units (Bi-GRUs). The evidence
sentences of each premise article are concatenated
with the claim and claimant, and then encoded by
a Bi-LSTM or Bi-GRU into a latent vector. For N
premise articles, the resulting Nvectors are then
averaged and passed through a softmax layer with 3
outputs corresponding to the predicted probabilities
of true, partly true/false and false.
4.3.2 HAN
Instead of concatenating the evidence sentences
of each premise article into a long sequence,
we can also use hierarchical attention networks
(HANs) (Yang et al., 2016; Mishra and Setty, 2019)
to compute sentence level embeddings that are then
combined into article level embeddings. A HAN is
used to embed each premise article with the claim
as follows. Each sentence (claimant with claim text
or each evidence sentence of the premise article)
is embedded as a sequence of hidden vectors (one
per word) by a bi-directional recurrent network (Bi-
LSTM or Bi-GRU). Then, a word-level attention
layer computes a sentence level embedding. Next,
those embeddings are fed to another bi-directional
recurrent network (Bi-LSTM or Bi-GRU) that com-
putes a sequence of hidden vectors (one per sen-
tence) and a sentence level attention layer computes
an embedding for the document-claim pair. Finally,
the embeddings of the document-claim pairs are
averaged and passed through a softmax over the
labels true, partly true/false and false.
4.3.3 Transformer
We ﬁnetune a RoBERTa-base (Liu et al., 2019)
model to perform claim veracity inference using
the claim and the evidence sentences. We concate-
nate the claim text, the name of the claimant, and
the evidence sentences extracted for that particular
claim in the ﬁrst stage to build a training data in-
stance. The input sequence is encoded using the
RoBERTa-base model and passed through a dense
linear layer followed by a softmax to obtain the
predicted claim veracity label distribution. We use
the cross entropy loss function to train the model.5 Experiments
We evaluate the two-stage process and the algo-
rithms described in the previous section on the
claim inference problem with our new dataset.
5.1 Stage-1 Results
In order to reduce the computational resources and
memory requirements, we implement the encoders
in the dense passage retrieval model using Distil-
RoBERTa (Dis). We use a batch size of 64 and
thein-batch negatives technique as described in
(Karpukhin et al., 2020).
We evaluate the stage-1 methods by comparing
their performance using the top-k recall rate metric.
The claim text is used to retrieve the ground truth
review article sentences from the corpus containing
all the sentences of all the review articles in the test
set. The test contains a total of 114;290sentences
and3;373claims. We report the top-k recall rate
fork= 10 ;25;50;100in Table 2. The results
clearly show that the DPR (dense passage retrieval)
method outperforms the method based on TF-IDF.
Top-k Recall TF-IDF DPR (DistilRoberta)
Top-10 Recall 18% 26%
Top-25 Recall 25% 38%
Top-50 Recall 30% 46%
Top-100 Recall 35% 54%
5.2 Stage-2 Results
To evaluate whether the inference models in stage-
2 can do better with the inclusion of additional
evidence sentences, we perform the experiments in
stage-2 in two settings: Pooled and Averaged.
Pooled: In this setting, for each claim we pool
all the sentences from every associated premise
article and rank them using the similarity score.
The evidence sentences are concatenated in the
descending order of their similarity score. After-
wards, the claim text and evidence sentences are
concatenated. The resulting text is then truncated
to the maximum sequence length capability of the
transformer model being used to perform claim ve-
racity inference. For each claim, we get exactly
one data instance.
Averaged: This refers to the setting where we gen-
erate one data instance per claim and associated
premise article. So, if a claim has mpremise ar-
ticles, we get mdata instances. For each premise1298article associated with a claim, we score the sen-
tences from that article and extract the top scoring
sentences to form a data instance. We concatenate
the evidence sentences in the descending order of
their similarity score. The evidence sentences are
then concatenated to the claim text and truncated
to the maximum sequence length capability of the
transformer model being used to perform claim
veracity inference. During training, each data in-
stance for a claim is used independently, but during
inference, we compute the average of the claim ve-
racity prediction distributions of the data instances
associated with a single claim. We show in our
reported results that the inclusion of additional ev-
idence in the form of mdata instances per claim
(instead of 1 data instance for the pooled setting)
does improve the performance when the retrieval
method of stage 1 is not very effective.
We use macro F1 as the evaluation metric. We
report the results in Table 3. We report all the
hyper parameters used in our experiments in the
appendix. The best performance when doing the
claim veracity inference is obtained by using the
DPR model in the ﬁrst stage and the RoBERTa-base
model in the second stage. We also report results
for claim entailment from the review articles as an
upper bound on the accuracy that could be achieved
for claim inference based on the premise articles.
5.2.1 Prequential Results
We note that the traditional experimental setup of
dividing a dataset at random into train, validation
and test does not reﬂect the streaming nature of
claims. When new topics arise (i.e., election, covid-
19), the nature of the claims and the premise arti-
cles changes. Randomly splitting the dataset into
train/validation/test ensures that all claim topics
are well represented across the train/validation/test
splits, which would not be the case in practice. In
reality, when a new topic arises, the test split may
have new types of claims that are not well repre-
sented in the train/validation splits. To evaluate the
effect of this distribution shift over time, we per-
formed a prequential evaluation (Bifet et al., 2015).
More precisely, we divide the dataset into subsets
corresponding to periods of 6 months. We repeat-
edly evaluate the performance for each 6-month
period by treating the claims in that period as the
test set and the claims in previous periods as the
train/validation sets. This corresponds to a realistic
setting where a claim veriﬁcation algorithm may
be re-trained every 6 months on the data seen sofar to predict the veracity of the claims for the next
6 months. Naturally, the time period between each
re-training iteration may be shorter than 6 months
in practice. We chose 6 months simply to ensure
that the size of the test set would be large enough
to obtain reliable results.
Fig. 3 shows the number of claims investigated
in each 6-month period in our dataset. We note
two peaks. The ﬁrst one in 2016 corresponds to a
sudden surge of claims investigated by some fact
checking websites regarding India politics. The
second peak in 2020 corresponds to the 2020 US
presidential election and the start of the covid-
19 pandemic. Fig. 4 shows the macro F1 results
achieved by the top 4 algorithms with DPR evi-
dence in each 6-month period. We note that the
prequential results are signiﬁcantly lower than the
results in the DPR column of Table 3. This drop of
accuracy is precisely due to the distribution shift
of claims that naturally occurs over time. We also
note a trend whereby the accuracy increases as time
passes by. This is explained by the fact that more
data is available for training in later time periods.
We strongly recommend that future algorithms be
evaluated in prequential mode since this evaluation
setup is more realistic.
6 Conclusion
This paper introduces a new dataset for automated
fact checking. WatClaimCheck includes premise
articles used by professional fact checkers and
therefore corresponds more closely to the task of
claim veracity inference in automated fact check-
ing. An important challenge is the extraction of
relevant facts from the premise articles since it is
not generally possible to apply heavyweight mod-
els on the entire content of all premise articles. To
that effect, we described how to train the encoders
of a dense passage retrieval technique with the re-
view articles and then transfer the resulting retrieval
technique to the premise articles. This increased
the overall performance of the claim veriﬁcation al-
gorithms. We also performed a prequential evalua-
tion that highlighted an important distribution shift
that caused a signiﬁcant drop in accuracy for all
algorithms. We strongly recommend that future al-
gorithms be evaluated in prequential mode. In fact,
an important direction for future research would
be to design algorithms based on transfer learning
or domain generalization that can cope better with
this distributional shift. We also note that the tech-1299Algorithm Review Evidence(TF-IDF) Evidence(DPR)
Bi-GRU 0.7790.009 0.4180.010 0.4530.009
Bi-LSTM 0.7770.008 0.4210.011 0.4540.010
HAN-Bi-GRU 0.8210.007 0.4450.010 0.4710.009
HAN-Bi-LSTM 0.8180.007 0.4440.008 0.4710.011
Roberta-base (pooled) 0.7410.005 0.5410.017 0.5800.009
Roberta-base (averaged) 0.7410.005 0.5630.010 0.5650.0091300niques that we evaluated are black boxes and there-
fore it is not clear how they do inference. Hence,
another direction for future research would be to
develop inference techniques that are explainable
in the sense that they could provide explanations
to the users to justify their veracity prediction for a
claim.
Acknowledgements
We thank the Schulich Foundation and the Natu-
ral Sciences and Engineering Research Council
of Canada (NSERC) for funding support. Re-
sources used in this work were provided, in
part, by the Province of Ontario, the Govern-
ment of Canada through CIFAR, and compa-
nies sponsoring the Vector Institute https://
vectorinstitute.ai/partners/ .
References13011302
A Appendix
A.1 Dataset
Since the dataset is larger than the 200 Mb limit for
the supplementary material, we include a sample
corresponding to the data collected from March
15 to July 1, 2021 in the supplementary material.
A link to the entire dataset will be made available
once the paper is accepted.
A.2 Hyperparameters
The code will be made public once the paper is
accepted. Table 4 lists the hyperparameters used
for the Bi-LSTM, Bi-GRU and HANs. Table 5 de-
scribes the hyperparameters of the DPR technique
in stage 1 and Table 6 lists the hyperparameters of
RoBERTA-base in stage 2.1303Parameter Value
Max Sentence Length 20 words per sentence
Max Sentence Count 30 sentences per claim
Embedding Dimension 100
Batch Size 64
Learning Rate 0.01
Convergence Patience 6 epochs
Optimizer Adam
Parameter Value
Max Sequence Length 512
Batch Size 64
Learning Rate 1e
Epochs 500
Optimizer AdamW
Parameter Value
Max Sequence Length 512
Batch Size 12
Learning Rate 1e
Epochs 10
Optimizer AdamW1304