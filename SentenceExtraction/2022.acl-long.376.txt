
Damián Blasi
Harvard UniversityAntonios Anastasopoulos
George Mason UniversityGraham Neubig
Carnegie Mellon University
Abstract
Natural language processing (NLP) systems
have become a central technology in commu-
nication, education, medicine, artiﬁcial intel-
ligence, and many other domains of research
and development. While the performance of
NLP methods has grown enormously over the
last decade, this progress has been restricted to
a minuscule subset of the world’s 6,500 lan-
guages. We introduce a framework for estimat-
ing the global utility of language technologies
as revealed in a comprehensive snapshot of re-
cent publications in NLP. Our analyses involve
the ﬁeld at large, but also more in-depth stud-
ies on both user-facing technologies (machine
translation, language understanding, question
answering, text-to-speech synthesis) as well as
foundational NLP tasks (dependency parsing,
morphological inﬂection). In the process, we
(1) quantify disparities in the current state of
NLP research, (2) explore some of its associ-
ated societal and academic factors, and (3) pro-
duce tailored recommendations for evidence-
based policy making aimed at promoting more
global and equitable language technologies.
1 Introduction
The past decade has seen a rapid advance in natural
language processing (NLP); it has grown from a
relatively technical niche to a fundamental tool in
virtually all domains that involve language data in
any shape or form. NLP is now instrumental to not
only bread-and-butter applications such as transla-
tion and question answering, but also tasks as wide
ranging as detection of neurodegenerative diseases
(Orimaye et al., 2017), exposing widespread gen-
der and ethnic biases in societies (Caliskan et al.,
2017), and predicting large-scale trends in collec-
tive consumer behavior (Kallus, 2014). Because
of this NLP has become a staple technology foreveryday frequent tasks in most contemporary soci-
eties of the world. For instance, an English speaker
with a smartphone can now easily get accurate in-
formation on many topics through a quick query
to a virtual assistant, they can consult an online
translation service to translate a foreign language
web page with a click, and they can interact with
many different machines and computers through
simple speech commands.
These technological capabilities can be at-
tributed to several developments over the last few
decades: 1. the advent of deep learning methods,
which allow for more effective creation of NLP
systems from existing data (Goldberg, 2017), 2. the
existence of standardized benchmark datasets and
evaluation metrics (Wang et al., 2018; Hu et al.,
2020), 3. the prestige afforded by the research
community to researchers who improve upon these
benchmarks, 4. the resulting large number of re-
sources, be they computation, data, or ingenu-
ity, that are poured into optimizing performance
thereon. As both a theoretical and technical en-
deavor, NLP is experiencing an explosive increase:
the annual conference of the Association of Com-
putational Linguistics (ACL) received in 2000 less
than 300 papers, growing in 2010 to slightly less
than 1,000, to over more than 3,500 submissions
in its 2020 edition. Largely as a result of this ex-
pansion of research effort, state-of-the-art systems
have also achieved evaluation benchmark scores on
par with human performance on a variety of NLP
tasks such as question answering on English (He
et al., 2021), or on automatic translation of news
from German, Russian, and Chinese to English
(Barrault et al., 2020).
These upward slanting curves on standard bench-
marks fail to show how uneven this development
has been for all potential NLP users. Extensive
research across NLP tasks have found systematic5486performance drops according to dimensions such
as gender, racial identity, and language varieties,
among others. The reasons for these biases are mul-
tifactorial and can be traced to virtually all stages
in the process of NLP development, from the data
used to train systems (Caliskan et al., 2017; Sap
et al., 2019; De-Arteaga et al., 2019; Tatman, 2017;
Tatman and Kasten, 2017; Buolamwini and Ge-
bru, 2018; Raji and Buolamwini, 2019) to the very
algorithms involved (Speicher et al., 2018; Bel-
lamy et al., 2018; Adebayo et al., 2016). The grow-
ing awareness of these biases in NLP technologies
brought by these studies, along with the develop-
ment of novel metrics and tests to evaluate these
disparities, have resulted in progressively more ef-
ﬁcient and principled strategies to understand and
mitigate them.
However, similarly systematic approaches are
still lacking in one fundamental dimension of vari-
ation across individuals: their languages. Out of
the over 6,500 languages spoken or signed in the
world today (Hammarström, 2015), only a hand-
ful are systematically represented in academia and
industry (Joshi et al., 2020; Yin et al., 2021). In
spite of the aforementioned near-human results on
translation or understanding of languages from the
world’s economic and political superpowers, the
experience of any NLP practitioner is that, for the
vast majority of languages, they fall far below such
standards. Critically, the languages of the world
showcase substantial variation in most domains of
description, and in fact, the performance of lan-
guage technologies has been shown to be sensi-
tive to diverse aspects of the language under study,
including morphology, word order, or phonolog-
ical repertoire, as well as more mundane aspects
like writing script or data availability (Arivazhagan
et al., 2019; Tsarfaty et al., 2020; Xia et al., 2020;
Muller et al., 2021). Hence, the transfer of NLP
developments from one language to another is far
from trivial, as it often means that building highly
functional language technologies for any particular
language is a non-automatic, costly, and technically
challenging task.
Taking all these considerations together, and
given that even the consequences brought by un-
equal NLP technologies across (racial, gender, so-
cioeconomic) groups within the same nominal lan-
guage are already substantial, there is a pressing
need for measuring and understanding NLP perfor-
mance inequalities across the world’s languages.Here we develop novel estimates on how the util-
ity afforded by NLP systems is distributed across
individuals, languages, and tasks at an unprece-
dented global scale. These estimates allow us to
identify which languages are systematically under-
served by language technologies and could beneﬁt
the most individuals from focused technology de-
velopment. We ﬁnally trace these inequalities to
the societal, economic, and academic correlates of
NLP systems’ performance, shedding light on its
latent causes, and indicate how our results favor
speciﬁc evidence-based policies in research and
development.
2 Methodology
2.1 Quantifying utility and demand
Our fundamental goal is evaluating the distribu-
tion of diverse representative language technolo-
gies (and their qualities) across the world’s lan-
guages and their populations. Minimally, we would
attempt to account for the patterns of association
between the demand of language technologies and
theutility they confer to users across languages.
Thus, the ﬁrst component of our analysis pertains
quantifying the utility users in a given language l
receive from a language technology. Ideally, such a
measure would capture to what extent a given NLP
system solves the speciﬁc problems an individual
can pose to them - for instance, how successfully
the user can obtain information from an automati-
cally translated web page, or how satisﬁed the user
is by a speech-based virtual assistant’s execution
of a series of verbal commands.
Intuitively, utility is associated with the nominal
performance of the technology - a more performant
system will allow the user to obtain a greater de-
gree of utility. How “performance” is measured
depends on the task (see Section 1). Since our pur-
pose is to allow for comparisons, we deﬁne the
utility of a task and language, u, as the correspond-
ing performance normalized by the best possible
performance afforded by such task, i.e.
u=performance
theoretical max performance
In cases where the best possible performance is
undeﬁned or technically unattainable, we take the
empirical maximum as an estimate of the theo-
retical one and normalize by the best-performing
language across all languages L, i.e. we re-
place the denominator in the above deﬁnition by
max(performance).5487
Deﬁning utility in this manner allow us to ex-
plore and contrast language technologies at the
broadest scale, which is possible thanks to some
necessary simplifying assumptions. As we pointed
out before, not all users of the same language tech-
nology might beneﬁt in the same manner given a
ﬁxed performance, and the relation between nom-
inal performance and “true" utility might be com-
plex and non-linear.
With these caveats in mind, we further quantify
the second component of our analysis, the demand
for a language technology in each language l,d.
We characterize dby taking into consideration de-
mographic and linguistic perspectives. Under the
ﬁrst perspective, the demand for a given technol-
ogy in a language is estimated to be proportional
to the number of speakers of the language itself n
(d/n). Under the second perspective, the de-
mand across the approximately 6,500 languages of
the world is identical ( d/1). These two alterna-
tives as well as any intermediate combination of
them can be simply parameterized through a single
exponent,
d=nPn
where= 1correspond to a demographic notion
of demand,= 0to a linguistic one, and 0< <
1is in between.
Equipped with these notions, we construct a sim-
ple family of global metrics ( M) revealing to what
degree the global demand for language technolo-
gies is actually met:
M=XduMhas a number of intuitive properties we
would like such a metric to have. Mis bounded
between 0 and 1; 0 corresponds to a case where
no-one beneﬁts from a given language technology,
whereas 1 would correspond to a situation where
all languages enjoy perfect technology. Increasing
the utility of a given language leads to an increase
inM, and the magnitude of this increase is inﬂu-
enced by both the size of the improvement and the
demand in that language.
2.2 NLP tasks
We apply our measures of utility and demand to a
set of diverse and representative NLP tasks, which
are described below and summarized in Table 1.
The ﬁrst three are tasks that technology users
interact with directly in their everyday life, so that
their output is already in a shape and form that is
usable for most individuals. Question Answering
(QA) consists of crafting a relevant answer to a
question formulated in natural language, such as
e.g. “what is the capital city of the Philippines?"
or “why do dogs like bones?". This task is ubiqui-
tous in online search or virtual assistants. Machine
Translation (MT) is the task of translating from one
language to another (e.g. from Tagalog to Estonian
or from Japanese to Basque), and is typically used
to facilitate inter-personal communication, infor-
mation gathering, and e-commerce. Text-to-speech
(TTS) is the task of rendering speech from textual
input, which is used widely in spoken virtual assis-
tants, car navigation systems, and is becoming a
gateway for internet-of-things devices.
Next, Natural Language Inference (NLI) is a
central task in AI and involves the evaluation of in-
formation presented in propositional format. More
speciﬁcally, given a sentence called the “premise”
(e.g. “the dog chewed a big bone”), NLI systems
decide whether a separate sentence called the “hy-
pothesis” is entailed by the premise (e.g. “the dog5488gnawed at a bone”), negated by it (e.g. “the dog was
sleeping”), or neither (e.g. “the dog likes bones”).
While not a user-facing task per se , it measures
the ability of NLP systems to adequately represent
(and “understand”) user queries.
Beyond these three (plus one) user-facing
tasks, we also consider two more foundational
linguistically-focused tasks, which often inform
part of the pipelines of the user-facing tasks but
which are rarely if ever encountered “in the wild"
by language technology users. Morphological In-
ﬂection (Inﬂection) is the task of generating an in-
ﬂected wordform given a lemma and a morpholog-
ical speciﬁcation, e.g. producing the third person
singular form for “run”:! .Syntac-
tic Parsing under the dependency formalism (DEP)
is the task of producing a syntactic parse of an input
sentence, e.g. given the sentence “dogs like bones”
specifying the “dogs” and “bones” are the subject
and object of “like” respectively.
2.3 Correlates of NLP utility
Beyond the performance of individual tasks, we
take a bird’s-eye-view of the ﬁeld of language tech-
nologies in general, as we analyze some of the
correlates of the scientiﬁc production in NLP. In
particular, we follow two broad guiding questions:
(1) does the system of academic incentives pro-
mote the development of a more linguistically di-
verse NLP? and (2) is economic centrality or sheer
demographic demand the best predictor of NLP
technologies in any given language?
While a full understanding of the complex causal
mechanisms binding society and NLP in general
is outside of the scope of the present article, we
set out to provide a ﬁrst large-scale exploration of
these matters by considering scientiﬁc publications
appearing in major international NLP conferences
as the basic units of science production. This sim-
pliﬁcation is not without challenges: for instance,
some widely used language technologies are de-
veloped outside of the traditional scientiﬁc circuit
based on proprietary technology, or they are pub-
lished in local conferences, possibly in languages
other than English.In spite of this, studying sci-
entiﬁc publications (and their correlates) allows us
to evaluate transparent questions on the basis of
publicly available data at a scale that is unfeasible
for in-depth analyses.Therefore, we study the ﬁrst question by deter-
mining whether the cumulative number of citations
a paper receives is correlated with the number of
languages it is associated with. We investigate
our second question by ﬁnding the best predictive
model of the number of NLP papers in any given
language by contrasting two predictors: estimated
number of users worldwide and approximate GDP
associated with its users. We model these regres-
sion problems in a Bayesian generalized mixed
effects framework (see Appendix B).
2.4 Data
We manually aggregate information on task per-
formance and demand for the tasks summarized
in Table 1 from a number of sources (we relegate
many details to Appendix A, and give a high-level
overview here). The data is taken from a combina-
tion of multilingual benchmarks, shared tasks and
published results in NLP conferences including:
Question answering: We use data from the TyDi-
QA (Clark et al., 2020), MLQA (Lewis et al.,
2020), and SD-QA (Faisal et al., 2021) bench-
marks and measure raw accuracy to calculate
utility.
Machine translation: We aggregate scores from
the WMT and IWSLT evaluation campaigns,
and 50 studies from the last three years’ ACL,
EMNLP, and NAACL conferences, using
BLEU (Papineni et al., 2002) as an accuracy
metric.
Text-to-speech: We use data from the CMU
Wilderness Project (Black, 2019) and use nor-
malized negative mel-ceptral distortion (Ku-
bichek, 1993) as an accuracy metric.
Natural language inference: We use results from
the XNLI leaderboard (Conneau et al., 2018)
and raw accuracy as the evaluation metric.
Syntactic parsing: We use the accuracies pro-
vided by UDPipe (Straka, 2018) and UD-
ify (Kondratyuk and Straka, 2019) on the
universal dependencies corpus (Zeman et al.,
2017), with labeled attachment score as an
accuracy metric.
Morphological inﬂection: We use results from
SIGMORPHON workshops inﬂection shared
tasks (e.g. (Vylomova et al., 2020)) measuring
utility with exact-match accuracy.5489
Demographic and linguistic information necessary
for the estimation of demands were obtained from a
variety of sources, including Ethnologue, Glottolog,
and the World Trade Organisation. For most tasks,
the number of ﬁrst-language speakers is used to
measure demand, but for MT we estimate the need
for translation between two languages based on
economic indicators of interaction between coun-
tries, and the language-speaking populations within
the countries where the language is spoken.
3 Results and Analysis
3.1 General observations
Figure 1 presents an overview of our main ﬁnd-
ings. Unsurprisingly, most NLP tasks we focus on
fare substantially better when utility is measured
demographically rather than linguistically.
Text-to-speech synthesis is the task with the most
linguistic coverage: the published results (due to
a single study (Black, 2019)) cover more than 630
languages (or about 10% of the world’s languages).
However, for the vast majority of these languages
the measured quality of the generated speech is
about half as good as the exceptionally good En-
glish system (Ren et al., 2021). The next most
linguistically diverse tasks are those regarding mor-
phosyntactic analysis, i.e. morphological inﬂection
and dependency parsing, which have been evalu-
ated over 140 and 90 languages respectively. For
these more esoteric tasks which do not necessar-
ily convey direct utility to a downstream user, the
majority of the systems are in general very good.
Natural language inference (NLI; a representa-tive natural language understanding task) and ques-
tion answering (QA) lie on the opposite side of the
spectrum: the established benchmarks have only
focused on up to 15 and 17 languages respectively,
leading to very low scores on the linguistic axis.
In Figure 1 (right panel) we observe the progress
of the utility metrics in tasks for which we had ac-
cess to comparable data across a span of the last
7 years. The extensive efforts of the UniMorph
project (Kirov et al., 2018) to cover as many lan-
guages as possible are visible in the “Inﬂection”
plot, with signiﬁcant improvements over time. On
the other hand, the machine translation ﬁeld is still
in the process of ramping up following demograph-
ics and/or socioeconomic priorities, with improved
linguistic coverage over the years.
The granularity of these ﬁndings can be in-
creased on the basis of available data. Figure 2
additionally presents demographic utility across
language populations for all tasks. The visualiza-
tion allows for identiﬁcation of ostensive gaps in re-
ceived utility. The two bottom plots of Figure 2 dis-
play our metrics over speakers of a single language,
based on question answering results for different
spoken Arabic and Swahili lectal varieties (Faisal
et al., 2021). This analysis shows that utility dif-
ferences are small between Arabic vernaculars al-
though these systems still lag behind the systems
for Modern Standard Arabic, while the utility level
of Coastal Swahili speakers in Tanzania is about
10% lower than that for speakers in Kenya.5490Dependency Parsing: M= 0:63 Morphological Inﬂection: M= 0:64
Natural Language Inference: M= 0:42 Question Answering: M= 0:36
Speech Synthesis: M= 0:32 Machine Translation (X !English):M= 0:49
Machine Translation (X !Spanish):M= 0:36Machine Translation (X !Bengali):M= 0:10
QA [on Arabic Vernaculars]: M= 0:58 QA [on Swahili Vernaculars]: M= 0:235491
3.2 Priorities in NLP development
Given the current snapshot of NLP systems, we
could ask which languages will lead to the largest
global utility improvement. The relative impor-
tance of linguistic vs. demographic demands deter-
mines the priority ranking, as it can be observed in
Figure 3 for a sample of ﬁve tasks. Improving on
the demographic-focused utility entails a greater
emphasis on Mandarin Chinese, Hindi, Spanish,
and other populous languages that are generally
well-served by current technologies. Balancing lin-
guistic and demographic considerations leads to
prioritizing a more diverse set of languages, mostly
Asian and African languages like Amharic, Bam-
bara, Bengali, Thai, or Yoruba, which are both
populous and under-served, along with also large
but severely under-served languages like Kurdish,
Urdu, and Oromo. Further emphasis on linguis-
tic utility would lead to prioritization of indige-
nous and potentially endangered languages of small
communities like Aimele, Itelmen, North Sami, or
Warlpiri, which are currently largely ignored by
NLP research (Bird, 2020).3.3 The role of society, economy, and
academia
Now we turn to our large-scale analysis of NLP
publications. First, this reveals that a substantial
proportion of publications do not even describe in
a clear and unequivocal manner the language (or
languages) they are dealing with (Bender, 2011).
Given the current prevalence of English of a lan-
guage of study in NLP, in most cases, the lack of an
explicit reference to a particular language entails
the system deals with English exclusively.
This reﬂects a more deep-seated issue reﬂected
in the citation of papers over time. Independently
of publication venue, year, or subﬁeld of NLP re-
search, the number of languages a publication deals
with is not predictive of how many citations it will
accrue over time (see Figure 4, top right panel).
In other words, if citations can be regarded as a
proxy for academic incentives, scientists and de-
velopers are presented with little to no additional
academic reward when tackling data, problems, or
tasks involving more than one language.
This naturally leads to the question of what ex-
plains the production of language technologies
across languages to start with, which will necessar-
ily involve agents, mechanisms, and data, outside
of the scope of NLP publications themselves. Nev-
ertheless, in order to contribute to this investigation,
we determined whether approximate measures of
economic centrality or number of language users
were better predictors of sheer number of papers
published for any given language (see Appendix C).
While both variables are substantially collinear, we
ﬁnd that approximate GDP (rather than number of
users) leads to a substantially smaller prediction
error of number of published papers.
4 Discussion
Our study, covering diverse NLP tasks and types of
evidence, makes apparent the immense inequality
in the development of language technologies across
the world’s languages. After English, a handful of
Western European languages dominate the ﬁeld -in
particular German, French, and and Spanish- as
well as even fewer non-Indo-European languages,
primarily Chinese, Japanese, and Arabic. Our pre-
liminary investigation suggests it is the economic
prowess of the users of a language (rather than
the sheer demographic demand) what drives the
development of language technologies.
In spite of this, for some tasks (such as In-5492
ﬂection) there is an encouraging trend of both
demographic- and linguistic-utility improving year-
over-year. This is due to the nature of the task; rea-
sonably accurate solutions can be achieved through
small but highly-curated data. Since linguistic ex-
pertise on the languages of the world is, naturally,
globally distributed, the main hurdle these tasks
face is to pool such expertise under the premise
of a common technical goal. In this respect, rela-
tively low-cost and bottom-up actions that gather
experts to work on speciﬁc NLP tasks (such as
Universal Dependencies and UniMorph) have suc-
ceeded in accelerating the cross-linguistic devel-
opment of language technologies. These prosper
mainly on the basis of academic incentives, as those
individuals or groups who contribute data and/or
expertise are rewarded with individual publications
or co-authorship in collective publications. Many
of these contributions - which do not necessarily
involve hefty resource investments but instead lin-
guistic expertise - are markedly different from the
typical publications in language technologies.
However, these more esoteric tasks are tenu-
ously associated with those that users are more
likely to interact with, such as Machine Transla-
tion or Speech Synthesis. User-facing tasks all
have in common a tight dependency on compu-
tational resources and large data, which in turnhinge on substantial ﬁnancial means. In a con-
text of pressing user needs across multiple popu-
lations and languages, we submit that future de-
velopments on policies aimed at furthering cross-
linguistic technologies would beneﬁt from clear
(and possibly standardized) metrics that assist in
streamlining complex decisions regarding resource
allocation. Our measures of global coverage fulﬁll
that role, and help identifying large but currently
under-served languages. While we do not attempt
to supplement the necessary in-depth evaluation of
the need of each individual group and language,
they provide a common ground for coordinating
global efforts across heterogeneous actors.
In addition, we would like to reiterate that our
work here has necessarily made a large number of
simplifying assumptions to even attempt to quan-
tify disparities in language technology utility on a
global scale. These most notably involve simpli-
fying assumptions regarding the measurement of
demand (based on native-speaker population and/or
economic indicators) and the measurement of util-
ity (based on simple accuracy metrics). Future
work may further clarify these assumptions, mak-
ing more accurate estimates of true user demand
on a technology-by-technology level, or more accu-
rately clarifying the relationship between standard
accuracy metrics and the utility derived by users.5493Acknowledgements
This work was supported by NSF Award 2040926.
References549454955496
A Materials
Publication data We rely on papers available
through the Anthology of the Association of Com-
putational Linguisticswhich hosts more than 60
thousand papers from all major NLP conferences.
We rely on Semantic Scholar (Ammar et al., 2018)
for citation information.
We make the working assumption that a mention
of a language in a research paper likely entails that
the underlying research involves this language. We
follow an automatic pipeline for ﬁnding language
mentions in a paper, which starts by converting
the paper PDF to a machine-readable format. We
then search within the paper for any mention of a
language’s English name(s), its endonym, as well
as its ISO or Glottolog code. We then apply a
post-processing step to ensure the precision of this
pipeline as our simple text-based search is prone to
false positives for languages whose names match
common English words (e.g. She, Male, Label,
Even, The, Are), common placenames (e.g. Col-
orado, Nara, Sydney), parts of author names (e.g.
Su, Kim, Dan, Ali, Rama), or mathematical nota-
tion (e.g. Dji, Dii).
In addition, we enrich each publication by imput-
ing its research area. There were 16 research areas
identiﬁed, based on the ones represented at recent
major NLP conferences (speciﬁcally starting with
the 2019 version of EMNLP, and removing some
of the areas that were unique to that conference).
For each area, we identiﬁed 1-6 publication venues
from the ACL Anthology, where more venues were5497chosen when each venue had relatively few publica-
tions. Based on the abstracts of papers from each of
these venues, we trained a bag-of-words classiﬁer
using the linear support vector machine implemen-
tation in scikit-learn, and applied this classiﬁer to
the abstracts of the papers we wanted to classify.
Necessary data and code to reproduce these results
are released in the supplementary material.
Data Sources and Metrics for Utility The ma-
jority of NLP research relies on automatic eval-
uation metrics over datasets annotated with gold-
standard outputs. The advantage of this approach
is that it allows consistent comparisons between
systems and a seamless evaluation of progress on
a speciﬁc evaluation set. On the other hand, there
is no guarantee that even statistically signiﬁcant
improvement on an automatic metric translates to
improvements on user-perceived utility. Neverthe-
less, the reality is that virtually all published NLP
research reports automatic evaluation metrics, with
only a tiny fraction diverging from the norm by e.g.
using human evaluations.
Our analysis assumes that all named languages
have standard versions that are comprehensible and
acceptable to all members of the population iden-
tiﬁed as “speakers” in our sources. However, we
have the demographic information necessary for
more ﬁne-grained analysis in only a handful of
languages. While this assumption is certainly an
oversimpliﬁcation, we nevertheless believe it does
not detract from our paper’s arguments.
For a completely fair comparison across lan-
guages, one would ideally compute automatic met-
rics over the same or an equally representative eval-
uation set. For our language understanding case
study this requirement is satisﬁed, as the XNLI 15
language test sets are translations of the same eval-
uation set. Utility in this case, where the evaluation
metricmis accuracy, will be equal to the accuracy
for each language’s ltest set: utility(l;m) =m.
Natural language understanding results are
sourced from the XNLI leaderboard (Conneau
et al., 2018), which contains test datasets with
premise-hypothesis pairs in 15 languages.
For question answering (QA) we aggregate re-
sults from two established multilingual bench-
marks, namely TyDi-QA (Clark et al., 2020) and
MLQA (Lewis et al., 2009). Both benchmarks fo-
cus on extractive question answering, i.e. ﬁnding
the text span of a given document that answers, ifpossible, a given question. We also include SD-
QA (Faisal et al., 2021) for additional dialectal
breakdown for some of the TyDi-QA languages.
The benchmarks jointly cover 17 languages. We
keep the highest results for languages that are
shared between the two datasets (English and Ara-
bic). For this task we equate utility with test set
F-score, a measure that meaningfully combines
precision and recall of the retrieved answer span.
For machine translation, we collected more
than 500 published MT results from all WMT and
IWSLT evaluations, as well as more than 50 MT
studies from the last three years’ ACL, EMNLP,
and NAACL conferences (Barzilay and Kan, 2017;
Gurevych and Miyao, 2018; Palmer et al., 2017;
Riloff et al., 2018; Knight et al., 2016; Walker et al.,
2018; Korhonen et al., 2019; Inui et al., 2019; Web-
ber et al., 2020). In the machine translation ﬁeld
the most popular evaluation metric is BLEU (Pa-
pineni et al., 2002). In our MT case studies we
estimate utility based on a normalized version of
BLEU, such that for translation from stotwith
BLEU(s;t)over an established test set, we have
utility(s;t;BLEU). The normaliz-
ing factorZ= maxBLEU is equivalent to
the largest reported BLEU, which we equate to the
largest attainable utility at the snapshot of interest.
In all our MT case studies we use Z= 70 , which
is the BLEU score reported for translation between
Serbian and Croatian (Arcan et al., 2016).
For text-to-speech synthesis, we relied on re-
sults from the CMU Wilderness project (Black,
2019), which builds TTS voices with FestV ox (Anu-
manchipalli et al., 2011), and compared them to the
English system of (Ren et al., 2021). The quality
of the synthesized audio is evaluated using mel-
cepstral distortion (Kubichek, 1993, MCD) a distor-
tion measure that compares synthesized examples
with originals (lower is better). Each MCD of xfor
a languagelwas converted to a relative utility score
by applying the transformation, where
xandxcorrespond to the highest (worst)
and lowest (best) observed MCD scores across all
languages.
For syntactic analysis through dependency pars-
ing, we relied on results from two state-of-the-art
systems, UDPipe (Straka, 2018) and UDify (Kon-
dratyuk and Straka, 2019). The systems are typi-
cally evaluated using two measures, Unlabeled and
Labeled Attachment Score (UAS and LAS), which
measure the overlap between human-created and5498automatically-produced syntactic trees, excluding
punctuation. For our metrics we use LAS, which
considers the semantic relation (e.g. Subj) used to
label the attachment between two words.
The results on morphological inﬂection were
taken from the ﬁndings of the corresponding shared
tasks that have been taking place as part of the SIG-
MORPHON workshop for the past 5 years (Cot-
terell et al., 2016, 2017, 2018; McCarthy et al.,
2019; Vylomova et al., 2020). The systems are
evaluated using exact-match accuracy over a pre-
deﬁned test set in each language, simply comparing
the correct inﬂected form with the system’s output.
Population Demand We compile population
statistics from various sources. We rely on Eth-
nologue (Eberhard et al., 2018) for language pop-
ulation statistics. We take special care when com-
puting population statistics over macro-languages
(e.g. Arabic, Chinese) and languages commonly
spoken by L2 speakers (e.g. English) or across
multiple dialects (e.g. for Spanish or Portuguese),
aggregating populations across all variants.
Economic Indicators for Demand We aggre-
gate economic information on international trade,
as provided from the World Trade Organisation
(WTO) through the World Integrated Trade Solu-
tion.Since each language community can be ge-
ographically associated with a member nation of
WTO, we can then estimate economic indicators
for and between language communities.
In a monolingual setting, we rely on the most re-
cent GDP estimates, associated with each language
community. For example, the 1.7 million Nahuatl
speakers represent about 1.3% of Mexico’s popu-
lation, and thus the ﬁnal GDP associated with the
Nahuatl language will be 1.3% of Mexico’s GDP.
Modeling demand in a bilingual setting (across
two languages) is also feasible using economic
indicators. For instance, the amount of trade be-
tween two language communities could be used to
approximate the need for translation between the
two. Speciﬁcally, if we use the normalized import
volume per language community then we can es-
timate demand for an s!ttranslation system as
demand(s;t)/vsuch thatPv=
1.Take the Azerbaijani language as an example:
Azerbaijan’s imports mainly come from the Rus-
sian Federation (16.8%), Turkey (14.7%), China
(11.2%), the US (8.5%), Ukraine (5.5%), and Ger-
many (5.5%).Hence, we can assign a proportional
weight to model demand for translation from Rus-
sian, Turkish, Chinese, English, Ukrainian, and
German into Azerbaijani respectively. One could
equivalently use the normalized volume of exports
instead.
This is only straightforward to compute in cases
where a language is easy to map to a speciﬁc coun-
try. In cases of languages that are commonly used
across many countries e.g. German (which is the
main language in both Germany and Austria) or
macro-languages spoken in larger regions of the
world, we combine the weights accordingly in or-
der to jointly model the demand for the whole lan-
guage community.
Table 3 presents the top-15 translation pairs
based on demand estimated from economic indica-
tors, namely the import (and export) partner share
of the target (source) language. We note that this
ranking does not take underlying populations into
account, using only the percentage of demand for
each language community. Several entries in Ta-
ble 3 are language pairs that are rarely, if ever, stud-
ied in MT case studies, like Belarusian-Russian,
Mongolian-Mandarin Chinese, Albanian-Italian, or
Russian-Armenian.
B Methods
Predicting Utility on Unseen Languages/Pairs
One of the main disadvantages of using solely pub-
lished results for estimating quality and, hence,
utility, is the lack of evaluations on all languages
or language pairs. Furthermore, not all languages
or pairs are consistently evaluated on newly devel-
oped models. To counter this issue, we propose a
more comprehensive approach which attempts to
predict the expected quality/utility over languages
or language pairs unseen in the collected literature.
A naive approach is to make the approximation
that utility on any unseen language is 0. However
crude, this could be a valid assumption in many
cases: consider the example of a language under-
standing system trained on all languages that ap-
pear in Wikipedia. Such a system, without proper5499modiﬁcations, would not be able to handle input
in Yupik or Dhivehi (Maldivian), since these lan-
guages are not represented in Wikipedia and they
use different writing systems than any other lan-
guage. Note that, in such a case, for a language
understanding system evaluated over a classiﬁca-
tion task as in a language understanding setting,
the expected utility is not 0, but is rather the ex-
pected quality of random outputs (33% in the case
of three-way classiﬁcation).
Future work could make use of models explicitly
trained to predict the accuracy (or other metrics)
of NLP models on unseen languages or language
pairs, such as the ones proposed by (Lin et al.,
2019) or (Xia et al., 2020).
Estimating MT quality with pivoting In the
case of machine translation, pivoting is a viable
approach for producing translations between any
arbitrary language pair, as long as the intermediate
systems exist. Even if no published results exist
on translation from German to Chinese, it is unrea-
sonable to assign an expected utility of 0 to such a
MT system, since there exist high-quality German-
English and English-Chinese systems.
In the case of cascaded systems, though, esti-
mating utility requires a careful approach, due to
error propagation. Consider a system A with accu-
racy 80% and a system B with accuracy also 80%.
A cascaded system where the output of system A is
provided as input to system B will have an expected
accuracy 64%, not 80%.
An important point is that there is no reason for
pivoting through a single language. Consider the
example of Catalan to Chinese translation. A path
from Catalan to Spanish, to English, to Chinese
might have a yield a higher estimated utility from a
single-language pivoting path, since its components
are of higher quality.
We devise a method that allows us to generalize
this notion in order to ﬁnd the highest estimated
utility for every language pair. We construct a
weighted directed graph G=(V;E)with each node
v2Vrepresenting a language. The weighted
directed edge ebetween nodes sandtwill have
a weight equal to the highest reported normalized
BLEU score on translation from stot. If no results
have been published on this language pair, we set
the weight of that edge to 0.
With graphGin hand, as long as a path from
nodesstotexists, we can estimate the expected
normalized BLEU of s ttranslation as the maxi-mum cumulative (multiplicative) weight over any
path fromstot. If a path does not exist, then the
estimation is 0. This is possible in cases where a
language is reported as only source or only target
in the literature; for example, Greek (ell) only ap-
pears as a source in a single study (reporting Greek–
English translation results) which allows us to esti-
mate Greek–X utility by pivoting through English,
but we cannot produce estimates for X–Greek. Ta-
ble 4 presents translation pairs were our estimated
utility (normalized BLEU score) is higher than the
published results.
C Bibliometric Analysis
Analysis of Citations To each publication we as-
sociate its citation percentile relative to its year
and event. We analyze normalized citations ( C)
through Bayesian generalized additive mixed ef-
fects models implemented in R with brms and Stan
(Bürkner, 2017; Carpenter et al., 2017) We utilize
default weakly informative priors for all parame-
ters and we run four MCMC chains for each model
which in all cases achieved convergence. The distri-
bution ofCis described through a beta distribution,
of which its expected value is given by
E[C] =logit(f(L) ++L) (1)
wheref(L)is a smooth function (on the basis of
thin plate splines) depending on the number of
languages dealt with in the paper ( L), andand
are random intercepts and slopes according to
each area, respectively. In order to evaluate the
support in favor of f(L), we compared the leave-
one-out (LOO) performance of this model against
a counterpart without this term,
E[C] =logit(+L) (2)
The difference in expected log pointwise predictive
density (which serves to inform model selection,
(Vehtari et al., 2017)) between the two models is
-0.9 (SE=0.6), which implies there is no major per-
formance difference between the two.
Analysis of Number of Publications We deter-
mine the total estimated number of papers in which
each language lwas involved ( P). The resulting
distribution has a large concentration of zero val-
ues, so we opt to model this through a zero-inﬂated
negative binomial distribution. We focus on two
parameters: the expected value of the number of
publications ( E[P]) and the mixture probability ( ).5500In both cases, we ﬁt models considering three pos-
sibilities: (1) A smooth (thin plate spline) function
of the log-GDP, (2) a smooth (thin plate spline)
function of the log-number of speakers, and (3) a
ﬁxed parameter. This leads to evaluating 9 models
through a LOO criterion. The model that involves
(1) for both parameters displays the best overall
performance (see SI).
D Machine Translation Case Studies
We use this section to expand on the discussion of
MT case studies.
Translation involving English Since translation
involves two languages and language communi-
ties, there are two natural ways for a speaker to
receive utility from a MT system: either by being
thesource (with their language being translated
into another) or by having another language trans-
lated into theirs ( target ). We disentangle the two
by only using each one at a time for our utility
calculations.
Utilities based on demographics for both settings
are similar, with M= 0:25(from English) and
M= 0:27(to English). Since published results
only cover 101 languages, the linguistic diversity
scores are much lower, with Maround 0:005.
Translation among all languages We extend
our study on translation among all languages (still
maintaining the distinction between a language
used as source or target). We base our estimates
for utility on any reported results, as well as on
accuracy estimates based on a pivoting approach.
Brieﬂy outlined, our pivoting estimation approach
ﬁnds the best performing translation path for lan-
guage pairs without reported results, i.e. since no
studies report translation accuracy when translat-
ing from Greek to Chinese, we ﬁnd that among all
possible translation paths, translating from Greek
to English and from English to Chinese yields the
highest expected accuracy. We outline the process
in the Materials and Methods section.
Perhaps unexpectedly, the best (and often only)
pivot is English in almost all cases. As a result, the
ﬁnal utility for a language X is very much depen-
dent on the utility of the X-Eng (or Eng-X) systems.
This is reﬂected by our scores for averaged by de-
mographics and languages being very similar to
the ones when we only focused on English. Nev-
ertheless, the differences between scores for differ-
ent languages are stark: the demographic-averagedutility for populous, well-studied languages like
German (M= 0:356), Chinese (M= 0:232), or
French (M= 0:309) is almost double than under-
served ones like Bengali ( M= 0:148), isiXhosa
(M= 0:156), Amharic ( M= 148 ), or Burmese
(M= 0:092). Figure 5 visualizes the different
scores for translation from 24 languages under the
demographic focus ( = 1).5501ara!X aze!X ben!X
cat!X cmn!X deu!X
ell!X eng!X ﬁn!X
fra!X glg!X hau!X
ita!X kin!X kor!X
por!X rus!X spa!X
swa!X tam!X tur!X
uig!X vie!X zul!X55025503rank Lang.pop Number of Studies
(M) X–eng/eng–X
1 cmn 908.8 16/ 4
2 spa 358.8 5/6
3 hin 299.5 3/1
4 ben 232.8 2/0
5 por 207.7 3/3
6 ara 205.4 9/6
7 rus 145.6 9/6
8 jpn 128.0 7/4
9 swa 89.2 1/1
10 msa 80.3 2/0
11 kor 77.3 4/0
12 vie 76.0 4/6
13 mar 73.0 2/0
14 tam 72.0 2/0
15 tur 65.9 9/4
16 guj 48.3 1/1
17 fra 47.1 12/17
18 ind 43.4 2/0
19 ita 42.8 8/6
20 urd 35.0 2/0
21 mya 31.4 2/0
22 mal 30.7 0/0
23 deu 30.4 25/33
24 orm 28.0 1/0
25 uzb 27.9 0/0
26 ukr 27.3 3/1
27 pol 25.0 2/0
28 aze 19.5 5/2
29 sin 17.6 1/1
30 ron 16.8 13/11RankBased on
Imports Exports
1 rus–bel bel–rus
2 rus–kaz mon–cmn
3 rus–hye sqi–ita
4 rus–mon hye–rus
5 rus–cmn tgl–jpn
6 spa–som nep–hin
7 hin–nep aze–ita
8 ita–sqi srp–bos
9 lit–lav lav–lit
10 rus–aze msa–jpn
11 cmn–mya lit–rus
12 rus–ﬁn mya–cmn
13 rus–ukr est–ﬁn
14 cmn–tha bos–hrv
15 jpn–tgl kat–rus5504Language BLEU ScorePivotPair Estimated Published
slv–srp 37.09 25.45 eng–hrv
eng–nep 10.56 6.8 guj–hin
eng–hrv 60.80 42.15 srp
eng–hin 13.78 12.5 guj
hrv–eng 50.42 48.07 srp
ron–deu 29.36 18.4 eng
ron–fra 33.98 26.53 eng
ces–rus 17.56 16.2 eng
ces–deu 23.36 19.3 eng
ces–fra 27.04 18.1 eng
ita–deu 26.08 19.85 eng
rus–ces 18.19 14.4 eng
pol–ces 9.90 7.2 eng
nld–deu 25.0 21.06 eng
heb–fra 27.41 23.25 eng
srp–slv 52.09 35.39 hrv
deu–ron 27.25 16.27 eng
deu–ces 25.19 20.1 eng
deu–ita 28.42 18.56 eng
deu–nld 26.48 20.31 eng
deu–fra 44.27 37.3 eng
fra–ron 23.52 19.3 eng
fra–ces 21.73 13.7 eng
fra–heb 18.88 13.54 eng
spa–ces 17.83 15.2 por–eng
ara–fra 26.83 25.07 eng
slv–hrv 55.64 40.44 eng–srp5505