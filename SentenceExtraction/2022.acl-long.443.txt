
Carlos E. Jimenez Olga Russakovsky Karthik Narasimhan
Princeton University
{carlosej,olgarus,karthikn}@princeton.edu
Abstract
We introduce CARETS, a systematic test
suite to measure consistency and robustness
of modern VQA models through a series
of six ﬁne-grained capability tests. In con-
trast to existing VQA test sets, CARETS fea-
tures balanced question generation to create
pairs of instances to test models, with each
pair focusing on a speciﬁc capability such as
rephrasing, logical symmetry or image obfus-
cation. We evaluate six modern VQA systems
on CARETS and identify several actionable
weaknesses in model comprehension, espe-
cially with concepts such as negation, disjunc-
tion, or hypernym invariance. Interestingly,
even the most sophisticated models are sensi-
tive to aspects such as swapping the order of
terms in a conjunction or changing the number
of answer choices mentioned in the question.
We release CARETS to be used as an exten-
sible tool for evaluating multi-modal model
robustness.
1 Introduction
The task of visual question answering integrates
the domains of computer vision and NLP by prob-
ing models’ understanding of images through nat-
ural language queries. After the introduction of
the Visual Question Answering (VQA) bench-
mark (Antol et al., 2015), subsequent work iden-
tiﬁed the presence of several superﬁcial correla-
tions and other weaknesses latent in the VQA
question gathering process (Goyal et al., 2017;
Agrawal et al., 2018), which lead to potentially
optimistic evaluations when considering accu-
racy alone. More recently developed benchmarks
(Hudson and Manning, 2019; Goyal et al., 2017;
Agrawal et al., 2018) explicitly avoid these weak-
nesses by introducing question, answer, and im-
age balancing, or distributional shifts. While theseefforts provide more difﬁcult benchmarks, a thor-
ough evaluation of model capabilities requires a
deeper and more detailed approach.
To this end, we introduce CARETS – a Con-
sistency And Robustness Evaluative Test Suite
for visual question answering. Inspired by re-
cent work in NLP that generates ‘unit tests’ for
models (Ribeiro et al., 2020b), CARETS contains
systematically generated VQA tests that evalu-
ate six different capabilities that any VQA model
should handle – robustness to question rephras-
ings, ontological reasoning, symmetrical logic,
visual perturbations, question negation, and at-
tribute antonymy. Each test point in CARETS
consists of a pair of instances which are small but
strategic variations of each other either visually
or in the question’s text. This allows us to con-
duct ﬁne-grained capability evaluations beyond
just measuring high-level accuracy scores.
Across tests, we generate over 190,000 instance
pairs in total using a programmatic approach that
ﬁlls in templates (from nearly 200 templates in
total) using ground-truth scene graphs (Krishna
et al., 2017) from the GQA (Hudson and Man-
ning, 2019) validation split. We then evaluate
six modern VQA models on each test using sev-
eral metrics: overall accuracy, self-consistency
andcomprehensive accuracy . Self-consistency
measures models’ ability to maintain their answer
across question variations, while comprehensive
accuracy estimates their ability to answer all in-
stance variants correctly.
Our experiments reveal several interesting ﬁnd-
ings: (1) most modern VQA systems achieve only
middling self-consistency ( 60-80%) which is
further not always correlated with their accuracy,
(2) all models struggle to comprehend the concept
of negation (self-consistency of 18-28% and com-
prehensive accuracy <17%), and (3) even simple
perturbations like changing the order of choices
in the question text can induce a substantial drop6392
(10-15%) in performance. Moreover, even state-
of-the-art models like LXMERT (Tan and Bansal,
2020) are highly sensitive to the type of ques-
tions (binary vs multi-choice) and the number of
choices provided. These results reveal several
shortcomings in modern VQA systems and hint
at potential areas for future improvement. Going
beyond our current discoveries, CARETS is an
extensible framework and may be easily extended
by adding new capability tests for ﬁne-grained
evaluation of future models.
2 Related Work
VQA evaluation. The textual, visual, and an-
swer biases discovered in the VQA dataset (Antol
et al., 2015) spurred on recent work seeking to im-
prove model evaluation for the task by eliminating
these biases (Goyal et al., 2017), introducing dis-
tributional shifts (Agrawal et al., 2018), requiring
model explanations (Li et al., 2018), thoroughly
analyzing biases in datasets and models (Man-
junatha et al., 2019), or evaluating on different
recognition subtasks (Kaﬂe and Kanan, 2017).
While debiased and challenging benchmarks are
important, their focus on accuracy as the sole eval-
uation metric leaves much to be desired (Ribeiro
et al., 2020a; Kervadec et al., 2020). In contrast,
our testbed provides question or image pairs that
compares models’ predictions between questions;
measuring their accuracy, consistency, and robust-
ness to a variety of text and image perturbations.Synthetic Dataset Generation for VQA. One
way in which we can generate diverse and bal-
anced datasets is to generate them synthetically,
as is done by (Johnson et al., 2015; Zhang et al.,
2016; Hudson and Manning, 2019). Synthetically
generating questions, images, or both, allows ﬁne
control over the distribution of questions, answers,
and images. Additionally for our case, synthetic
generation allows us to control not just the partic-
ular semantics of one question, but also how one
question relates to another question in a precisely
deﬁned way (e.g. one question is a negation of an-
other) while also remaining relevant and grounded
in the image. As both the CLEVR (Johnson et al.,
2015) and GQA (Hudson and Manning, 2019)
datasets use image scene graphs for question and
label generation, they contain questions combin-
ing a variety of required capabilities, including
compositionality. We feature real-world images
with synthetically generated questions as well, but
in contrast to GQA, our evaluation has instance
pairs to systematically test a focused set of capa-
bilities, showing that models may still struggle
with simpler, non-compositional questions.
Consistency as Model Comprehension. Some
recent work has sought to evaluate models us-
ing consistency and other metrics (Hudson and
Manning, 2019; Shah et al., 2019; Ribeiro et al.,
2020a; Selvaraju et al., 2020; Bitton et al., 2021;
Mouselinos et al., 2022). These evaluations of-6393ten evaluate consistency through question entail-
ment and implication, or simply contrasting ex-
amples in the case of (Bitton et al., 2021). The
concurrent work (Mouselinos et al., 2022) takes a
unique approach, performing discrete visual per-
turbations while preserving the semantic integrity
of a question; their work complements our own
as it focuses exclusively on visual perturbations
in a synthetic image setting. While we consider
these previous methods important for evaluating
model comprehension, they often combine ques-
tion types and capabilities, changing the kind of
expected answer, or evaluating consistency on a
tree or set of entailed questions. Though ideally
models would be consistent and robust for these
more complex types of tests, our approach reveals
that models can fail even on simple implications.
3 Fine-grained capability tests
Our goal is to provide a testbed for ﬁne-grained
evaluation of VQA models’ capabilities. To do
so, we generate multiple tests, each correspond-
ing to a speciﬁc model capability. In contrast to
standard VQA benchmarks (Antol et al., 2015;
Goyal et al., 2017; Hudson and Manning, 2019),
our test sets consist of a pair oforiginal andper-
turbed instancesh(I;q;a);(I;q;a)i, each
with an image, a question and an answer. The two
instances within a pair differ from each other in
a minimal yet carefully constructed way to hone
in on a particular capability, similar to BLiMP
(Warstadt et al., 2020). A model is then evaluated
on its overall accuracy ,self-consistency (ability
to produce consistent, even if wrong, answers to
both instances within a pair), and comprehensive
accuracy (ability to answer consistently and cor-
rectly for an instance pair).
Overall, we create six tests corresponding to
key capabilities. We provide a high-level descrip-
tion of each test here and describe generation de-
tails in Section 4. First, we create four invariance
teststhat use variations of the question phrasing
and expect the model to produce the same answer
to both questions within an instance pair:
1.Rephrasing invariance ( -)
measures the model’s understanding of mi-
nor, meaning-preserving textual changes,
e.g.: “What color is the bottle on the shelf,
white or blue?” and “ Does the color of the
bottle on the shelf seem more white or blue?”2.Ontological invariance ( -) measures understanding of ontology,
e.g. changing a hyponym in: “Do you see a
green jacket?” to a hypernym “Do you see
any green clothes ?”
3.Order invariance ( -) measures
understanding of logically equivalent ques-
tions containing different argument orders,
e.g.: “Is the black vehicle a van or a truck?”
and “Is the black vehicle a truck or avan?”
4.Visual obfuscation invariance ( -) measures the model’s answering ability
when parts of the image not directly relevant
to the visual question are removed. Speciﬁ-
cally, we explore blurring, masking and crop-
ping techniques to modify the image.
We also create directional expectation tests to
measure model behavior on instance pairs where
the answer is expected to change :
5.Attribute antonym directional expectation
( -) measures the model’s un-
derstanding of antonyms, e.g., “Do you think
that the wood table is short?” and “Do you
think that the wood table is long?”
6.Negation directional expectation
( -) measures a model’s
grasp of negation, e.g., “ Are there any
apples in this picture?” and “Are there no
apples in this picture?
4 Dataset generation
Each of the six test datasets start with the
generation of ‘original’, unperturbed instances
(I;q;a)(Section 4.1). Then, for each such in-
stance, we generate a variation (I;q;a)by ei-
ther perturbing the original question qor image
I(Section 4.2). Further, each test set is com-
posed of a diverse set of questions. These may
broadly be grouped into veriﬁcation (orbinary )
questions, with expected answers being either yes
orno, and multi-choice questions, with expected
answers derived from a list of objects or attributes
provided in the question.
4.1 Original instance generation
Questions for each test are generated from ques-
tion templates (examples for each are provided in
Appendix A.1) grouped into the following types.
Q1: Object veriﬁcation (54 templates): e.g., “Is
there a <obj> in the image?”6394Q2: Conjunctive veriﬁcation (18 templates):
e.g., “Is there a <obj1> and a <obj2> in
the image?”
Q3: Disjunctive veriﬁcation (18 templates):
e.g., “Is there a <obj1> or a<obj2> in
the image?”
Q4: Attribute veriﬁcation (25 templates): e.g.,
“Is the <obj> in the image <attr> ?”
Q5: Object multi-choice (25 templates): e.g.,
“Is the <obj-class> ,<choices> ?”
Q6: Attribute multi-choice (39 templates): e.g.,
“What sort of <attr-class> is the
<obj> ,<choices> ?”
Q7: Action multi-choice (28 templates): e.g.,
“What is the <action-class> that the
<obj> doing, <choices> ?”
Words in typewriter font represent template
arguments. Generally, each <obj> argument can
be ﬁlled by a singular object (“cup”) or an at-
tribute+object (“red cup”) while <attr> argu-
ments are ﬁlled with singular attributes (“shiny”).
For object veriﬁcation (Q1), attribute veriﬁca-
tion (Q4), attribute multi-choice (Q6), and action
multi-choice (Q7) questions, some templates let
<obj> arguments be ﬁlled with an object related
to another object (e.g. “red cup on the table”);
this type is excluded from conjunctive veriﬁcation
(Q2) and disjunctive veriﬁcation (Q3) questions
to prevent the generation of verbose questions.
For the multi-choice questions (Q5, Q6, Q7),
<choices> are replaced with a list of 2 or 3 sin-
gular objects, attributes, or actions respectively
(e.g. “cow or horse” or “wood, metal, or plas-
tic”). The <obj-class> argument is ﬁlled
with a hypernym of all object choices and always
appears with either an attribute or a related ob-
ject (“black animal”, “animal eating grass”). The
<attr-class> argument is ﬁlled with the at-
tribute category of all attribute choices (e.g. “ma-
terial”). Finally, the <action-class> argu-
ment is ﬁlled with the action category of all action
choices (e.g. “sport”).
Question argument generation. The question
arguments are generated using images from the
validation split of the GQA dataset (Hudson and
Manning, 2019) which contains 10,696 images
manually annotated with 1,536 different objects
and 603 different object attributes.
To generate questions, we sample objects and
attributes directly from an image’s scene graph an-
notation to populate a question type’s arguments.Forbinary question types this results in questions
with solely afﬁrmative answers. To produce an
answer balanced dataset, we run a second stage
of question argument generation for binary ques-
tions to generate plausible negative questions with
false objects or attributes. We sample false objects
from a distribution conditioned on an image’s ob-
jects, and optionally sample object attributes from
a distribution conditioned on the chosen object.
For<choices> arguments, false choices are
again generated from a distribution conditioned
on the object’s hypernym for Q5 questions, the
attribute category for Q6 questions, or the action
category for the Q7 questions. We additionally en-
sure that the generated choices are mutually exclu-
sive (e.g. “tan or beige” would be an invalid gen-
eration). To get more diverse multi-choice ques-
tions, we ﬁrst generate a large pool of question
candidates, and then select only a small number
of questions sampled from this pool with sample
probabilities inversely proportional to the count of
the questions’ hypernym, attribute class, or action
class, and the count of the generated answer.
Question argument reﬁnement. To improve
the reliability of generated questions, we apply a
variety of checks and constraints. For example,
when sampling false objects from the conditional
distribution, we ﬁlter out all objects (and their
hypernyms) present in the scene graph in order
to guarantee that the sampled object is truly not
present. We also ﬁlter out question arguments that
are not included in the image scene graph but are
sub-parts of objects that are annotated (e.g., “tire”
when a “car” is annotated). Finally, we enforce
various logical constraints on question arguments
to prevent trivial or malformed questions. For ex-
ample, for conjunctive and disjunctive questions
(Q2, Q3), we apply a hypernym exclusion con-
straint to prevent questions like “Is there a black
cat and a black animal in the image?”.
4.2 Perturbed pair generation
We now describe our procedure for creating per-
turbed instances (I;q;a)for the six tests. In all
tests except visual obfuscation, the image remains
unchanged, i.e. I=I.6395(a) Rephrasing invariance. Since each original
questionqwas generated using a text template,
we simply use a different template of the same
type to generate a valid rephrasing q. The image
and answer remain the same, i.e. I=I;a=
aand the model is expected to be invariant to
this rephrasing. We apply this to Q1, Q2, Q3, Q5,
Q6 and Q7.
(b) Ontological invariance. Here, we use ob-
ject veriﬁcation questions (Q1) only and perform
two types of transformations. For positive ques-
tions (i.e.a=yes), we ﬁlter question arguments
to only include objects that are valid hyponyms
(using WordNet again) and use those to generate
a perturbed question qby changing the hyponym
to a hypernym. For example, q= “Do you see a
jogging woman ?” witha=yesis paired with q
= “Do you see a jogging person ?” containing a
hypernym. Similarly, for negative questions ( a=
no), we ﬁlter question arguments to only include
valid hypernyms and generate a q: thus for ex-
ample,q= “Do you see a jogging person ” with
a=nois paired with q= “Do you see a jogging
woman ?” containing a hyponym, a=noalso.
(c) Order invariance. Order invariance tests
apply to conjunctive veriﬁcation, disjunctive veri-
ﬁcation, and all multi-choice question types; mod-
els are expected to be invariant to the logical order
of arguments. We perturb conjunctive veriﬁcation
and disjunctive veriﬁcation questions by swap-
ping the questions’ ﬁrst and second arguments
(<obj1>, <obj2> ). For multi-choice ques-
tion types, we perturb instances by generating
the<choices> argument with different orders.
The answer remains the same in both cases by
construction.
(d) Visual obfuscation invariance. For this test,
we letq=qanda=abut generate a per-
turbed image Iby obscuring parts of Ithat are
irrelevant to the question at hand using bounding
box annotations from Visual Genome (Krishna
et al., 2017). For all true objects in a question,
we consider the bounding boxes around these ob-
ject(s) to be the foreground and all other pixels
in the image to be the background . For negative
veriﬁcation questions asking about object(s) not
present in the image, we select one (or two) ran-
dom object bounding box(es) as the foreground
and consider everything else to be the background,
since focusing on any image region should notaffect the model’s answer.We then apply ﬁve
types of perturbations to obscure the background:
(i-iii) Gaussian blurring using the soft masking
method of (Yang et al., 2021) with light ( = 3),
medium (= 6), or heavy ( = 9) blurring, (iv)
Masking with the channel-wise averaged pixel
value from the GQA (Hudson and Manning, 2019)
training dataset, entirely obscuring the context,
and (v) Cropping to the smallest rectangular re-
gion including the foreground. Example images
are shown in Appendix A.2.
(e) Negation directional expectation. For the
negation directional test, we use object veriﬁca-
tion, conjunctive veriﬁcation, and disjunctive ver-
iﬁcation questions. Each question qis perturbed
by substituting the original’s text template with a
paired negated text template to create q. Since
each perturbed question represents the negation
of the original, the expected answers a6=a.
(f) Attribute antonym directional expectation.
We perturb the generated attribute veriﬁcation
questions by changing the <attr> question ar-
gument to its antonym using WordNet. All at-
tribute antonym relations are manually curated to
remove unintuitive examples; questions with argu-
ments without a valid antonym are discarded. The
original and perturbed questions of a pair have
opposite answers a6=a.
5 Experimental Setup
5.1 Human baseline
To assess the quality, difﬁculty and validity of
the generated tests, we sample 100 question pairs
(200 questions) from each question type for the 6
tests and procure 5 annotations per question from
workers on Amazon Mechanical Turk. Workers
are vetted for a 97% approval rating and mini-
mum of 500 completed HITs. Workers take 2
minutes per task on average and are compensated
$0:50per task and thus$15per hour. Each
HIT include 24 questions total, including 4 veriﬁ-
cation questions, and typically include a variety
of question types from each of our tests.6396Human agreement. In addition to “yes” and
“no” for binary questions and the appropriate
choices for multi-choice questions, all questions
offer an ambiguous option. Human answers are
the majority vote among the 5 workers; questions
failing to reach majority or with ambiguous as
the majority are always counted against accuracy.
This process is inspired by the human evaluations
of implied question pairs in Ribeiro et al. (2020a).
We report both human and model performance in
Section 6.
5.2 Evaluated models
We evaluate six recent models on our tests, and
compare them to human accuracy. Models are
trained on the GQA (Hudson and Manning, 2019)
balanced training split (using hyperparameters
suggested from the original papers). All models,
except LXMERT, are trained and ﬁnetuned using
the MMF (Singh et al., 2020) library and region of
interest (RoI) features from Faster R-CNN (Ren
et al., 2015) with a ResNeXt-152 (Xie et al., 2017)
backbone pre-trained on the Visual Genome (Kr-
ishna et al., 2017) dataset for object-based models.
More details are provided in Appendix A.3.
Model initialization and pre-training. Of the
six models evaluated, a deﬁning characteristic of
each model relates to their initializations, image-
encoding choice, and the use of multi-modal pre-
training. Our most basic model (CNN+LSTM)
is randomly initialized and uses no pre-trained
components; however, GloVe (Pennington et al.,
2014) word embeddings are used for representing
tokens. Another class of models use pre-trained
image encoders to extract object features from
images. Of our models, BAN (Kim et al., 2018) is
randomly initialized prior to training but ingests
pre-trained Faster R-CNN features which should
provide the model with enhanced visual capabil-
ities over the CNN+LSTM model. MultiModal
BiTransformer (MMBT) (Kiela et al., 2019), uses
similar pre-trained image features as BAN but is
further initialized with pre-trained BERT (Devlin
et al., 2019) weights prior to training on GQA.
The last class of models are multi-modal pre-
trained models; those that use pre-trained image
features and are pre-trained on multi-modal tasks,
such as image-based masked language modeling.
Models in this class include LXMERT (Tan andBansal, 2020), ViLBERT (Lu et al., 2019), and
VisualBERT (Li et al., 2019). Similar to MMBT,
ViLBERT and VisualBERT are also initialized
with pre-trained weights from BERT.
5.3 Metrics
Accuracy ().On our test datasets with K
paired instances, we deﬁne accuracy as:
1
2KX1[^a=a] + 1[^a=a]
where the model answers ^a;^aon the original
and perturbed questions respectively are com-
pared to the ground truth answers a;a.
Self-consistency ( ).We measure self-
consistency of the model predictions across the
original and perturbed questions as
1
KX1[^a= ^a]on invariance tests
1[^a6= ^a]on directional exp. tests
Note that these metrics only measure the internal
consistency of the model and do not include the
ground truth answers a;a.
Comprehensive accuracy (-).We deﬁne
comprehensive accuracy as:
1
KX1[^a=a^^a=a]
measuring whether model predictions are both
accurate and self-consistent across perturbations.
6 Results
(R1) Modern VQA models are not robust to in-
variance and directionality tests. Fig 2 details
the performance of various models under our suite
of tests. Each bar in the ﬁgure shows both
and- for the model, with the arrow repre-
senting the gap between the two. We ﬁrst observe
that all models achieve signiﬁcantly lower perfor-
mance (at least a 8% drop) compared to humans
(grey). Even simple tests such as -(Figure 2(a)) prove to be quite challenging,
with models managing <68% compared to
humans’ 86%. On tests like -(Fig-
ure 2(e)), models only get about 50% accuracy,
substantially worse than human scores of 78%.
Moreover,- is considerably lower than across the board, with as much as a 35%
gap on -and 14.5% on -tests, even for a state-of-the-art model like
LXMERT.Even though this gap is smaller on6397
other tests like -or -,
the performance drop is still at least 6-7% in most
cases. This means that models are not invariant
to textual rephrasings of questions and do not
have a strong grasp of concepts like attributes and
negation, despite negation of attributes appearing
in the GQA training dataset.
(R2) VQA systems are not self-consistent
in their predictions. Table 1 shows the self-
consistency scores for all models under our dif-
ferent tests. While humans achieve > 88%
in all the tests, VQA models are much worse – at
least 6% lower in all cases, with the best
performing model (LXMERT) still 26% lower
than human performance on average across tests
and models. Scores are especially low on the
directional tests (antonym and negation), whichOrig. Pert. Hyper- Hypo-
BAN 79 76 75 79
CNN+LSTM 49 56 75 72
LXMERT 89 82 80 87
MMBT 82 66 61 80
ViLBERT 85 72 69 84
Visual BERT 81 69 68 80
Human 96 96 91 96
means that models are confused in their decisions
simply with the addition of negation words – this
hints at issues of overﬁtting to spurious feature
without understanding the presence or absence of
speciﬁc concepts, corroborating the ﬁndings of
(Bitton et al., 2021). Interestingly, the best per-
forming model (LXMERT) is not always the most
consistent. Furthermore, there is no single model
that is the most self-consistent, with LXMERT,
ViLBERT and VisualBERT each returning the
highest consistency scores on different tests.
(R3) Models are more robust to hyponym than
hypernym variations. Breaking out the results
on the ontological invariance test (Figure 2 (c))
in the last two columns of Table 2, we see that
self-consistency is higher on the hyponym pertur-
bations (on negative answer questions) than on
hypernym perturbations (positive questions); this6398Conjunction Disjunction Y N O Y N O
BAN 52 53 47 0 527128 0
CNN+LSTM 39 53 47 0 356535 0
LXMERT 78 49 51 0 565932 9
MMBT 56 50 50 0 556334 2
ViLBERT 58 54 46 0 567921 0
VisualBERT 59 49 51 0 577030 0 -Human
Binary 74.5 89.3 69.8 82.8
Multi-choice 59.0 69.7 47.3 83.6
2-choice 62.4 70.8 49.9 83.4
3-choice 55.4 68.4 44.5 83.9
effect is particularly noticeable for MMBT and
ViLBERT with a 19% and 15% difference, respec-
tively. Thus, when an object is not detected in an
image its hyponym elicits a negative response as
expected; however when an object (like “steak”)
is detected, the hypernym question (“Is there any
meat in the image?”) may trip the model to gen-
erate a negative response. This points to the need
for more structured, hierarchical grounding of
concepts in these models.
(R4) Models perform better on conjunctive
rather than disjunctive tests. From Table 3,
we note that models generally have higher accu-
racy on conjunctive rather than disjunctive tests,
with the largest discrepancy for LXMERT at 81%
accuracy on conjunctive tests vs only 62% on dis-
junctive. Many models seem to exhibit a strong
positive bias for disjunctive questions, suggesting
they may just be short-cutting to answering ‘yes’
for disjunctive questions. LXMERT also seems
to frequently confuse disjunctive questions for an
open-ended or multi-choice question.
(R5) Models are sensitive to answer types and
the number of choices in a question. Table 4provides a breakdown of LXMERT’s scores for bi-
nary and multi-choice questions. It is evident that
multi-choice questions are harder for the model,
with self-consistency dropping by 16% between
binary and multi-choice questions, and-
dropping by 33%. This is surprising since the
multi-choice questions only include two or three
choices and hence are quite similar to the binary
(yes/no) questions. This may indicate a bias in
the models towards binary questions with simple
answers. Furthermore, Table 4 also shows that
models consistently perform worse on 3-choice
questions than 2-choice ones, with even the top-
performing LXMERT having a 7% drop from
62% to 55%. This hints that there may be some
effect of randomness in the way these models
pick their answers. In contrast and as expected,
humans are robust to the number of choices.
(R6) Visual perturbations are easier for models
to deal with. From Figure 2 and Table 1, we
notice that the models are slightly more robust
to visual perturbations on average compared to
the lexical ones. All models only have a drop of
4-8% from to-, while self-consistency
of all models is also 78% or higher. Appendix
A.2 provides a more detailed breakdown of all the
different visual perturbation tests we performed.
(R7) Direct data augmentation improves
CARETS evaluation. We show the feasibility
of high performance on CARETS through data
augmentation. We add 95,000 questions gener-
ated from CARETS question templates, and using
a similar distribution of question types, to the
original training split of GQA and re-train the
LXMERT model. Table 5 shows that this dra-6399matically improves the model on all three met-
rics (, self-consistency and-), with the
LXMERT(Augmented) model achieving near hu-
man performance on tests like -and -. Since CARETS is designed to
be an evaluation suite, these numbers show that
CARETS questions should generally be within the
capabilities of existing SOTA models, provided
that they are able to generalize appropriately.
7 Conclusion & Future Work
In this work, we have developed CARETS – a
new test suite for capability-focused robustness
testing and comprehension evaluation of visual
question answering (VQA) models. CARETS
consists of six different tests that use instance
pairs to evaluate models on their understanding of
various linguistic and visual concepts. Using this
test suite, we evaluated six modern VQA systems
to reveal several inconsistencies in state-of-the-
art models and provide a ﬁne-grained view of
their comprehension of different visuo-linguistic
concepts. Quite surprisingly, we ﬁnd that even
state-of-the-art models struggle with concepts like
negation, disjunction, order invariance and multi-
choice questions. CARETS can also support the
addition of more tests in the future and we view
it as a platform for continually stress testing and
improving VQA models.
CARETS emulates previous work in using text
templates to generate questions and their textual
perturbations (Hudson and Manning, 2019; John-
son et al., 2017; Ribeiro et al., 2020b). The use of
templates to generate perturbations is motivated
by the desire to maintain the grounded integrity
of generations, ensuring that they remain relevant
and that the generated label is true in the context
of the subject image. While we have sought to
generate a diverse language set through using a
large number of templates (nearly 200 in total),
there are some limitations to this approach. An
improvement to our approach may be able to gen-
erate more sophisticated questions and perturba-
tions through conditional text generation (Schick
and Schütze, 2021; Madaan et al., 2021; Wu et al.,
2021) while also preserving our other motivations,
such as atomicity and grounded relevancy.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant No.
2107048. Any opinions, ﬁndings, and conclu-sions or recommendations expressed in this ma-
terial are those of the author(s) and do not nec-
essarily reﬂect the views of the National Science
Foundation. We would also like to thank Tianyu
Gao, Austin W. Hanjie and Alexander Wettig for
their valuable feedback and advice.
References64006401
A Appendix
A.1 Test Dataset Examples
We provide examples of templates for each ques-
tion type here. In Section 4.1, we simpliﬁed
the template arguments for readability. Here, we
show template examples with their arguments as
they are actually represented for generation.
Q1: Object veriﬁcation (54 templates)
Q2: Conjunctive veriﬁcation (18 templates):
Q3: Disjunctive veriﬁcation (18 templates):Q4: Attribute veriﬁcation (25 templates):
Q5: Object multi-choice (25 templates):
Q6: Attribute multi-choice (39 templates):
Q7: Action multi-choice (28 templates):
A.2 Visual Obfuscation Invariance Details
Table 3 shows examples of context blurring, mask-
ing, and cropping. Five perturbations are done in
total, blurring (with 2f3;6;9g), masking con-
text by replacing pixel values with the channel-
wise average computed from the GQA training
data, and cropping around the tightest bounding
box containing the question’s objects.
A.3 Training Details
We provide greater detail on training environ-
ments and hyperparameter choices.
All models are trained using the GQA (Hud-
son and Manning, 2019) balanced training set and
validated on the balanced validation set (with min-
imal parameter tuning, aiming to stay faithful to6402Test Dataset BinaryMulti-
choiceTotal - 10,000 9,412 19,412 - 5,000 9,412 14,412 - 13,952 - 13,952 - 18,000 8,272 26,272 - 10,000 - 10,000 - 5,000 - 5,000
O M C G3 G6 G9
BAN 48.83 49.38 47.70 49.10 49.29 49.42
CNN+LSTM 40.28 39.29 40.14 39.45 39.18 39.35
LXMERT 69.64 66.33 61.82 66.59 66.89 67.06
MMBT 50.37 50.00 49.17 50.44 50.27 50.18
ViLBERT 51.60 51.12 49.99 51.69 51.36 51.22
VisualBERT 53.81 53.25 52.53 53.67 53.39 53.44
the original implementation). All non-LXMERT
models are trained and ﬁnetuned withing the
MMF (Singh et al., 2020) framework, trained us-
ing binary cross-entropy loss for a set maximum
number of epochs, taking the epoch checkpoint
that best performs on the validation set, and using
features from Faster R-CNN (Ren et al., 2015)
with a ResNext-152 (Xie et al., 2017) backbone
pre-trained on the Visual Genome (Krishna et al.,
2017) dataset for object-based models.
BAN. The Bilinear Attention Network (Kim
et al., 2018) (BAN) uses pre-trained Faster R-
CNN features (Ren et al., 2015) and GloVe (Pen-
nington et al., 2014) embeddings with an attention
model and early bilinear fusion mechanism. We
train “four glimpse” (with 4 attention heads) BAN
(Kim et al., 2018) (BAN) for 13 epochs using the
Adamax (Kingma and Ba, 2015) optimizer with
an initial learning rate of 1eand batch size of
256, decaying the learning rate at epochs 11 and
13. As in the original hyperparameter conﬁgura-
tion, we perform gradient clipping at 0.25.
LXMERT. LXMERT (Tan and Bansal, 2020) is
a transformer-based architecture (Vaswani et al.,
2017) with pre-trained Faster R-CNN features.
It undergoes an extended pre-training procedure
using 5 different pre-training tasks, including im-
age question answering. We ﬁrst pre-train a ver-
sion of LXMERT from scratch with all GQA val-idation instances removed from the pre-training
data to prevent direct leakage. We use all the de-
fault hyperparameters used in the author’s GitHub
repository.We then ﬁnetune LXMERT base on
the GQA training dataset for 4 epochs with the
same hyperparameter conﬁguration as the origi-
nal implementation, using a batch size of 32, and
initial learning rate of 1e. LXMERT base is
pre-trained using the MS COCO (Lin et al., 2014)
and Visual Genome datasets. LXMERT also uses
a Faster R-CNN (Ren et al., 2015) with a ResNet-
101 (He et al., 2016) backbone.
VisualBERT. VisualBERT (Li et al., 2019) is
similar in architecture and pre-training method to
BERT (Devlin et al., 2019). It performs an early
fusion of text and image features immediately
before several transformer layers. It uses Faster
R-CNN features, is initialized using weights from
BERT, and pre-trained on 2 different tasks using
the MS COCO dataset. We ﬁnetune an MS COCO
pre-trained version of VisualBERT using the same
hyper parameters and training scheme of the origi-
nal implementation for the VQA task. We use the
Adam W optimizer with an initial learning rate of
2eand a batch size of 64 for a maximum of 20
epochs.6403CNN+LSTM. This model uses a 6 layer CNN
module and a bidirectional LSTM module be-
fore concatenating the output of each module
and passing the combined output to a FC classi-
ﬁer. The LSTM module uses GloVe word embed-
dings (Pennington et al., 2014). Model weights
are randomly initialized. We train the model for
25 epochs using the Adam W optimizer with an
initial learning rate of 1eand batch size of 256.
This model uses a 6 layer CNN module and a bidi-
rectional LSTM module with a hidden size of 128,
and concatenates the output of each module be-
fore passing the combined output to 2 layer MLP
classiﬁer with a ReLU activation. The LSTM
module uses GloVe word embeddings (Penning-
ton et al., 2014) to represent questions.
MMBT. The MultiModal BiTransformer
(MMBT) (Kiela et al., 2019) is an early fusion
model, which uses Faster R-CNN features
projected to a common space and concatenated
with contextual BERT embeddings before
being passed to transformer layers. MMBT
uses pre-trained Faster R-CNN features and is
initialized with BERT pre-trained weights. We
ﬁnetune MMBT with the Adam W (Kingma and
Ba, 2015) optimizer with an initial learning rate
of5ewith a batch size of 64, for a maximum
of 15 epochs.
ViLBERT. ViLBERT (Lu et al., 2019) uses two
parallel transformer “streams” for vision and lan-
guage separately. These streams interact using
multi-modal co-attentional transformer blocks. It
uses Faster R-CNN features and is initialized
using some weights from BERT. ViLBERT is
pre-trained using 2 different tasks, using the MS
COCO dataset. We ﬁnetune the MS COCO pre-
trained version of ViLBERT in a similar manner
to the ﬁnetuning scheme used for the VQA task
of the original implementation. We use the Adam
W (Kingma and Ba, 2015) optimizer with an ini-
tial learning rate of 4eand batch size 64, for a
maximum of 20 epochs.
A.4 Test Results
We provide fuller test results for each test dataset,
including accuracy on the original instances and
perturbed instances in tables 8, 9, 10, 11, and
12. These results supplement the primary results
reported in Section 6.
A.5 Model comparison results
We provide additional model comparison results
for each test dataset in Tables 13, 14, 15, 16, 17.64046405