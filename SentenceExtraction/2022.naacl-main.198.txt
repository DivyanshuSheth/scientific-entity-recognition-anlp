
Orith Toledo-Ronen, Matan Orbach, Yoav Katz, Noam Slonim
IBM Research
{oritht, matano, katz, noams}@il.ibm.com
Abstract
Targeted Sentiment Analysis (TSA) is a central
task for generating insights from consumer re-
views. Such content is extremely diverse, with
sites like Amazon or Yelp containing reviews
on products and businesses from many differ-
ent domains. A real-world TSA system should
gracefully handle that diversity. This can be
achieved by a multi-domain model – one that is
robust to the domain of the analyzed texts, and
performs well on various domains. To address
this scenario, we present a multi-domain TSA
system based on augmenting a given training
set with diverse weak labels from assorted do-
mains. These are obtained through self-training
on the Yreviews corpus. Extensive exper-
iments with our approach on three evaluation
datasets across different domains demonstrate
the effectiveness of our solution. We further
analyze how restrictions imposed on the avail-
able labeled data affect the performance, and
compare the proposed method to the costly al-
ternative of manually gathering diverse TSA
labeled data. Our results and analysis show
that our approach is a promising step towards a
practical domain-robust TSA system.
1 Introduction
Customer reviews of products and businesses pro-
vide insights for both consumers and companies.
They help companies understand customer satisfac-
tion or guide marketing campaigns, and aid con-
sumers in their decision-making. Sentiment anal-
ysis plays a central role in the analysis of such
material, by aiming to understand the sentiment
expressed in a review document or in a single re-
view sentence (Liu, 2012). Beyond these high-level
trends, identifying the sentiment towards a specific
product feature or an entity is important. Such a
fine-grained analysis includes the key task of Tar-
geted Sentiment Analysis (TSA), aimed at detect-
ing sentiment-bearing terms in texts and classifying
the sentiment towards them. For example, in thesentence "The room was noisy, but the food was
tasty," the targets are room andfood with negative
and positive sentiments, respectively. Our focus in
this work is on TSA of user reviews in English.
A real-world TSA system has to successfully
process diverse data. From toothbrushes to phones,
airline companies to local retailers, the online con-
tent today covers a broad range of reviews in many
domains. Ideally, a system for such a multi-domain
scenario should be able to cope with inputs from
any domain, those that were seen during training,
and, perhaps more importantly, those that were not.
To the best of our knowledge, this work is the
first to pursue TSA in a multi-domain setup, in-
tending to support input from multiple unknown
domains. Many previous works have used the in-
domain setup of training and testing on data from
the same domain (e.g. Li et al. (2019b)). Newer
works focus on the cross-domain setup, yet most
have explored a pairwise evaluation of training
on one source domain and evaluating on a single
known target domain (e.g. Rietzler et al. (2020);
Gong et al. (2020)).
Broadly, multi-domain learning (Joshi et al.,
2012) includes training and evaluation using data
from multiple domains (e.g. Dredze and Cram-
mer (2008); Qin et al. (2020); Dai et al. (2021b)).
Sometimes, it is assumed that the input texts are
accompanied by a domain label (e.g. Joshi et al.
(2012)). Here, we do not assume a domain label
is given – this has the advantage of allowing eas-
ier practical use of our model, without having to
specify the domain as part of the input. In other
cases, evaluation is limited to domains represented
in training, or otherwise performed in a zero-shot
setup only on unseen domains (Wang et al., 2020).
Our system handles both cases simultaneously, pro-
cessing data from domains well-represented in the
training data as well as from unseen domains.
For practical reasons, implementing a multi-
domain system with a single model that can handle2751all domains is desirable. This can save valuable
resources such as memory or GPUs, which are in
high demand by contemporary language models
(LMs). For example, it is impractical to expect
that an online service providing TSA analysis will
have a per-domain model, each keeping its many
parameters in memory, along with perhaps a set of
pre-allocated GPUs. Our goal is therefore to have
a single multi-domain model that performs well
on both seen and unseen domains. This is reminis-
cent of works in multilingual NLP that develop a
single model that handles multiple languages (e.g.
M-BERT released by Devlin et al. (2019), Liang
et al. (2020), Toledo-Ronen et al. (2020)).
A possible approach to our setting is training on
a diverse TSA dataset, potentially encompassing
many of the domains that the system is applied to.
However, obtaining such a dataset is a challenge.
The existing TSA datasets are limited in their diver-
sity, and the collection of a new large scale diverse
TSA dataset is complex (Orbach et al., 2021).
The road we take is therefore based on augment-
ing a TSA dataset of limited diversity with assort-
ment of weak labels, through self-training – one
of the earliest ideas for utilizing unlabeled data in
training (Chapelle et al., 2009). To show that our
approach is feasible, we performed an extensive
empirical evaluation with several LMs that were
fine-tuned with labeled data from the SE
dataset of Pontiki et al. (2014) (henceforth SE).
This dataset is limited to two domains: restaurants
or laptops. Each model went through several self-
training iterations and evaluated on three TSA pub-
licly available datasets: SE, the MAMS dataset
of restaurant reviews (Jiang et al., 2019), and the
YASO dataset of open-domain reviews (Orbach
et al., 2021).
As part of our evaluation, we created two new
TSA resources. The first is an annotation layer on
top of the YASO dataset, specifying the domain of
each review. This allows a per-domain evaluation
providing insights on the performance of seen and
unseen domains. The second resource is a set of
manually annotated TSA reviews, which can be an
ad-hoc diverse TSA training set, an alternative to
the proposed method. We show that even in the
presence of such data in training our approach is
valuable. Both resources are available online.
In summary, the main contributions of this work
are: (i) the first exploration of TSA in a multi-domain setup; (ii) demonstrating the feasibility of
multi-domain TSA by an extensive evaluation on
three datasets and the use of self-training; (iii) the
release of additional TSA resources: a new annota-
tion layer for the YASO dataset, and a set of fully
annotated reviews.
2 Related work
TSA The TSA task has been extensively studied
in different scenarios. Some works considered it
as a pipeline of two subtasks: (i) aspect-term ex-
traction (TE) for identifying target terms in texts
(e.g. Li et al. (2018); Xu et al. (2018)), and (ii)
aspect-term sentiment classification (SC) for deter-
mining the sentiment towards a given target term
(e.g. Dai et al. (2021a); Li et al. (2019c); Wang
et al. (2018)). Full TSA systems may combine
these building blocks by running TE and then SC
in a pipeline. Others, like our system, use a sin-
gle engine that provides an end-to-end solution to
the whole task, and may be based on pre-trained
language models (e.g. Li et al. (2019b); Phan and
Ogunbona (2020)) or a generative approach (Yan
et al., 2021; Zhang et al., 2021). In a cross-domain
setup, TSA research includes Chen and Qian (2021)
on TE, Rietzler et al. (2020) on SC, Wang and Pan
(2020); Pereg et al. (2020) for joint TE and opinion
term extraction and Gong et al. (2020) for the full
TSA task. In contrast with our setup, these works
all evaluate on one known domain.
Domain Adaptation A plethora of domain adap-
tation (DA) methods have been developed for han-
dling data from domains that are under-represented
in training. Several DA variants exist, of which the
most common one handles a single known target
domain. For sentiment analysis, DA is especially
important, as sentiment baring words tend to dif-
fer between domains (Ruder et al., 2017). One
promising DA approach is adjusting a given LM to
a target domain using pre-training tasks performed
on unlabeled data from that domain (Xu et al.,
2019; Rietzler et al., 2020; Zhou et al., 2020). An-
other recently proposed direction of DA explored
self-training for sentiment analysis (e.g. Liu et al.
(2021)).
Self Training At the core of our approach is the
iterative process of self-training. This methodology
has been successfully applied for varied research
problems, e.g. object detection (Rosenberg et al.,
2005), parsing (McClosky et al., 2006), handwrit-
ten digit recognition (Lee, 2013) and image clas-2752sification (Zou et al., 2019) (see also the survey
by Triguero et al. (2015)). Since the emergence of
pre-trained LMs, several works have explored fine-
tuning these models through self-training. Some ex-
amples are works on sentiment and topic classifica-
tion (Yu et al., 2021), negation detection (Su et al.,
2021), toxic span detection (Suman and Jain, 2021),
text classification (Karamanolakis et al., 2021) and
machine translation (Sun et al., 2021).
3 Method
Our self-training approach augments a given TSA
training set with weak-labels (WL) generated from
a large multi-domain corpus. The process (depicted
in Figure 1) starts by training an initial TSA model
on that given training data. Then, that model pro-
duces TSA predictions on a large unlabeled corpus
of diverse reviews. Finally, some of the predictions
are selected and added as weak labels to the origi-
nal training set. A new model is then trained with
the augmented data, applied to produce new predic-
tions on the unlabeled data, and the whole process
(detailed below) can repeat for several iterations.
3.1 TSA Engine
We consider TSA as a sequence tagging problem,
where the model predicts a discrete label for each
token of the input sequence. The possible labels
are: positive (P), negative (N) or none (O). The first
two labels represent tokens that are part of a sen-
timent target, and the O label represents all other
non-target tokens. For example, given "Here is a
nice electric car" , the desired output is the target
electric car , identified from the output word-level
sequence (O,O,O,O,P,P). During inference, for
each sub-word piece within the input text, the la-
bels scores outputted by the transformer model are
converted into probabilities by applying softmax,
and the highest probability label is selected. The
sub-word pieces predictions within each word are
then merged by inducing the label of the first word
piece with sentiment on the other word-pieces. Fi-
nally, consecutive word sequences having the same
label (P or N) constitute one predicted target.
Our tagging scheme falls under the category of
a unified tagging scheme (Li et al., 2019a) with IO
labels. Previous works with a unified scheme used
the more complex IOBES labels (Li et al., 2019a,b),
where the B and E labels designate the beginning
and end of a target, respectively, and S represents
a single token target. Observing that the labeleddata rarely includes two adjacent targets, the B
and E labels were omitted (following Breck et al.
(2007)). The S label was excluded since in practice
tokenization was to sub-word pieces, making the
prediction of a single S label redundant.
3.2 Unlabeled Data Set
We use the Yreviews data to create the weakly-
labeled dataset for training. We start the process
by extracting 2Msentences from the Y cor-
pus. The corpus contains the text of the review
documents and a list of business categories that
correspond to each review. The reviews were ini-
tially selected at random, and then some reviews
were removed by two conditions: reviews that are
rated as not useful (with useful=0) and reviews of
businesses with no business categories. For each
review, we assigned a single representative domain
based on its business categories. The domain was
determined by the first match between the review’s
categories and a predefined list of domains con-
structed from the categories in the corpus ordered
by their popularity.
Following the document-level filtering, each re-
view was split into sentences, and the sentences
were further filtered by: 1) length: only sentences
with 10-50 words were selected; and 2) senti-
ment: at least one sentiment word should appear
in the sentence. For the sentiment filter, we used a
general-purpose lexicon – the Opinion Lexicon (Hu
and Liu, 2004) that was automatically expanded
by an SVM classifier and filtered as described in
Bar-Haim et al. (2017). From that lexicon, we took
all the sentiment words with score Swith confi-
dence threshold of |S|>0.7, resulting with 7497
sentiment words.
Finally, the representative domain of each review
was assigned to all its selected sentences. Overall,
we identified 18 different domains in the 2Mex-
tracted sentences, as shown in Table 1. We can
see that 60% of the extracted data is from restau-
rants reviews, but the other 40% of the data cover
a variety of other domains.
3.3 Generating Weak labels
The process, depicted in Figure 1, starts by train-
ing a model on TSA labeled data (henceforth, the
LD model), followed by iteratively generating TSA
weak labels by self-training. The initial LDmodel
is used for predicting TSA target spans and senti-2753
ments on the unsupervised data. Each prediction
is associated with a score S. We use it as a con-
fidence score and select a subset of the sentences
according to the following recipe: 1) targets : sen-
tences with targets that have confidence S > 0.9
are selected if all other targets in the same sentence
haveS < = 0.5. The high-confidence targets are
added to the TSA weak labels and the other predic-
tions are ignored; 2) non-targets : sentences with
no predictions or if all the predicted targets have
scoreS <= 0.5are selected and all the predictions
are ignored. To limit the amount of this part of the
data, these sentences are randomly selected from
10% of the data. 3) domain balancing : the number
of sentences per domain is limited to 20k for each
part of the data – for sentences with targets and
for those with no identified targets. This creates
a balance between the representation of different
domains in the data. Without balancing, about half
of the selected data is from restaurants and other
domains are under-represented.
The selected sentences (those with TSA weak
labels and those with no targets) are then added
to the labeled data, and a new TSA model is fine-
tuned and used for TSA prediction and sentence
selection over the entire unsupervised dataset in
the next iteration. We repeated the process of WL
generation and model training 3 times and used the
model from the third iteration for evaluation. The
total number of sentences in the WLdata generated
from the 2M sentences extracted from Y is
about 280k. This number depends on the initial LD
model and on the number of iterations performed.
4 Empirical Evaluation
4.1 Evaluation Data
YASO In Orbach et al. (2021), we presented the
YASO TSA dataset comprising of user reviews
from multiple sources. This dataset covers reviews
from many domains, and is thus a good choice formulti-domain evaluation. While YASO allows an
assessment on diverse reviews, its data is unbal-
anced between domains, thus biasing a standard
evaluation towards the more common domains. A
per-domain evaluation is therefore complementary,
and can help validate that a model performs well
on all domains, not just the common ones. Such
an evaluation can also aid in discerning between
performance on domains that are well-represented
in the labeled data and ones that are unseen, thus
verifying that the evaluated model performs well
in both cases.
To facilitate such a per-domain evaluation, we
augmented YASO with a domain label for each
of its annotated reviews. The assigned labels were
produced automatically, when possible, or other-
wise they were manually set by one of the authors.
Since YASO contains annotated reviews from mul-
tiple sources, the assigned label depended on the
source: reviews taken from the Stanford Sentiment
Treebank (Socher et al., 2013; Pang and Lee, 2005)
were assigned the movies domain label. Reviews
from the O source (Ganesan et al., 2010)
were assigned a label of electronics ,automotive or
hotels , based on the topic provided in that corpus
for each review. For example, reviews on transmis-
sion_toyota_camry_2007 were assigned to automo-
tive. In the Ysource, each review is associated
with a list of business categories. These categories
were used as domain labels: we manually selected
8prominent categories as domains, and automat-
ically matched the reviews to the domains using
the category lists. Reviews matched to multiple
categories were manually examined and assigned
the most fitting domain from the matched cate-
gories. Texts from the A source (Keung
et al., 2020) were manually read and labeled.
Finally, the assigned domain labels were cat-
egorized into: restaurants (with 400sentences),
electronics (412),hotels (161),automotive (144),
movies (500) and other (596). This extra annota-
tion layer of the YASO evaluation data is avail-
able online (see §1). As suggested in Orbach et al.
(2021), YASO is used solely for evaluation.
MAMS Jiang et al. (2019) collected the MAMS
dataset over restaurant reviews. In MAMS , each
sentence has at least two targetsannotated with
different sentiments. The sentiments are either pos-
itive, negative or neutral. To match our setup, the
neutral labels were removed from these data. The2754
500sentences of the MAMS test set serve as an
additional evaluation set.
SE Pontiki et al. (2014) created the popular SE
dataset of restaurants and laptops reviews. We fol-
low the standard split of SEinto two sets with 6072
training sentences and 1600 test sentences. In each
set, the sentences are balanced between the two
domains. As in MAMS , the neutral labels were
removed, as well as the mixed sentiment labels.
4.2 Language Models
The following four pre-trained LMs were used in
our experiments:
BERT-B (Devlin et al., 2019) The BERT-base
uncased model with 110M parameters.
BERT-MLM To adjust BERT-B to user reviews
and sentiment analysis, we further pre-train it on
the Masked Language Model (MLM) task, using
the2Mreview sentences extracted from Y(see
§3.2). Our masking includes two randomly se-
lected sets: (i) 15% of the words in each sentence,
as in BERT-B; (ii) 30% of the sentiment words
in each sentence. The sentiment words are taken
from the union of two sentiment lexicons, one of
Bar-Haim et al. (2017) (with a confidence thresh-
old of 0.7), and the other created by Toledo-Ronen
et al. (2018) (with a confidence threshold of 0.5,
yielding 445words not present in the first lexicon).
Our masking of sentiment words is similar to the
method used by Zhou et al. (2020), yet we do not
use the emoticon masking.
BERT-PT (Xu et al., 2019) A variant of BERT-B
post-trained on the MLM and Next Sentence Pre-
diction tasks using Ydata from the restaurants
domain, and question answering data.
SENTIX (Zhou et al., 2020) A sentiment-aware
language model for cross-domain sentiment analy-
sis. This model was pre-trained with reviews fromYelp and Amazon, using an MLM task that ran-
domly masks sentiment words, emoticons, and reg-
ular words.
4.3 Experimental setting
Training Our fine-tuning used a cross-entropy
loss, the Adam optimizer (Kingma and Ba, 2014)
with a learning rate of 3e-5 and epsilon of 1e-8.
The training process was running with batch size
of 32 on 2 GPUs with a maximum of 15 epochs and
early stopping with min_delta of 0.005. In each
experiment, 20% of the training set sentences were
randomly sampled and used as a development set.
The optimized metric on this set was the overall
tokenFclassification rate.
Evaluation For each experiment, we trained 10
models with different random seeds. Then, the
per-domain performance metrics were computed
for each run, and averaged for a final per-domain
result (mean and standard deviation). These per-
domain results were macro-averaged to obtain the
overall performance on each dataset. As evaluation
metrics we report the precision (P), recall (R), and
F(mean and std), of exact match predictions.
4.4 In-Domain Results
Before showing the multi-domain results that are
the focus of this work, we present the in-domain
performance of our system on the widely-used SE
evaluation data. These results, summarized in Ta-
ble 2, serve as a sanity check for our system on a
well-known benchmark in a well-explored setup.
Explicitly, several single-domain models were
created by fine-tuning each pre-trained LM with
training data from one SEdomain, either restau-
rants (R) or laptops (L). These models, denoted
SE, were evaluated on test data from the same
domain they were trained on. For BERT-B, the
results of this evaluation (top row of Table 2) were
inline with previous works (cf. Wang et al. (2021)).
For each LM, Table 2 further shows the results2755
with added WL data (SE+WL), created using
the corresponding SE model on the diverse
Ycorpus. Interestingly, in all cases augment-
ing the training set with these WL improves results
over the models trained without such data.
4.5 Multi-Domain Results
For the main evaluation of our approach, we fine-
tuned each LM with the full SEtraining set (with
data of both the R and L domains), generated the
WLdata by self-training starting from the baseline
model ( SE), and then fine-tuned the final model
(SE+WL). Table 3 presents the results obtained
with these fine-tuned models, on YASO ,MAMS ,
andSE. In all cases, Fis improved by employing
self-training. For example, with BERT-B, there is
a10% relative gain in FonYASO andMAMS ,
and a 3%relative gain on SE. Even with stronger
base models such as SENTIX or BERT-PT that
incorporate domain knowledge into the language
model, we see gains of several points in Fby
adding the WL data. The gain in Fis mostly due
to gain in precision, sometimes at some cost in
recall (specifically for MAMS ). The variance of
Facross the different training runs is significantly
reduced.
Figure 2 further details per-domain results on
YASO , showing precision/recall curves for each
fine-tuned LM with and without self-training. As
above, each curve is the average of 10 per-run
curves. In most cases, the self-trained models out-
perform the initial corresponding fine-tuned SE
models. This result is also apparent in Figure 3
forMAMS . Here, although recall is decreased for
self-trained models their precision is significantly
improved across the entire curve.Next, we compare our self-supervision approach
with the cross-domain TSA work of Gong et al.
(2020).To adjust their system to a multi-domain
setup, we use the full SEtraining set (R and L)
as the labeled data from the source domain (as in
our system), and a random sample from the Y
unlabeled data to represent the target domain. The
number of sentences in the sample equals the size
of the training set, as in their experiments. The
sample was also balanced across all 18domains.
Table 4 includes the results of this comparison.
OnYASO , their baseline results (Gong-BASE)
improve when integrating their domain adaptation
components (Gong-UDA), yet they are lower than
with our self-supervision results (except for on SE).
4.6 Impact of the Initial LD Model
The quality and quantity of the TSA labeled data
used for training the initial TSA model are im-
portant factors for the quality of the weak labels
induced by its predictions. This, in turn, affects
the quality of the entire self-training process. This
experiment explores this effect, by imposing restric-
tions on the training set of the initial TSA model.
In this context, we experimented with three vari-
ants. One model was fine-tuned with half of the
SEdata (SE), selected at random from each do-
main, such that overall the samples were balanced
between the two domains. Two more models were
fine-tuned with SEdata from one domain – restau-
rants (SE) or laptops (SE). For all models, the
number of sentences in the training set was half the
size of the full SE data.
Table 5 summarizes the results of our experi-
ments with these models, focusing on the BERT-2756
MLM pre-trained model. As expected, training on
a single domain, or with half of the data, leads to
lower performance. The results on the MAMS
restaurants data are typical for a cross-domain
setup. When training on laptop reviews alone, re-
call drops almost entirely to 2.6, and self-training
improves upon that poor performance to some ex-
tent. Overall, across all datasets and all training
data starting points, performance consistently im-
proves when self-supervision is used.
4.7 Diversifying the Training Set
An alternative to our weak-labeling approach is
diversifying the TSA training set by manual label-
ing. To explore this option, we collected an ad-hoc
TSA training dataset that contains 952sentences
of reviews from multiple domains. The collection
started with reviews written by crowd annotators
in a given domain, on a topic of their choice.The
reviews were then annotated for TSA by asking
annotators to mark all sentiment-bearing targets in
each sentence. This step is similar to the candi-
dates annotation phase described in Orbach et al.
(2021). However, unlike in our previous work, the
detected candidates we collected were not passed
through another verification step, to reduce costs.
This results in noisier data, unfit for evaluation pur-poses, yet a manual examination has shown it is of
sufficient quality for training.
Table 6 shows the performance obtained using
this new dataset for training. The collected multi-
domain labels (henceforth MD) were combined
with the SEdata for fine-tuning the BERT-B and
BERT-MLM models. Comparing the results of
fine-tuning with data from limited domains ( SE)
to fine-tuning with the additional MD data, per-
formance significantly improves on the diverse
YASO evaluation set. On MAMS the improve-
ment is small, presumably because the restaurants
domain is well covered in the SEtraining set. On
theSEtest set the improvement is negligible or
non-existent. When comparing our approach us-
ing the WL data to the MD alternative, there is an
improvement in F1 on both MAMS andSE, yet
results on YASO are somewhat lower. However,
the precision achieved by our approach is consis-
tently better on all three evaluation sets compared
to the alternative method. Similar trends are ob-
served using BERT-MLM. Overall, the results with
WL are better or close to those with MD, with the
advantage that no manual labeling is required.
5 Manual Error Analysis
The automatic evaluation reported above is based
on exact-span matches, and may be too strict in
some cases. For example, in "The best thing about
this place is the different sauces," theYASO la-2757
beled data contains the target the different sauces ,
thus counting a prediction of sauces as an error.
Alternative evaluation options may circumvent this
problem. For example, the above prediction would
be considered as correct using overlapping span
matches. However, changing the automatic eval-
uation can introduce new issues and and may betoo lenient. Continuing the above example, with an
overlapping span match, a prediction of the entire
sentence is also considered as correct.
Due to these issues, we complement the auto-
matic evaluation with a manual one, comparing
the output of an initial LD model to its self-trained
counterpart. The error analysis was performed on
one experimental setup, with the BERT-MLM pre-
trained model and the entire SEdataset for training
theLDmodel. We further focus on the YASO
dataset: for each model, 30predictions consid-
ered as errors by the automatic evaluation were
randomly sampled from each of the 6domains.
One of the authors categorized these predictions
into one of four options: invalid target, correct
target identified with wrong sentiment or span, bor-
derline target that can be accepted, and a clearly
correct target. The latter are presumably due to the
strictness of the exact-matches based evaluation.
Table 7 presents the results of this manual analy-
sis. Overall, the self-trained model (SE+WL) pre-
dicts less non-targets. Moreover, it identifies more2758
Error Analysis SE SE+WL
Invalid target 27.2% 17.8%
Wrong sentiment/span 18.3% 18.3%
Borderline target 14.4% 16.1%
Correct target 40.0% 47.8%
valid targets than the baseline model. As for the
other two categories of errors, the borderline and
wrong span/sentiment, the two models are on par.
These results emphasize the importance of manual
error analysis, and show that even in this detailed
analysis, which goes beyond the labeling informa-
tion available in the YASO evaluation set, we find
that the multi-domain model with the WLis better.
6 Conclusion
This work addressed a multi-domain TSA setting
in which a system is trained on data from a small
number of domains, and is applied to texts from
any domain. Our proposed method has employed
self-learning to augment an existing TSA dataset
with weak labels obtained from a large corpus.
An empirical evaluation of our approach has
demonstrated that the self-supervision technique,often used when having a training set of limited
size, is also effective for enhancing the diversity of
the training data. Specifically, our results show that
the self-trained multi-domain model consistently
improves performance, for various underlying LMs,
and with different starting points: data from two
domains, removing half of the data, or restricting
to only one domain. Interestingly, even in the pres-
ence of a diverse TSA labeled data, our approach
was comparable to the performance obtained with
that data. This allows avoiding the burden and costs
associated with manual TSA data collection.
In addition to finding targets and their senti-
ments, other related tasks aim to extract the corre-
sponding opinion term (Peng et al., 2020), identify
the relevant aspect category (Wan et al., 2020), or
both (Cai et al., 2021). As future work, our ap-
proach may be applied to these more complex tasks
as well. Similarly, it may be useful for developing a
multilingual TSA system, by utilizing weak labels
produced on unlabeled reviews data in non-English
languages.
Acknowledgments
We wish to thank Artem Spector for the develop-
ment of the experimental infrastructure. We also
thank the anonymous reviewers for their insightful
comments and feedback.2759References276027612762