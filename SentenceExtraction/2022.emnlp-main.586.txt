
Keqin Chen, Richong Zhang, Samuel Mensah, Yongyi MaoSKLSDE, Beihang University, Beijing, ChinaZhongguancun Laboratory, Beijing, ChinaDepartment of Computer Science, University of Sheffield, UKSchool of Electrical Engineering and Computer Science, University of Ottawa, Canada
{chenkq,zhangrc}@act.buaa.edu.cn
s.mensah@sheffield.ac.uk, ymao@uottawa.ca
Abstract
Weakly supervised phrase grounding aims to
learn an alignment between phrases in a cap-
tion and objects in a corresponding image us-
ing only caption-image annotations, i.e., with-
out phrase-object annotations. Previous meth-
ods typically use a caption-image contrastive
loss to indirectly supervise the alignment be-
tween phrases and objects, which hinders the
maximum use of the intrinsic structure of the
multimodal data and leads to unsatisfactory
performance. In this work, we directly use
the phrase-object contrastive loss in the con-
dition that no positive annotation is available
in the first place. Specifically, we propose a
novel contrastive learning framework based on
the expectation-maximization algorithm that
adaptively refines the target prediction. Ex-
periments on two widely used benchmarks,
Flickr30K Entities and RefCOCO+, demon-
strate the effectiveness of our framework. We
obtain 63.05% top-1 accuracy on Flickr30K
Entities and 59.51%/43.46% on RefCOCO+
TestA/TestB, outperforming the previous meth-
ods by a large margin, even surpassing a pre-
vious SoTA that uses a pre-trained vision-
language model. Furthermore, we deliver a
theoretical analysis of the effectiveness of our
method from the perspective of the maximum
likelihood estimate with latent variables.
1 Introduction
Phrase grounding aims to localize corresponding
objects in an image given a phrase in the image’s
caption. It is one of the most fundamental research
areas in multimodal learning (Ramachandram and
Taylor, 2017). This area has strong applications
in other complex visual language tasks, such as vi-
sual question answering (Khan et al., 2021), cross-
modal retrieval (Chen et al., 2017), etc.Figure 1: Previous methods neglect the intrinsic similar-
ity which can be seen as a soft co-occurrence. Utilizing
the intrinsic similarity between negative examples (dark
blue circle) and false-positive examples (light blue cir-
cle) can push the false-positive examples away from the
query phrase (light green triangle).
Current methods (Huang et al., 2021; Kamath
et al., 2021) have achieved great success but rely
heavily on bounding box annotations, which are ex-
pensive to acquire. Thus, weakly supervised phrase
grounding has recently received increased attention
(Liu et al., 2021; Wang et al., 2020; Dou and Peng,
2021) due to the low cost of obtaining image-text
pair annotations. It aims to ground phrases us-
ing only caption-image annotations, i.e., without
phrase-object annotations.
The most critical question in weakly super-
vised phrase grounding is how to provide suffi-
cient phrase-object alignment supervision. Pre-
vious methods typically provide oblique phrase-
object alignment signal by heuristically aggregat-
ing several fine-grained phrase-object similarities
to be a coarse-grained caption-image similarity
and performing contrastive learning at the caption-
image level (Wang et al., 2020; Zhang et al., 2020;
Gupta et al., 2020; Wang et al., 2021). While ex-
isting methods have gained some progress in the
task, they are limited in several ways: this approach
only provides a coarse alignment signal and has no8549theoretical support. As shown in Figure 1 (left),
A close caption-image pair does not guarantee a
near distance for the corresponding phrase-object
pairs and occasionally fails to disambiguate the
co-occurrence objects. In contrast, we propose to
conduct phrase-object contrastive learning directly.
However, contrastive learning always needs pos-
itive examples annotation. Here, they are corre-
sponding phrase-object pairs, which we do not have
under the weakly supervised setting. It seems that
we are stuck in a chicken or egg dilemma: a good
model needs high-quality labels to train; good la-
bels only come from effective models. The key
to get rid of the dilemma is the intrinsic similarity
in data. That is, image regions sharing the same
concept are close in feature space. For example,
“alaskan malamute” and “husky” have the same con-
cept (i.e., dog) and therefore their image features
are expected to be close in the feature space. This
intrinsic similarity is key to disambiguate the co-
occurrence objects and reveal the ground truth. As
shown in Figure 1 (right), for a single query phrase,
when a negative example (objects in another image)
is pushed away, the false-positive example (objects
in the paired image but not corresponding to the
query phrase) semantically similar to the negative
example is pushed away as a side effect. The trick
is that although we don’t know which one is likely
to be correct, we do have a fair estimate about
which one is more likely to be wrong.
We can leverages and amplifies this effect, by
gradually removing the false positives from the
positives via interleaving contrastive learning and
pseudo labels update. At first, we have no pref-
erence for each object and set the pseudo labels
on objects as uniform distribution. After one itera-
tion of contrastive learning, the model gains more
confidence on the ground truth. Now, we retrain
the model by updating the pseudo labels using the
current model predictions; then since the object
assignment is more correct, the retrained model is
expected to predict better. This idea can be for-
mulated as an expectation-maximization algorithm
(Dempster et al., 1977) and have solid theoretical
support from the MLE perspective. To the best of
our knowledge, no previous work has highlighted
the importance of the intrinsic similarity and devel-
oped a method accordingly.
To realize the above idea, we propose a novel
contrastive learning framework based on the EM al-
gorithm that adaptively refines the target prediction.Specifically, we treat all the objects in the paired
image as positive examples for contrastive learn-
ing at the phrase-object level. And we introduce
pseudo labels to describe how much each object is
likely to be the correct answer, resulting in different
importance for different objects in the contrastive
loss. Finally, we update the pseudo labels from a
moving average of model predictions. Our model
can progressively refine the target prediction by
iteratively minimizing the contrastive loss and up-
dating pseudo labels.
We conduct experiments on two benchmarks,
Flickr30K Entities and RefCOCO+, and achieve a
new SoTA. Also, we perform a detailed ablation
study and case study to show the effectiveness of
each component.
In summary, the contributions of this work are
threefold:
•We identify the significance of the intrinsic
similarity in solving weakly supervised phrase
grounding and propose a novel contrastive
learning framework accordingly.
•We achieve a new SoTA on Flickr30K Entities
and RefCOCO+, outperforming the previous
methods by a large margin.
•We conduct extensive experiments to verify
the effectiveness qualitatively and quantita-
tively and deliver a theoretical analysis of the
effectiveness from the perspective of the max-
imum likelihood estimate.
2 Related Work
Weakly Supervised Phrase Grounding Weakly
supervised phrase grounding has received consid-
erable attention, as the fully supervised setting re-
quires the labelling of phrases in image captions.
Previous mainstream methods can be divided into
two main categories: reconstruction-based methods
(Liu et al., 2021; Dou and Peng, 2021; Rohrbach
et al., 2016) and contrastive-based methods (Wang
et al., 2020, 2021; Zhang et al., 2020; Gupta et al.,
2020; Datta et al., 2019). Our work falls into the
line of contrastive-based methods.
Previous contrastive-based methods typically
conduct contrastive learning at the caption-image
level by heuristically aggregating several phrase-
object similarities to be caption-image similarities.
Align2Ground (Datta et al., 2019) aggregates sev-
eral features of objects to get a caption-conditioned8550image representation, and matches it with the cor-
responding caption. InfoGround (Gupta et al.,
2020) defines a compatibility function to mea-
sure the compatibility between images and BERT-
contextualized word representation. InfoGround
uses BERT to generate hard negative examples.
CCL (Zhang et al., 2020) defines an aggregation
function to compute the alignment score between
a phrase and a set of objects, and generates coun-
terfactual examples by the gradient. MAF (Wang
et al., 2020) uses the mean of phrase-wise maxi-
mum similarity as the caption-image similarity and
performs contrastive learning at the caption-image
level. These works are distinctively different in
their definition of the aggregation function and the
approach in which they generate positive and nega-
tive examples. None of these works use contrastive
learning at the phrase-object level nor do they high-
light the importance of intrinsic structure as we
do.
Contrastive Learning Contrastive learning has
recently attracted much attention, as it has con-
tributed to the success in unsupervised represen-
tation learning. There exist a line of work that
explore contrastive learning at the instance level
in computer vision, natual language processing
and multimodal learning. For example, MoCo (He
et al., 2020), SimCLR (Chen et al., 2020), SimCSE
(Gao et al., 2021), CLIP (Radford et al., 2021), and
ALBEF (Li et al., 2021a). Another line explore the
learning problem in a weakly supervised setting.
PiCO (Wang et al., 2022) is one of such works,
which uses prototypes to address label disambigua-
tion in partial label learning. Our work is closely
related to PiCO. However, our work differs from
PiCO in twofold: Firstly, our model is oriented to-
ward a multimodal setting while PiCO only works
with a unimodal setting (i.e., image). Secondly,
PiCO only suits classification problems where the
total number of labels is fixed while our model is
unconstrained to the number of labels. Specifically,
in phrase grounding, the total number of possible
objects is unlimited. Hence, PiCO cannot be di-
rectly applied to this task.
3 Problem Formulation
Given a caption-image pair (S, I), a grounding
model is expected to find the object kamong m
objects in image Iwhich refers to the given phrase
qin caption S:arg maxlogp(k|q;I, S) (1)
It is difficult to solve this objective due to the
lack of phrase-region annotations. Therefore, we
treat this annotation as a latent variable z. That is,
the phrase qrefers to the region kwhen z=j.
We then solve the following maximum likelihood
estimate problem instead:
max log p(q|I)
= max log/summationdisplayp(q, z=j|I)(2)
Note that Zhang et al. (2018) solve logp(k|S).
On the contrary, we solve max log p(q|I). This
is based on the observation that all phrases are
conditioned on images but not all regions have a
corresponding phrase. The maximum likelihood
estimate of p(k|S)decreases the alignment per-
formance when khas no corresponding phrase in
sentence S. A model that solves (2)provides a
competitive estimate for latent variable z.
For clarity, we give several definitions here. With
respect to a given query phase, an image region
falls into one of the three categories: true positive,
false positive, and (true) negative. All regions in
an image not paired with the phase are regarded as
negatives. All regions in the image paired with the
phrases are positives, in which only the one that
semantically corresponds to the phrase is the true
positive and the rest are false positives.
4 Methodology
In this section, we introduce the proposed con-
trastive learning framework. We follow a 2-stage
paradigm. In the first stage, we apply a pre-trained
object detector to extract object features. In the sec-
ond stage, we sort the objects by their similarities
with phrases. As shown in Figure 2, our framework
consists of three main components:
Image Encoder f: It extracts bounding boxes
and features of objects in an image. We let
k, k, . . . , k=f(I)denote object features,
where Iis an image and mis the number of objects
in the image.
Text Encoder f: It takes as input a caption
Sand outputs nphrase features, denoted by
q, q, . . . , q=f(S).
Contrastive Loss Module : It adaptively refines
the alignment score between objects and phrases.8551
The key idea is to apply an E-step and M-step iter-
atively for this purpose.
In the following subsections, we describe each
component in detail.
4.1 Image Encoder
Image Encoder is responsible for extracting ob-
ject features. We adopt a similar encoder as MAF
(Wang et al., 2020). Specifically, the object feature
is calculated as follows:
¯f=dropout (f)
¯k=l+W¯f
k=dropout (¯k)(3)
Here,f∈Ris the feature of the i-th object cal-
culated by a pre-trained object detector. Also, the
detector predicts a text label indicating which class
this object most likely belongs to. The embedding
of the label is denoted as l∈R.W∈Ris
a projection matrix. Wis zero-initialized to use
the text-text similarity at the initial stage, providing
a good initialization. During training, the model
evolves gradually from a fully text-only model to a
multi-modal model.
4.2 Text Encoder
The text encoder encodes each phrase in a caption.
Unlike MAF (Wang et al., 2020), we find that in-
tegrating visual information into phrase features
contributes little to alignment. Therefore we sim-
ply construct the phrase representation by apply-
ing a sum-pool on the phrase’s GloVe embeddings(Pennington et al., 2014):
¯q=1
σ/summationdisplayh
q=dropout (W¯q)(4)
Here, nis the number of words in the i-th phrase.
h∈Ris the GloVe embedding for the j-th
word in the i-th phrase. σis a hyperparameter to
scale the phrase representation. W∈Ris a
projection matrix, initialized as an identity matrix
to stabilize training.
4.3 Contrastive Loss Module
The contrastive loss aims to pull together query q
and positive examples kand push away qand neg-
ative examples k. InfoNCE (van den Oord et al.,
2018; He et al., 2020) adopts the inner product qk
as the similarity metric between qandk:
L=−logexp(qk/τ)/summationtextexp(qk/τ)(5)
Here, Kis the set of all the negative objects, and τ
is a temperature hyperparameter.
By applying InfoNCE loss at the phrase-object
level, negative examples for a given phrase are
straightforward to obtain: collecting objects from
other images in the same training batch. However,
obtaining a positive example for a phrase is chal-
lenging due to the lack of annotations. Moreover,
phrases and objects live in two modalities. This8552means augmentations of a phrase can not serve as
positive examples.
We handle the generation of positive examples
by introducing pseudo labels. Specifically, inspired
by the recent progress in prototypical contrastive
learning (Wang et al., 2022; Li et al., 2021b), we
treat all objects in the paired image as positive
examples and assign a pseudo label on each object
to describe how much it is likely to be positive. So
the InfoNCE loss can be rewritten as:
L=−/summationdisplayπlogexp(qk/τ)/summationtextexp(qk/τ)
(6)
Here, Kis the set of objects in the paired image,
Kis the set of all objects in the same batch, τis a
temperature hyperparameter, and πis the pseudo
label showing the confidence that the phrase qis
aligned to object k. Moreover, πis fixed during
the optimization of the loss Land satisfies the
following constraints:
/summationdisplayπ= 1
π= 0∀k̸∈K(7)
Initially, pseudo label πis assigned a uniform
distribution among all positive examples due to the
lack of prior knowledge about which object is more
likely to be aligned.
π=1
|K|∀k∈K(8)
During training, we interleave minimizing the loss
Lwith updating the pseudo label π. Instead of
computing πevery few steps, we adopt the mov-
ing average updating strategy introduced by PiCO
(Wang et al., 2022) to smoothen the training pro-
cedure. In every train step, we first minimize the
contrastive loss L, and then update πusing the
moving average strategy:
π=λπ+ (1−λ)s (9)
where λ∈(0,1)is a hyperparameter.
s=/braceleftbigg1 i= arg maxqk,
0 otherwise(10)
Here, mis the total number of objects in the paired
image. scan be treated as a hard version of model
predictions, i.e., assigns one to the most confident
object and zero to the others.4.4 An MLE Perspective
In this section, we deliver a derivation from the
perspective of the Maximum Likelihood Estimate
(MLE) to illustrate the relationship between the
Contrastive Loss Module and the EM algorithm,
which sheds light on why it works theoretically.
Recall that the MLE problem for phrase q:
maxlogP(q|θ, I)
= maxlog/summationdisplayP(q|z=i, θ, I )P(z=i|θ, I)
(11)
zis a latent variable, indicating phrase qdescribes
thei-th object in image Iwhen z=i. Omitting
some steps, we directly give the Q-function.
Q(θ, θ)
=/summationdisplayP(z=i|q, θ, I) logP(q, z=i|θ, I)
(12)
E-step aims to guess the probability of the la-
tent variable zusing θ. i.e., π=P(z=i|
q, θ, I). In our work, we apply a softmax on the
inner product of phrase qand object kto obtain
π. i.e., π=softmax(qk). Practically, we use a
moving average strategy to update πto smoothen
the training procedure.
M-step aims to maximize the Q-function. For
convenience of derivation, we give two mild as-
sumptions:
Assumption 1 : the prior distribution of z,
P(z=i|θ, I), is a uniform distribution. It is
independent of model parameters θ. It is only rele-
vant to the number of objects in I.
Assumption 2 :P(q|z=i, θ, I )is a Gaussian
distribution with identical variance 1.
Then we get:
maxQ(θ, θ) (13)
= max/summationdisplayπ·logP(q|z=i, θ, I ) (14)
= max/summationdisplayπ·−(q−k)
2σ(15)
= max/summationdisplayπ·qk (16)
Here, the reason for (13)→(14) is that P(z=
i|θ, I)is a constant (Assumption 1) and has no
effect on maximizing the Q-function. (14)→(15)8553is due to the same reason. (15)→(16) can be
explained by −(q−k)=−(q+k−2qk) =
2qk−2when qandkare normalized. Note that,
although we assume qandkare normalized here,
we get a higher performance in practice when the
features are not normalized. We attribute this to the
magnitude of the features that learns a prior of z.
Meanwhile, the contrastive loss is:
L=−/summationdisplayπlogexp/summationtextexp
= (−/summationdisplayπqk
τ)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright+ log/summationdisplayexpqk
τ
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
(17)
Here, (a)is regarded as an alignment item, and
(b)is viewed as a uniformity item (Wang and
Isola, 2020; Wang et al., 2022). The term (a)en-
sures that semantically similar samples are close
to each other, which corresponds to maximizing
Q-function in Eqn. (16). The term (b)is indispens-
able, although it does not appear in Eqn. (16). It is
said to ensure that sample features do not collapse
to a point and have rich semantics (Wang and Isola,
2020; Wang et al., 2022). Here, it also provides
signals to disambiguate the co-occurrence objects
and reveal the ground truth. More analysis about
the term (b)is in subsection 5.2 Ablation Study.
To conclude, we can solve the original problem
Eqn. (2)by minimizing the contrastive loss Land
updating pseudo label πiteratively.
5 Experiments
Here, we present our experiments and results.
Datasets We adopt two widely used benchmarks
Flickr30K Entities (Plummer et al., 2017) and Ref-
COCO+ (Yu et al., 2016; Kazemzadeh et al., 2014).
Flickr30K Entities is an ectension of Flickr30K
dataset (Young et al., 2014), built for the Phrase
Grounding task. It contains 30k, 1k and 1k training,
validation and testing images, respectively. Each
image is accompanied with five captions.
RefCOCO+ is a widely-used Referring Expres-
sion dataset collected in a two-player game within
a limited time. Unlike Flickr30K Entities which
contains complete sentences, RefCOCO+ typically
contains noun phrases. We adopt UNC split (Yu
et al., 2016), which contains four parts: train, vali-
dation, testA and testB.Evaluation Metrics Following the standard pro-
tocol in previous work (Wang et al., 2021; Gupta
et al., 2020; Wang et al., 2020; Rohrbach et al.,
2016), we adopt top-1 accuracy as the evaluation
metric. A boundary box that overlaps the ground
truth with IoU > 0.5is considered to be correct.
The annotation for a phrase may involve several
bounding boxes. We merge them following previ-
ous work (Wang et al., 2020).
Implementation Details The image features are
extracted by a Faster R-CNN (Ren et al., 2015)
pre-trained on the Visual Genome (Krishna et al.,
2017) dataset with a ResNet-101 (He et al., 2016)
backbone. We use the same Flickr30K image fea-
tures as MAF (Wang et al., 2020) and RefCOCO+
as VOLTA (Bugliarello et al., 2021), which can be
obtained from their respective repositories. The
text features are 300-dimension GloVe embeddings
(Pennington et al., 2014). Hyperparameters are
tuning on the validation set. Best moving average
hyperparameter λ= 0.85and the best scale hyper-
parameter σ= 10 . We use SGD as the optimizer,
without momentum or weight decay. We train the
model for 80 epochs using a batch size of 256 with
a learning rate of 5e-4. The temperature τis as-
signed to 1. Dropout rate is 0.1. we implement our
model using PyTorch on a Linux machine with a
GPU device Tesla P100 PCIE 16G.
5.1 Main Results
As shown in Table 1, our approach consistently
outperforms previous methods by a large mar-
gin on both datasets. We obtain 63.05% accu-
racy on Flickr30K, 59.51% on RefCOCO+ TestA
and 43.46% on TestB. That is, a gain of 0.95%,
11.62% and 5.26% when compared with previous
SoTA, respectively. Note that previous SoTA use
a complicated pre-tained visual language model to
model the relationship between objects and phrases,
whereas, we merely use the GloVe embedding and
inner product similarity. By comparing with the
prior contrastive learning methods, we also demon-
strate that our expectation-maximization strategy
is effective for the task.
5.2 Ablation Study
In this subsection, we analyze the contribution
of each model component by conducting ablation
studies. We demonstrate that the contrastive loss8554
and pseudo label update are both significant to op-
timization, and give a geometric understanding.
The model ablation are characterized by the fol-
lowing, (1) Without Pseudo Label Update : we
remove the pseudo label update step. In other
words, all the positive examples have a fixed and
uniformed confidence during the training; (2) With-
out Contrastive Loss : we set the number of neg-
ative objects from other pictures to zero. So the
model can only use objects in the paired image;
(3)Different Updating Strategy : we experiment
with different updating strategies, following PiCO
(Wang et al., 2022). We consider updating with
moving average (as shown in (9)), without moving
average (formulated as π=s), hard predic-
tion (as shown in (10)), or soft labels (formulated
ass=softmax(qk)); (4) Effect of Hyper-
parameters : we vary the hyperparameters λandσ
to observe its effect on the model performance.
Table 2 shows our ablation results. There is a dra-
matic drop in performance when we ignore the up-
date of pseudo labels or the contrastive loss. Specif-
ically, without updating pseudo labels, the model
always regards all positive samples as equally im-portant and pulls the phrase toward each positive
example with the same force. It is difficult to spot
the ground truth hidden in the positive samples,
even if we provide a large number of negative exam-
ples. Without the contrastive loss, that is, without
using objects in other images as negative samples,
the model does not have enough clues to distin-
guish which one is preferred and which is not. We
may update the pseudo label in a wrong direction.
Only combining them as a whole can bring in an
excellent performance.
To further understand the role of the unifor-
mity term (b)in Eqn. (17), we include more ab-
lations on the contrastive loss. Specifically, we
keep the batch size and vary the number of nega-
tive samples in the batch used for computing the
contrastive loss. When the number equals 0, the
model is the same as w/o contrastive loss; when the
number equals batch size, the model is the same
as the proposed one. For 0,1,4,64 negative sam-
ples, RefCOCO+ TestA/TestB accuracies are re-
spectively 19.58/24.87, 48.57/38.13, 56.37/40.76,
59.35/43.27, and Flickr30K Test accuracies are
54.79, 62.11, 62.88, 63.01 respectively. Compar-8555ing the result between 0 and 1 negative sample, we
observe that the absence of negative samples causes
a dramatic drop in accuracy. With an increasing
number of negative samples, we observe a slow
and steady increase in performance.
Since false positives are semantically similar to
the negatives, when we push away a negative sam-
ple from the query phrase, we push away the false-
positive samples as well. The model thus acquires
some ability to distinguish the false negatives from
the phrase encoding. Without negative samples,
the model will fail to distinguish false-positives
from the true-positive. The EM approach devised
in this work just leverages and amplifies this effect,
by gradually removing the false positives from the
positives via down-weighting them in the pseudo
label.
Moreover, we experiment with different updat-
ing strategies. Similar to the observation by Wang
et al. (2022), all four strategies obtain competitive
result. Meanwhile, the moving average strategy
degenerates the performance of the soft prediction
variant. Unlike the RefCOCO+ dataset, we find
that Flickr30K is more sensitive to updating strate-
gies.
We also observe the effect of the hyperparame-
tersλandσ. As shown in Figure 3 (left), a smaller
λresults in a quicker convergence but with rela-
tively lower performance. A similar observation is
also noted in PiCO. At λ= 0, we gain competitive
results. However, a large λalso hurts performance.
When λ= 1, i.e., not updating pseudo label, the
performance drops dramatically. It does not even
go through a growth phase. We get the best re-
sults when λ= 0.85. Moreover, the sum scale
parameter σis important in this task. As shown in
Figure 3 (right), scaling down the norm of phrase
features results in a quicker convergence. We get
best results when σ= 10 .5.3 Case Study
To better understand the capacity of our Contrastive
Loss Module, we visualize the model’s prediction
for different epochs in Figure 4. We can observe
that our method can refine the prediction progres-
sively. Initially, the probability is nearly uniform.
As training progresses, the model’s confidence in
the bounding box increases and gets closer to the
ground truth, as shown in epoch 30.
Figure 5 also shows a number of results on Re-
fCOCO+ validation set. We can observe that our
model can handle cases with simple phrases, which
occupies a large proportion in RefCOCO+. As
shown in Figure 5 (a) and (b), our model has a
proper understanding of gender, size, object name,
etc. The results demonstrate its effectiveness.
We also show two kinds of failure in our model.
Figure 5 (c) shows that our model cannot effec-
tively handle cases which require complex reason-
ing. We attribute this to the fact that our method
neglects the modelling of the sentence structure.
Accordingly, we experiment with an LSTM text en-
coder to capture the sentence structure but achieve
no performance gain. It is challenging to perform
weakly supervised learning on cases requiring com-
plex reasoning. We therefore leave it for future
exploration. (d) also shows that our model lacks a
tacit agreement with humans. It selects the proper
object if mirrored from the opposite side. This tacit
agreement is also hard to achieve because there
is no annotation to tell the model what the tacit
agreement is.
6 Conclusion
In this paper, we identify the significance of the
intrinsic similarity and propose a novel contrastive
learning framework based on the expectation-
maximization algorithm to solve weakly supervised
phrase grounding. Our method can adaptively re-
fine the model prediction by minimizing a con-
trastive loss and chronologically updating pseudo
targets. To validate our method, we conduct ex-
tensive experiments and deliver a theoretical anal-
ysis from the perspective of the maximum likeli-
hood estimate. Our proposed approach achieves
state-of-the-art performance on two widely used
datasets, Filckr30K Entities and RefCOCO+. For
future works, a more sophisticated language model
is worth to be explored for phrase modeling to as-
sociate complex phrases with an object. We are
also interested in investigating to what extent our8556
approach may take effect in other application prob-
lems of a “weak supervision” nature, for example,
weakly supervised referring expression segmenta-
tion.
Limitations
The main limitation is that our method follows a
2-stage paradigm: object detection and then phrase
grounding, which limits the scope of usage because
a satisfactory pre-trained object detector may be
hard to acquire. There is also a chance of error prop-
agation to the phrase grounding stage if the object
detector fails to extract the bounding boxes of the
ground truths. Besides, the grounding model will
require re-training when modifying the detector. It
is interesting to explore how to perform weakly
supervised grounding in a detector-free context.
Acknowledgements
This work was supported in part by the Na-
tional Key R&D Program of China under Grant2021ZD0110700, in part by the Fundamental Re-
search Funds for the Central Universities, in part by
the State Key Laboratory of Software Development
Environment. SM is supported by a Leverhulme
Trust Research Project Grant (No. RPG-2020-148).
References855785588559