
So Yeon MinHao ZhuRuslan SalakhutdinovYonatan Bisk
Machine Learningand Language Technologiesat Carnegie Mellon University
{soyeonm,hzhu2,rsalakhu,ybisk}@andrew.cmu.edu
Abstract
Embodied dialogue instruction following re-
quires an agent to complete a complex se-
quence of tasks from a natural language ex-
change. The recent introduction of benchmarks
(Padmakumar et al., 2022) raises the question
of how best to train and evaluate models for
this multi-turn, multi-agent, long-horizon task.
This paper contributes to that conversation, by
arguing that imitation learning (IL) and related
low-level metrics are actually misleading and
do not align with the goals of embodied dia-
logue research and may hinder progress.
We provide empirical comparisons of metrics,
analysis of three models, and make suggestions
for how the field might best progress. First,
we observe that models trained with IL take
spurious actions during evaluation. Second, we
find that existing models fail to ground query
utterances, which are essential for task comple-
tion. Third, we argue evaluation should focus
on higher-level semantic goals.
1 Introduction
Dialogue is key to how humans collaborate;
through dialogue, we query information, confirm
our understanding, or banter in a friendly man-
ner. Since communication helps us work more
efficiently and successfully, it is only natural to
imbue for collaborative agents with this same abil-
ity. Most work has focused on grounded dialogues
for embodied navigation (Thomason et al., 2020;
Chi et al., 2019; Roman et al., 2020) or limited
interaction (Suhr et al., 2019), which are narrower
domains than the larger instruction following lit-
erature (Tellex et al., 2011, 2020; Shridhar et al.,
2020; Blukis et al., 2018, 2021; Min et al., 2021).
The first step towards engaging in a dialogue,
is being able to understand and learn from it. Pic-
ture a child watching their parents with the goalto learn by imitation. They witness instructions,
clarifications, mistakes, and banter. Begging the
question: What should one learn from noisy natural
dialogues?
Unlike in alinguistic tasks where modeling hu-
mans has recently proved helpful for search strate-
gies (Deitke et al., 2022), we focus on language
based tasks that require learning lexical-visual-
action correspondences. We discuss and compare
three paradigms: Instruction Following (IF), ac-
tions from Entire Dialogue History (EDH) and Tra-
jectory from Dialogue (TfD). The novel TEACh
dataset (Padmakumar et al., 2021) proposes EDH
as the primary metric and uses the Episodic Trans-
former (ET) (Pashevich et al., 2021) trained with
behavior cloning as their baseline. We also include
comparisons to the EDH competitive Symbiote
system and we adapt FILM (Min et al., 2021), a
recent method for general IF, to dialog instruction
following (DIF) on TEACH. FILM and Symbiote
belong to a different family of models, focusing
on abstract planning trained at a higher semantic
level than behavior cloning. This approach appears
crucial for generalization and TfD evaluations.
Most importantly, we analyze the human behav-
iors in TEACH and the corresponding effect on ET,
Symbiote, and FILM, as representatives of exist-
ing model classes. From our findings, we suggest
there are three major challenges the community
must tackle to move forward in the nascent field of
Dialogue based Instruction Following:
Recognizing mistakes Behavior cloning encour-
ages replication of low-level errors, but not high-
level intentions. Agents should learn to construe
high-level intentions of demonstrations and to de-
viate from demonstration errors.
Grounding queries No approaches correctly
ground “queries" requesting information.9361Evaluation Agent evaluation should focus on
achieving goals rather than immitating procedures.
2 Related Work
Instruction Following A plethora of works have
been introduced for instruction following without
dialogue (Chen and Mooney, 2011; Matuszek et al.,
2012); an agent is expected to perform a task given
a language instruction at the beginning and visual
inputs at every time step. Representative tasks are
Visual Language Navigation (Anderson et al., 2018;
Fried et al., 2018; Zhu et al., 2020) and instruction
following (IF) (Shridhar et al., 2020; Singh et al.,
2020), which demands both navigation and manip-
ulation. Popular methods rely on imitation learning
(Pashevich et al., 2021; Singh et al., 2020) and mod-
ularly trained components (Blukis et al., 2021; Min
et al., 2021) (e.g. for mapping and depth).
Dialogue Instruction Following Instruction Fol-
lowing with Dialogue (She et al., 2014) has mostly
addressed navigation. Thomason et al. (2020); Suhr
et al. (2019) built navigation agents that ground
human-human dialogues, while Chi et al. (2019);
Nguyen and Daumé III (2019) showed that obtain-
ing clarification via simulated interactions can im-
prove navigation. Manipulation introduces ground-
ing query utterances that involve more complex
reasoning than in navigation-only scenarios (Tellex
et al., 2013); for example, the agent may hear that
the object of interest (e.g. “apple”) is inside “the
third cabinet to the right of the fridge.”
Imitation Learning vs Higher semantics While
behavior cloning (BC) is a popular method used
to train IF agents, it assumes that expert demon-
stration is optimal (Zhang et al., 2021; Wu et al.,
2019). TEACh demonstrations are more “ecologi-
cally valid" (de Vries et al., 2020) but correspond-
ingly suboptimal, frequently containing mistakes
and unnecessary actions. Popular methods that deal
with suboptimal demonstrations involve annotated
scoring labels or rankings for the quality of demon-
strations (Wu et al., 2019; Brown et al., 2019). Such
additional annotations are not available in existing
IF and DIF benchmarks. In this work, we empir-
ically demonstrate the effect of noisy demonstra-
tions on an episodic trained with BC for DIF.
3 Tasks
TEACh focuses on two tasks: Entire Dialogue His-
tory and Trajectory from Dialogue. Despite whatthe name implies, EDH is an evaluation over par-
tial dialogues (e.g. from state Sbegin execution to
S). TfD starts an agent at Sand asks for a com-
plete task completion provided the full dialogue.
In both settings, the agent (driver) completes
household tasks conditioned on text, egocentric
RGB observations, and the current view. An in-
stance of a dialogue will take the form of a com-
mand: Prepare coffee in a clean mug. Mugs are in
the microwave. , the agent response How many do
I need? , and commander’s answer: One, together
with a sequence of RGB frames and actions that
the agent performed during the dialogue. As in this
example, the agent has to achieve multiple subtasks
(e.g. find mug in the microwave, clean mug in the
sink, turn on the coffee machine, etc) to succeed.
In TfD, the full dialogue history is given, and the
agents succeeds if it completes the full task itself
(e.g. make coffee). In EDH, the dialog history is
partitioned into “sessions” (e.g. Fig. 1) with the
corresponding action/vision/dialogue history until
the first utterance of the commander ( Prepare ∼
microwave. ) being the first session and those after
it being the second. In EDH evaluation, the agent
takes one session as input and predicts actions until
the next session. An agent succeeds if it realizes all
state changes (e.g. Mug: picked up ) that the human
annotator performed. Succinctly, TfD measures the
full dialogue while EDH evaluates subsequences.
4 Models
TEACh is an important new task for the community.
We analyze the provided baseline (ET), retrofit the
ALFRED FILM model, and requested outputs from
the authors of Symbioteon the EDH leaderboard.
ET is a transformer for direct sequence imitation
approach, that produces low-level actions condi-
tioned on the accumulated visual and linguistic
contexts. In contrast, FILM consists of four sub-
modules - semantic mapping, language processing,
semantic policy, and deterministic policy modules.
For the adaptation, we refactored the original code
of FILM to the TEACH API, retrained the learned
components of the semantic mapping module for
the change in height and camera horizon, and re-
trained/rewrote the language processing module to
take a dialogue history as input. The language pro-
cessing (LP) module of FILM maps an instruction
to atask type and instruction-specific arguments .
For TfD this maps a dialogue to a sequence of tasks,
while for EDH only the subsequence is mapped to9362
an immediate action. Symbiote is a competitive
modular method for EDH whose language under-
standing component is designed for dialogues (§A).
5 Challenges of Human Traces
First we present how TEACh and, by extension,
future embodied dialogue settings present novel
training and evaluation challenges as the data, by
virtue of its authenticity, includes substantial noise
in both the training and evaluation (despite filtering
by the authors §C). See §B for how statistics were
computed, for those not explained in this section.
5.1 Explanation of Metrics
Evaluation for both EDH and TFD is done by SR
(success rate), GC (goal condition success rate),
and their path-length-weighted versions. Success
Rate (SR) is a binary indicator of whether all sub-
tasks were completed. The definition of “subtasks”
is different for EDH and TfD; for the former, they
are all tasks required to realize state changes done
by the human demonstration that are relevant to
the ultimate task (e.g. The demo state changes in
each session of Fig.1 (b)). Thus, the state changes
brought by the human is considered ground truth
in EDH evaluation; this brings multiple challenges
further discussed in §5.2. On the other hand, for
TfD, the subtasks are independent of what was
done in the demo; for example, as long as an agent“slices the tomato” correctly for the task of Fig.1
(b), its SR will be 1 for this task.
The goal-condition success (GC) of a model is
the ratio of goal-conditions completed at the end of
an episode. Both SR and GC can be weighted by
(path length of the expert trajectory)/ (path length
taken by the agent); these are called path length
weighted SR (PLWSR) and path length weighted
GC (PLWGC). Higher is better for all metrics.
5.2 Challenges in Evaluation
Irrelevant Actions Humans often explore the en-
vironment, or simply play around in the middle of a
task. This means they may flip a switch completely
unrelated to the goal. Table 1 are representative
state changes that do nothave direct correspon-
dence with the dialogue, and the percentage of
human demonstrations that contain these actions.
It is not always clear if this behavior is because
of misunderstandings, boredom, or curiosity. For
example, we can classify a large number of naviga-
tion and interaction "No Op"s, or action sequences
that return to the original state (e.g. turning around9363
in place). In principle, these might be information
seeking, to build a better map of the environment,
but in practice, many of the demonstrations do not
seem to exhibit those properties, particularly in ex-
treme cases like repeatedly picking up and putting
down the same object. The percentages of preva-
lence of these unnecessary actions in both training
and validation are shown in Table 2.
The prevalence of these actions can be viewed as
a positive for realism and even helpful if teaching
how to search, but pose a challenge for evaluation.
Penalizing Agents for Accuracy Using a hu-
man’s action trace as the ground truth, means
agents are penalized for skipping erroneous ac-
tions. This leads to a misleading mismatch in
performance between EDH and TfD. Additionally,
EDH inflates model performance as it includes sub-
sequences which are nearly deterministic (e.g. all
but the last “placing" action). Table 3 contains
EDH scores for our three comparison models and
TfD for ET/FILM. As suggested by authors of re-
lated papers, we treat Unseen Success Rate as the
most important metric (seen in blue).
Note, that an ideal evaluation would capture both
“actions in context" and “task success." In the fol-
lowing section breakdown the overall numbers pre-
sented here to understand if models more carefully.
5.3 Challenges in Training
Behavior Cloning with Suboptimal Demonstra-
tions We find that ET trained with behavior
cloning repeats the same mistakes in novel scenes
that are frequent in demonstrations. We examine
two kinds of mistakes in demonstrations - (1) No
Op interactions, in which consecutive interactions
produces futile state changes (e.g. Placing and
immediately picking up the same object) and (2)
Interactions with unrelated objects (e.g. picking up
“saltshaker” while making coffee). In Table 4 we
compare what percent of model predictions in seen
and unseen scenes replicate the no-op behavior.9364
While hard to quantify, we also note that the
higher intention of seemingly unnecessary human
demonstrations (e.g. to explore, to understand, etc)
are not replicated by ET. This is backed by our
observation that ET tends to be stuck in many (10 or
more) repetitions of the same No Op/ unnecessary
actions, until the end of the task or before resuming
to perform other actions.
Note that even Symbiote is exhibiting some
no-op behaviors, but as the model supervi-
sion/structure becomes more abstract (ET vs FILM)
this disappears, leaving only object choice errors.
Grounding Queries Key to dialogue is language
based information seeking. A target object may be
located in a closed receptacle (cabinet, etc); in this
case, the agent has to query the commander for its
location, as a human would. We examine whether
models ground query utterances into meaning and
accurate actions, since this is one essential aspect of
dialog grounding. While there are utterances with
other essential intents, such as confirmation, we
focus on query utterances since these are relatively
easy to extract mechanically.
In Table 5 we consider a subset of tasks that
involve “query utterances” that can be detected
automatically. Specifically, we present the perfor-
mance of models in terms of success rate and goal
condition success on tasks that require opening
a receptacle based on the answer to a question –
and then measure if the models leverage the query.
Not all query utterances will be of this type, but
these tasks necessarily involve grounding query
utterances for task success.
Queries are present in 23.05% and 25.31% of
valid seen and unseen splits, respectively. This isa key challenge as it demonstrates a clear use case
for dialogue and limitation of current models.
Given a statement like “the fork is in the cabinet
left to the refrigerator”, the evaluation mismatch
occurs if an agent grabs a different fork on a ta-
ble. This allows them to succeed, as measured by
SR/GC, but not in SR/GC with Query. Notably,
all models fail at query grounding, indicating they
are simply ignoring the language instructions. This
shows that enabling complex dialogue grounding
is an important open problem for DIF. Especially,
for the ultimate goal of two-agent task comple-
tion (TaTC), it is necessary that models can ground
query and other essential utterances in a dialogue.
6 Conclusion and Next Steps
This paper is not an indictment of TEACh, nor an
endorsement of a particular model, rather it seeks
to lay out important questions and challenges that
NLP will need to tackle as it moves into embodied
dialogue. Unlike existing work in dialogue that
looks to model human satisfaction (Ghandeharioun
et al., 2019) or state-tracking, DIF has the advan-
tage of explicit and verifiable semantic goals. We
pose a challenge to the community: How can we
build agents where success is not tied to specific
actions yet language understanding and production
are accurate and fluent? As a first step, we posit
that imitation learning should be avoided.
7 Limitations
We focus on a new embodied benchmark – there
is substantial work in dialogue (including goal di-
rected) for non-embodied environments which we
do not consider as aligned with the goals of embod-
ied DIF, but may have important insights. Addition-
ally, future work may overcome the issues raised
and it is unclear how to transfer our findings back to
the goal directed dialogue in the non-embodied set-
ting. Additional insights may derive from research
in social intelligence.
References936593669367A More Discussion of Symbiote
Symbiote has a modular structure, which consists
of language understanding, mapping, and low-level
planning components. It is not trained with im-
itation learning of low-level demonstrations (e.g.
move right, move left, etc.). Demonstrations are
used only in the sense that they provide subgoals
that suervise the training of the language under-
standing component.
More specifically, a pretrained T5 model (Raf-
fel et al., 2020) fine-tuned with the ground truth
subgoals ( edh_instance[‘future_subgoals’] ),
serves as the language understanding component.
The model takes the driver and commander’s dia-
logue and previous actions as input; it is trained to
output a sequence of subgoals of the form “{action}
{obj}”, where {action} is either “navigate” or any
of the primitive interactions commands "pickup",
"cut", "toggle", etc, and {obj} is any of the object
classes in ai2thor.
For the mapping component, a DETR detector
(Carion et al., 2020) was finetuned on the train set
scenes of TEACh and the depth prediction model
from FILM was used off-the-shelf. Frontier based
exploration is used for environment exploration.
Similarly as in FILM, the agent navigates to object
goals in the map using the fast marching method.
B How the Statistics of Section 5 were
Obtained
We explain how the statistics that appear in each
table of Section 4 were obtained. All analyses,
except for TfD results in Penalizing Agents for
Accuracy , were done on EDH tasks.
Irrelevant Actions The first table shows some
representative unnecessary state changes that EDH
tasks require for “task success’ in evaluation. For
example, in our common sense, it is not necessary
that we leave the coffee machine on to successfully
make coffee (indeed, it is better to turn it off after
use). However, since EDH evaluation requires that
the agent exactly follows state changes done in the
demonstration, the agent will have to leave coffee
machine turned on for a particular validation task,
if this was done in its corresponding demonstration.
Each row shows unnecessary state changes that
are exemplary and the average frequency of these
noises across relevant tasks. More specifically,
•Coffee Machine on/ off :‘Coffee’ tasks•Picked up and not placed : all tasks
•Faucet on/ off : all tasks that may involve
using the faucet ( ‘Coffee’, ‘Clean All X’,
‘Boil X’,‘Water Plant’,‘Sandwich’, ‘Breakfast’,
‘Plate Of Toast’, ‘Salad’ )
•Stove/ Microwave on/off : all tasks that may
involve using a heating appliance ( ‘Boil X’,‘N
Cooked Slices Of X In Y’ )
“Total” accounts for the percentage of EDH tasks
that fall into any of the above criteria. Please refer
to (Padmakumar et al., 2021) for the possible types
(e.g. ‘Coffee’ ) of tasks.
While the first table shows statistics of irrelevant
state changes of “relevant objects”, the second ta-
ble shows those of more random actions, at a lower
level. Navigation No Op, the first kind, was sim-
ply obtained by detecting the existence of consecu-
tive Turn Lef/Right x 4, Forward + Backward, Pan
Right + Pan Left, Turn Right + Turn Left. The sec-
ond kind, interaction No Op, was similarly detected.
Whether an consecutive and opposite interactions
were done on the same “object” was detected by
replaying the pred_actions in the model outputs.
Interaction w. unrelated objects denotes whether
the demonstration an object that is completely un-
related from task type (e.g. picking up saltshaker
for a task whose type is ‘Coffee’ ). Demonstrations
unaligned with dialogue were counted manually
since there is no automatic way to filter these.
Penalizing Agents for Accuracy The statistics
in this subsection were straightforwardly obtained
by averaging over the evaluation outputs (whose
formats follow that of the original ET code from
TEACh) of each task.
Behavior Cloning with Suboptimal Demonstra-
tions The same procedures for the second table
inIrrelevant Actions were used.
C TEACh Prefiltering
Only necessary state changes are checked in EDH
evaluation, but all are present in training. https://
github.com/alexa/teach#downloading-the-dataset
mentions that the authors filtered the EDH tasks
so that “the state changes checked for to evaluate
success are only those that contribute towards task
success in the main task of the gameplay session
the EDH instance is created from.” Our analysis is
on data that has already been filtered and cleaned
and yet still exhibits these problems.9368