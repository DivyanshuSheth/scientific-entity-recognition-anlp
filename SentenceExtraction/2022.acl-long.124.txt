
Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He,
Wenjie Pei, and Ruifeng XuSchool of Computer Science and Technology,
Harbin Institute of Technology, Shenzhen, ChinaJoint Lab of HITSZ and China Merchants Securities, Shenzhen, ChinaSIAT, Chinese Academy of Sciences, Shenzhen, ChinaDepartment of Computer Science, University of Warwick, UKThe Alan Turing Institute, UK,Peng Cheng Laboratory, Shenzhen, China
{bin.liang, xiangli}@stu.hit.edu.cn, louchenw@163.com
min.yang@siat.ac.cn, {lin.gui, Yulan.He}@warwick.ac.uk
wenjiecoder@outlook.com, xuruifeng@hit.edu.cn
Abstract
With the increasing popularity of posting mul-
timodal messages online, many recent studies
have been carried out utilizing both textual and
visual information for multi-modal sarcasm de-
tection. In this paper, we investigate multi-
modal sarcasm detection from a novel perspec-
tive by constructing a cross-modal graph for
each instance to explicitly draw the ironic re-
lations between textual and visual modalities.
Specifically, we first detect the objects paired
with descriptions of the image modality, en-
abling the learning of important visual informa-
tion. Then, the descriptions of the objects are
served as a bridge to determine the importance
of the association between the objects of im-
age modality and the contextual words of text
modality, so as to build a cross-modal graph for
each multi-modal instance. Furthermore, we
devise a cross-modal graph convolutional net-
work to make sense of the incongruity relations
between modalities for multi-modal sarcasm
detection. Extensive experimental results and
in-depth analysis show that our model achieves
state-of-the-art performance in multi-modal sar-
casm detection.
1 Introduction
Sarcasm is a peculiar form of sentiment expres-
sions, allowing individuals to express contempt
sentiment or intention that is converse to the authen-
tic/apparent sentiment information (Gibbs, 1986;
Dews and Winner, 1995; Gibbs, 2007). As such, ac-
curately detecting satirical/ironic expression couldFigure 1: Two multi-modal sarcastic examples. Boxes
and words in the same color denote highly correlated
sarcastic cues.
potentially improve the performance of sentiment
analysis and opinion mining (Pang and Lee, 2008;
Kumar Jena et al., 2020; Pan et al., 2020).
In today’s fast growing social media platforms,
it is common to post multi-modal messages. There-
fore, in addition to developing sarcasm detection
models for textual data (Riloff et al., 2013; Joshi
et al., 2015), it is increasingly popular to explore
sarcasm detection in multi-modal data such as text
and images (Schifanella et al., 2016; Cai et al.,
2019). Dealing with multimodal data requires an
understanding of the information presented in dif-
ferent modalities. As the sarcastic example shown
in Figure 1 (a), text-only approaches may erro-
neously identify it as a positive sentiment expres-
sion due to the phrase “ wonderful weather ”. This
post however contains a sarcastic expression with
negative sentiment, because it is accompanied by
an image with “ thunderstorm clouds ”. The key of
effective multi-modal sarcasm detection is to accu-
rately extract the incongruent sentiment cues from1767different modalities, allowing the detection of the
true sentiment conveyed in the message.
To perform multi-modal sarcasm detection on
data composed of text and image, several related re-
search efforts attempt to concatenate the textual and
visual features to fuse sarcastic information (Schi-
fanella et al., 2016), employ attention mechanism
to implicitly fuse the features of different modali-
ties based on external knowledge (Cai et al., 2019;
Xu et al., 2020; Pan et al., 2020), or build inter-
active graphs to model the relations of different
modalities (Liang et al., 2021a). Despite promising
progress made by existing models, they still suffer
from the following limitations: 1)Simply consider-
ing the whole image does not produce good results,
mostly due to the intricate visual information pre-
sented in an image; not to mention that only partic-
ular visual patches are related to the text. As in the
examples shown in Figure 1, the correct results can
be easily obtained by only tracking the visual infor-
mation in the bounding boxes. Therefore, discrimi-
nating key visual objects from the irrelevant ones
could lead to improved learning of visual informa-
tion. 2)Crucial visual information that relates to
the sarcastic cues of text modality may be scattered
in an image (Figure 1 (b)). As such, it is essential
to focus on drawing the intricate sentiment connec-
tions between text and image modalities, allowing
a good exploitation of the contradictory sentiment
information between modalities for learning sarcas-
tic clues.
To this end, we propose a novel cross-modal
graph convolutional networks (CMGCN) by con-
structing a cross-modal graph for each instance,
where the important visual information and the
related textual tokens are explicitly linked. This
allows for the extraction of incongruous implica-
tions between two modalities in sarcasm detec-
tion. Concretely, instead of trying to produce a
caption of the whole image, we first detect the
objects of the image to capture the important vi-
sual regions and the corresponding attribute-object
pairs via the approach proposed by Anderson et al.
(2018). Then, we explore a novel solution to assign
weights to the edges of the cross-modal graph by
means of computing the word similarities between
theobject descriptors of the attribute-object pairs
and textual words based on the WordNet (Miller,
1992). Further, to introduce the multi-modal sen-
timent relations into the cross-modal graphs, in-
spired by (Lou et al., 2021), we devise a modulat-ing factor of sentiment relation for each edge by
retrieving the affective weights of attribute descrip-
tors (usually adjectives with affective information)
and textual words from external affective knowl-
edge (SenticNet (Cambria et al., 2020)). As such,
the modulating factors can be adopted to refine the
edge weights of word similarities, allowing the cap-
ture of sentiment incongruities of the cross-modal
nodes in the graph. Further, in the light of cross-
modal graphs, we deploy a GCN architecture to
make sense of the incongruous relations across the
modalities for multi-modal sarcasm detection.
The main contributions of our work are summa-
rized as follows:
•To the best of our knowledge, we are the
first to explore the use of the graph model
based on auxiliary object detection for model-
ing the contradictory sentiments between key
textual and visual information in multi-modal
sarcasm detection.
•Using the attribute-object pairs of the image
objects as the bridge, a novel approach of con-
structing cross-modal graphs is developed to
explicitly link the two modalities by edges
with the varying degree of importance.
•A series of experiments on a publicly avail-
able multi-modal sarcasm detection bench-
mark dataset show that our proposed method
achieves the state-of-the-art performance.
2 Related Work
2.1 Multi-modal Sarcasm Detection
Previous work of sarcasm detection has been ap-
plied to textual utterances information (Zhang et al.,
2016; Tay et al., 2018; Babanejad et al., 2020). Dif-
ferent from text-based sarcasm detection, multi-
modal sarcasm detection aims to identify the sar-
castic expression among different modalities (Schi-
fanella et al., 2016; Castro et al., 2019). Schifanella
et al. (2016) firstly tackled the multi-modal sarcasm
detection task with text and image modalities by
manually designed features. Cai et al. (2019) cre-
ated a new dataset and proposed a hierarchical fu-
sion model for multi-modal sarcasm detection. Xu
et al. (2020) explored decomposition and relation
network to model both cross-modality contrast and
semantic association in sarcasm detection. Pan et al.
(2020) proposed inter-modality attention and co-
attention to learn the contradiction of sarcasm. For1768
the graph-based methods, Liang et al. (2021a) de-
ployed a heterogeneous graph structure to learn
the sarcastic features from both intra- and inter-
modality perspectives. However, this method tried
to grasp the visual information of the whole image,
and meanwhile ignore the sentiment expression
between different modalities. Therefore, differ-
ent from (Liang et al., 2021a), we explore a novel
cross-modal GCN model based on the important vi-
sual information and sentiment cues to leverage the
inconsistent implications between different modal-
ities and thus improve the performance of multi-
modal sarcasm detection.
2.2 Graph Neural Networks
Models based on graph neural networks (GNN), in-
cluding graph convolutional network (GCN) (Kipf
and Welling, 2017) and graph attention network
(GAT) (Velickovic et al., 2018), have achieved
promising performance in many recent research
studies, such as visual representation learning (Wu
et al., 2019; Xie et al., 2021), text representa-
tion learning (Yao et al., 2019; Lou et al., 2021;
Liang et al., 2021b, 2022), and recommendation
systems (Ying et al., 2018; Tan et al., 2020). Fur-
ther, there are also some research studies explored
graph models to deal with the multi-modal tasks,
such as multi-modal sentiment detection (Yang
et al., 2021), multi-modal named entity recogni-
tion (Zhang et al., 2021), cross-modal video mo-
ment retrieval (Zeng et al., 2021), multi-modal neu-ral machine translation (Yin et al., 2020), and multi-
modal sarcasm detection (Liang et al., 2021a).
3 Methodology
In this section, we describe our proposed Cross-
Modal Graph Convolutional Networks ( CMGCN )
model for multi-modal sarcasm detection in details.
As demonstrated in Figure 2, the architecture of the
proposed CMGCN contains four main components:
1)Text-modality representation , which employs
the pre-trained uncased BERT-base model (De-
vlin et al., 2019) as the text encoder to capture
the hidden representation of the text-modality; 2)
Image-modality representation , which deploys the
pre-trained Vision Transformer (ViT) (Dosovit-
skiy et al., 2021) as the image encoder to capture
the hidden representation of the image-modality
with respect to each bounding box (visual region);
3)Cross-modal graph , which constructs a cross-
modal graph for each multi-modal example based
on the external affective knowledge source and the
hidden representations of text and image modali-
ties; 4) Multi-modal fusion , which fuses the repre-
sentations from image and text modalities to cap-
ture the sarcastic features by means of a GCN struc-
ture and an attention mechanism.
3.1 Text-modality Representation
For text processing, given a sequence of words
s={w},nis the length of the text s. We
first adopt the pre-trained uncased BERT-base1769model (Devlin et al., 2019) to map each word w
into a d-dimensional embedding:
X= [x,x,···,x] = BERT( [CLS] s[SEP] )
(1)
Where Xis the embedding matrix of the input
text. Here, the representations of tokens [CLS]
and[SEP] are not utilized in constructing the
cross-modal graph. Subsequently, to unify the
dimensions of representations between different
modalities and capture the sequential relations of
the context, we utilize a bidirectional LSTM (Bi-
LSTM) to learn the text-modality representation of
the input text:
T={t,t,···,t}= Bi-LSTM( X)(2)
Where t∈Rdenotes the hidden state vec-
tor at time step jfrom the bidirectional LSTM,
ddenotes the dimensionality of the text-modality
hidden state representation.
3.2 Image-modality Representation
For image processing, given an image I, we
first adopt a trained toolkit proposed by Ander-
son et al. (2018) to derive a series of bounding
boxes (objects) paired with their attribute-object
pairs. For each visual region of the bounding box
I∈R, following (Xu et al., 2020), we first
resize it to 224×224, i.e.L=L=L= 224 .
Subsequently, following (Dosovitskiy et al., 2021),
we reshape the region I∈Rinto a sequence
I={p∈R}, where r=p×pis the
number of patches. Then, we flatten and map each
patch to a d-dimensional vector with a trainable
linear projection: z=pE.
For each sequence of image patches, a
[class] token embedding z∈Ris
prepended for the sequence of embedded patches,
and position embeddings are added to the patch
embeddings to retain positional information. The
input of each visual region Iis represented as:
Z= [z;z;z;···;z] +E (3)
Where Z∈Ris the input matrix of the
image patches, and E∈Ris the posi-
tion embedding matrix. Then, we feed the input
matrix Zinto the ViT encoder to acquire the rep-
resentation hof visual region I:
H= ViT( Z),h=H (4)We use the representation of the [class] token
embedding to represent the visual region. Finally,
the representation of the image Iis defined as:
X={h,h,···,h} (5)
Where mis the number of visual regions.
Subsequently, we employ a trainable Linear Pro-
jection to map each vto a2d-dimensional vector:
V={v,v,···,v}=XW(6)
Where W∈Ris a trainable parameter.
3.3 Cross-modal Graph
In this section, we describe how to construct a
cross-modal graph. To leverage the relations be-
tween multi-modal features, we employ a graph
structure to link the textual words with the associ-
ated image objects. Here, the nodes of the cross-
modal graph are the representations of text and im-
age modalities. Many GCN-based approaches have
demonstrated that the weights of the edges are cru-
cial in graph information aggregation (Liang et al.,
2021b; Yang et al., 2021; Lou et al., 2021). As
such, constructing a cross-modal graph boils down
to the setting of the edge weights in the graph.
To this end, we explore a novel approach of set-
ting the weights based on both word similarities
and affective clues between textual words and the
attribute-object pairs of the image regions, and the
dependency tree of the text-modality. The adja-
cency matrix A∈Rof the cross-
modal graph is defined as:
A=

1 ifDandi < n ,j < n
κ ifi < n ,j≥n
0 otherwise(7)
κ=Sim(w, o)×ξ+ 1 (8)
ξ=γ× |ω(w)−ω(a)|(9)
Where Dindicates that there is a relation be-
tween wandwin the dependency tree of the
sentence. Sim(·)represents the computation of
word similarity. We set Sim(·) = 0 if the re-
turn value is None .ξis a modulating factor
refers to the sentiment relation (sentiment incon-
gruity) between an image region and a text token.
ω(w)∈[−1,1]represents the affective weight of1770word wretrieved from SenticNet (Cambria et al.,
2020). We set ω(w) = 0 ifwcannot be found in
SenticNet. |·|represents absolute value calculation.
aandorespectively denote the attribute and the
object of the bounding box j. Inspired by Kipf
and Welling (2017), we construct the cross-modal
graph as an undirected graph, A=A, and set
a self-loop for each node, A= 1.
The intention of the cross-modal graph construc-
tion (Equations 7 and 9) is that: 1) As in the exam-
ples shown in Figure 1, the sarcastic information of
text-modality may be expressed by multiple words,
such as “ wonderful weather ”. Therefore, we in-
corporate the syntax-aware relations over the de-
pendency tree of the sentence into the cross-modal
graph to advance the learning of the contextual
dependencies. 2) We devise a coefficient κ,
which is associated with the affective weights, to
modulate the influence of contrary sentiment re-
lations. Here, γ >1is a tuned hyper-parameter
to regulate the bias of inconsistent sentiment rela-
tions. That is, if the polarities of ω(w)andω(a)
are opposite, the value of γis boosted, otherwise
the value is shrunk. Especially, the greater the af-
fective weights, the higher the confidence that the
value of γis boosted or shrunk. 3) We add 1to
the cross-modal edges to pay more attention to the
cross-modal nodes aggregation.
3.4 Multi-modal Fusion
For each instance, we explore a graph architecture
to extract the crucial sarcastic clues by aggregating
the correlation of nodes in the cross-modal graph.
Concretely, we feed the adjacency matrix of the
cross-modal graph Aand the corresponding nodes’
representations Rof each multi-modal example
into a multi-layers GCNs architecture to derive the
graph representation. For each graph convolutional
operation, each node in the l-th GCN layer is up-
dated according to the hidden representations of its
neighborhoods according to the adjacency matrices
of the cross-modal graph, which is defined as:
G= ReLU( ˜AGW+b) (10)
Where ˜A=DADis the normalized sym-
metric adjacency matrix. Dis the degree matrix
ofA, where D=PA.Gis the hid-
den graph representation evolved from the pre-
ceding GCN layer. W∈R,b∈Rare the trainable parameters of the l-th GCN
layer. The nodes input of the first GCN layer
are the concatenation of text-modality and image-
modality representations: G=R. Here, R=
{r,r,···,r}={t,···,t,v,···,v}.
Subsequently, inspired by (Zhang et al., 2019),
we employ a retrieval-based attention mechanism
to capture the graph-oriented attention information
from the concatenation of text and image repre-
sentations R={r,r,···,r}by means of
the graph representation gderived from the final
GCN layer. The intention is to retrieve crucially
associated cross-modal features that are explicitly
connected in the cross-modal graph. The attention
weights are computed as:
α=exp(β)Pexp(β), β=Xrg(11)
Where Cdenotes a set of indices in which nodes
contain cross-modal edges in the graph. ⊤repre-
sents the matrix transposition. The final sarcastic
representation is defined as:
f=Xαr (12)
Then, the final sarcastic representation is fed into
a fully-connected layer with a softmax function to
capture a probability distribution ˆy∈Rin the
sarcasm decision space:
ˆy= softmax( Wf+b) (13)
Where dis the dimensionality of sarcasm labels.
W∈Randb∈Rare trainable param-
eters.
3.5 Learning Objective
We minimize the cross-entropy loss via the stan-
dard gradient descent algorithm to train the model:
minL=−XXylogˆy+λ||Θ||(14)
where Nis the training data size. yandˆyre-
spectively represent the ground-truth and estimated
label distribution of instance i.Θdenotes all train-
able parameters of the model, λrepresents the co-
efficient of L-regularization.1771
4 Experimental Setup
4.1 Dataset
We conduct experiments on a publicly available
multi-modal sarcasm detection benchmark dataset
collected by Cai et al. (2019). This dataset contains
English tweets expressing sarcasm asPositive ex-
amples and those expressing non-sarcasm asNega-
tiveexamples. Each example in the dataset consists
of a text and an associated image. The statistics of
the dataset are shown in Table 1.
4.2 Experimental Settings
For a fair comparison, the data preprocessing fol-
lows (Cai et al., 2019). We set the maximum num-
ber of visual regions as 10 for object detection re-
sults. That is, we select the top 10 bounding boxes
with highest scores if the objects are greater than 10.
We utilize the pre-trained uncased BERT-base (De-
vlin et al., 2019) module to embed each word of
text-modality as a 768-dimensional embedding and
employ the pre-trained ViT(Dosovitskiy et al.,
2021) to embed each visual region patch as a 768-
dimensional embedding, i.e. d=d= 768 . The
resolution of visual region patch is set to L= 32 ,
correspondingly, p= 7, r= 49 .The number of
GCN layers is set to 2, which is the optimal depth
in the pilot experiments. The dimensionality of
hidden representations is set to d= 512 . The
coefficient λis set to 0.00001 . Adam is utilized
as the optimizer with a learning rate of 0.00002 ,
and the mini-batch size is 32. The dropout rate
with 0.1is utilized to avoid overfitting. We use
early-stopping with patience of 5. We set γ= 3
to compute the modulating factor of incongruous
multi-modal sentiment relations, which is the opti-
mal hyper-parameter in the pilot experiments.
Following (Cai et al., 2019), we use Accuracy ,
Precision ,Recall , and F1-score to measure the
model performance. Since the label distribution
of the dataset is imbalanced, following (Pan et al.,2020), we also report Macro-average results. The
experimental results of our models are averaged
over 10 runs with different random seeds to ensure
the final reported results are statistically stable.
4.3 Comparison Models
We compare our proposed CMGCN model with a
series of strong baselines, summarized as follow:
1) Image-modality methods : These models use
only visual information for sarcasm detection, in-
cluding Image (Cai et al., 2019), which employs
ResNet (He et al., 2016) to train a sarcasm clas-
sifier; and ViT (Dosovitskiy et al., 2021), which
utilizes the ‘ [class] ’ token representation of the
pre-trained ViT to detect the sarcasm.
2) Text-modality methods : These models use only
textual information, including TextCNN (Kim,
2014), a deep learning model based on CNN
for text classification; Bi-LSTM , a bidirectional
LSTM network for text classification; SIARN (Tay
et al., 2018), adopting inner-attention for textual
sarcasm detection; SMSD (Xiong et al., 2019), ex-
ploring a self-matching network to capture textual
incongruity information; and BERT (Devlin et al.,
2019), the vanilla pre-trained uncased BERT-base
taking ‘ [CLS] text [SEP] ’ as input.
3) Multi-modal methods : These models take both
text- and image-modality information. Including
HFM (Cai et al., 2019), a hierarchical multimodal
features fusion model for multi-modal sarcasm de-
tection; D&R Net (Xu et al., 2020), a Decompo-
sition and Relation Network modeling both cross-
modality contrast and semantic association; Res-
BERT (Pan et al., 2020), concatenating image fea-
tures and BERT-based text features for sarcasm
prediction; Att-BERT (Pan et al., 2020), explor-
ing an inter-modality attention and a co-attention
to model the incongruity of multi-modal sarcasm
detection; and InCrossMGs (Liang et al., 2021a),
a graph-based model to leverage the sarcastic rela-
tions from both intra- and inter-modal perspectives.
We also explore several variants of CMGCN
to analyze the impact of different components in
the ablation study: 1) w/o Gdenotes without cross-
modal graph, which only concatenates the represen-
tations of ‘ [class] ’ and ‘ [CLS] ’ tokens from
ViT and BERT for sarcasm detection; 2) w/o O
denotes without object detection. The whole im-
age is input into the image encoder, and the edge
weights are set to 1 in the cross-modal graphs; 3)
w/oSdenotes without using external knowledge.1772
All weights of edges are set to 1 in the cross-modal
graph. Further, 4) w/o Srepresents without using
affective knowledge; 5) w/o Ddenotes without us-
ing syntax-aware information of text-modality in
graph construction.
Further, to investigate the effectiveness of our
CMGCN when used with different pre-trained
models, we also set the following variants:
1) -GloVe+ResNet : We replace BERT with
GloVe (Pennington et al., 2014) to initialize each
word into a 300-dimensional embedding and ViT
with ResNet-152 (He et al., 2016) to embed each
image patch as a 2048-dimensional vector.
2) -GloVe+ViT : We use GloVe as text encoder and
use ViT as image encoder.
3) -BERT+ResNet : We use BERT as text encoder
and use ResNet-152 as image encoder.
5 Experimental Results
5.1 Main Results
We report the comparison results regarding Text-
modality ,Image-modality , and Text+Image modal-
ities in Table 2. From the results, we can draw the
following conclusions. 1)Our proposed CMGCN
outperforms existing baselines across all metrics.
This verifies the effectiveness of our proposed
model in multi-modal sarcasm detection. 2)We
conduct significance tests of our CMGCN over
the baseline models, the results show that our
CMGCN significantly outperforms the baseline
models in terms of most of the evaluation met-
rics (with p−value <0.05).3)Our CMGCN
model performs consistently better than the pre-
vious graph-based method (InCrossMGs), which
demonstrates that recognizing significant visual re-
gions and modeling sentiment relations can lead
to improved performance. 4)The methods based
on text modality achieve consistently better perfor-
mance than the methods based on image modality,
which shows that the expression of sarcastic/non-
sarcastic information primarily resides in the text
modality. 5)Methods based on both image and
text modalities perform better than the unimodal
baselines overall. This implies that leveraging the
information of both image and text modalities is
more effective for multi-modal sarcasm detection.
6)The results of macro metrics are better than other
commonly used metrics overall, which indicates
that models perform better in the “ negative ” class
due to the imbalanced class distribution.
5.2 Ablation Study
To analyze the impact of different components of
our proposed CMGCN , we conduct an ablation
study and report the results in Table 3. Note that
removal of cross-modal graph (w/o G) sharply de-
grades the performance, which verifies the signif-
icance of cross-modal in multi-modal features fu-
sion for learning sarcastic expressions in multi-
modal sarcasm detection. Removal of object de-1773
tection (w/o O) leads to considerable performance
degradation, which demonstrates that adopting ob-
ject detection to track important visual informa-
tion is effective for constructing crucial relations
between visual and textual information in the cross-
modal graphs. From the results of w/o SandS,
we conclude that exploiting the attribute-object pair
as a bridge to set edge weights based on word sim-
ilarity is effective when constructing cross-modal
graphs. Further, leveraging affective clues to cap-
ture multi-modal sentiment incongruity between
text- and image-modality is effective in sarcasm
detection, and thus leads to improved performance.
In addition, removal of syntax-aware information
of text-modality leads to slight performance degra-
dation, which indicates that incorporating syntactic
information in the graph makes better learning of
dependency relations of textual words and thus im-
proves the performance of sarcasm detection.
5.3 Generalizability of Cross-modal Graph
To investigate the generalizability and effectiveness
of our proposed cross-modal graph when used with
different pre-trained methods, we conduct experi-
ments with five variants of our proposed CMGCN
by using different text and image encoders. The ex-
perimental results are shown in Figure 3 (a). Note
that the proposed cross-modal graph can directly
work with various pre-trained models and performs
consistently better than that without cross-modal
graph (w/o G). This demonstrates the generalizabil-
ity and effectiveness of our proposed cross-modal
graph in multi-modal sarcasm detection. Further,
from the results, we can also conclude that superior
performance is obtained when using more powerful
pre-trained methods, such as ViT and BERT.
5.4 Impact of GCN Layers
In this section, we analyze the impact of the num-
ber of GCN layers on the performance of our pro-
posed CMGCN . We vary the layer number from
1 to 6 and report the results in Figure 3 (b). Note
that the 2-layer GCN architecture performs better
than others overall, and thus the number of GCN
layers is set to 2 in our model. Model with one
layer performs worse, which indicates that a shal-
low graph network structure is not able to learn
sarcastic features well. When the number of layers
is greater than 2, the performance tends to decline.
This shows that further increasing the number of
layers beyond 2 degrades the model performance
possibly due to the sharp increase of parameters.
5.5 Visualization
To qualitatively investigate how the proposed
CMGCN works in multi-modal sarcasm detection,
we present a visualization of cross-modal graph
construction and attention values of a multi-modal
sarcasm example. The results are shown in Fig-
ure 4. We first show a sarcasm example and its
corresponding object detection results in Figure 4
(a). Note that the correct label of this example can
be easily inferred if the relations of crucial sarcas-
tic clues of text (marked by the light red color)
and the corresponding visual regions are captured
by the model. To demonstrate how the proposed
CMGCN identifies the important sarcastic clues,
we show the adjacency matrix of the cross-modal
graph of this example in Figure 4 (b). Note that
highly correlated sarcastic clues in different modal-
ities are connected by edges with large weights in
the graph. This verifies the effectiveness of the pro-
posed cross-modal graph in learning multi-modal
sarcastic information. Further, based on the cross-1774modal graph, we show the attention visualization
of this example in Figure 4 (c). The crucial textual
tokens and the related image regions are highly
attended by our proposed CMGCN , which helps
identify the incongruity among the learned impor-
tant features for learning sarcastic expressions and
thus leads to improved performance of multi-modal
sarcasm detection.
6 Conclusion and Future Work
This paper has proposed a novel cross-modal graph
architecture for multi-modal sarcasm detection, in
which the crucial visual regions can be explicitly
connected to the highly correlated textual tokens
for learning the incongruity sentiment of sarcastic
expression. Specifically, unlike previous research
efforts that simply consider the visual information
of the whole image, we attempt to recognize the im-
portant visual regions via object detection results,
and further devise a novel cross-modal graph to ex-
plicitly establish the connections of scattered visual
regions and the associated textual tokens. More
concretely, owing to the object detection results,
theattribute-object pair descriptors of the objects
are served as a bridge to track the highly related
sarcastic cues between image and text modalities
and their connection weights, and then deriving the
cross-modal graphs based on external knowledge
bases. Afterwards, a GCNs architecture based on a
retrieval-based attention mechanism is employed
to capture the key incongruity sentiment expres-
sions across different modalities for multi-modal
sarcasm detection. To the best of our knowledge,
it is the first study of utilizing a cross-modal graph
to extract intricate multi-modal sarcastic relations
via object detection and sentiment cues from exter-
nal knowledge bases. Extensive experiments on a
public benchmark dataset show that our proposed
approach significantly outperforms state-of-the-art
baseline methods.
As described in Section 3.3, the weights of edges
in the cross-modal graph are computed based on
both word similarities and affective clues between
textual words and the attribute-object pairs of the
image regions, and the dependency tree of the text-
modality. The approach can be easily generalized
to other sentiment-related multi-modal learning
scenarios. Nevertheless, the cross-graph solution
might not be generalized well to other multi-modal
tasks or data genres, if there is a lack of affec-
tive knowledge or a difficulty in deriving depen-dency trees in low-resource settings. Therefore, fu-
ture research can consider exploiting alternatively
approaches to automatically learn the weights of
edges in the cross-modal graph without relying on
external knowledge sources.
Acknowledgments
This work was partially supported by the National
Natural Science Foundation of China (61876053,
62006062, 62176076, 62006060), UK Engineer-
ing and Physical Sciences Research Council (grant
no. EP/V048597/1, EP/T017112/1), Natural Sci-
ence Foundation of Guangdong Province of China
(No. 2019A1515011705), Shenzhen Foundational
Research Funding (JCYJ20200109113441941,
JCYJ20210324115614039), Shenzhen Science
and Technology Innovation Program (Grant No.
KQTD20190929172835662), Joint Lab of Lab of
HITSZ and China Merchants Securities. Yulan He
is supported by a Turing AI Fellowship funded by
the UK Research and Innovation (UKRI) (grant no.
EP/V020579/1).
References177517761777