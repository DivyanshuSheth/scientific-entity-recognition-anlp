
Paul Röttger,Debora Nozza,Federico Bianchi, and Dirk HovyUniversity of OxfordBocconi UniversityStanford University
Abstract
Hate speech is a global phenomenon, but most
hate speech datasets so far focus on English-
language content. This hinders the develop-
ment of more effective hate speech detection
models in hundreds of languages spoken by bil-
lions across the world. More data is needed, but
annotating hateful content is expensive, time-
consuming and potentially harmful to annota-
tors. To mitigate these issues, we explore data-
efficient strategies for expanding hate speech
detection into under-resourced languages. In
a series of experiments with mono- and mul-
tilingual models across five non-English lan-
guages, we find that 1) a small amount of
target-language fine-tuning data is needed to
achieve strong performance, 2) the benefits of
using more such data decrease exponentially,
and 3) initial fine-tuning on readily-available
English data can partially substitute target-
language data and improve model generalis-
ability. Based on these findings, we formulate
actionable recommendations for hate speech
detection in low-resource language settings.
Content warning : This article contains illus-
trative examples of hateful language.
1 Introduction
Hate speech is a global phenomenon, but most hate
speech datasets so far focus on English-language
content (Vidgen and Derczynski, 2020; Poletto
et al., 2021). This hinders the development of effec-
tive models for detecting hate speech in other lan-
guages. As a consequence, billions of non-English
speakers across the world are less protected against
online hate, and even giant social media platforms
have clear language gaps in their content modera-
tion systems (Simonite, 2021; Marinescu, 2021).
Zero-shot cross-lingual transfer, where large
multilingual language models are fine-tuned on
one source language and then applied to another
target language, may appear like a potential so-
lution to the issue of language-specific resourceFigure 1: Overview of our experimental setup. We
use ISO 639-1 codes to denote the different languages.
MHC is Multilingual HateCheck (Röttger et al., 2022).
scarcity. However, while this approach performs
well on some tasks (Conneau et al., 2020; Barbieri
et al., 2022), it fails on many others (Lauscher et al.,
2020; Hu et al., 2020). For hate speech detection
in particular, cross-lingual performance in zero-
shot settings is lacking (Stappen et al., 2020; Leite
et al., 2020). For example, zero-shot cross-lingual
transfer cannot account for language-specific taboo
expressions that play a key role in classification
(Nozza, 2021). Conversely, hate speech detection
models trained or fine-tuned directly on the target
language, i.e. in few- and many-shot settings, are
consistently found to perform best (Aluru et al.,
2020; Pelicon et al., 2021).
So,how do we build hate speech detection
models for hundreds more languages? We need
at least some labelled data in the target language to
make models effective, but data annotation is diffi-
cult, time-consuming and expensive. It requires re-
sources that are often very limited for non-English
languages. Annotating hateful content in particular
also risks exposing annotators to harm in the pro-
cess (Vidgen et al., 2019; Derczynski et al., 2022).5674In this article, we explore strategies for hate
speech detection in under-resourced languages that
make efficient use of labelled data, to build effective
models while also minimising annotation cost and
risk of harm to annotators. For this purpose, we
conduct a series of experiments using mono- and
multilingual models fine-tuned on differently-sized
random samples of labelled hate speech data in En-
glish as well as Arabic, Spanish, Hindi, Portuguese
and Italian. Our key findings are:
1.A small amount of labelled target-language
data is needed to achieve strong performance
on held-out test sets.
2.The benefits of using more such data decrease
exponentially.
3.Initial fine-tuning on readily-available English
data can partially substitute target-language
data and improve model generalisability.
Based on these findings, we formulate and dis-
cuss five recommendations for expanding hate
speech detection into under-resourced languages:
1. Collect and label target-language data.
2. Start by labelling a small set, then iterate.
3.Use diverse data collection methods to in-
crease marginal benefits of annotation.
4.Use multilingual models for unlocking
readily-available data in high-resource lan-
guages for initial fine-tuning.
5.Evaluate out-of-domain performance to reveal
potential weaknesses in generalisability.
With these recommendations, we hope to facili-
tate the development of new hate speech detection
models for yet-unserved languages.
Definition of Hate Speech Definitions of hate
speech vary across cultural and legal settings. Fol-
lowing Röttger et al. (2021), we define hate speech
asabuse that is targeted at a protected group or
at its members for being a part of that group . Pro-
tected groups are based on characteristics such as
gender identity, race or religion, which broadly
reflects Western legal consensus, particularly the
US 1964 Civil Rights Act, the UK’s 2010 Equal-
ity Act and the EU’s Charter of Fundamental
Rights. Based on these definitions, we approach
hate speech detection as the binary classification of
content as either hateful or non-hateful.2 Experiments
All our experiments follow the setup described in
Figure 1. We start by loading a pre-trained mono-
or multilingual transformer model for sequence
classification. For multilingual models, there is
an optional first phase of fine-tuning on English
data. This is to simulate using readily-available
data from a high-resource language that is not the
target language. For all models, there is then a sec-
ond phase of fine-tuning on differently-sized ran-
dom samples of data in the target language. This
is to simulate using scarce data from an under-
resourced language. Finally, all models are evalu-
ated on the held-out test set corresponding to the
target-language dataset they were fine-tuned on as
well as the target-language test suite from Multilin-
gual HateCheck (Röttger et al., 2022).
2.1 Data
For all our experiments, we use hate speech
datasets from hatespeechdata.com, which was first
introduced by Vidgen and Derczynski (2020) and
is now the largest public repository of datasets an-
notated for hate, abuse and offensive language. At
the time of our review in May 2022, the site listed
53 English datasets as well as 57 datasets in 24
other languages. From these datasets, we select
those for our experiments that a) contain explicit
labels for hate, and b) use a definition of hate for
data annotation that aligns with our own (§1).
2.1.1 Fine-Tuning 1: English
For the optional first phase of fine-tuning in
English, we use one of three English datasets.
D21_Eby Vidgen et al. (2021) contains
41,255 entries, of which 53.9% are labelled as hate-
ful. The entries were hand-crafted by annotators
to be challenging to hate speech detection mod-
els, using the Dynabench platform (Kiela et al.,
2021). F18_Eby Founta et al. (2018) con-
tains 99,996 tweets, of which 4.97% are labelled as
hateful. K20_Eby Kennedy et al. (2020) con-
tains 39,565 comments from Youtube, Twitter and
Reddit, of which 29.31% are labelled as hateful.
From each of these three English datasets, we
sample 20,000 entries for the optional first phase
of fine-tuning, plus another 500 entries for develop-
ment and 2,000 for testing. To align the proportion
of hate across English datasets, we use random
sampling for D21_E, while for K20_E
andF18_Ewe retain all hateful entries and5675then sample from non-hateful entries, so that the
proportion of hate in the two datasets increases to
50.0% and 22.0%, respectively.
2.1.2 Fine-Tuning 2: Target Language
For fine-tuning in the target language, we use one
of five datasets in five different target languages.
B19_Ecompiled by Basile et al. (2019) for
SemEval 2019 contains 4,950 Spanish tweets, of
which 41.5% are labelled as hateful. F19_P
by Fortuna et al. (2019) contains 5,670 Portuguese
tweets, of which 31.5% are labelled as hateful.
H21_Hcompiled by Modha et al. (2021) for
HASOC 2021 contains 4,594 Hindi tweets, of
which 12.3% are labelled as hateful. O19_A
by Ousidhoum et al. (2019) contains 3,353 Ara-
bic tweets, of which 22.5% are labelled as hateful.
S20_Icompiled by Sanguinetti et al. (2020)
for EvalIta 2020 contains 8,100 Italian tweets, of
which 41.8% are labelled as hateful.
From each of these five target-language datasets,
we randomly sample differently-sized subsets for
target-language fine-tuning. Like in English, we
set aside 500 entries for development and 2,000
for testing.From the remaining data, we sample
subsets in 12 different sizes – 10, 20, 30, 40, 50,
100, 200, 300, 400, 500, 1,000 and 2,000 entries –
so that we are able to evaluate the effects of using
more or less labelled data within and across differ-
ent orders of magnitude.Zhao et al. (2021) show
that there can be large sampling effects when fine-
tuning on small amounts of data. To mitigate this
issue, we use 10 different random seeds for each
sample size, so that in total we have 120 different
samples in each language, and 600 samples across
the five non-English languages.
2.2 Models
Multilingual Models We fine-tune and evaluate
XLM-T (Barbieri et al., 2022), an XLM-R model
(Conneau et al., 2020) pre-trained on an additional
198 million Twitter posts in over 30 languages.
XLM-R is a widely-used architecture for multilin-
gual language modelling, which has been shown to
achieve near state-of-the-art performance on multi-
lingual hate speech detection (Banerjee et al., 2021;
Modha et al., 2021). We chose XLM-T because
it strongly outperformed XLM-R across our target
language test sets in initial experiments.Monolingual Models For each of the five target
languages, we also fine-tune and evaluate a mono-
lingual transformer model from HuggingFace. For
Spanish, we use RoBERTuito (Pérez et al., 2021).
For Portuguese, we use BERTimbau (Souza et al.,
2020). For Hindi, we use Hindi BERT. For Arabic,
we use AraBERT v2 (Antoun et al., 2020). For Ital-
ian, we use UmBERTo. Details on model training
can be found in Appendix A.
Model Notation We denote all models by an ad-
ditive code. The first part is either M for a monolin-
gual model or X for XLM-T. For XLM-T, the sec-
ond part of the code is DE,FEorKE, for mod-
els fine-tuned on 20,000 entries from D21_E,
F18_EorK20_E. For all models, the
final part of the code is E,P,H,AorI, cor-
responding to the target language that the model
was finetuned on. For example, M+Idenotes the
monolingual Italian model, UmBERTo, fine-tuned
onS20_I, and X+KE+Adenotes an XLM-
T model fine-tuned first on 20,000 English entries
from K20_Eand then on O19_A.
2.3 Evaluation Setup
Held-Out Test Sets + MHC We test all mod-
els on the held-out test sets corresponding to
their target-language fine-tuning data, to evaluate
their in-domain performance (§2.4). For exam-
ple, we test X+KE+I, which was fine-tuned on
S20_Idata, on the S20_Itest set. Addi-
tionally, we test all models on the matching target-
language test suite from Multilingual HateCheck
(MHC). MHC is a collection of around 3,000 test
cases for different kinds of hate as well as chal-
lenging non-hate in each of ten different languages
(Röttger et al., 2022). We use MHC to evaluate
out-of-domain generalisability (§2.5).
Evaluation Metrics We use macro F1 to evalu-
ate model performance because most of our test
sets as well as MHC are imbalanced. To give con-
text for interpreting performance, we show baseline
model results in all figures: macro F1 for always
predicting the hateful class ("always hate"), for
never predicting the hateful class ("never hate")
and for predicting both classes with equal proba-
bility ("50/50"). We also show bootstrapped 95%
confidence intervals around the average macro F1,
which is calculated across the 10 random seeds for
each sample size. These confidence intervals are
expected to be wider for models fine-tuned on less
data because of larger sampling effects.5676
2.4 Testing on Held-Out Test Sets
When evaluating our mono- and multilingual mod-
els on their corresponding target-language test sets,
we find a set of consistent patterns in model per-
formance. We visualise overall performance in
Figure 2 and highlight key data points in Table 1.
First, there is an enormous benefit from even
very small amounts of target-language fine-tuning
data N. Model performance increases sharply
across models and held-out test sets up to
around N=200. For example, the performance of
X+DE+Eincreases from around 0.63 at N=10
to 0.70 at N=50, to 0.75 at N=200.
On the other hand, larger amounts of target-
language fine-tuning data correspond to much less
of an improvement in model performance. Across
models and held-out test sets, there is a steep de-
crease in the marginal benefits of increasing N.
M+I, for example, improves by 0.33 macro F1
from N=20 to N=200, and by just 0.06 from N=200
to N=2,000. X+Pimproves by 0.25 macro F1from N=20 to N=200, and by just 0.06 from N=200
to N=2,000. We analyse these decreasing marginal
benefits using linear regression in §2.6.
Further, there is a clear benefit to a first phase of
fine-tuning on English data, when there is limited
target-language data. Absolute performance dif-
fers across test sets, but multilingual models with
initial fine-tuning on English data (i.e. X+DE,
X+FEandX+KE) perform substantially better
than those without (i.e. M and X), up to around
N=200. At N=20, there is up to 0.26 macro F1
difference between the former and the latter. Con-
versely, models without initial fine-tuning on En-
glish data need substantially more target-language
data to achieve the same performance. For exam-
ple, X+DE+Pat N=100 performs as well as
M+Pat N=300 on F19_P.
Relatedly, there are clear differences in model
performance based on which English dataset was
used in the first fine-tuning phase, when there is
limited target-language data. Among the three En-5677glish datasets we evaluated, X+DEperforms best
for up to around N=200 on B19_E, whereas
X+FEwins out for the other four test sets.
After the strong initial divergence for low
amounts of target-language fine-tuning data, perfor-
mance tends to align across models, regardless of
whether they were first fine-tuned on English data
or only fine-tuned on the target language. From
around N=200 onwards, for a given test set, all
models we evaluate have broadly comparable per-
formance. For example, all models score 0.71 to
0.72 macro F1 on F19_Pfor N=1,000.
Finally, we find that if there is some divergence
in model performance on the held-out test sets
for larger amounts of target-language fine-tuning
data, then monolingual models tend to outperform
multilingual models. For example, despite the
disadvantage of monolingual models for N ≤200,
we find that M+Eachieves 0.84 macro F1 on
B19_Efor N=2,000, compared 0.82 macro F1
forX+DE+E. This difference is visible acrosstest sets and significant at 95% confidence for the
Spanish, Arabic and Italian test sets.
2.5 Testing on MHC
Performance on the target-language test suites from
MHC, which none of the models saw during fine-
tuning, differs strongly and systematically from
results on the held-out test sets (Figure 3). Key
data points are highlighted in Table 2.
First, as for the held-out test sets, there is an
initial benefit to a first phase of fine-tuning on En-
glish data, up to around N=500 for MHC. How-
ever, there are clear performance differences de-
pending on which English dataset was used. Mod-
els fine-tuned on D21_Eperform best by far
across MHC for any N. On Italian HateCheck for
example, X+DEachieves 0.79 macro F1 at N=20
compared to 0.55 for X+FE+Iand 0.29 for the
monolingual model M+I.
Second, for Spanish, Portuguese and Italian,
there is little to no benefit to fine-tuning on more5678target-language data, as measured by performance
on MHC. Only those models that were not first fine-
tuned on English data (i.e. M and X) are improved
by fine-tuning on more target-language data, up to
around N=500. None of the models benefit from
even larger N. On Portuguese HateCheck for exam-
ple,X+K+Pscores 0.62 macro F1 at N=500,
N=1,000 and N=2,000.
Third, for Hindi and Arabic, training on more
target-language data actively harms performance
on MHC. For larger N, all models degrade towards
only predicting the non-hateful class. On Arabic
HateCheck, for example, X+FE+Aachieves
0.26 macro F1 at N=2,000, which is only slightly
better than the “never hate” baseline at 0.23 macro
F1. This is despite the exact same model scoring
0.70 macro F1 on O19_Aat N=2,000. We
further discuss this finding in §3.
Lastly model performance on MHC is much
more volatile across random seeds compared to
performance on the held-out test sets. This can be
seen from the wider confidence intervals in Fig-
ure 3 compared to Figure 2.
2.6 Regression Analysis
When evaluating models on their corresponding
held-out test sets, we saw decreasing benefits
to adding more target-language fine-tuning data
(§2.4). To quantify this observation and evidence
its significance, we fit ordinary least squares (OLS)
linear regressions for each model on each test set.
OLS is commonly used in economics and the social
sciences to analyse empirical relationships between
an outcome variable and one or more regressor vari-
ables. In our case, the outcome is model perfor-
mance as measured by macro F1 and the regressor
is the amount of target-language fine-tuning data
N. Since the relationship between the two variables
is clearly non-linear and plausibly exponential, we
regress macro F1 on the natural logarithm of N
instead of just N, so that our regression equation
for a given random seed itakes the form of
F=β+βln(N) +ε
where Fis the observed macro F1 and εthe er-
ror term. β, the intercept, can be interpreted as the
expected zero-shot macro F1, i.e. at N=0. β, the
slope, can be interpreted as the expected increase
in macro F1 from increasing the amount of target-
language fine-tuning data N by 1%, due to our
log-linear regression setup. In addition to reportingregression coefficients, we also report adjusted R
as a goodness-of-fit measure, which is calculated as
the percentage of variance in macro F1 explained
by our regression. Lastly, we report the expected
percentage-point increase in macro F1 from dou-
bling N, denoted as 2N, which is calculated as
βln(2). Since we saw that variance in macro F1
changes with N, we use heteroskedasticity-robust
HC3 standard errors (Hayes and Cai, 2007).
We find that the natural logarithm of the amount
of target-language fine-tuning data N provides a
strikingly good explanation for model performance
as measured by macro F1 (Table 3). Rfor our re-
gressions, where N is the sole explanatory variable,
is generally very high, up to 91.97%, which indi-
cates near-perfect goodness-of-fit. This strongly
suggests that there is an exponential decrease in the
benefits of using more target-language fine-tuning
data. Conversely, there is a constant benefit to
doubling the amount of target-language fine-tuning
data: We expect the same increase in macro F1
whether we increase N ten-fold from 10 to 20 or
from 1,000 to 2,000.
Further, we can confirm our earlier findings
that a) initial fine-tuning on English data improves
performance for low N, while b) all models per-
form similarly well for larger N. In our regression,
this is visible in monolingual M models having
much lower estimated zero-shot performance β5679paired with much larger βand 2Neffects com-
pared to the X+DEmodels. For example, dou-
bling the amount of Spanish fine-tuning data for M
(β=0.29) corresponds to an expected increase in
macro F1 of 5.49 points on B19_E, whereas
forX+DE(β=0.55) we only expect an increase
of 2.52 points.
3 Discussion and Recommendations
Based on the sum of our experimental results,
we formulate five recommendations for the data-
efficient development of effective hate speech de-
tection models for under-resourced languages.
Recommendation 1
Collect and label target-language data.
We found enormous benefits from fine-tuning mod-
els on even small amounts of target-language data.
Zero-shot cross-lingual transfer performs worse
than naive baselines, but few shots go a long way.
Recommendation 2
Do not annotate all the data you collect right
away. Instead, start small and iterate.
We found that the benefits of using more target-
language fine-tuning data decrease exponentially
(Table 3), which suggests that the data in each
dataset is not diverse enough to warrant full an-
notation. The datasets we used in our experiments
contained several thousand entries, but most of the
useful signal in them can be learned from just a
few dozen or hundred randomly-sampled entries.
Thus, most of the annotation cost and the potential
harm to annotators in compiling these datasets cre-
ated very little performance benefit. Assuming con-
stant cost per additional annotation, improving F1
by just one percentage point grows exponentially
more expensive. Our results suggest that dataset
creators should at first annotate smaller subsets of
the data they collected and iteratively evaluate what
marginal benefits additional annotations are likely
to bring compared to their costs.
Recommendation 3
Use diverse data collection strategies to min-
imise redundant data annotation.We do not recommend against annotating target-
language data. Instead, we recommend against an-
notating too much data that was collected using the
same method. The H21_Hdataset, for exam-
ple, was collected using trending hashtags related
to Covid and Indian politics (Modha et al., 2021),
and Ousidhoum et al. (2019) used topical keywords
related to Islamic inter-sect hate for O19_A.
As a consequence, these datasets necessarily have
a narrow topical focus as well as limited linguis-
tic diversity. This kind of selection bias has been
confirmed for several other hate speech datasets
(Wiegand et al., 2019; Ousidhoum et al., 2020).
MHC, on the other hand, contains diverse state-
ments against seven different target groups in each
language (Röttger et al., 2022). Our results show
that very quickly additional entries from a given
dataset carry little new and useful information (Fig-
ure 2), and that fine-tuning on particularly narrow
datasets like H21_HandO19_Acan even
harm model generalisability as measured by MHC
(Figure 3). Conversely, more diverse strategies for
data collection, such as sampling from multiple
platforms, sampling from a diverse set of users, or
using a wider variety of topical keywords, are likely
to result in data that is more worth annotating.
Recommendation 4
Use multilingual models to unlock readily-
available data in high-resource languages for
initial fine-tuning.
We found that an initial phase of fine-tuning on
English data increases model performance on held-
out test sets when there is little target-language
fine-tuning data (Figure 2). Thus, English data
can partly substitute target-language data in few-
shot settings. It also substantially improves out-
of-domain generalisability to the MHC test suites
(Figure 3), with benefits varying by which English
dataset is used. Only multilingual models can ac-
cess these benefits, because they can be fine-tuned
on languages other than just the target language. In
our experiments, we used only three of the many
English hate speech datasets that have been pub-
lished to date. Of them, we use only one at a time,
and only a subset of the full data. Future work
could explore the benefits from initial fine-tuning
on larger and more diverse datasets, or a combina-
tion of datasets in English and other languages that
may be more available than the target language.5680Recommendation 5
Do not rely on just one held-out test set to eval-
uate model performance in the target language.
Monolingual models appear strong when evaluated
on held-out test sets, and sometimes even outper-
form multilingual models when there is relatively
much target-language fine-tuning data (Figure 2).
However, this hides clear weaknesses in their gener-
alisability as measured by MHC (Figure 3). These
findings suggest that monolingual models are more
at risk of overfitting to dataset-specific features
than multilingual models fine-tuned first on data
from another language. In higher-resource settings,
monolingual models are still likely to outperform
multilingual models (Martin et al., 2020; Nozza
et al., 2020; Armengol-Estapé et al., 2021).
4 Related Work
Cross-Lingual Transfer Learning Only a very
small number of the over 7,000 languages in the
world are well-represented in natural language pro-
cessing resources (Joshi et al., 2020). This has mo-
tivated much research into cross-lingual transfer,
where large multilingual language models are fine-
tuned on one (usually high-resource) source lan-
guage and then applied to another (usually under-
resourced) target language. Cross-lingual transfer
has been systematically evaluated for a wide range
of NLP tasks and for varying amounts of shots , i.e.
target-language training data. Evidence on the ef-
ficacy of zero-shot transfer is mixed, across tasks
and for transfer between different language pairs
(Artetxe and Schwenk, 2019; Pires et al., 2019; Wu
and Dredze, 2019; Lin et al., 2019; Hu et al., 2020;
Liang et al., 2020; Ruder et al., 2021, inter alia).
In comparison, few-shot approaches are generally
found to be more effective (Lauscher et al., 2020;
Hedderich et al., 2020; Zhao et al., 2021; Hung
et al., 2022). In many-shot settings, multilingual
models are often outperformed by their language-
specific monolingual counterparts (Martin et al.,
2020; Nozza et al., 2020; Armengol-Estapé et al.,
2021). In this paper, we confirmed and expanded
on key results from this body of prior work, such as
the effectiveness of few-shot learning, specifically
for the task of hate speech detection. We also intro-
duced OLS linear regression as an effective method
for quantifying data efficiency, i.e. dynamic cost-
benefit trade-offs in data annotation.Cross-Lingual Hate Speech Detection Re-
search on cross-lingual hate speech detection has
generally had a more narrow scope than ours.
Nozza et al. (2020) demonstrate weaknesses of
zero-shot transfer between English, Italian and
Spanish. Leite et al. (2020) find similar results
for Brazilian Portuguese, Bigoulaeva et al. (2021)
for German, and Stappen et al. (2020) for Spanish,
while also providing initial evidence for decreas-
ing returns to fine-tuning on more hate speech data
(see also Aluru et al., 2020). Pelicon et al. (2021)
and Ahn et al. (2020), like Stappen et al. (2020),
find that intermediate training on other languages
is effective when little target-language data is avail-
able. By comparison, we covered a wider range
of languages, with two datasets per language and
a consistent definition of hate across datasets. We
provided a more systematic evaluation of marginal
benefits to additional target-language fine-tuning
data based on OLS linear regression, as well as a
first assessment of out-of-domain performance us-
ing the recently-released Multilingual HateCheck
test suites. We are also the first to formulate con-
crete recommendations for expanding hate speech
detection into more under-resourced languages.
5 Conclusion
In this paper, we explored strategies for hate speech
detection in under-resourced language settings that
are effective while also minimising annotation cost
and risk of harm to annotators.
We conducted a series of experiments using
mono- and multilingual language models fine-
tuned on differently-sized random samples of la-
belled hate speech data in English as well as Ara-
bic, Spanish, Hindi, Portuguese and Italian. We
evaluated all models on held-out test sets as well
as out-of-domain target-language test suites from
Multilingual HateCheck, and used OLS linear re-
gression to quantify the marginal benefits of target-
language fine-tuning data. In our experiments, we
found that 1) a small amount of target-language
fine-tuning data is needed to achieve strong per-
formance on held-out test sets, 2) the benefits of
using more such data decrease exponentially, and
3) initial fine-tuning on readily-available English
data can partially substitute target-language data
and improve model generalisability. Based on our
findings, we formulated five actionable recommen-
dations for expanding hate speech detection into
under-resourced languages.5681Most hate speech research and resources so far
have focused on English-language content. How-
ever, hate speech is a global phenomenon, and hate
speech detection models are needed in hundreds
more languages. With our findings and recommen-
dations, we hope that we can facilitate the devel-
opment of models for yet-unserved languages and
thus help to close language gaps that right now
leave billions of non-English speakers world less
protected against online hate.
Acknowledgments
This article was completed during Paul Röttger’s
2022 research visit to Dirk Hovy’s research group
at Bocconi University in Milan. Paul Röttger was
funded by the German Academic Scholarship Foun-
dation. Debora Nozza received funding from Fon-
dazione Cariplo (grant No. 2020-4288, MONICA).
Federico Bianchi was a member of the Bocconi
Institute for Data Science and Analysis (BIDSA)
when parts of this work were completed. Dirk
Hovy received funding from the European Re-
search Council (ERC) under the European Union’s
Horizon 2020 research and innovation program
(grant agreement No. 949944). Debora Nozza and
Dirk Hovy are members of the Data and Market-
ing Insights (DMI) Unit of the Bocconi Institute
for Data Science and Analysis (BIDSA). We thank
the MilaNLP Lab and the Pierrehumbert Language
Modelling Group for their helpful comments, and
all reviewers for their constructive feedback.
Limitations
Our analysis is necessarily constrained by the avail-
ability of suitable hate speech datasets, particularly
for non-English languages. The datasets we used
are idiosyncratic, not just because their language
differs but also because they were collected using
different methods at different times. This is why we
focus on analysing general patterns across datasets
in different languages, rather than attempting like-
for-like comparisons between individual languages.
As new datasets are created, future work could ex-
pand our analysis to cover more languages as well
as more datasets within each language.
Further, our experiments and the recommenda-
tions we formulate based on their results presup-pose the availability of at least some models tai-
lored to the target language. Multilingual mod-
els like XLM-R and XLM-T are respectively pre-
trained on 100 and on 30 languages (Conneau et al.,
2020; Barbieri et al., 2022), and there are active
efforts to expand language model coverage to hun-
dreds of more languages (Wang et al., 2022). How-
ever, even the most multilingual models available
today cover only a fraction of the world’s languages.
Similarly, there is a growing number of monolin-
gual transformer models for different languages
(Nozza et al., 2020) but most languages are miss-
ing such a model.
Also, we focus on varying amounts of fine-
tuning data while assuming the existence of held-
out development and test sets to evaluate our ex-
periments. However large or small these sets are,
labelling them also creates costs and risks of harm
to annotators. This needs to be factored into cost
assessments for expanding hate speech detection
into an under-resourced language.
For reasons of scope and prioritisation, we did
not explore the full range of possibilities for im-
proving cross-lingual model performance. For ex-
ample, we chose English data for the initial fine-
tuning because it is most readily available, but En-
glish may not be the optimal choice (Lin et al.,
2019). Further, prior work on hate speech detec-
tion has found some benefits to data augmentation
based on machine translation (Pamungkas et al.,
2021; Wang and Banko, 2021) or semi-supervised
learning (Zia et al., 2022), which could be explored
in future research.
Lastly, our definition of hate speech (§1) is based
on a Western legal consensus, in which the pres-
ence of hate speech hinges on the target being
a protected group. This definition is consistent
throughout the datasets we use for our experiments.
However, which groups are considered protected
may differ across legal and cultural settings. The
United Arab Emirates, for example, do not con-
sider sexual orientation a protected characteristic
(UAE, 2022). To avoid Eurocentrism, potential
differences across legal and cultural settings need
to be accounted for when expanding hate speech
detection into new languages and when creating
new hate speech datasets.
Ethical Considerations
Language Representation Besides English, our
experiments cover a limited set of five languages:5682Arabic, Spanish, Hindi, Portuguese and Italian.
This is due to the very scarcity of suitable non-
English datasets for hate speech detection that our
work seeks to address. While certainly under-
resourced and under-represented in the context of
hate speech detection, the five non-English lan-
guages we consider are widely spoken, and they
have received at least some attention in hate speech
research. There is a clear need for creating hate
speech datasets in even more under-resourced lan-
guages, to facilitate the creation of hate speech
detection models in these languages and also to
enable even more comprehensive analyses of the
kind we conducted here.
Biases in Multilingual Models Zhao et al.
(2020) show that multilingual models exhibit simi-
lar biases as their monolingual counterparts. There-
fore, there is a risk that the models we trained, es-
pecially those trained on fewer target-language en-
tries, reflect the biases present in their training and
fine-tuning data. Our analyses focused on general
model efficacy rather than bias evaluation. Future
work could for example make use of Multilingual
HateCheck to evaluate model performance across
different targeted groups.
Environmental Impact We fine-tuned 3,000
models for our experiments: five model types for
each of five target languages with 12 differently-
sized samples for 10 random seeds in each lan-
guage. However, each model fine-tuning step was
very computationally inexpensive, especially be-
cause we used very small amounts of fine-tuning
data. We were able to run all our experimentation
on the University of Oxford’s Advanced Research
Computing cloud CPUs. Fine-tuning 120 models
for a given language took only around four hours
for monolingual models and around seven hours
for XLM-T. Relative to the concerns raised around
the environmental costs of pre-training large lan-
guage models (Strubell et al., 2019; Henderson
et al., 2020; Bender et al., 2021), or even larger-
scale fine-tuning with hyperparameter tuning, we
therefore consider the environmental costs of our
work to be relatively minor. Further, our findings
enable the training more effective models with less
data and may thus reduce the environmental impact
of future work.References5683568456855686
A Details on Model Training
Parameters We implemented XLM-T as well
as the five language-specific monolingual models
(§2.2) using the HuggingFace transformers li-
brary (Wolf et al., 2020). Training batch size was
16. Maximum sequence length was 128 tokens.
Otherwise, we used default parameters. The op-
tional first phase of fine-tuning on English data
was for three epochs. Fine-tuning on the target
language was for five epochs, with subsequent best
epoch selection based on macro F1 on the held-out
development set.
Computation We ran all experiments on our in-
stitution’s 16-core cloud CPUs. Fine-tuning 120
models for a one of the five target languages took
around four hours for monolingual models and
around seven hours for XLM-T.
B Macro F1 on Held-Out Test Sets
For larger versions of the graphs in Figure 2 from
the main body, see Figure 4 on the next page. See
Table 5 for a version of Table 1 across all N.
C Macro F1 on MHC
For larger versions of the graphs in Figure 3 from
the main body, see Figure 5 below. Similarly, see
Table 6 for a version of Table 2 across all N.
D Regression Results for All Models
See Table 4 to the right.56875688568956905691