
Jan Philip Wahle, Terry Ruas, Frederic Kirstein, Bela GippGeorg-August-Universität Göttingen, GermanyMercedes-Benz Group AG, Germanywahle@gipplab.org
Abstract
The recent success of large language models
for text generation poses a severe threat to aca-
demic integrity, as plagiarists can generate re-
alistic paraphrases indistinguishable from orig-
inal work. However, the role of large autore-
gressive transformers in generating machine-
paraphrased plagiarism and their detection is
still developing in the literature. This work ex-
plores T5 and GPT-3 for machine-paraphrase
generation on scientific articles from arXiv, stu-
dent theses, and Wikipedia. We evaluate the
detection performance of six automated solu-
tions and one commercial plagiarism detec-
tion software and perform a human study with
105 participants regarding their detection per-
formance and the quality of generated exam-
ples. Our results suggest that large models can
rewrite text humans have difficulty identify-
ing as machine-paraphrased (53% mean acc.).
Human experts rate the quality of paraphrases
generated by GPT-3 as high as original texts
(clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).
The best-performing detection model (GPT-
3) achieves a 66% F1-score in detecting para-
phrases. We make our code, data, and findings
publicly available for research purposes.
1 Introduction
Paraphrases are texts that convey the same mean-
ing while using different words or sentence struc-
tures (Bhagat and Hovy, 2013). Paraphrasing plays
an important role in related language understand-
ing problems (e.g., question answering (McCann
et al., 2018), summarization (Rush et al., 2015)),
but it can also be misused for academic plagiarism.
Academic plagiarism is serious misconduct as its
perpetrators can unjustly advance their careers, ob-
tain research funding that could be better spent,
and make science less reliable if their misbehavior
remains undetected (Meuschke, 2021).
Table 1: Example excerpt from a Wikipedia article and
its paraphrased versions using GPT-3. Important key-
words are highlighted in boldfont and color. Autoregres-
sive paraphrasing with GPT-3 keeps the same message
while generating text with the original structure. The
original example used is 3747-ORIG-44.txt .
Paraphrasing tools can be used to generate convinc-
ing plagiarized texts with minimum effort. Most
of these tools (e.g., SpinBot, SpinnerChief) use
relatively rudimentary heuristics, such as word re-
placements with synonyms, and they already de-
ceive plagiarism detection software (Wahle et al.,
2022a). However, these tools scratch the surface of
the possibilities compared to what large neural lan-
guage models can achieve in producing convincing
high-quality paraphrases (Zhou and Bhat, 2021).
Notably, large autoregressive language models with
billions of parameters, such as GPT-3 (Brown et al.,
2020), make paraphrase plagiarism effortless yet
exceedingly difficult to spot.
So far, large language models have found little ap-952plication in plagiarism detection. As language mod-
els are already easily accessible for applications
such as software developmentor accounting, us-
ing language models for machine-paraphrasing will
become as easy as a click of a button soon. There-
fore, the number of machine-plagiarized texts will
increase dramatically in the upcoming years. To
counteract this problem, we need robust solutions
before models are widely misused.
In this study, we generate machine-paraphrased text
with GPT-3 and T5 (Raffel et al., 2020) to compose
a dataset for testing against automatically generated
paraphrasing. We test different configurations of
model size, training schemes, and selection criteria
for generating paraphrases. To understand how hu-
mans perceive machine-paraphrased text, we also
performed an extensive study with 105 participants
assessing their detection performance and quality-
of-text assessments against existing automated de-
tection methods. We show that while humans can
spot paraphrasing of online tools and smaller au-
toencoding models, large autoregressive models
prove to be a more complex challenge as they can
generate human-like text containing the same key
ideas and messages from their original counterparts
(see Table 1 for an example). Popular paid plagia-
rism detection software (e.g. PlagScan, Turnitin)
is already deceived by rudimentary paraphrasing
methods and large language models make this task
even more challenging. We also test the models
used for the generation, which show the highest
performance in detecting machine-paraphrased pla-
giarism.
To summarize our contributions:
•We present a dataset with machine-
paraphrased text from T5 and GPT-3 based
on original work from Wikipedia, arXiv,
and student theses to train and evaluate
machine-paraphrased plagiarism.
•We explore the human ability to detect para-
phrase through three experiments, focusing
on (1) the detection difficulty of paraphras-
ing methods, (2) the quality of examples, and
(3) the accuracy of humans in distinguishing
between paraphrased and original texts.•We empirically test plagiarism detection soft-
ware (i.e., PlagScan) against machine learn-
ing methods and neural language models (au-
toencoding and autoregressive) in detecting
machine-paraphrased plagiarism.
•We show that paraphrases from GPT-3 pro-
vide the most realistic plagiarism cases that
both humans and automated detection solu-
tions fail to spot, while the model itself is
the best-tested candidate for detecting para-
phrases.
2 Related Work
Plagiarism Detection: Plagiarism describes the
use of ideas, concepts, words, or structures without
proper source acknowledgment (Meuschke, 2021).
Plagiarism datasets are limited to the number of
real plagiarism cases known. With the recent
success of artificial intelligence in natural lan-
guage processing (NLP) applications, paraphrase
generation and plagiarism detection methods
increasingly rely on dense text representations and
machine learning classifiers (Foltýnek et al., 2019).
Machine learning methods often fail to detect
substantial paraphrasing from neural language
models (Wahle et al., 2021). In particular, large
autoregressive language models (e.g., GPT-3)
can generate paraphrased content almost indis-
tinguishable from original work (Witteveen and
Andrews, 2019). However, these models are still
insufficiently explored in the domain of plagiarism
detection, even though their impact on the field is
already being discussed (Dehouche, 2021).
Machine-Paraphrase Detection: Machine-
paraphrasing can be described as the automatic
generation of text that is semantically close to its
source and written in other words (Bhagat and
Hovy, 2013). Machine-paraphrasing experiences a
growing research interest from NLP for learning
semantic representations and related applications
(Rush et al., 2015; McCann et al., 2018). However,
paraphrasing can be used in plagiarism detection to
deceive humans and thus needs detection solutions
to prevent it (Foltýnek et al., 2019).
Lexical substitution is a common paraphrase mech-
anism used by plagiarists (Barrón-Cedeño et al.,
2013). Many online paraphrasing tools also use
synonym replacements and other lexical perturba-
tions to paraphrase text automatically (Foltýnek953et al., 2020a). (Foltýnek et al., 2020b) showed that
machine-learning classifiers (e.g., Support Vector
Machine) could easily detect paraphrasing from
popular online paraphrasing tools such as SpinBot.
(Wahle et al., 2021) proposed a benchmark with
paraphrased examples from autoencoding models
(e.g., BERT(Devlin et al., 2019), RoBERTa(Liu
et al., 2019)), showing that neural language models
can generate more challenging paraphrasing than
traditional online tools (e.g., SpinnerChief, Spin-
Bot). In a follow-up study, (Wahle et al., 2022a)
evaluate neural language models (e.g., BERT) on
paraphrased texts from SpinnerChief, another in-
dependent paid online paraphrasing tool. Their
main finding was that neural language models out-
perform machine learning techniques and can ob-
tain super-human performance in all test cases.
(Foltýnek et al., 2020b; Wahle et al., 2021, 2022a)
results show that synonym replacements are simple
to detect with state-of-the-art neural language mod-
els. However, none of these studies explore using
large autoregressive models in their experiments.
So far, only a few studies have analyzed the im-
pacts of plagiarism using autoregressive models.
Seq2Seq models were first used by (Prakash et al.,
2016) with stacked residual LSTM networks to gen-
erate paraphrases. (Witteveen and Andrews, 2019)
train GPT-2 to generate paraphrased versions of a
source text and select paraphrased candidates with
the highest similarity according to universal sen-
tence encoder(Cer et al., 2018) embeddings and
low word overlap when compared to their original
counterparts. (Biderman and Raff, 2022) show that
GPT-J (Wang and Komatsuzaki, 2021), a smaller
version of GPT-3 with six billion parameters, can
plagiarize student programming assignments that
are not detected by MOSS, a popular plagiarism
detection tool. The scaling of models allows for the
generation of text indistinguishable from human
writing (Brown et al., 2020). In addition, the mod-
els’ increase in size and consequentially their per-
formance (Kaplan et al., 2020) have the potential
to make the paraphrase detection task even more
difficult.
3 Methodology
This study focuses on understanding how hu-
mans and machines perceive large autoregressive
machine-generated paraphrase examples. There-fore, we first generate machine-paraphrased text
with different model sizes of GPT-3 and T5. We
then generate a dataset composed of 200,000 exam-
ples from arXiv (20,966), Wikipedia (39,241), and
student graduation theses (5,226) using the best
configuration of both models.
We investigate how humans and existing detection
solutions perceive this newly automated form of
plagiarism. In our human experiments, we compare
paraphrased texts generated in this study to existing
data that use paid online paraphrasing tools and
autoencoding language models to paraphrase their
texts. Finally, we evaluate commercial plagiarism
detection software, machine-learning classifiers,
and neural language model-based approaches to
the machine-paraphrase detection task.
3.1 Paraphrase Generation
Method: We generate candidate versions of para-
graphs using prompts and human paraphrases as
examples in a few-shot style prediction (Table 2).
We provide the model with the maximum number
of human paraphrased examples that fit its context
window with a maximum of 2048 tokens total. For
both models, we use their default configuration.
Paraphrasing models’ goal is to mimic human para-
phrases. Instead of manually engineering suitable
prompts for the task, we use AutoPrompt (Shin
et al., 2020) to determine task instructions based on
the model’s gradients. As suggested by the authors,
we place the predict-token at the end of our prompt.
One example of a generated prompt was “Rephrase
the following sentence.” As humans tend to shorten
text when paraphrasing, we limit the maximum
number of generated tokens concerning the origi-
nal version to 90%, which is the approximate ratio
of human plagiarism fragments in (Barrón-Cedeño
et al., 2013). Table 2 provides an example of the
model’s input/output when generating paraphrases.
Candidate Selection: Paraphrases that are similar
to their source are of limited value as they have
repetitive patterns, while those with high linguistic
diversity often make models more robust (Qian
et al., 2019). The quality of paraphrases is typically
evaluated using three dimensions of quality (i.e.,
clarity, coherence, and fluency), where high-quality
paraphrases are those with high semantic similarity
and high lexical and syntactic diversity (McCarthy
et al., 2009; Zhou and Bhat, 2021). We aim to
choose high-quality examples semantically close to954
the original content without reusing the exact words
and structures (Witteveen and Andrews, 2019).
In this paper, we choose generated candidates that
maximize their semantic similarity against their
original counterparts while minimizing their count-
based similarity. We select the Pareto-optimal can-
didate that minimizes ROUGE-L and BLEU (i.e.,
penalizing the exact usage of words compared to
the original version) and maximizes BERTScore
(Zhang et al., 2019) and BARTScore(Yuan et al.,
2021) (i.e., encouraging a similar meaning com-
pared to the original version). Table 3 provides an
example for generated paraphrases and their scores.
While examples with high count-based similarity
usually convey the same essential message (e.g.,
Out 1 andOut 2 ), they also share a similar sentence
structure and word usage. Examples with high se-
mantic similarity and lower count-based similarity
(e.g., Out 3 ) state the same meaning but rephrase
the sentence with novel structure and similar words
describing the same idea.
Dataset Creation: To provide data for common
sources of academic plagiarism (i.e., scientific arti-
cles), we paraphrase the original examples of the
machine paraphrase corpus (MPC) (Wahle et al.,
2022a) which is mainly composed of publications
on arXiv, Wikipedia, and student’s graduation the-
ses. As human-authored examples, we sample
equally from two of the most popular paraphrase
datasets, i.e., P4P and PPDB 2.0 (Zhou and Bhat,
2021). The P4P database (Barrón-Cedeño et al.,
2013) is composed of realistic plagiarism cases
with the paraphrase phenomena they contain (e.g.,morphology-based, syntax-based, lexicon-based),
and the PPDB 2.0 database (Pavlick et al., 2015)
is a large-scale paraphrase corpus extracted with
bilingual pivoting from which we extract the high-
quality phrasal and lexical subsets.
3.2 Human Evaluation
Our human study aims to understand how partic-
ipants perceive machine-paraphrased plagiarism
compared to original work and human-paraphrased
text. We used Amazon’s Mechanical Turk (AMT)
service to obtain human assessments for para-
phrased text classification. Additionally, we asked
experts that actively published in the plagiarism
detection domain over the past five years. To have
adequate statistical power in our analyses (Card
et al., 2020), we included a total of 105 participants
(see Appendix A.1 for details on demographic in-
formation about participants).
In the first part of the human study (Q2 in Sec-
tion 4), 50 participants are provided with a mutually
exclusive choice of whether a text was machine-
paraphrased or original and a text field to justify
their reasoning. In the second part (Q3 in Sec-
tion 4), 50 participants from AMT and five experts
from the research community were provided with
a mutually exclusive choice of 5 points on a Likert
scale for each of the three parameters of clarity,
fluency, and coherence. For the first experiment,
each participant evaluated five texts for five models
resulting in 1,250 text evaluations. For the second
experiment, each participant evaluated ten texts for
three parameters, totaling 1,340 text evaluations.
Following common best practices on AMT (Berin-
sky et al., 2012), evaluators had to have over a
95% acceptance rate, be in the United States, and
have completed over 1,000 successful tasks. We
excluded evaluators’ assessments if their explana-
tions were directly copied text from the task (> 90%
text match), did not match their classification, or
were short, vague, or otherwise non-interpretable.
Across experiments, 138 assessments ( ≈10%) were
rejected and not included in the experiments.
4 Research Questions & Experiments
Q1: How does model size influence the quality of
generated paraphrases?
A. We ask this question to underline the problem’s
urgency as recently released models have a large
number of parameters. Figure 1 shows the influ-955
ence of model size on the similarity scores of gen-
erated candidates against their original candidates
on 500 random examples from the PPDB dataset.
We test the 220M, 770M, 3B, and 11B versions of
T5 and the 350M, 1.3B, 6.7B, and 175B versions
of GPT-3 (also known as Ada, Babbage, Curie,
and Davinci in the OpenAI APIrespectively).
With the increasing number of parameters, both
models’ semantic similarity scores (BERTScore,
BARTScore) also rise. T5 shows the highest in-
crease when extending the model from 3 billion
parameters to 11 billion. GPT-3 (175B) reaches
its overall highest semantic similarity, generating
sentences with similar meanings compared to the
source. Model’s generated candidates also have
higher count-based scores on average as they often
repeat text from the source. As described before,
we try to sample candidates with low word-count
scores to avoid repetition of words.
We conclude that scaling models’ size positively in-
fluences their performance at the task of paraphras-
ing, which agrees with previous research (Kaplan
et al., 2020). While the limits and details of scaling
models are still unknown, boosting their computing
power will allow for more human-like texts to be
produced.
Q2. Can humans identify whether a text is original,
or machine-paraphrased?
A. This question is inspired by the Turing (1950)
Test to differentiate machines from humans. To
answer this question, we asked participants to as-
sess whether texts were machine-generated (see
Appendix A.3 for more details). We compared
original work to an online paraphrasing tool
(SpinnerChief), two auto-encoding models (BERT,
RoBERTa), and two large auto-regressive mod-
els (T5, GPT-3). As examples, we sampled 30
machine-generated paragraphs for each model and
their corresponding 30 original texts with an equal
weighting between the three sources (Wikipedia,
arXiv, and student theses). We performed a
Bonferroni-corrected two-sided T-Test to test for
statistical significance compared to a control model.
As the control model, we chose SpinnerChief with
its default paraphrasing frequency as it was the
most difficult-to-detect online paraphrasing tool
tested in (Wahle et al., 2022a). Participants re-
ceived individual text examples with three annota-
tion options: “machine-paraphrased”, “original”,
and “I don’t know”. Participants were not shown
aligned examples (i.e., an original and its para-
phrased version) to avoid memorization effects.
Table 4 shows the mean human accuracy (i.e.,
the ratio of correct assignments to non-neutral as-
signments per participant) in detecting machine-
paraphrased text. The results show that humans can956adequately detect the control model with 82% accu-
racy on average (where 50% is a chance level per-
formance). In contrast, human accuracy at detect-
ing paraphrases produced by autoencoding models
was significantly lower, ranging from 61% to 71%
over all participants. Plagiarism cases generated
by large autoregressive models were usually hardly
above chance (53% for GPT-3 and 56% for T5).
For more information on the annotator agreement,
please see Appendix A.2. Human abilities to detect
machine-paraphrased text appear to decrease with
increasing model size and are particularly challeng-
ing for autoregressive models as they can change
sentence structure and word order instead of single
word replacements. Our findings on human detec-
tion against autoregressive models corroborate with
recent results (Clark et al., 2021), challenging the
common choice of humans as the gold standard.
Q3. How similar are machine-generated para-
phrases to human-paraphrases?
A. We sampled 500 examples pairs (i.e., orig-
inal, human-paraphrased) from the PPDB cor-
pus and paraphrased half of the original ver-
sions with GPT-3 (175B) and the other half
with T5 (11B). As a proxy for similarity be-
tween originals, human-paraphrased, and machine-
paraphrased examples, we calculated their similar-
ity using BERTScore. The average BERTScore
between human-paraphrases and originals (76%)
is lower than between machine-generated para-
phrases and originals (79%). The similarity be-
tween human-paraphrases and machine-generated
paraphrases is highest (81%). This result suggest
that machine-generated paraphrases are typically
closer to the human paraphrases than to the original,
which we assume is due to the model’s objective
to mimic human behavior, which are provided as
generation examples.
Q4. How do humans assess the quality of machine-
paraphrased plagiarism?
A. We asked human annotators to score gener-
ated paraphrases according to their clarity, fluency,
and coherence (Zhou and Bhat, 2021) (see Ap-
pendix A.3 for more details about the questions).
As quality assessments are challenging to evalu-
ate, we increased the requirements for participants.
We asked the second group of 50 participants that
required to have a higher education degree (bach-
elor’s, master’s, or Ph.D. degree). We also asked
additional five experts that have published at leasttwo peer-reviews papers on plagiarism detection in
the last five years. Each participant annotated ten
randomly drawn examples on a Likert scale from 1
to 5 regarding clarity, fluency, and coherence (Zhou
and Bhat, 2021).
Table 5 shows the average rating for all 55 partici-
pants While original contents achieve the highest
rating for all three dimensions, the largest version
of GPT-3 achieves similar ratings. SpinnerChief’s
quality of paraphrases is significantly lower. BERT
achieves convincing results as well, also because
the frequency of word changes (15%) for synonyms
is lower than SpinnerChief’s (50%), and therefore
generates examples closer to the original text.
Fluency was rated highest for all models, while clar-
ity and coherence were the lowest. We assume that
as source sentences come from diverse scientific
fields, they might already be difficult to understand;
thus, paraphrasing can confuse readers when tech-
nical terms are used wrong. For more information
on annotator agreement and the relation between
experts and their educational degree, please see
Appendix A.2.
Q5. How do existing detection methods identify
paraphrased plagiarism?
A. To test the detection performance of automated
plagiarism detection solutions, we evaluate five
methods and compare them to random guesses and
a human baseline. We presume automated detec-
tion solutions can identify paraphrases better than
humans as (Ippolito et al., 2020) showed that large
language models are optimized to fool humans at
the expense of introducing statistical anomalies
which automated solutions can spot. As a de-facto
solution for plagiarism detection, we test PlagScan,
one of the best-performing systems, in a compre-
hensive test conducted by the European Network
for Academic Integrity (Foltýnek et al., 2020a).
We test a combination of naïve bayes classifier and
word2vec (Mikolov et al., 2013), and three autoen-
coding transformers: BERT (Devlin et al., 2019),
RoBERTa(Liu et al., 2019), and Longformer (Belt-
agy et al., 2020) which are the best performing
models in machine-paraphrase detection of (Wahle
et al., 2021, 2022a). Additionally, we evaluate the
largest versions of T5 and GPT-3 using few-shot
prediction.
As paraphrasing models, we choose SpinnerChief;
the best performing paid online paraphrasing tool957Mean accuracy95% Confidence
Interval (low, hi)tcompared to
control ( p-value)“I don ´t know"
assignments
SpinnerChief (Control) 82% 76%-89% - 2.8 %
BERT 67% 63%–71% 14.2 (1 e-11) 4.9%
RoBERTa 65% 61%–70% 18.1 (1 e-29) 5.5%
T5 11B 56% 51%–59% 16.6 (1 e-16) 7.1%
GPT-3 175B 53% 49%–55% 19.2 (1e-34) 7.2%
tested in (Wahle et al., 2022a). Spinnerchief at-
tempts to change every fourth word with a syn-
onym. We use BERT as an autoencoding baseline
and set the masking probability to 15% as in (Wahle
et al., 2021). As a large autoregressive model, we
use GPT-3 175B, the best model for automated
similarity metrics and deceiving humans.
Table 6 shows the average F1-macro except for
the human baseline, which shows accuracy. For
PlagScan, we assume positive examples when
the text-match is greater than 50%. Looking at
paraphrased plagiarism of SpinnerChief, humans
reach between 79% and 85% accuracy on average.
PlagScan achieves results up to 7% over the ran-
dom baseline for Wikipedia articles but achieves
close to random performance for student theses.
As in (Wahle et al., 2022a), we assume PlagScan
indexes Wikipedia and arXiv but not student the-
ses used in the MPC. Neural approaches based on
naïve bayes reach between 58% and 67% F1-macro
scores while autoencoding models achieve up to
67% - 78% (Longformer). Large autoregressive
models achieve peak scores of 85% (T5 11B) and
87% (GPT-3 175B) on SpinnerChief’s paraphrases.Results of detection models on BERT paraphrasing
show similar patterns to SpinnerChief, as autoen-
coding models also replace masked words with syn-
onyms. While detection results are generally lower
for humans and PlagScan, autoencoding models
improve by a significant margin. As pointed out in
similar studies (Zellers et al., 2019; Wahle et al.,
2021), models generating the paraphrased content
are typically the best to detect it. The similarity
in the architecture of the autoencoding models al-
lows BERT, RoBERTa, and Longformer for the
largest performance increase over SpinnerChief.
Still, large autoregressive models achieve the best
results in detecting machine-paraphrasing of BERT
overall, with over 80% F1-score for GPT-3.
When looking at paraphrasing of GPT-3, all mod-
els detect paraphrases significantly worse. Hu-
mans, plagiarism detection software, and autoen-
coders can hardly achieve better results than ran-
dom chance, which underlines how convincing
paraphrased texts from large autoregressive models
are. T5 and GPT-3 can achieve low, but reasonable
results between 60% - 63% (T5) and 64% - 66%
(GPT-3) F1-macro.
While detection results on large autoregressive
paraphrasing seem low, models were not explic-
itly trained on the task and are predicted based on
previous fine-tuning on other data (upper part) or
not fine-tuning (lower part). We assume GPT-3
is the best detection solution because it generated
the paraphrased texts. Therefore, we see T5 as a
baseline when autoregressive paraphrasing models
are unknown.
In general, neural detection models reach their high-
est performance for Wikipedia articles which we958
assume is due to their pre-training data contain-
ing Wikipedia examples. Student theses pose the
most challenging scenario for both humans and
neural approaches, as it contains challenging ex-
amples and is written by non-native English as a
second language speakers. Across experiments,
PlagScan is not able to reliably identify machine-
paraphrasing. Large autoregressive models make
it challenging for PlagScan to find text matches
as phrasal and lexical substitutions can change the
words with synonyms and the order of words. The
automatic detection results on paraphrasing of GPT-
3 are alarming as many of the most used models fail
to detect its paraphrases. Even though the absolute
results of GPT-3 and T5 are low, they can perform
better than humans at the detection task. Therefore,
we assume that, similar to (Vahtola et al., 2021),
there exist statistical abnormalities and patterns that
automated solutions can leverage to increase their
detection performance.
5 Epilogue
Conclusion: We generated machine-paraphrased
plagiarism using large autoregressive models up to
175 billion parameters convincing paraphrased ex-
amples that deceived humans and plagiarism detec-
tion solutions. We tested the human ability to detect
machine-generated paraphrases of large models
and compared their assessments to well-established
online tools. We evaluated one plagiarism detection
software, one traditional machine-learning model,
three autoencoding, and two large autoregressive
models detecting machine-paraphrased examples.
Despite some limitations, our results suggest that
large language models may increase the numberof automated plagiarism cases through convincing
paraphrasing of original work.
Future Work: This study is an initial step toward
understanding how large language models can fos-
ter illicit activities in the scientific domain. We
plan to further examine the similarities and dif-
ferences between human- and machine-generated
paraphrases to understand whether humans have
difficulties in detecting paraphrases in general.
When looking at participants’ justifications for clas-
sifying machine-generated paraphrases, we plan to
analyze common terms and highlights to find pos-
sible markers for classification decisions. Over the
scope of English, our approach could be applied
to other languages and even generate paraphrases
from one language to another using multilinugal
models and data. Finally, as academic plagiarism
mainly relies on scientific articles, we want to ex-
tend our study to large scientific corpora with high
variation across domains and venues (Lo et al.,
2020; Wahle et al., 2022b).
Limitations
Although our experiments explore how human and
automated solutions struggle to identify machine-
paraphrased examples from large language models,
we did not detail the similarities and differences be-
tween human- and machine-generated paraphrases.
Comparing human paraphrases and machine para-
phrases - qualitatively and automatically - would
allow for a better understanding of what makes
paraphrasing so challenging. As the classification
from our language models currently does not pro-
vide references or sources for their results, these959models can only be used as a support tool to iden-
tify sentences and paragraphs for more detailed
deliberation. While our study has the above lim-
itations, the focus of this study was to underline
the urgency of the problem of machine-generated
plagiarism to promote better detection solutions in
the future.
Ethics Statement
Plagiarism is illegal, unethical, and morally un-
acceptable in all countries (Kumar and Tripathi,
2013). While the binary classification of machine-
paraphrased examples in this study can indicate
how automated detection solutions would point out
potential plagiarism cases, a team of experts should
make a final decision on such cases. False-positive
cases of wrongly accused researchers could ruin
their careers forever. Therefore, all cases should be
carefully evaluated before any final verdict. As this
study and related work show (Clark et al., 2021),
humans are unreliable enough for paraphrase de-
tection in the age of large neural language models.
The difficulty of machine-paraphrase identification
makes legal decisions on plagiarism cases partic-
ularly complex. We presume paraphrasing with
language models will lead to more plagiarists get-
ting unnoticed when using large models to gener-
ate their paraphrases. One exciting approach to
gain transparency would rely on reconstructing the
model’s potential inputs (Tu et al., 2017; Niu et al.,
2019) given the paraphrased version and classify-
ing original candidates using a hybrid approach
considering text-match and semantic features. We
adopted a binary classification in gender for our hu-
man evaluation, which we plan to improve in future
work so it can be more inclusive. Therefore, gender
might not represent the natural diversity included
in our dataset.
References960961
A Human Study
A.1 Demographic Information of Participants
Participants were given a choice to consent to pro-
viding additional anonymous information, includ-
ing - but not limited to - gender, age, nationality,
birth country, current country of residence, first lan-
guage, and current education level. Out of all 105participants, 99 provided demographic information.
For all participants, we received their total number
of completed tasks and the time taken to complete
our questions. The average time to rate ten ex-
amples was 8.07 ( ±6.82) minutes. The average
number of total successful tasks for participants
was 1200 (±590).
The majority of tasks in this study were performed
within 3 - 14 minutes (95% of mass in the interval
of [µ−2σ,µ+2σ]). Three participants took signif-
icantly longer (23, 27, and 43 minutes), and their
ratings were considered outliers on the distribution.
Age & Gender: Participants were 24 years old
on average (18 - 41). There was no significant
difference in age between men and women with
a two-sided T-Test (p=0.87). Figure 2 shows age
distribution by gender. The majority of participants
were younger than 25 years old.
Education & First Language: Most participants
from Q4had a bachelor’s degree (68%). The re-
mainder had a master’s degree (24%) or Ph.D. de-
gree (8%).
Unsurprisingly, as all participants reside in the US,
most of them (78%) had English as their first lan-
guage. The remainder had Chinese, Spanish, Viet-
namese, Russian, or Arabic as their first language.
A.2 Agreement
The inter-annotator agreement according to Fleiss
Kappa (Fleiss and Cohen, 1973) of participants for
Q2wasκ=0.84.
The inter-annotator agreement of the five experts
inQ4wasκ=0.66and for the remaining 50
participants in Q4it was κ=0.79.
The agreement between the expert group and the
AMT group was κ=0.41, showing that experts962
deviate strongly from average raters with a higher
education degree.
When looking at participants with a Ph.D. and
a bachelor’s degree, assessments of paraphrasing
quality deviated more κ=0.57than within the re-
spective groups of participants with a Ph.D. degree
κ=0.79and a master’s degree κ=0.77.
A.3 Details on Questions
For the experiments in Q2, participants were asked
the following question:
Question: Do you think the above example was
machine-paraphrased (which means a machine
rewrote some human-authored text) then choose
“machine-paraphrased”. If you think a human wrote
the example, please choose “original”. If you can-
not assign the example to either category, please
choose “I don’t know”.
For the experiments in Q4, participants were given
the following three instructions with the option to
rate on a scale from one to five.
Instruction 1: The first question is about fluency,
which refers to the ability to write grammatically
correctly and clearly. Does it sound like a native
speaker wrote it (high rating), or does it sound like
someone who just learned English (low rating)?
Instruction 2: The second question is about clarity,
which refers to the presentation of content and its
explanation. Is the content easy to follow (high
rating), or is it complicated and hard to understand
(low rating)?
Instruction 3: The third question is about coher-
ence, which refers to the consistency of contentthroughout the paragraph. Is the content following
a common central idea (high rating), or is the text
jumping from one (random) idea to another (low
rating)?963