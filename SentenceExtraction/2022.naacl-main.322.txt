
Zi-Yi Dou, Nanyun Peng
Department of Computer Science
University of California, Los Angeles
{zdou,violetpeng}@cs.ucla.edu
Abstract
The speaker-follower models have proven to
be effective in vision-and-language navigation,
where a speaker model is used to synthesize
new instructions to augment the training data
for a follower navigation model. However, in
many of the previous methods, the generated
instructions are not directly trained to optimize
the performance of the follower. In this paper,
we present , aFllower-ware speaker
Model that is constantly updated given the fol-
lower feedback, so that the generated instruc-
tions can be more suitable to the current learn-
ing state of the follower. Specifically, we opti-
mize the speaker using a bi-level optimization
framework and obtain its training signals by
evaluating the follower on labeled data. Ex-
perimental results on the Room-to-Room and
Room-across-Room datasets demonstrate that
our methods can outperform strong baseline
models across settings. Analyses also reveal
that our generated instructions are of higher
quality than the baselines.
1 Introduction
The task of vision-and-language navigation (VLN)
requires an agent to navigate in a real-world envi-
ronment given natural language instructions. In
VLN, one of the major challenges is the lack
of training data. To alleviate the issue, speaker-
follower models (Fried et al., 2018b) have been pro-
posed. Specifically, in the speaker-follower models,
an instruction-follower agent is trained to follow a
provided natural language instruction to complete a
specified goal, and a speaker model learns to model
how humans describe routes and synthesize new
instructions so as to create more training data for
the follower.
While speaker-augmented data is widely used in
VLN (Fried et al., 2018b; Wang et al., 2019; Ma
et al., 2019; Tan et al., 2019; Zhu et al., 2020a;Figure 1: Many of the previous methods use the speaker
to generate instructions from sampled routes and train
the follower. F (in red) further obtains feedback
from the follower on labeled data and updates the
speaker accordingly.
Hao et al., 2020; Wang et al., 2021; Chen et al.,
2021), most of the previous methods focus on im-
proving the follower navigation model. In contrast,
how to improve the speaker model to generate data
of higher quality is underexplored. In this line of
research, Fried et al. (2018a) build a pragmatic
speaker that can synthesize instructions based on
how the follower may interpret the instructions; Tan
et al. (2019) propose to randomly add noise into the
environments when generating instructions, so that
the noisy environments can mimic unseen environ-
ments and the generated instructions can be more
diverse; Kurita and Cho (2021) propose a genera-
tive approach for VLN where a speaker model is
trained and the actions of the follower are selected
by maximizing the probability of generating the
given instruction.
In this paper, we propose a follower-aware
speaker model ( F ) that optimizes the gener-
ated instructions by directly obtaining feedback
from the follower so that the generated instructions
can be more suitable for the follower. To this end,
we frame the idea as a bi-level optimization prob-
lem and obtain the feedback signals to improve the
speaker based on the follower performance on la-
beled data. As illustrated in Figure 1, the follower
and speaker are trained in an iterative manner: after4332updating the follower for one step, it is evaluated
on a batch of labeled data and the speaker is up-
dated given the performance of the follower. In this
way, the speaker is trained to directly optimize the
performance of the follower.
Experiments on Room-to-Room (Anderson et al.,
2018b) and Room-across-Room (Ku et al., 2020)
demonstrate strong performance of F over
baselines. Notably, F can achieve comparable
performance to a model pre-trained with over mil-
lions of text sentences and image-text pairs. Analy-
ses also reveal that our speaker generates instruc-
tions of higher qualities than baselines.
2 Methods
We first introduce the background before dis-
cussing the details of F .
2.1 Background
Base Settings. VLN requires an agent to follow
a given instruction and find a route in a photo-
realistic environment ( e.g.navigate in indoor liv-
ing spaces). Formally, in an environment e, the
follower Fparameterized by θlearns to model
the distribution P(r|i;θ), where iandrdenote
instruction and route variables, respectively.
The training data Dconsists of instruction-route
pairs from different environments. Given a batch
of instruction-route pairs (i,r)fromD, we train
the follower Fto minimize the cross-entropy loss
between its prediction F(i;θ) = P(ˆr|i;θ)
and the ground-truth label r. Here, we denote this
supervised loss as L:
Speaker-Follower Models. Fried et al. (2018b)
propose to train a speaker model Sparameterized
byθthat models the distribution of P(i|r;θ). As
in Figure 1, with the speaker, we can perform back
translation (Sennrich et al., 2016) on randomly sam-
pled routes ˆrfrom the training environments Efor
data augmentation. Specifically, we first train the
speaker Son the same training data as the follower.
Then, given a batch of sampled route ˆr∼ E ,
we synthesize their human-like textual instructions
ˆi=S(ˆr;θ). Afterwards, the synthesized train-
ing instances (ˆi,ˆr)are used to update F. Here,
we denote this loss as L:
2.2 Optimizing the Speaker
As we can see from Equation 2, the resulting fol-
lower parameters θdepends on the speaker pa-
rameters θ, and we can express the dependency as
θ(θ). However, existing speaker-follower mod-
els fail to incorporate θinto the optimization pro-
cess and θis always fixed during training.
Formulation. In this paper, we propose to op-
timize the parameters of both the follower and
speaker during back translation. Specifically, tak-
ing inspirations from Pham et al. (2021a,b), we
optimize the speaker based on the performance of
the follower on the labeled training data, which can
be expressed as:
The motivation of Equation 3 is that while the
speaker-augmented data can provide additional su-
pervisions for the follower, the main objective of
the speaker is to make the follower better follow
human instructions, thus we should focus on mini-
mizing follower’s loss on the labeled training data.
Approximation. Following previous work in bi-
level optimization (Finn et al., 2017; Liu et al.,
2018; Pham et al., 2021a,b), we can approximate
argmin with one-step gradient update and alterna-
tively update the parameters θandθ.
Specifically, at training step t, we first sample
a batch of routes and synthesize their instructions
using the speaker S. The generated data is used to
update the follower:
where ηis the learning rate.
Then, the speaker is updated to optimize the
objective L(θ))with
We can approximate the gradient ∇L(θ))
(derivation details in Appendix A) with4333We can see that this equation resembles REIN-
FORCE (Williams, 1992) in reinforcement learn-
ing. Therefore, this algorithm can also be inter-
preted as treating the similarity in the gradients of
the follower model on the labeled data and on the
augmented data as rewards, and update the speaker
model using reinforcement learning.
End-to-End Reconstruction Loss. In this paper,
we also propose to add a reconstruction loss for
the speaker. Concretely, we compute the gradient
of Equation 2 with respect to the speaker parame-
terθusing straight-through estimator, denoted as
∇L(θ, θ), and then update the speaker in an
end-to-end manner.
To sum up, in F , the final gradient of the
speaker is computed based on both the reconstruc-
tion loss (Equation 2) and the bi-level optimization
loss (Equation 6), and we will perform ablations
on the two objectives in the experiment section.
3 Experiments
Datasets. We evaluate the models on the Room-
to-Room (R2R) (Anderson et al., 2018b) and
Room-across-Room (RxR) (Ku et al., 2020)
datasets. The R2R dataset consists of 7,189 paths,
and each path has 3 English instructions with an
average length of 29. R2R is split into training, val-
idation, and test sets. The validation set is split into
val-seen , where paths are sampled from environ-
ments seen during training, and val-unseen , where
paths are sampled from environments that are not
seen during training. The paths in the test set are
from new environments unseen in the training and
validation sets. The RxR dataset follows the same
environment division as R2R and there are 16,522
paths in total. The instructions have an average
length of 78 and are in three languages, including
English, Hindi, and Telugu.
Evaluation Metrics. Our primary metric is suc-
cess rate (SR), and we also report navigation er-
ror (NE), success rate weighted by path length
(SPL) on R2R. Following the suggestion in Ku et al.
(2020), we also report normalized dynamic time
warping (nDTW) and success rate weighted by dy-
namic time warping (sDTW) (Magalhães et al.,
2019) on RxR.
Implementation Details. Following En-
vDrop (Tan et al., 2019),we build our speaker
and follower based on LSTM (Hochreiter and
Schmidhuber, 1997) and environmental dropoutis used during back-translation. The follower
is pre-trained with imitation and reinforcement
learning, and the speaker is pre-trained with
maximum likelihood training. Here, we refer
to this pre-trained follower as base follower .
The two models are pre-trained for 80k steps on
R2R and 200k steps on RxR, and then trained
with our method until the 300k-th iteration. We
perform environmental dropout during training
as in Tan et al. (2019), and also use their 176,776
paths randomly sampled from seen environments
for back translation. Different from Tan et al.
(2019), we use CLIP-ViT-224/16 (Radford et al.,
2021) to extract vision features as CLIP vision
encoders can be beneficial for VLN models (Shen
et al., 2022) and we demonstrate that using CLIP
vision encoder can obtain better performance than
ResNet-based models in the following parts. We
compute the cosine similarities between gradients
for Equation 6 following Pham et al. (2021b,a)
and also perform the same weighting for the
reconstruction loss. Each training takes about 3
days on 1 NVIDIA V100 GPU to finish. We report
numbers of a single run for evaluations.
3.1 Main Results
Room-to-Room. We report the main results on
R2R in Table 1. We can see that our implementa-
tion of the baseline EnvDrop model is better than
the previous work because of the stronger vision
encoder we use. Based on the strong baseline, our
model achieves further improvements on both val-
idation and test sets, outperforming EnvDrop by
2.2% in the success rate on the R2R test dataset,
suggesting that our framework is indeed effective.
Room-across-Room. We report the main results
on R2R in Table 2. From the table, we can see that
the improvements of our framework are not as good
on the RxR dataset, possibly because the instruc-
tions are much longer and thus it is hard to train a
good speaker. Specifically, we find that the baseline
speaker can only achieve a BLEU score of 7.4 on
the English validation set on RxR (compared with
over 30 BLEU scores on R2R as in Appendix B),
which leads to noisy augmented data and can im-
pact the performance of speaker-follower models.
3.2 Analysis
We then perform analyses to gain more insights
regarding our models:4334
Pre-exploration and Beam Search. We perform
experiments in both pre-exploration and beam
search settings following previous work (Tan et al.,
2019). Because both the speaker and follower are
used in the two settings, the evaluation results can
reflect the quality of both of the models. As shown
in Table 3, we find that the best configuration isusing ourfollower and ourspeaker, suggesting that
both our follower and speaker are more suitable
for VLN than the baselines. Notably, in the beam
search setting, our model can achieve a success rate
of 72.2%, which is comparable to VLN-BERT (Ma-
jumdar et al., 2020) that achieves a success rate of
73% and is pre-trained with over millions of text
sentences and image-text pairs.
Generated Instructions. The previous pre-
exploration and beam search results well indicate
that our generated instructions are more suitable
for our follower, suggesting the effectiveness of our
framework. In this paragraph, we also compare the
generated instructions with the reference instruc-
tions. In Figure 2, we plot the histogram of length
differences between the reference sentences and the
generated instructions using compare-mt (Neubig
et al., 2019). The figure suggests that the base-
line model can often generate shorter instructions
than the references, but our method can alleviate
this issue, indicating that our methods can indeed
improve the speaker quality during training. We
also find that our generated instructions are quan-
titatively and qualitatively better than the baseline
using automatic evaluations as in Appendix B.4335
Ablation Studies. As mentioned in Section 2.2,
we perform ablation studies on both of our pro-
posed objectives, namely the bi-level optimiza-
tion loss (Equation 5) and reconstruction loss. As
shown in Table 4, ablating either of the objectives
can lead to degraded performance on the R2R vali-
dation sets, indicating that both the objectives can
improve the model performance and they are com-
plementary to each other.
4 Related Work
We overview two lines of related work:
Vision-and-Language Navigation. Training em-
bodied navigation agents has been an increasingly
active research area (Anderson et al., 2018a,b;
Chen et al., 2019; Ku et al., 2020; Shridhar
et al., 2020; Padmakumar et al., 2022). Fried et al.
(2018b) propose to augment the training data with
the speaker-follower models, which is improve
by Tan et al. (2019) who add noise into the environ-
ments so that the speaker can generate more diverse
instructions. Zhao et al. (2021) propose methods
to measure the quality of the generated instruc-
tions and filter noisy samples. Liu et al. (2021) pro-
pose to adversarially sample the most difficult paths
for the follower and translate these paths into in-
structions using the speaker for data augmentation.
While using the speaker-augmented data has been
widely used in VLN, most of the existing work has
been focused on improving the follower naviga-
tion model (Wang et al., 2018; Li et al., 2019; Zhuet al., 2020b). For example, the self-monitoring
agent (Ma et al., 2019) improves cross-modal align-
ment through a visual-text co-grounding module
and a progress monitor; Zhu et al. (2020a) propose
to utilize four self-supervised auxiliary tasks that
can provide additional training signals for the agent.
Most similar to our work, Fried et al. (2018a) build
a speaker that reason about how the instructions
may be interpreted; Kurita and Cho (2021) propose
a generative approach where a speaker model is
trained to model the probability of an instructions
given actions, and the follower chooses actions that
maximize this probability.
Bi-level Optimization. Bi-level optimization al-
gorithms have been widely applied in various fields,
such as learning initialization parameters (Finn
et al., 2017), neural architecture search (Liu et al.,
2018), re-weighting training data (Wang et al.,
2020). Our method takes inspirations from (Pham
et al., 2021a), which is applied in pseudo labeling
and optimizes the teacher parameters given the stu-
dent feedback. Similar techniques have also been
used in machine translation (Pham et al., 2021b),
where a meta-validation set is constructed to evalu-
ate the model performance and provide feedback.
5 Conclusions
In this paper, we propose the F model where
we improve the speaker-follower model in vision-
and-language navigation by constantly updating the
speaker given the follower feedback during train-
ing. We frame the idea as a bi-level optimization
problem and obtain the feedback signal based on
the performance of the follower on labeled data.
Experimental results on Room-to-Room and Room-
across-Room datasets demonstrate that our method
can outperform strong VLN baselines in different
settings. Analyses also suggest that the quality of
our speaker model is indeed improved during train-
ing. Future directions include testing our method
on more datasets and investigating more options on
the feedback signals.4336Acknowledgement
We would like to thank the anonymous reviewers
for valuable suggestions and Te-Lin Wu for helpful
discussions. This work is supported in part by the
DARPA Machine Common Sense (MCS) program
under Cooperative Agreement N66001-19-2-4032
and NIH R01HL152270.
References4337
A Derivation of the Speaker Gradient
As shown in Section 2.2, at training step t, we
update the follower according to:
θ=θ−η∇L(θ, θ).(7)4338
We then derive the speaker gradient following pre-
vious work (Pham et al., 2021b,a). We define the
expected parameters of the follower as ¯θ:
Then, using the chain rule, we can obtain
∇L=∂L
∂¯θ∂¯θ
∂θ, (9)
where the first term can be approximated with.
Then, for the second term, we have
We can assume that θdoes not depend on θ
with Markov assumption (Pham et al., 2021a), and
apply the REINFORCE (Williams, 1992) equation
on the second term:
Using Monte Carlo approximation to approxi-
mate terms in Equation 11 using a batch of samples
and substituting the result into Equation 9, we can
get
Note that here ηis a hyper-parameter and can
be incorporated into the learning rate of the speaker
η, thus we remove this term in Section 2.2 and
our derivation is complete.
B Evaluations of the Generated
Instructions
Automatic Evaluations. As in Table 5, we
measure the quality of the generated instruc-
tions in BLEU (Papineni et al., 2002) andBERTScore (Zhang et al., 2020). We find that our
speaker can generate instructions of higher quali-
ties according to the two metrics.
Qualitative Examples. As in Table 6, we also
find that after training the speaker using our
method, the generated instructions are generally
longer than the baseline and are more accurate com-
pared with the references.
C License
We evaluate our models on the Room-to-Room
(R2R) (Anderson et al., 2018b) and Room-across-
Room (RxR) (Ku et al., 2020) datasets based on
Matterport3D (Chang et al., 2017). The datasets are
released under the Matterport3D Terms of Use.
The datasets do not contain any information that
names or uniquely identifies individual people or
offensive content. Our code is based on EnvDrop
that is released under the MIT license.We use the
datasets and code for research purposes, which is
consistent with their intended use.
D Limitations and Potential Risks
As in the experiments, our models may not work
well when the instructions are long and it is hard
to train a reasonable speaker model. Also, our
model requires fine-tuning the speaker during train-
ing based on the feedback of the follower, which4339introduces additional training costs to the model.
In addition, the datasets we use in the paper may
make our model biased towards environments of
American buildings.4340