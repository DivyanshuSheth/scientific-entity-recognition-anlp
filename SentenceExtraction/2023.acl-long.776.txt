
Jia Peng Lim
Singapore Management University
jiapeng.lim.2021@smu.edu.sgHady W. Lauw
Singapore Management University
hadywlauw@smu.edu.sg
Abstract
Automated coherence metrics constitute an im-
portant and popular way to evaluate topic mod-
els. Previous works present a mixed picture of
their presumed correlation with human judge-
ment. In this paper, we conduct a large-scale
correlation analysis of coherence metrics. We
propose a novel sampling approach to mine
topics for the purpose of metric evaluation, and
conduct the analysis via three large corpora
showing that certain automated coherence met-
rics are correlated. Moreover, we extend the
analysis to measure topical differences between
corpora. Lastly, we examine the reliability of
human judgement by conducting an extensive
user study, which is designed as an amalgama-
tion of different proxy tasks to derive a finer
insight into the human decision-making pro-
cesses. Our findings reveal some correlation
between automated coherence metrics and hu-
man judgement, especially for generic corpora.
1 Introduction
Topic modelling is an important tool in the anal-
ysis and exploration of text corpora in terms of
their salient topics (Blei et al., 2003). To evaluate
the effectiveness of topic models, the preponder-
ance of topic modeling literature rely on automated
coherence metrics. A key benefit is convenience, al-
lowing researchers to sidestep expensive and time-
consuming user studies. The basis for this reliance
is the assumption that the coherence metrics corre-
late with human judgement (Mimno et al., 2011;
Lau et al., 2014; Röder et al., 2015).
The presumed correlation with human judge-
ment should not be taken for granted. There are re-
cent works that challenge the assumption. Doogan
and Buntine (2021) highlight the inconsistencies of
automated coherence metrics via correlation anal-
ysis within each metric. In Hoyle et al. (2021),
they claimed some disagreement between human
judgement and automated coherence metrics.We postulate that the reasons behind such a
mixed picture could be the differences in the topic
samples as well as the underlying corpora from
which the statistics were derived, resulting in lo-
calised “biases” that affect the conclusions reached
by respective studies. Given their importance, we
seek to conduct an extended analysis of automated
coherence metrics on a larger scale than anything
previously attempted. This study includes orders
of magnitudes greater than the number of topics
typically analysed, covering three large corpora,
employing a comprehensive user study with exten-
sive labels, across most of the widely used metrics.
There is a strong motivation for quantity. Given
a vocabulary, a combinatorially large number of
possible topics exist. If each topic is a vector of its
scores on different metrics, the resulting curse of
dimensionality (Bellman and Kalaba, 1959) neces-
sitates a larger sample size. We argue that evaluat-
ing thousands of topics might not be sufficient, and
a larger sample size is required to approximate a
diverse distribution, where sampled topics is repre-
sentative of the corpus and the metrics.
We surmise that the previous practice of using
topic models to generate topics could introduce
a bias in the analysis. Firstly, topic models vary
in performance, Hoyle et al. (2021) compiled a
lengthy list. There is also emerging debate on the
performance between traditional and neural topic
models (Doogan and Buntine, 2021). Additionally,
some neural models might be inconsistent, produc-
ing different topic sets in independent runs (Hoyle
et al., 2022). Conversely, topic model might be too
stable and generate similar topics (Xing and Paul,
2018). To objectively evaluate whether the coher-
ence metrics are usable, we propose to generate
candidate topics independently of topic models.
In this paper, our contributions are three-fold.
First, we begin by analysing the inter- metric cor-
relations (see Section 4). We propose a novel
approach to sample “topics” for the purpose of13874evaluating automated coherence metrics (see Sec-
tion 4.1). Compared to prior works, we sample
these topics free from topic model bias, and in a
meaningful diverse manner. Evaluated on three
large corpora, we reaffirm that certain selected met-
rics do not contradict each other, and highlight the
underestimated effects of ϵ(see Section 4.2).
Second, we extend our analysis to investigate
inter- corpora correlations (see Section 5). We ex-
amine the understated differences of corpora statis-
tics on the metrics by comparing the correlations
across corpora. While such correlations do exist to
some degree, the metrics are still dependent on each
corpus. Thus, any expectation that these metrics
would correlate uniformly with human judgement
on all possible corpora may be misplaced.
Finally, pivotal to any interpretability research,
we design and conduct a user study, which is the
keystone of our work (see Section 6). Compared to
prior work, its design is more complex as we seek
to benchmark human judgement at a finer granu-
larity across different random user study groups
(see Section 6.1). We analyse the user study results
via a few novel proxy measures, revealing that hu-
man judgement is nuanced and varies between indi-
viduals, metric correlation to human judgement is
corpus-dependant, with the average participant be-
ing attuned to the generic corpora (see Section 6.2).
Our implementation and releasable resources
can be found here, and we hope that it will enable
convenient coherence evaluation of topic models
and to further advance interpretability research.
2 Related Work
Topic models. There are many approaches for
topic modelling Blei et al. (2003), from non-neural
based Zhao et al. (2017b); Hoffman et al. (2010),
to many other neural-based methods, via auto-
encoders (Kingma and Welling, 2014) such as Miao
et al. (2016); Srivastava and Sutton (2017); Dieng
et al. (2020); Zhang and Lauw (2020); Bianchi
et al. (2021), via graph neural networks (Yang et al.,
2020; Shen et al., 2021; Zhang and Lauw, 2022),
and hierarchical methods (Meng et al., 2020). A
common factor is the use of automated coherence
metrics to benchmark against baselines. We select
several popular metrics for evaluation as listed in
Section 3. Topic models are applied in downstream
tasks (Lau et al., 2017; Wang et al., 2019, 2020).
User studies in metric evaluation. Mimno et al.(2011) utilize expert annotators to independently
label 148 topics, using another 10 expert annotators
to evaluate the same topics via intruder word detec-
tion tasks. Röder et al. (2015) benchmark topics
against different permutations of metrics with the
largest evaluation set containing 900 topics with
human ratings aggregated from prior works (Ale-
tras and Stevenson, 2013; Lau et al., 2014; Rosner
et al., 2014). In Hoyle et al. (2021), a minimum of
15 crowdworkers were employed in simple rating
and word intrusion tasks evaluating 40 topic-model-
generated (Griffiths and Steyvers, 2004; Burkhardt
and Kramer, 2019; Dieng et al., 2020) and 16
synthetic random topics. In Doogan and Buntine
(2021), their largest user study required 4 subject
matter experts creating 3,120 labels across 390 top-
ics generated via topic models (Blei et al., 2003;
Zhao et al., 2017a). In comparison, our study has
both large quantities of topics and study partici-
pants, annotating 800 unbiased topics split between
40 study participants with at least an undergradu-
ate level of education, generating 180K word-pair
labels. Our automated experiments deal with hun-
dreds of thousands of unique topics.
Human involvement. There are many interest-
ing research that examine linguistic problems via
the human lens. Card et al. (2020) investigates the
number of annotators required to achieve signifi-
cant statistical power. Plank (2022) examines the
variation in human labels. Ethayarajh and Juraf-
sky (2022) questions the authenticity of annotators.
Clark et al. (2021) tests the human ability to learn
how to differentiate between machine-generated
and human-generated texts. Human-in-the-loop
systems or processes, such as Li et al. (2022), are
also being actively explored.
3 Preliminaries
In this section, we define the automated coherence
metrics that we will be using, and describe the
corpora we use to obtain the word probabilities.
3.1 Coherence Metrics
We follow the definition styles of Röder et al.
(2015), where direct confirmation measure mis
a function of a word-pair statistic. Direct coher-
ence metrics is defined as a mean aggregation of m
between word-pairs (Equation 1), where tis a topic
which is a k-sized set of words. For our evaluations,13875we set k= 10 . Within t, the words are arranged
based on P(w|t)in descending order. Since our
approach does not produce P(w|t), we can locally
optimize the word positions within a topic to obtain
the best possible score for position-sensitive met-
ricsC andC(See Appendix B). We use sub-
script sto denote alphabetical order and subscript
oto denote optimized positions. Let p=,
which represents the number of word-pairs in a
topic.
C(t, m) =1
p/summationdisplay/summationdisplaym(w, w) (1)
C (Equation 2) is the mean aggregation of
m, defined as Normalised Pointwise Mutual In-
formation (NPMI) (Bouma, 2009) value, between
word-pair statistics in a topic. We exclude Cas
it uses Point-wise Mutual Information (Church and
Hanks, 1990; Lau et al., 2014), which is correlated
to NPMI.
C(t) =1
p/summationdisplay/summationdisplaym(w, w)(2)
m(w, w) =log
−log(P(w, w) +ϵ)(3)
C is the mean ordinal aggregation of m
(Mimno et al., 2011), which measures the logcon-
ditional probability between ordered word-pair in
a topic:
C (t) =1
p/summationdisplay/summationdisplaym(w, w) (4)
m(w, w) = logP(w, w) +ϵ
P(w)(5)
Cis the mean ordinal aggregation of m, Fitel-
son’s coherence (Fitelson, 2003), interpreted as the
degree to which wsupports w, between ordered
word-pairs in a topic:
C(t) =1
p/summationdisplay/summationdisplaym(w, w) (6)
m(w, w) =P(w|w)−P(w|¬w)
P(w|w) +P(w|¬w)(7)C(Equation 8) is the final metric that we are us-
ing.Cis considered as an indirect coherence met-
ric, as it uses word-group relations as opposed to
word-pairs relations like aforementioned direct co-
herence metrics. Intuitively, it measures the mean
cosine similarity (Equation 9) between each word’s
feature vector and the topic’s feature vector repre-
sented as the sum of all of its words’ feature vectors
(Equation 10).
C(t, γ) =/summationtexts(v(w, t, γ),¯v(t, γ))
|t|(8)
s(⃗ v, ⃗ v) =/summationtext⃗ v·⃗ v
||⃗ v||· ||⃗ v||(9)
¯v(t, γ) =/summationdisplayv(w, t, γ) (10)
v(w, t, γ ) ={m(w, w)∀w∈t} (11)
For indirect confirmation measure ˜m, instead
of directly using word-word probabilities, it uses
mto create a vector of features v(Aletras and
Stevenson, 2013) that represent a word wfrom the
topic tit belongs to, distorted by hyper-parameter
γ(Equation 11). We will evaluate γat 1 and 2.
3.2 Corpora
We use word co-occurrences statistics obtained
from three large corpora:
ArXiv . We use ArXiv abstracts datasetwhere
we consider each abstract as a document. These
abstracts mainly comprise of research work related
to non-medical science disciplines.
Pubmed . We use PubMed Central (PMC) Open
Access Subsetthat contains journal articles and
pre-prints related to medical research and informa-
tion. We consider each article body as a document
and we remove citations within it.13876Wiki . We use the English-Wikipedia dump
of August’22 processed using Attardi (2015). We
consider the content of the article as a document.
To check for correctness, we also use the popular
benchmark Palmetto (Röder et al., 2015), which
uses a subset of Wikipedia’11.
For each corpus, we apply processing steps sug-
gested in Hoyle et al. (2021), retaining up to 40K
frequently occurring words. Moreover, we generate
a lemmatized (denoted with the suffix -lemma) and
unlemmatized variant (original) for further analysis.
More information on common vocabulary between
corpora can be found in Table 14, Appendix C.
4 Examining Inter-Metric Correlations
Intuitively, if two different metrics are to correlate
with human judgement, we would expect the scores
of these metrics to correlate. However, it is claimed
in Doogan and Buntine (2021) that these metrics
do not correlate well. For reasons described in
Section 1, we propose a new non-topic modelling
approach to sample topics to evaluate these metrics.
4.1 Approach: Balanced Sampling
There are few tested methods to generate topics:
from topic models (Aletras and Stevenson, 2013;
Lau et al., 2014), beam search optimized on co-
herence (Rosner et al., 2014), random sampling of
words (Hoyle et al., 2021). Considering only opti-
mized topics, or completely random topics (mostly
bad), would generate a skewed distribution. In
contrast, we seek to mine topics that emulates a
balanced distribution for a meaningful comparison.
We also desire uniqueness among topics, which
avoids repetition and is representative of the corpus.
Figure 1 illustrates an overview of our approach.
Mining topics of kwords can be framed as
the classical k-clique listing problem (Chiba and
Nishizeki, 1985; Danisch et al., 2018). To generate
meaningful topics, we can map the corpus-level
information as a graph, treating each word from
its vocabulary set Vas a vertex. Each word will
share an edge with every other word. We choose
mto determine the value of the edges between
two vertices as its normalised range is intuitive al-
lowing us to easily identify the range of values for
sub-graph generation. In contrast, using mand
mincreases sampling’s complexity as they are
order-dependant resulting in bi-directional edges
in its sub-graph. Sampling using any m, not only
m, might introduce bias, which our approach
seeks to mitigate.
The initial graph will be a complete graph of
|V|vertices. A topic of kwords would be a k-
sized sub-graph. Combinatorially, there are |V|
choose knumber of possible unique topics. It is
practically infeasible and unnecessary to list all k-
cliques. For a more tractable approach, we modify
the routine from Yuan et al. (2022) (pseudo-code
in Appendix A) to include:
Sub-graphs of varying quality. This routine
seeks to generate smaller graphs from the origi-
nal complete graph to cover the spectrum of topic
quality. We eliminate edges conditionally via their
value, and the remaining edges and connected ver-
tices constitute the new sub-graph. We generate
three different kinds of sub-graphs, poswhere edge-
values are above a given lower-bound, midwhere
edge-values are between threshold values, and neg
where edges are below an upper-bound.
Topic extraction. Inspired by Perozzi et al.
(2014), instead of iterating through all the neigh-
bouring nodes or searching for the next best node,
we randomly select a neighbour, that has an edge
with all explored nodes, to explore. We extract the
explored k-path as our sampled topic.
Topic uniqueness. To attain a variety of topics,
we remove all edges in a mined clique, making it
impossible to sample a similar topic from the same
sub-graph. Figure 2 illustrates this feature.
Balance distribution of topics. For a given cor-
pus, we further introduce common topics sampled
from a different corpora, which differ in its word
distribution. We refer to this segment of external
topics as ext. Lastly, random is a segment, com-
prising of groups of random words, included to
represent topics that might not have been covered
via the other segments. Table 2 shows the result
from this mining approach. The total would thus
be more balanced, comprising topics of varying
scores along the spectrum.13877
4.2 Evaluation: Metric Correlations Analysis
We evaluate the correlation (Pearson’s r) be-
tween different automated metrics measured on
Wiki (see Table 3), Pubmed, and ArXiv (see Ta-
ble 10, Appendix C). We expect a high positive
correlation score between metrics if they are both
purportedly measuring for coherence. Our first
inter-metric analysis (see Table 3a), with metrics
calculated at ϵ= 1e−12, shows the poor correla-
tion of Cmetrics against other metrics. Theoreti-
cally, Crelies on mas its features, and givenan unrelated topic, where word-pair scored on m
with ϵ= 1e−12produces similar mvectors
which scores highly on C. This phenomenon of
high cosine similarity between the equally nega-
tivemvectors, results in contradicting scores
between Cand other metrics.
Hence, for our second inter-metric analysis (see
Table 3b) we evaluate the metrics at ϵ= 0, denoted
with subscript ̸ϵ. For the resulting undefined cal-
culations, we default to 0. Intuitively, the purpose
of setting ϵ= 1e−12is to prevent and to penalise
word-pairs that produces undefined calculation. In
contrast, ϵ= 0 treats these word-pairs neutrally.
Comparing the new results in Table 3b to the pre-
vious results in Table 3a, we note that correlation
scores between Cmetric and other automated
coherence metrics improved greatly, suggesting al-
leviation of the contradicting factor. Additionally,
we note that for CandC ,ϵis essential. We
then examine these metrics with their better ϵmode
(see Table 4a), and most metrics (except C )
have a decent correlation with other metrics, imply-
ing that they do not contradict each other.
There could be a concern that the negandran-
dom sampled sections would have an outsized in-
fluence in the previous analysis. In this ablation,
we restrict the same analysis to only topics where
C>0. Comparing to the previous results (see
Table 4a), we derive a similar interpretation from
this constrained results (see Table 4b), suggesting
that our balanced sampling approach is effective as
the behaviour of the full set of data is similar to its
smaller subset.13878
5 Examining Inter-Corpus Correlations
A natural extension after inter-metrics comparison,
is to compare metrics measured on different cor-
pora. It is a common expectation that research
works would employ multiple corpora, with the dif-
ferences between corpora quantified superficially
(such as in Section 3.2). We propose an alternative
approach to quantify the differences, at a topical
level, using common topics measured using auto-
mated coherence metrics. If the corpora are themat-
ically similar, we would expect a high correlation.
Analysis. Using the common topics from the
paired corpora, we conduct a correlation analysis
on the scores measured on each corpus per metric.
Table 5 shows decent correlations between each
corpus. However, even as they are positive, these
correlations do not imply identical statistics in var-
ious corpora. Assuming that human judgement is
constant for a given topic, we posit that variance in
scores measured on different corpora could result
in a lower correlation due to the missing themes
within the shared vocabulary space in either corpus.
We conduct a control analysis on pairs of simi-
lar corpus differing in lemmatization, originating
from the same documents, in Table 6a. These cor-
pora would be thematically similar whilst being
superficially different. Our previous analysis in
Table 5, comparing to the control analysis in Ta-
ble 6a, shows lower correlation scores suggesting
some topical difference between the various cor-
pora. This difference highlights the metrics’ strong
dependency on the corpus used, with a subset of
common topics disagreeing on the scores, revealing
that these metrics are not a one-size-fits-all solution
for coherence evaluation.
Ablations. While we know how lemmatiza-
tion affects topic modelling (Schofield and Mimno,
2016), its effect on evaluation is unclear. We car-
ried out two additional ablations simulating lemma-
tizing topics post-training. For the first ablation, we
shortlist topics that contain at least one unlemma-
tized word, where if lemmatized, the lemmatized
word can be found in the same unlemmatized cor-
pus. We compare the correlation of the original
and lemmatized topic, with their scores measured
on the same unlemmatized corpus. Their scores
have a strong correlation (see Table 6b), suggesting
that the difference between lemmatized topics and13879unlemmatized topics is small. For the second ab-
lation, the shortlisting process is similar, however,
with lemmatized topics measured on the lemma-
tized corpus. Our results (see Table 6c) show a
strong correlation across the various metrics and
imply that post-processing topics for evaluation is
a viable option.
6 User Study
Previous works measure human judgement through
simple evaluation tasks such as rating the coherence
of a topic on a few-point ordinal scale (Mimno
et al., 2011; Aletras and Stevenson, 2013), iden-
tifying the intruder word that introduced into the
topic (Chang et al., 2009), or both (Lau et al., 2014;
Hoyle et al., 2021). For word intrusion, the de-
tection of outliers signals the cohesiveness of the
topic, which is similar to rating topics on an ordi-
nal scale. However for both tasks, qualitative gaps
might exist. In word intrusion, study participants
are restricted to just one outlier per topic, assum-
ing perfect coding, it results in exponential drop
in scoring, i.e. 100% detection for a perfect topic,
50% for a topic with a clear outlier, and so forth.
For topic ratings, topics of differing qualities might
get the same score, i.e. a perfect topic and a topic
with a clear outlier might both get the same scores.
Additionally, while the decisions between hu-
man annotators might be equivalent, it is not evi-
dent if their thought processes are similar. The key
reason for this line of inquiry stems from the obser-
vation that everyone is different in some aspects,
such as knowledge, culture, and experiences. As-
suming our understanding of words is influenced
by our prior beliefs, what and how we perceive
similarity and coherence might differ from person
to person.
For these reasons, we decide to design a user
study that combines both word intrusion and topic
rating tasks but measured at a finer granularity such
that we can quantify the decision-making process.
Users are tasked to cluster word-groups which in-
dicate coherent and outlier word-groups. We then
examine the relationships between automated co-
herence metrics and different proxy tasks derived
from the user study.
6.1 User Study Design
For our study S, we recruit 8 user study groups
U,S={U, . . . , U}, 5 study participants per
group. Majority of the participants recruited have
at least a graduate degree or are undergraduates.
For each study group, we prepared 8 unique ques-
tion sets Q={T, . . . , T}, each containing 100
10-word topics, T={t, . . . , t}andt=
{w, . . . , w}. For each participant u∈U,
we present each t∈Tindividually sorted alpha-
betically. We ask participants to cluster words in
tthat they deem similar to form coherent word
groups g, where their response Rtotis a
set of unique g. We constrain each word to only
belong to one coherent word group to limit the task
complexity. Additionally, a word considered to be
unrelated may form its group of one. We use Lik-
ert matrixas the response format (see Figure 3),
mandating a response for each word w∈t.
Actual instructions are shown in Appendix E.
Topic selection. We construct an initial pool
of 1000 topics. To achieve comparability between
corpus, we randomly sample 400 common topics
from Wiki, ArXiv, and Pubmed. To represent non-
scientific topics, we randomly sample 200 topics
from Wiki that do not appear in ArXiv/Pubmed.
For ArXiv/Pubmed exclusive topics, we randomly
sample 200 topics each, with these topics also ap-
pearing in Wiki. We sample in a 7:1:1:1 ratio of
pos/mid/neg/random segments of the corpus, seek-
ing to emulate a uniform score distribution. To
account for word familiarity, we select lemmatized
topics with words found in 20K most frequently
used words. For each user study, we randomly13880sampled 100 topics from the pool without replace-
ment. For topics not found in ArXiv or Pubmed,
we exclude them during evaluation of those corpus.
Proxy Tasks. Representing coherence as word-
clusters allows us to derive a deeper insight into
what we perceive as human judgement. From our
user study task, we further decompose this study
into a few proxy tasks, where we measure the corre-
lation (Spearman’s ρ) of its results to automated
coherent metrics. We propose three topic-level hu-
man coherence measures. Using density of human
agreement, we define Pas the mean agreement of
Uon all possible word-pairs on any topic t:
P(t) =/summationtext/summationtext|g|(|g| −1)
|U|(12)
Ifthas perfect agreement on coherence, we ex-
pectP(t)to have a value of 1, and for incoher-
ence, a value of 0.
Subsequently, we consider the largest selected
word group within t, and define Pas the mean
of this measure amongst U:
P(t) =1
|U|/summationdisplaymax({|g||g∈R})(13)
A value of 1 will suggest that each word in t
have no relations to each other and a value of |t|
suggest perfect agreement on coherence.
Lastly, we define Pas the mean number of
annotated word groups amongst U:
P(t) =1
|U|/summationdisplay|R| (14)
The interpretation of Pis the inverse of P.
While these group-wise measures might seem sim-
ilar, they measure different nuances of human-
annotated data. Pevaluates the sizes of multi-
word groups, weighted towards larger groups. P
only accounts for the largest word group, which
ignores the properties of the other remaining group.
Pignores group sizes to a certain extent and in-
cludes single-word "outlier" groups. We evaluate
these measures’ correlation against various C(t).
6.2 User Study Results
We find that the three different proxy tasks produce
similar results, shown in Table 7a, 7b, and 7c re-
spectively, indicating correlations between human
judgement and some automated coherence metrics.
Since most of our study participants have some
science-related background, we are surprised by
ArXiv’s lower correlation scores relative to Wiki in
each proxy task. These results imply that our per-
ception of coherence might be biased towards the
word distribution of a generic corpus such as Wiki.
Lastly, in each proxy task, the higher variances in
ArXiv’s and Pubmed’s correlation scores compared
to Wiki’s might imply increased subjectivity.
Inter-rater reliability (IRR). There are many
factors that wil affect the variation for IRR (Belur
et al., 2021). For our user study, we attempted to
mitigate some of these factors. In terms of fram-13881ing and education, study participants were given a
short introductory primer as well as some example
questions prior to starting the tasks (Appendix E).
To mitigate fatigue effect, we allowed the study
participants a week to work on the task, pausing
and continuing at their own pace. We were not
concerned about learning effect, as our presented
topics spans across a plethora of themes and the
correctness of the task is subjective to their own
personal preference. As our objective is to poll
for their beliefs, with many possible valid answers,
there is not a need to review and enforce consis-
tency between study participants.
We use Krippendorf’s α(Krippendorff, 2011) ,
defining pair-wise rater similarity as Jaccard dis-
tance measuring common answers between raters.
We treat each w∈tas a multi-classification
question, comprising of other words (in t) and
"not related" as categories, producing boolean vec-
tor representations. The mean ¯αis 0.366 with a
standard deviation of 0.04, lowest αat 0.325 and
highest αat 0.464 (see Table 15, Appendix C). A
completely random study response will have an α
of 0.12, being significantly less than the study’s
¯α, giving us some confidence about the reliability
of the responses. Overall, considering that there
are many possible combinations for each topic re-
sponse, the αreported suggests some degree of
similarity between different responses.
User study ablations. We examine if position-
ing affects position-dependent automated coher-
ence metrics via human pair-wise agreement proxy
taskP. We detail our optimizing approach in Ap-
pendix B. We define Pas the percentage of agree-
ment between any word-pairs wandwfrom t
from Tevaluated by its corresponding U:
P(w, w) =1
|U|/summationdisplay/summationdisplayw∈g∧w∈g
(15)We measure the correlation of P(w, w)in a
group to its pair-wise automated coherence metric
score via m(w, w)from different orderings. Our
results in Table 8 show some non-significant differ-
ences in correlation on the pair-wise level. How-
ever, that difference disappears when we evaluate
the topics as a group, with the sorted and optimized
variant achieving similar correlations (see Table 7).
Furthermore, this difference of coherence at the
pair-wise and group-wise levels, suggests that the
presence of other words in the topic has an influ-
ence on the human perception of word-pair coher-
ence. Finally, we replicate most experiments with
the corpus statistics from Palmetto (Röder et al.,
2015), which produced similar correlation results
to Wiki.
7 Conclusion
Our large-scale analysis reaffirms that these auto-
mated coherence metrics are still meaningful. We
are confident in using these metrics measured on
generic corpus such as Wiki, and specialised cor-
pora, Arxiv and Pubmed, for nicher tasks. Our
user study empirically supports this conclusion, as
our participants’ collective response correlates well
to metrics measured on Wiki, albeit weaker but
meaningful correlation on the specialized corpora.
This work shows that popular automated coherence
metrics, C ,C, and C, are alive and well,
and works regardless of lemmatization. Further-
more, we stress that the selection of the reference
corpus is just as important as the selection of the
metric, with Wiki being the best reference corpus
that correlates with human perception of coherence.
Moving forward, when evaluating for coherence
aligned towards human interpretability, we recom-
mend future topic models to be evaluated against
Wiki-variants. We also recommend calculating C
withϵ= 0, to avoid the confusion from its contra-
diction of other metrics at ϵ= 1e−12.
Acknowledgments
This research/project is supported by the National
Research Foundation, Singapore under its AI Sin-
gapore Programme (AISG Award No: AISG2-RP-
2021-020). Hady W. Lauw gratefully acknowl-
edges the support by the Lee Kong Chian Fellow-
ship awarded by Singapore Management Univer-
sity. We extend our gratitude to our user study
participants for their efforts, as well as, our review-
ers for their kind feedback.13882Limitations
User Study. Most, if not all, of the participants
are pursuing or have obtained at least a univer-
sity degree/bachelor’s. While we attempted to re-
cruit widely, majority of our participants’ education
background is science-related, with strong leanings
towards technology. Furthermore, we assume that
our participants are proficient in English from their
education level and the fact that they are based in a
city that uses English as the common language. It
is possible that there are some unknown common
bias such as culture or knowledge that might affect
the results. The tie-breaking constrain in our study,
where study participants are required to assign one
word to its most coherent group, might affect the
correlation scores for the user study.
Corpora. The selected corpora are constructed
from documents that are formal in prose, with the
purpose of being informative and instructional. We
do not know if the user study results are applica-
ble to a corpus with documents that are informal
in prose, such as that of a conversational nature.
However, one can always evaluate topics on a large
external generic corpus to determine coherence rel-
ative to human judgement.
Ethics Statement
User Study. Prior to carrying out our user study,
the survey methodology was reviewed and ap-
proved by our Institutional Review Board for ethi-
cal compliance. While unlikely, we examined each
question for its appropriateness. To ensure partic-
ipants’ anonymity, the responses are anonymized
and aggregated, and it is extremely unlikely that a
participant can be identified via their response. In
terms of fair compensation, we paid S$15 for each
complete response of 100 questions, assuming an
hour’s worth of work, it is higher than our insti-
tution’s prevailing rate for undergraduate student
work. To ensure their well-being, study participants
are allowed up to a week to complete the tasks, at
their own preferred pace and place.
Corpora. We select corpora that have open li-
censing agreements that allows for non-profit aca-
demic use, and the permissions allowing us to trans-
form and re-distribute the processed corpora as
word-pair counts.References138831388413885A Algorithm Pseudocode
Pre-processing steps to reduce complexity, Algo-
rithm 1 and Algorithm 2, remain unchanged from
Yuan et al. (2022). These steps can be skipped
when the graph is large and dense, such as during
negsub-graphs generation. Our modification in
Algorithm 3 and Algorithm 4 introduces random-
ness via permutations and early stopping, when
ak-clique is found in Algorithm 3 and a desired
number of k-cliques found in Algorithm 4. The sub-
graph reduction is implemented in Algorithm 3.
Algorithm 1 PRE-CORE( G,k)
Prune vertices with less than kedges from G
Algorithm 2 PRE-LIST( G,k)
Find exact k-cliques and remove them from G
foreach connected components C∈Gdo
m← |E(C)|, n← |V(C)|
ifm= (n−1)nthen
remove Cfrom G
output k-cliques C
end if
end for
A set of connected components refers to a set
of nodes where each node shares an edge with allAlgorithm 3 SDegreeList( k,R,C,⃗G)
foru∈Permutate (C)do
if|C| ≤l−2then
continue
end if
ifk <2then
return∅
end if
ˆC←N∩C
ifk= 2∧ |ˆC|>0then
O←R∪ {u}
remove (u, u)from ⃗G∀u, u∈O
return O
end if
if|ˆC|> l−2then
return SDegreeList( k−1,R,ˆC,⃗G)
end if
end for
other nodes in the set. Finding next connected com-
ponents ˆC, requires a set intersection operation
between all possible neighbours of randomly se-
lected node u, denoted N, and current connected
components C.
Algorithm 4 Main( G,k, target)
G←PRE-CORE (G, k)
G←PRE-LIST (G, k)
Generate DAG ⃗G
O← ∅
foru∈Permutate (⃗G)do
r←SDegreeList (k−1,{u}, N,⃗G)
if|r|==kthen
O=O∪ {r}
end if
iftarget ==|O|then
return O
end if
end for
The main algorithm gets invoked once per sub-
graph, we can generate multiple sub-graphs by se-
lecting a set of words that neighbours a randomly
chosen word. We then truncate the edges that do
not fulfill the edge-conditions.
B Optimizing Position-Based Scoring
Given a set of kwords as a topic, our goal is to
optimize the position-based score. We can reduce
this problem to a weighted activity selection prob-13886lem, which is equivalent to finding a max-weight
independent set in an interval graph and can be
solved in polynomial time (Bar-Noy et al., 2001).
Consider a word wat the jposition, index
starting from 0, we can visualize the ordering as
having jincoming edges, indicating precedence of
other words, and k−j+ 1outgoing edges, indi-
cating wprecedence to other ensuing words. An
activity will be defined by its start-time (position)
and its preceding and ensuing activities. Each ac-
tivity has an equal interval and the weight of the
activity is determined by the difference of outgoing
and incoming edges to all other words scored via
m. We can transform the activities into an interval
graph, with |C| · |C|combinatorial number
of possible instances for each word per time slot in
the schedule.
Our transformation will result in an interval
graph of kdisjoint graphs. While the number of
activities might seem to be combinatorially explo-
sive, selecting the first activity at T= 0, only
involves kactivities, and upon selection prunes
multiple branches, resulting in k−jchoices at
T=j. Hence, we are only required to select the
best activity within each disjoint graph conditioned
on availability (word not selected before).
C Supplementary Tables
This section lists tables with quantitative supple-
mentary information.
Table 10 details the results for ArXiv and
Pubmed corpus for inter-metric correlation anal-
ysis in Section 4.2.
Table 11 provides additional information on the
similarity between control and treated topics for
the lemmatization effect ablation in Section 5.
Table 12 provides a detailed breakdown of sub-
graph segments that is shortlisted for the lemmati-
zation effect ablation in Section 5.
Table 13 details the full complete results for inter-
corpus correlation analysis, its partial table can be
found in Table 5, Section 5.
Table 14 has additional quantitative information
regarding the quantity of common topics in corpus-
pairs used in the inter-corpus experiments of Sec-
tion 5.
Table 15 has the individual Krippendorf’s αfor
each user study group Ufor the user study in Sec-
tion 6.
Tables 16, 17, 18, and 19 has the individual cor-
relation scores of each user study group Uto thevarious coherence metrics for Proxy Task I, II, III,
and pair-wise ablation respectively. Its averages
are tabled in Tables 7a, 7b, 7c, and 8 in Section 6.13887
corpus ArXiv ArXiv-l. Pubmed Pubmed-l. Wiki Wiki-l. Palmetto
Total 26,620 22,184 38,829 39,997 40003 40,009 16,567
ArXiv - 19,637 13,138 10,527 12,955 10,230 6,827
ArXiv-l 19,637 - 9,636 11,015 9,563 10,504 7,130
Pubmed 13,138 9,636 - 23,328 15,459 12,565 8,006
Pubmed-l 10,527 11,015 23,328 - 12,637 14,112 8,932
Wiki 12,955 9,563 15,459 12,637 - 31,047 13,136
Wiki-l 10,230 10,504 12,565 14,112 31,047 - 14,392
Palmetto 6,827 7,130 8,006 8,932 13,136 14,392 -
Groups U U U U U U U U Mean (S.D)
Kripp’s α0.463 0.391 0.323 0.376 0.325 0.366 0.333 0.347 0.366 (0.04)13888Groups U U U U U U U U Mean (S.D)
ArXiv
C0.464 0.448 -0.021 0.330 0.399 0.437 0.218 0.281 0.319 ± 0.152
C0.503 0.469 0.030 0.281 0.459 0.462 0.344 0.300 0.356 ± 0.146
C 0.475 0.426 0.073 0.392 0.516 0.470 0.304 0.270 0.366 ± 0.136
C 0.368 0.490 -0.110 0.309 0.386 0.394 0.251 0.348 0.304 ± 0.169
C 0.372 0.455 -0.157 0.285 0.355 0.383 0.208 0.231 0.266 ± 0.178
C0.348 0.476 -0.162 0.256 0.309 0.261 0.152 0.305 0.243 ± 0.176
Pubmed
C0.609 0.560 0.372 0.550 0.462 0.511 0.526 0.535 0.516 ± 0.067
C0.662 0.622 0.356 0.465 0.415 0.543 0.492 0.521 0.510 ± 0.095
C 0.574 0.605 0.396 0.534 0.453 0.498 0.548 0.560 0.521 ± 0.064
C 0.479 0.447 0.165 0.531 0.442 0.368 0.453 0.537 0.428 ± 0.111
C 0.519 0.511 0.231 0.531 0.482 0.409 0.502 0.488 0.459 ± 0.093
C0.252 0.177 -0.115 0.327 0.280 0.043 0.087 0.417 0.183 ± 0.161
Wiki
C0.692 0.715 0.413 0.758 0.607 0.670 0.692 0.664 0.651 ± 0.099
C0.719 0.739 0.348 0.727 0.631 0.673 0.702 0.678 0.652 ± 0.119
C 0.737 0.718 0.445 0.760 0.608 0.670 0.706 0.664 0.664 ± 0.094
C 0.718 0.679 0.451 0.734 0.556 0.582 0.641 0.630 0.624 ± 0.087
C 0.658 0.695 0.422 0.737 0.585 0.671 0.684 0.621 0.634 ± 0.091
C0.405 0.322 0.226 0.427 0.381 0.272 0.272 0.326 0.329 ± 0.066
Palmetto
C0.696 0.690 0.401 0.740 0.614 0.715 0.696 0.668 0.653 ± 0.101
C0.726 0.705 0.363 0.739 0.646 0.726 0.706 0.685 0.662 ± 0.116
C 0.721 0.694 0.439 0.734 0.613 0.722 0.719 0.654 0.662 ± 0.093
C 0.647 0.610 0.464 0.697 0.562 0.666 0.699 0.638 0.623 ± 0.073
C 0.635 0.628 0.404 0.703 0.573 0.690 0.663 0.656 0.619 ± 0.089
C0.409 0.205 0.210 0.324 0.290 0.201 0.200 0.317 0.269 ± 0.07313889Groups U U U U U U U U Mean (S.D)
ArXiv
C0.497 0.418 -0.028 0.308 0.383 0.463 0.200 0.289 0.316 ± 0.159
C0.534 0.438 0.027 0.263 0.448 0.497 0.332 0.305 0.355 ± 0.153
C 0.524 0.383 0.094 0.372 0.488 0.509 0.298 0.283 0.369 ± 0.135
C 0.400 0.465 -0.130 0.282 0.361 0.425 0.266 0.353 0.303 ± 0.175
C 0.401 0.420 -0.175 0.260 0.315 0.415 0.209 0.235 0.260 ± 0.182
C0.352 0.469 -0.189 0.215 0.284 0.278 0.150 0.298 0.232 ± 0.182
Pubmed
C0.607 0.530 0.408 0.529 0.470 0.520 0.510 0.514 0.511 ± 0.053
C0.663 0.574 0.399 0.444 0.431 0.538 0.486 0.520 0.507 ± 0.080
C 0.579 0.572 0.432 0.505 0.456 0.516 0.534 0.546 0.517 ± 0.049
C 0.468 0.446 0.190 0.482 0.453 0.374 0.454 0.498 0.421 ± 0.094
C 0.518 0.504 0.256 0.502 0.492 0.409 0.492 0.456 0.454 ± 0.081
C0.234 0.196 -0.130 0.280 0.290 0.028 0.096 0.367 0.170 ± 0.152
Wiki
C0.682 0.701 0.367 0.754 0.624 0.683 0.678 0.657 0.643 ± 0.110
C0.715 0.726 0.310 0.724 0.652 0.695 0.690 0.675 0.648 ± 0.130
C 0.729 0.706 0.397 0.749 0.625 0.682 0.689 0.658 0.654 ± 0.104
C 0.708 0.672 0.413 0.712 0.568 0.594 0.635 0.616 0.615 ± 0.090
C 0.645 0.679 0.373 0.733 0.598 0.677 0.670 0.613 0.624 ± 0.103
C0.397 0.311 0.210 0.398 0.365 0.278 0.288 0.311 0.320 ± 0.060
Palmetto
C0.680 0.679 0.364 0.736 0.629 0.722 0.690 0.661 0.645 ± 0.111
C0.716 0.692 0.328 0.735 0.663 0.742 0.700 0.680 0.657 ± 0.127
C 0.706 0.685 0.397 0.728 0.630 0.725 0.712 0.651 0.654 ± 0.103
C 0.630 0.605 0.428 0.688 0.577 0.662 0.707 0.633 0.616 ± 0.081
C 0.617 0.618 0.373 0.695 0.591 0.691 0.662 0.649 0.612 ± 0.096
C0.392 0.206 0.218 0.283 0.289 0.194 0.245 0.310 0.267 ± 0.06113890Groups U U U U U U U U Mean (S.D)
ArXiv
C-0.533 -0.529 0.007 -0.350 -0.485 -0.447 -0.336 -0.384 -0.382 ± 0.164
C-0.563 -0.577 -0.026 -0.319 -0.520 -0.470 -0.454 -0.391 -0.415 ± 0.168
C -0.562 -0.584 -0.019 -0.428 -0.556 -0.499 -0.433 -0.388 -0.434 ± 0.171
C -0.429 -0.546 0.144 -0.330 -0.457 -0.376 -0.340 -0.405 -0.342 ± 0.195
C -0.448 -0.536 0.169 -0.290 -0.446 -0.364 -0.325 -0.320 -0.320 ± 0.200
C-0.387 -0.442 0.129 -0.299 -0.419 -0.229 -0.214 -0.352 -0.277 ± 0.172
Pubmed
C-0.608 -0.649 -0.298 -0.636 -0.459 -0.589 -0.579 -0.556 -0.547 ± 0.109
C-0.652 -0.720 -0.248 -0.549 -0.430 -0.586 -0.576 -0.565 -0.541 ± 0.135
C -0.594 -0.705 -0.280 -0.609 -0.474 -0.577 -0.591 -0.563 -0.549 ± 0.118
C -0.506 -0.457 -0.179 -0.590 -0.416 -0.434 -0.480 -0.560 -0.453 ± 0.118
C -0.519 -0.562 -0.225 -0.589 -0.438 -0.492 -0.548 -0.499 -0.484 ± 0.107
C-0.277 -0.155 0.004 -0.327 -0.234 -0.105 -0.114 -0.408 -0.202 ± 0.126
Wiki
C-0.713 -0.655 -0.473 -0.756 -0.561 -0.691 -0.680 -0.632 -0.645 ± 0.085
C-0.751 -0.679 -0.410 -0.722 -0.602 -0.686 -0.697 -0.641 -0.648 ± 0.100
C -0.759 -0.661 -0.496 -0.755 -0.572 -0.699 -0.693 -0.646 -0.660 ± 0.084
C -0.727 -0.623 -0.496 -0.764 -0.523 -0.627 -0.645 -0.608 -0.627 ± 0.085
C -0.684 -0.636 -0.483 -0.742 -0.538 -0.697 -0.675 -0.596 -0.631 ± 0.082
C-0.387 -0.358 -0.276 -0.455 -0.371 -0.342 -0.285 -0.357 -0.354 ± 0.053
Palmetto
C-0.698 -0.641 -0.454 -0.745 -0.572 -0.739 -0.667 -0.637 -0.644 ± 0.089
C-0.734 -0.648 -0.420 -0.736 -0.600 -0.745 -0.681 -0.644 -0.651 ± 0.100
C -0.733 -0.649 -0.489 -0.737 -0.582 -0.755 -0.684 -0.638 -0.658 ± 0.084
C -0.647 -0.579 -0.497 -0.719 -0.550 -0.720 -0.647 -0.625 -0.623 ± 0.073
C -0.635 -0.587 -0.447 -0.718 -0.537 -0.714 -0.632 -0.625 -0.612 ± 0.084
C-0.387 -0.242 -0.214 -0.365 -0.296 -0.267 -0.176 -0.340 -0.286 ± 0.07013891Groups U U U U U U U U Mean (S.D)
ArXiv
C0.262 0.232 0.051 0.170 0.211 0.219 0.072 0.183 0.175 ± 0.071
C0.287 0.224 0.038 0.166 0.203 0.219 0.079 0.208 0.178 ± 0.076
C 0.257 0.215 0 .104 0.176 0.254 0.221 0.105 0.188 0.190 ± 0.056
C 0.272 0.231 0.110 0.209 0.262 0.225 0.123 0.259 0.211 ± 0.058
C 0.299 0.238 0.079 0.193 0.230 0.242 0.120 0.202 0.201 ± 0.066
C 0.218 0.152 -0.019 0.101 0.124 0.125 0.091 0.126 0.115 ± 0.062
C0.280 0.213 0.061 0.140 0.228 0.228 0.111 0.220 0.185 ± 0.068
C0.193 0.146 -0.007 0.118 0.133 0.155 0.076 0.137 0.119 ± 0.057
Pubmed
C0.328 0.321 0.221 0.335 0.269 0.256 0.280 0.340 0.294 ± 0.041
C0.314 0.281 0.213 0.295 0.272 0.235 0.261 0.331 0.275 ± 0.037
C 0.240 0.259 0.205 0.269 0.242 0.184 0.229 0.291 0.240 ± 0.032
C 0.274 0.261 0.188 0.305 0.257 0.201 0.225 0.305 0.252 ± 0.041
C 0.294 0.286 0.206 0.306 0.261 0.225 0.256 0.316 0.269 ± 0.036
C 0.183 0.160 0.063 0.140 0.109 0.112 0.134 0.210 0.139 ± 0.043
C0.114 0.086 0.087 0.132 0.116 0.061 0.044 0.167 0.101 ± 0.037
C0.078 0.090 0.009 0.111 0.098 0.056 0.016 0.121 0.072 ± 0.039
Wiki
C0.560 0.527 0.300 0.547 0.406 0.494 0.422 0.485 0.468 ± 0.082
C0.543 0.518 0.299 0.527 0.399 0.484 0.405 0.470 0.455 ± 0.077
C 0.524 0.495 0.295 0.510 0.397 0.433 0.405 0.440 0.437 ± 0.070
C 0.518 0.498 0.297 0.507 0.396 0.429 0.395 0.454 0.437 ± 0.069
C 0.526 0.503 0.299 0.517 0.393 0.469 0.410 0.460 0.447 ± 0.072
C 0.384 0.338 0.094 0.379 0.218 0.336 0.265 0.269 0.285 ± 0.091
C0.243 0.257 0.159 0.217 0.199 0.202 0.149 0.243 0.209 ± 0.037
C0.165 0.163 0.058 0.173 0.103 0.126 0.070 0.163 0.128 ± 0.043
Palmetto
C0.553 0.503 0.292 0.542 0.398 0.516 0.428 0.496 0.466 ± 0.083
C0.538 0.491 0.299 0.515 0.398 0.509 0.418 0.486 0.457 ± 0.075
C 0.524 0.479 0.294 0.508 0.394 0.472 0.424 0.454 0.444 ± 0.069
C 0.526 0.479 0.295 0.514 0.391 0.472 0.416 0.468 0.445 ± 0.071
C 0.516 0.466 0.291 0.504 0.378 0.484 0.406 0.479 0.441 ± 0.072
C 0.411 0.325 0.104 0.354 0.209 0.342 0.261 0.325 0.291 ± 0.091
C0.217 0.203 0.136 0.172 0.166 0.181 0.146 0.209 0.179 ± 0.028
C0.155 0.145 0.070 0.145 0.103 0.110 0.080 0.153 0.120 ± 0.03213892D Topic Examples (User Study)
This set of 100 topics belongs to T, and were shown to U:1389313894E User Study Instructions
E.1 Primer on Task
Evaluating the relations between words from a computational lens serves to further the research and
understanding of artificial intelligence linguistic research.
A group of words can be considered coherent if they share a similar theme. For example, the group
"apples banana coconut durian" can be considered coherent as most people would identify "fruit", "food"
or "tree" as the common theme or link.
However, some group of words might be more ambiguous and the common theme might not be as
straightforward. For example, "trees ore corn hydrogen" might be considered incoherent to some, while
others might identify the common theme as "resources".
Ultimately, it is up to one’s personal preferences and experiences to decide on whether a group of words
are coherent.
E.2 Task Instructions
You will be presented with 10 English words. These words belongs to the 20,000 most frequently used
words, so it is unlikely that you will encounter strange words. If you do encounter words that you have
never seen before, you are free to use a dictionary or search engine (e.g. Google).
You will then be asked to assign each word to groups, where each group contains words that you think
are coherent when grouped together.
Given an example: alcohol athlete breakfast drink eat habit intake meal obesity sleep
Some might divide the words into two groups identifying Group 1 is "alcoholic"-themed and Group 2
is "healthy"-themed.
Group 1 Group 2 Group 3 Group 4 Not Related
alcohol O
athlete O
breakfast O
drink O
eat O
habit O
intake O
meal O
obesity O
sleep O
In another example given: atom calcium component material reduction temperature titanium typical
weight yield
Some might group most of the words as "chemistry"-themed.
Group 1 Group 2 Group 3 Group 4 Not Related
atom O
calcium O
component O
material O
reduction O
temperature O
titanium O
typical O
weight O
yield O13895If you believe that certain word(s) do not belong in any group, select the "Not Related" option in the
last column. There can be multiple words that are not related to each other.
For example: animal bed carrot fungible great osmosis paradise star telcommunication water
Group 1 Group 2 Group 3 Group 4 Not Related
animal O
bed O
carrot O
fungible O
great O
osmosis O
paradise O
star O
telcommunication O
water O
We want to emphasise that there are no right or wrong answers for the tasks, we wish to capture your
beliefs on what you think is "correct". We understand that at times, you might encounter words that
belong to multiple groups, however to simplify the tasks, we ask that you be the tiebreaker and assign it to
the word-group with the strongest similarity.13896ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract, 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
3, 4
/squareB1. Did you cite the creators of artifacts you used?
3 - cited 4 - ours
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Ethics
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Ethics
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Ethics
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
3, Limitations, Ethics
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
3, 4, 5, 6, Appendix C, D
C/squareDid you run computational experiments?
4, 5, 6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix A13897/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
3, Appendix C
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
4,5,6
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
6
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix E, Institute Review Board application withheld for anonymity
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Ethics
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Ethics
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Ethics
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Ethics13898