
Disclaimer: The paper contains content that may be profane, vulgar, or offensive.
Jiawen DengJingyan ZhouHao SunChujie Zheng
Fei MiHelen MengMinlie Huang
Abstract
Offensive language detection is increasingly
crucial for maintaining a civilized social media
platform and deploying pre-trained language
models. However, this task in Chinese is still
under exploration due to the scarcity of reliable
datasets. To this end, we propose a benchmark
–COLD for Chinese offensive language anal-
ysis, including a Chinese Offensive Language
Dataset – COLD and a baseline de-
tector – COLD which is trained on
the dataset. We show that the COLD bench-
mark contributes to Chinese offensive language
detection which is challenging for existing re-
sources. We then deploy the COLD
and conduct detailed analyses on popular Chi-
nese pre-trained language models. We first an-
alyze the offensiveness of existing generative
models and show that these models inevitably
expose varying degrees of offensive issues. Fur-
thermore, we investigate the factors that influ-
ence the offensive generations, and we find that
anti-bias contents and keywords referring to
certain groups or revealing negative attitudes
trigger offensive outputs easier.
1 Introduction
Offensive language detection task plays an essen-
tial role in maintaining social platforms and pro-
moting civilized communication (Davidson et al.,
2017; Noever, 2018; Dinan et al., 2019; Jahan and
Oussalah, 2021). With the rise of large-scale lan-
guage models (Zhang et al., 2020; Roller et al.,
2020; Wang et al., 2020; Zhang et al., 2021b), the
safety issues due to offensive generation continue
to be exposed (Gehman et al., 2020; Xu et al., 2021;
Bender et al., 2021; Mi et al., 2022), attracting
widespread attention from researchers and push-
ing the research boom on this task to new heights(Sheng et al., 2021; Sun et al., 2021; Dinan et al.,
2021).
To tackle the problem of offensive language
detection, a reliable and versatile benchmark is
a needed basis to accelerate in-depth research.
The datasets, including WTC (Wulczyn et al.,
2017), OLID (Zampieri et al., 2019), BAD (Xu
et al., 2020) and RealToxicPrompts (Gehman et al.,
2020), are proposed to study the safety issues from
different dimensions and granularities. The pub-
licly available detector, PerspectiveAPI, is widely
used for toxicity evaluation and contributes to cre-
ating safer environments for online communication
(Han and Tsvetkov, 2020; Liu et al., 2021). How-
ever, most existing works focus on English. The
issue of Chinese offensive language detection has
not been well studied due to the lack of labeled
datasets and reliable detectors.
In addition, large-scale language models often
lean biases in pre-training data and generates of-
fensive or unethical contents (Sheng et al., 2021;
Zhang et al., 2021a), which substantially hinders
the deployment of models in practice (Sun et al.,
2021; Dinan et al., 2021). Meanwhile, limited by
reliable benchmarks, the offensiveness of Chinese
language models has not yet been thoroughly stud-
ied. How offensive can Chinese language mod-
els be? What contents influence the triggering of
offensive generation? Diving deeper into these
questions will facilitate building more reliable and
deployable language models.
This paper proposes a benchmark named COLD
to tackle the above challenges in Chinese offensive
language research. The COLD (Chinese
Offensive Language Dataset), contains 37,480
comments with binary offensive labels and cov-
ers diverse topics of race, gender, and region. To
gain further insights into the data types and charac-
teristics, we annotate the test set at a fine-grained
level with four categories: attacking individuals ,11580
attacking groups ,anti-bias andother non-offensive .
We present a baseline detector, COLD ,
for offensive language detection, which adopts pre-
trained Chinese BERT and is fine-tuned on the
proposed dataset and performs satisfactorily com-
pared to other methods using existing resources
and technology.
With the proposed benchmark COLD , we evalu-
ate the offensiveness of popular Chinese generation
models, including CDialGPT (Wang et al., 2020),
CPM (Zhang et al., 2021b), and EV A (Zhou et al.,
2021), to investigate their strengths and weaknesses
in terms of safety. Experimental results show that
both offensive and non-offensive inputs have the
risk of inducing safety issues.Additionally, some
types of prompts, including anti-bias contents, cer-
tain target group keywords and negative attitude
words, can more easily trigger offensive outputs
than other inputs. Figure 1 presents two offensive
generation examples triggered by Anti-Bias input
(a) and Offensive input (b).
The contributions of this work are threefold:
•We present, to the best of our knowledge, the
first publicly available Chinese Offensive Lan-
guage Dataset: COLD . It contains
37,480 sentences and covers the topics of race,
gender and region.
•We provide the a baseline detector, COLD- , together with discussions on existing
detection methods. We show the contribution
of the proposed benchmark to offensive lan-
guage detection.•We evaluate popular open-source generative
models and reveal their varying degrees of
offensiveness. We also show that the safety
issue can be triggered by even non-offensive
inputs, such as anti-bias languages.
2 Related Work
2.1 Offensive Language Detection
Offensive language, toxic language, and hate
speech are highly related terms with blurred bound-
aries (Jahan and Oussalah, 2021). In this paper,
we do not distinguish them and use them inter-
changeably. The contents with any form of tar-
geted offense to individuals or groups are consid-
ered offensive language . It includes veiled or di-
rect offensive content expressing rudeness, disre-
spect, insults, threats and profanity based on as-
pects such as race, religion, sex, or sexual orienta-
tion (Zampieri et al., 2019; Cambrigdge dictionary;
Davidson et al., 2017).
Automatic offensive language detection can help
detoxify the online communities and safely de-
ploy large-scale language models (Warner and
Hirschberg, 2012; Schmidt and Wiegand, 2017),
which is an important task. Abundant efforts are
seeking to detect hate speech based on automatic
identification, such as topic analysis and keyword-
based detection (Warner and Hirschberg, 2012;
MacAvaney et al., 2019). Due to the develop-
ment of deep learning and pre-trained models like
BERT (Devlin et al., 2019), data-driven methods
are gradually becoming mainstream for detecting
hate speech (Wulczyn et al., 2017; Zampieri et al.,
2019). Meanwhile, numerous works have released
large-scale resources like Kaggle Challenges on
toxicity and bias, which offers significant support
for training a strong and robust detector. However,
offensive language detection in Chinese greatly
lags behind English (Jahan and Oussalah, 2021).
Moreover, due to the specificity of Chinese culture
and linguistics, translation-based methods contain
inherent defects (Sohn and Lee, 2019). In this
paper, we release an open-source Chinese offen-
sive language dataset and corresponding automatic
detection methods, which aims to guide the devel-
opment of related Chinese community.115812.2 Model Safety Analysis
With the emergence of large-scale pre-trained mod-
els (Devlin et al., 2019; Roller et al., 2020; Rad-
ford et al., 2019), their security ethics have raised
widespread attention (Xu et al., 2020). Numerous
previous research follow the language model anal-
ysisparadigm (Petroni et al., 2019) and attempt to
mine the relational knowledge presented in train-
ing data and stored in pre-trained language models.
They construct templates like the "fill-in-the-black"
cloze statement to analyze different safety issues,
including social bias (Nadeem et al., 2020; Nangia
et al., 2020; Schick et al., 2021), toxicity (Ousid-
houm et al., 2021) and morality (Schramowski
et al., 2021). Another popular approach evaluates
model safety by simulating the conversation and
evaluating the generated responses in terms of bias
and fairness (Liu et al., 2019), political prudence
(Bang et al., 2021), and toxicity agreement (Ba-
heti et al., 2021). This method requires proper
prompts to probe the safety issues. Gehman et al.
(2020) claims that prompts with varying degrees
of toxicity can all trigger toxic outputs. This pa-
per follows the above approach to explore model’s
internal knowledge for offensive language detec-
tion and thoroughly analyze the offensiveness of
generative language models.
2.3 Offensiveness in Chinese
Data-driven methods for offensive language detec-
tion and safety evaluation are proven effective in
practice. However, there remains a dire scarcity
of relevant resources in Chinese. In Table 1, we
list, to the best of our knowledge, all relevant ex-
isting datasets in Chinese. Yang and Lin (2020)
introduced a dataset for detecting and rephrasing
Chinese profanity, which is an extension of their
previous version containing 2k sentences (Su et al.,
2017). Tang et al. (2020) released a Chinese dataset
COLA for categorizing offensive language, but it
is not (yet) available at the time of writing of this
paper. Jiang et al. (2022) proposed the first Chinese
sexism dataset for identifying gender-related abu-
sive language. More recently, Zhou et al. (2022a,b)
presented a Chinese dialog bias dataset and studied
the implicit attitudes toward targeted groups in dia-
logues. To the best of our knowledge, there is no
open-source Chinese dataset for offensive language
detection. Detoxifying in online communities and
language model generations still rely mostly on the
blacklisting mechanism, which severely limits the
development of automatic Chinese offensive lan-
guage detection. This work aims to offer resources
and set up the benchmark to support the develop-
ment of Chinese offensive language detection.
3 Dataset Construction
We present COLD , a Chinese dataset con-
taining 37k sentences and covering the topics of
racial, gender, and regional bias. Our data collec-
tion process is in line with the suggestions provided
by Vidgen and Derczynski (2020) to achieve stan-
dardized and accountable research benchmarks.
3.1 Data Source
We investigate offensive language on Chinese so-
cial platforms and popular generative language
models during the preliminary research stage. We
find that name-calling, verbal violence, and other
types of offensiveness frequently occurs in discus-
sions of social bias-related topics of racial, gender,
and regional issues. Therefore, we study offensive-
ness of these topics in this paper.
We crawl real-world data posted on social media
platforms, including Zhihu andWeibo . We analyze
the data and find that the proportion of offensive
data is sparse because the platform maintains civ-
ilized speech. This way, we collect data by two
strategies: (1) keyword querying and (2) crawling
from related sub-topics.
Keyword querying To narrow down the search
scope and increase the density of the target data,
we use the keyword querying method. Under
each topic, we pre-collect keywords that occur
frequently, such as racism ,gender bias , and re-
gional discrimination , as well as various descrip-
tive words for target groups, such as black man11582(黑人) and ni**r /ni**a (黑鬼).The collected key-
words are shown in Appendix B.1. Using them,
high-density data relating to each topic can be ob-
tained from the crawled mass data.
Crawling from related sub-topics We search
some widely discussed sub-topics in Zhihu and
directly crawl data from the follow-up comments.
Compared to keyword queries , these data are not
limited by pre-collected keywords and can provide
a more comprehensive look at user discussions on
the topic, resulting in a broader range of content
and expressions.
The collected data are post-processed (refer to
Appendix B.2) and then mixed as candidate data
for further annotation during the model-in-the-loop
collection.
3.2 Model-in-the-loop Collection
To improve the collection efficiency, we follow the
model-in-the-loop setup and train a classifier to
discover target data from the candidates. We adopt
different labeling strategies for training and test set
to improve labeling efficiency.
Training Set Collection For the construction of
training set, we semi-automatically label the data
based on the model-in-the-loop setup. Firstly, We
initialize a classifier by manually labeling 500 sam-
ples ( Offen. orNon-Offen. ) as training data. Sec-
ondly, we adopt the classifier on a bunch of unla-
beled data and predict their offensiveness. Then,
the data are ranked by predicted scores and divided
into multiple bins for sample checking. We sample
around 10% data from each bin and manually label
them with the following strategy: (1) If the accu-
racy of the predicted labels is up to 90%, data in
the bin is directly added to the training set; Other-
wise, (2) the bin is manually relabeled entirely and
then added to the training set. By this means, we
iteratively update the classifier and training set for
6 rounds. Details can be found at Appendix B.3.
Test Set Collection To ensure the reliability of
test set annotation, we pick data from different
probability intervals and manually annotate them.
To give annotators a deeper understanding of our
task, we further categorize the data and conduct
more fine-grained annotation. The category of
Offensive is subdivided into (1) Attack Individu-
alsand (2) Attack Groups according to what is
targeted/attacked in the content (Waseem et al.,
2017; Vidgen and Derczynski, 2020), while Non-
Offensive is subdivided into (3) Anti-Bias and (4)
Other Non-Offensive . (Definitions of fine-grained
categories are detailed in Appendix C)
3.3 Human Annotation
We employed 17 native Chinese native workers
for labeling. They are evenly distributed by gen-
der (9 males and 8 females) and come from var-
ious regions of China. Following the annotation
suggestions provided by Vidgen and Derczynski
(2020), we iteratively develop guidelines and train
annotators to ensure the quality of annotation. The
remuneration for annotators is 60 CNY per hour.
For higher efficiency, auto-labeled training data
in each bin is checked and corrected by one anno-
tator. For quality assurance, each sample in test
set is assigned to three annotators, and the label
with the most votes becomes the final. We compute
the Inter-Annotator Agreement of the test set. The
Fleiss’ κ(Fleiss, 1971) of 2-class ( Offen. orNon-
Offen. ) is 0.819 (almost perfect agreement) and
4-class ( Attack Individuals/Groups ,Anti-Bias , and
Other Non-Offen. ) is 0.757 (substantial agreement).
More details of Data Collection andAnnotation
Guidelines are given in Appendix B and C.
3.4 Data Analysis
Table 2 and 3 give a snapshot of basic statistics of
COLD the fine-grained annotated test data.
To further understand the features of collected data,
we study whether the data are topic-related based
on the collected keywords listed in Appendix B.1.11583
If the sample contains keywords under a certain
topic, it is considered topic-related. We show the
number of sentences under each topic in Table 4.
As Table 4 shows, the collected data is relatively
evenly distributed among the three topics. About
10% of the data do not contain topic-related key-
words ( None ). These data are collected by subtopic
crawling, making our data distribution closer to
actual scenario. Table 4 also reveals the presence
of overlap between topics. For example, the over-
lap of race andgender exists in the sentences dis-
cussing African American Women , and the overlap
ofregion andgender exists when discussing rural
girls. This overlap makes this dataset more diverse
and consistent with the real scenarios.
4 Offensive Language Detection
The experiments of offensive language detection
are designed to verify the following two questions:
Can offensive language be detected with existing
resources and technology alone? Does proposed
COLD effectively advance the offensive
language detection task?
4.1 Experimental Setup
The aim of the offensive language detection task
is to assign the label y(Offen. orNon-Offen. ) to
the given text x. To investigate how well offensive
language is detected with the proposed dataset and
other existing resources, several detection methods
are evaluated.
COLD We train COLD on
the proposed COLD for offensive language
detection. COLD adopts transformer-
based architecture and is based on pre-trained
model BERT (Devlin et al., 2019). We use the
version of bert-base-chinese, which has 12 layers
and 12 attention heads, as the backbone of our de-
tector. According to the classical fashion, given a
textx, we add a special token [CLS] before thetext and input to the BERT model. We take out the
first hidden-state (corresponding to [CLS] ) in the
last layer of BERT as the sentence representation:
E=f (x), and then it is further processed
by a Linear layer: p=f(E) =sigmoid (W·
E+b)to generate the final prediction. All the
parameters in f (·)andf(·)are trained on
COLD with cross-entropy loss function:
L=/summationtext−[y·log(p) + (1 −y)·log(1−p)].
TranslJigsaw Detector ( TJD)Consider-
ing the lack of Chinese dataset, we explore the
performance of translated data from English to Chi-
nese (Shi et al., 2010; Nozza, 2021; Markov et al.,
2021). We pick a subset of 59k data labeled with
toxicity and a variety of identity attributes from the
Kaggle competition. (Jigsaw, 2018). Then they are
translated to Chinese with the Baidu General Trans-
lation API, and the statistical information is shown
in Appendix D.2. Similar with COLD ,
TranslJigsaw detector is obtained by fine-tuning
BERT model on the translated data.
Prompt-based Self-Detection ( PSD)In-
ternal knowledge stored in the model during pre-
training can be used to detect offensive language
(Schick et al., 2021; Prabhumoye et al., 2021).
We explore the performance of prompt-based self-
detection under the zero-shot setting. Given the
textTEXT to evaluate, the input of the model is
formulated as: "TEXT" Please judge whether the
above content is malicious or offensive? Answer:
[MASK]. BERT Model (Devlin et al., 2019) is used
to predict the scores of [MASK] token. We take the
scores of candidate words of 可(yes) and否(no)
as the self-detection results. An example is given
in the Appendix D.1.
Baidu Text Censor ( B TC)As a public
API, Baidu Text Censor aims to identify harmful
content including pornography, violence, terrorism,
political sensitivity, and abuse.11584Keyword Matching ( KM)Keyword
matching is frequently used in offensive language
filtering for safety maintenance of social platforms.
In this work, we use 14k sensitive words released
on Github, and the text containing any word in
this word list is considered offensive.
Random In the random setting, the label of of-
fensive or non-offensive is randomly assigned.
4.2 Performance of COLD
We present the results on the test set of COL-
D in Table 5. The proposed COLD - obtains the best performance ( 81% accuracy)
among all the methods and outperforms the second
place ( B TC) by a large margin ( 18% absolute
improvement in accuracy). These comparison re-
sults indicate that our benchmark can effectively
advance the offensive detection task in online com-
munities.
To further explore the detection performance,
we compare the three best-performing methods on
recognizing the labeled four subcategories, the re-
sults are shown in Table 6. COLD per-
forms well in detecting the sub-categories of Offen.
(79.51% and 85.49% accuracy of Attack individual
andAttack group ), indicating that COLD
is able to discover offensive samples well com-
pared to the other methods, contributing to higher
recall of Offen. (85%). The higher accuracy of
Other Non-Offen. (81.06%) indicates that COLD- can well distinguish Offen. from Other
Non-Offen. However, the accuracy of Anti-Bias is
only 38.32%, indicating that COLD are
easily tricked by Anti-Bias data and mis-classify
them as Offen. , affecting the precision of recalled
Offen. samples (72%).
In light of the challenges to classify Anti-Bias
data, we further analyzed the samples that suc-
cessfully fooled COLD . We observe
that a common form of expression in Anti-Bias
contents is acknowledgment followed by denial ,
e.g., " Women are often discriminated against in
the workplace, but I don’t think it’s right. ". Such
expressions can easily deceive the classifier into
focusing solely on the first half of the content and
ignoring the anti-bias statements following, leading
to incorrect predictions.
Though achieving satisfying performance ( 81%
accuracy), COLD still lags far behind the
performance of human experts as well as Englishtoxic detectors (Hanu and Unitary team, 2020).
First, the proposed detector is obtained by sim-
ply fine-tuning the BERT model and thus performs
slightly worse on discovering the covert offensive-
ness and anti-bias samples, which depends more
on the support from labeled implicit offensive data
(Lees et al., 2021). Second, our training data is col-
lected semi-automatically. Although sample check-
ing can ensure the accuracy of assigned labels to
a certain extent, it will inevitably introduce noise
through unchecked data. We believe that if all data
in the training set can be manually annotated in
the future, there will be improvements in detection
performance.
4.3 Offensive Language Detection with
Existing Resources
We analyze the performances of baselines based on
existing resources and find that it is challenging to
achieve satisfactory performance on this task only
relying on existing resources.
Discussion of Baidu Text Censor As the re-
sults in Table 5 and Table 6 show, B TCcan
hardly discover the offensive contents in COL-
D . The accuracy of B TCidentify-
ingAttacking Individual/Group asOffen. is only
21.39%/28.47%, while the accuracy of identifying
Anti-Bias /Other Non-Offen. asNon-Offen. is as
high as 83.08%/91.48%. The results indicate that
this task is challenging for B TC, which tends
to recognize most of the contents in COLD
asNon-Offen. , resulting in a low recall ( 22%) of
offensiveness. Second ,B TCis susceptible
to rude words. Sentences containing dirty words
tend to be detected as offensive. For example, the
Non-Offen. sentence "Oh my f**king god! This
black guy is awesome!" , which expresses praises
for black men, is recalled as Offen. byB TC
due to the sensitive words. These false recalls lead
to the relatively low precision (59%) of detecting
offensive contents.
Discussion of TranslJigsaw Detector The re-
sults in the Table 5 show that TJDperforms
well on recalling offensive sentences ( 72%), but the
performances on other metrics are unsatisfactory.
We further explore the compatibility of TranslJig-
saw data on this task, and the results are shown in
the Table 7. The detector trained on TranslJigsaw
performs well on the TranslJigsaw test set ( 91% ac-
curacy), while the performance drops precipitously
on the test set of COLD (60% accuracy).11585
Even mixing TranslJigsaw and COLD as
training data, the performance has no improvement
compared to the only COLD case (both
are 81% accuracy). It shows a significant gap be-
tween the translated data and the original Chinese
data. Firstly, there are language-specific character-
istics due to different cultural backgrounds (Nozza,
2021). Secondly, there is a noise produced during
machine translation process. The dataset proposed
in this paper relieves the resource limitations, con-
tributing to Chinese offensive language research.
Discussion of Prompt-based Self-Detection
As shown in the results, the performance of
PSD(59% accuracy) is better than R- andKM, demonstrating the potential
of mining the internal knowledge of the language
model for detection tasks. However, its contribu-
tion is far inferior to supervised learning-based
approaches ( 81% accuracy of COLD ).
Previous work show that exploring the appropriate
word pair and given prompt can effectively con-
tribute to the performance of self-detection (Schick
et al., 2021; Prabhumoye et al., 2021). We compare
different ways of prompt construction and present
results of the best practice in Table 5. Detailed
exploration of other prompts and word-pairs are
included in Appendix D.1.
Discussion of Keyword Matching The results
in Table 5 show the unsatisfactory performance ofkeyword matching ( 54% accuracy). Firstly , the
coverage and quality of the keyword list are de-
cisive for the detection accuracy. However, with
the continuous emergence of new words and the
diversification of vocabulary, achieving complete
coverage is almost impossible, leading to the low
recall of Offen. (63%).Secondly , it is inaccurate
to filter potentially sensitive samples by matching
keywords due to the potential occurrence of those
words in both Offen. andNon-Offen. samples.
Therefore, even if the text contains a sensitive word,
it does not necessarily express toxicity, which leads
to low precision ( 44%). Detailed analyses of the
occurrence of sensitive words in Offen. /Non-Offen.
contents are presented in Appendix D.3.
5 Evaluation of Generative LMs
With the proposed COLD andCOLD- , we evaluate the offensiveness of popular
Chinese generative language models. We mainly
investigate the following research questions. RQ1 :
How offensive are the Chinese generative language
models? RQ2 : What type of prompts can trigger
offensive generation?
5.1 Evaluation Metrics
We use the sentences in COLD as input
prompts and COLD as detector to evalu-
ate the offensiveness of generated content from the
evaluated models. We calculate the offensive rate
of each model, which is the proportion of offensive
generations among the total generations. A lower
offensive rate indicates lower offensiveness of the
model.
5.2 Evaluated Models
We evaluate the following publicly available Chi-
nese generative language models for offensiveness:11586
•CPM (Zhang et al., 2021b), a Chinese Pre-
trained Language Model with 2.6 billion pa-
rameters and 100GB training data. We evalu-
ate the versions of CPM-Generate and CPM-
Generate-distill.
•CDialGPT (Wang et al., 2020), a Chinese
dialog model (with 104M parameters) trained
on a cleaned conversational dataset LCCC . We
evaluate CDialGPT-Base and and CDialGPT-
Large models.
•EV A (Zhou et al., 2021), the largest Chi-
nese dialogue model (with 2.8B parameters)
trained on 1.4B Chinese dialogue data.
5.3 Evaluation Results
The automatic and human evaluation results of lan-
guage models are shown in Table 8 and Table 9.
Examples of offensive generations are shown in
Appendix E.2.
RQ1: Offensiveness of Different Models The
results in Table 8 show that each model has dif-
ferent degrees of offensiveness. CPM-Generate
has the greatest offensive exposure with the overall
offensive rate of 32.14% and even up to 43.43%
under offensive prompts. In the meantime, we ob-
serve that CDialGPT and EV A are much safer than
CPM models. Two reasons can be behind this:
First, the training data of CDialGPT and EV A is
strictly cleaned, and many offensive remarks are fil-
tered, contributing to safer generation (Wang et al.,
2020; Zhou et al., 2021). Secondly, CPM tends
to generate longer sentences, as shown in Table 8,
leading to risker exposure to offensive generation.
RQ2: Offensiveness Triggered by Different
Prompts As shown in Table 8 and Figure 2, both
Offen. andNon-Offen. prompts lead to numerous
offensiveness. Moreover, the fewer-studied Anti-11587
Bias inputs show a shockingly high risk of trig-
gering offensiveness. To investigate what contents
trigger risk, we conduct further studies of CPM-
Generation model by designing template-based in-
puts. The details are shown in Appendix E.1. We
find that offensive generation is sensitive to the fol-
lowing factors ：1) Target group keywords . The
model is significantly biased against some groups
such as feminist andblack man , and tends to gener-
ate more toxic outputs with these inputs than others
such as teenage girls , indicating the inherent bias
of the model. 2) Negative attitude words . There
is a higher offensive ratio when negative attitude
words appear in the prompt. For example, there
are higher ratios of both disgust andnot disgust
than not like .Anti-bias contents promote fairness
and oppose bias. They are more likely to contain
the above-mentioned target group keywords and
negative attitude words than other non-offensive
inputs, which explains why anti-bias inputs trigger
more offensive generations.
6 Conclusion
We present a new dataset named COLD
for Chinese offensive language analysis. We show
that the proposed COLD trained on our
data can effectively detect offensive content. It
can also be used as a benchmark for the offensive-
ness evaluation of language models. We evaluate
some popular used models and reveal that they
have different degrees of risk in generating offesive
contents. Besides, our work shows that, for lan-
guage models, non-offensive input can also induce
safety problems as offensive input, and is worth the
same attention. In particular, anti-bias language,
which is non-offensive but has hazards comparable
to offensive input, is often overlooked in existing
work.
We hope this new benchmark can provide the
basis for safety research in Chinese and shed light
on further studies. We call for more research toexpand the scope of offensive and other unsafe lan-
guage detection. Besides, we believe that, further
investigating on what types of input successfully
induce unsafe generation, will facilitate the safer
deployment of language models.
7 Acknowledgment
This work was supported by the National Sci-
ence Foundation for Distinguished Young Scholars
(with No. 62125604) and the NSFC projects (Key
project with No. 61936010 and regular project with
No. 61876096). This work was also supported
by the Guoqiang Institute of Tsinghua University,
with Grant No. 2019GQG1 and 2020GQG0005,
and sponsored by Tsinghua-Toyota Joint Research
Fund.
Ethical Considerations
Our work is a forerunner of a relatively comprehen-
sive benchmark for the study of offensive speech
in Chinese. However, our proposal may have the
following omissions and shortcomings.
•Our dataset may contain mislabeled data due
to the subjectivity of manual annotation. In
addition, our training set adopts the semi-
automatic annotation strategy and the incom-
plete data annotation also increases the label-
ing error. We appeal to data users to optionally
re-annotate the semi-automated labeled train-
ing data if required.
•We clearly understand that our dataset focuses
only on common topics of race, gender, and re-
gion, with limited data coverage and a simple
annotation schema. We do believe that con-
structing a greater dataset covering more top-
ics with a more fine-grained taxonomy would
contribute to a more robust Chinese offensive
detector, deserving more effort in future work.
•We are mindful that our benchmark detector
cannot detect all types of offensiveness due to
the limitation of data coverage and the training
techniques of the neural network.
All the data in the proposed benchmark is col-
lected from publicly available social platforms. We
strictly follow the protocols for the use of data
sources. The contents in our dataset do NOT repre-
sent our views or opinions.
Our resources and analyses are intended to help
create more harmonious online communities and11588promote the safer deployment of language models.
We acknowledge that it would also be misused in
problematic scenarios to create more offensive lan-
guage or make someone uncomfortable. However,
we believe that the proposed benchmark creates
more value than risks towards creating more har-
monious online communities and building more
reliable language models.
Limitations
This paper tackles the issues of Chinese offensive
language detection. In the section of Ethical Con-
siderations , we claim that the proposed dataset
has potentially mislabeled data and is limited in
data coverage, and the detectors fine-tuned on this
dataset can not ideally detect all offensive cate-
gories. We also discuss the ethical considerations
of data collection and data usage. Besides the
above-mentioned ethical concerns, we acknowl-
edge the following limitations of our work.
Limitation of contextual information Our work
is mainly devoted to studying the offensiveness at
sentence level and therefore contextual informa-
tion is not included in the proposed COLD .
We do believe that offensive expression in context-
sensitive scenarios (e.g., dialogue) would be more
challenging and require further exploration.
Limitation of baseline models In the offen-
sive language detection experiments (Section
4), we take BERT-base-Chinese as the back-
bone model for the three baseline models
(COLD ,TJD, and PSD) to
demonstrate the contribution of our dataset. We
acknowledge that adopting more backbone models
(e.g., mBART and xlm-Roberta) would contribute
to a more solid comparison, which is worth explor-
ing in more depth in the future.
References115891159011591A Data Statement
To enable researchers to better understand and use
our dataset, we present the data statement following
the professional practice for NLP systems devel-
oped by Bender and Friedman (2018).
Dataset We present a Chinese Offensive Lan-
guage Dataset ( COLD ) in this paper, which
containing 37,480 sentences and covering the top-
ics of racial, gender, and regional bias. There are
32,157 training data, which are semi-automatically
labeled with offensiveness labels ( Offen. orNot-
Offen. ). The test set contains 5,323 data, and they
are manually labeled with fine-grained categories,
including Attack Individual ,Attack Groups ,Anti-
Bias andOther Non-Offen.
Speaker The data in COLD is collected
from social platforms of Zhihu andWeibo and the
users who post on these platforms are the Speakers
who generate the data.
Annotator We employ 17 native Chinese native
workers for labeling, including 9 males and 8 fe-
males come from various regions of China, includ-
ing Henan Province, Beijing, and Northeast part
that are widely talked about in region discrimina-
tion and other less discussed regions. They are
highly trained on our offensive language annota-
tion task.
Curator The authors act as curators, determine
the scope of data collection, define the taxonomy,
design annotation guidelines, train annotators, and
control the quality of annotated data.
NLP System We design the rule-based methods
for data crawling and post-processing. Our an-
notation task is aided by an iteratively-optimized
classifier, which picks out candidate data that need
to be further manually annotated.
Stakeholders The researchers engaged in the
Chinese offensive language study will be the direct
stakeholders, and proposed COLD will ef-
fectively support their further research. The man-
agers of social platforms can use this dataset to
optimize their detector, contributing to better offen-
sive language filtering. Meanwhile, COLD
contributes to language model developers evalu-
ating their models’ offensiveness and facilitating
safer deployment.B Details of Dataset Construction
B.1 Keyword query
The collected keywords under each topic are given
in Table 10. They are used to obtain high-density
data from the crawled mass data.
B.2 Post-Processing
For the crawled data, we only keep samples of
length between 5 and 200 tokens. We clean the
noise and unusual characters, including emojis,
URLs, usernames and white space, and then de-
duplicate the samples so that the collected data is
more conducive to the data analysis in this task.
After post-processing, the remaining data will be
automatically selected and labeled in the model-in-
the-loop setup.
B.3 Model-in-the-loop Collection
We adopt the model-in-the-loop setup to discover
the target data and optimize the classifier perfor-
mance. The main flow is shown in Figure 3. A
small amount of data is manually labeled as the
initial data (500 sentences), and the following steps
are iteratively performed in each round: (1) Train
the classifier; (2) Select candidate training and test
data for annotation, respectively; (3) Update the
training/test data set.
Classifier We use BERT model with 12 lay-
ersin data collection, which has shown strong11592
power in natural language processing tasks (De-
vlin et al., 2019). Parameters of COLD
are optimized by BertAdam optimizer with a lin-
ear warmup (proportion 0.05) and a decay sched-
ule. We set the learning rate as 5e-5, batch size as
64, and max training epoch as 30. Early-stopping
mechanism is used to avoid overfitting. In each
round, the classifier is fine-tuned with updated data
from previous rounds. The performance of the
classifier is given in Figure 4, which shows that
after the second round, the performance tends to
increase steadily as the scale of data increases.
Dataset The expansion of the dataset is per-
formed in 6 rounds. In the first five rounds, both
training and test data are expanded, while only the
training data is expanded in the sixth round, as
shown in Figure 6 and 7.
It should note that the classifier is not reliable
in the beginning, and it is difficult to obtain high-
confidence predictions. For example, the accuracy
is only 58% in the first round. It is challenging to
learn a good decision surface due to the limitation
of the data scale. The prediction probability is
concentrated between 0.2 and 0.5, and the classifier
tends to predict all data as Non-Offensive . So, wepick data from this interval for annotation and the
returned data will boost the performance of the
classifier.
After the third round, the classifier’s perfor-
mance gradually stabilized, and the accuracy of the
predicted high-scoring samples steadily increased.
Therefore, we tend to select more data from high-
scoring samples to improve the efficiency of data
collection.
C Annotation Guideline
We provide annotators with annotation guidelines,
as shown in Figure 5. Annotators are first requested
to judge whether a given sample is offensive (Q1).
Then, The offensive samples are further divided
intoAttack individuals orAttack Groups according
to the target offended (Q2), while the Non-offensive
samples are divided into Anti-Bias orOther Non-
Offensive . For the training set, the annotator is only
required to answer the first question (Q1) to check
and relabel the automatically annotated samples.
We consider different categories referred to in
annotation guidelines as follows. More examples
can be found in Figure 5.
Offensive In this paper, we consider any form
of targeted attacks on individuals or groups to be
regarded as Offensive language. It includes im-
plicit or direct offensive content that is rude, dis-
respectful, insulting, threatening, profane, as well
as any other toxic content that makes others un-
comfortable or provokes further intense offensive
feedback. (Zampieri et al., 2019; Cambrigdge dic-
tionary; Davidson et al., 2017). Further, based on
the target, Offensive is subdivided into Attack Indi-
viduals andAttack Groups following Waseem et al.
(2017).
•Attack Individuals , mainly refers to offen-
sive content directed at individuals, and the
target is often referred to by a specific name
or a pronoun.
•Attacking groups , mainly refers to offensive
content towards generalized groups based on
their social identities related to race, religion,
gender, etc.
Non-Offensive Non-Offensive is subdivided into
Anti-bias andOther Non-Offensive . We make this
division because Anti-Bias is beneficial to fight of-
fensive language and maintain a harmonious com-
munication environment, which deserves further
study than other non-offensive speech.1159311594
•Anti-Bias , mainly refers to the expression
countering offensiveness, which is usually
considered fairness, fact-based contents ex-
pressed in a positive or neutral mood.
•Other Non-Offensive , refers to the non-
offensive contents other than anti-bias speech.
D Details of Offensive Detection
D.1 Prompt-based Self-Detection
Figure 8 gives an example of self detection. Bert-
base-chinese is taken as the model to predict the
scores of [MASK] token and we take the scores
of candidate words of 可(yes) and否(no) as the
results of self-detection.
We call for further research to explore the inter-
nal knowledge of language models to facilitate this
task, and the following tips can be considered. Thefirst is exploring appropriate word pairs. "Yes/No"
is often used in English (Schick et al., 2021), but
the candidate word pairs in Chinese are more var-
ied. We have explored the alternative word pairs
in Chinese, and the results are shown in Table 11,
indicating that different word pairs have signifi-
cant impacts on the results. Second , the detec-
tion performance is directly related to the given
prompt. Under the few-shot setting, it was found
that prompt-based methods can achieve results sim-
ilar to, even better than, fine-tuned models (Prab-
humoye et al., 2021). We call for more research to
investigate prompt-based self-detection methods to
further enhance their ability of offensive language
detection.11595
D.2 TranslJigsaw Detector
To explore the performance of translated data on
this task, we pick the data released for the Kaggle
competition Jigsaw Unintended Bias in Toxicity
Classification(Jigsaw, 2018). This dataset con-
tains 1.8 million data annotated by human raters.
A subset of the data is labeled with various iden-
tity attributes related to sexual, religious, racial,
and disability bias. We pick 59k data according
to whether it is toxic and bias-topic related, and
then translate them from English to Chinese. The
statistical information of translated data is shown
in Table 12.
D.3 Keyword Matching
The keyword matching method shows unsatisfac-
tory performance in the offensive detection task on
the proposed COLD . The main reason is
that sensitive words may appear in both offensive
and non-offensive sentences, as shown in Figure 9.
Some cases are given in Table 13.
As can be seen from Figure 9, most of the sen-
sitive words appear in both the offensive and non-
offensive samples, as shown in the region ①. Even
some sensitive words with strong offensive fre-
quently appear in anti-bias (non-offensive) content,
as shown in region ②. Although there are some
sensitive words that appear only in the offensive
samples, as shown in region ③, we believe that
these keywords will likewise appear in the non-
offensive sample when the scale and coverage of
theCOLD are large enough. Such results
suggest that it is challenging to rely solely on key-
word matching for offensive speech detection.
E Details of Evaluation
E.1 Impact factors of offensive generation
To further explore the impact factors of offensive
generations, we collected 103 target group key-
words and 9 templates, and constructed a total of
927 prompts. For each prompt, 20 responses are
generated by CPM-Generate model (max length
is set to 200 tokens). The offensive ratio of each
keywords is shown in Figure 11 and that of each
prompt is shown in Table 14. We also analyze
the influence of the length of generated contents11596
and the results in Figure 10 indicate that the longer
generations bring greater offensive risk.E.2 Case study
Offensive generations detected by COLD - As shown in Table 15, we list some exam-
ples of offensive generations discovered by pro-
posed COLD . These examples show that
both Offen. andNon-offen. contents can trigger
Offen. generations.
Failure cases of offensive generation detection
The proposed COLD effectively discov-
ers offensive languages in generated texts. How-
ever, as (Sun et al., 2021; Dinan et al., 2021)
pointed out, in dialogue scenarios, the system tends
to cater to users and generate responses of toxic-
ity agreement. Our COLD focuses on
sentence-level offensive language and is insuffi-
cient to detect context-sensitive cases. Some failure
cases are shown in Table 16. Further research will
be conducted on offensive analysis in dialog scenar-
ios, along with the proposed sentence-level COLD- , to formulate more rigorous strategies to
ensure the safe deployment of generative models.115971159811599