
Yupeng Zhang, Hongzhi Zhang, Sirui Wang, Wei Wuand Zhoujun LiBeihang University, Beijing, ChinaMeituan Inc., Beijing, China
{G0vi_qyx, lizj}@buaa.edu.cn
{zhanghongzhi03, wangsirui, wuwei30}@meituan.com
Abstract
A wide range of NLP tasks benefit from
the fine-tuning of pretrained language mod-
els (PLMs). However, a number of redundant
parameters which contribute less to the down-
stream task are observed in a directly fine-tuned
model. We think the gap between pretraining
and downstream tasks hinders the training of
these redundant parameters, and results in a
suboptimal performance of the overall model.
In this paper, we present PATS ( Perturbation
According ToSensitivity), a noisy training
mechanism which considers each parameter’s
importance in the downstream task to help
fine-tune PLMs. The main idea of PATS is
to add bigger noise to parameters with lower
sensitivity and vice versa, in order to activate
more parameters’ contributions to downstream
tasks without affecting the sensitive ones much.
Extensive experiments conducted on different
tasks of the GLUE benchmark show PATS can
consistently empower the fine-tuning of differ-
ent sizes of PLMs, and the parameters in the
well-performing models always have more con-
centrated distributions of sensitivities, which
experimentally proves the effectiveness of our
method.
1 Introduction
With a huge number of model parameters and well
designed training objectives, pretrained language
models (PLMs) have brought a new era to NLP
(Guu et al., 2020; Liu, 2019; Zhu et al., 2020b;
Qiu et al., 2020). Fine-tuning PLMs such as BERT
(Devlin et al., 2019) has become a basic and effec-
tive way in many downstream tasks (Wadden et al.,
2019; Sun et al., 2019; Howard and Ruder, 2018).
However, recent study has shown that aggres-
sive fine-tuning can induce an unstable and subop-
timal performance of the models especially with
insufficient data (Dodge et al., 2020; Raffel et al.,
2019), which attracts some researchers to figureout the culprits and explore effective methods to
solve them (Peters et al., 2019; Houlsby et al., 2019;
Mosbach et al., 2020). For example, there are some
regularization methods like RecAdam (Chen et al.,
2020) and Mixout (Lee et al., 2020), and adversar-
ial training techniques like SMART (Jiang et al.,
2020) and FreeLB (Zhu et al., 2020a) to alleviate
the overfitting of data in downstream tasks; Be-
yond that, Wu et al. (2022) proposed NoisyTune
with the argument that in addition to the overfitting
of the limited downstream data, there could also
exist overfitting in pretraining tasks, which could
result in enormous gaps between pretraining and
downstream task data. In order to overcome the
gaps, NoisyTune simply adds some noise to pa-
rameters in the PLM before fine-tuning. Besides,
it has also been demonstrated that the existence
of a large number of redundant parameters could
also be a factor in the suboptimal performances
of aggressively fine-tuned PLMs (Fan et al., 2019;
Sanh et al., 2020; Dalvi et al., 2020). Consider-
ing the redundant parameters in a model are not
insufficiently trained, Liang et al. (2022) proposed
a learning rate scheduler named SAGE in which
larger learning rates are assigned to these param-
eters of low sensitivity (a measure of parameter’s
importance to downstream tasks).
There could be some connection between the
gaps caused by overfitting of pretraining tasks and
the redundancy of parameters. We consider it could
be the gaps between pretraining and downstream
tasks that hinder the training of these redundant
parameters. SAGE enlarges the learning rates of
insensitive parameters to help their training. How-
ever, with the sensitivity measurement considered,
the insensitive parameters usually have smaller gra-
dients, so enlarged learning rates may help them
little to escape the sub-optimal areas compared
to involving additional noise. One noisy training
method to alleviate the gaps is NoisyTune, in which
parameters of a matrix in a PLM are added with3680noise according to the standard deviation of the
matrix before fine-tuning. Nevertheless, there are
few explanations about why or whether the param-
eters in the same matrix should be perturbed with
the same intensity. Considering different parame-
ters have different contributions to the model, noise
from a unified distribution may disturb knowledge
of some sensitive parameters, resulting in a loss
of performance. Besides, since each task needs to
capture an appropriate textual pattern and the data
of it usually comes from a special domain, differ-
ent downstream tasks could have different kinds
of gaps with those of the pretraining. So the noise
added to overcome the gaps should also be related
to the downstream task data.
In this paper, we propose a novel parameter-
wise noisy fine-tuning method called PATS
(Perturbation According ToSensitivity) to make
full use of perturbation on parameters to handle
the problems above. We focus on balancing the
contributions of all parameters in the model by ac-
tivating the insensitive ones to play better roles in
downstream tasks. So the main idea of our method
is adding different intensities of noise to parame-
ters according to their sensitivity when fine-tuning
PLMs, different from NoisyTune (Fig. 1 (b)) in
which noise added to a matrix of parameters is
from a unified distribution and unrelated to down-
stream task data. Specifically, during fine-tuning
in PATS (Fig. 1 (c)), larger noise will be added to
the parameters with lower sensitivity (such as the
parameter shown in red), while sensitive parame-
ters (such as the parameter shown in purple) will
be barely perturbed.
Our contributions can be summarized as follows:
1) We propose a simple but effective method to
help all parameters be trained sufficiently when
fine-tuning PLMs in downstream tasks. 2) Among
all the training methods with noise, PATS is the
first sensitivity-aware one which perturbs models
with noise of different distributions according to pa-
rameters’ sensitivity, to the best of our knowledge.
3) Extensive experiments on the GLUE benchmark
show PATS makes a difference in boosting the per-
formance of PLMs in downstream NLP tasks.
2 Approach
In this section, we present our PATS for PLMs
fine-tuning. Previous matrix-wise noisy methods
perturb a PLM by adding noise from a uniform
distribution to a matrix of parameters. Different
from them, in PATS, each parameter even from
the same matrix will be paid to different attention
according to its sensitivity. It is also worth noting
that in PATS, a PLM is not perturbed in advance
like NoisyTune, instead the perturbation happens
during training as the task data comes in. In the fol-
lowing sections, we will introduce the calculation
of parameter sensitivity first and then present the
noisy learning mechanism in detail.
2.1 Sensitivity Measurement
The sensitivity of a parameter is used to measure
the change of the output or loss after setting it to
zero (Molchanov et al., 2017, 2019; Ding et al.,
2019; Xiao et al., 2019; Lee et al., 2019). To be
specific, given a BERT-like pre-trained language
model Mwith parameters Θ={θ, θ,···, θ} ∈
R, the sensitivity of the j-th parameter θis writ-
ten as s, which can be defined as:
s=|L(Θ)− L(θ,···, θ,0, θ,···, θ)|
≈ |θ∇L(Θ)|, (1)
where Lis a loss function and we use the first-
degree Taylor polynomial to approximate signor-
ing the higher order remainder to accelerate the
calculation of it.
In order to avoid huge oscillation of scaused
by an abnormal batch of data, we adopt the expo-3681nential moving average of sused in many other
models and optimizers (Liang et al., 2022; Klinker,
2010; Zhuang et al., 2022; Shahidi et al., 2020)
as the real sensitivity indicator, which can be ex-
pressed by the following equation:
¯s=β¯s+ (1−β)s, β∈(0,1), (2)
where ¯sand¯sare the exponential moving aver-
age of sin the current and previous iteration. βis
a hyper-parameter used to adjust the importance of
scalculated by the current batch of data.
2.2 Training with Noise
Algorithm 1 PATS for Adamax(max( ·) returns a
matrix with the maximum values of each element of
the input matrices or vectors; sum( ·) returns a scalar
equal to the sum of all values of a matrix or vector;
Idenotes an all-ones matrix; ⊙denotes Hadamard
product and ⊘denotes Hadamard division)
Input: Step size α; Model parameters Θ∈R;
Number of training iterations T; Number of
parameters in the current matrix N; Exponen-
tial decay rates β, β, β∈[0,1); Basic noise
λ; Minimum effective sensitivity indicator γ; A
small number that prevents an error of divid-
ing by zero ϵ∈(0,1); Data D; Loss function
L(·).Initialize/tildewideS←0∈R.Initialize M←0∈R.Initialize U←0∈R.fort←1toTdod←− D .G← ∇L(d,Θ).S←Θ⊙G.M←βM+ (1−β)G.U←max(βU,|G|)./tildewideS←β/tildewideS+ (1−β)S. R←λmax(sum(/tildewideS)I⊘(N/tildewideS+ϵI)−
γI,0). Q∼ N(0,R). Z∼ B(N, p). Θ←Θ−(α/(1−β))M⊘U+
Q⊙Z. t←t+ 1end for
Our goal is to mainly activate the contributions
of less sensitive parameters by perturbing them
with bigger noise and leave parameters with larger
sensitivity less affected at the same time. In ourframework, we use a hyper-parameter λas initial
noise and the degree of perturbation to different
parameters will be scaled up and down based on
it according to their sensitivity. The intensity of
perturbation can be formulated by the following
equations:
¯s=1
N/summationdisplay¯s (3)
r=λ·max(¯s
¯s+ϵ−γ,0),0< ϵ≪1(4)
In Eq. 3, ¯sis the average sensitivity of the ma-
trix containing θwithNparameters. rin Eq. 4
means the intensity of the noise to be added on a
parameter θ, which is scaled on λby the division
of¯sand¯s.ϵis a small number used to prevent
zero denominator. Since rand¯sare inversely
correlated, the intensity of noise added to every
parameter with a lower sensitivity than the average
will be larger than λ, and vice versa as we expect.
As for the reason why ¯sis restricted to the cur-
rent matrix, as we found, the value distributions
of different matrix parameters are sometimes very
different. For example, values of parameters in ma-
trixAare significant higher than those in matrix B.
And with the sensitivity measurement considered,
sensitive parameters are usually themselves large
on value. So if ¯sis calculated based on all parame-
ters of the model, some matrices of parameters with
low values and sensitivity may be perturbed fiercely
to some values very far from their original ones,
which has unstable performances on experiments.
To further reduce the perturbation on sensitive pa-
rameters and let them keep regular gradient-driven
update, we use a margin constant γin Eq. 4 to
zero-out the noise added on the parameters that are
highly sensitive.
For each parameter θ, the noise qthat may
finally be added to it is independently randomly
sampled from a Gaussian distribution with the
mean of zero and the standard deviation of σas
q∼N(0, σ), where σ=√r. So in an itera-
tion, we update each parameter by:
/tildewideθ=θ−η· ∇L(Θ) +q·z, (5)
where ηis learning rate and z∼B(1, p)is a
random value sampled from Bernoulli distribution
which outputs 1with probability pand0with prob-
ability 1−p. Algorithm 1 shows the PATS algo-
rithm for Adamax (Kingma and Ba, 2014) opti-
mizer.3682
3 Experiments
3.1 Datasets and Baselines
We conduct extensive experiments on the eight
tasks of the GLUE benchmark (Wang et al., 2018)
and adopt the publicly available BERT-base (De-
vlin et al., 2019) and RoBERTa-large (Liu et al.,
2019) models on every task individually. The fol-
lowing three baselines are selected for comparison:
(1)Standard PLM fine-tuning , which fine-tunes
PLMs directly; (2) NoisyTune (Wu et al., 2022),
which is a noisy training method that adds matrix-
wise noise before fine-tuning; (3) SAGE (Liang
et al., 2022), which is an optimized learning rate
schedule which adjusts the learning rate of every
parameter according to its sensitivity.
3.2 Performance Evaluation
On each task, we repeat our experiments 5 times
with different random seeds and report the aver-
age scores of every model, which are shown in
Table 1.According to the results, PATS opti-
mized models consistently outperforms directly
fine-tuned ones on different downstream tasks, es-
pecially on those with small datasets (CoLA &
MRPC & RTE). Specifically, PATS improves by
around 2 points on CoLA and RTE, and around 1
point on MRPC. In addition, as a parameter-wise
method based on sensitivity, PATS experimentally
outperforms the matrix-wise noisy method Noisy-
Tune and the sensitivity-based learning rate sched-
uler SAGE on 7 out of the 8 tasks. The experimen-
tal results demonstrate the effectiveness of PATS.
3.3 Empirical Analysis
In this section, we conduct additional analyses on
sensitivity of parameters in the fine-tuned models.
Fig. 2 shows the sensitivity distribution of the
model parameters fine-tuned in different ways. It
is found that the sensitivity of the parameters in the
PATS optimized models is more tightly clustered
than that in the models fine-tuned in the common
way. Besides, there remain fewer insensitive pa-
rameters in PATS optimized models than those in
baseline models. And it is no longer a few high-
sensitive parameters that dominate the models as
what happens in normal fine-tuning, which indi-
cates that perturbation helps parameters with low
sensitivity gain more attention during training and
lets the contribution of each parameter in the opti-
mized models more balanced.
To further investigate the effect of PATS on small
datasets, we also post the accuracies of the models
fine-tuned on different proportions of training data
sampled from the CoLAdataset with and with-3683
out PATS. Fig. 3 shows PATS optimized models
consistently outperform directly fine-tuned ones on
different sizes of datasets, demonstrating the gener-
alizability of the approach. Moreover, we can also
observe that as the size of training data increases,
the performances of the models improve along with
concomitant decreases in the standard deviations
of sensitivity. This phenomenon further indicates
that training with limited data will lose some of
the performance capabilities of PLMs by leaving
more undertrained or insensitive parameters, be-
cause small datasets is insufficient for PLMs to
overcome the gap between pretraining and down-
stream tasks. The inverse correlation between ac-
curacy and sensitivity concentration justifies our
original intention of balancing the sensitivity of
parameters. And the displayed performances ex-
perimentally demonstrate its availability.
4 Conclusion
We propose a novel noisy training method called
PATS to optimize fine-tuning of PLMs. Since ag-
gressive fine-tuning PLMs will leave a large num-
ber of insensitive parameters which contribute little
to the overall model, PATS activates them and bal-
ance the contributions of all parameters in down-
stream tasks by adding noise to each parameter
according to its sensitivity in the process of train-
ing. PATS is a simple mechanism without much
computational and memory overhead compared
to adversarial training which requires additional
backwards passes. Extensive experiments on eight
tasks of the GLUE benchmark show that PATS can
consistently improve the performance of PLMs on
downstream tasks with the sensitivity of the pa-rameters more concentrated, which is especially
pronounced on small datasets.
Limitations
PATS introduces four additional hyperparameters,
which increases some work of users on hyperparam-
eter tuning. For example, a too small λcould make
few differences while an overlarge λmay result in
unstable performances of models. Though we have
summarized effective parameter configurations on
the NLU tasks of the GLUE benchmark, it cannot
guarantee that these settings are still applicable on
other tasks such as neural machine translation. We
will explore the connections between the hyperpa-
rameters in theory and narrow the search ranges of
the hyperparameter group in future work.
References368436853686Model COLA MRPC RTE STS-B QQP QNLI MNLI SST
BERTbase1e-4 1e-4 1e-4 2e-4 1e-4 2e-4 8e-5 8e-5
RoBERTalarge3e-5 5e-5 5e-5 5e-5 1e-4 1e-5 3e-5 3e-5
BERTbase+PATS 1e-4 3e-4 3e-4 3e-4 2e-4 2e-4 1e-4 3e-4
RoBERTalarge+PATS 8e-5 8e-5 8e-5 5e-5 1e-4 3e-5 1e-5 1e-5
Hyperparameters Range
λ {5e-7, 8e-7, 1e-6, 2e-6, 3e-6}
γ {1e-3, 2e-3, 3e-3, 5e-3, 8e-3,2e-2}
β {0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85}
learning rate {1e-5, 3e-5, 5e-5, 7e-5, 8e-5, 1e-4, 2e-4, 3e-4, 5e-4}
A Appendix
A.1 Datasets
The experiments are conducted on the GLUE
benchmark, which contains several types of Natural
Language Understanding (NLU) tasks such as lin-
guistic acceptability (CoLA, Warstadt et al. 2019),
text similarity (STS-B, Cer et al. 2017) and natural
language inference (RTE & MNLI & QNLI, Da-
gan et al. 2005; Bar-Haim et al. 2006; Giampiccolo
et al. 2007; Bentivogli et al. 2009; Williams et al.
2018; Bowman et al. 2015; Rajpurkar et al. 2016)
tasks. Among the nine tasks, WNLI (Levesque
et al., 2012) task is excluded in our experiments, on
which BERT-like models have no obvious advan-
tage over other mainstream baselines (Hou et al.,
2020; Clark et al., 2020; Huang et al., 2021). Con-
sistent with previous works (Bao et al., 2020; Wu
et al., 2022), we evaluate results on the dev set of
GLUE.
A.2 Training Details
For all the baseline models and our proposed PATS,
we adopt a linear-decay learning rate schedule and
choose Adamax (Kingma and Ba, 2014) which is
the best-performing optimizer for baseline models
on the GLUE benchmark to optimize the training.
In PATS, we perturb the parameters of all the en-
coder layers except the Layer Normalization layers.
In our training process, we set λ= 2×10,
γ= 0.002,β= 0.75,p= 0.2for all tasks. In
addition, we adopt a linear warm-up learning rate
schedule with 0.1 of total training iterations. The
batch size of models is uniformly set to 32. We
post the best performance models on each task after10 epochs of training. The learning rates that yields
the best generalization performance of models op-
timized by PATS and Standard PLM fine-tuning
on each task are listed in Table 2. We present the
searching range of hyperparameters in Table 3.
Our implementation is based on the MT-DNN
code-base.. And we use Nvidia V100 GPUs for
all experiments.
A.3 Other Implemention Details
All datasets of the GLUE benchmark
are downloaded from https:// gluebench-
mark.com/tasks. For the baseline model SAGE,
we use the code from the Github respository
https://github.com/cliang1453/SAGE. The other
baseline models are implemented by ourselves.
For the distribution of sensitivity shown in Fig.
2, we discard some outliers and only choose the
parameters with sensitivity in the range of [5e-8,
1e-5] for visualization.3687