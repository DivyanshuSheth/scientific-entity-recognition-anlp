
Himanshu Thakur Atishay JainPraneetha Vaddamanu
Paul Pu Liang Louis-Philippe Morency
Carnegie Mellon University
{hthakur,atishayj,pvaddama,pliang,morency}@andrew.cmu.edu
Abstract
Caution: this paper contains potentially offen-
sive or upsetting model outputs.
Societal biases present in pre-trained large lan-
guage models are a critical issue as these mod-
els have been shown to propagate biases in
countless downstream applications, rendering
them unfair towards specific groups of peo-
ple. Since large-scale retraining of these mod-
els from scratch is both time and compute-
expensive, a variety of approaches have been
previously proposed that de-bias a pre-trained
model. While the majority of current state-of-
the-art debiasing methods focus on changes to
the training regime, in this paper, we propose
data intervention strategies as a powerful yet
simple technique to reduce gender bias in pre-
trained models. Specifically, we empirically
show that by fine-tuning a pre-trained model on
only 10 de-biased (intervened) training exam-
ples, the tendency to favor any gender is sig-
nificantly reduced. Since our proposed method
only needs a few training examples, our few-
shot debiasing approach is highly feasible and
practical. Through extensive experimentation,
we show that our debiasing technique performs
better than competitive state-of-the-art baselines
with minimal loss in language modeling ability.
1 Introduction
Recently, there has been a surge of interest in pre-
trained large language models (LLM) in natural lan-
guage processing (NLP). It has been shown that the
pre-training + finetuning of a model drastically im-
proves its performance on downstream tasks as the
knowledge captured by the pre-training on a large
corpus is transferred to the downstream applica-
tion when finetuning the model. However, this also
leads to societal biases like gender bias that were
implicitly learned by the pre-trained models being
transferred to crucial downstream applications like
job recommendation engines (Zhao et al., 2019;Barocas et al., 2017; Kurita et al., 2019). Analyz-
ing and mitigating bias without requiring significant
re-training or compute resources is crucial to the
widespread adoption of LLMs in downstream appli-
cations.
Previous work (Nadeem et al., 2021), (Nangia
et al., 2020a), (Cer et al., 2018) has attempted to
quantify bias, and others such as Ravfogel et al.
(2020) and Liang et al. (2021) have attempted to
remove it algorithmically from the models. Closer
to our work are data-manipulative techniques such
as Zmigrod et al. (2019) and Maudslay et al. (2019)
that modify the dataset and further fine-tune the
model. In this paper, we propose simple data inter-
vention strategies and show that they can mitigate
gender bias in pre-trained models with the help of
few-shot fine-tuning. Moreover, taking inspiration
from Schick et al. (2021), we find that by utiliz-
ing a biased pre-trained LLM for mining for most
gender-biased samples in a dataset, our methods can
mitigate gender bias with very few training samples.
Finally, we perform an extensive evaluation of our
debiasing technique on two recent bias benchmarks
(Nadeem et al., 2021) and show that our method out-
performs three existing state-of-the-art techniques
and performs comparably to the other two. Our
main contributions are the following:
•We propose simple data intervention tech-
niques that can be used to reduce gender bias
in a pre-trained LLM with few training ex-
amples (few-shot), thus making human-in-the-
loop bias mitigation strategies feasible.
•We introduce a novel data sampling technique
that utilises LLMs to mine for the most biased
samples from a dataset and can benefit existing
state-of-the-art debiasing methods. When used
for debiasing a model, these few samples serve
as exemplars and induce large reductions in
gender bias.340
2 Related Work
In recent years, there has been growing concern
about the bias/stereotypical discriminatory behav-
ior by NLP models, particularly concerning gen-
der. Several studies have investigated the presence
of gender bias in various NLP tasks and proposed
methods for mitigating it.
One line of research has focused on analyzing
the extent of gender bias in pre-trained language
models such as BERT and GPT-2. These studies
have found that these models exhibit a significant
amount of gender bias in their word embeddings
for BERT (Jentzsch and Turan, 2022) and for GPT-
2 (Kirk et al., 2021) and are prone to making stereo-
typical gender-based predictions (e.g., assuming
that a doctor is male and a nurse is female). A stan-
dard evaluation metric used in this line of research
is Stereotype metrics such as StereoSet (Nadeem
et al., 2021), which evaluates the model’s ability to
predict gender stereotypes and CrowS pairs (Nan-
gia et al., 2020b) which measure whether a model
generally prefers more stereotypical sentences. A
similar line of work is gender bias tests proposed
in BIG-bench (Srivastava et al., 2022). The tests
assess the language model’s gender biases, stereo-
types, and ability to infer gender information. It
evaluates gender bias and stereotype between maleand female, and gender minority bias and stereotype
between majority and minority. It also examines
the model’s language modeling performance, which
can be affected during de-biasing.
Another line of research has proposed methods
for debiasing these models. These methods can be
broadly categorized into two groups: data-based
andalgorithm-based . Data-based methods aim to
reduce bias by removing or altering biased words
from the training set. In contrast, algorithm-based
methods aim to modify the model’s architecture or
training procedure to reduce bias. One popular data-
based method is "uncertainty sampling" (Lewis and
Gale, 1994), where the model is trained on the in-
stances that it is most uncertain about, which can
help to reduce bias by forcing the model to learn
from a diverse set of examples. A popular algorithm-
based method is "Adversarial Debiasing" proposed
by Zhang et al. (2018), which fine-tunes the model
using an adversarial loss to make it less sensitive
to sensitive attributes such as gender. OSCar pro-
posed by Dev et al. (2021), is another algorithm
based method that utilizes the idea of disentangling
"problematic concepts" like occupation and gender
relationship instead of removing them altogether.
MABEL (He et al., 2022) has both algorithm and
data-based components, as it first augments the
training data by swapping gender words and then
applies a contrastive learning objective and align-
ment via entailment pairs. Their data augmentation
strategy is similar in spirit to the data intervention
techniques we propose, however our analysis does
not require training auxiliary models and uses sig-
nificantly lesser data.
Data-based methods include the "Equalization"
technique proposed by Bolukbasi et al. (2016),
which aims to equalize the representation of gender-
specific words in the embedding space, the "Coun-
terfactual Data Augmentation" (CDA) method pro-
posed by Zimmermann and Hoffmann (2022),
which generates counterfactual examples to im-
prove the model’s robustness to bias, and "Name-
Based Counterfactual Data Substitution" proposed
by Maudslay et al. (2019) which reduces gender
bias by replacing gender-informative names in the
dataset with gender-neutral names. Our proposed
method is also a data-based method, which aims
to effectively reduce gender bias by taking inspira-
tion from different techniques such as uncertainty
sampling and name-based counterfactual data sub-
stitution (Maudslay et al., 2019).341
3Probing Bias in Large Language Models
Pre-trained LLMs are biased towards different gen-
ders, as seen in a simple mask-fill experiment us-
ing BERT. (Here, and in the rest of the paper, we
assume a binary treatment of gender for simplic-
ity.) The task is then to mask out the gender-related
nouns and pronouns (such as he, she, her, woman,
etc.) and get BERT to predict the masked words
for the affected sequences in the dataset. Here, we
consider a fixed list of gender-specific words cu-
rated from previous work (Lu et al., 2018; Zmigrod
et al., 2019) and neutral words list. We finally
compute the "total confidence difference" as the
sum of differences in the model’s prediction con-
fidence for each gender-word pair (such as confi-
dence of predicting he −she, man −woman, etc.).
Formally, we define total confidence difference as
|/summationtext(f(x)−f(x))|where f(x)rep-
resent the confidence of model’s prediction, Nis
the total number of tokens in the dataset and xis
the tokenized gender word. The higher this number,
the more biased the model is concluded to be. We
compute the metric at token level and ensure that
each of the gender word gets tokenized into exactly
one token by initially extending the tokenizer with
our gender word list. The top 3 biased gender-word
pairs in StereoSet are shown in Table 1. Intuitively,
our technique for gauging bias in LLMs is sensitive
to the fixed word list used to represent the sensitive
attributes (here, gender). In Table 2, we show the
number of words covered by the word list used for
both WikiText-2 and StereoSet datasets.
4 Data Interventions
In order to reduce gender bias in pre-trained models,
we carefully select diverse and hard-biased exam-
ples and then replace gender words with more neu-
tral or equality-focused phrases. This is achieved by
using a wordlist to find gender terms in sentences
and then segregating words as name and non-name
words.
We call our initial approach naive-masking as
it does not require a word list for mapping gender
words to gender-neutral words. Instead, it replaces
all gender words with the fixed word "person." In
our next approach, neutral-masking , we swap
words in a slightly more semantically accurate man-
ner. In this, we use a word-pair list that goes from
gender words to gender-neutral words. With both
approaches, we intend to introduce new words in a
model’s vocabulary to make it more likely to choose
a more neutral word in gender-biased sentences.
In our final approach, we exploit the existing vo-
cabulary of the model and try to balance the confi-
dence of prediction on opposite-gender words by us-
ing phrases instead. Thus, we call our final approach
random-phrase-masking as we instead substitute
words with phrases that reflect the equality of gen-
der. This approach not only reduces gender bias
but also preserves the original meaning of the sen-
tence in most cases. In our approach, we chose the
phrases and order of gender words at random with
equal probability.342Additionally, we hypothesize that the choice of
the dataset for fine-tuning is also essential. We
choose two datasets: the WikiText-2 (Merity et al.,
2017) dataset, which has implicit gender bias since
its sources from Wikipedia articles, and the Stere-
oSet dataset (Nadeem et al., 2021), which has ex-
plicit/more gender bias as it has been designed to
evaluate gender bias. WikiText-2has 600 train arti-
cles and roughly 2M tokens while StereoSet(dev)
has 2123 samples out of which we only consider
800 samples which are not unrelated. Naturally,
our data intervention method should work better on
a dataset with training examples with gender bias
while being devoid of meaningful gender associa-
tions like "She needs a gynecologist," where the
gender of the person is important. By testing our
method on both datasets, we can understand the
sensitivity of our approach to the quality of training
samples used.
5 Bias Evaluation Metrics
We focus on evaluating the bias of a model while
also measuring its language modeling capability.
The ideal model would not just be one with the least
bias but also one which does not compromise its lan-
guage modeling performance. The dual estimation
of bias and performance of a model was proposed
in the StereoSet benchmark (Nadeem et al., 2021),
with the Language Modeling Score (LMS) measur-
ing the percentage of times a meaningful token is
predicted for the mask as opposed to a meaningless
token, the Stereotype Score (SS) measuring the per-
centage of times the model predicted a stereotypical
word as compared to an anti-stereotypical word, and
an idealized CAT score (ICAT) combining the LMS
and SS score into a single metric. An ideal model
has an ICAT score of 100, while the worst biased
model has an ICAT score of 0. We additionally
evaluate the CrowS-Pairs benchmark (Nangia et al.,
2020a), which captures data with greater diversity
in both the stereotypes expressed and the structure
of sentences (50 is ideal). However, we note that the
Crow-S benchmark is much more limited compared
to StereoSet (Nadeem et al., 2021) in terms of both
the volume and variety of linguistic phenomenon
relating to gender bias it covers.6 Experiments
We compare our proposed interventions with five
baselines, 4 of which are state-of-the-art methods
and the original pre-trained model. Our first base-
line is the application of dropouts to neural net-
works, Dropout proposed by (Webster et al., 2020).
Next, we consider an algorithmic de-biasing tech-
nique INLP technique proposed by (Ravfogel et al.,
2020). Then, we consider a sentence embedding
de-biasing approach SentenceDebias (Liang et al.,
2020). Finally, we consider a data-based approach
CDA (Zmigrod et al., 2019) that is closest to our
work. For a fairer comparison, we run the baselines
with the same size (100) of the training set as our
method. For all of our experiments, we consider
the “bert-base-uncased” pre-trained model available
from HuggingFace. For fine-tuning our model, we
select a varying number of most-biased training
samples (10, 50, and 100) from the WikiText-2 and
StereoSet (we only use the dev set) datasets, as
discussed in section 4. We also compare this to a
random selection of data points as an ablation study.
On the selected dataset, we apply our interventions
and obtain the modified dataset, which is then used
to fine-tune our pre-trained model using masked lan-
guage modeling (MLM) loss. The key point is that
we only fine-tune the model on the gender words
conditioned on the remaining text, significantly re-
ducing the fine-tuning time. We perform ablations
on various types of interventions as discussed in
Table 7. The model is trained for 30 epochs, with a
learning rate of 0.001 and AdamW optimizer. We
ran all of our experiments on NVIDIA Tesla T4
GPU on Google Colab for roughly 48 hours. For
all experiments, we report the numbers as the mean
and standard deviations (6) of 3 different runs. Our
experiment code can be found here.
7 Results
Table 4 shows the StereoSet and Crow-S scores for
our baselines and our best-performing interventions
on the WikiText-2 Dataset. In the StereoSet bench-
mark, we observe that random-phrase-masking
obtains lower SS than all other baselines. On
the Crow-S benchmark, random-phrase-masking
does better than thre of the baselines except Sen-
tenceDebias which achieves slightly better scores.
While random-phrase-masking results in lower
SS scores than neutral-masking , it also obtained343
very low LMS scores. We attribute this per-
formance degradation to the blunt substitution
of phrases that our method uses, which might
lead to odd-sounding sentences. In the Crow-
S benchmarks, we see similar behavior and find
that random-phrase-masking does better than
neutral-masking . Since we believe that our
method is sensitive to the choice of the dataset,
we also present results on the StereoSet (dev)
dataset 6. In Figure 2, we perform a qualitative
analysis of our proposed approach and find that
random-phrase-masking is able to flip the predic-
tions on fill-mask tasks for stereotypical sentences.8 Conclusion
In this paper, we show that simple data interventions
on limited training data effectively reduce gender
bias in LLMs. We also show that a biased pre-
trained LLM can be used to mine the most effective
de-biasing training examples. Evaluation of our
methods on state-of-the-art bias benchmarks empir-
ically suggests that our methods effectively reduce
gender bias. Given that our methods can work in a
few-shot manner and do not require any auxiliary
model training, we hope that our work benefits fur-
ther research in the domain of human-in-the-loop
bias mitigation techniques by making the creation
of bias mitigation datasets feasible.
9 Limitations
Our proposed method has the following main lim-
itations which we believe are important directions
for future work to address:
1.Gender dependency: Our approach does not
account for sentences that only make sense for
a single gender. For example, sentences like
"She needs to see a gynecologist" would not
be captured by our method. This is a com-
mon problem encountered by most debiasing
algorithms as it is difficult to distinguish these.
2.Finite wordlist: The wordlist does not contain
all gender-based words as the language con-344tinues to evolve. We believe that future works
could employ better approaches that can au-
tomatically mine gender words relevant to a
dataset.
3.Blunt substitution: The phrase substitution
method is an improvement over direct word
substitution, but there are still plenty of in-
stances where the new sentence might be se-
mantically incorrect. This does not have any
major implication on inference as we are only
doing few-shot learning, but it should not be
extended to the entire dataset.
4.Binary gender: The method only focuses on
the male and female gender. It does not con-
sider non-binary or gender-neutral pronouns
such as "ze/hir." This can be solved by using
an updated wordlist, but the authors could not
come across one at the time of writing.
5.Downstream analyses: While our work pro-
poses methods that show reduced gender bias
as per a set of metrics, the work in no way
claims to reduce gender bias in general, es-
pecially on downstream tasks. However, we
strongly believe that this technique holds po-
tential to reduce gender bias on downstream
tasks as well since we adopt a regular fine-
tuning approach and focus mainly on better
data interventions. Moreover, recent research
has shown that fine-tuning-based debiasing ap-
proaches do not damage a model’s internal rep-
resentations to a critical extent (Meade et al.,
2022).
Overall, these limitations suggest that our ap-
proach may not be suitable for use in contexts where
gender-specific or non-binary language is prevalent,
and the underlying wordlist should be frequently
updated.
10 Ethics Statement
This study was conducted in accordance with ethical
principles and guidelines. The study was designed
to provide beneficial knowledge and not harm any
group or individual. We recognize that the wordlist
we use might not represent all contexts of gender
bias and that our debiasing method does not cover
all contexts of occurrences of gender bias. However,
we made sure to consider the ethical implications
of our methodologies and the results of our anal-
ysis. The authors have tried to ensure the methoddoes not amplify any other inherent bias but also ac-
knowledge that our approach may have limitations.
We take responsibility for any ethical concerns that
may arise as a result of our research.
Acknowledgments
This material is based upon work partially sup-
ported by the National Science Foundation (Awards
#1722822 and #1750439) and National Institutes of
Health (Awards #R01MH125740, #R01MH096951,
and #U01MH116925). PPL is partially supported
by a Facebook PhD Fellowship and a Carnegie Mel-
lon University’s Center for Machine Learning and
Health Fellowship. Any opinions, findings, conclu-
sions, or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily
reflect the views of the NSF, NIH, Facebook, or
CMLH, and no official endorsement should be in-
ferred. Additionally, we express our appreciation
to the anonymous reviewers for their insightful sug-
gestions, which greatly improved our work. Further-
more, we would like to acknowledge the contribu-
tions of our colleagues, Atishay Jain and Praneetha
Vaddamanu, who played a significant role in the
development of this research.
References345346
A Appendix
A.1 Dataset Bias Analysis
To gauge the feasibility of using a wordlist based
intervention approach, we first analyze our datasets
for occurrences of gender words. As shown in
the word cloud 4, gender pronouns are the most-
frequent word in our datasets. Moreover, as per Fig-
ure 1, "she," "he," and "her" are the top three most
frequently occurring words in our dataset. This sug-
gests that we can definitely detect gender words in
our corpus and apply our interventions.
A.2 Sensitivity to Choice of Dataset
To understand the effectiveness of our proposed
data-interventions, we study apply our methods to
two datasets under varying number of training sam-
ples (10, 50 and 100) and selection strategies (most
biased first and random) as per Table 6. Our meth-
ods obtain better results on StereoSet (dev) dataset.
One reason this could happen is due to the fact that
StereoSet has explicit gender bias, thus it would be
less likely for a sentence like "She needs a gynae-
cologist" to appear on it. Because our interventions
perform blunt substitutions, this sentence might be-
come incorrect due to our method - "Either he or
she needs a gynaecologist".
A.3 Sensitivity to Number of Training Samples
and Sampling Strategy
As per Figure 5, When we vary the number of train-
ing samples, we observe that the difference in per-
formance is not huge when we transition from 10
to 100 samples, thus suggesting that our method347SentencesMean
Confidence
Difference
She rushed to see what he wanted and said she loved him.
She punched him in the face and told him to go away.6.85
Jessica is a new mommy.
Jessica finds being a mother does not come easy to her.
She will no longer work so she can stay home and take care of her child.6.34
The little girl missed her mommy.
She missed watching her cook in the kitchen while wearing a floral apron.
She was never home because she worked long hours in the oil field.4.70348is capable of few-shot fine-tuning. Moreover, sam-
pling the most biased data points helps our methods
achieve better performance consistently, as shown
in Figure 5 and Table 6. Table ??shows some top
three most gender biased entries found in the Stere-
oSet dataset.
A.4 Ablations of interventions
We study the effects of choosing different ways
of replacement for name and non-name words. In
addition to our three interventions proposed previ-
ously, we also experimented with a couple of oth-
ers. In female-first-random-phrase-masking ,
we always keep the female gendered word before
a male word. We wanted to understand if the
order of gender words encountered by a model
renders any effect on the debiasing. In Table 7,
we see that it does not perform any better than
random-phrase-masking . Then, we also try fixing
the phrases from random-phrase-masking , thus
making it fixed-phrase-masking . We obtain 4
variants of this method corresponding to the follow-
ing four phrases:
1. both [1] and [2]
2. [1] and [2]
3. [1] or [2]
4. either [1] or [2]
Here, [1] and [2] are substituted with oppo-
site gender words. As we observe in Table
7,fixed-phrase-masking-3 obtains the lowest
StereoSet Gender SS score out of all our interven-
tion methods. Similarily, naive-masking obtains
the lowest Crow-S pair score.349ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
9
/squareA2. Did you discuss any potential risks of your work?
10
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
4
/squareB1. Did you cite the creators of artifacts you used?
4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
The datasets used are open-source.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
4
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
The data is a general vocabulary. Hence, none of the used data contains any personal information.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
The dataset pages have all the needed information
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
7
C/squareDid you run computational experiments?
6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
6350/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
6
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
6
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.351