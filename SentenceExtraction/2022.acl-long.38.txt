
Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Chonghua Liao
Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, Zhilin YangTsinghua University,BAAI,MIT CSAIL,Carnegie Mellon University,Google Research,Shanghai Qi Zhi Institute
{zyanan, jietang, zhiliny}@tsinghua.edu.cn ,
zhouj18@mails.tsinghua.edu.cn ,ruder@google.com
Abstract
The few-shot natural language understanding
(NLU) task has attracted much recent attention.
However, prior methods have been evaluated
under a disparate set of protocols, which hin-
ders fair comparison and measuring progress
of the ﬁeld. To address this issue, we intro-
duce an evaluation framework that improves
previous evaluation procedures in three key
aspects, i.e., test performance, dev-test corre-
lation, and stability. Under this new evalua-
tion framework, we re-evaluate several state-
of-the-art few-shot methods for NLU tasks.
Our framework reveals new insights: (1) both
the absolute performance and relative gap of
the methods were not accurately estimated in
prior literature; (2) no single method domi-
nates most tasks with consistent performance;
(3) improvements of some methods diminish
with a larger pretrained model; and (4) gains
from different methods are often complemen-
tary and the best combined model performs
close to a strong fully-supervised baseline. We
open-source our toolkit, FewNLU, that imple-
ments our evaluation framework along with a
number of state-of-the-art methods.
1 Introduction
Few-shot learning for natural language understand-
ing (NLU) has been signiﬁcantly advanced by
pretrained language models (PLMs; Brown et al.,
2020; Schick and Schütze, 2021a,b). With the goal
of learning a new task with very few (usually less
than a hundred) samples, few-shot learning beneﬁts
from the prior knowledge stored in PLMs. Various
few-shot methods based on PLMs and prompting
have been proposed (Liu et al., 2021b; Menon et al.,
2021; Gao et al., 2020).Although the research of few-shot NLU is devel-
oping rapidly, the lack of a standard evaluation
protocol has become an obstacle hindering fair
comparison between various methods on a com-
mon ground and measuring progress of the ﬁeld.
While some works (Schick and Schütze, 2021b;
Menon et al., 2021) experimented with a ﬁxed set
of hyper-parameters, prior work (Perez et al., 2021;
Zhang et al., 2020) noted that such a setting might
be exposed to the risk of overestimation .Other
works (Liu et al., 2021b; Gao et al., 2020; Perez
et al., 2021) proposed to use a small development
set to select hyper-parameters, but their evaluation
protocols vary in a few key aspects (e.g., how to
construct data splits), which in fact lead to large
differences as we will show (in Section 4.2). The
above phenomena highlight the need for a com-
mon protocol for the evaluation of few-shot NLU
methods. However, the fact that few-shot learn-
ing is extremely sensitive to subtle variations of
many factors (Dodge et al., 2020; Gao et al., 2020)
poses challenges for designing a solid evaluation
protocol.
In this work, aiming at addressing the aforemen-
tioned challenge, we propose an evaluation frame-
work for few-shot NLU. The evaluation framework
consists of a repeated procedure—selecting a hyper-
parameter, selecting a data split, training and eval-
uating the model. To set up a solid evaluation
framework, it is crucial to specify a key design
choice—how to construct data splits for model se-
lection. We conduct a comprehensive set of ex-
periments to answer the question. Speciﬁcally, we
propose a “Multi-Splits” strategy, which randomly
splits the available labeled samples into training
and development sets multiple times, followed by
aggregating the results from each data split. We
show that this simple strategy outperforms several501baseline strategies in three dimensions: (1) the test
set performance of the selected hyper-parameters;
(2) correlation between development set and true
test set performance; and (3) robustness to hyper-
parameter settings.
We then take a step further to re-evaluate recent
state-of-the-art few-shot NLU methods under this
common evaluation framework. Our re-evaluation
leads to several key ﬁndings summarized in Sec-
tion 2.
To aid reproducing our results and benchmarking
few-shot NLU methods, we open-source FewNLU,
a toolkit that contains implementations of a number
of state-of-the-art methods, data processing utili-
ties, as well as our proposed evaluation framework.
To sum up, our contributions are as follows.
1.We introduce a new evaluation framework of
few-shot NLU. We propose three desiderata of
few-shot evaluation and show that our frame-
work outperforms previous ones in these aspects.
Thus our framework allows for more reliable
comparison of few-shot NLU methods.
2.Under the new evaluation framework, we bench-
mark the performance of recent methods indi-
vidually as well as the best performance with
a combined approach. These benchmarks re-
ﬂect the current state of the art and will serve as
important baselines for future research.
3.Throughout our exploration, we arrive at several
key ﬁndings summarized in Section 2.
4.We open-source a toolkit, FewNLU, to facilitate
future research with our framework.
2 Summary of Findings
For reference, we collect our key ﬁndings here and
discuss each of them throughout the paper.
Finding 1. Our proposed Multi-Splits is a more
reliable data-split strategy than several baselines
with improvements in (1) test performance, (2) cor-
relation between development and test sets, and (3)
stability w.r.t. the number of runs.
Finding 2. The absolute performance and the rela-
tive gap of few-shot methods were in general not
accurately estimated in prior literature. It highlights
the importance of evaluation for obtaining reliable
conclusions. Moreover, the beneﬁts of some few-
shot methods (e.g., ADAPET) decrease on larger
pretrained models.
Finding 3. Gains of different methods are largely
complementary. A combination of methods largelyoutperforms individual ones, performing close to a
strong fully-supervised baseline with RoBERTa.
Finding 4. No single few-shot method dominates
most NLU tasks. This highlights the need for the
development of few-shot methods with more con-
sistent and robust performance across tasks.
3 Related Work
The pretraining-ﬁnetuning paradigm (Howard and
Ruder, 2018) shows tremendous success in few-
shot NLU tasks. Various methods have been devel-
oped such as [CLS] classiﬁcation (Devlin et al.,
2018), prompting-based methods with discrete
prompts (Schick and Schütze, 2021b; Gao et al.,
2020) or continuous prompts (Liu et al., 2021b;
Shin et al., 2020; Li and Liang, 2021; Lester et al.,
2021), and methods that calibrate the output distri-
bution (Yang et al., 2021; Zhao et al., 2021).
The fact that few-shot learning is sensitive to
many factors and thus is extremely unstable (Liu
et al., 2021a; Lu et al., 2021; Zhang et al., 2020;
Dodge et al., 2020) increases the difﬁculty of few-
shot evaluation. Several works address evaluation
protocols to mitigate the effects of instability: Gao
et al. (2020) and Liu et al. (2021b) adopt a held-out
set to select models. Perez et al. (2021) proposed
K-fold cross-validation and minimum description
length evaluation strategies. Our work differs from
these works on few-shot evaluation in several as-
pects: (1) we propose three metrics to evaluate data
split strategies; (2) while most prior work proposed
evaluation protocols without justiﬁcation, we con-
duct comprehensive experiments to support our key
design choice; (3) we formulate a general evalu-
ation framework; (4) our re-evaluation under the
proposed framework leads to several key ﬁndings.
Though there have been a few existing few-shot
NLP benchmarks, our work is quite different in
terms of the key issues addressed. FLEX (Bragg
et al., 2021) and CrossFit (Ye et al., 2021) stud-
ied principles of designing tasks, datasets, and
metrics. FewGLUE (Schick and Schütze, 2021b)
is a dataset proposed for benchmarking few-shot
NLU. CLUES (Mukherjee et al., 2021) pays at-
tention to the uniﬁed format, metric, and the gap
between human and machine performance. While
the aforementioned benchmarks focus on “what
data to use” and “how to deﬁne the task”, our work
discussed “how to evaluate” which aims at estab-
lishing a proper evaluation protocol for few-shot
NLU methods. Since FewNLU is orthogonal to the502aforementioned prior work, it can also be employed
on the data and tasks proposed in previous work.
4 Evaluation Framework
Formally, for a few-shot NLU task, we have a small
labeled setD=f(x; y)gand a large test set
D=f(x; y)gwhere Nis the number of
labeled samples, xis a text input (consisting of
one or multiple pieces), and y2Y is a label. The
goal is to ﬁnetune a pretrained model with Dto
obtain the best performance on D. An unlabeled
setD=fxgmay additionally be used by
semi-supervised few-shot methods (§5.1).
4.1 Formulation of Evaluation Framework
Our preliminary results (in Appendix §A.1) show
that using a ﬁxed set of hyper-parameters (Schick
and Schütze, 2021a,b) is sub-optimal, and model se-
lection is required. It motivates us to study a more
robust evaluation framework for few-shot NLU.
The goal of an evaluation framework is twofold:
(1) benchmarking few-shot methods for NLU tasks
such that they can be fairly compared and evalu-
ated; and (2) obtaining the best few-shot perfor-
mance in practice. In light of the two aspects, we
propose the few-shot evaluation framework in Al-
gorithm 1.
The framework searches over a hyper-parameter
spaceHto evaluate a given few-shot method M,
obtaining the best hyper-parameter setting hand
its test set results.The measurement for each h
is estimated by performing training and evaluation
on multiple data splits (obtained by splitting D
according to a strategy) and reporting their average
dev set results. Finally, the method is evaluated
onDusing the checkpoints corresponding to
h. For benchmarking, we report the average and
standard deviation over multiple test set results.
Otherwise, that is, to achieve a model with the
best practical performance, we re-run on the entire
Dwithh.
The framework requires specifying a key design
choice—how to construct the data splits, which we
will discuss in §4.2.Algorithm 1:
4.2 How to Construct Data Splits
4.2.1 Desiderata: Performance, Correlation,
and Stability
We ﬁrst propose the following three key desiderata
for the evaluation of different data split strategies.
1.Performance of selected hyper-parameter. A
good data split strategy should select a hyper-
parameter that can achieve a good test set perfor-
mance. We use the same metrics as (Schick and
Schütze, 2021b), along with standard deviations.
2.Correlation between dev and test sets (over a
hyper-parameter distribution) . Since a small
dev set is used for model selection, it is impor-
tant for a good strategy to obtain a high corre-
lation between the performances on the small
dev set and test set over a distribution of hyper-
parameters. We report the Spearman’s rank cor-
relation coefﬁcient for measurement.
3.Stability w.r.t. number of runs K.The choice
of the hyper-parameter Kshould have small im-
pacts on the above two metrics (i.e., performance
and correlation). To analyze the stability w.r.t K,
we report the standard deviation over multiple
different values of K. Besides, it is desirable to
have reduced variance when Kincreases. Thus
we report the above two metrics with different503values of Kand the standard deviation of test
scores over Kruns.
4.2.2 Data Split Strategies
We consider several data split strategies. Some are
proposed by previous work, including K-fold cross
validation (CV) (Perez et al., 2021), minimum de-
scription length (MDL) (Perez et al., 2021), and
bagging (BAG) (Breiman, 1996). We also consider
two simple strategies worth exploring, including
random sampling (RAND) and model-informed
splitting (MI). And we propose a new data split
strategy, Multi-Splits (MS). Besides, we also ex-
periment a special case of CV when Kequals the
number of labeled sample, which is leave-of-out
cross validation (LOOCV). Since LOOCV takes
much longer time and suffers from efﬁciency prob-
lem, we only experimented on several tasks and
left the results in Appendix A.2.4. They all ﬁt into
the pipeline of the proposed framework in §4.1:
1.K-fold CV equally partitions D intoK
folds. Each time, it uses the kfold for vali-
dation and the other K 1folds for training.
2.MDL assigns half ofDas the joint training
data and equally partitions the other half into K
folds. Each time, it uses the kfold for vali-
dation, and all its previous k 1folds together
with the joint training data for training.
3.Bagging samples Nr(r2(0;1]is a ﬁxed ra-
tio) examples with replacement from the labeled
sample as the training set, leaving samples that
do not appear in the train set for validation.
4.Random Sampling performs random sampling
without replacement from Dtwice, respec-
tively sampling NrandN(1 r)data as
the training and development sets.
5.Model-Informed Splitting computes represen-
tations of each labeled example using a model,
and clusters them into two distinct sets, respec-
tively as the training and development sets.
6.Multi-Splits randomly splitsDinto training
and development sets using a ﬁxed split ratio r.
Essentially, these data split strategies differ in
several key aspects.
1.For CV and MDL, Kcontrols the number of
runs and the split ratio. For Multi-Splits, BAG
and RAND, the split ratio is decoupled from K
and is controlled by r. For MI, the split ratio and
number of runs depend on D.
2.They use a different amount of data for training
and development sets as Table 1 shows.
3.There are cases when CV and MS share the same
split ratio. The difference is that MS allows
overlap between splits while CV does not.
4.BAG allows duplicated training data, while
RAND and Multi-Splits do not. The training
and development sets do not overlap for BAG
and Multi-Splits but overlap for RAND.
In the limit, our Multi-Splits is similar to leave-
P-out cross-validation (LPOCV; Celisse, 2014)
where LPOCV runs 
times ( Pis the number
of dev set examples) while Multi-Splits runs K
times. As Kincreases, Multi-Splits gradually ap-
proaches LPOCV . Since it is impossible to enumer-
ate the large number of possible splits in practice,
Multi-Splits can be viewed as a practical version
of LPOCV . Compared to the strategy of (Gao et al.,
2020) that uses multiple datasets, our Multi-Splits
uses multiple data splits for a single dataset. It
is thus more practical as in real-world scenarios,
it is hard to obtain multiple labeled datasets for
a true few-shot problem; otherwise, it could be
formulated as a fully-supervised learning problem.
The strategy in (Liu et al., 2021b) is a special case
of Multi-Splits when K= 1, which suffers from
higher variance.
4.2.3 Experimental Setup
To evaluate different data split strategies, we exper-
iment on the FewGLUE benchmark (Schick and
Schütze, 2021b). We evaluate strategies based on
the widely used prompt-based few-shot method
PET (Schick and Schütze, 2021b) with DeBERTa
as the base model.We run experiments on the
same tasks with the same hyper-parameter space504
to ensure a fair comparison; in this experiment
we search learning rate, evaluation ratio, prompt
pattern and maximum training step. More experi-
mental details are in Appendix A.2.
4.2.4 Main Results and Analysis
Table 2, Table 3 and Figure 1 show the main results
with 64 labeled samples.
It is noteworthy that we also experimented with
32 labeled samples and have observed that varying
the number of labeled examples does not affect the
following conclusion (see Appendix A.2).
Test Performance and Correlation. From both
Table 2 and Table 3, we ﬁnd that Multi-Splits
achieves the best average test set performance as
well as the best average correlation among all strate-
gies. We analyze them as follows:
1.Multi-Splits uses fewer labeled samples for train-
ing (i.e., 128) while CV and MDL use more (i.e.,
192 and 176). Despite using more training data,
both CV and MDL do not perform better. This
indicates few-shot performance is limited by not
being able to select the best model rather than
not having sufﬁcient training data. Both CV and
MDL use fewer data for validation (i.e., 64 and
32) than Multi-Splits (i.e., 128), thus leading to
poor correlation.
2.Although Multi-Splits and BAG use the same
number of training data (i.e., 128), there could be
duplication in the training set of BAG, making it
poor in diversity and further leading to lower test
performance, compared to Multi-Splits. This in-
dicates diversity of training sets is crucial when
constructing few-shot data splits.
3.RAND uses similar-sized dev and train sets to
BAG and MS but performs worse in test perfor-
mance. Since there could be overlap between
train and dev sets, the model may have memo-
rized data, leading to poor test performance.
4.MI constructs very different train and dev sets.
Overﬁtting on one of them and validating on
the other pose more challenges for the few-shot
method on out-of-distribution tasks.
Stability w.r.t. the number of runs K.Figure 1
shows the results on stability. In light of limited
computation resources, we only experiment with505some representative strategies. Both CV and MDL
represent strategies whose number of runs are cou-
pled with the size of data split, while Multi-Splits
represents strategies that have a ﬁxed ratio and in-
dependent K. We observe: (1) Multi-Splits (blue
lines) is the most stable in correlation and perfor-
mance, while other strategies CV and MDL are
more sensitive to the choice of K. (2) Multi-Splits
shows the smallest variance over multiple runs on
both BoolQ and RTE. For COPA, though Multi-
Splits shows high variance when K= 2, the vari-
ance becomes smaller with larger K, while CV and
MDL suffer from increasing or unstable variance.
A possible explanation is that increasing Kdoes
not affect the number of training and development
examples for Multi-Splits; instead, it increases the
conﬁdence of results. An important practical ben-
eﬁt of Multi-Splits is that one can always choose
to increase Kfor lower variance. However, for CV
and MDL, the sizes of training and development
sets are affected by K, where extremely large K
value leads to a failure mode and extremely small
Kleads to unstable results. In practice, it is hard
to know which value of Kto use a priori.
To sum up, based on the aforementioned results
and analysis, we arrive at the following ﬁnding.
Finding 1. Our proposed Multi-Splits is a more
reliable data-split strategy than several baselines
with improvements in (1) test performance, (2) cor-
relation between development and test sets, and (3)
stability w.r.t. number of runs.
Remark Our evaluation framework is better in
terms of test performance, dev-test correlation, and
stability, which proves it can achieve possible peak
performance, reliably select the corresponding hy-
perparameters according to dev results without
overﬁtting, and mitigate the effects of randomness
to the maximum extent. Therefore, the estima-
tion of our evaluation framework for model perfor-
mance is more reliable than previous evaluations.
5 Re-Evaluation of State-of-the-Art
Methods
5.1 Few-Shot Methods
We now proceed to re-evaluate state-of-the-art few-
shot methods under our evaluation framework with
the Multi-Splits strategy. We consider two types:
minimal few-shot methods , which only assume ac-
cess to a small labeled dataset, including Classiﬁ-
cation (CLS; Devlin et al., 2018), PET (Schick and
Schütze, 2021b), ADAPET (Menon et al., 2021),P-tuning (Liu et al., 2021b) and FlipDA (Zhou
et al., 2021); and semi-supervised few-shot meth-
ods, which allow accessing an additional unlabeled
dataset, including PET+MLM (Schick and Schütze,
2021a), iPET (Schick and Schütze, 2021b) and
Noisy Student (Xie et al., 2020).
5.2 Experimental Setup
The same benchmark datasets, metrics, and hyper-
parameter space as in §4.2.3 are used. We use
32 labeled samples for training. We consider two
labeling strategies to obtain the pseudo-labels on
unlabeled samples used by the semi-supervised
methods for self-training, including single-split la-
beling andcross-split labeling . In the single-split
setting (Schick and Schütze, 2021b), pseudo-labels
are generated by the models trained on the same
data split. In the cross-split setting in our evalua-
tion framework, the pseudo-labels are generated by
the models trained on multiple different data splits.
More conﬁguration details are in Appendix A.4.
5.3 Main Results and Analysis
Re-Evaluation Results Table 4 shows our re-
evaluation results. The prompt-based ﬁne-tuning
paradigm signiﬁcantly outperforms the classiﬁca-
tion ﬁne-tuning on all tasks and on both pretrained
models (with an advantage of more than 15 points
on average). DeBERTa outperforms ALBERT con-
sistently. We observe signiﬁcant differences in per-
formance between different prompt-based minimal
few-shot methods with ALBERT (e.g., ADAPET
and FlipDA outperform PET respectively by about
4 points and 2 points on average) while differences
with DeBERTa are slight (e.g., PET, ADAPET, P-
tuning, and FlipDA have a performance gap of
only about 1.0 points on average). In contrast, semi-
supervised few-shot methods (i.e., iPET and Noisy)
generally improve 1–2 points on average compared
to minimal few-shot methods on both models.
Comparison to Prior Evaluations Since we
have proved that our evaluation framework is
more reliable in estimating method performance as
shown in Section 4.2.4, we conduct experiments to
compare the estimates by our evaluation framework
and prior evaluations to study whether model per-
formance was accurately estimated in prior work.
Table 6 lists the absolute performance from prior
evaluations and our evaluation. Results show the
absolute performance of few-shot methods in prior
evaluations was generally overestimated on RTE506507
and COPA. Similar ﬁndings have been highlighted
in prior works (Perez et al., 2021; Zhang et al.,
2020), and our evaluation framework conﬁrms the
ﬁndings under a more reliable setup. This results
from a more reliable evaluation procedure that em-
phasizes dev-test correlation to prevent overﬁtting
(discussed in Section 4.2).
Besides, the relative gaps between different
methods were not accurately estimated by the prior
reported numbers. For example, according to the
reported results in prior works, ADAPET outper-
forms P-Tuning on COPA and P-Tuning beats
ADAPET on WiC, while our evaluation reveals the
opposite. On one hand, this is because prior results
were obtained under a less reliable evaluation pro-
cedure (discussed in Section 4.2). Deviation in the
estimates of absolute performance contributes to
inaccuracy in the estimates of relative performance.
On the other, prior experiments were not conducted
under a shared evaluation procedure. These two
factors are corrected by our re-evaluation under the
more reliable proposed framework.
To sum up, our re-evaluation compares all meth-
ods on a common ground, revealing the following:
Finding 2. The absolute performance and the rela-
tive gap of few-shot methods were in general not
accurately estimated in prior literature. This is
corrected by our new evaluation framework with
improved reliability. It highlights the importance
of evaluation for obtaining reliable conclusions.Moreover, the beneﬁts of some few-shot methods
(e.g., ADAPET) decrease on larger pretrained mod-
els like DeBERTa.
5.4 What is the Best Performance Few-Shot
Learning can Achieve?
We further explore the best few-shot performance
by combining various methods, and evaluating un-
der our evaluation framework. For combined op-
tions, we consider ﬁve minimal few-shot methods
(i.e., CLS, PET, ADAPET, P-tuning, and FlipDA),
ﬁve training paradigms (i.e., single-run, iPET (sin-
gle/cross), and Noisy Student (single/cross)), and
the addition of a regularized loss (+MLM). We ex-
periment with all possible combinations and report
the best for each task.
“Best (few-shot)” in Table 4 achieves the best
results on all tasks among all methods. Existing
few-shot methods can be practically used in com-
bination. Compared to RoBERTa (fully-sup) (Liu
et al., 2019), the performance gap has been further
narrowed to 2.89 points on average.Compared to
DeBERTa (fully-sup), there is still a sizeable gap
between few-shot and fully-supervised systems.
We list the best-performing combination for each
task in Table 5. The best combinations are very dif-
ferent across tasks, and there is no single method
that dominates most tasks. PET and ADAPET as
well as iPET and Noisy Student are about equally
preferred while cross-split labeling and no regular-
ization term perform better. We thus recommend
future work to focus on the development of meth-
ods that achieve consistent and robust performance
across tasks. We summarize the following ﬁndings:
Finding 3. Gains of different methods are largely
complementary. A combination of methods largely
outperforms individual methods, performing close
to a strong fully-supervised baseline on RoBERTa.
However, there is still a sizeable gap between the
best few-shot and the fully-supervised system.
Finding 4. No single few-shot method dominates
most NLU tasks. This highlights the need for the
development of few-shot methods with more con-
sistent and robust performance across tasks.
6 FewNLU Toolkit
We open-source FewNLU, an integrated toolkit
designed for few-shot NLU. It contains implemen-508tations of state-of-the-art methods, data processing
utilities, a standardized few-shot training frame-
work, and most importantly, our proposed evalua-
tion framework. Figure 2 shows the architecture.
We hope FewNLU could facilitate benchmarking
few-shot learning methods for NLU tasks and ex-
pendit the research in this ﬁeld.
7 Conclusions
We introduce an evaluation framework, re-evaluate
a number of few-shot learning methods under the
evaluation framework with a novel Multi-Splits
strategy, and release a few-shot toolkit. Apart from
this, we also aim at advancing the development of
few-shot learning by sharing several new experi-
mental ﬁndings. We identify several new directions
for future work: (1) In practice, how to deﬁne the
hyper-parameter search space a priori is a challenge.
(2) It is critical for the community to iterate and
converge on a common evaluation framework. (3)
Few-shot natural language generation might also
be studied in a similar framework.
Acknowledgements
We thank Dani Yogatama for valuable feedback on
a draft of this paper. Tang is funded by NSFC for
Distinguished Young Scholar (61825602). Zheng,
Ding, Tang, and Yang are funded by the National
Key R&D Program of China (2020AAA0105200)
and supported by Beijing Academy of Artiﬁcial
Intelligence (BAAI). Zheng is Funded by China
Postdoctoral Science Foundation (2021M690471).
Zhou and Li are supported in part by the Na-
tional Natural Science Foundation of China Grant
62161146004, Turing AI Institute of Nanjing and
Xi’an Institute for Interdisciplinary Information
Core Technology.
References509510A Appendix
A.1 Fixed Hyper-Parameters are not
Optimal
Some prior works (Schick and Schütze, 2021a,b;
Menon et al., 2021) perform few-shot learning with
a ﬁxed set of hyper-parameters (determined by
practical considerations and experiences) without
early stopping and any model selection.
We ﬁrst study how well ﬁxed hyper-parameters
transfer to a new scenario, e.g. switching to an-
other base pretrained model. We perform prelim-
inary experiments on FewGLUE with 64 labeled
sample based on DeBERTa. Firstly, we experiment
with the ﬁxed hyper-parameters used for ALBERT
in (Schick and Schütze, 2021b). Secondly, we man-
ually try other hyper-parameters to ﬁnd out whether
there are better conﬁgurations. From Table 7, we
observe:
1.Certain factors, especially the patterns, impact
the performance a lot (best 80.26%, and worst
61.13%). However, we cannot differentiate be-
tween them without a development set.
2.There exists a hyper-parameter (“Optimal” in
Table 7) that performs much better than the ﬁxed
one. A mechanism to identify the best hyper-
parameter setting is thus necessary.
3.Results show a good hyper-parameter on AL-
BERT does not work well on DeBERTa. Fixed
hyper-parameters are not optimal and we need
to re-select them given new conditions.
A.2 Details of How to Construct Data Splits
A.2.1 Datasets
To justify the proposed evaluation framework, we
perform experiments on the few-shot SuperGLUE
benchmark, which was constructed to include someof the most difﬁcult language understanding tasks
for current NLP approaches (Wang et al., 2019a).
Unlike other NLU benchmarks (e.g., GLUE (Wang
et al., 2019b)) that contain single-sentence tasks,
SuperGLUE consists of complicated ones that are
sentence-pair or sentence-triple tasks, which de-
mand advanced understanding capabilities. Seven
SuperGLUE tasks are considered, including ques-
tion answering (BoolQ (Clark et al., 2019) & Mul-
tiRC (Khashabi et al., 2018)), textual entailment
(CB (De Marneffe et al., 2019) & RTE (Dagan et al.,
2005)), word sense disambiguation (WiC (Pilehvar
and Camacho-Collados, 2018)), causal reasoning
(COPA (Roemmele et al., 2011)), and co-reference
resolution (WSC (Levesque et al., 2012)).
A.2.2 Hyper-parameters
To quantitatively evaluate different data-split strate-
gies, we perform extensive experiments with the
following hyper-parameter search space. Data-split
experiments are based on DeBERTa-xxLarge. The
hyper-parameter search space is shown in Table 8.
We use the same prompt patterns as in (Schick and
Schütze, 2021b). To observe the changes of perfor-
mance and correlation metrics w.r.t different Kval-
ues, we also experimented with K=f2;4;8;16g
over three tasks (i.e., BoolQ, RTE and COPA).
A.2.3 Evaluation Results with 32 Labeled
Data
In the data-split strategy evaluation, in addition to
the 64-data-setting results in the main text, we also
experimented with 32 labeled data as (Schick and
Schütze, 2021b,a; Menon et al., 2021). The 32-
data-setting results are also provided in Table 10.
A.2.4 Leave-One-Out Cross Validation
Results
We also experiment with another useful data split
strategy, leave-one-out cross validation (LOOCV).
In fact, LOOCV is a special case of K-fold cross
validation when Kequals the number of labeled
data. Since LOOCV takes even longer time than
any other data split strategies, we only experi-511
mented on three tasks, including BoolQ, RTE
and WiC tasks. Both performance and correla-
tion results are shown in Table 9. Our results
show that compared to other strategies, LOOCV
achieved worse test performance as well as cor-
relation. LOOCV only uses a single instance for
validation each time, and thus leads to poor corre-
lation and random model selection. As a result, the
performance estimation is subject to much random-
ness.
A.3 How to Deﬁne the Hyper-parameter
Search Space
Aside from how to construct the data splits, another
important question for the evaluation framework
is how to deﬁne the hyper-parameter search space.
We left this question in the future work. However,
we did several preliminary experiments that could
reveal certain insights into the problem.
A.3.1 Should We Search Random Seeds?
We focus on two types of factors that affect few-
shot evaluation, hyper-parameters and randomness.
Randomness could cause different weight initial-
ization, data splits, and data order during training.
Empirically, how randomness is dealt with differs
depending on the use case. In order to obtain the
best possible performance, one could search over
sensitive random factors such as random seeds.
However, as we focus on benchmarking few-shot
NLU methods, we report mean results (along with
the standard deviation) in our experiments in order
to rule out the effects of randomness and reﬂect the
average performance of a method for fair compari-
son and measurement.
A.3.2 Experiments
Experimental Setup To examine how a certain
factor affects few-shot performance, we assign mul-
tiple different values to a target factor while ﬁxingother hyper-parameters. We report the standard de-
viation over the multiple results. Larger values in-
dicate that a perturbation of the target factor would
largely inﬂuence the few-shot performance and the
factor thus is crucial for searching. We experiment
on BoolQ, RTE, CB, and COPA tasks. Consid-
ered factors include: sample order during training,
prompt pattern, training batch size, learning rate,
evaluation frequency, and maximum train steps.
Results and Analysis Results are in Table 11. We
mark values larger than a threshold of 2:0in bold.
We can see that the prompt pattern is the most in-
ﬂuential factor among all, indicating the design or
selection of prompt patterns is crucial. Training
example order also signiﬁcantly affects the perfor-
mance. The evaluation frequency affects the score
on the small development but not on the test set.
We speculate that a lower frequency selects a model
with better performance on the small development
set, but the gains do not transfer to the test set be-
cause of partial overﬁtting. To conclude:
Finding 5. We recommend to at least search over
prompt patterns during hyper-parameter tuning,
and it is also beneﬁcial to search others. All com-
parison methods should be searched and compared
under the same set of hyper-parameters.
A.3.3 Detailed Conﬁguration
For a given task and a target factor, we ﬁxed the
hyper-parameters to be the best-performing ones
obtained in Section 4.2, and assigned multiple val-
ues for the target factor. For the prompt pattern,
we assigned it with the same values as (Schick and
Schütze, 2021b). Possible values for other hyper-
parameters are in Table 12.
A.4 Details of Re-Evaluation
A.4.1 Methods
The ﬁve considered minimal few-shot methods are
introduced as follows.
1.Classiﬁcation is a conventional ﬁnetuning algo-
rithm, which uses the hidden states of a special
[CLS] token for classiﬁcation.
2.PET is a prompt-based ﬁnetuning algorithm. It
transforms NLU problems into cloze problems
with prompts, and then converts the cloze out-
puts into the predicted class.
3.ADAPET is based on PET and decouples the
losses for the label tokens. It proposes a label-
conditioned masked language modeling (MLM)
objective as a regularization term.512
4.P-tuning is also based on PET and automati-
cally learns continuous vectors as prompts via
gradient update.
5.FlipDA is similar to PET but uses both labeled
data and augmented data for training. The aug-
mented data are automatically generated by tak-
ing labeled data as inputs.
The three semi-supervised few-shot methods are
introduced as follows.
1.PET+MLM is based on PET and additionally
adds an auxiliary language modeling task per-
formed on unlabeled dataset. It was ﬁrst pro-posed by (Schick and Schütze, 2021a) to resolve
catastrophic forgetting.
2.iPET is a self-training method. It iteratively per-
forms PET for multiple generations. At the end
of each generation, unlabeled data are assigned
with pseudo-labels by the fully-trained model,
and will be used for training along with train
data in the next generation.
3.Noisy Student is similar to iPET with the differ-
ence that Noisy Student injects noises into the
input embeddings of the model.
A.4.2 Hyper-parameter Search Space
The hyper-parameter search space for other few-
shot methods are shown in Table 17.
A.4.3 The Searched Best Hyper-parameters
We list the searched best hyper-parameter conﬁgu-
ration for different tasks and methods in Table 13,
Table 14, Table 15, Table 16.
A.4.4 More Discussion on ADAPET
Since it is observed ADAPET shows less improve-
ment on DeBERTa than it has achieved on AL-
BERT, we further discuss the phenomena by rais-
ing the question what other differences it has made.
We respectively visualize the few-shot performance
distribution over the same hyper-parameter space
of PET and ADAPET in Figure 3. We observe
that PET is more likely to obtain extremely bad513514
results on BoolQ and RTE, while ADAPET shows
stable results. It suggests that ADAPET appears
to be more robust to the hyper-parameters, and
overall achieves good performance regardless of
hyper-parameter selection. However, ADAPET is
less inclined to produce better peak results. To
sum up, we can conclude: Loss regularization (e.g.,
ADAPET (Menon et al., 2021)) enhances stability
w.r.t. hyper-parameters.
A.4.5 More Discussion on Semi-supervised
Few-shot Methods
We focus on semi-supervised methods that itera-
tively augment data (i.e., iPET and Noisy Student),
which have demonstrated promising results on both
models in Table 4. Several key points for their suc-
cess are especially discussed.
1.For semi-supervised methods such as iPETand Noisy Student, it is time-consuming when
searching over a large hyper-parameter space for
each generation. We directly use the searched
best hyper-parameters for PET in each gener-
ation. From Table 4, we can see that their
results show advantages over PET (by more
than 1 points). It suggests that the best hyper-
parameters can be transferred to such methods,
to reduce the cost of time and computational re-
sources. If we search for each generation, results
might be even better.
2.Comparing the single-split labeling strategy, the
cross-split labeling strategy works better. As the
results show, both iPET (cross) and Noisy (cross)
outperform iPET (single) and Noisy (single) in
most tasks on both models.
3.Another simple and effective technique is our
proposed ensemble labeling strategies. (Schick
and Schütze, 2021b) utilizes the ensemble results
over all patterns to label unlabeled data, since it
is hard to select patterns. Under the Multi-Splits
strategy, self-training methods can recognize the
best pattern, and only ensemble trained models
for the best pattern when labeling unlabeled data.
Table 18 shows the results of iPET on WiC and
RTE tasks, respectively ensemble over multiple
patterns or ensemble over the only best pattern.
We can see that results of ensemble with the
best pattern signiﬁcantly outperform results of
ensemble with all patterns at every generation.515516