
Sewon MinMike LewisLuke ZettlemoyerHannaneh HajishirziUniversity of WashingtonMeta AIAllen Institute for AI
{sewon,lsz,hannaneh}@cs.washington.edu mikelewis@fb.com
Abstract
We introduce MetaICL ( Meta -training for In-
Context Learning), a new meta-training frame-
work for few-shot learning where a pretrained
language model is tuned to do in-context learn-
ing on a large set of training tasks. This meta-
training enables the model to more effectively
learn a new task in context at test time, by sim-
ply conditioning on a few training examples
with no parameter updates or task-speciﬁc tem-
plates. We experiment on a large, diverse col-
lection of tasks consisting of 142 NLP datasets
including classiﬁcation, question answering,
natural language inference, paraphrase detec-
tion and more, across seven different meta-
training/target splits. MetaICL outperforms a
range of baselines including in-context learn-
ing without meta-training and multi-task learn-
ing followed by zero-shot transfer. We ﬁnd
that the gains are particularly signiﬁcant for
target tasks that have domain shifts from the
meta-training tasks, and that using a diverse
set of the meta-training tasks is key to im-
provements. We also show that MetaICL
approaches (and sometimes beats) the per-
formance of models fully ﬁnetuned on the
target task, and outperforms much bigger
models with nearly 8x parameters. Finally,
we show that MetaICL is complementary to
human-written instructions, and the best per-
formance can be achieved by combining both
approaches.
1 Introduction
Large language models (LMs) have recently been
shown to be able to do in-context learning (Brown
et al., 2020), where they learn a new task simply
by conditioning on a few training examples and
predicting which tokens best complete a test input.
This type of learning is attractive because the model
learns a new task through inference alone, without
any parameter updates. However, performance sig-
niﬁcantly lags behind supervised ﬁnetuning, results
are often high variance (Zhao et al., 2021; Perezet al., 2021), and it can be difﬁcult to engineer the
templates that convert existing tasks to this format.
In this paper, we address these challenges by in-
troducing MetaICL: Meta -training for In-Context
Learning. MetaICL tunes a pretrained language
model on a large set of tasks to learn how to in-
context learn, and is evaluated on strictly new un-
seen tasks. Each meta-training example matches
the test setup—it includes k+ 1training examples
from one task that will be presented together as
a single sequence to the language model, and the
output of the ﬁnal example is used to calculate the
cross-entropy training loss. Simply ﬁnetuning the
model in this data setup directly leads to better in-
context learning—the model learns to recover the
semantics of the task from the given examples, as
must be done for in-context learning of a new task
at test time. This approach is related to recent work
that uses multi-task learning for better zero-shot
performance at test time (Khashabi et al., 2020;
Zhong et al., 2021; Mishra et al., 2022; Wei et al.,
2022; Sanh et al., 2022). However, MetaICL is dis-
tinct as it allows learning new tasks from kexam-
ples alone, without relying on a task reformatting
(e.g., reducing everything to question answering)
or task-speciﬁc templates (e.g., converting different
tasks to a language modeling problem).
We experiment on a large, diverse collection of
tasks taken from Ye et al. (2021) and Khashabi et al.
(2020), including 142 text classiﬁcation, question
answering, natural language inference and para-
phrase detection datasets. We report seven different
settings, all with no overlap between meta-training
and target tasks. This leads to 52 unique target
tasks in total, which is the largest among all recent
related work to the best of our knowledge.
Experimental results show that MetaICL consis-
tently outperforms baselines including (1) a variety
of LM in-context learning baselines without meta-
training (Brown et al., 2020; Zhao et al., 2021;
Holtzman et al., 2021; Min et al., 2022), and (2)2791multi-task learning followed by zero-shot trans-
fer (Zhong et al., 2021; Wei et al., 2022; Sanh
et al., 2022). Gains over multi-task zero-shot trans-
fer are particularly signiﬁcant when meta-training
tasks and target tasks are dissimilar, e.g. there
are large differences in task formats, domains, or
required skills. This demonstrates that MetaICL
enables the model to recover the semantics of the
task in context during inference even when the tar-
get does not share similarities with meta-training
tasks. MetaICL often gets close to (and sometimes
beats) the performance of models trained with su-
pervised ﬁnetuning on the target datasets, and per-
form as well as models with 8x parameters. We
also perform extensive ablations to identify key in-
gredients for success of MetaICL such as the num-
ber and diversity of meta-training tasks. Finally,
we demonstrate MetaICL without any templates is
better than recent work using human-written nat-
ural instructions, while the best performance is
achieved by combining both approaches. Code
and data are publicly released at github.com/
facebookresearch/MetaICL .
2 Related Work
In-context learning Brown et al. (2020) propose
to use a language model (LM) conditioned on a
concatenation of training examples for few-shot
learning with no parameter updates. It has been
further improved by later work (Zhao et al., 2021;
Holtzman et al., 2021; Min et al., 2022), showing
promising results on a variety of tasks. However,
in-context learning with an LM achieves poor per-
formance when the target task is very different from
language modeling in nature or the LM is not large
enough. Moreover, it can have high variance and
poor worst-case accuracy (Perez et al., 2021; Lu
et al., 2021).
Our paper is based on the core idea of in-context
learning by conditioning on training examples. We
show that, by explicitly training on an in-context
learning objective, MetaICL achieves substantial
improvements even with smaller LMs.
Meta-training via multi-task learning Our
work is broadly inspired by a large body of work
in meta-learning (Vilalta and Drissi, 2002; Finn
et al., 2017) and multi-task learning (Evgeniou
and Pontil, 2004; Ruder, 2017). Prior work has
shown that multi-task learning on a large collec-
tion of tasks leads to better performance on a newtask, either when tested zero-shot (Khashabi et al.,
2020; Zhong et al., 2021; Mishra et al., 2022; Wei
et al., 2022) or when further ﬁnetuned (Aghajanyan
et al., 2021; Ye et al., 2021). In particular, the for-
mer is closely related to our work, as it eliminates
the need for parameter updates on a target task.
However, these zero-shot models are either limited
to tasks sharing the same format as training tasks
(e.g., a question answering format) (Khashabi et al.,
2020; Zhong et al., 2021), or rely heavily on task-
speciﬁc templates (Mishra et al., 2022; Wei et al.,
2022; Sanh et al., 2022) which are difﬁcult to en-
gineer due to high variance in performance from
very small changes (Mishra et al., 2021).
In this paper, we propose a meta-training method
for better in-context learning that improves few-
shot performance. We show that it effectively
learns semantics of a new task with no manual
effort, signiﬁcantly outperforming zero-shot trans-
fer methods.Furthermore, while Wei et al. (2022)
show that meta-training helps only when the model
has 68B or more parameters, our experiments
demonstrate improvements with a much smaller
model (770M).
Chen et al. (2022), concurrently to our work, pro-
pose meta-training for in-context learning. Our ap-
proach differs in a number of ways: we remove re-
quirements of human-written templates or instruc-
tions, and include more diverse tasks, stronger base-
lines, and extensive experiments in much larger
scale with many meta-training/target splits.
3 MetaICL
We introduce MetaICL: Meta -training for In-
Context Learning. Table 1 provides an overview
of the approach. The key idea is to use a multi-task
learning scheme over a large collection of meta-
training tasks, in order for the model to learn how
to condition on a small set of training examples, re-
cover the semantics of a task, and predict the output
based on it. Following previous literature (Brown
et al., 2020), the training examples are concate-
nated and provided as an single input to the model,
which is feasible for k-shot learning (e.g., k= 16 ).
At test time, the model is evaluated on an unseen
target task that comes with ktraining examples,
and inference directly follows the same data format
as in meta-training.2792
3.1 Meta-training
The model is meta-trained on a collection of tasks
which we call meta-training tasks. For every itera-
tion, one meta-training task is sampled, and k+ 1
training examples (x, y),···,(x, y)are
sampled from the training examples of the cho-
sen task. We then supervise the model by feed-
ing the concatenation of x, y,···, x, y, x
to the model as an input and train the model to gen-
erate yusing a negative log likelihood objec-
tive. This simulates in-context learning at inference
where the ﬁrst kexamples serve as training exam-
ples and the last (k+ 1) -th example is regarded as
the test example.
3.2 Inference
For a new target task, the model is given ktrain-
ing examples (x, y),···,(x, y)as well as a
test input x. It is also given a set of candidates
Cwhich is either a set of labels (in classiﬁcation)
or answer options (in question answering). As in
meta-training, the model takes a concatenation of
x, y,···, x, y, xas the input, and compute the
conditional probability of each label c∈C. The
label with the maximum conditional probability is
returned as a prediction.
3.3 Channel MetaICL
We introduce a noisy channel variant of MetaICL
called Channel MetaICL, following Min et al.
(2022). In the noisy channel model, P(y|x)is
reparameterized to∝P(x|y)P(y). We
follow Min et al. (2022) in using P(y) =and
modeling P(x|y)which allows us to use the chan-
nel approach by simply ﬂipping xandy. Specif-
ically, at meta-training time, the model is given
a concatenation of y, x,···, y, x, yand is
trained to generate x. At inference, the model
computes argmaxP(x|y, x,···, y, x, c).
4 Experimental Setup
4.1 Datasets
We use a large collection of tasks taken
from C F(Ye et al., 2021) and U- QA (Khashabi et al., 2020). We have 142
unique tasks in total, covering a variety of prob-
lems including text classiﬁcation, question answer-
ing (QA), natural language inference (NLI) and
paraphrase detection. All tasks are in English.
We experiment with seven distinct settings as
shown in Table 2, where there is no overlap be-
tween the meta-training and target tasks. The num-
ber of unique target tasks in total is 52, which is sig-
niﬁcantly larger than other relevant work (Khashabi
et al., 2020; Zhong et al., 2021; Mishra et al., 2022;2793
Wei et al., 2022; Sanh et al., 2022). Each target
task is either classiﬁcation or multi-choice, where
a set of candidate options ( Cin Table 1) is given.
HR→LR(High resource to low resource): We ex-
periment with a setting where datasets with 10,000
or more training examples are used as meta-training
tasks and the rest are used as target tasks. We think
using high resource datasets for meta-training and
low resource datasets as targets is a realistic and
practical setting for few-shot learning.
X→X (X={Classiﬁcation, QA}) : We experiment
with two settings with meta-training and target
tasks sharing the task format, although with no
overlap in tasks.
Non-X→X (X={Classiﬁcation, QA, NLI, Para-
phase}) : Lastly, we experiment with four settings
where meta-training tasks do not overlap with tar-
get tasks in task format and required capabilities.
These settings require the most challenging gener-
alization capacities.
Each setting has a subset of target tasks with
no domain overlap with any meta-training tasks
(e.g., ﬁnance, poem, climate or medical). We report
both on all target tasks or on target tasks with no
domain overlap only. Full details of the settings and
datasets with citations are provided in Appendix A.
4.2 Baselines
We compare MetaICL and Channel MetaICL with
a range of baselines, as summarized in Table 3.
0-shot : We use a pretrained LM as it is and run
zero-shot inference, following Brown et al. (2020).
In-context : We use the pretrained LM as it is and
use in-context learning by conditioning on a con-
catenation of ktraining examples, following Brown
et al. (2020).
PMI 0-shot, PMI In-context : We use the PMI
method from Holtzman et al. (2021); Zhao et al.
(2021) for 0-shot and In-context learning.
Channel 0-shot, Channel In-context : We use the
noisy channel model from Min et al. (2022) for
0-shot and In-context learning.
Multi-task 0-shot : We train the LM on the same
meta-training tasks without in-context learning ob-
jective, i.e., maximize P(y|x)without kother train-
ing examples, and then use zero-shot transfer on
a target task. This is equivalent to MetaICL with
k= 0. This is a typical multi-task learning ap-
proach from previous work (Khashabi et al., 2020;
Zhong et al., 2021; Wei et al., 2022).
Channel Multi-task 0-shot : We have a channel
variant of Multi-task 0-shot.
Fine-tune : We ﬁne-tune the LM on an individual
target task. This is not directly comparable to other
methods as parameter updates are required for ev-
ery target task.
Fine-tune w/ meta-train : We train the LM on
meta-training tasks ﬁrst and then further ﬁne-tuned
it on a target task. This is not directly comparable
to other methods for the same reason as above.2794
4.3 Evaluation
We use Macro-F1and Accuracy as evaluation met-
rics for classiﬁcation tasks and non-classiﬁcation
tasks, respectively.
For a target task, we use k= 16 training exam-
ples, sampled uniformly at random. We relax the
assumption of perfect balance between labels on
ktraining examples, following Min et al. (2022).
Because in-context learning is known to have high
variance (Zhao et al., 2021; Perez et al., 2021; Lu
et al., 2021), we use 5 different sets of ktraining
examples. We ﬁrst compute the average and the
worst-case performance over seeds for every target
task, and then report the macro-average of them
over all target tasks.
4.4 Experiment Details
As a base LM, we use GPT-2 Large (Radford
et al., 2019) which consists of 770M parameters.
For baselines without meta-training (raw LMs), we
also compare with GPT-J (Wang and Komatsuzaki,2021), which is the largest public causal LM at the
time of writing, consisting of 6B parameters.
Elimination of templates Prior work uses
human-authored templates to transform the input-
output pair to a natural language sentence (Zhong
et al., 2021; Mishra et al., 2022; Wei et al., 2022;
Chen et al., 2022). They require expensive manual
effort (as 136 different templates are required for
136 tasks in this paper) and cause unstable model
performance due to many different ways of writ-
ing (Mishra et al., 2021). We eliminate templates,
using the given input (or a concatenation of in-
puts if there are multiple) and label words provided
in the original datasets.A comparison of input-
output schemes from prior work and our approach
is shown in Table 4.
Training details All implementation is done in
PyTorch (Paszke et al., 2019) and Transform-
ers (Wolf et al., 2020). For meta-training, we use2795
up to 16,384 training examples per task. We use a
batch size of 8, learning rate of 1×10and a se-
quence length of 1024 . For multi-task 0-shot base-
lines (the baselines with no in-context learning), we
use a sequence length of 256. We train the model
for30,000steps.To save memory during meta-
training, we use an 8-bit approximation (Dettmers
et al., 2022) of an Adam optimizer (Kingma and
Ba, 2015) and mixed precision (Micikevicius et al.,
2017). Training was done for 4.5 hours with eight
32GB GPUs. This is drastically more efﬁcient than
recent prior work, e.g., 270 hours of a 512GB TPU
in Sanh et al. (2022).
More details about preprocessing and training
can be found in Appendix B.
5 Experimental Results
5.1 Main Results
Table 5 reports the full results using GPT-2 Large,
where we compute the average and the worst-case
performance of every target task and report the
macro-average over them. The top and the bottom
respectively evaluate on all target tasks and target
tasks in unseen domains only.
Our baselines are strong We ﬁrst discuss the re-
sults of ours baselines. Among raw LMs without
meta-training (the ﬁrst six rows of Table 5), we
observe that channel in-context baselines are the
most competitive, consistent with ﬁndings from
Min et al. (2022). We then ﬁnd that Multi-task 0-
shot baselines do not outperform the best raw LMbaseline in most settings, despite being supervised
on a large set of meta-training tasks. This some-
what contradicts ﬁndings from Wei et al. (2022);
Sanh et al. (2022). This is likely for two rea-
sons. First, our models are much smaller than
theirs (770M vs. 11B–137B); in fact, Wei et al.
(2022) reports Multi-task 0-shot starts to be better
than raw LMs only when the model size is 68B or
larger. Second, we compare with much stronger
channel baselines which they did not; Multi-task
0-shot outperforms non-channel LM baselines but
not channel LM baselines.
MetaICL outperforms baselines MetaICL and
Channel MetaICL consistently outperform a range
of strong baselines. In particular, Channel MetaICL
achieves the best performance in 6 out of 7 set-
tings. Gains are particularly signiﬁcant in the
HR→LR, non-NLI→NLI and non-Para→Para set-
tings (6–15% absolute). This is noteworthy be-
cause HR→LR targets the common low-resource
case where new tasks have very few labeled ex-
amples, and the other two represent large data dis-
tribution shifts where the test tasks are relatively
different from the meta-training tasks. This demon-
strates that MetaICL can infer the semantics of new
tasks in context even when there are no closely
related training tasks.
While MetaICL signiﬁcantly outperforms base-
lines in most settings, it only marginally outper-
forms Multi-task 0-shot in the QA →QA setting,
as an exception. This is likely because the meta-
training and target tasks are relatively similar, al-
lowing the Multi-task 0-shot baseline to achieve
very strong performance. Nonetheless, perfor-2796
mance of Multi-task 0-shot in QA signiﬁcantly
drops when the model is trained on non-QA tasks,
while performance of MetaICL drops substantially
less.
Gains are larger on unseen domains Gains
over Multi-task 0-shot are more signiﬁcant on tar-
get tasks in unseen domains. In particular, Multi-
task 0-shot is generally less competitive compared
to raw LM baselines, likely because they require
more challenging generalization. MetaICL suffers
less from this problem and is consistently better or
comparable to raw LM baselines across all settings.
Comparison to ﬁne-tuning MetaICL matches
or sometimes even outperforms ﬁne-tuned mod-
els without meta-training. This is a promising
signal, given that no prior work has shown mod-
els with no parameter updates on the target can
match or outperform supervised models. Nonethe-
less, ﬁne-tuning with meta-training exceeds both
MetaICL and ﬁne-tuning without meta-training, be-
cause meta-training helps in supervised learning as
it does in in-context learning. This indicates that
there is still room for improvement in methods that
allow learning without parameter updates .
Comparison to GPT-J In Table 6, we compare
GPT-2 Large based models with raw LM baselines
based on GPT-J which consists of 6B parameters.
MetaICL, despite being 8x smaller, outperforms or
matches GPT-J baselines.
5.2 Ablations
Varying number of training examples We vary
the number of training examples ( k) from 0, 4, 8,
16 to 32. In-context learning with k= 0is equiv-
alent to the zero-shot method. Results are shown
in Figure 1. Increasing kgenerally helps across all
models, and Channel MetaICL outperforms the raw
in-context learning over all values of k. We addi-
tionally ﬁnd that the performance tends to saturate
when kis closer to 16, likely because the sequence
length limit of the language model makes it hard to
encode many training examples.
Number of meta-training tasks To see the im-
pact of the number of meta-training tasks, we sub-
sample{7,15,30}meta-training tasks out of 61 in
the HR→LR setting. For each, we use ten different
random seeds to additionally see the impact of the
choice of meta-training tasks.
Figure 2 reports the results. On average, perfor-
mance generally increases as the number of tasks
increase, which is consistent with results in Mishra
et al. (2022); Wei et al. (2022). Across different
numbers of meta-training tasks, Channel MetaICL
consistently outperforms other models. Nonethe-
less, there is nonnegligible variance across different
choices of meta-training (the bottom of Figure 2),
indicating that a choice of meta-training gives sub-
stantial impact in performance.
Diversity in meta-training tasks We hypothe-
size that the diversity in meta-training tasks may
impact performance of MetaICL. To verify this hy-
pothesis, we create two settings by subsampling 132797
out of 61 meta-training datasets in the HR →LR set-
ting. One setting is diverse in their task formats and
required capacities: QA, NLI, relation extraction,
sentiment analysis, topic classiﬁcation, hate speech
detection and more. The other setting is less di-
verse, including tasks related to sentiment analysis,
topic classiﬁcation and hate speech detection only.
A full list of datasets is reported in Appendix A.
Using these two settings, we compare multi-task
zero-shot transfer baselines and MetaICL.
Results are reported in Table 7. We ﬁnd that
MetaICL with a diverse set outperforms MetaICL
with a non-diverse set by a substantial margin. This
shows that diversity among meta-training tasks
is one of substantial factors for the success of
MetaICL.
In Appendix C.3, we include ablations that pro-
vide more insights on the choice of meta-training
tasks, such as (1) high quality data with diverse
domains tend to help (e.g., GLUE family (Wang
et al., 2018)) and (2) adversarially collected data
tends to be unhelpful. However, more systematic
studies on how to choose the best meta-training
tasks and how they relate to particular target tasks
should be done, which we leave for future work.
Are instructions necessary? Most recent work
has used human-written natural instructions for
zero- or few-shot learning (Mishra et al., 2022; Wei
et al., 2022; Sanh et al., 2022). While we argue for
not using instructions to avoid manual engineering
and high variance, we also ask: are instructions
still useful with MetaICL? On one hand, learning to
condition on kexamples may remove the necessity
of instructions. On the other hand, instructions may
still be complementary and provide the model with
extra useful infomration.
We aim to answer this question by using 32 meta-
training tasks and 12 target tasks from the HR →LR
setting for which human-written instructions are
available in Sanh et al. (2022).We have two vari-
ants: (a) using one instruction per meta-training
task, and (b) using all available instructions includ-
ing 267 instructions in total (8.3 per meta-training
task) which Sanh et al. (2022) found to be better
than (a). We then compare MetaICL and a range of
baselines with and without instructions.
Results are reported Table 8. As in Wei et al.
(2022) and Sanh et al. (2022), Multi-task 0-shot
outperforms the raw-LM 0-shot baseline. How-
ever, MetaICL with no instructions is better than
Multi-task 0-shot with instructions. Furthermore,
MetaICL achieves further improvements when in-
structions are jointly used, signiﬁcantly outperform-
ing all baselines. In fact, when increasing the num-
ber of instructions per task from 0, 1 to 8.3, per-
formance of MetaICL improves much more than
performance of Multi-task 0-shot does. To sum-
marize, (1) learning to in-context learn (MetaICL)
outperforms learning to learn from instructions; (2)
MetaICL and using instructions are largely comple-
mentary, and (3) MetaICL actually beneﬁts more
from using instructions than Multi-task 0-shot does.
Importantly, Channel MetaICL trained on avail-2798able tasks and instructions still achieves lower
performance than Channel MetaICL without tem-
plates/instructions ( 46.9from Table 8 vs. 49.1
from Table 5). This is likely because the model
with instructions was trained with less meta-
training tasks, which was unavoidable since in-
structions are only available on 32 out of 61 meta-
training tasks. This supports our earlier choice
of not using human-written templates/instructions,
since writing templates and instructions for every
task requires extensive effort.
It is worth noting that, it is nonetheless difﬁcult
to make direct comparisons with Wei et al. (2022)
and Sanh et al. (2022) because there are many mov-
ing components: size of LMs, types of LMs (e.g.,
causal LM vs. masked LM), splits between meta-
training and target tasks, and more.
6 Conclusion
In this paper, we introduced MetaICL, a new few-
shot learning method where an LM is meta-trained
to learn to in-context learn, i.e. condition on train-
ing examples to recover the task and make pre-
dictions. We experiment with a large, diverse col-
lection of tasks, consisting of 142 unique tasks in
total and 52 unique target tasks, using seven dif-
ferent settings. MetaICL outperforms a range of
strong baselines including in-context learning with-
out meta-training and multi-task learning followed
by zero-shot transfer, and outperforms or matches
8x bigger models. We identify ingredients for suc-
cess of MetaICL such as the number and diversity
of meta-training tasks. We also demonstrate that,
while MetaICL is better than recent work using
natural instructions, they are complementary and
the best performance is achieved by integrating
MetaICL with instructions.
Limitation & Future work Our work is limited
in multiple dimensions. First, in-context learn-
ing approaches in general requires much longer
context at both meta-training and inference due to
feeding the concatenation of the training data, thus
being less efﬁcient compared to baselines that do
not use in-context learning. Second, our work ex-
periment with a casual language model with mod-
est size (GPT-2 Large, 770M parameters). Future
work may investigate extending our approach to a
masked language model and a larger model. Third,
our experiments focus on classiﬁcation and multi-
choice tasks where a set of candidate options is
given. Future work may study applying our ap-proach for a wider range of tasks including free-
form generation. Other avenues for future work
include further improving MetaICL to outperform
supervised models with meta-training, identiﬁca-
tion of which meta-training tasks are helpful on
target tasks, and how to better combine human-
written instructions and MetaICL.
Acknowledgements
We thank Ari Holtzman and Victoria Lin for com-
ments and discussions, and Tim Dettmers for help
with experiments. This research was supported
by NSF IIS-2044660, ONR N00014-18-1-2826,
an Allen Distinguished Investigator Award, and a
Sloan Fellowship.
References279928002801280228032804A Dataset List
Table 14 and Table 15 report a list of datasets used
in the settings detailed in Section 4.1. The ﬁrst 10
rows are for settings described in Section 4.1; the
next two rows are for settings used for ablations
on the diversity of meta-training tasks (Table 7 of
Section 5.2); the last two rows are for settings used
for ablations on using natural instructions (Table 8
of Section 5.2). Bold datasets are target datasets
with no overlap in domain with meta-training tasks.
All datasets are taken from C F(Ye et al.,
2021) (except we exclude datasets that are unavail-
able from their repositoryor the scope is notably
different from other tasks, e.g., solving math prob-
lems or breaking down compositional questions)
and U QA (Khashabi et al., 2020).
How meta-training/target splits are determined
The HR→LR setting is created based on the train-
ing data size as described in Section 4.1. Settings
involving Classiﬁcation, NLI and Paraphrase are
taken from C F. Settings involving QA are
created by combining QA datasets from C F
and datasets from U QA.
Statistics are reported in Table 2 and Table 9.
The number of tasks is the largest among recent
related work: we have 142 unique tasks, while
Khashabi et al. (2020), Zhong et al. (2021), Mishra
et al. (2022), Wei et al. (2022) and Sanh et al.
(2022) use 32, 62, 61, 42 and 62 tasks, respec-
tively. References for all datasets are provided in
Table 15. Data and splits are available at github.
com/facebookresearch/MetaICL .
B Implementation Details
Preprocessing details For all models with meta-
training and the raw GPT-J, we separate the input
and the output with one newline ( \n), and separate
between examples with three newlines. For the raw
GPT-2, we use spaces instead of newlines. This
choice was made in order to report the best baseline
performance we were able to achieve: when raw
LMs are used, GPT-2 is signiﬁcantly better with
spaces than with newlines, and GPT-J is signiﬁ-
cantly better with newlines than with spaces.We
note that MetaICL is less sensitive to these format-
ting differences, having less than 2% differences
between using spaces and using newlines.
When the concatenation of kexamples is too
long, we truncate each example to have at most
256tokens, and truncate the earlier tokens of the
concatenation so that the LM sees the recent tokens.
Additionally, for extractive question answering
datasets as meta-training tasks, the input passage
is truncated with a guarantee that the groundtruth
answer is included in the input passage. We do not
do this truncation for target datasets.
Comparison with baselines in training and in-
ference cost Although being trained for the same
global steps (30,000 steps), it takes 3 hours to train
Multi-task 0-shot baselines (in contrast to 4.5 hours
for MetaICL), likely because the sequence length
is 4x shorter. At inference, Multi-task 0-shot base-
lines are roughly 4x more efﬁcient, also because
the sequence length is 4x shorter.We did not con-
trol for the training time and the inference time for
comparison since both models are efﬁcient enough.
Ablations in using instructions When we
choose one instruction per task at meta-training
tasks, we choose one by (1) ﬁrst excluding the
instruction if its name contains no_option ,
(2) then taking the instruction which name con-
tainsmultiple_choice ,most_correct or2805
most_suitable if there are any, and (3) if not,
then randomly sampling one. We choose one in-
struction per target task at test time using the same
process. This is different Sanh et al. (2022) where
the median of the performance over all instructions
is reported. We think our choice better reﬂects the
real use-case scenario—choosing one instruction
that looks the most reasonable to human.
C Additional Results & Analyses
C.1 GPT-J results
Table 10 reports the full results of raw LM baselines
based on GPT-J, consisting of 6B parameters. See
Section 5.1 for discussion.
C.2 Varying LM sizes
We vary the size of the GPT-2 models—small,
medium, large, and XL—with 124M, 355M, 774M,
and 1.5B parameters, respectively. Results are re-
ported in Table 11. We ﬁnd that (1) increasing
the model size generally helps, (2) for all model
sizes, Channel MetaICL signiﬁcantly outperforms
baselines, and (3) MetaICL enables a much smaller
model to outperform a bigger model, e.g., Chan-nel MetaICL based on GPT-2 Small outperforms
the GPT-2 XL baseline that is 12x bigger (46.2 vs.
43.5).
C.3 Which meta-training tasks are more
helpful?
Based on large variance across different choices of
meta-training (Figure 2 of Section 5.2), we think
certain tasks are more helpful for meta-training
than other tasks. In this context, we create 50
sets of seven meta-training tasks using 50different
random seeds. We then measure the correlation
between tasks/task pairs/task triples and average
performance of Channel MetaICL when the task is
included in the meta-training tasks.
Table 12 reports the result. We ﬁrst ﬁnd that high
quality datasets with diverse domain like GLUE
family (Wang et al., 2018) are often helpful. We
also ﬁnd that datasets that are collected adversar-
ially (e.g. paws ,art) or are notably dissimilar
from all other tasks (e.g. wikisql that requires
semantic parsing) are often unhelpful. Nonethe-
less, we were not able to ﬁnd good explanations for
other cases, e.g., many sentiment analysis datasets
being particularly helpful even though only 3 out2806
of 26 target datasets are sentiment analysis, and
dbpedia_14 /cosmos_qa /race-middle be-
ing unhelpful. Moreover, we think which tasks
are helpful largely depends on the choice of target
tasks, and we should not make early conclusions
that certain tasks are helpful/unhelpful in all cases.
We think future work should investigate these im-
pacts in a more systematic way.
C.4 Does MetaICL generalize when semantic
hints from label words are removed?
Our experiments use label words taken from the
original dataset, which often contain semantic
hints —hints on what each label is supposed to
mean ( entailment andnot_entailment
for the NLI task, and positive andnegativefor the sentiment analysis task). If the model
is truly learning the task in-context, it should
generalize when label words are replaced with
random English words, e.g., entailment and
not_entailment are replaced with apple
andorange , thus not giving any hints about the
task. In this context, we run experiments where
each label word is replaced with a random word
sampled from 61,569 common English words.
We use ﬁve seeds for sampling random words, and
report the average and the worst-case performance.
Results in Table 13 show that raw LMs (the ﬁrst
block of the table) and models trained on the orig-
inal data (the second block) achieve near random
guessing performance. This indicates that having
semantic hints from label words is a necessary con-
dition for all models to perform the task.
Next, we meta-train the MT 0-shot baseline and
MetaICL where, for each iteration of meta-training,
we similarly map label words with random words.
The mapping from the label set to sampled English
words is independent for each iteration, so that the
model never sees the same mapping during meta-
training and hence does not overﬁt to a speciﬁc
mapping. Results are reported in the third block of
Table 13. MT 0-shot baselines are still not better
than random guessing, which is expected as they
have no way to grasp the meaning of each label.
On the other hand, MetaICL beneﬁts from training
on the replaced data, improving performance from
30.1% to 43.5% while retaining most performance
on the original data ( 43.4%→40.7%).
Still, overall performance is relatively poor. We
think future work should investigate the model that
can in-context learn anytask.28072808
D Potential Risks
MetaICL is based on the large language model that
is pretrained on a web corpus, which potentially
includes harmful and biased context, despite the
original authors’ best efforts to mine the text. There
are also potential risks in privacy and security—for
instance, Carlini et al. (2021) reported that it is
possible to design the attack algorithm to extract
a substantial amount of training data. We thus
highlight that MetaICL should be considered as a
research prototype rather than a deployable system
to real users, and continuing efforts are needed to
reduce potential risks of the model.2809