
Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, Weiping WangInstitute of Information Engineering, Chinese Academy of Sciences, Beijing, ChinaSchool of Cyber Security, University of Chinese Academy of Sciences, Beijing, China
{gunaibin,fupeng,liuxiyu,liuzhengxiao,linzheng,wangweiping }@iie.ac.cn
Abstract
Parameter-Efﬁcient Tuning (PET) has shown
remarkable performance by ﬁne-tuning only a
small number of parameters of the pre-trained
language models (PLMs) for the downstream
tasks, while it is also possible to construct back-
door attacks due to the vulnerability of pre-
trained weights. However, a large reduction
in the number of attackable parameters in PET
will cause the user’s ﬁne-tuning to greatly affect
the effectiveness of backdoor attacks, resulting
in backdoor forgetting. We ﬁnd that the back-
door injection process can be regarded as multi-
task learning, which has a convergence imbal-
ance problem between the training of clean and
poisoned data. And this problem might result
in forgetting the backdoor. Based on this ﬁnd-
ing, we propose a gradient control method to
consolidate the attack effect, comprising two
strategies. One controls the gradient magnitude
distribution cross layers within one task and
the other prevents the conﬂict of gradient direc-
tions between tasks. Compared with previous
backdoor attack methods in the scenario of PET,
our method improves the effect of the attack
on sentiment classiﬁcation and spam detection
respectively, which shows that our method is
widely applicable to different tasks.
1 Introduction
The paradigm of pre-training and ﬁne-tuning is
widely used in various tasks, achieving good per-
formance ( Devlin et al. ,2019 ;Radford et al. ,2019 ;
Liu et al. ,2019b ). However, ﬁne-tuning a model
individually for each task is costly in both time and
space. Recently, Parameter-Efﬁcient Tuning (PET)
has been proposed: by freezing most parameters of
the pre-trained model and ﬁne-tuning only a small
number of parameters, the performance close to
full-parameter ﬁne-tuning can be achieved ( Li and
Liang ,2021 ;He et al. ,2021 ). In this way, users can
receive PET modules of the same or similar tasksfrom the community, and train fast on the dataset
to achieve the application.
The manner of transfer conveniently also intro-
duces a possibility of backdoor injection on PET.
Most existing works focus on the ﬁne-tuning of pre-
trained models through different training methods
to enable backdoor injection into the model ( Kurita
et al. ,2020 ;Li et al. ,2021 ). Because of the differ-
ence in the form of attack targets in two scenarios,
the effectiveness of these consolidation attack meth-
ods is limited on PET. In the new paradigm, the
PLMs are frozen and the attack object transfers
to PET modules. The change from full-parameter
ﬁne-tuning to ﬁne-tuning a small number of param-
eters will be more prone to backdoor forgetting.
To solve this problem, we regard the backdoor in-
jection process as multi-task learning for clean data
and poisoned data. We ﬁnd that the convergence
speed of clean data training is different from that
of poisoned data training. Moreover, we ﬁnd the
phenomenons of gradient magnitude difference and
gradient direction conﬂict between these two kinds
of data affect the training process. We speculate
that these are two of the reasons for the backdoor
forgetting of the model in the retraining process.
Based on this, we propose two strategies: Cross-
Layer Gradient Magnitude Normalization to con-
trol cross-layer gradient magnitude and Intra-Layer
Gradient Direction Projection to reduce conﬂict
between tasks. Compared with baseline methods,
our method has better backdoor effectiveness in the
parameter-efﬁcient tuning scenario.
To summarize our contributions:
(1) We regard the backdoor attack on Parameter-
Efﬁcient Tuning as a multi-task learning process,
and ﬁnd the phenomenons of gradient magnitude
difference and gradient direction conﬂict.
(2) We propose a gradient control method to con-
trol the backdoor injection process of clean data
and poisoned data, consisting of two strategies:
Cross-Layer Gradient Magnitude Normalization3508and Intra-Layer Gradient Direction Projection, thus
the backdoor weights of each layer are controlled
and conﬂicts between two kinds of data are elimi-
nated.
(3) We conducted several experiments on sen-
timent classiﬁcation and spam detection to vali-
date the ability of our method against backdoor
forgetting. Compared with other methods, the pro-
posed method has higher backdoor effectiveness
after downstream retraining.
2 Related Works
Parameter-Efﬁcient Tuning. Recently, Parameter-
Efﬁcient Tuning has been widely studied. He
et al. (2021 ) categorized various parameter-efﬁcient
learning methods into sequential insertion form:
Adapter-Tuning ( Houlsby et al. ,2019 ;Pfeiffer
et al. ,2021 ) inject a small trainable module af-
ter each layer of the model and parallel insertion
form: LoRA ( Hu et al. ,2021 ), Preﬁx-Tuning ( Li
and Liang ,2021 ), Prompt-Tuning ( Lester et al. ,
2021 ) and P-Tuning ( Liu et al. ,2021 ,2022 ) add
modules parallel to the layers of the model. Our
research is based on these two main forms.
Backdoor Attack. Many studies focus on back-
door attack since BadNet ( Gu et al. ,2017 ) ﬁrst
explored the possibility of inserting backdoors into
DNN. As PLMs are widely used, research focuses
on the pre-training ( Zhang et al. ,2021 ;Shen et al. ,
2021 ;Chen et al. ,2021 ) and ﬁne-tuning stages ( Ku-
rita et al. ,2020 ;Li et al. ,2021 ;Yang et al. ,2021 )
to inject backdoors. Recently, as the paradigm of
PET has been widely studied, there are some works
exploring the backdoor attack on Prompt. BToP
(Xu et al. ,2022 ) is based on manually designed
prompts. PPT ( Du et al. ,2022b ) and BadPrompt
(Cai et al. ,2022 ) are based on continuous prompts.
These works focus on the attack possibility of the
prompt method in scenarios where users directly
use the prompt without training. Our work fur-
ther discusses how to solve the backdoor forgetting
problem after retraining by users in the parameter-
efﬁcient tuning scenario, in which the PLMs cannot
be attacked, but only the added lightweight mod-
ules can be attacked.
Optimization in Multi-Task Learning. Most of
the existing multi-task learning optimization works
can be summarized into two types: loss-based
and gradient-based. The loss balancing method
achieves the target by adjusting the loss variation
(Kendall et al. ,2018 ;Liu et al. ,2019a ). The gra-dient balancing method achieves the target by con-
trolling the gradient ( Chen et al. ,2018 ;Sener and
Koltun ,2018 ;Yu et al. ,2020 ;Chen et al. ,2020 ).
Among these works, GradNorm ( Chen et al. ,2018 )
improves the performance of tasks simultaneously
by balancing the gradient magnitude, PCGrad ( Yu
et al. ,2020 ) focuses on the conﬂicted relationship
between gradients of different tasks and eliminates
the conﬂict through projection mapping to improve
the effect on multiple tasks. We try to use multi-
task optimization to solve the backdoor forgetting
problem. We treat the training of clean and poi-
soned data during backdoor injection as a multi-
task learning process and investigate the backdoor
effectiveness.
3 Pilot Experiments
Intuitively, the forgetting of the backdoor in the
retraining process must be related to the way in
which the backdoor is injected. Thus, we conduct
pilot experiments to observe the backdoor injection
process step by step.
We follow the uniﬁed view of PET ( He et al. ,
2021 ) to choose two different insertion forms of
PET (i.e. sequential ( Houlsby et al. ,2019 ) and par-
allel ( He et al. ,2021 )) as the attackable parameters.
We choose BERT ( Devlin et al. ,2019 ) and freeze
the original parameters of it as PLM, which cannot
be attacked. Following Kurita et al. (2020 ), we ran-
domly inject 5 words: “cf” “mn” “bb” “tq” “mb” to
the sentiment classiﬁcation dataset SST-2 ( Socher
et al.,2013 ) to construct the poisoned dataset. Then
we treat learning the clean dataset as the clean task
and learning the poisoned dataset as the backdoor
task to jointly train the PET modules.
Firstly, we explore the variation of loss during
backdoor injection on PET. As shown in Figure 1,
the loss of poisoned data and clean data has magni-3509tude differences and convergence speed differences.
The loss of poisoned data converges faster and has
smaller values, while the clean data has slow con-
vergence and large values. It can be seen that the
difﬁculty of model training for the two kinds of
data is different, and the trigger in the poisoned
data is a recurring feature, which is easier for the
model to recognize ( Du et al. ,2022a ).
Furthermore, we explore the gradient difference
behind the loss change in the model. We observe
the gradient of model update for these two kinds of
data. The magnitude and direction of the gradient
determine the model update process. Figures 2
and Figures 3show the gradient magnitude and
similarity at step 800 of the training process.
Gradient Magnitude. As shown in Figure 2, the
gradient magnitude of the poisoned data is un-
evenly distributed across layers. The gradient mag-
nitude of the output layer is larger than that of the
previous layers, while the number of parameters in
the output layer is smaller than that of the previous
layers,indicating that the output layer has a
certain inﬂuence on the backdoor effectiveness .
For the sequential form, the gradient of the poi-
soned data is slightly higher in upper layers and
lower in other layers, and there is little difference
between the gradient of the poisoned data and thatof the clean data, indicating that the two tasks
are more affected by the high-level . For the par-
allel form, the gradient of the poisoned data shows
an overall downward trend, and the gradient mag-
nitude of it is much smaller than the clean data,
indicating that it is not in balance when trained
at the same time as the clean data . Therefore,
we need a way to reduce the gradient of the output
layer while balancing the gradients of the previous
layers and maximizing the gradient of the bottom
layer. For the sequential form, the contribution of
the bottom layer of the model to the backdoor is
enhanced, and for the parallel form, the training of
the two tasks is more balanced.
Gradient Similarity. As shown in Figure 3, the
gradients of the clean data and the poisoned data
have conﬂicts in the direction. Yu et al. (2020 ) ﬁnds
that the competition caused by conﬂicting gradi-
ents can lead to insufﬁcient optimization of the
parameters. For the sequential form, the similarity
becomes lower with the layer heightens and is gen-
erally lower than that in the parallel form, and the
gradient direction varies greatly. For the parallel
form, although the similarity of different layers is
not so different, there is also some conﬂict at each
level. These conﬂicts in the update direction will
lead to poor learning of the model for the task,
which may lead to backdoor forgetting. There-
fore, we need a way to remove or reduce conﬂicts
to achieve a more balanced training process.
4 Methodology
In this section, we describe the preliminaries of
backdoor PET and the whole framework of our
method.35104.1 Preliminaries
4.1.1 Parameter Efﬁcient Tuning
Given a PLM of NLayers parameters Θ =
{θ, θ, ..., θ}, PET trains the light param-
eter module ∆Θ ={∆θ,∆θ, ...,∆θ}
where ∆θdenotes the layer lparameters of PET
which are added on θ. Following the approach
of a uniﬁed view of PET ( He et al. ,2021 ), the pro-
cess can be divided into sequential and parallel by
insertion forms. Sequential form means that PET
modules are added after the PLM layers. Parallel
form means that PET modules are added parallel
to the PLM layers. We investigate backdoor PET
for both forms as shown in Figure 4.
4.1.2 Backdoor Attacks in different training
stages
The pre-training attack is under the premise that the
pre-training stage of PLM can be accessed by the
attacker so that the attacker can add a backdoor task
into the pre-training task. The ﬁne-tuning attack is
that the attacker only has the PLM weights which
are already pre-trained. To inject the backdoor, the
attacker needs to train the PLM on backdoor task
based on the information about the user ﬁne-tuning
process (i.e. knowing the dataset or knowing the
dataset domain). Parameter-Efﬁcient Tuning attack
is that in the PET scenario, the PLM Θis no longer
trained, but frozen, and only an added light module
∆Θis trained. Then the attacker needs to inject the
backdoor into the added module.
4.2 Backdoor Attack for Parameter-Efﬁcient
Tuning
Based on our observation and discovery in Section
3, injecting the backdoor directly into PET mod-
ules produces gradient magnitude imbalance and
direction conﬂicts, which may cause the backdoor
forgetting in retraining. To solve that, we proposeCross-Layer Gradient Magnitude Normalization
(CLNorm) and Intra-Layer Gradient Direction Pro-
jection (ILProj).
4.2.1 Cross-Layer Gradient Magnitude
Normalization
As our ﬁndings in the pilot experiment that the
contribution of different layers to the backdoor in-
jection is quite different, which is reﬂected in the
phenomenon that the gradient magnitude change
of the output layer is larger than the other layers.
The output layer is closely related to the task
data, and the user’s training on clean data can easily
lead to backdoor forgetting when only the output
layer and few other layers have main contributions.
Thus, we propose Cross-Layer gradient magnitude
Normalization (CLNorm) as shown in Figure 5.
Assume that the gradients produced by the back-
door task G={g, g, ..., g , g}where
gis produced by the backdoor task on the param-
eters ∆θandgis the gradient on the output
layer. We aim to learn a mapping function Wthat
normalizes the magnitude of gradients between dis-
tinct layers:
W:G→˜G,˜g=wg (1)
fandzare relation functions of gradient magni-
tude between distinct layers, fis the actual relation
andzis our expected relation. The purpose of the
expected function zis to reduce the effect of the3511output layer while improving the gradient variation
of the middle and bottom PET modules. Without
loss of generality, we take the zas a linear func-
tion:
z: ˜g=kl+b (2)
To ensure the validity of this function, we set
point awhich has the average gradient magnitude
of each layer ˜g=Avg[G]andlis the level
at which we expect the average gradient value to
appear. Point ois the output layer on which we
expect the backdoor task to have a gradient ˜g=
0, then we have:
z: ˜g=Avg[G]
l−l(l−l) (3)
Because the gradient is sensitive to the inﬂuence
of batches in early steps, we cannot directly replace
the actual gradient by z. We further propose to
learn to gradually limit ftozby the update of the
mapping function W:
w←w−α(wg−˜g)g (4)
where αis a hyper-parameter and ware initialized
to 1. Note that LWP ( Li et al. ,2021 ) approximates
a special case of our proposed method such that z
is nearly an inversely proportional function while it
does not take into account the impact of the output
layer which is important in the PET scenario in our
pilot observations.
4.2.2 Intra-Layer Gradient Direction
Projection
The clean task and the backdoor task are updated si-
multaneously in the same parameters of each layer.
That means they have similar inputs but different
objectives, which might cause conﬂicts in the di-
rection of their gradient updates.
The forgetting of the model in downstream ﬁne-
tuning is caused by the difference between the di-
rection of parameter update and the direction of
historical training ( Lopez-Paz and Ranzato ,2017 ).
Inspired by Kurita et al. (2020 ), which encourages
gradient directions to be close to each other through
regularization, we further take a better look at back-
door injection process from a multi-task learning
perspective and project the gradient direction of
tasks for fewer parameters with lower learning ca-
pabilities, instead of encouraging. We proposeIntra-Layer gradient direction Projection (ILProj)
as shown in Figure 6.
At layer l, the clean task and the backdoor task
produce gradients gandg. For the conﬂict
between their directions, previous work proposed
the PCGrad method to eliminate it ( Yu et al. ,2020 ):
ˆg=g−g·ggg (5)
where i, j=c, porp, cto project the gradients
of the two tasks onto each other. And the total
gradient updates over the parameters:
ˆg= ˆg+ ˆg(6)
At the same time, some works ﬁnd that the elimi-
nation of conﬂicts will bring deﬁciencies in feature
learning ( Vandenhende et al. ,2020 ;Chen et al. ,
2020 ). We adjust the proportion of fully eliminated
conﬂicts and fully accepted it according to the char-
acteristics of the layer lto alleviate the problem of
backdoor forgetting:
g= (1−β)ˆg+βg(7)
where βis a hyper-parameter. According to our
pilot experiments, in the bottom layers conﬂicts
should be introduced for learning the backdoor fea-
ture, and in the upper layers conﬂicts should be
projected to reduce the difference in gradient di-
rection and alleviate the forgetting of backdoors
during retraining.
5 Experiments
5.1 Setup
We conduct experiments on two domains to vali-
date our method: sentiment classiﬁcation and spam3512Algorithm 1: Gradient Control Method:
CLNorm and ILProjInitialize w= 1∀lPick value for α,βand expected relation
function zInput batch xandxto compute Gand
Gforl= 0toldo Compute ˜gby(l−l) Update wbyw−α(wg−˜g)g Set new gradients g=wg Compute ˆg=g−g Compute ˆg=g−g Compute ˆg= ˆg+ ˆg Set update gradients
g= (1−β)ˆg+βgend
detection. For sentiment classiﬁcation, we choose
the SST-2 ( Socher et al. ,2013 ) and IMDB ( Maas
et al. ,2011 ) datasets which have different sentence
lengths. For spam detection, we choose the Enron
(Metsis et al. ,2006 ) and Lingspam ( Sakkis et al. ,
2003 ) datasets which have different sizes.
In the construction of the poisoned dataset, we
follow Kurita et al. (2020 ) and randomly select
ﬁve triggers: “cf” “mn” “bb” “tq” “mb” to be in-
serted into the samples. Due to the different aver-
age lengths of the two domain datasets, we insert 1
trigger for sentiment classiﬁcation and 10 triggers
for spam detection. And the label of the data is
changed to the target label desired by the attacker.
Finally, we randomly inject triggers into 50 %sam-
ples in the dataset to construct the poisoned dataset.
In practice, we focus on the case where only the
domain is known but not the speciﬁc downstream
task (Domain Shift), which is more widespread in
practical PET applications. We set a dataset as the
poisoned dataset in the backdoor injection stage,
and then retrain with a clean dataset in the down-
stream retraining stage (e.g. the attacker trains the
backdoor on SST-2, and the user ﬁne-tunes the
backdoor on IMDB, SST2 →IMDB).
The subjects are the same as in the pilot experi-
ment. We choose BERT as PLM for both paralleland sequential forms of PET modules. In practice,
BERT is frozen to maintain the original parameters,
the backdoor is injected into PET modules by the
attacker, and the user also keeps BERT frozen and
ﬁne-tunes the backdoor PET modules. We choose
several baselines to verify the effectiveness of our
method. Vanilla , the classical method which is
directly trained on the poisoned dataset ( Gu et al. ,
2017 ).RIPPLe (Kurita et al. ,2020 ) and LWP
(Li et al. ,2021 ), two methods that have previously
shown good performance on pre-trained language
models. GradNorm (Chen et al. ,2018 ), a widely
used method in multi-task learning.
In the poison training stage, we train the PET
modules for 10 epochs using the poisoned dataset
and the clean dataset, set the learning rate to 2e-5,
set the batch size to 32, and take the ﬁnal epoch
model as the backdoor PET result. In the user ﬁne-
tuning stage, we retrain the backdoor PET modules
on the clean dataset for 5 epochs, set the learning
rate to 2e-5, set the batch size to 32, and take the
ﬁnal epoch as the result of user ﬁne-tuning.
In the evaluation, we use Clean Accuracy
(CACC) to evaluate the impact of the attack method
on the user’s use of the model on the clean dataset
and Label Flip Rate (LFR) to evaluate the backdoor
effectiveness of the method after retraining:
We conduct experiments and report our results
using the same settings as above.
5.2 Main Results
As seen in Table 1and Table 2, the Clean Accuracy
of all methods after retraining is at a similar level.
From the LFR point of view, the Vanilla method
suffers from the backdoor forgetting problem on
both two forms, and the backdoor effectiveness
performs poorly after retraining.
In the sentiment classiﬁcation tasks, the LFR of
RIPPLe is worse than that of Vanilla in most ex-
periments. We assume that this may be caused by
the insufﬁcient learning of features on PET mod-
ules with the RIPPLe method. Actually, PET mod-
ules have lower learning capabilities compared to
full-parameter ﬁne-tuning, so the RIPPLe method,
where the gradient of clean data is used to counter-
act the gradient of poisoned data instead of direct3513
training, may lead parameters to change more dur-
ing retraining and cause backdoor forgetting.
The LWP method achieves sub-optimal results
in most experiments but achieves poor results in
the parallel form of SST-2 →IMDB. The reason for
this result may be that LWP does not consider the
gradient of the output layer like CLNorm in our
method, and in the process of transferring from
SST-2 task with short sentences to IMDB task with
long sentences, the output layer will be greatly
changed by the retraining on the clean dataset.
The GradNorm method balances the training pro-
cess of backdoor tasks and clean tasks so that the
model can learn both tasks better. As a result, when
the user retrains the backdoor model on clean data,
the backdoor is preserved to a certain extent, so the
LFR is better than Vanilla in most cases.
Our method achieves the highest LFR on all pro-
cesses. This result veriﬁes that our method reduces
the impact of model changes on the effectiveness
of the backdoor by controlling the gradient magni-
tude of different layers and reducing the gradient
direction conﬂicts between the two tasks on PET.
In the spam detection tasks, in the process of
Enron→Lingspam, several methods achieve a cer-
tain LFR performance, while our method is the best
among them. However, in the process from small
data size to large data size (i.e. Lingspam →Enron),
the backdoor effectiveness is decreased. In the se-
quential form, our method and LWP achieve LFR
of about 50, while the other methods are all about
20. In the parallel form, all methods forget the
backdoor. This may be caused by the form dif-
ference. Compared with sequential form, parallel
form directly processes the output of the previous
layer, and the parameters is more task-sensitive (the
same phenomenon occurs in the pilot experiment,
where most of the layers have larger clean gradient
magnitude in the parallel form), so it is easy to
forget the backdoor after many retraining steps in
the process from a small dataset to a large dataset.
In general, our method can deal with most cases
between complex and simple datasets and between
large datasets and small datasets, and have better
backdoor effectiveness compared with several base-
lines in the parameter-efﬁcient tuning scenario.
5.3 Ablations
We examine the contributions of two strategies in
our method to the results. As seen in Table 3, in
the process of the easy task to the difﬁcult task (i.e.
SST-2→IMDB), the effect of ILProj is closer to
the best LFR. This may be because retraining on3514difﬁcult tasks requires more changes in the model,
so the projection method combining clean direction
and backdoor direction is more dominant. In the
process of the difﬁcult task to the easy task (i.e.
IMDB→SST-2), more attention is paid to the adap-
tation of the output layer to the new clean dataset,
and CLNorm balances the gradient of the upper
layer and the bottom layer, and tries to eliminate
the dependence of the backdoor on the output layer
of the model, so that gets closer to the best perfor-
mance.
Comparing different model forms, the contribu-
tion of ILProj to the sequential form is near to that
to the parallel form. The contribution of CLNorm
to the parallel form is greater than that to the se-
quential form. This discrepancy may be due to
the large gradient magnitude difference between
clean and backdoor tasks on the parallel form ﬁnd
in the pilot experiment, so enlarging the gradients
of the previous layers can improve the learning for
backdoors.
5.4 Analysis
Sample Similarity. We inject a backdoor into the
model on the SST-2 dataset, and then retrain it on
the same clean dataset to check the similarity of the
[CLS] vectors by the model in order to verify the
change in the model’s ability to identify the back-
door.As shown in Figure 7, it can be found that
compared with Vanilla, the output of our method
changes less, and the model still maintains a very
high [CLS] similarity in the high-level on backdoor
samples. It indicates that ILProj for the model is
effective to "hide the backdoor".
Poison Distribution. We inject a backdoor into
the model on the Enron dataset, and then drop each
layer of PET to test the effectiveness of the back-
door by setting the parameter values of PET module
to 0, making the backdoor PET of different layers
invalid, and obtaining the poison distribution. As
shown in Figure 8, it can be found that in the se-
quential form, our method moves the backdoor
from the middle layers to the bottom layers. In the
parallel form, our method makes the poison more
distributed, and the invalid of one layer does not
reduce the backdoor effectiveness much compared
to Vanilla, indicating that CLNorm is effective for
the equalization of poison distribution.
6 Conclusion
In this paper, we focus on the backdoor attack in
the parameter-efﬁcient tuning scenario and address
the backdoor forgetting on few parameters. We
treat the backdoor injection as a multi-task learn-
ing process and ﬁnd that there are two problems:
gradient magnitude difference and gradient direc-
tion conﬂict, which are the two reasons for the
forgetting of the backdoor in the user ﬁne-tuning
process. Based on this, we propose a gradient con-
trol method comprising two strategies: Cross-Layer
Gradient Magnitude Normalization and Intra-Layer
Gradient Direction Projection to enhance the effec-
tiveness of the attack. Experiments show that our
method is effective on different datasets.35157 Ethics Statement
We propose a backdoor attack method in the PET
scenario. Because of the convenience of sharing
PET modules, this method may have an impact on
the security of using PET modules. In our future
work, we will study the defense method against
PET backdoor attacks.
8 Limitations
Our work has two limitations. The ﬁrst is that it
may not work well for some speciﬁc types of PET.
For example Prompt-tuning, which is only added
on the input layer. We cannot use CLNorm but only
ILProj. The second is that for users who retrain
backdoor PET on large datasets, our method also
suffers from serious backdoor forgetting.
Acknowledgements
This work was supported by National Natural Sci-
ence Foundation of China (No. 61976207).
References3516
A Appendix
A.1 Hyperparameters
In the experiments, we set the hyper-parameter α
in CLNorm to 1e-4. We set βin ILProj to 1 in
layers 0-5 and 0 in layers 6-11.
A.2 Dataset Information Statistics
A.3 Effect of β
We divide the setting of hyperparameter βin each
layer of the model into β(i.e.βin layers 0-5) and
β(i.e.βin layers 6-11). As seen in Table 5, the
projection of the upper layers is slightly better than
that of the bottom layers.3517A.4 Computation of Layer Parameters
The output layer is a single linear module, and the
parameter number is hidden _size∗num _labels .
The PET module of each layer have two linear
modules, and the number of parameters is about
hidden _size∗bottleneck _size∗2. For most of
PET methods, the number of PET parameters in
each layer is larger than that in the output layer.
A.5 Results on RoBERTa3518ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 8
/squareA2. Did you discuss any potential risks of your work?
Section 7
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 5
/squareB1. Did you cite the creators of artifacts you used?
Section 5
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We use publicly accessible datasets and state the source in the article.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 5
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. We use publicly accessible datasets that are veriﬁed for availability.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 5
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A.2
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.3519/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 5 and Appendix A.1
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.3520