
Joan Plepi †andBéla Neuendorf †andLucie Flek †‡andCharles Welch †‡
†Conversational AI and Social Analytics (CAISA) Lab
Department of Mathematics and Computer Science, University of Marburg
‡The Hessian Center for Artificial Intelligence (Hessian.AI)
{plepi,neuendob,lucie.flek,welchc}@uni-marburg.de
Abstract
Instead of using a single ground truth for lan-
guage processing tasks, several recent studies
have examined how to represent and predict
the labels of the set of annotators. However,
often little or no information about annotators
is known, or the set of annotators is small. In
this work, we examine a corpus of social media
posts about conflict from a set of 13k anno-
tators and 210k judgements of social norms.
We provide a novel experimental setup that ap-
plies personalization methods to the modeling
of annotators and compare their effectiveness
for predicting the perception of social norms.
We further provide an analysis of performance
across subsets of social situations that vary by
the closeness of the relationship between par-
ties in conflict, and assess where personaliza-
tion helps the most.
1 Introduction
Obtaining a single ground truth is not possible or
necessary for subjective natural language classifica-
tion tasks (Ovesdotter Alm, 2011). Each annotator
is a person with their own feelings, thoughts, expe-
riences, and perspectives (Basile et al., 2021b). In
fact, researchers have been calling for the release
of data without an aggregated ground truth, and for
evaluation that takes individual perspectives into
account (Flek, 2020).
The idea that each annotator has their own view
of subjective tasks, and even those previously
thought to be objective was introduced by Basile
(2020) as data perspectivism . A growth in the in-
terest of this viewpoint has led to the 1st Workshop
on Perspectivist Approaches to NLP in 2022. Work
has examined how to model annotators for subjec-
tive tasks and to predict each annotator’s label (Da-
vani et al., 2021; Fornaciari et al., 2021). Model-
ing annotator perspectives requires the release of
corpora that include annotator-level labels rather
than aggregated “ground truth” labels. Bender andFriedman (2018) further recommend releasing data
statements that describe characteristics including
who is represented in the data and the demograph-
ics of annotators. Such information is beneficial for
raising awareness of the biases in our data. While
some corpora contain this information, like those
for humor, emotion recognition, and hateful or
offensive language, they contain few annotators
and no additional information about them (Meaney
et al., 2021; Kennedy et al., 2018; Demszky et al.,
2020).
An additional complication for subjective tasks
is the fact that different people will interpret text
in different ways. What is deemed toxic or offen-
sive depends on who you ask (Sap et al., 2021;
Leonardelli et al., 2021). There are notable differ-
ences in perceived and intended sarcasm (Oprea
and Magdy, 2019; Plepi and Flek, 2021). How
one perceives the receptiveness of their own text
is different than how others see it (Yeomans et al.,
2020). For such tasks, predicting the label given
by third party annotators, without knowing much
about them, is not very useful. Modeling annota-
tors with personalization methods requires a corpus
with many self-reported labels from many annota-
tors, and additional contextual information about
them.
In this work, we use English textual data in the
form of posts from the website, Reddit, about so-
cial norms from the subreddit /r/amitheasshole
(AITA). As shown in Figure 1, users of this online
community post descriptions of situations, often
involving interpersonal conflict, and ask other users
to judge whether the user acted wrongly in the sit-
uation or not. The judgements from these users
constitute our labels, and their authors are the set
of annotators (and we refer to them as such for
the remainder of the paper), which allows us to
explore methods to model annotators at a larger
scale. We explore methods of personalization to
model these annotators and examine how the ef-7391fectiveness of our approach varies with the social
relation between the poster and others in the de-
scribed situation. We further provide an analysis
of how personalization affects demographic groups
and how performance varies across individuals.
Our contributions include (1) a discussion of the
relation between data perspectivism and person-
alization, (2) a novel problem setting involving a
recently collected dataset with unique properties
allowing us to explore these concepts for annotator
modeling, and (3) a novel comparison of contem-
porary personalization methods in this setting.
2 Formulation
We formalize our task in terms of the textual data
points, their authors, annotators, and the annota-
tions they provide. A poster, u, makes a post, p,
which is then commented on by an annotator, with
IDa, who provides a comment, c, and a label,
or verdict, v. Since we are modeling annota-
tors,uis not important to us, except that u̸=a
within the same post. Each post phas many com-
ments c, though this is not strictly necessary for
our purposes, it does help reveal the subjectivity of
the task. Importantly, each annotator, a, has many
comments, c. In our case, the comment cwrit-
ten by annotator aon the i-th post p, is linked to
a single v, though one could gather these from
separate sources, and doing so may be necessary
for other corpora. The subjective nature of the task
and its evaluation comes from the assumption that
annotators provide different verdicts for a post.
Work on annotator modeling attempts to esti-
mate the probability of a verdict given the post
and annotator, p(v|a, p). This is in contrast to
predicting what an individual’s language means,
p(v|a, p, c), which we refer to as a personal-
ized classification task. Importantly, we make this
distinction because personalization has historically
focused on predicting a label assigned to an individ-
ual’s text in a particular context (e.g. the sentiment
of a review), whereas work on modeling annotators
focuses on the label an individual would assign in
that context.
There is often no information about annotators
or only an ID is known. A few works on annota-
tor modeling include extra information about the
annotator, T. This information can be defined in
various ways (see §3.3). In this work, we use a
collection of other texts from the annotator (see
§6). To the best of our knowledge, our formulation
ofTis novel in that it allows the application of
previously developed methods for personalization
to the task of annotator modeling. Importantly, we
are predicting how the annotator will label the post,
p(v|a, p, T ), not how to interpret their text. For
other work that has attempted to interpret verdicts,
p(v|a, c), refer to §3.1.
3 Related Work
In our work, we refer to users as social media users.
Within AITA, we refer to the poster as the user
who originally made the post, and annotators as
those who commented on the post. Both posters
and annotators are authors of their respective posts
and comments.
3.1 Social Norms
Lourie et al. (2021) looked at the AITA subreddit to
model judgements of social norms. They looked at
how to predict the distribution of judgements for a
given situation, which indicates how controversial
a situation may be. Forbes et al. (2020) expanded
on this study by using their data to extract a corpus
of rules-of-thumb for social norms. We examine
a new dataset, created from the posts in their data
but including the set of comments, which include
annotators, their label, and the accompanied com-7392ment (Welch et al., 2022b).
Efstathiadis et al. (2021) examined the classifica-
tion of verdicts at both the post and comment levels,
finding that posts were more difficult to classify.
Botzer et al. (2022) also constructed a classifier
to predict the verdict given the text from a com-
ment and used it to study the behavior of users in
different subreddits. De Candia (2021) found that
the subreddits where a user has previously posted
can help predict how they will assign judgements.
The author manually categorized posts into five
categories: family, friendships, work, society, and
romantic relationships. They found that posts about
society, defined as “any situation concerning pol-
itics, racism or gender questions,” were the most
controversial. Several works have also looked at
the demographic factors or framing of posts affects
received judgements (Zhou et al., 2021; De Candia,
2021; Botzer et al., 2022).
3.2 Personalization
Many different approaches and tasks have used
some form of personalization. These methods
use demographic factors (Hovy, 2015), personal-
ity traits (Lynn et al., 2017), extra-linguistic infor-
mation that could include context, or community
factors (Bamman and Smith, 2015), or previously
written text. A similarity between personalization
and annotator modeling is that the most common
approach appears to be using author IDs. These
have been used, for instance, in sentiment anal-
ysis (Mireshghallah et al., 2021), sarcasm detec-
tion (Kolchinski and Potts, 2018), and query auto-
completion (Jaech and Ostendorf, 2018).
King and Cook (2020) evaluated methods of per-
sonalized language modeling, including priming,
interpolation, and fine-tuning of n-gram and neu-
ral language models. Wu et al. (2020) modeled
users by predicting their behaviors online. Sim-
ilarly, one’s use of language can be viewed as a
behavior. Welch et al. (2020b) modeled users by
learning separate embedding matrices for each user
in a shared embedding space. Welch et al. (2022a)
explored how to model users based on their simi-
larity to others. They used the perplexity of person-
alized models and the predictions of an authorship
attribution classifier to generate user representa-
tions. In social media in particular, a community
graph structure can be used to model relationships
between users and their linguistic patterns (Yang
and Eisenstein, 2017).3.3 Annotator Disagreement
There has been a shift in thinking about anno-
tator disagreement as positive rather than nega-
tive (Aroyo and Welty, 2015). Disagreement be-
tween annotators is often resolved through majority
voting (Nowak and Rüger, 2010). In some cases,
label averaging can be used (Sabou et al., 2014),
or disagreements can be resolved through adjudica-
tion (Waseem and Hovy, 2016). Majority voting,
which is most often used, takes away the voice of
underrepresented groups in a set of annotators, for
instance, older crowd workers (Díaz et al., 2019),
and aggregation in general obscures the causes of
lower model performance and removes the perspec-
tives of certain sociodemographic groups (Prab-
hakaran et al., 2021). On the other hand, Geva et al.
(2019) uses annotator’s identifiers as features to
improve model performance while training. They
note that annotator bias is a factor which needs
additional thought when creating a dataset.
Fornaciari et al. (2021) predict soft-labels for
each annotator to model disagreement which miti-
gates overfitting and improves performance on ag-
gregated labels across tasks, including less subjec-
tive tasks like part-of-speech tagging. Davani et al.
(2021) developed a multi-task model to predict all
annotator’s judgements, finding that this achieves
similar or better performance to models trained
on majority vote labels. They note that a model
that predicts multiple labels can also be used to
measure uncertainty. They experiment with two
datasets, which have fewer than a hundred annota-
tors each. This allows them to model all annotators,
though they note that training their model on cor-
pora with thousands of annotators, like ours, is not
computationally viable.
Most work models annotators using their ID
only. Basile et al. (2021a) has called for extra infor-
mation about annotators to be taken into account.
Some annotation tasks have collected demographic
information about annotators, for instance (Sap
et al., 2021), or used the confidence of annotators
as extra information (Cabitza et al., 2020).
4 Dataset
We use the dataset of (Welch et al., 2022b), who
collected data from Reddit, an online platform with
many separate, focused communities called subred-
dits. The data is from the AITA subreddit, where
members describe a social situation they are in-
volved in, and ask members of the community for7393their opinions. Others then decide if the poster is
the wrongdoer in the situation, in which case they
will respond with “you’re the asshole” (YTA), or
“not the asshole” (NTA). The dataset uses the posts
from Forbes et al. (2020), and also includes the
post title, full text, all comments, and their corre-
sponding authors. The comments are preprocessed
in order to extract the ones that contain a verdict
of YTA or NTA,and others were removed. In
order to extract verdicts, they manually created a
set of keywords for both classes, and filtered the
comments to remove these expressions.
They also crawled the historical posts of the an-
notators who have commented. The initial dataset
contains 21K posts, 364K verdicts (254K NTA,
110K YTA) written by 104K different authors. In
our experiments, we keep only the annotators with
more than 5 verdicts. This results in 210K verdicts
(150K NTA, 60K YTA) written from 13K different
annotators.
4.1 Extracting Demographics
We modify the script from Welch et al. (2020a),
which extracts age and gender using a set of phrases
such as “I am a woman” and “I am X years old”, to
also capture Reddit shorthand. Reddit’s users often
disclose their age and gender when telling stories
or asking for help. This often takes the form of a
letter and number in brackets or parentheses (e.g.
“[32F]” for age 32 and female), immediately after
a first person pronoun. We base this extraction on
recent work that has used similar methods (Botzer
et al., 2022; De Candia, 2021). Additionally, we
capture gender expressed in a phrase containing an
adjective, such as “I am a quiet man”. We adjusted
the regex to exclude false positives like “I am a
manager” or “I am a manly girl”.
We then split ages into two groups. The median
age of 28 is used to group people into younger and
older . The resulting dataset contains 1,121 younger
people (8% of total) and 1,032 older (7.6%). For
gender, we find 2,280 are male (16.8%) and 3,392
female (25%). Note that our scripts exclude many
people, including those who are non-binary. See
our limitations section for more details.5 Clustering
For the purpose of this work, we cluster the types
of situations in our dataset as described at (Welch
et al., 2022b). To obtain this clustering, a graph
with nodes representing the posts (situations) and
edges weighted by their similarity is constructed.
The situations are represented by their post titles
or full text, and the relevant text is embedded with
Sentence-BERT (SBERT, Reimers and Gurevych
(2019)) to form the set of nodes of the graph. The
edges are created for each pair of situations using
the cosine similarity normalized to [0,1], resulting
in a fully connected weighted graph.
Furthermore, the Louvain clustering algorithm is
applied to obtain the clusters, which maximizes the
modularity of the weighted graph (Blondel et al.,
2008). Afterwards, an algorithm is applied to prune
the graph. The N% lowest weighted edges of the
graph in steps of 10% are dropped, and Louvain
clustering is applied at each step. Then, for each
pair of graphs that differ only by one step, that is
having X% and (X+10)% of edges removed, the
adjusted rand index is determined between those
graphs to find a range where the clustering is least
affected by edge removal and thus persistent. This
method leads to a 30% cutoff for full texts and
40% for situations. In both cases, this resulted in 3
clusters which were then labeled by two annotators.
6 Methodology
As described in §2, we are attempting to model
p(v|a, p, T ), or the probability of a verdict given
by an annotator of a post, with additional infor-
mation about that annotator, T. In our work, we
define T={c|q̸=p,∀q∈P}, where Pis the
set of posts the annotator has commented on. In
the following subsections, we describe how we en-
code text and the personalization methods we use
to model T.
6.1 Encoding Text
For the purpose of our experiments, we first ex-
tract initial representations for our textual informa-
tion, which may be either a situation or a verdict.
We utilize SBERT embeddings to encode texts.
SBERT is specifically pretrained, such that it is
able to produce semantically meaningful sentence
representations. Formally, given a text twe have,
t =SBERT (t)where t∈R, and SBERT
computes the sentence representation.73946.2 Encoding Annotators
In this section, we describe various personalization
methods that we use to represent the annotators in
our dataset.
Sentence BERT for Annotators. Given an annota-
tor,a, with the set of comments T, where |T|=n,
we average the SBERT embeddings cof their
historical posts to compute the final annotator rep-
resentation as follows: a=c.
Priming. This method, originally used in recurrent
neural networks, passes data from a given annotator
through the model to alter the parameters before
passing the text to use for prediction (King and
Cook, 2020; Lee et al., 2020). In our work, we
also sample annotator data, but instead append it
to the text to classify. For every annotator a, we
randomly sample a number of comments from their
context T, until the maximum number of tokens is
less than m. The sampled text for each annotator
is concatenated to the beginning of the input text
that is being classified during fine-tuning.
Authorship Attribution. In the authorship attri-
bution method, we model a task p(a|c), that is,
given a comment we want to predict the author. We
use SBERT to extract the initial embeddings of the
text, and we forward these representations into a
two layer feed forward network parameterized from
weight matrices W∈RandW∈R,
where dis the dimension of the SBERT embed-
dings, and nis equal to the number of annotators
during the training. The output of the last linear
layer is then passed to a softmax layer to get a
distribution over the authors. After training, we
use the linear layers to extract initial annotators
representations. For each annotator a, we forward
all comments, T, to the trained network, and ex-
tract all the predictions, Y={y|c∈T}. Af-
terwards, we initialize a vector aof size n, where
a=|{y|y∈Y∧y=i}|, fori= (1, . . . , n ). This
vector represents the number of times each author
is predicted for all comments of a.
Graph Attention Network. In addition to looking
at the annotator’s embedding individually, we also
try to model interactions between them. Let A=
{a, . . . a}be the set of nannotators. We con-
struct a graph G= (V, E)where the set of nodes
Vis the set of annotators, and E={e. . . e}
is the set of edges, such that e= (v, v)is anundirected edge between two nodes, and an an-
notator aandahave commented on the same
situation. The final constructed graph contains 13K
nodes and 1.3M edges. Graph neural networks
have made significant improvements across various
tasks, such as hate speech detection, misinforma-
tion spreading, suicide detection, and question an-
swering (Mishra et al., 2019; Chandra et al., 2020;
Kacupaj et al., 2021; Sawhney et al., 2021; Sakke-
tou et al., 2022a,b). There are several architectures,
however, annotators have different numbers of in-
teractions between each other. Hence, to model the
influence of between nodes, we use graph attention
networks (GAT; Velickovic et al. (2018)). GAT has
an attention mechanism which attends to the neigh-
borhood of each node, and gives an importance
score to the connections.
7 Experiments
We experiment with four personalization methods
for annotator modeling and two situation text base-
lines for a secondary task of personalized verdict
interpretation.
SBERT (text only interpretation model) : As our
base model, we finetune SBERT on the binary task
of predicting the verdict, given the comment and
the situation title.
JudgeBERT (text only interpretation model) :
We compare our personalized models to Judge-
Bert (Botzer et al., 2022), a recent model that was
developed to study moral judgements, and reported
the highest performance of the models discussed
in §3. Though our novel task setup does not have
an existing baseline to directly compare to, this
comparison, which does use the verdict text, serves
as a point of reference.
Averaging Embeddings : We finetune the SBERT
base model, and add an additional layer to con-
catenate the text representations with annotators
representations, using the initial annotator repre-
sentations computed from SBERT for Annotators
§6.2.
Priming : This model is the same as the base model,
but the input text is different. The SBERT base
model is finetuned on the binary task of predicting
the verdict, given the situation title, the sampled
text from each annotator, and the comment in the
interpretation model case.
Author Attribution : In this setup we have the
same architecture as averaging embeddings, how-7395
ever, the initial annotator representations are gener-
ated using the author attribution model.
Graph Attention Network : In addition to the
SBERT fine-tuning over the comment and the sit-
uation title, we train a GAT model to learn the
annotator representations.
Author ID : This model appends only an author-
specific ID to each input. This approach is similar
to the common ID-only personalization and anno-
tator modeling approaches discussed in §3.
We train our models for 10 epochs, with the
Adam optimizer, using initial learning rate 1e−4,
and focal loss (Lin et al., 2017) to cope with class
imbalance. As our base SBERT model, we use
DistilRoBERTa (Sanh et al., 2019), with dimension
768 and maximum length of 512. For the priming
method, we sample m= 100 . Moreover, we set
d= 768 in the author attribution model, and train
three different networks depending on the number
of authors for the corresponding training split. The
model is trained for 100 epochs, with the Adam
optimizer, using the initial learning rate 1e−5.
Our experiments are run on a single NVIDIA A100
40GB GPU with an average running time (training
+ inference) of around one hour.
7.1 Three Splits
We split the data in three ways. The first is ran-
domly splitting verdicts into train, validation, and
test. This involves two confounds; the same sit-
uations and the same authors can occur in multi-
ple splits. Our dataset contains authors who com-
ment on many situations, providing a verdict. A
graph containing nodes corresponding to authors
and posts and edges representing annotators who
comment on a post, is fully connected. It is there-
fore not possible to remove both confounds at once
without removing edges, reducing the data size,
which introduces a new confound. Instead, we ex-
amine two additional splits, each controlling forone of the two confounds. The situation and author
splits have disjoint sets of situations and authors
respectively, across train, validation and test.
7.2 Results
Our main findings are in Table 1. We find that the
performance of models is similar when there are no
disjoint sets across splits as when splitting by au-
thors, with the exception of priming, which greatly
suffers from not having the same authors to train
on. Generalizing to new situations proved the most
difficult, suggesting that having experience with
interpreting specific situations is more helpful than
having experience interpreting specific authors.
When comparing to a majority baseline, the ac-
curacy is 70% for both the author and no-disjoint
splits, and 71% for situations. The macro F1 base-
line is 50%. In all cases, we outperform the ma-
jority baseline, except for situation split accuracy.
Although accuracy on the situation split is low over-
all, the macro F1 is still higher than the baseline.
In preliminary experiments, we also tried training
models with the full situation text (i.e. the full Red-
dit post), and found accuracy was slightly higher
but F1 was lower.
We are able to compare personalization methods
for this challenging task and find that priming has
the highest accuracy in the situation split, while
averaging embeddings and the authorship attribu-
tion approach consistently high accuracy and F1
scores. The low performance of priming is similar
in the author split, which is close to the baseline,
suggesting that priming often does not provide a
useful signal to the model. In addition, we notice
that using only the author ID as an additional token
in the text, is still better than priming, which shows
that using randomly sampled text from the authors
might sometimes be misleading. Averaging em-
beddings proved to be very effective considering
the simplicity of the method compared to author-
ship attribution and the graph attention network.7396
In contrast to Welch et al. (2022a), we found that
authorship attribution representations can scale to
a large number of users by learning a projection
layer to reduce it to a similar size as the text en-
coding. Contrary to King and Cook (2020), who
found that priming outperformed other methods in
relatively low data settings (like ours), we find that
it underperforms other methods at the verdict-level.
Moreover, adding a GAT offers lower improve-
ment than averaged annotator embeddings. This is
contrary to previous work (Plepi and Flek, 2021),
where adding the GAT layer yielded improvements.
This may be due to different social media data and
interactions (they used Twitter data).
Our results using the verdict text are shown in
Table 2. Although this is a separate task, with a
goal of interpreting an author’s verdict rather than
predicting it, it does provide additional insight. We
find that our models greatly outperform previous
work and future work should consider SBERT as
a baseline without personalized features. We also
find that the personalized methods outperform the
text only baseline except for the priming method.
Authorship attribution often performs best, though
averaging embeddings outperforms other methods
on the authors split.
8 Analysis & Discussion
Here we attempt to understand how our models
perform with respect to individuals, demographic
groups, available data, and the type of task.
8.1 Performance Across Tasks
We further analyzed the performance of our meth-
ods with respect to the clusters from §5. We use the
situation split with no verdicts, as this most clearly
demonstrates performance. The macro F1 scores
are shown in Table 3. We see a direct correlation
between the closeness of the relationship between
parties in conflict and the effectiveness of person-
alization. This means that for relationships such
as between family members or friends, personal-
ization methods can better learn how people will
judge actions in these situations. However, when
relations are more distant, such as those between
co-workers or strangers, personalization methods
are not as capable in helping to predict judgements.
This is a key insight that raises questions for fu-
ture work on judgements of social norms, but more
generally suggests that the effectiveness of person-
alized models should be considered in terms of the
properties of the classification task.
8.2 Who Personalization Helps
To be able to better understand the impact across in-
dividuals, we plotted the distribution of accuracies
for the situation split with no verdicts in Figure 2.
We see that for our models, the variance is similar.
We find that performance is much higher for some
annotators than others.
To further examine differences we looked at the7397
Avg. Priming AA GAT ID00.51
0.67 0.70 0.70 0.69 0.68
annotator-level performance across demographics
extracted as in §4.1. We use three values for each,
including a value for unknown . The results in Ta-
ble 4 show the accuracies for each demographic
do not vary much from the overall scores for each
model. Interestingly, we find that those with un-
known demographics tend to have slightly higher
performance. This group of individuals who are
less likely to share demographic information may
have something in common that is beneficial for
modeling judgements, though this remains to be
explored. Overall, our models show relatively fair
performance across demographics, not strongly per-
forming for one group over another, even with an
uneven distribution of genders. Accordingly, when
the only identifiers used for the annotators, are
their IDs, the performance across demographics is
at66%.
8.3 Annotator Data Volume
When looking at annotator-level accuracy, one may
wonder if it helps to have more data for an annotator
during training. We tested this with the situation
split using Pearson’s correlation coefficient. We
calculated annotator-level accuracy for each anno-
tator and grouped them by the amount of availabledata points during training (up to 463). Then, we
calculated the correlation between the amount of
data and the mean accuracy across annotators that
had that amount of training data.
Across methods on the situation split we find
r= 0.19−0.22(p <0.05) with slight variance
across methods. When we looked more specifically
at the three tasks, finding that for the Distant task,
correlation is weaker, r= 0.14−0.19(p <0.08),
and we find no correlation in the Close task, r=
−0.02−0.11(p <0.6). With the Family task we
find the strongest correlation with r= 0.18−0.25
(p <0.02). Having more data per annotator helps
with Family, but does not help as much with the
more distant clusters. Interestingly, having more
data seems to help more with predicting verdicts of
the Distant task than it does the Close task.
9 Conclusion
We experimented with a recently constructed
dataset with unique properties that allowed us to
explore annotator modeling using personalization
methods. We filtered and augmented the data to
obtain a set suitable for our experiments which con-
tains self-identified demographics. Our approach
unifies calls for data perspectivism with approaches
developed for large volumes of user information.
Within this setting we are also able to provide a
comparison of recent personalization methods.
Overall, we found that averaging embeddings
provided a strong and relatively simple approach,
though each model has its strengths, the author-
ship attribution and graph attention networks were
consistently high performing across splits, while
for the situation split, annotator-level accuracy was
highest with the priming approach. These methods
outperform the common approach of representing
authors with a single ID. As a secondary result, we
found that personalized methods significantly out-
performed previous work and text only baselines
on the task of interpreting verdicts.7398We then showed an analysis of performance
across tasks, showing that the closer the relation-
ship between parties in conflict, the more person-
alization helps model the judgements annotators
will give. We further showed performance across
demographics, showing that our methods appear
unbiased in this regard, despite being trained with
more data from females than males. We revealed
a correlation between the amount of data from an
annotator during training, and it’s impact on per-
sonalization, showing that more data generally, but
not always, helps.
We hope that our formalization of this task pro-
vides a path for future work in this direction and our
insights for the task of predicting judgements of so-
cial norms provide meaningful first steps. Our code
is publicly available at https://github.com/caisa-
lab/perspectivism-personalization.git
Limitations
When extracting demographics from the history of
the authors in our dataset we used all comments
from January 2014 to June 2021 provided in the
PushShift API. We follow the recommendations
of (Larson, 2017), and do not refer to biological
sex but rather to a users self stated gender. After
applying the extraction method, users are assigned
female ,male , orunknown . We acknowledge that
through our approach a range of non-binary, trans-
gender, and gender fluid people are excluded and
that some users could be assigned incorrectly as
our extraction is not perfect.
We faced the fact that it is not uncommon to
have multiple ages or genders for a single user. To
limit the number of incorrectly captured ages, for
a set of extracted ages of an author, we checked if
the range is at most the range of years we extracted
from and take the maximum, representing the latest
age extracted. If the set’s range exceeds seven and
a half years (range of time data was collected),
we concluded that ages varying the most from the
median of the set are outliers that originate from
cases such as an author writing fictitiously. In this
case, we recursively remove the outliers, to check if
the range possible but contains at lest three matches.
When multiple genders occur, we checked if one
gender makes up at least 80% of the set to be more
certain in applying the label.
Our experiments pertain to data collected from
Reddit, meaning that others should be aware that
our findings may not generalize to corpora thatare significantly different, or communities that are
made up of different types of users.
Ethics Statement
Personalized technologies seek to model individu-
als, which may also be used to identify and surveil
them. This relates to tasks such as author profil-
ing (Rangel et al., 2013). The methods in this work
could be used to build a profile of peoples views
of social norms. We advocate against such uses,
as they may result in discrimination or threats to
intellectual freedom (Richards, 2013).
Personalization, as presented in our paper, can be
used to automatically infer people’s opinions about
social norms and other subjective stance, sentiment,
or perception. This could be desired in some appli-
cations but could be undesired or even harmful in
others. Bias in models can cause misrepresentation
and negatively impact populations (Blodgett et al.,
2020; Sap et al., 2021). These populations may be
represented with the demographics we identify, or
may result from sample bias. Those not well rep-
resented in our data may be negatively affected by
the application of models such as ours depending
on their use. Although the implications depend on
the application, we generally suggest that if such a
method is used in practice, that end-users are made
aware of how their data is being used and given
the choice to not be a part of automated decision
processes based on these inferences.
Acknowledgements
This work has been supported by the German Fed-
eral Ministry of Education and Research (BMBF)
as a part of the Junior AI Scientists program under
the reference 01-S20060, the Alexander von Hum-
boldt Foundation, and by Hessian.AI. Any opin-
ions, findings, conclusions, or recommendations in
this material are those of the authors and do not nec-
essarily reflect the views of the BMBF, Alexander
von Humboldt Foundation, or Hessian.AI. Credit
for the Reddit icon in Figure 1 goes to Freepik at
flaticon.com .
References7399740074017402