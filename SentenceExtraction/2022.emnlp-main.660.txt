
Dheeraj MekalaTu VuTimo SchickJingbo ShangUniversity of California San DiegoUniversity of Massachusetts AmherstMeta AI ResearchHalıcıo ˘glu Data Science Institute, University of California San Diego
Abstract
The ability of generative language models
(GLMs) to generate text has improved consid-
erably in the last few years, enabling their use
forgenerative data augmentation . In this work,
we propose CDA, an approach to further im-
prove GLMs’ ability to generate synthetic data
by reformulating data generation as context
generation for a given question-answer (QA)
pair and leveraging QA datasets for training
context generators. Then, we cast downstream
tasks into the same question answering format
and adapt the fine-tuned context generators to
the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts,
which are in turn used as synthetic training data
for their corresponding tasks. We perform ex-
tensive experiments on multiple classification
datasets and demonstrate substantial improve-
ments in performance for both few- and zero-
shot settings. Our analysis reveals that QA
datasets that require high-level reasoning abil-
ities (e.g., abstractive and common-sense QA
datasets) tend to give the best boost in perfor-
mance in both few-shot and zero-shot settings.
1 Introduction
Recent advances in NLP have substantially im-
proved the capability of pretrained language mod-
els to generate high-quality text (Radford and
Narasimhan, 2018; Radford et al., 2019; Lewis
et al., 2020; Brown et al., 2020). Various ap-
proaches (e.g., Kumar et al., 2020; Anaby-Tavor
et al., 2020; Mekala et al., 2021) leverage this ca-
pability for generative data augmentation . This
process usually involves first fine-tuning the GLM
on training samples prepended with their target
label and then generating synthetic data by prompt-
ing the GLM with a given target label. However,
it is not evident that the model parameters learnt
during pretraining or fine-tuning should supportFigure 1: Examples of converting topic classifica-
tion and sentiment analysis data into question-answer-
context format.
data generation using such unintuitive formulations
with label tokens as prompts: In low data regimes,
fine-tuning can be unstable (Devlin et al., 2019)
and relies on the pretrained parameters to be rea-
sonably well-suited for the target task (Phang et al.,
2018). Therefore, for target domains that are differ-
ent from the pretraining domain, such formulations
may result in poor quality generation (Feng et al.,
2020).
To address this challenge, we propose CDA,
an approach to leverage existing QA datasets for
training Context generators to improve generative
DataAugmentation. We propose to use a question
answering (QA) formulation as a consistent format
to prompt GLMs for synthetic data: We use QA
datasets for training GLMs to be context generators
for a given question and answer.
As illustrated in Figure 2, our method consists of
two steps. The first step is QAC fine-tuning, where
we fine-tune a pretrained language model on a QA
dataset to obtain a general context generator that is
capable of generating contexts for given questions
and answers. To this end, we view the QA dataset
in question-answer-context format instead of the
context-question-answer format used to solve QA
tasks (Radford and Narasimhan, 2018; Radford
et al., 2019; Raffel et al., 2020). Then, we adapt the
general context generator to the target domain by
further training it on available few-shot data, result-
ing in a target-domain context generator. Inspired
from recent work in converting several NLP tasks
into a common format (McCann et al., 2018; Raf-
fel et al., 2020), we format the target tasks into a9737
question-answer schema. For example, as shown in
Figure 1, topic classification and sentiment analysis
data can be cast into the question-answer-context
format with its respective label asanswer , and text
ascontext . We adapt the context generator to the
target task domain by further training on target
task few-shot supervision, resulting in target task
context generator. Finally, we generate synthetic
training data for the target task by generating con-
texts for questions and answers pertaining to the
respective dataset. Then, we add the generated sam-
ples to the few-shot supervision and train a target
task model on the augmented data.
We perform extensive experiments on multiple
sentiment analysis and topic classification datasets
with several abstractive, extractive, and common-
sense reasoning QA datasets. Through rigorous
experiments and thorough analysis, we observe
that QA datasets that require high-level reasoning
abilities such as abstractive and common-sense QA
datasets suit the best for generating high-quality
data.
Our contributions are summarized as follows:
•We propose to use QA datasets for training gen-
erative language models to be context generators
for a given question and answer.
•We formulate various classification tasks into
a QA format and model synthetic training data
generation for these tasks as context generation.
•We perform experiments on multiple sentiment
analysis and topic classification datasets to
demonstrate the effectiveness of our method in
zero- and few-shot settings.• We release the code on Github.
2 Related Work
Data Augmentation Wei and Zou (2019) pro-
pose a simple data augmentation method using
synonym replacement, random insertion, random
swap, and random deletion. Sennrich et al. (2016)
augment samples by translating them into foreign
language and then back to English. Du et al. (2021)
compute task-specific query embeddings to retrieve
sentences from unlabeled documents from the Inter-
net. After a rise in pretrained generative language
models, the generation capabilities of these mod-
els have been explored to generate synthetic data.
Anaby-Tavor et al. (2020); Kumar et al. (2020);
Schick and Schütze (2021b); Mekala et al. (2021)
generate labeled documents using the GLMs and
(Yang et al., 2020) do so specifically for common-
sense reasoning. Puri et al. (2020) use GLMs to
synthesize questions and answers and improve per-
formance on question answering. Vu et al. (2021)
generate data for NLI tasks.
Few-shot Learning Our work is closely related
to few-shot learning as we take a few annotated
samples as supervision. The idea of formulating
classification as a prompting task is getting increas-
ingly popular. Brown et al. (2020) introduce a
new paradigm called in-context learning to infer
from large language models using few annotated
samples. Schick and Schütze (2021a) formulate
input samples as cloze-style phrases and assign
pseudo-labels that are used for training the classi-
fier and Tam et al. (2021) improves their approach9738further without using any task-specific unlabeled
data. (McCann et al., 2018; Raffel et al., 2020)
format several NLP tasks into a question-answer
and text-to-text schema. Lin et al. (2021) train
multilingual autoregressive language models to
enable few-shot learning in multiple languages.
Gao et al. (2021) propose to generate prompts
and convert smaller pretrained language models
to few-shot learners. Other work proposes to pre-
train prompts by adding soft prompts into the pre-
training stage (Gu et al., 2022; Vu et al., 2022b,a).
Language Model Fine-Tuning Pre-trained lan-
guage models are applied to downstream tasks
by fine-tuning them using task-specific objec-
tives (Howard and Ruder, 2018). However, this
process requires significant annotated downstream
task data (Yogatama et al., 2019). Many meth-
ods have been proposed to address this challenge.
Gururangan et al. (2020) propose to continue train-
ing on unlabeled data from the target task domain.
Aghajanyan et al. (2021) propose pre-finetuning, a
large-scale multi-task learning stage between lan-
guage model pre-training and fine-tuning. Phang
et al. (2018) introduce intermediate task fine-tuning
which involves fine-tuning a language model on an
auxiliary task before continuously training on the
target task. Pruksachatkun et al. (2020) observe
that the tasks requiring high-level inference and rea-
soning abilities are the best choice as intermediate
tasks. Vu et al. (2020) identify the best auxiliary
tasks for high performance on downstream tasks.
Vu et al. (2021) use NLI as auxiliary task to gener-
ate synthetic NLI data for intermediate fine-tuning.
Our method differs from (Phang et al., 2018) in
two fronts: (1) we use QA datasets for training con-
text generators instead of answering the question
, and (2) we use the fine-tuned GLM to generate
synthetic data instead of training directly for the
downstream tasks. It also differs from (Vu et al.,
2021) in terms of the generated data, where they
consider NLI as an auxiliary task and generate syn-
thetic samples in target-domain for the NLI task
irrespective of the target task and perform interme-
diate task fine-tuning. CDAformats target tasks
into question-answer format and directly generates
samples relevant for target task.
3 CDA: QA Datasets for Generative
Data Augmentation
In this section, we describe the problem statement,
and explain our method including QAC fine-tuning,target-domain adaptation, and synthetic training
data generation.
3.1 Problem Formulation
For a given task T, the input in a few-shot set-
ting contains a very small labeled dataset L=
{(D, l),(D, l), . . . ,(D, l)} and mtar-
get classes C= {C,C, . . . ,C}. Our method re-
quires users to provide a question per dataset that is
representative of the task to be solved. Our aim is
to build a model for the task Tthat assigns a label
C∈Cto each document D.
3.2 QAC Fine-tuning
We consider question-answering datasets Qcon-
taining triplets (q, a, c)of a question q, the corre-
sponding answer a, and a context crequired to
derive the correct answer. Question-answering
datasets can roughly be divided into extractive (Ra-
jpurkar et al., 2016; Trischler et al., 2017; Joshi
et al., 2017; Reddy et al., 2019) and abstractive
datasets (Ko ˇciský et al., 2018; Huang et al., 2019;
Xiong et al., 2019; Sap et al., 2019). For extractive
QA datasets, the answer can be found as a con-
tiguous span in the context, whereas in abstractive
QA datasets, the answer needs to be generated in
natural language without being able to rely on the
vocabulary of the question or context.
We transform the QA dataset Qinto training
dataD for fine-tuning GLM. To this end,
each triplet ( q,a,c) is converted into a single
text by prepending “question:” ,“answer:” and
“context:” , respectively, and concatenating q,a
andcseparated by newlines. For example, a
preprocessed training document in D from an
extractive QA dataset might look as follows:
question : when did battle of plassey happen?
answer : 23 june 1757
context : the battle of plassey was a decisive
victory of the british east india company over the
nawab of bengal and his french allies on 23 june
1757.
We fine-tune a pretrained GLM GonD
to obtain a general context generator Gusing
a language modeling objective to maximize the
log-likelihood of the ( q,a,c) triplet. The general
context generator Gis capable of generating con-
texts for given questions and answers.97393.3 Domain Adaptation and Synthetic
Training Data Generation
We adopt Gto the target domain by fine-tuning
it further on available few-shot data. To preserve
its context generating ability, we perform QAC
fine-tuning instead of regular language model
fine-tuning. This is enabled by transforming the
few-shot supervision into our question-answer-
context format. First, we manually design one
question per dataset that is representative of the
task and the dataset. Furthermore, following
Schick and Schütze (2021a), we define a verbalizer
as a mapping v:C→Vthat maps each label in C
to a word from G’s vocabulary V. Finally, for
every document Dand its respective label lin our
few-shot data, we consider the verbalizer mapping
of the label, v(l), as answer and the text Das
context. For example, a negative review “I hate
this movie” from the IMDb dataset (Maas et al.,
2011) is transformed as follows:
question : is the movie good or bad?
answer : bad
context : i hate this movie.
We fine-tune Gon the converted few-shot data
to obtain a target task context generator G.
Synthetic Training Data Generation Recall
that our method requires a question qfor every
dataset that is representative of the task to be solved.
To obtain synthetic training data, for every distinct
labelC, we create a question-answer prompt with
qas question, v(C)as answer and let Ggener-
ate the context c. The generated context c
and label Care considered as a synthetic training
sample. We repeat this process multiple times to
generate nsamples that we collect in a synthetic
training dataset denoted by D.
As a final step, we train the target task model on
the combination of Dand our original few-shot
dataset L. We use this trained target-task model
for inference.
4 Experiments
In this section, we evaluate our method against
several data augmentation and few-shot methods
on sentiment analysis and text classification tasks.
4.1 QA Datasets
We consider several extractive, abstractive, and
common-sense QA datasets. Common-sense QA
datasets are also abstractive datasets that require
common-sense reasoning to answer the questions.
The QA dataset statistics are provided in Table 1.
The details of these datasets are as follows:
•SQuAD (Rajpurkar et al., 2016, 2018) is a collec-
tion of questions and answers based on Wikipedia
articles.
•NewsQA (Trischler et al., 2017) is a challenging
QA dataset in the News domain where crowd-
workers were shown a news article’s headline
and summary, and asked to formulate a question
about the article without accessing its content.
•TweetQA (Xiong et al., 2019) is a QA dataset
made from a collection of tweets sampled from
two major news websites (CNN and NBC).
•SocialIQA (Sap et al., 2019) is a QA dataset
that tests social common-sense intelligence. The
data is made of common phrases from stories and
books.
•CosmosQA (Huang et al., 2019) is a
commonsense-based reading comprehension
task formulated as multiple-choice questions.
Answering questions requires reasoning not only
based on the exact text spans in the context, but
also abstractive commonsense reasoning.
4.2 Target Task Datasets
We evaluate our method on six English text classifi-
cation datasets. In particular, we consider the three
sentiment analysis datasets: IMDb reviews (Maas9740et al., 2011), Yelp, and SST-2 (Socher et al., 2013),
as well as three topic classification datasets: Ya-
hoo (Zhang et al., 2015), The New York Times
(NYT), and AGNews (Zhang et al., 2015). The
dataset-representative questions, and their respec-
tive verbalized labels of target task datasets are
mentioned in Table 2. We follow and adapt Mc-
Cann et al. (2018) for questions in sentiment anal-
ysis datasets. The question for topic classification
is intuitive and straightforward. More details about
the datasets can be found in Appendix A.1.
4.3 Compared Methods
We compare with a wide range of data augmenta-
tion and intermediate-task fine-tuning (ITFT) meth-
ods described below:
•BERT-FT trains the BERT-base-uncased clas-
sifier (Devlin et al., 2019) on the few-shot super-
vision.
•ITFT- X(Phang et al., 2018) first trains a model
on dataset Xand fine-tunes it further on the
target task. We compare with ITFT-MNLI
and ITFT-SQuAD fine-tuned intermediately on
MNLI (Williams et al., 2018) and SQuAD
datasets respectively.
•BackTranslation (Sennrich et al., 2016) aug-
ments samples by translating them into a non-
English language and translating them back to
English. We translate them to French, Spanish,
and Portuguese thereby augmenting three syn-
thetic samples for every sample.
•PEGASUS (Zhang et al., 2019) is a state-of-the-
art paraphrasing model. We paraphrase the input
text and consider it as a synthetic sample and
augment it to the training set.
•EDA (Wei and Zou, 2019) generates synthetic
samples by synonym replacement, random in-
sertion, random swap, and random deletion and
augment them to the training set.
•LAMBADA (Anaby-Tavor et al., 2020) fine-
tunes a GLM on few-shot supervision prepended
with their target labels and then generates syn-
thetic data by prompting the GLM with a given
target label.
We denote our method as CDA, which in-
cludes QAC fine-tuning, domain adaptation, syn-
thetic samples generation, and training the target
task classifier. CDA-Xrepresents that the QAC
fine-tuning of GLM is performed on QA datasetX. We also compare with CDA\QAwhere we
perform no QAC fine-tuning and directly fine-tune
the GLM on target dataset.
4.4 Experiment Settings
We consider two low-data regimes: few-shot and
zero-shot. We consider 8 annotated samples per
label in the few-shot setting. In the zero-shot
setting, we skip the domain adaptation step and
useGdirectly for synthetic training data gener-
ation and train the target task model only on the
generated synthetic training data. We use GPT2-
Medium (Radford et al., 2019) as our GLM and
fine-tune it for 3 epochs in QAC-fine-tuning and
domain adaptation steps. While generating syn-
thetic training samples, we use top- ksampling with
k=20, a maximum length of 200 tokens, and gen-
eraten=450synthetic samples per label. We use
BERT-base-uncased (Devlin et al., 2019) as target
task classifier. We feed [CLS] representation into
the classification head and train all the parameters
on the downstream target tasks. Following (Devlin
et al., 2019), we fix the number of epochs of target
task BERT classifier training to 4 unless mentioned
otherwise. We perform 3 random restarts and re-
port the mean and standard deviation.We use
the Transformers library (Wolf et al., 2020) and
NVIDIA RTX A6000 GPUs for our experiments.
To enable a fair comparison, we generate the
same number of samples per label as CDA(i.e.,
450) for all data augmentation baselines. We use
BERT-base-uncased as the target task classifier
for all baselines. CDA\QAfor zero-shot set-
ting implies a pre-trained GPT2. While training
the target task classifier, since the number of train-
ing samples for baselines like BERT-FT, ITFT are
different than data augmentation baselines and our
method CDA, we set the number of epochs for
all baselines such that the number of update steps
remain the same for a fair comparison.
4.5 Results and Discussion
Results for few- and zero-shot settings are shown
in Table 3 and Table 4, respectively, using Micro-
and Macro-F1 as evaluation metrics. We discuss
the effectiveness of our method below.
CDAvs Baselines. In the few-shot setting,
CDAwith abstractive and common-sense based
datasets outperforms all baselines for most of the
datasets, beating the best baseline in five out of9741
six cases. CDAperforms better than BERT-FT
on all datasets, achieving up to 14% improvement
on SST-2. Although ITFT performs better than
vanilla fine-tuning, CDAdemonstrates better
performance than ITFT on all datasets. For exam-
ple,CDA-TweetQA shows 11% improvement
over ITFT-SQuAD on AG-News. CDAdemon-
strates higher performance than data-augmentation
baselines on all datasets except NYT. The compari-
son between CDAand LAMBADA shows that
our QA formulation prompt is more intuitive and
informative than just the target label. We attribute
the superior performance of CDAto the context-
generating ability acquired during QAC fine-tuning
that is efficiently leveraged by generating synthetic
samples, which are added to the training set.
Abstractive vs Extractive QA Datasets. We ob-
serve that the performance of CDAwith ab-
stractive QA datasets is significantly better than
CDAwith extractive QA datasets in both few-
shot and zero-shot settings. For example, CDA-
TweetQA has an improvement of more than 20%
over CDA-SQuAD on IMDb in few-shot set-
ting. We surmise that this is because of the in-
trinsic nature of extractive QA datasets (i.e., theanswer always being present in the context as a
contiguous span). We observe that GLMs fine-
tuned on an extractive QA dataset retain the ability
to generate contexts that encompass the answer.
Note that, while generating synthetic training sam-
ples, the answer in the prompt is its respective
topic. For example, out of 500generated samples
byCDA-SQuAD for Yelp dataset, 213 sam-
ples contain at least one occurrence of its corre-
sponding verbalized label whereas it is only 73for
CDA-CosmosQA. Thus, many synthetic sam-
ples generated contain their corresponding label
in text. Therefore, a classifier trained on synthetic
samples that have their corresponding labels in the
text, easily overfits on the label tokens and does not
generalize well to unseen test data.
Comparison with CDA\QA. CDAwith
abstractive QA datasets perform better than
CDA\QAin both few-shot and zero-shot set-
tings, attaining improvements up to 40% and35%
respectively in macro-F1 on SST-2. This demon-
strates that the context generating abilities are
learnt and reinforced during the QAC fine-tuning
on QA datasets which is efficiently utilized by gen-
erating synthetic samples.9742
Zero-shot Performance. The zero-shot perfor-
mance of CDAfollows a similar trend as few-
shot performance: abstractive and common-sense
reasoning QA datasets lead to better performance
than extractive datasets and no QAC fine-tuning.
4.6 Ablation Study
To understand the impact of domain adaptation and
few-shot samples, we compare CDAwith two
ablated versions in Table 5: (1) CDA-DArepre-
sents our method without domain adaptation (i.e.,
generating synthetic data using Gand training
the classifier on combined few-shot supervision
and synthetic data generated by G), (2) CDA-
Few Shot represents the classifier trained only on
the samples generated by G. We also present
the results of our complete pipeline for reference.
CDAperforms better than CDA-Few shot
in most cases, demonstrating the importance of
including few-shot samples in the training set for
the classifier. The comparison between CDA
andCDA-DA suggests that fine-tuning the lan-
guage model further on the target dataset helps in
some scenarios but does not always improve per-
formance. This is in line with previous research
findings (Du et al., 2021; Vu et al., 2021; Pryzant
et al., 2022). We conjecture that domain adapta-
tion is important when the structure of the target
task dataset is very different from the QA dataset.
For example, domain adaptation helps most of the
QA datasets on SST-2 dataset because the text in
SST-2 is a single sentence, whereas most of the
QA datasets have paragraphs as context. More-
over, it also depends on the number of samples
the language model is fine-tuned on during domain
adaptation. We observe that the higher the number
of samples, the more positive their impact. For
example, the number of few-shot samples is the
highest in Yahoo compared to other datasets and
domain adaptation positively contributes to the per-
formance on Yahoo for all QA datasets.
4.7 Larger Generative Language Models
Experimental results with GPT2-Large as the GLM
are shown in Table 6. We observe that the rela-
tive performance trend remains the same as GPT2-
Medium i.e. CDAwith abstractive datasets per-
forms better than CDAwith extractive datasets
andCDA\QA-L. This indicates that QAC fine-
tuning improves the performance of generative data
augmentation with larger GLMs as well.
4.8 Performance vs No. of Generated Samples
We fix the few-shot supervision size to 8 samples
per label and vary the number of generated sam-
ples per label and plot the performance of CDA-
TweetQA, CDA-SocialIQA, and baselines such
as LAMBADA and EDA on AGNews and IMDb
datasets, shown in Figure 3. We repeat each setting
with three different seeds and plot the mean perfor-
mance. We observe that the performance increases
and it plateaus after a while. This shows that syn-
thetic training data can give a substantial boost to
the few-shot training data, minimizing the human
effort in manual annotations; however, it cannot
replace the original training data completely as it9743
requires more human annotated data to improve
beyond some limit.
4.9 Performance vs Few-shot supervision Size
We fix the number of generated samples to 450 per
label and vary the number of annotated samples and
plot the performance of CDA-CosmosQA and
CDA-SocialIQA on SST-2 and Yahoo datasets
in Figure 4. We also plot the performance of base-
lines such as BERT-FT, EDA, BackTranslation for
comparison. We repeat each experiment with three
random seeds and plot the mean performance. We
observe that the performance of CDAincreases
with the size of supervision and the improvement
over baselines in the low-data regime is substantial.
For example, with only 4 annotated samples per
label in Yahoo dataset, the macro F1 of CDA-
CosmosQA outperforms BERT-FT by 22% and
EDA by 15%. However, we also observe that the
performance gap between CDAand baselines
decreases with increase in supervision size and gets
stagnated after a while. As the size of supervision
increases, the supervision by itself is sufficient for
high performance, thus reducing the performance
boost due to synthetic training data.
4.10 Self-Training
We perform an experiment to demonstrate that the
performance can be further improved through self-
training when in-domain unlabeled samples are pro-
vided. In-domain unlabeled samples are often eas-
ily available in real-world scenarios. Self-training
is a commonly-used approach to bootstrap the clas-
sifier on unlabeled samples (Mekala and Shang,
2020; Mekala et al., 2020; Vu et al., 2021). Follow-
ing Vu et al. (2021), we obtain pseudo-labels by
predicting on unlabeled samples using the trained
classifier and train the classifier further on the avail-
able labeled and pseudo-labeled data. We consider
the training set without ground truth labels as un-
labeled data and experiment on SST-2, NYT, and
AGNews datasets. We repeat this process for 3 iter-
ations without any filtering of pseudo-labels. From
the results in Table 8, we can observe a significant
performance improvement up to 4 points with self-
training. It is noteworthy that this improvement
is consistent for both GPT2-Medium and Large
models respectively.
4.11 Synthetic Data Adds Value
Unsupervised language model pre-training(LMPT)
on target-task unlabeled data can improve perfor-
mance (Gururangan et al., 2020). We consider
training set without ground truth labels as unla-
beled data for LMPT and present a comparison in
few-shot setting in Table 7. We observe CDA
performs better than LMPT demonstrating the qual-
ity and importance of generated synthetic data.
4.12 Case study: Evaluating Context
Generator
We hypothesize that our method results in high-
quality context generators that are capable of gen-
erating context for a given question and answer. To9744validate this hypothesis in in-domain and out-of-
domain settings, we perform two experiments on
QA task.
In-domain Analysis. In this experiment, we val-
idate whether the context generator is capable of
generating context for question, answer pairs be-
longing to the same domain as QA dataset used
for QAC fine-tuning. We consider SQuAD dataset
and partition it into training set with 1000 (ques-
tion, answer, context) triplets, dev set of size 1700
with only (question, answer) pairs and a test set
of size 6570. First, we consider GPT2-Medium as
GLM and perform QAC fine-tuning on the train-
ing set. Then, we generate contexts for the dev
set and augment the (question, answer, generated
context) triplets to the training set. Finally, we
train a BERT-base-uncased QA model on the aug-
mented data. We compare it with the BERT model
trained only on the original training set. We report
F1 scores on test set in Table 9. We observe a boost
of4%using our synthetic training data, validating
our hypothesis in the in-domain setting.
Out-of-domain Analysis. In this experiment, we
validate our hypothesis in the out-of-domain setting
i.e. the domain of target dataset is different than the
QA dataset used for QAC fine-tuning. We follow
our proposed pipeline and consider SQuAD as the
QA dataset for QAC fine-tuning and NewsQA as
the target dataset. We partition NewsQA dataset
into 1000 (question, answer, context) triplets for
domain adaptation, 17000 (question, answer) pairs
for context generation, and test on 10000 samples.
We fine-tune GPT2-medium on SQuAD to obtain
general context generator and adapt to the NewsQA
domain by training it further on 1000 question, an-
swer, context triplets from NewsQA. Using the
target task context generator, we generate contexts
for 17000 question, answer pairs, augment it to
the training set, and train BERT-base-uncased QA
model on the augmented data. From F1 scores re-
ported in Table 9, we can observe more than 10%
improvement in the performance, demonstrating
the efficiency of our method in out-of-domain set-
ting.
5 Conclusion
In this paper, we propose to train generative lan-
guage models to be context generators for a given
question and answer. To facilitate this, we use ques-
tion answer as a format and utilize QA datasets for
training generative language models into context
generators. We view sentiment and topic classi-
fication tasks in question-answer form and gener-
ate contexts using our fine-tuned generative lan-
guage models. These generated contexts are used
as synthetic training data to augment existing few-
shot data for training a classifier. Extensive exper-
iments on multiple sentiment and topic classifica-
tion datasets demonstrate strong performance of
our method in few-shot and zero-shot settings.
6 Limitations
One limitation of our approach is the synthetic
training data generated can boost the performance
up to an extent and beyond that it requires more
annotated samples. So, the generated synthetic
training data cannot replace the training data al-
together but could minimize the annotation effort
significantly. Moreover, some tasks such as NER
are challenging to cast into question-answering for-
mat, which hinders generating synthetic data using
our method.
7 Acknowledgements
We thank anonymous reviewers and program chairs
for their valuable and insightful feedback. The
research was sponsored in part by National Sci-
ence Foundation Convergence Accelerator under
award OIA-2040727 as well as generous gifts
from Google, Adobe, and Teradata. Any opinions,
findings, and conclusions or recommendations ex-
pressed herein are those of the authors and should
not be interpreted as necessarily representing the
views, either expressed or implied, of the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for government
purposes not withstanding any copyright annota-
tion hereon.9745References974697479748
Dataset # Test Examples
IMDb 25000
Yelp 50000
SST-2 2211
Yahoo 60000
NYT 10582
AGNews 114000
A Appendix
A.1 Target Task Datasets
The details of target task datasets are as follows:
•IMDb: (Maas et al., 2011) is a movie review
dataset with positive and negative as sentiments.
•Yelp:is a collection of reviews written by Yelp
users with five fine-grained sentiment ratings.
•SST-2: (Socher et al., 2013) is a binary sentiment
classification dataset with single sentence texts.
•Yahoo: (Zhang et al., 2015) is a topic classifi-
cation dataset with question and answer pairs.
Using these pairs, the task is to predict their cor-
responding topic.
•The New York Times (NYT): : contains news
articles written and published by The New York
Times that are classified into 5 wide genres.
•AGNews: (Zhang et al., 2015) is a topic cate-
gorization dataset in news domain from AG’s
corpus.
The size of test sets is mentioned in Table 10.
A.2 Performance vs k
We vary k in top-k sampling and plot the perfor-
mance of CDA-SocialIQA on IMDb, SST-2,
AGNews, and Yahoo datasets in Figure 5. We fixthe few-shot supervision size to 8 samples per label
and generate 450 samples per label. We repeat each
experiment thrice and plot the mean performance.
Upon manual inspection, We observe that the sam-
ples generated with k=20 are more diverse than
k=10, however, the influence of k on performance
is not significant.
A.3 Experiments with a validation set
We perform experiments with a validation set.
Since large validation sets are impractical in few-
shot settings (Oliver et al., 2018), we consider the
validation set to be of same size as the few-shot
training set i.e. 8 annotated samples per label. In
the experiments with validation set, we perform
early stopping based on validation set performance.
We present experimental results on few-shot setting
with validation set in Table 11. We seldom observe
significant improvement upon introducing the val-
idation set. This is because a small validation set
which is of same size as few-shot supervision is not
large enough to tune the hyperparameters.
A.4 Examples of Generated Training Data
Table 12 shows a few examples of synthetic train-
ing data corresponding to IMDb and AGNews
datasets generated by our method with all QA
datasets.97499750