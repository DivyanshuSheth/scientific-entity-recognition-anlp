
Jia-Huei Ju, Yu-Shiang Huang, Cheng-Wei Lin, Che Lin,and Chuan-Ju WangResearch Center for Information Technology Innovation, Academia Sinica,Graduate Program of Data Science, National Taiwan University and Academia Sinica,Graduate Institute of Communication Engineering, National Taiwan University,Department of Electrical Engineering, National Taiwan University,
{jhjoo, yushuang, lcw.1997}@citi.sinica.edu.tw,
chelin@ntu.edu.tw, cjwang@citi.sinica.edu.tw
Abstract
In this paper, we address the challenge of dis-
covering financial signals in narrative finan-
cial reports. As these documents are often
lengthy and tend to blend routine informa-
tion with new information, it is challenging
for professionals to discern critical financial
signals. To this end, we leverage the inher-
ent nature of the year-to-year structure of re-
ports to define a novel signal-highlighting task;
more importantly, we propose a compare-and-
contrast multistage pipeline that recognizes dif-
ferent relationships between the reports and lo-
cates relevant rationales for these relationships.
We also create and publicly release a human-
annotated dataset for our task. Our experiments
on the dataset validate the effectiveness of our
pipeline, and we provide detailed analyses and
ablation studies to support our findings.
1 Introduction
With the rapid growth of information, many tasks
in the field of natural language processing (NLP)
involve streamlining information comprehension.
One such task is summarization, which selects a
subset of sentences or generates new content that
best represents the given document (Hermann et al.,
2015; See et al., 2017; Cohan et al., 2018). This
task helps humans save time and effort by iden-
tifying important information in a text. In the fi-
nance context, comprehending regulatory narrative
reports is a classic example of efficiently mining
signals from a large amount of text. As these re-
ports often contain rich information concerning spe-
cific financial entities, discovering valuable insights
is crucial for academia and the finance industry.
Much research has shown that textual features
from financial reports contain valuable financial sig-
nals about future firm performance and market reac-
tions (e.g., Badertscher et al., 2018; Ertugrul et al.,
2017; You and Zhang, 2009). However, authorities
such as the Securities and Exchange Commission(SEC) require that companies provide comprehen-
sive and detailed information about their current
status in these reports, which often contain much
unimportant and already-known information. For
example, the token overlap ratio between annual
10-K reports of the same company between adja-
cent years is often high,making it a challenging
and tedious task to acquire important signals in new
reports (termed as the overlapping characteristic
hereafter).
Recent advances in NLP technology have in-
cluded attempts to efficiently and effectively com-
prehend lengthy financial documents. One ap-
proach to address this problem is through summa-
rization (e.g., Zmandar et al., 2021b; Orzhenovskii,
2021; Gokhan et al., 2021). Other approaches addi-
tionally leverage numerical metrics, such as stock
return volatility and abnormal trading volumes, to
locate essential financial signals in reports (e.g.,
Kogan et al., 2009; Tsai and Wang, 2017; Rekab-
saz et al., 2017; Agrawal et al., 2021; Lin et al.,
2021). However, these approaches often require
high-quality human annotation or suitable financial
measures, which poses significant limitations in
practical scenarios.
In this study, we approach financial report com-
prehension from a novel perspective by leveraging
the intrinsic year-to-year characteristic of reports
(i.e., the overlapping characteristics). Specifically,
for a particular company, we use the document pub-
lished in the previous year as an information anchor
(i.e., the reference) to construct a year-to-year struc-
ture and locate important financial signals in the re-
port of the subsequent year (i.e., the target). This in-
herent structure enables us to mine financial signals
in a compare-and-contrast self-supervised manner,
compared to existing supervised approaches.
Based on the year-to-year structure, we propose14307acompare-and-contrast multistage pipeline to ef-
fectively locate financial signals in reports. We first
identify a few types of relationships between ref-
erence and target financial reports at the segment
level. Then, using these recognized relationships,
we present a novel financial signal-highlighting
task together with a domain-adaptive highlighting
model. The goal of this task is to identify the
rationales, represented by the importance of cer-
tain words, for a specific pair of year-to-year seg-
ments. Therefore, the words with high importance
are deemed to be crucial financial signals in these
reports. For experiments, we present a synthetic
dataset consisting of 30,400 reference-to-target seg-
ment pairs for financial signal highlighting.Ex-
perimental results validate the effectiveness of the
proposed pipeline; detailed analyses and ablation
studies are also provided.
2 Problem Definition
The year-to-year nature of financial reports allows
us to take advantage of the differences between a
company’s documents in consecutive years. These
differences may reveal complex but insightful re-
lationships within a pair of documents. To better
understand these relationships, we investigate them
through rationales (represented by the word impor-
tance), which are considered essential signals in
financial reports.
2.1 Reference-to-target Structure
Formally, for each company, Dis a set containing
all segments in its financial report at year ℓ, where
each element d∈ Drefers to a single segment.
While we regard a focal company’s financial report
at year ℓ,D, as the target document, we view
the same company’s report at year ℓ−1,D, as
thereference document. Given the annual nature
(i.e., the reference-to-target structure) of financial
reports, we further break down the document-to-
document relationship between DandDinto
enumerated segment-to-segment relationships. We
denote the set of enumerated segment pairs as ¯T.
However, as ¯Tincludes all pairs of segments
enumerated from D,andD(i.e.,|D||D|
pairs), intuitively, most segment pairs in ¯Thave
no interesting relationship. Hence, we reduce the
set¯TtoTby removing irrelevant segment pairs
based on their syntactical similarities. Specifi-
cally, for each target segment t∈D, we cal-
culate the ROUGE-2 (Lin, 2004) scores between
the target segment tand all reference segments
r∈ Dand sort the reference segments ac-
cording to their scores in descending order as
¯S(t) =/parenleftbig
r, r, . . . , r/parenrightbig
.With ¯S(t), we then
discard reference segments that fall behind the
largest ROUGE-2 difference out of all possible
ROUGE-2 differences, resulting in a truncated set
S(t).Note that the difference is calculated be-
tween the two consecutive ROUGE-2 scores in
¯S(t). Finally, with S(t), the reduced segment pair
set isT={(r, t)|(r, t)∈¯T ∧r∈S(t)}.
To locate meaningful financial signals revealed
by segment pair differences, we further classify
each pair (r, t)∈ T into the following two sets:
1.Tcontains reference-to-target segment pairs
with largely similar meanings (see Table 1(a)).
Generally, there is no additionally notewor-
thy content in target segment tcompared to
reference segment r.
2.T=T \ Tcontains segment pairs with
dissimilar meanings (see Table 1(b)). Pairs in
Tare further classified into two types based
on their syntactic and semantic similarity, as
discussed in Section 3.2.
Note that all the aforementioned terminologies and
notations can also be found in Figure 1; they will
be used throughout the following sections of this
paper.14308
2.2 Highlighting Task
We consider pairs in Tas the pairs of interest and
provide rationales of underlying pairwise relation-
ships by predicting the word importance for each
segment pair (r, t)∈ Tas
R=P(t|r), (1)
where Rindicates the word importance of a target
segment tconditioned on reference segment r, and
the highlighting model is denoted as f(detailed in
Sections 3.3 and 3.4).
3 Proposed Pipeline
Here we describe the proposed multistage pipeline
for discovering the rationale behind the reference-
to-target structure in financial reports, as illustrated
in Figure 1.
3.1 S: Document Segmentation
Financial reports are multimodal, often covering
multiple aspects and topics; each aspect or topic
usually uses one to three consecutive sentences to
convey its meaning. Therefore, instead of consid-
ering sentences as the basic unit of text, we here
regard uni-modal segments as the smallest unit for
financial documents. We first use spaCy API for
sentence segmentation.Then, we utilize the fine-
tuned cross-segment BERT (Lukasik et al., 2020)
to obtain coherent uni-modal segments. Note that
some studies show that breaking a document into
uni-modal segments benefits downstream applica-
tions (Shtekh et al., 2018; Qiu et al., 2022; Chivers
et al., 2022).3.2 S: Relation Recognition
In this stage, a systematic procedure manages re-
lation types TandTwith semantic and syn-
tactic similarity. Specifically, we use two func-
tions, ROUGE-2 and Sentence-BERT (Reimers
and Gurevych, 2019) cosine similarity,to assess
the syntactic and semantic similarity between each
reference-to-target pair (r, t)∈ T.The scores for
the syntactic and semantic similarity are denoted
asϕ(r, t)andϕ(r, t), respectively.We em-
pirically design a rule-based procedure and classify
each segment pair into three types.
1.Insignificant relations ( T) correspond to un-
informative segment pairs with highly similar
syntactic and semantic meanings between tar-
get and reference segment (i.e., ϕ> ϵ
andϕ> ϵ).
2.Revised relations ( T) correspond to segment
pairs that differ in some words only but dis-
close quite different meanings, resulting in a
highϕ(r, t)but a relatively low ϕ(r, t)
(i.e.,ϕ> ϵandϕ< ϵ).
3.Mismatched relations ( T) correspond to seg-
ment pair meanings that are to some ex-
tent mutually exclusive, resulting in a low
ϕ(r, t)(i.e.,ϕ< ϵ).
The procedure and the setting of the two thresholds
(ϵandϵ) are also summarized in Figure 4 in
Appendix C.143093.3 S: Out-of-domain Fine-tuning
Here we pinpoint financial signals for segment
pairs in T=T∪ T. Specifically, for each
segment pair (r, t)∈ T, we discover rationales
through predicted word importance in target seg-
ment t, where the rationales are inferred condi-
tioned on reference segment r(see Eq. (1)).
Binary token classification To accomplish this,
we cast the word importance prediction as super-
vised binary token classification. First, we leverage
the pre-trained BERT (Devlin et al., 2019) model
to construct contextualized reference-to-target pair
representations, where each pair of interest consti-
tutes an input with special tokens as
h=BERT ([CLS] r[SEP] t),
where h∈ Ris the contextualized token
representation of the pair, dis the dimension of
each token representation, and nis the number of
tokens in segment pair (r, t). Second, on top of the
token representation h, we add a highlighting
model f(·)(an MLP layer) with softmax function.
The resultant conditional word importance P(t|r)
for the j-th word in target segment tis
P(t|r) =exp/parenleftig/parenleftig
f/parenleftig
h/parenrightig
[1]/parenrightig
/τ/parenrightig
/summationtextexp/parenleftig/parenleftig
f/parenleftig
h/parenrightig
[i]/parenrightig
/τ/parenrightig,
(2)
where hdenotes the token representation of
thej-th word in target segment t(i.e., the j-th
row vector of h),f(·) : R→ R, and τ
is a hyperparameter that controls the probability
distribution.
Signal highlighting warm-up As we view sig-
nal highlighting as binary token classification, we
first fine-tune the model f(·)on e-SNLI (Cam-
buru et al., 2018), an external human-annotated
dataset, to obtain a zero-shot model. Note that
e-SNLI was compiled for explanation generation
with human-annotated rationales to distinguish rela-
tions of aligned sentence pairs (r, t)(i.e., premise
and hypothesis) in natural language inference. We
then treat the annotated words as the ground truth
for the premise-to-hypothesis relation,which is
similar to our reference-to-target structure. For-
mally, we adopt the binary cross-entropy objec-
tive for each token in hypothesis tto fine-tune theBERT token representations and the highlighting
model f(·)as
L=/summationdisplay−/parenleftig
YlogP/parenleftbig
t|r/parenrightbig/parenrightig
+/parenleftig
1−Y/parenrightig
log/parenleftig
1−P(t|r)/parenrightig
,
where Yis a vector in which each element Y
indicates the binary label of word importance for
thej-th word in hypothesis t. For instance, Y=
1implies the j-th word in tis annotated as an
important word conditioned on the given premise
sentence r. We thus construct the out-of-domain
zero-shot highlighting model by fine-tuning on e-
SNLI, which is regarded as a baseline to proceed
with the following financial domain adaptation (see
Figure 1).
3.4 S: In-domain Fine-tuning
Generally, for applications, particularly in niche do-
mains like finance, models with a zero-shot setting
may not be effective enough. Also, several studies
show that language models exhibit poor perfor-
mance under domain shift scenarios (Ben-David
et al., 2006; Han and Eisenstein, 2019; Gururangan
et al., 2020; Li et al., 2022). We account for this
by equipping the proposed pipeline with an extra
in-domain fine-tuning stage to enable our highlight-
ing model to adapt properly to the financial do-
main. Specifically, we construct a domain-adaptive
financial signal highlighting model f(·)with the
following learning strategies: (1) pseudo-labeling
with revised segment pairs in T, and (2) further
fine-tuning with soft labels.
Pseudo-labeling with revised segment pairs
We introduce a simple yet effective pseudo-labeling
approach that uses revised segment pairs (i.e., T)
collected from stage S(see Section 3.2). Recall
that these segment pairs differ in some words only
but have quite different meanings. Given such a
property, we establish a heuristic labeling approach
for pseudo-labels of financial signals. Intuitively,
we treat all revised words in target segment tas
important words and mark them as positive, and
randomly sample other words as negative ones.
Further fine-tuning with soft labels To com-
pensate for deficiencies in such assertive binary
pseudo-labels, we use soft labeling to make the14310token representations more generalized. Initially,
as illustrated in Figure 1, we leverage the zero-
shot highlighting model f(·)learned at stage S
to calculate the approximate word importance of
the revised segment pairs, the results of which are
regarded as soft labels compared to the assertive
pseudo-binary labels. We then construct the soft-
labeling objective Las
L=γL+ (1−γ)L, (3)
where
L=/summationdisplay−KL/parenleftig
P(t|r)/vextenddouble/vextenddouble/vextenddoubleP(t|r)/parenrightig
(4)
andγis a hyperparameter that controls the im-
pact of soft labeling. In Eqs. (3) and (4), KL(·)
denotes Kullback–Leibler (KL) divergence, and
P(t|r)andP(t|r)indicate the estimated prob-
ability distributions predicted by f(·)andf(·),
respectively. Finally, we fine-tune the highlighting
model f(·)with the pseudo-labels annotated on
segments in Tby optimizing Lin Eq. (3). Note
that we not only utilize probabilities P(t|r)as our
training targets (i.e., soft labels) for Lbut we
also adopt the warm-start token representations and
highlighting layer f(·)as the initial checkpoint for
fine-tuning f(·). In addition, we discover that hy-
perparameters τandγaffect the performance sig-
nificantly. We discuss the hyperparameter search
in Appendix B.
4 The FINAL Dataset
We constructed FINAL ( FINancial- ALpha), a fi-
nancial signal highlighting dataset, consisting of
30,400 reference-to-target segment pairs in ∈ T.
4.1 Financial 10-K Corpus Preprocessing
We used Form 10-K filings collected from the Soft-
ware Repository for Accounting and Finance,
where a Form 10-K is an annual report required
by the U.S. SEC. Specifically, we used 10-K fil-
ings from 2011 to 2018, which comprise 63,336
filings from 12,960 public companies. To make
the best use of the year-to-year information, we
discarded companies for which the reports in some
years were missing during the period; 3,849 com-
panies (3,849 ×8=30,792 reports total) remained
after this filtering. We then randomly sampled 200
companies from the 3,849 companies with their
annual reports to construct the dataset. In addition,
while every 10-K annual report contains 15 sched-
ules (e.g., Items 1, 1A, 1B, 2, 3, . . . , 7, 7A, . . . ,
15),we extracted only Item 7 (Management’s
Discussion and Analysis of Financial Conditions
and Results of Operations (“MD&A”)) to form the
FINAL dataset.Finally, we aligned each docu-
mentDwith its corresponding last-year document
D, resulting in 1,400 reference-to-target docu-
ment pairs (i.e., 200 companies ×7 year-to-year
pairs).
4.2 Year-to-year Segment Pair Generation
After preprocessing, we followed the proposed mul-
tistage pipeline by first passing each document pair
through stage Sto obtain an enumerated set of
segment pairs ¯T; we then reduced ¯TtoTby re-
moving irrelevant segment pairs (see Section 2.1).
Next, we followed the relation recognition stage S
in Section 3.2 to obtain the two groups of segment
pairs:TandT. From each of these two groups,
we randomly sampled 200 pairs for human annota-
tion as our evaluation sets. Likewise, we randomly
sampled 30,000 pairs from the rest of the revised
segment pairs (i.e., T) as the training set for the
pseudo-labeling approach in Section 3.4.
4.3 Human Annotation
To evaluate the empirical effectiveness of the pro-
posed pipeline, we manually annotated the sam-
pled 400 segment pairs. For each segment pair
(r, t), we collected the labels of rationales from
three annotators. Specifically, the annotators were
to distinguish which words in each target segment t
to regard as important financial signals according
to the context of the corresponding reference seg-
ment r. That is, the words with positive labels14311were to characterize the reference-to-target rela-
tionship or disclose extra information of interest,
whereas the rest of the words in twere labeled as
negative. We further assessed the inter-rater relia-
bility of the three annotations with Fleiss’ κ(Fleiss,
1971). For simplicity, we treat the prediction for the
importance of each word in the target segment as in-
dependent classification tasks (containing roughly
12K words in the 400 evaluation pairs): for eval-
uation pairs from T,κ= 0.71; for those from
T,κ= 0.60. The training and evaluation sets are
described in Table 2(a), where Avg. |t|and Avg.
|r|are the average lengths of target and reference
segments, respectively, and Avg. #wand Avg.
#ware the average numbers of words annotated
as positive and negative, respectively.
5 Experiments
5.1 Evaluation Datasets
FINAL We evaluated the highlighting perfor-
mance on the two evaluation sets with the human-
annotated ground truth (see Table 2(a)).
e-SNLIWe additionally evaluated the perfor-
mance on e-SNLI. Particularly, in this paper, we
used only the premise-to-hypothesis sentence pairs
labeled as contradiction (denoted as e-SNLI) in
the test set of the e-SNLI dataset for evaluation (see
Table 2(b)).
5.2 Evaluation Metrics
Recall-sensitive metric In practice, financial
practitioners are usually concerned more about the
recall of the discovered signals than their precision
due to the high cost of missing signals. Accord-
ingly, we borrow the idea of R-precision (Buckley
and V oorhees, 2000), a metric from the information
retrieval field. In our case, R-precision ( R-Prec)
is the precision at R, where Ris the number of
annotated words in each target segment: if there
arerannotated words among the top- Rpredicted
words, then the R-precision is r/R.
Sequence agreement of word importance In
addition, we measure the agreement between the
predicted importance of words for each target seg-
ment (considered as a number sequence) and its
corresponding ground-truth sequence. Specifically,
we use the Pearson correlation coefficient (PCC)
for evaluation.
Note that for R-Prec, we use majority voting
to derive single ground-truth labels from the three
annotators, whereas for PCC, we take the mean
agreement of the three annotations as the ground
truth. Note also that neither of the above two met-
rics requires a hard threshold to determine the im-
portant words for evaluation. Whereas R-Prec con-
siders the words with the top- Rhighest predicted
probabilities, PCC directly leverages the predicted
probabilities of words as the importance of words
for calculation.
5.3 Compared Methods
Zero-shot We fine-tuned the BERT-base model
on the e-SNLItraining set with the binary to-
ken classification cross-entropy objective (See Sec-
tion 3.3 for details) and used this as a zero-shot
approach for financial signal highlighting.
Pseudo few-shot Instead of using e-SNLI, we
fine-tuned the BERT-base model on the 30,000
revised segment pairs in T(see the “Train” data
in Table 2(a)) with the pseudo-label tokens (see
pseudo-labeling introduced in Section 3.4) and use
this as a pseudo few-shot approach.
Domain-adaptive Using the zero-shot highlight-
ing model as the initialization, we further per-
formed in-domain fine-tuning (see stage Sin Sec-
tion 3.4) for domain adaptation.
5.4 Empirical Results
5.4.1 Main Results for Signal Highlighting
Performance on FINAL Table 3 tabulates the
highlighting performance under four conditions
(i.e., #1–#4), where W.U. denotes that e-SNLIis
used for warm-up fine-tuning (i.e., the zero-shot
highlighting model), PandSdenote pseudo and
soft labeling, respectively, and ’ ∗’ denotes statisti-
cal significance with respect to the performance of14312zero-shot learning (#1) under a paired t-test with
p <0.05.
We first focus on the results of the main task on
FINAL, where the listed results are those evaluated
on the union of the two evaluation sets (including
400 segment pairs in total). As shown in the table,
the proposed domain-adaptive approach using both
pseudo and soft labeling techniques (i.e., condi-
tion #4) achieves the best R-Prec of 0.7865 and
PCC of 0.7290. In addition, from the performance
increase from condition #2 to #3, we observe that
warm-up fine-tuning (W.U.) plays an essential role
in financial signal highlighting. Similarly, soft la-
beling is also beneficial for our task, bringing a
10% performance improvement in both evaluation
metrics (by comparing the results of conditions #3
and #4). However, from the results of conditions #1
and #3, we observe that adopting pseudo-labeling
alone might not be helpful for this task, perhaps
because the pseudo-labels constructed by the pro-
posed heuristic approach (see Section 3.3) are too
aggressive for unimportant tokens, resulting in a
biased highlighting model. In sum, we offer two
main observations from Table 3.
•The proposed domain-adaptive fine-tuning
with pseudo and soft labeling is effective for
signal highlighting in financial reports.
•Warm-up fine-tuning and soft labeling are two
crucial components to constructing an effec-
tive domain-adaptive highlighting model.
Generalization ability between domains Ta-
ble 3 also lists the results on the e-SNLItesting
data: only the model with condition #4 performs
on par with or even outperforms that with con-
dition #1 (i.e., zero-shot), showing that the high-
lighting model fine-tuned by the propose domain-
adaptive approach exhibits good generalizability.
5.4.2 Analyses on Different Types of
Relationships
To better understand the empirical advantages of
the domain-adaptive approach, we further inves-
tigate the highlighting performance for different
kinds of reference-to-target relations, T(revised )
andT(mismatched ). Figure 2 compares the re-
sults of the zero-shot (#1) and domain-adaptive
(#4) methods in terms of two metrics. We here
focus on PCC, as R-precision considers only the
set of important words (i.e., labeled as positive)
instead of all the words in each target segment. In
the figure, we see that despite the significant PCC
improvements on both revised and mismatched
pairs, the benefit of domain adaptation on mis-
matched pairs is markedly greater than that on
revised pairs, yielding a PCC improvement of ap-
proximately 23%. Perhaps the important words in
the mismatched pairs are more uncertain, necessi-
tating intensive domain adaptation more than those
in the revised pairs. Note that we fine-tuned the
model on only 30,000 revised segment pairs in T
for domain adaptation; however, the highlighting
results of mismatched pairs Texhibit more signif-
icant improvement. This suggests that the proposed
domain-adaptive approach addresses domain shift
and yields a superior ability to infer word impor-
tance even for unfamiliar (unseen) relationships
(See Appendix E also).
5.5 Ablation Studies
5.5.1 Impact of Referenced Sources
We first determined the impact of the reference seg-
ment, which is viewed as the context of a given
target segment in terms of discovering the financial
signals in the target segment. To this end, for each
reference-to-target pair (r, t), we substituted the
original reference segment r(i.e., the most syntac-
tically similar segment in the previous years’ doc-
ument D) for other text and constructed a few
variants of variant-to-target segment pairs for in-
ference using the highlighting model. Specifically,
we fixed the target segment but recast the BERT
contextualized representation of variant pairs as
•Empty : A single [PAD] token is used as the
reference segment (implying none in BERT);
•Same : The target segment is used as the refer-
ence segment;14313
•Random : A randomly selected segment is
used as the reference segment.
In Table 4, the original setup significantly outper-
forms the other three settings in both FINAL and
e-SNLI, showing that the knowledge provided by
the reference segments is critical for capturing im-
portant financial signals in the corresponding target
segment.
5.5.2 Effect of Lexicon-based labeling
Recall that in Section 3.4, we introduced a heuris-
tic pseudo-labeling approach that views all revised
words in target segment tas important words and
marks them as positive while we randomly sample
other words as negative words. We here test the
effect of additionally incorporating an external fi-
nancial lexicon for pseudo-labeling. Specifically,
we adopt the most representative financial senti-
ment lexicon—the Loughran–McDonald Master
Dictionary (Loughran and Mcdonald, 2011)—and
assume that in addition to the revised words in the
heuristic approach, the 3,872 sentiment words in
the dictionary also reveal important financial sig-
nals (i.e., are labeled as positive). Additionally, we
treat the 20K most frequently-occurring words, as
well as the standard stopwords, as negative words.
As shown in Table 5, surprisingly, adding the lex-
icon for pseudo-labeling does not improve perfor-
mance but instead worsens the highlighting results.
Although these financial sentiment words convey
important financial signals, they are globally im-
portant among all financial reports. However, this
characteristic precludes the use of the lexicon for
company-specific reference-to-target highlighting,
which is focused more on local relationships be-
tween a pair of segments.
6 Related Work
Research on financial report analysis has been on-
going for many years, with various studies utilizing
both textual and numerical features to identify sig-
nals in reports. For instance, some researchers
have used the relationship between tokens and
quantitative indicators from the financial market
to identify financial risks (e.g., Kogan et al., 2009;
Tsai and Wang, 2017; Lin et al., 2021). Others
have adapted unsupervised methods to recognize
information and classify risk factors in financial
reports (e.g., Huang and Li, 2011; Lin et al., 2011).
However, previous research has mostly focused on
risk factors in a global context rather than company-
specific signals, which is the focus of this study.
Recently, transformer-based language models
such as BERT, GPT-3, and T5 (Devlin et al., 2019;
Brown et al., 2020; Raffel et al., 2020) have made
significant strides in the summarization task. In
2020, Zmandar et al. (2021a) proposed the Finan-
cial Narrative Summarization shared task (FNS
2020), which aims to summarize annual UK reports.
While some methods for this task have achieved
satisfactory performance using ROUGE as a met-
ric (e.g., Zmandar et al., 2021b; Orzhenovskii,
2021; Gokhan et al., 2021), they have been crit-
icized for sometimes omitting essential signals un-
der a ROUGE-guided policy. Additionally, the sig-
nals discovered through these approaches are heav-
ily dependent on high-quality human-annotated
summaries, making it challenging to apply them in
real-world scenarios.
In the field of NLP, some research has focused
on developing rationalizing models related to the
concept of our highlighting model. For example,
Lei et al. (2016) proposed a method for learning
the rationale (words) to justify a model’s prediction
by selecting a subset of text inputs. More recently,
some studies have proposed methods that can ra-
tionalize the relationship of sentence pairs, such as
natural language inference (Jiang et al., 2021) and
query-document relevance (Kim et al., 2022). Ad-
ditionally, DeYoung et al. (2020) released a bench-
mark to facilitate the development of interpretable
NLP models with faithfulness.143147 Conclusion
This paper addresses the task of identifying ratio-
nales as insightful financial signals between two
narrative financial reports in consecutive years. We
use the reference-to-target structure of financial re-
ports to develop a compare-and-contrast multistage
pipeline, comprising mainly of relation recognition
and signal highlighting stages. In particular, we
propose domain-adaptive learning strategies for the
financial signal highlighting task, including out-of-
domain warm-up and in-domain fine-tuning. Our
empirical results confirm the effectiveness of the
proposed approaches. We also present the newly
constructed FINAL dataset.
Some future works include (a) improving high-
lighting effectiveness by developing multitask
learning on large financial corpora as financial pre-
trained representations; (b) increasing efficiency
by integrating dense retrieval methods in the Rela-
tion Recognition stage; (c) analyzing broader rela-
tionships beyond the year-to-year ones (e.g., cross-
company); (d) identifying important words in both
reference segments and target segments (i.e., two-
way rationalization), which may provide a more
in-depth financial analysis. And we believe this
research can facilitate NLP techniques applied in
finance domain.
8 Limitations
We identify crucial financial signals in reports
which can help financial practitioners to digest long
financial documents efficiently. However, factors
such as macroeconomics, stock prices, and pub-
lic policies may affect how a financial practitioner
views financial reports in practice. Confidential
intelligence or social media may greatly affect the
analysis results. Therefore, we limit our task to
the scenario in which the content in the reports is
the sole information available to users. Accord-
ingly, to prevent bias in the annotation process, we
acquire annotations from annotators under similar
scenarios (graduate students majoring in account-
ing or other related fields) rather than from financial
professionals. In addition, language partially con-
strains our methods since the data we used in stage
Sis in English; adding a machine translation mod-
ule may have sub-optimal effectiveness of financial
signal highlighting. This is mainly because the
financial signals highly depend on many language-
specific knowledge or country regulations.Acknowledgements
We would like to thank the anonymous reviewers
for the reviews. The work was supported by the
grants from the National Science and Technology
Council (Grant number NSTC 111-3111-E-002-
003 and MOST 111-2622-8-002-028-111C2415),
National Taiwan University (Grant number NTU-
CC-112L891104), and NTU IoX Center (Grant
number 109-3111-8-002-002-09HZA27003F and
MOST 111-2622-8-002-028-111C2415).
References143151431614317
A Training Detail
All model fine-tuning and inference (in Section 5
and Section 5.5) were conducted on an NVIDIA
Tesla V100 32GB GPU. Each model fine-tuning
can be done within three hours. We also ran all of
the models with shared training settings, including
the number of training steps, optimizers, and token
batch sizes; we set other related training parameters
as the settings in Huggingface Trainer.
B Hyperparameter Search
Recall that while the hyperparameter τin Eq. (2)
controls the probability distribution of the word
importance, γin Eq. (3)controls the impact of soft
labeling. Figure 3 shows the performance in terms
ofR-Prec with different hyperparameter settings,
where the left panel shows the results of τfixed
at 1 with γranging from 0 to 1, and the right panel
shows that of γfixed at 0.1 with τranging from 0.1
to 2. In the left panel of the figure, on FINAL,
we see that solely adopting cross-entropy loss L
(γ= 1) is not effective for fine-tuning the signal
highlighting model, nor is adopting KL loss L
(γ= 0) (see Eq. (3));γ= 0.1achieves the best
R-Prec. These empirical results again validate the
effectiveness of the proposed soft labeling for our
highlighting task. In addition, we froze γat 0.1
and experimented with different settings for the
temperature parameter τ, the results of which are
shown in the right panel of Figure 3, showing that
τ= 2 is the most effective setting. We thus set
our final hyperparameters to τ= 2andγ= 0.1to
yield the best performance.
C Empirical Thresholds
For the relation recognition procedure in S(see
Section 3.2 and Figure 4), we empirically set the
thresholds ϵ= 0.6296 andϵ= 0.9011 .
Both numbers are the 50 percentiles of the cor-
responding similarity scores calculated from the re-
duced segment pair set ( T). Note that, in this work,
we adopt a rule-based heuristic method for rec-
ognizing relations using similarity functions with
hard thresholds. We leave the exploration of other
similarity functions, thresholds, and approaches to
future work.
D Annotation Guidelines
For each segment pair, the annotators were to focus
on the semantic difference regarding the reference-
to-target relationship and annotate words in the
target segment as positive when the words were
considered important financial signals. The fol-
lowing guidelines were given for the annotators’
reference.
•Changes: Changing numbers or objects are
important signals in financial reports (e.g.,
sales, cost, partnership, products, etc.).
•Opposition: Descriptive phrases that indi-
cate distant semantic meanings (e.g., in-
creased/decreased, effective/ineffective, etc.).
•Precise: Labeling words with high confi-
dence as positive only (i.e., leaving ambiguous
words as negative).
•Extra information: Identifying new informa-
tion according to the context, for which the an-
notators considered the reference segment as
the context (e.g., new policy, canceled deals,
newly published products, etc.).
E Empirical Cases
In Table 6, we take few revised segment pairs ( T)
and mismatched segment pairs ( T) as examples.
The underlined words are with the top- khighest
importance predicted by the proposed pipeline.1431814319ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 8.
/squareA2. Did you discuss any potential risks of your work?
We have checked and discussed the ethical policies; we didn not ﬁnd potential risks in our work.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1.
/squareA4. Have you used AI writing assistants when working on this paper?
We used Grammarly to check if there are any misused grammar, typos in very beginning manuscripts.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Section 3 (spaCy, e-SNLI dataset); Section 4(Software Repository for Accounting and Finance(SRAF))
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We will update the license or terms for use of our released dataset if we open-sourced our codes and
data (they are in anonymous repository now.) According to Software Repository for Accounting and
Finance(SRAF), all software and data are provided without warranties, and are for non-commercial
purposes.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 3.3 (the detail settings of our intended use of external data.); Section 4(our created dataset).
We will update the license or terms for use of our released dataset if we open-sourced our codes and
data (they are in anonymous repository now.)
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We used the original corpus from SRAF without any revisions or other related resources. We believe
the contents in this data do not contain the sensitive information about ethical issues.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
To our knowledge, the raw corpus we used is from 10-K ﬁnancial report which is regulated by U.S.
SEC; therefore, the data is covered in ﬁnancial domain and in English only.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4.14320C/squareDid you run computational experiments?
Section 5.4 (main results); Section 5.5 (ablation studies).
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix A (training detail).
/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix B (hyperparameters search).
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
In section 5, we reports the experimental results; we also report the p-value of pair t-test in the
results table.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
We cite the URL of spacy’s API we used. ROUGE setup is the default setting from original paper.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4.3.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix D.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 4.3 (human annotation); Section 8 (limitation).
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
We have informed and acquired their approval for the further usage in this paper. The annotators
were agreed and awared of the purpose of our work. We have also acquired their approval for
releasing the dataset with their annotations.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
We have checked and followed the ethical policies in the process of this work.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Section 8.14321