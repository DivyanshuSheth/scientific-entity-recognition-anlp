
Weicheng Ma,Brian Wang,Hefan Zhang,Lili Wang,
Rolando Coto-Solano,Saeed Hassanpour, and Soroush VosoughiDepartment of Computer Science, Dartmouth CollegeDepartment of Linguistics, Dartmouth CollegeDepartment of Biomedical Data Science, Dartmouth Collegeweicheng.ma.gr@dartmouth.edusoroush.vosoughi@dartmouth.edu
Abstract
Syntactic probing methods have been used
to examine whether and how pre-trained lan-
guage models (PLMs) encode syntactic rela-
tions. However, the probing methods are usu-
ally biased by the PLMs’ memorization of com-
mon word co-occurrences, even if they do not
form syntactic relations. This paper presents
a random-word-substitution and random-label-
matching control task to reduce these biases
and improve the robustness of syntactic prob-
ing methods. Our control tasks are also shown
to notably improve the consistency of probing
results between different probing methods and
make the methods more robust with respect to
the text attributes of the probing instances. Our
control tasks make syntactic probing methods
better at reconstructing syntactic relations and
more generalizable to unseen text domains. Our
experiments show that our proposed control
tasks are effective on different PLMs, probing
methods, and syntactic relations.
1 Introduction
To explain the high performance of PLMs on var-
ious natural language processing (NLP) tasks, ef-
forts have been made to examine the syntactic
relation-encoding ability of these models. For ex-
ample, Manning et al. (2020) attempt to recon-
struct syntactic relations from the attention heads
of Transformer models (Vaswani et al., 2017) us-
ing raw attention scores. Leave-one-out probing
methods (Brunner et al., 2020), instead, measure
the influence of ablating parts of each syntactic re-
lation on the hidden representations of the models.
However, the probing results may not faithfully
reflect the encoding of syntactic relations as the
memorization of common word co-occurrences in
the training data of PLMs can lead to incorrect
and non-generalizable probing results (Hewitt and
Liang, 2019). We observe the same issues in our
experiments, where many highly-ranked attentionFigure 1: Top: An instance labeled with the correct
“subject” dependency relation (Positive); Middle: the in-
stance generated by Random-Word-Substitution where
the instance is labeled with the correct pair of words but
incorrect word form; Bottom: the instance generated by
Random-Label-Matching where the instance is labeled
with an incorrect pair of words. The head verb is in blue
and the dependent is in red for all the examples.
heads by the attention-as-classifier and leave-one-
out probing methods highlight frequent word pairs
regardless of whether there is a syntactic relation
between them. This reduces the trustworthiness
of the probing methods and any model interpreta-
tion that relies on them. To address this issue and
improve the correctness, robustness, and general-
izability of existing probing methods, we design
two control tasks to reduce the adverse effects of
the PLMs’ memorization of word co-occurrences.
Therandom-word-substitution control task sub-
stitutes one component word (i.e., the head or de-
pendent words) of each syntactic relation with its
other forms to make the text ungrammatical. The
random-label-matching control task randomly402matches one component word of each syntactic
relation with a random irrelevant word in the sen-
tence to make the syntactic-relation labels incorrect.
Figure 1 shows examples for each control task. The
control instances (i.e., negative instances) are gen-
erated automatically by substituting words or labels
of instances in the positive datasets.
By down-weighting the attention heads that are
ranked highly by the probing methods on the con-
trol tasks, we observe notably more consistent prob-
ing results between the attention-as-classifier and
leave-one-out methods on the BERT (Devlin et al.,
2019) and RoBERTa (Liu et al., 2019) models,
with improvements above 0.1 for the Spearman’s
rank correlation coefficients (Spearman’s ρ)..
The layer-wise distributions of top-ranked attention
heads also become notably more consistent across
different text attributes of the probing instances.
The results demonstrate the effectiveness of our
proposed control tasks for improving the quality
and robustness of syntactic probing methods.
2 Syntactic Probing Methods
Different families of probing methods rely on dif-
ferent assumptions (Belinkov and Glass, 2019) and
as such, probing results from different families can-
not be meaningfully compared. Hence, we exam-
ine two probing methods that are both based on
attention distributions: (1) Given a sentence and
a headword for a syntactic relation, the attention-
as-classifier method (Manning et al., 2020) pre-
dicts another word as the dependent if it puts the
highest attention score on the headword; (2) As an
attention-based version of the leave-one-out prob-
ing method used by Meister et al. (2021), we mask
the headword of each syntactic relation for each
sentence and predict the word whose attention dis-
tribution changes the most as the dependent word.
Following Kobayashi et al. (2020), we additionally
examine two variant methods, norm-as-classifier
andleave-one-out-norm methods which predict
the dependent words based on the distributions or
changes of attention norms, respectively. We calcu-
late the importance of each attention head for en-
coding each syntactic relation by evaluating the top-
3 accuracy (ACC@3) of the predictions; defined
as the percentage of instances where the dependent
words from the ground truth are ranked among the
top-3 in the predictions. We use ACC@3 sincein many cases, the highest attention scores fall on
separator tokens such as “[SEP]” and punctuation
marks (Clark et al., 2019a).
3 Probing Datasets
We use the “subject” (subj), “object” (obj), “nom-
inal modifier” (nmod), “adverbial modifier” (adv-
mod), and “coreference” (coref) relations in our
analyses. We use the English dataset for the
CoNLL-2009 shared task (Haji ˇc et al., 2009) to
construct our positive and control probing datasets.
Figure 1 shows an example instance from the posi-
tive dataset and each control dataset.
3.1 Positive Datasets
Our positive dataset for each syntactic relation con-
tains the correct annotations of words that make
up the syntactic relation, e.g., the subject words
and the corresponding verbs for “subj”. The gold-
standard dependency annotations in the CoNLL-
2009 dataset are used for the “subj”, “obj”, “nmod”,
and “advmod” relations and the SpanBERT model
(Joshi et al., 2020) is used to annotate the “coref”
relation.
3.2 Random-Word-Substitution Control
If an attention head in a Transformer model en-
codes a specific syntactic relation, it should not
highlight the connections between words that do
not form that syntactic relation. To measure and
control for this effect, we construct the random-
word-substitution control dataset by substituting
one component word of the syntactic relation in
each instance of the positive datasets with another
part of speech of the same word (e.g., changing a
verb to its noun form) to make the instance ungram-
matical but not greatly change its semantics. We
use the Language Tool, a grammar correction tool,
to verify that the sentences become ungrammatical
after word substitution.
3.3 Random-Label-Matching Control
We also extend the existing method of the random
control task (Hewitt and Liang, 2019) to construct
the random-label-matching control dataset. Specif-
ically, for each instance in our positive datasets, we
use our gold-standard labels and coreference labels
generated by SpanBERT to remove word pairs that403are syntactically related, leaving us with words that
are not syntactically related. These words are then
used to create syntactically unrelated pairs by com-
bining known head words with randomly selected
dependent words. We then (intentionally) mislabel
each pair as forming a specific syntactic relation,
depending on the positive dataset from which the
instance was taken. Attention heads that encode
the relations between these syntactically unrelated
word pairs are likely memorizing the co-occurrence
of frequent word pairs without regard to syntactic
correctness and thus should not be ranked highly
by syntactic probing methods.
4 Experimental Results
We conduct three sets of experiments to exam-
ine our probing methods’ sensitivity to “spurious”
word correlations (Section 4.1), consistency (Sec-
tion 4.2), and robustness to text attributes (Section
4.3). We run the experiments using the BERT-base
and RoBERTa-base models for generality. All the
experiments are run on an Nvidia RTX-6000 GPU.
4.1 Syntactic Relation Reconstruction
We follow Manning et al. (2020) to evaluate the cor-
rectness of attention-head rankings produced by the
probing methods via syntactic relation reconstruc-
tion experiments. Specifically, for a given head-
word, we use the attention scores (for attention-
as-classifier) or norms (for norm-as-classifier) be-
tween that headword and all other words in the
instance to predict the dependent word. Similarly,
We use the distribution changes of the attention
scores (for leave-one-out) or norms (for leave-one-
out-norm) when the headword is masked to predict
the dependent word. Contributive attention heads
for encoding a particular syntactic relation should
achieve high syntactic-relation reconstruction per-
formance (in ACC@3) given syntactically correct
(positive) labels and low performance given incor-
rect (negative/control) labels.
We use the left-out development set of the
CoNLL-2009 dataset (labeled using the ground-
truth annotations and SpanBERT) as one positive
probing dataset (pos-main) and the correspond-
ing random-word-substitution and random-label-
matching control instances as two negative datasets.
We construct an additional positive probing dataset
(pos-uncommon) by substituting the dependent
words with other words that have the same part
of speech but rarely co-occur (<5 times) with thecorresponding headwords in the English Wikipedia
corpus. This dataset enables us to study the effect
of co-occurrence for syntactically related pairs of
words on the syntactic relation reconstruction task.
We use the English Wikipedia corpus as it is rep-
resentative of the data used to pre-train BERT and
RoBERTa. All the evaluations are conducted on
the top-5 attention heads according to each probing
method (with and without control tasks), and the
scores are averaged across syntactic relations and
heads.
Results show that applying our proposed control
tasks does not harm the syntactic-relation recon-
struction performance of the four probing meth-
ods on the pos-main dataset. In contrast, apply-
ing the random control task (Hewitt and Liang,
2019) occasionally leads to a performance drop
of 1.32. This suggests that our proposed control
tasks are more robust than the existing random con-
trol task. On the pos-uncommon dataset, our pro-
posed control tasks lead to an average increase of
9.17±0.13(BERT) and 4.07±0.15(RoBERTa) in
the syntactic-relation reconstruction performance.
Additionally, the control tasks on average reduce
the incorrect prediction of syntactic relations in our
two negative datasets by 11.70±0.09(BERT) and
12.69±0.06(RoBERTa). These results suggest
that our proposed control tasks can reduce the influ-
ence of the PLMs’ memorization of syntactically-
irrelevant word co-occurrences for encoding syn-
tactic relations. The complete results of these ex-
periments are shown in Appendix A.
4.2 Consistency of Attention-Head Rankings
We also observe that our control tasks lead to higher
consistency between the two categories of probing
methods. Without any control task, the Spearman’s
ρbetween the head rankings produced by the four
probing methods are always lower than 0.38 (for
BERT) and 0.49 (for RoBERTa), while applying
the control tasks improves the consistency from
a minimum of 0.10 to 0.79 (for BERT) and 0.14
to 0.53 (for RoBERTa), in Spearman’s ρ. Further-
more, the highest consistency improvements are
achieved when applying both our random-word-
substitution and random-label-matching control
tasks. Applying the random control task indepen-
dently or jointly with our two control tasks does
not lead to higher consistency improvements. The
complete results of these experiments are shown in404
Appendix B.
Prior work has shown that only a small focused
set of heads contributes to the encoding of each
linguistic feature (Michel et al., 2019; V oita et al.,
2019), and as such, a good probing method should
highlight these select contributive heads. Figure 2
shows the percentage of attention heads in common
among the top-k heads ( 1≤k≤144) between
each pair of probing methods, either with or with-
out control tasks. We find that applying the control
tasks generally improves the agreement between
attention-head rankings, with the effect being more
pronounced for the top 15% of the heads, i.e., the
attention heads that are deemed the most impor-
tant for encoding each syntactic rule. These results
show that our control tasks aid the probing methods
in highlighting the small set of contributive heads.
4.3 Robustness to Text Attributes
The literature suggests that most contributive at-
tention heads for encoding syntactic relations lie
on the middle layers of Transformer models (He-
witt and Manning, 2019; Vig and Belinkov, 2019;Goldberg, 2019; Jawahar et al., 2019; Clark et al.,
2019b). Consequently, the layer-wise distribution
of the attention heads ranked highly by a robust
syntactic probing method should follow a similar
pattern and not be greatly affected by the variation
in the text attributes.
We divide the pos-main dataset into nine sub-
sets with different sentence lengths ( <20tokens,
20−30tokens, and >30tokens), numbers of
clauses (1, 2, and >2clauses), and distances be-
tween the head and dependent words ( 1−2tokens,
3−5tokens, and >5tokens). The parameters
for each of the attributes were selected to create
a relatively uniform distribution of sentences for
each of the datasets for a given attribute. We repeat
all the experiments with the attention-as-classifier
and leave-one-out probing methods on these nine
datasets. The layer-wise distributions of top-5 at-
tention heads for each probing method (aggregated
for the five syntactic relations) are shown in Figure
3. We show the results for the two probing methods
with both our combined control tasks and without
any control.405
We note that the overall trend (represented by
the blue line in each figure) shows that the top-
ranked attention heads are over-represented on the
middle layers, either with or without control tasks.
This is well-aligned with the literature, suggest-
ing that the most contributive attention heads for
encoding syntactic relations (i.e., middle layers)
are identified by the probing methods even with-
out any control tasks (Hewitt and Manning, 2019;
Vig and Belinkov, 2019; Goldberg, 2019; Jawa-
har et al., 2019). However, the probing methods
without control tasks also put high weights on the
low-level layers (below Layer 2) more frequently
than those with control tasks. We speculate the
cause to be the sensitivity of the probing methods
(without control tasks) to the memorization of com-
mon word co-occurrences on each attention head;
since the lower-layer attention heads are closer to
the embedding layer, they usually encode richer
lexical features (Limisiewicz and Mare ˇcek, 2021).Our claim is further supported by the observation
that there is greater variation in the attention-head
rankings between the individual probing results for
each of the nine attributes when no control is used.
This can be visually observed in Figure 3 by com-
paring the deviation between different colored bars
(corresponding to different attributes) on the left
and right figures, corresponding probing without
and with controls, respectively. We additionally
measure this difference in variation quantitatively
by examining the consistency of the attention-head
rankings over the entire 144 heads for individual
probing results for each of the nine attributes. The
Spearman’s ρof the rankings between all settings
(i.e., using the entire development set or any of the
nine subsets) range from 0.75 to 0.96 when using
the combination of the random-word-substitution
and random-label-matching control tasks. In com-
parison, Spearman’s ρof the rankings between the
settings drops to 0.22 and 0.38 when no control
task is applied and between 0.51 and 0.60 when
the random control task is used. These experiments
suggest that our proposed control tasks can improve
syntactic probing methods’ robustness and reduce
syntactic probing methods’ fragility to the models’
memorization of common word co-occurrences.
5 Conclusion and Future Work
This paper proposes two control tasks to improve
the syntactic probing of PLMs and reduce the noise
in the probing results of the PLMs’ memorization
of common word co-occurrences. By applying
these control tasks, we observe notable improve-
ments in the correctness and consistency of the
results produced by four attention-based probing
methods across two categories of five diverse syn-
tactic relations. The improvements are also robust
to different PLMs’ and attributes of the probing
instances, suggesting the general applicability of
our proposed control tasks.
Future work can expand the use of our proposed
control tasks to other models or syntactic relations.
Acknowledgement
This work was partially funded by Dr. V osoughi’s
2022 Google Research Award.
Limitations
While our study provides promising results in re-
ducing biases and improving the robustness of syn-406tactic probing methods, there are some limitations
that must be discussed:
First, our experiments only utilized attention-
based probing approaches, and it is unclear whether
our results would generalize to other families of
probing methods. Therefore, further investigation
is needed to determine the effectiveness of our con-
trol tasks for other types of probing methods. Sec-
ond, we only explored a subset of syntactic rela-
tions in English, including subject, object, nominal
modifier, adverbial modifier, and coreference. Our
results may not be generalizable to other syntactic
relations or languages. Future studies could expand
the exploration of other syntactic features and in-
vestigate the effectiveness of our control tasks in
different languages. Third, our experiments only fo-
cused on two pre-trained language models, namely
BERT and RoBERTa. It is unclear whether our
control tasks would be effective for other types of
PLMs, and further studies could investigate the ef-
fectiveness of our control tasks on other types of
PLMs. Finally, our study only focused on syntactic
probing methods and did not investigate probing
methods for other types of NLP tasks, such as nat-
ural language inference, machine translation, and
summarization. Therefore, further studies could
explore the effectiveness of our control tasks on
other types of NLP tasks.
Despite these limitations, our proposed control
tasks have shown promising results in reducing
biases and improving the robustness of syntactic
probing methods, and we hope that our work will
inspire further research in this direction.
Ethics Statement
This paper used publicly available pre-trained mod-
els (bert-base-cased and roberta-base models and
the SpanBERT model) and a publicly available
dataset (CoNLL-2009). No sensitive information is
introduced to the data annotations or experiments.
Also, we only examine the ways pre-trained lan-
guage models encode general syntactic relations,
which should not introduce stereotypes or biases
into our results and analyses. We do not foresee any
potential ethical concerns in our work. However,
we should note that our work is limited to English
syntactic relations and should not be generalized to
other languages without additional experiments.References407408A syntactic relation Reconstruction
Results
We display in Tables A1 - A4 the average syn-
tactic relation reconstruction performance on the
top-5 attention heads produced by each probing
method for the five syntactic relations (“subj”,
“obj”, “nmod”, “advmod”, and “coref”) on the pos-
main, pos-uncommon, random-word-substitution,
and random-label-matching datasets, respectively.
B The Inconsistency Across Probing
Methods
The attention-head rankings produced by different
probing methods are inconsistent when no control
task is applied. As Figure B1 shows, the Spear-
man’s ρbetween each pair of probing methods
are always lower than 0.38 for BERT and below
0.49 for RoBERTa, which falls under the “weak
to moderate correlation” range given the interpre-
tation of Akoglu (2018). As shown in Figures B2
and B3, by applying the random-word-substitution
or the random-label-matching control tasks, Spear-
man’s ρacross probing methods improve greatly,
in some cases yielding Spearman’s ρabove 0.7
(i.e., “very strong” correlations). Though not as ef-
fective as our proposed control tasks, applying the
random control task also improves the consisten-
cies of attention-head rankings across the probing
methods.
As shown in the figures, combining our two con-
trol tasks generates the most consistent results for
all four probing methods.409410411412413ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitation.
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3.
/squareB1. Did you cite the creators of artifacts you used?
Section 3.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
All the datasets we use are publicly available, and they are cited in Section 3.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
sets we use are publicly available, and they are cited in Section 3.
C/squareDid you run computational experiments?
Section 4.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 4.414/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Not applicable. We conducted probing experiments that do not require training or hyperparameter
search.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 3.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.415