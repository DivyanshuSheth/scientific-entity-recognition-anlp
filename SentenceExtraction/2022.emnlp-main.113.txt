
Zhen WanQianying Liu
Zhuoyuan MaoFei ChengSadao KurohashiJiwei LiKyoto University, JapanZhejiang University, China
{zhenwan, ying, zhuoyuanmao}@nlp.ist.i.kyoto-u.ac.jp
{feicheng, kuro}@i.kyoto-u.ac.jp
{jiwei_li}@zju.edu.cn
Abstract
Relation extraction (RE) has achieved remark-
able progress with the help of pre-trained lan-
guage models. However, existing RE models
are usually incapable of handling two situa-
tions: implicit expressions and long-tail rela-
tion types, caused by language complexity and
data sparsity. In this paper, we introduce a sim-
ple enhancement of RE using knearest neigh-
bors ( kNN-RE). kNN-RE allows the model to
consult training relations at test time through a
nearest-neighbor search and provides a simple
yet effective means to tackle the two issues
above. Additionally, we observe that kNN-
RE serves as an effective way to leverage dis-
tant supervision (DS) data for RE. Experimen-
tal results show that the proposed kNN-RE
achieves state-of-the-art performances on a va-
riety of supervised RE datasets, i.e., ACE05,
SciERC, and Wiki80, along with outperform-
ing the best model to date on the i2b2 and
Wiki80 datasets in the setting of allowing us-
ing DS. Our code and models are available at:
https://github.com/YukinoWan/kNN-RE.
1 Introduction
Relation extraction (RE) aims to identify the rela-
tionship between entities mentioned in a sentence,
and is beneficial to a variety of downstream tasks
such as question answering and knowledge base
population. Recent studies (Zhang et al., 2020;
Zeng et al., 2020; Lin et al., 2020; Wang and Lu,
2020; Cheng et al., 2020; Zhong and Chen, 2021)
in supervised RE take advantage of pre-trained lan-
guage models (PLMs) and achieve SOTA perfor-
mances by fine-tuning PLMs with a relation classi-
fier. However, we observe that existing RE models
are usually incapable of handling two RE-specific
situations : implicit expressions andlong-tail re-
lation types .
Implicit expression refers to the situation where
a relation is expressed as the underlying messageFigure 1: Left: the retrieved example has a similar struc-
ture but with the phrase “younger brother”, it becomes
easier to infer. Right : Referring to the gold labels
of nearest neighbors can reduce the bias. Highlighted
words may directly influence on the relation prediction.
that is not explicitly stated or shown. For exam-
ple, for the relation “sibling to”, a common expres-
sion can be “ Hehas a brother James ”, while an
implicit expression could be “He is the youngest
son of Liones, comparing with Samuel Liones
and Henry Liones .” In the latter case, the rela-
tion “sibling to” between “ Samuel Liones ” and
“Henry Liones ” is not directly expressed but could
be inferred from them both are brothers of the same
person. Such underlying message can easily con-
fuse the relation classifier. The problem of long-
tail relation types is caused by data sparsity in
training. For example, the widely used supervised
RE dataset TACRED (Zhang et al., 2017) includes
41 relation types. The most frequent type “per:title”
has 3,862 training examples, while over 22 types
have less than 300 examples. The majority types
can easily dominate model predictions and lead to
low performance on long-tail types.
Inspired by recent studies (Khandelwal et al.,
2020; Guu et al., 2020; Meng et al., 2021) us-
ingkNN to retrieve diverse expressions for lan-
guage generation tasks, we introduce a simple but
effective kNN-RE framework to address above-
mentioned two problems. Specifically, we store the
training examples as the memory by a vanilla RE
model and consult the stored memory at test time1731
through a nearest-neighbor search. As shown in
Figure 1, for an implicit expression , the expres-
sion “son of” may mislead to an incorrect predic-
tion while its retrieved nearest neighbor contains a
direct expression “brother of”, which is a more ex-
plicit expression of the gold label “sibling to”. The
prediction of long-tail examples, as shown in Fig-
ure 1, is usually biased toward the majority class.
Nearest neighbor retrieval provides direct guidance
to the prediction by referring to the labels of its
nearest neighbors in the training set, and thus can
significantly reduce the imbalanced classification.
Additionally, we observe that kNN-RE serves
as an efficient way to leverage distant supervision
(DS) data for RE. DS augments labeled RE datasets
by matching knowledge base (KB) relation triplets
and raw text entity pairs in a weak-supervision fash-
ion (Mintz et al., 2009; Lin et al., 2016; Vashishth
et al., 2018; Chen et al., 2021). Recent studies (Bal-
dini Soares et al., 2019; Ormándi et al., 2021; Peng
et al., 2020; Wan et al., 2022), which apply PLMs
to the DS labeled data to improve supervised RE,
require heavy computation due to the fact that they
require pre-training on DS data, whose size is usu-
ally dozens of times that of supervised datasets.
To address this issue, we propose a lightweight
method to leverage DS data to benefit supervised
RE by extending the construction of stored memory
forkNN-RE to DS labeled data and outperforming
the recent best pre-training method with no extra
training.
In summary, we propose kNN-RE: a flexible
kNN framework to solve the RE task. We con-
duct the experiments for kNN-RE with three dif-
ferent memory settings: training, DS, and the com-
bination of training and DS. The results show that
ourkNN-RE with the training memory obtains
a 0.84%-1.15% absolute F1 improvement on five
datasets and achieves state-of-the-art (SOTA) F1
scores on three of them (ACE05, SciERC and
Wiki80). In the DS setup, kNN-RE outperforms
SOTA DS pre-training methods on two datasets
(i2b2, Wiki20) significantly without extra training.
2 Methodology
2.1 Background: Vanilla RE model
For the vanilla RE model, We follow the recent
SOTA method PURE (Zhong and Chen, 2021). To
encode an input example to a fixed-length represen-
tation by fine-tuning PLMs such as BERT (Devlin
et al., 2019), PURE adds extra marker tokens to
highlight the head and tail entities and their types.
Specifically, given an example x: “Hehas
a brother James .”, the input sequence is “ He has a brother James . ” where “PER” is the entity type if1732
provided. Denote the n-th hidden representation of
the BERT encoder as h. Assuming iandjare the
indices of two beginning entity markers
and , we define the relation representation
asx=h⊕hwhere ⊕stands for concatenation.
Subsequently, this representation is fed into a lin-
ear layer to generate the probability distribution
p(y|x)for predicting the relation type.
2.2 Proposed Method: kNN-RE
Training Memory Construction For the i-th
training example (x, r), we construct the key-
value pair (x, r)where the keyxis the relation
representation obtained from the vanilla RE model
and the value rdenotes the labeled relation type.
The memory (K,V) ={(x, r)|(x, r)∈ D} is
thus the set of all key-value pairs constructed from
all the labeled examples in the training set D.
DS Memory Construction In this paper, with
the awareness of the unique feature of RE to gen-
erate abundant labeled data by DS, we extend our
method by leveraging DS examples for memoryconstruction. Similar to training memory construc-
tion, we build key-value pairs for all the DS labeled
examples with the vanilla RE model.
Inference Given the test example x, the RE
model outputs its relation representation xand gen-
erate the relation distribution p(y|x)between
two mentioned entities. We then query the memory
withxto retrieve its knearest neighbors Naccord-
ing to a distance function d(., .)byLdistance
with the KBF kernel. We weight retrieved exam-
ples by a softmax function on the negative distance
and make an aggregation on the labeled relation
types to predict a relation distribution p(y|x):
p(y|x)∝/summationdisplay1exp(−d(x,x))
T
(1)
whereTdenotes a scaling temperature. Finally, we
interpolate the RE model distribution p(y|x)and
kNN distribution p(y|x)to produce the final
overall distribution:
p(y|x) =λp(y|x) + (1 −λ)p(y|x)
(2)
where λis a hyperparameter.
3 Experiment settings
Supervised Datasets We evaluate our proposed
method on five popular RE datasets. Table 1 shows
the statistics. ACE05 and TACRED datasets are
built over an assortment of newswire and online
text. Wiki80 (Han et al., 2019) is derived from
Wikipedia crossing various domains. The i2b21733
2010V A dataset collects medical reports while Sci-
ERC (Luan et al., 2018) collects AI paper abstracts
and annotated relations, specially for scientific
knowledge graph construction.
DS Datasets We evaluate our DS memory con-
struction method on two supervised datasets
Wiki80 and i2b2 2010V A. For i2b2 2010V A, we
generate DS examples from MIMIC-III based on
triplets extracted from the training set. For Wiki80
dataset, as it is derived from the Wikidata KB, we
leverage the existing DS dataset Wiki20m derived
from the same KB which leads to shared relation
types.
Refer to Appendix B for implementation details.
4 Results and Analysis
4.1 Main results
Table 2 compares our proposed kNN-RE with
previous SOTA methods. For training memory
construction, we can observe that: (1) Our ap-proach outperforms vanilla RE model PURE on
all datasets and achieves new SOTA performances
on ACE05, Wiki80 and SciERC (2) By comparing
PURE and “ kNN only”, we find that the kNN pre-
diction itself has a performance improvement over
the vanilla RE model from +0.13on SciERC to
+1.07on ACE05. This leads to a large λforkNN.
For DS and combined memory construction, we
can observe that: (1) The best λbecomes smaller
while kbecomes larger which is reasonable as DS
suffers from the noise but still benefit the model
inference. (2) Compared with the previous best pre-
training method CP (Peng et al., 2020) that requires
huge computation, our kNN-RE on DS achieves
higher performance without extra training and is
further improved by combining with the training
memory to achieve a 1.62F1 score improvement
over PURE on the Wiki80.
Besides, we also compare performances on the
development set of two datasets as shown in Ta-
ble 3, the experiment results emphasize the consis-
tent improvement of our proposed methods.
4.2 Analysis
Case Study for Implicit Expressions We select
two typical test examples to better illustrate the
amendment by kNN retrieval as shown in Table 4.
For the first example, the implicit relation between
“maracanazo ” and “ 1950 world cup ” need to be in-
ferred by other contextual information and the RE
model makes an incorrect prediction as competi-
tion usually belongs to the object of the relation
“participant of” as the second retrieved examples.
However, the nearest example contains a simpler
expression and rectifies the prediction. Refer to
Appendix A for visualized analysis.
For the second example, the implicit expression
leads to another confusing relation "subsidiary"
while the nearest example captures the same struc-
ture that contains an "of" between two entities and
makes the final prediction easier.
Performance on Long-tail Relation Types We
check the performance kNN-RE with training
memory on several most long-tail relation types
from the TACRED dataset and show in table 5.
Note that all long-tail relation types benefit from
the effectiveness of kNN prediction except for “sta-
teorprovince of birth”, which contains only 8 test
examples leading to an unconvincing performance.1734
Retrieval Ability in Low-Resource Scenario
We also check the retrieval ability by varying the
percentage of the training set to constraint the rep-
resentation quality in the memory (Figure 3). We
can observe that with the decreasing number of the
training examples, our kNN-RE (training) tends
to achieve greater improvement even the train-
ing memory is also limited by the low resource.
Surprisingly, our kNN-RE (DS) achieves the F1-
score of 74.31(an improvement gap of 42.35over
PURE) with only 1%training examples provided,
which indicates that the model can still retrieve
accurate nearest neighbors from the DS memory.
We believe this is due to the modern PLMs have
learned robust representations during pre-training.
5 Conclusion
We propose kNN-RE: a flexible kNN framework
with different memory settings for solving implicit
expression and long-tail relation issues in RE. The
results show that our kNN-RE with training mem-
ory outperforms vanilla RE model and achieves
SOTA F1 scores on three datasets. In the DS setup,
kNN-RE also outperforms SOTA DS pre-training
methods significantly without extra training.
Limitations
In this paper, we use kNN-based strategy in the
inference stage to address the language complexity
and data sparsity problem. It is more challenging
for a model to learn the characteristics of these
examples. While our approach is light-weighted
and flexible, it cannot directly help the model to
improve the classification of the implicit expres-
sion examples or long-tail relation examples types
during the training stage. The representations of
these examples remain coarse-grained. Incorpo-
rating the kNN manner strategies in the trainingstage by providing additional nearest neighbor ref-
erences for the model could help the model learn
better representations of the examples, which we
leave as future work.
Acknowledgements
This work is partially supported by JST SPRING
Grant No.JPMJSP2110, MHLW PRISM Grant
Number 21AC5001, KAKENHI Number
21H00308 and KAKENHI Number 22J13719,
Japan.
References173517361737
A T-SNE Visualization
As shown in figure 4, the test example with the gold
label “part of” is incorrectly classified to another
relation “participant of”. However, with the help
of the 1st nearest example closer to the “part of”
clustering, kNN makes a correct prediction.
B Implementation Details
During the construction of DS data for i2b2
2010V A, we use the preprocessing tool NLTK to
split raw corpora into sentences. We use bert-base-
uncased (Devlin et al., 2019) as the base encoders
for ACE05, Wiki80 and TACRED for a fair com-
parison with previous work. We also use scibert-
scivocab-uncased (Beltagy et al., 2019) as the base
encoder for SciERC and BLUEBERT (Peng et al.,
2019) for i2b2 2010V A as in-domain PLMs are
more effective.
For the baseline PURE (Zhong and Chen, 2021),
we follow their single-sentence to keep consistency
among datasets as Wiki80 and TACRED are both
sentence-level RE datasets. For another baseline
CP (Peng et al., 2020), we modify their official
implementations to pre-train on our DS set. For
ourkNN-RE, we choose hyperparameters k,T
andλby greedy search where kis the power of
2from 2to256,λandTboth increase from 0to
1. All experiment results are the average of 3 to 5
times running.
We used 2 NVIDIA RTX3090 for training.1738