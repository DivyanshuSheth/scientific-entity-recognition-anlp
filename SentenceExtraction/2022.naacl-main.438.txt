
Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu,
Eric Xin Wang,Qi Wu,Miguel Eckstein,William Yang WangUC Santa Barbara,University of Adelaide,Google,UC Santa Cruz , ,
Abstract
Vision-and-language navigation (VLN) is a
multimodal task where an agent follows nat-
ural language instructions and navigates in vi-
sual environments. Multiple setups have been
proposed, and researchers apply new model
architectures or training techniques to boost
navigation performance. However, there still
exist non-negligible gaps between machines’
performance and human benchmarks. More-
over, the agents’ inner mechanisms for navi-
gation decisions remain unclear. To the best
of our knowledge, how the agents perceive the
multimodal input is under-studied and needs
investigation. In this work, we conduct a series
of diagnostic experiments to unveil agents’ fo-
cus during navigation. Results show that indoor
navigation agents refer to both object and direc-
tion tokens when making decisions. In contrast,
outdoor navigation agents heavily rely on di-
rection tokens and poorly understand the object
tokens. Transformer-based agents acquire a bet-
ter cross-modal understanding of objects and
display strong numerical reasoning ability than
non-Transformer-based agents. When it comes
to vision-and-language alignments, many mod-
els claim that they can align object tokens with
specific visual targets. We find unbalanced at-
tention on the vision and text input and doubt
the reliability of such cross-modal alignments.
1 Introduction
A key challenge for Artificial Intelligence (AI) re-
search is to move beyond Independent and Iden-
tically Distributed (i.i.d.) data analysis: We need
to teach AI agents to understand multimodal input
data, and jointly learn to reason and perform incre-
mental and dynamic decision-making with the help
from humans. Vision-and-Language Navigation
(VLN) has received much attention due to its ac-
tive perception and multimodal grounding setting,
dynamic decision-making nature, rich applications,
Table 1: There exists salient gaps between machines’
vision-and-language navigation (VLN) performance and
human benchmarks. Navigation success rates are re-
ported on the R2R (Anderson et al., 2018) and the RxR
dataset (Ku et al., 2020b) for indoor VLN and the Touch-
down dataset (Chen et al., 2019) for outdoor VLN.
and accurate evaluation of agents’ performances
in language-guided visual grounding. As the AI
research community gradually shifts its attention
from the static empirical analysis of datasets to
more challenging settings that require incremental
decision-making processes, the interactive task of
VLN deserves a more in-depth analysis of why it
works and how it works.
Various setups have been proposed to address to
the VLN task. Researchers generate visual trajec-
tories and collect human-annotated instructions for
indoor (Anderson et al., 2018; Jain et al., 2019a;
Ku et al., 2020a; Chen et al., 2021) and outdoor
environment (Chen et al., 2019; Mehta et al., 2020;
Mirowski et al., 2018). There are also interactive
VLN settings based on dialogues (Nguyen et al.,
2019; Nguyen and III, 2019; Zhu et al., 2020c), and
task that navigates agents to localize a remote ob-
ject (Qi et al., 2020c). However, few studies ask the
Why andHow questions: Why do these agents work
(or do not work)? How do agents make decisions
in different setups?
Through the years, agents with different model
architectures and training mechanisms have been
proposed for indoor VLN (Anderson et al., 2018;
Fried et al., 2018; Hao et al., 2020; Hong et al.,
2020a,b; Huang et al., 2019; Ke et al., 2019; Li
et al., 2019; Ma et al., 2019a; Qi et al., 2020b; Tan5981et al., 2019; Wang et al., 2020a, 2019, 2018, 2020b;
Zhu et al., 2020a) and outdoor VLN (Chen et al.,
2019; Ma et al., 2019b; Mirowski et al., 2018; Xia
et al., 2020; Xiang et al., 2020; Zhu et al., 2020b).
Back-translation eases the urgent problem of data
scarcity (Fried et al., 2018). Imitation learning and
reinforcement learning enhance agents’ generaliza-
tion ability (Wang et al., 2019, 2018). With the
rise of BERT-based models, researchers also apply
Transformer and pre-training to further improve
navigation performance (Hao et al., 2020; Hong
et al., 2020b; Zhu et al., 2020b). While apply-
ing new techniques to the navigation agents might
boost their performance, we still know little about
how agents make each turning decision. Treatment
of the agents’ processing of instructions and per-
ception of the visual environment as a black box
might hinder the design of a generic model that
fully understands visual and textual input regard-
less of VLN setups. Table 1 shows non-negligible
performance gaps between neural agents and hu-
mans on both indoor and outdoor VLN tasks.
Therefore, we focus on analyzing how the nav-
igation agents understand the multimodal input
data in this work. We conduct our investigation
from the perspectives of natural language instruc-
tion, visual environment, and the interpretation
of vision-language alignment. We create counter-
factual interventions to alter the instructions and
the visual environment in the validation dataset,
focusing on variables related to objects, direc-
tions and numerics. More specifically, we modify
the instruction by removing or replacing the ob-
ject/direction/numeric tokens, and we adjust the
environment by masking out visual instances or
horizontally flipping the viewpoint images. Subse-
quently, we examine the interventions’ treatment
effects on agents’ evaluation performance while
keeping other variables unchanged. We set up ex-
periments on the R2R (Anderson et al., 2018) and
the RxR dataset (Ku et al., 2020b) for indoor VLN
and the Touchdown dataset (Chen et al., 2019) for
outdoor VLN. We examine nine VLN agents on the
three datasets with quantitative ablation diagnostics
on the text and visual inputs.
In summary, our key findings include:
1.Indoor navigation agents refer to both objects
and directions in the instruction when mak-
ing decisions. In contrast, outdoor navigation
agents heavily rely on directions and poorly
understand visual objects. (Section 4)2.Instead of merely staring at surrounding ob-
jects, indoor navigation agents are able to set
their sights on objects further from the current
viewpoint. (Section 5)
3.Transformer-based agents display stronger nu-
merical reasoning ability (Section 4), and
acquire better cross-modal understanding of
objects, compared to non-Transformer-based
agents. (Section 6)
4.Indoor agents can align object tokens to cer-
tain targets in the visual environment to a cer-
tain extent, but display in-balanced attention
on text and visual input. (Section 6)
We hope these findings reveal opportunities and
obstacles of current VLN models and lead to new
research directions.
2 Related Work
Instruction Following is a long-standing topic
in AI studies that ask an agent to follow natural
language instructions and accomplish target tasks,
which can be dated back to the SHRLDU (Wino-
grad, 1971). Efforts made to tackle this classic
problem spans from defining templates (Klingspor
et al., 1997; Antoniol et al., 2011), designing hard-
encoded concepts to ground visual attributes and
spatial relations (Steels and V ogt, 1997; Roy, 2002;
Guadarrama et al., 2013; Kollar et al., 2013; Ma-
tuszek et al., 2014), to constructing varies datasets
and learning environments (Anderson et al., 1991;
Bisk et al., 2016a; Misra et al., 2018). Many meth-
ods have been proposed to map the instructions
into sequence of actions, such as reinforcement
learning (Branavan et al., 2009, 2010; V ogel and
Jurafsky, 2010; Misra et al., 2017), semantic pars-
ing (Chen and Mooney, 2011; Artzi and Zettle-
moyer, 2013), alignment-based model (Andreas
and Klein, 2015), and neural networks (Bisk et al.,
2016b; Mei et al., 2016; Tan and Bansal, 2018).
Vision-and-Language Navigation is a task
where an agent comprehends the natural language
instructions and reasons through the visual envi-
ronment. Many studies aim at improving VLN
agents’ performance in one way or another. To en-
rich training data, a line of work (Fried et al., 2018;
Zhu et al., 2020b) use back-translation to generate
augmented instructions. To enforce cross-modal
grounding, RPA and RCM (Wang et al., 2018,
2019) use reinforcement learning, SMNA (Ma
et al., 2019a) uses a visual-textual co-grounding
module to improve cross-modal alignment, Rel-5982Graph (Hong et al., 2020a) uses graphs for task
formulation. To address the generalizability prob-
lem to unseen environment, PRESS (Li et al.,
2019) introduces a stochastic sampling scheme,
EnvDrop (Tan et al., 2019) proposes environment
dropout. To utilize visual information from the
environment, AuxRN (Zhu et al., 2020a) uses aux-
iliary tasks to assist semantic information extrac-
tion, VLN-HAMT (Chen et al., 2021) incorporates
panorama history with a hierarchical vision trans-
former. FAST (Ke et al., 2019) makes use of asyn-
chronous search and allows the agent to backtrack
if it discerns a mistake after attending to global
and local knowledge. With the success of BERT-
related models in NLP, researchers also start to
build Transformer-based navigation agents and add
a pre-training process before fine-tuning on the
downstream VLN task (Hao et al., 2020; Hong
et al., 2020b; Zhu et al., 2020b; Chen et al., 2021).
Model Behavior Analysis As multimodal stud-
ies gain more and more attention, there are lines of
works that focus on explaining models’ behaviors
to better understand and handle the tasks. Some
generate textual explanations by training another
model to mimic human explanations (Hendricks
et al., 2016; Park et al., 2018; Wu and Mooney,
2019). Others generate visual explanations with
the help of attention mechanism (Lu et al., 2016) or
gradient analysis (Selvaraju et al., 2017). There are
also attempts to provide multimodal explanations,
e.g., Li et al. (2018) breaks up the end-to-end VQA
process and examines the intermediate results by
extracting attributes from the visual instances. An-
other line of work examines model performance
by conducting ablation studies on input data. Re-
cent analyses on language modelling (O’Connor
and Andreas, 2021), machine translation (Fernan-
des et al., 2021), and instruction following (Dan
et al., 2021) ablate/perturb both training and vali-
dation data. A study on multimodal models (Frank
et al., 2021) only applies ablation during evaluation,
which is the same as our settings.
3 Background and Research Questions
We first bring in the task of Vision-and-Language
Navigation and introduce the datasets and agents
used for comparison. Then we list out the research
questions to study in this work.
3.1 Vision-and-Language Navigation
In the vision-and-language navigation task, the nav-
igation agent is asked to find the path to reach the
target location following the instructions X. The
navigation procedure can be viewed as a sequen-
tial decision-making process. At each time step t,
the visual environment presents an image view v.
With reference to the instruction Xand the visual
view v, the agent is expected to choose an action
asuch as turn left orstop.
Datasets We conduct indoor navigation experi-
ments on the Room-to-Room (R2R) dataset (Ander-
son et al., 2018) and the Room-across-Room (RxR)
dataset (Ku et al., 2020b), and test outdoor VLN
on Touchdown (Chen et al., 2019). R2R and RxR
are built upon real estate layouts and contain sepa-
rate graphs for each apartment/house. Unlike R2R,
which shoots for the shortest path, RxR has longer
and more variable paths. R2R only contains En-
glish instructions, while RxR also includes instruc-
tions in Hindi and Telugu. In this study, we only
cover the English subset for RxR, and will refer to
it as RxR-en in the following sections. Navigation
in Touchdown occurs in the urban environment,
where the viewpoints form a huge connected graph.
Compared to indoor environments, Touchdown has
more complicated visual environments and a more
extensive search space. The evaluation results are
reported on the validation unseen sets for R2R and
RxR-en and on the test set for Touchdown.
Models Table 2 lists out the models covered in
our study. We use the code and trained checkpoints
shared by the authors in the following experiments.
For indoor navigation on R2R, we study a widely
adopted base model Envdrop (Tan et al., 2019),
a backtracking framework for self-correction
FAST (Ke et al., 2019), and two SoTA models
VLN⟳BERT (Hong et al., 2020b) and PREV A-5983LENT (Hao et al., 2020). The Envdrop intro-
duces environment dropout on top of the Speaker-
Follower (Fried et al., 2018) model, FAST conducts
an asynchronous search for backtracking, PREV A-
LENT, and VLN ⟳BERT are Transformer-based
agents with pre-trained models.
For navigation on RxR-en, we examine CLIP-
ViL (Shen et al., 2021) and VLN-HAMT (Chen
et al., 2021). CLIP-ViL shares the same model
structure with EnvDrop. The only difference is
that CLIP-ViL uses CLIP-ViT (Radford et al.,
2021) to extract visual features, while EnvDrop
uses ImageNet ResNet (Szegedy et al., 2017) fea-
tures. VLN-HAMT incorporates a long-horizon
history into decision-making by encoding all the
past panoramic observations via a hierarchical vi-
sion Transformer.
For outdoor navigation on Touchdown, we con-
sider the common baseline RCONCAT (Chen et al.,
2019), and two SoTA models ARC (Xiang et al.,
2020) and VLN-Transfomer (Zhu et al., 2020b).
RCONCAT encodes the trajectory and the instruc-
tion in an LSTM-based manner. ARC improves
RCONCAT by paying special attention to the
stop signals. VLN-Transfomer is a Transformer-
based agent that applies pre-training on an external
dataset for outdoor navigation in urban areas.
Metrics In the following experiments, we evalu-
ate navigation performance with Success Rate (SR)
for indoor agents and Task Completion (TC) rate
for outdoor agents. Both SR and TC measure the
accuracy of completing the navigation task, reflect-
ing the agents’ overall ability to finish navigation
correctly. An indoor navigation task is considered
complete if the agent’s final position locates within
3 meters of the target location. For outdoor navi-
gation, the task is considered complete if the agent
stops at the target location or one of its adjacent
nodes in the environment graph.
3.2 Research Questions
Current VLN studies have reached their bottleneck
as only minor performance improvements have
been achieved recently, while a significant gap still
exists between machine and human performance.
This motivates us to find the reasons.
To better understand how VLN agents make de-
cisions during navigation, we conduct a series of
experiments on indoor and outdoor VLN tasks, aim-
ing to answer the following questions that might
help us locate the deficiencies of current model
designs and explore future research directions:
1.What can the agents learn from the instruc-
tions? Do they pay more attention to object
tokens or directions tokens? Do they have the
ability to count? (Section 4)
2.What do agents see in the visual environment?
Are they staring at the closely surrounded ob-
jects or also browsing further layout? Do
they focus on individual visual instances or
perceive the overall outline? (Section 5)
3.Can agents match textual tokens to visual en-
tities? How reliable are such connections?
(Section 6)
4 Analysis on Instruction Understanding
This section examines whether and to what extent
the agent understands VLN instructions. We fo-
cus on how the agent perceives object-related to-
kens, direction-related tokens, and numeric tokens,
and their effects on final navigation performance.
Table 3 shows exemplar instructions of the three5984
datasets covered in our study. As shown in Ta-
ble 4, Touchdown’s trajectory length is significantly
longer than the other two indoor datasets. RxR-en
and Touchdown have longer instructions than R2R.
The ratios of object and direction tokens in all three
datasets are comparable, involving about two times
more object tokens than direction tokens.
4.1 The Effect of Object-related Tokens
We first create counterfactual interventions on in-
structions by masking out the object tokens. We use
Stanza (Qi et al., 2020a) part-of-speech (POS) tag-
ger to locate object-related tokens. A token will be
regarded as an object token if its POS tag is NOUN
orPROPN . During masking, we replace the object
token with a specified mask token [MASK] . Then
we examine the average treatment effects of the in-
tervention on agents’ performance, while keeping
other variables unchanged.
Noticeably, when we mask out the object tokens,
the number of visible tokens in the provided instruc-
tion also decreases, which is a coherent factor with
masking object tokens and might interfere with our
analysis. To eliminate the effect of reducing visi-
ble tokens, we add a controlled trial in which we
randomly mask out the same amount of tokens. Ta-
ble 5 gives an example of masking object tokens
(#2) and its corresponding controlled trial (#4).We follow each agent’s original experiment set-
ting for all the experiments in this study and train
it on the original train set. Then we apply masking
to object tokens in the validation set, and report
agents’ relative performance changes under each
setting. We conduct five repetitive experiments and
report the average scores for settings that involve
random masking or replacing.
Table 6 presents how the agents’ navigation per-
formance change when object tokens are masked
out (#2 & #3). Intuitively, not knowing what ob-
jects are mentioned in the instruction lowers all
models’ performance. Comparing the masking ab-
lations with the controlled trial for indoor VLN, we
notice that masking out the object tokens result in a
more drastic decrease in success rate than masking
out random tokens. This holds for all indoor agents,
which verifies that indoor agents depend onobject
tokens more than other tokens . However, when
we compare results on the Touchdown for outdoor
VLN, we notice in surprise that masking out the ob-
ject tokens has a weaker impact on task completion
rate than masking out random tokens. This sug-
gests that current outdoor navigation agents donot
fully take object tokens into consideration when
making decisions. This may be caused by the
weak visual recognition module in current outdoor
agents. As addressed in Table 2, all three outdoor5985agents rely on visual features extracted by ResNet-
18, which may not be powerful enough to fully
incorporate the complicated urban environments.
4.2 The Effect of Direction-related Tokens
We regard the following tokens as direction-related
tokens: left, right, back, front, forward, stop . Simi-
lar to how we ablate the object tokens, we mask out
direction tokens from the instruction and examine
the impact on agents’ navigation performance. Ta-
ble 5 provides examples of direction tokens mask-
ing (#5), and its controlled trial (#7) where the
same amount of random tokens are masked out.
Table 6 shows agents’ performance under various
direction tokens ablation settings (#4 & #5).
For indoor agents, masking out the direction
tokens cause a sharper drop in success rate com-
pared to masking out random tokens, which means
the indoor navigation agents do consider the di-
rection tokens during navigation. We also notice
that agents are more sensitive to the loss of di-
rection guidance on RxR-en than on R2R. Such
difference may be caused by the way these two
datasets are designed. R2R’s ground-truth tra-
jectories are the shortest path from start to goal.
Previous studies have noted that R2R has the
danger of exposing structural bias and leaking
hidden shortcuts (Thomason et al., 2019) , and that
such design encourages goal-seeking over path
adherence (Jain et al., 2019b) . RxR is crafted to
include longer and more variable paths to avoid
such biases. Naturally, agents on RxR-en would
pay more attention to direction tokens since they
may approach their goal indirectly.
For outdoor navigation agents, masking out di-
rection tokens leads to a drastic decline in task com-
pletion rate, compared to random masking. This
indicates that current outdoor navigation agents
heavily relyonthedirection tokens when making
decisions. Given the complicated visual environ-
ments and instructions in the outdoor navigation
task, current agents fail to fully use the instructions,
especially ignoring the rich object-related infor-
mation. The ARC model shows the most salient
performance decline of 90% to the instructions ab-
lated by direction token masking. Aside from the
classifier that predicts the next direction to take,
ARC also uses a stop indicator to decide whether
to stop at each step or not. Its unique mechanism
for detecting stop signals might explain why it is
more sensitive to the existence of direction tokens.
4.3 The Effect of Numeric Tokens
We conduct ablation studies on agents’ understand-
ing of numeric tokens on RxR-en for indoor agents
and Touchdown for outdoor agents. We select a
subset of examples whose instructions contain nu-
meric tokens,and construct ablated instructions
on top. Table 7 provides the statistics of the instruc-
tions for numeric ablations. Table 8 lists out the
results. The VLN-HAMT on RxR-en and VLN-
Transformer on Touchdown have comparable per-
formance when masking numeric tokens over ran-
dom tokens, and have worse performance when
replacing numeric tokens. This suggests that these
two Transformer-based agents have the ability to
conduct numerical reasoning to some extent. In
contrast, other non-Transformer-based agents have
less salient performance drops when replacing nu-
meric tokens. For RCONCAT and ARC, replacing
numeric tokens even leads to higher task comple-
tion rates. This implies the insufficient counting
ability of the non-Transformer-based agents.
5 Analysis on Visual Environment
This section investigates what the agent perceives
in the visual environment. We set an eye on inspect-
ing the agent’s understanding of the surrounding
objects and direction-related information.
5.1 Effect of Objects in the Environment
Built upon the Matterport dataset (Chang et al.,
2017), R2R and RxR obtain detailed object in-
stance annotations and serve as an excellent source5986
for our visual object studies. Touchdown is based
on Google Street View and does not acquire object-
related annotations. Thus, we conduct experiments
on the indoor VLN environment.
We designed several ablation settings for visual
objects. The “mask all visible” setting applies
masking to all the visible visual objects in the
environment (except for wall/ceiling/floor). The
“mask foreground” setting ablates the visual objects
within 3 meters of the camera viewpoint, which
we refer to as the foreground area. The region
beyond 3 meters from the camera viewpoint is re-
garded as the background area. Figure 1shows
an example for comparison. We choose 3 meters
as the boundary because the bounding box anno-
tations for objects within 3 meters are provided in
REVERIE (Qi et al., 2020c) . We denote the num-
ber of visual objects within 3 meters as k, and add
a controlled trial that masks out krandom visual
objects from all the visible objects at the current
viewpoint, regardless of their depth.
Table 9compares the number of visual objects
under various ablation settings. We mask out the
objects in each view by filling the corresponding
bounding boxes with the mean color of the sur-
rounding. Then we follow original experiment set-
tings and use ResNet-152 (He et al., 2016) CNN
to extract image features for R2R agents, and use
CLIP-ViT-B/32 (Radford et al., 2021) to extract
visual features for RxR-en agents.
Results for visual object ablations are shown in
Table 10. We examine the influence of masking out
different quantities of visual objects by comparing
the “mask all visible” setting with the controlled
trial (#2 vs. #4). It comes naturally that masking
out all the visible objects has a more salient impact
on the success rate for all the listed indoor agents.
We study the influence of masking visual objects
at different depths by comparing the “mask fore-
ground” setting with the controlled trial (#3 vs. #4).
Noted here that the number of foreground objects is
limited. Thus only a few objects are being masked
out in both settings. Still, all listed indoor agents
display worse performance on the controlled trial.
Such results state that masking out further visual in-
stances in the background, even only a tiny amount,
will hurt navigation performance. This indicates
that the tested agents consider all the objects in the
visual environment during navigation, instead of
merely staring at the closely surrounding objects.
Notice that the agent designs, the dataset do-
mains, and the visual feature extractors are three
coherent factors that may result in performance dif-
ferences. We further justify this by adding another
set of ablation studies, where we apply ImageNet
ResNet-152 and CLIP-ViT to extract visual fea-
tures for R2R and RxR-en, and evaluate with the
same agent model EnvDrop. Results are shown
in Table 11. The trend of different masking set-
tings aligns with our previous findings in Table 10,
and verifies that the background information is also
crucial in the visual features.59875.2 Effect of Directions in the Environment
In this ablation setting, we randomly flip some of
the viewpoints horizontally. The objects’ relative
positions at the flipped viewpoints will be reversed.
Presumably, suppose the agent can follow the in-
struction and find the corresponding direction to
approach. In that case, the flipped viewpoints will
misguide the agent in the opposite direction and
lower the navigation success rate. As shown in Ta-
ble 10, flipping the viewpoints leads to drastic de-
clines in the success rate for all listed indoor agents.
This verifies our previous finding that indoor agents
can understand directions in the instruction. We
notice that FAST is the only listed model that is
less affected by the direction flipping ablation than
by the object masking ablation (#2 vs. #5). This
suggests that FAST’s asynchronous backtracking
search is able to adjust and recover from errors that
occur when choosing directions to some extent.
6 Analysis on Vision-Language
Alignment
This section examines the agents’ ability to learn
vision-language alignment when executing the nav-
igation. We focus on whether the agents can un-
derstand the objects mentioned in the instruction
and align them to the correct visual instance in the
environment, which is crucial to completing this
multimodal task. To verify the existence of vision-
language alignment, we add perturbations to the
visual and textual input, and check how they affect
agents’ performance.
6.1 Instruction Side Perturbation
We add noise to the textual input by randomly re-
placing object tokens with random object tokens
in the instruction. Table 5 shows an example (#3).
This experiment aims to verify whether the agent
can line the object tokens up to certain visual tar-
gets. The assumption is that if the agent can cor-
rectly align objects mentioned in the instruction
to some targets in the visual environment, then
replacing the object token will confuse and mis-
guide the agent. Examining Figure 2, we notice
that for all three datasets, the Transformer-based
models have worse performance when replacing
the object tokens, compared to simple masking.
This indicates that Transformer-based models have
abetter cross-modal understanding ofobjects, and
canalign object tokens tothevisual targets . Such
superior performance may result from the fact that
the Transformer-based models are often pre-trained
on multimodal resources, thus displaying a slightly
more vital ability to form alignment.
6.2 Environment Side Perturbation
We add noise to the visual input by conducting the
following ablations. In the “dynamic mask” set-
ting, we dynamically mask out the visual object
regions mentioned in the instruction. We randomly
mask out the same amount of visual objects at each
viewpoint in its controlled trial. We also compare
with the “mask tokens” setting, where we mask
out all the object tokens in the instruction, while
leaving the visual environment untouched. This5988experiment aims to determine if the agent aligns
the textual object tokens to the correct visual target.
The assumption is that if the agent builds proper
vision-language alignment and we mask out visual
objects mentioned in the instruction, then the agent
may get confused since it can not find the counter-
part in the visual environment.
Results are shown in Table 12. The success rate
witnesses a decline when dynamically masking out
the visual objects. However, we notice in surprise
that when all visual objects mentioned in the in-
struction are masked out, the agents can still reach
a success rate higher than 44% on R2R and higher
than 32% on RxR-en. This contradicts the previous
assumption and casts doubt on the reliability of the
navigation agents’ vision-language alignment.
Comparing “dynamic mask” with the “mask
tokens” setting, we notice that the visual ob-
ject ablation has much smaller impact on nav-
igation performance than the text object abla-
tions, which suggests that current models have
unbalanced attention onvision and text forthe
VLN task. Recent studies on pre-trained vision-
and-language models (Frank et al., 2021) reveal
that such asymmetry is also witnessed in other mul-
timodal tasks. Future studies may follow the line
of constructing a more balanced VLN agent.
7 Conclusion
In this paper, we inspect how the navigation agents
understand the multimodal information by conduct-
ing ablation diagnostics input data. We find out
that indoor navigation agents refer to both object
tokens and direction tokens in the instruction when
making decisions. In contrast, outdoor navigation
agents heavily rely on direction tokens and poorly
understand the object tokens. When it comes to
vision-and-language alignments, we witness unbal-
anced attention on text and vision, and doubt the
reliability of cross-modal alignments. We hope this
work encourages more investigation and research
into understanding neural VLN agents’ black-box
and improves the task setups and navigation agents’
capacity for future studies.
Acknowledgement
We would like to show our gratitude to the anony-
mous reviewers for their thought-provoking com-
ments. We would like to thank the Robert N. Noyce
Trust for their generous gift to the University of Cal-
ifornia via the Noyce Initiative. The UCSB authorswere also sponsored by the U.S. Army Research
Office, and this work was accomplished under Con-
tract Number W911NF19-D-0001 for the Institute
for Collaborative Biotechnologies. The views and
conclusions contained in this research are those of
the authors and should not be interpreted as repre-
senting the sponsors or the U.S. government’s offi-
cial policy, expressed or inferred. Regardless of any
copyright notation herein, the United States gov-
ernment is authorized to reproduce and distribute
reprints for government purposes.
References59895990599159925993