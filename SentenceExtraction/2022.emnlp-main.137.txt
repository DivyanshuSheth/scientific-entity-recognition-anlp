
Swarnadeep SahaPeter HaseNazneen RajaniMohit BansalUNC Chapel HillHugging Face
{swarna, peter, mbansal}@cs.unc.edu
nazneen@huggingface.co
Abstract
Recent work on explainable NLP has shown
that few-shot prompting can enable large pre-
trained language models (LLMs) to generate
grammatical and factual natural language ex-
planations for data labels. In this work, we
study the connection between explainability
and sample hardness by investigating the fol-
lowing research question – “Are LLMs and hu-
mans equally good at explaining data labels
for both easy and hard samples?” We an-
swer this question by ﬁrst collecting human-
written explanations in the form of generaliz-
able commonsense rules on the task of Wino-
grad Schema Challenge (Winogrande dataset).
We compare these explanations with those gen-
erated by GPT-3 while varying the hardness
of the test samples as well as the in-context
samples. We observe that (1) GPT-3 expla-
nations are as grammatical as human expla-
nations regardless of the hardness of the test
samples, (2) for easy examples, GPT-3 gener-
ates highly supportive explanations but human
explanations are more generalizable, and (3)
for hard examples, human explanations are sig-
niﬁcantly better than GPT-3 explanations both
in terms of label-supportiveness and generaliz-
ability judgements. We also ﬁnd that hardness
of the in-context examples impacts the quality
of GPT-3 explanations. Finally, we show that
the supportiveness and generalizability aspects
of human explanations are also impacted by
sample hardness, although by a much smaller
margin than models.
1 Introduction
Prior work on explainable NLP (Wiegreffe and
Marasovic, 2021) has explored different forms
of explanations ranging from extractive ratio-
nales (Zaidan et al., 2007; DeYoung et al.,
2020), semi-structured, and structured explana-
tions (Jansen et al., 2019; Mostafazadeh et al.,2020; Saha et al., 2021) to free-text explana-
tions (Camburu et al., 2018). Due to the ﬂexibility
of free-text explanations, they have emerged as a
popular form of explanations with multiple bench-
marks developed around them, as well as models
that generate such explanations using seq2seq lan-
guage models (Ehsan et al., 2018; Camburu et al.,
2018; Rajani et al., 2019; Narang et al., 2020).
Few-shot prompting (Radford et al., 2019; Schick
and Schütze, 2021) with Large Language Mod-
els (LLMs) like GPT-3 (Brown et al., 2020) has
been shown to produce highly ﬂuent and factual
natural language explanations that are often pre-
ferred over crowdsourced explanations in existing
datasets (Wiegreffe et al., 2022). However, past
work has not yet explored a critical dimension of
datapoint-level explanations, which is how hard
the data point is to classify correctly. Given re-
cent work on measuring hardness of individual
data points (Swayamdipta et al., 2020), we study
how sample hardness inﬂuences both LLMs’ and
humans’ ability to explain data labels. In sum-
mary, we are interested in investigating the follow-
ing three research questions:
1.RQ1. Do LLMs explain data labels as well as
humans for both easy and hard examples?
2.RQ2. How much do LLM explanations vary
based on the size and the hardness of the re-
trieval pool for choosing in-context samples?
3.RQ3. Are humans equally good at explaining
easy and hard examples?
As a case study, we investigate these questions
for a classical commonsense reasoning task, Wino-
grad Schema Challenge (Levesque et al., 2012) on
a large-scale dataset, Winogrande (Sakaguchi et al.,
2020) (examples in Fig. 1). We ﬁrst collect gen-
eralizable rule-based explanations from humans
like “If X is larger than Y , then X does not ﬁt in
Y .”. To measure data hardness , we use Data Maps
(Swayamdipta et al., 2020), an approach based on
the training dynamics of a classiﬁcation model.2121
Similar to Wiegreffe et al. (2022), we generate
post-hoc explanations by conditioning on the an-
swer leveraging GPT-3 with in-context learning.
We perform human evaluation of the crowdsourced
and model-generated explanations and compare
them on the basis of ‘grammaticality’, ‘supportive-
ness’ and ‘generalizability’. In summary, we report
the following ﬁndings:
•LLM-generated explanations match the grammat-
icality/ﬂuency of human-written explanations re-
gardless of the hardness of test samples.
•For easy examples, both models and humans
write ‘supportive’ explanations, but humans write
more ‘generalizable’ explanations that can ex-
plain multiple similar data points. For hard ex-
amples, humans write explanations that are not
only more ‘generalizable’ but also signiﬁcantly
more ‘supportive’ of the label.
•While choosing in-context examples, factors like
size and hardness of the retrieval pool affect the
quality of model-generated explanations.
•Humans, while much better than models in ex-
plaining hard examples, also struggle with writ-
ing generalizable explanations for these points,
succeeding only about 2/3rd of the time.
2 Method and Experimental Setup
Our method ﬁrst estimates hardness of the samples
using Data Maps (Swayamdipta et al., 2020) and
then chooses a subset of easy, medium, and hard
examples, for which we collect human-written ex-
planations and generate explanations from a state-of-the-art model. Next, we answer our research
questions by comparing the explanations against
multiple granular evaluation axes.
Data Maps. We estimate sample hardness
via a model-based approachcalled Data
maps (Swayamdipta et al., 2020). Data Maps
characterize points xin a dataset along two
dimensions according to a classiﬁer’s behavior
during training: (1) conﬁdence ˆµwhich measures
the mean model probability of the true label y
acrossEepochs, and (2) variability ˆσwhich
measures the standard deviation of the model
probability of the true label across epochs.
wherepdenotes the model’s probability with pa-
rametersθat the end of the eepoch. These two
metrics give rise to different portions in the dataset
including easy-to-learn examples where the model
consistently predicts the sample correctly across
epochs (high conﬁdence, low variability), hard-to-
learn examples where the model rarely predicts the
sample correctly (low conﬁdence, low variability)
andambiguous examples where the model is inde-
cisive about its predictions (high variability). We2122ﬁne-tune RoBERTa-large (Liu et al., 2019) on the
Winogrande dataset to compute the conﬁdence and
variability of each training sample in the dataset.
The two metrics are then used to rank the sam-
ples from easy tohard (most conﬁdent to least
conﬁdent) and least-ambiguous tomost-ambiguous
(least variable to most variable). As discussed later,
we choose a subset of these examples to compare
human and model-generated explanations.
Explanations for Winograd Schema. Next, we
deﬁne the structure of explanations for the Wino-
grad Schema Challenge (Levesque et al., 2012).
Speciﬁcally, these are semi-structured if-then com-
monsense rules as shown in Fig. 1. This charac-
terization of explanations allows us to (1) capture
generalizable commonsense knowledge via place-
holders X (and Y) capable of explaining a number
of similar data points, (2) enforce the common
structural form of an if-then rule for all data points
in this task, while still maintaining the ﬂexibility
of free-text explanations (see Fig. 1 for some ex-
amples), (3) ensure non-trivial explanations that do
not leak the label (Hase et al., 2020), with the aim
of avoiding explanations that only repeat the label
without providing generalizable background knowl-
edge (a common issue in past explanation datasets),
(4) evaluate explanation properties with reduced hu-
man subjectivity due to their semi-structural form.
Human Explanation Collection. Using the
above criteria for constructing explanations (see
detailed instructions in Fig. 6), we collect human-
written explanations on Amazon Mechanical Turk.
In order to ensure that the explanations do not ex-
plicitly leak the label, the annotators are asked to
write explanations in the form of generalizable com-
monsense rules consisting of placeholders X (and
Y) without mentioning the actual options. We col-
lect explanations for 500 easiest and 500 hardest
samples, along with 100 examples with medium
hardness (around the median conﬁdence). We do
not collect explanations separately for least and
most ambiguous samples because ambiguity corre-
lates strongly with hardness, i.e., the least ambigu-
ous examples are often the easiest while the most
ambiguous examples are also typically the hardest.
Explanation Generation via GPT-3. Next, we
select GPT-3 (Brown et al., 2020) as a representa-
tive candidate of today’s NLP model landscape to
generate explanations from. For each set of 500
easy and hard samples, we randomly split theminto 400 samples for retrieving in-context samples
and 100 samples for testing. We generate explana-
tions for the test samples using the largest (175B)
“text-davinci-002” InstructGPT model of GPT-3
by conditioning on the context and the gold label
(as shown in Fig. 8). The in-context samples are
chosen by computing the embeddings of the test
sample and the retrieval samples using Sentence
BERT (Reimers and Gurevych, 2019) and selecting
the top-k samples (see Appendix C for examples).
We setkto 5 in our experiments. Further details of
our prompting method are in Appendix B.
Explanation Evaluation. Having obtained hu-
man and model explanations, we now describe
their evaluation process. Due to the limitations of
automatic metrics for evaluating explanation qual-
ity (Clinciu et al., 2021), we follow Wiegreffe et al.
(2022) to conduct human evaluation of both crowd-
sourced and GPT-3 explanations on MTurk based
on three attributes – grammaticality ,supportive-
ness, and generalizability . When evaluating expla-
nations for grammaticality , we evaluate their syn-
tax and ﬂuency while ignoring spelling mistakes
and typos (which also hardly ever appear in model
explanations). Given the semi-structured nature
of our explanations, we evaluate supportiveness
as whether, when appropriately replacing X and Y
with the two options, the explanation answers the
question “Why does this point receive the label it
does?” (Miller, 2019). Lastly, we evaluate gener-
alizability as how applicable the explanation is for
other samples with different X and Y . We maintain
a trained pool of annotators for both explanation
authoring and veriﬁcation while ensuring that they
do not verify their own data. Each explanation is
evaluated by 3 different annotators and the ﬁnal
results are obtained by majority voting. We report
moderate inter-annotator agreement scores of Krip-
pendorff’sα(Krippendorff, 2011) between 0.4-0.6,
details of which are discussed in Appendix A.
3 Results
3.1 RQ1: Do LLMs explain data labels as
well as humans for both easy and hard
examples?
In Fig. 2, we compare the human and GPT-3 ex-
planations for easy, medium, and hardexamples2123
along the three axes. We observe that GPT-3 is not
only able to learn the if-then structure of the expla-
nations but also matches humans in terms of gener-
ating grammatically ﬂuent explanations, regardless
of the sample hardness. For easy examples, GPT-3
explanations are almost as supportive of the label
as human explanations, sometimes even outper-
forming humans. However, humans are typically
better at writing more generalizable explanations
that apply to broader contexts (see examples 1-3
in Fig. 1). For hard examples, GPT-3 often fails to
generate sufﬁciently supportive explanations and
hence signiﬁcantly underperforms humans in more
than 46% of the cases (see examples 4-6 in Fig. 1
and Appendix D for some common errors). This,
in turn, also hurts the generalizability aspect of
the model-generated explanations. Medium-hard
examples show a trend similar to easy examples
because their conﬁdence values are much closer to
the easy examples than the hard ones.
Signiﬁcance Testing. Pertaining to the above
results, we further use a non-parametric boot-
strap test (Efron and Tibshirani, 1994) to evaluate
whether the human win-rate differs signiﬁcantly
from the model win-rate, while treating ties as neu-
tral. We encode human wins as 1, model wins as
-1, and ties as 0 and test whether the average score
is not equal to 0 (meaning that the win-rate differs
between human and model). In summary, for easy
and medium samples, humans’ generalizability is
signiﬁcantly better than the model’s (difference in
win rate is 20 points with p<0.001), while for hard
samples, both humans’ generalizability and sup-
portiveness are better than the model’s (differences
in win rates are 0.38 and 0.4 respectively, with
p<1e-4). Next, for grammaticality, we test if GPT-
3 explanations matches human explanations within
a win-rate threshold of ±rpoints. For a thresh-
old ofr=0.1(testing that grammaticality win-rates
are within 10 percentage points of each other), we
obtainp<0.05, and a threshold of r=0.15yieldsp<1e-4. This suggests that the model’s grammati-
cality signiﬁcantly matches human’s for threshold
values around 0.1.
3.2 RQ2: How much do model explanations
vary based on the size and the hardness
of the retrieval pool for choosing
in-context samples?
We investigate RQ2 by conducting two experiments
in which we compare the explanations generated
by GPT-3 for 100 easy examples. In the ﬁrst, we
vary the size of the retrieval pool (RP) for selecting
in-context examples from 400 to 200 while keeping
the average hardness constant, and in the second,
we vary the hardness of the retrieval pool from easy
to hard examples with the size of the pool (400)
remaining constant. As shown in Fig. 3, the gram-
maticality of the explanations is unaffected in both
experiments. However, supportiveness drops when
the in-context samples are retrieved from a smaller
pool. A larger pool increases the likelihood of hav-
ing more similar in-context examples to the test
sample, and we conclude that similar in-context
examples improve the supportiveness of the expla-
nation. We also ﬁnd that when explaining easy
examples, having a retrieval pool of similar easy
examples helps the model generate better explana-
tions, possibly because of more similar in-context
examples. Combining with RQ1, we conclude that
hardness of both in-context and test samples can
affect the quality of model explanations.
We also conduct a similar study for comparing
the explanation quality of 100 hard test examples
by varying the hardness of the retrieval pool. In
contrast to easy test examples, we do not observe
statistically signiﬁcant differences in explanation
quality for hard examples when the retrieval pool’s
hardness is varied. In particular, with respect to
supportiveness, the win percentages for hard and
easy pool are 20% and 18% respectively, with re-
maining 62% being ties, while for generalizability,
they are 33% and 25% respectively, with remain-2124
ing 42% being ties. We believe that the quality of
hard examples may not be sensitive to changes in
the in-context examples simply because the corre-
sponding explanations are not very good to begin
with.
3.3 RQ3: Are humans equally good at
explaining easy vs. hard examples?
In RQ1, we compared the relative performance of
model and humans in explaining easy, medium,
and hard examples. RQ3 now evaluates the ab-
solute quality of human-written explanations. In
particular, we ask the annotators to rate whether
the explanations demonstrate acceptable grammat-
icality, supportiveness, and generalizability. Fig. 4
shows the fraction of acceptable human explana-
tions along these three axes for easy and hard ex-
amples. We observe that humans also ﬁnd it hard to
write generalizable explanations for some hard ex-
amples. Overall, the quality of human explanations
is also impacted by the hardness of the samples,
although to a lesser extent than GPT-3 since human
explanations become clearly preferable to model
explanations as hardness increases (RQ1).
4 Related Work
There has been signiﬁcant progress made in recent
years on both curating natural language explana-
tion datasets (Camburu et al., 2018; Rajani et al.,
2019; Brahman et al., 2021; Aggarwal et al., 2021,
inter alia ) as well as generating them (Rajani et al.,
2019; Shwartz et al., 2020). Related to the Wino-
grad Schema Challenge, WinoWhy (Zhang et al.,
2020) contains explanations only for the WSC273
test set (Levesque et al., 2012) and does not follow
the structure of our commonsense rule-based ex-
planations, thereby leading to label leakage. Label
leakage makes evaluation of explanations harder
because supportiveness can become trivial. Our
study builds on top of prior works that also gen-
erate free-text explanations using in-context learn-
ing with GPT-3 (Marasovi ´c et al., 2022; Wiegreffe
et al., 2022). However, our novelty lies in investi-
gating the connection between explainability and
sample hardness. A number of concurrent works
have also explored free-text explanations for in-
context learning in various reasoning tasks (Nye
et al., 2021; Chowdhery et al., 2022; Wei et al.,
2022; Lampinen et al., 2022; Wang et al., 2022;
Ye and Durrett, 2022), primarily focusing on im-
proving model performance with explanations and
not evaluating explanation properties or factors that
might inﬂuence them.
5 Conclusion
We studied the effect of sample hardness on the
quality of post-hoc explanations generated by
LLMs for data labels. We concluded that while
LLM explanations are as ﬂuent as human explana-
tions regardless of the sample hardness, humans
are typically better at writing more generalizable
explanations and speciﬁcally, for hard examples,
human explanations are also more supportive of
the label. Factors like the hardness and size of the
retrieval pool for choosing in-context examples can
further impact the explanation quality. We also
observe that the generalizability aspect of human
explanations drops for harder examples, although
by a smaller margin than models.2125Limitations
The goal of our study is to evaluate how well mod-
els explain the data labels and not their own an-
swers for the data points. Hence, both humans and
models write or generate post-hoc explanations by
conditioning on the gold labels. This also leads
us to evaluate the explanations for how acceptable
they are to the humans rather than their faithful-
ness to the model decisions (Wiegreffe and Pinter,
2020; Jacovi and Goldberg, 2020). The notion of
data maps-driven instance difﬁculty (Swayamdipta
et al., 2020) is primarily model dependent, and
it is conceivable that different choices of mod-
els (or model-families) would yield different rank-
ordering of data points by hardness. However, we
measure the relative hardness of the data points
and it is very unlikely that the k-easiest samples
for RoBERTa (which is used to estimate sample
hardness) will be the k-hardest samples for GPT-3
(which is used to generate explanations) or vice
versa. In addition, we ﬁnd that humans also strug-
gle to explain our estimated ‘hard’ examples. These
factors make our results fairly generalizable and
future work can explore this direction further. It
would also be interesting to see how our results
generalize to other forms of explanations in NLP
like rationales or structured explanations.
Acknowledgements
We thank the reviewers for their helpful feedback
and the annotators for their time and effort. This
work was supported by NSF-CAREER Award
1846185, NSF-AI Engage Institute DRL-2112635,
DARPA MCS Grant N66001-19-2-4031, ONR
Grant N00014-18-1-2871, and Google PhD Fel-
lowship. The views contained in this article are
those of the authors and not of the funding agency.
References21262127
A Crowdsourcing Details
All our crowdsourcing studies are done on Amazon
Mechanical Turk. We select crowdworkers who are
located in the US with a HIT approval rate higher
than 96% and at least 1000 HITs approved. We
conduct qualiﬁcation tests before crowdworkers are
allowed to write and verify explanations. As shown
in Figure 5, it tests the annotator’s understanding
of the Winograd Schema Challenge by asking to
choose the correct option given the sentence and
get all questions correct. In Figure 6, we show
the instructions and interface for collecting human-
written explanations. Finally, in Figure 7, we show
the interface for explanation veriﬁcation. We pay
annotators $0.10 for each HIT of explanation con-
struction and $0.15 for each HIT of explanation
veriﬁcation at an hourly wage of $12-15.Easy Hard
Grammaticality 0.63 0.61
Supportiveness 0.51 0.43
Generalizability 0.45 0.37
Inter-annotator Agreement. Each explanation
is evaluated by three annotators. We report inter-
annotator agreement using Krippendorff’s α(Krip-
pendorff, 2011). Despite the subjective nature of
our task, we observe moderate agreement scores
among annotators, as reported in Table 1. Per-
haps unsuprisingly, we ﬁnd the agreement score for
grammaticality to be the highest and that of gener-
alizability to be the lowest. For supportiveness, we
observe anαin the range of 0.4–0.5. Between easy
and hard examples, the agreement scores for hard
examples are lower, which also shows that these
examples are harder for humans to agree on.
B Prompting Details
We avoid prompt tuning by largely follow-
ing Wiegreffe et al. (2022) for prompt construction
and choosing a layout that resembles Wiegreffe
et al. (2022)’s CommonsenseQA prompt. Follow-
ing Liu et al. (2022), we order the in-context sam-
ples in increasing order of similarity to the test
sample such that the most similar sample is last in
the context. All our generated explanations are ob-
tained using the largest “text-davinci-002” model
of GPT-3with greedy decoding and maximum
token limit of 50. While prior works (Zhao et al.,
2021; Lu et al., 2022) have shown that in-context
learning methods have high variance based on the
hyperparameters chosen or the order of examples,
we ﬁnd that our generated explanations are fairly ro-2128
bust to such variations due to their semi-structured
form. We also note that ﬁnding the most optimal
prompt is not the main focus of our work. Instead,
we are interested in understanding the connection
between explanation quality and sample hardness
when other factors like hyperparameters, decoding
strategy, etc are kept unaltered.
C Examples of Similar Examples
Retrieved for In-Context Learning
We ﬁnd that our simple method of using sentence
embeddings to retrieve top-k similar examples for
in-context learning works well in practice. In
Table 3, we show some representative examples,
demonstrating the presence of similar common-
sense knowledge between the test sample and the
top-2 similar samples.
D Analysis of GPT-3-generated
Explanations for Hard Samples
In Table 4, we show more examples of bad expla-
nations generated by GPT-3 for some of the hard
examples. While the model is able to learn the semi-
structured nature of the explanations, it often makes
mistakes in identifying what X and Y are (ﬁrst ex-
ample), misses the core reasoning concept (second
and third examples) or are non-contextual (last ex-
ample), thereby either not properly supporting the
label or completing refuting the label (fourth exam-
ple). Consequently, the ‘generalizability’ aspect of
these explanations also suffer.212921302131