
Yuqi Chen, Keming Chen, Xian Sun and Zequn Zhang
Aerospace Information Research Institute
Key Laboratory of Network Information System Technology(NIST)
School of Electronic, Electrical and Communication Engineering
University of Chinese Academy of Sciences
chenyuqi19@mails.ucas.ac.cn ,ckmdejob@hotmail.com , {sunxian,zqzhang1}@mail.ie.ac.cn
Abstract
Aspect Sentiment Triplet Extraction (ASTE)
is a new fine-grained sentiment analysis task
that aims to extract triplets of aspect terms, sen-
timents, and opinion terms from review sen-
tences. Recently, span-level models achieve
gratifying results on ASTE task by taking ad-
vantage of the predictions of all possible spans.
Since all possible spans significantly increases
the number of potential aspect and opinion can-
didates, it is crucial and challenging to effi-
ciently extract the triplet elements among them.
In this paper, we present a span-level bidirec-
tional network which utilizes all possible spans
as input and extracts triplets from spans bidi-
rectionally. Specifically, we devise both the
aspect decoder and opinion decoder to decode
the span representations and extract triples from
aspect-to-opinion and opinion-to-aspect direc-
tions. With these two decoders complement-
ing with each other, the whole network can ex-
tract triplets from spans more comprehensively.
Moreover, considering that mutual exclusion
cannot be guaranteed between the spans, we
design a similar span separation loss to facili-
tate the downstream task of distinguishing the
correct span by expanding the KL divergence
of similar spans during the training process; in
the inference process, we adopt an inference
strategy to remove conflicting triplets from the
results base on their confidence scores. Exper-
imental results show that our framework not
only significantly outperforms state-of-the-art
methods, but achieves better performance in
predicting triplets with multi-token entities and
extracting triplets in sentences contain multi-
triplets.
1 Introduction
Aspect-based sentiment analysis (ABSA) is an im-
portant field in natural language processing (NLP).Figure 1: An example of ABSA subtasks. The spans
highlighted in blue are aspect terms. The spans in red
are opinion terms. Sentiments are marked with green.
The ABSA task contains various fundamental sub-
tasks, such as aspect term extraction (ATE), opin-
ion term extraction (OTE), and aspect-level senti-
ment classification (ASC). Recent studies focus on
solving these tasks individually or doing a combi-
nation of two subtasks, such as aspect term po-
larity co-extraction (APCE), aspect opinion co-
extraction (AOCE), and aspect-opinion pair extrac-
tion (AOPE). However, none of these subtasks aims
to extract the aspect terms (AT) with their corre-
sponding opinion terms (OT) and sentiment polar-
ity (SP) simultaneously. To tackle this problem,
(Peng et al., 2020) propose the aspect sentiment
triplet extraction (ASTE) task which aims to ex-
tract ( AT, OT, SP ) triplets such as ( hot dogs, top
notch, positive ) and ( coffee, average, negative ) in
the example of Figure 1.
To solve the ASTE task, recent works (Peng
et al., 2020; Wu et al., 2020; Mao et al., 2021)
use sequential token-level methods and formu-
late this task as a sequence tagging problem. Al-
though these works achieve competitive results,
their token-level models suffer from cascading er-
rors due to sequential decoding. Therefore, (Xu
et al., 2021) propose a span-level model to capture4300the span-to-span interactions among ATs and OTs
by enumerating all possible spans as input. Despite
the exciting results their work has yielded, sev-
eral challenges remain with the existing span-level
model. First , since both aspect terms and opinion
terms can trigger triplets, it is a challenge to iden-
tify triplets bidirectionally. Second , unlike token-
level methods, span-level input cannot guarantee
mutual exclusivity among the spans, so the similar
spans (spans that have shared tokens) such as hot
dogs,dogs, and the hot dogs , may cause confusion
in downstream tasks. Thus, it is challenging for
span-level models to effectively distinguish these
similar span. Third , the existence of similar spans
enables span-level models to generate conflicting
triples in the results, such as ( hot dogs, top notch,
positive ), (hot dogs,are top notch, positive ), and
(the hot dogs, top notch, positive ). How to properly
extract non-conflicting triplets is also challenging.
To address these challenges, we propose a span-
level bidirectional network for ASTE task. Un-
like prior span-level works (Xu et al., 2021), our
network decodes all possible span representations
from both aspect-to-opinion and opinion-to-aspect
directions through the cooperation of the aspect de-
coder and opinion decoder. In the aspect-to-opinion
direction, the aspect decoder aims to extract ATs
such as {hot dogs, coffee }, and the opinion decoder
aims to extract OTs such as {top notch }for each
specific AT like {hot dogs }. Analogously, in the
opinion-to-aspect direction, the opinion decoder
and aspect decoder are utilized to extract OTs and
their corresponding ATs, respectively. Furthermore,
we design the similar span separation loss to direct
the model deliberately distinguishing similar span
representations during the training process; and an
inference strategy employed in the prediction pro-
cess is also proposed for eliminating the conflicting
triplets in the extraction results. To verify the ef-
fectiveness of our framework, we conduct a series
of experiments based on four benchmark datasets.
The experimental results show our framework sub-
stantially outperforms the existing methods. In
summary, our contributions are as follows:
•We design a span-level bidirectional network
to extract triplets in both aspect-to-opinion
and opinion-to-aspect directions in a span-
level model. By this design, our network can
identify triplets more comprehensively.
•We propose the similar span separation loss
to separate the representations of spans thatcontain shared tokens. Based on these dif-
ferentiated span representations, downstream
models can discriminate the span representa-
tion more precisely.
•We design an inference strategy to eliminate
the potential conflicting triplets due to the lack
of mutual exclusivity among spans.
2 Related Work
Aspect based sentiment analysis (ABSA) is a fine-
grained sentiment analysis task that consists of
various subtasks, including aspect term extraction
(ATE) (Wang et al., 2016; Li and Lam, 2017; Xu
et al., 2018; Li et al., 2018; Ma et al., 2019), opin-
ion term extraction (OTE) (Poria et al., 2016; Fan
et al., 2019; Wu et al., 2020), aspect-level sentiment
classification (ASC) (Dong et al., 2014; Tang et al.,
2016; He et al., 2018; Li et al., 2019b). Since these
subtasks are solved individually, recent studies at-
tempted to couple two subtasks as a compound task,
such as aspect term polarity co-extraction (APCE)
(Li and Lu, 2017; He et al., 2019; Li et al., 2019a),
aspect and opinion co-extraction (Qiu et al., 2011;
Liu et al., 2013; Yu et al., 2019), aspect category
and sentiment classification (Hu et al., 2019), and
aspect-opinion pair extraction (AOPE) (Chen et al.,
2020; Zhao et al., 2020; Gao et al., 2021), and
aspect-opinion pair extraction (AOPE) (Gao et al.,
2021; Zhao et al., 2020; Wu et al., 2021). Although
many works have achieved great progress on these
tasks, none of these tasks aims to identify the as-
pect terms as well as their corresponding opinion
term and sentiment polarity.
To tackle this issue, (Peng et al., 2020) proposed
the aspect sentiment triplet extraction (ASTE) task,
which aimed to extract aspect terms, the senti-
ments of the aspect terms, and the opinion terms
causing the sentiments. Some methods (Xu et al.,
2020; Wu et al., 2020) designed a unified tagging
scheme to solve this task. Some others (Chen et al.,
2021; Mao et al., 2021) formulated this task as
a multi-turn machine reading comprehension task
and solve it with machine reading comprehension
frameworks. Recently, (Xu et al., 2021) had pro-
pose a span-level model to extract ATs and OTs
first and then predict the sentiment relation for each
(AT, OT) pairs.
3 Methodology
As shown in Figure 2, our network consists of four
parts: span generation, similar span separation loss,4301
bidirectional structure, and the inference strategy.
In the following subsections, we first give the defi-
nition of ASTE tasks and then detail our network
structure.
3.1 Task Definition
For a sentence S={w, w, . . . , w}consisting n
words, the goal of the ASTE task is to extract a set
of aspect sentiment triplets T={(a, o, c )}
from the given sentence S, where (a, o, c )refers to
(aspect term, opinion term, sentiment polarity) and
c∈ {Positive, Neutral, Negative }.
3.2 Span Generation
Given a sentence Swith ntokens, there are
mpossible spans in total. Each span s=/braceleftbig
w, . . . , w/bracerightbig
is defined by all the to-
kens from start (i)toend(i)inclusive, and the
maximum length of span sisl:
1≤start (i)≤end(i)≤n (1)
end(i)−start (i)≤l (2)
To obtain span representations, we need to get
the token-level representations first. In this paper,
we utilize BERT (Devlin et al., 2018) as a sentence
encoder to obtain token-level contextualized repre-
sentations {h,h, . . . , h}of the given sentence
S. Then, the token-level representations are com-
bined by max pooling. Note that various methods
can be applied to generate the representations for
spans, the effectiveness of these span generation
methods will be investigated in the ablation study
in Appendix. We define the representation of span
sas:
g=Max (h(i),h(i), . . . , h(i))
(3)where Max represents max pooling.
3.3 Similar Span Separation Loss
After generating the representation of span, most
previous models directly use the span representa-
tions for downstream tasks. However, enumerating
all possible spans in a sentence inevitably gener-
ates lots of spans that have same tokens with each
other, and the model may suffer from the limita-
tions in processing these similar spans due to their
adjacent distribution. To separate the spans with
similar distributions, we propose a similar span
separation loss based on KL divergence to separate
similar spans, as shown in Figure 2. The similar
span separation loss is defined as:
KL(g||G) =/summationdisplaysoftmax (g)logsoftmax (g)
softmax (g)(4)
KL(G||g) =/summationdisplaysoftmax (g)logsoftmax (g)
softmax (g)(5)
J=/summationdisplaylog(1 +2
KL(G||g) +KL(g||G))(6)
where Gindicates the set of the representations of
spans which share at least one token with s. Note
that we have not directly used the KL divergence
as the separation loss but in combination with the
log(1 + 1 /x)function to achieve the effect that
when KL divergence is small the separation loss is
large and vice versa.43023.4 Bidirectional Structure
As the aspect sentiment triplet can be triggered by
an aspect terms or an opinion terms, we propose
a bidirectional structure to decode the span repre-
sentations. As shown in Figure 2, the bidirectional
structure consists of an aspect decoder and an opin-
ion decoder. The details of each component in the
bidirectional structure are given in the following
subsections.
3.4.1 Aspect-to-opinion Direction
In aspect-to-opinion direction (Blue arrows and
modules in Figure 2), the aspect decoder aims to
extract all ATs along with their sentiment from the
sentence. We can obtain the confidence score as
well as the probability of the sentiment of AT as
follows:
u=FFNN(g, θ) (7)
q =wu (8)
p =softmax (q ) (9)
where FFNNrepresents the FFNN of aspect de-
coder, θis the parameter for the FFNN, w∈
Ris a trainable weight vector, and c∈
{V alid, Invalid }is the number of categories.
Then, giving a set Gof original span represen-
tations of all valid ATs g∈G, we apply the
opinion decoder to identify all OTs along with their
sentiment for each particular valid AT by exploit-
ing attention mechanism. Similarly, we obtain the
probability distribution of the OT’s sentiment along
with its confidence score via:
u=FFNN(g, θ) (10)
α=exp(u)
exp(g)(11)
q =w/parenleftbig
u+α·g/parenrightbig
(12)
p =softmax (q ) (13)
where FFNNrepresents the FFNN of opin-
ion decoder, θis the parameter for the FFNN,
w∈Ris a trainable weight vector, and
c∈ {Positive, Neutral, Negative, Invalid }
is the number of sentiment polarity. Furthermore,
we define the loss of aspect-to-opinion direction
as:
J=−/summationdisplayy log (q )
−/summationdisplay/summationdisplayy log/parenleftig
q/parenrightig (14)where y andy are ground truth labels
of the sentiments for AT and OT given a specific
valid AT, respectively.
3.4.2 Opinion-to-aspect Direction
As for opinion-to-aspect direction (Red arrows and
modules in Figure 2), the opinion decoder is de-
ployed first to extracts all the OTs along with their
sentiment from the sentence. To minimize the num-
ber of model parameters, the opinion decoder in
both aspect-to-opinion and opinion-to-aspect direc-
tions shares the FFNN features, as described in
Equation (10). The probability distribution of the
sentiments of OTs as well as the confidence scores
can be obtained as:
q =wu (15)
p =softmax (q ) (16)
where w∈Ris a trainable weight vec-
tor.
Given a set Gif original span representations
of all valid OTs g∈G, the aspect decoder is
deployed to identify the ATs and their sentiment
for each particular valid OTs. Note that the aspect
decoder in opinion-to-aspect direction also shares
same FFNN features described in Equation (7)with
the aspect decoder in aspect-to-opinion direction.
The logits of ATs and their confidence scores in
opinion-to-aspect direction can be obtained by:
α=exp(u)
exp(g)(17)
q =w/parenleftbig
u+α·g/parenrightbig
(18)
p =softmax (q ) (19)
where w∈Ris a trainable weight vec-
tor.
Finally, the loss for opinion-to-aspect direction
is defined as:
J=−/summationdisplayy log (q )
−/summationdisplay/summationdisplayy log/parenleftig
q/parenrightig (20)
where y andy are the ground truth la-
bels. Then, we combine the above loss functions
to form the loss objective of the entire model:
J=J+J+J (21)4303Algorithm 1 Inference Strategy
Input: T,T
Tdenotes the triplet extraction results in
aspect-to-opinion direction
Tdenotes the triplet extraction results in
opinion-to-aspect directionGet the overall triplets in both extract direc-
tionsT=T∪ Tfort∈ T do fort∈(T − { t})do t= (a, o, c, s), t= (a, o, c, s),
sandsare the confidence score of the
corresponding triplets ifa∩a̸=∅ando∩o̸=∅then ifs> sthen T=T − { t} else T=T − { t} end if end if end forend forreturn T
3.5 Inference
In contrast to the mutual exclusivity of the triplets
in the token-level method, span-level model can-
not guarantee that there are no conflicts between
any two triples. Therefore, we propose an infer-
ence strategy to eliminate the potential conflicting
triplets during the inference process. As illustrated
in Algorithm 1, We first combine the extraction
results in both directions by taking the union set T
(line 1). Afterwards, for each pair of triplets in the
overall triplets set Tthat have duplicates in both
aspect aand opinion o(line 5), the conflicting re-
sults are eliminated by discarding the triplets with
lower confidence scores s(line 6-9). Note that in
the condition of determining whether two triplets
conflict with each other (line 5), the determination
of whether the union set is empty is performed on
the position index, rather than on the tokens.
4 Experiments
4.1 Datasets
To verify the effectiveness of our network, we con-
duct experiments on four benchmark datasets(Xu
et al., 2020) , which are constructed based on
the original SemEval ABSA Challenges and the
datasets of (Fan et al., 2019). Table 1 lists the
statistics of these datasets.
4.2 Experimental Setting
We adopt the cased base version of BERT (De-
vlin et al., 2018) in our experiments, which con-
tains 110M parameters. During training, we use
AdamW (Loshchilov and Hutter, 2017) to optimize
the model parameters. The fine-tuning rate for
BERT and the learning rate for other models are
set to 1e-5 and 1e-4, respectively. Meanwhile, the
mini-batch size is set to 16 and the dropout rate is
set to 0.1. The maximum length of generated spans
is set to 8. We train our framework in a total of 120
epochs on a NVIDIA Tesla V100 GPU.
4.3 Evaluation
To comprehensively evaluate the performance of
different methods, we use precision ,recall ,F1-
score as the evaluation metrics. The extracted ATs
and OTs are considered correct if and only if pre-
dicted spans exactly match the ground truth spans.
In the experiments, we select the testing results
when the model achieves the best performance on
the development set.
4.4 Baselines
To demonstrate the effectiveness of our network,
we compare our method with the following base-
lines:
•Peng-two-stage (Peng et al., 2020) is a two-
stage pipeline model. Peng-two-stage extracts4304
both aspect-sentiment pairs and opinion terms
in the first stage. In the second stage, Peng-
two-stage pairs up the extraction results into
triplets via an relation classifier.
•JET (Xu et al., 2020) is an end-to-end model
which proposes a novel position-aware tag-
ging scheme to jointly extracting the triplets.
It also designs factorized feature representa-
tions so as to effectively capture the interac-
tion among the triplet factors.
•GTS (Wu et al., 2020) is an end-to-end model
which formulates ASTE as a unified grid tag-
ging task. It first extracts the sentiment feature
of each token, and then gets the initial predic-
tion probabilities of toke pairs based on these
token-level features. It also designs an infer-
ence strategy to exploit the potential mutual
indications between different opinion factors
and performs the final prediction.
•Dual-MRC (Mao et al., 2021) is a joint train-
ing model which consists of two machine
reading comprehensions. One of the MRC
is for aspect term extraction, and another is
for aspect-oriented opinion term extraction
and sentiment classification.
•B-MRC (Chen et al., 2021) formalizes the
ASTE task as a multi-turn machine reading
comprehension task, and proposes three types
of queries to extract targets, opinions and the
sentiment polarities of aspect-opinion pairs,
respectively.
•Span-ASTE (Xu et al., 2021) considers all
possible spans in a sentence and build the in-
teraction between the whole spans of aspect
terms and opinion terms when predicting their
sentiment relation. They also propose a dual-
channel span pruning strategy to ease the high
computational cost caused by span enumera-
tion.
4.5 Main Results
Table 2 reports the results of our framework and
baseline models. According to the results, our
framework achieves state-of-the-art performance
on all datasets. Specifically, our framework sur-
passes the best baselines by an average of 2.3 F1-
score on ASTE. This result demonstrates that our
framework can take advantage of bidirectional de-
coding and efficiently distinguish the span repre-
sentation. Although some of the recall scores are
slightly lower than Span-ASTE, the increase in
precision significantly outperforms the previous
baselines in most datasets, which shows the higher
prediction accuracy of our network. It is worth
noting that BMRC and Dual-MRC achieve better
performance than JET and PENG-two-stage. This
is probably because BMRC and Dual-MRC formal-
ize the ASTE task as a multi-turn machine reading
comprehension task and benefit from asking the
model questions. Unlike those approaches, Span-
ASTE and our method both utilize the span-level
interactions to handle the ASTE task and avoid
the cascading errors. Moreover, our model outper-
forms Span-ASTE because our method identify the4305triplets from both aspect-to-opinion and opinion-to-
aspect directions, rather than matching each aspect
span and opinion span. Besides, our network also
take advantage of similar span separation loss and
inference strategy to overcome the drawback of
mutual exclusivity absence among spans.
4.6 Ablation Study
To validate the origination of the significant im-
provement in our network, we conduct ablation
experiments on 14LAP datasets. As shown in Ta-
ble 3, our bidirectional model yields better results
than unidirectional models, which clearly indicates
the superiority of the collaboration in both two di-
rections on decoding the span representations. And
the inference results from opinion terms to aspect
terms are better than the other direction, which may
due to the simplicity of extracting opinion terms in
the 14LAP dataset.
Moreover, the inference strategy has exhibited
the enhancement on model performance. However,
the improvement brought by the inference strategy
is not significant, because conflicting triplets tend
to exist among multi-token results, and only a small
percentage of triplets containing multi-token terms
in 14LAP dataset. We believe the effect of the
inference strategy will be more obvious in datasets
enriched with multi-token triplets.
In addition, to demonstrate the effectiveness of
our proposed similar span separation loss based on
KL divergence, we further design similar range sep-
aration losses based on JS divergence, Euclidean
distance and cosine similarity. The experimental
results show that all these loss functions have a
boosting effect on our network, and the separation
loss based on KL divergence performs the best.
Note that numerous similarity measures can be
used to separate similar spans, among which there
may be some better measures that can bring more
improvement to the model.
4.7 Effect of Entity Length
To investigate the performance of different methods
on the ATE and OTE with different entity lengths,
we report the F1 scores of our framework, Span-
ASTE, GTS, and B-MRC on the extraction task
with different lengths of entities. The results are il-
lustrated in Figure 3. As the entity length increases,
the performance gap between our framework and
other models becomes more obvious. Since our
method directly models span-level feature for each
entity and and alleviates the drawback of no mu-
tual exclusivity among spans, our method will not
be greatly affected with entity lengths increasing.
In fact, most of the contribution to the improve-
ment in our model comes from the performance in
multi-token entities.
4.8 Effect of Multiple Triplets
To further verify the ability of our framework
to handle multiple triplets, we compare the per-
formance of our network and other baselines on
ASTE task with different number of triplets in the
sentences, and the results are shown in Table 4.
We divide the sentences in 14LAP testset into 5
subclasses. Each subclass contains sentences with
1, 2, 3, 4, or ≥5 triplets, respectively. When ex-
tracting triplets from sentences that contain 1 or 2
triplets, the performance of our framework is com-
petitive to other models. However, when the num-
ber of triplets increases, the performance of Span-
ASTE, GTS, and B-MRC decrease significantly,
while the performance of our network remains sta-
ble or even slightly increases. These experimental
results demonstrate the efficiency and stability of
our framework in handling multiple triplets in a
sentence.
5 Conclusions
In this work, we propose a span-level bidirectional
network for ASTE tasks. This span-level model
can take advantages from both aspect-to-opinion
and opinion-to-aspect directions. The bidirectional
decoding can ensure that either an AT or an OT can4306trigger an aspect sentiment triplet, which is more
in line with human perception. For the shortcom-
ing that mutual exclusivity cannot be guaranteed
among spans, we deploy the similar span sepa-
ration loss to guide the model in discriminating
similar spans. We further design an inference strat-
egy to eliminate conflicting triplet results that are
specific to span-level models. The experimental
results demonstrate that our network significantly
outperforms the compared baselines and achieves
state-of-the-art performance.
Limitations
Although in the previous section we showed the
advanced performance of the network we designed,
there are still some weaknesses in our model.
First, our model uses spans as input, and enu-
merating all possible spans inevitably increases the
input size of the model, so span-level models tend
to have larger computations than token-level mod-
els. As shown in Table 5, our network requires
about 6 times more floating-point computations
than the B-MRC model. While the Span-ASTE
and GTS models require more computation, this is
because Span-ASTE needs to match every aspect
terms and opinion terms and GTS model extracts
triplets by classifying the internal elements of a
square matrix consisting of sentences in rows and
columns.
Second, to reduce the input size of the model, we
set the maximum span length of the spans to 8 to
include as many potential aspect terms and opinion
terms as possible. However, in some datasets with
long extraction targets, the span-level model must
increase the maximum span limit, thus affecting the
performance of the model. Therefore, our model
is suitable only for the tasks of extracting short
targets.
Third, both the similar span separation loss and
inference strategy proposed in this paper are used
to alleviate the shortcoming of the missing mutual
exclusivity in span-level models, while the inputs
of token-level models are naturally mutually exclu-
sive. So the similar range separation loss and in-ference strategies are not applicable to token-level
models.
Ethics Statement
This article does not contain any study with human
participants or animals performed by any of the
authors. And all authors declare that they have no
known competing financial interests or personal
relationships that could have appeared to influence
the work reported in this paper.
References430743084309