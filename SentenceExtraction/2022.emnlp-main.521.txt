
Jiarui Wang, Richong Zhang, Junfan Chen, Jaein Kim, Yongyi MaoSKLSDE, Beihang University, Beijing, ChinaZhongguancun Laboratory, Beijing, ChinaSchool of Electrical Engineering and Computer Science, University of Ottawa, Canada
{wangjr,zhangrc,chenjf}@act.buaa.edu.cn
jaein@buaa.edu.cn,ymao@uottawa.ca
Abstract
Text style transfer is an important task in nat-
ural language processing with broad applica-
tions. Existing models following the masking
and filling scheme suffer two challenges: the
word masking procedure may mistakenly re-
move unexpected words and the selected words
in the word filling procedure may lack diversity
and semantic consistency. To tackle both chal-
lenges, in this study, we propose a style transfer
model, with an adversarial masking approach
and a styled filling technique (AMSF). Specif-
ically, AMSF first trains a mask predictor by
adversarial training without manual configura-
tion. Then two additional losses, i.e. an entropy
maximization loss and a consistency regular-
ization loss, are introduced in training the word
filling module to guarantee the diversity and
semantic consistency of the transferred texts.
Experimental results and analysis on two bench-
mark text style transfer data sets demonstrate
the effectiveness of the proposed approaches.
1 Introduction
Stylistic attributes of natural language text have in-
trigued researchers in natural language processing
(NLP) for a long time. Text style transfer aims to
convert the text into a target style while preserv-
ing the content of the source text, which has broad
industrial applications such as computer-aided writ-
ing (Klahold and Fathi, 2020) and advertising sys-
tems (Jin et al., 2020). Due to the lack of parallel
data, text style transfer is usually treated as an un-
supervised learning task.
One of the mainstream approaches is to exploit
the Autoencoder to disentangle the style and con-
tent representation of the source text, then use
the decoder to generate a transferred text with the
disentangled content representation and the target
style (Fu et al., 2018; John et al., 2018; Shen et al.,
2017; Nangi et al., 2021). The weakness of sucha sequence-to-sequence manner is that the model
usually performs poorly on content preservation
because the intrinsic of generative models bring
a significant difference between source and target
text. Another branch of models follows the mask-
ing and filling scheme (Devlin et al., 2018; Xu et al.,
2018; Sudhakar et al., 2019; Wu et al., 2019), which
first explicitly masks out the stylistic words of the
source style in the text and then replaces them with
words from the target style. These models usually
use either average attention scores (Bahdanau et al.,
2014) or the higher frequency ratio as criteria to
select the stylistic words in the word masking pro-
cedure. However, these masking approaches may
mistakenly remove the non-stylistic words when
the attention distribution is insignificant (Lee et al.,
2021).
When filling the masked words, previous works
utilize BERT (Devlin et al., 2018) to fill the masked
positions with the supervision from the style classi-
fier (Wu et al., 2019) or retrieve words with similar
attributes (Li et al., 2018; Sudhakar et al., 2019)
from the target domain. The limitation of these
filling approaches is that the diversity and semantic
consistency of filled words are not guaranteed. We
argue that diversity and semantic consistency are
significant in text style transfer. An ideal style trans-
fer model should replace source stylistic words
with their semantically-consistent counterpart in
the target domain. For example, in sentiment trans-
fer task, it is more reasonable to transfer "fast" to
"slow" rather than "poor", and "worst" is preferred
to be transferred from "best" instead of "good".
To overcome the imprecise word masking and
the limitation of lacking diversity and consistency
in word filling, in this study, we propose an adver-
sarial masking approach and a styled filling tech-
nique for the text style transfer task. Specifically,
to improve the word masking quality, we introduce
an adversarial gating strategy in training the mask
predictor. The mask predictor is trained as a gen-7654erator in an adversarial way with a discriminator
identifying the source style from the masked se-
quence. In addition, the number of masked words
is also restricted to prevent the mask predictor from
sacrificing non-stylistic words. The mask predictor
trained with an adversarial gating strategy is experi-
mentally shown to be effective in selecting stylistic
words and flexible in controlling of masked ratio
without additional statistical or manually labelled
information.
To improve the diversity and consistency of the
filled words, we develop a styled filling approach
based on BERT Masked Language Model to predict
the substituting vocabulary distribution. This ap-
proach introduces an entropy-based diversity loss
and a semantic-based consistency loss to encour-
age the word-level language diversity and semantic
consistency. Language diversity of styled filling is
realized by punishing the low diversity of generated
target stylistic words. The motivation to incorpo-
rate consistency loss is that source stylistic words
usually share similar context with their antonyms
and have closer word embedding vectors compared
to other words in the target domain. The designed
consistency loss can shorten the distance between
transferred word embedding and source stylistic
word embedding as the guidance for our model to
predict the antonyms.
Empirical studies show that our model AMSF
outperforms existing approaches in terms of the
overall score of content preservation and transfer
accuracy. In addition, further comprehensive anal-
ysis of the diversity and the semantic consistency
of generated text confirms the effectiveness of the
proposed approach.
In summary, the contributions of this study are
as follows:
•We introduce an adversarial masking ap-
proach that can more accurately mask out
stylistic words without additional statistical
information and automatically generate a gate
sequence without introducing extra manually
set rules.
•We present a word filling method that lever-
ages the semantic distance and utilizes the
vocabulary entropy to improve the semantic
consistency and language diversity of gener-
ated text.
•The empirical studies on two benchmark textstyle transfer data sets demonstrate the superi-
ority of our proposed approach AMSF.
2 Related Work
Due to the scarcity of parallel data in text style
transfer, the mainstream recent research has re-
garded the task of text style transfer as an unsu-
pervised learning task.
In recent works, one category of the methods
aims to first implicitly filter out the style-related
words in the text by learning a latent representa-
tion of content and style and then generating new
text of the target style. Hu et al. (2017) learns the
latent representation of text utilizing variational
autoencoder (V AE) (Kingma and Welling, 2013)
framework and utilizes a style classifier to learn a
style attribute vector. Fu et al. (2018) leverages an
adversarial network to train a content encoder, the
encoded content vector is then transferred by the
style-specific decoder. John et al. (2018) adopts
V AE to separate the style representation and con-
tent representation, with the decoder directly taking
the concatenation of the encoded content represen-
tation and target style representation as input.
There is a branch of study that performs text style
transfer task without disentangling the source text
into the style and content. For instance, the dual
structure, which learns the source-to-target and
target-to-source mappings, is applied to achieve the
goal. One of the works(Li et al., 2020) trains the
transfer models in the way of training de-noising
autoencoders (DAE) (Vincent et al., 2008) with
noisy text using neighbourhood sampling approach
and the models are trained cyclically to reconstruct
origin text from noisy input.
Another mainstream of the works attempts to ex-
plicitly replace the keywords and generate a text of
the target style. Identifying style-related words in
the given text is of great importance in this method.
While Li et al. (2018) selects the stylistic words
according to their frequency in text from a spe-
cific style, most early works utilize the attention
mechanism as an indicator of stylistic words. Xu
et al. (2018) and Wu et al. (2019) take the aver-
age attention score as the threshold for filtering
out the style-related words. Sudhakar et al. (2019)
calculates attention on each (layer, head) pair of
Transformer (Vaswani et al., 2017) and selects one
pair with the highest score. Lee et al. (2021) also
leverages attention score as style information, but
the reversed attention is taken as weights on content7655
representation instead of masking words directly in
this work.
3 Approach
3.1 Task Definition
Text style transfer aims to convert the style of the
given text, e.g., from the positive sentiment to the
negative sentiment. Formally, we are given two
non-parallel corpus D={X}andD={Y},
with corresponding styles sandsrespectively.
Each XorYrepresents a text in the corpus. The
goal of text style transfer is to train a style transfer
model that enables to transfer a text from source
style totarget style while preserving its original
content. Namely, it should convert a text Xwith
stylesto style sor conversely transfer a text Y
with style sto style s.
3.2 Model Overview
We propose a model that converts the style of a
text by identifying the stylistic words and explic-
itly replacing them. To that end, we build a three-
phase training procedure. The first phase is to train
a mask predictor Gin both corpora that produce
a mask vector indicating the positions of stylis-
tic words in a text which should be replaced by
the opposite-style words. The second phase is to
pre-train a cloze model Cusing a source-style cor-
pus to predict new stylistic words at the positions
masked by the trained mask predictor G. The third
phase is to train a styled filling model by fine-tuning
cloze model Cpre-trained on the target-style cor-
pus. This fine-tuning process is also supervised by
a frozen classifier pre-trained on both source-styleand target-style corpus. The network architecture
is illustrated in Figure1.
To simplify the description of the model, we
only discuss the style transfer process when sand
sare respectively treated as the source and target
style with source corpus Dand target corpus D.
A similar process is adopted when we transfer the
text from style stos.
3.3 Mask Predictor
The mask predictor takes a source text Xas
the input and produces a mask vector g=
[g, g,···, g], where g∈ { 0,1}implies
whether the tword in text Xis a stylistic word.
g= 0indicates that the tword in the source text
is related to the style of the text and thus should be
masked out when transferring to target style. g= 1
indicates the ttoken of the source text is more
related to content other than style and therefore
should be preserved. For example, if Xis “The
waiters are friendly andnice”, the mask predictor
should generate a mask vector g= [1,1,1,0,1,0],
which indicates to mask out the stylistic words
friendly andnice. A masked sequence ˜Xis then
produced by replacing stylistic words in Xwith a
special token [MASK]. Namely, “The waiters are
[MASK] and [MASK]”.
In this paper, we propose a novel mask predictor
by an adversarial gating strategy that simultane-
ously trains a Discriminator Dto identify the style
of the input text and a Gate Generator Gto cheat
the discriminator and predict the mask vector. We
next introduce the Discriminator, Gate Generator,
and the adversarial training process in detail.
Gate Generator. The Gate Generator first encodes7656the input text Xby a BERT encoder, and produces
a sequence of hidden states {h}, each corresponds
to a word in X. Let ¯hbe the average vector of all
hidden states in {h}, the binary indicator gfor
thetword in Xis then computed by
g=1
1 + exp( −h¯h)(1)
In practice, the number of words that should be
masked in a text is unknown to the model, and
the Gate Generator tends to mask out more words
than needed. To overcome this unexpected over-
masking problem, we regularize the Gate Generator
by the following loss L.
L=λ/radicaltp/radicalvertex/radicalvertex/radicalbt(/summationdisplayg− |X|(1−α)) (2)
where αandλare hyperparameters.
Lmakes the number of masked words control-
lable. It encourages the mask predictor to mask
out no more than α|X|tokens, thus preventing the
model from masking more words than expected.
We can also analyze the trade-off between style
transfer strength and content reservation by adjust-
ingα.
Discriminator. The Discriminator Dtakes the
masked word embedding sequence uof a text as
input and identifies the style sof the text. Specifi-
cally, let xbe the embedding of the tword in X,
then the telement in the masked word embedding
sequence ˜xis computed by
˜x=gx (3)
A bidirectional GRU is then used to summarize
the masked word embedding sequence to a vector
representation mfollowed by a softmax classifier
P= softmax( Wm+b) (4)
where Pis a two-dimensional vector that repre-
sents the probabilities of a text belonging to sand
s.
Letsbe the style of u, the Discriminator Dis
then trained on the total dataset {D,D}with the
following cross-entropy loss:
L=−E[log(s|u)] (5)
It is worth noting that as the Discriminator Dis
also used as a frozen classifier to supervise the style
transfer model at the third training phase, it is thusfirstly pre-trained on the total dataset {D,D}. It
is optional to keep this pre-trained classifier as the
Discriminator (Fixed θ) or further tune it (Learn-
ableθ) in adversarial training.
Adversarial Training. The Gate Generator Gand
Discriminator Dare trained in an adversarial man-
ner. Specifically, the Gate Generator Gis trained to
generate a mask vector that indicates the positions
of the stylistic words and challenges the Discrimi-
nator D, whereas the Discriminator Dtries to iden-
tify the source style. The loss of the adversarial
training process is:
L=−wL+wL (6)
where wandware hyperparameters that control
the training of Gate Generator Gand Discriminator
D. The two components are trained iteratively to
enable the Gate Generator to automatically produce
mask vectors that indicate the stylistic words. We
expect this adversarial gating strategy without the
need for additional statistical information and man-
ually set threshold to be a more powerful alternative
to existing mask techniques.
3.4 Cloze Model
After obtaining the mask vector of a text, the next
training phase is to learn a model that can replace
the [MASK] tokens in the masked sequence with
stylistic words. This can be realized by introduc-
ing a Cloze Model Cthat completes the words at
the positions of the [MASK] tokens based on the
contextual information. In this work, we use BERT-
based Masked Language Model (Devlin et al.,
2018) as the Cloze model C,Cand pre-train
them with corpus D,Drespectively.
To simplify the discussion, we take the pre-
training process of Cas an example, with C
pre-trained in the same way. The Cloze Model
Cis trained to recover the stylistic words in
Xfrom the masked text ˜Xat the positions of
[MASK] tokens. At each position iof sequence
˜X, the Cloze Model Coutputs a score vector
c={c, c,···, c}, where Vdenotes the
vocabulary. For each position of the [MASK] to-
ken, we can then compute its probabilities of being
replaced by the words in the vocabulary as
P= softmax( c) (7)
The Cloze Model Cis also optimized by the cross-
entropy loss based on the expected stylistic word
in text X.76573.5 Styled Filling
In the second training phase, the Cloze Model Cis
optimized to recover the stylistic words masked by
the Mask Predictor, it, however, can not be directly
applied to style transferring because it only learns
to recover the words from s. To enable the Cloze
Model Cto fill in words with s, we introduce a
third training phase to fine-tune the Cloze Model
Con the source corpus D. This fine-tune phase
furthermore takes into account the diversity and se-
mantic consistency of the recovered stylistic words.
We next introduce this training phase in detail.
Styled Filling Guided by Discriminator. To use
the Cloze Model Cin filling in target-style words,
we utilize the frozen classifier Dpre-trained on
the total dataset {D,D}to supervise the Cloze
Model C. As this frozen classifier has powerful
style classification ability (97.5% and 99.6% ac-
curacy on the validation set of Yelp and IMDB
respectively), it provides positive guidance encour-
aging the Cloze Model Cto replace stylistic words
fromswith the target style s, when the classifier
Dis forced to identify the replaced text as a target-
style text. Specifically, let ˜xdenotes the iword
in the masked sequence ˜Xof target style and V
denotes the jword in the vocabulary, we define a
function σupon each output score cof the Cloze
Model Cas follows
σ(c) =

c, g= 0,
0, g= 1,˜x̸=V,
1, g= 1,˜x=V,(8)
The above function keeps the scores of the stylistic
words unchanged and re-scales the scores of non-
stylistic words. As the inputs to the frozen classifier
Dis the word-embedding sequence, and the back-
propagation would be failed with hard selection of
word embedding, we thus choose to approximate
the word embedding by weight sum embeddings
in the vocabulary. To that end, we recompute the
probabilities of each position on the vocabulary by
P=σ(softmax(c
β)) (9)
where β <1is a hyperparameter that ensures the
weighted summed word embedding close to real
word embedding by a sharpness operation. Let v
as the word embedding of V, theiembedding
input to the frozen classifier Dis then computed as
o=/summationdisplayPv (10)where Pis the jelement of P. The style of
the input text Pis then predicted by inputting the
embedding sequence {o}and we have the follow-
ing cross-entropy loss
L=−E[logP(s|{o})] (11)
Diversity Loss. Supervision from pre-trained clas-
sifierDmay lead the Cloze Model Cto repeatedly
produce high-frequency stylistic words and result
in poor diversity. To cope with this problem, we
introduce a diversity loss Lthat can increase the
diversity of generated words by maximizing the
entropy. Specifically, let ˆcdenote the score vector
of the igenerated word produced by the Cloze
Model CandPdenote the probability of this
word to be the jvocabulary word, we have
P= softmax( ˆc) (12)
Let¯Pdenote the vocabulary distribution averaged
over all replaced words and ¯Pdenote its jele-
ment, the entropy loss is then defined as
L=/summationdisplay¯Plog¯P(13)
The above entropy loss encourages the generated
words to be more evenly distributed in the vocabu-
lary and prevents the Cloze Model from constantly
generating high-frequency stylistic words, thus in-
creasing the diversity of generated words.
Consistency Loss. We expect the Cloze Model to
maintain semantic consistency with the source-
style text while transferring to the target style. For
example, when the source-style text is “The service
is fast. ” , we prefer the output “The service is slow. ”
to“The service is poor. ” We design a consistency
lossLto keep the semantic consistency of the
model. Note that word embedding is constructed
based on the assumption that words in similar con-
texts should have similar meaning (Hill et al., 2014).
The key idea is that source stylistic words usually
share similar context with their antonyms and thus
have a closer word embedding vector compared to
other words in the target domain. Specifically, let
Edenotes the average embedding of the stylistic
words in the source text. EandˆP={ˆp}denote
the average embedding of maximum probability
filled words with the target style and corresponding
probabilities. The consistency loss is then formu-
lated as
L= (cos( E,E)−1)×log(/productdisplayˆp)(14)7658where function cosdenotes the cosine similarity.
The overall loss of style transferring is:
L=wL+wL+wL (15)
where w,wandware hyperparameters that
control the three components of loss functions.
4 Experiment
4.1 Dataset
The proposed models are evaluated on Yelp (Shen
et al., 2017) and IMDB (Dai et al., 2019) data sets.
Yelp consists of review data for businesses includ-
ing restaurants and home services. Following previ-
ous works, we split the Yelp into 444,101 texts for
training, 63,483 texts for validation and 126,670
texts for test. As for IMDB movie review data set,
it consists of 366,466 texts for training, 4,000 texts
for validation and 2,000 texts for test set.
4.2 Evaluation Metrics
To comprehensively evaluate the proposed model,
we conduct two aspects of evaluation. Specifically,
we evaluate the model on four automatic metrics
and human evaluation metric.
Automatic Evaluation Metrics. Automatic evalu-
ation metrics include the follows: ACC measures
how accurately the model transfer the text style.
BLEU compares the gold reference and transferred
text to measure how well the non-stylistic text to-
kens are retained after being transferred from the
original text (Papineni et al., 2002). PPL is used
to measure the fluency of the transferred text (Lee
et al., 2021; Heafield, 2011). G-mean is the geo-
metric mean of BLEU and ACC.
Human Evaluation. We conduct human evalua-
tion to more flexibly and comprehensively evaluate
the models. Based on the experimental results on
Yelp and IMDB, the style transfer accuracy, content
preservation and fluency are measured separately.
We randomly sample 100 outputs from test sets for
each model. Given the source style and source text,
the annotators are asked to score the generated text
in the range from 1 (Very Bad) to 5 (Very Good).
We compare the models with average scores given
by three annotators as shown in Table 2.
Baseline Models. We compare AMSF with the
following models: Dis V AE (John et al., 2018),
T-V AE-VF (Nangi et al., 2021) , D&R (Li
et al., 2018), B-GST (Sudhakar et al., 2019),
RACoLN (Lee et al., 2021), NAST (Huang et al.,2021), DGST (Li et al., 2020), ControlledGen (Tian
et al., 2018), StyleTransformer (Dai et al., 2019).
4.3 Implementation Details
In the experiment, we set the dimensions of word
embedding and hidden state in GRU as 512 and
250, respectively. Following the previous work
(Wolf et al., 2020), we train the BERT model
with batch size of 128 and 5e-5 lr. When train-
ing the mask predictor with a learnable discrimi-
nator, we set w= 0.6, w= 2and the training
process runs 45 epochs on Yelp and 50 epochs on
IMDB. When the discriminator is fixed, we set
w= 1.5, w= 0.1and the training process
runs 20 epochs on Yelp and 30 epochs on IMDB.
When training the Cloze model, the pre-training
runs 50 epochs with dataset masked by mask pre-
dictor. During fine-tuning, Cloze Models runs 10
epochs with masked dataset masked by mask pre-
dictor with w= 1, w= 0.3, w= 1.5, β= 0.5.
The hyper-parameters are tuned by experience and
the models are trained on V100 NVLINK GPUs.
4.4 Results and Analysis
Overall Performance On Automatic Evaluation.
The Automatic evaluation results on Yelp and
IMDB are presented in Table 1 where AMSF
(Learnable θ) refers to mask predictor trained
with discriminator, and AMSF (Fixed θ) refers to
mask predictor with fixed discriminator.
On both Yelp and IMDB, all of our models out-
perform the baseline models with G-mean score
and AMSF (Fixed θ) remarkably improves on
G-mean by at least 7 points on Yelp and 8 points
on IMDB. In terms of ACC score, AMSF reach7659
the SOTA score of 98.1% and 95.3% respectively.
As for content preservation, our proposed models
achieved competitive results. Especially, on IMDB,
AMSF achieve the highest BLEU.
It is worth mentioning that there is a tendency
of trade-off between style accuracy and content
preservation. As a result, measuring the balanced
performance of these two metrics, G-mean, which
is the geometric mean of ACC and BLEU, is more
important. As shown in the Table 1, our models
outperform all other baselines on G-mean. In con-
clusion, automatic evaluation results on Yelp data
set and IMDB data set confirm that AMSF-based
models are able to attain SOTA score on ACC and
at the same time get highly competitive scores on
BLEU. These experiment results prove that AMSF
can achieve a balanced performance of both style
accuracy and content preservation.
Overall Performance On Human Evaluation.
We compared our model AMSF (Learnable θ)
and AMSF (Fixed θ) with Disen V AE and DGST.The average scores evaluated by three annotators
on three dimensions: style transfer strength (Sty),
content preservation (Cont) and fluency (Flu) are
shown in Table 2. Our models not only outperform
other models in terms of style transfer strength and
content preservation, which is in accordance with
the results of the automated evaluation, but their
performance on fluency is also competitive.
Trade-off between BLEU and ACC. We further
explore and analyze the experiment result of the
trade-off between BLEU and ACC. This can be
analysed by adjusting a masking ratio of stylistic
words. Specifically, the mask predictor’s hyper-
parameter αis used to adjust the ratio of mask-
ing stylistic words in the source text. As shown
in Figure 2 (a), it can be observed that the like-
lihood of text being masked is increased and the
percentage of stylistic words in all masked words
is decreased when αis bigger (for instance, when
αis 0.30 rather than 0.05). Therefore, by adjust-
ingα, we can achieve a compromise between style
transfer accuracy and content preservation. That
is, raising αleads to more replacement in text and
hence a lower BLEU score, but on the other hand,
it increases transfer accuracy, as shown in Table 3.
Figure 2 (b) and (c) are the trade-off curve depict-
ing performance of our model compared with other
baseline models (Li et al., 2020; Lee et al., 2021;
Huang et al., 2021; Sudhakar et al., 2019; Li et al.,
2018) on Yelp and (Li et al., 2020; Lee et al., 2021;
Dai et al., 2019; Tian et al., 2018) on IMDB. The
score points of the baseline models are below the
trade-off curve of our model, explaining that our
model achieves higher transfer accuracy when the
content preservation score is similar and achieves
better content preservation when the style transfer
accuracy is at the same level.7660
Mask Predictor Performance. Following the pre-
vious works that use attention scores as an indica-
tor of masking stylistic words, we trained a BERT
structural attention-based classifier on Yelp and uti-
lized the average attention score as the threshold
for filtering stylistic words. This attention-based
classifier obtained 98% accuracy on the validation
set. Data is sampled from Yelp with manually la-
belled stylistic words to evaluate the performance
of mask predictors trained in adversarial and atten-
tion methods. The precision score and recall score
on sampled data are shown in Teble 4.
Our adversarial strategy (Adv BERT) surpasses
the attention-based method (Att BERT) in terms of
precision, recall and F1-score. The significant im-
provement in precision score, in particular, demon-
strates that our method can precisely mask stylistic
words without compromising non-stylistic words.
Effectiveness of diversity loss. To measure the di-
versity of the generated target-style text, the trace of
the covariance matrix of word embedding (Trace)
and the entropy of the infilled word distribution
(Entropy) on test dataset are introduced. For the
Trace, a higher score indicates greater variance in
predicted words. For Entropy, a higher value de-
notes a more diversified distribution. We conduct
experiments on Yelp data by varying the weight
ofLin training and evaluate the Trace and En-
tropy metrics. As illustrated in Table 5, it has been
shown that more diverse words are predicted with
the increasing weight of diversity loss. This fact
confirms that our model effectively ensures the di-
versity of the generated words in target style.
The distributions of the predicted words over
vocabulary with different weights of Lare il-
lustrated in Figure3. In Figure 3 (a), it is shown
that the model tends to repeatedly predict the same
word to cater to the classifier without punishment
L. In Figure 3 (b), it is observed that the pre-
dicted vocabularies are more diverse as the weight
of diversity loss is increased to 0.3. Figure 3 (c)
illustrates the vocabulary distribution when the
weight of diversity loss is set to 0.5. It is ob-
served that the occurring vocabs are more evenly
distributed in this setting, however, the non-stylistic
words such as feeling are predicted with higher fre-
quency. This suggests that by adjusting the weight
of diversity loss in the training phase, we may be
able to keep a balance between linguistic variety
and stylistic correctness in the transferred text.7661
Effectiveness of consistency loss. The effective-
ness of consistency loss is assessed by comparing
aligned word pairs generated by the models trained
with and without the consistency loss. Some stylis-
tic words from the source text and their most fre-
quently transferred words on Yelp and IMDB are
listed in Table6. The result indicates that the
model trained with Lperforms better in terms
of maintaining lexical consistency and transferring
to antonyms when transferring the same stylistic
word from the source domain to the target domain.
Style Transfer Examples. The examples of trans-
ferred text results with different trade-off ratios α
corresponding to the source text on two data sets
are shown in 7 and 8 with replaced stylistic words
emphasized.
5 Conclusion
In this paper, we proposed adversarial masking and
styled filling model AMSF to address the prob-
lem of text style transfer with unparalleled corpus.
AMSF improves the word masking quality by train-
ing the mask predictor in an adversarial way and
promotes the diversity and semantic consistency of
generated sentences by regularization losses. The
experimental results on Yelp and IMDB data sets
demonstrate that our model is competitive in terms
of content consistency and transfer strength. Hu-
man evaluation results further corroborate our styletransfer model’s superiority as well as competitive-
ness in fluency. The effectiveness of our proposed
approach in promoting language diversity and se-
mantic consistency is also verified by the ablation
study.
Limitations
It is also worth noting that our model simply substi-
tutes the stylistic words from the source text right in
the same place. This pattern is not flexible enough
when it comes to more intricate cases. Besides,
the masking and filling scheme meets the inher-
ent linguistic properties of the sentiment text style
transfer task but is not necessarily applicable to
other domains. In future work, we will improve
our model and conduct experiments on domains
other than sentiment.
Acknowledgement
This work was supported in part by the Na-
tional Key R&D Program of China under Grant
2021ZD0110700, in part by the Fundamental Re-
search Funds for the Central Universities, in part by
the State Key Laboratory of Software Development
Environment.7662References7663