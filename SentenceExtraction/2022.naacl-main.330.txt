
Emmy Liu, Chenxuan Cui, Kenneth Zheng, Graham Neubig
Language Technologies Institute
Carnegie Mellon University
{mengyan3,cxcui,kzheng2,gneubig}@cs.cmu.edu
Abstract
Figurative and metaphorical language are com-
monplace in discourse, and ﬁgurative expres-
sions play an important role in communica-
tion and cognition. However, ﬁgurative lan-
guage has been a relatively under-studied area
in NLP, and it remains an open question to
what extent modern language models can inter-
pret nonliteral phrases. To address this ques-
tion, we introduce Fig-QA, a Winograd-style
nonliteral language understanding task consist-
ing of correctly interpreting paired ﬁgurative
phrases with divergent meanings. We evalu-
ate the performance of several state-of-the-art
language models on this task, and ﬁnd that al-
though language models achieve performance
signiﬁcantly over chance, they still fall short
of human performance, particularly in zero- or
few-shot settings. This suggests that further
work is needed to improve the nonliteral rea-
soning capabilities of language models.
1 Introduction
All our words are but crumbs that fall down from
the feast of the mind (Gibran, 1926). When humans
read such a metaphorical phrase, how do they inter-
pret it? Conceptual metaphors structure our every-
day language and are used to map everyday phys-
ical experiences and emotions onto abstract con-
cepts (Lakoff and Johnson, 1981). They allow us
to communicate complex ideas, to emphasize emo-
tions, and to make humorous statements (Fussell
and Moss, 2008). However, despite relating words
in a way that differs from their accepted deﬁni-
tion, these phrases are readily interpreted by human
listeners, and are common in discourse (Shutova,
2011), occurring on average every three sentences
(Mio and Katz, 1996; Fussell and Moss, 2008)
The ability to interpret ﬁgurative language has
been viewed as a bottleneck in natural language un-derstanding, but it has not been studied as widely as
literal language (Shutova, 2011; Tong et al., 2021).
Figurative language often relies on shared common-
sense or cultural knowledge, and in some cases may
be difﬁcult to solve using language statistics. This
presents a challenge to language models (LMs), as
strong LMs trained only on text may not be able
to make sense of the physical world, nor the social
or cultural knowledge that language is grounded in
(Bender and Koller, 2020; Bisk et al., 2020).
Most previous work on ﬁgurative language fo-
cuses on metaphor detection, where a model is
trained to identify the existence of metaphors in
text (Tsvetkov et al., 2014; Stowe and Palmer,
2018; Leong et al., 2020), with datasets consisting
mostly of conventionalized metaphors and idioms
in wide use. However, identifying these common
metaphors that already appear often in language
may be an easy task for LMs, and not fully test
their ability to interpret ﬁgurative language. The
little work that exists on metaphor interpretation
frames it as a task linking metaphorical phrases
to literal rewordings, either through paraphrase de-
tection (Bizzoni and Lappin, 2018) or paraphrase
generation (Shutova, 2010; Su et al., 2017; Mao
et al., 2018) (details in § 7) Another line of work
probes for metaphorical understanding in LMs, but
this is similar to the metaphor detection task, in that
the LM is not actually asked to choose an interpre-
tation for the metaphor (Pedinotti et al., 2021; Ag-
hazadeh et al., 2022). While interesting, this work
does not take into account the fact that metaphors
are rich with different implications that may vary
depending on the context.
In this work, we ask whether or not LMs can
correctly make inferences regarding creative, rela-
tively novel metaphors generated by humans. This
task is harder for two reasons: (1) inference is
harder than identiﬁcation orparaphrasing , as it
requires understanding the underlying semantics,
and (2) the metaphors in our dataset are novel cre-4437ations, and many may not appear even once in the
LMs’ training data. We propose a minimal task
inspired by the Winograd schema (Levesque et al.,
2012), where LMs are tasked with choosing the
entailed phrase from two opposite metaphorical
phrases. An example of a paired sentence is "Her
commitment is as sturdy as (plywood/oak)". The
correct answer would be either "She was (commit-
ted/uncommitted)". This can also be seen as an
entailment task, where input xis the premise, and
the outputyis the hypothesis.
We crowdsource a benchmark Fig-QA , consist-
ing of 10,256 such metaphors and implications
(§ 2), which can be used to evaluate the nonlit-
eral reasoning abilities of LMs or for more broad
studies of ﬁgurative language in general (we pro-
vide preliminary analyses in § 3). Through exten-
sive experiments over strong pre-trained LMs (§ 4),
we ﬁnd that although they can be ﬁne-tuned to do
reasonably well, their few-shot performance falls
signiﬁcantly short of human performance (§ 5). An
in-depth analysis (§ 6) uncovers several insights:
(1) LMs do not make use of the metaphorical con-
text well, instead relying on the predicted proba-
bility of interpretations alone, (2) the task of asso-
ciating a metaphor with an interpretation is more
difﬁcult than the reverse, (3) even strong models
such as GPT-3 make inexplicable errors that are
not well-aligned with human ones, indicating that
further work is needed to properly model nonliteral
language.
2 Dataset Creation and Validation
2.1 Crowdsourcing Task
We crowdsourced data from workers on Amazon
Mechanical Turk ( details in Appendix A). Workers
were asked to generate paired metaphors with dif-
ferent meanings, as well as literal implications of
the two metaphors in context. We instructed work-
ers to try to generate rare or creative metaphors,
namely “metaphors that would not appear often in
text on the internet, books, social media, or news
sites, but that can still be easily understood by peo-
ple.” Workers were given examples of valid pairs
that ﬁt the format, and examples of invalid ones
to discourage errors. Some examples of generated
pairs are displayed in Table 1.In order to help workers, we employ the random-
ness as genesis andnarrow limits of change prin-
ciples of Cognitive Load Theory (Sweller, 2006).
To add soft constraints, we generate 3 different ran-
dom words to be shown to each batch of workers.
However, workers were not required to use these
words, as we wanted to encourage maximal diver-
sity. In order to ensure that the random words were
metaphorically rich, we selected them based on
metaphorical frames in Lakoff and Johnson (1981).
2.2 Data Validation
The dataset was manually validated by three au-
thors of this paper. Each author covered roughly
one-third, evenly split between training, validation,
and test. Examples were excluded if they (a) did
not make sense given the ﬁgurative expression, (b)
had grammar or spelling errors that rendered them
unintelligible, or (c) did not follow the format of the
task. Examples of excluded samples are included
in Appendix B. We collected 13,324 sentences and
interpretations from the crowdsourcing task, and
10,256 sentences remained after ﬁltering.
2.3 Final Dataset
The release version of our dataset contains the
named data splits in Table 2. The medium train,
dev, and test splits were generated from partitioning
the ﬁrst stage of data collected. The large train split
additionally contains all the new examples from the
second collection stage, and the small train split is
a small random sample.
3 Figurative Language Typologies
In this sample, we perform an analysis of the col-
lected data to demonstrate its trends and categorize
examples for further error analysis. Speciﬁcally,
we examine (a) subjects, objects, and relations, and
(b) types of common-sense knowledge needed to
interpret the metaphor.
3.1 Figurative Language Structure
We note that most metaphors and similes can
be characterized by three components, (S,R,O ),
whereSis a subject,Ris a relation, and Ois an ob-
ject. For instance, "Her commitment is as sturdy as
plywood" can be written (Her commitment, sturdy,
plywood). Interpretation involves inferring an at-
tribute of the subject by extracting a relational at-
tribute from the object (Fauconnier and Turner,
2003). In a simile, Ris explicit, whereas it is usu-
ally implicit in a metaphor. The most common4438
subjects, relations, and objects in the medium train
dataset are shown in Figure 1. These were obtained
by ﬁrst segmenting the phrases with syntactic pat-
terns constructed from observation, followed by
lemmatization and removal of punctuation and de-
terminers "the", "an", "a" and "that". There are 441
unique subjects, 646 unique relations, and 1,198
unique objects in the medium training set.3.2 Common-sense Knowledge Types
Next, we examined the test set to determine
the types of commonsense knowledge needed to
interpret metaphors. Through thematic analy-
sis, we devised 4 categories based on common-
sense knowledge, which are not mutually exclu-
sive: common-sense object knowledge, visual
metaphors, common-sense social understanding,
and cultural knowledge. The same 3 paper authors
annotated the test set for these categories, with an-
notators responsible for separate categories.
Common-sense object knowledge consisted of
metaphors that made reference to properties of com-
mon objects and animals, such as volume, height or
mass of objects, or properties of materials. 68.35%
of the test-set was found to require common-sense
object knowledge.
Visual metaphors were a subset of common-
sense object metaphors, primarily relying on the
visual modality, including attributes such as bright-
ness or colour. Some visual metaphors also
sketched a vivid visual scene. These examples
comprised 14.73% of the test set.
Common-sense social understanding exam-
ples required knowing about how humans would re-
act in different circumstances, or required knowing
about human emotions. These examples comprised
27.55% of the test set.
Cultural metaphors required knowing cultural
traditions, works of art/artefacts, or religion. Due
to crowdworkers being recruited from the US, these
are centered around US culture. These examples
comprised 16.56% of the test set.
4 Baseline Models and Evaluation
4.1 Auto-regressive Language Models
Auto-regressive LMs generate a probability distri-
bution of the next token given all preceding tokens.4439
As such, we can directly compute the probability
of a sentence by multiplying the conditional proba-
bility of each token at every time step.
˜P(w...w) =p(w)p(w|w...w)
The ability to directly extract probabilities en-
ables the zero-shot reasoning of these LMs. For a
pair of metaphorical expressions xandxwith
two corresponding interpretations yandy, we
feed in the concatenation of the metaphor and the
interpretation to the pretrained model without ﬁne-
tuning. We deﬁne “forward” and “backward” prob-
abilities assigned to interpretations and ﬁgurative
language expressions, respectively. For the for-
ward probability , for ﬁgurative phrase xand cor-
rect answery, we take
P(y|x) =P(x,y)
P(x,y) +P(x,y)
since there are only two answer options. From
this, we can calculate accuracy when we taking
the indicator of P(y|x)>0.5. Similarly for the
backward probability (predicting phrase based
on answer), we take
P(x|y) =P(x,y)
P(x,y) +P(x,y)
with analogous backward accuracy.
We examine three state-of-the-art large
transformer-based LMs of this category: GPT-2
(with 117M parameters, trained on 40GB of
text), GPT-neo (with 1.3B parameters, trained on
800GB of text) and GPT-3 (4 variants between
350M and 175B parameters, trained on 45TB
on text) (Radford et al., 2019; Black et al.,2021; Brown et al., 2020). We also examine the
performance of these models after ﬁnetuning on
the training data. GPT-2 and GPT-neo were trained
with a batch size of 8, with early stopping on the
medium dataset with a patience of 1 epoch, and
a minimal hyperparameter search was done with
learning rates 1e-5 to 5e-5. GPT-3 was trained with
the default parameters of the GPT-3 ﬁnetuning
API.
4.2 Masked Language Models
We also evaluate the performance of masked LMs
on this task. Unlike auto-regressive LMs, masked
LMs cannot directly output the probability of a
sentence, so it is not possible to directly test the
zero-shot performance of these models. Instead,
we test the transfer performance by ﬁrst ﬁnetun-
ing them in two ways: on WinoGrande, which
is also a binary choice task based on common-
sense reasoning, and on several NLI datasets, in-
cluding SNLI, MNLI, FEVER-NLI and ANLI
(Nie et al., 2020; Sakaguchi et al., 2020). The
input to the model trained on WINOGRANDE
is formatted as [CLS][metaphor][SEP]
[answer1][SEP][answer2] , and we use an
extra linear layer on the [CLS] token embedding
to perform the classiﬁcation. In addition to the
transfer performance, we also use contrastive ﬁne-
tuning by feeding in each metaphor along with both
answer choices, and training the model with our
dataset to classify which answer is correct. For the
NLI model, we examine accuracy using all three
labels the model was originally trained with (entail-
ment, neutral, and contradiction), as well as using
a forced binary choice paradigm in which the log-
its for the contradiction label are subtracted from
the logits for the entailment label, and the higher
"entailment score" is the ending the model pre-
dicts. We treat these two conditions as the analog
of “zero-shot" for these models.
We examine two masked LMs that are com-
monly used as baselines on many NLP tasks:
BERT (Devlin et al., 2019), a transformer-based
LM jointly trained on the masked LM and next sen-4440tence prediction objectives, and RoBERTa (Liu
et al., 2019), an improved variant of BERT which
consistently outperforms BERT across most tasks.
We use the large variant of both models (350M pa-
rameters). BERT and RoBERTa were ﬁnetuned on
the medium dataset for 8 epochs with batch size
8, following the setting in (Sakaguchi et al., 2020).
A hyperparameter search was done with learning
rates 5e-6 to 2e-5. Both BERT and RoBERTa were
used for the Winogrande experiments, while only
RoBERTa was used for the NLI experiment.
4.3 Forced-choice Paradigm
Due to the inherent creativity of metaphors, there
may be different interpretations of the same
metaphor. For instance, in Table 1, the exam-
ple "he hustles like he’s a billionaire’s son" could
also be interpreted in other ways, for instance "he
uses his father’s contacts and social privileges to
make money". In a structural-mapping context, the
forced choice between two answers constrains the
possible meaning of the metaphor to be along one
axis (Gentnder and Bowdle, 2008). In this case,
it would be whether or not he is required to work
hard.
Of course, many of these metaphors have other
valid interpretations. In the "billionaire’s son" ex-
ample, another valid axis of interpretation could
be the manner in which he works. For instance,
the alternative pair could be "he hustles like he’s
a (billionaire’s son | single mother working three
jobs)" with answers "he (uses his contacts and so-
cial privileges to make money | works extremely
long hours with multiple ventures to make money)".
It is possible that LMs could come up with other
valid interpretations that are not the ones originally
intended, motivating us to also look at generation
performance in section § 5.2.
4.4 Human Performance
To estimate the expected human performance on
this task, we ran a benchmark on the test set with 10
human volunteers who were not authors of the pa-
per. The human annotators were not shown any
training examples, so this would be equivalent
to the zero-shot setting for models. Participants
ranged from 20 to 29 years old, and there were 5
male and 5 female participants. 5 each were native-
and non-native English speakers respectively. Par-
ticipants were mainly graduate student volunteers.
We shufﬂed the test set and split it into 10 par-
titions of≈115 examples for each annotator. The
examples were presented with pairs shufﬂed and
separated, in order to create a better comparison
with model performance.
Due to differences in vocabulary or cultural back-
ground, we instructed participants to mark exam-
ples where they weren’t conﬁdent, such as those
that contained words or cultural references they
didn’t understand.
5 Results
5.1 Inference Results
The ﬁrst question is whether strong LMs can in-
terpret metaphors at all when presented with
two opposing meanings, in zero-shot or super-
vised settings . These results are presented in Ta-
ble 4. The results for masked language models
are higher than those for autoregressive language
models, and ﬁne-tuning signiﬁcantly improves per-
formance for all models.
Zero-shot Performance For the zero-shot set-
ting, we examine the test accuracy based on zero-
shot forward probabilities for the GPT models, and
the pseudo "zero-shot" transfer performance for
BERT and RoBERTa using models pretrained on
the WinoGrande task (Sakaguchi et al., 2020). As
shown, the GPT-3 models outperform the GPT-2
and GPT-neo models. Among the GPT-3 mod-
els, there is a clear correlation between model size
and performance, with the largest model (GPT-3
Davinci) achieving the highest zero-shot test ac-
curacy. BERT and RoBERTa achieve accuracies4441within the range of GPT-3 models. While our mod-
els mostly perform much better than chance in the
zero-shot setting, there is still a large gap of 26 per-
centage points between our best model and human
level performance.
Fine-tuned Performance For the ﬁne-tuned set-
ting, all listed models are ﬁne-tuned on the small
dataset split. GPT models were trained with lan-
guage modeling loss, whereas BERT and RoBERTa
are trained with contrastive loss. We did not eval-
uate ﬁne-tuning of GPT-3 Davinci due to bud-
get. Overall, ﬁne-tuning improved accuracy sig-
niﬁcantly for all models, with GPT-3 models uni-
formly improving by about 13 percentage points,
and BERT/RoBERTa improving by about 25 points.
Our best model after ﬁne-tuning is RoBERTa,
which reaches within 5% of our human perfor-
mance.
Prompting We also experiment with prompting
methods. Firstly, we use a simple sufﬁx prompt-
ing method, where we simply append the phrase
"that is to say" between the metaphor and the in-
terpretation, which we hypothesized may "explain"
to the LM that the previous statement is ﬁgura-
tive. We also evaluate the effectiveness of the ex-
amples method, by appending krandom correct
metaphor/interpretation pairs before the actual pair
we are testing. The results of these experiments
can be seen in Figure 2. We found that the suf-
ﬁx method provided a small (1-2%) improvement
over the baseline, while the example method was
generally ineffective.
Backward accuracies Note the accuracies re-
ported in this section are for the forward direc-
tion, and the backward direction is reported in Ap-
pendix C. Backward accuracies are lower, with
GPT-3 Curie for example having a 7% reduction
in accuracy in the zero-shot case. This suggests
that selecting a metaphorical expression to match a
literal phrase is more challenging than the reverse
for LMs.
Paired Evaluation Because our dataset is for-
matted as a Winograd schema, we can take ad-
vantage of group scoring to evaluate models more
stringently (Elazar et al., 2021). We found that
performance for autoregressive models plummeted
under this evaluation scheme, while masked lan-
guage models also suffered in accuracy. The hu-
man scores were least affected. Details are in Ap-pendix D. This is most likely related to the phe-
nomenon found in § 6.1.
5.2 Generation Results
Next, we examine if models can generate sensi-
ble interpretations for metaphors . Given the dif-
ﬁculty of evaluating text generation, compounded
by the difﬁculty of ﬁgurative language, we opted
for manual evaluation of one tenth of the test
dataset using generations of the strongest auto-
regressive model: GPT-3 Davinci ( ≈175B parame-
ters).
The metaphor was given as input to the model,
and 4 completions were generated for each
metaphor, with a maximum length of 100 tokens.
Completions were also truncated to the ﬁrst sen-
tence, as initial experiments showed that contra-
dictory statements (e.g. "he was talented. But he
was not very talented") were often generated across
subsequent sentences. Sufﬁx prompting was also
used because of the lack of context, with "That is
to say, " appended to each metaphor. Only the ﬁrst
sentence of the output was evaluated. The tem-
perature parameter was determined through grid
search through values [0.2, 0.4, 0.6, 0.8, 1] on a
small separate set of metaphors. A human annota-
tor inspected the generated completions and found
that a temperature of 0.4 produced the most correct
results.
Three paper authors labelled completions gener-
ated by GPT-3 Davinci as either correct, incorrect,
or literal. In some cases, there were valid inter-
pretations that were not the same as the answer
given by crowdworkers, which were also marked
correct. If the model simply restated the metaphor
with no interpretation, the completion was marked
as literal. Because some metaphors are ambiguous
when presented without context, those examples4442were not counted. The inter-rater reliability was
moderate due to differing standards for correctness
(Krippendorff’s α= 0.5567). The majority vote
was taken between annotators’ judgments.
GPT-3 Davinci’s accuracy, counting literalized
metaphors as incorrect, was 50.8%. Not count-
ing literalized metaphors, accuracy was 63.9%. In
37.7% of cases, GPT-3 generated contradictory
completions among the 4 completions. There was
at least one correct completion for 78.1% of the
metaphors, but only 19.3% of metaphors had all
completions correct. Examples of annotated gener-
ations can be found in Appendix G.
6 Performance and Error Analysis
With these results in mind, we examine what kinds
of errors models make, and what factors make
the task difﬁcult. . This is covered in § 6. We
ﬁnd that autoregressive models rely on the pre-
dicted probability of each answer by itself to pre-
dict the answer, and that this holds true for all
models, before and after training. We ﬁnd that
models have difﬁculty in interpreting "sarcastic"
metaphors, and sometimes inexplicably interpret
very simple metaphors wrong. We also examine
error typology according to the commonsense ty-
pology of § 3.2 and ﬁnd that models improve signif-
icantly on object, visual and social commonsense
when trained, but not on cultural commonsense.
6.1 Reliance on Probability of Answers
We ﬁnd that models often rely solely on the pre-
dicted probability of answers yandyto make
their ﬁnal predictions, regardless of the context.
This led models to make the same prediction for
the paired sentences in many cases. Figure 3 and
Table 5 show that this trend improves with ﬁne-
tuning, and that GPT-3 is best able to disentangle
the probability of yand the probability of P(y|x),
but all three models show a heavy tendency to pre-
dict based on the relative probability of an answer
alone.
We hypothesize that this may be one reason why
BERT and RoBERTa achieve the best ﬁnetuned per-
formance; they use a contrastive ﬁnetuning strategy
providing both the correct and incorrect options as
input to the model. On the other hand, the GPT
models were ﬁnetuned with only the correct option,
making the comparison unfair. One way to ﬁne-
tune GPT models contrastively is to include both
options into a cleverly engineered prompt, but we
leave this as a direction for future work.
6.2 Other Factors Inﬂuencing Correctness
We also examined the inﬂuence of several other fac-
tors on correctness. The point-biserial correlation
between length of the context phrase and the bi-
nary correctness value was -0.1544 with a p-value
of1.50×10, indicating that longer phrases are
harder to interpret correctly. The point-biserial cor-
relation between answer probability and binary cor-
rectness was 0.1840, with a p-value of 3.50×10,
indicating that examples where the answer was
already more probable were more likely to be an-4443swered correctly, in line with our ﬁndings that mod-
els tended to predict the answer that was already
more plausible alone.
Furthermore, we conducted an analysis on sub-
jects, objects, and relations as deﬁned in § 3.1. We
examined accuracy by part of speech patterns in
each part of the metaphor, as well as by wordnet
hypernyms present in each part of the metaphor.
This is detailed in Appendix E and Appendix F
(Fellbaum, 1998). We used NLTK for POS tagging
(Loper and Bird, 2002).
6.3 Qualitative Analysis of Error Trends
Common Sense Knowledge We ﬁrst examine
the error tendencies by the type of common sense
knowledge described in § 3.2. Table 6 summa-
rizes accuracies for these types of commonsense
questions compared to humans.
We ﬁnd that both humans and trained models
tend to ﬁnd object commonsense and visual com-
monsense metaphors easier to interpret. We ﬁnd
that as models improve, most of the performance
gain comes from the object, visual and social com-
monsense categories. Interestingly, the untrained
models do quite well on cultural examples, but do
not improve much on the culture category when
trained. This makes sense, as the cultural examples
tend to be quite disparate, so training would not
help as much with other examples.
Sarcastic Metaphors For both humans and
LMs, many of the errors are "sarcastic" metaphors,
such as saying "the girl was as bubbly as still water"
to mean "the girl was bland", rather than "the girl
was vivacious". These sentences can be difﬁcult ifthe model or human focuses on simple word asso-
ciation (bubbly with vivacious) without reading the
entire sentence to understand the sarcasm.
Inexplicable Errors We examined the errors
made by GPT-3 Curie (trained) and found that there
was little overlap with mistakes made by humans.
Of the 64 human errors, 13 were also errors made
by GPT-3. GPT-3 made many more "obvious" er-
rors, such as predicting "The ball is a big red sun"
to mean "the ball is small" rather than "the ball is
big and red" This is in contrast to the sentences in
which humans made errors, which often contained
rare vocabulary or unfamiliar cultural references.
7 Related work
7.1 Figurative Language Identiﬁcation
Most existing work focuses on identifying ﬁgura-
tive language at the word level. The VU Amster-
dam Metaphor Corpus (VUA) is the largest avail-
able corpus of metaphorical language, annotated by
humans (Steen et al., 2010). Two shared tasks on
metaphor identiﬁcation have been run (Leong et al.,
2018, 2020). Both have utilized the VUA corpus,
and the latter also introduced the TOEFL corpus,
sampled from essays written by non-native English
speakers (Leong et al., 2020; Beigman Klebanov
et al., 2018). Most participants in the shared tasks
used neural models, notably BERT, RoBERTa, and
Bi-LSTMs (Leong et al., 2020; Bizzoni and Gha-
nimifard, 2018; Gao et al., 2018; Pramanick et al.,
2018). These models are generally improved when
augmented with semantic data, such as concrete-
ness, and multimodal information.
Another line of work focuses on probing mod-
els to determine the extent of metaphor recogni-
tion. For instance, BERT assigns higher pseudo-
log-likelihood scores to metaphors than nonsense
expressions, and its contextualized representations
show some signs of contextualizing the object do-
main (Pedinotti et al., 2021). Another study uses
linear probes trained on layers of BERT to predict
whether a word usage is literal or nonliteral, and
ﬁnds that this can be done effectively, especially
using middle layers as a representation (Aghazadeh
et al., 2022),
Despite the utility of these tasks and datasets,
they have drawbacks. Most of the metaphor use is
conventional, so this task does not capture novel
metaphors well. The word-level annotation also
does not lend itself well to capturing extended con-4444ceptual metaphors. Finally, metaphor interpretation
may be a more difﬁcult, although less studied, task.
7.2 Figurative Language Interpretation
Recent studies mostly focus on metaphor para-
phrases, either through identiﬁcation (Bizzoni and
Lappin, 2018) or generation (Shutova, 2010; Su
et al., 2017; Mao et al., 2018). However, there has
not been as much work done on interpretation as
on detection, and framing metaphor interpretation
as a paraphrase task may not capture the emergent
meaning of metaphors, such as the intended emo-
tion, or the interaction of subject, relation and ob-
ject in the metaphor (Tong et al., 2021; Mohammad
et al., 2016).
Other work has focused on interpreting ﬁgurative
language in narratives in context, based on plau-
sible continuations of ﬁgurative language such as
idioms and similes from stories (Chakrabarty et al.,
2021a) or dialogues (Jhamtani et al., 2021). This
represents a promising direction, and our work fo-
cuses on expanding our understanding of LMs’ abil-
ity to interpret non-conventionalized metaphors.
7.3 Other Figurative Language Datasets
We note that there are several other challenging
NLI datasets available which contain ﬁgurative lan-
guage, including the DNC corpus, and the RTE
dataset (Poliak et al., 2018; Chakrabarty et al.,
2021b). Other datasets, such as RiddleSense, ex-
plicitly test models through difﬁcult commonsense
inference, involving ﬁgurative language (Lin et al.,
2021).
Our work is distinguished by the Winograd
schema format, as this format provides a better
guard against the possibility that models have sim-
ply memorized common word associations that oc-
cur in ﬁgurative language. Additionally, we specif-
ically instructed crowdworkers to be creative, and
this resulted in longer ﬁgurative phrases which re-
quire more detailed commonsense knowledge. It is
likely that a fair number of these ﬁgurative phrases
have never appeared in any training corpus. How-
ever, our ﬁgurative phrases also differ from riddles,
as they are not supposed to be difﬁcult to reason
about, given that the source, relation and object are
properly contextualized.
7.4 Human Language Processing
Humans typically do not have any more difﬁculty
processing metaphorical statements in context com-
pared to literal statements (Fussell and Moss, 2008;Glucksberg, 2003). This may be because certain
words serve as a dual reference , which is to say they
refer simultaneously to a physical referent and an
abstract superordinate category (Glucksberg, 2003).
For instance, "shark" may refer to literal sharks, as
well as anything that is considered vicious, leading
to utterances such as "that lawyer is a shark".
Metaphorical language processing has also been
studied in second-language learners, in the case of
idioms. In most cases, the meaning of an unfamiliar
idiom is inferred from the context or from word
association (Cooper, 1999; Carston and Wearing,
2011; Wolff and Gentner, 2000).
As LMs excel at word-association based tasks,
this is an encouraging ﬁnding. However, there is
still a gap between LM and human performance
even in our task, in which one answer is obviously
wrong when the input is correctly understood.
We take into account that these results are for
conventionalized ﬁgurative language and that some
of the more creative phrases in this dataset may take
a longer time to process for humans as well. This
is especially true for non-native English speakers.
However, the high human accuracy on this task
with half the participants being non-native English
speakers suggests that this was not a major barrier.
8 Conclusion
We present a Winograd-like benchmark task to test
the ability of LMs to reason about ﬁgurative lan-
guage, based on large-scale collection of creative
metaphors written by humans. We ﬁnd a large gap
between LM zero-shot and human performance on
this dataset, but show that models can be ﬁne-tuned
to perform well on this particular task.
We hope that this work will encourage further
study of nonliteral reasoning in LMs, especially
in few-shot settings. Given that metaphorical rea-
soning may play a role in problem-solving and
linguistic creativity, the development of models,
training methods, or datasets that enable metaphor-
ical reasoning may improve models’ abilities to
reason creatively and draw analogies between sit-
uations that may appear to be different on the sur-
face. One avenue we hope to investigate is multi-
modal metaphors, as this dataset currently includes
only text-based metaphors. Nonliteral expres-
sions also remain understudied cross-linguistically,
but further work on identifying and interpreting
metaphors in other languages may also improve
the abilities of multilingual models.44459 Ethical Considerations
9.1 Potential Risks
Figurative language has the potential to be used in
a harmful way, especially against minority and his-
torically disadvantaged groups. Such language is
often emotionally charged or used to insult others,
so we took care to remove any examples that were
potentially offensive, especially toward protected
groups. We acknowledge that this was based on
our own judgment, and generically insulting lan-
guage (for instance, a metaphor that implies that
someone is ugly) was not removed because it was
not insulting toward any particular individual.
All examples from Fig-QA are also in English,
as it is the language that all authors speak, and
this was a preliminary dataset, being the ﬁrst of its
type that the authors have worked on. However,
ﬁgurative language is not just important in English,
and we leave investigation of ﬁgurative language
in other languages as future work.
9.2 Terms of Use of Artefacts Used
Additional datasets we used were the Winogrande
dataset, SNLI, MNLI, FEVER-NLI and ANLI.
Winogrande is licensed under the Apache 2.0 li-
cense, which allows modiﬁcation and distribution,
ﬁtting our use case. SNLI is licensed under a Cre-
ative Commons Attribution ShareAlike 4.0 Interna-
tional license, which allows us to share and adapt
the work as long as we give attribution. The ma-
jority of MNLI is licensed under OANC, which
allows free use. The ﬁction section of this dataset
consists mostly of works in the public domain, but
several stories are licensed: Seven Swords is avail-
able under a Creative Commons Share-Alike 3.0
Unported License, while Living History andPass-
word Incorrect are available under Creative Com-
mons Attribution 3.0 Unported Licenses. These
licenses allow sharing and adaptation with attri-
bution. FEVER-NLI is licensed under an MIT
license, which also allows modiﬁcation, distribu-
tion, and reuse. ANLI is licensed under Creative
Commons Attribution-NonCommercial 4.0 Inter-
national, which also allows sharing and reuse as
long as we give attribution.
Models used were GPT-2, GPT-neo, GPT-3,
BERT and RoBERTa. GPT-2 and GPT-neo are
licensed under an MIT license, which does not
place any restrictions on its use. BERT is licensed
under an Apache License 2.0, which allows modiﬁ-
cation and distribution. RoBERTa is licensed undera GNU General Public License v2.0. This ﬁts our
use case as we are only running and studying the
model. GPT-3 is licensed by Microsoft, and we
used the public API to receive output.
9.3 Computational Infrastructure and
Computing Budget
To run our computational experiments, we had ac-
cess to a compute cluster, but minimal compute is
needed to run the experiments in this paper. We
generally did not use more than 2 GPUs at a time.
The only models that required GPU parallelism
were the GPT-neo models. An estimated 20 GPU
hours are required.
Our computing budget was roughly 100 USD.
We also used roughly 20 USD on credits for the
GPT-3 API.
Acknowledgements
We thank Pengfei Liu, Lyuyang Hu, and Chih-Hao
Wang for helping us set up the leaderboard for this
dataset on Explainaboard. We also thank Pengfei
Liu for helping run GPT-3, Danish Pruthi for guid-
ance on setting up the MTurk task, and all partic-
ipants who contributed to the human benchmark.
Lastly, we thank all the Turkers who contributed
metaphors to the dataset.
This work was supported in part by a CMU Pres-
idential Fellowship and National Science Founda-
tion Award No. 1761548.
References444644474448A Crowdsourcing Details
We crowdsource metaphorical expressions and
their interpretations through Amazon Mechanical
Turk. Workers were recruited from the United
States and were limited to those who had a >98%
approval rating on the platform, and who had also
completed more than 1000 Human Intelligence
Tasks (HITs). Data collection was split into two
stages: in the ﬁrst stage, 1458 train examples, and
all the dev and test examples were collected. In the
second stage, the remaining 6558 training examples
were collected. We identiﬁed some workers who
created especially good examples in the ﬁrst stage,
and recruited them back for more examples in the
second stage. Workers were paid $0.33 for each
pair of sentences and were asked to generate 3 pairs
at a time. An author of this paper wrote an initial
pilot set of sentences, and timed themselves while
writing some sentences. They found that each pair
took around 1 minute to write, though this varied
(less creative examples took less time, while more
creative examples took more time). This extrap-
olates to an hourly rate of 19.80USD, which is
above the minimum wage in all US states, where
workers were located.
Our HIT task was structured as follows: At the
top of the page, the workers are shown the follow-
ing instructions: "Your task is to generate three
pairs of sentences with opposite or very differ-
ent meanings, both of which contain rare/creative
metaphors, which means metaphors that would not
appear often in text on the internet, books, social
media or news sites, but that can still be easily un-
derstood by people. For each metaphor, you should
also provide a literal (non-metaphorical) sentence
with the same meaning." Then, we display one ex-
ample of a valid sentence pair. There is a button that
opens a modal with more detailed instructions and
some more valid/invalid examples for reference.
Below that, we display three random words, which
workers are encouraged to use in their sentences
if they get stuck. Finally, we display three sets of
5 text ﬁelds for workers to ﬁll in: one for the start
phrase, two for each metaphorical phrase, and two
for each literal interpretation. As the user types
in each start phrase, we prepend a copy of their
phrase before the corresponding metaphor ﬁelds in
the UI using some embedded JavaScript, which we
found helped reduce confusion and resulted in less
improperly formatted responses.
We launched many batches of these HITs untilwe had collected the desired quantity of data. Then,
we converted the form responses into sentence pairs
and validated each pair by hand before adding it to
our dataset.
B Invalid Examples
Figurative language examples collected from
crowdworkers were excluded if they (a) did not
make sense given the meaning and the metaphori-
cal expression, (b) had grammar or spelling errors
that rendered them unintelligible, or (c) did not
follow the format speciﬁed by the task template.
Examples are given below:
1.Do not make sense given the meaning and the
metaphorical expression
2. Signiﬁcant grammar or spelling errors
3. Do not follow format
Efforts were made to ensure that the ﬁnal dataset
contains no offensive content or personally iden-
tiﬁable information. WorkerID and other poten-
tailly personally identifying information were not
included.4449C Backward accuracies
Model Zero-shot Fine-tuned (L)
GPT-2 52.18 52.00
GPT-neo 1.3B 54.36 63.44
GPT-3 Curie 58.46 74.83
D Paired accuracies
ModelAccuracy
(pairs correct)
GPT-2 zero-shot 6.63
GPT-2 ﬁnetuned 5.06
GPT-neo zero-shot 10.3
GPT-neo ﬁnetuned 10.3
GPT-3 Curie zero-shot 17.4
GPT-3 Curie ﬁnetuned 50.0
BERT ﬁnetuned 70.6
RoBERTa ﬁnetuned 80.4
Human 89.7
E Accuracy breakdown by
Part-of-Speech
E.1 Subject
Part of speech Accuracy Frequency
NN 0.8569 538
PRP 0.8526 156
PRP$ NN 0.9 110
NN NN 0.8889 63
DT NN 0.8182 44
NN NN NN 0.9375 32
JJ NN 0.9167 12
E.2 RelationPart of speech Accuracy Frequency
VBZ NN IN 0.8421 152
VBD RB JJ IN 0.8904 146
VBZ RB JJ IN 0.8889 99
VBZ 0.8352 91
VBD NN IN 0.8806 67
VBD 0.9180 61
VBN IN 0.9545 22
NN IN 0.8636 22
VBD JJ IN 0.9048 21
NNS IN 0.8889 18
VBD IN 0.8462 13
VBZ IN 1.0 13
VBD RB VBN IN 0.8182 11
E.3 Object
Part of speech Accuracy Frequency
NN 0.8788 429
NN NN 0.8992 129
JJ NN 0.8352 91
NN IN NN 0.8372 43
JJ NN NN 0.8710 31
NN NN NN 0.9130 23
VBG NN 0.9545 22
NN IN JJ NN 0.6154 13
PRP$ NN 1.0 11
JJ 0.6364 11
NN IN NN NN 0.8182 11
F Accuracy breakdown by hypernyms
F.1 Subject4450Synset Accuracy Frequency
adult.n.01 0.8736 182
male.n.02 0.8684 152
woman.n.01 0.7391 46
female.n.02 0.9130 46
show.n.03 0.875 24
product.n.02 0.8636 22
motor_vehicle.n.01 0.9048 21
activity.n.01 0.8421 19
emotion.n.01 0.6667 18
publication.n.01 0.8333 18
feline.n.01 0.9375 16
being.n.01 0.7143 14
performer.n.01 0.8333 12
canine.n.02 12
body_covering.n.01 0.8333 12
vessel.n.03 0.8333 12
sound.n.01 1.0 12
domestic_animal.n.01 0.9167 12
person.n.01 0.8 10
scheme.n.01 0.9 10
contestant.n.01 1.0 10
F.2 ObjectSynset Accuracy Frequency
time_period.n.01 0.85 20
natural_object.n.01 0.8947 19
person.n.01 0.8824 17
large_integer.n.01 0.9286 14
adult.n.01 1.0 14
solid.n.01 0.9167 13
male.n.02 1.0 13
child.n.02 0.8333 12
body_of_water.n.01 0.75 12
body_covering.n.01 0.8333 12
digit.n.01 0.9167 12
region.n.01 0.8182 11
beverage.n.01 0.8182 11
juvenile.n.01 0.8182 11
container.n.01 0.9 10
rodent.n.01 0.9 10
feline.n.01 0.7 10
building.n.01 0.8 10
time_unit.n.01 0.9 10
travel.v.01 0.7 10
G Generation examples
Generation examples can be found in Table 17.44514452