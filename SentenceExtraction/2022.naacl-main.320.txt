
Wenxuan Zhou, Qiang Ning, Heba Elfardy, Kevin Small, Muhao ChenUniversity of Southern California,Amazon
{zhouwenx,muhaoche}@usc.edu {qning,helfardy,smakevin}@amazon.com
Abstract
Current question answering (QA) systems pri-
marily consider the single-answer scenario,
where each question is assumed to be paired
with one correct answer. However, in many
real-world QA applications, multiple answer
scenarios arise where consolidating answers
into a comprehensive and non-redundant set
of answers is a more efficient user interface.
In this paper, we formulate the problem of an-
swer consolidation , where answers are parti-
tioned into multiple groups, each representing
different aspects of the answer set. Then, given
this partitioning, a comprehensive and non-
redundant set of answers can be constructed
by picking one answer from each group. To
initiate research on answer consolidation, we
construct a dataset consisting of 4,699 ques-
tions and 24,006 sentences and evaluate multi-
ple models. Despite a promising performance
achieved by the best-performing supervised
models, we still believe this task has room for
further improvements.
1 Introduction
Open-domain question answering (QA) sys-
tems (V oorhees, 1999) aim to answer natural lan-
guage questions using large collections of refer-
ence documents, contributing to real-world appli-
cations such as intelligent virtual assistants and
search engines. Current QA systems (Zhu et al.,
2021) usually adopt a three-stage pipeline consist-
ing of: (1) a passage retriever (Yang et al., 2019;
Karpukhin et al., 2020) that selects a small set of
passages relevant to the question, (2) a machine
reader that examines the retrieved passages and ex-
tracts (Wang et al., 2017, 2019) or abstracts (Lewis
et al., 2020) the candidate answers, and (3) an an-
swer reranker (Wang et al., 2018; Kratzwald et al.,2019) that fuses features from previous stages to ei-
ther select one final answer or return the top-ranked
answer from the previous stage.
Current QA research (Joshi et al., 2017;
Kwiatkowski et al., 2019) primarily examines the
case where each question is assumed to have a sin-
gle correct answer. However, in practice, many
questions can have multiple correct answers. For
example, the question “ Is coffee good for your
health? ” can be answered with respect to different
aspects (e.g., “ coffee can help you with weight loss ”,
“coffee can cause insomnia and restlessness ”). To
correctly identify different aspects of answers to the
same question while mitigating aspect-level redun-
dancy, it is important to consolidate the answers.
Answer consolidation is particularly desirable for
applications such as intelligent assistants, where re-
sponses are desired to be both comprehensive and
concise. Additionally, in scenarios where QA is
used for knowledge extraction (Bhutani et al., 2019;
Du and Cardie, 2020) or claim verification (Yin and
Roth, 2018; Zhang et al., 2020), consolidation is
also an essential step to identify salient knowledge
or evidence while mitigating duplication.
To effectively recognize multiple aspects of an-
swers in QA systems, our first contribution is to
introduce and formalize the answer consolidation
problem. Specifically, given a question paired with
multiple answer snippets, answer consolidation
first partitions the snippets into groups where each
group represents a single aspect within the answer
space. Once partitioned, the final answer set is pro-
duced by returning a representative snippet from
each group. In this formulation, the answer con-
solidation task is a post-processing stage that takes
predicted answer-mentioning snippets (in this work,
sentences) from previous QA stages and produces
an answer set that maximizes answer aspect cover-
age while minimizing answer duplication.
To foster research on the answer consolidation
problem, our second contribution is the collec-4314tion of a new dataset, namely, QA(Question-
Answer consol idation). QAconsists of 4,699
questions such that each question is paired with
multiple answer-mentioning sentences grouped ac-
cording to different aspects. Starting with a Quora-
based question source (Chen et al., 2018), noting
the potential for multi-aspect answers, we first re-
trieve 10 relevant answer sentences from the web
for each question. These sentences are then ex-
amined by three crowd-sourced workers, who ex-
clude sentences that do not contain an answer and
group the remaining ones. Finally, individual sen-
tence groupings from different workers are aggre-
gated into a single partitioning. QAconsists of
24,006 sentences and 19,676 groups, correspond-
ing to an average number of 4.18 aspects per ques-
tion and 1.22 sentences per group.
Ourthird contribution is a comprehensive bench-
marking for the answer consolidation problem
based on QA. Specifically, we consider two
evaluation settings: (1) classification , where the
model predicts whether two sentences are in the
same group. (2) sentence grouping , where the
model groups the answer sentences. We evalu-
ate a wide selection of zero-shot and supervised
methods, including various SoTA sentence embed-
ding models (Reimers and Gurevych, 2019; Gao
et al., 2021), cross-encoders (Devlin et al., 2019;
Liu et al., 2019), and a newly proposed answer-
aware cross-encoder model. In the supervised set-
ting, the answer-aware cross-encoder achieves the
best results based on the Matthew correlation coeffi-
cient (MCC) score of 87.8% (classification setting)
and an adjusted mutual information (AMI) score
of 68.9% (sentence grouping setting). As this per-
formance is notably far from perfect, our findings
indicate the need for future investigation on this
meaningful, but challenging, task.
2 Related Work
QA with multiple answers. Many QA datasets
have assumed that a question has a single cor-
rect answer (Joshi et al., 2017; Kwiatkowski et al.,
2019), while in real scenarios, many questions can
have multiple correct answers. Fewer datasets for
QA or machine reading comprehension (MRC)
have been proposed with the consideration of multi-
ple answers. In extractive MRC, MASH-QA (Zhu
et al., 2020) allows one question to be answered by
multiple non-consecutive text spans. In abstractive
MRC/QA, MS MARCO (Campos et al., 2016) sim-ply treats different workers’ answers as different
answers. DuReader (He et al., 2018) merges similar
answers during data construction. QReCC (Anan-
tha et al., 2021) allows one worker to provide mul-
tiple different answers. Beyond these, WebQues-
tions (Berant et al., 2013) and GooAQ (Khashabi
et al., 2021) include lists of diverse answers, and
TREC-QA (Baudiš and Šediv `y, 2015) uses regu-
lar expressions to capture multiple answers. How-
ever, none of the aforementioned efforts have in-
vestigated effective consolidation of multiple an-
swers. In this work, we formally define and collect
a dataset for the answer consolidation problem as a
complement to previous work. From another per-
spective, AmbigQA (Min et al., 2020) focuses on
the case where a question can be interpreted in dif-
ferent ways, leading to the question disambiguation
task. This is fundamentally different from our work
that partitions answers to the same question into
different coherent subsets. Stance detection (Liu
et al., 2021) is concerned with the focused problem
of collecting approving/disapproving opinions for a
yes-no question, unlike our studied problem where
all multi-answer questions are considered and the
answers are not limited to binary opinions.
Answer Summarization. Questions with multi-
ple answers are common in online communities.
For example, Liu et al. (2008) observe that no
more than 48% of best answers on Yahoo! An-
swers are unique. Many efforts (Song et al., 2017;
Chowdhury and Chakraborty, 2019; Fabbri et al.,
2021) have been devoted to summarizing reusable
answers in community QA. Particularly, Answer-
Summ (Fabbri et al., 2021) proposes a dataset
where different answers are rewritten to bullet
points by humans. While training on summariza-
tion data may enable the model to return salient and
non-redundant answers, such training only works
for abstractive machine readers. A more related
work is BERT-DDP (Fujita et al., 2020), which
considers the problem of getting a diverse and non-
redundant answer set. They construct a dataset
based on Yahoo! Chiebukuro where workers are
asked to provide an answer set given a question.
However, the correct answer set is not unique when
answers are equivalent. As they treat all but the
annotated answer set as wrong, both training and
inference are prone to false negatives. In this pa-
per, we group the answers with respect to their
aspects and provide a discriminant rule, such that
the correct group assignment is unique.4315Diverse passage retrieval. Many information re-
trieval efforts address the problem of retrieving
diverse documents for a query (Clarke et al., 2008;
Fan et al., 2018; Abdool et al., 2020). In QA, Min
et al. (2021) examine answer diversity in passage
retrieval and propose a self-supervised dynamic or-
acle training objective. However, as passages may
contain irrelevant information to the question, the
retriever faces the challenge of identifying and inte-
grating answers in passages when assessing answer
diversity. In this work, we consider a dedicated
task of answer consolidation and leave the problem
of identifying answers to previous QA stages.
3 Answer Consolidation Task
Motivation. Many questions can have multiple cor-
rect answers, including questions explicitly asking
for a multi-answer list (e.g., What are the symptoms
of flu? ) or questions where different people have
different opinions (e.g., debate questions), amongst
others. To provide users with a comprehensive
view of the answers, the QA system needs to ac-
tively identify different answers as opposed to only
returning the most popular or top-ranked answer.
Additionally, as the same answer may be repeated
or paraphrased many times in the reference cor-
pus (e.g., web), the QA system may also need to
eliminate the redundant answers. We address these
requirements within answer consolidation.
Basic concepts. When answering a specific ques-
tion, different answers may be given regarding dif-
ferent perspectives, opinions, angles, or parts of the
overall answer. We regard such answers as those
pertaining to different aspects . Furthermore, we
refer to two sentences as equivalent if they contain
the same answer aspect(s) and distinct if they ex-
press different answer aspect(s). To better identify
equivalent/distinct sentences, we propose the fol-
lowing operational discriminant rule: Given two
answer-mentioning sentences s, sfor the same
question q, we can rewrite the answers contained
insandsinto yes-no questions qandq, which
can be answered by yes/no/irrelevant.Then, if s
andsgive coherent answers of yes/notoqand
q, respectively, then sandsare considered to
represent equivalent aspects. Otherwise, they are
considered distinct from each other.We take the following example:
Q: Is coffee good for your health?
S1: Coffee can make you slim down.
S2: Coffee can relieve headache.
S3: Coffee can help with weight loss.
Then we rewrite the answers contained in sen-
tences as the following questions:
Q’1: Can coffee make you slim down?
Q’2: Can coffee relieve headache?
Q’3: Can coffee help you with weight loss?
We can tell that S1 and S3 are equivalent, as they
both give coherent answers (yes) to each others’
yes-no questions. We can also tell that S2 is distinct
from S1 and S3, as it gives irrelevant answers to
Q’1 and Q’3.
Task definition. A formal definition for answer
consolidation is that given a question and a set of
answer-mentioning sentences, answer consolida-
tion aims at putting sentences into groups such that:
(1) each sentence belongs to exactly one group,
and (2) sentences from the same/different groups
are equivalent/distinct. In this way, each sentence
group corresponds to the same answer aspect(s).
We show in §4.3 that although this definition may
fail for a pair of sentences if they are partially rele-
vant, it only occurs for 2.6% of sentences, which
shows that our operational task definition works
well for the majority of sentences in practice.
In this work, we treat answer consolidation as
a stand-alone process applied after QA retrieval
such that we only take the question and answer-
mentioning sentences as input. In this way, the
answer consolidation model is independent of the
retriever and the reader architectures, and can flexi-
bly adapt to different QA systems. We show in §6.3
that taking sentences instead of answer spans as
input leads to better performance.
4 Question-Answer Consolidation
Dataset (QA)
In this section, we describe the creation of our
Question-Answer consolidation dataset ( QA)
including corpus collection (§4.1) and dataset an-
notation (§4.2). We then provide statistical and
quantitative analysis of QA(§4.3).
4.1 Corpus
We created QAbased on the Quora question
pairs (QQP) corpus (Chen et al., 2018), which con-
sists of 364k questions pairs, originally designed
for predicting whether pairs of questions have the4316same meaning. We start with QQP since Quora
questions have a high expected propensity of hav-
ing multiple correct answers. In preprocessing, we
removed questions containing spelling errors or
non-English words using the Enchant library.and
questions containing personal pronouns including
{“I”, “you”, “we”, “my”, and “your”} as we found
that such questions frequently ask about very spe-
cific personal experiences for which the answers
may not necessarily contain any noteworthy claims,
opinions, or facts in the answer.
Next, we retrieved sentences that were likely to
contain multiple answers to the questions. Given
a question, we retrieved relevant sentences from
the web using a SoTA industry QA retriever, where
each sentence was associated with a relevance score
and a URL. To ensure the questions have multiple
answers, we first removed sentences retrieved from
quora.com and kept the questions if the rele-
vance scores of the top three retrieved sentences
were larger than specified low-confidence thresh-
old. Finally, we kept the top 10 sentences for each
remaining question and pass to crowd workers for
sentence group annotation.
4.2 Annotation
We used Amazon Mechanical Turk (AMT) to
annotate QA. Each AMT HIT consisted of
a question and 11 sentences (including the top-
10 relevance scores and one additional attention-
check sentence) where the crowd workers were
required to: (1) identify sentences that actually
contain answers to the question and (2) put answer-
mentioning sentences into sentence groups with
respect to their aspects. The workers were allowed
to skip an answer-mentioning sentence if it was
hard to put it into any groups (e.g., sentences con-
taining more than one aspect).
The AMT interface is shown in Figure 1. An-
notation was performed by dragging the sentences
between blue boxes. The sentence groups could be
added or removed using the two buttons. To submit
the HIT, workers needed to put all sentences into
boxes corresponding to either a specific sentence
group, not an answer , orhard to put into groups .
Cost. Each HIT was assigned to three annotators
with pay of $0.50/HIT, leading to target an hourly
pay rate of $15. We randomly sampled 5k ques-
tions from §4.1 for HIT submission to AMT.
Quality Control. We used three strategies to en-
sure the annotation quality:
1.Workers selection. We only allowed crowd
workers with acceptance rate ≥98% and had
completed at least 5k hits to work on the task.
We provided annotation guidelines and examples
of 3 annotated hits to instruct the workers.
2.Qualification test. We manually annotated three
hits as the qualification test. Workers were re-
quired to practice on these three hits and get the
correct sentence groups on at least 2 hits to con-
tinue working on the task. We pay $0.05 for each
submitted hit in the qualification test. Although
we had provided detailed instructions, only 24%
of workers passed the qualification test.
3.Attention checker. For all hits, we added an
attention-check sentence that was randomly sam-
pled from other questions, so that it was unlikely
to answer the question. As a part of the task,
the worker needed to identify that this attention-
check sentence did not involve an answer, other-
wise she/he would be blocked from continuing
to work on the task.
Label aggregation. To ensure data quality, we
aggregated worker annotations. To derive the sen-
tence set for answer consolidation, we begin by
only considering sentences put into any sentence
group(s) by all crowd workers as eligible, keeping
37,588 out of 50k sentences. Next, we derived the
aggregated sentence groups from AMT annotations.
As we are not aware of existing methods for this
process, we proposed the following algorithm for
constructing new sentence groups. First, we sort
the sentences by their relevance scores and create
a sentence group with the most relevant sentence4317
being the only member. We then iterate over the
remaining sentences with the following procedure.
For a sentence s, there are three possible cases:
1.If there existed one group Gsuch that ∀s∈
G,sandswere put into the same group by
all workers, and for all already added sentences
s/∈ G,sandswere put into different groups
by all workers, we added stoG.
2.If for all already added s,sandswere put
into different groups by all workers, we created
a new group with sbeing the only member.
3.Otherwise, we discarded sentence s, since there
was disagreement on this sentence.
Finally, we keep each question for which the num-
ber of preserved sentences was larger than one. Our
aggregation algorithm produced sentence groups
on a subset of sentences, on which all workers
agree on each pair of sentences about whether they
belong to the same group or not. After this pro-
cess, 4,699 out of 5,000 questions and 24,006 out
of 37,588 sentences were kept.
4.3 Dataset Analysis
We provide statistical and qualitative analyses re-
garding QAin this section.
Annotation quality. We first analyze the quality
of data annotation before label aggregation. In the
first annotation task of identifying whether a sen-
tence contains an answer, AMT workers achieved
an inter-annotator Fleiss’ kappa of 0.62, an average
agreement rate of 90.2%, and a worker agreement
with aggregate (WAWA) of 82.5% in F. WAWA isused to compare the majority vote with all workers’
annotations. In the second annotation task of sen-
tence grouping, we first get the set of sentences that
all workers put into some groups. We then calculate
the workers’ agreement on each pair of sentences
regarding whether they belong to the same group
or not. The inter-annotator Fleiss’ kappa, average
agreement rate, WAWA Fare 0.46, 84.8%, and
75.9%, respectively. These results show that the
overall annotation quality is usable, but with room
for improvement. Accordingly, to further improve
the data quality, we only keep group annotations
on which all workers agree (as stated in §4.2).
Dataset statistics. Our final dataset consists of
4,699 questions, 24,006 sentences, and 19,676
groups. On average, there are 4.18 groups per ques-
tion, and 1.22 sentences per group. Specifically,
97.7% of questions have multiple aspects (sentence
groups), and 45.4% of questions have at least one
pair of equivalent sentences. In terms of sentence
groups, 86.6% of groups have only one sentence,
8.8% of groups have two sentences, and the re-
maining 4.6% of groups have 3 or more sentences.
Overall, this analysis shows that our dataset con-
tains both multi-aspect and redundant answers that
align with the challenges of answer consolidation.
Types of equivalent sentences. To get a better
understanding of the required knowledge to iden-
tify equivalent sentences, we randomly sampled
100 sentence pairs in the same group and manu-
ally labeled the pairs with the types shown in Ta-
ble 1. We observed that if the machine reader has
the correct answer spans, 56% (formatting and ex-4318act match) of the equivalent sentences could be
directly identified by string comparison. Another
11% of the equivalent sentences only differed at the
lexical level, which may be identified using lemma-
tization, removal of stop words, or a dictionary of
synonym words. 30% of equivalent sentences are
semantic variations such that identifying equiva-
lence requires understanding of their meanings and
potentially even commonsense reasoning. E.g., for
the example given in Table 1, the answer consolida-
tion model needs to understand that oxygen isgood
airandcarbon dioxide corresponds to bad air . For
the remaining 3% of pairs, we do not agree with the
annotation. Either the sentences do not answer the
question, or they do not contain the same aspect(s).
Limits of the task definition. During data anno-
tation, 2.6% of answer-mentioning sentences are
denoted as “ hard to put into groups ”. After inspec-
tion, we find that these sentences contain more than
one aspect of answers. For example, given the ques-
tionWhat are the best places to visit and things to
do in San Diego, CA? , one sentence may be The
San Diego Zoo, Balboa Park, and SeaWorld are the
top tourist attractions in San Diego. , which con-
tains 3 different answers. This sentence overlaps
with multiple groups and thus cannot be placed in
a single group. Given the low prevalence, we leave
consideration of such sentences to future work.
5 Approach
In this section, we first tackle the classification
setting of answer consolidation (§5.1). Given a
question and answer-mentioning sentences, the task
is to predict for a pair of sentences whether they
are in the same group. We consider different types
of models, including sentence embedding models,
cross-encoders, and answer-aware cross-encoders.
Then we consider the sentence grouping setting,
presenting the method of transforming pairwise
predictions to sentence groups (§5.2). For all meth-
ods, we use RoBERTa (Liu et al., 2019) as
the encoder, noting that other pretrained language
models (PLMs) can easily be incorporated as part
of these methods.
5.1 Sentence Pair Classification
Sentence embedding models. Sentence embed-
ding models (Reimers and Gurevych, 2019; Gao
et al., 2021) produce for each sentence an embed-
ding vector, with which we can use metrics such
as cosine to calculate their similarity. Specifically,given a question qand a sentence s, we first to-
kenize them to XandXusing the RoBERTa
tokenizer, and then concatenate them as inputs:
Following Gao et al. (2021), we take the <s>
embedding in the last layer of PLM as the sentence
embedding. Then for a pair of sentences, whether
they are in the same group is decided by the cosine
similarity of the sentence embedding. The similar-
ity can be converted to binary predictions using the
best threshold that is selected on the validation set.
The sentence embedding models can work in
both zero-shot and supervised settings. In the zero-
shot setting, we directly use the pretrained sen-
tence embedding model to make predictions with-
out fine-tuning. In the supervised setting, given
a pair of sentence embedding h,h, and label
y∈ {0,1}, where 0and1mean not in/in the same
group respectively,we fine-tune the PLM based
on the following regression objective of sentence-
transformers (Reimers and Gurevych, 2019):
L= (cos ( h,h)−y).
Cross-encoders. Cross-encoders (Devlin et al.,
2019; Liu et al., 2019) take a pair of sentences as
the input and predict whether they are in the same
group or not. Given a question qand two answer-
mentioning sentences sands, we first tokenize
them as X,X, and Xusing the RoBERTa
tokenizer, and then take XXandXXas two
segments of inputs, following the input formats of
sentence pair classification tasks (Liu et al., 2019):
Prediction is independently performed on sen-
tence pairs using a binary classifier on the first
special (classification) token <s> embedding in
the last layer of the PLM. The cross-encoders work
in both zero-shot and supervised settings. In the
zero-shot setting, we fine-tune the model on the
MNLI (Williams et al., 2018) dataset and take en-
tailment asin the same group . In the supervised
setting, given the sentence pair embedding (ob-
tained from <s>)hand the label y, we fine-tune4319the model using the binary cross-entropy loss:
p=σ(wh),
L=−(ylogp+ (1−y) log (1 −p)),
where pis the probability that the sentences are in
the same group, σis the sigmoid function, wis a
parameter of the classifier. In inference, we con-
vertpto binary predictions using the best threshold
selected on the validation set. The cross-encoders
require predicting on all sentence pairs and have
higher computational costs than sentence embed-
ding models. However, we observe in experiments
that cross-encoders consistently outperform sen-
tence embedding models in the supervised setting.
Answer-aware (A) cross-encoders. §4.3 shows
that 56% of equivalent sentences can be di-
rectly identified if the model knows the answer
spans. Therefore, we provide the answer consol-
idation models with answers generated from the
UnifiedQA model (Khashabi et al., 2020). As
the UnifiedQA is trained on both extractive and
abstractive datasets, the answers may not be text
spans of sentences. Specifically, given a question
q, two sentences sands, and the generated an-
swers aanda, we first tokenize them as X,X,
X,X, andX, respectively, then construct the
input to cross-encoders as:
The training process and inference process are the
same as the cross-encoders.
5.2 Sentence Grouping
In answer consolidation, our ultimate goal is to ob-
tain the consolidated sentence groups. This is done
in a two-step approach. The first step is to get the
matrix of distances Dbetween pairs of sentences
for a question. We perform this step using models
trained in the classification setting. For sentence
embeddings, Dis adopted as the pairwise cosine
distance matrix. For cross-encoders, each entry of
Dequals to 1 minus the predicted probability for
a sentence pair. As Dderived in this way may not
be always symmetric, we use(D+D)as the
distance matrix instead.
The next step is to transform the distance matrix
into sentence groups. Here we apply agglomerate
clustering (Han et al., 2011). It uses a bottom-up
strategy, starting from letting each sentence form
its own cluster, and then recursively merging theclusters if their distance is smaller than a threshold.
We use the average distance of sentence pairs as
the inter-cluster distance measure and select the
best threshold on the validation set. Agglomerate
clustering stops when the distances between all
clusters are larger than the threshold.
6 Experiments
In this section, we present the experimental
setup (§6.1), show the main results (§6.2), con-
duct an ablation study (§6.3), and provide error
analysis (§6.4).
6.1 Experimental Setup
Dataset. We randomly split the 4,699 questions
into an 80/10/10 split, which serves as the training,
validation, and test set, respectively.
Evaluation metrics. We use different evaluation
metrics in the two evaluation settings. For the clas-
sification setting, we first use the micro Fmea-
sure. Considering that classes in the dataset are
highly imbalanced (only 11% of sentence pairs
are in the same group), we additionally use the
Matthews correlation coefficient (MCC; Matthews
1975), which is considered a more class-balanced
metric. For the sentence grouping setting, we
use clustering metrics including adjusted rand in-
dex (ARI; Rand 1971) and adjusted mutual infor-
mation (AMI; Nguyen et al. 2009). These two
metrics take the predicted grouping and the ground-
truth grouping, and measure the similarity between
them. For all metrics, larger values indicate better
performance, and a value of 100% indicates perfect
classification/grouping.
Configuration. We implement the models using
Huggingface’s Transformers (Wolf et al., 2020).
The models are optimized with Adam (Kingma
and Ba, 2015) using a learning rate of 1e−5, with
a linear decay to 0. We fine-tune all models for 10
epochs with a batch size of 32 questions (includ-
ing all associated sentence pairs). The best model
checkpoint and thresholds are selected based on
the validation set. We report the average results on
5 runs of training using different random seeds.
Models. We use RoBERTa (Liu et al., 2019)
as the encoder for all models. For sentence em-
bedding models, we try RoBERTa fine-tuned on
two intermediate tasks: 1) SRoBERTa (Reimers
and Gurevych, 2019) is fine-tuned on natural lan-
guage inference (NLI) datasets, achieving better4320
results on semantic textual similarity (STS) tasks,
and 2) SimCSE-RoBERTa (Gao et al., 2021) is
fine-tuned in a self-supervised fashion, taking a
sentence and predicting itself using a contrastive
learning objective. For cross-encoders, in addition
to directly running supervised fine-tuning on our
data, we also try supplementary training on an in-
termediate labeled-data task (Phang et al., 2018),
which fine-tunes cross-encoders on MNLI before
supervised fine-tuning. Particularly in the latter set-
ting, we observe it being necessary to re-initialize
the classifier before supervised fine-tuning to ob-
tain more promising performance.
6.2 Main Results
The experimental results on both the pairwise clas-
sification and sentence grouping settings are re-
ported in Table 2. We observe that in the zero-
shot setting, intermediate-task training improves
answer consolidation, while the performance re-
mains far behind supervised models. In the super-
vised setting, cross-encoders consistently outper-
form the sentence embedding models. Overall, the
answer-aware cross-encoder intermediately tuned
on MNLI (ARoBERTa-MNLI) achieves the best
results on all metrics, showing that intermediate-
task training on MNLI improves performance. Be-
sides, we find that answer-aware cross-encoders
outperforms regular cross-encoders, showing that
answers generated by the machine reader provide
additional information that helps consolidation.
6.3 Ablation Study
In this section, we study the model performance
based on different input information given to super-
vised models. We denote the questions, sentences
as Q, S, and answers generated by UnifiedQA as
A. The results are shown in Table 3. Overall, mod-
els trained on all inputs (Q+S+A) achieve better
results than those that have observed only a sub-
set of the available inputs on most metrics. Re-
moving the sentences leads to the largest drops
in performance (e.g., 20.9% in Ffor SimCSE-
RoBERTa and 22.3% in Ffor RoBERTa-MNLI),
which shows that sentences provide useful informa-
tion for answer consolidation. Using sentences
only leads to the second-largest drop in perfor-
mance, showing that without grounding to ques-
tions and answers, consolidation is not simply ad-
dressed only with the sentences. Besides, removing
questions also leads to more significant drops in
performance than removing answers (e.g., 4.9%
inFfor SimCSE-RoBERTa and 2.3% in Ffor
RoBERTa-MNLI). This shows that it is necessary
to understand the answer equivalence within the
question context in order to consolidate answers.
6.4 Error Analysis
To get a sense of what knowledge is needed to
further improve model performance, we examined
sentence pairs incorrectly classified as not in the
same group by ARoBERTa in the validation and
test sets, where 318 out of 2,614 pairs (12.2%)
are wrongly classified. We randomly sample 50
such error cases and categorize them by the answer-
equivalence type as defined in Table 1. Of the 504321
pairs, 1 (2%) is from exact match , 8 (16%) are
ambiguous, and the remaining 41 (82%) are from
thesemantic variation category, showing that it is
the most challenging type to tackle.
We further study the specific causes of errors on
the 42 unambiguous pairs. Examples of distinct
error causes are described in Table 4 We find that
16.7%of the falsely classified sentence pairs con-
tain one answer that entails the other instead of
expressing the exact same answers, which should
however be considered redundant answers by our
definition. 80.9%of pairs are equivalent but require
understanding the semantic meanings or entity-
specific/commonsense knowledge. The rest 2.4%
contain spelling errors that negatively affect model
inference.
7 Conclusion
In this paper, we formulate and propose the an-
swer consolidation task that seeks to group answers
into different aspects. This process can be used to
construct a final set of answers that is both com-
prehensive and non-redundant. We contribute the
Question-Answer consolidation dataset ( QA)
for this task and evaluate various models, includ-
ing sentence embedding models, cross-encoders,
and answer-aware cross-encoders. While the
best-performing supervised models have achieved
promising performance, without that abundant an-notation, unsupervised methods still remain far
from perfect. This suggests room for further stud-
ies on more robust and generalizable solutions for
answer consolidation that would largely benefit
real-world open-domain QA systems.
Acknowledgement
We appreciate the reviewers for their insightful
comments and suggestions. Wenxuan Zhou and
Muhao Chen are supported by the National Science
Foundation of United States Grant IIS 2105329.
References4322432343244325