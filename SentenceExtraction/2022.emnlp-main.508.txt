
Yongwei Zhou, Junwei Bao, Chaoqun Duan, Youzheng Wu,
Xiaodong He, Tiejun ZhaoHarbin Institute of TechnologyJD AI Research
ywzhou@hit-mtlab.net baojunwei001@gmail.com tjzhao@hit.edu.cn
Abstract
Question answering requiring discrete reason-
ing, e.g., arithmetic computing, comparison,
and counting, over knowledge is a challeng-
ing task. In this paper, we propose UniRPG ,
a semantic-parsing-based approach advanced
in interpretability and scalability, to perform
Unified d iscrete Reasoning over heterogeneous
knowledge resources, i.e., table and text, as
Program Generation. Concretely, UniRPG con-
sists of a neural programmer and a symbolic
program executor, where a program is the com-
position of a set of pre-defined general atomic
and higher-order operations and arguments ex-
tracted from table and text. First, the program-
mer parses a question into a program by gen-
erating operations and copying arguments, and
then, the executor derives answers from table
and text based on the program. To alleviate the
costly program annotation issue, we design a
distant supervision approach for programmer
learning, where pseudo programs are automati-
cally constructed without annotated derivations.
Extensive experiments on the TAT-QA dataset
show that UniRPG achieves tremendous im-
provements and enhances interpretability and
scalability compared with previous state-of-the-
art methods, even without derivation annota-
tion. Moreover, it achieves promising perfor-
mance on the textual dataset DROP without
derivation annotation.
1 Introduction
Question answering requiring discrete reasoning,
e.g., arithmetic computing, sorting, comparison,
and counting, over knowledge is a challenging
task and has attracted wide attention. Advanced
approaches to this task are roughly divided into
mixture-of-experts (MoE)-based and semantic pars-
ing (SP)-based methods. The former divides ques-
tions into several limited categories according toFigure 1: The upper block shows a training instance
from the TAT-QA dataset. It consists of a table T, ques-
tionQ, paragraphs P, answer Aand derivation D. The
bottom block displays the constructed programs Gunder
w/ and w/o derivation settings. The gold program is in
red, and the others are in green.
their required reasoning type and designs multi-
ple specialized predictors to deal with each cate-
gory (Dua et al., 2019; Ran et al., 2019; Chen et al.,
2020a; Zhu et al., 2021; Zhou et al., 2022a). The
latter translates questions into programs such as
SPARQL queries (Bao et al., 2014, 2016; Chen
et al., 2020d; Gan et al., 2021; Cao et al., 2021;
Hui et al., 2022), to derive the answers. Compared
with MoE-based methods, SP-based methods have
advantages in interpretability and scalability, and
we mainly explore discrete reasoning along this
technical route in this paper.
As to works based on SP-based methods, they
always research over homogeneous knowledge re-
source, i.e., either unstructured text (Dua et al.,
2019; Ran et al., 2019; Chen et al., 2020a; Zhou
et al., 2022a) or structured tables and knowledge
graphs (Pasupat and Liang, 2015; Herzig et al.,
2020; Liu et al., 2022). Respectively, they design7494a specialized semantic parser for each knowledge
category according to the corresponding data char-
acteristic. However, in practical scenarios (e.g., fi-
nancial reports and sports news analysis), it usually
requires a model to comprehensively understand
heterogeneous knowledge resources (e.g., tables
and text) and perform reasoning over them. There-
fore, to extend SP-based methods to more general
and realistic scenarios, it needs a unified frame-
work for discrete reasoning over heterogeneous
knowledge resources. Meanwhile, to mitigate the
program annotation cost, it also needs an automatic
approach to produce pseudo programs.
To address the above challenges, we propose
UniRPG , a SP-based approach, to perform Unified
discrete Reasoning over table and text as Program
Generation. Specifically, UniRPG consists of a
neural programmer and a deterministic symbolic
program executor, where a program is the compo-
sition of a set of pre-defined general atomic and
higher-order operations and arguments extracted
from table and text. The programmer leverages a
structure-aware knowledge reader to encode het-
erogeneous knowledge, and a program generator
to decode programs through generating operations
and copying arguments from table and text. Dur-
ing inference, decoding constraints are leveraged
to ensure program legality. Then, the executor de-
rives answers from table and text based on the pro-
gram. To alleviate the costly program annotation
issue, we design a distant supervision approach
for programmer learning, where pseudo programs
are automatically constructed without annotated
derivations. Furthermore, a re-weight strategy is
introduced to measure the importance of pseudo
programs for noise reduction in training.
To verify the effectiveness and generality of
UniRPG, we conduct comprehensive experiments
on the TAT-QA (Zhu et al., 2021) and DROP (Dua
et al., 2019) datasets under w/ and w/o derivation
settings. Experiment results show that UniRPG
achieves tremendous improvements and enhances
interpretability and scalability compared with pre-
vious state-of-the-art (SOTA) methods on the TAT-
QA dataset. Specifically, it outperforms the pre-
vious SOTA model by 17.0 EM and 18.0 F1 on
the test set under full supervision. Besides, our
weakly supervised model based on automatically
constructed pseudo-programs only lags behind
fully supervised model 1.0 EM and 1.4 F1 points.
Moreover, it also achieves promising performanceon the textual dataset DROP in weak supervision.
In conclusion, the contributions of this work are
summarized as follows: (1) We propose an effec-
tive program-generation-based framework for dis-
crete reasoning over table and text; (2) To alleviate
costly program annotation, we design a distant su-
pervision approach to construct pseudo programs
automatically; (3) We conduct comprehensive ex-
periments to verify the effectiveness and scalability
of UniRPG on the TAT-QA and DROP datasets.
2 Methodology
2.1 Task Definition
The task of discrete reasoning over hybrid data
aims to predict the answer from the given input
(e.g., tables and text). Formally, given a question
Q, a table T, and a set of paragraphs P, the target
is to derive the answer AfromTandP.
A= arg max P(A|Q,T,P;θ). (1)
Specifically, the table Tconsists of m×ncells
{c}, where mandnare the number of rows and
columns. The paragraph set P={p, p, ..., p}
contains kparagraphs which are relevant to the ta-
ble. Notably, the answer Ain this task is either a
span extracted from the table and the paragraphs
or a number computed based on them. Therefore,
it requires the model to perform discrete reasoning
such as arithmetic operations, counting, compari-
son, sorting, and their compositions.
2.2 Framework Overview
As mentioned above, the method of this task needs
to be capable of performing discrete reasoning over
heterogeneous data. To this end, we first define
a set of operations as reasoning units, which con-
sists of atomic and high-order operations. Based
on these operations, we propose to perform uni-
fied discrete reasoning over table and text as pro-
gram generation (UniRPG). Concretely, UniRPG
consists of a neural programmer and a symbolic
program executor, and it can be formulated as:
P(A|Q,T,P;θ)
=/summationdisplayI(f(G) =A)P(G|Q,T,P;θ),(2)
where P(G|Q,T,P;θ)denotes the neural pro-
grammer andf(·)is the symbolic program execu-
tor.I(f(G) =A)is an indicator function with
value 1, if the answer Acan be derived based on
the program G, and 0 otherwise.7495
2.3 Operations
As shown in Table 1, we define 15 operations,
which consist of 4 atomic operations and 11 high-
order operations. The atomic operations indicate
extracting a piece of text or a number from the ta-
ble or paragraphs. Specifically, SPAN andVALUE
are used to extract content from paragraphs while
CELL andCELL _VALUE (CV)are used to extract con-
tent from the table. The atomic operations are the
foundation for unified discrete reasoning over table
and text. As to high-order operations, they indi-
cate how to process the arguments extracted based
on atomic operations. For example, KV,ARGMAX ,
andARGMIN are tailored for questions requiring
comparison or sorting. COUNT andMULTI _SPANS
are used to solve counting and multi-spans extrac-
tion problems, respectively. We also introduce
some arithmetic operations to deal with math ques-
tions, including SUM,DIFF ,TIMES ,DIV,AVGand
CHANGE _R. Therefore, we formulate the priority
of atomic operations higher than high-order opera-
tions. In addition, we also propose some constants
that are commonly used in math problems.
2.4 Neural Programmer
In this work, we adopt the BART (Lewis et al.,
2020) to implement the Neural Programmer. To
adapt it to this task, we have improved its encoder
and decoder respectively.
2.4.1 Structure-Aware Knowledge Reader
The original encoder of BART only takes textual
data as input. However, in this task, besides para-
graphs, the input contains tables as well. Followingprevious works (Liu et al., 2022; Zhou et al., 2022b;
Zhang et al., 2020), we first flatten the table into
a sequence and concatenate it with the paragraphs
together, and then feed the processed paragraphs
and table into the encoder. Formally, we rank para-
graphs based on the similarity between each one
with the question. As to the table, we flatten it by
rows and then construct the input as follows:
S= [⟨s⟩;Q;⟨/s⟩;T;⟨/s⟩;P;⟨/s⟩].(3)
Then, we feed it into the encoder to learn the hybrid
knowledge representation H={h}, where
Lis the length of the input sequence.
Unlike the text that only contains the linear
structure, tables have a more complex 2D struc-
ture. Therefore, simple concatenation can not com-
pletely capture the structural information in the
table. Inspired by Zhang et al. (2020), we inject ta-
ble structure information into self-attention layers
with a structure-aware attention mask. Specifically,
in the lower layers, each cell only takes informa-
tion from other cells in the same row, question, and
paragraphs into account. In the upper layers, the
information of cells in the same column is also in-
tegrated into the representation of the target cell.
This way, the encoder can learn the lexical features
in the lower layers and capture complex cross-row
reasoning in the upper layers. Finally, we denote
this encoder as a structure-aware knowledge reader.
2.4.2 Programmer Generator
In this work, the program generator is responsible
for generating programs that are used to perform
discrete reasoning over table and text. Specifically,7496
an executable program contains several operations.
For the atomic operations, they need to take the
start and end indexes of the content in the text and
the table as input. For the high-order operations,
they take the output of the atomic operations as
input. Therefore, the generator should decode the
program based on operations, text, and table. How-
ever, the original decoder of the BART (Lewis et al.,
2020) is used to generate sentences. To bridge the
gap, we substitute the prediction vocabulary with
the operation set and integrate the copy mecha-
nism (Gu et al., 2016; See et al., 2017) into the
BART to retrieve reasoning-related numbers or text
pieces from table and text as operation arguments
when decoding programs.
Formally, we extra define a trainable embedding
matrix {h}to indicate the operations and the
j-th row corresponds to the j-th operation o∈ O.
Based on the decoder of the BART, we can compute
two sets of scores at each step as follows:
s=s(h,h), i∈[1,2,3, ..., L ], (4)
s=s(h,h), j∈[1,2, ..., Z ], (5)
where sandsdenote the scores of the i-th
token of the input and j-th operation. his the
hidden state of the decoder at the t-th step. his
the contextual representation of the i-th token in
the input. s(·)ands(·)are both cosine similarity
functions. s(·)is a pointer to locate the copied
numbers and text pieces from the input sequence
S.s(·)acts as a generator to select an operation
from the operation set O. After obtaining these two
scores, we normalize them with a softmax function.2.5 Program Executor
In this work, the program is executable, and we
can directly apply it to perform discrete reason-
ing over table and text. Therefore, we imple-
ment a symbolic program executor rather than
a specific neural module. Given a program, we
can execute it based on the meaning of each
operation in it. For example, given a program
SUM(CV(s,e),CV(s,e)), since the priority of
atomic operations is higher than high-order opera-
tions, CV(s,e)andCV(s,e)are first executed
to extract value from the table. And then, SUM(·)
is executed to return the summation of extracted
value from the table as the output.
3 Training and Inference
3.1 Training
Training Instance Construction: w/ Derivation
Each training instance I={Q,T,P,A,D}con-
sists of a question Q, a table T, associated para-
graphs P, an answer Aand a derivation D. The
derivation Dindicates the answer position in ta-
ble and text and the reasoning solution to derive
the answer. We can deterministically construct a
program based on such a training instance Ithat
contains annotated derivation D. As the exam-
ple shown in Figure 1, we determine the unique
program G=DIFF (CV(131,134),CV(135,138))
based on the reasoning solution 0.06 = 0 .53−0.47
and the positions of these two numbers.
Training Instance Construction: w/o Derivation
In addition, to alleviate the costly derivation anno-
tation, we design a distant supervision method to
construct pseudo programs automatically without
derivation. We design a template library Bshown7497in Table 9 in the Appendix A.3, which consists
of five categories, namely text/number extraction,
multi-spans extraction, counting, comparison/sort-
ing, and arithmetic computing. We search all the
possible programs for each training instance based
on{Q,T,P,A,B}. For text/number extraction,
we find all the occurrences of answers from ta-
ble and text. Concerning multi-spans extraction,
we find all the occurrences of each span and then
combine them. For counting questions, building a
fine-grained counting program fails due to a lack
of the positions of answer clues. To address the
issue, we convert a question with a multi-spans
answer into a counting question by replacing its in-
terrogatives (e.g. “What”, “Which”, “Who”) with
“How many”, and leverage its answer items to build
the counting programs. For questions requiring
comparison/sorting, we first find the answer posi-
tion in the table and construct key-value pairs with
cells in the same row/column and cells in other
rows/columns that contain values. If performing
theARGMAX /ARGMIN operation on the list of key-
value pairs yields the answer, we then append it
to the program candidates. Additionally, we de-
fine several math templates that conduct specific
operations on numbers in table and text for math
problems. Afterward, for each training instance in
TAT-QA, more programs (eight on average) that
can derive correct answers are found without using
annotated derivation D. For instance, another pro-
gram DIFF (CV(131,134),CV(135,138)) is found
under w/o derivation setting shown in Figure 1.
Moreover, based on the templates, we find at least
one pseudo program for 89% training instances in
the TAT-QA dataset (Zhu et al., 2021).
Training Objective We consider all the pro-
grams as supervision constructed under the w/
derivation setting. For w/o derivation setting, we
introduce a re-weight strategy to measure the im-
portance of pseudo programs for noise reduction in
training. Formally, the program generation loss L
is defined as follows:
L=−/summationdisplayαlogp(G|Q,T,P;θ).(6)
Precisely, we calculate a weight αfor each pseudo
program G ∈Ωas the reciprocal of the number of
pseudo programs with the same operations. Based
on the weight distribution, losses of pseudo pro-
grams are weighted summed to treat each type of
program fairly. In addition, TAT-QA has another
task to predict the scale of answer and it may be
None ,Thousand ,Million ,Billion andPercent . A
correct final answer requires that both the predicted
answer text and scale are correct. We propose a
5-categories classifier for scale prediction and joint
train it with the neural programmer. Denote the
cross-entropy loss for scale prediction as Land
then the total loss can be calculated with a weight
λasL=L+λL.
3.2 Inference with Constraint Decoding
To ensure the legality of the program, we propose
the following three categories of decoding con-
straints and utilize a decoding mask to filter the
illegal program candidates at each step. (1) Index
constraints . For example, with (s, e)as arguments
to an atomic operation, the end index emust be
equal to or larger than the start index s. (2) Type
constraints . For example, the operation CVshould
return a number from the table rather than para-
graphs. (3) Composition constraints . Each oper-
ation must take the correct type of operations as
arguments. More decoding constraint details are
shown in Appendix A.2.
4 Experiment
4.1 Dataset and Evaluation Metrics
The experiments are conducted on two QA
datasets, including TAT-QA (Zhu et al., 2021) and
DROP (Dua et al., 2019). TAT-QA is a large-scale
QA dataset in finance created to facilitate discrete
reasoning over table and text. DROP is a large-
scale QA dataset requiring discrete reasoning over
text. It is crowd-sourced based on passages from
Wikipedia. The TAT-QA dataset especially pro-
vides the derivation for each training instance, but
the DROP dataset does not. The statics information
of the two datasets is summarized in Table 2. We
employ Exact Match (EM) and F1 as the evaluation
metrics.
4.2 Implementation Details
Note that the implementation details are described
in the Appendix A.1.74984.3 Baselines
Baselines for TAT-QA (1)Textual QA Models:
BERT-RC (Devlin et al., 2019) is a simple extrac-
tive reading comprehension (RC) model that pre-
dicts a text span as the answer. NumNet+v2 (Ran
et al., 2019) leverages a graph neural network to
perform reasoning over numbers in tables and para-
graphs. When applying RC models to TAT-QA,
the table is flattened into a sequence by row and
concatenated with questions and paragraphs. (2)
Tabular QA Models: The baseline TaPas for WTQ
employs TaPas (Herzig et al., 2020) as the knowl-
edge encoder. TaPas contains prior knowledge by
pre-training over large-scale tabular data and in-
jects table structure information in the embedding
layer. (3) Hybrid QA Models: HyBrider (Chen
et al., 2020c) first links the question with a table
cell and feeds paragraphs and the selected cell into
the RC module to infer the answer. TAGOP (Zhu
et al., 2021) is an MoE-based model that defines
multiple operations as predictors to support discrete
reasoning. It first detects the evidence from tables
and paragraphs and selects an answer predictor to
infer the answer based on the evidence.
Baselines for DROP NAQANet (Dua et al.,
2019) designs three specific modules to handle dif-
ferent types of questions, including span extraction,
counting, and arithmetic calculation. MTMSN (Hu
et al., 2019) introduces an extra answer predictor to
address negation questions and a re-ranking mecha-
nism to rank arithmetic expression candidates con-
sidering their context information. NumNet (Ran
et al., 2019) builds a bi-directional fully-connected
graph to model the magnitude of numbers and em-
ploys a graph neural network to reason over it. Gen-
BERT (Geva et al., 2020) injects numerical rea-
soning capability into BERT by pre-training with
large-scale synthetic numerical data.
4.4 Main Results
Result on TAT-QA Table 3 displays the exper-
iment results of baselines and UniRPG on the
TAT-QA dataset. All the baseline results come
from previous work (Zhu et al., 2021). We im-
plement UniRPG based on BART (Lewis et al.,
2020) and train it under the w/ and w/o deriva-
tion settings, respectively. As shown in Table 3,
UniRPG(BART-base) and UniRPG(BART-large)
respectively exceed the previous SOTA method
TAGOP by 14.3 EM/15.6 F1 and 17.0 EM/18.0 F1
on the test set under full supervision (w/ derivation).
In addition, our framework also works well under
weak supervision (w/o derivation), exceeding all
the baselines by a remarkable margin. Moreover,
our weakly-supervised model based on automat-
ically constructed pseudo programs only lags be-
hind fully-supervised models 1.0 EM and 1.4 F1 on
the test set. We think the promising improvement
comes from the following aspects: (1) UniRPG can
solve more complex and more types of questions,
such as compositional reasoning and abstract an-
swers (answers do not appear in context). (2) As
visualized in Figure 3, UniRPG captures the rela-
tionship between programs and hybrid knowledge.
Moreover, Table 5 displays the detailed ex-
periment results of baseline (TAGOP) and our
UniRPG(BART-large) under w/ derivation setting
on the test set w.r.t answer types and sources. Our
method significantly outperforms TAGOP, whether
the answers come from table, text, and collabora-
tion of table and text. Furthermore, UniRPG has
advantages in the following three types of answers,
including Span, Spans, and Arithmetic answers, yet
disadvantages in Counting answers. We speculate
the phenomenon stems from the lack of training
instances with Counting answers, which only ac-
count for 2%.7499
Results on DROP We verify the effectiveness
of UniRPG over text based on the DROP dataset.
In this setting, unlike reasoning over table and
text, UniRPG does not need to consider some op-
erations tailored for table and some non-involved
reasoning types, such as CELL ,CELL _VALUE and
TIMES /DIV. As shown in Table 4, UniRPG also
achieves promising performance on the textual
dataset DROP with 77.08 EM and 80.42 F1 scores,
which means UniRPG is not only suitable for rea-
soning over hybrid data but also feasible for text.
4.5 Ablation Study
We investigate the effect of decoding constraints,
modeling of table structure, and re-weight strategy
for noise reduction in training on the development
set of the TAT-QA dataset. As shown in Table 6,
the performance declines by 2.0 EM/1.1 F1 and 1.2
EM/1.2 F1 on average under w/o and w/ derivation
settings when removing the decoding constraints
(w/o constraints). It demonstrates that constraint
decoding can significantly reduce errors caused
by illegal programs. Moreover, if removing the
modeling of table structure, the performance de-
clines by 2.5 EM/2.2 F1 and 4.2 EM/4.2 F1 on
average under w/ and w/o derivation settings, re-
spectively. It indicates that modeling table structure
significantly contributes to the performance of our
models. Additionally, we explore the influence of
the reweighted mechanism for pseudo programs
under the w/o derivation setting. Table 6 shows the
re-weight strategy effectively improves the perfor-
mance by 2.0 EM and 1.8 F1 on average, which
treats each type of pseudo program fairly and alle-
viates negative impact of noise in training.
5 Discussion
Interpretability Analysis As shown in Figure 7-
9, our model generates fine-grained programs that
intuitively demonstrate the discrete reasoning pro-
cess. Furthermore, we visually investigate how
the neural programmer decodes programs based on
questions and hybrid knowledge resources. Fig-
ure 3 displays the cross-attention score distribu-
tion from the last layer of the decoder based on a
random sample. The horizontal axis is the input
sequence S, and the vertical axis represents the
decoded program units at different time steps. We
observe that cross-attention visualization reflects
the underlying relationships between the question
prefix “ What is the change ” and operation DIFF .
Moreover, CVcorrectly extracts the two numbers
44.1 and 56.7 from the table.7500Error Analysis We analyze our error predictions
on the dev set of TAT-QA and observe the error
instances fall into the following four categories.
The first category is that atomic operations extract
wrong text pieces or numbers due to index error,
accounting for 62%. We think it is intractable to
derive the start and end index from the entire in-
put sequence precisely, which typically has hun-
dreds of tokens. The second category is caused by
operation errors when decoding programs and ac-
counts for 20%. We speculate that this error stems
from some questions being inherently confusing.
For example, given a question “ What is the aver-
age USD-EUR exchange rate in FY 2019? ”, the
programmer wrongly predicts the operation AVG
to solve the question since the keyword average
appears in the question. However, the correct an-
notated derivation is “70.07/80.82”, i.e., DIVis the
proper operation. The remaining errors come from
the scale prediction and others, which account for
18%. In summary, more accurately extracting text
pieces and numbers from table and text is essential
to further improve our model.
Influence of Program Scale For each training in-
stance in TAT-QA, there are 8 pseudo programs to
derive answers correctly on average under the w/o
derivation setting. To explore the influence of the
program scale, we respectively train models with
TopNprograms of each instance and all the pro-
grams, where N= 1,2, ...,12. Figure 5 displays
the F1 curve of our models on the development set
of TAT-QA with respect to the scale of programs
N. The F1 score increases with the program scale,
then shows a relatively stable performance, reach-
ing a peak when N= 10 . It indicates the initial
increase of F1 is due to gradually sufficient training
data, and our model shows good robustness as the
noisy data increases.
6 Case Study
We compare our UniRPG with the baseline TAGOP
on 50 instances randomly sampled from the devel-
opment set of TAT-QA, where TAGOP predicts
wrongly, but our model predicts correctly. UniRPG
has advantages in the following three aspects com-
pared with TAGOP. (1) Capture of argument order.
TAGOP introduces a 2-classifier to learn the argu-
ment order with a one-hot order label, ignoring
the modeling of the interaction between two ar-
guments. Our framework can naturally learn the
parameter order of operations because program se-
quences are inherently ordered. As shown in exam-
ple (a), our methods can avoid 60% error instances
of TAGOP (Zhu et al., 2021) due to the wrong ar-
gument order. (2) Evidence detection. TAGOP
extracts evidence by classifying each token of the
input and then predicts answers based on the evi-
dence, which is prone to accumulating errors. We
employ a copy mechanism to extract content from
the input, which only needs to predict the start and
end tokens. Example (b) shows our methods reduce
20% error predictions caused by inaccurate evi-
dence compared with TAGOP. (3) Compositional
reasoning. As shown in example (c), our model has
better scalability that can perform compositional
with multiple operations compared to TAGOP.
7 Related Work
Hybrid QA Hybrid QA aims to derive answers
to given questions with heterogeneous knowledge,
such as table/knowledge graph and text. Many chal-
lenging datasets and promising approaches (Asad-
ifar et al., 2018; Chen et al., 2020b,c; Zhu et al.,
2021; Zhong et al., 2022; Wang et al., 2022; Lei
et al., 2022) are recently proposed to stimulate the
progress of this topic. Chen et al. (2020c) proposed
HybridQA, in which each question is aligned with
a Wikipedia table and multiple paragraphs linked
with cells in the table. Chen et al. (2020b) pro-
posed OTT-QA, a large-scale open table-and-text
QA dataset that requires retrieval tables and text to
derive answers. Unlike HybridQA and OTT-QA,
which study multi-hop reasoning on table and text,
we study TAT-QA (Zhu et al., 2021), a large-scale
QA dataset requiring discrete reasoning over hy-
brid data. FinQA (Chen et al., 2021) is a recent
work similar to ours, aiming to address the task
of numerical reasoning over financial data. Com-
pared to it, UniRPG supports more reasoning ca-
pabilities, such as SPAN /CELL , which can perform
extraction from paragraphs/tables but is not limited7501
to the arithmetic computing problem. This enables
UniRPG to be available for more discrete reason-
ing tasks. Moreover, the atomic and higher-order
operation framework provides a unified paradigm
to define operations, which makes UniRPG con-
venient to be adapted to other discrete reasoning
tasks. In addition, we provide a weak-supervised
approach for discrete reasoning over hybrid context
based on question-answer pairs, largely reducing
the cost of program annotations.
Discrete reasoning for QA Dua et al. (2019) pro-
posed DROP, a large-scale RC dataset to investi-
gate discrete reasoning over text. Many customized
models are proposed to tackle this task, and these
fall into two dominant directions, the first direction
is to devise specific modules for each types of ques-
tion, such as NAQANET (Dua et al., 2019), Num-
Net (Ran et al., 2019), QDGAT (Chen et al., 2020a),
EviDR (Zhou et al., 2021) and OPERA (Zhou et al.,
2022a). The second direction is to predict programs
that would solve the questions (Andor et al., 2019;
Gupta et al., 2020; Chen et al., 2020d). It is similar
to ours, but our model supports discrete reasoning
over hybrid data.
8 Conclusion
In this work, we propose to perform discrete rea-
soning over table and text as program generation.
Our framework consists of a neural programmer
to parse questions into executable programs and
a symbolic program executor to derive answers
based on decoded programs. Furthermore, we de-
sign a distant supervision method to automaticallyconstruct pseudo programs to alleviate the cost of
manual annotation. Comprehensive experiments
on the TAT-QA and DROP datasets demonstrate
the effectiveness and generality of our model under
both full and distant supervision.
Limitations
In this section, we discuss the limitations of this
work. First, we heuristically search for possible
programs based on manually-designed patterns
under weak supervision. It may require expand-
ing and modifying the patterns to derive possi-
ble programs when adapted to other tasks. How-
ever, the cost of expanding and modifying the
patterns is much less than manually labeling pro-
grams(derivation). Second, we consider all the
possible pseudo programs for each instance and ig-
nore explicitly modeling to choose the correct one
from noisy pseudo programs. Experiment results
show that the distantly supervised model based on
automatically constructed pseudo-programs only
lags behind fully supervised model 1.0 EM and 1.4
F1 on the test set. In our experiments, modeling se-
lection of the correct programs from noisy pseudo
programs has limited room for improvement, but it
remains a valuable topic to explore.
Acknowledge
This work is supported by the National Key
Research and Development Program of China
(No. 2020AAA0108600) and the project of the
National Natural Science Foundation of China
(No.U1908216).7502References75037504A Appendix
A.1 Implementation Details
In our experiments, we employ BART (Lewis et al.,
2020) as the initialization parameters of UniRPG
and optimize it with AdamW (Loshchilov and Hut-
ter, 2019). Table 7 displays all the hyperparameters
for the TAT-QA and DROP datasets during training
and inference. Our experiments are all performed
on 1 NVIDIA A100 GPU.
A.2 Decoding Constraints
Table 8 displays three categories of decoding con-
straints to ensure program legality, including index,
type, and compositional constraints. We implement
these constraints by proposing a decoding mask to
filter illegal program units at each step during in-
ference.A.3 Templates for searching pseudo programs
Table 9 displays the template library for searching
pseudo programs under the w/o derivation setting.
FandFis anyone of {SUM,DIFF,TIMES ,DIV}.
CVis the abbreviation of operation CELL _VALUE .
Cmeans a constant defined in Table 1. Note that
each operation with a numeric subscript means an
instance of this operation.
A.4 Examples
Figure 7- 9 displays several instances, decoded
programs and predictions. Take the first instance
in Figure 7 as example, UniRPG translates
the question “ In which year is the amount
of total sales the largest? ” as the program
G=ARGMAX (KV(CELL (27,27),CV(62,67)),
KV(CELL (28,28),CV(68,73)),
KV(CELL (29,29),CV(74,79))). Instead of
only outputting a string “2019”, we de-
code the program Gthat selects the key
with largest value from the key-value pairs
{⟨2019,1496.5⟩,⟨2018,1202.9⟩,⟨2017,1107.7⟩}
to derive the answer. Intuitively, our model
generates fine-grained programs, illustrating the
discrete reasoning process over table and text.750575067507