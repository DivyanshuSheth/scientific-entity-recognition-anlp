
Lajanugen Logeswaran, Yao Fu, Moontae Lee, Honglak Lee
LG AI Research, University of Illinois at Chicago, University of Michigan
Abstract
Pre-trained language models have shown suc-
cessful progress in many text understanding
benchmarks. This work explores the capability
of these models to predict actionable plans in
real-world environments. Given a text instruc-
tion, we show that language priors encoded
in pre-trained models allow us to infer fine-
grained subgoal sequences. In contrast to re-
cent methods which make strong assumptions
about subgoal supervision, our experiments
show that language models can infer detailed
subgoal sequences from few training sequences
without any fine-tuning. We further propose
a simple strategy to re-rank language model
predictions based on interaction and feedback
from the environment. Combined with pre-
trained navigation and visual reasoning com-
ponents, our approach demonstrates compet-
itive performance on subgoal prediction and
task completion in the ALFRED benchmark
compared to prior methods that assume more
subgoal supervision.
1 Introduction
Developing autonomous agents that can complete
specific tasks given goal descriptions embodies
human-level intelligence. Successful agents in this
setting require multiple reasoning capabilities in-
cluding natural language understanding, visual rea-
soning, and acting over long temporal horizons.
Training black-box models that map instructions
and observations to suitable actions has proven to
be difficult due to challenges in interpreting and
reasoning with multimodal information, especially
in the absence of strong supervision. Thus, general-
ization in this setting demands effective strategies
for planning, exploration, and incorporating feed-
back from the environment.
Generalization in human agents, on the other
hand, stems from our ability to naturally brainstorm
abstract subgoals, better calibrating executable ac-
tions and their sequences. Planning at the level
of subgoals instead of low-level actions allows us
to better adapt to unfamiliar settings. We posit
that language supervision can help realize such
planning capabilities effectively in artificial agents.
First, text is a natural API for interacting with intel-
ligent agents that act in the real world to complete
tasks. Knowledge available in the form of text cor-
pora, descriptions and instructions can be exploited
to build better agents (Branavan et al., 2012; Zhong
et al., 2020). Second, strong language priors are
useful to reason about causal sequences of events
(Li et al., 2021). Language priors can further in-
form about object affordances (e.g. an apple is
sliceable, whereas a table is not) and other contex-
tual knowledge (e.g. a slicing task is more likely to
be performed in a kitchen than a bathroom) (Chen5493et al., 2020). Recent advances have demonstrated
that large language models are able to capture such
priors, as evidenced by their strong capabilities in
language understanding and beyond (Devlin et al.,
2019; Radford et al., 2019; Brown et al., 2020;
Bommasani et al., 2021). This leads to the natu-
ral question of whether priors learned by language
models can help reason about subgoals.
We study the ability of language models to rea-
son about plans composed of a sequence of inter-
mediate goals for completing basic object manipu-
lation tasks in a household environment specified
using text instructions. In particular, we use the
in-context learning ability (Brown et al., 2020) of
large pre-trained language models to reason about
subgoals. In contrast to prior methods that fine-
tune language models to predict subgoals/actions
(Jansen, 2020; Yao et al., 2020), we show that
they can predict subgoal sequences effectively with-
out any fine-tuning. We teach the model how in-
structions translate into subgoal sequences by con-
structing a prompt using few examples. Given the
prompt and a query instruction the language model
predicts likely subgoal sequences (see Figure 1 for
an illustration).
While language models are capable of generat-
ing strong hypotheses, we observe that these predic-
tions may not be directly usable by agents acting in
real environments. First, they suffer from calibra-
tion issues: Language models have a tendency to
repeat content from the prompt (Zhao et al., 2021).
We show that mutual-information inspired metrics
help mitigate calibration issues and lead to better
ranking of model generated hypotheses.
Second, real-world agents have to update their
beliefs and predictions based on interaction and
feedback from the environment. Without such feed-
back we cannot expect the predicted plan to be
executable in the environment. We execute plans
proposed by the language model in the environ-
ment using a pre-trained low-level policy and col-
lect feedback about task success/failure. We use
this feedback as a learning signal to train a ranking
model that re-ranks language model predictions. In
contrast to prior methods that rely on strong sub-
goal supervision and task level expert trajectories,
we show that combining subgoal predictions with
a pre-trained subgoal execution policy leads to a
strong embodied agent baseline.
We make the following contributions in this
work. We show that•Large language models can predict subgoals from
text instructions with very little supervision using
in-context learning.
•Incorporating a small amount of feedback from
interaction with the environment such as agent
state and task success/failure outcome improves
language model predictions.
•Combining predicted subgoals with a pre-trained
low-level policy for navigation and visual reason-
ing leads to a simple modular agent policy that
performs well on an embodied learning setting.
2 Related work
Language models for planning and interaction
The use of language models for planning and action
prediction has been explored in prior work. Jansen
(2020) fine-tuned a language model to predict sub-
goal sequences for text instructions from the AL-
FRED benchmark. Micheli and Fleuret (2021) take
a similar approach, but show that imitation learning
with few instances combined with reinforcement
learning produces models that work well on the
ALFWorld benchmark (Shridhar et al., 2021). Yao
et al. (2020) demonstrate a similar approach for in-
teractive fiction games (Hausknecht et al., 2020). In
contrast to these prior methods, our approach does
not assume strong supervision and we demonstrate
generalization with limited training examples. Fur-
thermore, in order to exploit the generalization ca-
pabilities of large language models, we do not fine-
tune these models and instead use their in-context
learning ability. Finally, our approach allows us to
build policies that inherit the strong generalization
capabilities of these large pre-trained models such
as compositional generalization.
Large language models and few-shot learning
Brown et al. (2020) showed that pre-trained large
language models have few-shot learning capabil-
ities. Given a few examples {(x, y=f(x))}
that define a task fsuch as classification or trans-
lation and a query instance x,prompting a lan-
guage model with a string such as " x=y;x=
y;...;x=y;x=" leads to meaningful com-
pletions by the language model y≈f(x). This
few-shot learning capability of language models
has since then been studied and improved upon
with approaches like prefix engineering (Schick
and Schütze, 2021), prompt tuning (Li and Liang,
2021), model calibration (Zhao et al., 2021) and
other methods (Min et al., 2021a). We adopt a
similar approach for few-shot subgoal inference.5494We assume that subgoal supervision is available
for a small number of training tasks and use the
language model to infer subgoals for unseen tasks.
Instruction following There is rich literature on
agents that follow language instructions (Branavan
et al. (2009); Mei et al. (2016); Fried et al. (2018);
Suhr et al. (2019) inter alia). Recent develop-
ments in simulated environments and benchmarks
with human annotated instructions have driven
progress in embodied agents that learn from text
instructions (Shridhar et al., 2020; Kolve et al.,
2017). Successful agents in these settings require
multiple reasoning capabilities including language
understanding, visual reasoning and learning to act
over long time-horizons. Recent embodied learning
literature exploit subgoal supervision, pre-trained
visual reasoning components and pre-trained trans-
former models to do well on the task (Singh et al.,
2020; Suglia et al., 2021; Zhang and Chai, 2021;
Corona et al., 2021; Blukis et al., 2022). Unlike
these methods, we do not assume access to strong
subgoal supervision or task level expert supervision.
We combine language model predictions with pre-
trained low-level navigation and interaction poli-
cies to obtain a competitive agent policy.
Few-shot semantic parsing Subgoal inference
from text instructions can be considered a seman-
tic parsing problem where the subgoal sequences
serves as a formal representation of text. Shin et al.
(2021) show that few-shot semantic parsers can
be derived from language models and demonstrate
their applicability on text-to-SQL (Finegan-Dollak
et al., 2018) and SCAN (Lake and Baroni, 2018)
benchmarks. Furrer et al. (2020) and Herzig et al.
(2021) further study the compositional generaliza-
tion ability of such semantic parsers. In our work
we make use of ideas introduced in these works
such as dynamic prompt creation, constrained de-
coding and intermediate representations.
3 Approach
We first consider subgoal inference as a semantic
parsing problem where a text instruction needs to
be translated to a sequence of subgoals and propose
an approach to few-shot subgoal inference based
on pre-trained language models in Section 3.1. We
extend this setting to an agent acting in a simulated
environment which can execute these subgoals, ob-
serve feedback, and improve upon language modelpredictions for more accurate subgoal inference in
Section 3.2.
3.1 Few-shot subgoal inference
Subgoals We are interested in a particular sub-
class of instruction following problems which in-
volve performing a sequence of object interac-
tions in an embodied environment. Each object
interaction requires navigating to a particular ob-
ject and performing an action on it. A task is
considered successfully completed if the state of
objects in the end satisfy a set of task-specific
constraints (for instance, objects that need to be
sliced/warmed/cooled/cleaned have the appropriate
state change). It is thus natural to define a subgoal
as one or more sequence of object interactions. A
subgoal gis specified as g= (b, o)∈ B×O where
b∈ B={Pickup, Clean, Heat, ..} is one of a pre-
defined set of abstract actions and o∈ O={Apple,
Microwave, DeskLamp, Ottoman, ..} is an object
category.
Subgoal inference problem Given a text instruc-
tionτ, subgoal inference seeks to predict a se-
quence of subgoals τ∝⇕⊣√∫⊔≀→g= (g, ..., g). To
perform in-context learning with a language model,
we consider a representation v(g)ofgthat looks
like natural text, where vis a pre-defined invertible
mapping. Such representations have been referred
to in the literature as verbalizers (Min et al., 2021a),
intermediate representations (Herzig et al., 2021)
and canonical representations (Shin et al., 2021),
where the purpose is to represent the output in a
format the language model understands. In a slight
abuse of notation, we will use gto refer to either a
subgoal sequence or it’s textual representation v(g)
depending on the context.
Generating subgoals We assume that a small
amount of training data {(τ, g),···,(τ, g)}is
given. The language model is prompted with a
comma separated concatenation of the training ex-
amples, each in the format " τ=g", followed by
a query τ, formatted as " τ=". We assume that the
probability of a hypothesis h(i.e., text representa-
ton of a subgoal sequence) can be modeled as in
Equation (1), where his the itoken of hand the
token probabilities are derived from the language
model.
p(h|τ) =/productdisplayp(h|h, τ,{τ, g})(1)
We use beam search to identify the top-k hypothe-
ses according to p(h|τ). Generated hypotheses are5495
constrained to be valid representations of subgoal
sequences by considering only tokens which lead
to a valid partial prefix of a subgoal sequence at
each step of beam search.
Re-ranking predictions Recent studies have
found that language models have popularity and
recency biases: the tendency to repeat content men-
tioned in the prompt, especially content appearing
later in the prompt (Zhao et al., 2021). They consid-
ered a simple approach to mitigate such biases in
model predictions for classification tasks by com-
paring the likelihood of an output label with and
without the query. In contrast to this ‘direct model’
which models the probability of a label given the in-
putp(y|x), Min et al. (2021a) showed that a ‘chan-
nel model’ which models p(x|y)leads to better,
more stable models.
Inspired by these observations, we propose to
usep(τ|h)to score hypotheses in addition to
p(h|τ). Mutual Information based ranking metrics
are a natural candidate and they have been explored
in the text generation literature (Li et al., 2016; Li
and Jurafsky, 2016). We generate multiple hypothe-
ses from the model using p(h|τ)and the generated
hypotheses are re-scored using the weighted mutual
information metric (1−λ)logp(h|τ)+λlogp(τ|h)
where λis a hyperparameter. To compute p(τ|h),
we again use the language model prompted with
"g=τ, ..., g=τ, h=" as the query and com-
pute the conditional probability of τ. We expect
this paradigm of generating a set of strong hypothe-
ses, followed by accurate re-ranking is more gen-
erally applicable to other few-shot language under-
standing problems.3.2 Agent policy and incorporating
environment feedback
We next consider building an agent that acts in a
visual environment to complete tasks given text in-
structions. While Section 3.1 treated the language
model as a knowledge extraction system, in the real
world plans need to be updated based on interac-
tion and feedback from the environment. We thus
propose a method to improve language model pre-
dictions based on environment interaction. Since
our goal is to learn the planning component of the
agent, we assume a pre-trained low-level policy
is provided and optimize over the space of plans.
Jointly learning both components is beyond the
scope of this work and left as future work.
We assume that a representation of the agent
statesis available. The state representation cap-
tures information about the environment (e.g. ob-
jects present and their locations) estimated based
on the agent’s visual observations. As the agent
explores the environment and collects new obser-
vations the state representation is updated. As-
suming that a low-level policy πpre-trained to
execute a given subgoal is provided, our goal is
to train a high-level policy πwhich proposes
the subgoals to be executed by the low-level pol-
icy. More formally, the high-level policy models
π(g|τ, s, g)where τis a text instruction,
sis the state representation and gis the se-
quence of subgoals completed so far at high-level
time-step t.
While the language model can generate com-
pelling subgoal hypotheses, it doesn’t take into
account information about the environment. For
instance, knowledge about the type of room the5496agent is in (kitchen, bathroom, etc.) and the ob-
jects present in it are useful to infer the kind of
tasks and subgoals that can be performed. We
propose to re-rank hypotheses generated by the
language model based on information from the en-
vironment to construct π. The plans generated
by the language model are executed in the envi-
ronment using π. The success/failure outcomes
of these plan executions are used to construct a
labeled dataset of instructions τ, plans gand agent
states. A supervised ranking model f(g, τ, s ;θ)
is trained using this data to re-rank the language
model predictions. We represent the ranking model
asf(g, τ, s ;θ) = θconcat (f(s), f(τ, g))
where f(s)is a state embedding, f(τ, g)is
a joint encoding of τandgproduced by a text
encoder and θis a parameter vector. Although
a text embedding can be derived from the lan-
guage model, we use a BERT encoder in favor
of obtaining a smaller dimensional representation
(f=BERT). See Appendix C for more
details.
During inference, an instruction τis given, and
we use the procedure in Section 3.1 to generate
top-k hypotheses. At each step, hypotheses incon-
sistent with the sequence of subgoals executed so
far are pruned and the remaining hypotheses are
re-ranked based on the current agent state using
f. The agent attempts the next subgoal proposed
by the top hypothesis. The process ends when the
stop subgoal is predicted. See Figure 2 for an illus-
tration and Appendix D for more details about the
training and inference algorithms.
4 Experiments
4.1 Data
We use data from the ALFRED benchmark pro-
posed by Shridhar et al. (2020) in our experiments.
The ALFRED task requires an agent to execute
instructions specified in text to accomplish basic
tasks in an embodied environment. A given task is
described using a high-level language directive as
well as low-level step-by-step instructions (We only
use the high-level description). The dataset consists
of 7 task types (and 11 fine-grained types), and has
more than 20k natural language task descriptions
collected from human annotators. In addition, ex-
pert demonstrations computed by a planner are also
made available. Tasks require acting over many
time-steps, with an average of 50 actions, and the
longest tasks require 100+ steps.
The ground truth subgoal sequences in thedataset consist of both navigation subgoals and ob-
ject interaction subgoals. We discard the navigation
subgoals and only retain the interaction subgoals
for the following reasons. First, the interaction
subgoals are sufficient for an agent to successfully
complete the task. Second, predicting navigation
subgoals from the text instruction alone may not al-
ways be possible as they often depend on the scene
layout.
Subgoal representation A subgoal gis spec-
ified as g= (b, o)∈ B × O where |B|= 7
and|O|= 80 . We define a textual representa-
tionv(b)of each action type (e.g. v(Pickup) =
‘pick up’, v(Heat) =‘heat in’). The object types
oare identified by a text string v(o)in the dataset
and we directly use them as the text representa-
tion with minimal pre-processing (e.g. v(apple) =
‘apple’, v(desklamp) =‘desk lamp’). The sub-
goal is represented as v(g) =‘v(b)v(o)’ (e.g.
v((Pickup, apple)) =‘pick up apple’). A sub-
goal sequence g= (g, ..., g)is represented
asv(g) =‘v(g), ...,v(g).’. Text representa-
tions of all subgoals are given in Appendix B. Note
that there are many plausible choices for the repre-
sentation vand a different set of choices can lead
to different results.
Metrics We use top-k recall to evaluate the abil-
ity of language models to generate plans from in-
structions by comparing against ground truth plans.
In addition, we also evaluate the performance of an
agent acting in the AI2-Thor (Kolve et al., 2017)
simulator to complete tasks using task success rate
(the percentage of tasks successfully completed).
4.2 Few-shot subgoal inference
We construct a training set of N= 22 instances by
randomly choosing two instances per fine-grained
task type. The language model is prompted with
a concatenation of these training examples and
the query instance. We perform constrained beam
search decoding with a beam size of 10 to generate
subgoal sequences. At each step of beam search,
only tokens which lead to a valid partial prefix of a
subgoal sequence are considered. All model gener-
ated hypotheses thus correspond to valid subgoal
sequences. We evaluate models on the valid-seen
split of the dataset which has 800 instances.
Table 2 shows subgoal inference results cate-
gorized by task type. We use publicly available
pre-trained transformer language models GPT2-XL
(Radford et al., 2019) and GPT-J (Wang and Komat-5497
suzaki, 2021) via the HuggingFace library (Wolf
et al., 2020), which respectively have 1.5B and 6B
parameters, in our experiments. The first six of the
seven task types have two object arguments each.
The pick place movable task type has three object
arguments and hence a lower recall than the other
task types. The top-10 recall of GPT2-XL and GPT-
J are respectively 59% and 71%, which shows that
large language models have strong ability to reason
about plans from few training examples.
Re-ranking hypotheses The top-k recall perfor-
mance reported in Table 2 is based on log p(h|τ).
We confirmed that the biases reported in the liter-
ature such as predicting content from the prompt
are present in model predictions (Zhao et al., 2021).
Consider the query example Place a martini glass
with a fork on it on the table . The following two
are among the top generated hypotheses:
a) pick up fork, put in cup, pick up cup, put in sink.
b) pick up fork, put in cup, pick up cup, put in
table.
When the prompt contains training examples that
mention ‘sink’, the model assigns the following log
p(h|τ)to these hypotheses: a) -2.4 and b) -4.3.
However, when all training instances in the prompt
involving ‘sink’ are removed, the log probabilities
now become, a) -13.7 and b) -9.1 The incorrect hy-
potheses involving ‘sink’ is now ranked below the
correct hypothesis involving table. While languagemodels can retrieve strong hypotheses as indicated
by the high top-10 recall, this observation shows
that the ranking of these hypotheses, as determined
byp(h|τ), may not be accurate. We thus consider
mutual information based ranking approaches. Ta-
ble 1 shows top-1 recall when model generated
hypotheses are ranked according to different crite-
ria. We also vary the number of training examples
Nby randomly choosing respectively 1, 2 and 3
instances per fine-grained task type.
We first observe that p(τ|h)ranks hypotheses
better than p(h|τ)with very limited supervision
(N= 11 ). However, it is often worse when more
supervision is available. In contrast, combining the
two log probabilities with λ=yields consistently
better performance across models and number of
training examples. This shows that generating a
large number of hypotheses with a language model,
followed by more accurate re-ranking using Mutual
Information inspired metrics can be an effective
paradigm for few-shot generation tasks with in-
context learning.
Comparison with prior work We compare our
prediction performance against prior work in Ta-
ble 3. Jansen (2020) fine-tunes a GPT2-Medium
model (325M parameters) to predict subgoals from
instructions and report prediction resultswhen the
model is trained on varying amounts of training
data: 1%, 10%, 25%, 100% of the training set,
which has 7793 instances. We ignore the naviga-
tion subgoals in this evaluation and only compare
the sequence of object interactions. We report pre-
diction performance of GPT-J using our approach
on the same test set. The results show that large
language models encode useful knowledge that can
help plan from instructions effectively when su-
pervision is limited. However, fine-tuning can be
effective when more supervision is available due
to the fixed context length limitation of in-context5498
learning. See Section 5 for ablations and more
discussion about fine-tuning.
Prediction errors We examine prediction errors
in identifying task type and object type. Key
sources of model errors include annotation issues
and ambiguity in object types. Table 4 shows the
object types that have the least prediction accu-
racy, along with the object categories the model is
confused about. Annotations can fail to correctly
identify the target object - identifying a butter knife
as aknife or apepper shaker assalt shaker . Am-
biguity can also arise from identifying an object
with different names, depending on the context.
For instance, depending on the scene layout, the
argument for a look at object in light task can be a
floor lamp or a desk lamp. Unless the type of lamp
is identified precisely in the instruction, it is not
possible to correctly predict the type of lamp. Cor-
rectly identifying these objects requires feedback
from interaction with the environment.
The experiments so far evaluate the ability of a
language model to retrieve ground truth subgoal
sequences. Next we examine embodied agents that
make use of these predictions and collect more su-
pervision in order to improve subgoal predictions.
4.3 Agent policy and incorporating
environment feedback
We now use subgoal predictions to construct an
agent policy that acts in a simulated environment
to complete tasks. The agent state representation
and pre-trained low-level subgoal policy are bor-
rowed from the HLSM model proposed in Blukis
et al. (2022). HLSM represents the agent state
as a spatial persistent voxel representation of the
room which models the location and category of ob-
jects present. The representation is constructed us-
ing modules that estimate segmentation and depth
maps and other visual reasoning components and is
updated as the agent gathers new observations. We
use pre-trained models made available by the au-
thorsfor state estimation and the low-level policy
in our experiments.
We combine subgoal predictions with the pre-
trained HLSM low-level policy and evaluate the
overall agent policy on the ALFRED task in Ta-
ble 5. Unlike the results reported in Section 4.2
which were based on the static dataset, these re-
sults are based on subgoals executed against the
AI2-Thor simulator. In addition to task success
rate, we also report the percentage of goal condi-
tions satisfied, which rewards the model for partial
task completions.
We compare against the following baselines
on the ALFRED task. Seq2seq (Shridhar et al.,
2020) is a simple sequence-to-sequence baseline
trained to map text instructions to low-level ac-
tions. MOCA (Singh et al., 2020) improves on
Seq2seq using subgoal supervision and pre-trained
visual reasoning components. Recent work such
as HLSM (Blukis et al., 2022) and FiLM (Min
et al., 2021b) build and use spatial semantic state
representations and achieve stronger performance
on the task. Note that, unlike these prior methods
(MOCA, HLSM, FiLM) that rely on full subgoal
supervision (20k instances), our approach is based
on a small amount of subgoal supervision and addi-
tional supervision collected using active interaction
with the environment. In addition, our approach
does not require task-level expert trajectories and
only assumes that a subgoal execution policy is
provided.
Using the top language model prediction as is
without using any information from the environ-
ment leads to 20% success rate. Next, we collect
plan execution feedback for 1000 text instructions
to train the ranking model described in Section 3.2.
Re-ranking language model predictions using the
trained ranking model improves the performance5499
to 24%, which shows the importance of incorpo-
rating feedback from environment interaction. In
comparison, the HLSM model with full subgoal su-
pervision has success rate 30%. Although our pre-
dictions fall short of HLSM, they are competitive
with the other baselines with subgoal supervision.
The performance upper bound estimated using or-
acle subgoals is 37%, which shows the room for
improvement over our predictions. These results
show that accurate subgoal inference coupled with
pre-trained low-level components leads to agents
that perform well in embodied environments.
Figure 1 shows an example where the ranking
model uses environment information to identify
better plans. In this example, the instruction am-
biguously specifies the receptacle as ‘white table
with shelving’. The language model’s top two pre-
dictions for the target receptacle are ‘side table’
and ‘shelf’, neither of which are present in the envi-
ronment. The agent state captures this information
and helps identify ‘dining table’ as the correct re-
ceptacle.
5 Ablations
We perform a series of ablations to identify the
robustness of model predictions. We compare the
performance of in-context learning to a sequence-
to-sequence model fine-tuned to translate instruc-
tions to subgoal sequences. In addition, we observe
the effect of varying the number of training exam-
ples and choice of training examples.
Number of training examples Figure 3 shows
model recall for varying number of training ex-
amples. For zero training examples, the prompt
consists of just the query instruction and the model
decodes subgoals. Beyond 44 examples (4 exam-
ples per task type), the model prompt no longer
fits the sequence length restriction (1024 tokens) of
GPT models. A steady increase in performance can
be initially observed when increasing the number
of training examples and the performance saturates
towards the end. In-context learning further has
the limitation of not being able to accommodate
a larger number of training examples due to the
length restriction. It would be interesting to ex-
plore how to make effective use of large number of
training examples in future work.
Choice of training examples We also estimate
performance variance by varying the random seed
for choosing examples randomly from the training
set and compute standard deviation based on five
random seeds for each setting. The plot shows that
top-1 predictions from in-context learning have
lower variance compared to fine-tuning.
Comparison with fine-tuning In order to under-
stand how well the in-context learning approach
compares to fine-tuned models, we fine-tune a T5-
large model (Raffel et al., 2019) with 770M param-
eters on varying amounts of training data (this was
the largest model we could fine-tune on our com-
pute infrastructure). Note that this is not a head-to-
head comparison between in-context learning and
fine-tuning due to the difference in model size. Fur-
thermore, there are other fine-tuning mechanisms
such as prompt tuning and head tuning (Min et al.,
2021a) which are not considered here. However,
the result suggests that in-context learning with
large pre-trained models can be favorable when
computational constraints do not allow full fine-
tuning of large models.
These ablations show that the in-context learn-
ing ability of large language models leads to pre-5500dictions that are accurate, robust and stable in the
presence of a small amount of training data.
6 Conclusion
This work explores the use of pre-trained lan-
guage models for planning in real-world tasks. We
showed that language models have strong capabil-
ity to reason about subgoal sequences given a small
number of training examples. We further demon-
strated some simple mechanisms to incorporate
feedback from interaction with the environment
and show that this leads to more usable predictions.
Finally, we show that combining subgoal predic-
tions with a pre-trained low-level policy yields a
strong baseline for embodied agent learning.
Our ablations demonstrate that in-context learn-
ing with a small amount of subgoal demonstrations
has robust generalization properties. However, we
also point out that in-context learning has the limi-
tation of not being able to incorporate a large num-
ber of training examples due to the fixed context
length restriction. It would further be beneficial to
perform end-to-end learning with language model
based subgoal prediction and a low-level policy,
which would be interesting to explore in future
work.
References550155025503A Mutual Information based scoring
Mutual Information between random variables
X, Y is defined as in Equation (2). We consider a
weighted Mutual Information metric as defined as
in Equation (3) similar to Li and Jurafsky (2016)
and introduce the hyperparameter λ. Identifying Y
that maximizes the weighted Mutual Information
is equivalent to maximizing the expression in Equa-
tion (4). We use this metric to rank hypotheses
generated by the language model.
MI(X, Y ) =logp(x, y)
p(x)p(y)(2)
wMI(X, Y ) =logp(x, y)
p(x)p(y)(3)
argmaxwMI(X, Y )
= argmaxlogp(x, y)
p(x)p(y)
= argmaxlog/parenleftbiggp(x, y)
p(x)/parenrightbigg/parenleftbiggp(x, y)
p(y)/parenrightbigg1
p(x)
= argmax(1−λ)logp(y|x) +λlogp(x|y)
−λlogp(x)
= argmax(1−λ)logp(y|x) +λlogp(x|y)
(4)
B Subgoal representation
Table 6 shows the subgoal representation we use in
this work.
Subgoal Representation
(Pickup, X) pick up X
(Put, X) put in X
(Heat, X) heat in X
(Cool, X) cool in X
(Clean, X) clean in X
(Slice, X) slice X
(ToggleOn, X) turn on X
Table 6: Subgoals and corresponding text represen-
tation. X represents an object argument.C Ranking model: Architecture
State embedding HLSM represents the agent
state as a semantic voxel representation s∈
[0,1]where the value s(x, y, z, c )rep-
resents if there is an object of type cat position
(x, y, z )of the room layout. We pool across the
spatial dimensions of the representation and project
it using a linear mapping to obtain f(s). We
use this encoding as the state embedding.
Instruction and subgoal sequence encoding
The instruction τand a candidate subgoal se-
quence gare jointly processed using a BERT
encoder and the CLS representation is used as
a representation vector BERT(τ, g). The
ranking model is represented as f(g, τ, s ;θ) =
θconcat (f(s),BERT(τ, g))where θis a
parameter vector.
D Ranking Model: Training and
Inference
Formally, the learning problem is a MDP
(S,G,L,R,T), where s∈ S is the agent state,
g∈ G is a subgoal, τ∈ L is a text instruc-
tion,R(τ, s)is a reward function that provides
success/failure feedback for completing a given
instruction, T: (s, g)→sis a state tran-
sition function where sis computed by a low-
level policy πpre-trained to execute a given sub-
goalg. Our goal is to train a high-level policy
π(g|τ, s, g)where sis the agent state and
gis the sequence of subgoals completed so far
at high-level time-step t.
Algorithm 1 describes how we collect training
data to train the ranking model. Algorithm 2 shows
how the ranking model is used during inference.5504Algorithm 1 Training
Given: epochs = 100 ,D(set of instructions)
Collect training data
D ← {} (Initialize training set)
forτinDdo
Generate plans and re-rank using mutual information metric g, . . . , g∼p(·|τ)
fori= 1. . . k do
Initialize agent state s
S← {s}(Record agent states)
forj= 1. . .|g|do
s← T (s, g)(Execute gusing π)
S←S∪ {s}
end for
ifR(τ, s)>0then (Task succeeded)
D ← D ∪ { (g, τ, s)|s∈S}
break
end if
end for
end for
Train model
fori= 1. . .epochs do
loss←0
for(g, τ, s )∈ D do
Generate plans g, .., g∼p(·|τ)
loss←loss−log
end for
θ←Optimizer Update (θ,∇f)
end for
return f
Algorithm 2 Inference
Given: Instruction τ,i= 10
Generate plans g, . . . , g∼p(·|τ)
G← {g, . . . , g}
Initialize agent state s
i←0(subgoal index)
g←argmaxf(g, τ, s ;θ)
while|G| ̸= 0andg̸=<stop> and i < ido
s← T (s, g)(Execute gusing π)
G← {h|h∈Gandh=g}(Retain plans consistent with subgoals completed so far)
g←argmaxf(g, τ, s ;θ)
i←i+ 1
end while
return g, s5505E Analysis of model errors
Figure 4 shows the confusion matrix for object type prediction. Predictions are from top-1 subgoal
sequences predicted by GPT-J.5506