A Contrastive Framework for Learning Sentence Representations from Pairwise and Triple - wise Perspective in Angular Space Yuhao Zhang1 , Hongji Zhu1 , Yongliang Wang1 , Nan Xu2 , Xiaobo Li1 , BinQiang Zhao1 1Alibaba Group 2Institute of Automation , Chinese Academy of Sciences { zyh277500,zhj283587,wangyongliang.wyl,xiaobo.lixb,binqiang.zhao}@alibaba-inc.com xunan2015@ia.ac.cn Abstract Learning high - quality sentence representations is a fundamental problem of natural language processing which could benefit a wide range of downstream tasks . Though the BERT - like pretrained language models have achieved great success , using their sentence representations directly often results in poor performance on the semantic textual similarity task . Recently , several contrastive learning methods have been proposed for learning sentence representations and have shown promising results . However , most of them focus on the constitution of positive and negative representation pairs and pay little attention to the training objective like NT - Xent , which is not sufficient enough to acquire the discriminating power and is unable to model the partial order of semantics between sentences . So in this paper , we propose a new method ArcCSE , with training objectives designed to enhance the pairwise discriminative power and model the entailment relation of triplet sentences . We conduct extensive experiments which demonstrate that our approach outperforms the previous state - of - the - art on diverse sentence related tasks , including STS and SentEval . Figure 1 : Sentence representations visualization . We generate the representations of three related sentences by passing them to BERTbase , SimCSE - BERTbase and ArcCSE - BERTbase multiple times . With different dropout masks , we can generate different representations for each sentence . Then we normalize the embeddings and use t - SNE for dimensionality reduction . or applying mean pooling on the last layers of a BERT - like pre - trained language model ( Reimers and Gurevych , 2019 ) . However , the sentence embeddings coming from a pre - trained language model without further fine - tuning could not capture the semantic meaning of sentences very well as shown in Figure 1(a ) , and sometimes even underperform non - contextualized embeddings like GloVe ( Pennington et al . , 2014 ) . 1 Introduction Learning sentence representations , which encodes sentences into fixed - sized dense vectors such that semantically similar ones stay close , is a fundamental problem of natural language processing . It could benefit a wide range of downstream applications such as information retrieval , semantic similarity comparison , question answering , and so on . Recently , with the great success of pre - trained Transformer - based language models ( Devlin et al . , 2019 ; Liu et al . , 2019 ; Raffel et al . , 2020 ; Brown et al . , 2020 ; Liu et al . , 2019 ) like BERT , they have been widely adopted for generating sentence representations . A straightforward way is by leveraging the [ CLS ] embedding ( Devlin et al . , 2019 ) To make pre - trained language models more suitable for generating sentence embeddings , supervised methods like SBERT ( Reimers and Gurevych , 2019 ) are proposed , which improve the performance by fine - tuning on a labeled dataset . As labeled data is not available or expensive to annotate in many tasks or domains , it is of great value for developing unsupervised / self - supervised approaches for learning sentence representations . So recent works like BERT - Flow ( Li et al . , 2020 ) and BERT - Whitening ( Su et al . , 2021 ) propose postprocessing methods to improve the BERT - based sentence representation . They address that the nonsmooth anisotropic semantic space of BERT is a bottleneck and alleviate the problem through normalizing flows and whitening operation . To further improve the quality of sentence representations , several works ( Kim et al . , 2021 ; Yan et al . , 2021 ; Giorgi et al . , 2021 ; Carlsson et al . , 2021 ; Gao et al . , 2021 ) adopt self - supervised contrastive learning approach , which learns sentence representations by minimizing the distance of positive sentence representation pairs and maximizing the distance of negative pairs . In these works , positive pairs are often constituted through data augmentation or encoders with different structure or parameters , while negative pairs are derived from different sentences within the same batch . Then contrastive learning objective like normalized temperature - scaled crossentropy loss ( NT - Xent ) ( Chen et al . , 2020 ; Gao et al . , 2021 ) is used for optimizing . A typical example unsup - SimCSE ( Gao et al . , 2021 ) achieves state - of - the - art performance with a simple and effective idea of using standard dropout for data augmentation . between sentences . To alleviate these problems , in this paper , we propose a new approach ArcCSE for sentence representation learning . For pairwise sentence relation modeling , we propose Additive Angular Margin Contrastive Loss ( ArcCon Loss ) , which enhances the pairwise discriminative power by maximizing the decision margin in the angular space . Besides , in order to model the partial order of semantics between sentences , we propose a new self - supervised task that captures the entailment relation among triplet sentences . The task is implemented through automatically constituted triplet sentences with entailment relation among them . A visualization example of the generated representations through ArcCSE is shown in Figure 1(c ) . We evaluate our method on standard semantic textual similarity ( STS ) tasks and SentEval transfer tasks , and it outperforms the previous state - of - the - art approaches . 2 Related Work 2.1 Unsupervised Sentence Representation Learning Early works usually learn sentence representations by augmenting the idea of word2vec ( Mikolov et al . , 2013 ) , such as predicting surrounding sentences ( Kiros et al . , 2015 ; Hill et al . , 2016 ; Logeswaran and Lee , 2018 ) or summing up n - gram embeddings ( Pagliardini et al . , 2018 ) . With the rise of pre - trained language models , many works try to generate sentence representations through BERT - like models . A common way is leveraging the [ CLS ] embedding or applying mean pooling on the last layers of BERT ( Reimers and Gurevych , 2019 ; Li et al . , 2020 ) . Instead of using BERT embeddings directly , BERT - Flow ( Li et al . , 2020 ) and BERT - Whitening ( Su et al . , 2021 ) further improve sentence representation through post - processing . Though existing contrastive methods for learning sentence representation have shown promising results , most of them focus on the positive and negative pairs constitution , and the optimization objective itself is not fully exploited . The contrastive learning objective NT - Xent loss used in recent works ( Yan et al . , 2021 ; Giorgi et al . , 2021 ; Gao et al . , 2021 ) is a variation of cross - entropy loss with softmax function . Recent studies ( Wang et al . , 2018 ; Deng et al . , 2019 ) have shown that the traditional softmax - based loss is insufficient to acquire the discriminating power , as shown in Figure 1(b ) in which SimCSE - BERTbase adopts the NT - Xent loss and could not separate sb and sc completely . In addition , the current optimization objectives only models sentence relations in a pairwise perspective , which tries to pull sentences with similar semantics closer and push dissimilar ones away from each other . However , there are different degrees of semantic similarity among related sentences . For example in Figure 1(d ) , sb is more similar to sa than sc is . The current optimization objectives lack the ability to model the partial order of semantics Recently , several works adopt the contrastive learning framework for sentence representation learning . They propose different strategies to constitute contrastive pairs , either through different data transforming methods ( Zhang et al . , 2020 ; Yan et al . , 2021 ; Giorgi et al . , 2021 ) , or through encoders with different structures or parameters ( Carlsson et al . , 2021 ; Kim et al . , 2021 ; Gao et al . , 2021 ) . A typical example SimCSE ( Gao et al . , 2021 ) uses dropout as data augmentation strategy and achieves state - of - the - art performance . However , most existing works pay little attention to the training objective and use the traditional contrastive Figure 2 : The framework of ArcCSE . ArcCSE models pairwise and triple - wise sentence relations simultaneously . For pairwise sentence relation modeling , we pass sentences to a BERT - like encoder with dropout turn on twice . Then we feed the representations into ArcCon loss which is more discriminative than NT - Xent loss . Triplet sentences are constituted through masking . We pass them to the same BERT - like encoder with dropout turn off and use a triplet loss to model their relations . loss directly , which is insufficient in discrimination and unable to model the partial order of semantics between sentences . So , in our work , we propose a new approach that jointly models the pairwise and triple - wise sentence relations and further improves the sentence representations â€™ quality . ing results in supervised metric learning . Typical examples like Center Loss ( Wen et al . , 2016 ) , SphereFace ( Liu et al . , 2017 ) , CosFace ( Wang et al . , 2018 ) and ArcFace ( Deng et al . , 2019 ) are widely adopted in deep learning applications such as face recognition and sentence classification ( Coria et al . , 2020 ) . However , these losses need class labels and are not suitable for learning sentence representations . So inspired by ArcFace , we propose a new training objective ArcCon that does not need class labels and can model pairwise sentence relations with more discriminative power than traditional contrastive training objectives . 2.2 Deep Metric Learning Objectives The goal of Deep Metric Learning ( DML ) is to learn a function that maps objects into an embedded space , in which similar objects stay close and dissimilar ones are far away . In order to achieve this goal , many approaches have been proposed , and designing appropriate loss functions plays a key role in it . Contrastive training objectives like Contrastive Loss ( Chopra et al . , 2005 ) , N - Pair Loss ( Sohn , 2016 ) , Structured Loss ( Song et al . , 2016 ) and Triplet Margin Loss ( Ma et al . , 2021 ) apply the definition of metric learning directly . These objectives are among the earliest training objectives used for deep metric learning . Later , softmax - based losses which learn a center for each class and penalize the distances between deep features and their corresponding class centers achieve more promis3 Method In this section , we present ArcCSE , an angular based contrastive sentence representation learning framework , which could generate superior sentence embeddings from unlabeled data . Given a pretrained language model M and an unlabeled text dataset D , the task is fine - tuning M on D so that the sentence representations generated through M could be more semantic discriminative . Our framework consists of two components that model pairwise and triple - wise sentence relations simultaneously , as shown in Figure 2 . We start with angular margin based contrastive learning in Section 3.1 , which models pairwise relations between sentences by pulling semantic similar ones closer while pushing dissimilar ones away . Then we introduce the method which models the partial order of semantics between automatically constituted triplet sentences in Section 3.2 . Figure 3 : Comparison of NT - Xent loss and ArcCon loss . For sentence representation hi , we try to make Î¸i , iâˆ— smaller and Î¸i , j larger , so the optimization direction follows the arrow . With an extra margin m , ArcCon is more discriminative and noise - tolerant . 3.1 Angular Margin based Contrastive Learning To model the positive / negative pairwise relations between sentences , we first need to generate sentence representations and group them into positive and negative pairs . Then we feed these pairs to a training objective for optimizing . The decision boundary for hi in NT - Xent is Î¸i , iâˆ— = Î¸i , j , as show in Figure 3 . Due to lack of decision margin , a small perturbation around the decision boundary may lead to an incorrect decision . Given a collection of sentences D = { si}N i=1 , we generate the sentence representations through a BERT - like pre - trained language model M. Following SimCSE , we use dropout as the data augmentation method . For each sentence si , we generate two different representations hi and hâˆ—i from si by passing si to M twice with independently sampled dropout masks . These two representations with the same semantics constitute a positive pair , while the negative pairs are derived from the representations of different sentences within the same batch . To overcome the problem , we propose a new training objective for sentence representation learning by adding an additive angular margin m between positive pair hi and hâˆ—i . We named it Additive Angular Margin Contrastive Loss ( ArcCon Loss ) , which can be formulated as follows : ecos(Î¸i , iâˆ— + m)/Ï„ Larc = âˆ’log ecos(Î¸i , iâˆ— + m)/Ï„ + ( cid:80 ) j = i ecos(Î¸j , i)/Ï„ ( 3 ) In this loss , the decision boundary for hi is Î¸i , iâˆ— + m = Î¸i , j , as show in Figure 3 . Compared with NT - Xent , it further pushed hi towards to the area where Î¸i , iâˆ— get smaller and Î¸i , j get larger , by increasing the compactness of sentence representations with the same semantics and enlarging the discrepancy of different semantic representations . This help enhance the alignment and uniformity properties ( Wang and Isola , 2020 ) , which are two key measures of representation quality related to contrastive learning , indicating how close between positive pair embeddings and how well the embeddings are uniformly distributed . The quantitative analysis is illustrated in Section 4.5 . Besides , the decision boundary leaves an extra margin m to boundary Î¸i , iâˆ— = Î¸i , j which is often used during inference , making it more tolerant to noise and more robust . All these properties make ArcCon loss more discriminative than traditional training objectives like NT - Xent . Compared with Arcface ( Deng et al . , 2019 ) which is often used in largescale fine - grained categorization in computer vision community , ArcCon loss does not need clasÌ¸ After getting the positive and negative sentence pairs , we put them into a training objective for model fine - tune . The most widely adopted training objective is NT - Xent loss ( Chen et al . , 2020 ; Gao et al . , 2021 ) , which has been used in previous sentence and image representation learning methods and can be formulated as follows : esim(hi , hâˆ— j=1 esim(hi , hj ) /Ï„ i ) /Ï„ LNT - Xent = âˆ’log ( 1 ) ( cid:80)n where sim ( hi , hj ) is the cosine similarity hT i hj , Ï„ is a temperature hyperparameter and hi hj ||âˆ—|| || n is the number of sentences within a batch . || Though the training objective tries to pull representations with similar semantics closer and push dissimilar ones away from each other , these representations may still not be sufficiently discriminative and not very robust to noise . Let us denote angular Î¸i , j as follows : ( cid:18 ) ( cid:19 ) hT i hj ||hi|| âˆ— ||hj|| Î¸i , j = arccos ( 2 ) As sâ€²i is more similar to si in semantics than sâ€²â€²i is , we could model such relation with a triplet objective : Ltri = max ( cid:0)0 , sim(Â¯hi , Â¯hâ€²â€²i ) âˆ’ sim(Â¯hi , Â¯hâ€²i ) + m(cid:1 ) ( 4 ) in which Â¯hi is the sentence representation of si generated without dropout noise and sim(i , j ) is the cosine similarity between i and j. As the semantic difference between sâ€²i and sâ€²â€²i may be subtle depending on the original sentence si and the masked words , here we set m to zero . sification labels , and could handle contrastive task properly . 3.2 Modeling Entailment Relation of Triplet Sentences Previously the training objectives for sentence representation learning like NT - Xent loss only considered pairwise sentence relations , in which sentences are either similar or dissimilar in semantics . But in fact , there are varying degrees of semantic similarity . For example , sentence s2 could be more similar to sentence s1 than sentence s3 to s1 . Existing methods lack the ability to model such partial order of semantics between sentences . Combine formula ( 3 ) and formula ( 4 ) , the final form of our training objective is : In order to distinguish the slight differences in semantics between different sentences , we propose a new self - supervised task which models the entailment relation of automatically generated triplet sentences . For each sentence si in the text dataset D , we first generate an external sentence sâ€²i by masking contiguous segments of si with a masking rate of 20 % . Then we enlarge the masking area and get a new sentence sâ€²â€²i with a masking rate of 40 % to si . The masking rates are set up experimentally , and an ablation study about the effect of masking rates is illustrated in Section 4.4 . An example of the masking procedure is shown as follows : L = Larc + Î»Ltri ( 5 ) in which Î» is a coefficient . 4 Experiments 4.1 Setups Evaluation Tasks We evaluate our method on two kinds of sentence related tasks : â€¢ Unsupervised Semantic Textual Similarity ( STS ): These tasks measure the model â€™s ability to estimate the semantic similarities between sentences . si Al Jaber â€™s first long distance travel was of 800 km which he covered by circling Qatar . â€¢ SentEval Transfer Tasks : These tasks measure the effectiveness of sentence embeddings used in downstream transfer tasks . sâ€²i Al Jaber â€™s first long distance travel was of 800 km which he covered by circling Qatar . Baselines We compare ArcCSE to several representative methods on STS and SentEval tasks , such as average GloVe embeddings ( Pennington et al . , 2014 ) , Skip - thought ( Kiros et al . , 2015 ) , average BERT embeddings from the last layer ( Devlin et al . , 2019 ) , BERT - Flow ( Li et al . , 2020 ) , and BERT - Whitening ( Su et al . , 2021 ) . We also include the recently proposed contrastive learning methods , such as ISBERT ( Zhang et al . , 2020 ) , CT - BERT ( Carlsson et al . , 2021 ) , ConSERT ( Yan et al . , 2021 ) , and the current state - of - the - art method SimCSE ( Gao et al . , 2021 ) . sâ€²â€²i Al Jaber â€™s first long distance travel was of 800 km which he covered by circling Qatar . We can constitute a triplet ( si , sâ€²i , sâ€²â€²i ) with entailment relation among them . Though in rare cases , the strategy may generate sentences that do not exhibit the desired relationship and introduce some noise , the entailment relation holds true most of the time . We expect encountering enough data will reinforce the correct ones whereas the impact of incorrect ones will diminish . Implementation Details We train ArcCSE with the pre - trained checkpoints of BERTbase and BERTlarge ( Devlin et al . , 2019 ) . We also employ our method to SBERT ( Reimers and Gurevych , 2019 ) , which has been trained on NLI datasets , to verify the generalizability of our method . Since the si , sâ€²i and sâ€²â€²i are similar literally and semantically , generating their representations with dropout noise may obscure their entailment relation and add inaccurate signals to the representation learning process . So we turn off the dropout of the encoder when modeling the triplet relation . Following SimCSE ( Gao et al . , 2021 ) , we use the output of the MLP layer on top of the [ CLS ] as Method STS12 STS13 STS14 STS15 STS16 STS - B SICK - R Avg . GloVe ( avg . ) BERTbase ( last avg . ) BERTbase - flow BERTbase - whitening IS - BERTbase CT - BERTbase ConSERTbase SimCSE - BERTbase ArcCSE - BERTbase 55.14 30.87 58.40 57.83 56.77 61.63 64.64 68.40 72.08 69.94 69.66 70.66 59.89 67.10 66.90 69.24 76.80 78.49 82.41 84.27 82.34 81.92 59.73 47.73 60.85 60.90 61.21 68.47 69.07 74.38 76.25 75.08 75.33 68.25 60.29 75.16 75.08 75.23 77.50 79.72 80.91 82.32 83.08 82.79 63.66 63.73 71.22 71.31 70.16 76.48 75.95 78.56 79.54 78.97 79.55 58.02 47.29 68.66 68.24 69.21 74.31 73.97 76.85 79.92 78.59 79.56 53.76 58.22 64.47 63.73 64.25 69.19 67.31 72.23 72.39 71.13 71.94 61.32 52.57 66.55 66.28 66.58 72.05 72.74 76.25 78.11 77.02 77.25 w/o ArcCon loss w/o Triplet loss ConSERTlarge SimCSE - BERTlarge ArcCSE - BERTlarge 70.69 70.88 73.17 82.96 84.16 86.19 74.13 76.43 77.90 82.78 84.50 84.97 76.66 79.76 79.43 77.53 79.26 80.45 70.37 73.88 73.50 76.45 78.41 79.37 SBERTbase SimCSE - SBERTbase ArcCSE - SBERTbase 70.97 69.41 74.29 76.53 80.76 82.95 73.19 74.37 76.63 79.09 82.61 83.90 74.30 77.64 79.08 77.03 79.92 80.95 72.91 76.62 75.64 74.89 77.33 79.06 SBERTlarge SimCSE - SBERTlarge ArcCSE - SBERTlarge 72.27 76.16 76.36 78.46 83.77 85.72 74.90 77.27 78.22 80.99 84.33 85.20 76.25 79.73 80.04 79.23 81.67 82.25 73.75 77.25 77.01 76.55 80.03 80.69 Table 1 : Sentence representation performance on the STS tasks . We employ our method to BERT and SBERT in both base and large versions and report Spearman â€™s correlation . the sentence representation during training , and use the [ CLS ] output without MLP layer for evaluation . The dropout rate is set to 0.1 . For ArcCon loss , we set the angular margin m to 10 degrees and the temperature Ï„ to 0.05 . When modeling the entailment relation of triplet sentences , we set the masking ratios as 20 % and 40 % respectively . Since the semantic difference between triplet sentences is more obvious for long sentences , we filter out sentences with less than 25 words and use the left ones for the triplet loss . The loss coefficient Î» is set to 0.1 experimentally . models and 1e-5 for large models . We search the batch size within { 8 , 16 , 32 } and always update the parameters every 64 steps . The model is optimized by the AdamW with Sharpness - Aware Minimization ( Foret et al . , 2021 ) and default configurations . We evaluate our model every 125 training steps on the development set of STS - B , and the best checkpoint is used for the final evaluation on test sets . Our implementation is based on HuggingFace â€™s Transformers ( Wolf et al . , 2020 ) . 4.2 Unsupervised STS Tasks We use one million random sampled sentences from English Wikipedia for training , which has been used in previous work ( Gao et al . , 2021 ) 1 . During training , the sentences are sampled by length . We set different maximum sentence lengths for ArcCon loss and triplet loss to save memory . The length is set to 32 for the ArcCon loss in large models , and to the maximum length within a batch for all other cases . We train our model for one epoch and the learning rate is set to 3e-5 for base We conduct experiments on 7 semantic textual similarity ( STS ) tasks , including STS tasks 20122016 ( Agirre et al . , 2012 , 2013 , 2014 , 2015 , 2016 ) , STS Benchmark ( Cer et al . , 2017 ) , and SICKRelatedness ( Marelli et al . , 2014 ) . Within these datasets , each sample contains two sentences and a gold score between 0 and 5 which indicates their semantic similarity . We use SentEval toolkit ( Conneau and Kiela , 2018 ) for evaluation and report the Spearman â€™s correlation following previous works ( Reimers and Gurevych , 2019 ; Gao et al . , 2021 ) . 1https://huggingface.co/datasets/princeton-nlp/datasetsThe evaluation results are shown in Table 1 , for - simcse / resolve / main / wiki1m_for_simcse.txt Method MR CR SUBJ MPQA SST TREC MRPC Avg . GloVe ( avg . ) Skip - thought BERTbase ( last avg . ) IS - BERTbase SimCSE - BERTbase ArcCSE - BERTbase 77.25 76.50 78.66 81.09 81.18 79.91 78.30 80.10 86.25 87.18 86.46 85.25 91.17 93.60 94.37 94.96 94.45 99.58 87.85 87.10 88.66 88.75 88.88 89.21 80.18 82.00 84.40 85.96 85.50 84.90 83.00 92.20 92.80 88.64 89.80 89.20 72.87 73.00 69.54 74.24 74.43 74.78 81.52 83.50 84.94 85.83 85.81 86.12 BERTlarge ( last avg . ) SimCSE - BERTlarge ArcCSE - BERTlarge 84.30 85.36 84.34 89.22 89.38 88.82 95.60 95.39 99.58 86.93 89.63 89.79 89.29 90.44 90.50 91.40 91.80 92.00 71.65 76.41 74.78 86.91 88.34 88.54 Table 2 : Sentence representation performance on SentEval transfer tasks . We report the accuracy results of both BERTbase and BERTlarge level models . from which we can see that ArcCSE outperforms the previous approaches . Compared with the previous state - of - the - art method SimCSE , ArcCSEBERTbase raises the average Spearman â€™s correlation from 76.25 % to 78.11 % , and ArcCSE - BERTlarge further pushes the results to 79.37 % . The performance is even better than strong supervised method SBERT , which has already been trained on NLI datasets . Furthermore , we can also employ our method to SBERT and improve its performance to 79.06 % and 80.69 % for the base and large models respectively , which is more effective than SimCSE . We also explore the improvements made by the ArcCon loss and triplet loss independently based on BERTbase . From Table 1 we can see that with ArcCon loss alone , the average Spearman â€™s correlation is 77.25 % . When combining the traditional NT - Xent loss with our proposed triplet loss , the average Spearman â€™s correlation is 77.02 % . Both of them outperform the previous state - of - the - art method SimCSE , whose average Spearman â€™s correlation is 76.25 % . This demonstrates the effectiveness of ArcCon and triplet loss we proposed . The results are shown in Table 2 . We can see that ArcCSE performs on par or better than baseline methods in both BERTbase and BERTlarge level . This demonstrates the effectiveness of our method in learning domain - specific sentence embeddings . 4.4 Ablation Studies Effect of Angular Margin The angular margin m in ArcCon loss affects the discriminative power directly . To investigate the effect of m , we conduct an experiment by varying m from 0 degrees to 20 degrees , increased by 2 degrees at each step . We tune the hyper - parameter based on Spearman â€™s correlation on the development set of STS - B following previous works ( Kim et al . , 2021 ; Gao et al . , 2021 ) . The results are shown in Figure 4 . Figure 4 : Effect of the angular margin m in ArcCon loss . Results are reported on the development set of STS - B based on the Spearman â€™s correlation . 4.3 SentEval Tasks We evaluate our model with SentEval toolkit on several supervised transfer tasks , including : MR ( Pang and Lee , 2005 ) , CR ( Hu and Liu , 2004 ) , SUBJ ( Pang and Lee , 2004 ) , MPQA ( Wiebe et al . , 2005 ) , SST-2 ( Socher et al . , 2013 ) , TREC ( Voorhees and Tice , 2000 ) and MRPC ( Dolan and Brockett , 2005 ) . For each task , SentEval trains a logistic regression classifier on top of the sentence embeddings and tests the performance on the downstream task . For a fair comparison , we do not include models with auxiliary tasks like masked language modeling . We can see that the best performance is achieved when m = 10 , either larger or smaller margin degrade the performance . This matches our intuition since small m may have little effect , and large m may negatively influence the positive pair relation modeling . Effect of Temperature The temperature Ï„ in ArcCon Loss affects its effectiveness , so we carry out an experiment with Ï„ varying from 0.01 to 0.1 , increased by 0.01 at each step . The results are shown in Figure 5 . We can see that the model ArcCSE - BERTbase STS12 STS13 STS14 STS15 STS16 STS - B SICK - R Avg . 72.08 70.51 69.62 84.27 83.59 83.13 76.25 75.85 74.42 82.32 82.30 82.15 79.54 78.87 78.39 79.92 78.74 78.39 72.39 71.58 70.89 78.11 77.35 76.71 w/ Dropouton / off w/ Dropoutmix / off w/ Dropouton / on Table 3 : Effect of on - off Switching of Dropout . We use different dropout settings to generate sentence embeddings used for ArcCon loss and triplet loss respectively . The " on " , " off " and " mix " mean turn dropout on , turn dropout off and use different settings for two passes separately . performs best when Ï„ = 0.05 , so we use this value throughout our experiments . makes the triplet loss less helpful . The best performance is achieved when r1 is 20 % and r2 is 40 % , and the corresponding Spearman â€™s correlation is 0.847 . We use them as our hyper - parameters . Effect of on - off Switching of Dropout The onoff switching of dropout in the BERT - like sentence encoder affects the generated sentence representations directly . Since dropout performs a kind of averaging over the ensemble of possible subnetworks , an embedding generated with dropout turned off can be seen as a kind of " averaging " representation , while an embedding generated with dropout turned on can be seen as generated through a subnetwork . In ArcCSE , we use the embeddings generated with the encoder dropout turned on as input for ArcCon loss , which regularizes the network by making representations generated through different subnetworks similar . When modeling the entailment relation , we generate " averaging " representations with dropout turn - off to avoid inaccurate signals . In order to verify our intuition , we conduct two experiments with different dropout settings . In the first experiment , we feed ArcCon two sentence representations generated with dropout turns on and off respectively . We carry out this experiment with angular margins ranging between 2 degrees to 12 degrees and report the best result . In the second one , we feed the triplet loss representations that are generated with dropout turns on and maintain the other settings . The results are shown in Table 3 . We can see that the original settings that turn dropout on for ArcCon and turn dropout off for triplet loss achieve the best performance , which confirms our intuition . Figure 5 : Effect of the temperature Ï„ in ArcCon loss . Results are reported on the development set of STS - B based on the Spearman â€™s correlation . Effect of Masking Ratios The masking ratios determine the sentences generated for the entailment relation modeling and their differences in semantics , so we conduct an experiment to explore the effect of different masking ratios . The first masking ratio r1 is varied from 10 % to 25 % , increased by 5 % for each step . The second masking ratio r2 is derived by adding an extra value rd to r1 . rd is varied from 10 % to 35 % , increased by 5 % for each step . The results are shown in Figure 6 . Figure 6 : Effect of the masking ratios . Different lines correspond to different values of r1 . The abscissa is rd , representing the difference between r1 and r2 . Results are reported on the development set of STS - B based on the Spearman â€™s correlation . Effect of Coefficient in the Training Objective The coefficient Î» in the final optimization objective adjusts the relative weights between ArcCon and the triplet loss , as shown in formula ( 5 ) . To find the most suitable Î» , we conduct an experiment by varying Î» from 0 to 1.2 and increased by 0.1 at each step . The results are shown in Figure 7 . We can see that large differences between the two masking ratios tend to lead lower Spearman â€™s correlation compared to the smaller ones . The reason may be that the larger the semantic difference is , the easier for the model to estimate the entailment relations among the triplet sentences , which We can see that the best performance is achieved 