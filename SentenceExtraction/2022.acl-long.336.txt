
Yuhao Zhang, Hongji Zhu, Yongliang Wang, Nan Xu, Xiaobo Li, BinQiang ZhaoAlibaba GroupInstitute of Automation, Chinese Academy of Sciences
{zyh277500,zhj283587,wangyongliang.wyl,xiaobo.lixb,binqiang.zhao}@alibaba-inc.com
xunan2015@ia.ac.cn
Abstract
Learning high-quality sentence representations
is a fundamental problem of natural language
processing which could benefit a wide range of
downstream tasks. Though the BERT-like pre-
trained language models have achieved great
success, using their sentence representations
directly often results in poor performance on
the semantic textual similarity task. Recently,
several contrastive learning methods have been
proposed for learning sentence representations
and have shown promising results. However,
most of them focus on the constitution of posi-
tive and negative representation pairs and pay
little attention to the training objective like
NT-Xent, which is not sufficient enough to ac-
quire the discriminating power and is unable to
model the partial order of semantics between
sentences. So in this paper, we propose a new
method ArcCSE, with training objectives de-
signed to enhance the pairwise discriminative
power and model the entailment relation of
triplet sentences. We conduct extensive exper-
iments which demonstrate that our approach
outperforms the previous state-of-the-art on di-
verse sentence related tasks, including STS and
SentEval.
1 Introduction
Learning sentence representations, which encodes
sentences into fixed-sized dense vectors such that
semantically similar ones stay close, is a fundamen-
tal problem of natural language processing. It could
benefit a wide range of downstream applications
such as information retrieval, semantic similarity
comparison, question answering, and so on.
Recently, with the great success of pre-trained
Transformer-based language models (Devlin et al.,
2019; Liu et al., 2019; Raffel et al., 2020; Brown
et al., 2020; Liu et al., 2019) like BERT, they have
been widely adopted for generating sentence rep-
resentations. A straightforward way is by lever-
aging the [CLS] embedding (Devlin et al., 2019)Figure 1: Sentence representations visualization. We
generate the representations of three related sentences
by passing them to BERT, SimCSE-BERTand
ArcCSE-BERTmultiple times. With different
dropout masks, we can generate different representa-
tions for each sentence. Then we normalize the embed-
dings and use t-SNE for dimensionality reduction.
or applying mean pooling on the last layers of a
BERT-like pre-trained language model (Reimers
and Gurevych, 2019). However, the sentence
embeddings coming from a pre-trained language
model without further fine-tuning could not cap-
ture the semantic meaning of sentences very well
as shown in Figure 1(a), and sometimes even un-
derperform non-contextualized embeddings like
GloVe (Pennington et al., 2014).
To make pre-trained language models more suit-
able for generating sentence embeddings, super-
vised methods like SBERT (Reimers and Gurevych,
2019) are proposed, which improve the perfor-
mance by fine-tuning on a labeled dataset. As
labeled data is not available or expensive to an-
notate in many tasks or domains, it is of great valuefor developing unsupervised/self-supervised ap-
proaches for learning sentence representations. So
recent works like BERT-Flow (Li et al., 2020) and
BERT-Whitening (Su et al., 2021) propose post-
processing methods to improve the BERT-based
sentence representation. They address that the non-
smooth anisotropic semantic space of BERT is a
bottleneck and alleviate the problem through nor-
malizing flows and whitening operation. To further
improve the quality of sentence representations,
several works (Kim et al., 2021; Yan et al., 2021;
Giorgi et al., 2021; Carlsson et al., 2021; Gao et al.,
2021) adopt self-supervised contrastive learning
approach, which learns sentence representations by
minimizing the distance of positive sentence rep-
resentation pairs and maximizing the distance of
negative pairs. In these works, positive pairs are
often constituted through data augmentation or en-
coders with different structure or parameters, while
negative pairs are derived from different sentences
within the same batch. Then contrastive learning
objective like normalized temperature-scaled cross-
entropy loss (NT-Xent) (Chen et al., 2020; Gao
et al., 2021) is used for optimizing. A typical ex-
ample unsup-SimCSE (Gao et al., 2021) achieves
state-of-the-art performance with a simple and ef-
fective idea of using standard dropout for data aug-
mentation.
Though existing contrastive methods for learn-
ing sentence representation have shown promising
results, most of them focus on the positive and
negative pairs constitution, and the optimization
objective itself is not fully exploited. The con-
trastive learning objective NT-Xent loss used in
recent works (Yan et al., 2021; Giorgi et al., 2021;
Gao et al., 2021) is a variation of cross-entropy loss
with softmax function. Recent studies (Wang et al.,
2018; Deng et al., 2019) have shown that the tradi-
tional softmax-based loss is insufficient to acquire
the discriminating power, as shown in Figure 1(b)
in which SimCSE-BERTadopts the NT-Xent
loss and could not separate sandscompletely. In
addition, the current optimization objectives only
models sentence relations in a pairwise perspective,
which tries to pull sentences with similar semantics
closer and push dissimilar ones away from each
other. However, there are different degrees of se-
mantic similarity among related sentences. For
example in Figure 1(d), sis more similar to s
thansis. The current optimization objectives lack
the ability to model the partial order of semanticsbetween sentences.
To alleviate these problems, in this paper, we
propose a new approach ArcCSE for sentence rep-
resentation learning. For pairwise sentence relation
modeling, we propose Additive Angular Margin
Contrastive Loss (ArcCon Loss), which enhances
the pairwise discriminative power by maximizing
the decision margin in the angular space. Besides,
in order to model the partial order of semantics be-
tween sentences, we propose a new self-supervised
task that captures the entailment relation among
triplet sentences. The task is implemented through
automatically constituted triplet sentences with en-
tailment relation among them. A visualization ex-
ample of the generated representations through
ArcCSE is shown in Figure 1(c). We evaluate
our method on standard semantic textual similarity
(STS) tasks and SentEval transfer tasks, and it out-
performs the previous state-of-the-art approaches.
2 Related Work
2.1 Unsupervised Sentence Representation
Learning
Early works usually learn sentence representations
by augmenting the idea of word2vec (Mikolov
et al., 2013), such as predicting surrounding sen-
tences (Kiros et al., 2015; Hill et al., 2016; Lo-
geswaran and Lee, 2018) or summing up n-gram
embeddings (Pagliardini et al., 2018). With the
rise of pre-trained language models, many works
try to generate sentence representations through
BERT-like models. A common way is leveraging
the [CLS] embedding or applying mean pooling on
the last layers of BERT (Reimers and Gurevych,
2019; Li et al., 2020). Instead of using BERT em-
beddings directly, BERT-Flow (Li et al., 2020) and
BERT-Whitening (Su et al., 2021) further improve
sentence representation through post-processing.
Recently, several works adopt the contrastive
learning framework for sentence representation
learning. They propose different strategies to con-
stitute contrastive pairs, either through different
data transforming methods (Zhang et al., 2020;
Yan et al., 2021; Giorgi et al., 2021), or through
encoders with different structures or parameters
(Carlsson et al., 2021; Kim et al., 2021; Gao et al.,
2021). A typical example SimCSE (Gao et al.,
2021) uses dropout as data augmentation strategy
and achieves state-of-the-art performance. How-
ever, most existing works pay little attention to the
training objective and use the traditional contrastive
loss directly, which is insufficient in discrimination
and unable to model the partial order of semantics
between sentences. So, in our work, we propose a
new approach that jointly models the pairwise and
triple-wise sentence relations and further improves
the sentence representations’ quality.
2.2 Deep Metric Learning Objectives
The goal of Deep Metric Learning (DML) is to
learn a function that maps objects into an embed-
ded space, in which similar objects stay close and
dissimilar ones are far away. In order to achieve
this goal, many approaches have been proposed,
and designing appropriate loss functions plays a
key role in it. Contrastive training objectives like
Contrastive Loss (Chopra et al., 2005), N-Pair Loss
(Sohn, 2016), Structured Loss (Song et al., 2016)
and Triplet Margin Loss (Ma et al., 2021) apply
the definition of metric learning directly. These ob-
jectives are among the earliest training objectives
used for deep metric learning. Later, softmax-based
losses which learn a center for each class and penal-
ize the distances between deep features and their
corresponding class centers achieve more promis-ing results in supervised metric learning. Typi-
cal examples like Center Loss (Wen et al., 2016),
SphereFace (Liu et al., 2017), CosFace (Wang et al.,
2018) and ArcFace (Deng et al., 2019) are widely
adopted in deep learning applications such as face
recognition and sentence classification (Coria et al.,
2020). However, these losses need class labels and
are not suitable for learning sentence representa-
tions. So inspired by ArcFace, we propose a new
training objective ArcCon that does not need class
labels and can model pairwise sentence relations
with more discriminative power than traditional
contrastive training objectives.
3 Method
In this section, we present ArcCSE, an angular
based contrastive sentence representation learning
framework, which could generate superior sentence
embeddings from unlabeled data. Given a pre-
trained language model Mand an unlabeled text
dataset D, the task is fine-tuning MonDso that
the sentence representations generated through M
could be more semantic discriminative.
Our framework consists of two components thatmodel pairwise and triple-wise sentence relations
simultaneously, as shown in Figure 2. We start with
angular margin based contrastive learning in Sec-
tion 3.1, which models pairwise relations between
sentences by pulling semantic similar ones closer
while pushing dissimilar ones away. Then we intro-
duce the method which models the partial order of
semantics between automatically constituted triplet
sentences in Section 3.2.
3.1 Angular Margin based Contrastive
Learning
To model the positive/negative pairwise relations
between sentences, we first need to generate sen-
tence representations and group them into positive
and negative pairs. Then we feed these pairs to a
training objective for optimizing.
Given a collection of sentences D={s},
we generate the sentence representations through a
BERT-like pre-trained language model M. Follow-
ing SimCSE, we use dropout as the data augmen-
tation method. For each sentence s, we generate
two different representations handhfromsby
passing stoMtwice with independently sampled
dropout masks. These two representations with the
same semantics constitute a positive pair, while the
negative pairs are derived from the representations
of different sentences within the same batch.
After getting the positive and negative sentence
pairs, we put them into a training objective for
model fine-tune. The most widely adopted training
objective is NT-Xent loss (Chen et al., 2020; Gao
et al., 2021), which has been used in previous sen-
tence and image representation learning methods
and can be formulated as follows:
L =−loge()
Pe(1)
where sim(h, h)is the cosine similarity,τis a temperature hyperparameter and
nis the number of sentences within a batch.
Though the training objective tries to pull repre-
sentations with similar semantics closer and push
dissimilar ones away from each other, these rep-
resentations may still not be sufficiently discrimi-
native and not very robust to noise. Let us denote
angular θas follows:
θ= arccoshh
||h|| ∗ ||h||
(2)
The decision boundary for hin NT-Xent is θ=
θ, as show in Figure 3. Due to lack of decision
margin, a small perturbation around the decision
boundary may lead to an incorrect decision.
To overcome the problem, we propose a new
training objective for sentence representation learn-
ing by adding an additive angular margin mbe-
tween positive pair handh. We named it Ad-
ditive Angular Margin Contrastive Loss (ArcCon
Loss), which can be formulated as follows:
L=−loge()
e()+Pe
(3)
In this loss, the decision boundary for his
θ+m=θ, as show in Figure 3. Compared
with NT-Xent, it further pushed htowards to the
area where θget smaller and θget larger, by
increasing the compactness of sentence represen-
tations with the same semantics and enlarging the
discrepancy of different semantic representations.
This help enhance the alignment and uniformity
properties (Wang and Isola, 2020), which are two
key measures of representation quality related to
contrastive learning, indicating how close between
positive pair embeddings and how well the embed-
dings are uniformly distributed. The quantitative
analysis is illustrated in Section 4.5. Besides, the
decision boundary leaves an extra margin mto
boundary θ=θwhich is often used during
inference, making it more tolerant to noise and
more robust. All these properties make ArcCon
loss more discriminative than traditional training
objectives like NT-Xent. Compared with Arcface
(Deng et al., 2019) which is often used in large-
scale fine-grained categorization in computer vi-
sion community, ArcCon loss does not need clas-sification labels, and could handle contrastive task
properly.
3.2 Modeling Entailment Relation of Triplet
Sentences
Previously the training objectives for sentence rep-
resentation learning like NT-Xent loss only con-
sidered pairwise sentence relations, in which sen-
tences are either similar or dissimilar in semantics.
But in fact, there are varying degrees of semantic
similarity. For example, sentence scould be more
similar to sentence sthan sentence stos. Exist-
ing methods lack the ability to model such partial
order of semantics between sentences.
In order to distinguish the slight differences in
semantics between different sentences, we propose
a new self-supervised task which models the en-
tailment relation of automatically generated triplet
sentences. For each sentence sin the text dataset
D, we first generate an external sentence sby
masking contiguous segments of swith a masking
rate of 20%. Then we enlarge the masking area and
get a new sentence swith a masking rate of 40%
tos. The masking rates are set up experimentally,
and an ablation study about the effect of masking
rates is illustrated in Section 4.4. An example of
the masking procedure is shown as follows:
sAl Jaber’s first long distance travel
was of 800km which he covered by
circling Qatar.
sAl Jaber’s first long distance travel
wasof800km which he covered by
circling Qatar.
sAl Jaber’s first long distance travel
was of800km which hecovered by
circling Qatar.
We can constitute a triplet (s, s, s)with entail-
ment relation among them. Though in rare cases,
the strategy may generate sentences that do not ex-
hibit the desired relationship and introduce some
noise, the entailment relation holds true most of
the time. We expect encountering enough data will
reinforce the correct ones whereas the impact of
incorrect ones will diminish.
Since the s,sandsare similar literally and
semantically, generating their representations with
dropout noise may obscure their entailment relation
and add inaccurate signals to the representation
learning process. So we turn off the dropout of the
encoder when modeling the triplet relation.Assis more similar to sin semantics than
sis, we could model such relation with a triplet
objective:
L=max 
0, sim (¯h,¯h)−sim(¯h,¯h) +m
(4)
in which ¯his the sentence representation of sgen-
erated without dropout noise and sim(i, j)is the
cosine similarity between iandj. As the semantic
difference between sandsmay be subtle de-
pending on the original sentence sand the masked
words, here we set mto zero.
Combine formula (3) and formula (4), the final
form of our training objective is:
L=L+λL (5)
in which λis a coefficient.
4 Experiments
4.1 Setups
Evaluation Tasks We evaluate our method on two
kinds of sentence related tasks:
•Unsupervised Semantic Textual Similarity
(STS): These tasks measure the model’s abil-
ity to estimate the semantic similarities be-
tween sentences.
•SentEval Transfer Tasks: These tasks measure
the effectiveness of sentence embeddings used
in downstream transfer tasks.
Baselines We compare ArcCSE to several rep-
resentative methods on STS and SentEval tasks,
such as average GloVe embeddings (Pennington
et al., 2014), Skip-thought (Kiros et al., 2015), av-
erage BERT embeddings from the last layer (De-
vlin et al., 2019), BERT-Flow (Li et al., 2020), and
BERT-Whitening (Su et al., 2021). We also include
the recently proposed contrastive learning methods,
such as ISBERT (Zhang et al., 2020), CT-BERT
(Carlsson et al., 2021), ConSERT (Yan et al., 2021),
and the current state-of-the-art method SimCSE
(Gao et al., 2021).
Implementation Details We train ArcCSE
with the pre-trained checkpoints of BERTand
BERT(Devlin et al., 2019). We also employ
our method to SBERT (Reimers and Gurevych,
2019), which has been trained on NLI datasets, to
verify the generalizability of our method.
Following SimCSE (Gao et al., 2021), we use
the output of the MLP layer on top of the [CLS] asMethod STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.
GloVe (avg.) 55.14 70.66 59.73 68.25 63.66 58.02 53.76 61.32
BERT(last avg.) 30.87 59.89 47.73 60.29 63.73 47.29 58.22 52.57
BERT-flow 58.40 67.10 60.85 75.16 71.22 68.66 64.47 66.55
BERT-whitening 57.83 66.90 60.90 75.08 71.31 68.24 63.73 66.28
IS-BERT 56.77 69.24 61.21 75.23 70.16 69.21 64.25 66.58
CT-BERT 61.63 76.80 68.47 77.50 76.48 74.31 69.19 72.05
ConSERT 64.64 78.49 69.07 79.72 75.95 73.97 67.31 72.74
SimCSE-BERT 68.40 82.41 74.38 80.91 78.56 76.85 72.23 76.25
ArcCSE-BERT 72.08 84.27 76.25 82.32 79.54 79.92 72.39 78.11
w/o ArcCon loss 69.94 82.34 75.08 83.08 78.97 78.59 71.13 77.02
w/o Triplet loss 69.66 81.92 75.33 82.79 79.55 79.56 71.94 77.25
ConSERT 70.69 82.96 74.13 82.78 76.66 77.53 70.37 76.45
SimCSE-BERT 70.88 84.16 76.43 84.50 79.76 79.26 73.88 78.41
ArcCSE-BERT 73.17 86.19 77.90 84.97 79.43 80.45 73.50 79.37
SBERT 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89
SimCSE-SBERT 69.41 80.76 74.37 82.61 77.64 79.92 76.62 77.33
ArcCSE-SBERT 74.29 82.95 76.63 83.90 79.08 80.95 75.64 79.06
SBERT 72.27 78.46 74.90 80.99 76.25 79.23 73.75 76.55
SimCSE-SBERT 76.16 83.77 77.27 84.33 79.73 81.67 77.25 80.03
ArcCSE-SBERT 76.36 85.72 78.22 85.20 80.04 82.25 77.01 80.69
the sentence representation during training, and use
the [CLS] output without MLP layer for evaluation.
The dropout rate is set to 0.1. For ArcCon loss,
we set the angular margin mto 10 degrees and
the temperature τto 0.05. When modeling the
entailment relation of triplet sentences, we set the
masking ratios as 20% and 40% respectively. Since
the semantic difference between triplet sentences
is more obvious for long sentences, we filter out
sentences with less than 25 words and use the left
ones for the triplet loss. The loss coefficient λis
set to 0.1 experimentally.
We use one million random sampled sentences
from English Wikipedia for training, which has
been used in previous work (Gao et al., 2021). During training, the sentences are sampled by
length. We set different maximum sentence lengths
for ArcCon loss and triplet loss to save memory.
The length is set to 32 for the ArcCon loss in large
models, and to the maximum length within a batch
for all other cases. We train our model for one
epoch and the learning rate is set to 3e-5 for basemodels and 1e-5 for large models. We search the
batch size within {8, 16, 32} and always update the
parameters every 64 steps. The model is optimized
by the AdamW with Sharpness-Aware Minimiza-
tion (Foret et al., 2021) and default configurations.
We evaluate our model every 125 training steps
on the development set of STS-B, and the best
checkpoint is used for the final evaluation on test
sets. Our implementation is based on Hugging-
Face’s Transformers (Wolf et al., 2020).
4.2 Unsupervised STS Tasks
We conduct experiments on 7 semantic textual
similarity (STS) tasks, including STS tasks 2012-
2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016),
STS Benchmark (Cer et al., 2017), and SICK-
Relatedness (Marelli et al., 2014). Within these
datasets, each sample contains two sentences and a
gold score between 0 and 5 which indicates their
semantic similarity. We use SentEval toolkit (Con-
neau and Kiela, 2018) for evaluation and report the
Spearman’s correlation following previous works
(Reimers and Gurevych, 2019; Gao et al., 2021).
The evaluation results are shown in Table 1,Method MR CR SUBJ MPQA SST TREC MRPC Avg.
GloVe (avg.) 77.25 78.30 91.17 87.85 80.18 83.00 72.87 81.52
Skip-thought 76.50 80.10 93.60 87.10 82.00 92.20 73.00 83.50
BERT(last avg.) 78.66 86.25 94.37 88.66 84.40 92.80 69.54 84.94
IS-BERT 81.09 87.18 94.96 88.75 85.96 88.64 74.24 85.83
SimCSE-BERT 81.18 86.46 94.45 88.88 85.50 89.80 74.43 85.81
ArcCSE-BERT 79.91 85.25 99.58 89.21 84.90 89.20 74.78 86.12
BERT(last avg.) 84.30 89.22 95.60 86.93 89.29 91.40 71.65 86.91
SimCSE-BERT 85.36 89.38 95.39 89.63 90.44 91.80 76.41 88.34
ArcCSE-BERT 84.34 88.82 99.58 89.79 90.50 92.00 74.78 88.54
from which we can see that ArcCSE outperforms
the previous approaches. Compared with the pre-
vious state-of-the-art method SimCSE, ArcCSE-
BERTraises the average Spearman’s correlation
from 76.25% to 78.11%, and ArcCSE-BERT
further pushes the results to 79.37%. The perfor-
mance is even better than strong supervised method
SBERT, which has already been trained on NLI
datasets. Furthermore, we can also employ our
method to SBERT and improve its performance to
79.06% and 80.69% for the base and large models
respectively, which is more effective than SimCSE.
We also explore the improvements made by the
ArcCon loss and triplet loss independently based
on BERT. From Table 1 we can see that with
ArcCon loss alone, the average Spearman’s corre-
lation is 77.25%. When combining the traditional
NT-Xent loss with our proposed triplet loss, the
average Spearman’s correlation is 77.02%. Both
of them outperform the previous state-of-the-art
method SimCSE, whose average Spearman’s corre-
lation is 76.25%. This demonstrates the effective-
ness of ArcCon and triplet loss we proposed.
4.3 SentEval Tasks
We evaluate our model with SentEval toolkit on sev-
eral supervised transfer tasks, including: MR (Pang
and Lee, 2005), CR (Hu and Liu, 2004), SUBJ
(Pang and Lee, 2004), MPQA (Wiebe et al., 2005),
SST-2 (Socher et al., 2013), TREC (V oorhees and
Tice, 2000) and MRPC (Dolan and Brockett, 2005).
For each task, SentEval trains a logistic regression
classifier on top of the sentence embeddings and
tests the performance on the downstream task. For
a fair comparison, we do not include models with
auxiliary tasks like masked language modeling.The results are shown in Table 2. We can see
that ArcCSE performs on par or better than base-
line methods in both BERTand BERTlevel.
This demonstrates the effectiveness of our method
in learning domain-specific sentence embeddings.
4.4 Ablation Studies
Effect of Angular Margin The angular margin
min ArcCon loss affects the discriminative power
directly. To investigate the effect of m, we conduct
an experiment by varying mfrom 0 degrees to 20
degrees, increased by 2 degrees at each step. We
tune the hyper-parameter based on Spearman’s cor-
relation on the development set of STS-B following
previous works (Kim et al., 2021; Gao et al., 2021).
The results are shown in Figure 4.
We can see that the best performance is achieved
when m= 10 , either larger or smaller margin de-
grade the performance. This matches our intuition
since small mmay have little effect, and large m
may negatively influence the positive pair relation
modeling.
Effect of Temperature The temperature τin
ArcCon Loss affects its effectiveness, so we carry
out an experiment with τvarying from 0.01 to
0.1, increased by 0.01 at each step. The results
are shown in Figure 5. We can see that the modelArcCSE-BERT STS12 STS13 STS14 STS15 STS16 STS-B SICK-R Avg.
w/ Dropout 72.08 84.27 76.25 82.32 79.54 79.92 72.39 78.11
w/ Dropout 70.51 83.59 75.85 82.30 78.87 78.74 71.58 77.35
w/ Dropout 69.62 83.13 74.42 82.15 78.39 78.39 70.89 76.71
performs best when τ= 0.05, so we use this value
throughout our experiments.
Effect of Masking Ratios The masking ratios
determine the sentences generated for the entail-
ment relation modeling and their differences in
semantics, so we conduct an experiment to explore
the effect of different masking ratios. The first
masking ratio ris varied from 10% to 25%, in-
creased by 5% for each step. The second masking
ratioris derived by adding an extra value rto
r.ris varied from 10% to 35%, increased by 5%
for each step. The results are shown in Figure 6.
We can see that large differences between the
two masking ratios tend to lead lower Spearman’s
correlation compared to the smaller ones. The rea-
son may be that the larger the semantic difference
is, the easier for the model to estimate the entail-
ment relations among the triplet sentences, whichmakes the triplet loss less helpful. The best perfor-
mance is achieved when ris 20% and ris 40%,
and the corresponding Spearman’s correlation is
0.847. We use them as our hyper-parameters.
Effect of on-off Switching of Dropout The on-
off switching of dropout in the BERT-like sentence
encoder affects the generated sentence representa-
tions directly. Since dropout performs a kind of av-
eraging over the ensemble of possible subnetworks,
an embedding generated with dropout turned off
can be seen as a kind of "averaging" representation,
while an embedding generated with dropout turned
on can be seen as generated through a subnetwork.
In ArcCSE, we use the embeddings generated
with the encoder dropout turned on as input for Arc-
Con loss, which regularizes the network by making
representations generated through different subnet-
works similar. When modeling the entailment rela-
tion, we generate "averaging" representations with
dropout turn-off to avoid inaccurate signals. In
order to verify our intuition, we conduct two ex-
periments with different dropout settings. In the
first experiment, we feed ArcCon two sentence rep-
resentations generated with dropout turns on and
off respectively. We carry out this experiment with
angular margins ranging between 2 degrees to 12
degrees and report the best result. In the second
one, we feed the triplet loss representations that
are generated with dropout turns on and maintain
the other settings. The results are shown in Table
3. We can see that the original settings that turn
dropout on for ArcCon and turn dropout off for
triplet loss achieve the best performance, which
confirms our intuition.
Effect of Coefficient in the Training Objective
The coefficient λin the final optimization objective
adjusts the relative weights between ArcCon and
the triplet loss, as shown in formula (5). To find
the most suitable λ, we conduct an experiment by
varying λfrom 0 to 1.2 and increased by 0.1 at
each step. The results are shown in Figure 7.
We can see that the best performance is achieved
when λ= 0.1, and the corresponding Spearman’s
correlation is 0.847. This demonstrates that we can
get the best performance by combining ArcCon
and the triplet loss with proper λ.
4.5 Alignment and Uniformity Analysis
Alignment and uniformity are two properties
closely related to contrastive learning and could
be used to measure the quality of representa-
tions(Wang and Isola, 2020). Alignment favors
encoders that generate similar representations for
similar instances. It could be defined with the ex-
pected distance between embeddings of the positive
paired instances:
ℓ= Ef(x)−f 
x(6)
where pdenotes the distribution of positive
paired instances. Uniformity prefers uniformly
distributed representations, which helps preserve
maximal information. It could be defined as:
ℓ = log Ee(7)
where pdenotes whole data distribution.
To justify the inner workings of our approach,
we calculate the alignment and uniformity metrics
every 10 steps during training on the STS-B devel-
opment set. We compare our approach with Sim-
CSE and visualize the results in Figure 8. We can
see that compared to the original BERT checkpoint,
both ArcCSE and SimCSE improve the alignment
and uniformity measures during training. ArcCSE
performs better on the alignment measure and on
par with SimCSE on the uniformity measure. This
verifies the intuition of our approach and demon-
strates that ArcCSE could help improve the quality
of sentence representations.
5 Conclusion
In this work, we propose ArcCSE, a self-supervised
contrastive learning framework for learning sen-
tence representation. We propose a new optimiz-
ing objective ArcCon loss to model pairwise sen-
tence relations with enhanced discriminating power,
and a new self-supervised task to model the par-
tial order of semantics between sentences. Ex-
perimental results on semantic textual similarity
tasks (STS) and SentEval tasks demonstrate that
both techniques bring substantial improvements
and our method outperforms previous state-of-the-
art method for sentence representation learning.
References