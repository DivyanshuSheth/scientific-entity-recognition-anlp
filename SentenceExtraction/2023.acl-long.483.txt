
Chenghao Yang, Fan Yin, He He, Kai-Wei Chang, Xiaofei Ma, Bing XiangUnversity of ChicagoUniversity of California, Los AngelesNew York UniversityAWS AI LabsAmazon Alexa AI
yangalan1996@gmail.com, fanyin20@cs.ucla.edu
{hehea, kaiweic, xiaofeim, bxiang}@amazon.com
Abstract
Despite the popularity of Shapley Values in ex-
plaining neural text classification models, com-
puting them is prohibitive for large pretrained
models due to a large number of model evalua-
tions. In practice, Shapley Values are often esti-
mated with a small number of stochastic model
evaluations. However, we show that the esti-
mated Shapley Values are sensitive to random
seed choices – the top-ranked features often
have little overlap across different seeds, espe-
cially on examples with longer input texts. This
can only be mitigated by aggregating thousands
of model evaluations, which on the other hand,
induces substantial computational overheads.
To mitigate the trade-off between stability and
efficiency, we develop an amortized model that
directly predicts each input feature’s Shapley
Value without additional model evaluations. It
is trained on a set of examples whose Shapley
Values are estimated from a large number of
model evaluations to ensure stability. Experi-
mental results on two text classification datasets
demonstrate that our amortized model esti-
mates Shapley Values accurately with up to 60
times speedup compared to traditional methods.
Furthermore, the estimated values are stable as
the inference is deterministic. We release our
code at https://github.com/yangalan123/
Amortized-Interpretability .
1 Introduction
Many powerful natural language processing (NLP)
models used in commercial systems only allow
users to access model outputs. When these systems
are applied in high-stakes domains, such as health-
care, finance, and law, it is essential to interpret how
these models come to their decisions. To this end,
post-hoc black-box explanation methods have been
proposed to identify the input features that are most
critical to model predictions (Ribeiro et al., 2016;
Lundberg and Lee, 2017). A famous class of post-Figure 1: Heatmaps of explanation scores of an example
from Yelp-Polarity based on two runs of KernelSHAP
(KS) using different random seeds. KS is run on a
fine-tuned BERT model using 200samples per instance
(approx. 3.47s per instance on average using a single
A100 GPU, more than 150times slower than one for-
ward inference of the BERT model). The darker each
token is, the higher its explanation score. Clearly, inter-
pretation results are significantly different when using
different seeds.
hoc black-box local explanation methods takes ad-
vantage of the Shapley Values (Shapley, 1953) to
identify important input features, such as Shapley
Value Sampling (SVS) (Strumbelj and Kononenko,
2010) and KernelSHAP (KS) (Lundberg and Lee,
2017). These methods typically start by sampling
permutations of the input features (“ perturbation
samples ”) and aggregating model output changes
over the perturbation samples. Then, they assign an
explanation score for each input feature to indicate
its contribution to the prediction.
Despite the widespread usage of Shapley Values
methods, we observe that when they are applied
to text data, the estimated explanation score for
each token varies significantly with the random
seeds used for sampling. Figure 1 shows an exam-
ple of interpreting a BERT-based sentiment classi-
fier (Devlin et al., 2019) on Yelp-Polarity dataset,
a restaurant review dataset (Zhang et al., 2015) by
KS. The set of tokens with high explanation scores8666
varies significantly when using different random
seeds. They become stable only when the num-
ber of perturbation samples increases to more than
2,000. As KS requires model prediction for each
perturbation sample, the inference cost can be sub-
stantial. For example, it takes about 183 seconds
to interpret each instance in Yelp-Polarity using
the KS Captum implementation (Kokhlikyan et al.,
2020) on an A100 GPU. In addition, this issue be-
comes more severe when the input text gets longer,
as more perturbation samples are needed for reli-
able estimation of Shapley Values. This sensitivity
to the sampling process leads to an unreliable in-
terpretation of the model predictions and hinders
developers from understanding model behavior.
To achieve a better trade-off between efficiency
and stability, we propose a simple yet effective
amortization method to estimate the explanation
scores. Motivated by the observation that different
instances might share a similar set of important
words (e.g., in sentiment classification, emotional
words are strong label indicators (Taboada et al.,
2011)), an amortized model can leverage similar
interpretation patterns across instances when pre-
dicting the explanation scores. Specifically, we
amortize the cost of computing explanation scores
by precomputing them on a set of training exam-
ples and train an amortized model to predict the
explanation scores given the input. At inference
time, our amortized model directly outputs expla-
nation scores for new instances. Although we need
to collect a training set for every model we wish
to interpret, our experiments show that with as few
as 5000 training instances, the amortized model
achieves high estimation accuracy. We show our
proposed amortized model in Figure 2.
The experimental results demonstrate the effi-ciency and effectiveness of our approach. First, our
model reduces the computation time from about
3.47s per instance to less than 50ms,which is
60 times faster than the baseline methods. Sec-
ond, our model is robust to randomness in training
(e.g., random initialization, random seeds used for
generating reference explanation scores in the train-
ing dataset), and produces stable estimations over
different random seeds. Third, we show that the
amortized model can be used along with SVS to
perform local adaption , i.e., adapting to specific
instances at inference time, thus further improving
performance if more computation is available (6.3).
Finally, we evaluate our model from the function-
ality perspective (Doshi-Velez and Kim, 2017; Ye
and Durrett, 2022) by examining the quality of the
explanation in downstream tasks. We perform case
studies on feature selection and domain calibration
using the estimated explanation scores, and show
that our method outperforms the computationally
expensive KS method.
2 Related Works
Post-Hoc Local Explanation Methods Post-hoc
local explanations are proposed to understand the
prediction process of neural models (Simonyan
et al., 2014; Ribeiro et al., 2016; Lundberg and
Lee, 2017; Shrikumar et al., 2017). They work
by assigning an explanation score to each feature
(e.g., a token) in an instance (“local”) to indicate its
contribution to the model prediction. In this paper,
we focus on studying KernelSHAP (KS) (Lund-
berg and Lee, 2017), an additive feature attribution
method that estimates the Shapley Value (Shapley,
1953) for each feature.8667There are other interpretability methods in NLP.
For example, gradient-based methods (Simonyan
et al., 2014; Li et al., 2016), which use the gradi-
ent w.r.t. each input dimension as a measure for
its saliency. Reference-based methods (Shrikumar
et al., 2017; Sundararajan et al., 2017) consider the
model output difference between the original input
and reference input (e.g., zero embedding vectors).
Shapley Values Estimation Shapley Values are
concepts from game theory to attribute total contri-
bution to individual features. However, in practice
estimating Shapley values requires prohibitively
high cost for computation, especially when explain-
ing the prediction on long documents in NLP. KS
works as an efficient way to approximate Shapley
Values. Previous work on estimating Shapley Val-
ues mainly focuses on accelerating the sampling
process (Jethani et al., 2021; Covert and Lee, 2021;
Parvez and Chang, 2021; Mitchell et al., 2022)
or removing redundant features (Aas et al., 2021;
Covert et al., 2021). In this work, we propose a
new method to combat this challenge by training
an amortized model.
Robustness of Local Explanation Methods De-
spite being widely adopted, there has been a long
discussion on the actual quality of explanation
methods. Recently, people have found that expla-
nation methods can assign substantially different
attributions to similar inputs (Alvarez-Melis and
Jaakkola, 2018; Ghorbani et al., 2019; Kindermans
et al., 2019; Yeh et al., 2019; Slack et al., 2021;
Yin et al., 2022), i.e., they are not robust enough,
which adds to the concerns about how faithful these
explanations are (Doshi-Velez and Kim, 2017; Ade-
bayo et al., 2018; Jacovi and Goldberg, 2020). In
addition to previous work focusing on robustness
against input perturbations, we demonstrate that
even just changing the random seeds can cause the
estimated Shapley Values to be weakly-correlated
with each other, unless a large number of perturba-
tion samples are used (which incurs high computa-
tional cost).
Amortized Explanation Methods Our method is
similar to recent works on amortized explanation
models including CXPlain (Schwab and Karlen,
2019) and FastSHAP (Jethani et al., 2021)), where
they also aim to improve the computational effi-
ciency of explanation methods. The key differ-
ences are: 1) We do not make causal assumptions
between input features and model outputs; and 2)
we focus on text domains, where each feature isa discrete token (typical optimization methods for
continuous variables do not directly apply).
3 Background
In this section, we briefly review the basics of Shap-
ley Values, focusing on its application to the text
classification task.
Local explanation of black-box text classifi-
cation models. In text classification tasks, in-
puts are usually sequences of discrete tokens
X= [w, w, . . . , w]. Here Lis the length
ofXand may vary across examples; wis
thej-th token of X. The classification model
Mtakes the input Xand predict the label as
ˆy= arg maxM(X) [y]. Local explanation
methods treat each data instance independently and
compute an explanation score ϕ(j, y), representing
the contribution of wto the label y. Usually, we
care about the explanation scores when y= ˆy.
Shapley Values (SV) are concepts from game the-
ory originally developed to assign credits in co-
operative games (Shapley, 1953; Strumbelj and
Kononenko, 2010; Lundberg and Lee, 2017; Covert
et al., 2021). Let s∈ {0,1}be a masking of the
input and define X={w}as the perturbed
input that consists of unmasked tokens x(where
the corresponding mask shas a value of 1). In this
paper, we follow the common practice (Ye et al.,
2021; Ye and Durrett, 2022; Yin et al., 2022) to
replace masked tokens with [PAD] in the input be-
fore sending it to the classifier. Let |s|represent
the number of non-zero terms in s. Shapley Values
ϕ(i, y)(Shapley, 1953) are computed by:
ϕ(i, y) =1
L/summationdisplay/parenleftbiggL−1
|s|/parenrightbigg
(M(X∪ {w}) [y]−M(X) [y]).(1)
Intuitively, ϕ(i, y)computes the marginal contri-
butions of each token to the model prediction.
Computing SV is known to be NP-hard (Deng
and Papadimitriou, 1994). In practice, we estimate
Shapley Values approximately for efficiency. Shap-
ley Values Sampling (SVS) (Castro et al., 2009;
Strumbelj and Kononenko, 2010) is a widely-used
Monte-Carlo estimator of SV:8668Here σ∈Π(L)is the sampled ordering and[σ]
is the non-ordered setof indices for σ.[σ]
represents the setof indices ranked lower than iin
σ.S([σ])maps the indices set[σ]to a mask s∈
{0,1}such that s=1[i∈[σ]].mis the number
ofperturbation samples used for computing SVS.
KernelSHAP Although SVS has successfully re-
duced the exponential time complexity to polyno-
mial, it still requires sampling permutations and
needs to do sequential updates following sampled
orderings and computing the explanation scores,
which is an apparent efficiency bottleneck. Lund-
berg and Lee (2017) introduce a more efficient
estimator, KernelSHAP (KS), which allows better
parallelism and computing explanation scores for
all tokens at once using linear regression. That is
achieved by showing that computing SV is equiva-
lent to solving the following optimization problem:
ϕ(·, y)≈arg min1
m
/summationdisplay[M/parenleftbig
X/parenrightbig
[y]−⃗ s(k)ϕ(·, y)],
(3)
s.t. 1ϕ(·, y) =M(X) [y]−M(∅) [y],
where ⃗ s(k)is the one-hot vector corresponding to
the masks(k)sampled from the Shapley Kernel
p(s) =
().mis again the number of
perturbation samples. We will use “SVS- m” and
“KS-m” in the rest of the paper to indicate the sam-
ple size for SVS and KS. In practice, the specific
perturbation samples depend on the random seed
of the sampler, and we will show that the explana-
tion scores are highly sensitive to the random seed
under a small sample size.
Note that the larger the number of perturbation
samples, the more model evaluations are required
for a single instance, which can be computationally
expensive for large Transformer models. Therefore,
the main performance bottleneck is the number of
model evaluations.
4 Stability of Local Explanation
One of the most common applications of SV is
feature selection, which selects the most important
features by following the order of the explanationscores. People commonly use KS with an afford-
able number of perturbation samples in practice
(the typical numbers of perturbation samples used
in the literature are around 25,200,2000 ). How-
ever, as we see in Figure 1, the ranking of the
scores can be quite sensitive to random seeds when
using stochastic estimation of SV . In this section,
we investigate this stability issue. We demonstrate
stochastic approximation of SV is unstable in text
classification tasks under common settings, espe-
cially with long texts. In particular, when ranking
input tokens based on explanation scores, Spear-
man’s correlation between rankings across different
runs is low.
Measuring ranking stability. Given explanation
scores produced by different random seeds using
an SV estimator, we want to measure the difference
between these scores. Specifically, we are inter-
ested in the difference in the rankings of the scores
as this is what we use for feature selection. To mea-
sure the ranking stability of multiple runs using
different random seeds, we compute Spearman’s
correlation between any two of them and use the av-
erage Spearman’s correlation as the measure of the
ranking stability. In addition, we follow Ghorbani
et al. (2019) to report Top-K intersections between
two rankings, since in many applications only the
top features are of explanatory interest. We mea-
sure the size of the intersection of Top-K features
from two different runs.
Setup. We conduct our experiments on the valida-
tion set of the Yelp-Polarity dataset (Zhang et al.,
2015) and MNLI dataset (Williams et al., 2018).
Yelp-Polarity is a binary sentiment classification
task and MNLI is a three-way textual entailment
classification task. We conduct experiments on 500
random samples with balanced labels (we refer to
these datasets as “Stability Evaluation Sets” sub-
sequently). Results are averaged over 5different
random seeds.We use the publicly available fine-
tuned BERT-base-uncased checkpoints(Morris
et al., 2020) as the target models to interpret and
use the implementation of Captum (Kokhlikyan
et al., 2020) to compute the explanation scores for
both KS and SVS. For each explanation method,
we test with the recommended numbers of pertur-8669
bation samplesused to compute the explanation
scores for every instance. For Top-K intersections,
we report results with K= 5andK= 10 .
Trade-off between stability and computation
cost. The ranking stability results are listed in
Table 1 and Table 2 for Yelp-Polarity and MNLI
datasets. We observe that using 25 to 200 perturba-
tion samples, the stability of the explanation scores
is low (Spearman’s correlation is only 0.16). Sam-
pling more perturbed inputs makes the scores more
stable. However, the computational cost explodes
at the same time, going from one second to two min-
utes per instance. To reduce the sensitivity to an ac-
ceptable level (i.e., making the Spearman’s correla-
tion between two different runs above 0.40, which
indicates moderate correlation (Akoglu, 2018)), we
usually need thousands of model evaluations and
spend roughly 33.40seconds per instance.
Low MSE does not imply stability. Mean Squared
Error (MSE) is commonly used to evaluate the dis-
tance between two lists of explanation scores. In
Table 1, we observe that MSE only weakly corre-
lates with ranking stability (e.g., For Yelp-Polarity,
R=−0.41andp < 0.05, so the correlation is
not significant). Even when the difference of MSE
for different settings is as low as 0.01, the corre-
lation between rankings produced by explanations
can still be low. Therefore, from users’ perspec-
tives, low MSEs do not mean the explanations are
reliable as they can suggest distinct rankings.
Longer input suffers more from instability. Wealso plot the Spearman’s correlation decomposed
at different input lengths in Figure 3. Here, we
observe a clear trend that the ranking stability de-
grades significantly even at an input length of 20
tokens. The general trend is that the longer the
input length is, the worse the ranking stability. The
same trend holds across datasets. As many NLP
tasks involve sentences longer than 20 tokens (e.g.,
SST-2 (Socher et al., 2013), MNLI (Williams et al.,
2018)), obtaining stable explanations to analyze
NLP models can be quite challenging.
Discussion: why Shapley Values estimation is
unstable in text domain? One of the most promi-
nent characteristics of the text domain is that in-
dividual tokens/n-grams can have a large impact
on the label. Thus they need to be all included
in the perturbation samples for an accurate esti-
mate. When the input length grows, the number
of n-grams will grow fast. As shown in Section 3,
the probability of certain n-grams getting sampled
is drastically reduced as each n-gram will be sam-
pled with equivalent probability. Therefore, the ob-
served model output will have a large variance as
certain n-grams may not get sampled. A concurrent
work (Kwon and Zou, 2022) presented a related
theoretical analysis on why the uniform sampling
setting in SV computation can lead to suboptimal
attribution.
5Amortized Inference for Shapley Values
Motivated by the above observation, we propose to
train an amortized model to predict the explanation
scores given an input without any model evaluation
on perturbation samples . The inference cost is thus
amortized by training on a set of pre-computed8670
reliable explanation scores.
We build an amortized explanation model for
text classification in two stages. In the first stage,
we construct a training set for the amortized model.
We compute reliable explanation scores as the ref-
erence scores for training using the existing SV
estimator. As shown in Section 4, SVS-25 is the
most stable SV estimator and we use it to obtain
reference scores. In the second stage, we train a
BERT-based amortized model that takes the text
as input and outputs the explanation scores using
MSE loss.
Specifically, given input tokens X, we use
a pretrained language model Mto encode
words into d-dim embeddings ⃗ e=M(X) =
[⃗ e, . . . ,⃗ e]∈R. Then, we use a linear
layer to transform each ⃗ eto the predicted expla-
nation score ϕ(i,ˆy) =W⃗ e+b. To train the
model, we use MSE loss to fit ϕ(i,ˆy)to the pre-
computed reference scores ϕ(i,ˆy)over the training
setX. This is an amortized model in the sense
that there are no individual sampling and model
queries for each test example Xas in SVS and
KS. When a new sample comes in, the amortized
model makes a single inference on the input tokens
to predict their explanation scores.
Algorithm 1 Local Adaption5.1 Better Fit via Local Adaption
By amortization, our model can learn to capture
the shared feature attribution patterns across data
to achieve a good efficiency-stability trade-off. We
further show that the explanations generated by our
amortized model can be used to initialize the expla-
nation scores of SVS. This way, the evaluation of
SVS can be significantly sped up compared with
using random initialization. On the other hand,
applying SVS upon amortized method improves
the latter’s performance as some important tokens
might not be captured by the amortized method
but can be identified by SVS through additional
sampling (e.g., low-frequency tokens). The de-
tailed algorithm is shown in Algorithm 1. Note
that here we can recover the original SVS computa-
tion (Strumbelj and Kononenko, 2010) by replacing
ϕ←M(X)to be ϕ←0.Mis the amor-
tized model trained using MSE as explained earlier.
6 Experiments
In this section, we present experiments to demon-
strate the properties of the proposed approach in
terms of accuracy against reference scores (6.1)
and sensitivity to training-time randomness (6.2).
We also show that we achieve a better fit via a local
adaption method that combines our approach with
SVS (6.3). Then, we evaluate the quality of the
explanations generated by our amortized model on
two downstream applications (6.5).
Setup. We conduct experiments on the validation
set of Yelp-Polarity and MNLI datasets. To gener-
ate reference explanation scores, we leverage the
Thermostat (Feldhus et al., 2021) dataset, which
contains 9,815 pre-computed explanation scores of
SVS-25 on MNLI. We also compute explanation
scores of SVS-25 for 25,000 instances on Yelp-
Polarity. We use BERT-base-uncased (Devlin et al.,86712019) for M. For dataset preprocessing and
other experiment details, we refer readers to Ap-
pendix C.
To our best knowledge, FastSHAP (Jethani et al.,
2021) is the most relevant work to us that also takes
an amortization approach to estimate SV on tabu-
lar or image data. We adapt it to explain the text
classifier and use it as a baseline to compare with
our approach. We find it non-trivial to adapt Fast-
SHAP to the text domain. As pre-trained language
models occupy a large amount of GPU memory,
we can only use a small batch size with limited per-
turbation samples (i.e., 32perturbation samples per
instance). This is equivalent to approximate KS-32
and the corresponding reference explanation scores
computed by FastSHAP are unstable. More details
can be found in Appendix A.
6.1 Shapley Values Approximation
To examine how well our model fits the pre-
computed SV (SVS-25), we compute both Spear-
man’s correlation and MSE over the test set. As
it is intractable to compute exact Shapley Values
for ground truth, we use SVS-25 as a proxy. We
also include different settings for KS results over
the same test set. KS is also an approximation to
permutation-based SV computation (Lundberg and
Lee, 2017). Table 3 shows the correlation and MSE
of aforementioned methods against SVS-25.
First, we find that despite the simplicity of our
amortized model, the proposed amortized models
achieve a high correlation with the reference scores
(0.61>0.60) on Yelp-Polarity. The correlation
between outputs from the amortized models and
references is moderate ( 0.42>0.40) on MNLI
when data size is limited. During inference time,
our amortized models output explanation scores
for each instance within 50milliseconds, which
is about 40-60times faster than KS-200 and 400-
600times faster than KS-2000 on Yelp-Polarity
and MNLI. Although the approximation results are
not as good as KS-2000/8000 (which requires far
more model evaluations), our approach achieves
reasonably good results with orders of magnitude
less compute.
We also find that the amortized model achieves
the best MSE score among all approximation meth-
ods. Note that the two metrics, Spearman’s correla-
tion and MSE, do not convey the same information.
MSE measures how well the reference explanation
scores are fitted while Spearman’s correlation re-
flects how well the ranking information is learned.
We advocate for reporting both metrics.
Cost of training the amortized models To pro-
duce the training set, we need to pre-compute the
explanation scores on a set of data. Although this is
a one time cost (for each model), one might wonder
how time consuming this step is as we need to run
the standard sample-based estimation. As the learn-
ing curve shows in Figure 4, we observe that the
model achieves good performance with about 25%
(≈5,000on Yelp-Polarity) instances. Addition-
ally, in Section 6.4, we show this one-time training
will result in a model transferable to other domains,
so we may not need to train a new amortized model
for each new domain.
6.2 Sensitivity Analysis
Given a trained amortized model, there is no
randomness when generating explanation scores.
However, there is still some randomness in the8672
training process, including the training data, the
random initialization of the output layer and ran-
domness during update such as dropout. Therefore,
similar to Section 4, we study the sensitivity of the
amortized model. Table 4 shows the results with
different training data and random seeds. We ob-
serve that: 1) when using the same data (100%),
random initialization does not affect the outputs of
amortized models – the correlation between differ-
ent runs is high (i.e., 0.77on MNLI and 0.76on
Yelp-Polarity). 2) With more training samples, the
model is more stable.
6.3 Local Adaption
The experiment results for Local Adaption (Sec-
tion 5.1) are shown in Table 5. Here we can see
that: 1) by doing local adaption, we can further
improve the approximation results using our amor-
tized model, 2) by using our amortized model as
initialization, we can improve the sample efficiency
of SVS significantly (by comparing the perfor-
mance of SVS-X and Adapt-X). These findingshold across datasets.
6.4 Domain Transferability
To see how well our model performs on out-of-
domain data, we train a classification model and
its amortized explanation model on Yelp-Polarity
and then explain its performance on SST-2 (Socher
et al., 2013) validation set. Both tasks are two-
way sentiment classification and have significant
domain differences.
Our amortized model achieves a Spearman’s cor-
relation of approximately 0.50 with ground truth
SV (SVS-25) while only requiring 0.017s per in-
stance. In comparison, KS-100 achieves a lower
Spearman’s correlation of 0.46 with the ground
truth and takes 1.6s per instance; KS-200 performs
slightly better in Spearman’s correlation but re-
quires significantly more time. Thus, our amor-
tized model is more than 90 times faster and more
correlated with ground truth Shapley Values. This
shows that, once trained, our amortized model can
provide efficient and stable estimations of SV even
for out-of-domain data.
In practice, we do not recommend directly ex-
plaining model predictions on out-of-domain data
without verification, because it may be misaligned
with user expectations for explanations, and the out-
of-domain explanations may not be reliable (Hase
et al., 2021; Denain and Steinhardt, 2022). More
exploration on this direction is required but is or-
thogonal to this work.
6.5 Evaluating the Quality of Explanation
Feature Selection. The first case study is fea-
ture selection, which is a straightforward appli-
cation of local explanation scores. The goal is
to find decision-critical features via removing in-
put features gradually according to the rank given
by the explanation methods. Following previous
work (Zaidan et al., 2007; Jain and Wallace, 2019;
DeYoung et al., 2020), we measure faithfulness by
changes in the model output after masking tokens
identified as important by the explanation method.
The more faithful the explanation method is to the
target model, the more performance drop will be
incurred by masking important tokens.
We gradually mask Top- αtokens ( α=
1%,5%,10%,20%) and compute the accuracy
over corrupted results using the stability evalua-
tion sets for MNLI and Yelp-Polarity datasets as
mentioned in Section 4. As the results show in Fig-
ure 5, the amortized model is more faithful than KS-8673
200 but underperforms KS-2000/8000 and SVS-25.
However, the amortized model is more efficient
than these methods. So amortized model achieves
a better efficiency-faithfulness trade-off.
Explanation for Model Calibration. Recent work
suggests that good explanations should be informa-
tive enough to help users to predict model behav-
ior (Doshi-Velez and Kim, 2017; Chandrasekaran
et al., 2018; Hase and Bansal, 2020; Ye et al., 2021).
Ye and Durrett (2022) propose to combine the lo-
cal explanation with pre-defined feature templates
(e.g., aggregating explanation scores for overlap-
ping words / POS Tags in NLI as features) to cali-
brate an existing model to new domains. The ratio-
nale behind this is that, if the local explanation truly
connects to human-understandable model behavior,
then following the same way how humans trans-
fer knowledge to new domains, the explanations
guided by human heuristics (in the form of feature
templates) should help calibrate the model to new
domains. Inspired by this, we conduct a study us-
ing the same calibrator architecture but plugging in
different local explanation scores.
We follow Ye and Durrett (2022) to calibrate
a fine-tuned MNLI modelto MRPC. The exper-
iment results are shown in Table 6. In the table,
“BOW” means the baseline that uses constant ex-
planation scores when building the features for the
calibration model. Compared with the explana-
tion provided by KS-2000, the explanation given
by the amortized model achieves better accuracy,
suggesting that the amortized model learns robust
explanation scores that can be generalized to out-
of-domain data in downstream applications.
7 Conclusion
In this paper, we empirically demonstrated that it is
challenging to obtain stable explanation scores on
long text inputs. Inspired by the fact that different
instances can share similarly important features,
we proposed to efficiently estimate the explanation
scores through an amortized model trained to fit
pre-computed reference explanation scores.
In the future, we plan to explore model architec-
ture and training loss for developing effective amor-
tized models. In particular, we may incorporate
sorting-based loss to learn the ranking order of fea-
tures. Additionally, we could investigate the trans-
ferability of the amortized model across different
domains, as well as exploring other SHAP-based
methods instead of the time-consuming SVS-25 in
the data collection process to improve efficiency
further.
Limitations
In this paper, we mainly focus on developing an
amortized model to efficiently achieve a reliable
estimation of SV . Though not experimented with
in the paper, our method can be widely applied
to other black-box post-hoc explanation methods
including LIME (Ribeiro et al., 2016). Also, due
to the limited budget, we only run experiments
on BERT-based models. However, as we do not8674make any assumption for the model as other black-
box explanation methods, our amortized model can
be easily applied to other large language models.
We only need to collect the model output and our
model can be trained offline with just thousands
of examples as we show in our method and experi-
ments.
Comparison and Training with Exact Shapley
Values Computing exact SV is computationally
prohibitive for large language models (LLMs) on
lengthy text inputs, as it necessitates the evaluation
of LLMs on an exponential (in sequence length)
number of perturbation samples per instance. As a
result, we resort to using SVS-25, which serves as a
reliable approximation, for training our amortized
models.
Acknowledgements
We want to thank Xi Ye and Prof. Greg Durrett
for their help regarding their previous work and
implementation on using SV for calibration (Sec-
tion 6.5). We thank the generous support from
AWS AI on computational resources and external
collaborations. We further thank Prof. Chenhao
Tan for the high-level idea discussion on explain-
ability stability issues at an early stage of this paper,
and thank Prof. Yongchan Kwon and Prof. James
Zou for their in-depth theoretical analysis of sub-
optimality of uniform sampling of computing SV .
We thank all anonymous reviewers and chairs at
ACL’23 and ICLR’23 for their insightful and help-
ful comments. Yin and Chang are supported in part
by a CISCO grant and a Sloan Fellowship. HH is
supported in part by a Cisco grant and Samsung
Research (under the project Next Generation Deep
Learning: From Pattern Recognition to AI).
References867586768677A Adaption for FastSHAP Baseline
As we mentioned in Section 6, we build our amor-
tized models upon a pre-trained encoder BERT (De-
vlin et al., 2019). However, using the pre-trained
encoder significantly increases the memory foot-
print when running FastSHAP. In particular, we
have to host two language models on GPUs, one
for the amortized model and the other one for the
target model. Therefore, we can only adopt the
batch size equal to 1 and 32perturbation samples
per instance. Following the proof in FastSHAP,
this is equivalent to teaching the amortized model
to approximate KS-32, which is an unreliable inter-
pretation method (See Section 6.2).
In experiments, we find that the optimization of
FastSHAP is unstable. After an extensive hyper-
parameter search, we set the learning rate to 1e-6
and increased the number of epochs to 30. How-
ever, this requires us to train the model on a single
A100 GPU for 3 days to wait for FastSHAP to
converge.
B Scientific Artifacts License
For the datasets used in this paper, MNLI (Williams
et al., 2018) is released under ONAC’s license.
Yelp-Polarity (Zhang et al., 2015) and SST-
2 (Socher et al., 2013) datasets does not provide
detailed licenses.
For model checkpoints used in this paper, they
all come from textattack project (Morris et al.,
2020) and they are open-sourced under MIT li-
cense.
For implementation, we mainly use Cap-
tum (Kokhlikyan et al., 2020) and Thermo-
stat (Feldhus et al., 2021). Captum is open-sourced
under BSD 3-Clause "New" or "Revised" License
and Thermostat is open-sourced under Apache Li-
cense 2.0.
C Training Details
In this section, we introduce our dataset preprocess-
ing, hyperparameter settings and how we train the
models.
For both MNLI and Yelp-Polarity datasets, we
split them into 8:1:1 for training, validation, and
test sets.
The hyperparameters of amortized models are
tuned on the validation set. We use Adam (Kingma
and Ba, 2015) optimizer with a learning rate of
5e-5, train the model for at most 10 epochs and do
early stopping to select best model checkpoints.8678ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 8
/squareA2. Did you discuss any potential risks of your work?
We use the common public datasets, models, and scripts provided by Huggingface to investigate a
more efﬁcient computation method for Shapley Values.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Introduction is in section 1, and abstract is in page 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 4 and 6
/squareB1. Did you cite the creators of artifacts you used?
Section 4 and 6
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section E in Appendix
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 4 and 6
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We use public datasets, and according to the author of the original dataset, they have been
anonymized.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4 and 6
C/squareDid you run computational experiments?
Section 4 and 6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 1, 4 and 68679/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4 and 6
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 1, 4 and 6
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4 and 6
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.8680