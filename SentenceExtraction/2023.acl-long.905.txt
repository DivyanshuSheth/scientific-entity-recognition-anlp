
Mounica Maddela, Yao Dou, David Heineman, Wei Xu
School of Interactive Computing
Georgia Institute of Technology
http://lens-score.com/
Abstract
Training learnable metrics using modern lan-
guage models has recently emerged as a promis-
ing method for the automatic evaluation of ma-
chine translation. However, existing human
evaluation datasets for text simplification have
limited annotations that are based on unitary
or outdated models, making them unsuitable
for this approach. To address these issues, we
introduce the SE corpus that contains:
SE, comprising 12K human rat-
ings on 2.4K simplifications of 24 past systems,
andSE, a challenging simplifica-
tion benchmark consisting of over 1K human
ratings of 360 simplifications including GPT-
3.5 generated text. Training on SE, we
present L, aLearnable Evaluatio nMetric
for Text Simplification. Extensive empirical
results show that L correlates much bet-
ter with human judgment than existing metrics,
paving the way for future progress in the evalu-
ation of text simplification. We also introduce
R & R, a human evaluation framework
that rates simplifications from several models
in a list-wise manner using an interactive in-
terface, which ensures both consistency and
accuracy in the evaluation process and is used
to create the SE datasets.
1 Introduction
Text simplification is a text-to-text generation task
that aims to make a text easier to read while pre-
serving its original meaning (Saggion, 2017). Auto-
matic evaluation of text simplification is challeng-
ing because a sentence can be simplified in many
ways, such as paraphrasing complex words, delet-
ing insignificant information, and splitting long
sentences into shorter ones. An ideal automatic
metric should accommodate these diverse choices
while capturing semantic similarity and fluency.
However, existing metrics such as SARI (Xu et al.,
2016) and BERTScore (Zhang et al., 2020) struggleFigure 1: Automatic metric and human evaluation
scores on simplifications of a complex sentence from
ourSE.L achieves the best correla-
tion with humans. SARI penalizes simplifications that
deviate from the reference (first two examples), and
BERTScore fails to penalize hallucinations with high
lexical overlap (third example).
to capture all the aspects and achieve a high cor-
relation with human evaluation (Alva-Manchego
et al., 2021) (see Figure 1). These metrics fail even
more when evaluating high-quality systems that
have close performance, calling for a more robust
and accurate metric for text simplification.
Prior work on machine translation (Rei et al.,
2020; Sellam et al., 2020) has seen the success of
using language models as an automatic metric by
training them on human judgments, such as Direct
Assessment (DA) that rates generations on a 0-100
scale. However, existing human evaluation datasets
(Alva-Manchego et al., 2021; Sulem et al., 2018)
for text simplification are not suitable for training
metrics because they include a limited number of
annotations or systems. Besides, these datasets do
not include state-of-the-art generation models such16383as GPT-3.5 (Ouyang et al., 2022) that have been
shown to generate human-like quality text (Dou
et al., 2022; Goyal et al., 2022).
In this work, we introduce L, aLearnable
Evaluatio nMetric for Text Simplification. L is
the first supervised metric for the task and uses an
adaptive ranking loss to promote fair comparison
of simplifications that have undergone different
edits (i.e., splitting, paraphrasing, and deletion).
To train L, we collect 12K human judgments
on 2.4K simplifications by 24 simplification sys-
tems from the literature, which we name as the
SE dataset. We also create SE- to evaluate L and other metrics in a
realistic and challenging setting of assessing sim-
plifications by state-of-the-art language models on
the more recent, complex, and longer sentences
published on Wikipedia after Oct 22nd, 2022. S-E contains over 1K human ratings on 360
simplifications generated by 4 SOTA models, in-
cluding GPT-3.5, and 2 humans.
Empirical experiments show that Lachieves
a higher correlation of 0.331 with human ratings
onSE, which is more than twice as
high as the correlation scores of 0.112 and 0.149
by BERTScore and SARI, respectively. We further
demonstrate that incorporating L into decod-
ing process, using minimum Bayes risk framework
(Fernandes et al., 2022), can directly improve the
automatic text simplification system’s performance.
We expect that our data collection method, includ-
ingR & R, a list-wise human evaluation
interface, can be easily adapted to other text gener-
ation tasks.
2 Background
Issues of Existing Automatic Metrics. SARI
(Xu et al., 2016) is the most commonly used metric
for text simplification that computes F1/precision
scores of the n-grams inserted, deleted, and kept
when compared to human references. As SARI
measures n-gram string overlap, it penalizes simpli-
fications that are synonymous to the reference but
uses different words (see first two examples in Fig-
ure 1). Alva-Manchego et al. (2021) showed that
BERTScore (Zhang et al., 2020), which measures
similarity based on BERT (Devlin et al., 2019) em-
beddings, is better at capturing semantic similarity
between the simplification system’s outputs and the
references. However, it fails to penalize conserva-
tive systems that make trivial or no changes to the
input as illustrated in Figure 2. This figure shows
average metric scores by SARI and BERTScore
for a conservative system that copies the input
(Copy ), an oracle system of human simplification
(Human ), and three disfluent corrupted systems
that drop 10% of the input words ( Drop ), scramble
5% of the input words ( Scramble ), or insert a pe-
riod in the middle ( Split). BERTScore ranks some
conservative or corrupted systems above human
simplifications. SARI penalizes the conservative
system because it focuses on the edits performed
by the system but ranks the disfluent systems above
the conservative system. An ideal automatic metric
for text simplification should capture both semantic
similarity and the edits performed by the system.
Lack of Human Evaluation Data. There is a
lack of suitable human evaluation datasets for fine-
tuning pre-trained language models for simplifi-
cation evaluation. Alva-Manchego et al. (2021)
released 0-100 continuous scale ratings on fluency,
meaning preservation, and simplicity for only 600
simplifications sampled from six systems. Sulem
et al. (2018) released 5-point Likert scale ratings
on the same three dimensions for 1,960 simplifica-
tions generated by different configurations of six
systems. Both datasets are relatively small and
cover too few different system designs. For exam-
ple, they do not include the newer state-of-the-art
systems (Sheang and Saggion, 2021; Martin et al.,
2022) that are based on T5 (Raffel et al., 2020) and
other large pre-trained language models.163843 Automatic Evaluation Metric
To tackle the challenge of limited human evaluation
data, we curate SE, a corpus containing
over 13K human judgements on 2.8K simplifica-
tion from 26 systems. This facilitates the training
and evaluation of L (ALearnable Evaluatio n
Metric for Text Simplification), the first supervised
automatic metric for text simplification evaluation.
In this section, we first describe the creation of
SE datasets in §3.1 and then Lin §3.2.
3.1 Collecting Human Judgements
We collect SE , containing 12K human
ratings on 2.4K simplifications from 24 systems on
sentences from TurkCorpus (Xu et al., 2016), to
train L. For evaluating L and other simpli-
fication metrics, we create SEthat con-
sists of 1,080 human ratings on 360 simplifications
from both humans and SOTA models, including
GPT-3.5. It features more complex sentences from
Wikipedia written after Oct 22nd, 2022, very recent
to the time we conduct the experiments, to reduce
the risk of “data contamination” (i.e., appearing in
the training data of LLMs) and serve as a more chal-
lenging test bed for large language models. Table 1
shows the summary of both datasets.
A Diverse Set of Simplification Systems. We
consider the following systems (further details
in Appendix C): (i) two GPT-3.5outputs under
zero-shot and 5-shot settings; (ii) eight fine-tuned
Transformer-based systems of varied sizes and pa-
rameter settings (Sheang and Saggion, 2021; Raf-
fel et al., 2020; Martin et al., 2020; Maddela et al.,
2021); (iii) three supervised BiLSTM-based sys-
tems that use vanilla RNN, reinforcement learning
(Zhang and Lapata, 2017), or explicit editing (Dong
et al., 2019); (iv) one unsupervised and one semi-
supervised system utilizing auto-encoders (Surya
et al., 2019); (v) two systems that apply statisti-
cal machine translation approaches to simplifica-
tion (Wubben et al., 2012; Xu et al., 2016); (vi) a
rule-based system (Kumar et al., 2020); (vii) three
hybrid systems that combine linguistic rules with
data-driven methods (Narayan and Gardent, 2014;
Sulem, 2018; Maddela et al., 2021); (viii) two naive
baselines that copy the input or scramble 5% of the
input words; and (ix) six human-written simplifica-
tions, including two from ASSET (Alva-Manchego
et al., 2020), one from TurkCorpus (Xu et al., 2016),
one from Simple Wikipedia that were automatically
aligned (Kauchak, 2013), and two newly written
by our trained in-house annotators.
Complex Sentences Selection. For SE- , we sample 100 complex sentences from
the test set of the widely-used TurkCorpus (Xu
et al., 2016) and ASSET (Alva-Manchego et al.,
2020) evaluation benchmarks, which share the
same set of complex sentences but have different
human simplifications in terms of edit operations.
ASSET contains 10 human references for each
complex sentence that are used to train L. GPT
is pre-trained on a vast amount of web text, and16385the sentences in TurkCorpus/ASSET are derived
from the 12-year-old Parallel Wikipedia Simplifi-
cation corpus (Zhu et al., 2010), which may have
already been encountered by GPT-3.5. To address
this “data contamination” issue and provide a more
challenging benchmark for state-of-the-art models,
we manually select 60 long, complex sentences
covering recent world events such as the Twitter
acquisition and World Cup for SE.
These sentences are from new revisions or arti-
cles added to Wikipedia between October 22nd and
November 24th, 2022. They have an average length
of 38.5 tokens, much longer than the 19.7-token
average of TurkCorpus and ASSET.
Data Annotation. We ask in-house annotators to
rate each simplification on a single 0-100 overall
quality scale using our R & Rframework
(more details in §6). We provided an extensive tu-
torial and two training sessions to the annotators,
which involved rating system outputs for five input
sentences (screenshots in Appendix F). We period-
ically inspected the annotations to prevent the de-
terioration of quality over time. All annotators are
English speakers who are university students. Each
simplification in SE receives 5 ratings,
while each simplification in SEis rated
by 3 annotators. We follow WMT (Akhbardeh
et al., 2021) to normalize the raw scores by the
mean and standard deviation of each rater, also
known as the z-score, which are later used to train
L. The inter-annotator agreement for SE- is 0.70 using the interval Krippendorff’s
αon z-scores, and 0.32 for SE, partly
because it contains GPT-3.5 outputs that are quite
competitive with human. Both are considered fair
to good agreement (Krippendorff, 2004).
3.2 A New Learnable Metric – L
Given an input text c, the corresponding sys-
tem output s, and a set of nreferences R=
{r, r, . . . , r},L produces a real-valued
score z= max(z)that maximizes over
the quality scores zofsin regards to each ref-
erence r. Our model encodes all texts into vec-
tors (c,s,r) using Transformer-based encoders
such as RoBERTa (Liu et al., 2019), then com-
bines them into an intermediate representation
H=/bracketleftbig
s;r;s⊙c;s⊙r;|s−c|;|s−r|/bracketrightbigby concatenation ( [; ]) and element-wise product
(⊙), which is then fed to a feedforward network to
predict z.
For training, besides considering all references
equally (i.e., Lwhen k=nin Eq. (1)), we
also adopt a reference-adaptive loss that selects a
subset of references closer to sin terms of edit op-
erations rather than the entire set R. It encourages
the metric to consider that different simplifications
(e.g., paraphrasing-focused, deletion-focused, with
or without splitting) can be acceptable, as long as
they are close to some (not necessarily all) of the
human references. We compute this loss ( L )
as:
L =1
km/summationdisplay/summationdisplay(h−z)(1)
where his human rating and mis the training
data size. We compute the set of predicted scores
Z={z, z, . . . , z}corresponding to references
inRand then choose top k(k≤n) values from
Zto form a subset Z⊆Z. Finally, we calculate
the mean squared error (MSE) between the human
rating hand each score in Z. By selecting top
kscores in Z, we focus the training of metric on
references similar to sin terms of writing style or
editing operations. This loss also aligns the training
step with the inference step, where we select the
best-predicted score zcorresponding to R. Al-
though multiple references are ideal, the proposed
loss can also use a single reference, similar to the
standard MSE loss. We train L on our S-E dataset (details in §3.1). We provide
further implementation details in Appendix A.
Similar to the COMET (Rei et al., 2020) and
BLEURT (Sellam et al., 2020) metrics for MT eval-
uation, L is trained on human ratings after z-
score normalization, which are real values predom-
inantly lie between [-3, 3]. To make L more
interpretable, we rescale the predicted scores to the
range between [0, 100] as the percentage of area
under a normal curve of mean 0and standard devia-
tion1corresponding to z. We present the rescaled
scores for experiments and analyses in this paper.
Besides, we use RoBERTa-large as the underly-
ing model of L throughout the paper, unless
otherwise specified (§5).
4 Experiments
We benchmark L metric against the existing
automatic metrics (i) for evaluating text simplifi-
cation system outputs and (ii) for training better16386
τ τ τ ττ τ τ
automatic simplification models when used as al-
ternative reward functions.
4.1 Correlation with Human Evaluation
We demonstrate that L correlates better with
human judgments than the existing metrics.
Evaluation Datasets. We compare L to the
existing metrics, namely SARI (Xu et al., 2016),
BERTScore (Zhang et al., 2020),BLEU, and
Flesch-Kincaid Grade Level readability (FKGL)
(Kincaid, 1975) on three datasets: SE- (§3.1), W-DA released by Alva-
Manchego et al. (2021) with 0-100 continuous
scale ratings on fluency, meaning preservation, and
simplicity for 600 simplifications across six sys-
tems, and N -L collected by Mad-
dela et al. (2021) with 5-point Likert scale ratings
on the same dimensions for 500 simplifications
across five systems. While SE and
W-DA are derived from Wikipedia, N -
L is derived from news articles in Newsela
(Xu et al., 2015), a widely used corpus for textsimplification. When calculating metric scores for
SEthat contains two human simplifi-
cations, we use one as the reference and the other
as the oracle simplification system. We remove the
complex sentences in W-DA that overlap with
SE , the training dataset for L.
Evaluation Setup. We report Kendall Tau-like
correlation for SEto capture the abil-
ity of metrics to distinguish two systems, which
is close to the real-world scenario. Kendall Tau-
like correlation is predominantly used in machine
translation metric evaluation at WMT (Bojar et al.,
2017; Ma et al., 2018) for the same reason as
it focuses on the relative ranking of the outputs.
Given an input cand its simplifications from N
systems S={s, . . . , s, . . . , s, . . . , s}, we
extract (s, s)pairs, where 1≤m < n ≤N,
and calculate Kendall Tau-like coefficient τ:
τ=|Concordant | − |Discordant |
|Concordant |+|Discordant |(2)
where Concordant is the set of pairs where the
metric ranked (s, s)in the same order as hu-
mans ranked and Discordant is the set of the pairs
where the metric and humans disagreed. We re-
port separate coefficients for paraphrase- ( τ),16387and split-focused ( τ) simplifications, along with
(τ) for all of them. Following the literature, we
only used the (s, s)pairs for which the differ-
ence in ratings is more than 5 out of the 100 points,
and all the annotators agreed on the ranking or-
der. To make results comparable to existing work
(Alva-Manchego et al., 2021, 2020) that evaluated
onW-DA andN -L , we report
Pearson correlation ( ρ) between the metric scores
and the human ratings.
Results. Table 2 shows that L outperforms
the existing metrics on SE andW-
DA belonging to the Wikipedia domain, and
N -L based on newswire domain.
The difference is more substantial on SE- consisting of similar performing SOTA
systems, where the τofLexceeds the
τof BERTScore by 0.22 points. Training using
topkreferences ( L) has improved τon
SEandρalong the simplicity dimen-
sion on the rest when compared to using all the
references ( L). Figure 3 shows the 95% con-
fidence intervals for τonSEandρ
for simplicity, which is deemed to be the most im-
portant dimension in prior work (Alva-Manchego
et al., 2021; Xu et al., 2016), on W-DA and
N -L calculated using bootstrapping
methods by Deutsch et al. (2021). A smaller inter-
val indicates that the metric exhibits lower variance
and higher reliability. L exhibits smaller inter-
vals than the other metrics.
4.2 L as Training or Decoding Objectives
We also incorporate L into training as an alter-
native reward function in minimum risk training
framework (Kumar and Byrne, 2004; Smith and
Eisner, 2006) and into decoding as a reranking
objective in minimum Bayes risk decoding (Fer-
nandes et al., 2022; Freitag et al., 2022) to improve
the quality of generated simplifications.
Minimum Risk Training (MRT). Given the in-
putcand reference r, we generate a set of can-
didates Sfor each training step and calculate the
expected risk ( L) as follows:
L=/summationdisplaycost(c, r, s )P(s|c)/summationtextP(s|c)(3)
P(s|c) =/productdisplayP(s|c, s;θ)
cost(c, r, s ) = 1−Metric (c, r, s )
where Metric (c, r, s )can be any evaluation metric,
θare the model parameters, and s=s, . . . s
is a partial generation of s.
Following previous work (Shen et al., 2016; Wi-
eting et al., 2019), we first train the seq-to-seq gen-
eration model with maximum likelihood estimation
(MLE) for a few epochs and then we change the
loss to a combination of L and expected risk:
L =γL + (1−γ)L. (4)
We choose γ= 0.3and|S|= 10 for our experi-
ments.
Minimum Bayes Risk Decoding (MBR). We
adopt the MBR framework proposed by Fernandes
et al. (2022), where we first generate a set of can-
didates Sfor input cduring inference then rerank
them by comparing each candidate s∈Sto all the
other candidates in the set:
ˆu = arg max1
|S|/summationdisplayMetric (c, s, s).(5)
For our experiments, we generate the candidates
using beam search with beam size =|S|.
Experiment Setup. We fine-tune a T5 model that
prepends control tokens to the input (Sheang and
Saggion, 2021) to control various aspects of the
generated simplification and has shown state-of-
the-art performance for the task. We use W-
A (Jiang et al., 2020) for training, ASSET16388
for validation, and the challenging, complex sen-
tences in SE for testing. WA
consists of 400k complex-simple sentence pairs
extracted from Normal and Simple Wikipedia doc-
ument pairs. For our experiments, we trained T5
towards L, BLEU, SARI, and BERTScore. We
provide more implementation details in Appendix
B. We report the same metrics on the test set along
with self-BLEU to capture the diversity of the out-
puts. We also ask 3 annotators to rate the system
outputs from SE using our evaluation
interface (§6) and report the averaged ratings.
Results. Table 3 shows that L integrated
into minimal Bayes risk decoding (MBR- L)
achieves the best human evaluation results while
it makes the most number of edits to the input
(less copying) as indicated by its lowest self-BLEU
score. Although MRT- L has slightly lower av-
erage human ratings than MRT-BERTScore and
MRT-SARI, the pairwise comparison shows that τ τ τ
they are very comparable: MRT-LENS generates
28% better, 28% worse, and 44% equal quality sim-
plifications compared to MRT-SARI. Additionally,
BERTScore and SARI show a decrease in quality
when used in decoding than standard maximum
likelihood estimation (MLE). It is noteworthy that
we use a beam size of 10 for MLE rather than a
large search space of beam size 100 because gen-
eration quality degrades with increased beam size
as shown in Table 4 as well as in existing literature
(Stahlberg and Byrne, 2019; Meister et al., 2020).
Given the success of using L as utility func-
tion of MBR decoding on T5-base, we further apply
it to larger models, including T5-3B and T5-11B.
As displayed in Table 4, MBR-LENS with 100 can-
didates improves over standard beam search by an
increase of over 11 points of L score across
all model sizes. Although MBR may inflate the
results of the utility function it uses (Fernandes
et al., 2022), our human evaluations solidify the
assertion that T5-11B with MBR-LENS decoding
exceeds standard beam search, thereby establishing
state-of-the-art (SOTA) performance among open-
source models. When compared to close-source
large language models, T5-11B with MBR-LENS
achieves on-par performance with GPT-3.5.16389
5 LENS Analysis
In this section, we delve into the impact of the
underlying model architecture and the number of
references on the performance of L.
Model Architecture. Table 6 shows the Kendall
Tau-like correlation ( τ,τ, and τ) of
L metric trained on various encoders on S-E.L metrics trained on RoBERTa
encoders (Liu et al., 2019) perform better than their
respective L metrics trained on the BERT en-
coders (Devlin et al., 2019). Among all models,
Ltrained on RoBERTa-large achieves the high-
est overall correlation. It has a substantial improve-
ment in τwith a trade-off in τ, in comparison
to RoBERTa-base.
Number of References. Figure 4 shows the Pear-
son correlation of L metric on W-DA for
a varied number of references used during infer-
ence. Although the metric performs the best with
10 references, we see only a slight drop with one
reference, demonstrating that L is capable of
evaluating with single or multiple references.
6 R & R Framework
To facilitate consistent evaluation and enable the
training of L, we develop R & R, a
human evaluation framework to assist annotators in
efficiently comparing and rating many ( >20) sys-
tem outputs at once in a list-wise ranking manner.
6.1 Methodology
We describe the three-step annotation methodology
for R & R(Figure 5) as follows:Step 1 - Categorizing System Outputs. As there
are different acceptable ways to simplify the in-
put text, we first display the system outputs in
groups as split-, deletion-, or paraphrase-focused
based on the following criteria: (i) outputs with
multiple sentences are split-focused, (ii) outputs
with a compression ratio less than 0.5or gener-
ated by deleting words from the original sentence
are deletion-focused, (iii) the rest are paraphrase-
focused. Annotators can adjust the initial automatic
categorization during the annotation process.
Step 2 - Highlighting Edits. To help annotators
notice the changes made by each system and sub-
sequently improve the accuracy of their ratings, we
use a state-of-the-art word alignment model by Lan
et al. (2021) to highlight the edits performed by
the system and classify them into three types: (i)
Deletion-edits are phrases or words with no match
in the modified output, (ii) Paraphrase-edits are new
phrases or words added or modified, and (iii) Split-
edits are any added periods (“.”). Deletion-edits
are marked with a red caret (“ ∧”), paraphrase-edits
with bolded text, and split-edits with two vertical
bars (“ ||”) (see Figure 5). We ask annotators to
correct the misclassified edits using an interactive
pop-up window (see Appendix F).
Step 3 - Ranking and Rating System Outputs.
Following the machine translation evaluation at
WMT (Ma et al., 2018, 2019; Barrault et al., 2020;
Akhbardeh et al., 2021), we ask annotators to rate
the quality of system outputs on a 0-100 continuous
scale instead of the 5-point Likert scale because the
former was shown to provide higher levels of inter-
annotator consistency (Novikova et al., 2018) and16390
is more suitable to apply on many statistical models
(Graham et al., 2013). Our interactive interface al-
lows the annotators to move the system outputs up
and down to rank them and compare similar quality
outputs more easily by placing them together.
We provide the annotation instructions and screen-
shots of the interface in Appendix F.
6.2 Human Evaluation Comparison
We compare R & Rwith the existing hu-
man evaluation methods: 5-point Likert (Sulem
et al., 2018) and Direct Assessment with a con-
tinuous scale of 0 to 100 (Alva-Manchego et al.,
2021), which were both conducted on three dimen-
sions: fluency, adequacy, and simplicity. For a fair
comparison, we annotate the same set of simplifi-
cations using each method, resulting in SL-andSDA. Table 7 shows similar
rankings of the systems by the three methods with
very slight differences. We also calculate the inter-
val Krippendorff’s α(Krippendorff, 2011) for inter-
annotator agreement. R & Rachieves an
αof 0.32, which is higher than the 0.23 αby Likert
and 0.25 αby DA. All values are considered fair
(Krippendorff, 2004).
7 Other Related Work
Text-to-text Generation Metrics. There is cur-
rently no single automatic metric to evaluate all the
text-to-text generation tasks that revise text. SARI
(Xu et al., 2016) is the main metric for text simplifi-
cation and has been used by other generation tasks
such as sentence splitting and fusion (Rothe et al.,
2020; Kim et al., 2021), decontextualization (Choi
et al., 2021), and scientific rewriting (Du et al.,
2022). Style transfer tasks (Hu et al., 2017; Rao and
Tetreault, 2018; Prabhumoye et al., 2018; Krishna
et al., 2020; Xu et al., 2012; Ma et al., 2020) use dif-
ferent automatic metrics to measure each aspect oftext: (i) text similarity metrics such as BLEU, ME-
TEOR (Lavie and Agarwal, 2007), and BERTScore
to measure content preservation, (ii) text classifi-
cation models (Sennrich et al., 2016; Luo et al.,
2019; Krishna et al., 2022b) or embedding-based
edit distance metrics such as MoverScore (Zhao
et al., 2019; Mir et al., 2019) to evaluate target
style, and (iii) perplexity or pseudo log likelihood
(Salazar et al., 2020) to measure fluency.
Incorporating Evaluation Metrics into Train-
ing and Inference. Prior studies have improved
MLE-trained generation systems with alternative
training approaches that integrate evaluation met-
rics based on reinforcement learning (Ranzato et al.,
2015; Li et al., 2016; Gong et al., 2019), minimum
risk (Smith and Eisner, 2006; Shen et al., 2016),
and ranking (Hopkins and May, 2011; Xu et al.,
2016; Krishna et al., 2022a). Incorporating metrics
into decoding has also been explored by reranking
the generations using discriminative rankers (Shen
et al., 2004; Lee et al., 2021), energy-based rankers
(Bhattacharyya et al., 2021), and minimum risk
(Kumar and Byrne, 2004; Freitag et al., 2022). We
use minimum risk as it has been shown to help ma-
chine translation systems (Wieting et al., 2019; Fer-
nandes et al., 2022; Amrhein and Sennrich, 2022).
8 Conclusion
We introduce L, the first supervised auto-
matic metric for text simplification. We show that
Lexhibits higher human correlation than other
automatic metrics. We also introduce R &
Rframework, which allows annotators to eval-
uate multiple systems’ outputs at once in a list-wise
manner. Using it, we create SE to
train L, and SE as a new metric
evaluation benchmark for text simplification. We
hope our metric, data, and framework will facilitate
future research in text simplification evaluation.16391Limitations
In this paper, we show that L shows better hu-
man correlation than other metrics on Wikipedia
and news domains. Future research can further
experiment and extend L to other domains,
such as medical and children’s books, as the pref-
erence for different simplification operations can
vary depending on the domain and user. Addition-
ally, our work focuses on sentence-level simplifica-
tion, and future work can extend L to evaluat-
ing paragraph- and document-level simplification.
SE dataset and L are also limited to
the English language.
Ethics Statement
We used in-house annotators to collect human
ratings in SE datasets and write simpli-
fications in SE. The annotators are
university-level undergraduate and graduate stu-
dents, including both native and non-native speak-
ers of English. We did not collect any personal
information from the annotators. We paid each
annotator $15 per hour, which is above the US fed-
eral minimum wage. We ensured that the content
shown to the annotators was not upsetting and let
them know that they could skip the task if they
felt uncomfortable at any point. We also let the
annotators know the purpose of the collected data.
The original complex sentences in the SE- datasets are from the publicly available
Wikipedia. The simplifications are either from
the existing work or human simplifications col-
lected from our annotators. We used the author-
released simplification outputs if they are available.
For T5 (base, large, and 3B) systems, we trained
our own simplification models using open-sourced
code from the Hugging Face Transformerslibrary.
Acknowledgments
We thank Nghia T. Le, Tarek Naous, Yang Chen,
and Chao Jiang as well as three anonymous review-
ers for their helpful feedback on this work. We
also thank Marcus Ma, Rachel Choi, Vishnesh J.
Ramanathan, Elizabeth Liu, Alex Soong, Govind
Ramesh, Ayush Panda, Anton Lavrouk, Vinayak
Athavale, and Kelly Smith for their help with hu-
man evaluation. This research is supported in part
by the NSF awards IIS-2144493 and IIS-2112633,ODNI and IARPA via the BETTER program (con-
tract 2019-19051600004) and the HIATUS pro-
gram (contract 2022-22072200004). The views
and conclusions contained herein are those of the
authors and should not be interpreted as necessarily
representing the official policies, either expressed
or implied, of NSF, ODNI, IARPA, or the U.S. Gov-
ernment. The U.S. Government is authorized to
reproduce and distribute reprints for governmental
purposes notwithstanding any copyright annotation
therein.
References163921639316394163951639616397A L Metric - Implementation Details
We leverage Transformers and PyTorch Light-
ning libraries to implement the metric. We used
RoBERTa model (Liu et al., 2019) as the en-
coder. We fine-tuned the metric with the style-
adaptive loss for 20 epochs and selected the check-
point with the best validation loss. We used the ten
human references from ASSET to calculate metric
scores while training L using SE .
We used a batch size of 8, Adam optimizer with a
learning rate of 3e-05 for the encoder and 1e-05 for
other layers. We use dropout of 0.15 and freeze the
encoding layers for 1 epoch. We train the model
on one Quadro RTX 6000 GPU with 25GB RAM,
which takes around 3 hours.
B Incorporating Evaluation Metrics into
Training and Inference -
Implementation Details
We implement the controllable sentence simplifi-
cation system proposed by Sheang and Saggion
(2021), a T5-base model. The input sentence is
prepended with four control tokens, namely char-
acter length ratio, dependency tree depth ratio,
character-level Levenshtein similarity, and inverse
frequency ratio, to control various aspects of the
generated simplification. During training, each
control value is calculated using the corresponding
training pair and discretized into bins of width 0.05.
During inference, we set the control tokens to the
average value of the training set. We used 0.9 for
the length ratio and 0.75 for the rest. We also fine-
tune the controllable T5-3B and T5-11B versions
using the same approach.
We use the Hugging Face Transformers library
for implementing the base model and the MRT
framework. We first train the model for 5 epochs
using MLE loss and then fine-tune it for 5 epochs
using MRT. We use Adam optimizer with a learn-
ing rate of 1e-4, linear learning rate warmup of 4k
steps, weight decay of 0.1, epsilon of 1e-8, and
batch size of 16. During MRT, we use a beam
size of 8 to generate candidates. The rest of the
parameters are left with default values from the
library. During inference, we use a beam size of 10.
Our models are trained on 2 A40 GPUs with 45GB
RAM for 48 hours.
We used the code released by Fernandes et al.
(2022)for the MBR framework. We generatedcandidates using beam search of size 100 and se-
lected the top 100 candidates for reranking. We
used the above T5 model fine-tuned using MLE to
generate the candidates. The rest of the parameters
are left with default values from the library.
C Systems in SE Datasets
Data-driven Neural Models. We use ten super-
vised models: (i) three fine-tuned T5 (Raffel et al.,
2020) models of various sizes, namely T5-base,
T5-large, and T5-3B, (ii) a controllable T5-base
model that prepends tokens to the input to con-
trol the lexical and syntactic complexity of the out-
put (Sheang and Saggion, 2021), (iii) two BART
(Lewis et al., 2020) models that also use control
tokens where one is trained on Wikipedia (Martin
et al., 2020), and the other is fine-tuned on a com-
bination of Wikipedia and web-mined paraphrases
(Martin et al., 2022), (iv) a BERT-initialized Trans-
former (Maddela et al., 2021), (v) a BiLSTM edit-
based approach that first generates the edit oper-
ations, and then the simplification (Dong et al.,
2019), (vi) a BiLSTM that directly generates sim-
plifications using reinforcement learning (Zhang
and Lapata, 2017), and (vii) a vanilla BiLSTM
model. In addition, we also include one unsuper-
vised model and one semi-supervised model by
Surya et al. (2019) that uses an auto-encoder with
adversarial and denoising losses.
Few-shot Methods. We include simplifications
generated by GPT-3.5under zero-shot and 5-shot
settings (prompts are provided in Appendix D.2).
Data-driven Statistical Methods. We incorpo-
rate two systems that applied statistical machine
translation (MT) approaches to text simplification:
(i) a phrase-based MT system that reranks the out-
puts based on their dissimilarity with the input
(Wubben et al., 2012), and (ii) a syntactic MT sys-
tem that uses paraphrase rules for lexical simplifi-
cation (Xu et al., 2016).
Rule-based Methods. Kumar et al. (2020) iter-
atively generates candidates using rules and ranks
them with a linguistic scoring function.
Hybrid Methods. We utilize three hybrid sys-
tems that combine linguistic rules with data-driven
methods: (i) Narayan and Gardent (2014) uses se-
mantic structure to predict sentence splitting and16398paraphrases with a phrase-based MT system, (ii)
Sulem et al. (2018) performs splitting and dele-
tion using linguistic rules and paraphrasing using a
BiLSTM, and (iii) Maddela et al. (2021) generates
candidates with different amounts of splitting and
deletion and then paraphrases the best candidate
with a BERT-initialized Transformer.
Naive Baseline Methods. Existing metrics are
biased towards conservative systems because their
outputs are generally fluent and exhibit high lexical
overlap with the input/reference (Pang and Gimpel,
2019; Krishna et al., 2020). We add two conser-
vative systems that are challenging for automatic
metrics: (i) a system that always copies the input
and (ii) a content-preserving but nonsensical sys-
tem that scrambles 5% of the input words.
Humans. We also add human-written simplifi-
cations using different instructions from ASSET
(Alva-Manchego et al., 2020) and TurkCorpus (Xu
et al., 2016), two widely used evaluation bench-
marks for sentence simplification, and an auto-
aligned one from the SimpleWiki (Kauchak, 2013).
D Implementation Details for
Simplification Systems
D.1 T5 Setup
We use the Hugging Face Transformers library.
We fine-tune T5-base, T5-large, and T5-3B on 4
A40 GPUs of a total batch size of 64 for 20 epochs
(10.4K steps), 16 epochs, and 8 epochs, respec-
tively. We use a learning rate of 3e-4. We save
checkpoints every 5K steps and select the best one
by performing a manual inspection on a set of 60
simplifications from the development set, result-
ing in 80K, 60K, and 25K steps for T5-base, T5-
large, and T5-3B respectively, which are in a sim-
ilar range to FLAN-T5 (Chung et al., 2022). We
use AdamW (Loshchilov and Hutter, 2017) as the
optimizer.
D.2 GPT-3.5 Setup
Hyperparameters. We use the text-davinci-003
GPT-3.5 model from OpenAI API. To generate
simplification, we use the following hyperparame-
ters: temperature=1 and top-p=0.95.
Prompts. We use the instruction from ASSET
(Alva-Manchego et al., 2021) to prompt GPT-3.5.Zero-shot setting: Please rewrite the following
complex sentence in order to make it easier to
understand by non-native speakers of English.
You can do so by replacing complex words with
simpler synonyms (i.e. paraphrasing), deleting
unimportant information (i.e. compression),
and/or splitting a long complex sentence into
several simpler ones. The final simplified sentence
needs to be grammatical, fluent, and retain the
main ideas of its original counterpart without
altering its meaning.
Input: {input}
Output:
5-shot setting: Please rewrite the following
complex sentence in order to make it easier to
understand by non-native speakers of English.
You can do so by replacing complex words with
simpler synonyms (i.e. paraphrasing), deleting
unimportant information (i.e. compression),
and/or splitting a long complex sentence into
several simpler ones. The final simplified sentence
needs to be grammatical, fluent, and retain the
main ideas of its original counterpart without
altering its meaning.
Examples:
Input: {random sampled from ASSET}
Output: {random sampled human reference of the
Input from ASSET}
Input: {random sampled from ASSET}
Output: {random sampled human reference of the
Input from ASSET}
Input: {random sampled from ASSET}
Output: {random sampled human reference of the
Input from ASSET}
Input: {random sampled from ASSET}
Output: {random sampled human reference of the
Input from ASSET}
Input: {random sampled from ASSET}
Output: {random sampled human reference of the
Input from ASSET}
Please rewrite the following complex sen-
tence in order to make it easier to understand by
non-native speakers of English. You can do so by16399replacing complex words with simpler synonyms
(i.e. paraphrasing), deleting unimportant infor-
mation (i.e. compression), and/or splitting a long
complex sentence into several simpler ones. The
final simplified sentence needs to be grammatical,
fluent, and retain the main ideas of its original
counterpart without altering its meaning.
Input: {input}
Output:16400E L metric - Additional Results
τ τ τττ τ16401F Annotation Interface
F.1 Step - 1: System Output Categorization.16402F.2 Step - 2: Highlighting System Edits.16403F.3 Step - 3: Rating and Ranking System Outputs.16404F.4 Annotation Instructions1640516406ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations section
/squareA2. Did you discuss any potential risks of your work?
Limitations section Ethics Statement section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3
/squareB1. Did you cite the creators of artifacts you used?
Section 3, Appendix sections A - D.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Sections 3 Ethics Statement section
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Sections 3 Ethics Statement section
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Ethics Statement section
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Sections 3 Ethics Statement section
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 3
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix Sections A - D16407/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3 Appendix Sections A - D
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix Sections A - D
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 3
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Appendix section F
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Sections 3 Ethics Statement section
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Ethics Statement section
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No. This study does not meet the deﬁnition of "human subjects" research by our institutional IRB
review board, as we did not: "(1) Interact with, intervene with, or obtain/access private, identiﬁable
information or data about, a living individual (includes online surveys) or (2) Conduct research on
a drug, biologic, or medical device". The annotations are linguistic judgements provided by hired
in-house employees.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Ethics Statement section16408