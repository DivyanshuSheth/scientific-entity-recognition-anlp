
Robin M. Schmidt Telmo Pessoa Pires Stephan Peitz Jonas Lööf
Apple
{robin_schmidt, telmo, speitz, jloof}@apple.com
Abstract
Non-autoregressive approaches aim to improve
the inference speed of translation models by
only requiring a single forward pass to gener-
ate the output sequence instead of iteratively
producing each predicted token. Consequently,
their translation quality still tends to be inferior
to their autoregressive counterparts due to sev-
eral issues involving output token interdepen-
dence. In this work, we take a step back and re-
visit several techniques that have been proposed
for improving non-autoregressive translation
models and compare their combined transla-
tion quality and speed implications under third-
party testing environments. We provide novel
insights for establishing strong baselines us-
ing length prediction or CTC-based architec-
ture variants and contribute standardized B,F++, and Tscores using sacreBLEU
on four translation tasks, which crucially have
been missing as inconsistencies in the use of
tokenized B lead to deviations of up to 1.7
B points. Our open-sourced code is inte-
grated into fairseq for reproducibility.
1 Introduction
Traditional sequence-to-sequence models aim to
predict a target sequence e=e, . . . , e, . . . , e
of length Igiven an input sequence f=
f, . . . , f, . . . , fof length J. In the autoregres-
sive case, this is done token by token, and the prob-
ability distribution for the output at timestep iis
conditioned on the source sentence fbut also
on the preceding outputs of the model e, and
parameterized by θ:
p(e|f) =/productdisplayp(e|e, f). (1)
Even though these types of models are widely de-
ployed, one of the major drawbacks is the inherentleft-to-right factorization that requires iterative gen-
eration of output tokens, which is not efficiently
parallelizable on modern hardware such as GPUs or
TPUs. Non-autoregressive translation, on the other
hand, assumes conditional independence between
output tokens, allowing all tokens to be generated
in parallel. Effectively, it removes the dependence
on the decoding history for generation:
p(e|f) =/productdisplayp(e|f). (2)
This modeling assumption, despite its computa-
tional advantages, tends to be more restrictive and
introduces the multimodality problem (Gu et al.,
2018) where consecutive output tokens are repeated
or fail to correctly incorporate the preceeding in-
formation to form a meaningful translation. Over-
coming this problem is one of the key challenges
to achieving parity with autoregressive translation
on a wide variety of tasks and architectures.
Closest to the presented work is a study by Gu
and Kong (2021) that also analyzes recent works
regarding their effectiveness and successfully com-
bines them to achieve remarkable translation qual-
ity. In this work, we additionally address sev-
eral shortcomings in the literature: 1) standardized
B,F++, and Tscores using sacreBLEU
accompanied by open source fairseq code; 2)
more realistic non-autoregressive speed-up expec-
tations, by comparing against faster baselines; 3)
a call for clarity in the community regarding data
pre-processing and evaluation settings.
2 Experimental setup
Datasets & knowledge distillation We per-
form our experiments on two datasets: WMT’14
English ↔German ( EN↔DE, 4M sentence pairs),
and WMT’16 English ↔Romanian ( EN↔RO,
610K sentence pairs), allowing us to evaluate on
4translation directions. As is common practice in2785the non-autoregressive literature, we train our mod-
els with sequence-level knowledge distillation (KD,
Kim and Rush, 2016), obtained from Transformer
base teacher models (Vaswani et al., 2017) with
a beam size of 5. We tokenize all data using the
Moses tokenizer and also apply the Moses scripts
(Koehn et al., 2007) for punctuation normalization.
Byte-pair encoding (BPE, Sennrich et al., 2016) is
used with 40,000merge operations.
ForEN↔RO, we use WMT’16 provided scripts
to normalize the ROside, and to remove diacritics
forRO→ENonly. EN→ROkeeps diacritics for
producing accurate translations which explains the
gap between our numbers and those claimed in
previous works (Gu and Kong, 2021; Qian et al.,
2021), who besides computing B on tokenized
text, compared on diacritic-free text. More details
are summarized in Appendix D.1.
Models In this work, we focused our efforts
on four models/techniques for training non-
autoregressive transformer (NAT) models: Vanilla
NAT (Gu et al., 2018), Glancing Transformer
(GLAT, Qian et al., 2021), Connectionist Temporal
Classification ( CTC , Graves et al., 2006; Libovický
and Helcl, 2018), and Deep Supervision (DS,
Huang et al., 2022), see Appendices A and B for a
short overview. For all NAT models, we use learned
encoder and decoder positional embeddings, shared
word embeddings (Press and Wolf, 2017), no label-
smoothing, keep A betas and epsilon as de-
faults with (0.9,0.98)and1e−8respectively, train
for200k (EN↔DE) /30k (EN↔RO) update steps
with10k /3k warmup steps using an inverse square
root schedule (Vaswani et al., 2017) where the 5
best checkpoints are averaged based on validation
B. For CTC -based models, the source upsam-
pling factor is 2. More details can be found in our
open-sourced training procedures. We intentionally
did not fine-tune hyperparameters for each trans-
lation direction and instead opted for choices that
will most likely transfer across datasets.
For autoregressive transformer (AT) models, we
focus on the base (Vaswani et al., 2017) architec-
ture, as well as a deep encoder ( 11layers) & shal-
low decoder ( 2layers) variant (Kasai et al., 2021).
3 A call for clarity
From our experiments with different algorithms,
we noticed a few problems that are currently not
properly addressed in the literature. These issues
make it harder to do a rigorous comparison, sohere, we explain how we addressed them, and give
recommendations for future work.
3.1 Evaluation: The return of sacreBLEU
In many works, the method for comparing novel
non-autoregressive translation models is tokenized
B (Papineni et al., 2002). However, this can
cause incomparable translation quality scores as
the reference processing is user-supplied and not
metric-internal . Well known problems with user-
supplied processing for B include different tok-
enization and normalization schemes applied to the
reference across papers that can cause B devi-
ations of up to 1.8 B points (Post, 2018). For
the NAT literature, in particular, the basis for com-
parison has been the distilled data released by Zhou
et al. (2020) where binarizationis handled by the
individual researchers. One instance for such devi-
ations can be found for the B computations on
WMT’14 EN→DEof Huang et al. (2022) using
processed references instead of the original ones.
Applying a vocabulary obtained from the distilled
training data for processing the references results
in<unk> tokens appearing in the references for
the German left ( „) and right ( “) quotation marks
because these tokens have not been generated dur-
ing the distillation process. However, using such
processed references leads to artificially inflated
B scores. We evaluated our models and open-
sourced models of previous works using processed
references, original references, and sacreBLEU and
observe deviations of up to 1.7 B points across
multiple previous works. This fact, together with
arbitrary optimization choices that can not be ac-
credited to the presented method, hinders mean-
ingful progress in the field by making results in-
comparable across papers without individual re-
implementation by subsequent researchers.
Our approach To alleviate these problems, we
urge the community to return to sacreBLEU(Post,
2018) for evaluation. We provide standardized
B (Papineni et al., 2002),F++ (Popovi ´c,
2017), and T(Snover et al., 2006) scores in Ta-
bles 1, A3, and A4 for current state-of-the-art non-
autoregressive models that are reproducible with
our codebase, as well as easy to configure through
flags, and can be used as baselines for novel model
comparisons. No re-ranking is used for all numbers.2786
See Appendix D.2 for the used sacreBLEU evalua-
tion signatures. Statistically non-significant results
(p≥0.05) are marked withfor all main results
using paired bootstrap resampling (Koehn, 2004).
For more information on how we select base and
reference systems for these statistical tests, please
see Appendix D.3. As such, we directly try to ad-
dress the issues found by Marie et al. (2021) for
the non-autoregressive translation community.
3.2 Establishing a realistic AT speed baseline
Traditionally, speed comparisons in the NAT litera-
ture compare to weak baselines in terms of achiev-
able autoregressive decoding speed which leads to
heavily overestimated speed multipliers for non-
autoregressive models which has been recently crit-
icized by Heafield et al. (2021).
Our approach We deploy a much more competi-
tive deep encoder ( 11layers) & shallow decoder ( 2
layers) (Kasai et al., 2021) autoregressive baseline,
with average attention (AA, Zhang et al., 2018)
and shortlists (SL, Schwenk et al., 2006; Junczys-
Dowmunt et al., 2018b). It achieves a similar ac-
curacy with comparable number of parameters in
comparison to the base architecture (see Table 1).
Our latency measurements (mean processing
time per sentence over 3runs) were collected using
a single NVIDIA A100 GPU (Latency GPU) ora single-threaded Intel Xeon Processor (Skylake,
IBRS) @ 2.3 GHz (Latency CPU), both with batch
size of 1which faithfully captures the inference on
a deployed neural machine translation model.
3.3 Establishing an accurate NAT baseline
Hyperparameters and smaller implementation de-
tails are often not properly ablated and only copied
between open sourced training instructions, caus-
ing ambiguity for researchers on which parts of
the proposed approach actually make a difference.
Such choices include the decoder inputs and many
more optimization configurations such as dropout
values, initialization procedures, and activation
functions. Here, we provide our insights from ex-
perimenting with these variants.
3.3.1 No source embeddings as decoder input
It is common in NAT models to feed the source em-
beddings as decoder inputs using either Uniform-
Copy ,SoftCopy (Wei et al., 2019), or sometimes
even more exotic approaches without any details
such as an “attention version using positions” (Qian
et al., 2021), in this paper referenced as PositionAt-
tention . While it seems to be common consensus
that using any of these variants benefits the final
translation quality of the model, our experiments
with these have shown the clear opposite when
compared to simply feeding the embedding of a2787
special token such as <unk> (see Table 2).
In our experiments, we could not discern any
consistent improvement on the final translation
quality from any of the methods. For SoftCopy ,
the learned temperature parameter has a strong in-
fluence on translation quality and is hard for the
network to train. Notably, for Vanilla NAT and
GLAT the effects of adding any such decoder input
is much more severe than for CTC -based models.
One of the only cases where we see improvement is
when adding PositionAttention to the Vanilla NAT
model but since it does not show consistent B
gains for GLAT andCTC models, we do not in-
vestigate this any further. Looking at the validation
losses from Figure 1, one can observe higher fluc-
tuations for any variant of guided decoder input
compared to disabling it.
Interestingly, for a setup without guided decoder
input, especially the token embedding as well as
the self-attention component in the first decoder
layer supposedly contain only little information
since only consecutive special tokens i.e. <unk> ’s
are passed. We investigate the performance con-
tribution of these in Table 3 and observe that 1)
the self-attention component in the first decoder
layer only has minor impact and 2) solely relying
on learned positional embeddings is sufficient for
achieving comparable performance. However, we
do keep both components in the main results for
comparability reasons.
3.3.2 Influence of dropout
Dropout is one of the hyperparameters which either
prior works did not mention or simply used the de-
fault values from previous studies. We performed
a sweep for different architectures and language
pairs, where we varied the dropout probability from
0to0.5, with increments of 0.1(see Figure A1).
We observe that choosing an appropriate dropout
rate is important for achieving high translation qual-
ity. In particular, for EN↔DE,0.1achieves the
best validation performance for all models, con-
firming the choice in prior works. For EN↔RO,
we find that the value used in prior works, 0.3, is
optimal for non- CTC models, but, for CTC -based
approaches, higher values seem to outperform.
3.3.3 Miscellaneous optimization choices
Many previous works use the Gaussian Error Lin-
ear Unit (GELU, Hendrycks and Gimpel, 2016)
activation function, initialize the parameters with
the procedure from BERT (Devlin et al., 2019),
use length offset prediction, or a larger A
(Kingma and Ba, 2015) ϵvalue for optimization.
In Appendix C.2 we summarize the effect of these
choices and find that for Vanilla NAT models these
variations regularly have a positive impact on trans-
lation quality but for CTC -based models the pic-
ture is less clear and rarely leads to consistent
B improvements. Based on these results, we
deploy all choices besides ϵtuning for non- CTC
variants but omit them for CTC-based models.
3.4 Model comparison
Our results are consistent across the language pairs
and metrics shown in Tables 1, A3, and A4, and
align with former work (Gu and Kong, 2021). The
most significant translation quality improvement
can be achieved by switching from the traditional2788
length prediction module to a CTC -based vari-
ant, confirming the strong dependence on accu-
rate length predictions (Wang et al., 2021). This
amounts to the largest accuracy increase that is
achievable with current non-autoregressive meth-
ods but comes with a slight CPU latency increase
due to the upsampling process. On top of this,
GLAT consistently increases translation quality
further by a significant margin. Deep Supervision
can further increase the quality but, for the most
part, not by a significant amount. Notably, though,
it rarely hurts accuracy for our experiments across
all three metrics and can sometimes even achieve a
significant increase on top of CTC (RO→ENon
all three metrics and EN↔DEonT). See Ap-
pendix E for analysis on the predicted translations.
Comparing the strongest AT and NAT system in
Table 4, we can see that despite B parity on
WMT’14 DE→EN, the C scores (Rei et al.,
2020)and our human evaluation results still indi-
cate a quality gap between the two. Hence, metrics
beyond B are vital for rating NAT quality.
In terms of expected inference speed for non-
autoregressive models, we believe that our latency
numbers are more realistic than what is commonly
reported in the literature. By considering addi-
tional speed-up techniques, improvements shrink
to roughly 7.0×on GPU and around 2.5×on CPU.
4 Conclusion
In this work, we provide multiple standardized
sacreBLEU metrics for non-autoregressive ma-
chine translation models, accompanied by repro-
ducible and modular fairseq code. We hope that
our work solves comparison problems in the litera-
ture, provides more realistic speed-up expectations,
and accelerates research in this area to achieve par-
ity with autoregressive models in the future.2789Limitations
While our work improves upon the state of transla-
tion quality and speed comparisons in the NAT lit-
erature, we acknowledge that there are many more
low-level optimizations (e.g. porting the Python
code to C++and writing dedicated CUDA kernels)
that could be made to improve the presented work.
This would allow for an even more realistic infer-
ence speed comparison and enable direct compa-
rability to e.g. Marian (Junczys-Dowmunt et al.,
2018a) which is currently not given. There exists a
concurrent work by Helcl et al. (2022) that tries to
address some of these shortcomings by providing
aCTC implementation in C++and comparing the
inference speed across different batching scenarios.
Apart from this limitation specific to our work,
current non-autoregressive models still have a
strong dependency on appropriate knowledge dis-
tillation from an autoregressive teacher that is able
to effectively reduce data modes and limit the mul-
timodality problem. While this already shows to
be an important factor for achieving good trans-
lation quality on benchmarking datasets such as
WMT, it is even more relevant for larger and/or
multilingual datasets (Agrawal et al., 2021) that
tend to be of higher complexity. Even though non-
autoregressive models can lead to improved infer-
ence speed, this strong dependency still requires
additional effort for tuning effective teacher archi-
tectures, conducting multiple training procedures,
and generating simplified data through one or more
distillation rounds. Until these problems are solved,
it seems unrealistic that the average practitioner,
who does not have access to a lot of compute, can
effectively deploy these in production settings as
general-purpose translation models. Apart from
that, there is still more work needed in reducing
the multimodality problem through more suitable
architectures, optimization objectives, or a better
data selection process.
Acknowledgements
We would like to thank Matthias Sperber for coor-
dinating the publication process and Yi-Hsiu Liao
for providing the Shortlist implementation. Further,
we’d like to thank Sarthak Garg, Hendra Setiawan,
Luke Carlson, Russ Webb, and Jesse Allardice for
their helpful comments on the manuscript. Many
thanks to Sachin Mehta and David Harrison for
suggesting interesting ideas and the rest of Apple’s
Machine Translation Team for their support.References279027912792
A Related work
Many works have tried to accomplish the goal
of autoregressive translation quality with a
non-autoregressive model in recent years. The
first paper to outline the vanilla non-autoregressive
model for machine translation was Gu et al. (2018).
After that, a few branches of work emerged that
either focus on the multimodality problem (Ma
et al., 2019; Ding et al., 2020; Qian et al., 2021;
Ran et al., 2021; Bao et al., 2021; Song et al., 2021;
Huang et al., 2022), on improving the knowledge
transfer from autoregressive to non-autoregressive
models (Li et al., 2019; Wei et al., 2019; Sun
and Yang, 2020; Zhou et al., 2020; Guo et al.,
2020; Hao et al., 2021; Xu et al., 2021), on
applying different training objectives to train the
non-autoregressive model (Libovický and Helcl,
2018; Ghazvininejad et al., 2020; Du et al., 2021;
Shao et al., 2021), or on incorporating language
models to boost accuracy (Su et al., 2021).2793There is also a branch of work that fo-
cuses on semi-autoregressive models that try to
find a balance between autoregressive and non-
autoregressive models to leverage the benefits of
both approaches (Stern et al., 2018; Lee et al., 2018;
Gu et al., 2019; Ghazvininejad et al., 2019; Kasai
et al., 2020; Chan et al., 2020). However, since
they often mitigate the expected speed-up gains,
we do not consider them in this work.
B Methods
In this section, we want to highlight some of the
building blocks we used and explain their usage
and configuration in our models.
B.1 Glancing Transformers
The Glancing Transformer (GLAT, Qian et al.,
2021) tackles the multimodality problem by en-
couraging word interdependence through glancing
sampling . In that, they adaptively select a number
of ground truth tokens by comparing them to the
model predictions ˆeand feeding the embeddings
of the selected tokens to the decoder during train-
ing. Formally, the probability distribution can be
described as
p(e|f) =/productdisplayp(e|Φ(e,ˆe), f),(3)
where Φ(e,ˆe)represents the subset of tokens
selected by the glancing sampling strategy. This
sampling process can be split into two parts. First,
determining the number of sampled tokens Sby
comparing eandˆe, here defined by the function
Γ(e,ˆe). Secondly, selecting Stokens from the
ground truth that form the final output of the glanc-
ing sampling. Qian et al. (2021) deploy a random
function for this purpose which we adopt and de-
fine here as the function Ω(e, S). Combining all
these, the sampling strategy can be formulated as
Φ(e,ˆe) = Ω( e,Γ(e,ˆe)), (4)
with the number of sampled tokens Sbeing deter-
mined by Γ(e,ˆe) =λ·d(e,ˆe)where d(e,ˆe)
is a distance measure between the ground truth e
and the predicted tokens ˆewhile λis a hyperpa-
rameter. For this distance measure, there are many
choices that can be deployed in theory such as the
Levenshtein distance (Levenshtein, 1966) or, with
slight adaptions, one of the Bregman divergences(Banerjee et al., 2005). For their experiments, Qian
et al. (2021) choose the Hamming distance (Ham-
ming, 1950) defined as d(e,ˆe) =/summationtexte̸= ˆe
which has the useful property that it takes into ac-
count the current prediction quality of the model,
causing the number of sampled tokens Sto be
high initially and decrease over time. In their
implementation, they additionally deploy a lin-
early decreasing schedule with a lower bound i.e.
λ=λ−λ·where uis the current and Uis the
maximum number of update steps with λ= 0.5
andλ= 0.2to further control the number of
sampled tokens. We adopt their configuration.
B.2 Connectionist Temporal Classification
Traditional sequence-to-sequence models output
target tokens autoregressively until an end-of-
sentence token is encountered and are trained us-
ing the standard cross-entropy loss. In a non-
autoregressive setting, though, the number of to-
kens in the output is not known a priori . One
solution is to train a length prediction module to
determine the number of output tokens, but this
approach still has drawbacks, as it doesn’t properly
address token repetition and the predicted length
can easily be either too short or too long. The qual-
ity of the length prediction module is influenced by
properties of the respective language pairs such as
alignment patterns, word orders or intrinsic length
ratios (Wang et al., 2021). These fluctuations in
accuracy are especially influential on the final trans-
lation quality (Wang et al., 2021).
The Connectionist Temporal Classification
(CTC , Graves et al., 2006; Libovický and Helcl,
2018) approach tries to improve this property by
setting a large enough output length ˜Jand giving
the model the option to flexibly adjust the target
offsets using an additional <blank> token. Specif-
ically, by marginalizing all possible alignments
using dynamic programming, for all aligned se-
quences athat can be reduced to the target e, the
conditional probability corresponds to
p(e|f) =/summationdisplayp(a|f)
=/summationdisplay/productdisplayp(a|f)(5)
where Ais a one-to-many map that produces all
possible alignments aandAis a many-to-one2794map that collapses an alignment to recover the tar-
get sequence by first removing repeated and then
<blank> tokens e.g.
A(a−abb/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright) =A(aa−ab/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright) =aab/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright(6)
with−representing the <blank> token for brevity.
This approach requires setting an alignment length
˜Jthat is determined by a scaling factor scom-
monly chosen to be 2–3times the source length
J, i.e. ˜J=J·s. In the example of Equation (6),
the alignment length is ˜J= 5.
Combining with GLAT To effectively combine
CTC with GLAT , an alignment for the target to-
kens is required to enable the glancing sampling
correspondence with the decoder input tokens. For
that, Gu and Kong (2021) suggest using the Viterbi
aligned tokens
ˆa(f) = arg maxp(a|f) (7)
which we adopt in our implementation and obtain
leveraging the best_alignment method from the
Imputer(Chan et al., 2020). Similar to previous
works, we use greedy decoding instead of beam
search for all our experiments to keep a strict non-
autoregressive property.
B.3 Deep Supervision
The Deeply Supervised Layer-wise Prediction-
aware Transformer (DSLP, Huang et al., 2022) in-
troduces three strategies to address the multimodal-
ity problem . Namely, Deep Supervision, Layer-
Wise Prediction-Awareness, and Mixed Training.
In a nutshell, Deep Supervision refers to predicting
the output tokens at every decoder layer, comput-
ing the layer-wise losses, and aggregating them to
form the overall loss used for optimizing the model.
Formally, we can maximize the log-likelihood
L=1
L/summationdisplay/summationdisplaylogp(e|h), (8)
where his the hidden state of decoder layer l
corresponding to position iandLis the overall
number of decoder layers. Intuitively, this approach
loses its value when moving to a deep encoder and
shallow decoder (Kasai et al., 2021) with L= 1.While Deep Supervision keeps the model param-
eter count constant and only increases the used
memory, the other two proposed techniques in-
crease the number of trained parameters. For this
work, we only adopt the Deep Supervision part due
to the aforementioned drawback.
C Ablations
C.1 Influence of dropout
Figure A1 shows the B score plotted against
the dropout rate for all translation directions and
multiple architectures. In general, our results agree
with the choices of prior work with 0.1performing
best for EN↔DE, and 0.3forEN↔RO. For the
CTC -based methods, higher dropouts seem to give
slightly better results.
C.2 Optimization choices
We conduct experiments to assess the value of more
optimization choices in Table A1. From these, it
seems that most of them seem to have a positive
impact when being deployed for non- CTC vari-
ants while they don’t seem to benefit CTC -based
variants.
Table A1 suggests that choosing a slightly higher
A ϵmight have a positive effect for training
non-autoregressive models. To this end, we ran a
grid search in the range [1e−9,1e−1]with a delta
factor between samples of ∆ = 10forCTC +
GLAT , one of our top-performing models, to verify
the observed gains. The results of this experiment
can be seen in Table A2.2795
From our results we conclude that the observed
gains are neither consistent nor statistically signif-
icant as they don’t transfer to the CTC +GLAT
model without further tuning of the remaining hy-
perparamaters such as the learning rate ηor the
A momentum terms βandβ. To simplify
hyperparameter transfer to evolving production
datasets and randomness in the training procedure
without any additional tuning, we keep the default
ofϵ= 1e−8(similar to Schmidt et al., 2021) for
our models that use a combination of methods such
asCTC +GLAT orCTC +DSand advise the
community to avoid this type of tuning for compa-
rability. While it may seem appealing at first, it candistort expected translation quality gains for novel
methods as commonly this sort of tuning is only
conducted for the introduced method at hand but
not, or only marginally, for baselines or competing
algorithms. Any gain that is achieved through such
practices should notbe accredited to the introduced
method as other algorithms are most likely able to
achieve similar gains through an equal amount of
hyperparameter tuning.
D Result details
D.1 Dataset
We preprocessed the datasets using Moses scripts
(Koehn et al., 2007) to tokenize all datasets,
clean the data ( clean-corpus-n.pl ), and normal-
ize punctuation ( normalize-punctuation.perl ).
No True-Casing is used. For RO→ENwe nor-
malize Romanian and remove diacritics using
WMT’16 provided scripts.EN→ROkeeps di-
acritics for producing accurate translations which
explains the gap between our numbers and those
claimed in previous works (Gu and Kong, 2021;
Qian et al., 2021; Gu et al., 2018), who besides
computing B on tokenized text, compared on
diacritic-free text.
For ablations, which are only conducted on2796WMT’14 EN→DEdistilled training data, we addi-
tionally report accuracy on the WMT’13 EN→DE
evaluation set, serving as validation performance.
We use a shared source and target vocabulary for
all language directions, obtained using BPE with
40k merge operations where the distillation process
dropped around 400−800tokens depending on
the respective target language.
D.2 Signatures: sacreBLEU
We use sacreBLEU version 2.0.0. The evaluation
signatures for B (Papineni et al., 2002) are
nrefs:1 | case:mixed | eff:no | tok:13a |
smooth:exp . Additionally we also reportF++
(Popovi ´c, 2017) with nrefs:1 | case:mixed |
eff:yes | nc:6 | nw:2 | space:no and T
(Snover et al., 2006) with nrefs:1 | case:mixed |
tok:tercom | norm:no | punct:yes | asian:no
for the main results.
D.3 Paired boostrap resampling
For the boostrap resampling used to identify non-
significant results in the main tables, we always
use the root level of the sub-blocks (separated by
dashed lines) as base system within that specific
sub-block. For models that deploy multiple ad-
ditional methods, e.g. CTC +GLAT +DS, we
always use the previous row as the base system,
e.g.CTC +GLAT in this case. This procedure is
especially relevant for the significance tests on the
autoregressive baselines that use knowledge distil-
lation, average attention, and shortlists and helps
to show the significance of the individual methods
on the translation quality. While we want signifi-
cant results for the NAT category, it is beneficial
to have non-significant results for the methods that
are deployed to speed-up CPU inference (i.e. av-
erage attention and shortlists) as this shows that
the respective model is not statistically different
while offering faster CPU inference. Finally, for
the root levels of each sub-block we use the first
sub-block of each category as the base system i.e.
both CTC andCTC (11-2) are compared to Vanilla-
NAT while Transformer base (11-2) is compared
to Transformer base. The null hypothesis is al-
ways that the reference system and the baseline
translations are essentially generated by the same
underlying process. If they are marked with, the
null hypothesis could not be rejected.
Additional Metrics Both Tables A3 and A4,
which reportF++ andT, support our con-clusions drawn in Section 3.4 of the main paper, sta-
tistical significance sometimes deviates marginally.
E Analysis
Hypotheses characteristics Comparing one of
the best performing NAT models, CTC +GLAT
(11-2) to its autoregressive counterpart Transformer
base (11-2), we try to understand the nature of oc-
curring errors and their similarity. For that, we
compare the Levenshtein distance between each
models’ hypotheses and the reference as well as
their hypotheses with each other in Figure A2
on the WMT’14 DE→EN test set. Both AT
and NAT follow a similar distribution of distance
when compared to the reference. However, when
comparing their hypothesis to each other, only
351/3003 hypotheses match exactly and often we
can observe Levenshtein distances between 0−50.
If we compare their sentence embeddings using
BERT from sentence-transformers (Reimers
and Gurevych, 2019) in Figure A3, we observe
a similar trend when comparing them to the ref-
erence, although the autoregressive model is able
to achieve slightly higher cosine similarity. Look-
ing at samples with low cosine similarity between
AT and NAT, we observe NAT problems that have
been pointed out by prior work including repeated
tokens and non-coherent sentences.
Looking at the effect of sentence length on
B in Figure A4, we observe that for sequence
lengths <40the autoregressive models outper-
form the non-autoregressive models while for se-
quence lengths >40we can see CTC +GLAT2797
outperforming. We attribute this to the significantly
smaller sample size for longer sentences.
Human Evaluation In addition to comparing the
best AT model against the best NAT model based
on automatic metrics such as B,F++,TandC , we conducted a human evaluation. To
determine the human preference between AT and
NAT model output, we showed professional trans-
lators both translations together with the source
sentence in a side-by-side acceptability evaluation.
We asked the translators to provide an acceptability2798
score between 0 (nonsense) and 100 (perfect trans-
lation) for each translation. As a guideline, which
was provided to the translators, a score higher than
66 means the translation retains core meaning with
minor mistakes. Scores lower or equal to 66 should
be assigned to translations which preserve only
some meaning with major mistakes. The transla-
tors further had the option to provide comments
about why they assigned a specific score and/or
preferred a certain translation. Each of the 3003
sentences in the WMT’14 test sets was annotated
once.2799