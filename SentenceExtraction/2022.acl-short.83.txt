
Guillaume Le Berre, Christophe Cerisara, Philippe Langlais, Guy LapalmeUniversity of Lorraine, CNRS, LORIA, FranceRALI/DIRO, University of Montreal, Canada
{leberreg, felipe, lapalme}@iro.umontreal.ca, cerisara@loria.fr
Abstract
Pre-trained models have shown very good per-
formances on a number of question answering
benchmarks especially when fine-tuned on mul-
tiple question answering datasets at once. In
this work, we propose an approach for generat-
ing a fine-tuning dataset thanks to a rule-based
algorithm that generates questions and answers
from unannotated sentences. We show that the
state-of-the-art model UnifiedQA can greatly
benefit from such a system on a multiple-choice
benchmark about physics, biology and chem-
istry it has never been trained on. We further
show that improved performances may be ob-
tained by selecting the most challenging distrac-
tors (wrong answers), with a dedicated ranker
based on a pretrained RoBERTa model.
1 Introduction
In the past years, deep learning models have greatly
improved their performances on a large range of
question answering tasks, especially using pre-
trained models such as BERT (Devlin et al., 2019),
RoBERTa (Liu et al., 2019) and T5 (Raffel et al.,
2020). More recently, these models have shown
even better performances when fine-tuned on mul-
tiple question answering datasets at once. Such a
model is UnifiedQA (Khashabi et al., 2020), which,
starting from a T5 model, is trained on a large
number of question answering datasets including
multiple choices, yes/no, extractive and abstractive
question answering. UnifiedQA is, at the time of
writing, state-of-the-art on a large number of ques-
tion answering datasets including multiple-choice
datasets like OpenBookQA (Mihaylov et al., 2018)
or ARC (Clark et al., 2018). However, even if Uni-
fiedQA achieves good results on previously unseen
datasets, it often fails to achieve optimal perfor-
mances on these datasets until it is further fine-
tuned on dedicated human annotated data. This
tendency is increased when the target dataset deals
with questions about a very specific domain.One solution to this problem would be to fine-
tune or retrain these models with additionnal hu-
man annotated data. However, this is expensive
both in time and resources. Instead, a lot of work
has been done lately on automatically generating
training data for fine-tuning or even training com-
pletely unsupervised models for question answer-
ing. One commonly used dataset for unsuper-
vised question answering is the extractive dataset
SQUAD (Rajpurkar et al., 2016). Lewis et al.
(2019) proposed a question generation method for
SQUAD using an unsupervised neural based trans-
lation method. Fabbri et al. (2020) and Li et al.
(2020) further gave improved unsupervised perfor-
mances on SQUAD and showed that simple rule-
based question generation could be as effective as
the previously mentioned neural method. These
approches are rarely applied to multiple-choice
questions answering in part due to the difficulty
of selecting distractors. A few research papers
however proposed distractor selection methods for
multiple-choice questions using either supervised
approaches (Sakaguchi et al., 2013; Liang et al.,
2018) or general purpose knowledge bases (Ren
and Q. Zhu, 2021).
In this paper, we propose an unsupervised pro-
cess to generate questions, answers and associated
distractors in order to fine-tune and improve the per-
formance of the state-of-the-art model UnifiedQA
on unseen domains. This method, being unsuper-
vised, needs no additional annotated domain spe-
cific data requiring only a set of unannotated sen-
tences of the domain of interest from which the
questions are created. Contrarily to most of the
aforementioned works, our aim is not to train a
new completely unsupervised model but rather to
incorporate new information into an existing state-
of-the-art model and thus to take advantage of the
question-answering knowledge already learned.
We conduct our experiments on the SciQ
dataset (Welbl et al., 2017). SciQ contains multiple-732Question:
What type of organism is commonly used in
preparation of foods such as cheese and yogurt?
(A) mesophilic organisms (B) protozoa
(C) gymnosperms (D) viruses
Support text:
Mesophiles grow best in moderate temperature,
typically between 25C and 40C (77F and
104F). Mesophiles are often found living in or
on the bodies of humans or other animals. The
optimal growth temperature of many pathogenic
mesophiles is 37C (98F), the normal human
body temperature. Mesophilic organisms have
important uses in food preparation, including
cheese, yogurt, beer and wine.
choice questions (4 choices) featuring subjects cen-
tered around physics, biology and chemistry. An
example of question can be found in Figure 1.
We focus on the SciQ dataset because it has not
yet been used for training UnifiedQA and it re-
quires precise scientific knowledge. Furthermore,
our experiments reveal that the direct application
of UnifiedQA on the SciQ benchmark leads to a
much lower performance than when fine-tuning it
on the SciQ training set (see Section 4). Our ob-
jective in this work is to solve this gap between
UnifiedQA and UnifiedQA fine-tuned on super-
vised data with the unsupervised question genera-
tion approach described in Section 2. We addition-
ally test our method on two commonly used multi-
ple choice question answering datasets: Common-
senseQA (Talmor et al., 2019) and QASC (Khot
et al., 2020). These datasets contain questions with
similar domains to SciQ even though the questions
are slightly less specific. Furthermore, neither of
them has been used during the initial training of
UnifiedQA.
2 Question Generation Method
We propose a method for generating multiple-
choice questions in order to fine-tune and improve
UnifiedQA. This process is based on 3 steps. First,
a set of sentences is being selected (Section 2.1)
from which a generic question generation system is
applied (Section 2.2). Then a number of distractors
are added to each question (Section 2.3).Dataset Sentences Questions
SciQ data 53 270 77 873
SciQ data (train only) 45 526 66 552
Wikipedia data 45 327 62 848
2.1 Sentence Selection
Our question generation method uses a set of unan-
notated sentences from which the questions will be
generated. We compare three selection methods.
First, we consider a scenario where the applica-
tion developer does not manually collect any sen-
tence, but simply gives the name (or topic) of the
target domain. In our case, the topics are “Physics”,
“Biology” and “Chemistry” since these are the main
domains in SciQ. A simple information retrieval
strategy is then applied to automatically mine sen-
tences from Wikipedia. We first compute a list
of Wikipedia categories by recursively visiting all
subcategories starting from the target topic names.
The maximum recursion number is limited to 4. We
then extract the summary (head paragraph of each
Wikipedia article) for each of the articles matching
the previously extracted categories and subcate-
gories. We only keep articles with more than 800
average visitors per day for the last ten days (on
April 27, 2021), resulting in 12 656 pages.
The two other selection methods extract sen-
tences from SciQ itself and therefore are not en-
tirely unsupervised but rather simulate a situation
where we have access to unannotated texts that
precisely describe the domains of interest such as
a school book for example. The SciQ dataset in-
cludes a support paragraph for each question (see
Figure 1). Pooled together, these support para-
graphs provide us with a large dataset of texts about
the domains of interest. We gather the paragraphs
corresponding to all questions and split them into
sentences to produce a large set of sentences that
are no longer associated with any particular ques-
tion but cover all the topics found in the questions.
We compare two different setups. In the first one,
we include all the sentences extracted from the
train, validation and test sets thus simulating a per-
fect selection of sentences that cover all the knowl-
edge expressed in the questions. Still, we only
use the support paragraphs and not the annotated
questions themselves. As compared to the classical733
supervised paradigm, this setting removes all anno-
tation costs for the application developer, but it still
requires to gather sentences that are deemed useful
for the test set of interest. We then compare this
setup with another one, where only the sentences
from the train set are included. This scenario ar-
guably meets more practical needs since it would
suffice to gather sentences close to the domain of
interest. The number of sentences for each dataset
is presented in Table 1.
2.2 Questions Generation
The generation of questions from a sentence re-
lies on the jsRealB text realizer (Lapalme, 2021)
which generates an affirmative sentence from a con-
stituent structure. It can also be parameterized to
generate variations of the original sentence such
as its negation, its passive form and different types
of questions such as who,what ,when , etc. The
constituency structure of a sentence is most often
created by a user or by a program from data. In this
work, it is instead built from a Universal Depen-
dency (UD) structure using a technique developed
forSR’19 (Lapalme, 2019). The UD structure of aQuestion:
What often is found living in or on the bodies
of humans or other animals?
Right answer: mesophile
Random distractors:
(A) the most magnetic material in nature
(B) this energy
(C) climate
Refined distractors:
(A) carbohydrates
(B) small cell fragments called platelet
(C) echinoderm
sentence is the result of a dependency parse with
Stanza (Qi et al., 2020). We thus have a pipeline
composed of a neural dependency parser, followed
by a program to create a constituency structure used
as input for a text realizer, both in JavaScript. Used
without modification, this would create a complex
echo program for the original affirmative sentence,
but by changing parameters, its output can vary.
In order to create questions from a single con-
stituency structure, jsRealB uses the classical gram-
mar transformations: for a who question, it re-
moves the subject (i.e. the first noun phrase before
the verb phrase), for a what question, it removes
the subject or the direct object (i.e. the first noun
phrase within the verb phrase); for other types of
questions ( when ,where ) it removes the first prepo-
sitional phrase within the verb phrase. Depending
on the preposition, the question will be a when or
awhere . Note that the removed part becomes the
answer to the question.
In order to determine which questions are appro-
priate for a given sentence, we examine the depen-
dency structure of the original sentence and check
if it contains the required part to be removed before
parameterizing the realization. The generated ques-
tions are then filtered to remove any question for
which the answer is composed of a single stopword.
Table 1 shows the number of questions generated
for each dataset. An example of a synthetic ques-
tion is shown in Figure 3.7342.3 Distractors Selection
Since SciQ is a multiple-choice dataset, we must
add distractors to each question we generate, to
match the format of SciQ. A simple solution to this
problem is to select random distractors among an-
swers to other similar questions generated from the
dataset of sentences we gathered. Obviously, select-
ing random distractors may lead to a fine-tuning
dataset that is too easy to solve. Therefore, we
propose another strategy that selects hard distrac-
tors for each question. To do so, starting from our
synthetic dataset with random distractors, we fine-
tune RoBERTa (Liu et al., 2019) using the standard
method of training for multiple choices question
answering. Each pair question/choice is fed to
RoBERTa and the embedding corresponding to the
first token (“[CLS]”) is given to a linear layer to
produce a single scalar score for each choice. The
scores corresponding to every choice for a given
question are then compared to each other by a soft-
max and a cross-entropy loss. With this method,
RoBERTa is trained to score a possible answer for a
given question, based on whether or not it is a cred-
ible answer to that question. For each question, we
then randomly select a number of candidate distrac-
tors from the answers to other questions and we use
our trained RoBERTa to score each of these can-
didates. The 3 candidates with the highest scores
(and thus the most credible answers) are selected.
The idea is that during this first training, RoBERTa
will learn a large amount of simplistic logic. For
example, because of the initial random selection of
distractors, it is highly unlikely that even one of the
distractors will be close enough to the question’s se-
mantic field. Furthermore, a lot distractors have an
incorrect grammar (eg: a distractor might be plural
when the question expects a singular). Therefore,
in this initial training, RoBERTa might learn to iso-
late the answer with a corresponding semantic field
or the one with correct grammar. The re-selection
then minimizes the amount of trivial distractors and
models trained on this new refined dataset will have
to focus on deeper and more meaningful relations
between the questions and the answers. The pro-
cess is better shown in Figure 4, and an example of
refined distractors can be found in Figure 3.
The number of scored candidate distractors is
an hyper-parameter. A small number of candidates
may result in a situation where none of the candi-
dates are credible enough, while a large number
requires more computation time, since the score of
each candidate for every question needs to be com-
puted, and has a higher risk of proposing multiple
valid answers. In our experiments, we use a num-
ber of 64 candidates in order to limit computation
time.
3 Training and Implementation Details
To refine distractors, we use the “Large” version of
RoBERTa and all models are trained for 4 epochs
and a learning rate of 1×10. These hyper-
parameters are chosen based on previous exper-
iments with RoBERTa on other multiple-choice
datasets. The final UnifiedQA fine-tuning is done
using the same multiple choices question answer-
ing setup as the one used in the original UnifiedQA
paper (Khashabi et al., 2020). We use the “Large”
version of UnifiedQA and all the models are trained
for 4 epochs using Adafactor and a learning rate
of1×10. The learning rate is loosely tuned
to get the best performance on the validation set
during the supervised training of UnifiedQA. We
use the Hugging Face pytorch-transformers (Wolf
et al., 2020) library for model implementation. Ex-
periments presented in this paper were carried out
using the Grid’5000 testbed (Balouek et al., 2013),
supported by a scientific interest group hosted by
Inria and including CNRS, RENATER and sev-
eral Universities as well as other organizations (see
https://www.grid5000.fr ).
4 Results
Accuracy results in Table 2 have a 95% Wald con-
fidence interval of ±2.8%. The first row of Table 2
presents the accuracy results of a vanilla UnifiedQA
large model on SciQ. The second line shows the ac-
curacy when UnifiedQA is fine-tuned over the full
training corpus. Our objective is thus to get as close
as possible to this accuracy score using only un-735supervised methods. The results using Wikipedia
are the only ones that are unsupervised and there-
fore are the ones directly comparable to UnifiedQA
with no fine-tuning or other unsupervised methods.
The other results serve to illustrate what could be
obtain with a tighter selection of sentences.
Model Dev Test
UnifiedQA (no fine-tuning) 64.6 63.4
UnifiedQA (supervised) 78.7 78.7
Unsupervised - Random distractors
SciQ data 71.3 70.8
SciQ data (train only) 70.9 70.1
Wikipedia data 68.3 67.5
Unsupervised - Refined distractors
SciQ data 75.4 74.2
SciQ data (train only) 73.1 72.4
Wikipedia data 70.6 69.4
Fine-tuning UnifiedQA on synthetic questions
with random distractors improves the results as
compared to the baseline and, as expected, the
closer the unlabeled sentences are to the topics
of the questions, the better is the accuracy. Hence,
generating questions from only the train set of SciQ
gives performances that are comparable but slightly
lower to the ones obtained from the combined train,
dev and test set of SciQ. Finally, questions selected
from Wikipedia also improve the results, despite
being loosely related to the target test corpus. Our
distractor selection method further boosts the ac-
curacy results in all setups. This suggests that a
careful selection of distractors is important, and
that the hard selection criterion used here seems
adequate in our context.
The results for CommonsenseQA and QASC us-
ing the same selection of sentences from Wikipedia
are reported in table 3. Overall, we obtain similar
results to SciQ with a large improvement of perfor-
mances when generating questions and a further
boost with refined distractors. However compared
to SciQ, the improvement brought by the distractor
refining process is less significant. This could be
partly explained by the fact that the distractors inModel CQA QASC
UnifiedQA (no fine-tuning) 60.9 44.5
UnifiedQA (supervised) 74.3 61.0
Wikipedia data (Random) 64.9 57.2
Wikipedia data (Refined) 65.1 59.4
the original QASC and CommonsenseQA datasets
are overall easier and therefore it is less advanta-
geous for a model to be trained on harder questions.
5 Conclusion
In this work, we proposed a multiple-choice ques-
tion generation method that can be used to fine-tune
the state-of-the-art UnifiedQA model and improve
its performance on an unseen and out of domain
dataset. Our contributions are:
•We have shown that simple unsupervised
methods could be used to finetune existing
multipurpose question answering models (in
our case UnifiedQA) to new datasets or do-
mains.
•We propose a novel distractor refining method
able to select harder distractors for a given
generated question and show its superiority
compared to a random selection.
Future work includes comparing our method to
other question generation methods (including su-
pervised methods: Liu et al. (2020), Puri et al.
(2020)) in order to assess the effect of both the
generation method and the questions quality on the
final performances of our models. Also, we will
further compare different variations of our ques-
tion generation and distractor refining methods in
order to more thoroughly understand the effect of
hyper-parameters such as the number of candidate
distractors.
References736737738