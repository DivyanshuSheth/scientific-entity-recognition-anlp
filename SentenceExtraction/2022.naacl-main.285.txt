
Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-RadAAII, University of Technology Sydney, NSW, Australia
Zihan.Zhang-5@student.uts.edu.au ,Ling.Chen@uts.edu.auEindhoven University of Technology, Eindhoven, the Netherlands
m.fang@tue.nlNIASRA, University of Wollongong, NSW, Australia
mrad@uow.edu.au
Abstract
Recent work incorporates pre-trained word em-
beddings such as BERT embeddings into Neu-
ral Topic Models (NTMs), generating highly
coherent topics. However, with high-quality
contextualized document representations, do
we really need sophisticated neural models to
obtain coherent and interpretable topics? In
this paper, we conduct thorough experiments
showing that directly clustering high-quality
sentence embeddings with an appropriate word
selecting method can generate more coherent
and diverse topics than NTMs, achieving also
higher efficiency and simplicity.
1 Introduction
Topic modelling is an unsupervised method to un-
cover latent semantic themes among documents
(Boyd-Graber et al., 2017). Neural topic mod-
els (NTMs) (Miao et al., 2016; Srivastava and
Sutton, 2017) incorporating neural components
have significantly advanced the modelling results
than the traditional Latent Dirichlet Allocation
(LDA; Blei et al. 2001). Later, contextualized
word and sentence embeddings produced by pre-
trained language models such as BERT (Devlin
et al., 2019) have demonstrated the state-of-the-art
results in multiple Natural Language Processing
(NLP) tasks (Xia et al., 2020), which attracts atten-
tions from the topic modelling community. Recent
work has successfully incorporated these contextu-
alized embeddings into NTMs, showing improved
topic coherence than conventional NTMs that use
Bag-of-Words (BoW) as document representations
(Bianchi et al., 2021a,b; Jin et al., 2021). Despite
the promising performance, existing NTMs are gen-
erally based on a variational autoencoder frame-
work (V AE; Kingma and Welling 2014), which
suffers from hyper-parameters tuning and compu-
tational overheads (Zhao et al., 2021). Moreover,the integration of the pre-trained embeddings to the
standard V AE framework adds additional model
complexity. With high-quality contextualized doc-
ument representations, do we really need sophisti-
cated NTMs to obtain coherent and interpretable
topics?
Recent work (Aharoni and Goldberg, 2020; Sia
et al., 2020; Thompson and Mimno, 2020; Grooten-
dorst, 2020) has shown that directly congregating
contextualized embeddings can get semantically
similar word or document clusters. Specifically,
Sia et al. (2020) cluster vocabulary-level word em-
beddings and obtain top words from each cluster
using weighing and re-ranking, while Thompson
and Mimno (2020) consider polysemy and perform
token-level clustering. However, the use of term
frequency (TF) to select topic words fails to cap-
ture the semantics of clusters precisely because
words with high frequency may be common across
different clusters. Grootendorst (2020) propose a
class-based Term Frequency- Inverse Document
Frequenc (c-TF-IDF) method that extract impor-
tant words from each clustered documents, which
tends to choose representative words within each
cluster to form topics. However, it overlooks the
global semantics between clusters which could be
incorporated. In addition, all above works only
compare the performance with the traditional LDA
while ignoring the promising NTMs proposed re-
cently. The performance of the clustering-based
topic models is still yet uncovered.
Is neural topic modelling better than simple em-
bedding clustering ? This work compares the per-
formance of NTMs and contextualized embedding-
based clustering systematically. Our main fo-
cus is to provide insights by comparing the two
paradigms for topic models, which has not been
investigated before. We employ a straightforward
framework for clustering. In addition, we explore
different strategies to select topic words for clus-
ters. We evaluate our approach on three datasets3886with various text lengths.
Our contributions are as follows: First, we find
that directly clustering high-quality sentence em-
beddings can generate as good topics as NTMs,
providing a simple and efficient solution to uncover
latent topics among documents. Second, we pro-
pose a new topic word selecting method, which is
the key to producing highly coherent and diverse
topics. Third, we show that the clustering-based
model is robust to the length of documents and
the number of topics. Reducing the embedding
dimensionality negligibly affects the performance
but saves runtime. From our best knowledge, we
are the first to compare with NTMs, using con-
textualized embeddings that produced by various
transformer-based models.
2 Models
This study compares embedding clustering-based
models with LDA and a series of existing NTMs
as follows. Implementation details are supplied in
Appendix A.
LDA (Blei et al., 2001): the representative tradi-
tional topic model in history that generates topics
via document-topics and topic-words distributions.
ProdLDA (Srivastava and Sutton, 2017): a
prominent NTM that employs the V AE (Kingma
and Welling, 2014) to reconstruct the BoW repre-
sentation.
CombinedTM (Bianchi et al., 2021a): extends
ProdLDA by concatenating the contextualized
SBERT (Reimers and Gurevych, 2019) embed-
dings with the original BoW as the new input to
feed into the V AE framework.
ZeroShotTM (Bianchi et al., 2021b): also builds
upon ProdLDA, but it replaces the original BoW
with SBERT embeddings entirely.
BERT+KM (Sia et al., 2020): a clustering-based
method that first uses K-Means (KM) to cluster
word embeddings, then apply TF to weight and
re-rank words to obtain topic words.
BERT+UMAP+HDBSCAN (i.e., BERTopic)
(Grootendorst, 2020): a clustering-based method
that first leverages HDBSCAN (McInnes and
Healy, 2017) to cluster BERT embeddings of the
sentences and Uniform Manifold Approximation
Projection (UMAP) (McInnes et al., 2018) to re-
duce embedding dimensions, then use a class-based
TFIDF (i.e. c-TF-IDF) to select topic words within
each cluster. Note that BERTopic may not generate
the specified number of topics.
Contextual Embeddings+UMAP+KM (our
method CETopic): we use a simple clustering
framework with contextualized embeddings for
topic modelling, as shown in Figure 1. We first
encode pre-processed documents to obtain contex-
tualized sentence embeddings through pre-trained
language models. After that, we lower the dimen-
sion of the embeddings before applying clustering
methods (e.g., K-Means; KM) to group similar doc-
uments. Each cluster will be regarded as a topic.
Finally, we adopt a weighting method to select rep-
resentative words as topics.
We believe that high-quality document embed-
dings are critical for clustering-based topic mod-
elling. We thus experiment with different embed-
dings including BERT, RoBERTa (Liu et al., 2019),
and SBERT. We also adopt SimCSE (Gao et al.,
2021), a recently proposed sentence embeddings
of contrastive learning, that has shown the state-of-
the-art performance on multiple semantic textual
similarity tasks. Both supervised and unsupervised
SimCSE are investigated in our experiment (e.g.,
Table 2).
Pre-trained contextualized sentence embeddings
often have high dimensionalities. To reduce the
computational cost, we apply the UMAP in our
implementation to reduce the dimensionality while
maintaining the essential information of the embed-
dings. We find that reducing dimensionality before
clustering has a negligible impact on performance
(Section 4.4).
We cluster the dimension-reduced sentence em-
beddings using K-Means because of its efficiency
and simplicity. Semantically close documents are
gathered together, and each cluster is supposed to
represent a topic.
3 Topic Words for Clusters
Once we have a group of clustered documents, se-
lecting representative topic words is vital to iden-3887tify semantics of topics. Inspired by previous
works (Ramos et al., 2003; Grootendorst, 2020),
we explore several weighting metrics to obtain
topic words in clusters. Let nbe the frequency
of word tin document d,/summationtextnbe the to-
tal words’ frequency in the document, and Dbe
the entire corpus. Term Frequency-Inverse Docu-
ment Frequency (TFIDF) is defined as TFIDF =·log/parenleftig/parenrightig
. While capturing the
word importance across the entire corpus, TFIDF
ignores that semantically similar documents have
been grouped together. To address this issue, we
consider two alternative strategies. First, we con-
catenate the documents within a cluster to be a
single long document and calculate the term fre-
quency of each word in each cluster:
where nis the frequency of word tin cluster i,/summationtextnis the total word frequency in the cluster.
Second, for each cluster i, we apply TFIDF:
where ndenotes the frequency of word tin
document d, which is in cluster i, and|D|is the
number of documents in cluster i.
Besides the two local cluster-based strategies,
we further incorporate the global word importance
with local term frequency within each cluster:
and we combine the global word importance with
term frequency across clusters:
where |K|is the number of clusters and |{t∈K}|
is the number of clusters that word tappears.
4 Experiments
4.1 Datasets
We adopt three datasets of various text lengths in
our experiments, namely 20Newsgroups, M10
(Lim and Buntine, 2015), and BBC News (Greene
and Cunningham, 2006). We follow OCTIS (Ter-
ragni et al., 2021) to pre-process these raw datasets.
The statistics of the datasets are shown in Table 1.
4.2 Evaluation Metrics
We evaluate the topic quality in terms of both topic
diversity and topic coherence: Topic Diversity ( TU)
(Nan et al., 2019) measures the uniqueness of the
words across all topics; Normalized Pointwise Mu-
tual Information ( NPMI ) (Newman et al., 2010)
measures topic coherence internally using a slid-
ing window to count word co-occurrence patterns;
Topic Coherence ( C) (Röder et al., 2015) is a
variant of NPMI that uses the one-set segmenta-
tion to count word co-occurrences and the cosine
similarity as the similarity measure.
4.3 Results & Analysis
We report the main results in Table 2.
Directly clustering high-quality sentence em-
beddings can generate good topics. From Table 2,
it can be observed that SBERT and SimCSE-based
clustering models achieve the best averaged topic
coherence among the three datasets while maintain-
ing remarkable topic diversities. Conversely, clus-
tering RoBERTa achieves similar or worse results
than contextualized NTMs. The results suggest
that contextualized embeddings are essential to get
high-quality topics.
Topic words weighting method is vital. We
can see in Figure 2 that inappropriate word se-
lecting methods ( TFIDF ×TFandTF) lead
to worse topic coherence than the contextualized
NTMs (i.e., CombinedTM and ZeroShotTM), and
even the BoW-based ProdLDA. Moreover, from
Table 2, BERT+KM adopt TF to obtain top
words for each cluster, which ignores that the words
may also be prevalent in other clusters, thus hav-
ing poor topic diversities. It is also worthy to note
that although BERT+UMAP+HDBSCAN (i.e.
BERTopic) reaches the highest topic diversity on
20Newsgroups, it cannot produce the specified
topic numbers. Thus its performance may be
boosted because of the reduced topic numbers.
Moreover, our proposed methods, i.e. BERT38883889
and BERT+UMAP outperforms BERTopic in
most metrics, especially on topic coherence. This
suggests that c-TF-IDF tends to discover incoher-
ent words from each cluster to maintain a high
topic uniqueness. Instead, our proposed method,
TFIDF ×IDF, considers the locally important
words and globally infrequent words at the same
time. We provide more comparison of the word
selecting methods in Section 4.4.
Clustering-based topic models are robust to
various lengths of documents. From Table 2 and
Figure 2, we find that clustering-based models with
high-quality embeddings (SBERT and SimCSE)
consistently perform better than conventional LDA
and NTMs, especially on the short text dataset M10,
even with different word selecting methods.
4.4 Ablation Studies
We further investigate the impact of the topic word
selecting methods, different embedding dimension-
alities, as well as the topic numbers.
Topic word selecting methods. Table 3 shows
the comparison between different word weighting
methods. TFIDF ×IDFachieves significantly
better results among all methods. This indicates
thatTFIDF marks out the important words to
each document in the entire corpus, while IDF
penalizes the common words in multiple clusters.
Conversely, the other three methods ignore that
frequent words in a cluster may also be prevalent
in other clusters, hence selecting such words lead-
ing to low topic diversities. A further analysis in
Appendix B also supports the observation.
Embedding dimensionality reduction. We ap-
ply UMAP to reduce the dimensionality of the sen-
tence embeddings before clustering. As shown in
Figure 3, the embeddings dimensionality negligibly
affects topic quality for all word selecting methods.
However, reducing to a lower dimensionality de-
creases the computational runtime as shown in Ta-
ble 4. We compare the model runtime between the
contextualized NTM CombinedTM and clustering-
based models. We reduce the dimensionality of
the sentence embeddings to 50 using UMAP. All
models run on NVIDIA T4 GPU.
Topic numbers K.We investigate the impact
of the different number of topics Kon the perfor-
mance of the models. Figure 2 plots the trends of
TUandCon three datasets. We observe that the
TUof clustering-based topic models, especially the
models using TFIDF ×IDF, decrease slowly
compared to others when Kincreases. The similar
trend can be observed for topic coherence, while
theCof LDA and NTMs either fluctuates signifi-
cantly or stays at a low level.
5 Conclusion
We conduct a thorough empirical study to show that
a clustering-based method can generate commend-
able topics as long as high-quality contextualized
sentence embeddings are used, together with an ap-
propriate topic word selecting strategy. Compared
to neural topic models, clustering-based models are
more simple, efficient and robust to various doc-
ument lengths and topic numbers, which can be
applied in some situations as an alternative.
Acknowledgement
This work could not have been done without the
support of TPG Telecom. We thank anonymous
reviewers for their valuable comments. We also
thank Yunqiu Xu for valuable discussions and sug-
gestions.3890References38913892A Configuration Details
We implement LDA and NTMs based on OCTIS (Terragni et al., 2021)and use their default settings.
Specifically, ProdLDA, CombinedTM, and ZeroShotTM share the same configurations, i.e. one hidden
layer with 100 neurons, ADAM optimizer and Momentum as 0.99; we randomly dropout 20% hidden
units; we run 100 epochs of each model, and the batch size is 64. For BERT+KM, we follow Sia et al.
(2020) by reducing embedding dimension to 50 using Principal Component Analysis (PCA) and adopting
TF to select words. For BERT+UMAP+HDBSCAN, we follow BERTopic Grootendorst (2020) and
allows it to reduce the topic numbers. For our methods, we implement clustering-based experiments based
on BERTopic (Grootendorst, 2020). We reduce embedding dimension to 5 using UMAP. We use BERT,
RoBERTa, and SBERT embeddings provided by HuggingFace, and SimCSE embeddings provided from
its official Github.
B Comparison of Topic Words
We run Sup-SimCSE(RoBERTa)+UMAP on 20Newsgroup and show the differences of topic diversi-
ties produced by distinct word selecting methods in Table 5. It is clear that TFIDFandTFtend to
choose common words across multiple topics.3893