
Ninareh Mehrabi, Palash Goyal, Apurv Verma, Jwala Dhamala,
Varun Kumar, Qian Hu, Kai-Wei Chang, Richard Zemel,
Aram Galstyan, Rahul Gupta
Amazon Alexa AI-NU
Abstract
Natural language often contains ambiguities
that can lead to misinterpretation and miscom-
munication. While humans can handle ambigu-
ities effectively by asking clarifying questions
and/or relying on contextual cues and common-
sense knowledge, resolving ambiguities can be
notoriously hard for machines. In this work,
we study ambiguities that arise in text-to-image
generative models. We curate the Text-to-
image Ambiguity Benchmark (TAB) dataset to
study different types of ambiguities in text-to-
image generative models.We then propose the
Text-to-ImagE Disambiguation (TIED) frame-
work to disambiguate the prompts given to the
text-to-image generative models by soliciting
clarifications from the end user. Through au-
tomatic and human evaluations, we show the
effectiveness of our framework in generating
more faithful images aligned with end user in-
tention in the presence of ambiguities.
1 Introduction
Natural conversations contain inherent ambigui-
ties due to potentially multiple interpretations of
the same utterance. Different types of ambiguities
can be attributed to syntax (e.g., “the girl looks at
the boy holding a green bag” — is the girl hold-
ing the green bag?), semantics (e.g., “a picture of
cricket” — is cricket referring to an insect or a
game?), and underspecification (e.g., “doctor talk-
ing to a nurse” — is the doctor/nurse male or fe-
male?). Ambiguities pose an important challenge
for many natural language understanding tasks and
have been studied extensively in the context of ma-
chine translation (Stahlberg and Kumar, 2022), con-
versational question answering (Guo et al., 2021),
and task-oriented dialogue systems (Qian et al.,
2022), among others.
In this paper, we study the effect of ambiguity
in text-to-image generative models (Ramesh et al.,Figure 1: Examples of ambiguous prompts and cor-
responding generated images. The icon between the
images depicts alternative interpretations. The images
corresponding to first two prompts are generated by Ope-
nAI’s DALL-E (Ramesh et al., 2022), and images corre-
sponding to last two prompts are generated by DALL-E
Mega (Dayma et al., 2021) models.
2021, 2022; Saharia et al., 2022; Yu et al., 2022)
and demonstrate that ambiguous prompts provided
to such models might result in undesired outcomes
and poor user experience. In particular, ambiguities
due to underspecification can lead to biased out-
comes with possible implications on fairness of the
underlying models (e.g., when prompted with “doc-
tor talking to a nurse”, the model might generate
images with disproportionate number of male doc-
tors and female nurses). We also propose a frame-
work for mitigating ambiguities existing in prompts.
We choose this setting to study ambiguity as vi-
sual scenes provide readily human-interpretable
alternative understandings of text, thus helping in
evaluating ambiguities as well as mitigation strate-
gies. Figure 1 illustrates a few ambiguous prompts
and corresponding outputs from the state-of-the-
art text-to-image model, DALL-E (Ramesh et al.,
2022; Dayma et al., 2021). We observe that ambi-14367
guity in prompts confuses the model resulting in a
varied set of generated images.
Humans tend to resolve ambiguities by asking
clarifying questions, relying on other forms of
modalities (such as vision), using contextual sig-
nals, and leveraging common-sense and/or an exter-
nal source of knowledge (Achimova et al., 2022).
Inspired by this observation, we propose a new
framework (Figure 2) in which we incorporate
a language model-based prompt disambiguation
filter to process the prompts fed to text-to-image
generative models. This filter is capable of either
asking clarifying questions or generating differ-
ent possible textual descriptions of visual setups
which would later be resolved through end user
interactions (in this work, we define end user as
a human-agent who interacts with the system and
might interchangeably use human to refer to the
end user). Ultimately, the disambiguation filter
helps the text-to-image model to identify a single
visual setup for image generation. In this work,
we define visual setups as textual descriptions of
different possible visual interpretations of a given
ambiguous prompt.
To better understand the weaknesses of current
text-to-image generative models, and to evaluate
the effectiveness of our proposed mitigation frame-
work, we curate a benchmark dataset consisting
of ambiguous prompts covering different types of
ambiguities that are especially relevant to text-to-
image models. We propose new automatic evalu-
ation procedures to evaluate faithfulness of gener-ations in text-to-image generative models to end
user intention. We perform various automatic and
human evaluation experiments (on over 15k im-
ages) and observe that our proposed framework can
improve faithful image generation by an overall av-
erage of over 11% through prompt disambiguation.
Overall, we make the following contributions:
1.We introduce a Text-to-image Ambiguity
Benchmark (TAB) dataset containing differ-
ent types of ambiguous prompts along with
different visual setups (Section 2).
2.We propose the Text-to-ImagE Disambigua-
tion (TIED) framework which can be applied
on top of any text-to-image model for ambi-
guity resolution (Section 3).
3.We propose new automatic evaluation proce-
dures to evaluate ambiguity resolution in text-
to-image models. We perform automatic and
human experiments to demonstrate the effect
of TIED on ambiguity resolution and faithful
image generations (Sections 4 and 5).
2 Text-to-image Ambiguity Benchmark
(TAB)
Our Text-to-image Ambiguity Benchmark (TAB)
is a modified version of the LA V A corpus (Berzak
et al., 2015). The original LA V A corpus contains
various types of ambiguous sentences that can only
be resolved through visual signals from their cor-
responding images/videos. We use the ambiguous14368
prompts (templates) from LA V A and not the im-
ages — as images in our case would be generated
by text-to-image generative models.
We make various modifications to the LA V A cor-
pus to create TAB. These modifications include: (i)
adding new ambiguous sentences to TAB to cover
more diverse objects, scenes, and scenarios com-
pared to existing ambiguous sentences in LA V A,
(ii) removing examples relevant to video domain
from LA V A and only keeping examples relevant
to static images in TAB, (iii) including fairness
prompts in TAB that cover different activities (Zhao
et al., 2017) and occupations (Nadeem et al., 2021)
in which the identities of the individuals are am-
biguous, (iv) adding more structurally complex sen-
tences, and (v) curating additional labels for TAB
(e.g., whether the visual setup or interpretation of
an ambiguous sentence is commonsensical or not).
As a result of some of the modifications mentioned
above, TAB ends up covering 963 more ambiguous
sentences (prompts) and 4,192 more visual setups
(textual descriptions of possible visual interpreta-
tions for each ambiguous sentence) compared to
LA V A. LA V A covers 237 ambiguous sentences and
498 visual setups, while TAB covers 1,200 ambigu-
ous sentences and 4,690 visual setups.
On a high level, TAB covers six main types of
prompt ambiguities, including fairness and linguis-
tic type ambiguities. We add some additional com-
plex cases on top of the six main types of prompt
ambiguities. In these complex cases, we take a
sample of prompts from TAB and manually make
structurally more complex version of each sentence.
This process is done in a way such that the ambi-
guities and meaning of the constituent sentences
are kept intact, but the structure of a sentence ismade more complex through addition of more in-
formation, extra words, adverbs, and adjectives.
For instance, “ The girl waves at the old man and
woman. ” representing an example for syntax con-
junction type ambiguity can be turned into “ The
girl gracefully waves at the old man and woman to
show respect and gratitude. ” with a more complex
sentence structure. We also add some additional
miscellaneous cases, which are not covered by six
main types of ambiguities. In addition, we add
combination cases where we combine fairness and
linguistic type ambiguities and make new varia-
tions from our existing prompts.
Each of the 1,200 ambiguous prompts in TAB
are accompanied with possible visual setups. We
also curate questions that can be surfaced to the
end user to clarify the visual setup they have in
mind amongst the set of visual setups available
for a given ambiguous prompt. The objective of
TIED framework is to either generate different vi-
sual setups or clarifying questions to the end user
to disambiguate the prompts through end user in-
teraction. Therefore, we use these questions and
visual setups in TAB as ground truth to evaluate
the TIED framework (details in Section 4). Each of
the elements (e.g., visual setups) in TAB are gener-
ated by an expert annotator and are cross-checked
by two additional annotators to ensure that they
are sensible. Additional detailed statistics about
TAB can be found in Table 1. Appendix A has
definitions, additional examples for each of the am-
biguities covered in TAB, details of modifications
made to LA V A, and the dataset schema.
3 Text-to-ImagE Disambiguation (TIED)
Framework
Given an ambiguous prompt, our Text-to-ImagE
Disambiguation (TIED) framework uses a Lan-
guage Model (LM) to obtain disambiguation feed-
back from the end user through an interactive sys-
tem. Our goal in TIED is to use in-context learning
to seek user feedback that can help us disambiguate
the prompts. After obtaining the disambiguation
feedback, TIED concatenates the obtained signals
to the original ambiguous prompts and creates fi-
nal disambiguated prompts as shown in Figure 2.
TIED then uses the final disambiguated prompts to
generate images using text-to-image models.
We test two resolution approaches in TIED: 1)
Question Answering-TIED (“ QA-TIED ”) which
resolves ambiguities by language models gener-14369ating questions and seeking answers to disam-
biguate prompts; 2) Visual Setup-TIED (“ VS-
TIED ”) which resolves ambiguities by language
models directly generating different possible visual
setups (textual descriptions of different visual sce-
narios/interpretations) given an ambiguous prompt
and seeking signals in the form of a visual setup
being chosen. The overall TIED framework along
with QA-TIED and VS-TIED ambiguity resolution
approaches are shown in Figure 2.
3.1 QA-TIED
In QA-TIED, we perform in-context learning on
the language model with few-shot examples con-
taining ambiguous prompts as well as related clar-
ifying questions that can result in visual resolu-
tion. Then, given an ambiguous prompt at infer-
ence time, we ask the model to generate clarifying
questions. The question generated by the language
model is presented to an end user, who is expected
to provide a disambiguating response (assuming
the question is useful to the end user and they can
express their intention as a response). If the ques-
tion is irrelevant, we expect the question to be left
unanswered. The end user response is then concate-
nated to the original ambiguous prompt and a final
disambiguated prompt is obtained as shown in Fig-
ure 2 (left). After obtaining the final disambiguated
prompt, the prompt is provided to the text-to-image
model and the corresponding image is generated.
3.2 VS-TIED
In VS-TIED, we perform in-context learning on the
language model with few-shot examples containing
ambiguous prompts as well as textual descriptions
of possible visual scenarios that can result in visual
resolution. Then, given an ambiguous prompt at in-
ference time, we ask the model to generate possible
textual descriptions of visual scenarios. Similar to
the QA-TIED setup, the end user interacts with the
language model and, this time instead of providing
answers to clarifying questions, they pick the tex-
tual description of visual scenario that they have in
mind out of all the possible ones generated by the
language model. The chosen textual description of
the visual scenario is then concatenated to the orig-
inal ambiguous prompt and a final disambiguated
prompt is obtained as shown in Figure 2 (right).
If the generated visual scenarios are irrelevant or
may not result in visual resolution, we expect the
end user to not pick any generated scenario. Lastly,
after obtaining the final disambiguated prompt, theprompt is provided to the text-to-image model and
the corresponding image is generated.
4 Experiments
We evaluate TIED on two complementary aspects:
(i) whether language models incorporated in TIED
generate appropriate clarifying questions (in the
case of QA-TIED) or textual descriptions of visual
setups (in the case of VS-TIED), resulting in visual
resolution; (ii) whether modified prompts gener-
ated through TIED result in faithful image gener-
ation aligned with end-user intention. We discuss
respective experiments for both of these evalua-
tions below. In our experiments, we use three lan-
guage models: GPT-2 (Radford et al., 2019), GPT-
neo (Black et al., 2021), and OPT (Zhang et al.,
2022). In addition, we use OpenAI’s DALL-E
(Ramesh et al., 2022) and DALL-E Mega (Dayma
et al., 2021) models as our text-to-image models to
generate images in our experiments.
4.1 Language Model-based Disambiguation
To evaluate the ability of language models in gen-
erating disambiguation questions and/or textual de-
scriptions of visual scenarios, we provide each of
the three language models one example from each
of the main six types of ambiguities as few-shot ex-
amples that are externally sourced and not present
in TAB (note that we only need a handful of few-
shot examples which are tabulated in Appendix B).
We then perform automatic and human evaluations
on the results generated by the language models
given the ambiguous prompts in TAB as test in-
stances as described below.
Automatic Evaluations. In automatic evalua-
tions, we report the alignment of generations by
language models to ground truths provided in the
TAB dataset. We remind the reader that in TAB,
for each ambiguous prompt (e.g., “ The girl looks
at the bird and the butterfly; it is green ”) different
possible textual descriptions of visual interpreta-
tions that can be associated to a prompt are present
(e.g., (1) “ The bird is green ”, (2) “ The butterfly
is green ”). These visual interpretations serve as
our ground truth in our automatic evaluations for
the VS-TIED approach. In addition to visual in-
terpretations, TAB contains the question format of
each of those interpretations (e.g., (1) “ Is the bird
green? ”, (2) “ Is the butterfly green? ”) that serve as
our ground truth in our automatic evaluations for
the QA-TIED approach. In automatic evaluations,14370
we report the BLEU (Papineni et al., 2002) and
ROUGE (Lin, 2004) scores by comparing the gen-
erations to the ground truth visual setups/clarifying
questions in the TAB dataset.
Human Evaluations. It is possible that the auto-
matic metrics might not capture different variations
of the ground truth labels. It is also possible that
the automatic metrics might give high scores even
if the generated prompts are not useful for disam-
biguation. Therefore, we perform human evalu-
ations to ensure that the generations are relevant
and that the automatic metrics are reliable metrics
for this use-case. In human evaluations, we report
the fraction of generations by language models for
which an end user provides an answer or a selection
in the TIED framework indicating the fraction of
successful generations. Due to human evaluation
task being labor intensive, we report the human
evaluation results only on the GPT-neo model as
we obtained best automatic results for this model.
4.2 Faithful Image Generation in TIED
To evaluate the effectiveness of TIED in faithful
image generation through prompt disambiguation,
we compare generated images using disambiguated
prompts obtained through TIED vs the original
ambiguous prompts coming from TAB. For each
prompt, four images are generated through the text-
to-image models used in our experiments. Overall,
we generate and study over 15k images for different
experimental setups and models (Appendix C has
more details and statistics on prompts and images).Automatic Evaluations. In our automatic evalu-
ations, we use a Visual Question Answering (VQA)
model to check whether end user’s intention is sat-
isfied in the generated image. TAB provides each
prompt associated with an image with an end user
intention in the question format. We use both im-
age and its corresponding question as inputs to the
VQA model as shown in Figure 3. Ideally, if the im-
age aligns to end user intention, we would expect
the VQA model to output a "Yes" as an answer
to the question. Thus, we report the fraction of
times the VQA model outputs a "Yes" as an answer
as the fraction of faithful generations aligned with
end user intention amongst all the generations. For
our automatic evaluation, we use the VILT VQA
model (Kim et al., 2021).
Human Evaluations. In addition to the pro-
posed evaluation framework, we perform human
evaluation in which we replace the VQA model
with a human evaluator which checks whether the
image satisfies the end user intention. Human eval-
uations would give us an unbiased understanding
of TIED’s effectiveness in faithful image genera-
tion by text-to-image models and would identify
if human and VQA approaches agree. The human
evaluation experiments are performed on Amazon’s
mechanical turk (mturk) platform. Overall, 400 im-
ages are annotated by mturk workers. Each image
is annotated by three workers; thus, we obtain 1200
annotations in total (Appendix C has more details
on the mturk experiments along with our survey).
Paraphrasing Evaluations. To disambiguate
the prompts in TIED, we concatenate the dis-
ambiguation signals with the original ambiguous
prompts. This can give us complex and unnatu-
ral looking sentences. It might be beneficial to
restate the sentences to obtain better results. Thus,
we explore the effect that paraphrasing the disam-
biguated prompts can have on creating prompts
more aligned with end user intention and hence
more faithful image generation. Here, we take
all the disambiguated prompts obtained through
TIED, which are concatenation of disambiguated
signals provided by the end user to the ambiguous
prompts, and apply sentence paraphrasing model
fine-tuned on BART (Lewis et al., 2020) over them.
We then compare the results from providing the
text-to-image model the ambiguous prompt vs the
disambiguated prompt which is obtained from sim-
ple concatenation of end user provided signal to
the original prompt vs a paraphrased version of14371
the disambiguated prompt from the previous step.
We report whether paraphrasing helps the model to
generate more faithful images to end user intention.
4.3 Ablation Studies
In our first ablation study, we demonstrate the effect
of the number of few-shot examples provided to a
language model on its performance. In this study,
for a given ambiguity type we vary the amount
of few-shot examples provided to the model. We
then report model’s performance on resolving the
specific type of ambiguity for which the few-shot
examples are given and its generalization ability
in resolving other ambiguity types. In our second
ablation study, we test model’s ability to resolve
ambiguities for complex vs simple sentence struc-
tures existing in TAB. In this study, we compare the
performance disparities between language models’
ability in resolving existing ambiguities in simple
sentences vs similar sentences with more complex
sentence structures that we curate in TAB.5 Results
5.1 Language Model-based Disambiguation
Automatic Evaluations. From results in Table 2,
we observe that language models perform reason-
ably well to generate good quality clarifying ques-
tions when given an ambiguous prompt as an input
according to BLEU ( ∼0.40) and ROUGE ( ∼0.60)
metrics in the few-shot QA-TIED setup. Here,
we report the results for the QA-TIED approach
in which language models generate one clarify-
ing question per given prompt. Additional results
for VS-TIED and the case in which we generate
multiple clarifying questions per prompt in QA-
TIED are in Appendix B.1. Similarly, we observe
reasonable BLEU and ROUGE scores for the VS-
TIED setup. However, we note that better scores
are obtained in QA-TIED setup compared to VS-
TIED. This suggests that the task of directly gener-
ating multiple scenarios given few-shot examples is
harder for these models than generating clarifying
questions given an ambiguous prompt. We believe
that better scores are obtained for QA-TIED com-
pared to VS-TIED since writing a disambiguation
question has less diversity and is less complicated
while there might be different ways that one would
describe a disambiguated scenario. Thus, provid-
ing one way of generating a disambiguation sce-
nario for some in-context examples would not be
enough for the model to generalize and learn the
task efficiently.
In addition to reporting overall results on our
overall TAB benchmark dataset, fine-grained re-
sults for the six different ambiguity types (as re-
ported in Table 2) suggest that there exists disparity
in how different ambiguities are handled by each
of these language models. For instance, language
models obtain higher BLEU and ROUGE scores
on generating clarifying question in QA-TIED for
ambiguity type Syntax Verb Phrase (VP) than ambi-
guity type Syntax Propositional Phrase (PP) . This14372
suggests that some types of ambiguities are eas-
ier for the language models to resolve compared
to others, although they see similar number of ex-
amples provided per ambiguity type as few-shot
examples using the in-context inference strategy.
These results also demonstrate that the sentence
structure has more room for variation in some am-
biguity types making it harder to be resolved in the
in-context learning setup.
Human Evaluations. Figure 4 shows results
from our human evaluation studies which shows
similar trend as automatic evaluations discussed
above. We report the fraction of generations that
are successful according to end user (results for
VS-TIED in Appendix B.1). From our human
evaluation results as demonstrated in Table 3, we re-
port the Pearson correlation across ambiguity types
between human and ROUGE score of 0.863 and
between human and BLEU score of 0.546. These
results further show the agreement between our
performed automatic and human evaluations.
5.2 Faithful Image Generation in TIED
Human Evaluations. First, we demonstrate the
effectiveness of QA-TIED in generating faithfulimages aligned with end user intention according
to human evaluations. As per Fleiss Kappa (Fleiss,
1971), we observe an inter-annotator agreement
of 0.86, denoting significant agreement. Figure 5
shows the fraction of times the models generate
faithful images. We observe that overall, disam-
biguation helps with faithful generations by im-
proving the results from the baseline that uses the
original ambiguous prompts. Despite the overall
positive impact of disambiguation, the fine-grained
results in Figure 5 demonstrate that disambigua-
tion has an adverse effect for some ambiguity types
(e.g., PP type ambiguity due to the inherent struggle
of text-to-image models with prepositions). In addi-
tion, we observe that it is harder to generate faithful
images with correct interpretations for some ambi-
guity types (e.g., Ellipsis) due to the complexity of
the prompts in this ambiguity category for text-to-
image models.
Automatic Evaluations. Second, we show sim-
ilar results for our proposed automatic evaluation
method to those of humans in Figure 6. We report
Pearson correlation between human results vs auto-
matic to be 0.83 and 0.75 for DALL-E Mega and
OpenAI’s DALL-E, respectively. This shows that14373
the proposed automatic metric is in agreement with
human annotators and can be used as a proxy to
human evaluation, saving time and cost. For addi-
tional results on other setups (e.g., VS-TIED) and
more evaluated images refer to Appendix C.1.
Fairness Evaluations. Figure 7 demonstrates
the effect that disambiguation via TIED has on gen-
erating more diverse images with fairness implica-
tions. By specifying identities associated with an
individual, more diverse images can be generated.
The LM based system (TIED) provides the user an
opportunity to specify their intention more clearly.
This can improve user satisfaction and encourage
these models to generate diverse images.
Paraphrasing Evaluations. Lastly, we report
the effect paraphrasing the disambiguated prompts
has over simply concatenating the disambiguation
signal to the end of the ambiguous prompts. Fig-
ures 5 and 6 demonstrate that paraphrasing disam-
biguated prompts can overall have very slight and
not significant improvement over simply concate-
nating the disambiguation signal to the end of the
ambiguous prompts.
5.3 Ablation Studies
We report our main findings here (see Appendix B
for details). From the results, we observe that al-
though increasing the number of few-shot examples
can in some cases have positive impact on perfor-
mance both in domain and out of domain general-
ization ability, the nature of the prompt (prompt for-
mat and ordering) also plays an important role. Our
results also match the previous findings in (Zhao
et al., 2021) in which authors study the effect offew-shot examples provided to language models to
perform various tasks. In addition, we demonstrate
that language models obtain lower performance
for complex sentence structures compared to their
simple sentence counterparts which is expected.
6 Related Work
Resolving ambiguities in different NLP applica-
tions has been a prominent research direction
due to its importance. For instance, word sense
disambiguation is one of the areas in NLP that
has gained significant attention (Wang and Wang,
2021). Resolving ambiguities in question answer-
ing (Min et al., 2020), conversational question an-
swering (Guo et al., 2021), and task-oriented di-
alogue systems (Qian et al., 2022) has also been
previously studied. Ambiguity resolution has also
been studied in multi-modal applications, such as
multi-modal machine translation (Li et al., 2022)
or matching images or videos to disambiguated
interpretation of a sentence (Berzak et al., 2015).
Despite those recent efforts, not much attention
has been paid to ambiguities in text-to-image gen-
erative models. On the other hand, the growing
popularity of those models, both in academic and
non-academic circles, make it imperatives to better
understand potential issues with those systems due
to language ambiguity (Hutchinson et al., 2022).
7 Conclusion
In this work, we study the role of prompt ambiguity
in text-to-image generative models and propose a
disambiguation framework to generate more faith-
ful images better aligned with user intention. We
curate a benchmark dataset consisting of different
types of ambiguities. We measure the ability of
various language models in obtaining disambigua-14374tion signals through end user interaction by either
generating clarifying questions or visual setups.
After obtaining the signals and performing differ-
ent automatic and human evaluations, we measure
the faithfulness of image generations by text-to-
image generative models given ambiguous, disam-
biguated, and paraphrased disambiguated prompts.
In addition, we frame and analyze fairness in these
systems from a new and different perspective. Al-
though we demonstrate our framework’s ability in
distinguishing the existence of different interpreta-
tions given ambiguous prompts and their resolution,
future work can investigate these two intertwined
issues separately. In this work, our focus was on
ambiguity resolution. Future work can focus on
ambiguity detection in this domain.
Limitations
We acknowledge that our benchmark dataset does
not cover all the existing ambiguities and that am-
biguities related to fairness do not cover all the
possibilities. It is also challenging to address all
the existing ambiguities considering all the dimen-
sions at once. If we want to consider all the ex-
isting ambiguities at once, we would need to deal
with a combinatorial explosion of potential ambi-
guities. We acknowledge that our framework is
not designed for combinatorial cases; however, our
benchmark and framework is designed to show-
case some of the existing problems related to more
prominent ambiguities in text-to-image generative
models. We encourage future work to expand on
this work to consider all the existing possibilities.
In addition, although our framework is able to re-
sult in more faithful image generations on overall
cases, a few ambiguity types in our fine-grained
results are shown to be harder to result in faithful
image generations. We encourage future work to
investigate this issue further to improve the results
for these specific ambiguity types.
Ethical Considerations
In this work, we study and propose solutions to
resolve existing ambiguities in prompts given to
text-to-image generative models. In addition to
resolving ambiguities in prompts, this work not
only frames and analyzes fairness from a new and
different perspective, but it also results in more
faithful image generations aligned with end user
intention. These aspects can contribute to numer-
ous positive impacts to the research community.Not only one can generate more diverse images
through disambiguating fairness type ambiguities,
but our framework can also improve user satisfac-
tion by generating aligned images to end user’s
intention despite existing ambiguities in the pro-
vided prompts. Resolving ambiguities can also
avoid spread of misinformation and development
of fallacies. Despite the aforementioned positive
impacts, we also acknowledge the limitations as-
sociated with this work. We acknowledge that our
benchmark dataset is just a very small sample of
different types of ambiguous prompts that can be
provided to a system. In addition, for the fairness
type ambiguities, we only consider gender (male vs
female), skin color (dark vs light), and age (young
vs old). We acknowledge that these are only a
limited number of characteristics that can represent
identity of an individual and that we do not cover all
the cases possible. We agree that we do not cover
all the cases possible; however, our intent is to
showcase a few examples through our benchmark
(TAB) and highlight existing flaws associated with
these systems encountering ambiguous prompts.
In our experiments, we also utilize human anno-
tators. We ensure to provide appropriate guidelines
with a proper compensation to our workers (around
12$ per hour). We also utilize master workers based
in the United States with proper expertise (comple-
tion of more than 1000 HITs with an acceptance
rate above 85%). In addition, we provide the work-
ers the opportunity to raise any concerns about our
task. Based on the feedback, we believe that the
task and the pay was satisfactory to the workers.
We hope that our study can provide valuable in-
sights to the research community with the positive
implications out-weighting its limitations. We also
open-source our benchmark dataset for the com-
munity to benefit from our work. As future work,
researchers can investigate and propose better al-
ternatives than our proposed framework for resolv-
ing ambiguities in text-to-image generative models
along with extension of our work to semantic ambi-
guities in addition to the ones studied in this paper.
Our benchmark dataset can also serve as a valuable
resource for research in commonsense reasoning
studies in text-to-image generative models which
is less explored in our current work. We provide
information in our benchmark dataset (whether an
interpretation is commonsensical or not) which can
be accessible to interested researchers in this area.14375References1437614377Appendix
In this appendix, we will include details that were
left out from the main text of the paper due to space
limitations including experimental setup details as
well as additional results and discussions. We ran
all the experiments on an AWS p3.2xlarge EC2
instance.
A Details About Benchmark Dataset
Here, we will first define each of the different types
of ambiguities existing in our benchmark dataset
(TAB) with a corresponding example. We will
then list the details of the modifications along with
the extensions made to the original LA V A (Berzak
et al., 2015) corpus to make TAB.
A.1 Definitions
Syntax Prepositional Phrase (PP): For this type
of syntactic ambiguity, we borrowed the following
template NNP V DT [JJ] NNIN DT [JJ] NN
from the LA V A corpus (Berzak et al., 2015) to con-
struct most of the cases in TAB. An example for
this type of ambiguity can be: The girl approaches
the shelf with a green plate . It is possible that 1.
the green plate is with the girl or 2. is on the shelf.
Syntax Verb Phrase (VP): For this type of syntac-
tic ambiguity, we borrowed the following template
NNPV [IN] NNPV [JJ] NN from LA V A to con-
struct most of the cases in TAB. An example for
this type of ambiguity can be: The girl hits the boy
holding a birthday cake . It is possible that 1. the
girl is holding the birthday cake or 2. the boy is
holding the birthday cake.
Syntax Conjunction: For this type of syntactic
ambiguity, we borrowed the following templates
NNP[and NNP] V DT JJ NNand NNand
NNP V DT NNor DT NNand DT NNfrom
the LA V A corpus (Berzak et al., 2015) to construct
most of the cases in TAB. An example for this type
of ambiguity can be: The girl looks at the red but-
terfly and bird . It is possible that 1. the bird is red
or 2. the bird is not red.
Discourse Anaphora: For this type of discourse
ambiguity, we borrowed the following template
NNP V DT NNand DT NN. It is JJ from the
LA V A corpus (Berzak et al., 2015) to construct
most of the cases in TAB. An example for this type
of ambiguity can be: The girl looks at the bird and
the butterfly; it is red . It is possible that 1. the bird
is red or 2. the butterfly is red.
Discourse Ellipsis: For this type of discourse ambi-guity, we borrowed the following template NNPV
NNP. Also NNPfrom the LA V A corpus (Berzak
et al., 2015) to construct most of the cases in TAB.
An example for this type of ambiguity can be: The
wolf eats the rabbit. Also the cat. It is possible that
1. the wolf eats both the rabbit and the cat or 2. the
wolf and the cat eat the rabbit.
Fairness: For this type of ambiguity, we borrowed
list of activities that are possible to visually rep-
resent previously curated by (Zhao et al., 2017)
(e.g. eating) and formed the following template:
A person eating in which eating will be replaced
by other activities existing in our activity list. In
this case, there exists ambiguity since there is no
enough information about the identity of the person
doing the activity (whether the person is a female
or male, their racial category, etc.). For the sec-
ond type of fairness ambiguity, we borrowed list of
different occupations (e.g. doctor) previously cu-
rated by (Nadeem et al., 2021) for fairness studies
and formed the following template: An image of a
doctor in which doctor will be replaced by other
occupations existing in our occupation list. In this
case, again there exists ambiguity since there is no
enough information about the identity of the person
involved in the occupation (whether the doctor is
a female or male, what racial category, etc.). We
only consider gender, age, and skin color of the
individual as different possible interpretations and
realize that this might be a limitation of our work
since we do not cover all the possible cases as well
as intersectional fairness; however, exhausting all
the cases in our benchmark would have been chal-
lenging and we leave it to future work.
Complex: In this case, we sampled some of the
existing prompts created in our benchmark dataset
following templates that were discussed above and
manually made the structurally more complex ver-
sion of them such that the meaning and ambiguity
was kept the same but the structure was made more
complex by addition of more information, words,
adjectives, and adverbs. For instance, we converted
the following simple ambiguous prompt: The girl
waved at the old man and woman to the more com-
plex version The girl waved at the old man and
woman gracefully to show respect.
Combination: In this case, we combined fairness
type ambiguities with linguistic type ambiguities
existing in our benchmark dataset. For instance,
The police threatened the doctor with a gun com-
bines the existing linguistic type ambiguity in our14378
benchmark dataset since it is not clear whether the
police is with the gun or the doctor. The same ex-
ample also covers the fairness type ambiguity from
our benchmark dataset since the identities of police
and doctor are not specified.
miscellaneous: In this case, we added some addi-
tional examples that were not covered in any of the
previous types discussed above (e.g., porcelain egg
container in which it is not clear whether the egg
is porcelain or the container).
Our benchmark schema is shown in Table 4. Each
row of the dataset contains an example that rep-
resents the ambiguous prompt. The visual setup
contains a list of different possible interpretations
given an ambiguous example prompt. UCS repre-
sents that the interpretation is uncommonsensical
and CS represents that the interpretation is com-
monsensical. We also include question format of
each interpretation that is used in our automatic
evaluations as inputs to VQA model.
A.2 Modifications and Extensions
Additions: From the original LA V A cor-
pus (Berzak et al., 2015), we borrowed 112 exam-
ples (prompts) that were suitable for our usecase
(e.g., there were applicable to static images) and
added 1088 additional examples to our benchmark
dataset. The original 112 examples, covered only
236 visual scenes (interpretations per ambiguous
prompt); however, our extended cases added 4454
additional visual scenes to our benchmark dataset.
Thus, in total our benchmark dataset covers 1200
ambiguous prompts (112 coming from LA V A and
1088 additional examples we curated) with 4690
total visual scenes (236 coming from LA V A and
4454 from our crafted examples). Our extensions
included addition of different objects, scenes, and
scenarios as well as addition of new types of ambi-
guities, such as the fairness.
Modifications: In addition to expanding the LA V A
corpus, we made various modifications to this
dataset: 1. Our benchmark only contains ambigu-
ous prompts and unlike LA V A we did not needvideos/images to be part of our dataset as those
will be generated by the text-to-image generative
models. We would then evaluate faithfullness of
generations using our benchmark dataset. 2. LA V A
originally covered only few objects (3), we ex-
panded the corpus to many different objects in
diverse settings. 3. Added the fairness compo-
nent. 4. Added the complex component. 5. Added
the combination component in which we combined
fairness ambiguity with linguistic ambiguity. 6.
Added commonsensical vs uncommensensical la-
bel which represents whether each of the interpre-
tations associated to a scene is commonsensical or
not. E.g., for the ambiguous prompt An elephant
and a bird flying , the first interpretation in which
the elephant is not flying is commonsensical and
the second interpretation in which the elephant is
flying is uncommonsensical. Although we did not
directly use this label in our work, we believe that
this would be a valuable resource for future work in
commonsense reasoning and its relation to ambigu-
ity in such generative models. 7. Lava used proper
names to address people in the images/videos. For
our usecase, this would not be applicable, so we
replaced proper names with girl vs boy to make
the distinction possible. 8. Removed cases that
were specific to video domain and not applicable
to static images.
B Details for LM Experiments
For this set of experiments we utilized three differ-
ent language models: GPT-2, GPT-neo, and OPT.
For the GPT-2 model, we utilized the 117M param-
eter pretrained model from huggingface. for the
GPT-neo model, we utilized the 2.7B parameter
model from huggingface. Lastly, for the OPT
model, we utilized the 350M parameter pretrained
model from huggingface.
For the few-shot prompts provided to these lan-14379
guage models refer to Table 5. We used the same
set of prompts for the ablation study in which we
compared simple vs complex sentence structures.
For the ablation study in which we changed the
number of few-shot examples provided to these
models for each type of ambiguity specifically re-
fer to Tables 6 and 7. Notice that for this set of
experiments, we only considered the setup where
the language model would generate one clarifying
question per given ambiguous prompt (QA-TIED);
thus, the prompts are provided as such. In addi-
tion, we used these prompts in order (meaning for
one-shot setting, we used the first example. For
two-shot setting, we used the first and second ex-
amples and so on.).
For the automatic evaluation metrics, we used
BLEU-4and ROUGE-1scores and their imple-
mentations from huggingface. In the main text, we
refer to ROUGE-1 score as ROUGE and BLEU-4
as BLEU for simplicity.
B.1 Results
Automatic evaluation results from generating mul-
tiple clarifying questions as well as generating dif-
ferent possible visual setups (VS-TIED) can be
found in Tables 8 and 9 respectively. Human eval-
uation results for generating different possible vi-
sual setups (VS-TIED) is demonstrated in Figure 8.
The Pearson correlation between ROUGE and hu-
man scores are 0.829 and 0.424 between BLEU
and human scores. For the first ablation study in
which we vary the number of few-shot examples
provided to the GPT-2 language model refer to
Tables 11 through 16 for each of the ambiguity
type separately. Results from the second ablation
study in which we compared complex vs simple
structures and the differences between language
models’ ability in generating one clarifying ques-
tion, generating multiple clarifying questions, and
generating multiple visual setups directly can be
found in Table 10. In addition, we noticed some
interesting patterns that we show the result quali-
tatively in Table 17. We noticed that even for the14380
same sentence, usage of different words caused
the model to generate different outcomes. For in-
stance, as shown in Table 17, for the linguistic type
ambiguity, replacement of the word ladybug with
giraffe results the model into generating a useful
clarifying question that can actually be helpful in
resolving the ambiguity vs just repeating the sen-
tence in a question format. Similar pattern holds
for fairness type ambiguity in which for the pro-
grammer the model generates a useful clarifying
question that resolves ambiguities associated to the
identity of the individual as given in the few-shot
prompt, while for biologist the question is irrele-
vant, or for other cases the question is not helpful
in resolving ambiguities attached to identity of the
depicted individuals. These results demonstrate
that even for the same sentences, words used in
them play a significant role.14381GPT-2 GPT-neo OPT
Ambiguity Type BLEU ROUGE BLEU ROUGE BLEU ROUGE
Total Benchmark 0.31 0.56 0.43 0.57 0.41 0.58
Syntax Prepositional Phrase (PP) 0.12 0.66 0.08 0.61 0.16 0.65
Syntax Verb Phrase (VP) 0.50 0.77 0.60 0.79 0.64 0.82
Syntax Conjunction 0.18 0.65 0.25 0.68 0.09 0.57
Discourse Anaphora 0.12 0.53 0.13 0.54 0.69 0.82
Discourse Ellipsis 0.42 0.70 0.41 0.62 0.62 0.79
Fairness 0.25 0.53 0.54 0.56 0.48 0.57
GPT-2 GPT-neo OPT
Ambiguity Type BLEU ROUGE BLEU ROUGE BLEU ROUGE
Total Benchmark 0.23 0.52 0.20 0.44 0.31 0.60
Syntax Prepositional Phrase (PP) 0.07 0.61 0.06 0.58 0.07 0.60
Syntax Verb Phrase (VP) 0.39 0.80 0.30 0.69 0.39 0.81
Syntax Conjunction 0.15 0.64 0.14 0.56 0.12 0.67
Discourse Anaphora 0.0 0.57 0.06 0.47 0.0 0.76
Discourse Ellipsis 0.0 0.58 0.14 0.60 0.20 0.76
Fairness 0.29 0.50 0.19 0.41 0.40 0.601438214383C Details for Text-to-Image Experiments
Through end user vs GPT-neo interactions in the
setup where GPT-neo would generate one clarify-
ing question (QA-TIED), we obtained 812 visual
setups disambiguated by the end user (3248 im-
ages for 4 images per prompt and 4872 for six
images per prompt) that represented our prompts
for this setup. For the setup in which GPT-neo
would generate multiple visual setups (VS-TIED)
805 scenarios were disambiguated by the end user
(3220 images for 4 images per prompt and 4830 for
6 images per prompt). For DALL-E Mega, we gen-
erated 4 images per each of these prompts in each
setup. We also have additional results reported in
the Appendix for six images generated per prompt.
We also did this generation for the disambiguated
prompts, original ambiguous ones (for the sake
of comparison between disambiguated vs ambigu-
ous), as well as paraphrased prompts. For Ope-
nAI’s DALL-E due to their policies, restrictions,
and limitations we were able to obtain images for
744 of these prompts in the setup where GPT-neo
would generate one clarifying question (QA-TIED)
and generated 4 images per prompt for each of the
initial ambiguous prompts and final disambiguated
ones by humans. For some portion of the prompts
we have six images per prompt. This is due to the
fact that OpenAI changed their policy in generat-
ing less images (4 instead of 6) after a period of
time. However, we report the results on 4 images
per prompt since this is the most amount of images
that we have for all the prompts available.
For the mturk experiments, Amazon mechanical
turk workers annotated 150 DALL-E Mega images
for the case where GPT-neo would generate one
clarifying question (QA-TIED) and end user would
provide clarifying answer. 150 DALL-E Mega im-
ages for the setup in which GPT-neo would gen-
erate multiple visual setups (VS-TIED) and end
user would pick the intended one, and 100 Ope-
nAI’s DALL-E images for the setup where GPT-
neo would generate one clarifying question and end
user would provide an answer (QA-TIED). Overall,
this gave us 400 images. Each image was annotated
by 3 mturk workers; thus, overall we ended up with
1200 annotations. The mturk survey provided to
mturk workers is included in Figure 16. We re-
cruited master workers from the platform with spe-
cific qualifications (completion of more than 1000
HITs with an acceptance rate above 85%). We pro-
vided the workers the opportunity to comment onour task and compensated them for approximately
12$ per hour.
C.1 Results
Automatic as well as human evaluation results re-
porting the percentage of faithful image genera-
tions in DALL-E Mega for the setup in which dif-
ferent possible visual setups are generated (VS-
TIED) by the language model and end user picking
the best option and generated images from this
signal attached to the initial ambiguous prompt
is demonstrated in Figures 9 and 10 respectively.
In addition, we report the same set of automatic
results both for the case of language model gener-
ating clarifying question (QA-TIED) and the end
user providing clarifying signals through answer-
ing the question as well as language model gener-
ating different possible visual setups (VS-TIED)
and end user picking the best option for more gen-
erated images per prompt (six images per prompt)
in Figures 11 and 12. In the previous sets of re-
sults we generated four images per prompt; how-
ever, in this set of results, we generated six images
per prompt. Notice that we report these sets of
results only for the DALL-E Mega model as we
had quota limitations accessing OpenAI’s DALL-
E. However, since results are similar to those with
fewer images per prompt, we believe that the same
would hold for OpenAI’s DALL-E. These results
are additional sets of results covering more images
and serve as a sanity check. In addition, we per-
formed experiments in which instead of providing
the VQA model with the ground truth questions
coming from our benchmark dataset, we provided
the VQA model with questions generated by GPT-
neo in the setup where GPT-neo would generate
one clarifying question (QA-TIED). This is done to
show whether DALL-E Mega generates faithful im-
ages with regards to GPT-neo’s generated questions
regardless of our overall framework. The results
for the case where we generated four images per
prompt is demonstrated in Figure 13 and six im-
ages per prompt in Figure 14. In this case, instead
of reporting the percentage of "Yes"s outputed by
the VQA model, we reported the percentage of
answers that matched end user provided answers
to generated questions by GPT-neo (to report the
faithfulness to end user intention). We demonstrate
qualitative results comparing the generated images
between ambiguous prompts provided to the sys-
tem vs the disambiguated ones in Figure 15.143841438514386ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
The limitations are discussed under the "Limitations" section after conclusion.
/squareA2. Did you discuss any potential risks of your work?
Potential risks are discussed in the "Ethical Considerations" section.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
The abstract and introduction summarize the paper’s main claims made throughout the paper.
/squareA4. Have you used AI writing assistants when working on this paper?
We have only used overleaf and its built in spell-checker.
B/squareDid you use or create scientiﬁc artifacts?
We used scientiﬁc artifacts in our experiments (section 4).
/squareB1. Did you cite the creators of artifacts you used?
Experiments section (section 4).
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
We used only open-sourced artifacts and properly cited them as required by the provider (section 4).
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. We used only open-sourced artifacts and properly cited them as required by the
provider (section 4).
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Our data used does not contain any sensitive information.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Appendix section contains details about artifacts used.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 2 contains detailed statistics about the dataset we used/created (along with additional
information in the appendix section).
C/squareDid you run computational experiments?
Section 4 and 5.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix section contains detailed experimental setup discussions along with sections 4 and 5.14387/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4 and appendix section.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Sections 4, 5, and appendix.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Sections 4, 5, and appendix.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4 and 5 under human evaluation.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
appendix includes the amazon mechanical turk survey screenshot including the instructions and
additional details along with sections 4 and 5.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
appendix, sections 4 and 5 as well as "Ethical Considerations" section includes detailed discussions.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
appendix, sections 4 and 5 as well as "Ethical Considerations" section includes detailed discussions
on our human studies.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
appendix, sections 4 and 5 as well as "Ethical Considerations" section includes detailed discussions
on our human studies.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
appendix, sections 4 and 5 as well as "Ethical Considerations" section includes detailed discussions
on our human studies.14388