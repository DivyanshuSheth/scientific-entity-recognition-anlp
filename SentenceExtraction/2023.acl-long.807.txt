
Dheeru DuaEmma StrubellSameer SinghPat VergaUniversity of California, IrvineCarnegie Mellon UniversityGoogle DeepMind
Abstract
Recent advances in open-domain question an-
swering (ODQA) have demonstrated impres-
sive accuracy on general-purpose domains like
Wikipedia. While some work has been investi-
gating how well ODQA models perform when
tested for out-of-domain (OOD) generalization,
these studies have been conducted only under
conservative shifts in data distribution and typ-
ically focus on a single component (i.e., re-
triever or reader) rather than an end-to-end sys-
tem. This work proposes a more realistic end-
to-end domain shift evaluation setting covering
five diverse domains. We not only find that end-
to-end models fail to generalize but that high
retrieval scores often still yield poor answer pre-
diction accuracy. To address these failures, we
investigate several interventions, in the form
of data augmentations, for improving model
adaption and use our evaluation set to elucidate
the relationship between the e fficacy of an in-
tervention scheme and the particular type of
dataset shifts we consider. We propose a gen-
eralizability test that estimates the type of shift
in a target dataset without training a model in
the target domain and that the type of shift is
predictive of which data augmentation schemes
will be e ffective for domain adaption. Over-
all, we find that these interventions increase
end-to-end performance by up to ∼24 points.
1 Introduction
General-purpose open-domain question answer-
ing (ODQA; Chen et al. (2017); Lee et al. (2019);
Izacard et al. (2022)) is an important task that auto-
mates reading and understanding a large corpus of
documents to answer a given question succinctly.
It is especially crucial in fields such as biomedicine,
legal, news, etc., where more documents are added
daily, outpacing the speed at which a user can
process the information. Current state-of-the-art
ODQA systems perform a two-stage pipeline pro-
cess (Izacard et al., 2022): 1) Given a questionFigure 1: Effect of interventions on dataset shifts .
Top: Average end-to-end performance of source do-
main model is quite poor when applied to OOD datasets.
Source model (trained on general-purpose domain) per-
formance improves when adapted to unseen target do-
main with interventions. Bottom: Drill-down of perfor-
mance into zero and few-shot data augmentations aver-
aged over target datasets exhibiting these shifts shows
covariate and concept shifts respond to zero and few-
shot data augmentations. Target datasets with No shift
do not improve much with any intervention while full
shift benefits most from Few-shot.
and document corpus, a retriever (Karpukhin et al.,
2020; Izacard et al., 2021; Ra ffel et al., 2020) se-
lects relevant passages and 2) a question answer-
ing model, also known as a reader (Izacard and
Grave, 2021) answers the given question based
on the retrieved passages. This decoupling allows
for independent advances in domain adaptation of
general-purpose retrievers (Thakur et al., 2021) and
question-answering (Fisch et al., 2019) models.
To enable practical application, an ODQA sys-
tem should assist humans in keeping up with new
knowledge without requiring annotations for every
new domain or concept. For this, the system should
be resilient to changes in the document, question,14429and answer distributions. Unfortunately, the cur-
rent work in ODQA focus solely on Wikipedia
corpus and do not study e ffectiveness of a model
trained on such a general-purpose domain when
applied to an unseen domain. To gauge how likely
it is for a source domain model to succeed on an
unseen domain we need to understand its ability to
work out-of-the-box or even adapt to a new target
domain, under varying types and degrees of dataset
shifts. (Quinonero-Candela et al., 2008).
In this work, we study the challenges and inter-
ventions for generalizing ODQA models to new
domains via four contributions. First, to under-
stand how well the state-of-the-art ODQA system
(trained on the general-purpose domain) performs
on a variety of target distributions, we define a
collection of datasets for evaluating domain gen-
eralization. We aggregate a set of seven ODQA
datasets spanning five di fferent domains (§2). We
observe that the source ODQA model does not
generalize well (Fig.1, Top) on this collection (§4).
Second, to automatically determine the type of data
shift with only a small number of labeled target do-
main examples, we propose a generalizability test .
This test assesses the type and degree of shift a
new domain su ffers with respect to the source do-
main (§3). Third, to understand the adaptability of
the source model to a target domain, we analyze
the performance of various intervention schemes,
including existing zero-shot in-domain question
generation and a novel few-shot language model-
aided generation. These schemes create data akin
to the target domain which is augmented with the
source domain to learn an adapted version of the
source model. Overall, we observe improvement in
performance across all the target datasets (Fig. 1).
The degree of improvement depends on the inter-
vention scheme and underlying dataset shift (§5).
Finally, we propose a simple and e ffective few-shot
method that improves the performance by up to
24% in F1. This method prompts a large language
model with 8 examples to generate examples for
further adaptation.
Putting it all together, we use the generalizability
test to gauge the type and degree of dataset shift
in a target dataset. Then, we empirically show that
certain types of dataset shifts respond well to spe-
cific intervention schemes (§5, Fig. 1). This helps
ascertain whether we can adapt a source model to
unseen domain with minimal supervision. The re-
sources used in this work are released at https://github.com/dDua/adapt-or-annotate
2 Background and Evaluation Setup
An ODQA model learns interactions among three
random variables: Question ( Q), answer ( A) and
context corpus ( C). For a given q∈Q, first the
retrieverRreturns a set of passages, C=R(q,C).
These passages are then sent to a reader model M
to obtain the final answer, ˆ a←M (a|q,C).
Following prior work, we evaluate retriever per-
formance with the Acc@k metric, which computes
if the oracle answer is found in the top- kretrieved
passages. We set k=100 in all of our experiments.
For reader performance, we compute token-level
F1 between the oracle and predicted answer.
2.1 Datasets
We test the generalization capabilities of a model
trained on a source domain when applied to seven
datasets in five very di fferent target domains .
Source Domain: For source domain we use
documents from English Wikipedia and QA
pairs for supervision from NaturalQuestions
(NQ) (Kwiatkowski et al., 2019) and BoolQ (Clark
et al., 2019). We treat this domain as our source
as it is used for the vast majority of current work
in ODQA (and many other areas of language re-
search). In addition to the supervised training
data from NQ and BoolQ, we also consider cloze-
style questions derived from the QA pairs in NQ.
For each QA pair, we retrieve a sentence from
Wikipedia with the highest BM25 similarity score.
We convert the retrieved sentence into a cloze-style
question by replacing the answer string in the sen-
tence with sentinel markers (Ra ffel et al., 2020).
Target Domains: For our target corpora, we
re-purpose seven open-domain QA and /or read-
ing comprehension datasets spanning five di ffer-
ent domains (Stack Overflow, Reddit, Pubmed,
Japanese Statute Law codes, CNN /DailyMail, and
Wikipedia). The datasets Quasar-S (Dhingra
et al., 2017), Quasar-T (Dhingra et al., 2017),
SearchQA (Dunn et al., 2017) and BioASQ (Ba-
likas et al., 2015) were introduced as ODQA14430datasets over Stackoverflow, Reddit, Wikipedia,
and Pubmed corpus respectively. Additionally,
we re-purpose reading comprehension datasets,
NewsQA (Trischler et al., 2017) and CliCR (Šuster
and Daelemans, 2018) as ODQA datasets, by re-
trieving a set of passages for each QA pair from
Pubmed and CNN /Dailymail corpus. For COL-
IEE (Rabelo et al., 2022), we convert the original
entailment questions into boolean questions and
retrieve passages from legal code statutes provided
with the task. We confirm that these reading com-
prehension datasets can be reasonably re-purposed
for our ODQA setup by achieving a reasonable end-
to-end performance of ODQA models trained on
gold target domain QA pairs with BM25 retrievals
from the target corpus (UB-Ret, Fig. 3).
2.2 Models
We compare four retrievers : (1) BM25 (Robert-
son and Spärck Jones, 1994) (sparse and unsuper-
vised), (2) Contriever, semi-supervised with MS-
MARCO (Izacard et al., 2021), (3) Dense Passage
Retriever (DPR) (Karpukhin et al., 2020), and
(4) the state-of-the-art source domain model Spi-
der (Ram et al., 2022). DPR and Spider are dense
and supervised. As for reader , we use the state-
of-art fusion-in-decoder (FiD) model (Izacard and
Grave, 2021) that uses the top 100 documents to
generate the final answer.
3 Generalizability Test
There are many aspects that determine in what
ways and to what extent one data distribution di ffers
from another. It is often challenging to quantify the
degree of generalizability or diverseness for a new
domain without collecting enough samples to train
a model in the new domain. To address this issue,
we propose a method to assess the type and degree
of diversity by utilizing only a few examples from
the target domain as an evaluation set.
3.1 Types of dataset shift
Different types of dataset shifts (Quinonero-
Candela et al., 2008) have been proposed in the
literature but they are often studied in a classifica-
tion setup. For our application, we consider con-
cept andcovariate shifts which are more amenable
to our pipelined ODQA setup — with input as a
joint distribution over question and contexts and
output as a distribution over answers given question
and contexts as input.No shift occurs when the input and output distri-
butions match across the source and target domains.
Concept shift (Widmer and Kubát, 2004) oc-
curs when the input distribution of the source and
target domains match, i.e., p(x)=p(x)while
the output distribution between source and target
domain does not match, p(y|x)/nequalp(y|x).
Covariate shift (Zadrozny, 2004) occurs when
the source and target input distributions do not
match, i.e. p(x)/nequalp(x)while the output dis-
tributions match p(y|x)=p(y|x).
Full shift occurs when both the source and target
input and output distributions do not match.
3.2 Calculating shift for ODQA
We characterize the shift in ODQA as a two-step
process. First, we compute the input distribution,
i.e, the joint question and context distribution us-
ing un-normalized (energy) scores from a dense
retriever (Karpukhin et al., 2020) that quantifies
the compatibility between a given question, qand
a context, cviaR(q,c). Then, we normalize the
scores from the retriever over a set of contexts. Ide-
ally, the set of contexts should be the entire target
domain document corpus, however, that can be
prohibitively computationally expensive and also
results in a high entropy distribution. Instead, we
use a subset of contexts, C, from the entire cor-
pusC. We ignore the prior over questions since
it remains constant when calculating the context
distribution for a specific question. Instead, we
approximate the joint with conditional distribution
over contexts given question.
p(q,c)∝R(q,c)/summationtextR(q,c)(1)
In the second step, we test whether the output
distributions match by computing the likelihood of
generating the oracle answer given a question, q,
and the relevant contexts, C. In an ideal scenario,
we can do this by performing global normaliza-
tion (Goyal et al., 2019) over all possible answer
spans in the corpus which is intractable. Instead,
we use a sub-sample of answers, A, to compute the
output distribution as shown below.
p(a|q,C)=/producttextM(a|a,q,C)/summationtext/producttextM(a|a,q,C)(2)14431
3.3 Predicting type of dataset shift
To compute the type of shift (§3.1), we need a
model trained on the target domain ( p) which
requires a large number of examples. However,
our goal is to determine if a source model can be
adapted to the target dataset with only a few exam-
ples for target evaluation. To do this, we conceptu-
alize adapting or fine-tuning a pre-trained source
model as a Bayesian framework. In this frame-
work, the source model acts as a prior which when
exposed to interventional data (for adapting) and
target data (for fine-tuning), results in an adapted
or fine-tuned posterior distribution. If the prior
(source model) contains an informative signal with
respect to the target dataset then we do not require
much supervision to learn an e ffective posterior.
However, if the prior is non-informative we need a
lot of supervision to learn the posterior. Towards
this end, we devise a generalizability test , where
we use a small set of evaluation examples sam-
pled from each target dataset to compute input and
output distribution using the source domain model.
Then, we compare these distribution with the a non-informative prior like uniform distribution and in-
formative prior like the oracle distribution to gauge
if the source model is closer to uniform or oracle
distribution. This helps us assess the e ffectiveness
of the source model towards the target dataset with-
out having to train a model in the target domain.
Input /Retriever Distribution: To determine if
the input distribution contains informative signal
with respect to target evaluation set, we need to
compute the distance of the input distribution from
uniform and oracle distribution. To do this, we
follow Eq. 1 and compute the input distribution,
with passages from across examples in the entire
target evaluation set as the subset for normalizer
computation. Then, for a each question, we com-
pute the Wasserstein distance, w, (Kantorovich,
1960) between the input distribution and the uni-
form distribution and average these values over all
the examples in the target evaluation set. Similarly,
we also compute the distance between the gold or
oracle distribution and the input distribution as w.
Ifw>w, we conclude that the target distribution
is far from the uniform distribution and closer to the
gold distribution, indicating that the source model
is compatible with the target distribution (Fig. 2).
Output /Reader Distribution: In similar vein as
input distribution, we need to compare the output
distribution with corresponding uniform and oracle
distribution over answers. To do this, we follow
Eq. 2 and compute the output distribution, with set
of answer spans from across all the examples in
the target evaluation set for normalizer computa-
tion. Then, we compute the Wasserstein distance
between the uniform and output distribution aver-14432aged over the target evaluation set as v.
In an ideal scenario, we would compare the dis-
tance between and oracle and output distribution
with v, similar to input distribution. However, em-
pirically we find that output distribution is always
closer to uniform than oracle, even when evaluated
on source domain. We believe this is because of
two reasons. First, the conditional answer gener-
ation model (M) is not trained with a contrastive
loss like the retriever, resulting in a high entropy
answer likelihood distribution. Second, the sup-
port set of answers used for normalization contains
only grammatically correct answer spans making
the likelihood scores attenuated. To address these
issues, we use a reference answer conditional dis-
tribution to de-bias the likelihood scores with a
threshold. To obtain this threshold, we consider the
source distribution as a reference and compute the
distance between output distribution evaluated on
examples from source evaluation set and the uni-
form distribution as v. Since the reference based
output distribution is in-domain, it should be far
from the uniform distribution and closer to oracle
distribution. As a result, if v−vis close to 0, we
assess that the target is far from uniform and that
source model is compatible with the target dataset.
In Figure 2, we put this altogether as a decision
tree to identify the type of dataset shift. We observe
that SearchQA falls under the No shift category as
it is close to the source domain, hence, we con-
jecture that it will observe minimal improvements
under most data intervention schemes as the source
model already captures the target distribution (§5).
We also conjecture that datasets falling under Con-
cept shift andCovariate shift are more amenable
to zero-shot data interventions, while, Full shift
would benefit more from few-shot or in-domain
annotations from the target domain. We consider
few shot augmentations as a proxy for annotating
examples in the target domain because they are
generated with supervision from target dataset.
4 How Well do Models Generalize?
We test the OOD performance of the source model
on target datasets and analyze the failures.
4.1 Reader Generalization
In Fig. 3, we test the end-to-end performance of
three model variants:
Source: a reader trained with source dataset and
contexts retrieved by BM25, demonstrating zero-shot generalization performance.
Upperbound-Reader (UB-READ): a reader
trained on the target dataset with contexts retrieved
by BM25 – the overall strongest retriever.
Upperbound-Retriever (UB-RET): a reader
trained on the target dataset with gold contexts
to approximate upper-bound performance.
We observe large performance drops when eval-
uating the source model on target domains (Fig. 3),
especially when the target corpus di ffers from
Wikipedia, such as in Quasar-S (Stack Overflow)
and CliCR (PubMed), even though the model re-
quires similar reading capabilities to those needed
in the source domain. Interestingly, even though
BM25 retriever accuracy is relatively high on the
target datasets (Fig. 4, ∼83% Acc@100 on Quasar-
S), that accuracy does not translate to strong reader
performance (Fig. 3, ∼11% F1 on Quasar-S).
To understand this performance gap, we
manually sample 50 predictions from each target
dataset where retrieved passages contain the
oracle answer but the reader produced an incorrect
prediction. We observe that in ∼65% cases, the
Acc@100 metric yields a false positive , where
the passage contains an exact string match of the
correct answer, but the context does not actually
answer the given question. In other cases, the
reader is unable to understand the context. For
example, for the question: and answer: , the retrieved
passage: is credited
(incorrectly) as context answering the question.
4.2 Retriever Generalization
We compare the zero-shot generalization of four
retrieval models in Fig. 4. Spider, which is the
best performing model on the source domain, ex-
hibits improvement on SearchQA ( ∼1%) (which
is similar to source distribution), but shows large
drops in performance when applied to the target
datasets:∼40% on NewsQA, ∼28% on Quasar-T
and, Quasar-S. To understand the drop, we manu-
ally analyze 50 random incorrect predictions from
Spider. We observe two major failure modes.
First, we find that dense models are sensitive
to changes in the length of contexts. When ex-
posed to documents with heterogeneous lengths,
models tend to over-retrieve shorter contexts. To14433
quantify the sensitivity to changes in lengths on
source domains itself, we pool passages from all
target corpus into a combined index. We ob-
serve that the performance of Spider when ex-
posed to this combined index reduces by ∼15%
and restricting the minimum length of contexts to
50 words alleviates the problem and recovers the
original performance. The second common fail-
ure mode occurs due to changes in distribution
of entity types from source to target. For exam-
ple, words like in refers to in Wikipedia, while in case of
PubMed it often refers to living organic matter (Sci-
avolino et al., 2021). Overall, BM25, being an
unsupervised method, has the best performance
across all domains.
5Interventions for Improving Adaptation
Domain Adaptation is shown to be a causal inter-
vention (Jin et al., 2021) mechanism to e ffectively
understand impact of an augmentation technique
without much concern about spurious correlations.
5.1 Zero-shot adaptation methods
We perform a set of zero-shot data intervention
methods, where we consider the e ffect of change
in distribution of each random variable: Question,
answer and context one at a time, while keeping
the other two fixed.
Varying context distribution To test the e ffect
of change in context distribution, we pool passages
from all corpora into a combined index. We ob-
serve that supervised models like Spider are sen-
sitive to out-of-domain distractors, unlike BM25,
especially when the target dataset uses same cor-
pus as source (Wikipedia). For example, SearchQA
suffers a performance drop of ∼15%. On average
we see a performance improvement of ∼2% (w /o
COLIEE) when the target index is changed to the
combined index. BM25 still out-performs Spider
on average by 19.1% with the combined index.
However, we observe a drop in performance of
the FiD reader of up to ∼5% in F1 for NewsQA
with the combined index. More details are in the
appendix (Figs. 5 and 6.)
Varying answer distribution Many works (Gu-
rurangan et al., 2018; Dua et al., 2020; Jiang and
Bansal, 2019) have shown that bias in the answer
prior distribution can introduce spurious correla-
tions in model learning. This e ffectively improves
the model performance at the cost pf OOD gen-
eration. To test whether we can improve the per-
formance of adapted source model by varying the
answer distribution, we experiment with a variety
of answer distributions over plausible set of answer
spans. To obtain the set of answer spans, we extract
and annotate coarse-grained entity types from the14434
target corpus using spaCy. We use this coarse-
grained entity type information as a set of classes
from which to choose 50k entities with four di ffer-
ent sampling strategies: Most frequent, uniform,
randomly sampled based on entity type categories,
and sampling in proportion to entity type distribu-
tion of answers in the target training set.
The source model has reasonable end-to-end per-
formance on BioASQ, even with passages from
the source corpus (Wikipedia), suggesting that
it contains su fficient information for answering
many BioASQ questions. Consequently, we se-
lect BioASQ for these controlled experiments (Ap-
pendix Fig. 6). This allows us to use the Wikipedia
corpus alone for retrieval, which makes it easier to
fix the passage distribution. In Table 2, we show
that uniform sampling boosts retriever performance
compared to random sampling, allowing the model
to learn from all types of answers and generalize
better to unseen answer distributions. On the other
hand, the best reader model performance is when
we know the correct answer distribution of the tar-
get dataset up front, showing that the answer pri-
ors influence reader performance. However, in a
zero-shot setup, we do not have access to this dis-
tribution, so we adopt the second-best technique,
uniform sampling from across the entity type cate-
gories, in the following experiments.
Varying question distribution We vary the ques-
tion distribution by augmenting the source domain
with QA pairs generated using two di fferent meth-
ods. Our first approach (QGen) uses a question gen-
eration model (Subramanian et al., 2017) trained
on the source domain to generate a question given
a passage and an answer. This question genera-
tion model is applied to a new target passage and
a plausible answer span from the passage (Shakeriet al., 2020; Krishna and Iyyer, 2019; Song et al.,
2018; Klein and Nabi, 2019). The second approach
(Cloze QA), which has been less explored previ-
ously, converts a sentence in the target corpus to a
fill-in-the-blank style cloze question (Taylor, 1953)
by masking a plausible answer span (entity men-
tion) in the sentence. We sample answer spans
uniformly based on an entity type distribution from
the target corpus and then query our combined in-
dex to create a dataset containing cloze style ques-
tions aligned with relevant documents. We use
these same sampled answers to generate standard
QGen QA pairs as well. We combine these data
interventions with our initial source domain data
to train a DPR retriever and a FiD reader (Table 3).
We observe similar average performance across
both intervention types in retriever and reader mod-
els. However, cloze QA pairs are computationally
much more e fficient to generate as they do not re-
quire additional question generation models.
Discussion on generalizability test In §3, we
hypothesized that datasets with less severe shift
(Quasar-S, Quasar-T, and BioASQ) would show
more performance improvements with zero-shot
adaptation as compared to datasets with severe shift
(CliCR and NewsQA). Indeed, we observe an avg.
improvement of about 8.5% F1 on datasets having
Concept and Covariate shift while only 3.5% F1
on datasets with Full shift in Table 3. Moreover,
in Fig. 1, we see that target datasets with No shift ,
do not show much improvement with any inter-
vention as the source model already captures the
distribution. Datasets with Full shift need few-shot
examples for better adaptation while datasets with
Concept andCovariate shift are able to adapt with
zero-shot data interventions.
5.2 Few-shot Generalizability and
Adapatability
Zero-shot adaptation does not work well when the
target distribution is far from the source. For these
cases, we experiment with few-shot adaptation.
Few-shot data generation Zero-shot interven-
tions like QGen are trained on the source and do
not produce generations that are fully compatible
with the target domain and thereby do not provide
much useful signal. An alternative approach would
be to train a question generation model with a few
examples from the target domain. However, it is
difficult to adapt or fine-tune a question genera-14435tion and answering model (for validating QA pair
correctness) with very few examples.
To capture target distribution without a lot of su-
pervision, we propose a few-shot technique (Data-
Gen) that prompts a large language model (LLM;
Chowdhery et al. (2022)) to generate a sentence
given a passage. We use eight seed examples from
the target domain to generate additional training
data to help bootstrap adaptation in the target do-
main. We observe that it is easier for large language
models to condition on a single variable (context)
and compress (Goyal et al., 2022) multiple facts
from the passage into a single sentence, as com-
pared to conditioning on a context and answer span
together. Moreover, in section 5.1 we observed
that augmentation with cloze-style QA pairs yields
similar performance to using question-formatted
QA pairs, o ffering evidence that the precise format
is not as important as the content itself.
We prompt the model in the following for-
mat: for PubMed arti-
cles. For other target corpora we replace
with , , and for Stack
Overflow, DailyMail, and Reddit respectively. To
filter out invalid sentences, we remove any genera-
tion that: 1) includes a number, 2) does not repeat
part of the passage verbatim, and 3) has less than
75% word set overlap with the passage (after re-
moving stopwords). To gauge the precision of our
generations, we sampled 20 generated sentences
for each dataset and found that they are correct
more than 70% of the time. To test retriever perfor-
mance, we train a DPR model with source domain
data and∼8k examples containing pairs of origi-
nal passage and generated sentence for each target
dataset. We observe performance improvements of∼18% on NewsQA, ∼13% on CliCR, and ∼24%
on Quasar-S (Table 4). Moreover, LLMs contain
substantial factual knowledge in their parameters
and we observe that they do particularly well in a
closed-book setting on datasets with trivia-based
factual questions, like SearchQA and Quasar-T, but
do not perform well in other cases. Following our
conjecture in §3, datasets with Full shift on aver-
age show an improvement of 12.1% with few-shot
interventions, compared to 3.5% with zero-shot,
which is also evident in Fig. 1. We show qualitative
examples in Appendix (Fig. 8).
6 Related Work
Domain generalization in readers The most
popular work in generalization in reading com-
prehension was introduced as part of the
MRQA (Fisch et al., 2019) challenge, which fo-
cuses on transfer learning from multiple source
datasets to unseen target datasets (Gottumukkala
et al., 2020). This multi-task learning setup re-
quires the model to perform complex reasoning at
test time that may be unseen at training. However,
in this work, we focus on the generalization capa-
bilities of an end-to-end ODQA setup that is able
to understand passages in the new domain and not
the ability to perform unseen reasoning.
Domain generalization in retrievers A recent
line of work that tests domain generalization of re-
trievers (Petroni et al., 2021; Ram et al., 2022; Izac-
ard et al., 2022) focuses on conservative changes
to the source domain, for instance, testing gener-
alization of a model trained on Natural Questions
applied to WebQuestions (Berant et al., 2013) or
TriviaQA (Joshi et al., 2017), all of which use the
same Wikipedia corpus. BEIR is a recent retrieval
benchmark, (Thakur et al., 2021) tests the general-
izasbility of only the retriever in isolation and not
end-to-end ODQA performance, which is a brittle
metric.
Domain adaptation work in retrievers (Dai et al.,
2022) generate passages using few shots but do not
require the answer to be correct. Ma et al. (2021)
performs a zero-shot adaptation using noisy labels
for retrievers. Siriwardhana et al. (2022) utilizes
examples from the target domain in a transfer learn-
ing setup while we work in a zero to a few shot
setting.
Domain generalization in other tasks Inciden-
tal supervision signals in (He et al., 2021) deter-14436mine which dataset has a useful signal for a target
classification task. Similar to (Fisch et al., 2019),
in machine translation, various works (Dua et al.,
2022; Fedus et al., 2022) learn to balance positive
and negative feature transfer from multiple source
domains to a target domain.
7 Conclusion
We investigate failures of ODQA models under
non-conservative dataset shift. We also propose a
way to test compatibility of source model with new
domains without much supervision. We establish
how di fferent dataset shift behave under a variety
of intervention schemes. We hope future research
will adopt these target datasets for evaluation.
8 Limitations
This work focuses on English QA datasets only.
Similar techniques should apply in other languages
as well; however, we did not evaluate them. The
augmentations generated are di fficult to validate for
yes/no questions for the few-shot method. More-
over, it can be challenging to generate these aug-
mentations if access to large LM is unavailable.
However, under those scenarios, data in the target
domain should be annotated, which ideally would
perform better than the few-shot setting. Our mod-
els also su ffer from similar problems as LLMs, like
hallucinations, misinformation, etc.
9 Ethics Statement
This works focuses on testing the generalization
and adaptability of general-purpose models to vari-
ous domains. It uses existing training data and con-
ventional methods of testing model performance.
This work does not deal with any social impacts or
biases in natural language processing systems.
Acknowledgements
We would like to thank William Cohen, Haitian
Sun, Tom Kwiatkowski and the anonymous review-
ers for their feedback. This work was partly sup-
ported by the DARPA MCS program under Con-
tract No. N660011924033 with the United States
Office Of Naval Research.
References14437144381443914440A Experimental details
We used JAX on TPUs for training reader models
and PyTorch on GPU for training retriever mod-
els. We used open-source github implementations
for DPR, Contrieverand Spider. For retriev-
ing top-100 passages for reader input, we used
ScaNNlibrary. We use T5-base model for reader
and BERT-base for retriever. We fine-tune the re-
triever and reader with learning rate 1e-5 and 5e-5
respectively.
B How are evaluation sets curated?
We consider validation sets from each of the tar-
get dataset, BioASQ, CliCR, Quasar-S, Quasar-T,
NewsQA, SearchQA, COLIEE as part of our evalu-
ation set. SearchQA, Quasar-S and Quasar-T were
already published as ODQA datasets so we used
them as it is while we had re-purpose some of
the other datasets that were not originally ODQA
dataset by processing them as described below.
COLIEE: The COLIEE Shared Task 4provides
a list of Japanese legal codes in English language.
To convert these legal codes from a flat list into
paragraphs, first we split them into specific article
sections with regex string "Article [0-9] +". We
further split each article into passages containing a
maximum of 256 words.
NewsQA: We created an index on
CNN /Dailymail documents by splitting them into
passages of 256 words and pooled them together
to create a corpus.
CliCR and BioASQ: We used PubMed corpus
published as part of BEIR (Thakur et al., 2021)
benchmark. We split the pubmed abstracts in this
corpus into passages of size 256 words.
C Varying context distribution
As described in §5.1, we test retriever (Fig. 5) and
reader performance (Fig. 6) when exposed to di ffer-
ent set of passage. Fig. 6 shows reader performance
with passages retrieved with BM25 on source (i.e.
wikipedia), target (i.e. respective target corpus)
and combined (i.e. all corpora pooled together).
Fig. 5 compared performance of Spider and BM25with Target (i.e. dataset specific target corpus) and
Comb (i.e. all corpora pooled together)
D Varying answer distribution and
pre-training corpus
Following §5.1 we try to understand the impact
of pre-training and fine-tuning corpus on answer
distribution. We do this by comparing the perfor-
mance of the FiD reader initialized from T5 pre-
trained on common-crawl dataset(C4) compared to
one that was pre-trained on PubMed articles (Ta-
ble 5). After pre-training, both models are then
fine-tuned on our source domain data. In this case,14441
we observe that fine-tuning on a domain that di ffers
from that used in pre-training results in deteriora-
tion of model performance.
E Degree of domain shift
In Table 1, we showed only di fferences that gov-
erned which side of decision tree the shift types
were categorized into, while in Table 6 we show all
the raw distance values.
F Statistical Significance
The number of examples in all datasets except
COLIEE are in the order of thousands, making
the performance improvements significant. In the
case of COLIEE, which has a boolean output space
(i.e. answers are yes /no), we performed a binomial
test to test the significance of few-shot reader per-
formance in Table 4. The number of samples n
=116 (number of test examples), p=0.468 and
p=0.616. We will reject the null hypothesis that
baseline and few-shot distribution are equivalent,
when P(X>=p∗n)<=0.05, where X is drawn
from a binomial distribution, i.e., X∼B(n,p)
(Berg-Kirkpatrick et al., 2012) and we can compute
the L.H.S to be, P(X>=0.616∗116) =0.00006
making it significant.144421444314444ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 8
/squareA2. Did you discuss any potential risks of your work?
This is a study of existing work, we do not use a lot of compute resources for running these experiments
so this point is not applicable.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1(introduction)
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Not applicable. Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Not applicable. Left blank.
C/squareDid you run computational experiments?
Section 4 and 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix14445/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4 and 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4, 5 and Appendix
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.14446