
Ao Liu, An Wang, Naoaki Okazaki
Tokyo Institute of Technology
liu.ao@nlp.c.titech.ac.jp
wang@de.cs.titech.ac.jp
okazaki@c.titech.ac.jp
Abstract
Formality style transfer (FST) is a task that in-
volves paraphrasing an informal sentence into
a formal one without altering its meaning. To
address the data-scarcity problem of existing
parallel datasets, previous studies tend to adopt
a cycle-reconstruction scheme to utilize addi-
tional unlabeled data, where the FST model
mainly benefits from target-side unlabeled sen-
tences. In this work, we propose a simple yet
effective semi-supervised framework to better
utilize source-side unlabeled sentences based
on consistency training. Specifically, our ap-
proach augments pseudo-parallel data obtained
from a source-side informal sentence by en-
forcing the model to generate similar outputs
for its perturbed version. Moreover, we em-
pirically examined the effects of various data
perturbation methods and propose effective
data filtering strategies to improve our frame-
work. Experimental results on the GYAFC
benchmark demonstrate that our approach can
achieve state-of-the-art results, even with less
than 40% of the parallel data.
1 Introduction
Formality style transfer (FST) (Rao and Tetreault,
2018) has garnered growing attention in the text
style transfer community, which aims to transform
aninformal -style sentence into a formal one while
preserving its meaning. The large amount of user-
generated data from online resources like tweets
often contain informal expressions such as slang
words (e.g., gonna ), wrong capitalization or punc-
tuations, and grammatical or spelling errors. FST
can clean and formalize such noisy data, to benefit
downstream NLP applications such as sentiment
classification (Yao and Yu, 2021a). Some examples
of FST data are presented in Table 1.
With the release of the FST benchmark Gram-
marly Yahoo Answers Corpus (GYAFC) (Rao and
Table 1: Examples of informal-formal sentence pairs.
Tetreault, 2018), previous studies on FST tend
to employ neural networks such as sequence-to-
sequence (seq2seq) models to utilize parallel (infor-
mal and formal) sentence pairs. However, GYAFC
only contains 100k parallel examples, which lim-
its the performance of neural network models.
Several approaches have been developed to ad-
dress the data-scarcity problem by utilizing un-
labeled sentences. In a previous study, Zhang
et al. (2020) proposed several effective data aug-
mentations methods, such as back-translation, to
augment parallel data. Another line of research
(Shang et al., 2019; Xu et al., 2019; Chawla and
Yang, 2020) conducted semi-supervised learning
(SSL) in a cycle-reconstruction manner, where
both forward and backward transfer models were
jointly trained while benefiting each other by gen-
erating pseudo-parallel data from unlabeled sen-
tences. Under this setting, both additional informal
and formal sentences are utilized; however, the
forward informal →formal model mostly benefits
from the target -side (formal) sentences, which are
back-translated by the formal →informal model to
construct pseudo training pairs. Conversely, the
formal→informal model can only acquire extra su-
pervision signals from informal sentences. Because
the main objective of FST is the informal →formal
transfer, the additional informal sentences were
not well utilized in previous studies. In addition,
these semi-supervised models incorporate many
auxiliary modules such as style discriminators,
to achieve state-of-the-art results, which result in
rather complicated frameworks and more model
parameters.
As noisy informal sentences are easier to ac-4689quire from online resources, we attempt to take a
different view from existing approaches, by adopt-
ing additional source -side (informal) sentences via
SSL. We gain insights from the state-of-the-art
approaches for semi-supervised image and text
classification (Sohn et al., 2020; Xie et al., 2020;
Berthelot et al., 2019; Zhang et al., 2021; Chen
et al., 2020) and propose a simple yet effective
SSL framework for FST using purely informal sen-
tences. Our approach employs consistency training
to generate pseudo-parallel data from additional
informal sentences. Specifically, we enforce the
model to generate similar target sentences for an
unlabeled source-side sentence and its perturbed
version, making the model more robust against
the noise in the unlabeled data. In addition, a su-
pervised loss is trained simultaneously to transfer
knowledge from the clean parallel data to the unsu-
pervised consistency training.
Data perturbation is the key component of con-
sistency training and significantly affects its per-
formance. To obtain a successful SSL framework
for FST, we first empirically study the effects of
various data perturbation approaches. Specifically,
we explore easy data augmentation methods, such
as random word deletion , and advanced data aug-
mentation methods, such as back-translation . We
also handcraft a line of rule-based data perturba-
tion methods to simulate the features of informal
sentences, such as spelling error injection . Further-
more, we propose three data filtering approaches
in connection with the three evaluation metrics of
FST: style strength, content preservation, and flu-
ency. Specifically, we adopt style accuracy ,source -
BLEU, and perplexity as three metrics to filter out
low-quality pseudo-parallel data based on a thresh-
old. We also propose a dynamic threshold algo-
rithm to automatically select and update the thresh-
olds of source-BLEU and perplexity.
We evaluate our framework on the two domains
of the GYAFC benchmark: Entertainment & Music
(E&M) and Family & Relationships (F&R). We fur-
ther collect 200k unpaired informal sentences for
each domain to perform semi-supervised training.
Experimental results verify that our SSL frame-
work can enhance the performance of the strong
supervised baseline, a pretrained T5-large (Raf-
fel et al., 2020) model, by a substantial margin,
and improve the state-of-the-art results by over 2.0
BLEU scores on both GYAFC domains. Empiri-
cally, we also deduce that simple word-level dataaugmentation approaches are better than advanced
data augmentation methods that excessively alter
the sentences, and spelling error injection is espe-
cially effective. In addition, our evaluation-based
data filtering approach can further improve the per-
formance of the SSL framework. Furthermore, we
also conduct low-resource experiments by reducing
the size of parallel data. Surprisingly, our frame-
work could achieve the state-of-the-art results with
only less than 40% of parallel data, demonstrat-
ing the advantage of our method in low-resource
situations.
2 Related Work
Formality style transfer FST is an important
branch of text style transfer. For FST, Rao and
Tetreault (2018) released a high-quality parallel
dataset - GYAFC, comprising two sub-domains and
approximately 50k parallel data for each domain.
Previous studies (Rao and Tetreault, 2018; Niu
et al., 2018; Xu et al., 2019; Zhang et al., 2020) typ-
ically train seq2seq encoder-decoder models on this
benchmark. Recent studies (Wang et al., 2019; Yao
and Yu, 2021b; Chawla and Yang, 2020; Lai et al.,
2021) have deduced that fine-tuning large-scale pre-
trained models such as GPT-2 (Radford et al., 2019)
and BART (Lewis et al., 2020) on the parallel cor-
pora can improve the performance. To address the
data-scarcity problem of parallel datasets, Zhang
et al. (2020) proposed three data augmentation tech-
niques to augment pseudo-parallel data for training.
Similar to prior research on text style transfer that
adopt back-translation (Zhang et al., 2018; Lample
et al., 2018; Prabhumoye et al., 2018; Luo et al.,
2019), some other approaches on FST (Shang et al.,
2019; Xu et al., 2019; Chawla and Yang, 2020)
adopt a cycle-reconstruction scheme, where an ad-
ditional backward transfer model is jointly trained
together with the forward transfer model, and the
two models generate pseudo-paired data for each
other via iterative back-translation. Although Xu
et al. (2019) and Chawla and Yang (2020) train a
single model to perform bidirectional transfer, the
generation of both directions remain disentangled
by a control variable, making each direction rely on
the unlabeled data of its target side. Therefore, the
unlabeled informal sentences exert no direct effects
on the informal →formal transfer. In contrast, our
work focuses on how to better utilize source-side
unlabeled data (i.e., informal sentences) using SSL
and does not introduce any extra models.4690
SSL with consistency regularization SSL is
popular for its advantage in utilizing unlabeled
data. Consistency regularization (also known as
consistency training) (Sajjadi et al., 2016) is an
important component of recent SSL algorithms on
image and text classification (Miyato et al., 2018;
Tarvainen and Valpola, 2017; Berthelot et al., 2019;
Sohn et al., 2020). It enforces a model to produce
invariant predictions for an unlabeled data and its
perturbed version. These studies developed differ-
ent data perturbation (Xie et al., 2020; Berthelot
et al., 2019) or data filtering (Zhang et al., 2021;
Xu et al., 2021) approaches to improve the per-
formance. However, few studies have been made
on how to apply consistency training in natural
language generation (NLG) tasks such as FST be-
cause of the different target spaces, i.e., instead
of single class labels or probabilities, the output
of NLG is the combination of discrete NL tokens.
This renders the experiences in classification tasks
not applicable to FST. For instance, classification
probabilities are typically adopted as the metric to
filter high-confidence pseudo-examples for consis-
tency training in classification tasks (Sohn et al.,
2020; Xie et al., 2020; Zhang et al., 2021), which
is implausible in FST. A similar study (He et al.,
2019) improved self-training by injecting noise into
unlabeled inputs and proved its effectiveness on
machine translation and text summarization; how-
ever, self-training involves multiple iterations to
collect pseudo-parallel data and retrain the model,
hence the training is not end-to-end. In this study,
we explore various data perturbation strategies and
propose effective data filtering approaches to real-ize a successful consistency-based framework for
FST, which may also provide useful insights for
future studies on semi-supervised NLG.
3 Method
3.1 Base Model
FST involves rewriting an informal sentence into
a formal one. Formally, given a sentence x=
(x, x, . . . , x)of length nwith style S, our ob-
jective is to transform it into a target sentence
y= (y, y, . . . , y)of length mand style T,
while preserving its content.
Following prior studies (Rao and Tetreault, 2018;
Zhang et al., 2020; Chawla and Yang, 2020; Lai
et al., 2021) on FST, we employ the supervised
baseline as a seq2seq encoder-decoder model that
directly learns the conditional probability P(y|x)
from parallel corpus Dcomprising (x,y)pairs.
The objective is the cross-entropy loss between
the decoder outputs and the ground-truth target
sentences:
L=E[−logP(y|x;θ)]
=E[−XlogP(y|y,x;θ)],
(1)
where θdenotes the model parameters.
3.2 Consistency Training
Our approach leverages the idea of consistency
regularization (Sajjadi et al., 2016) and enforces a
model to generate similar target sentences for an4691original and perturbed unlabeled sentence. Simul-
taneously, the model is also trained on the super-
vised data. Accordingly, the knowledge garnered
from supervised training can be gradually trans-
ferred to unsupervised training. An overview of
our framework is presented in Figure 1. Typically,
the consistency training loss is computed on the
divergence between predictions on an unlabeled
inputuand its perturbed version ˜u=c(u), where
c(·)is the perturbation function and u∈ Urep-
resents a source-side unlabeled sentence (in our
case, an informal sentence). Formally, consistency
training can be defined as minimizing the following
unsupervised loss:
ED[P(y|u;θ)||P(y|c(u);θ)],(2)
where D[·||·]denotes a divergence loss. In practice,
we adopt pseudo-labeling (Lee et al., 2013) to train
the unsupervised loss, for which we fix the model
parameter θto predict a “hard label” (pseudo tar-
get sentence) ˆyforuand enforce the consistency
of model prediction by training θwith (c(u),ˆy).
Hence the unsupervised objective can be optimized
as a standard cross-entropy loss as follows:
L =EE[−logP(ˆy|c(u);θ)],
(3)
where ˆθdenotes a fixed copy of θ. This training
process does not introduce additional model param-
eters. The entire additional training cost to super-
vised learning is a training pass and a generation
pass for each unlabeled sentence.
As the overall objective, we train a weighted
sum of the supervised loss in Equation (1)and the
unsupervised loss in Equation (3):
L=L+λL, (4)
where λrepresents a hyper-parameter for balancing
the effects of supervised and unsupervised training.
To achieve a good initial model for consistency
training, we first pretrain the model on the super-
vised loss for several warm-up steps.
3.3 Data Perturbation Strategies
Data perturbation is the key component of
consistency-based SSL algorithms (Xie et al., 2020;
Chen et al., 2020) and significantly affects the per-
formance. In this section, we briefly introduce a
collection of different data perturbation methods
explored in this research.First, we consider some easy data augmentation
methods commonly used for supervised data aug-
mentation, which includes
•word deletion (drop): to randomly drop a
proportion of words in the sentence.
•word swapping (swap) : to randomly swap a
proportion of words with their neighbouring
words.
•word masking (mask) : to randomly replace
words with a mask token “_”.
•word replacing with synonym (synonym) :
to randomly replace some words with a syn-
onym based on WordNet (Fellbaum, 1998).
In addition, we consider advanced data augmen-
tation methods that have proven effective in semi-
supervised text classification (Xie et al., 2020):
•back-translation : to translate a sentence into
a pivot language, then translate it back to ob-
tain a paraphrase of the original one.
•TF-IDF based word replacing (tf-idf) : to
replace uninformative words with low TF-IDF
scores while retaining those with high TF-IDF
values.
Furthermore, we handcraft a set of rule-based
data perturbation for FST. There are some typical
informal expressions in the parallel corpus, such as
the use of slang words and abbreviations, capital-
ized words for emphasis, and spelling errors. Some
existing studies (Wang et al., 2019; Yao and Yu,
2021b) adopt editing rules to revise such informal
expressions as a preprocessing step. Inspired by
these, we propose the adoption of opposite rules to
synthesize such noises. We consider the following
methods:
•spelling error injection (spell) : to randomly
inject spelling errors to a proportion of words
by referring to a spelling error dictionary.
•word replacing with abbreviations (abbr) :
to replace all the words in the sentence with
their abbreviations or slang words (e.g., “are
you”→“r u”) by referring to an abbreviation
dictionary.
•word capitalization (capital) : to randomly
capitalize a proportion of words.4692These rule-based methods can inject noise into the
unlabeled informal sentences without changing its
informality, but strengthening it instead.
3.4 Evaluation-Based Data Filtering
In the consistency training loss, the noisy pseudo-
target ˆyis generated from the decoder model and
may exert negative effects on the training. There-
fore, we propose three evaluation-based data filters
in connection with the evaluation metrics of FST.
Specifically, we attempt to measure the qual-
ity of pseudo-target sentences by considering the
three most important evaluation criteria of text style
transfer: style strength ,content preservation , and
fluency . Next, we comprehensively explain each
evaluation metric and the corresponding data filter.
Style strength measures the formality of gen-
erated sentences. Typically, people adopt binary
classifiers such as TextCNN (Chen, 2015) classi-
fiers to judge the formality of a sentence (Lai et al.,
2021). Inspired by this, we pretrain a TextCNN for-
mality classifier on the parallel training corpus (i.e.,
GYAFC) to distinguish between informal and for-
mal sentences. For an unlabeled informal sentence
uand its pseudo target sentence ˆy, we maintain
(c(u),ˆy)for unsupervised training only when
p(ˆy)−p(u)> σ, (5)
where p(·)represents the probability of the sen-
tence being formal, predicted by the style classi-
fier and σis a threshold of the probability. This
guarantees that only the sentence pairs with strong
style-differences are used for consistency training.
Content preservation is another important eval-
uation metric of FST, typically measured with
BLEU between the ground-truth target sentence
and the model generations. In unsupervised text
style transfer where no ground-truth target exists,
source -BLEU is adopted as an alternative, i.e., the
BLEU scores between the source input sentence
and the generated target sentence. Similarly, we
propose the adoption of source -BLEU between u
andˆyas the metric to filter out pseudo targets that
present poor content preservation.
Fluency is also used to evaluate the quality of
generated sentences. We follow (Hu et al., 2020)
to pretrain an N-gram language model on the train-
ing data to estimate the empirical distributions of
formal sentences. Then, the perplexity score is cal-
culated for the pseudo target sentence ˆyby the lan-
guage model. The motivation is that the sentenceswith lower perplexity scores match the empirical
distribution of formal sentences better, and are thus
considered as more fluent.
A natural idea is to filter out pseudo-parallel data
based on a source -BLEU or a perplexity threshold.
However, it is infeasible to determine the optimal
threshold for the two metrics beforehand because
the pseudo paired data are generated on-the-fly dur-
ing the training and we cannot know the distribu-
tion of the BLEU or perplexity scores. In addition,
choosing the BLEU/perplexity threshold is not as
easy as tuning the style probability σbecause they
heavily depend on the data distribution and exhibit
varying ranges of values.
3.5 Dynamic Threshold Selection
To realize the selection of thresholds for the BLEU-
and perplexity- based filters, we propose a dynamic
threshold strategy based on the distribution of the
scores computed for already generated pseudo-
paired sentences. Specifically, we maintain an
ordered list Lto store the scores calculated for
previously generated pseudo data and update it
continuously following the training. At each it-
eration, a batch of new scores are inserted into
Lwhile maintaining the decreasing order of the
list. Subsequently, we update the threshold as the
value at a certain position L[ϕ×len(L)]in the
score list, where len(L)denotes the length of the
current score list and ϕ∈[0,1]represents a ratio
that determines the threshold’s position in the list.
We only keep pseudo data with scores higher (or
lower for perplexity scores) than the threshold for
consistency training. This actually makes ϕap-
proximately the proportion of pseudo data we keep
for training, making it more convenient to control
the trade-off between the qualities and quantities
of selected pseudo data. More details are provided
in Appendix B, C.
4 Experiments
We introduce the experimental settings in Section
4.1. To obtain relevant findings on how to build
an effective consistency training framework for
FST, we first empirically study the effects of mul-
tiple data perturbation methods in Section 4.2 and
prove the effectiveness of consistency training via
comparisons with the base model. Then, we vali-
date our consistency training model with different
data filtering methods in Section 4.3 and demon-
strate their additional effects on the SSL frame-4693
work. Based on the findings in these two experi-
ments, we further compare our best models with
previous state-of-the-art models in Section 4.4. We
also include case studies in Section 4.4 to present
some qualitative examples. Finally, we conduct
low-resource experiments (Section 4.5) to demon-
strate our method’s advantage when less parallel
data are available.
4.1 Experimental Settings
Datasets We evaluate our framework on the
GYAFC (Rao and Tetreault, 2018) benchmark for
formality style transfer. It comprises crowdsourced
informal-formal sentence pairs split into two do-
mains, namely, E&M and F&R. The informal sen-
tences in the dataset were originally selected from
the same domains in Yahoo Answers L6 corpus.
We focus on the informal-formal style transfer be-
cause it is more realistic in applications. We further
collected massive amounts of informal sentences
from each of the two domains in Yahoo Answers
L6 corpus as the unsupervised data. The statistics
of the datasets are presented in Table 2.
Implementation Details We employ PyTorch
(Paszke et al., 2019) for all the experiments. We
pretrain a TextCNN style classifier on the super-
vised data for each domain of GYAFC, following
the setting in (Lai et al., 2021). The same classifier
is adopted for both the style accuracy evaluation
and the style strength filter in our SSL framework.
We adopt HuggingFace Transformers (Wolf et al.,
2020) library’s implementation of pretrained T5-
Large (Raffel et al., 2020) as the base model. We
adopt the Adam (Kingma and Ba, 2014) optimizer
with the initial learning rate 2×10to train all
the models. More details of hyper-parameters and
model configurations are provided in Appendix A.
Evaluation Metrics The main evaluation metric
for FST is the BLEU score between the generated
sentence and four human references in the test set.
We adopt the corpus BLEU in NLTK (Loper and
Bird, 2002) following (Chawla and Yang, 2020). In
addition, we also pretrained a TextCNN formalityclassifier to predict the formality of transferred sen-
tences and calculate the accuracy (Acc.). Further-
more, we compute the harmonic mean of BLEU
and style accuracy as an overall score, following
the settings in (Lai et al., 2021).
4.2 Effects of Data Perturbation Methods
In this experiment, we validate the effectiveness of
our consistency training framework and compare
the effects of different data perturbation methods.
Specifically, we adopt the nine data perturbation
methods introduced in Section 3.3 and include the
no-perturbation variant that indicates directly us-
ing an unlabeled sentence and its pseudo target to
train the unsupervised loss. We adopted no data
filtering strategy in this experiment to simplify the
comparison.
As shown in Table 3, our framework could con-
sistently improve the base model by using different
perturbation methods; however, back-translation
resulted in mostly lower results than the base
model. This contradicts the conclusion in (Xie
et al., 2020) that back-translation is especially pow-
erful for semi-supervised text classification. We at-
tribute this to the fact that back-translation tends to
change the entire sentence into a semantically sim-
ilar but syntactically different sentence. Compared
with other word-level perturbation strategies, back-
translation triggers a larger mismatch between the
perturbed input and the pseudo-target sentence gen-
erated from the unperturbed input, leading to a
poor content preservation ability of the model. In
contrast, simple word-level noises achieved consis-
tently better results, especially spell error ( spell ),
random word swapping ( swap ), and abbreviation
replacing ( abbr ). These three methods tend to alter
the words but do not lose their information while
other methods eliminate the entire word by delet-
ing (drop, mask ) or replacing it with another word
(synonym, tf-idf ). This may also cause a larger
mismatch between the pseudo input and output.
Hence, we draw the conclusion that simple word-
level perturbations tend to bring more effects . This
differs from the observations in text classifica-
tion (Xie et al., 2020) because content preservation
is important in FST. In particular, we also found
thatspell achieved the highest BLEU scores on
both datasets. However, adding no perturbation
even resulted in a worse performance than the base
model. Moreover, capital is also relatively weaker
than the other two rule-based methods because it4694
only changes the case of a chosen word. This sug-
gests that the perturbation should not be too simple
either.
4.3 Effects of Data Filtering
In this section, we analyze whether our proposed
data filters are beneficial to the performance of our
consistency training framework. Specifically, we
chose the most effective data perturbation method
spell to analyze the effects of adding the three data
filters: style strength ( style), content preservation
(bleu), and fluency ( lm) filters. As presented in
Table 4, the results for different datasets and differ-
ent filters have different tendencies. For example,
adding the style filter on the E&M dataset caused
negative effects while contributing the best results
to the F&R domain.
Although a filter does not necessarily improve
the result, this is reasonable because filters result
in less pseudo data for model training and it is dif-
ficult to control the trade-off between the quality
and the quantity of selected data. Nevertheless, we
still observe that the bleu filter contributes to the
highest performance of spell for all the metrics on
the E&M domain, while style benefits the perfor-
mance of spell the most on F&R, leading to thebest performing models of our approach.
4.4 Comparison with Previous Works
We compare our best model with the following
previous studies on GYAFC.
•NMT (Rao and Tetreault, 2018) is an LSTM-
based encoder-decoder model with attention.
•GPT-CAT (Wang et al., 2019) adopts GPT-
2 and rule-based pre-processing for informal
sentences.
•NMT-Multi-task (Niu et al., 2018) jointly
solves monolingual formality transfer and
formality-sensitive machine translation via
multi-task learning.
•Hybrid Annotations (Xu et al., 2019) trains a
CNN discriminator in addition to the transfer
model and adopts a cycle-reconstruction loss
to utilize unsupervised data.
•Transformers (DA) (Zhang et al., 2020)
uses three data augmentation methods, includ-4695
ing back-translation, formality discrimination,
and multi-task transfer.
•CARI (Yao and Yu, 2021b) improves GPT-
CAT by using BERT (Devlin et al., 2018) to
select optimal rules to pre-process the infor-
mal sentences.
•Chawla’s (Chawla and Yang, 2020) uses lan-
guage model discriminators and maximizing
mutual information to improve a pretrained
BART-Large (Lewis et al., 2020) model, along
with a cycle-reconstruction loss to utilize un-
labeled data.•BART-large+SC+BLEU (Lai et al., 2021)
improves BART-large by incorporating rein-
forcement learning rewards to enhance style
change and content preservation.
We also report the results of Ours (base) , our back-
bone T5-large model, and Ours (best) , our best
performing models selected from Table 4.
As observed in Table 5, Ours (best) outperforms
previous state-of-the-art models by a substantial
margin and improves the BLEU scores from 76.17
and 79.92 to 78.75 and 81.37, respectively, on the
E&M and F&R domains of the GYAFC benchmark.
Although BART-large+SC+BLEU achieved bet-
ter results on the Acc. of F&R, the only released
official outputs of BART-large+SC+BLEU were
obtained from a model that was trained on the train-
ing data of both domains and adopted rewards to
directly optimize style accuracy; hence, it is not
directly comparable to our model. Ours (best) im-
proves the fine-tuned T5-large baseline by a large
margin as well, demonstrating the effectiveness of
our SSL framework.
Human Evaluation We also conduct human
evaluation to better capture the quality of the mod-
els’ outputs. Following (Zhang et al., 2020), we
measure the Formality ,Fluency , and Meaning
Preservation of generated sentences by asking two
human annotators to assign a score ranging from
{0, +1, +2} regarding each aspect. We randomly
sampled 50 examples from the test set of each do-
main and compare the generated outputs of Ours
(base) ,Ours (best) , and the previous state-of-the-
artChawla’s model trained on the single-domain
data. In addition, the annotators were unaware of
the corresponding model of each output. As shown4696
in Table 6, the human evaluation results are con-
sistent with the automatic evaluation results: Ours
(base) is competitive compared with Chawla’s ,
while Ours (best) improves over the base model
and outperforms the previous state-of-the-art on all
the metrics, except that it presents lower results on
Meaning than Ours (base) on F&R. More details
on human evaluation can be found in Appendix D.
Qualitative Examples We present some of the
generated outputs of Ours (base) ,Ours (best) ,
andChawla’s in Table 8. It can be observed that
all the models can produce high-quality outputs
with considerable formality, meaning preservation
and fluency. Nevertheless, Ours (best) exhibits a
stronger capability to modify the original sentence,
especially for some informal expressions, leading
to the best performance on the Formality metric.
For example, it replaced “like” with “similar to” in
Example 2 and deleted the informal word “guys”
in Example 3. However, it may alter the original
sentence so much that the meaning of the sentence
is changed to some extent (Example 1). This may
explain why Ours (best) achieves a lower Meaning
score than Ours (base) on F&R.
4.5 Low-Resource Experiments
We also simulate the low-resource settings by fur-
ther reducing the size of available parallel data.
Specifically, we randomly sample from the original
training data with a size in the range of {100, 1000,
5000, 20000} and compare the results of the base
model T5-Large with our SSL model. The size
of unlabeled data remains 200k for each domain.
We adopt the spell data perturbation without any
data filter and avoid exhaustive hyper-parameter
tuning. Table 7 demonstrates that our framework isespecially effective under few-shot settings when
only 100 parallel data are available. By comparing
with previous state-of-the-art results on FST, we
can observe that our approach can achieve compet-
itive results with only 5000 ( <10%) parallel train-
ing data, and even better results with only 20000
(<40%) parallel examples.
5 Conclusion
In this study, we proposed a simple yet effective
consistency-based semi-supervised learning frame-
work for formality style transfer. Unlike previ-
ous studies that adopted cycle-reconstruction to
utilize additional target-side sentences for back-
translation, our method offers a different view, to
leverage source-side unlabeled sentences. With-
out introducing additional model parameters, our
method can easily outperform the strong supervised
baseline and achieve the new state-of-the-art re-
sults on formality style transfer datasets. For future
work, we will attempt to generalize our approach
to other text generation scenarios.
Acknowledgements
This paper is based on results obtained from a
project, JPNP18002, commissioned by the New
Energy and Industrial Technology Development
Organization (NEDO). Ao Liu acknowledges fi-
nancial support from the Advanced Human Re-
source Development Fellowship for Doctoral Stu-
dents, Tokyo Institute of Technology.
References46974698
A Detailed Experimental Settings
A.1 Hyper-Parameters
We set the max length of input sentences to 50
Byte-Pair Encoding (Sennrich et al., 2016) tokens.
The weight of unsupervised loss λis set to 1.0 in
all our experiments, which is an empirical choice
from previous studies (Sohn et al., 2020). The
batch size is 8 for the supervised objective and 56
for the unsupervised objective, such that the model
can leverage more unlabeled data for training. The
threshold σfor the style strength filter is set to 0.8
and the threshold ratio ϕis set to 0.4 for both the
content preservation and fluency filters. We tested
σin the discrete range between 0.5 and 0.9 and for4699ϕ, we searched over the values between 0.1 and
0.8. Although the chosen values of σandϕare not
necessarily the best for all the datasets, we fix them
in later experiments for their reasonable results.
A.2 Training Details
We train two binary style classifiers on each do-
main of GYAFC. The training data are the for-
mal and informal sentences in the original training
sets of the E&M and F&R domain. The classi-
fiers are validated on the formal sentences in the
original validation set. The classifier for E&M
could achieve 95.69% accuracy on the validation
set, while the classifier for F&R achieved 94.70%.
We adopt a 4-gram Kneser-Ney language model
to compute perplexity scores for the fluency data
filter. During semi-supervised training, we first
pretrain the model solely on the supervised data
for 2000 steps to achieve a good initialization of
the model parameters. Then, we jointly train the
supervised and consistency losses simultaneously.
The model checkpoint is validated with an interval
of 1000 steps and selected based on the best BLEU
score on the validation set. Early stopping is also
adopted with patience 10. We employ beam search
with beam width 5 for the model’s generations and
pseudo-target prediction. All our experiments are
conducted on NVIDIA A100 (40GB) GPUs.
A.3 Details of Unlabeled Data Collection
We collected 200k from each of the E&M and F&R
domains of Yahoo Answers L6 corpus. The col-
lection procedure is as follows. (1) We chose the
passages labeled “<bestanswer>” in the corpus and
tokenized them into separate sentences. (2) We
filtered out sentences with formality scores larger
than 0.5 (i.e. judged as formal) predicted by the
style classifier we built for model evaluation. (3)
We built an N-gram language model by training
on the informal sentences in the original training
data of GYAFC, and used it to generate perplexity
scores for these sentences. We kept 200k sentences
with lowest perplexity scores, such that we ob-
tained a collection of the most informal sentences
in the corpus. We only observed one overlapping
sentence with the test set of each domain, which
we considered negligible and kept in the data.A.4 Details of Data Perturbation
All our data perturbation methods are implemented
based on the nlpauglibrary. We set the ratio of per-
turbed words in a sentence to 0.1 for all word-level
perturbation methods and deduced that increasing
the ratio could often result in lower results, as that
will enhance the difference between the original
and perturbed sentences, which is consistent with
our conclusion in Section 4.2. We present examples
of all data perturbation methods in Table 9.
We also attempted mixing different perturbations
with spell , but did not obtain better results than
single spell . This can also be attributed to the con-
clusion that simple perturbations are even better.
B Formal Description of the Algorithm
Here, we provide a formal algorithmic description
of our consistency training framework in Algorithm
1 and assume that we adopt content preservation
(BLEU) data filtering or fluency (perplexity) data
filtering in this algorithm to include the formal
description of our dynamic threshold strategy. We
omit the case when we adopt style strength filtering
because it does not use the dynamic threshold and
is more straightforward to understand.
CDetails of Dynamic Threshold Selection
Here, we provide more details of the dynamic
threshold strategy for the content preservation and
fluency filters. In practice, we do not filter any
pseudo data in the initial warm-up steps of consis-
tency training, to initialize the score list. Further-
more, after iterating an epoch of the unsupervised
data, we keep the current threshold fixed and do
not update the score list any more. The score list
is implemented as a skiplist to enable O(logN)
insertion into an ordered list. The overall time com-
plexity of the data filtering is O(log 1 + log 2 +
···+ log N) =O(logN!) =O(NlogN), where
Nis the number of unlabeled data.
D Details of Human Evaluation
We describe the rating criteria in the human eval-
uation. We ask two well-educated annotators to
rate the formality ,fluency , and meaning preserva-
tionon a discrete scale from 0 to 2 for the model
outputs, following (Zhang et al., 2020). During
the annotation, we randomly shuffle the sentences4700
Algorithm 1 Training Procedure of our approach using dynamic threshold selection
from the three models and make the model names
invisible to annotators.
Formality The annotator are asked to rate the
formality change level given a source informal sen-
tence and the generated output sentence, regardless
of the fluency and meaning preservation. If the out-
put sentence improves the formality of the source
sentence significantly, the score will be 2 points. If
the output sentence improves the formality but still
keeps some informal expressions, or the improve-
ment is minimal, it will be rated 1 point. If there is
no improvement on the formality, it will be rated 0
points.
Fluency The fluency is rated 2 points if the out-
put sentence is meaningful and has no grammatical
error. If the target sentence is meaningful but con-
tains some minor grammatical errors, it will berated 1 point. If the sentence is incoherent, it will
be rated 0 points.
Meaning Preservation Given a source sentence
and a corresponding output sentence, the raters
are asked to ascertain how much information is
preserved in the output sentence compared to the
input sentence. If the two sentences are exactly
equivalent, the output obtains 2 points. If they
are mostly equivalent but different in some trivial
details, the output will receive 1 point. If the output
omits important details that alter the meaning of
the input sentence, it is rated 0 points.4701