
Chris Callison-Burch
University of Pennsylvania
ccb@upenn.eduGaurav Singh Tomar
Google Research
gtomar@google.comLara J. Martin
University of Pennsylvania
Daphne Ippolito
University of Pennsylvania
Google ResearchSuma Bailis
Google ResearchDavid Reitter
Google Research
Abstract
AI researchers have posited Dungeons and
Dragons (D&D) as a challenge problem to test
systems on various language-related capabil-
ities. In this paper, we frame D&D specif-
ically as a dialogue system challenge, where
the tasks are to both generate the next conver-
sational turn in the game and predict the state
of the game given the dialogue history. We cre-
ate a gameplay dataset consisting of nearly 900
games, with a total of 7,000 players, 800,000
dialogue turns, 500,000 dice rolls, and 58 mil-
lion words. We automatically annotate the
data with partial state information about the
game play. We train a large language model
(LM) to generate the next game turn, condi-
tioning it on different information. The LM
can respond as a particular character or as the
player who runs the game—i.e., the Dungeon
Master (DM). It is trained to produce dialogue
that is either in-character (roleplaying in the
ﬁctional world) or out-of-character (discussing
rules or strategy). We perform a human evalu-
ation to determine what factors make the gen-
erated output plausible and interesting. We fur-
ther perform an automatic evaluation to deter-
mine how well the model can predict the game
state given the history and examine how well
tracking the game state improves its ability to
produce plausible conversational output.
1 Introduction
Artiﬁcial Intelligence has a long and rich history of
using games as challenge problems that lead to ad-
vances in the ﬁeld. In many cases, AI game-playing
systems have gone on to rival human champions of
the game. Dungeons and Dragons has been identi-
ﬁed as an appropriate challenge for the next stage
of artiﬁcial intelligence (Ellis and Hendler, 2017;
Louis and Sutton, 2018; Martin et al., 2018b). Ellis
and Hendler (2017) proposed open-ended creative
games like D&D as the next challenge for AI af-
ter the human-level successes of AI at Chess andGo, which are zero-sum, deterministic, sequential
two-player games with perfect information. Louis
and Sutton (2018) understood the importance of
narrative in natural language processing (NLP) and
generation (NLG). In particular, they saw how co-
operative story generation between humans already
exists in these games and can be used for auto-
mated generation. Martin et al. (2018b) outlined
some of the speciﬁc challenges D&D presents to
the NLP community; such as a state of the game
world distributed across the Dungeon Master (DM)
and other players or dealing with the intrinsic re-
wards players get from taking certain actions that
would not necessarily provide them with points in
the game.
D&D involves multiple players who roleplay
characters in a fantasy setting, guided by a Dun-
geon Master who sets obstacles and adventures and
plays as monsters. In roleplaying games like Dun-
geons and Dragons, the gameplay happens through
language rather than moves on a game board, mak-
ing it an interesting domain for NLP research. To
have an AI successfully play D&D , it would re-
quire abilities like
•Language generation (multi-party dialog, gen-
erating descriptions of the world/actions, sto-
rytelling)
•Language understanding (knowledge acquisi-
tion and representation, state tracking, auto-
mated reasoning)
•Planning / strategic play during battles (simi-
lar to chess or go)
Appendix A gives an example of D&D gameplay
and the AI challenges presented by it.
Is it possible to design an AI system that is capa-
ble of playing a game of D&D either as a character
in the game or as the Dungeon Master using current
AI technology? We argue that now is the perfect
time for this challenge, since large scale neural
language models like GPT have shown impressive
generation results (Brown et al., 2020), and since9379incorporating neural LMs into a game setting both
exercises their strengths and exposes their weak-
nesses.
In this paper, we introduce a new dataset of “ac-
tual play” game transcripts. Each turn is labeled
with game state variables like character informa-
tion and whether the conversational turn was in-
character or out-of-character. Our data is a novel,
large scale, real-world conversational dataset. It
is unique in that the dialog turns are generated
entirely through player collaboration and written
interaction in a multi-player game. We propose our
dataset as a challenge for dialogue systems for the
following reasons:
•It is naturally occurring conversational dialog
that covers a spectrum of task oriented and
non-task oriented (e.g. chit chat) dialog.
•It is strongly history dependent – a substan-
tive criticism of recent dialog datasets is their
history independence (Mosig et al., 2020).
•It has many participants in the conversation,
since there are several players in the game.
•It conveys narrative elements including de-
scriptions of events that denote changes in the
state of the game.
Unlike existing dialog datasets, our data reﬂects
the challenging nature of the D&D game as a multi-
party dialogue with creative roleplaying and under-
lying game states.
2 Tasks
We trained a large language model (LLM) to per-
form two tasks: Next Utterance Prediction and
Game State Tracking .
Next Utterance Prediction. We trained our lan-
guage model on a corpus of human conversations
(see Section 3) to predict the next utterance. We
varied the conditioning information to examine the
effects on the quality of predicted next utterance.
In all variations, we included the conversational
history as input. Given the conversational input
(and other input in the variant models), the LLM
must generate the next utterance, such that it is
both interesting and a plausible next turn in the
D&D game.
Game State Tracking. In this task, rather than
producing the next utterance, we had the model
predict the game state for a given dialogue turn
in the conversation. We have kept the state deﬁni-
tion similar to task-oriented dialogue state trackingPlay-By-Post Corpus
Number of campaigns 896
Average players per campaign 8
Average turns per campaign 910
Average words per campaign 64,941
Total turns 815,106
Total words 58,187,526
Average dice rolls per campaign 594
Total dice rolls 532,270
(DST). In DST, the dialogue state is a collection
of slot-value pairs. In our case, each slot is a state
variable feature related to D&D games. Our target
slot values do not need to appear as a word in the
dialogue context. We track several game states as-
pects including some that remain relatively static
throughout the game (character attributes like their
pronouns, class, fantasy race, and their inventory),
some that change periodically (like being in com-
bat or out of combat), and some that change from
turn to turn (like what action the player is taking).
3 Dataset
For this paper, we have created a novel dataset for
our dialogue-oriented test of AI’s ability to play
Dungeons & Dragons. We scraped Play-By-Post
data from a web forumwhere people play by tak-
ing turns posting on the forum to describe their
move.
Figure 1 shows an example of part of the game-
play from the play-by-post forums from D&D Be-
yond. D&D Beyond provides a mechanism in its
forum to roll dice using a “roll” tag. Their dice
roller allows players to conduct the rolls that are
used for D&D ability checks and in combat.
Table 1 summarizes the amount of play-by-post
data that we collected from the D&D Beyond web-
site (with permission from the company).
3.1 Heuristic annotation of game states
We designed a set of rule-based heuristics to ex-
tract game state information from the play by post.
These were implemented using regular expressions
and NLP tools like named entity recognizers (Gard-
ner et al., 2018). Although this heuristically ex-
tracted information is not perfect, it provides a
reasonable approximation of the game state. It is9380
useful for testing whether large language models
can beneﬁt from inclusion of complex state infor-
mation for next utterance prediction and whether
LLMs can be used for state tracking. We designed
rules to extract state information relating to charac-
ter properties, combat and player actions.
Character properties
•Name: Perform NER on all the player’s turns
in a campaign. The character’s name is as-
signed to be the player’s most frequently men-
tioned name, on the assumption that they tend
to describe their own character’s actions.
•Class: Count how many times each
D&D classis mentioned by each player.
Most frequently mentioned class is their char-
acter’s class.
•Race: On a player’s ﬁrst turn, check whether
any of the D&D fantasy racesare mentioned.
Assign it to character. If not, guess based on
the most frequently mentioned race.
•Pronouns: Count pronouns mentioned by a
player. Assign their character’s pronouns to
be the most frequent pronouns used by the
player.
•Inventory: Use regex to match items occurring
after character’s personal pronouns (e.g. her
sword).•Spells known: Regex that matches cast fol-
lowed by a spell name
The DM is assumed to be the player who has the
ﬁrst post in the game. The DM’s entries in the
dataset are scrubbed of other character properties,
since they play multiple NPCs (non-player charac-
ters) and monsters.
Combat
•We detect the start of combat when there is
a roll for initiative, or when there are attack
rolls before initiative (from surprise attacks).
•Combat continues while there are attack rolls
happening.
•Combat concludes after there are no rolls for
a number of turns.
•In a combat span, we extract a list of monsters
mentioned, and heuristically guess the number
of each kind of monster.
Actions
•Dice rolls are marked in D&D Beyond posts.
We detect the associated actions based on the
kind of die used (D20 = a check, other dice
are used for calculating damage if an attack
check is successful)
•We use a regex to match the nearest pattern,
which includes attack or a list of abilities like
acrobatics, animal handling, arcana, athletics,
etc.
•Damage rolls are matched with damage, dmg,9381
cure, heal, healing, points .
Our heuristics resulted in features for around
60% of all conversational turns. We train a convo-
lutional neural network classiﬁer using these con-
versational turns to predict all of the above control
features for each conversational turn in training
data. Appendix C estimates the accuracy of the
model’s prediction on these state features.
3.2 In-Character Versus Out-Of-Character
Text
In addition to labeling the game states in our Play
by Post data, we also labeled the text of each turn
as being either spoken in-character (IC) or out-of-
character (OOC). To do so, we crawled another
Play by Post forum hosted at Giant in the Play-
ground, where play happens on two discussion
boards – one in-character and one out-of-characters.
For example, here is an IC post:
Kuros pulls the feathered shaft of the
arrow back to his cheek winning easily
against the resistance of the bowstring.
He pulls a lungful of air to keep himself
steady, takes aim at the Bandit with the
deer, and lets ﬂy.
And here is its corresponding OOC post:
Surprise round so only 1 standard
or move action. Shoot the bow:(1d20+6)[20] vs Flat Footed AC at Ban-
dit 1. Damage: (1d8+2)[10]
We train a classiﬁer to predict IC versus OOC text,
and then apply it to each paragraph in our D&D Be-
yond forum data.
4 Models
For our large language model, we use a 64B param-
eter version of Google’s LaMDA language model
(Thoppilan et al., 2022), which was trained on con-
versations. LaMDA is similar to other Transformer-
based pre-trained language models like GPT-3. As
with other pre-trained language models (Howard
and Ruder, 2018), LaMDA can be ﬁnetuned to
different tasks. The two tasks that we ﬁnetune
LaMDA to perform are game state tracking and
response generation. In both cases, the LLM can
be thought of as a function that maps inputs onto an
output. For instance, game state tracking is a lan-
guage understanding task where the function takes
in inputs like f(current utterance, previous state,
history )→new state, and response generation is
a language generation task where f(current state,
history )→next utterance. The LLM functions are
trained via the ﬁne tuning process.
In our experiments we try a variety of different
inputs to our LLM functions to see how they enable
better learning of the tasks. We train our LLMs on
the conversation history (which is typical in dia-
log modeling) and we also augment the conversa-
tions by conditioning other explicit signals. These9382conditioning signals can be thought of as sophis-
ticated “control features”, inspired by the CTRL
language model (Keskar et al., 2019). During train-
ing, the model learns a relationship between the
control features and appropriate responses. In turn,
during inference, one can explicitly inﬂuence di-
mensions of the conversation – enabling more com-
pelling dialogue – by setting the values of control
features. These control features can be set dynami-
cally, without necessitating ﬁnetuning or additional
post-processing. Table 2 describes the control fea-
tures we have proposed and describes how they
could steer generation. Note that we use the terms
‘control features’ and ‘state variables’ interchange-
ably when referring to our next utterance prediction
models.
4.1 Baseline Pre-Training Data
LaMDA is trained on turn-based conversational
data. For a conversation of length n, LaMDA takes
as input the ﬁrst n−1turns, and the nth turn as the
target. For all models, we used the 7 most recent
conversational turns as input, and predict turn 8.
4.2 D&D FineTuning Data
Here is an example of the data used in our versions
of LaMDA that are ﬁnetuned to on our D&D data.
4.3 Next Utterance Prediction Models
LLM-Dialog: We call our baseline model LLM-
Dialog. It is a LaMDA dialogue model that does
not use not use any D&D data.
LLM-DND: LLM-Dialog that has been ﬁne-
tuned on Play-by-post D&D gameplay dataset us-
ingnocontrol features
LLM-DND-ALL-CTRL: LLM-Dialog that has
been ﬁnetuned on Play-by-post D&D gameplay
dataset using control features (state variables) for
alldialog turns including the state variables for the
current turn the utterance is being predicted for.
LLM-DND-PREV-CTRL: LLM-Dialog that
has been ﬁnetuned on Play-by-post D&D gameplay
dataset using control features for all previous
dialog turns, not including the current turn.
LLM-DND-CURRENT-CTRL: LLM-Dialog
that has been ﬁnetuned on Play-by-post
D&D gameplay dataset using control fea-
tures (state variables) for only the current turn
the utterance is being predicted for.
4.4 Dev Set Perplexity During Training
Each of our models starts from a pretrained
LaMDA model trained for 600K steps and then
is ﬁnetuned for a further 60K steps. Figure 2 plots
the Negative log perplexity on our development set,
and Table 3 shows the ﬁnal perpexity and token
accuracies on the dev set. At the end of ﬁnetun-
ing, the models with the best perplexity scores and
the best token accuracy scores were LLM-DND-
CURRENT-CTRL and LLM-DND-ALL-CTRL,
which used our control features.9383
5 Manual Evaluation
To evaluate the quality of our models for the task
of next utterance prediction in D&D , we perform a
human evaluation. We recruited professional raters
to perform a manual evaluation. They read a ver-
sion of the content that was provided to the models
– the seven turns of conversational history plus a list
of players and the names/classes of the characters
that they played. Then they were shown several
model outputs for the context (or the “gold”, which
was the actual next turn in the game), The anno-
tators asked to rate each output along the three di-
mensions, following the evaluation procedure used
for the Meena LM (Adiwardana et al., 2020):
• Does the response make sense ? (yes/no)
• Is the response speciﬁc ? (yes/no)
•How interesting is the response? (10 point
scale)
The full annotator instructions and the annotation
interface are given in Appendix D.
5.1 Raters
Because of the specialized nature of the D&D do-
main, we recruited 6 professional raters rather than
crowd workers to perform the task. The raters were
selected based on their professed interest in the fan-
tasy genre, and on their background with D&D. All
raters were fantasy fans, and 5 of the 6 had played
D&D. 3 raters had been the DM in a game before.
5.2 Inter-Rater Agreement
Our raters annotated 500 system outputs with 3-
way redundancy on each output. For the binary
sense and speciﬁc scores, pairwise annotator agree-
ment was 0.8, with a chance-adjusted Randolph
Kappa score of 0.6. For the scalar interestingness
scores, the Kendall’s Tau correlation was 0.46.
5.3 Analysis
Model Comparison. Table 4 shows the average
sense, speciﬁc and interestingness scores for the
systems, and for the human-written gold response.
All of the D&D adapted systems outperform the
vanilla dialogue system. On average, the adapted
systems make sense 6.75% more often than the
baseline, are speciﬁc 4% more often, and are 0.37
points more interesting. However, the added con-
trol features do not seem to differ substantially
from the LLM that is adapted to the D&D data
without any control features. Why then use the
control features at all?
Control Features. One reason is that the control
features are useful to allow the model to role-play.
For example, we can ask the model to play an elf
wizard via the control features. Table 5 gives a
qualitative example of this. The Table shows out-
puts from two LLMs without control features, plus
3 outputs from our LLM-DND-CURRENT-CTRL
model roleplaying different characters. As input,
we gave the models the ﬁnal seven turns in the
game play example from Appendix A –from “With
his eagle eyes, Magnus spots two dead horses. . . ”
to “... What do you do next?”, and then generate
what each model thinks the next utterance ought to
be. The vanilla dialogue system with no D&D data
generates an irrelevant reply about Discord. The
dialogue system ﬁne-tuned on D&D data without
control feature generates a relevant response. The
dialgoue system that adds conditioning control fea-
tures allows the model to generate speciﬁc replies
for each of the different characters. These replies
are relevant to the character classes – the wizard
casts a spell, and the Dwarf cleric shouts a battle
cry by invoking the name of a Dwarf god.
In-Character Turns Are More Interesting.
Among our most impactful control features was the
one that allowed systems to generate in-character
(IC) versus out-of-character (OOC) turns. Table 6
shows that control models’ scores substantially in-
creased on IC turns compared to when their output
was generated OOC. The pronounced increase in
intersestingness makes sense because IC turns are
ones where the players describe their characters in9384
the ﬁctional world often with evocative language,
whereas OOC turns usually discuss rules or me-
chanics. Our control features allowed the system
to intentionally generate IC responses, resulting in
substantially improved interestingness scores for
those in-character turns.
6 Game State Tracking Model
We conducted an experiment to evaluate whether
a LLM could be ﬁnetuned to perform game state
tracking for D&D using our heuristically annotated
game state features. We trained a new model LLM-
DND-GST (Game State Tracking). It is a LLM-
Dialog that has been ﬁnetuned on our Play-by-post
D&D gameplay dataset. As input, it takes all pre-
vious dialog turns and their state variables, plus
the text of the current turn, and then it outputs thecorresponding state variables for the current turn.
We analyzed the accuracy of the LLM-DND-
GST model its ability to do slot-ﬁlling for each
of the individual game states, and compared its
performance to a simple baseline that always output
the the majority class. The results are shown in
Table 7. The average accuracy of the dialogue state
tracker is better than the majority class baseline,
but likely falls short of being useful when it comes
to joint accuracy. The joint accuracy for LLM-
DND-GST is 58%. This suggests that accurately
tracking the full game state may require additional
machinery beyond a ﬁnetuned LLM.
7 Related Work
Previous work has examined AI to play text adven-
tures games (Haroush et al., 2018; Yao et al., 2020;
Dambekodi et al., 2020). These games are simpler
than D&D because they have a limited vocabulary
and more straightforward game states. Creating
text adventure games (Ammanabrolu et al., 2020a;
Fan et al., 2020) is more challenging than playing
them, and is similar to the world-building job of
the DM in D&D . There has also been work on per-
sona/character generation in stories (Prabhumoye
et al., 2019), and within D&D itself (Louis and
Sutton, 2018). Others (Urbanek et al., 2019; Am-
manabrolu et al., 2020b) have realized that NPCs
are lacking in their abilities to speak and act in text
games.
Findings of the automated story generation com-
munity are relevant for D&D AI systems. Neural
language models have become increasingly more
popular for story generation (Roemmele, 2018;
Martin et al., 2018a; Mathewson et al., 2019; Hou
et al., 2019). We have also started to see story-
telling with transformers (See et al., 2019; Peng
et al., 2021; Branch et al., 2021). Transformer-9385based storytelling systems have even been intro-
duced to the general public thanks to the popularity
of AI Dungeon (Walton, 2019). Although neural
networks possess a lot of power in terms of what
text they generate, they are still limited in their
ability to produce longer spans of coherent text.
Many (Fan et al., 2018; Yao et al., 2019; Ippolito
et al., 2019; Tambwekar et al., 2019; Ammanabrolu
et al., 2020b; Rashkin et al., 2020) have improved
the coherence of neural storytellers by splitting the
generation into two steps: ideation of the story plot,
followed by the realization of sentences. This con-
trollable story generation is the focus of a lot of
current work in neural automated story generation.
Due to the conversational nature of D&D , we
decided to use a dialog-based system. Deep neural
networks have been used for dialog agents for a
while (Serban et al., 2016), with a shift toward us-
ing transformers in recent years (Zhang et al., 2019;
Ghazarian et al., 2021). Like in automated story
generation and other neural text generation tasks,
we are also seeing controllability being an impor-
tant factor being integrated into systems. This
includes using deep reinforcement learning tech-
niques to guide the dialog toward a goal (Li et al.,
2016; Saleh et al., 2020) or controlling for style
(Zhang et al., 2018; Smith et al., 2020).
In this paper, we use LaMDA, a transformer-
based open-domain dialogue system that builds on
the Meena model (Adiwardana et al., 2020). The
original Meena model was an end-to-end model
trained on public conversations found on social me-
dia. Controllable text generation with transformers
has been seen before with CTRL (Keskar et al.,
2019), a language model that is conditioned on
a given “control code” in addition to the textual
history. This work takes a similar approach. We
integrate contextual information such as character
descriptions, actions, and in- and out-of-character
classiﬁcations.
We have ﬁnetuned our LaMDA models on data
crawled from D&D Beyond. This data contains
both in-character and out-of-character dialog and
can be used in conjunction with Rameshkumar
and Bailey (2020)’s dataset from Critical Role (a
D&D podcast), Louis and Sutton (2018)’s dataset
from roleplayerguild.com (a D&D forum), Ur-
banek et al. (2019)’s crowdsourced LIGHT dataset,
and/or Akoury et al. (2020)’s STORIUM dataset
for human+AI collaborative story generation.8 Discussion and Conclusions
We have demonstrated that training on D&D data
results in much higher quality outputs than a vanilla
dialogue system (as expected), that controlling the
model to generate in-character responses results
in substantially more interesting output, and that
conditioning on game state information qualita-
tively results in responses that are appropriate to
the character class. Our preliminary experiments
with using the large language models to perform
game state tracking show low performance even
after ﬁnetuning, suggesting that other models may
be required for an AI to play D&D track the full
state of the game.
Although our models are unable to play
D&D fully autonomously by acting as the Dun-
geon Master, they could act as an aid for novice
DMs. Since our models can generated evocative,
in-character text that is appropriate for the context
and the game state, DMs could use it as inspiration
as they narrate the adventure to the other players.
Here is some model output to inspire your next
adventure:
You get a much closer look than the other
two... the sarcophagi have the inscrip-
tions of some sort of magic, probably
to keep the dead inside, but you can not
read them to save your life.
What will you do next? Download our datasetto
start your new adventure!
9 Limitations
One limitation of our human evaluation is that it is
a static evaluation. The raters are simply reading
the outputs of the model, and there is no interactive
evaluation wherein they engage in gameplay with
the system. An interactive user-study would be
required before any claims could be made about
how well AI is able to play D&D alongside human
players.
Because our state information was created
heuristically, it therefore potentially contains er-
rors. It is also incomplete. There are several kinds
of state tracking variables that would be useful to
include, but were not possible to heuristically ex-
tract from our data. To address this problem in the
future, we have begun a collaboration with the de-
veloper of Avrae, which is a Discord bot for playing9386D&D online. Avrae contains many state variables
that are missing from our current annotations, such
as HIT points and slot-ﬁller values for attacks.
10 Acknowledgments
We would like to thank Antony Pegg of Fandom
for granting us permission to use D&D Beyond’s
forum data for research purposes.
Thank you to Rich Burlew and forum moderator
truemane for granting us permission to crawl the
Giant in the Playground forum and to build models
using the forum posts.
We would like to thank the many Google Re-
searchers who gave valuable input on this project,
especially Dipanjan Das and Suma Bailis.
Chris Callison-Burch is grateful for having the
opportunity to be a part-time visiting researcher at
Google for two years. It was amazing to spend time
among such incredibly smart people, and it was
eye opening to see large LMs before they became
widely available. Thanks!
References93879388
A Example D&D Game Session
Instead of the game being a series of moves on
a game board, RPGs D&D are language-based.
Players create characters that have a class (wizard,
ﬁghter, thief, etc.) that denotes their abilities, and a
fantasy race (elf, dwarf, human, etc.). Players de-
scribe what they want their character to do and roll
dice to determine if they are successful. The dun-
geon master (DM) acts as the narrator who shapes
the overall story. The DM describes scenarios and
locations, and takes on the role of non-player char-
acters (NPCs), and monsters.
A common element to the game play is an en-
counter with monsters. Battles are governed by
rules, and unfold in a turn-based fashion where
the DM controls the monsters and each player con-
trols their character. Each player and monster has
a health meter (called their HIT points), an armor
class (which indicates the threshold of the dice roll
needed to damage them), and a set of possible at-
tack or move actions.
Table 8 provides example dialogue from a game
of D&D being played between 3 players – Travis
(playing a human ﬁghter named Magnus Burn-
sides), Clint (playing Merle Highchurch, a dwarfcleric), Justin (playing Taako an elf wizard), and
DM Grifﬁn. We add comments about each dialogue
turn to describe what is happening in the game, and
to highlight the challenges that would need to be
addressed if an AI system were to play the game
either as a player or as the DM.
The game session is taken from the podcast The
Adventure Zone . In this episode, the hosts are
playing an adventure module called Lost Mine of
Phadelver , an expert of which is given in Appendix
B. In the ﬁrst episode of the podcast, the hosts
explain the rules of D&D .
B Lost Mine of Phadelver Adventure
Here is an excerpt from the adventure book that the
Dungeon Master was using in our example game
play. The adventure book provides boxed text,
which is descriptive text to be read aloud verba-
tim or to paraphrase. It also gives details about the
combat that is about to ensure, and links to relevant
game rules (like stealth checks, and statistics about
the monsters that the characters will be in combat
with).93899390
C Estimated accuracy of predicted state
variables
In addition to the heuristics that we used to recover
state variables for each turn in the game (described
in Section 3.1), we used a CNN to ﬁll in state values
when our heuristics did not ﬁre. Table 9 estimates
gives an estimate of the CNN’s performance on
ﬁlling in the state variables where the rule-based
heuristic did not extract a value. The CNN classiﬁer
only uses current post text as input (no additional
context).
D Annotation Guidelines and
Annotation Interface
D.1 Annotation task
In this task, you will see part of a conversation be-
tween a few people playing D&D . The players and
their characters are listed at the beginning of the
conversation. The conversations that are shown as
context are real conversations from players. Your
job is to read the context and then rate different
responses for a player/character given conversa-
tional context. Please note that the context you aregiven represents only a part of the players’ past
conversations/interactions with one another during
the game.
For each response, you would be asked the fol-
lowing questions.
• Does the response make sense?
–Use your common sense here. Is the re-
sponse completely reasonable in terms
of the rules of D&D ?
–The response “makes sense” if it is cohe-
sive as a standalone statement, consistent
with the rules of the game, and the ele-
ments/entities mentioned are plausible,
given the prior context.
–If anything seems off—not ﬂuent, con-
fusing, illogical, out of context, or wrong
according to the rules of D&D —then
rate it as Does not make sense. If in
doubt, choose Does not make sense.
• Is the response speciﬁc?
–You may be asked to assess whether the
response is speciﬁc to a given context.
In other words, do you think that the re-
sponse represents a good thing for the
character to do now?
–The response is "speciﬁc" if it ﬂows log-
ically from the narrative established by
the prior context.
*Note: It is possible for a response to
"make sense" (due to being cohesive,
consistent and plausible in and of it-
self), but be marked "not speciﬁc"
when it is not a logical next step in
the overall game progression.
*Note: "Speciﬁc" for the purposes of
this task does not have to do with
how detailed the response is per se;
a response can be fairly general in
its language, but still qualify as "spe-
ciﬁc" when it is a logical next step in
the overall game progression.
• How interesting is the response?
–You may be asked to score the response
for its interestingness on a scale of 10.
Choose a high score for “Interesting” if
the response would likely catch some-
one’s attention or arouse curiosity in the
game; or it is insightful, creative, or
witty with respect to the game. If the
response is monotonous and predictable,
or if you’re unsure, then it is Less Inter-9391State variable Model Type Multi-valued Availability Evaluation metric Performance
Character Span labeller Text No 42% - -
Race Classiﬁer Text No >58% Macro AUC 0.45
Class Classiﬁer Text No >75% Macro AUC 0.71
Pronous Classiﬁer Text No 42% Macro AUC 0.92
Inventory Span labeller Text Yes 11% - -
In combat? Classiﬁer Score No 100% Accuracy 0.91
Action Classiﬁer Text Yes 20% Macro AUC 0.92
esting.
D.2 Annotation Interface
A mock up of the annotation user interface is given
in Figure 3.
D.3 Survey of Raters
We recruited raters who had a background in role
playing games and an understanding of the fantasy
genre. We surveyed our raters, asking them the
following questions:
1.Have you ever played Dungeons and Dragons
or another role playing game before?
2. If so,
•roughly how many times have you
played
•were you a player or a game master or
both
3. If not,
•what kind of exposure do you have to
Dungeons and Dragons? (For example,
have you seen it referred to in TV or
movies)
4.Are you a fan of the fantasy genre (like Lord
of the Rings)?
Our 6 raters responded to the survey as follows:
5 out of the 6 have played D&D or another role
playing game before. All 5 of those who have
played D&D /other role playing games before have
played more than 6 times. Of the 5 who have
played D&D /other role playing games before, 3
played as both Game Master and Player. For the
one who had not played D&D /other role playing
games, they indicated they had not had much expo-
sure to D&D through TV or other channels. All 6
answered that they were fans of the fantasy genre.93929393