
Jianguo Mao, Wenbin Jiang, Xiangdong Wang, Zhifan Feng,
Yajuan Lyu, Hong Liu, Yong ZhuBeijing Key Laboratory of Mobile Computing and Pervasive Device,
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, ChinaBaidu Inc., Beijing, China
{maojianguo20s,xdwang,hliu}@ict.ac.cn
{jiangwenbin,fengzhifan,lvyajuan,zhuyong}@baidu.com
Abstract
Existing video question answering (video QA)
models lack the capacity for deep video under-
standing and flexible multistep reasoning. We
propose for video QA a novel model which per-
forms dynamic multistep reasoning between
questions and videos. It creates video semantic
representation based on the video scene graph
composed of semantic elements of the video
and semantic relations among these elements.
Then, it performs multistep reasoning for better
answer decision between the representations of
the question and the video, and dynamically
integrate the reasoning results. Experiments
show the significant advantage of the proposed
model against previous methods in accuracy
and interpretability. Against the existing state-
of-the-art model, the proposed model dramati-
cally improves more than 4%/3.1%/2%on the
three widely used video QA datasets, MSRVTT-
QA, MSRVTT multi-choice, and TGIF-QA,
and displays better interpretability by backtrac-
ing along with the attention mechanisms to the
video scene graphs.
1 Introduction
Video question answering (video QA) aims to an-
swer questions according to the given videos. It is
usually defined as a classification task, where the
most appropriate answer is chosen from a candidate
list for the given question and video. Existing meth-
ods for video QA conduct direct answering selec-
tion based on the multimodal encoding of questions
and videos (Jang et al., 2017; Lei et al., 2018, 2020).
In recent years, researchers have proposed many
optimization strategies for better performance in
video question answering, e.g., designing delicate
encoding mechanisms (Kim et al., 2020a; Nuamah,
2021; Gao et al., 2018; Li et al., 2019; Fan et al.,
2019; Le et al., 2020; Jiang et al., 2020; Kim et al.,
2020b; Seo et al., 2021), introducing video sceneFigure 1: Error cases of the previous state-of-the-art
method (Lei et al., 2021) that needed multistep reason-
ing. We demonstrate the multistep reasoning process
based on question and video(left).
graphs (Garcia and Nakashima, 2020), adopting
video pre-trained language models (Li et al., 2020;
Zellers et al., 2021; Li and Wang, 2020; Lei et al.,
2021; Sun et al., 2019), and leveraging external
knowledge or resources (Chadha et al., 2020; Gar-
cia et al., 2020; Liu et al., 2020b; Song et al., 2021;
Garcia and Nakashima, 2020). Compared with
conventional monomodal question answering tasks
such as text QA (Oguz et al., 2021; Zhou et al.,
2018; Lin et al., 2018) and table QA (Cao et al.,
2021; Wang et al., 2019). Video QA is more diffi-
cult due to the need for crossmodal understanding
and reasoning of the video and the question. Ex-
isting methods are mainly concerned with how to
encode the crossmodal features better. When faced
with a complex question, they usually lack the abil-
ities of deep understanding and complex reasoning.
Similar to the situations in text QA and table
QA, it is also necessary for video QA to deeply
understand the semantics of the context, namely,
the video, and the reasoning on the context and the
question. Statistical analysis on several datasets3894
reveals that a significant percentage of failed cases
of the state-of-the-art (SOTA) model ClipBERT
(Lei et al., 2021) is caused by the lack of deep un-
derstanding and reasoning. Figure 1 shows some
error cases of the SOTA model on MSRVTT-QA.
We randomly select one hundered error cases and
find that about 24% of the error cases need multiple
steps of reasoning on the question and the video,
and about 15% of the error cases need a deep under-
standing of the semantic of the video. These cases
could be solved by a model emphasizing deep un-
derstanding and reasoning. The questions in these
three datasets are relatively simple according to the
building procedures (Xu et al., 2017a; Yu et al.,
2018a; Jang et al., 2017). It will be more valuable
for video QA to enable deep understanding and
reasoning on the question and the video in realistic
application scenarios.
In this work, we propose a novel dynamic rea-
soning model for video QA to overcome the weak-
ness of previous models in deep understanding and
reasoning. It first creates the video semantic rep-
resentation from the video scene graph, which is
composed of the semantic elements of the video
and the semantic relations between these elements.
Then it conducts multistep reasoning of the ques-
tion based on the video semantic representation to
generate a series of video-aware question represen-
tations. Finally, it generates the most appropriate
question representation for the final answering deci-
sion by dynamically integrating these video-aware
question representations according to the reasoningcomplicity prediction. Figure 2 shows the overall
architecture of the proposed model and the com-
parison with previous methods. It simulates the
reasoning procedure of human beings, while pre-
vious methods follow the pipeline of multimodal
encoding and answering selection. In addition, the
proposed model enables the decomposition of ques-
tion understanding and video understanding, thus
leading to more opportunities for future optimiza-
tion. On the one hand, more external knowledge
resources and better reasoning architectures can be
introduced for better video QA performance. On
the other hand, it can act as a unified framework
for different QA tasks such as video QA, table QA,
and text QA.
We verify the proposed model on three well-
known datasets, MSRVTT-QA (Xu et al., 2017a),
MSRVTT multi-choice (Yu et al., 2018a), and
TGIF-QA (Jang et al., 2017), widely used in re-
cent video QA works (Jang et al., 2017; Gao et al.,
2018; Li et al., 2019; Fan et al., 2019; Le et al.,
2020; Zhu and Yang, 2020; Lei et al., 2021; Seo
et al., 2021). Experiments show that our model
achieves dramatic improvement over the powerful
state-of-the-art model ClipBERT (Lei et al., 2021),
with an average accuracy increment of more than 3
percentage points. Ablation studies show that the
dynamic reasoning strategy significantly outper-
forms previous implicit simple reasoning strategies.
The video semantic representation based on the
video scene graph makes the dynamic reasoning
strategy work better. Backtracing along with the at-3895
.
tention mechanism to the video scene graph clearly
shows the semantic elements that answer decision
relies on each reasoning step, thus giving better
interpretability than most of the previous methods.
2 Background
Given the question q, video question answering
requires choose the correct answer ˆafrom the can-
didates set Ωaccording to the video content V.
ˆa= argmaxp(a|q, V;θ) (1)
As Figure 2 (a) shows, most existing works uti-
lize offline (stop gradient) extracted dense video
features and text features. As Figure 2 (b) shows,
ClipBERT (Lei et al., 2021) achieves the state-of-
the-art by using sparsely sampled clips and raw text
for end-to-end training, yet suffer from two main
drawbacks: (i) Lack of a deep understanding of
the video content and reasoning on the question
based on the video. (ii) Strong coupling between
video and question modeling process, which needs
additional image-text pair data for pre-training to
enhance the ability of the text encoder to model
multimodal features and leads to poor scalability
and repeated computation. As shown in Figure 2
(c), we propose a simple but effective architecture
to solve the weakness of previous works. It first de-couples the question understanding module and the
video understanding module. Then, we introduce
the video scene graph to get a better representa-
tions of the video semantic information, which can
enhance the understanding of the video content
and its graph structure is also better for reasoning.
At last, we design a dynamic multistep reasoning
mechanism to iteratively deepen the understanding
of the question according to the video content.
3 Method
3.1 Overall Architecture
Figure 3 gives an overview of the model architec-
ture. For the visual representation, we use both
video scene graphs and image features. On the
one hand, we construct a video scene graph to rep-
resent semantic information in a structural form.
On the other hand, we use Swin Transformer (Liu
et al., 2021) to extract image features to make up
for the missing information of the scene graph. The
reasoning module ( Reasoner ) will iteratively up-
dates the understanding of the question according
to the video content. The Evaluator will decide
the number of reasoning steps according to the
complexity of the question. The Integrator will
integrate all intermediate reasoning results to get
the final reasoning results. The answer decision3896module chooses an answer according to the final
comprehensive understanding of the question.
3.2 Video Representation Learning
We chose a structured video scene graph to describe
video semantics which is better for reasoning. We
also extract image features by Swin Transformer
(Liu et al., 2021) to make up for the missing infor-
mation in the scene graph. The video scene graph
and the image features constitute the Video Repre-
sentation Memory shown in Figure 3, a memory
for the Reader module to access.
3.2.1 Video Scene Graph
The video scene graph is the basis for conducting
dynamic reasoning, it is a graph-based semantic
representation of video content, representing the
objects in the video, their attributes, and their re-
lationships in a structured form. Unlike the im-
age scene graph commonly used in visual question
answering, our video scene graph is semantically
richer and contains spatio-temporal information of
the video. Specifically, We first use an image cap-
tioning model to generate captions for each clip.
Then we use the scene parser (Schuster et al., 2015)
to convert each caption sentence into a semantic
sub-graph and integrate the same nodes of each
sub-graph to obtain a video scene graph. Com-
pared to caption sentences, the video scene graph
represents the video-level semantic information in
a structured form, better modeling the visual se-
mantic information.
Graph Representation Learning We first obtain
the embeddings of nodes n={n, n, ..., n}and
edges in the video scene graph via a parameter-
sharing language encoder. Then, we use graph
attention neural network (Veli ˇckovi ´c et al., 2017)
iteratively to update the representation of the scene
graph. Each node updates its representation based
on the correlation with its neighbor nodes.
n=aWn+/summationdisplayaWn (2)
where Wis a weight matrix, ais the attention
weight of node nandn, andNis the neighbors
of the node nin the graph. In our experiments, we
use standard graph attention neural network setting,
applying the LeakyReLU nonlinearity (with neg-
ative input slope α= 0.2). At the same time, the
edges have explicit meanings of relations between
nodes, so we also consider edges features ewhencalculating attention weight. The attention weight
aare computed as
a=exp(LeakyReLU(A( n,n,e)))/summationtextexp(LeakyReLU(A( n,n,e)))
(3)
A(n,n,e) =W[Wn||Wn||We]
(4)
where .represents transposition, ||is the concate-
nation operation and the WandWare weight
matrices.
3.2.2 Image Features
We extract image features to make up for the miss-
ing information in the scene graph. First, we use
Lei et al.’s sparse sampling method to sparsely
and randomly sample N clips{c}from
video. N is typically much smaller than the
entire video length N. This sampling method can
reduce the computation cost and obtain better per-
formance than dense sampling. For inference, we
uniformly sample Nclips of the same duration.
Swin Transformer (Liu et al., 2021) is one of the
mainstream visual backbone networks. It allevi-
ates the problem of large variations in the scale of
visual entities and the high resolution of pixels in
images. We use it as a vision encoder Eto extract
clip features {F}, F=E(c)∈ R,
where wis the window size and dis the feature
dimension.
3.3 Dynamic Multistep Reasoning
Humans will deepen their understanding of a com-
plex question through repeated reading the context
information. The more complex the question, the
more repetitions are required (Chang and Millett,
2013; Gorsuch and Taguchi, 2008; Carver and Hoff-
man, 1981). At each step of reading, people will
focus on different parts of the context information.
Inspired by it, we designed the dynamic multistep
reasoning mechanism. It will iteratively update the
understanding of the question based on the video
representations. We first extract the question repre-
sentation by language model RoBERTa (Liu et al.,
2019). Specifically, we concatenate the question
text with a special token [CLS] as the input and take
the[CLS] ’s hidden state Ras the representation
of the question. At the first reasoning step, we se-
lectRas the input. Then, we get question-related
information from Video Representation Memory3897through the Reader , which is an attention mecha-
nism. Then, we use this retrieved video information
to update the understanding of the question and get
the first reasoning step result Rthrough the Up-
dater . At the next reasoning step, we select Ras
the input. After Ssteps of reasoning, we obtain all
results R={R, R, R, ..., R}.
R= Updater( R, V) (5)
Updater consists of two linear transformations
with a ReLU activation in between.
where WandWare weight matrices and b
andbare biases. Vrepresents question-related
video information.
V= Reader( R,Vid) (7)
where Vid consists of the node features
{n, n, ..., n}of the video scene graph and im-
age features {F, F, ..., F}. We use Scaled
Dot-Product Attention (Vaswani et al., 2017) as the
Reader . The input consists of query Q, and context
Kof dimension d.
Reader( Q, K ) = softmax(QK
√d)K (8)
The word dynamic has two meanings: (i). dy-
namically decide the number of reasoning steps
according to the question’s complexity. (3.3 Evalu-
ator) . (ii). dynamically integrate the results of all
reasoning steps as the final result. (3.3 Integrator) .
Evaluator When humans begin faced with dif-
ferent complexity questions, they will dynamically
adjust the number of times to read relevant infor-
mation (Chang and Millett, 2013; Gorsuch and
Taguchi, 2008; Carver and Hoffman, 1981). We
propose the first dynamic reasoning strategy by im-
itating the human reading and understanding mech-
anisms. It will decides the number of reasoning
steps according to the complexity of the question.
Specifically, we perform a nonlinear transforma-
tion with GumbelSoftmax as activation function
on the question representation R, and output an
S-dimensional vector D(R)∈ Rto represents
the distribution probability of the number of rea-
soning steps from 1 to S.
D(R) = GumbelSoftmax( WR+b)(9)Where Wis a weight matrix, and bis a bias.
We choose the one with the greatest probability as
the number of the reasoning steps S.
Integrator In the reasoning process, the Reader
pays attention to the different parts of the video con-
tent at each step to gradually deepen the question’s
understanding. Therefore, we think the intermedi-
ate reasoning results are also helpful in choosing
an answer. After Ssteps of reasoning, we select all
intermediate reasoning results as input and perform
a nonlinear transformation with softmax function
to calculate the distribution of the weight of Rand
get the weighted sum as the final reasoning results
R.
R= softmax( WR+b)R (10)
where Wis a weight matrix and bis a bias.
3.4 Answer Decision
Given the final reasoning results R, we use two
fully-connected layers as a classifier to obtain the
logits lfor the answer options. Then we use a soft-
max function to obtain the probability distribution
of each answer option and apply cross-entropy loss
as our model loss L.
l= classifier( R) (11)
ˆy= softmax( l),L=−/summationdisplayylogˆy (12)
4 Related Work
Video QA requires fine-grained modeling of multi-
modal features. We have witnessed many efforts de-
voted to video understanding for video QA. Some
methods use visual techniques such as object de-
tection (Ren et al., 2016) and image captioning
(Johnson et al., 2016; Rennie et al., 2017) to extract
additional visual information (Kim et al., 2020a).
In recent years, visual pre-training based on large-
scale data has become a popular method to improve
video applications including video QA (Li et al.,
2020; Zellers et al., 2021; Li and Wang, 2020; Sun
et al., 2019). In addition, several advanced tech-
niques such as contrastive self-supervised learning
(Kim et al., 2020b) and symbolized video scene
graph (Garcia and Nakashima, 2020) are proposed
to improve the performance of video understanding3898for video QA. We extract video semantic represen-
tations based on both the visual pre-training model
(Liu et al., 2021) and the video scene graphs. Ex-
periments show the amazing complementarity of
the two kinds of information.
Video QA also requires flexible reasoning for
complicated questions and videos. Although the
reasoning capability is rarely emphasized in previ-
ous work for video QA, it is broadly investigated
in other QA tasks (Clark et al., 2018). For example,
text QA resorts to iterative update of the question
and the context (Das et al., 2019; Liu et al., 2020a),
table QA generates a structural query such as SQL
which is then executed on the tabular data (Guo
et al., 2019), and visual QA builds a specific mod-
ule network according to the question and runs it
on the image (Andreas et al., 2016; Cao et al., 2018;
Hu et al., 2017). The overall architecture of the pro-
posed method is similar to the iterative reasoning
strategies for text QA but with significant innova-
tion. Our model can dynamically determine the
best integration strategy for the intermediate rea-
soning results according to the given question and
video, leading to better interpretability and much
better performance.
5 Experiments
In this section, we validate our method on three
mainstream video QA datasets. We conduct com-
parison experiments with previous works and per-
form ablation experiments to analyze the critical
improvement in our proposed method. We use
standard train/val/test splits for all datasets and use
accuracy to measure the performance. All experi-
mental results are the mean and standard deviation
of ten replicate experiments.
5.1 Datasets
MSRVTT-QA MSR-VTT (Xu et al., 2016) is a
large video description dataset. It provides 10k web
video clips with 41.1 hours and 200k clip-sentence
pairs in total. MSRVTT-QA (Xu et al., 2017a) is
created based on clip-sentence pairs in MSR-VTT
automatically through a program. It contains 243k
open-ended questions with 1500 answers.
MSRVTT-MC MSRVTT-MC (multiple-choice)
(Yu et al., 2018a) is a dataset for video-text match-
ing tasks built on MSR-VTT with videos are used
as queries, captions as answers. Each video con-
tains five captions. Only one is correct.TGIF-QA TGIF-QA (Jang et al., 2017) dataset
contains 165K QA pairs for the animated GIFs
from the TGIF dataset. We experiment on 3 TGIF-
QA tasks: Repeating Action and State Transition
for multiple-choice QA and Frame QA for open-
ended QA. We follow most previous works and
ClipBERT’s (Lei et al., 2021) approach to leave
the Count task as future work as it requires directly
modeling full-length videos.
5.2 Results and Analysis
5.2.1 Comparison with existing approaches
As shown in Table 1, our method reaches
the new state-of-the-art and achieves
41.6%/91.4%/84.6%,90.1%,62.5%accuracy
on MSRVTT-QA/MSRVTT-MC (multi-choice)
/TGIF-QA (Action, Transition, FrameQA), with
4.2%/3.2%/1.8%,2.3%,2.2% improvement
over the previous state-of-the-art method Clip-
BERT (Lei et al., 2021). The Pre-training data
column represents that the model was pre-trained
with additional data. The results show that our
method achieves the best performance on all three
video question answering datasets without using
additional data.
5.2.2 Ablations Analysis
Comparsion of Different Architectures We
compare the three architectures shown in Figure 2,
namely, the widely-adopted architecture based on
Cross-Modal Encoding, ClipBERT, and the archi-
tecture proposed in this paper. For all architectures,
we use RoBERTa (Liu et al., 2019) as a language
encoder and Swin Transformer (Liu et al., 2021)
as a vision encoder. We sparsely sample 8 clips
from each video, then uniformly sample a single
frame within each clip. In addition, we remove
the video scene graph from our model and only
conduct single-step reasoning to ensure fairness.
Other hyper-parameters are the same for all models.
The results are given in Table 2, which show that
our architecture achieves the best performance, in
spite of the removing of the video scene graph and
only conducting single-step reasoning. This may
because that the proposed architecture makes deci-
sions based on the global video information, unlike
ClipBERT (Lei et al., 2021) and most other meth-
ods, which integrates the decision made by each
clip to get the final decision. Furthermore, unlike
previous works, we make a more apparent distinc-
tion between the process of question understanding,
video understanding, reasoning, and answer deci-3899
sion, providing a basis for introducing video scene
graphs and dynamic multistep reasoning.
Analysis of Video Scene Graph We introduce
the video scene graph to get a structural visual
semantic representation which is better for reason-
ing. We use all architectures shown in Figure 2 as
the benchmarks to evaluate the effect of the video
scene graph. We use an image captioning model
to extract captions for each clip. Then we use the
scene parser (Schuster et al., 2015) to convert each
caption sentence into a semantic sub-graph and
integrate the same nodes of each sub-graph to ob-
tain a video scene graph. We use a 2-layer Graph
Attention Network (Veli ˇckovi ´c et al., 2017) with
12 heads to learn the representations of the scene
graph. As shown in Table 3, adding the clip-level
captions improves performance. And the video
scene graph brings further improvement. The video
scene graph contains video-level semantic informa-
tion in a structural form. Therefore, it can better
represent key semantic information and reduce re-
dundant information.
Analysis of Dynamic Reasoning As Table 4
shows. We first evaluate the simple static reason-
ing mechanism adopted by most previous works.
Then we evaluate the performance of the 3.3 Eval-
uator and 3.3 Integrator . Row 0 shows the result
of the single-step reasoning, achieving 39.5% ac-
curacy on MSRVTT-QA. As expected, the static
multistep reasoning mechanism (row 1-4) performs
better and achieves the best performance around
Step = 3. But the performance does not increase
when setting a larger number of steps. It means that
the average complexity of all questions is moderate,
which can be handled well with three-step reason-3900
ing. The Evaluator can dynamically decide the
number of reasoning steps (≤5)for each question.
Row 5 shows that using different numbers of rea-
soning steps for questions of different complexity
can perform better than a static multistep reason-
ing mechanism. At the same time, we believe that
each step of reasoning pays attention to different
parts of the video content and gradually deepens
the understanding of the question. Therefore, the
intermediate reasoning results are also helpful in
choosing an answer. Row 6-17 show that the Inte-
grator can further improves the performance and
has better integration ability than the traditional
pooling method. Row 18 shows that the Evaluator
and Integrator can promote each other and achieve
the best performance.
5.3 Interpretability
Our model is interpretable by visualizing the at-
tention weight of the Reader . As Figure 4 shows,
we conclude with the following two conclusions:
(1)Video scene graph can better represents the
visual semantic information. The Video Scene
Graph contains the global semantic information
of the video, and the structured form can not only
highlight the critical semantic elements but also re-
duce the interference of redundant information. (2)
Dynamic multistep reasoning mechanism can
deepen the understanding of the question iter-
atively. With the reasoning process, the semantic
nodes related to the question obtain more and more
attention. On the contrary, the attention weight of
the irrelevant nodes reduced gradually. It shows
that the dynamic multistep reasoning mechanism
can deepen the understanding of the question.
6 Conclusion
We propose a dynamic reasoning mechanism based
on video scene graph for video QA to alleviate the
drawback of existing methods, that is, lack of deep
understanding and multistep reasoning. Experi-
ments show that our method significantly surpasses
previous methods on multiple video QA datasets
due to better understanding and reasoning mecha-
nisms and achieves much better interpretability by
backtracing along with the attention mechanism to
the video scene graph. In addition, different from
the conventional manners that perform classifica-
tion after crossmodal feature encoding, the model
realizes the decoupling of question understanding
and video understanding and the decoupling of
understanding and decision-making, thus provid-
ing more possibilities for improvement. On the
one hand, we can optimize video QA by introduc-
ing external knowledge, designing more effective
reasoning mechanisms, defining and constructing
better video scene graphs. On the other hand, we
can also jointly model multimodal QA, such as QA
based on videos, tables, texts, and graphs.3901Acknowledgements
This work is supported by the Beijing Natural Sci-
ence Foundation (Z190020). This work is also
supported by Baidu and CASICT Joint Project. We
would like to thank the anonymous reviewers for
their valuable feedback.
References390239033904