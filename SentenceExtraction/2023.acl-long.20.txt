
Linyang Li Demin Song, Xipeng Qiu
School of Computer Science, Fudan University
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
{linyangli19, dmsong20, xpqiu}@fudan.edu.cn
Abstract
Adversarial purification is a successful defense
mechanism against adversarial attacks without
requiring knowledge of the form of the incom-
ing attack. Generally, adversarial purification
aims to remove the adversarial perturbations
therefore can make correct predictions based
on the recovered clean samples. Despite the
success of adversarial purification in the com-
puter vision field that incorporates generative
models such as energy-based models and dif-
fusion models, using purification as a defense
strategy against textual adversarial attacks is
rarely explored. In this work, we introduce a
novel adversarial purification method that fo-
cuses on defending against textual adversarial
attacks. With the help of language models, we
can inject noise by masking input texts and
reconstructing the masked texts based on the
masked language models. In this way, we con-
struct an adversarial purification process for
textual models against the most widely used
word-substitution adversarial attacks. We test
our proposed adversarial purification method
on several strong adversarial attack methods in-
cluding Textfooler and BERT-Attack and exper-
imental results indicate that the purification al-
gorithm can successfully defend against strong
word-substitution attacks.
1 Introduction
Adversarial examples (Goodfellow et al., 2014) can
successfully mislead strong neural models in both
computer vision tasks (Carlini and Wagner, 2016)
and language understanding tasks (Alzantot et al.,
2018; Jin et al., 2019). An adversarial example
is a maliciously crafted example attached with an
imperceptible perturbation and can mislead neural
networks. To defend attack examples of images,
the most effective method is adversarial training
(Goodfellow et al., 2014; Madry et al., 2019) which
is a mini-max game used to incorporate perturba-
tions into the training process.Defending adversarial attacks is extremely im-
portant in improving model robustness. How-
ever, defending adversarial examples in natural
languages is more challenging due to the discrete
nature of texts. That is, gradients cannot be used
directly in crafting perturbations. The substitution-
based adversarial examples are more complicated
than gradient-based adversarial examples in im-
ages, making it difficult for neural networks to de-
fend against these substitution-based attacks.
The first challenge of defending against adver-
sarial attacks in NLP is that due to the discrete na-
ture, these substitution-based adversarial examples
can have substitutes in any token of the sentence
and each substitute has a large candidate list. This
would cause a combinatorial explosion problem,
making it hard to apply adversarial training meth-
ods. Strong attacking methods such as Jin et al.
(2019) show that using the crafted adversarial ex-
amples as data augmentation in adversarial training
cannot effectively defend against these substitution-
based attacks. Further, defending strategies such as
adversarial training rely on the assumption that the
candidate lists of the substitutions are accessible.
However, the candidate lists of the substitutions
should notbe exposed to the target model; that is,
the target model should be unfamiliar to the candi-
date list of the adversarial examples. In real-world
defense systems, the defender is not aware of the
strategy the potential attacks might use, so the as-
sumption that the candidate list is available would
significantly constrain the potential applications of
these defending methods.
Considering that it is challenging to defend
against textual adversarial attacks when the form of
the attacks cannot be acknowledged in advance, we
introduce a novel adversarial purification method
as a feasible defense mechanism against these at-
tacks. The adversarial purification method is to
purify adversarially perturbed input samples before
making predictions (Srinivasan et al., 2021; Shi338et al., 2021; Yoon et al., 2021). The major works
about adversarial purification focus on purifying
continuous inputs such as images, therefore these
works explore different generative models such as
GANs (Samangouei et al., 2018), energy-based
models (EBMs) (LeCun et al., 2006) and recently
developed diffusion models (Song et al., 2021; Nie
et al., 2022). However, in textual adversarial at-
tacks, the inputs are discrete tokens which makes
it more challenging to deploy previous adversarial
purification methods.
Therefore, we introduce a purification mecha-
nism with the help of masked language models.
We first consider the widely used masking process
to inject noise into the input; then we recover the
clean texts from the noisy inputs with the help of
the masked language models (e.g. a BERT (De-
vlin et al., 2018)). Further, considering that the
iterative process in previous adversarial purifica-
tion algorithms can be extremely costly (e.g. a
VP-SDE process in diffusion models (Song et al.,
2021)), we instead simplify the iterative process
to an ensemble-purifying process that conducting
adversarial purification multiple times to obtain an
ensembled result as a compromise to the time cost
in traditional adversarial purification process.
Through extensive experiments, we prove that
the proposed text adversarial purification algorithm
can successfully serve as defense against strong at-
tacks such as Textfooler and BERT-Attack. Exper-
iment results show that the accuracy under attack
in baseline defense methods is lower than random
guesses, while after text purification, the perfor-
mance can reach only a few percent lower than the
original accuracy when the candidate range of the
attack is limited. Further, extensive results indicate
that the candidate range of the attacker score is
essential for successful attacks, which is a key fac-
tor in maintaining the semantics of the adversaries.
Therefore we also recommend that future attacking
methods can focus on achieving successful attacks
with tighter constraints.
To summarize our contributions:
(1) We raise the concern of defending
substitution-based adversarial attacks without ac-
knowledging the form of the attacks in NLP tasks.
(2) To the best of our knowledge, we are the first
to consider adversarial purification as a defense
against textual adversarial attacks exemplified by
strong word-substitution attacks and combine text
adversarial purification with pre-trained models.(3) We perform extensive experiments to demon-
strate that the adversarial purification method is
capable of defending strong adversarial attacks,
which brings a new perspective to defending tex-
tual adversarial attacks.
2 Related Work
2.1 Adversarial Attacks in NLP
In NLP tasks, current methods use substitution-
based strategies (Alzantot et al., 2018; Jin et al.,
2019; Ren et al., 2019) to craft adversarial exam-
ples. Most works focus on the score-based black-
box attack, that is, attacking methods know the
logits of the output prediction. These methods use
different strategies (Yoo et al., 2020; Morris et al.,
2020b) to find words to replace, such as genetic
algorithm (Alzantot et al., 2018), greedy-search
(Jin et al., 2019; Li et al., 2020) or gradient-based
methods (Ebrahimi et al., 2017; Cheng et al., 2019)
and get substitutes using synonyms (Jin et al., 2019;
Mrkši ´c et al., 2016; Ren et al., 2019) or language
models (Li et al., 2020; Garg and Ramakrishnan,
2020; Shi et al., 2019).
2.2 Adversarial Defenses
We divide the defense methods for word-
substitution attacks by whether the defense method
requires knowledge of the form of the attack.
When the candidate list is known, recent works
introduce defense strategies that incorporate the
candidates of the words to be replaced as an aug-
mentation. Jin et al. (2019); Li et al. (2020); Si et al.
(2020) uses generated adversaries to augment the
classifier for better defense performances; Jia et al.
(2019); Huang et al. (2019) introduce a certified
robust model to construct a certified space within
the range of a candidate list therefore the substitu-
tions in the candidate list cannot perturb the model.
Zhou et al. (2020); Dong et al. (2021) construct a
convex hull based on the candidate list which can
resist substitutions in the candidate list.
To defend unknown attacks, NLP models can in-
corporate gradient-based adversarial training strate-
gies (Miyato et al., 2016; Madry et al., 2019)
since recent works (Ebrahimi et al., 2017; Cheng
et al., 2019; Zhu et al., 2019; Li and Qiu, 2020)
show that gradient-based adversarial training can
also improve defense performances against word-
substitution attacks.339
2.3 Adversarial Purification
Adversarial purification is a defense strategy that
uses generative models to purify adversarial inputs
before making predictions, which is a promising
direction in adversarial defense. Samangouei et al.
(2018) uses a defensive GAN framework to build
clean images to avoid adversarial attacks. Energy-
based models (EBMs) are used to purify attacked
images via Langevin dynamics (LeCun et al., 2006).
Score-based models (Yoo et al., 2020) is also in-
troduced as a purification strategy. Recent works
focus on exploring diffusion models as the purifi-
cation model in purifying the attacked images (Nie
et al., 2022). Though widely explored, adversarial
purification strategy is less explored in the NLP
field.
3 Text Adversarial Purification
3.1 Background of Adversarial Purification
A classic adversarial purification process is to grad-
ually purify the input through Tsteps of purifica-
tion runs. As seen in Figure 1, the purification
process in the image domain is to first constructan input xfrom the perturbed input xby injecting
random noise. Then the purification algorithm will
recover the clean image /hatwidexfrom the noisy image x
which usually takes multiple rounds. The intuition
of such a purification process is that the recovered
inputs will not contain adversarial effects.
Specifically, in the score-based adversarial purifi-
cation (Yoo et al., 2020), the sample injected with
random noise is x=x+εwhere ε∼ N(0, σI)
and the goal is to purify xwith score network s.
In a continuous time step where x=x, the goal
is to recover xthrough a score-based generative
model x=x+αs(x)where αis the
step size related to x. After Ttimes of gener-
ation, the recovered /hatwidex=xis used in the final
prediction which contains less adversarial effect.
As for the diffusion-based purification methods
(Nie et al., 2022), the process includes a forward
diffusion process and a reverse recovery process.
The noise injection process is a forward stochastic
differential equation (SDE), that is, the noisy input
x=x(T)and initial perturbed input x=x(0).
The diffusion process is x(T) =/radicalbig
α(T)x(0) +/radicalbig
1−α(T)εwhere αis a hyper-parameter and340ε∼ N(0, σI). The final purified input /hatwidex=/hatwidex(0)
where/hatwidex(0)is the reverse-time SDE generated input
from the diffused input x(T).
3.2 Text Adversarial Purification with BERT
Instead of the iterative purification process used in
purifying images, we introduce a novel purification
method that purifies the input texts via masking and
masks prediction with pre-trained masked language
models exemplified by BERT (Devlin et al., 2018).
As seen in Figure 1, instead of gradually adding
noise and recovering the clean sample from the
noisy samples, we inject random noise into the
input texts multiple times and recover the noisy
data to a clean text based on the mask-prediction
ability of the masked language model F(·).
Considering that the perturbed text is X, we
can inject noise to construct multiple copies X=
[w,···,[MASK] , w,···,]. We use two simple
masking strategies: (1) Randomly mask the input
texts; (2) Randomly insert masks into the input
texts. Such a random masking process is similar
to adding a random noise ε∼ N(0, σI)to the
inputs x.
After constructing multiple noisy inputs, we run
the denoise process via masked language models:
/hatwideX=F(X).
With Nrecovered texts, we are able to make
predictions with the classifier F(·):S=/summationtext/parenleftig
Softmax (F(/hatwideX))/parenrightig
.
Unlike continuous perturbations to images,
word-substitution adversarial samples only contain
several perturbed words. Therefore, we consider
using a multiple-time mask-and-recover process
as text adversarial purification, which makes full
use of the pre-trained ability of the masked lan-
guage models. Compared with the generation pro-
cess used in image adversarial purification, masked
language model-based purification method is eas-
ier to implement and utilize in pre-trained model-
based applications as a defense against strong word-
substitution adversarial attacks.
3.3 Combining with Classifier
Normal adversarial purification methods are plug-
and-play processes inserted before the classifica-
tion, however, the masked language model itself
is a widely used classification model. That is,
the purification model F(·)and the classification
model F(·)can share the same model. There-
fore, instead of using a normal masked languagemodel such as BERT, we train the classifier and the
mask-filling ability as multi-tasks. The classifica-
tion loss is L=L(F(X), y, θ)+L(F(X), y, θ)
and the masked language model loss is L=
L(F(X), X, θ ). Here, the input Xis the clean
text used in training the classifier and the Xis the
random masked text. The loss function L(·)is the
cross-entropy loss used in both the text classifica-
tion head and masked language modeling head in
the pre-trained models exemplified by BERT.
In this way, we are utilizing the pre-trained mod-
els to their full ability by using both the mask-filling
function learned during the pre-training stage as
well as the generalization ability to downstream
tasks.
Algorithm 1 Adversarial Training
Require: Training Sample X, adversarial step TX←Inject Noise Xδ←N(0, σ)// Init Perturbfort= 0,1, ...Tdo g← ▽(L+L)// Get Perturbation δ←/producttext(δ+α·g/||g||)L← L(F(X+δ), X, θ ) X←X+δ// Update Input g=g+▽(L+L+L)θ←θ−g// Update model parameter θ
3.4 Combining with Adversarial Training
Different from the image field where adver-
saries are usually generated by gradients, word-
substitution attacks do not have direct connections
with gradient-based adversaries in the text domain.
Therefore, it is intuitive to incorporate gradient-
based adversarial training in the purification pro-
cess when the purification process is combined with
the classifier training.
We introduce the adversarial training process
therefore the purification function F(·)includes
mask-prediction and recovering clean texts from
inputs with gradient-based perturbations, which
leads to stronger purification ability compared with
a standard BERT.
Following standard adversarial training process
with gradient-based adversaries introduced by Zhu
et al. (2019); Li and Qiu (2020). In the adversarial
training process, a gradient-based perturbation δis
added to the embedding output of the input text X
(for simplicity, we still use XandXto denote the341embedding output in the Algorithm 1). Then the
perturbed inputs are added to the training set in the
training process. We combine gradient-based ad-
versarial training with the text purification process.
As illustrated in Algorithm 1, for an adversarial
training step, we add perturbations to the masked
textXand run Ttimes of updates. We calculate
gradients based on both classification losses Land
masked language modeling losses L; further,
as seen in line 6, we also calculate the loss that the
masked language model will predict the texts from
the perturbed text X+δ, which enhanced the text
recover ability from noisy or adversarial texts.
4 Experiments
4.1 Datasets
We use two widely used text classification datasets:
IMDB(Maas et al., 2011) and AG’s News
(Zhang et al., 2015) in our experiments. The IMDB
dataset is a bi-polar movie review classification
task; the AG’s News dataset is a four-class news
genre classification task. The average length is
220 words in the IMDB dataset, and 40 words in
the AG’s News dataset. We use the test set fol-
lowing the Textfooler 1k test set in the main re-
sult and sample 100 samples for the rest of the
experiments since the attacking process is seriously
slowed down when the model is defensive.
4.2 Attack Methods
Popular attack methods exemplified by genetic Al-
gorithm (Alzantot et al., 2018), Textfooler (Jin
et al., 2019) and BERT-Attack (Li et al., 2020)
can successfully mislead strong models of both
IMDB and AG’s News task with a very small per-
centage of substitutions. Therefore, we use these
strong adversarial attack methods as the attacker to
test the effectiveness of our defense method. The
hyperparameters used in the attacking algorithm
vary in different settings: we choose candidate list
sizeKto be 12, 48, and 50 which are used in the
Textfooler and BERT-Attack methods.
We use the exact same metric used in Textfooler
and BERT-Attack that calculates the after-attack
accuracy, which is the targeted adversarial evalu-
ation defined by Si et al. (2020). The after-attack
accuracy measures the actual defense ability of the
system under adversarial attacks.4.3 Victim Models and Defense Baselines
The victim models are the fine-tuned pre-train mod-
els exemplified by BERT and RoBERTa, which we
implement based on Huggingface Transformers
(Wolf et al., 2020). As discussed above, there are
few works concerning adversarial defenses against
attacks without knowing the candidates in NLP
tasks. Moreover, previous works do not focus on
recent strong attack algorithms such as Textfooler
(Jin et al., 2019), BERT-involved attacks (Li et al.,
2020; Garg and Ramakrishnan, 2020) Therefore,
we first list methods that can defend against adver-
sarial attacks without accessing the candidate list
as our baselines:
Adv-Train (Adv-HotFlip) : Ebrahimi et al.
(2017) introduces the adversarial training method
used in defending against substitution-based adver-
sarial attacks in NLP. It uses gradients to find actual
adversaries in the embedding space.
Virtual-Adv-Train (FreeLB) : Li and Qiu
(2020); Zhu et al. (2019) use virtual adversaries
to improve the performances in fine-tuning pre-
trained models, which can also be used to deal with
adversarial attacks without accessing the candidate
list. We follow the standard FreeLB training pro-
cess to re-implement the defense results.
Further, there are some works that require the
candidate list, it is not a fair comparison with de-
fense methods without accessing the candidates, so
we list them separately:
Adv-Augmentation : We generate adversarial
examples of the training dataset as a data augmen-
tation method. We mix the generated adversarial
examples and the original training dataset to train
a model in a standard fine-tuning process.
ASCC : Dong et al. (2021) also uses a convex-
hull concept based on the candidate vocabulary as
a strong adversarial defense.
ADA : Si et al. (2020) uses a mixup strategy
based on the generated adversarial examples to
achieve adversarial defense with variants AMDA-
SMix that mixup the special tokens.
FreeLB++ : Li et al. (2021) introduces a variant
of FreeLB method that expands the norm bound.
RanMASK : Zeng et al. (2021) introduces a
masking strategy that makes use of noises to im-
prove robustness.342
4.4 Implementations
We use BERT-BASE and RoBERTa-BASE models
based on the Huggingface Transformers. We mod-
ify the adversarial training with virtual adversaries
based on the implementation of FreeLB, TA V AT,
and FreeLB++. The training hyper-parameters we
use are different from FreeLB and TA V AT since we
aim to find large perturbations to simulate adver-
saries. We set adversarial learning rate α=1e-1 to
and normalization boundary ϵ=2e-1 in all tasks.
We set the multiple purification size N=to 16 for
all tasks and we will discuss the selection of Nin
the later section.
For our text adversarial purification method, weuse the model that is trained with gradient-based
adversarial training as the purification model F(·)
and the classifier F(·)for the main experiments
and conduct thorough ablations to explore the ef-
fect of combining purification with classifier and
adversarially trained classifier.
As for implementing adversarial attack methods,
we use the TextAttack toolkit while referring the
official codes of the corresponding attack methods(Morris et al., 2020a). The similarity thresholds
of the word-substitution range are the main fac-
tors of the attacking algorithm. We tune the USE
(Cer et al., 2018) constraint 0.5 for the AG task
and 0.7 for the IMDB task and 0.5 for the cosine-
similarity threshold of the synonyms embedding
(Mrkši ´c et al., 2016) which can reproduce the re-
sults of the attacking methods reported.
4.5 Results
As seen in Table 1, the proposed Text Adversar-
ial Purification algorithm can successfully defend
strong attack methods. The accuracy of our defend-
ing method under attack is significantly higher than
non-defense models (50% vs 20% in the IMDB
dataset). Compared with previous defense methods,
our proposed method can achieve higher defense
accuracy in both the IMDB task and AG’s News
task. The Adv-HotFlip and the FreeLB methods343
are effective, which indicates that gradient-based
adversaries are not very similar to actual substitu-
tions. We can see that Adv-HotFlip and FreeLB
methods achieve similar results (around 30% when
K= 12 ) which indicates that gradient-based adver-
sarial training methods have similar defense abili-
ties no matter whether the adversaries are virtual or
real since they are both unaware of the attacker’s
candidate list. Also, the original accuracy (on the
clean data) of our method is only a little lower
than the baseline methods, which indicates that the
purified texts still contain enough information for
classification. The RoBERTa model also shows
robustness using both original fine-tuned model
and our defensive framework, which indicates our
purification algorithm can be used in various pre-
trained language models. Compared with methods
that specifically focus on adversarial defense, our
proposed method can still surpass the state-of-the-
art defense system FreeLB++ (Li et al., 2021) and
RanMASK (Zeng et al., 2021).
Further, the candidate size is extremely im-
portant in defending against adversarial attacks,
when the candidate size is smaller, exemplified by
K= 12 , our method can achieve very promising
results. As pointed out by Morris et al. (2020b),
the candidate size should not be too large that the
quality of the adversarial examples is largely dam-
aged.
As seen in Table 2, we compare our method
with previous access-candidates defense methods.When defending against the widely used Textfooler
attack and genetic attack (Alzantot et al., 2018),
our method can achieve similar accuracy even
compared with known-candidates defense meth-
ods. As seen, the data augmentation method can-
not significantly improve model robustness since
the candidates can be very diversified. Therefore,
using generated adversarial samples as an aug-
mentation strategy does not guarantee robustness
against greedy-searched methods like Textfooler
and BERT-Attack.
4.6 Analysis
4.6.1 Ablations
As we design an adversarial purification algo-
rithm with masked language models and propose
a multiple-recovering strategy, we aim to explore
which process helps more in the purification de-
fense system. Plus, we combine classifiers within
the purification model so it is also important to
explore whether such a combination is helpful.
For each type of purification method, we test
whether the specific purification process we pro-
pose is effective. That is, we test whether making
multiple recoveries in the purification process is
helpful; also, we test whether using both masking
tokens and inserting additional masks is helpful.
As seen in Table 3, we can summarize that:
(1) Multi-time recovering is necessary: in the
image domain, multiple reconstructions with a con-
tinuous time purification process are necessary.344
Similarly, the multi-recovery process is important
in obtaining high-quality purification results. We
can observe that one-time recovery cannot achieve
promising defense performances.
(2) Combining classifiers is effective: we can
observe that when we use trained classifiers and
masked language models, the defense perfor-
mances are better than using fine-tuned classifier
and vanilla BERT as a masked language model,
indicating that such a combined training process is
helpful in obtaining more strong defense systems.
Also, with gradient-based adversarial training, the
purification process can obtain a further boost, indi-
cating that our proposed text purification algorithm
can be used together with previous defense meth-
ods as an advanced defense system.
4.6.2 Example of Purification Results
As seen in Table 4, we construct multiple recoveries
and use the averaged score as the final classifica-
tion result. Such a purification process is effective
compared with vanilla fine-tuned BERT.
We can observe that the adversarial sample that
successfully attacked the vanilla BERT model only
achieves this by replacing only a few tokens. While
with the purification process, the attack algorithm
is struggling in finding effective substitutions to
achieve a successful attack. Even replacing a large
number of tokens that seriously hurt the semantics
of the input texts, with the purification process in-
volved, the classifier can still resist the adversarialeffect. Further, by observing the purified texts, we
can find that the purified texts can make predictions
correctly though some substitutes still exist in the
purified texts, indicating that making predictions
based on purified texts using the combined trained
classifier can obtain a promising defense perfor-
mance. That is, our proposed method, though is
not a plug-and-play system, can be used as a gen-
eral system as a defense against substitution-based
attacks.
5 Conclusion and Future Work
In this paper, we introduce a textual adversar-
ial purification algorithm as a defense against
substitution-based adversarial attacks. We utilize
the mask-infill ability of pre-trained models to re-
cover noisy texts and use these purified texts to
make predictions. Experiments show that the pu-
rification method is effective in defending strong
adversarial attacks without acknowledging the sub-
stitution range of the attacks. We are the first to
consider the adversarial purification method with
a multiple-recovering strategy in the text domain
while previous successes of adversarial purifica-
tion strategies usually focus on the image field.
Therefore, we hope that the adversarial purification
method can be further explored in NLP applica-
tions as a powerful defense strategy.345Limitations
In this paper, we discuss an important topic in the
NLP field, the defense against adversarial attacks
in NLP applications. We provide a strong defense
strategy against the most widely used word substi-
tution attacks in the NLP field, which is limited in
several directions.
•We are testing defense strategies using down-
stream task models such as BERT and
RoBERTa, and the purification tool is a model
with a mask-filling ability such as BERT. Such
a process can be further improved with strong
models such as large language models.
•We study the concept of adversarial purifica-
tion in the adversarial attack scenarios with
word-substitution attacks on small fine-tuned
models. The concept of adversarial purifica-
tion can be further expanded to various NLP
applications. For instance, the purification of
natural language can be used in malicious text
purification which is more suitable in applica-
tions with large language models.
Acknowledgement
This work was supported by the National Natu-
ral Science Foundation of China (No. 62236004
and No. 62022027) and CAAI-Huawei MindSpore
Open Fund.
References346347Appendix
Recovery Number Analysis
One key problem is that how many recoveries
we should use in the recovering process, as finding
a proper Tis also important in the image-domain
purification process. We use two attack methods
withK= 12 to test how the accuracy varies when
using different recovery number N.
As seen in Fig. 2 (a), the ensemble size is actu-
ally not a key factor. Larger ensemble size would
not result in further improvements. We assume that
larger ensemble size will smooth the output score
which will benefit the attack algorithm. That is,
the tiny difference between substitutes can be de-
tected by the attack algorithm since the confidence
score is given to the attack algorithms. Still, we
can conclude that a multiple recovery process is ef-
fective in the purification process and quite simple
to implement.
Candidate Size Analysis
The attack algorithms such as BERT-Attack and
Textfooler use a wide range of substitution set (e.g.
K=50 in Textfooler means for each token to replace,
the algorithm will find the best replacement in 50
candidates), which seriously harms the quality of
the input texts.
As seen in Fig. 2 (b), when the candidate is 0, the
accuracy is high on the clean samples. When the
candidate is 6, the normal fine-tuned BERT model
cannot correctly predict the generated adversarial
examples. This indicates that normal fine-tuned
BERT is not robust even when the candidate size
is small. After purification, the model can tolerate
these limited candidate size attacks. When the can-
didate size grows, the performance of our defense
framework drops by a relatively large margin. We
assume that large candidate size would seriously
harm the semantics which is also explored in Mor-
ris et al. (2020b), while these adversaries cannot
be well evaluated even using human-evvaluations
since the change rate is still low.348ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Left blank.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Left blank.349/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Left blank.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Left blank.350