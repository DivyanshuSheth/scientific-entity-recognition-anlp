
Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai,
Hua Wu,Boxi Cao,Xianpei Han,Le SunChinese Information Processing LaboratoryState Key Laboratory of Computer Science
Institute of Software, Chinese Academy of Sciences, Beijing, ChinaBaidu Inc., Beijing, ChinaUniversity of Chinese Academy of Sciences, Beijing, China
{jiawei2020,yaojie,hongyu,boxi2020,xianpei,sunle}@iscas.ac.cn
{loujie,jiawei07,daidai,wu_hua}@baidu.com
Abstract
Named entity recognition in real-world ap-
plications suffers from the diversity of en-
tity types, the emergence of new entity
types, and the lack of high-quality annota-
tions. To address the above problems, this
paper proposes an in-context learning-based
NER approach, which can effectively inject
in-context NER ability into PLMs and recog-
nize entities of novel types on-the-fly using
only a few demonstrative instances. Specif-
ically, we model PLMs as a meta-function
λ .M, and a new en-
tity extractor can be implicitly constructed by
applying new instruction and demonstrations
to PLMs, i.e., (λ.M)(instruction, demonstra-
tions)→ F where Fwill be a new entity ex-
tractor, i.e., F: text→entities. To inject the
above in-context NER ability into PLMs, we
propose a meta-function pre-training algorithm,
which pre-trains PLMs by comparing the (in-
struction, demonstration)-initialized extractor
with a surrogate golden extractor. Experimental
results on 4 few-shot NER datasets show that
our method can effectively inject in-context
NER ability into PLMs and significantly out-
performs the PLMs+fine-tuning counterparts.
1 Introduction
Named entity recognition (NER) aims to detect
and classify named entities in text, such as Peo-
ple,Disease , and Movie . Traditional NER meth-
ods (Lample et al., 2016; Li et al., 2020c; Yan
et al., 2021) have achieved remarkable successFigure 1: Illustration of in-context NER, which uses
instruction, demonstrations, and text as input to identify
entities. The in-context learning model can be regarded
as a meta-function that takes instruction and demonstra-
tions as input and produces an entity extractor capable
of identifying the desired entities (Akyürek et al., 2022).
when entity types are pre-defined and massive high-
quality annotations are provided. Unfortunately,
real-world NER still suffers from the diversity of
entity types (e.g., the extraction of Movie is very
different to Disease ), the emergence of new en-
tity types (e.g., Virus of Cov-19 ), and the lack of
high-quality annotations.
To address these problems, recent studies often
employ few-shot learning techniques, including
fine-tuning-based and metric-based methods. Fine-
tuning-based methods extract entities of new types
by adjusting model weights using new instances
(Ma et al., 2022a; Chen et al., 2022a; Das et al.,
2022). The main drawbacks of these methods are
that re-training is often expensive (especially for
large-scale models) and new entity types cannot
be addressed on-the-fly. Metric-based methods are
free from updating parameters and identifying en-
tities by learning to compare query instances with
support instances (or prototypes) (Yang and Kati-
yar, 2020; Tong et al., 2021). These methods are
limited to the matching architectures and are sensi-
tive to domain shift since they do not fully explore
the information of target domain (Ma et al., 2022c).
In this paper, we propose an in-context learning-13661based NER approach, which can effectively address
the above problems by injecting in-context NER
ability into PLMs and then recognizing entities of
new types on-the-fly using only a few demonstra-
tive instances. Specifically, we model PLMs as
a meta-function (Akyürek et al., 2022) for NER
λ .M, and a new entity ex-
tractor can be implicitly constructed by applying
new instruction and demonstrations to PLMs, i.e.,
(λ.M)(instructions, demonstrations) → F where
Fwill be a new entity extractor F: text→entities.
For example, in Figure 1, our method can construct
entity extractors of new Disease andVirus types
on-the-fly by applying PLMs using demonstrations
such as “Text: Cancer is a leading cause of death
worldwide. Entities: Cancer is disease”. Further-
more, we propose a meta-function pre-training al-
gorithm to inject the above in-context NER ability
into PLMs. The algorithm pre-trains PLMs by com-
paring the implicitly (instruction, demonstration)-
constructed extractor with an explicitly fine-tuned
surrogate golden extractor. The comparison en-
sures that the meta-function (λ.M)will generate
an entity extractor Ffrom instructions and demon-
strations as accurately as possible.
The proposed method can seamlessly leverage
the powerful language understanding and genera-
tion capabilities of large-scale PLMs (Brown et al.,
2020), effectively address diverse and new entity
types through in-context learning, and only requires
a couple of demonstrations for each entity type.
Compared to fine-tuning methods, our method does
not require expensive retraining, and new entity
types can be extracted on-the-fly, with no need
for model weight adjusting. Compared with metric-
based methods, our method can dynamically utilize
the information entailed in instruction and demon-
strations rather than be limited to the fixed metric
space.
To verify the effectiveness of our method, we fur-
ther pre-train PLMs using a large-scale distantly an-
notated NER dataset from Wikipedia and Wikidata.
Experimental results on 4 few-shot NER bench-
marks show that our method can effectively inject
in-context NER ability into PLMs and significantly
outperforms the PLMs+fine-tuning counterparts.
In general, this paper’s main contributions are:
•We propose an in-context NER method that
can effectively extract entities of novel typeson-the-fly using only a few demonstrative in-
stances.
•We design a meta-function pre-training algo-
rithm, which models PLMs as a meta-function
and injects in-context NER ability into PLMs
by comparing the (instruction, demonstration)-
constructed extractor with a surrogate golden
extractor.
•How to inject in-context ability into small
models is an important research direction of
NLP in the big model era. Our work can ben-
efit new directions for future works.
2 Related work
Few-shot NER Few-shot learning is a promis-
ing technique for low-resource NER. Currently,
there are two main categories of FS-NER methods:
fine-tuning-based methods and metric-based meth-
ods. Fine-tuning-based FS-NER methods re-train
NER models using new instances. Metric-based
methods identify entities by pre-training to com-
pare query instances with support instances (Snell
et al., 2017; Fritzler et al., 2019; Yang and Kati-
yar, 2020; Tong et al., 2021; Wang et al., 2022; Ji
et al., 2022) using given NER datasets. FS-NER
is a challenging task, and several improvements
have been proposed to enhance its performance.
These include leveraging label information (Hou
et al., 2020; Wang et al., 2021a; Lu et al., 2022b;
Ma et al., 2022a; Chen et al., 2022a; Yang et al.,
2022), designing new paradigms such as decom-
position methods (Ji et al., 2022; Ma et al., 2022c;
Yang et al., 2022), prompt-based methods (Cui
et al., 2021; Liu et al., 2022; Ma et al., 2022b),
and demonstration-based methods (Lee et al., 2022;
Zhang et al., 2022a); , and proposing new learn-
ing strategies like meta-learning (Li et al., 2020a,b;
de Lichy et al., 2021; Ma et al., 2022c), contrastive
learning (Das et al., 2022), and self-training (Huang
et al., 2021; Wang et al., 2021b). In this paper, we
address FS-NER via in-context learning (Gutiér-
rez et al., 2022), which empowers PLMs with in-
context learning ability and entities of new entity
types can be extracted on-the-fly.
In-context learning The in-context learning abil-
ity has been observed in large-scale PLMs such as
GPT-3 (Brown et al., 2020), and has been widely
applied in different tasks such as “chain of thought”
reasoning (Wei et al., 2022). Recent studies13662
aim to enhance in-context learning by selecting
valuable demonstrations (Liu et al., 2021; Rubin
et al., 2022), optimizing the order of demonstra-
tions (Lu et al., 2022a), and calibrating output dis-
tributions (Zhao et al., 2021). Some studies try
to replicate in-context learning in smaller mod-
els (Min et al., 2022a; Chen et al., 2022b). Ad-
ditionally, some researchers attempt to replicate in-
context learning using smaller models (Min et al.,
2022b; Chan et al., 2022). Furthermore, there
are efforts to understand the underlying mecha-
nisms (Akyürek et al., 2022) of in-context learning
which suggest that it can be compared to a meta-
function and facilitate implicit fine-tuning (Dai
et al., 2022; von Oswald et al., 2022). This paper
is inspired by previous studies and considers in-
context named entity recognition (NER) as a meta-
function. To enhance the ability of pre-trained lan-
guage models (PLMs) to perform in-context NER,
we propose an effective pre-training algorithm.
Unlike MetaICL (Min et al., 2022a), which only
transforms multi-task learning into the form of in-
context learning for pre-training, our approach also
includes meta-function pre-training (Section 4.3)
based on the underlying mechanisms of in-context
learning.
3 In-context Named Entity Recognition
This section describes how to recognize entities
through in-context NER. In in-context learning, the
model will read the information of target entity
types from both instruction and demonstrations,
and then extract entities of target types within the
text. In this way, new entity types can be extracted
on-the-fly, without the need for model retraining.Concretely, this paper formulates in-context
NER as a sequence-to-sequence generation pro-
cess. The input X= [I;D;T]includes instruction
I, demonstrations D, and text Twhile the output is
a list of extracted entities Y= [e, ..., e]. Figure 2
shows an example, where an in-context NER model
will identify that the target entity types are Disease
andVirus , distill the knowledge about these types
from demonstrations(e.g., the context patterns of a
disease), and finally recognize "SARS-CoV-2" as
virus and “COVID-19” as disease using the above
knowledge. The details are described as follows.
Instruction The instruction is a sequence of tar-
get entity types, guiding the model to extract what
entity types (Min et al., 2022a). The instruction
for target entity types {l, . . . , l}isI=“Target
types: l;. . .;l”. For example, in Figure 2 the
instruction is “Target types: disease; virus”.
Demonstrations Demonstrations provide the
intra-class knowledge of target entity types (e.g.,
entity semantics and context patterns) and illus-
trate the form of outputs. As shown in Figure 2, the
demonstrations contain the illustrative instances for
different target types, and each instance is “Text:
{text} Entities: {extractions}”, where {extractions}
are entities presented in the {text}.
Extractions The output of the extraction process
is a list of entities, denoted as Y= [e, . . . , e]
where eisi-th extracted entities. Each extraction
eis represented as “ E istype”. For instance,
in Figure 2, the extraction “COVID-19 is disease.”
indicates that “COVID-19” is an entity mention
with the type “Disease”. This natural language-
like representation allows us to better utilize the
text generation capabilities of pre-trained language
models. During inference, we locate all mentions
in the text and further output their locations.
Architecture Given the above task formula-
tion, we employ an encoder-decoder network like
T5 (Raffel et al., 2020), where the encoder en-
codes <instruction, demonstrations, text> and the
decoder generates all extractions as a tokenized text
sequence Y= [y, . . . , y].
The success of in-context NER depends on two
critical abilities: the in-context learning ability and
the extraction ability. For in-context learning, the
models should be able to implicitly construct accu-
rate extractors of new entity types by following the
instruction and capturing the knowledge in demon-13663strations. In this way, we can see a PLM as a
meta-function, i.e., a function of extractors whose
input is (instruction, demonstrations) and whose
output is an entity extractor. For extraction, the
models should be able to locate specific spans and
categorize them into target entity types. The fol-
lowing section demonstrates how to inject such an
in-context learning ability into PLMs and construct
an effective in-context NER model.
4 Meta-Function Pre-training for
In-Context NER
In this section, we will explain how to incorporate
in-context named entity recognition (NER) capabil-
ities into pre-trained language models (PLMs). Al-
though large-scale PLMs like GPT-3 have demon-
strated the ability to learn in-context, this capability
is not always controllable or predictable. Addition-
ally, unlike classification and question-answering
tasks that align with the pre-training objective of
language models (i.e., producing natural text out-
put), NER requires more complex span extraction
and type specification. As a result, Gutiérrez et al.
(2022) show that LMs aren’t well-suited for in-
context NER tasks. In this paper, we propose meta-
function pre-training, an algorithm that can inject
in-context NER ability into PLMs in a controllable
and predictable way.
Specifically, we model PLMs as a meta-
function (Akyürek et al., 2022) for NER
λ .M, and a new entity ex-
tractor can be implicitly constructed by applying
new instructions and demonstrations to PLMs, i.e.,
(λ.M)(instructions, demonstractions) → F where
Fwill be a new entity extractor F:text→entities.
Based on the meta-function formulation, we further
pre-train PLMs for in-context NER abilities by:
•optimizing PLMs via a meta-function
loss, so that the implicitly (instruction,
demonstration)-constructed extractor F
will be as close as an explicitly fine-tuned
surrogate golden extractor;
•optimizing PLMs via an extraction loss, so
that the in-context NER can effectively locate
and categorize entities in a text.
The details are described in the following.
4.1 Pre-training Settings
Pre-training Corpus Construction To continu-
ally pre-train PLMs for in-context NER, we first
collect an in-context pre-training NER corpusD ={x, x, ..., x}, where each xis an
in-context NER task represented as a tuple = (in-
struction, demonstrations, text, entities).
Specifically, to sample in-context NER task x,
we use traditional NER corpus Dwhere each
NER instance is a (text, entities) pair as follows:
1.In-context Task Sampling : To construct an
in-context NER task x = (instruction, demon-
strations, text, entities): (1) we first sample N
target entity types from Dto form instruc-
tion and sample Kinstances for each type to
form demonstrations; (2) then we sample the
text and the entities of x by either randomly
sample an instance from Ntarget entity types,
or randomly sample from instances of other en-
tity types, i.e., their extractions are NIL. We
sample NIL instances because in real-world ap-
plications many instances will not contain target
entities, and NIL instances are sampled with a
predefined proportion γ.
2.Type Anonymization : To ensure the models
rely on in-context demonstrations for entity
knowledge and avoid overfitting to entity type
names, we anonymize entity types by randomly
substituting them with a set of type indicators
{<type1>, . . ., <type99>}, rather than directly
using the original type names such as Disease
and Virus . We found this anonymization strat-
egy can significantly improve the in-context
learning ability of PLMs. Specifically, we ran-
domly substitute each entity type name with
pre-defined 99 type indicators {<type1>, . . .,
<type99>}, and the substitute probability for
each name is 80%.
Pre-training Loss Based on the in-context pre-
training corpus D , we pre-train our in-
context NER model by optimizing the loss:
L=αL +L (1)
whereL is the meta-function loss which
ensures PLMs can implicitly generate accurate en-
tity extractors (Section 4.2), L is the extrac-
tion loss which ensures PLMs have good extraction
ability (Section 4.3), αis the coefficient of meta-
function loss.
4.2 Meta-function Pre-training
As mentioned above, a good in-context NER model
should be able to implicitly construct an accurate
entity extractor by partially applying PLMs with13664
instruction Iand demonstrations D:
(λ.M)(I, D) =F (2)
For example, given the instruction and demonstra-
tions in Figure 2, we want PLMs to implicitly build
an accurate extractor for Disease andVirus . There-
fore if we know the golden extraction function F
for target entity types, we can optimize PLMs for
in-context NER ability by minimizing the distance
||F− F||.
Unfortunately, the golden extraction function
Fis unknown. In this paper, we approximate
Fusing a surrogate extractor which is the fine-
tuned counterpart using demonstrations D. That is,
for each in-context pre-training task x, we first re-
cover all NER (text, entities) instances from xasx,
then we fine-tune the model and use the fine-tuned
encoder Fas the surrogate of F. The overall
meta-function pre-training is shown in Figure 3.
Formally, given instruction I, demonstration D,
and text T, we first feed them into the encoder and
obtain the feature of IandT,
Then we obtain the feature of the implicitly gen-
erated function Fusing the features of instruc-
tionIand text T, and ignore the features of D:
F= [l, ...,l,t, ...,t]. In Figure 3, the feature
Fcan be seen as the output of Disease andVirus
extractor F.
To obtain the feature of the fine-tuned counter-
part, we perform a one-step gradient descentonthe encoder using the instances in the demonstra-
tionDand get the surrogate encoder, which can
be seen as an approximation of golden F. Note
that this fine-tuning operation is performed after
the model has been copied, so there is no impact on
the parameters of the original model. In the exam-
ple in Figure 3, Encoderis aDisease andVirus
extractor. After performing one-step updating, we
feed instruction and text [I;T]into the surrogate
encoder to get their features:
F=Encoder(I;T) (4)
where F={l, . . . ,l,t, . . . ,t}is features of
instruction Iand text T. In the example in Figure 3,
the feature Fcan be seen as the estimated output
of golden extractor FforVirus andDisease entity
types.
Then, we pre-train our in-context NER model to
be a good meta-function by making the output of
FandFconsistent, i.e., minimizing the distance
between FandF. The meta-function loss is:
L =1
n+k/summationdisplayd(F,F) (5)
where d(·)is euclidean distance. Note that when
calculating the gradient of L ,Fis seen
as constant. To this end, the meta-function gradient
can be estimated as:
∇θ =∂L
∂X(6)
where θ is the parameters of the encoder and
X= [I;D;T]is the input. The estimated gradi-
ent will be used to update the parameters of the
encoder.13665In this way, the in-context NER models will be
trained to be a good meta-function (Akyürek et al.,
2022), which can also be seen as an ability for
implicit fine-tuning (Dai et al., 2022; von Oswald
et al., 2022).
4.3 Extraction Function Pre-training
Besides the in-context learning ability, we also pre-
train PLMs to be good extractors via extraction loss.
Given instruction I, demonstrations D, and text T,
the sequence-to-sequence entity extractor directly
models the generation probability token by token
in an auto-regressive way. Formally, we optimize
the model parameters θby minimizing the negative
likelihood of in-context instances:
L =−log/productdisplayP(y|y, X, θ )(7)
And the extraction gradient is computed as:
∇θ=∂L
∂X(8)
To learn the above extraction ability, we design
two extraction pre-training tasks, including an en-
tity extraction task and a pseudo extraction lan-
guage modeling task:
Entity Extraction Task. This task is used to train
the ability to extract entities from text, we use both
in-context NER settings whose input is (instruc-
tion, demonstrations, text) and traditional NER set-
tings whose input is (instruction, text), and output
is entities. Note that type anonymization is only
conducted in in-context NER setting.
Pseudo Extraction Language Modeling Task .
Because there is a mismatch between the entity ex-
traction task and the original language modeling
task, and the size of the NER corpus is usually far
smaller than the text corpus for language modeling
pre-training, we design a pseudo extraction LM
task to bridge the above gap. Specifically, we ran-
domly sample unlabeled sentences from the text
corpus and automatically build pseudo extraction
(instruction, demonstrations, text, pseudo entities)
tasks. For instance, given a demonstration sentence
such as “ I think this movie is cool and I really
like it very much ” and a text “ I do not like it. ”:
(1) To begin with, we choose some spans from
demonstrations (such as "this movie" and "like")
and designate them as pseudo entities. We assignrandom types to these entities from type indicators.
For instance, we consider "this movie" as a pseudo
entity of type <type2> and "like" as a pseudo entity
of type <type14>. (2) The input of the pseudo ex-
traction task is instruction="Target types:<type2>;
<type14>"; the demonstrations="Text: [MASK1]
is cool and I really [MASK2] it [MASK3] . Enti-
ties: [MASK1] is <type2>. [MASK2] is <type14> "
where the entities (“this movie” and “like”) and
other random spans (“very much”) in demonstra-
tions are masked. The text=“Text: I do not like
it.” which is not masked. (3) The output of
the pseudo extraction task is “ like is <type14> ”
since the model will learn from demonstrations
that <type14> corresponds to "like". (4) We also
conduct traditional NER settings whose input is
(instruction, text). The entities in the text will be
masked as in demonstrations, e.g. “Target types:
this movie; like Text: I [MASK1] not [MASK2] it. ”.
The output will be “Entities: [MASK2] is like. ”.
We can see that the pseudo extraction LM task
can benefit in-context NER in two ways. Firstly,
it can significantly increase the size and diversity
of in-context NER pre-training tasks from a large-
scale unlabeled corpus. Secondly, this task pre-
trains PLMs with a mixture of extraction target
and span prediction task, therefore avoiding PLMs
overfit to only extraction task.
When pre-training, We transformed the NER and
language model tasks into a uniform format and
sampled input instances alternately.
5 Experiments
This section evaluates our method by conducting
experiments on few-shot NER settings.
5.1 Experimental Settings
Pre-training settings. Following Chen et al.
(2022a), we build a large-scale distant NER dataset
by aligning Wikipedia and Wikidata. Specifically,
our dataset was made from Wikipedia text with hy-
perlinks to Wikidata, where we labeled entity types
using the linked Wikidata item’s attributes. En-
tity types were gathered from Wikidata’s Subclas-
sOf and InstanceOf attributes for each span. We
filtered ambiguous and low-frequency types (oc-
currences <100k) to obtain higher-quality demon-
strations. Finally, we retained 2046 types and 55
million (text, entities) pairs and use a 40/15 million
split for training/validation. We sample 5 million
in-context tasks for training and 10k for valida-13666
tion, where each instance with type number Nis
10 and instance number Kis 10. We employ the
T5-v1.1-large (Raffel et al., 2020) model as the
initial model for MetaNER and further pre-train
500k steps with learning rate=5e-5 and warm-up
steps=10k. In this paper, we refer to the pre-trained
model as MetaNER .
Few-shot settings. Our experiments follow the
standard k-shot NER setting Huang et al. (2021):
For each entity type, we sample ktraining instances
as in-context demonstrations. We evaluate models
by micro-F1 and report the average performance
by repeating each experiment 10 times.
We conducts experiments on 4 datasets across
differnt domains: (1) CoNLL03 (Sang and Meul-
der, 2003) from news domain. (2) WNUT17 (Der-
czynski et al., 2017) from social media domain. (3)
NCBI-disease (Do ˘gan et al., 2014) from biology
domain. (4) SEC-filings (Alvarado et al., 2015)
from finance domain.
Baselines. For fair comparison, we use frozen
models for all baselines in the in-context learn-
ing experiments, i.e., a pre-trained language/NER
model is used for entity extraction without fine-
tuning. In addition, we will discuss fine-tuning
based methods in section 5.3.3. Two kinds of base-
lines are compared:1)Pre-trained language models include models
with different scales and architectures: (1) Encoder-
decoder models – T5 models (Raffel et al., 2020),
includes T5-v1.1-large (770M), T5-xl (3B) and T5-
xxl (11B). (2) Causal LM models – GPT and OPT
models (Radford et al., 2019; Zhang et al., 2022b),
includes GPT2-xl (1.5B), GPT-j-6B (Wang and Ko-
matsuzaki, 2021), GPT-Neox-20B (Black et al.,
2022), OPT-13B, OPT-30B and OPT-66B. Notice
that, for PLMs, we use original type names rather
than type indicators to capture the label semantics.
For encoder-decoder models like T5, we formulate
in-context NER as a span corruption task and the
model will generate the extraction task. For exam-
ple, for input “Target entity types: disease. Text:
COVID-19 is spreading. Entities: COVID-19 is
disease. Text: HIV is spread by three main routes.
Entities: <extra_id_0>”, the span corruption task
requires the decoder to generate the extraction re-
sult “<extra_id_0> HIV is disease.”.
2)Pre-trained NER models are metric-based
few-shot methods, includes prototype network (Pro-
toNet) (Snell et al., 2017), NNshot (Yang and Kati-
yar, 2020), StructShot (Yang and Katiyar, 2020)
and CONTAINER (Das et al., 2022). We em-
ployed BERT-Large (Devlin et al., 2019) as the
backbone and pre-trained them using the same
dataset as MetaNER. For a fair comparison, we13667also pre-train a 220M T5-v1.1-base (Raffel et al.,
2020) model with our meta-function pre-training
algorithm (MetaNER-base).
5.2 Main Results
The experimental results are shown in Table 1. We
can see that:
1)Few-shot NER is challenging even for large
language models, while MetaNER can achieve
good in-context NER performance. Compare
with best-performed PLMs, MetaNER achieves
8.4% F1 improvements. Moreover, due to the gap
between language model task and NER task, large
language models achieve poor in-context learning
performance on some datasets.
2)Our in-context NER method can achieve
robust performance, even under a large source-
target domain gap. Compared with best-
performed metric-based NER models, MetaNER-
base and MetaNER achieves 26.8% and 40.7% F1
improvement. Specifically, the performance im-
provement is more significant when source-target
domain gap is larger, i.e., the NCBI-disease (biol-
ogy domain) and SEC-filings (finance domain).
3)Meta-function pre-training can effectively
inject in-context learning ability into both
small and large PLMs. Both MetaNER-base
and MetaNER achieve impressive performance
in 1-shot and 5-shot settings, which verified that
MetaNER can effectively inject in-context NER
ability into small PLMs, although currently in-
context learning has been seen an ability only
emerged only on large language models such as
GPT-3.
5.3 Detailed Analysis
5.3.1 Ablation Studies
To analyze and understand the effect of type
anonymization, meta-function pre-training, entity
extraction pre-training, and pseudo extraction LM
pre-training, we conduct the following ablation
experiments: (1) MetaNER w/o MF: remove the
meta-function pre-training; (2) MetaNER w/o LM:
remove pseudo extraction LM pre-training; (3)
MetaNER w/o anonymization: we use the origi-
nal entity type names in both pre-training and in-
context NER, without using type anonymization.
The results are shown in Table 2, we can see that:
1)meta-function pre-training is critical for
in-context learning ability. By removing the
meta-function pre-training, the results drop sig-
nificantly when the domain gaps are larger, i.e.,
NCBI-disease. At the same time, meta-function
pre-training is helpful for the model to make more
precise predictions.
2)The pseudo extraction LM task signif-
icantly benefits in-context NER. We found
MetaNER w/o LM results in a performance drop
than MetaNER. We believe this is because, al-
though using an automatically constructed pseudo
dataset, this task can significantly improve the size
and the diversity of in-context NER tasks, mean-
while can retain a good language modeling ability.
3)Type name anonymization prevents in-
context NER model from type name overfitting,
and therefore enhances the in-context learning
ability. The ablation of type name anonymization
results a 5.7% performance drop in Table 2. We
believe this is because type names will let mod-
els tend to memorize entity knowledge using type
names, thus the model will not learn to capture
entity knowledge from demonstrations on-the-fly.
5.3.2 Effects of Meta-function Pre-training
One main idea of this paper is that in-context NER
model can be viewed as a meta-function which can13668implicitly build new entity extractors. To demon-
strate whether meta-function pre-training can train
a good meta-function, we sample 1000 instances
from each dataset, and show the difference between
the (instruction, demonstrations)-initialized entity
extractor Fand the surrogate entity extractor F,
i.e., ||F− F || in Section 4.2 in Figure 4. We
can see that meta-function pre-training can equip
PLMs with a good meta-function ability, i.e., the
(instruction, demonstrations)-initialized entity ex-
tractor after pre-training is significantly close to its
fine-tuned counterpart.
5.3.3 In-context Learning vs Fine-tuning
MetaNER can also be directly fine-tuned using tra-
ditional NER instances. We employed the identical
fine-tuning approach as previous works (Huang
et al., 2021; Lu et al., 2022b; Chen et al., 2022a).
Following Lu et al. (2022b), we also implemented
theRejection Mechanism when fine-tuning the T5-
v11-large and MetaNER to achieve better few-shot
performance.
To compare in-context NER with fined-tuned
NER, Table 3 reports the performance of the fine-
tuned counterpart of MetaNER – MetaNER-FT(its
training is similar to surrogate entity extractor but
with multi-step gradient descent until coverage), to-
gether with several fine-tuned few-shot NER base-
lines. We can see that: 1) MetaNER is an effec-
tive architecture, which achieves good performance
on both in-context learning and fine-tuning set-
tings; 2) Currently, fine-tuning can achieve better
performance than their in-context learning coun-
terpart. We believe this is because fine-tuned
models’ parameters need to be specialized to spe-
cific entity types, meanwhile in-context learning
needs to generalize to different types on-the-fly,
i.e., generalization-specialization trade-off. We be-
lieve this also verified the reasonableness of usinga fine-tuned surrogate extractor to approximate the
golden extractor.
6 Conclusion
In this paper, we propose an in-context learning-
based NER approach and model PLMs as a meta-
function, which can inject in-context NER ability
into PLMs and recognize entities of new types on-
the-fly using only a few demonstrative instances.
Experimental results show that our method is ef-
fective for in-context NER. For future work, we
will extend our method to different NLP tasks like
event extraction and relation extraction.
Limitations
In-context learning is an useful ability, this paper
only focuses on in-context named entity recogni-
tion, leaves the learning of other NLP tasks’ in-
context learning abilities for future work.
Currently, we learn in-context learning via meta-
function pre-training, by comparing an in-context
extraction function and a fined-tuned surrogate ex-
traction function at the representation level of their
encoders. There are two approximation here: one
is fined-tuned surrogate extraction function for ap-
proximating golden extraction function, and the
difference between representations for approximat-
ing the divergence between functions. We think the
above two approximations can be further improved
for better and faster in-context learning.
Acknowledgements
We sincerely thank the reviewers for their insight-
ful comments and valuable suggestions. This re-
search work is supported by the CAS Project for
Young Scientists in Basic Research under Grant
No.YSBR-040 and the National Natural Science
Foundation of China under Grants no. 62122077,
62106251.
References136691367013671
A Experiment Details
A.1 Datasets for the extraction language
model task
Rather than randomly generating spans to form
target labels in instruction, we use informative
spans (Bian et al., 2021) as target labels. Unlike in-
formative span selection at passage level for MRC
(Bian et al., 2021), we select informative spans
at a cross-document level. Specifically, we take
10 Wikipedia documents as a set and select infor-
mative spans according to the following rules: (1)
spans that have appeared simultaneously in at least
two and at most five documents. (2) spans that
have appeared in only one document but have ap-
peared in more than two. Rule (1) avoids some13672low-information general spans, such as stop words,
and rule (2) retains some important spans in each
document. Note that we consider at most 4-gram
as a span and select the target labels from the infor-
mative spans during pre-training.
A.2 Cost of pre-training
We used one A-100 80g GPU for pre-training the
base/large model, which took approximately one
to three days. The total FLOPs for the base model
are 2.30e+18 and for the large model are 7.64e+18.13673ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
In the 7-th Section
/squareA2. Did you discuss any potential risks of your work?
The data used for pre-training is based on publicly and widely used wikidata and wikipedia.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
In abstract and the ﬁrst section
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Left blank.
/squareB1. Did you cite the creators of artifacts you used?
No response.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
No response.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
No response.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
No response.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
No response.
C/squareDid you run computational experiments?
In Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
In Section 513674/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyper-
parameter values?
Not applicable. We conduct experiments in few-shot settings, which is unable to conduct hyperpa-
rameters and we use the hyperparameters as pervious works.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
In Section 5
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
In Section 5
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.13675