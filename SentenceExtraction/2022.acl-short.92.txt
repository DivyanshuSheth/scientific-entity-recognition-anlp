
Lütﬁ Kerem Senel, Timo Schick andHinrich Schütze
Center for Information and Language Processing (CIS), LMU Munich, Germany
lksenel@gmail.com ,schickt@cis.lmu.de
Abstract
Pretrained language models (PLMs) have
achieved superhuman performance on many
benchmarks, creating a need for harder
tasks. We introduce CoDA21 (Context Def-
inition Alignment), a challenging benchmark
that measures natural language understanding
(NLU) capabilities of PLMs: Given a deﬁni-
tion and a context each for kwords, but not the
words themselves, the task is to align the kdef-
initions with the kcontexts. CoDA21 requires
a deep understanding of contexts and deﬁni-
tions, including complex inference and world
knowledge. We ﬁnd that there is a large gap
between human and PLM performance, sug-
gesting that CoDA21 measures an aspect of
NLU that is not sufﬁciently covered in exist-
ing benchmarks.
1 Introduction
Increasing computational power along with the de-
sign and development of large and sophisticated
models that can take advantage of enormous cor-
pora has drastically advanced NLP. For many tasks,
ﬁnetuning pretrained transformer-based language
models (Vaswani et al., 2017; Devlin et al., 2019;
Radford et al., 2018) has improved the state of
the art considerably. Language models acquire
knowledge during pretraining that is utilized dur-
ing task-speciﬁc ﬁnetuning. On benchmarks that
were introduced to encourage development of mod-
els that do well on a diverse set of NLU tasks
(e.g., GLUE (Wang et al., 2018) and SuperGLUE
(Wang et al., 2019)), these models now achieve
superhuman performance (He et al., 2020). The
pretrain-then-ﬁnetune approach usually requires a
great amount of labeled data, which is often not
available or expensive to obtain, and results in spe-
cialized models that can perform well only on a
single task. Recently, it was shown that genera-
tive language models can be applied to many tasks
Figure 1: The CoDA21 task is to ﬁnd the correct align-
ment between contexts and deﬁnitions: C1-D4,C2-
D1,C3-D2,C4-D3. The target words ( C1:“dust”,
C2:“soil”, C3:“marble”, C4:“feathers”; not provided
to the model) are replaced with a placeholder <xxx> .
without ﬁnetuning when the task is formulated as
text generation and the PLM is queried with a natu-
ral language prompt (Radford et al., 2019; Brown
et al., 2020).
Motivated by recent progress in zero-shot learn-
ing with generative models as well as the need for
more challenging benchmarks that test language
understanding of language models, we introduce
CoDA21 ( Context Deﬁnition Alignment), a difﬁ-
cult benchmark that measures NLU capabilities of
PLMs for the English language. Given a deﬁni-
tion and a context each for kwords, but not the
words themselves, the task is to align the kdef-
initions with the kcontexts. In other words, for
each deﬁnition, the context in which the deﬁned
word is most likely to occur has to be identiﬁed.
This requires (i) understanding the deﬁnitions, (ii)
understanding the contexts, and (iii) the ability to
match the two. Since the target words are not given,
a model must be able to distinguish subtle meaning
differences between different contexts/deﬁnitions
to be successful. To illustrate the difﬁculty of the
task, Figure 1 shows a partial example for k= 4
(see Table 5 in the supplementary for the full ex-815ample). We see that both complex inference (e.g.,
<XXX> can give rise to a cloud by being kicked up
)<XXX> must be dry)<XXX> can be dust, but
not soil) and world knowledge (what materials are
typical for monuments?) are required for CoDA21.
We formulate the alignment task as a text pre-
diction task and evaluate, without ﬁnetuning, three
PLMs on CoDA21: BERT (Devlin et al., 2019),
RoBERTa (Liu et al., 2019) and GPT-2 (Radford
et al., 2019). Poor performance of the PLMs and a
large gap between human and PLM performance
suggest that CoDA21 is an important benchmark
for designing models with better NLU capabilities.
2 CoDA21
2.1 Dataset
We construct CoDA21 by ﬁrst deriving a set Gof
synset groupsfG;G;:::gfrom Wordnet (Miller,
1995). A synset group Gis a group of synsets
whose meanings are close enough to be difﬁcult to
distinguish (making the task hard), but not so close
that they become indistinguishable for human and
machine. In a second step, each synset group G
is converted into a CoDA21 group G– a set of
triples, each consisting of the synset, its deﬁnition,
and a corpus context. A CoDA21 group can be
directly used for one instance of the CoDA21 task.
Synset groups. Each synset group Gconsists
of5k10synsets. To create a synset group,
we start with a parent synset ^sand construct a co-
hyponym group G(^s)of its children:
G(^s) =fsjs<^s;s =2Dg
where<is the hyponymy relation between synsets
andDis the set of synsets that have already been
added to a synset group. The intuition for grouping
synsets with a common parent is that words sharing
a hypernym are difﬁcult to distinguish (as opposed
to randomly selected words).
We iterate ^sthrough all nouns and verbs in Word-
Net. At each iteration, we get all hyponyms of ^s
that have not been previously added to a synset
group; not reusing a synset ensures that different
CoDA21 subtasks are not related and so no such
relationships can be exploited.
We extract synset groups from co-hyponym
groups by splitting them into multiple chunks of
sizek. In an initial exploration, we found that
the task is hard to solve for human subjects if
two closely related hyponyms are included, e.g.,“clementine” and “tangerine”. We therefore em-
ploy clustering to assemble a set of mutually dis-
similar hyponyms. We ﬁrst compute a sentence
embedding for each hyponym deﬁnition using the
stsb-distilbert-base Sentence Transformer model.
We then cluster the embeddings using complete-
link clustering, combining the two most dissimilar
clusters in each step. We stop merging before the
biggest cluster exceeds the maximum group size
(k= 10 ) or before the similarity between the last
two combined clusters exceeds the maximum simi-
larity (= 0:8). The largest cluster Gis added to
the setGof synset groups. We then iterate the steps
of (i) removing the synsets in the previous largest
clusterGfrom G(^s)and (ii) running complete-link
clustering and adding the resulting largest cluster
GtoGuntil fewer than ﬁve synsets remain in G(^s)
or no cluster can be formed whose members have
a similarity of less than .
CoDA21 groups. For each synset s, we extract
its deﬁnition d(s)from WordNet and a context c(s)
in which it occurs from SemCor(Miller et al.,
1994). SemCor is an English corpus tagged with
WordNet senses. Let C(s)be the set of contexts
ofsin SemCor. IfjC(s)j>1, we use as c(s)
the context in which bert-base-uncased predicts s
with the highest log probability when it is masked,
where sis the word tagged with the sense s– this
favors contexts that are speciﬁc to the meaning of
the synset. Finally, we convert each synset group
GinGto a CoDA21 group G:
G=f(s;d(s);c(s))js2Gg
That is, a CoDA21 group Gis a set of triples of
sense, deﬁnition and context. In PLM evaluation,
each CoDA21 group Ggives rise to one context-
deﬁnition alignment subtask.
We name the resulting dataset CoDA21-noisy-
hard:noisy because ifjC(s)jis small, the selected
context may not be informative enough to identify
the matching deﬁnition; hard because the synsets in
a CoDA21 group are taxonomic sisters, generally
with similar meanings despite the clustering-based
limit on deﬁnition similarity. We construct a clean
version of the dataset by only using synsets with
jC(s)j5. We also construct an easy version by816
taking the “hyponym grandchildren” sof a parent
synset ^s(s < l^l <^s) instead of its hyponym
children. This reduces the similarity of synsets in
a CoDA21 group, making the task easier. Table 1
gives dataset statistics.
2.2 Alignment
Recall the CoDA21 task: given a deﬁnition and a
context each for kwords (but not the words them-
selves), align the kdeﬁnitions with the kcontexts.
That is, we are looking for a bijective function (a
one-to-one correspondence) between deﬁnitions
and contexts. Our motivation in designing the task
is that we want a hard task (which can guide us in
developing stronger natural language understand-
ing models), but also a task that is solvable by
humans. Our experience is that humans can at
least partially solve the task by ﬁnding a few initial
“easy” context-deﬁnition matches, removing them
from the deﬁnition/context sets and then match the
smaller remaining number of deﬁnitions/contexts.
The number of context-deﬁnition pairs scales
quadratically ( O(k)) withkand the number of
alignments factorially ( O(k!)). We restrict kto
k10to make sure that we do not run into com-
putational problems and that humans do not ﬁnd
the task too difﬁcult.
In order to connect contexts to deﬁnitions with-
out using the target words, we replace the target
words by a made-up word. This setup resembles
the incidental vocabulary acquisition process in hu-
mans. Lettbe a target word, ca context in which
toccurs andma made-up word. To test PLMs on
CoDA21, we use the following pattern:
Q(c;m) =cDeﬁnition of mis
whereciscwith occurrences of treplaced bym.
We calculate the match score of a context-
deﬁnition pair (c;d)aslogP(djQ(c;m)), i.e.,log generation probability of the deﬁnition dcon-
ditioned onQ(c;m). Our objective is to maximize
the sum of the kmatch scores in an alignment. We
ﬁnd the best alignment by exhaustive search. Accu-
racy for a CoDA21 group Gis then the accuracy
of its best alignment, i.e., the number of contexts
inGthat are aligned with the correct deﬁnition,
divided by the total number of contexts jGj.
2.3 Baselines
We calculate P(djQ(c;m))for a masked lan-
guage model (MLM) Mand an autoregressive lan-
guage model (ALM) Aas follows:
P(djQ) =QP(djQ;d)
P(djQ) =QP(djQ;d;:::;d)
whereQ=Q(c;m),dis theiword in deﬁnition
danddis the deﬁnition with the iword masked.
We evaluate the MLMs BERT and RoBERTa
and the ALM GPT-2. We experiment with both
base and large versions of BERT and RoBERTa
and with all four sizes of GPT-2 (small, medium,
large, xl), for a total of eight models, to investigate
the effect of model size on performance.
The made-up word mshould ideally be unknown
so that it does not bias the PLM in any way. How-
ever, there are no truly unknown words for the
models we investigate due to the word-piece to-
kenization they apply to the input. Any made-up
word that is completely meaningless to humans will
have a representation in the models’ input space
based on its tokenization. To minimize the risk
that the meaning of the made-up word may bias
the model, we use m=bkatuhla , a word with
an empty search result on Google that most likely
never appeared in the models’ pretraining corpora.
In addition to PLMs, we also evaluate 2 re-
cent sentence transformer models(Reimers and
Gurevych, 2019), paraphrase-mpnet-base-v2 (mp-
net) and paraphrase-MiniLM-L6-v2 (MiniLM),
and fastText static embeddings(Mikolov et al.,
2018). To calculate the match score of a context-
deﬁnition pair, we ﬁrst remove the target word from
the context and represent contexts and deﬁnitions
as vectors. For sentence transformers, we obtain
these vectors by simply encoding the input sen-
tences. For fastText, we average the vectors of the817words in contexts and deﬁnitions. We then cal-
culate the match score as the cosine similarity of
context and deﬁnition vectors.
3 Results
Table 2 presents average accuracy of the investi-
gated models on the four CoDA21 datasets. As
can be seen, fastText performs only slightly bet-
ter than random. MLMs also perform better than
random chance by only a small margin. This poor
performance can be partly explained by the gener-
ation style setup we use, which is not well suited
for masked language models. Even the smallest
GPT-2 model performs considerably better than
RoBERTa-large, the best performing MLM. Perfor-
mance generally improves with model size. GPT-
2achieves the best results among the LMs on
almost all datasets. Interestingly, sentence trans-
former all-mpnet-base-v2 performs comparably
to GPT-2on most datasets despite its simple,
similarity based matching compared to generation
based matching of GPT-2 models. Based on this
observation it can be argued that current state-of-
the-art language models fail to perform complex,
multi-step reasoning and inference which are nec-
essary to solve the CoDA21 tasks. Overall, MLMs
perform slightly better on verbs than nouns while
the converse is true for GPT-2. As expected, all
models perform better on the easy datasets. Perfor-
mance on noisy andclean datasets are comparable;
this indicates that our contexts are of high quality
even for the synsets with only a few contexts.
Human performance on CoDA21. We asked
two NLP PhD studentsto solve the task on S20,
a random sample of size 20 from the noun part of
CoDA21- clean-easy . Table 2 shows results on S20
for these two subjects and our models. Human per-
formance is 0.86 – compared to 0.48 for GPT-2,
the best performing model. This difference indi-
cates that there is a large gap in NLU competence
between current language models and humans and
that CoDA21 is a good benchmark to track progress
on closing that gap.
To investigate the effect of the made-up word
m, we experiment with several other words on the
noun part of CoDA21- clean-easy . Speciﬁcally, we
investigate another nonce word “opyatzel”, a single
letter “x” and two frequent words “orange” and
“cloud”. Table 3 shows the results of the mod-
els for different made-up words. MLMs do not
show signiﬁcant variability in performance, and
perform comparably poor for all words tried. GPT2
versions, which perform considerably better than
MLMs on CoDA21, perform similarly for the two
nonce words and single letter “x”, which do not
have a strong meaning. Their performance drops
signiﬁcantly when the two frequent words are used
as the made-up word, due to the effect of prior
knowledge models have about these words.
To investigate the effect of the pattern , we com-
pared our pattern Q(c;m)with two alternative pat-
terns by evaluating GPT-2on the noun part of
CoDA21- clean-easy . Patterns and the evaluation
results are shown in Table 4. The results suggest
that the effect of the pattern on performance is min-
imal.
Effect of the alignment setup. We constructed
CoDA21 as an alignment dataset which uses the
fact that matching between the deﬁnitions and con-
texts is one-to-one. This setup makes the task818
more intuitive and manageable for humans. How-
ever, context-deﬁnition match scores can be used
to evaluate models on CoDA21 samples also with-
out the alignment setup by simply picking context-
deﬁnition pairs with the highest match score for
each deﬁnition. We additionally evaluated GPT-
2model on CoDA21- clean-easy dataset using
this simple matching approach which yielded 0.38
average accuracy compared to the 0.49 accuracy
achieved with the alignment setup. This result sug-
gests that language models can also make use of
the alignment style evaluation, similar to humans.
Table 5 (in the Appendix) presents a sample
of size 7 from the noun part of the CoDA21-
clean-easy dataset. Figure 2 displays all 49 match
scores of the context-deﬁnition pairs for this sam-
ple obtained using GPT-2. 5 of the 7 deﬁnitions
(2,3,4,5,7) are matched with correct contexts with
the alignment setup while 4 deﬁnitions (4,5,6,7) are
matched correctly for the simple matching setup.
Alignment setup enabled the model to match sec-
ond and third deﬁnitions with their corresponding
contexts even though their match scores are not the
highest ones.
To get a better sense of why the task is hard
for PLMs, we give an example, from the CoDA21
subtask in Figure 1 (also Figure 2 and Table 5
refer to the same subtask), of a context-deﬁnition
match that is scored highly by GPT-2, but is not
correct. Context: “these bees love a ﬁne-grained
<XXX> that is moist”. Deﬁnition: “ﬁne powdery
material such as dry earth or pollen”. (context 6 and
deﬁnition 1 in Figure 2) GPT-2most likely gives
a high score because it has learned that bees and
pollen are associated. It does not understand that
the mutual exclusivity of “moist” and “powdery”
makes this a bad match.
4 Related Work
There are many datasets (Levesque et al., 2012; Ra-
jpurkar et al., 2016; Williams et al., 2018) for eval-
uating language understanding of models. Many
adopt a text prediction setup: Lambada (Paperno
et al., 2016) evaluates the understanding of dis-
course context, StoryCloze (Mostafazadeh et al.,
2016) evaluates commonsense knowledge and so
does HellaSwag (Zellers et al., 2019), but exam-
ples were adversarially mined. LAMA (Petroni
et al., 2019) tests the factual knowledge con-
tained in PLMs. In contrast to this prior work,
CoDA21 goes beyond prediction by requiring the
matching of pieces of text. WIC (Pilehvar and
Camacho-Collados, 2019) is also based on match-
ing, but CoDA21 is more complex (multiple con-
texts/deﬁnitions as opposed to a single binary
match decision) and is not restricted to ambigu-
ous words. WNLaMPro (Schick and Schütze,
2020) evaluates knowledge of subordinate rela-
tionships between words, and WDLaMPro (Senel
and Schütze, 2021) understanding of words using
dictionary deﬁnitions. Again, matching multiple
pieces of text with each other is much harder and
therefore a promising task for benchmarking NLU.
5 Conclusion
We introduced CoDA21, a new challenging bench-
mark that tests natural language understanding ca-
pabilities of PLMs. Performing well on CoDA21
requires detailed understanding of contexts, per-
forming complex inference and having world
knowledge, which are crucial skills for NLP. All
models we investigated perform clearly worse than
humans, indicating a lack of these skills in the cur-
rent state of the art in NLP. CoDA21 therefore is a
promising benchmark for guiding the development
of models with stronger NLU competence.819References820821A Appendices
A.1 CoDA21 group examples822Hidden word Context
dust 1. He came spurring and whooping down the road , his horse kicking up clouds of
<XXX> , shouting :
marble 2. Pels also sent a check for $ 100 to Russell ’s widow and had a white <XXX>
monument erected on his grave .
wastewater 3. The high cost of land and a few operational problems resulting from excessive
loadings have created the need for a <XXX> treatment system with the operational
characteristics of the oxidation pond but with the ability to treat more organic matter
per unit volume .
feathers 4. It was a ﬁne broody hen , white , with a maternal eye and a striking abundance of
<XXX> in the under region of the abdomen .
fraction 5. It was then distilled at least three times from a trap at - 78 ‘ to a liquid air trap with
only a small middle <XXX> being retained in each distillation .
soil 6. The thing is that these bees love a ﬁne-grained <XXX> that is moist ; yet the water
in the ground should not be stagnant either .
cards 7. And the coffee shop on Drexel Street , where the men spent their evenings and
Sundays playing <XXX> , had a rose hedge beneath its window .
Synset Deﬁnition
dust.n.01 1. ﬁne powdery material such as dry earth or pollen that can be blown about in the air
marble.n.01 2. a hard crystalline metamorphic rock that takes a high polish; used for sculpture and
as building material
efﬂuent.n.01 3. water mixed with waste matter
feather.n.01 4. the light horny waterproof structure forming the external covering of birds
fraction.n.01 5. a component of a mixture that has been separated by a fractional process
soil.n.02 6. the part of the earth’s surface consisting of humus and disintegrated rock
card.n.01 7. one of a set of small pieces of stiff paper marked in various ways and used for
playing games or for telling fortunes823Hidden word Context
suggestion 1. This was Madden ’s <XXX> ; the police chief shook his head over it .
concept 2. The <XXX> of apparent black-body temperature is used to describe the radiation
received from the moon and the planets .
ideals 3. Religion can summate , epitomize , relate , and conserve all the highest <XXX>
and values - ethical , aesthetic , and religious - of man formed in his culture .
reaction 4. That much of what he calls folklore is the result of beliefs carefully sown among
the people with the conscious aim of producing a desired mass emotional <XXX>
to a particular situation or set of situations is irrelevant .
feeling 5. He had an uneasy <XXX> about it .
programs 6. The Federal program of vocational education merely provides ﬁnancial aid to
encourage the establishment of vocational education <XXX> in public schools .
meaning 7. Indeﬁnite reference also carries double <XXX> where an allusion to one person
or thing seems to refer to another .
theme 8. Almost nothing is said of Charles ’ spectacular victories , the central <XXX>
being the heroic loyalty of the Swedish people to their idolized king in misfortune
and defeat .
Synset Deﬁnition
suggestion.n.01 1. an idea that is suggested
concept.n.01 2. an abstract or general idea inferred or derived from speciﬁc instances
ideal.n.01 3. the idea of something that is perfect; something that one hopes to attain
reaction.n.02 4. an idea evoked by some experience
impression.n.01 5. a vague idea in which some conﬁdence is placed
plan.n.01 6. a series of steps to be carried out or goals to be accomplished
meaning.n.02 7. the idea that is intended
theme.n.02 8. a unifying idea that is a recurrent element in literary or artistic work824