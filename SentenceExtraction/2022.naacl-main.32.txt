∗
Jiahao Xu, Yubin Ruan, Wei Bi, Guoping Huang,
Shuming Shi, Lihui Chen, Lemao LiuNanyang Technological University,Tencent AI Lab{jiahao004@e.,elhchen@}ntu.edu.sg{brantruan,victoriabi,donkeyhuang,shumingshi,redmondliu}
@tencent.com
Abstract
Back translation (BT) is one of the most sig-
nificant technologies in NMT research fields.
Existing attempts on BT share a common char-
acteristic: they employ either beam search or
random sampling to generate synthetic data
with a backward model but seldom work stud-
ies the role of synthetic data in the performance
of BT. This motivates us to ask a fundamen-
tal question: what kind of synthetic data con-
tributes to BT performance? Through both the-
oretical and empirical studies, we identify two
key factors on synthetic data controlling the
back-translation NMT performance, which are
quality and importance. Furthermore, based
on our findings, we propose a simple yet ef-
fective method to generate synthetic data to
better trade off both factors so as to yield a
better performance for BT. We run extensive
experiments on WMT14 DE-EN, EN-DE, and
RU-EN benchmark tasks. By employing our
proposed method to generate synthetic data, our
BT model significantly outperforms the stan-
dard BT baselines (i.e., beam and sampling
based methods for data generation), which
proves the effectiveness of our proposed meth-
ods.
1 Introduction
Since the birth of neural machine translation
(NMT) (Bahdanau et al., 2014; Sutskever et al.,
2014) back translation (BT) (Sennrich et al., 2016a)
has quickly become one of the most signifi-
cant technologies in natural language processing
(NLP) research field. This is because 1) it pro-
vides a simple yet effective approach to advance
the supervised NMT by leveraging monolingual
data (Edunov et al., 2018) and it also serves as a key
learning objective in unsupervised NMT (Artetxe
et al., 2018; Lample et al., 2018); 2) back transla-
tion even plays a significant role in other NLP re-search fields beyond translation such as paraphras-
ing (Mallinson et al., 2017) and style transfer (Prab-
humoye et al., 2018; Zhang et al., 2018).
Back translation consists of two steps, namely
synthetic corpus generation with a backward model
and parameter optimization for the forward model.
Various contributions have been made on im-
proving back translation, for instance, iterative
back translation (Hoang et al., 2018), tagged
back translation (Caswell et al., 2019), confidence
weighting (Wang et al., 2019), data diversifica-
tion (Nguyen et al., 2020). Although these efforts
differ in some aspects, all of them share a common
characteristic: they employ a default way to gen-
erate synthetic data in the first step of BT which
is either beam search or random sampling with a
backward model. Seldom work studies the conse-
quences of synthetic corpus to back translation and
hence it is unclear how synthetic data influences
the final performance of BT.
The early study empirically suggests the qual-
ity of the synthetic corpus is vital for BT perfor-
mance (Sennrich et al., 2016a). However, recent
studies illustrate better test performance can be
achieved by low quality synthetic corpus (Edunov
et al., 2018). This contradictory observation indi-
cates the quality of synthetic data is not the only
element that affects the BT performance. Hence,
this fact naturally raises a fundamental question:
what kind of synthetic data contributes to back
translation performance?
In this paper, we attempt to take a step for-
ward toward the above fundamental question. To
this end, we start from a critical objective in
semi-supervised learning, which is defined by the
marginal distribution of a target language. Then we
derive an approximate lower bound of the objective
function, which is closely related to the objective
of back translation. Corresponding to this lower
bound, we theoretically find two related elements
for maximizing such a lower bound: quality of syn-419thetic bilingual data and importance weight of its
source. Since both elements are mutually exclusive
to some extent, it may induce contradictory obser-
vation if one judges the BT performance according
to a single element. In addition, such a theoretical
explanation is supported by our empirical exper-
iments. Furthermore, based on our findings, we
propose a new heuristic approach to generate syn-
thetic data whose both elements are better balanced
so as to yield improvements over both sampling
and beam search based methods. Extensive ex-
periments on three WMT14 tasks show that our
BT consistently outperforms the standard sampling
and beam search based baselines by a significant
margin.
Our contributions are three folds:
1.We point out that importance weight and qual-
ity of synthetic candidates are two key factors
that affect the NMT performance.
2.We propose a simple yet effective method for
synthetic corpus generation, which could bet-
ter balance the quality and importance of syn-
thetic data.
3.Our experiments prove the effectiveness of
the aforementioned strategy, it outperforms
beam or sampling decoding methods on three
benchmark tasks.
2 Revisiting Back Translation
NMT builds a probabilistic model p(y|x;θ)with
neural networks parameterized by θ, which is used
to translate a sentence xin source language Xto
a sentence yin target language Y. The standard
wisdom to train the model is to minimize the fol-
lowing objective function over a given bilingual
corpus B={(x, y)}:
ℓ(B;θ) =/summationdisplaylogp(y|x;θ) (1)
Recently Sennrich et al. (2016a) propose a re-
markable method called Back Translation (BT) to
improve NMT by using a monolingual corpus M
in target language Ybesides Band back transla-
tion becomes one of the most successful techniques
in NMT (Fadaee and Monz, 2018; Edunov et al.,
2018). At a high level, back translation can be
considered as a semi-supervised method because
it leverages both labeled and unlabeled data. Sup-
posep(x|y;π)is the backward translation modelwhose parameter πis optimized over B, the key
idea of back translation can be summarized as the
following two steps:
•Synthetic Corpus Generation: It firstly
back-translates each target sentence y∈ M to
ˆxobtain a synthetic bilingual corpus {(ˆx, y)|
y∈ M} byp(x|y;π).
•Parameter Optimization: It combines both
authentic corpus Band the synthetic corpus
and then optimizes the parameter θby mini-
mizing the loss
ℓ(B;θ) +/summationdisplaylogp(y|ˆx;θ) (2)
To make BT more efficient, the standard configura-
tion is widely adopted: each sentence yis required
to generate a single source ˆxand both two steps
are performed for a single pass. We follow this
standard in this paper for generality but our idea
in this paper is straightforward to apply to other
configurations such as (Graça et al., 2019; Hoang
et al., 2018; Nguyen et al., 2020).
In the first step, there are two main strategies to
generate the synthetic corpus, i.e., deterministically
decoding and randomly sampling with p(x|y;π).
The first strategy aims to search the best candidate
as follows,
ˆx= arg max p(ˆx|y;π) (3)
The above optimization is achieved by the beam
search decoding, which can be regarded as a de-
generated shortest path problem with respect to the
logp(ˆx|y;π)with limited routing attempts. The al-
ternative strategy is random sampling: it randomly
samples a token with respect to the distribution
estimated by a back-translation model at each de-
coding step. Such a process can be modelled by,
ˆx=rand{p(ˆx|y;π)} (4)
Research Question Prior work points out (Sen-
nrich et al., 2016a) that the synthetic corpus with
high quality is beneficial to the final performance
of back translation. However, the recent studies
(Edunov et al., 2018) find that NMT models with
unsatisfactory BLEU score corpus, for instance, the
corpus generated by sampling based strategy, also
establish the state-of-the-art (SOTA) achievement
among back-translation NMT models.420This contradictory fact indicates that the quality
of synthetic corpus is not the sole element for back
translation. This motivates us to study a funda-
mental question for back translation: what kind of
synthetic corpus is beneficial to back translation?
3 Understanding Synthetic Data by Two
Factors
To answer the fundamental question presented
in the previous section, we first start from the
marginal likelihood objective defined on the target
language Y, and then we theoretically explain two
factors (i.e., quality and importance) that are highly
related to the training objective of back transla-
tion. Finally, we empirically explain why synthetic
corpus with low quality may lead to better perfor-
mance than synthetic corpus with high quality by
measuring both factors.
3.1 Theoretical Explanation
Maximizing marginal likelihood is an important
principle to leverage unlabeled data. Therefore, we
rethink back translation from this principle because
it makes use of target monolingual corpus M. For
eachy∈ M , the marginal likelihood objective can
be derived by the Bayesian Equation (5), Jansen
Inequality (6), and importance sampling (7) as fol-
lows:
logp(y;θ) = log/summationdisplayp(x)p(y|x;θ) (5)
≥/summationdisplayp(x) logp(y|x;θ) (6)
=/summationdisplayp(x|y)p(x)
p(x|y)logp(y|x;θ)
=E/braceleftigp(ˆx)
p(ˆx|y)logp(y|ˆx;θ)/bracerightig
≈p(ˆx)
p(ˆx|y)logp(y|ˆx;θ) (7)
where p(x)is a language model on source language
X,p(x|y)is a backward translation model from
YtoXwhich serves as the proposal distribution
for importance sampling, and ˆxis sampled from
p(x|y). Ifp(x|y)is set as the backward model
p(x|y;π)optimized on B, the last term in Equation
7 is the same as the second term in BT loss (i.e.,
logp(y|ˆx)in Eq. 2), and the unique difference is
the multiplicative term called importance weight:
Imp(ˆx;y) =p(ˆx)
p(ˆx|y)(8)The denominator is the candidate conditional prob-
ability to target, and the numerator is the candidate
distribution on source language distribution. Since
Imp(ˆx;y)is constant with respect to the parameter
θ, maximizing logp(y|ˆx;θ)in BT loss implicitly
maximizes Imp(ˆx;y) logp(y|ˆx), which indicates
that back translation aims to implicitly maximize
the marginal likelihood objective. More impor-
tantly, according to Equation 7 we can find that the
following two factors are critical to influence the
marginal likelihood logp(y;θ):
•Factor 1: The quality of ˆxas a translation of
ycorresponding to the logp(y|ˆx;θ)in Eq. 7.
•Factor 2: The importance of ˆxas a translation
ofycorresponding to Imp(ˆx;y)in Eq. 7.
Theoretically, if ˆxis of higher quality and con-
tains more semantic information in y,p(y|ˆx;θ)
would be higher and thus it would lead to a higher
logp(y;θ), which is well acknowledged by prior
work (Sennrich et al., 2016a; Wang et al., 2019).
In particular, if ˆxis with higher importance weight,
maximizing logp(y|ˆx;θ)is more helpful to maxi-
mize logp(y;θ). On the contrary, if Imp(ˆx;y)is
very small, it needs to avoid such a sample ˆxfrom
p(x|y), which is essentially the rejection control
strategy in importance sampling theory (Liu et al.,
1998; Liu and Liu, 2001).
Unfortunately, in practice, both factors are mu-
tually exclusive to some extent: if ˆxis with high
quality, p(ˆx|y;θ)would be higher as well leading
to lower importance weight. This fact can explain
the contradictory observation in Sec 2 that BT with
high-quality synthetic data sometimes leads to bet-
ter testing performance, while it may deliver worse
performance at other times, which will be later
justified in Sec 3.2.
Estimating Two Factors To measure the quality
ofˆxfor each y, it is natural to use the evaluation
metric such as BLEU if the reference translation
xofyis available. Otherwise, as a surrogate, we
use the log likelihood logp(ˆx|y;π)of the back-
ward translation model πwhich is trained on the
authentic data B. Similarly, in order to estimate the
importance of ˆx, we train an additional language
model p(x;ω)with GPT (Radford et al., 2018) on
a large monolingual corpus for X. In this way, the
importance weight is estimated by
Imp(ˆx)≈p(ˆx;ω)
p(ˆx|y;π)421
3.2 Empirical Justification
In this subsection, we aim to justify the following
statements: 1) encouraging the quality of synthetic
corpus may to some extent hurt the performance of
BT due to the decrease of importance; 2) judging
the testing performance in terms of quality only
may be dangerous while it would be meaningful
to judge the testing performance by taking into ac-
count both factors rather than either factor. To this
end, we run some quick experiments on WMT14
datasets whose settings will be shown in Sec 5 later.
We set up two back translation systems with
two different options (i.e., beam search and sam-
pling) to generate synthetic corpus by using the
best checkpoint of p(ˆx|y;π)tuned on the develop-
ment set. Both beam search and sampling based
BT systems are denoted by beam and sampling. In
addition, we pick another checkpoint of p(ˆx|y;π)
which is trained for only 1 epoch, and we use this
weak checkpoint to set up another beam search
based BT system, which is denoted as beam*. Ta-
ble 1 shows BLEU on test dataset, the quality and
importance on the development set according to
three systems on WMT14 DE-EN task.
In Table 1, beam is better than sampling in the
quality of synthetic corpus but its testing perfor-
mance is worse. This is meaningful because the
former relies on the synthetic corpus with lower
importance weight according to our theoretical ex-planation. In addition, when comparing beam with
beam*, we can find that beam delivers better test-
ing performance because its quality is better mean-
while its importance weight is almost similar to that
of beam*. Table 2 consistently demonstrates that
it is meaningless to take into account quality only
when evaluating BT. These facts justify our state-
ments and provide an answer to the fundamental
question in section 2.
4 Improving Synthetic Data for BT
As shown in the previous section, both importance
and quality of synthetic corpus are beneficial to
the overall testing performance of back translation.
It is a natural idea to promote both factors when
generating synthetic corpus such that running BT
on such corpus leads to better testing performance.
However, this is difficult because both factors are
mutually exclusive as discussed in Section 3. In this
section, we instead propose two methods (namely
data manipulation and gamma score) to trade off
both factors in the hope to yield better BT perfor-
mance.
4.1 Data Manipulation
Since the synthetic data in sampling based BT
is of high importance yet low quality whereas
the case for the synthetic data in beam search
based BT is opposite, we propose a data manip-
ulation method to trade off importance and quality
by combining both synthetic datasets. Through
balancing the ratio between beam and sampling
based synthetic corpora, we expect to find an op-
timized beam/sampling ratio to further improve
NMT model performance.
Specifically, we randomly shuffle Mand divide
it into two parts with the first part accounting for γ
(0< γ < 1); then we generate translations for the
first part with beam search while generating trans-
lations for the second part with sampling. Formally,
we use the following corpus Mas the synthetic
corpus for BT:
M={(ˆx, y)} ∪ {(ˆx, y)}
k=⌊γ|M|⌋
Where ˆxdenotes a translation of ygenerated by
p(x|y;π)with beam search and ˆxis a translation
with sampling, | · |means the size of the corpus,
andγis the combination ratio for beam and sam-
pling synthetic corpora. By tuning γhere, one can422modify the weightage for the number of beam and
sampling sentences, to improve back-translation
performance by training models on a combined
synthetic corpus.
Although this method is easy to implement, its
limitation is obvious. Since each ˆxis either from
beam search or from sampling, the quality of M
is generally worse than that of beam search and its
importance weight is generally worse than that of
sampling. Consequently, we propose an alternative
method in the next part of this section.
4.2 Gamma Score
The key idea to the alternative method is that it
employs a score that balances both quality and
importance to generate a translation ˆxfor each
y∈ M . A natural choice of such a score is defined
by the interpolation score as follows:
γlog Imp(ˆ x;ω, π) + (1 −γ) logp(ˆx|y;π)
where γis used to trade off both factors as in corpus
manipulation. With the help of this score, one may
optimize the ˆxby beam search whose interpolation
score is the best among all possible translations of
y∈ M . Unfortunately, such an implementation
leads to limited performance in our preliminary
experiments, due to two major challenges.
On one hand, the estimations of quality and im-
portance weight of ˆxare not well calibrated, and
in particular, quality and importance are mutually
exclusive as mentioned before. As a result, beam
search with the interpolation score over the expo-
nential space can not guarantee a desirable transla-
tionˆxfor each y. On the other hand, quality and
importance weight of ˆxare not at the same scale
for different y, it is difficult to balance both factors
with a fixed γin the interpolation score for different
y.
To alleviate these issues, we propose a simple
method as follows. Specifically, firstly, instead
of beam search with the interpolation score, we
simply utilize the backward translation p(x|y;π)
to randomly sample a set of candidate translations
which is denoted by A(y) ={ˆx}(N= 50 in
this paper as it works well).Then we pick a
ˆxamong A(y)according to the balancing score.
Secondly, for each ˆx, we normalize the log values
of importance and quality of each candidate by itssequence length, then normalize these values with
respect to all Ncandidates as follows:
˜F(ˆx) =log/parenleftbig
F(ˆx)/parenrightbig
/len(ˆx)−µ
σ(9)
where Fis either importance weight or quality es-
timations, and µ=/summationtextlogF(ˆx)andσ=are mean and variance of Nsam-
pled candidates with length normalized. Finally,
the Gamma score is defined on the normalized val-
ues of importance and quality as follows:
Γ(ˆx;ω, π) =
exp/parenleftbig
γ˜Imp(ˆx;ω, π) + (1 −γ)˜p(ˆx|y, π)/parenrightbig
/summationtextexp/parenleftbig
γ˜Imp(ˆx;ω, π) + (1 −γ)˜p(ˆx|y, π)/parenrightbig
(10)
where ˜Imp and˜pare the normalized log value of
importance weight and backward translation model
p(ˆx|y, π)as defined in Equation 9.
Once the gamma score in Equation 10 is com-
puted, there are two methods to select ˆxfromA(y),
which are deterministic and stochastic methods.
For deterministic selection, we simply select the
candidates with maximum gamma score among
Ntranslation candidates; and for sampling, we
sample a candidate according to its gamma score
distribution. These two methods are called gamma
selection and gamma sampling in our experiments.
5 Experiments
5.1 Settings
We run all the experiments by using fairseq (Ott
et al., 2019) framework. For dataset settings, since
datasets WMT14 EN-DE and DE-EN are widely
used (Li et al., 2019b; Zhu et al., 2020; Li et al.,
2020; Fan et al., 2021; Le et al., 2021), we fol-
low both standard benchmarks and additionally we
employ WMT14 RU-EN as the third dataset to val-
idate the effectiveness of the proposed methods.
For back translation experiment, we use an equal
scale monolingual corpus randomly sampled from
Newscrawl 2020 (Barrault et al., 2019) compris-
ing 4.5 million monolingual sentences for DE-EN
language pair and 2.5 million for RU-EN direction,
thus total 9 million sentences for DE-EN pair and 5
million for RU-EN direction are used. We tokenize
the parallel corpus using Mose tokenizer (Koehn
et al., 2007), and learn a source and target shared
Byte-Pair-Encoding (BPE) (Sennrich et al., 2016b)423
with 32K types. We develop on newstest2013 and
report the results on newstest2014.
As for model architecture, we employ
all the translation models using architecture
transformer_wmt_en_de_big , which is a
Big Transformer architecture with 6 blocks in the
encoder and decoder, and is widely used as a stan-
dard backbone on various NMT research studies.
We use the same hyperparameter settings across
all the experiments, i.e., 1024 word representation
size, 4096 inner dimensions of feed-forward layers,
and dropout is set to 0.3 for all the experiments.
In addition, for monolingual models, we apply
transformer_lm_gpt architecture (Radford
et al., 2018) on source language side of the
corpus without any extra corpus.The detailed
hyperparameters used for training translation and
language models are shown in Appendix.
For baseline models, we train them for 400K
updating steps, and train the models with back-
translation data for 1.6M updating steps. We save
the checkpoints every 100k updating intervals, and
only select the checkpoints with highest develop
set performance. As for the back-translation data,
we study beam decoding and sampling decoding
as baselines since they are the common practice
for BT research (Roberts et al., 2020; Wang et al.,
2019). We use baseline models’ checkpoints at
400K updating steps to generate default beam5 de-
coding and sampling decoding synthetic corpus
without any penalty. For monolingual models,
we only select the checkpoints with the best de-
velop set performance. When tuning γon dev sets
for data manipulation methods we select it from
{0,1/4,1/2,3/4,1}and the optimal is γ= 1/2.
For the Gamma Score method, γis tuned among
{0.1,0.2,0.3,0.4,0.5}and it is set γ= 0.2for all
three tasks.
All the experiments are conducted using 8
Nvidia V100-32GB graphic cards without any gra-
dient accumulation or bitext upsampling, and the
results in this paper are measured in case-sensitive
detokenized BLEU with SacreBLEUby Post
(2018).
5.2 Main Results
5.2.1 Results on DE-EN
Data Manipulation We conduct two experi-
ments to study the data manipulation for back-
translation NMT model performance using afore-
mentioned corpus with and without authentic cor-
pus.
Table 3 show the data manipulation results com-
pared with baseline. Firstly, for synthetic corpus
experiment, we find that even if only monolingual
corpus is used, the performance of back-translation
NMT model can still be significantly improved
to 31.3 from 29.2 by sampling or 27.6 by beam,
and it is only 0.7 lower than bitext baseline by
BLEU score measure. Secondly, for the experi-
ments with bitext, the best performance by data
manipulation only helps the back-translation NMT
model achieves almost the same performance with
sampling BT. This means data manipulation meth-
ods cannot achieve a higher BLEU score than sam-
pling or beam.
Gamma Score In this paragraph, we conduct the
experiments based on gamma score method. We
conduct both of the methods in this experiment: we
select the candidate with highest gamma score for
the deterministic method whereas sample the candi-424System EN-DE RU-EN
Transformer 27.4 34.1
Beam BT 29.7 35.9
Sampling BT 30.0 35.6
Gamma selection BT 31.0* 36.1*
Gamma sampling BT 30.9* 36.3*
date by gamma score distribution for the stochastic
method.
Once again, we use synthetic gamma corpus
combined with bitext to train the back-translation
NMT models on each corpus, the results are listed
in 4. From the table, we can see that our proposed
gamma sampling significantly outperforms the sam-
pling based and beam search based back-translation
baselines by 0.9 and 2.3 BLEU scores in terms of
SacreBLEU. And our two proposed gamma score
based methods outperform the data manipulation
method as well.
In the rest of the experiments, we report results
for both gamma selection and gamma sampling as
the proposed methods and their hyperparameter γ
for other tasks is fixed to 0.2.
5.3 Results on other Datasets
We conduct the experiments on WMT14 EN-DE
and RU-EN for both gamma selection and gamma
sampling as well, and table 5 shows that our pro-
posed gamma based methods significantly outper-
form beam and sampling based back-translation
methods on both en-de and ru-en translation for al-
most 1 and 0.4 BLEU score respectively. Recently,
Edunov et al. (2020) point out that BLEU might
overlook the contributions from back translation
since it poorly correlates with human evaluation on
the data generated in back translation scenario. Fol-
low their suggestions, to better reflect the scenario
of back translation, we also evaluate our experi-
ment using COMET metric suggested by Rei et al.
(2020). The results are shown in table 6 and we
can see that the proposed methods perform well in
terms of COMET.
Discussion on Efficiency Since our method re-
quires to run sampling with size of 50 to generatesynthetic data, its efficiency is about 10x slower
than that of beam BT with size of 5 and 50x slower
than that of sampling BT with size 1. Luckily, be-
cause the bottleneck of BT is not the synthetic data
generation but the parameter optimization on both
synthetic and authentic data, our overall overhead
is less than 0.5x slower than sampling BT. In addi-
tion, since decoding is very easy to be parallelized
on GPU or CPU machines, the cost of decoding is
not a serious issue for our method, which makes it
possible to run our method on a large scale dataset.
5.4 Analysis on Synthetic Corpus
In this subsection, we analyze the synthetic cor-
pus of proposed gamma score methods on both
sentence level and token level.
Sentence Level We evaluate the back-translation
synthetic source sentences by their sentence rep-
resentations. We use the baseline model to gener-
ate the hidden representations at the end-of-speech
token as the sentence representation. Here, we
compute the singular value spectrum of the rep-
resentations for different back-translation corpora.
The spectrum is shown in figure 1(a). From
the spectrum, sampling has a more uniform distri-
bution whereas beam has the worst variety. Our
proposed methods have moderate variety between
sampling and beam, and gamma sampling consists
of higher linguistic information richness compared
with gamma selection.
Figure 1(b) shows the sequence length of the
synthetic corpora of different generation methods.
Beam generates the shortest synthetic sentences
and gamma sampling generates the longest syn-
thetic sentences on average. Between them, sam-
pling and gamma selection generate almost the
same sequence length, which means gamma selec-
tion candidates provide more learning signals than
random sampling under the same length.
Token Level Figure 1(c) is the token frequency
histogram, which shows beam has higher probabil-
ity to decode high frequency tokens while sampling425
prefers more low frequency tokens.
We also measure the vocabulary size, finding
that the proposed gamma sampling shares the same
vocabulary size as sampling method. This could
be the reason that gamma sampling is based on
random sampling for candidates generation.
6 Related Work
This section describes prior arts in back-translation
for NMT, data augmentation, and semi-supervised
machine translation.
Back-translation NMT Bojar and Tamchyna
(2011) firstly proposed back-translation, then
Bertoldi and Federico (2009); Lambert et al. (2011)
apply back translation to solve the domain adapta-
tion problems in phrase-based NMT systems. Sen-
nrich et al. (2016a) further extend the back transla-
tion for training NMT models integrally.
For understanding the back-translation synthetic
corpus, Currey et al. (2017) use a copy of target as
a pseudo source, and find that NMT model perfor-
mance can still be improved under the low resource
settings. Caswell et al. (2019) propose tagged
back-translation to indicate to the model that the
given source is synthetic. To further find an op-
timum back-translation corpus decoding method,
Imamura et al. (2018) firstly use sampling basedsynthetic corpus and find such a stochastic decod-
ing method outperforms beam search on boosting
NMT model performance, and Edunov et al. (2018)
broaden the investigation of a number of back-
translation generation methods for synthetic source
sentences. Their contribution shows that sampling
or noisy synthetic data gives a much stronger train-
ing signal. Graça et al. (2019) reformulate back-
translation in the context of optimization and clari-
fying to improve sampling based decoding method
search space, thus proposing N best list sampling.
Recently, Nguyen et al. (2020) diversify the train-
ing data by multiple forward and backward models
translations and combine them with the original
datasets.
Data Augmentation for NMT NMT researchers
are the pioneers of data augmentation studies since
back-translation is a natural type of data augmen-
tation method. (Sennrich et al., 2016a; Norouzi
et al., 2016; Zhang and Zong, 2016; Bi et al., 2021).
To balance the token frequency in NMT corpus,
Fadaee et al. (2017) create new sentences contain
low-frequency words. However, as observed by
Wang et al. (2018), the improvement across dif-
ferent translation tasks is not consistent, and they
invent SwitchOut data augmentation policy. Recht
et al. (2018, 2019); Werpachowski et al. (2019) also
observe such an inconsistency of variance between
training corpus and testing set as well as in the
generation tasks. Recently, Li et al. (2019a) try to
understand data augmentation from input sensitiv-
ity and prediction margin, thus obtaining relatively
low variance in generation.
Semi-supervised Machine Translation How-
ever, as high quality bitext is always limited and
costly to collect, Gulcehre et al. (2015) study meth-
ods for effectively leveraging monolingual data in426NMT systems. He et al. (2016) develop a dual-
learning mechanism, under such a learning objec-
tive, a NMT system is able to automatically learn
from unlabeled data, thus improving NMT perfor-
mance iteratively. Based on iterative learning, Lam-
ple et al. (2018) investigates how to learn NMT
systems when only large monolingual corpora can
be used in each language.
For supervision of models, Gulcehre et al. (2017)
employ the target language model hidden states
into NMT decoder to further improve performance.
Edunov et al. (2020) show that back-translation
improves translation quality of both naturally oc-
curring text and translationese according to pro-
fessional human translators. For supervision of
learning corpus, Wu et al. (2019) study both the
source-side and target-side monolingual data for
NMT.
7 Conclusion
In this work, we answer a fundamental question
about synthetic data for back translation. We the-
oretically and empirically show two key factors
namely quality and importance weight of synthetic
data play an important role in back translation, and
then we propose a new method to generate syn-
thetic data which better balances both factors so
as to boost the back-translation performance. For
future work, we think it would be of significance
to apply our synthetic data generation method to
other BT methods or even to more broad NLP tasks
such as paraphrasing and style transfer.
Acknowledgements
We would like to thank anonymous (meta) review-
ers for suggestions on this paper. L. Liu is the
corresponding author.
References427428
A Model Details
The models are optimized using Adam optimizer
(Kingma and Ba, 2015), with β= 0.9, β=
0.98. We use the same learning rate schedular as
(Vaswani et al., 2017) with maximum learning rate
7×10, and 4000 warmup updates. We use the
fairseq 10.2 as the framework and the training com-
mand as well as the model hyperparameters are
listed below,
fairseq-train \
--arch transformer_wmt_en_de_big
--share-all-embeddings
--dropout 0.3
--weight-decay 0.0
--criterion
label_smoothed_cross_entropy429--label-smoothing 0.1
--optimizer adam
--adam-betas ’(0.9, 0.98)’
--clip-norm 0.0
--lr-scheduler inverse_sqrt
--warmup-updates 4000
--max-tokens 4096
--max-update 1600000
--validate-interval-updates 10000
--save-interval-updates 100000
--lr 7e-4
--upsample-primary 1
And the GPT model we employ is only trained
on source side of bitext corpus, without extra
datasets. The training command line and core set-
tings are listed below.
fairseq-train \
--task language_modeling
--arch transformer_lm_gpt
--share-decoder-input-output-embed
--dropout 0.1
--optimizer adam
--adam-betas ’(0.9, 0.98)’
--weight-decay 0.01
--clip-norm 0.0
--lr 7e-5
--lr-scheduler inverse_sqrt
--warmup-updates 8000
--tokens-per-sample 512
--sample-break-mode none
--max-tokens 4096
--update-freq 1
--max-update 1000000
--keep-last-epochs 5
--validate-interval-updates 10000
--save-interval-updates 10000430