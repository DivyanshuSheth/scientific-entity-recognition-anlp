
Libo Qin, Qiguang Chen, Tianbao Xie, Qixin Li,
Jian-Guang Lou, Wanxiang Che, Min-Yen KanResearch Center for Social Computing and Information Retrieval
Harbin Institute of Technology, ChinaMicrosoft Research Asia, Beijing, ChinaDepartment of Computer Science, National University of Singapore
{lbqin,tianbaoxie,qxli,car}@ir.hit.edu.cn; jlou@microsoft.com; kanmy@comp.nus.edu.sg
Abstract
Due to high data demands of current meth-
ods, attention to zero-shot cross-lingual spo-
ken language understanding (SLU) has grown,
as such approaches greatly reduce human an-
notation effort. However, existing models
solely rely on shared parameters, which can
only perform implicit alignment across lan-
guages. We present Global– LocalContrastive
Larning Framework (GL-CLF) to address
this shortcoming. Speciﬁcally, we employ con-
trastive learning, leveraging bilingual dictio-
naries to construct multilingual views of the
same utterance, then encourage their represen-
tations to be more similar than negative exam-
ple pairs, which achieves to explicitly aligned
representations of similar sentences across lan-
guages. In addition, a key step in GL-CLF
is a proposed Local andGlobal compo-
nent, which achieves a ﬁne-grained cross-
lingual transfer (i.e., sentence-level Local in-
tent transfer, token-level Local slot transfer,
andsemantic-level Global transfer across in-
tent and slot). Experiments on MultiATIS++
show that GL-CLF achieves the best perfor-
mance and successfully pulls representations
of similar sentences across languages closer.
1 Introduction
Spoken language understanding (SLU) is a critical
component in task-oriented dialogue systems (Tur
and De Mori, 2011; Qin et al., 2021b). It usu-
ally includes two sub-tasks: intent detection to
identify users’ intents and slot ﬁlling to extract
semantic constituents from the user’s query. With
the advent of deep neural network methods, SLU
has met with remarkable success. However, ex-
isting SLU models rely on large amounts of an-
notated data, which makes it hard to scale to low-
resource languages that lack large amounts of la-
beled data. To address this shortcoming, zero-shotFigure 1: (a) Prior work (Implicit Alignment); (b) GL-
CLF (Explicit Alignment). Different color denotes
representations across different languages. [CLS] rep-
resents the sentence representation.
cross-lingual SLU generalization leverages the la-
beled training data in high-resource languages to
transfer the trained model to a target, low-resource
language, which gains increasing attention.
To this end, many works have been explored for
zero-shot cross-lingual SLU. Multilingual BERT
(mBERT) (Devlin et al., 2019), a cross-lingual
contextual pre-trained model from a large amount
of multi-lingual corpus multi-lingual corpus, has
achieved considerable performance for zero-shot
cross-lingual SLU. Liu et al. (2020) further build
an attention-informed mixed-language training by
generating bi-lingual code-switched data to implic-
itly align keywords (e.g., slots) between source
and target language. Qin et al. (2020) extend
the idea to a multilingual code-switched setting,
aligning the source language to multiple target
languages. This approach currently achieves the2677state-of-the-art performance for zero-shot cross-
lingual SLU. Though achieving promising perfor-
mance, as shown in Figure 1 (a), the above methods
solely rely on shared parameters and can only per-
form implicit alignment across languages, which
brings two challenges. First, such implicit align-
ment process seems to be a black box, which not
only seriously affects the alignment representation
but also makes it hard to analyze the alignment
mechanism. Second, prior work do not distinguish
between the varying granularities of the tasks: the
intent detection is sentence-level and the slot ﬁlling
istoken-level , which does not offer ﬁne-grained
cross-lingual transfer for token-level slot ﬁlling.
To solve the aforementioned challenges, we
propose a Global– Local Contrastive Larning
Framework ( GL-CLF) for zero-shot cross-
lingual SLU. For the ﬁrst challenge, as shown in
Figure 1 (b), the key insight in GL-CLFis to
explicitly ensure that representations of similar sen-
tences across languages are pulled closer together
via contrastive learning (CL). Speciﬁcally, we lever-
age bilingual dictionaries to generate multi-lingual
code-switched data pairs, which can be regarded as
cross-lingual views with the same meaning. With
the use of CL, our model is able to learn to dis-
tinguish the code-switched utterance of an input
sentence from a set of negative examples, and thus
encourages representations of similar sentences be-
tween source language and target language closer.
For the second challenge, SLU requires accom-
plishing tasks at two different levels: token-level
slot ﬁlling and sentence-level intent detection. As
such, simply leveraging ordinary sentence-level
contrastive learning is ineffective for ﬁne-grained
knowledge transfer in token-level slot ﬁlling. There-
fore, we ﬁrst introduce a Local module in GL-
CLFto learn different granularity alignment rep-
resentations (i.e., sentence-level Local intent CL
andtoken-level local slot CL). To be speciﬁc,
sentence-level Local intent CL and token-level
local slot CL are introduced for aligning similar
sentence and token representations across different
languages for intent detection and slot ﬁlling, re-
spectively. In addition, we further argue that slot
and intent are highly correlated and have similar se-
mantic meanings in a sentence. This phenomenon
can serve as a signal for self-supervised alignment
across intent and slots. Therefore, a Global mod-
ule named semantic-level global intent–slot CL
is further proposed to bring the representations of
slot and intents within a sentence closer together.
We conduct experiments on MultiATIS++ (Xu
et al., 2020), which includes nine different lan-
guages. Our experiments show that GL-CLF
achieves state-of-the-art results of 54.09% sen-
tence accuracy, outperforming the previous best
by 10.06% on average. Besides, extensive analysis
experiments demonstrate that GL-CLFhas suc-
cessfully reduced the representation gap between
different languages.
To facilitate further research, codes are pub-
licly available at https://github.com/
LightChen233/GL-CLeF .
2 Background
We ﬁrst describe traditional SLU before the
speciﬁcs of zero-shot cross-lingual version of SLU.
Traditional SLU in Task-oriented Dialogue.
SLU in Task-oriented Dialogue contains two sub-
tasks: Intent Detection andSlot Filling .
Intent Detection: Given input utterance x, this is a
classiﬁcation problem to decide the corresponding
intent labelo.
Slot Filling: Often modeled as a sequence label-
ing task that maps an input word sequence x=
(x;:::;x)to slots sequence o= (o;:::;o),
wherendenotes the length of sentence x.
Since the two tasks of intent detection and slot
ﬁlling are highly correlated, it is common to adopt
a joint model that can capture shared knowledge.
We follow the formalism from Goo et al. (2018),
formulated as (o;o) =f(x), wherefis the
trained model.
Zero-shot Cross-lingual SLU. This means that
a SLU model is trained in a source language, e.g.,
English ( cf.Figure 2 (a)) and directly applied to
other target languages ( cf.Figure 2 (b)).2678
Formally, given each instance xin a target lan-
guage, the model fwhich is trained on the source
language is directly used for predicting its intent
and slots:
(o;o) =f(x); (1)
wheretgtrepresents the target language.
3 Model
We describe the general approach to general SLU
task ﬁrst, before describing our GL-CLFmodel
which explicitly uses contrastive learning to ex-
plicitly achieve cross-lingual alignment. The main
architecture of GL-CLFis illustrated in Figure 3.
3.1 A Generic SLU model
Encoder. Given each input utterance x=
(x;x;:::;x), the input sequence can be
constructed by adding speciﬁc tokens x=
([CLS];x;x;:::;x;[SEP] ), where [CLS]
denotes the special symbol for representing the
whole sequence, and [SEP] can be used for sep-
arating non-consecutive token sequences (Devlin
et al., 2019). Then, we follow Qin et al. (2020)
to ﬁrst generate multi-lingual code-switched data.Then, we employ mBERT model to take code-
switched data for encoding their representations
H= (h,h, . . . ,h,h).
Slot Filling. Since mBERT produces subword-
resolution embeddings, we follow Wang et al.
(2019) and adopt the ﬁrst sub-token’s represen-
tation as the whole word representation and use
the hidden state to predict each slot: o=
softmax( Wh+b);where hdenotes the ﬁrst
sub-token representation of word x;Wandb
refer to the trainable parameters.
Intent Detection. We input the sentence repre-
sentation hto a classiﬁcation layer to ﬁnd the
labelo:o= softmax( Wh+b), where
Wandbare tuneable parameters.
3.2 Global–local Contrastive Learning
Framework
We introduce our global–local contrastive learning
framework ( GL-CLF) in detail, which consists of
three modules: 1) a sentence-level local intent
contrastive learning (CL) module to align sentence
representation across languages for intent detection,
2) atoken-level local slot CL module to align to-
ken representations across languages for slot ﬁlling,2679and 3) semantic-level global intent–slot CL to
align representations between a slot and an intent.
3.2.1 Positive and Negative Samples
Construction
For contrastive learning, the key operation is to
choose appropriate positive and negative pairs
against to the original (anchor) utterance.
Positive Samples. Positive samples should pre-
serve the same semantics compared against the
anchor utterance. Therefore, given each anchor ut-
terance x= ([CLS];x;x;:::;x;[SEP] ), we
follow Qin et al. (2020) to use bilingual dictionar-
ies (Lample et al., 2018) to generate multi-lingual
code-switched data, which is considered as the pos-
itive samples x. Speciﬁcally, for each word x
inx,xis randomly chosen to be replaced with a
translation provisioned from a bilingual dictionary
to generate a positive sample. For example, given
an anchor utterance “watch sports movie” in En-
glish, we can generate a positive multi-lingual code-
switched sample “看(watch/zh) スポツ (sports/ja)
película (movie/es)” (cf.Figure 3). Such a pair of
anchor utterance and multi-lingual code-switched
sample can be regarded as cross-lingual views of
the same meaning across different languages. xis
fed into mBERT to obtain the corresponding repre-
sentations H= (h,h, . . . ,h,h).
Negative Samples. A natural approach for gener-
ating negative samples is randomly choosing other
queries in a batch. However, this method requires
the recoding of the negative samples, hurting efﬁ-
ciency. Inspired by He et al. (2020), in GL-CLF,
we maintain a negative sample queue, where the
previously encoded original anchor utterance x,
positive samples xand previous negative sam-
plesxare also progressively reused as negative
samples. This enables us to reuse the encoded
samples from the immediate preceding batches, so
as to eliminate the unnecessary negative encoding
process. The negative sample queues for [CLS]
and sentence representation are represented as:
H=fhg,H=fHg, where
K is the maximum capacity for negative queue.
3.2.2 Local Module
Sentence-level Local Intent CL. Since intent
detection is a sentence-level classiﬁcation task,
aligning sentence representation across languages
is the goal of zero-shot cross-lingual intent detec-
tion task. Therefore, in GL-CLF, we proposeasentence-level local intent CL loss to explic-
itly encourage the model to align similar sentence
representations into the same local space across
languages for intent detection. Formally, this is
formulated as:
wheres(p;q)denotes the dot product between p
andq;is a scalar temperature parameter.
Token-level Local Slot CL. As slot ﬁlling is a
token-level task, we propose a token-level local
slot CL loss to help the model to consider token
alignment for slot ﬁlling, achieving ﬁne-grained
cross-lingual transfer. We apply toke-level CL for
all tokens in the query. Now, we calculate the ith
token CL loss for simplicity:
where the ﬁnalLis the summation of all tokens
CL loss.
3.2.3 Global Module
Semantic-level Global Intent-slot CL. We
noted that slots and intent are often highly related
semantically when they belong to the same query.
Therefore, we think that the intent in a sentence
and its own slots can naturally constitute a form
of positive pairings, and the corresponding slots in
other sentences can form negative pairs. We thus
further introduce a semantic-level global intent–
slot CL loss to model the semantic interaction be-
tween slots and intent, which may further improve
cross-lingual transfer between them. Formally:
where we consider CL loss from both anchor sen-
tences (L) and code-switched sentence ( L),
and add them to do semantic-level contrastive learn-
ing (L) .26803.3 Training
3.3.1 Intent Detection Loss
L, X^ylog 
o
; (2)
where ^yare the gold intent label and nis the
number of intent labels.
3.3.2 Slot Filling Loss
L, XX^ylog
y
; (3)
where ^yare the gold slot label for jth token;n
is the number of slot labels.
3.3.3 Overall Loss
The overall objective in GL-CLFis a tuned linear
combination of the individual losses:
whereare tuning parameters for each loss com-
ponent.
4 Experiments
We use the latest multilingual benchmark dataset
of MultiATIS++ (Xu et al., 2020) which consists of
9 languages including English (en), Spanish (es),
Portuguese (pt), German (de), French (fr), Chinese
(zh), Japanese (ja), Hindi (hi), and Turkish (tr).
4.1 Experimental Setting
We use the base case multilingual BERT (mBERT),
which hasN= 12 attention heads and M= 12
transformer blocks. We select the best hyperpa-
rameters by searching a combination of batch size,
learning rate with the following ranges: learn-
ing ratef210;510;110;2
10;510;610;510;510g;
batch sizef4;8;16;32g; max size of negative
queuef4;8;16;32g; For all experiments, we se-
lect the best-performing model over the dev set
and evaluate on test datasets. All experiments are
conducted at TITAN XP and V100.
4.2 Baselines
To verify the effect of GL-CLF, we compare our
model with the following state-of-the-art baselines:
1)mBERT. mBERTfollows the same model ar-
chitecture and training procedure as BERT (Devlin
et al., 2019), but trains on the Wikipedia pages of104 languages with a shared subword vocabulary.
This allows mBERT to share embeddings across
languages, which achieves promising performance
on various cross-lingual NLP tasks;
2)Ensemble-Net. Razumovskaia et al. (2021)
propose an Ensemble-Net where predictions
are determined by 8 independent models through
majority voting, each separately trained on a single
source language, which achieves promising perfor-
mance on zero-shot cross-lingual SLU;
3)AR-S2S-PTR. Rongali et al. (2020) pro-
posed a uniﬁed sequence-to-sequence models with
pointer generator network for cross-lingual SLU;
4)IT-S2S-PTR. Zhu et al. (2020) proposed a
non-autoregressive parser based on the insertion
transformer. It speeds up decoding and gain im-
provements in cross-lingual SLU transfer;
5)CoSDA. Qin et al. (2020) propose a data aug-
mentation framework to generate multi-lingual
code-switching data to ﬁne-tune mBERT, which
encourages the model to align representations from
source and multiple target languages.
4.3 Main Results
Following Goo et al. (2018), we evaluate the per-
formance of slot ﬁlling using F1 score, intent pre-
diction using accuracy, and the sentence-level se-
mantic frame parsing using overall accuracy which
represents all metrics are right in an utterance.
From the results in Table 1, we observe that:
(1)CoSDA achieves better performance than no
alignment work mBERT and even outperforms the
Ensemble-Net . This is because that such im-
plicit alignment does align representations to some
extent, compared against mBERT . (2) Our frame-
work achieves the state-of-the art performance
and beats CoSDA with 10.06% average improve-
ments on overall accuracy. This demonstrates that
GL-CLFexplicitly pull similar representations
across languages closer, which outperforms the im-
plicit alignment manner.
4.4 Analysis
To understand GL-CLFin more depth, we per-
form comprehensive studies to answer the follow-
ing research questions (RQs):
(1) Do the local intent and slot CLs bene-
ﬁtsentence- andtoken-level representation align-
ment? (2) Can semantic-level global intent-slot
CL boost the overall sentence accuracy? (3) Are
local intent CL and local slot CL complemen-
tary? (4) Does GL-CLFpull similar representa-2681
tions across languages closer? (5) Does GL-CLF
improve over other pre-trained models? (6) Does
GL-CLFgeneralize to non pre-trained models?
(7) Is GL-CLFrobust to the one-to-many transla-
tion problem?
Answer 1: Local intent CL and slot CL
align similar sentence and token representa-
tions across languages. We investigate the ef-
fect of the local intent CL and local slot CL
mechanism, by removing the local intent CL and
slot CL, respectively (Figure 4, “– LI” and “– LS”
(Col 1,2)). For the effectiveness of local intent
CL, we ﬁnd the performance of intent detection
averaged on 9 languages drops by 3.52% against
the full system ( ibid. ﬁnal, RHS column). This
is because sentence-level intent CL loss can pull
sentence representations closer across languages.
Similarly, considering the effectiveness of
local slot CL, we ﬁnd the performance of slot
ﬁlling averaged on 9 languages drops by 2.44%
against the full system. We attribute performance
drops to the fact that local slot CL successfully
make a ﬁne-grained cross-lingual knowledge trans-
fer for aligning token representation across lan-
guages, which is essential for token-level cross-
lingual slot ﬁlling tasks.
Answer 2: Semantic-level global intent-slot
successfully establishes a semantic connection
across languages. We further investigate the ef-
fect of the semantic-level intent-slot CL mechanism
when we remove the global intent-slot CL loss(Figure 4, “– GIS” (Col 3)). We ﬁnd the sentence
overall performance drops a lot (from 54.09% to
46.94%). Sentence overall metrics require model
to capture the semantic information (intent and
slots) for queries. Therefore, we attribute it to the
proposed semantic-level global intent-slot CL.
As it successfully establishes semantic connection
across languages, it boosts overall accuracy.
Answer 3: Contribution from local intent CL
and slot CL module are complementary. We
explore whether local intent CL and slot CL
module are complementary. By removing all
theLocal CL modules (including sentence-level
local intent CL and token-level local slot CL),
results are shown in Figure 4 (–Local Col 4). We
ﬁnd that the experiments are lowest compared
with only removing any single local CL mod-
ule, which demonstrates the designed two local
CL module works orthogonally.
Answer 4: GL-CLF pulls similar representa-
tions across languages closer. We choose test
set and use representations of [CLS] of each sen-
tence for visualization. Figure 5 (a, LHS) shows the
t-SNE visualization of the mBERT output, where
we observe that there very little overlap between
different languages, which shows that the distance
of the representations of different languages are
distant. In contrast, the GL-CLFrepresentations
(b, RHS) ﬁne-tuned model in different languages
are closer and largely overlap with each other. The
stark contrast between the ﬁgures demonstrates2682
thatGL-CLFsuccessfully aligns representations
of different languages.
Answer 5: Contributions from contrastive
learning and pre-trained model use are comple-
mentary. To verify the contribution from GL-
CLFis still effective when used in conjunction
with other strong pre-trained models, we perform
experiments with XLM-R (Conneau et al., 2020).
XLM-R demonstrates signiﬁcant gains for a wide
range of cross-lingual tasks. From the results in
Table 2, we ﬁnd GL-CLFenhances XLM-R ’s per-
formance, demonstrating that contributions from
the two are complementary. This also indicates
thatCL-CLFis model-agnostic, hinting that GL-
CLFmay be applied to other pre-trained models.
Answer 6: GL-CLF still obtains gains over
BiLSTM. A natural question that arises is
whether GL-CLFis effective for non pre-trained
models, in addition to transformers. To answer
the question, we replace mBERT withBiLSTM ,
keeping other components unchanged. The results
are shown in Table 2. We can see that GL-CLF
outperforms BiLSTM in all metrics, further demon-
strating that GL-CLFis not only effective over
mBERT but also ports to general encoders for both
pre-trained models and non pre-trained models.
Answer 7: GL-CLF is robust. It is worth not-
ing that words in the source language can have
multiple translations in the target language. We
follow Qin et al. (2020) to randomly choose any of
the multiple translations as the replacement target
language word. Their work veriﬁed that random
selection effective method (Qin et al., 2020). A
natural question that arises is whether GL-CLF
is robust over different translation selections. To
answer the question, we choose 15 different seeds
to perform experiment and obtain the standard devi-
ation, which we take as an indicator of the stability
and robustness of models’ performance. Results2683
in Figure 6 shows a lower standard deviation on
each metric, indicating our model is robust to dif-
ferent translation. Finding and using the absolutely
correct contextual word-to-word translation is an
interesting direction to be explored in the future.
5 Related Work
Traditional Spoken Language Understanding.
Since slot ﬁlling and intent detection are two corre-
lated tasks, traditional SLU approaches mainly ex-
plore a joint model for capturing shared knowledge
across the two tasks. Speciﬁcally, Zhang and Wang
(2016); Liu and Lane (2016a,b); Hakkani-Tür et al.
(2016) consider an implicit joint mechanism using
a multi-task framework by sharing an encoder for
both tasks. Goo et al. (2018); Li et al. (2018); Qin
et al. (2019) consider explicitly leveraging intent
detection information to guide slot ﬁlling. Wang
et al. (2018); E et al. (2019); Zhang et al. (2020);
Qin et al. (2021a) use a bi-directional connection
between slot ﬁlling and intent detection.
Zero-shot Cross-lingual Spoken Language Un-
derstanding. Traditional SLU has largely been
limited to high-resource languages. To solve this
problem, zero-shot cross-lingual SLU has gained
increasing attention. Recently, cross-lingual con-
textualized embeddings have achieved promisingresults (e.g., mBERT (Devlin et al., 2019)). Many
works target improving mBERT at the pre-training
stage (Conneau and Lample, 2019; Huang et al.,
2019; Yang et al., 2020; Feng et al., 2020; Conneau
et al., 2020; Xue et al., 2021; Chi et al., 2021a,b).
Compared with their work, our focus is on enhanc-
ing mBERT at the ﬁne-tuning stage.
In recent years, related work also considers align-
ing representations between source and target lan-
guages during ﬁne-tuning, eschewing the need for
an extra pre-training process. Speciﬁcally, Liu
et al. (2020) propose code-mixing to construct
training sentences that consist of both source and
target phrases for implicitly ﬁne-tuning mBERT.
Qin et al. (2020) further propose a multi-lingual
code-switching data augmentation to better align a
source language and all target languages. In con-
trast to their work, our framework consider aligning
similar representation across languages explicitly
via a contrastive learning framework. In addition,
inGL-CLF, we propose a multi-resolution loss
to encourage ﬁne-grained knowledge transfer for
token-level slot ﬁlling.
Contrastive Learning. Contrastive learning is
now commonplace in NLP tasks. Wu et al. (2020)
adopt multiple sentence-level augmentation strate-
gies to learn a noise-invariant sentence representa-
tion. Fang and Xie (2020) apply back translation
to create augmentations of original sentences for
training transformer models. Wang et al. (2021)
propose contrastive learning with semantically neg-
ative examples (CLINE) to improve the robustness
under semantically adversarial attack. Inspired by
the success of CL, we utilize contrastive learning
to explicitly align similar representations across
source language and target language.
6 Conclusion
We introduced a global–local contrastive learning
(CL) framework ( GL-CLF) to explicitly align rep-
resentations across languages for zero-shot cross-
lingual SLU. Besides, the proposed Local CL
module and Global CL module achieves to learn
different granularity alignment (i.e., sentence-level
local intent alignment, token-level local slot align-
ment, semantic-level global intent-slot alignment).
Experiments on MultiATIS++ show that GL-CLF
obtains best performance and extensive analysis in-
dicate GL-CLFsuccessfully pulls closer the rep-
resentations of similar sentence across languages.26847 Ethical Considerations
Spoken language understanding (SLU) is a core
component in task-oriented dialogue system, which
becomes sufﬁciently effective to be deployed in
practice. Recently, SLU has achieved remarkable
success, due to the evolution of pre-trained models.
However, most SLU works and applications are
English-centric, which makes it hard to generalize
to other languages without annotated data. Our
work focuses on improving zero-shot cross-lingual
SLU model that do not need any labeled data for
target languages, which potentially is able to build
multilingual SLU models and further promotes the
globalization of task-oriented dialog systems.
Acknowledgements
We also thank all anonymous reviewers for their
constructive comments. This work was supported
by the National Key R&D Program of China via
grant 2020AAA0106501 and the National Natural
Science Foundation of China (NSFC) via grant
61976072 and 62176078.
References26852686