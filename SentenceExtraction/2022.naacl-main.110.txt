
Giscard BiambyGrace LuoTrevor Darrell Anna Rohrbach
University of California, Berkeley
Abstract
Detecting out-of-context media, such as “mis-
captioned” images on Twitter, is a relevant
problem, especially in domains of high pub-
lic significance. In this work we aim to develop
defenses against such misinformation for the
topics of Climate Change, COVID-19, and Mil-
itary Vehicles. We first present a large-scale
multimodal dataset with over 884k tweets rel-
evant to these topics. Next, we propose a de-
tection method, based on the state-of-the-art
CLIP model, that leverages automatically gen-
erated hard image-text mismatches. While this
approach works well on our automatically con-
structed out-of-context tweets, we aim to vali-
date its usefulness on data representative of the
real world. Thus, we test it on a set of human-
generated fakes created by mimicking in-the-
wild misinformation. We achieve an 11% de-
tection improvement in a high precision regime
over a strong baseline. Finally, we share in-
sights about our best model design and analyze
the challenges of this emerging threat.
1 Introduction
Out-of-context images are a popular form of mis-
information where an image is miscaptioned to
support a false claim (Fazio, 2020). Such image re-
purposing is extremely cheap yet can be as damag-
ing as more sophisticated fake media. In this work
we focus on domains important for society and na-
tional security, where implications of inexpensive
yet effective misinformation can be immense.
Specifically, we analyze multimodal Twitter
posts that are of significant public interest, re-
lated to topics of COVID-19, Climate Change and
Military Vehicles. Our goal is to learn to cat-
egorize such image-text posts as pristine or fal-
sified (out-of-context) by means of detecting se-
mantic inconsistencies between images and text.
To that end, we first collect a large-scale dataset
ofmultimodal tweets, Twitter-COMMs , withover 884k tweets. In our approach, we fuse
input image and text embeddings generated by
CLIP (Radford et al., 2021) via an elementwise
product, and train a classifier to distinguish real
tweets from automatically constructed random and
hard mismatches. To validate this approach and
demonstrate the usefulness of the Twitter-COMMs
dataset, we report results on human-generated
test data, created to mimic real-world misinfor-
mation. We discuss the results and model abla-
tions, and provide additional insights into the chal-
lenges of this task. Our dataset is publicly avail-
able at: https://github.com/GiscardBiamby/Twitter-
COMMs.
2 Related Work
There exist a number of large-scale Twitter datasets
concentrated on topics such as COVID-19 (Banda
et al., 2021) or Climate Change (Littman and
Wrubel, 2019). However, it remains difficult to
collect labeled misinformation. Researchers have
collected COVID-19 misconceptions on social me-
dia via manual annotation (Hossain et al., 2020) or
by linking to fact checking articles (Patwa et al.,
2021). Not only are these datasets small (a few
thousand samples), but they focus on false claims
rather than multimodal inconsistency. Here, we
curate social media posts that are topical and multi-
modal, and we demonstrate an application to mis-
information detection of human-generated fakes.
Recent work has developed approaches for multi-
modal fact checking, e.g., Jaiswal et al. (2017) and
Müller-Budack et al. (2020), who query an external
knowledge base. Similar to Luo et al. (2021) in the
news domain, we use a large pretrained model that
does not require an external reference set.
3 Twitter-COMMs Dataset
Here, we describe the data collection strategies
behind Twitter-COMMs , which consists of mul-1530
timodal tweets covering the topics of COVID-19,
Climate Change, and Military Vehicles.
Data Collection: We collected data using Twit-
ter API v2in three stages for COVID-19 and Cli-
mate Change, and two stages for Military Vehicles,
refining the filters at each stage to acquire more
relevant tweets. COVID-19 and Climate Change
stages progressed from simple high level keywords
towards more specific ones in stage two and tweets
authored by news organizations in the final stage.
For Military Vehicles the first stage used high level
search terms such as “military”, “aircraft”, “tank”,
which resulted in noisy data, so the second stage
used a large number of highly specific terms related
to vehicle models. Full details can be found in Ap-
pendix A.1. We employed the following global
filters for all topics: (1) language=English, (2) has
at least one image, and (3) not a retweet.
In total, we have collected 884,331tweets, each
having at least one image (composed of 24% Cli-
mate Change, 64.5% COVID-19, and 11.5% Mili-
tary Vehicles tweets), see Table 1. Tweets for Cli-
mate Change and Military Vehicles were collected
starting from June 2016 and for COVID-19 starting
from February 2020, all ending in September 2021.
Falsified Samples: In addition to the pristine
samples, we automatically generate falsified sam-
ples where there is some inconsistency between
image and text. We create random negatives (de-
noted as “Random”) by selecting an image for a
given caption at random. We also create hard nega-
tives (denoted as “Hard”) by retrieving the image
of the sample with the greatest textual similarity for
a given caption (following the “Semantics / CLIPText-Text” split from Luo et al. (2021)). We mainly
generate mismatches within each topic (COVID-19,
Climate Change, Military Vehicles), except for a
small set of random mismatches across topics (de-
noted as “Cross Topic”). Our dataset is balanced
with respect to labels, where half of the samples
are pristine and half are falsified. Table 1 presents
summary statistics for the training samples. We
detail our development set and other data used for
evaluation in the next section.
Qualitative Analysis: We present random ex-
amples from our training set in Figure 1. Overall,
we see that the collected Twitter samples tend to
be “on topic” and the amount of noise is low. Hard
negatives are often visually grounded, while ran-
dom negatives contain image/text pairs that are
only weakly related, since they pertain to the same
topic. The Climate Change hard negative depicts
an image of flooded homes to represent “droughts,
fires and floods” while the random negative depicts
an image of cars relevant to climate but inconsistent
with “polar bears”. The COVID-19 hard negative
uses an image of a Nigerian spokesman to depict
news pertaining to “ECOWAS” while the random
one uses a stock photo of lab testing to represent
Covid. These entity-level, rather than topic-level,
alignments more closely resemble real-world out-
of-context images that often reference and misrep-
resent visually depicted entities. Note the diver-
sity of images and text in our training set, where
there exist both natural images and info-graphics,
and language varies from organizational announce-
ments and news headlines to personal opinions.
4 Experiments
Next, we discuss the data used for evaluation,
present our approach and ablate various design
choices, report results on our evaluation sets, and
provide additional analysis of the task difficulty.
4.1 Evaluation Sets
We report results on three evaluation sets. (a) We
validate our approach on samples synthetically gen-
erated using the same procedure as our training
set (denoted Dev), where all topics and falsifica-
tion methods are equally represented (i.e., the ra-
tio of random vs. hard negatives is 50-50). We
also evaluate on human-curated samples from the1531
DARPA Semantic Forensics (SemaFor) Program
derived from (b) news images and captions (de-
noted hNews) and (c) Twitter (denoted hTwitter).
To generate this data, humans manually introduced
inconsistencies to pristine image-caption pairs.
While hNews/hTwitter data is not realmisinforma-
tion, it is in-the-wild w.r.t. our synthetic training
data and much more representative of real-world
human-generated misinformation. All three eval-
uation sets contain a mixture of samples relevant
to the topics of COVID-19, Climate Change, and
Military Vehicles (Figure 2). Table 2 provides the
number of samples in each set. While the hNewsset is available to us, the hTwitter set is hidden.
4.2 Approach and Design Choices
For our approach we fine-tune CLIP (Radford
et al., 2021), a large pretrained multimodal model
that maps images and text into a joint embedding
space via contrastive learning. Our model gener-
ates CLIP embeddings using the RN50x16 back-
bone, multiplies the image and text embeddings,
and passes the result to a classifier that scores the
pair as pristine or falsified. We use a learning rate
of 5e-08 for CLIP and 5e-05 for the classifier and1532
train for 16 epochs. For our baseline CLIP Zero
Shot model, we generate CLIP embeddings of-the-
shelf and compute a dot product, which is used to
score the pair. For more details, see the Appendix.
We report metrics for varying thresholds over the
predicted scores; in most tables we report balanced
classification accuracy at equal error rate (Acc @
EER). We also report falsified class accuracy at two
thresholds (pD @ 0.1 FAR and pD @ EER).
Multimodal Fusion: First, we compare differ-
ent multimodal fusion techniques, see Table 3. We
try three fusion methods: concatenating the CLIP
image and text embeddings (Concat), concatenat-
ing the embeddings and their dot product (Concat
+ Dot), and multiplying the embeddings element-
wise (Multiply). Inspired by how CLIP was trained
to maximize the dot product of normalized image-
text pairs, Concat + Dot and Multiply incentivize
the classifier to stay faithful to the pre-initialized
joint embedding space. These architecture choices
yield on average a 7% performance improvement
over simple concatenation. For future experiments
we choose the Multiply method to minimize train-
able parameters and maintain a simple approach.
Percentage of Hard Negatives: Next, we an-
alyze the importance of using hard negatives in
our training data. Specifically, we measure the im-
pact of different percentages of hard negative sam-
ples, where the rest are random negatives. Table 4presents the results. More hard negatives in train-
ing generally improves the performance on hard
negatives in our development set, but there is also
a trade-off in performance on random negatives.
Given that we care about samples that more closely
mimic challenging real-world misinformation but
also want to avoid degrading performance on easy
samples, we opt for a ratio of 75% hard and 25%
random negatives for future experiments.
4.3 Results and Analysis
Results on hNews, hTwitter Sets: Our final model
was directly fine-tuned on the entire training set
of over 2M training samples, with a ratio of 75%
hard and 25% random negatives. We report results
in Table 5, comparing to CLIP Zero Shot. We
improve by 11% in pD @ 0.1FAR, meaning that
our method is able to detect more falsified samples
with minimal false alarms. At equal error rate we
improve by 5% in both detection and accuracy. We
emphasize that the hTwitter data is unseen to us.
Next, we analyze the performance of our final
model w.r.t. several characteristics on our Dev set.
OCR Coverage: Given that text present in
images can often be used to corroborate captions,
we break down model performance by the amount
of text detected by an English OCR model. In1533
Table 6 (top), we report results broken down by
the % of the image covered by text (the area of the
union of text detections divided by the image size).
Each bucket roughly corresponds to natural im-
ages, natural images with scene text, graphics, and
screenshots of text. The presence of any text yields
more than a 6% improvement for pD @ 0.1FAR
and performance peaks at 10-50% coverage.
Text-Image Relationship: Within social media,
there exist more complex interactions than the di-
rect relationships seen in formats like image alt-text.
As such, we trained a CLIP model on the dataset
presented by (Vempala and Preo¸ tiuc-Pietro, 2019)
to characterize these relationships: classifying if
the image content adds additional meaning (image
adds / does not add) or if there is semantic overlap
between the text and image (text represented / notrepresented).As observed in Table 6 (middle), for
samples with text represented model performance
improves by 8% and for samples where image
adds performance improves by 4% for detection in
a high precision regime (pD @ 0.1FAR). Although
the text-image relationship model has somewhat
noisy classifications for the text task, the text rep-
resented class generally contains samples with a
shared entity between image and text, which would
make fine-grained misinformation detection eas-
ier. The image adds class mostly contains info-
graphics, likely due to training data bias, which
aligns with the OCR coverage experiments above.
Tweet Text Clustering : Finally, we analyze the
sub-topics obtained as a result of clustering Tweets
within each topic. This allows us to tease out clus-
ters, e.g., vaccination for COVID-19, floods for
Climate Change or drones for Military Vehicles.
Recall that our model performs the best on Climate
Change and the worst on the Military Vehicles (Ta-
ble 4). Possible factors include the smaller amount
of training data and visual similarity of different
vehicle types. We also observe that among the hard
negatives for Military Vehicles, only 39% are cross-
cluster (while Climate Change and COVID-19 have
51% and 58% respectively), indicating the Military
Vehicles set contains a larger proportion of harder
fakes. These factors may explain the larger differ-
ence between cross/within cluster performance for
this topic (Table 6, bottom).
5 Conclusion
In this work we tackle a real-world challenge
of detecting out-of-context image-text tweets on
COVID-19, Climate Change, Military Vehicles top-
ics. To approach it, we collect Twitter-COMMs , a
large-scale topical dataset with multimodal tweets,
and construct corresponding hard mismatches. We
design our approach based on the CLIP model with
several important design choices, e.g. multiplying
the embeddings for multimodal fusion and increas-
ing the percentage of hard negatives in our training
data. This approach substantially improves over
a powerful baseline, an off-the-shelf CLIP model,
when evaluated on human-curated in-the-wild mis-
matches. We hope our work and insights will bene-
fit multimedia forensics practitioners.15346 Ethical Considerations
Here, we discuss ethical considerations regarding
our work. Image repurposing is a prominent so-
cietal issue that lacks sufficient training data in
general, and in particular for content on social me-
dia platforms such as Twitter. Even more, our work
aims to be proactive in studying the threat of out-of-
context media and proposes an approach for detect-
ing such misinformation. By presenting a dataset,
a detection approach, and several key observations
about falsified out-of-context Tweets, we hope that
our work serves as a net benefit for society.
How was the data collected? We collected data
using the Twitter Search API v2. Our methodology
is described in detail in Appendix A.1.
What are the intellectual property rights?
Twitter owns the intellectual property for the
Tweets in our Twitter-COMMs dataset. We adhere
to the restrictions they place on Tweets downloaded
via their API, namely that we may not share the
content downloaded from the API, but we have re-
leased the Tweet ID’s — which others can use to
download the Tweets and images from the API.
How did we address participant privacy
rights? N/A
Were annnotators treated fairly? Did we re-
quire review from a review board? N/A
Which populations do we expect our dataset
to work for? Our dataset is specific to social media
posts from Twitter that are written in English; it
will primarily be useful for audiences from English
speaking countries, such as the US, UK, Australia,
and India. The biases inherent to the short text style
(280 characters or less) and of Tweets with images
will be useful for those interested in researching
multimodal misinformation on Twitter.
What is the generalizability of our claims?
Our results apply primarily to Tweets on our three
topics of interest (COVID-19, Climate Change,
Military Vehicles) written in English and having at
least one attached image.
How did we ensure dataset quality? Our data
collection methodology is described in detail in
Appendix A.1. To address data quality for Military
Vehicles we created an image classifier to filter
out tweets that did not have images of military
vehicles or aircraft (Appendix A.1.2). Additionally,
the sub-topic clustering we performed (Section 4.3,
Appendix A.3.4) reveals that most of the text falls
into clusters that are related to the three main topics.
We also provide some statistics for tweets withpossibly sensitive content as flagged by Twitter in
Table 14 (Appendix).
What is the climate impact? Our final model
used 8 days of training on 10 GPUs. Additional
experiments such as the investigation of text im-
age relationships used 4 days on a single GPU,
and tweet text clustering used 10 hours on a sin-
gle GPU. The GPU used for all experiments were
GeForce 2080 RTX Ti’s. In total we used 2,026
GPU hours, and total emissions are estimated to
be 218.81 kgCOeq of which 0 percents were di-
rectly offset. Estimations were conducted using the
MachineLearning Impact calculator presented in
(Lacoste et al., 2019).
What are the potential dataset biases? Here,
we focus on our method used to generate hard fal-
sified samples to understand the potential biases
learned during training. Specifically, we note poten-
tial age, race, and gender biases present in CLIP, the
underlying model used to generate our mismatches.
Radford et al. (2021) find the CLIP exhibits sig-
nificant performance differences when classifying
individuals of different races and ages into cate-
gories related to crime or animals. Agarwal et al.
(2021) also find gender biases in the CLIP embed-
dings when classifying occupations. These biases
primarily affect the synthetically generated training
set, not the pristine data. However, we can not rule
out that the pristine Twitter data may also capture
some human biases or harmful stereotypes.
7 Acknowledgements
We would like to thank PAR Tech, Syracuse Uni-
versity, and the University of Colorado, Denver
for creating the evaluation data. We thank the SRI
team, including John Cadigan and Martin Gracia-
rena, for providing the WikiData-sourced news or-
ganization Twitter handles. We would also like to
thank Dong Huk (Seth) Park, Sanjay Subramanian,
and Reuben Tan for helpful discussions on fine-
tuning CLIP. This work was supported in part by
DoD including DARPA’s LwLL, and/or SemaFor
programs, and Berkeley Artificial Intelligence Re-
search (BAIR) industrial alliance programs.
References15351536A Appendix
In Section A.1 we provide additional details about data collection, including our strategy and search
keywords. Section A.2 provides dataset statistics, including information on tweet counts, geographical
information, possibly sensitive content, and image availability. We include additional experiments in
Section A.3.
A.1 Data Collection
A.1.1 COVID-19 and Climate Change
Our data collection consisted of three stages. The first employed simple topic, keyword, and hashtag
filters, the second stage used more specific keyword and topic combinations, while the third focused on
collecting topical data from Twitter accounts of various news organizations.
In the first stage we collected roughly 100,000 tweets each for COVID-19 and Climate Change topics.
We used the “COVID-19” topic of the Twitter API’s Entity Annotations feature, which allows users
to find tweets related to predetermined topics. For Climate Change we filtered with an OR clause
on keywords “climate change”, “global warming”, and (#globalwarming, #climatechange) hashtags.
Inspection of the stage 1results revealed a lot of off-topic tweets. For example, a Twitter user might
post a tweet about working from home during the pandemic and tag the tweet with a COVID-related
hashtag. While this type of content is somewhat related to COVID-19, we wanted to focus on data where
misinformation/disinformation might be more relevant, such as more topical/newsworthy tweets (e.g. bad
actors may spread propaganda related to the COVID-19 pandemic by making false or misleading claims).
To that end, in stage 2we filtered by combining each topic phrase with one of the 19 topical search terms
(e.g. “agriculture”, “crops”, “death”, “vaccination”). The resulting data appeared much more relevant
than the initial collection effort. Table 7 contains a list of the search terms we used to collect data for
COVID-19 and Climate Change tweets. Finally, related to the argument above, in the third collection
stage we focused on tweets authored by news organizations, as opposed to random users. For that, 7k
news organization Twitter handles were sourced from WikiData.
A.1.2 Military Vehicles
Collecting data about the Military Vehicles topic proved more challenging than the other two topics. We
initially tried simple keyword filters such as “military”, “aircraft”, “tank”, etc, but found that those resulted
in a lot of irrelevant content such as tweets related to video games, or tweets where “tank” took a different
meaning (e.g., “fish tank” or “tank tops”). This initial approach did not return many relevant results. The
WikiData news organization approach used in the other two topics also did not provide enough usable
data. As a result we crafted two different, highly customized stages for Military Vehicles. We gathered
a list of both civilian and military vehicles and aircraft from eight different publicly available datasets
(see Table 8). The datasets were annotated either for image classification or object detection tasks. We
queried the Twitter Search API using the vehicle and aircraft names from this set, but returned a lot of
off-topic data. We then trained an EfficientNet (Tan and Le, 2019) image classifier that categorized images
as either civilian ground vehicle, civilian aircraft, military ground vehicle, military aircraft, or other. (The
“other” category training set consisted of several thousand manually annotated images from the initial data
collection effort that did not contain any military or civilian vehicles or aircraft.) We trained the classifier1537to 97% accuracy and used it to filter out any tweets predicted to be in the “other” category. For the second
collection stage we combined the military vehicle and aircraft names with custom keywords (Table 9).
A.2 Dataset Statistics
Table 10 shows a summary of the dataset. The “Geo-tagged” column refers to the geolocation data
provided by tweet authors. This property is empty in most cases, but when present, can be in the form of
a Twitter “place” which contains a display name, a geo polygon (which in some cases is as broad as an
entire country), as well as other fields, such as country name. It is also possible for the geo data to be in
the form of latitude and longitude, but that is rarer. The “Countries” columns is extracted from the geo
location data and because of the small amount of geo-tagged tweets we can only report countries for a
small fraction of tweets in the dataset (Table 11).
One oddity to note is that although we included an English-only search filter (“lang:en”) in all API
calls, the API still returned a small number of non-English tweets (Table 12). We are not sure why this is,
but manual inspection of some of these examples shows that a good portion of them are in fact in English.Figure 3 shows high-level “word cloud” summaries for the hashtags in the tweets for each topic.
Table 14 shows the number of tweets that Twitter flagged as containing possibly sensitive material,
i.e., samples that may contain adult content or graphic violence. We encourage users to be aware of such
tweets, which account for about 1% of the data, and may be undesirable for certain downstream tasks.1538
The total number of images/tweets is shown in Table 15. Twitter allows users to include 1-4 images in
a tweet. As seen in Table 16, 90% of the tweets have a single image. In cases where a tweet contained1539more than one image, we only used the first image (according to the order of the images returned by the
Twitter API).
A.3 Additional Experiments
All experiments reported in this paper are for a single run, as we find that variance across multiple runs
is low. All ROC curves and metrics are computed using sklearn’s roc_curve function. All models are
implemented in PyTorch. For our experiments, we make the following design choices:
•We use the RN50x16 backbone. We find that this backbone consistently yields a 2-3% improvement
compared to other released backbones, such as ViT/B-32. Our final CLIP model contains ∼300M
parameters initialized from the RN50x16 backbone and ∼600k parameters randomly initialized for
our classifier.
•We tune the upper layers and keep CLIP’s lower layers frozen. We find that this scheme is more
memory efficient and yields more stable convergence than tuning all the layers.
•We use a learning rate of 5e-08 for CLIP and 5e-05 for the classifier. From our hyperparameter
sweeps we find this setting to be the most appropriate, as CLIP is pretrained while the classifier is
randomly initialized.
•We multiply CLIP image and text embeddings before passing that as an input to the classifier. This is
different from Luo et al. (2021), who used a simple feature concatenation.
A.3.1 Expert vs. Joint Training
Here we study whether training a joint model on all three topics at once may be inferior to training three
topic-specific experts, see Figure 4. We find that the joint model performs on par with or better than the
expert models, thus we use a joint model in all the other experiments.
A.3.2 Fine-Tuning Scheme
Since we only know the high-level topics but not the precise composition of samples in our hidden
set hTwitter, we investigate methods for out-of-domain robustness. Specifically, we try the scheme from
(Kumar et al., 2022), where the authors first optimize the classifier while keeping the pretrained feature
extractor frozen (linear probing), then optimize the entire network (fine-tuning). The intuition behind
this method is that a good initialization from linear probing minimizes the chance of feature distortion,
i.e. when the pretrained model overfits to in-domain data. We report the results in Table 17. In fact, we
find that direct fine-tuning (FT) achieves slightly better performance on both in-domain Twitter data and
out-of-domain news data (hNews). Thus, in other experiments we use direct fine-tuning.1540
A.3.3 Training Set Size
We also investigate the influence of training set size on performance. We report the binary classification
accuracy as we use 500k, 1M, and 2M samples, as seen in Table 18. We observe that increasing training
data size generally leads to improved performance, with most of the gains coming from higher accuracy
on hard negatives.
A.3.4 Tweet Text Clustering
We investigate the sub-topical clusters of the tweet text, and also evaluate the performance of the final fine-
tuned model in terms of how well it performs on a set of the hard falsified samples and their corresponding
pristine samples.
We use the method of (Grootendorst, 2020) to generate clusters, which entails computing SentenceBERT
(Reimers and Gurevych, 2019) embeddings for each Tweet text, using UMAP (McInnes et al., 2020) to
reduce the number of embedding dimensions from 768 to 20, and then running the HDBSCAN hierarchical
clustering algorithm (McInnes and Healy, 2017) on the UMAP output. We compute the ten most important
words for each cluster using the TF-IDF scores, and use this word list to gain insight into the concepts
present in the texts of each cluster.
For UMAP we use the 10 nearest neighbors. For Climate Change HDBSCAN hyperparameters are:
minimum topic size=400, and a cluster selection distance threshold = 0.56. For COVID-19 HDBSCAN:
minimum topic size=1200, and cluster selection distance threshold = 0.65. For Military Vehicles HDB-
SCAN: minimum topic size = 100, cluster selection distance threshold = 0.60. The cluster selection size
setting determines when clusters are merged, clusters within a smaller distance than this threshold setting
will get merged together (see HDBSCAN( ˆϵ) parameter in section IV of (Malzer and Baum, 2020)).
As discussed in the main paper, we are interested in analyzing model performance on within-cluster
vs. cross-cluster hard samples. First, the training data statistics per topic are presented in Table 19. Next,
Figure 6 shows the ROC curves for the within-cluster and cross-cluster samples.1541
To gain insight into the sub-topics, we concatenate the 3top scoring words from each cluster to obtain
the cluster “names”, as seen in the Tables 20, 21, 22 with cluster names and word scores. We get between
20 and 30 clusters for each topic. We observe such sub-topics as ocean-sea-flood-flooding, plastic-
recycling-recycle-sustainability for Climate Change, vaccine-vaccination-clinic-appointment, school-
student-education-university for COVID-19, tank-abrams-army-m1, drone-ai-uav-drones for Military
Vehicles. The hierarchy visualizations in Figures 7, 8, 9 provide further insight into the sub-topic structure.15421543154415451546154715481549