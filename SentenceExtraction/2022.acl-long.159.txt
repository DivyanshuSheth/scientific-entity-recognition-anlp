
Xu Han, Yuqi Luo, Weize Chen, Zhiyuan Liu
Maosong Sun, Botong Zhou, Fei Hao, Suncong ZhengDept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University
Beijing National Research Center for Information Science and TechnologyInstitute Guo Qiang, Tsinghua UniversityInternational Innovation Center of Tsinghua UniversityBeijing Academy of Artificial Intelligence, BAAITencent AI Platform Department, Tencent Inc
{hanxu17,yq-luo19,chenwz21}@mails.tsinghua.edu.cn
{liuzy,sms}@tsinghua.edu.cn
Abstract
Fine-grained entity typing (FGET) aims to clas-
sify named entity mentions into fine-grained
entity types, which is meaningful for entity-
related NLP tasks. For FGET, a key chal-
lenge is the low-resource problem — the com-
plex entity type hierarchy makes it difficult
to manually label data. Especially for those
languages other than English, human-labeled
data is extremely scarce. In this paper, we pro-
pose a cross-lingual contrastive learning frame-
work to learn FGET models for low-resource
languages. Specifically, we use multi-lingual
pre-trained language models (PLMs) as the
backbone to transfer the typing knowledge
from high-resource languages (such as English)
to low-resource languages (such as Chinese).
Furthermore, we introduce entity-pair-oriented
heuristic rules as well as machine translation to
obtain cross-lingual distantly-supervised data,
and apply cross-lingual contrastive learning on
the distantly-supervised data to enhance the
backbone PLMs. Experimental results show
that by applying our framework, we can easily
learn effective FGET models for low-resource
languages, even without any language-specific
human-labeled data. Our code is also available
at https://github.com/thunlp/CrossET.
1 Introduction
Recently, various efforts have been devoted to ex-
ploring fine-grained entity typing (FGET) (Ling
and Weld, 2012; Li et al., 2020), aiming to iden-
tify concrete fine-grained entity types for named
entity mentions in sentences (Figure 1). Since the
type information of named entity mentions is use-
ful for understanding textual semantics, FGET is
widely applied to enhance entity-related tasks, suchFigure 1: The illustration of classifying named entity
mentions in sentences into fine-grained entity types.
as coreference resolution (Khosla and Rose, 2020),
entity linking (Onoe and Durrett, 2020; Chen et al.,
2020a), relation extraction (Ren et al., 2017; Zhou
and Chen, 2021) and event extraction (Nguyen
et al., 2016; Yang and Mitchell, 2016).
Despite the success of FGET, the low-resource
problem is always a challenge of FGET, since the
complex type hierarchy makes it difficult to man-
ually label data. To alleviate the low-resource
problem, besides utilizing auto-labeled data (Ling
and Weld, 2012; Gillick et al., 2014; Xin et al.,
2018; Dai et al., 2021), manually building FGET
datasets is the most effective approach (Sang and
De Meulder, 2003; Hovy et al., 2006; Ling and
Weld, 2012; Choi et al., 2018; Ding et al., 2021).
However, existing FGET datasets are mainly in
English. For datasets in specific languages other
than English, such as Chinese (Lee et al., 2020),
Japanese (Suzuki et al., 2016), Dutch and Span-
ish (van Erp and V ossen, 2017), their scale and qual-
ity are not comparable to those English datasets. In
this paper, we introduce a cross-lingual framework
to learn FGET models for low-resource languages,
via utilizing the data in high-resource languages2241(e.g. utilizing English datasets).
Transferring the typing knowledge from high-
resource languages to low-resource languages is
not easy. As different languages have quite dif-
ferent patterns, it is challenging to understand the
semantics of both high-resource and low-resource
languages at the same time. With only a few ex-
amples of low-resource languages and no parallel
data, it is also hard to bridge different languages
for knowledge transfer. To handle these issues: (1)
we use multi-lingual pre-trained language models
(PLMs) as backbone. Multi-lingual PLMs such
as M-BERT (Devlin et al., 2019) are pre-trained
on large-scale multi-lingual corpora, taking it as
the backbone can well encode data in different lan-
guages into the same semantic space (Han et al.,
2021). (2) we apply heuristic rules and cross-
lingual contrastive learning to bridge multiple lan-
guages. We design several entity-pair-oriented
heuristic rules to obtain distant supervision, which
can automatically annotate entity types by utiliz-
ing latent relations between entity pairs. Machine
translation is used on the auto-labeled data to estab-
lish a connection between high-resource and low-
resource languages. Finally, we apply contrastive
learning to learn similarities between cross-lingual
auto-labeled types, instead of using pseudo-labels
to learn a classifier, which can enhance the type
recognition ability and reduce the side effect of
auto-labeled data.
For convenience, we name our cross-lingual con-
trastive learning framework “ C -C” in the fol-
lowing sections. We conduct experiments on two
popular FGET datasets: Open-Entity (Choi et al.,
2018) and Few-NERD (Ding et al., 2021), and
translate their test sets into non-English versions
to evaluate the effectiveness of C -Cfor low-
resource languages. Quantitative experimental re-
sults show that applying C -Ccan easily train
effective FGET models for low-resource languages,
even without any language-specific human-labeled
data. Besides quantitative experiments, we also pro-
vide some visualization of feature spaces and con-
duct case studies for qualitative analysis to show
how C -C works.
2 Method
In this section, we will introduce our cross-lingual
framework to learn FGET models for low-resource
languages. We will first give some essential no-
tations and definitions, and then elaborate on thedetails of our framework.
2.1 Notations and Definitions
As shown in Figure 1, given a sentence xand one
named entity mention min the sentence, our goal
is to determine types from a fine-grained type set T
according to the sentence context for the mention
m. Note that FGET is a multi-label classification
problem, since multiple types can be assigned to a
single named entity mention.
For a high-resource language h, sufficient
human-labeled data {X,Y}exists, where X=
{x, x, . . .}is the sentence set and Y=
{y, y, . . .}is the label set. Each sentence
x∈ Xcontains a named entity mention m,
andy⊆ T is the fine-grained type set of the
named entity mention m.
Similarly, we define the dataset {X,Y}for a
low-resource language l, where |X| ≪ |X|. In
this paper, we use {X,Y},{X,Y}and large-
scale unlabeled multi-lingual data to train a FGET
model for the low-resource language l.
2.2 Multi-Lingual Pre-Trained Encoder
We use multi-lingual BERT (M-BERT) (Devlin
et al., 2019) as the framework backbone to encode
the input. M-BERT has the same architecture as
BERT, but is pre-trained on the multi-lingual cor-
pora in 104 languages. Therefore, M-BERT has
a good ability to transfer knowledge across lan-
guages (Pires et al., 2019; Selvaraj et al., 2021),
making it suits our setting well. Note that, our
framework does not depend on a specific PLM, any
other multi-lingual PLMs can also be used as the
backbone to encode the input.
Given a sentence x= [w, . . . , m, . . . , w],
where mis the named entity mention, we addition-
ally insert an entity marker [ENT] on each side
of the mention m. By feeding the sentence with
entity markers into M-BERT, we can get representa-
tions [h, . . . , h ,h,h , . . . , h]for
all input tokens. The left entity marker representa-
tionh is used to represent the named entity
mention. For simplicity, we denote this process as
m=M-PLM (x), where mis the entity mention
representation and xis the input sentence. Given
each entity type t∈ T , the probability that the
mention min the sentence xcan be classified as2242
the type tis given as
where σis the sigmoid function, tis the repre-
sentation of the entity type t, and θindicates all
learnable model parameters.
With the data {X,Y}in the high-resource lan-
guage hand the data {X,Y}in the low-resource
language l, the overall optimization objective is as
where L(θ)andL(θ)respectively indicate
the loss functions for the high-resource language
hand the low-resource language l. These loss
functions are defined as
For the function δ, if the condition cis satisfied,
thenδ= 1, otherwise δ= 0.
2.3 Heuristic Rules for Data Augmentation
As we mentioned before, there are only a few
human-labeled examples in low-resource lan-
guages. Although multi-lingual PLMs can pro-
vide an effective backbone to understand multi-
lingual semantics, more examples are still required
to bridge different languages.The existing distantly-supervised methods an-
notate the mentions of the same entity in multiple
sentences with the same pseudo label (Ling and
Weld, 2012; Gillick et al., 2014; Xin et al., 2018).
However, in Figure 2, the mention “Mark Twain”
requires to be annotated with “writer” or “miner”
according to specific semantics. Hence, these
single-entity-oriented heuristic rules inevitably
bring much noise.
To this end, we introduce heuristic rules orient-
ing entity pairs to automatically annotate data with
less noise. Instead of annotating specific entity
types, we annotate whether two named entity men-
tions are of similar types. On the one hand, this
strategy can consider the correlation and similarity
between different types. On the other hand, this
strategy is suitable for contrastive learning, which
can reduce the side effect of data noise. In fact, in
relation extraction, recent works have adopted simi-
lar strategies (Soares et al., 2019; Peng et al., 2020)
and achieved promising results. More specifically,
as shown in Figure 2, we take three rules to obtain
distantly-supervised data:
(1)Rules without knowledge bases . As shown
in Figure 2(a), without using knowledge bases, if
one entity pair is mentioned by two sentences, the
mentions of the same entity in these two sentences
are considered to have similar types.
(2)Rules with knowledge bases . As shown
in Figure 2(b), by using knowledge bases, if en-
tity pairs in two sentences have same relations in
knowledge bases, and these pairs have shared en-
tities, the mentions of corresponding entities are
considered to have similar types.2243(3)Building cross-lingual data with machine
translation . As shown in Figure 2(c), we use ma-
chine translation to translate the data from the high-
resource language to the low-resource language.
Owing to the translation, the above-mentioned auto-
labeled examples and their translated versions con-
stitute a cross-lingual distantly-supervised dataset.
By making full advantage of distant supervision
and machine translation, we can greatly expand our
dataset to bridge high-resource and low-resource
languages, and further transfer the typing knowl-
edge between these languages. To make FGET
models pay more attention to textual contexts rather
than merely focusing on entity names, we use the
[MASK] token to mask named entity mentions
with a probability of 0.5.
2.4 Cross-Lingual Contrastive Learning
With all above-mentioned heuristic rules in Sec-
tion 2.3, we can get the distantly-supervised data
˜X={˜x,˜x, . . .}in the high-resource lan-
guage h, the distantly-supervised data ˜X=
{˜x,˜x, . . .}in the low-resource language l,
and the translated data ˜X={˜x,˜x, . . .}.
Given any two sentences x, xin these distantly-
supervised datasets, we use the function s(x, x)
to measure the similarity between the entity men-
tions of the two sentences. In practice, we take the
cosine similarity with temperature τas the function
s(x, x):
where M-PLM (·)is the entity mention representa-
tion computed by multi-lingual PLMs.
The cross-lingual contrastive learning consists of
two important objectives. One is the mono-lingual
objective for each language, and the other is the
cross-lingual objective. For both the high-resource
language hand the low-resource language l, their
mono-lingual objectives are defined as follows,where P(˜x)⊆˜XandN(˜x)⊆˜Xare re-
spectively the positive set and the negative set of
the example ˜x.P(˜x)andN(˜x)are defined
in a similar way for the example ˜x.
To ensure that the model does not push the repre-
sentations of different languages far away, so that
the low-resource language lcan benefit from the
high-resource language h, we further use ˜Xand
its translated set ˜Xto define the cross-lingual ob-
jective as follows,
where P(˜x)⊆˜X∪˜Xis the positive set of
the example ˜x. The final objective of the cross-
lingual contrastive learning is to optimize
2.5 Pre-Training and Fine-Tuning
We divide the whole learning process into two
stages: pre-training and fine-tuning. The pre-
training stage is to use Eq. (7) to optimize parame-
ters on the distantly-supervised data. Considering
computational efficiency, every time we sample
a batch of examples for contrastive learning, and
then sample multiple positive examples for each
example in the batch. After the pre-training stage,
we use Eq. (2) to fine-tune parameters on human-
labeled data to learn classifiers for FGET.
3 Experiment
In this section, we evaluate the effectiveness of
our framework C -Con two typical entity-
related datasets: Open-Entity and Few-NERD. For
each dataset, we conduct experiments in both low-
resource (few-shot or zero-shot) and full-set set-
tings. In addition to quantitative experiments, to
further show how our method works, we also pro-
vide some visualization of feature spaces for quali-
tative analysis.
3.1 Dataset Settings
Open-Entity (Choi et al., 2018) and Few-
NERD (Ding et al., 2021) are both popular FGET
datasets. Open-Entity includes 9 general types and
121 fine-grained types. Each example in Open-
Entity may correspond to multiple entity types.2244Few-NERD includes 8 general types and 66 fine-
grained types. Both of these two datasets have a
clear type hierarchy, which is suitable for evalu-
ating the model performance on the entity typing
task. In our experiments, we require models to pre-
dict both general types and fine-grained types for
each entity mention in sentences.
3.2 Experimental Settings
In this paper, we select English as a high-resource
language and Chinese as a low-resource language.
We attempt to use human-labeled English data and
large-scale unlabeled multi-lingual data for learn-
ing, to obtain an effective Chinese FGET model.
This is very difficult, since no any Chinese human-
labeled data is used in this process.
To obtain distantly-supervised data, we apply
our heuristic rules to automatically annotate the En-
glish and Chinese Wikipedia pages. We then use
machine translation (Klein et al., 2017; Tan et al.,
2020) to translate the English distant-supervised
examples into corresponding Chinese versions for
cross-lingual contrastive learning.
All test sets of Open-Entity and Few-NERD are
translated into Chinese for evaluation. Although
the test set built by machine translation may exist
some errors, the overall semantics of the translated
examples can still support determining the types
of entity mentions. Taking human-labeled exam-
ples for evaluation is better, yet large-scale human-
annotated entity typing datasets are still lacking.
The experiments are performed under three set-
tings:
Few-shot setting . This setting requires models
to infer entity types with a few supervised examples.
We randomly sample 2, 4, 8, 16 examples for each
entity type for training.
Zero-shot setting . This setting requires models
to infer entity types without any supervised training,
i.e., no human-labeled example is used for training.
Full-set setting . In this setting, all supervised
examples in datasets are used for training.
We follow the widely-used setting of Ling and
Weld (2012), use the loose micro Fscores to eval-
uate the performance of models.
3.3 Baseline Settings
We use M-BERT (Devlin et al., 2019) as the back-
boneto implement all baseline models and ourmodel “C -C”. We use “F-T”to denote di-
rectly using English human-labeled data to fine-
tune M-BERT, which is demonstrated the effective-
ness in Selvaraj et al. (2021). We use “M -
C”to denote only using mono-lingual contrastive
learning objectives for pre-training, and then use
English human-labeled data to fine-tune pre-trained
parameters. All above-mentioned models are opti-
mized by AdamW with the learning rate {5e-6,1e-
5,3e-5,5e-5 }. The batch size used for pre-training
and fine-tuning is from {8,16,32,64,128,256 }. For
cross-lingual contrastive learning, we only traverse
large-scale distantly-supervised data once. For fine-
tuning models on human-labeled data, the epochs
are from {1,3,5,7,10 }. The temperature τused for
the cosine similarity is 0.5.
3.4 The Overall Performance in
Low-Resource Settings
The results of few-shot entity typing for Chinese
are reported in Table 1. The table shows that:
(1) Using a multi-lingual PLM as the backbone
can lead to an effective FGET model for those low-
resource languages. All methods, including both
the baseline models and our C -C, can achieve
non-trivial entity typing results on the Chinese test
sets, without using any Chinese human-labeled ex-
amples for training models.
(2) Using distantly-supervised data for con-
trastive learning can significantly improve the typ-
ing capabilities of the backbone PLMs. Compared
with directly fine-tuning a multi-lingual PLM with
human-labeled data in high-resource languages,
conducting contrastive learning on multi-lingual
distantly-supervised data can better bridge high-
resource languages and low-resource languages,
which is beneficial to obtain effective models in
low-resource languages.
(3) Compared with mono-lingual contrastive
learning, our cross-lingual contrastive learning can
better improve the transfer of typing knowledge
from high-resource languages to low-resource lan-
guages. Our C -C achieves the best results in
all shot settings. And the improvements of C -
Cwill gradually increase as the number of shots
decreases. These results show that our method can
effectively improve model performance for low-
resource languages even without any high-quality
supervised language-specific data.
We also report the entity typing performance on
the original English test sets in Table 2. From the2245
table we can see:
(1) In our low-resource settings, although there
are no human-labeled Chinese data at all, there are
still some high-quality English examples for each
entity type. Therefore, the improvements of con-
trastive learning on the English test sets are not as
obvious as on the Chinese test sets. However, com-
pared with directly fine-tuning PLMs, contrastive
learning methods still bring significant improve-
ments, demonstrating the power of using distant
supervision for data augmentation.
(2) Owing to multi-lingual data, which makes
models in multiple languages learn from each other,
our cross-lingual contrastive learning further bringsadditional improvements over the mono-lingual
contrastive learning. This proves the effectiveness
of our cross-lingual contrastive framework.
Table 3 shows the results of zero-shot entity typ-
ing on the Chinese test sets. In this table, we can
see that: without a trained type classifier, our cross-
lingual contrastive learning still brings the back-
bone PLM a strong type recognition ability in the
pre-training stage.
3.5 The Overall Performance in Full-Set
Settings
We show the model performance curve as the num-
ber of supervised examples increases in Figure 3.
Note that only the supervised examples of the high-
resource language English are used for training
models. There is still no human-labeled data for
the low-resource language Chinese. The results in
the figure show that:
(1) For high-resource languages, by using more
supervised examples, the improvements brought
by contrastive learning are gradually decreasing,
which is in line with our intuition. But we should
also notice that even in the full-set setting, con-
trastive learning methods achieve comparable or
even slightly better results than fine-tuning PLMs.
This means that taking contrastive learning can
well reduce the impact of data noise while enhanc-2246
ing performance by making full use of distantly-
supervised data.
(2) In both low-resource and full-set settings, the
results of our contrastive learning on the Chinese
test sets are always significantly higher than other
baseline models. This shows that our framework
can utilize the supervised data of high-resource
languages and large-scale unlabeled multi-lingual
data to handle FGET for low-resource languages.
3.6 Ablation Experiments and Model
Visualization
In order to show how our C -Cworks more
intuitively, we conduct comprehensive ablation ex-
periments. The results of the ablation experiments
are shown in Table , where “ −cc” means that we
drop the cross-lingual contrastive objective for pre-
training the backbone PLM, “ −zc” means that
we drop the mono-lingual contrastive objective on
the Chinese distantly-supervised data, and “ −ec”
means that we drop the mono-lingual contrastive
objective on the English distantly-supervised data.
From Table , we can find that: both the mono-
lingual contrastive objectives and the cross-lingual
objective play an important role in enhancing the
backbone PLM, and the combination of them can
lead to greater improvements. This is also the rea-
son that our cross-lingual contrastive learning in-
cludes both mono-lingual and cross-lingual con-trastive objectives for pre-training the backbone.
We also give the visualization of the model dur-
ing the ablation experiments of C -Cin Fig-
ure 4. From the visualization results, we can find
that it is difficult to bridge high-resource languages
and low-resource languages without using any con-
trastive learning. As we gradually increase the
number of contrastive learning objectives, the dis-
tinction between entity types becomes more obvi-
ous, and the fusion of multi-lingual semantics also
becomes better.
4 Related Work
As one of the most important tasks in the field
of information extraction, FGET has been stud-
ied for a long time. Ling and Weld (2012); Yosef
et al. (2012) first propose to classify named en-
tity mentions into various fine-grained entity types,
instead of just a few coarse-grained types (Sang
and De Meulder, 2003; Hovy et al., 2006). Since
fine-grained types bring informative semantics for
language understanding, these types are widely
used to enhance entity-related NLP tasks, such as
coreference resolution (Khosla and Rose, 2020),
entity linking (Onoe and Durrett, 2020; Chen et al.,
2020a), relation extraction (Ren et al., 2017; Zhou
and Chen, 2021) and event extraction (Nguyen
et al., 2016; Yang and Mitchell, 2016). Some re-
cent efforts further incorporate entity types to learn2247
entity-enhanced PLMs (Zhang et al., 2019; Sun
et al., 2019; Liu et al., 2020).
Distantly-supervised FGET methods . Since
entity types have complex hierarchies, manually
annotating FGET data is not easy, and thus the low-
resource problem is one of the key challenges of
FGET. To alleviate this issue, distantly-supervised
methods have been widely explored for FGET.
One typical distantly-supervised approach is us-
ing knowledge bases to automatically annotate
entities mentioned in the text. Ling and Weld
(2012); Gillick et al. (2014) collect anchors in
the Wikipedia pages that correspond to entities in
knowledge bases, and then label these anchors with
entity types in knowledge bases. This approach
is then followed by a series of works (Ren et al.,
2017; Xin et al., 2018; Choi et al., 2018; Dai et al.,
2019; Jin et al., 2019; Lee et al., 2020) to obtain
pseudo labels. Other approaches use various noun
phrases in sentences as type pseudo labels (Dai
et al., 2020, 2021), which can make full use of the
recently proposed PLMs for data augmentation.
Human-labeled FGET datasets . In addition to
the distantly-supervised methods, the construction
of FGET datasets is also advancing. CoNLL (Sang
and De Meulder, 2003) and Ontonotes (Hovy et al.,
2006) are the earliest datasets, although they just
cover several coarse-grained types. Then, Ling
and Weld (2012); Gillick et al. (2014); Ding et al.
(2021) introduce about a hundred fine-grained
types and annotate a large number of examples
for each type. Choi et al. (2018) further extend
FGET by introducing an ultra-fine set contain-
ing thousands of types. Since annotating FGETexamples is time-consuming and labor-intensive,
many of the ultra-fine types proposed by Choi et al.
(2018) only have distantly-supervised examples.
However, all these efforts only focus on English.
There are also some efforts to build datasets in
other languages, such as Chinese (Lee et al., 2020),
Japanese (Suzuki et al., 2016), Dutch and Span-
ish (van Erp and V ossen, 2017), but the scale and
quality of these non-English datasets are still not
comparable with English datasets, i.e., non-English
human-labeled data are still scarce.
Cross-lingual and contrastive learning for
FGET . Although cross-lingual learning has been
widely explored in entity linking (Sil et al., 2018;
Upadhyay et al., 2018; Rijhwani et al., 2019) and
named entity recognition (Pan et al., 2017; Xie
et al., 2018; Rahimi et al., 2019; Zhou et al., 2019),
cross-lingual entity typing has not yet been ex-
plored much (Selvaraj et al., 2021). For contrastive
learning (Chen et al., 2020b; Oord et al., 2018),
some preliminary works have explored it for ex-
tracting relations between entities (Soares et al.,
2019) and achieved promising results. Peng et al.
(2020) further use contrastive learning to analyze
the impact of entity information on relation extrac-
tion. Similar to cross-lingual learning, the explo-
ration of contrastive learning for FGET is still in
the preliminary stage.
5 Conclusion and Future Work
In this paper, to learn effective FGET models for
those low-resource languages, we propose an ef-
fective cross-lingual contrastive learning frame-
work C -Cto transfer the typing knowledge
from high-resource languages to low-resource lan-
guages. Specifically, the framework C -C
uses a multi-lingual PLM — M-BERT as the
framework backbone, which can simultaneously
capture multi-lingual semantics in a unified fea-
ture space. Furthermore, to bridge the gap be-
tween high-resource languages and low-resource
languages, we introduce entity-pair-oriented heuris-
tic rules as well as machine translation to auto-
matically obtain high-quality cross-lingual data,
and then apply cross-lingual contrastive learning
on this distantly-supervised data to enhance the
backbone PLM. The experimental results show
that by applying C -C, the typing knowledge
can be transferred from high-resource languages
to low-resource languages, and we can learn effec-
tive FGET models without any language-specific2248human-labeled data for those low-resource lan-
guages. In the future:
(1) We will explore how to better utilize unsuper-
vised data to deal with the low-resource problem
of FGET, such as using better PLMs and more ef-
fective tuning methods.
(2) We will also promote the construction of
cross-lingual FGET datasets, which will advance
the development of FGET in specific languages,
especially for those low-resource languages other
than English.
Acknowledgements
This work is supported by the National Key R&D
Program of China (No. 2020AAA0106502), In-
stitute Guo Qiang of Tsinghua University, Beijing
Academy of Artificial Intelligence (BAAI), and
International Innovation Center of Tsinghua Uni-
versity. This work is also supported by Tencent AI
Platform Department.
References22492250