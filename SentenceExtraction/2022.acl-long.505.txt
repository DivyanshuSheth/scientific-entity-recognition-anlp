mLUKE : The Power of Entity Representations in Multilingual Pretrained Language Models Ryokan Ri1,2∗ ryo0123@ousia.jp Ikuya Yamada1,3 ikuya@ousia.jp Yoshimasa Tsuruoka2 tsuruoka@logos.t.u-tokyo.ac.jp 1Studio Ousia , Tokyo , Japan 2The University of Tokyo , Tokyo , Japan 3RIKEN AIP , Tokyo , Japan Abstract bilingual word dictionaries ( Conneau et al . , 2020b ) or parallel sentences ( Conneau and Lample , 2019 ) . Another source of such information is the crosslingual mappings of Wikipedia entities ( articles ) . Wikipedia entities are aligned across languages via inter - language links and the text contains numerous entity annotations ( hyperlinks ) . With these data , models can learn cross - lingual correspondence such as the words Tokyo ( English ) and 東 京 ( Japanese ) refers to the same entity . Wikipedia entity annotations have been shown to provide rich cross - lingual alignment information to improve multilingual language models ( Calixto et al . , 2021 ; Jiang et al . , 2022 ) . However , previous studies only incorporate entity information through an auxiliary loss function during pretraining , and the models do not explicitly have entity representations used for downstream tasks . Recent studies have shown that multilingual pretrained language models can be effectively improved with cross - lingual alignment information from Wikipedia entities . However , existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks . In this study , we explore the effectiveness of leveraging entity representations for downstream cross - lingual tasks . We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word - based pretrained models in various crosslingual transfer tasks . We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language - agnostic features . We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset . We show that entity - based prompt elicits correct factual knowledge more likely than using only word representations . Our source code and pretrained models are available at https : //github.com / studio - ousia / luke . In this study , we investigate the effectiveness of entity representations in multilingual language models . Entity representations are known to enhance language models in mono - lingual settings ( Zhang et al . , 2019 ; Peters et al . , 2019 ; Wang et al . , 2021 ; Xiong et al . , 2020 ; Yamada et al . , 2020 ) presumably by introducing real - world knowledge . We show that using entity representations facilitates cross - lingual transfer by providing languageindependent features . To this end , we present a multilingual extension of LUKE ( Yamada et al . , 2020 ) . The model is trained with the multilingual masked language modeling ( MLM ) task as well as the masked entity prediction ( MEP ) task with Wikipedia entity embeddings . 1 Introduction Pretrained language models have become crucial for achieving state - of - the - art performance in modern natural language processing . In particular , multilingual language models ( Conneau and Lample , 2019 ; Conneau et al . , 2020a ; Doddapaneni et al . , 2021 ) have attracted considerable attention particularly due to their utility in cross - lingual transfer . In zero - shot cross - lingual transfer , a pretrained encoder is ﬁne - tuned in a single resource - rich language ( typically English ) , and then evaluated on other languages never seen during ﬁne - tuning . A key to solving cross - lingual transfer tasks is to obtain representations that generalize well across languages . Several studies aim to improve multilingual models with cross - lingual supervision such as We investigate two ways of using the entity representations in cross - lingual transfer tasks : ( 1 ) perform entity linking for the input text , and append the detected entity tokens to the input sequence . The entity tokens are expected to provide languageindependent features to the model . We evaluate this approach with cross - lingual question answering ( QA ) datasets : XQuAD ( Artetxe et al . , 2020 ) ∗ Work done as an intern at Studio Ousia . [ MASK ] token is different from the word [ MASK ] token for MLM . and MLQA ( Lewis et al . , 2020 ) ; ( 2 ) use the entity [ MASK ] token from the MEP task as a languageindependent feature extractor . In the MEP task , word tokens in a mention span are associated with an entity [ MASK ] token , the contextualized representation of which is used to train the model to predict its original identity . Here , we apply similar input formulations to tasks involving mention - span classiﬁcation , relation extraction ( RE ) and named entity recognition ( NER ): the attribute of a mention or a pair of mentions is predicted using their contextualized entity [ MASK ] feature . We evaluate this approach with the RELX ( Köksal and Özgür , 2020 ) and CoNLL NER ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) datasets . The experimental results show that these entitybased approaches consistently outperform wordbased baselines . Our analysis reveals that entity representations provide more language - agnostic features to solve the downstream tasks . The model takes as input a tokenized text ( w1 , w2 , ... , wm ) and the entities appearing in the text ( e1 , e2 , ... , en ) , and compute the contextualized representation for each token ( hw1 , hw2 , ... , hwm and he1 , he2 , ... , hen ) . The word and entity tokens equally undergo self - attention computation ( i.e. , no entity - aware self - attention in Yamada et al . ( 2020 ) ) after embedding layers . The word and entity embeddings are computed as the summation of the following three embeddings : token embeddings , type embeddings , and position embeddings ( Devlin et al . , 2019 ) . The entity tokens are associated with the word tokens through position embeddings : the position of an entity token is deﬁned as the positions of its corresponding word tokens , and the entity position embeddings are summed over the positions . Model Conﬁguration . The model conﬁgurations of mLUKE follow the base and large conﬁgurations of XLM - RoBERTa ( Conneau et al . , 2020a ) , a variant of BERT ( Devlin et al . , 2019 ) trained with CommonCrawl data from 100 languages . Before pretraining , the parameters in common ( e.g. , the weights of the transformer encoder and the word embeddings ) are initialized using the checkpoint from the Transformers library.1 We also explore solving a multilingual zero - shot cloze prompt task ( Liu et al . , 2021 ) with the entity [ MASK ] token . Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts ( Petroni et al . , 2019 ; Cui et al . , 2021 ) . Typically , the answer tokens are predicted from the model ’s word - piece vocabulary but here we incorporate the prediction from the entity vocabulary queried by the entity [ MASK ] token . We evaluate our approach with the mLAMA dataset ( Kassner et al . , 2021 ) in various languages and show that using the entity [ MASK ] token reduces language bias and elicits correct factual knowledge more likely than using only the word [ MASK ] token . The size of the entity embeddings is set to 256 and they are projected to the size of the word embeddings before being fed into the encoder . 2.2 Training Corpus : Wikipedia We use Wikipedia dumps in 24 languages ( Appendix A ) as the training data . These languages are selected to cover reasonable numbers of languages that appear in downstream cross - lingual datasets . We generate input sequences by splitting the content of each page into sequences of sentences comprising ≤ 512 words with their entity annotations ( i.e. , hyperlinks ) . During training , data are sampled from each language with ni items with the following multinomial distribution : 2 Multilingual Language Models with Entity Representations 2.1 Model : mulitlingual LUKE To evaluate the effectiveness of entity representations for cross - lingual downstream tasks , we introduce a new multilingual language model based on a bidirectional transformer encoder : Multilingual LUKE ( mLUKE ) , a multilingual extension of LUKE ( Yamada et al . , 2020 ) . The model is trained with the masked language modeling ( MLM ) task ( Vaswani et al . , 2017 ) as well as the masked entity prediction ( MEP ) task . In MEP , some of the input entity tokens are randomly masked with the special entity [ MASK ] token , and the model is trained to predict the original entities . Note that the entity nα i k=1 nα k , pi = ( 1 ) ( cid:80)N where α is a smoothing parameter and set to 0.7 following multilingual BERT.2 1https://huggingface.co/transformers/ 2https://github.com/google-research/ bert / blob / master / multilingual.md Figure 1 : How to use entity representations in downstream tasks . The input entity embeddings are associated with their mentions ( indicated by dotted lines ) via positional embeddings . Entity Vocabulary . Entities used in mLUKE are deﬁned as Wikipedia articles . The articles from different languages are aligned through inter - language links3 and the aligned articles are treated as a single entity . We include in the vocabulary the most frequent 1.2 M entities in terms of the number of hyperlinks that appear across at least three languages to facilitate cross - lingual learning . Optimization . We optimize the models with a batch size of 2048 for 1 M steps in total using AdamW ( Loshchilov and Hutter , 2019 ) with warmup and linear decay of the learning rate . To stabilize training , we perform pretraining in two stages : ( 1 ) in the ﬁrst 500 K steps , we update only those parameters that are randomly initialized ( e.g. , entity embeddings ) ; ( 2 ) we update all parameters in the remaining 500 K steps . The learning rate scheduler is reset at each training stage . For further details on hyperparameters , see Appendix A. tion impact the performance . Since earlier studies ( Liu et al . , 2019 ; Lan et al . , 2020 ) indicated longer pretraining would simply improve performance , we train another model based on XLM - Rbase with extra MLM pretraining following the same conﬁguration of mLUKE . mLUKE - W is an ablation model of mLUKE - E. This model discards the entity embeddings learned during pretraining and only takes word tokens as input as with the other baseline models . The results from this model indicate the effect of MEP only as an auxiliary task in pretraining , and the comparison with this model will highlight the effect of using entity representations for downstream tasks in mLUKE - E. The above models are ﬁne - tuned with the same hyperparameter search space and computational budget as described in Appendix B. We also present the results of XLM - K ( Jiang et al . , 2022 ) for ease of reference . XLM - K is based on XLM - Rbase and trained with entity information from Wikipedia but does not use entity representations in downstream tasks . Notice that their results are not strictly comparable to ours , because the pretraining and ﬁne - tuning settings are different . 2.3 Baseline Models We compare the primary model that we investigate , multilingual LUKE used with entity representations ( mLUKE - E ) , against several baselines pretrained models and an ablation model based on word representations : mBERT ( Devlin et al . , 2019 ) is one of the earliest multilingual language models . We provide these results as a reference . XLM - R ( Conneau et al . , 2020a ) is the model that mLUKE is built on . This result indicates how our additional pretraining step and entity representa3 Adding Entities as Language - Agnostic Features in QA We evaluate the approach of adding entity embeddings to the input of mLUKE - E with cross - lingual extractive QA tasks . The task is , given a question and a context passage , to extract the answer span from the context . The entity embeddings provide language - agnostic features and thus should facilitate cross - lingual transfer learning . 3https://en.wikipedia.org/wiki/Help : Interlanguage_links . We build an inter - language database from the wikidatawiki dump from November 30 , 2020 . XQuAD en es de el ru tr ar vi th zh hi avg . mBERT XLM - Rbase + extra training mLUKE - Wbase mLUKE - Ebase 84.5 84.0 86.1 85.7 86.3 76.1 76.5 76.9 78.0 78.9 73.1 76.4 76.5 77.4 78.9 59.0 73.9 73.7 74.7 73.9 70.2 74.4 74.7 75.7 76.0 53.2 67.8 66.3 68.3 68.8 62.1 68.1 68.2 71.7 71.4 68.5 74.2 74.5 75.9 76.4 40.7 66.8 67.7 67.1 67.5 58.3 61.5 64.7 65.1 65.9 57.0 68.7 66.6 69.9 72.2 63.9 72.0 72.4 73.6 74.2 81.4 81.3 81.4 XLM - Rlarge mLUKE - Wlarge mLUKE - Elarge 88.5 89.0 88.6 82.4 83.1 83.0 82.0 82.4 81.7 81.2 81.3 80.8 75.5 75.3 75.8 75.9 77.9 77.7 80.7 81.2 81.9 72.3 75.1 75.4 67.6 71.5 71.9 77.2 77.3 77.5 78.6 79.6 79.6 MLQA en es de ar hi vi zh avg . G - XLT avg . mBERT XLM - Rbase + extra training mLUKE - Wbase mLUKE - Ebase 79.1 79.7 81.3 81.3 80.8 65.9 67.7 69.8 69.7 70.0 58.6 62.2 65.0 65.4 65.5 48.6 55.8 54.8 60.4 60.8 44.8 59.9 59.3 63.2 63.7 58.5 65.3 65.6 68.3 68.4 58.1 62.5 64.2 66.1 66.2 59.1 64.7 65.7 67.8 67.9 40.9 33.4 50.2 54.0 55.6 XLM - K ( Jiang et al . , 2022 ) 80.8 69.2 63.8 60.0 65.3 70.1 63.8 67.7 74.7 74.3 74.5 XLM - Rlarge mLUKE - Wlarge mLUKE - Elarge 83.9 84.0 84.1 69.9 70.3 70.5 64.9 66.2 66.2 69.9 70.2 71.4 73.3 74.2 74.3 70.3 69.7 70.5 72.4 72.7 73.1 65.3 67.4 67.7 Table 1 : F1 scores on the XQuAD and MLQA dataset in the cross - lingual transfer settings . The scores without reference are from the best model tuned with the English development data . 3.1 Main Experiments in MLQA ) , which indicates the input entity tokens provide useful features to facilitate cross - lingual transfer . The usefulness of entities is demonstrated especially in the MLQA ’s G - XLT setting ( full results available in Appendix F ) ; mLUKE - Ebase exhibits a substantial 1.6 point improvement in the G - XLT average score over mLUKE - Wbase . This suggests that entity representations are beneﬁcial in a challenging situation where the model needs to capture language - agnostic semantics from text segments in different languages . Datasets . We ﬁne - tune the pretrained models with the SQuAD 1.1 dataset ( Rajpurkar et al . , 2016 ) , and evaluate them with the two multilingual datasets : XQuAD ( Artetxe et al . , 2020 ) and MLQA ( Lewis et al . , 2020 ) . XQuAD is created by translating a subset of the SQuAD development set while the source of MLQA is natural text in Wikipedia . Besides multiple monolingual evaluation data splits , MLQA also offers data to evaluate generalized cross - lingual transfer ( G - XLT ) , where the question and context texts are in different languages . Models . All QA models used in this experiment follow Devlin et al . ( 2019 ) . The model takes the question and context word tokens as input and predicts a score for each span of the context word tokens . The span with the highest score is predicted as the answer to the question . We also observe that XLM - Rbase beneﬁts from extra training ( 0.4 points improvement in the average score on XQuAD and 2.1 points in MLQA ) . The mLUKE - Wbase model further improves the average score from XLM - Rbase with extra training ( 1.2 points improvement in XQuAD and 2.1 points in MLQA ) , showing the effectiveness of the MEP task for cross - lingual QA . mLUKE - E takes entity tokens as additional features in the input ( Figure 1 ) to enrich word representations . The entities are automatically detected using a heuristic string matching based on the original Wikipedia article from which the dataset instance is created . See Appendix C for more details . Results . Table 1 summarizes the model ’s F1 scores for each language . First , we discuss the base models . On the effectiveness of entity representations , mLUKE - Ebase performs better than its word - based counterpart mLUKE - Wbase ( 0.6 average points improvement in the XQuAD average score , 0.1 points in MLQA ) and XLM - K ( 0.2 points improvement By comparing large models , we still observe substantial improvements from XLM - Rlarge to the mLUKE models . Also we can see that mLUKEElarge overall provides better results than mLUKEWlarge ( 0.4 and 0.3 points improvements in the MLQA average and G - XLT scores ; comparable scores in XQuAD ) , conﬁrming the effectiveness of entity representations . 3.2 Analysis How do the entity representations help the model in cross - lingual transfer ? In the mLUKE - E model , 4 The Entity MASK Token as Feature the input entity tokens annotate mention spans on which the model performs prediction . We hypothesize that this allows the encoder to inject languageagnostic entity knowledge into span representations , which help better align representations across languages . To support this hypothesis , we compare the degree of alignment between span representations before and after adding entity embeddings in the input , i.e. , mLUKE - W and mLUKE - E. Task . We quantify the degree of alignment as performance on the contextualized word retrieval ( CWR ) task ( Cao et al . , 2020 ) . The task is , given a word within a sentence in the query language , to ﬁnd the word with the same meaning in the context from a candidate pool in the target language . Dataset . We use the MLQA dev set ( Lewis et al . , 2020 ) . As MLQA is constructed from parallel sentences mined from Wikipedia , some sentences and answer spans are aligned and thus the dataset can be easily adapted for the CWR task . As the query and target word , we use the answer span4 annotated in the dataset , which is also parallel across the languages . We use the English dataset as the query language and other languages as the target . We discard query instances that do not have their parallel data in the target language . The candidate pool is all answer spans in the target language data . Models . We evaluate the mLUKE - Wbase and mLUKE - Ebase models without ﬁne - tuning . The retrieval is performed by ranking the cosine similarity of contextualized span representations , which is computed by mean - pooling the output word vectors in the span . Results . Table 2 shows the retrieval performance in terms of the mean reciprocal rank score . We observe that the scores of mLUKE - Ebase are higher than mLUKE - Wbase across all the languages . This demonstrates that adding entities improves the degree of alignment of span representations , which may explain the improvement of mLUKE - E in the cross - lingual QA task . Extractor in RE and NER In this section , we evaluate the approach of using the entity [ MASK ] token to extract features from mLUKE - E for two entity - related tasks : relation extraction and named entity recognition . We formulate both tasks as the classiﬁcation of mention spans . The baseline models extract the feature of spans as the contextualized representations of word tokens , while mLUKE - E extracts the feature as the contextualized representations of the special language - independent entity tokens associated with the mentions ( Figure 1 ) . We demonstrate that this approach consistently improves the performance in cross - lingual transfer . 4.1 Relation Extraction Relation Extraction ( RE ) is a task to determine the correct relation between the two ( head and tail ) entities in a sentence . Adding entity type features have been shown to be effective to cross - lingual transfer in RE ( Subburathinam et al . , 2019 ; Ahmad et al . , 2021 ) , but here we investigate an approach that does not require predeﬁned entity types but utilize special entity embeddings learned in pretraining . Datasets . We ﬁne - tune the models with the English KBP-37 dataset ( Zhang and Wang , 2015 ) and evaluate the models with the RELX dataset ( Köksal and Özgür , 2020 ) , which is created by translating a subset of 502 sentences from KBP-37 ’s test set into four different languages . Following Köksal and Özgür ( 2020 ) , we report the macro average of F1 scores of the 18 relations . Models . In the input text , the head and tail entities are surrounded with special markers ( < ent > , < ent2 > ) . The baseline models extract the feature vectors for the entities as the contextualized vector of the ﬁrst marker followed by their mentions . The two entity features are concatenated and fed into a linear classiﬁer to predict their relation . For mLUKE - E , we introduce two special entities , [ HEAD ] and [ TAIL ] , to represent the head and tail entities ( Yamada et al . , 2020 ) . Their embeddings are initialized with the entity [ MASK ] embedding . They are added to the input sequence being associated with the entity mentions in the input , and their contextualized representations are extracted as the feature vectors . As with the wordbased models , the features are concatenated and input to a linear classiﬁer . ar de es hi vi zh avg . mLUKE - Wbase 55.6 66.1 68.4 60.4 69.7 56.1 62.7 56.9 68.1 70.4 61.5 71.2 60.0 64.7 mLUKE - Ebase Table 2 : The mean reciprocal rank score of the CWR task with the MLQA dev set . 4Answer spans are not necessarily a word , but here we generalize the task as span retrieval for our purpose . RE NER en es fr de tr avg . en de nl es avg . mBERT XLM - Rbase + extra training mLUKE - Wbase mLUKE - Ebase 65.0 66.5 67.0 68.7 69.3 61.6 62.9 62.9 65.8 65.2 58.9 60.9 64.3 62.1 64.7 57.3 60.8 61.3 64.3 64.5 56.2 57.7 61.9 65.0 68.7 59.8 61.7 63.5 65.2 66.5 89.7 91.5 91.8 91.6 93.6 70.0 74.3 75.7 75.1 77.2 75.2 80.7 80.3 80.2 81.8 77.1 79.8 79.8 79.2 77.7 78.0 81.6 81.9 81.5 82.6 XLM - K ( Jiang et al . , 2022 ) 90.7 73.3 80.0 76.6 80.1 XLM - Rlarge mLUKE - Wlarge mLUKE - Elarge 68.0 66.2 68.1 65.0 68.1 67.8 63.3 66.5 66.4 65.3 65.3 65.8 64.1 64.7 64.4 65.1 66.2 66.5 92.5 92.3 94.0 75.1 76.5 78.3 82.9 82.6 83.5 80.5 80.7 81.4 82.8 83.0 84.3 Table 3 : F1 scores on relation extraction ( RE ) and named entity recognition ( NER ) . 4.2 Named Entity Recognition de es fr tr Named Entity Recognition ( NER ) is the task to detect entities in a sentence and classify their type . We use the CoNLL-2003 English dataset ( Tjong Kim Sang and De Meulder , 2003 ) as the training data , and evaluate the models with the CoNLL2003 German dataset and the CoNLL-2002 Spanish and Dutch dataset ( Tjong Kim Sang , 2002 ) . Models . We adopt the model of Sohrab and Miwa ( 2018 ) as the baseline model , which enumerates all possible spans in a sentence and classiﬁes them into the target entity types or non - entity type . In this experiment , we enumerate spans with at most 16 tokens . For the baseline models , the span features are computed as the concatenation of the word representations of the ﬁrst and last tokens . The span features are fed into a linear classiﬁer to predict their entity type . mLUKE - Wbase mLUKE - Ebase 0.71 0.25 0.74 0.28 0.74 0.24 0.84 0.36 Table 4 : The modularity of word and entity features computed with the same mLUKE model . The data are from pairs of English and the other languages in the RELX dataset . token extracts better features for predicting entity attributes because it resembles how mLUKE is pretrained with the MEP task . We hypothesize that there exists another factor for the improvement in cross - lingual performance : language neutrality of representations . The entity [ MASK ] token is shared across languages and their contextualized representations may be less affected by the difference of input languages , resulting in features that generalize well for cross - lingual transfer . To ﬁnd out if the entity - based features are actually more languageindependent than word - based features , we evaluate the modularity ( Fujinuma et al . , 2019 ) of the features extracted for the RELX dataset . The input of mLUKE - E contains the entity [ MASK ] tokens associated with all possible spans . The span features are computed as the contextualized representations of the entity [ MASK ] tokens . The features are input to a linear classiﬁer as with the word - based models . 4.3 Main Results Modularity is computed for the k - nearest neighbor graph of embeddings and measures the degree to which embeddings tend to form clusters within the same language . We refer readers to Fujinuma et al . ( 2019 ) for how to compute the metric . Note that the maximum value of modularity is 1 , and 0 means the embeddings are completely randomly distributed regardless of language . The results are shown in Table 3 . The mLUKE - E models outperform their word - based counterparts mLUKE - W in the average score in all the comparable settings ( the base and large settings ; the RE and NER tasks ) , which shows entity - based features are useful in cross - lingual tasks . We also observe that XLM - Rbase beneﬁts from extra training ( 1.8 average points improvement in RE and 0.3 points in NER ) , but mLUKE - E still outperforms the results . We compare the modularity of the word features from mLUKE - Wbase and entity features from mLUKE - Ebase before ﬁne - tuning . Note that the features here are concatenated vectors of head and tail features . Table 4 shows that the modularity of mLUKE - Ebase is much lower than mLUKE - Wbase , 4.4 Analysis The performance gain of mLUKE - E over mLUKEW can be partly explained as the entity [ MASK ] ar en ﬁ fr i d ja ru vi zh avg . mBERT XLM - Rbase + extra training mLUKE - Wbase mLUKE - Ebase ( [ Y ] ) mLUKE - Ebase ( [ X ] & [ Y ] ) 17.1 14.2 21.2 22.3 27.8 42.4 36.8 27.2 35.0 31.3 37.5 47.5 24.0 16.2 23.0 18.4 30.4 44.2 24.3 14.9 22.2 19.6 28.4 35.9 42.9 28.2 46.8 46.7 44.2 56.2 14.3 11.9 19.6 18.4 28.9 40.3 19.5 11.7 17.5 16.7 25.8 35.5 39.4 25.1 34.4 31.9 42.1 55.2 26.2 17.6 30.7 29.3 33.4 46.7 27.2 18.5 27.8 26.1 33.2 44.9 Table 5 : The top-1 accuracies from 9 languages from the mLAMA dataset . demonstrating that entity - based features are more language - neutral . However , with entity - based features , the modularities are still greater than zero . In particular , the modularity computed with Turkish , which is the most distant language from English here , is signiﬁcantly higher than the others , indicating that the contextualized entity - based features are still somewhat language - dependent . word - based and entity - based prediction , we restrict the candidate entities to the ones found in the entity vocabulary and exclude the questions if their answers are not included in the candidates ( results with full candidates and questions in the dataset are in Appendix G ) . Results . We experiment in total with 16 languages which are available both in the mLAMA dataset and the mLUKE ’s entity vocabulary . Here we only present the top-1 accuracy results from 9 languages on Table 5 , as we can make similar observations with the other languages . 5 Cloze Prompt Task with Entity Representations In this section , we show that using the entity representations is effective in a cloze prompt task ( Liu et al . , 2021 ) with the mLAMA dataset ( Kassner et al . , 2021 ) . The task is , given a cloze template such as “ [ X ] was born in [ Y ] ” with [ X ] ﬁlled with an entity ( e.g. , Mozart ) , to predict a correct entity in [ Y ] ( e.g. , Austria ) . We adopt the typed querying setting ( Kassner et al . , 2021 ) , where a template has a set of candidate answer entities and the prediction becomes the one with the highest score assigned by the language model . Model . As in Kassner et al . ( 2021 ) , the word - based baseline models compute the candidate score as the log - probability from the MLM classiﬁer . When a candidate entity in [ Y ] is tokenized into multiple tokens , the same number of the word [ MASK ] tokens are placed in the input sequence , and the score is computed by taking the average of the logprobabilities for its individual tokens . We observe that XLM - Rbase performs notably worse than mBERT as mentioned in Kassner et al . ( 2021 ) . However , with extra training with the Wikipedia corpus , XLM - Rbase shows a signiﬁcant 9.3 points improvement in the average score and outperforms mBERT ( 27.8 vs. 27.2 ) . We conjecture that this shows the importance of the training corpus for this task . The original XLM - R is only trained with the CommonCrawl corpus ( Conneau et al . , 2020a ) , text scraped from a wide variety of web pages , while mBERT and XLM - R + training are trained on Wikipedia . The performance gaps indicate that Wikipedia is particularly useful for the model to learn factual knowledge . The mLUKE - Wbase model lags behind XLMRbase + extra training by 1.7 average points but we can see 5.4 points improvement from XLM - Rbase + extra training to mLUKE - Ebase ( [ Y ] ) , indicating entity representations are more suitable to elicit correct factual knowledge from mLUKE than word representations . Adding the entity corresponding to [ X ] to the input ( mLUKE - Ebase ( [ X ] & [ Y ] ) ) further pushes the performance by 11.7 points to 44.9 % , which further demonstrates the effectiveness of entity representations . Analysis of Language Bias . Kassner et al . ( 2021 ) notes that the prediction of mBERT is biased by the input language . For example , when queried in Italian ( e.g. , “ [ X ] e stato creato in [ MASK ] . ” ) , the model tends to predict entities that often appear in Italian text ( e.g. , Italy ) for any question to answer On the other hand , mLUKE - E computes the logprobability of the candidate entity in [ Y ] with the entity [ MASK ] token . Each candidate entity is associated with an entity in mLUKE ’s entity vocabulary via string matching . The input sequence has the entity [ MASK ] token associated with the word [ MASK ] tokens in [ Y ] , and the candidate score is computed as the log - probability from the MEP classiﬁer . We also try additionally appending the entity token of [ X ] to the input sequence if the entity is found in the vocabulary . To accurately measure the difference between en ja fr mBERT XLM - Rbase + extra training mLUKE - Wbase mLUKE - Ebase ( [ Y ] ) mLUKE - Ebase ( [ X ] & [ Y ] ) The Bahamas , 41 % ( 355/870 ) London , 78 % ( 664/850 ) Australia , 27 % ( 247/899 ) Germany , 22 % ( 198/895 ) London , 37 % ( 310/846 ) London , 27 % ( 213/797 ) Japan , 82 % ( 361/439 ) Pays - Bas , 71 % ( 632/895 ) Japan , 99 % ( 437/440 ) Allemagne , 96 % ( 877/916 ) Japan , 99 % ( 437/442 ) Allemagne , 93 % ( 854/917 ) Japan , 97 % ( 428/442 ) Allemagne , 99 % ( 906/918 ) Suède , 40 % ( 362/908 ) Japan , 56 % ( 241/430 ) Suède , 30 % ( 266/895 ) Japan , 44 % ( 176/401 ) Table 6 : The top incorrect predictions in three languages for the template “ [ X ] was founded in [ Y ] . ” for each model . The predictions in the original language are translated into English . location . We expect that using entity representations would reduce language bias because entities are shared among languages and less affected by the frequency in the language of questions . with mMLM exhibit a strong cross - lingual ability without any cross - lingual supervision ( K et al . , 2020 ; Conneau et al . , 2020b ) , several studies aim to develop better multilingual models with explicit cross - lingual supervision such as bilingual word dictionaries ( Conneau et al . , 2020b ) or parallel sentences ( Conneau and Lample , 2019 ) . In this study , we build a multilingual pretrained language model on the basis of XLM - RoBERTa ( Conneau et al . , 2020a ) , trained with mMLM as well as the masked entity prediction ( MEP ) ( Yamada et al . , 2020 ) with entity representations . We qualitatively assess the degree of language bias in the models looking at their incorrect predictions . We show the top incorrect prediction for the template “ [ X ] was founded in [ Y ] . ” for each model in Table 6 , together with the top-1 incorrect ratio , that is , the ratio of the number of the most common incorrect prediction to the total false predictions , which indicates how much the false predictions are dominated by few frequent entities . The examples show that the different models exhibit bias towards different entities as in English and French , although in Japanese the model consistently tends to predict Japan . Looking at the degree of language bias , mLUKE - Ebase ( [ X ] & [ Y ] ) exhibits lower top-1 incorrect ratios overall ( 27 % in fr , 44 % in ja , and 30 % in fr ) , which indicates using entity representations reduces language bias . However , lower language bias does not necessarily mean better performance : in French ( fr ) , mLUKEEbase ( [ X ] & [ Y ] ) gives a lower top-1 incorrect ratio than mBERT ( 30 % vs. 71 % ) but their numbers of total false predictions are the same ( 895 ) . Language bias is only one of several factors in the performance bottleneck . 6.2 Pretrained Language Models with Entity Knowledge Language models trained with a large corpus contain knowledge about real - world entities , which is useful for entity - related downstream tasks such as relation classiﬁcation , named entity recognition , and question answering . Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model ( Zhang et al . , 2019 ; Peters et al . , 2019 ; Wang et al . , 2021 ; Xiong et al . , 2020 ; Févry et al . , 2020 ; Yamada et al . , 2020 ) . When incorporated into multilingual language models , entity information can bring another beneﬁt : entities may serve as anchors for the model to align representations across languages . Multilingual knowledge bases such as Wikipedia often offer mappings between different surface forms across languages for the same entity . Calixto et al . ( 2021 ) ﬁne - tuned the top two layers of multilingual BERT by predicting language - agnostic entity ID from hyperlinks in Wikipedia articles . As our concurrent work , Jiang et al . ( 2022 ) trained a model based on XLM - RoBERTa with an entity prediction task along with an object entailment prediction task . While the previous studies focus on improving cross - lingual language representations by pretraining with entity information , our work investigates a multilingual model not only pretrained 6 Related Work 6.1 Multilingual Pretrained Language Models Multilingual pretrained language models have recently seen a surge of interest due to their effectiveness in cross - lingual transfer learning ( Conneau and Lample , 2019 ; Liu et al . , 2020 ) . A straightforward way to train such models is multilingual masked language modeling ( mMLM ) ( Devlin et al . , 2019 ; Conneau et al . , 2020a ) , i.e. , training a single model with a collection of monolingual corpora in multiple languages . Although models trained 