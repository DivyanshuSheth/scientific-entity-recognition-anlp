
Vivek Kumar, Rishabh Maheshwary and Vikram Pudi
Data Sciences and Analytics Center, Kohli Center on Intelligent Systems
International Institute of Information Technology, Hyderabad, India
{vivek.k, rishabh.maheshwary}@research.iiit.ac.in, vikram@iiit.ac.in
Abstract
Existing Math Word Problem (MWP) solvers
have achieved high accuracy on benchmark
datasets. However, prior works have shown that
such solvers do not generalize well and rely on
superficial cues to achieve high performance.
In this paper, we first conduct experiments to
showcase that this behaviour is mainly associ-
ated with the limited size and diversity present
in existing MWP datasets. Next, we propose
several data augmentation techniques broadly
categorized into Substitution andParaphras-
ingbased methods. By deploying these meth-
ods we increase the size of existing datasets
by five folds. Extensive experiments on two
benchmark datasets across three state-of-the-art
MWP solvers shows that proposed methods in-
crease the generalization and robustness of ex-
isting solvers. On average, proposed methods
significantly increase the state-of-the-art results
by over five percentage points on benchmark
datasets. Further, the solvers trained on the aug-
mented dataset performs comparatively better
on the challenge test set. We also show the
effectiveness of proposed techniques through
ablation studies and verify the quality of aug-
mented samples through human evaluation.
1 Introduction
A Math Word Problem (MWP) consists of natu-
ral language text which describes a world state
involving some known and unknown quantities,
followed by a question text to determine the un-
known values. The task is to parse the problem
statement and generate equations that can help find
the value of unknown quantities. An example of a
simple MWP is shown in Table 1. In recent years,
the challenge of solving MWP has gained much
attention in the NLP community as it needs the
development of commonsense multi step reason-
ing with numerical quantities. With the rise of
deep learning, performance of math solvers has
also increased significantly over the years (Wang
Table 1: A MWP and its augmentation examples gen-
erated by our methods with preserved equation labels.
Blue and Violet colours denote the changes made after
the primary stage and secondary stage respectively.
et al.; Zhang et al.). However, recent analysis con-
ducted in (Kumar et al., 2021) and (Patel et al.,
2021) show that these deep learning based solvers
rely on shallow heuristics to solve vast majority
of problems. They curated adversarial examples
and SV AMP challenge set respectively to infer that
MWP solvers (1)do not understand the relation-
ship between numbers and their associated entities,
(2)do not focus on the question text and (3)ig-
nore word order information. In this paper, we first
conduct experiments to establish that the above
drawbacks are due to the limited size and diversity
of problems present in the existing MWP datasets.
Next, we propose various augmentation methods to
create diverse and large number of training exam-
ples to mitigate these shortcomings. Our methods
are focused on: (1)Increasing the number of prob-
lems in the existing datasets and (2)enhancing the
diversity of the problem set.
Training deep neural models effectively requires
large number of data points (Longpre et al., 2020).
Constructing large datasets which are annotated,
labeled and have MWPs of similar difficulty level
is a very expensive and tedious task. To address
these key challenges, we resort to data augmenta-4194tion techniques. Our motivation behind generat-
ing augmentations is that humans require sufficient
practice to understand MWPs. Humans learn to
solve MWPs by going through a variety of similar
examples and slowly become capable enough to
tackle variations of similar difficulty levels. We aim
to generate augmentations such that sufficient lin-
guistic variations of a similar problem are present
in the dataset. These variations will make the solver
more robust in tackling MWP, increase their rea-
soning ability and numerical understanding.
Data augmentation for MWPs is a challenging
task as we need to preserve the equation labels
while generating new samples (Kumar et al., 2021).
The generated samples should be (1)semantically
similar to their original counterpart, (2)must have
the same numerical values and preserve relation-
ship with their respective entities and (3)should
maintain the same sequence of events in the prob-
lem text. Existing augmentation methods (Wei and
Zou) cannot be directly applied due to the above
mentioned reasons. Our methods can be broadly
classified as follows:
•Paraphrasing Methods: It generates varia-
tions of the question text by re-statement such
that the semantic and syntactic meaning along
with the equation labels is preserved.
•Substitution Methods: These methods gen-
erate variations of the problem statement by
identifying and substituting some of the key-
words such that the augmentations are seman-
tically and syntactically correct.
To ensure high quality augmentations, we propose
a selection algorithm which selects samples that
have high similarity with original problem and in-
cur high loss values when tested on existing solvers.
This algorithm helps selecting only those samples
that can make existing solvers more robust. Further,
we also verify the validity and the quality of gener-
ated augmentations through human evaluation.
Most of the existing MWP datasets are either in
languages other than English or contain problems
of varying difficulty levels (Koncel-Kedziorski
et al., 2016; Wang et al.; Huang et al., 2016;
Amini et al., 2019; Miao et al., 2020). We fo-
cus on strengthening existing English language
datasets which can facilitate the development ofbetter MWP solvers. We consider datasets con-
taining MWP that can be solved using linear
equations in one variable. These datasets in-
clude MaWPS (Koncel-Kedziorski et al., 2016) and
ASDiv-A (Miao et al., 2020) both having 2,373
and1,213problems respectively. Following are
the key contributions made in this paper:
•To the best of our knowledge, this is the first
work that extensively evaluates data augmen-
tation techniques for MWP solving. This is
the first attempt to generate MWP problems
automatically without manual intervention.
•Accuracy of the state of the art solvers in-
creases after training on the proposed aug-
mented dataset. This demonstrates the effec-
tiveness of our methods. To verify the validity
of generated augmentations we conduct hu-
man evaluation studies.
•We increase the diversity of the training
dataset through augmentations and obtain
comparatively better results than state-of-the-
art solvers on the SV AMP challenge set.
2 Related Work
Math Word Solvers: Many research efforts have
been undertaken in the recent past to solve the
challenging MWP task. Broadly, these solvers
can be categorized into statistical learning based
and deep learning based models. Traditional
approaches focused more on statistical machine
learning (Kushman et al., 2014; Hosseini et al.,
2014) with the aim of categorizing equations
into templates and extracting key patterns in the
problem text. Recently, due to the advent of
deep learning in NLP, solvers have witnessed
a considerable increase in their performances.
(Wang et al.) modelled MWP task as a sequence
to sequence task and used LSTM’s (Hochreiter
and Schmidhuber, 1997) for learning problem
representations. (Chiang and Chen, 2018) focused
on learning representations for operators and
operands.(Wang et al., 2019; Xie and Sun, 2019)
used tree structures for decoding process. (Zhang
et al.) modelled question as a graph to map quanti-
ties and their attributes. Existing datasets which
have been used as benchmark for english language
includes MaWPS (Koncel-Kedziorski et al., 2016)
and Chinese language dataset Math23K (Wang
et al.). These datasets although constrained by4195their size deal with algebraic problems of similar
difficulty levels. Recently, ASDiv (Miao et al.,
2020) has been proposed, which has diverse
problems which includes annotations for equations,
problem type and grade level. Other large datasets
in English language include MathQA (Amini
et al., 2019) and Dolphin18k (Huang et al., 2016).
Although, these datasets have larger problem set
but they are noisy and contain problems of varied
difficulty levels.
Text Data Augmentation: To effectively train
deep learning models, large datasets are required.
Data augmentation is a machine learning technique
that artificially enlarges the amount of training data
by means of label preserving transformations (Tay-
lor and Nitschke, 2018). (Longpre et al., 2020)
hypothesize that textual data augmentation would
only be helpful if the generated data contains new
linguistic patterns that are relevant to the task and
have not been seen in pre-training. In NLP, many
techniques have been used for generating augmen-
tations, (Wei and Zou) introduced noise injection,
deletion, insertion and swapping of words in text.
(Rizos et al., 2019) used recurrent neural networks
and generative adversarial networks for short-text
augmentation (Maheshwary et al., 2021b). Re-
cently, hard label adversarial attack models have
also been used (Maheshwary et al., 2021a). Other
frequently used methods include inducing spelling
mistakes (Belinkov and Bisk, 2018), synonym re-
placement (Zhang et al., 2016), identifying close
embeddings from a defined search space (Alzan-
tot et al., 2018), round trip translations (Sennrich
et al., 2016), paraphrasing techniques (Kumar et al.,
2019) and words predicted by language model
(Kobayashi, 2018) among many others. These
methods are specific to the task at hand and needs
to be adapted such that the generated augmenta-
tions bring diversity in the concerned dataset.
3 Proposed Augmentation Approach
Data augmentation generates new data by modi-
fying existing data points through transformations
based on prior knowledge about the problem do-
main. We introduce carefully selected transforma-
tions on well known text augmentation techniques
to develop examples suited for the task of MWP.
These transformations help in increasing the diver-
sity and size of problem set in existing datasets.3.1 Problem Definition
A MWP is defined as an input of ntokens, P=
{w, w..w}where each token wis either a nu-
meric value or a word from a natural language. The
goal is to generate a valid mathematical equation
EfromPsuch that the equation consists of num-
bers from P, desired numerical constants and math-
ematical operators from the set {/,∗,+,−,=,(,)}.
LetF:P → Ebe an MWP solver where Eis
the equation to problem P. Our task is to generate
augmented problem statement Pfrom the original
inputPsuch that Pis:(1)semantically similar
to the initial input P,(2)preserves the sequence of
events in the problem statement, (3)keeps the nu-
merical values intact and (4)the solution equation
is same as E.
3.2 Deficiencies in Existing Models
As showcased by (Patel et al., 2021), existing MWP
solvers trained on benchmark datasets like MaWPS
and ASDiv-A focus their attention only on certain
keywords in the problem statement and do not pay
much heed to the question text. We further show
that even after performing significant transforma-
tions on the test set such as (1)dropping the ques-
tion text, (2)randomly shuffling the sequence of
sentences, (3)random word deletion, and (4)ran-
dom word reordering, the solvers are still able to
produce correct equations. Upon introducing these
transformations we should expect a very high drop
in accuracy values as the transformed problems are
now distorted. Surprisingly, the decrease in accu-
racy scores is relatively very less than expected as
shown in Table 2. We only observe a relatively
moderate drop for word reordering. From this anal-
ysis, we can say that instead of focusing on the
sequence of events, question text and semantic rep-
resentation of the problem, solvers pick word pat-
terns and keywords from the problem statement.
We hypothesize that the drop in accuracy for word
reordering experiment indicates that the solvers try
to identify a contiguous window of words having
some keywords and numbers in them, and gener-
ates equation based on these keywords. We further
probe on this hypothesis by visualizing the atten-
tion weights in the experiment section.
3.3 Augmentation Methods
A MWP can also be expressed as P=
(S, S..S, Q)where Qis the question and
(S, S..S)are the sentences constituting the prob-4196
lem description. To mitigate the deficiencies in
MWP solvers, we propose a two stage augmenta-
tion paradigm consisting of primary and secondary
stage. In primary stage, we generate base aug-
mentation candidates which then proceed to the
secondary stage and get modified accordingly to
become potential candidates. After identifying the
potential candidates, we filter out the best candi-
dates using proposed candidate selection algorithm.
Table 1 shows changes in MWP after primary and
secondary stage. Following are the details:
•Primary Stage: In the primary stage, our
focus is on inducing variations in the ques-
tion text Qof a given problem statement P.
For this, we first generate nbase candidates
{b, b, ..., b}fromQusing T5paraphrasing
model (Raffel et al., 2020). The key intuition
behind this step is to ensure that each aug-
mentation of a given problem has a different
question text. This will empower the solver to
learn variations from the question text as well.
•Secondary Stage: After the generation of
base candidates, we implement augmenta-
tion methods to generate potential candidates.
These methods although well known, require
careful tuning to adapt for MWP generation.
Table 3 showcases MWP examples and their
generated augmentations. Detailed descrip-
tion of these techniques follow.3.3.1 Paraphrasing Methods
Paraphrasing has proved to be an effective way
of generating text augmentations (Witteveen
and Andrews). It generates samples having
diverse sentence structures and word choices
while preserving the semantic meaning of the
text. These additional samples guide the model
to pay attention to not only the keywords but its
surroundings as well. This is particularly beneficial
for the task of MWP solving, where most of the
problem statements follow a general structure.
Problem Reordering: Given original prob-
lem statement P= ( S, S, ...S, Q), we
alter the order of problem statement such that
P= (Q, S, S, ..., S). To preserve the seman-
tic and syntactic meaning of problem statement we
use filler phrases like ’Given that’ and ’If-then’. To
make these paraphrases more fluent, we use named
entity recognition and co-reference resolution
to replace the occurrences of pronouns with
their corresponding references. Please note that
this method is better than random shuffling of
sentences as it preserves the sequence of events in
the problem statement.
Round Trip Translations: Round trip transla-
tions, more commonly referred as back-translation
is an interesting method to generate paraphrases.
This idea has evolved as a result of the success
of machine translation models (Wu et al., 2016).
In this technique, sentences are translated from
their original language to foreign languages and
then translated back to the original language. This
round trip can be between multiple languages as
well. The motivation behind using this technique
is to utilize the different structural constructs and
linguistic variations present in other languages.
Back-translation is known to diverge uncontrol-
lably (Tan et al., 2019) for multiple round trips.
This may lead to change in the semantics of the
problem statement. Numerical quantities are frag-
ile to translations and their order and representation
may change. To overcome these challenges, we
worked with languages that have structural con-
structs similar with English. For instance, lan-
guages like Finnish which are gender neutral, can
become problematic as they can lead to semantic
variance in augmented examples. To preserve nu-
merical quantities, we replace them with special
symbols and keep a map to restore numerical quan-4197
tities in the generated paraphrases. We have used
the following round trips:
English - Russian - English: Although Russian
is linguistically different from English, we still
chose it as word order does not affect the syntactic
structure of a sentence in Russian language (V oita
et al., 2019). For single round trip, we preferred
Russian as it has the potential to generate different
paraphrase structures.
English - German - French - English: German and
french are structurally similar to English language
(Kim et al., 2019), we chose them for multiple
round trips to both maintain semantic in-variance
and induce minor alterations in the paraphrases.
3.3.2 Substitution Methods
In this class of methods, the focus is on generating
variations of the problem statement by identifying
and substituting some of the keywords such
that the augmentations are semantically and
syntactically correct, with the equation labels
preserved. Substitution is effective for MWP
solving as it guides the solvers focus away from
certain keywords, allowing it to distribute its
attention and generalize better. We propose thefollowing methods:
Fill-Masking: In this technique, we model the
challenge of generating candidates as a masked
language modelling problem. Instead of randomly
choosing words for masking, we use part of speech
tags to focus on nouns and adjectives, preferably
in the vicinity of numerical quantities. We replace
these identified keywords with mask tokens. These
masked candidate sentences are then passed
through a masked language model (Devlin et al.,
2019a) and suitable words are filled in masked
positions to generate our candidate sentences.
Synonym Replacement: In this method, after
stop-word removal, we select keywords randomly
for substitution. Unlike fill-mask technique, where
masked language models were deployed, here we
use Glove embeddings (Pennington et al., 2014) to
find the top kcandidates that are close synonyms
of the keywords. To ensure syntactic correctness in
candidates, we maintain the part of speech tags for
the substitute candidates. These synonyms are then
used to substitute the keywords in the problem
statement and generate augmented candidates.4198Named-Entity Replacement: A common
occurrence in MWP is the usage of named entities.
These entities play a crucial role in stating the
problem statement, but the solution equations do
not change on altering these entities. Following
this insight, we first identify the named entities
such as person, place and organizations present
in the problem statement. Then we replace these
named entities with their corresponding substitutes,
like a person’s name is replaced by another
person’s name to generate the potential candidates.
Table 4 reports the statistics of augmented datasets
on both MaWPS and ASDiv-A. All the techniques
described in paraphrasing and substitution methods
are used for generating the potential candidates
for a problem statement. After generation of the
potential candidates for augmenting a problem
statement, the best possible candidate is selected
by using Algorithm 1. Key motivation behind
developing this algorithm is to select candidates
on which the solver does not perform well and
which are similar to the original problem statement.
We use negative log likelihood as the loss
function Land Sentence-BERT (Reimers and
Gurevych, 2019) fine tuned on MWP equation gen-
eration task as sentence embedding generator S.
We calculate the similarity of each candidate em-
bedding with the original problem representation
using cosine similarity as shown in Line 3of the
algorithm. Further, for each candidate sentence,
we evaluate their loss values and select the candi-
date with the maximum mean normalized loss and
similarity score.
4 Experiments
Datasets and Models: To showcase the effec-
tiveness of proposed augmentation methods, weAlgorithm 1 MWP Candidate Selection Algorithm
Requires: Mis augmentation method, Sis simi-
larity model, Fis solver model, Lis Loss function.
Input: Problem text P
Output: Augmented Text PE← F (P)Candidates ← M (P)forCin Candidates :do S← S(C,P) L←(L(C)− L(P))/L(P) CandidateScore.add (S∗L)P= arg maxCandidateScore (C)end
select three state-of-the-art MWP solvers: (1)
Seq2Seq (Wang et al.) having an LSTM encoder
and an attention based decoder. (2)GTS (Xie
and Sun, 2019) having an LSTM encoder and a
tree based decoder and (3)Graph2tree (Zhang
et al.) consists of a both tree based encoder and
decoder. Seq2Seq serves as our base model for
experimentation. Many existing datasets are not
suitable for our analysis as either they are in
Chinese (Wang et al.) or they have problems
of higher complexities (Huang et al., 2016) .
We conduct experiments across the two largest
available English language datasets satisfying our
requirements: (1)MaWPS (Koncel-Kedziorski
et al., 2016) containing 2,373 problems (2)
ASDiv-A (Miao et al., 2020) containing 1,213
problems. Both datasets have MWPs with linear
equation in one variable.
Experiment Setup: We train and evaluate the three
solvers on both MaWPS and ASDiv-A using five
fold cross validation. Evaluation is conducted on
both original and augmented datasets. We use the
same hyperparameter values as recommended in
the original implementation of these solvers. Fur-
ther, each solver has been trained from scratch and
by using BERT embeddings (Devlin et al., 2019b).
We also evaluate the models on SVAMP (Patel et al.,
2021) challenge set. This test set has been designed
specifically to examine the robustness and adapt-
ability of the solvers. Ablation studies have been
conducted to assess the effectiveness of candidate
selection algorithm and augmentation techniques.4199
4.1 Results and Analysis
Table 5 shows the result of proposed methods.
These results have been reported on BERT em-
beddings. Table 11 shows a comparison between
training from scratch and using BERT embeddings.
By training these state-of-the-art models on the
augmented dataset we achieve better results for
both MaWPS and ASDiv-A. On average, we were
able to increase the accuracy significantly by more
than five percentage points. Both paraphrasing and
substitution methods have performed well indepen-
dently and in combination. Further, we conduct
ablation studies to analyze the performance of each
augmentation method. In Table 6 we illustrate
some examples on which existing models generate
incorrect equations. However, after being trained
with augmented dataset they generate correct
equations. Additionally, in Problem 2 the base
model generates syntactically incorrect solution,
but post augmentation it generates syntactically
correct equation. These examples show the
increased robustness and solving abilities of
solvers.
Attention Visualizations: Through this investi-
gation, we aim to ascertain our hypothesis that
to generate equations MWP solvers focus only
on certain keywords and patterns in a region.
They ignore essential information like semantics,
sequence of events and content of the question text
present in the problem statement. In Table 7, we
show some sample problem statements with their
attention weights. These weights are generated
during the decoding process using Luong attention
mechanism (Luong et al., 2015). Moreover, to
illustrate the effectiveness of our augmentation
techniques, we show the distribution of attention
weights for models trained on the augmented
dataset. We can infer from the examples showcased
in Table 7 that before augmentation the focus
of the solver is limited to a fixed region around
numerical quantities and it does not pay heed
to the question text. However, after training on
the augmented dataset the solver has a better
distribution of attention weights, the weights are
not localised and and the model is also able to pay
attention on the question text.
Ablation Studies: To assert the effectiveness of
our methods, we conduct the following ablations:4200
Candidate Selection Algorithm: For testing the
usefulness of candidate selection algorithm, we
compare it with a random selection algorithm.
In this, we randomly select one of the possible
candidates as augmented problem statement. We
evaluate the accuracy of models trained on the
augmented datasets, generated using both the
algorithms. Result in Table 8, shows that candidate
selection algorithm performs better than random
selection algorithm and this demonstrates the
effectiveness of our algorithm.
Augmentation Methods: To examine the effec-
tiveness of proposed augmentation techniques, we
evaluate the models on each of the proposed tech-
niques independently and report the results in Table
9. Although, all the methods contribute towards
increase in accuracy but Round trip translations
and synonym replacement perform marginally bet-
ter than others. This behaviour can be linked to
the structural diversity and keyword sensitivity that
round trip translations and synonym replacement
bring respectively (Feng et al., 2021).
SV AMP Challenge Set: SV AMP (Patel et al.,
2021) is a manually curated challenge test set
consisting of 1,000math word problems. These
problems have been cherry picked from MaWPS
and ASDiv-A, then altered manually to modify
the semantics of question text and generate
additional equation templates. This challenge set
is suitable for evaluating a solver’s performance as
it modifies problem statements such that solver’s
generalization can be checked. The results are
shown in Table 10. Although, our proposed
augmented dataset has very limited equation
templates, still it performs comparatively better
than state-of-the-art models on SV AMP challenge
set. This result signifies the need for a larger and
diverse dataset with enhanced variety of problems.
Further, it demonstrates the effectiveness of our
method which is able to perform better on SV AMP
test set and increase model’s accuracy despite the
challenges.
BERT Embeddings: We train the solvers in
two different settings, using pre-trained BERT
embeddings and training from scratch. We chose
BERT specifically as we require contextual
embeddings which could be easily adapted for the
task of MWP. Moreover, existing models have also
shown results using BERT and it would be fair to
compare their performances when trained using
similar embeddings. Results obtained are shown in
Table 11. We observe that for solver’s trained using
BERT, accuracy is higher than models trained from
scratch.4201Human Evaluation: To verify the quality of aug-
mented examples, we conduct human evaluation.
The focus of this evaluation is: (1)To check if the
augmentations will result in the same linear equa-
tion as present in the original problem statement,
(2)To evaluate if the numerical values for each aug-
mentation example is preserved, (3)Evaluate each
sample in the range 0to1for its semantic similar-
ity with the original problem statement, (4)On a
scale of 1to5rate each augmented example for its
grammatical correctness. We conduct the human
evaluations on randomly shuffled subsets consist-
ing of around 40% of the total augmented examples
for both the datasets. This process is repeated three
times with different subsets, five human evaluators
evaluate each example in all subsets, and the mean
results are computed as shown in Table 12.
5 Future Work and Conclusion
We showcase that the existing MWP solvers are
not robust and do not generalize well on even sim-
ple variations of the problem statement. In this
work, we have introduced data augmentation tech-
niques for generation of diverse math word prob-
lems. We were able to enhance the size of existing
dataset by 5 folds and significantly increase the
performance of state-of-the-art solvers by over 5
percentage points. Future works could focus on
developing techniques to generate data artificially
and making robust MWP solvers.
6 Acknowledgment
We thank IHub-Data, IIIT Hyderabadfor financial
support.
References420242037 Appendix
7.1 Implementation Details
For conducting our experiments we have used two
Boston SYS-7048GR-TR nodes equipped with
NVIDIA GeForce GTX 1080 Ti computational
GPU’s having 11GB of GDDR5X RAM. All im-
plementations of training and testing is coded in
Python with Pytorch framework. The number of
parameters range from 20M to 130M for differ-
ent models. We use negative log likelihood as the
loss criterion. Hyper-parameter values were not
modified, and we follow the recommendations of
the respective models. To reduce carbon footprint
from our experiments, we run the models only on
a single fold for searching hyperparameter values.
We chose the number of base candidates after pri-
mary stage nas 7. Generating augmentation ex-
amples using Paraphrasing Methods took around
12 minutes on average for MaWPS and 8 minutes
for ASDiv-A datasets. Substitution methods took
around 5 minutes on average for both MaWPS and
ASDiv-A dataset. The experiments conducted by
us are not computation heavy. Each of the state-
of-the-art models get trained within 5 hrs of time,
with Graph2Tree taking the maximum time.
7.2 Additional Augmented Examples
In this section, we present some additional valid as
well as invalid augmented examples. Additionally,
we also show some more examples with their at-
tention weights. Table 13 shows some additional
examples with their attention weight distribution.
These weights have been shown for the base model
trained before augmentation and after augmenta-
tion on MaWPS dataset. Table 14 illustrates some
additional problem statements for all the techniques
in paraphrasing methods and substitution methods.
In Table 15, we present some invalid augmented ex-
amples which do not satisfy our human evaluation
criteria. These examples are such that they alter the
semantics of the original problem statement.420442054206