
Cong Wang, Zhiwei Jiang, Yafeng Yin, Zifeng Cheng, Shiping Ge, Qing Gu
State Key Laboratory for Novel Software Technology, Nanjing University, China
cw@smail.nju.edu.cn, {jzw,yafeng}@nju.edu.cn,
{chengzf,shipingge}@smail.nju.edu.cn, guq@nju.edu.cn
Abstract
Automated Essay Scoring (AES) aims to eval-
uate the quality score for input essays. In
this work, we propose a novel unsupervised
AES approach ULRA, which does not require
groundtruth scores of essays for training. The
core idea of our ULRA is to use multiple heuris-
tic quality signals as the pseudo-groundtruth,
and then train a neural AES model by learning
from the aggregation of these quality signals.
To aggregate these inconsistent quality signals
into a unified supervision, we view the AES
task as a ranking problem, and design a spe-
cial Deep Pairwise Rank Aggregation (DPRA)
loss for training. In the DPRA loss, we set a
learnable confidence weight for each signal to
address the conflicts among signals, and train
the neural AES model in a pairwise way to
disentangle the cascade effect among partial-
order pairs. Experiments on eight prompts
of ASPA dataset show that ULRA achieves
the state-of-the-art performance compared with
previous unsupervised methods in terms of
both transductive and inductive settings. Fur-
ther, our approach achieves comparable per-
formance with many existing domain-adapted
supervised models, showing the effectiveness
of ULRA. The code is available at https:
//github.com/tenvence/ulra .
1 Introduction
Automated Essay Scoring (AES) that aims to score
the writing quality of essays without human in-
tervention, is an important application of natural
language processing in education. State-of-the-art
AES models are typically trained in a supervised
way with large labeled corpora, comprising essays
and their groundtruth quality scores (Cozma et al.,
2018; Ke and Ng, 2019; Kumar et al., 2022; Wang
et al., 2022). However, collecting labeled essays is
time-consuming and labor-intensive, especially for
essays written specific to new prompts and when
there is no professional scoring staff available.Unsupervised AES can get rid of the require-
ment of groundtruth scores for training, and thus
has significant potential in both scientific research
and practical applications. Its importance can be
summarized in three key aspects: 1) Unsupervised
AES models can handle special scenarios that lack
labeling resource, such as the absence of profes-
sional scoring staff, the need for rapid essay scoring
without timely labeled data, or the cold start scor-
ing of an AES system without historical labeled
data; 2) Unsupervised AES models can serve as
pseudo-label generators or validators for essay scor-
ing based on semi-supervised learning, few-shot
learning, or transfer learning; 3) In practical writ-
ing tests, unsupervised AES models can rapidly
provide a preliminary decision-making basis for
scoring staff prior to scoring.
Early work tackles unsupervised AES by using
the clustering method (Chen et al., 2010). To solve
the problem of unordered clusters (i.e., cannot map
clusters to ordinal scores), Chen et al. (2010) pro-
pose to use a heuristic quality signal the number
of unique term in essay as the initial score of each
essay, and then iteratively propagate the scores to
other essays in the same cluster. However, such un-
supervised clustering process is uncontrollable (i.e.,
there is no guarantee that clusters are generated to-
wards to essay quality). Recently, researchers pro-
pose to use a heuristic quality signal word count as
the weak supervision to train a neural AES model
(Zhang and Litman, 2021). However, they demon-
strate that directly regressing the predicted score to
a real-valued quality signal (i.e., word count) leads
to poor performance.
The above unsupervised AES methods provide a
good idea that heuristic quality signals can be used
as an alternative of groundtruth scores for model
training, but have two major drawbacks. 1) Sig-
nal values are too noisy to be directly regressed to.
Considering that the quality signal and groundtruth
score may have completely different values but sim-13999ilar partial orders, it is better to utilize the partial
orders in signal rather than the values. 2) Single sig-
nal is too weak to provide good supervision. Since
a single quality signal cannot comprehensively de-
scribe the quality of essay, more quality signals
should be introduced to bring stronger and more
robust crowdsourcing-like supervision.
To this end, we propose a novel framework for
Unsupervised AES by Learning from Rank Ag-
gregation (ULRA). The core idea of our ULRA
is to introduce multiple heuristic quality signals
as the pseudo-groundtruth, and then train a neu-
ral AES model by learning from the aggregation
of these quality signals. Specifically, our ULRA
contains a Heuristic Essay Ranking (HER) mod-
ule which views each signal as a ranking metric to
generate multiple rank lists, and a Deep Pairwise
Rank Aggregation (DPRA) module which can ag-
gregate the inconsistent quality signals for model
training. In the HER module, we introduce three
types of classic quality signals for essay ranking.
In the DPRA module, we set a learnable confi-
dence weight for each signal to address the conflicts
among signals, and train the neural AES model in
a pairwise way to disentangle the cascade effect
among partial-order pairs. We conduct experiments
on eight prompts of ASAP dataset, which demon-
strate that our proposed ULRA significantly out-
performs previous unsupervised methods, and can
even achieve comparable performance to many ex-
isting domain-adapted supervised methods.
2 Related Work
Automated Essay Scoring. Early supervised AES
systems are developed with handcrafted features
(Attali and Burstein, 2004; Phandi et al., 2015;
Yannakoudakis et al., 2011). While with the de-
velopment of deep learning, most of recent AES
methods try to use neural model to solve the prob-
lem (Taghipour and Ng, 2016; Alikaniotis et al.,
2016; Dong et al., 2017; Nadeem et al., 2019; Tay
et al., 2018; Wang et al., 2018; Liu et al., 2019;
Uto et al., 2020). These methods are effective
but labor-intensive for labeling. To reduce the re-
liance on labels, the generic method (Attali et al.,
2010), cross-prompt methods (Cao et al., 2020;
Dong et al., 2017; Jin et al., 2018), and one-shot
method (Jiang et al., 2021) are proposed. For unsu-
pervised AES setting, Chen et al. (2010) propose a
voting algorithm that iteratively updates the scores
by the heuristic quality signals. Zhang and Litman(2021) try to directly regress the predicted score to
a real-valued quality signal, but achieve poor per-
formance, because of the weak information of the
quality feature. Overall, the effective unsupervised
AES methods have not been widely explored.
Rank Aggregation. Rank aggregation (RA) is to
aggregate multiple ranked list (i.e. base rankers)
into one single list (i.e. aggregated ranker), which
is intended to be more reliable than the base rankers
(Deng et al., 2014). Many prior work have been
proposed to effectively and efficiently solve the RA
problem. These works can be roughly divided into
three categories, which are the permutation-based
methods (Mallows, 1957; Qin et al., 2010), the ma-
trix factorization methods (Gleich and Lim, 2011),
and the score-based probabilistic methods (Bradley
and Terry, 1952; Luce, 2012; Pfeiffer et al., 2012;
Thurstone, 1927; Chen et al., 2013). The score-
based methods are gradually developed, which usu-
ally predicts a score for each object based on the
base rankers, and obtains the aggregated ranker
based on the scores. Bradley-Terry (BT) model
(Bradley and Terry, 1952) is an early work that is
instructive for the following researches. It is pro-
posed to model a probabilistic relationship between
objects, according to the achieved scores, which is
suitable for pairwise comparison. Then, Thurstone
model (Thurstone, 1927) extends the BT model
by assuming that the score for each object has a
Gaussian distribution. Another important exten-
sion is Crowd-BT (Chen et al., 2013) model, which
assigns a learnable weight for each base ranker,
and optimizes the scores and the weights by active
learning and online learning. Crowd-BT achieves
competitive performance on the task of reading
difficulty ranking, and directly inspires our work.
3 Problem Definition
We firstly introduce some notation and formalize
the unsupervised AES problem. Let X={x}
be a set of essays which are written to a prompt, and
Y={1,2,···, L}be the pre-defined scores. For
unsupervised AES, we are given a set of unlabeled
essays D={x}⊆ X for model training. The
purpose of unsupervised AES is to train a model
Fwith parameters θto predict the score of each
essay x∈ T ⊆ X into the score set Y, by
ˆy=F(x;D)∈ Y. (1)
In this paper, the model Fis a neural AES
model, and we consider two settings of unsuper-14000
vised AES, transductive and inductive. For the
transductive setting, the test set is just the training
setT=D. For the inductive setting, the test set
does not intersect the training set T ∩ D =∅.
4 The ULRA Framework
4.1 An Overview of ULRA
Our ULRA framework involves two stages, model
training and model inference. As shown in Figure 1,
in the model training stage, the ULRA framework
contains two modules: 1) Heuristic Essay Rank-
ingmodule, which can generate partial-order pairs
by ranking essays according to heuristic quality
signals, and 2) Deep Pairwise Rank Aggregation
module, which trains a neural AES model by aggre-
gating the partial-order pairs derived from multiple
quality signals into a unified supervision. In the
model inference stage, considering that the essay
scores predicted by the neural AES model may
have a different range from the pre-defined score
setY, we propose a Scoring Strategy to transform
the predicted scores given by the neural AES model
into the range of the pre-defined score set. In ad-
dition, it should be noted that Figure 1 only shows
the case of transductive setting, while for induc-
tive setting, the trained neural AES model can be
directly used to score unseen essays.4.2 Heuristic Essay Ranking
As illustrated in Figure 1, the heuristic essay rank-
ing module contains three components: quality
signals, essay ranking, and partial-order pairs gen-
eration. Among them, multiple classic quality sig-
nals are introduced to describe the quality of essays
from different aspects. Each quality signal can then
be used to rank essays according to signal values
and generate a rank list. Finally, each rank list can
be transformed into many partial-order pairs for
later model training.
Quality Signals. The quality signals are important
to the ULRA framework, since it provides all super-
vision for model training. To obtain high-quality
supervision, we investigate a lot of studies based
on handcrafted quality signals (Lagakis and Deme-
triadis, 2021; Chen and He, 2013; Uto et al., 2020;
Phandi et al., 2015). Considering that in practical
unsupervised AES, there is no labeled data avail-
able as standard to select the most relevant signals,
we just employ a set of quality signals designed in a
classic work (Chen and He, 2013), which contains
three aspects of signals, i.e., surface, preposition,
and readability. In experiments, we find that not
all of these signals are highly correlated with the
groundtruth score. Moreover, for many of these
signals, they may be highly correlated with the
groundtruth score in one prompt but less correlated14001in other prompts. Such quality signals with un-
stable supervision pose great challenges for the
robustness of model training.
Essay Ranking. Compared with calculating the
quality score of an essay based on its quality sig-
nals, it is easier to judge the relative quality of
two essays based on their quality signals. There-
fore, for each quality signal, we only reserve the
partial-order relationship among essays by rank-
ing the essays. Specifically, we rank the essays
in a batch-wise way. Let B={x}denote an
essay batch with Nessays, and R={r}
denote Kheuristic quality signals. As shown in
Figure 1, we can calculate each quality signal ron
each essay xto get a signal value g=r(x),
which can then be used for essay ranking. For the
k-th quality signal, we can rank all essays xbased
on their signal values gto get a partial-order
rank list x≻··· ≻x, where ≻denotes
the partial-order relation defined by the k-th qual-
ity signal and pdenotes the index of j-th essays
in the rank list. Finally, we can get Krank lists
{x≻··· ≻x}.
Partial-Order Pairs Generation. Considering
that only part of the partial-order information in
each rank list is correct, we transform each rank list
into a set of partial-order pairs, which allows the
incorrect partial-order pairs to be corrected by other
rank lists. Specifically, for a batch B, each rank
list can be transformed into a total of N(N−
1)/2partial-order pairs. Then, we can use a binary
matrix with size of N(N−1)/2×Kto record
the transformed partial-order pairs, that is,
M=/bracketleftbig
1/bracketrightbig, (2)
where i̸=jand 1is an indicator function. M
reflects the partial-order relationship between two
essays in terms of different heuristic quality signals,
and will be used as the supervision information to
train the neural AES model in the next module.
4.3 Deep Pairwise Rank Aggregation
This module mainly deals with how to address the
inconsistent partial-order supervision from multi-
ple quality signals, so that the neural AES model
can learn how to judge the partial-order relationship
of essay quality. To address this problem, we de-
sign a deep pairwise rank aggregation loss, which
set a learnable confidence weight for each signal to
measure the importance of each signal.Neural AES Model. We denote the neural AES
model as F(·)with learnable parameters θ. By
feeding an essay xinto the model, we can get the
predicted score s=F(x)∈R(not the final
score) for the essay x. The neural AES model con-
sists of two components, an essay encoder which
maps the essay into an essay embedding, and a
fully-connected ( fc) layer which maps the embed-
ding into a predicted score.
Confidence Weight. Considering that two signals
may provide opposite partial order for an essay
pair, we expect to measure which one is more trust-
worthy. Therefore, we set a learnable confidence
weight ηfor the k-th quality signal to measure its
confidence. The learnable weight ηcan be defined
as the probability that the partial-order information
in the k-th rank list agrees with the groundtruth
score. Inspired by Crowd-BT (Chen et al., 2013),
we formalize ηas
η≡P (x≻x|x≻x), (3)
where x≻xandx≻xdenote the partial-
order relationship between two essays in the k-th
rank list and the ground truth score, respectively. In
ULRA, ηis generated by applying sigmoid func-
tion on learnable parameters W={c···, c},
η= sigmoid( c)∈[0,1], (4)
where cis a learnable parameter and is optimized
with model parameter θtogether.
Deep Pairwise Rank Aggregation Loss. Based
on the partial-order pairs derived from multiple
signals and the confidence weight corresponding to
each signal, we can define a special Deep Pairwise
Rank Aggregation (DPRA) loss for model training.
Specifically, given an essay pair (x, x), we
can use the neural AES model to get their pre-
dicted scores sands, respectively. Inspired by
the Bradley-Terry model for paired comparisons
(Bradley and Terry, 1952), we can define the pre-
dicted probability of x≻xas
P (x≻x) = sigmoid ( s−s). (5)
Ifs≫s,P(x≻x)tends to be 1; If s≪s,
P(x≻x)tends to be 0; While s=s,P(x≻
x) = 0 .5.
To further get the predicted probability of the
partial-order pair x≻xgenerated by the k-th14002
quality signal, we can make use of the confidence
weights and apply the law of total probability as
P(x≻x)
=P(x≻x|x≻x)·P(x≻x)
+ P(x≻x|x≺x)·P(x≺x)
=P(x≻x|x≻x)·P(x≻x)
+ P(x≺x|x≻x)·P(x≺x)
=η·P(x≻x) + (1 −η)·P(x≺x).(6)
Here, P(x≻x)is the predicted probability of
x≻x, the label of which is 1. Then, the
loss function for s,s, and ηcan be formulated
as a negative log likelihood loss, which is
L(s, s, η) =− 1log P( x≻x).(7)
For each essay batch B, a set of N(N−1)/2
partial-order pairs are obtained, which is denoted
asS. Based on the supervision of M, the loss
function can be formulated as
L=/summationdisplay/summationdisplay−M·log P( x≻x).
(8)
4.4 Scoring Strategy
Considering that the range of predicted score is not
constrained during training process, the predicted
score can be any real number. Therefore, we should
cast the predicted score sof an essay xinto the
range of the pre-defined score set Y={1,···, L}
to get the final scores ˆy∈ Y. Here, we can get
the final score ˆyofxby min-max transformation/bracketleftig
(L−1)/bracketrightig
+ 1.
5 Experiments
5.1 Dataset and Evaluation Metric
Experiments are conducted on the Automated Stu-
dent Assessment Prize(ASAP) dataset, which is
widely used for the AES task. A total of 12,978 es-
says in ASAP are divided into 8 different sets, each
of which corresponds to a prompt. The statistics of
the dataset is shown in Table 1.
Quadratic Weighted Kappa (QWK) is adopted
to be the evaluation metric. Specifically, given a
score set Y={1,···, L}, QWK is calculated to
measure the automated predicted scores (Rater A)14003
and the resolved human scores (Rater B),
κ= 1−/summationtextw·O/summationtextw·E(9)
where w= (i−j)/(L−1)is the difference
between scores of the raters, Ois an L-by-Lhis-
togram matrix, Ois the number of essays that
received a score iby Rater A and a score jby Rater
B, and Eis the normalized outer product between
each rater’s histogram vector of scores.
5.2 Implementation Details
Quality Signals Setting. The employed 20 quality
signals, which are commonly used in some earlier
work (Chen and He, 2013; Uto et al., 2020; Phandi
et al., 2015), fall into following three categories:
•Surface Signals : character number (CH), word
number (W), commas number (CO), and numberof unique words (UW);
•Preposition Signals : number of noun-plural
words (NNP), number of determiner words (DT),
number of noun-singular words (NN), number of
adverb words (RB), number of adjective words
(JJ), and number of preposition/subordinating-
conjunction words (IN);
•Readability Signals : Gunning Fog (GF) index
(Gunning, 1969), SMOG index (Mc Laughlin,
1969), RIX (Anderson, 1983), Dale-Chall (DC)
index (Dale and Chall, 1948), wordtype num-
ber (WT), sentence number (S), number of long
words (LW), number of complex words (CW),
number of non-basic words (NBW), and number
of difficult words (DW).
In Table 2, we demonstrate QWK between each sig-
nal and the groundtruth of each prompt. It indicates
that single quality signal carry noisy partial-order14004
information of groundtruth scores, which results in
poor performance.
Dataset Setting. For the transductive setting, the
model is trained on the entire dataset (w/o labels),
and is tested on the entire dataset, which means
that all test essays have been seen during training.
For the inductive setting, the dataset (w/o labels)
is divided into the training set, the validation set,
and the test set in a ratio of 6:2:2, which means that
all test essays have not been seen during training.
Due to the unsupervised setting, the validation set
is useless and therefore discarded for ULRA.
Model Setting. The model is implemented by
PyTroch (Paszke et al., 2019) and Higgingface
Transformers (Wolf et al., 2020) libraries. BERT
(Devlin et al., 2019) with pretained parameters
bert-base-uncased is adopted as the essay encoder,
whose hidden size is 712. The essay embedding is
achieved by mean-pooling the token embeddings of
BERT output. The fclayer of the neural AES model
maps the essay embeddings into scalars. Each con-
fidence weight ηis initialized as 0.9.
Training. AdamW (Loshchilov and Hutter, 2017)
is adopted as the optimizer, whose weight decay is
set as 5e-4. The learning rates for the neural AES
model and all ηare 5e-5 and 5e-2, respectively.
The batch size is set as 32. Our model is trained for
30 epochs, and the model which achieves minimum
loss is selected to report the results.
5.3 Comparison Methods
We mainly compare our method with previous un-
supervised AES methods, Signal Clustering (Chen
et al., 2010) and Signal Regression (Zhang and Lit-
man, 2021). Considering that they only employ onequality signal as supervision, we extend them by in-
troducing the 20 signals we used into their method.
Four variants are tested: (1) averaged signal as su-
pervision , (2)averaged output as prediction , (3)ag-
gregated signal as supervision , and (4) aggregated
output as prediction . Here, aggregated means that
multiple rank lists are aggregated into one rank lists
based on a rank aggregation algorithm (Chen et al.,
2013). We also list two additional baselines, which
directly apply the mean and maximum of the 20
quality signals as the predicted scores, respectively.
Moreover, we list the performance of several state-
of-the-art supervised methods (including general
supervised, cross-prompt, and one-shot).
5.4 Performance Comparison
We can find that ULRA outperforms all unsu-
pervised methods with a large improvement, and
achieves the average QWK of 0.615 and 0.614 un-
der transductive (Table 3) and inductive (Table 4)
settings, respectively. It indicates that ULRA can
perform well on both seen andunseen essay sets.
Compared with the cross-prompt and one-shot
methods, we can find that ULRA achieves compet-
itive performance, which is only 0.047 and 0.073
lower than that of the cross-prompt and one-shot
methods, respectively. By observing the general
supervised methods, we can find that the perfor-
mance of ULRA is still much lower than theirs,
due to the lack of strong supervision. But on
some prompts, ULRA achieves comparable per-
formance with the handcrafted features-based su-
pervised method BLRR (e.g., prompt 1 and 3).
By observing the variants of two unsupervised
methods, we can find that both unsupervised meth-
ods achieve improvements after introducing 2014005
quality signals. Among the four variants, two ag-
gregated variants outperform two averaged vari-
ants. It indicates that the aggregation operation is
better than the averaging operation, no matter as
supervision or as prediction.
5.5 Ablation Study
We firstly study the effect of confidence weight η
and neural model on the performance. As shown
in Table 5, by replacing learnable ηwith fixed
η= 1, the performance drops a lot. It indicates
that the learnable ηcan address conflicts among
inconsistent signals. The performance also drops
a lot when using the non-pretrained encoder, or
directly setting the essay scores sas learnable
parameters. It indicates that a good essay encoder
can make full use of the textual information of
essays to improve scoring performance.
We then study the effect of signals on the per-
formance by removing some types of signals from
supervision. As show in Table 5, the performance
drops by about 0.02 after removing one type, and
continues to drop after further removing another
one. It indicates that all three types of quality sig-
nals are useful for model training.
We finally study the effect of using the four vari-
ants on the performance. As shown in Table 5,
using the same 20 signals, aggregating signals dur-
ing training (i.e., ULRA) is superior to aggregat-
ing before training (i.e., averaged/aggregated sig-
nal as supervision ) or during inference (i.e., aver-
aged/aggregated output as prediction ).
5.6 Model Analysis
Effect of More Unlabeled Essays. We study the
impact of varying the number of unlabeled es-
says on the performance of ULRA, i.e., whether
our ULRA requires numerous unlabeled essays to
achieve a good enough performance. To this end,
we vary the ratio of essays for training from 0.2 to
1.0 step by 0.2. As shown in Figure 2(a), we can
find that the lines show a trend that goes up first
and then keeps stable after the ratio about 0.6. It
indicates that about 60% of unlabeled essays are
sufficient to train a good enough ULRA model.
Effect of More Training Pairs. We study the im-
pact of varying the number of training pairs on the
performance of ULRA, i.e., whether our ULRA
framework can benefit from more training pairs.
To this end, we vary the batch size from 2 to 32,
so that the number of training pairs in a batch will
accordingly vary from 1 to 496. As shown in Fig-
ure 2(b), we can find that all the lines show a trend
that goes up. It indicates that a larger number of
training pairs can lead to better performance.
Effect of Weak Signals. We study the impact of
weak signals (i.e., low correlation with groundtruth14006
scores) on the performance of ULRA. To this end,
we add additional 0 to 80 weak signal(s) to the 20
quality signals. Note that the details can be seen
in Appendix A. As shown in Figure 2(c), we can
find that almost all lines show an overall downward
trend. It indicates that weak signals will weaken the
supervision and thus reduce the model performance.
However, when between 0 and 10, all lines do not
go down too much, and some lines even go up (e.g.
prompt 1, 3, and 4). It indicates that ULRA is
robust to weak signals if the weak signals are not
yet dominant the signal set.
Effect of More Signals. We study the impact of the
number of employed signals on the performance
of ULRA, assuming that these signals have similar
correlations with the groundtruth scores. To this
end, we conduct experiments based on the best-
Nquality signals and the worst- Nquality signals,
according to QWKs with the groundtruth scores
which are shown in Table 2. By varying Nfrom 1
to 10, as shown in Figure 3, we can find that all the
lines of best- Nand most lines of worst- Nshow
an upward trend. It indicates that more signals can
often lead to better performance. By comparing
the lines of best- N(with blue color) and worst- N
(with red color), we can find that in most prompts,
the performance differences between best- Nand
worst- Ndecrease with the growth of N. This
may be because that more signals can help bet-
ter address conflicts among signals, and therefore
achieve more robust performance.
Effect of Confidence Weights. We study the im-
pact of the learnable confident weights in ULRA.
To this end, we calculate the Spearman’s correla-
tion coefficient the learned confidence weights and
the corresponding QWKs listed in Table 2 under
each prompt. As shown in Table 6, we can find
that they are highly correlated under both transduc-
tive and inductive settings, which indicate that the
learned confidence weights can indeed reflect the
confidences of quality signals.
Effect of Different Scoring Strategies. We study
the impact of different scoring strategies on the
performance of ULRA. To this end, we test other
four scoring strategies, which conduct score trans-
formation based on predefined distributions. These
distributions include groundtruth distribution (G),
normal distribution (N), triangle distribution (T),
and uniform distribution (U). Note that the de-
tails can be seen in Appendix B. As shown in Ta-
ble 7, we can find that our scoring strategy outper-
forms all strategies except for the strategy based on
groundtruth distribution. It indicates that our scor-
ing strategy can adaptively learn the score distri-
bution and ULRA can achieve better performance
once the distribution of groundtruth score is known.
Groundtruth as Signal. In our ULRA framework,
the applied 20 heuristic quality signals contain the
information about score ranking, but also noise. We
want to explore how the model would perform if the
input signal contains no noise at all. Therefore, we
conduct experiments by feeding the groundtruth
scores as the signal in our ULRA. As shown in
Table 8, the average QWK of our ULRA is 0.220
and 0.049 lower than which applies the groundtruth
scores under the transductive and inductive settings.
It indicates that the signals with less noise can help
model to achieve better performance.
6 Conclusion
In this paper, we aim to perform essay scoring
under the unsupervised setting. To this end, we
propose a novel ULRA framework to train a neural
AES model by aggregating the partial-order knowl-
edge contained in multiple heuristic quality signals.
To address the conflicts among different signals
and get a unified supervision, we design a deep
pairwise rank aggregation loss for model training.
Experimental results demonstrate the effectiveness
of ULRA for unsupervised essay scoring.14007Limitations
Although our ULRA outperforms all unsupervised
baseline methods, there are still some limitations.
The first limitation is that there is still a gap be-
tween the performance of our unsupervised method
and that of some supervised methods. Although our
ULRA can complete the AES task without label an-
notations, it is still worth exploring an unsupervised
AES method whose performance is comparable to
the state-of-the-art supervised method.
The second limitation is that the essay encoder
which adopted in our ULRA (i.e., BERT) is pre-
trained on the English-based corpora, and the es-
says for training is also written by English. Thus,
our ULRA works mostly for English, which means
a well-trained ULRA model may fail to perform
well on the essays written by other languages. An
unsupervised AES system which supports multiple
languages needs to be further explored.
The third limitation is that it requires about 25G
GPU memory for training, which may fail on de-
vices with small GPU memory. A possible solution
is to set a smaller batch size, but this may take
longer time. However, the evaluation process only
requires about 2G GPU memory, which can run in
most of GPU devices, or even CPU devices.
Acknowledgements
This work is supported by National Natural Science
Foundation of China under Grant Nos. 61972192,
62172208, 61906085, 41972111. This work is par-
tially supported by Collaborative Innovation Center
of Novel Software Technology and Industrializa-
tion.
References1400814009
A Details of Weak Signals
In Section 5.6, we study the impact of using weak
signals on the performance of our ULRA. We add
additional 0 to 80 weak signals into the set of em-
ployed 20 quality signals. These 80 weak signals
include: (1) mean of characters per word, (2) vari-
ance of characters per word, (3) mean of word per
sentence, (4) variance of word per sentence, (5)
mean of NNP per sentence, (6) number of PRP, (7)
mean of PRP per sentence, (8) number of NNS, (9)
mean of NNS per sentence, (10) number of VBZ,
(11) mean of VBZ per sentence, (12) mean of DT
per sentence, (13) mean of NN per sentence, (14)
mean of RB per sentence, (15) number of POS,
(16) mean of POS per sentence, (17) number of
TO, (18) mean of TO per sentence, (19) number of
VB, (20) mean of VB per sentence, (21) mean of
JJ per sentence, (22) number of VBP, (23) mean of
VBP per sentence, (24) mean of IN per sentence,
(25) number of CC, (26) mean of CC per sentence,(27) number of VBG, (28) mean of VBG per sen-
tence, (29) number of VBN, (30) mean of VBN per
sentence, (31) number of WP, (32) mean of WP
per sentence, (33) number of MD, (34) mean of
MD per sentence, (35) number of WRB, (36) mean
of WRB per sentence, (37) number of VBD, (38)
mean of VBD per sentence, (39) number of NNPS,
(40) mean of NNPS per sentence, (41) number of
CD, (42) mean of CD per sentence, (43) number of
JJR, (44) mean of JJR per sentence, (45) number
of JJS, (46) mean of JJS per sentence, (47) num-
ber of RBR, (48) mean of RBR per sentence, (49)
number of RBS, (50) mean of RBS per sentence,
(51) number of RP, (52) mean of RP per sentence,
(53) number of EX, (54) mean of EX per sentence,
(55) number of WDT, (56) mean of WDT per sen-
tence, (57) number of UH, (58) mean of UH per
sentence, (59) number of PDT, (60) mean of PDT
per sentence, (61) number of LS, (62) mean of LS
per sentence,(63) mean of clause per sentence, (64)
mean of clause length, (65) number of maximum
of clause per sentence, (66) mean of tree depth of
sentences, (67) mean of average leaf depth of sen-
tences, (68) number of error words, (69) ratio of
stop words, (70) ratio of positive sentiment, (71)
ratio of negative sentiment, (72) ratio of neutral
sentiment, (73) ratio of compound sentiment, (74)
Kincaid index, (75) ARI index, (76) Coleman-Liau
index, (77) Flesch Reading Ease index, (78) LIX,
(79) sentence beginnings with pronoun, and (80)
sentence beginnings with preposition.
Through comparing Table 9 and Table 2, we can
find that the 80 weak signals are less correlated
with the groundtruth scores, compared with the 20
quality signals used in ULRA.
B Details of Different Scoring Strategies
In Section 5.6, we study the impact of different
scoring strategies on the performance of our ULRA.
We test other four scoring strategies, which con-
duct score transformations based on predefined dis-
tributions (i.e., groundtruth distribution, normal14010distribution, triangle distribution, and uniform dis-
tribution).
For the scoring strategy based on groundtruth
distribution , we denote the distribution of the
groundtruth labels in Xas{a,···, a}, where
ais the number of essays with the score i∈ Y.
We fist rank all essays in Xaccording to their pre-
dicted scores {s}, and get a rank list of essays
{x,···, x}, where ris the rank index. Fi-
nally, for each essay x, if its ranking index r
satisfies/summationtexta< r≤/summationtextafort∈ Y, the
corresponding final score ˆyis set as t.
For the scoring strategy based on normal distri-
bution , we first rank all essays in Xaccording to
their predicted scores {s}, and get a rank list of
essays {x,···, x}, where ris the rank index.
Next, we use the normal distribution N(,1)to
calculate the proportionof samples in i-th final
score to the total number of samples after the score
transformation for all i∈ Y, which is
ϕ= exp/parenleftigg
−/parenleftbigg
i−1−L−1
2/parenrightbigg
/2/parenrightigg
,(10)
Φ=ϕ//summationdisplayϕ, (11)
where Φis the proportion of samples in i-th final
score to the total number of samples. Then, the
sample number in i-th final score after the score
transformation is
Ψ=⌊NΦ⌋, (12)
where⌊·⌋is the floor function. Finally, for each es-
sayx, if its ranking index rsatisfies/summationtextΨ<
r≤/summationtextΨfort∈ Y, the corresponding fi-
nal score ˆyis set as t. Note that we additionally
define Φ= 0.
For the scoring strategy based on triangle dis-
tribution , we first rank all essays in Xaccording
to their predicted scores {s}, and get a rank
list of essays {x,···, x}, where ris the rank
index. Then, we use the triangle distribution to
calculate the proportion of samples in i-th final
score to the total number of samples after the score
transformation for all i∈ Y, which is
ϕ=−/vextendsingle/vextendsingle/vextendsingle/vextendsinglei−1−L−1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle+L+ 1
2. (13)Same as the scoring strategy based on normal distri-
bution, the sample number in i-th final score after
the score transformation is
Ψ=⌊Nϕ//summationdisplayϕ⌋. (14)
Finally, for each essay x, if its ranking index r
satisfies/summationtextΨ< r≤/summationtextΨfort∈ Y, the
corresponding final score ˆyis set as t. Note that
we additionally define Ψ= 0.
For the scoring strategy based on uniform dis-
tribution , we fist rank all essays in Xaccording to
their predicted scores {s}, and get a rank list of
essays {x,···, x}, where ris the rank index.
The final score ˆyofxis set as ⌊r⌋+ 1.14011ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section Limitations
/squareA2. Did you discuss any potential risks of your work?
Our work provides methodological contributions that do not have direct boarder impacts. Although
our work might indirectly lead to future researches and applications, it is premature to predict their
positive or negative impacts.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 5.1
/squareB1. Did you cite the creators of artifacts you used?
Section 5.1
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 5.1
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 5.1
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Section 5.1
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 5.1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 5.1
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 5.2 & Section Limitations14012/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 5.2
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 5.2
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 5.2
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.14013