
Nadezhda ChirkovaGermán KruszewskiJos RozenMarc DymetmanNaver Labs EuropeIndependent Researcher
{nadia.chirkova, german.kruszewski, jos.rozen}@naverlabs.com
marc.dymetman@gmail.com
Abstract
Autoregressive language models (LMs) map to-
ken sequences to probabilities. The usual prac-
tice for computing the probability of any char-
acter string (e.g. English sentences) is to first
transform it into a sequence of tokens that is
scored by the model. However, there are expo-
nentially many token sequences that represent
any given string. To truly compute the proba-
bility of a string one should marginalize over
all tokenizations, which is typically intractable.
Here, we analyze whether the practice of ig-
noring the marginalization is justified. To this
end, we devise an importance-sampling-based
algorithm that allows us to compute estimates
of the marginal probabilities and compare them
to the default procedure in a range of state-of-
the-art models and datasets. Our results show
that the gap in log-likelihood is no larger than
0.5% in most cases, but that it becomes more
pronounced for data with long complex words.
1 Introduction
Language models are probability distributions over
text strings. In practice, these distributions are de-
fined over a vocabulary of tokens , such as words,
punctuation marks, and other special symbols (Ju-
rafsky, 2000; Goldberg, 2017). As long as a unique
token sequence encodes any given string, the proba-
bility of a string according to the language model is
equal to the probability of the corresponding token
sequence. However, with today popular sub-word-
level tokenizations this is not the case, as there
are (exponentially) many possible tokenizations for
any given string. For example, with the vocabulary
V={a, ab, b, c, ca, cab }, the string “cab” can be
tokenized into cab, c/a/b, ca/b, c/ab . Therefore,
thetrueprobability that the language model assigns
to the corresponding string is that obtained after
marginalizing over all possible tokenizations . Yet,
the common practice disregards this fact, comput-
ing the string probability by scoring a single default
tokenization (e.g., cab). The implicit assumptionFigure 1: Illustration of the proposed procedure for
sampling tokenization Tand calculating its proposal
probability Q=Q(T|S)from a sequence of blocks B,
produced by splitting sequence S.
from the community is that the probability mass of
non-default tokenizations is negligible. However,
this assumption has not been adequately evaluated
yet.
In part, Cao and Rimell (2021) addressed this
very same question, by conducting a pioneer
study to quantify the gap between the default and
marginalized probabilities. Their experiments with
Transformer-XL pretrained on the WMT data (En-
glish and German) show negligible changes in per-
plexity with respect to using a single default to-
kenization for in-domain data and 0.9–1.9% im-
provement in perplexity for out-of-domain data,
such as arXiv articles. Because exact marginaliza-
tion is intractable in practice, marginalized proba-
bilities were estimated using importance sampling.
Importance sampling computes an unbiased esti-
mate of the marginalized probabilities as an aver-
age over tokenizations sampled from a proposal
distribution. Cao and Rimell (2021) exploited
the probabilistic nature of the UnigramLM tok-
enizer (Kudo, 2018) to define such a proposal. As a
consequence, their results do not necessarily ex-
tend to the more popular language models like
GPT-2 (Radford et al., 2019), GPT-3 (Brown et al.,
2020), BLOOM (Scao et al., 2022), T5 (Raffel
et al., 2020), among others, trained using other to-
kenization schemes such as BPE (Sennrich et al.,
2016), WordPiece (Schuster and Nakajima, 2012),
among others.1In this work, we devise a new proposal distribu-
tion that allows us to quantify the effect of marginal-
ization for any given tokenizer. Equipped with
this algorithm, we inspect the effect of marginal-
ization over tokenizations for two LMs, GPT-2
(126M parameters, English) and the recently re-
leased BLOOM (1.7B parameters, multilingual),
on various domains and languages. Our importance
sampling estimates show that in practice marginal-
ization does not influence log-likelihood much
(usually less than 0.5% improvement), the high-
est influence (1–2% improvement) being for data
with long, complex words and distribution shift.
Because the results will vary for different models
and data, we provide a tool for researchers and prac-
titioners to measure the gap in their specific setting
to decide whether the usual practice is warranted.
To this end, we release our code, which can be ap-
plied to models from the transformers library.
2 Methodology
2.1 Preliminaries
Let us consider a sequence of characters Sthat
we wish to score with an autoregressive language
model P. Typically, Sis split into a sequence
T=t, . . . , tof tokens t∈V, where Vis the
model’s vocabulary, a process commonly known
astokenizing the sequence. Then we can compute
a score for a tokenization Tof the sequence S,
P(T, S), using the chain rule:
P(T, S) = 1[T→S]/productdisplayP(t|t, . . . , t)
where T→Sindicates that Tis a valid tokeniza-
tion of S. Commonly used tokenization algorithms
such as BPE or WordPiece provide a determinis-
tic procedure for obtaining a particular way of
tokenizing SintoT, which we refer to as the de-
fault tokenization. Yet, in general, for the same se-
quence, there exist (exponentially) many possible
tokenizations with vocabulary V, which also typi-
cally receive some probability mass by the LM. To
obtain the trueprobability score for the sequence S,
we should marginalize over all valid tokenizations:
P(S) =/summationtextP(T, S).
However, computing P(S)is typically in-
tractable given the exponential number of validAlgorithm 1 Proposal algorithm
tokenizations. Nonetheless, this value can be es-
timated through importance sampling, as follows.
Introducing a proposal distribution Q(T|S)over
all tokenizations Tof a sequence S, such that
P(T, S)>0⇒Q(T|S)>0, we can rewrite
the probability P(S), as follows:
P(S) =/summationdisplayP(T, S) =EP(T, S)
Q(T|S)(1)
Now we can estimate P(S)by sampling Kinde-
pendent tokenizations from the proposal:
P(S)≈1
K/summationdisplayP(T, S)
Q(T|S), T∼Q(T|S)(2)
The quality of this estimate depends on the cho-
sen proposal distribution: the closer the pro-
posal Q(T|S)is to the true posterior distribution
P(T|S), the smaller the variance of the unbiased
estimate (2) tends to be.
2.2 Proposed approach
We introduce a novel proposal Q(T|S)based on
the LM itself with the intention to make it naturally
closer to the posterior. Importantly, this proposal
can be used for anytokenizer enabling its applica-
tion to well-known state-of-the-art systems. The
procedure for sampling from this proposal is pre-
sented in Algorithm 1 and also illustrated in Figure
1. In summary, the algorithm samples a tokeniza-
tionTby building it incrementally as the concate-
nation of token subsequences T. Each token subse-
quence is sampled from the language model while2always ensuring that the resulting tokenization is
valid for the target S. To achieve this, the algorithm
breaks Sinto a sequence of character blocks B, and
only samples tokenizations Tthat are valid for the
corresponding block B. Notably, in the extreme
case of splitting Sinto a single block B=S,
our proposal Q(T|S)turns into the true posterior
P(T|S), allowing to compute the exact marginal-
ization with a single sample, as noted in footnote 2.
However, because sampling a valid tokenization of
a block requires renormalizing over allsuch valid
tokenizations, this extreme instantiation would de-
feat the purpose of the algorithm as it would be
equivalent to computing the full marginalization.
Instead, we consider block sizes over which we
can practically compute the renormalization con-
stant by, for example, using whitespace-separated
words as blocks. Still, because this can sometimes
lead to impractically-sized blocks with a number of
tokenizations that can exceed what we can reason-
ably score with a LM, we limit the maximum block
size to a parameter Land we only score the top M
block tokenizations inversely sorted by their num-
ber of tokens. The resulting algorithm requires
O(|B| ×M)evaluations of the LM per-sample,
where|B|is the number of blocks used to split the
sequence S. In Appendix E, we validate that, for
short sentences with a tractable amount of possible
tokenizations, for which we can actually compute
the true value of the marginalization, our algorithm
provides quite precise estimates.
3 Experiments
Experimental setup. We experiment with two
language models, GPT-2 (Radford et al. 2019,
126M parameters, English) and the recently re-
leased BLOOM (Scao et al. 2022, 1.7B parame-
ters, 45 natural and 12 programming languages).
We select the following datasets for evaluating
the LMs, which cover different styles and lan-
guages: Wikipedia articles (En), Twitter posts
(En), CNN news (En), Transcriptions of White
House Speeches (En), Flores-200 (sentences from
Wikipedia in various languages, including high-
resource, low-resource, latin and non-latin scripts),
Python and C++ code (one recently released repos-
itory for each language). We concatenate texts into
sequences of length 800 tokens (as measured by
the default tokenization) to provide longer context
for the LM. We evaluate on 100 sequences per
dataset (Flores-200, CNN news and Code datasets
are shorter). We refer to Appendix A for more de-
tails on the data and how we check that the LMs
were not trained on the evaluation data.
We measure the cross entropy (in BPC) be-
tween the data and the model according to the de-
fault tokenization ( BPC) and between the data
and the marginalized model according to the im-
portance sampling estimate ( BPC), as well as
their difference BPC−BPCreferred to as
the BPC gap, and also the normalized difference
(BPC−BPC)/BPC(relative BPC gap). Fur-
thermore, we compute a 90% confidence inter-
val[BPC,BPC]around BPC, using boot-
strap resampling (Wasserman, 2004, Chapter 8)
forn= 1000 trials. Additionally, we report the
proportion of blocks for which our algorithm sam-
ples non-default tokenizations (%ND).
As for hyperparameters, we use M= 128 and
choose Lto be the maximum token length in the
default tokenization of the evaluation data. We
provide empirical validation for both these hyper-3
parameters in Appendices D and C, respectively.
We sample K= 30 tokenizations per sequence.
Results Table 1 presents our main results. We
generally observe a low relative BPC gap ( <
0.5%), but in some cases exceeding 1%, e.g. 1.3–
1.5% on Twitter, 2% on transcribed speech data,
1.3% on the Basque language (Eus) or 1% on the
Urdu language (Urd). We note that dataset/model
pairs with higher relative gap tend to be con-
nected with low-resource languages (Basque and
Urdu), non-latin scripts (Urdu and Chinese), and
data distribution shift (transcribed speech, Twit-
ter). Moreover, we observe a higher gap to be
associated with a higher percentage of non-default
tokenizations sampled by our algorithm (%ND).
To learn more about the factors driving the proba-
bility of sampling the default tokenization, we bin
blocks (which roughly correspond to words) from
Wikipedia by the probability that our proposal as-
signs to their default tokenization, Q(df.), when
using GPT-2 as a model. Table 2 shows a few
examples of blocks from each bin alongside the
bin’s frequency. As can be seen, high probability
of sampling the default tokenization usually corre-
sponds to common and simple words, whereas low
probability corresponds to complex and rare words.
From this observation, we conjecture that higher
gaps are at least in part driven by the presence of
long complex words in the datasets.
Finally, Figure 2 visualizes confidence intervals
on BPC gaps for individual sequences across sev-
eral datasets. Additional results are given in Ap-
pendix F. In particular, we plot the left limit of
the confidence interval for the BPC gap ( BPC−
BPC) on the x-axis and the width of the interval
(BPC−BPC) on the y-axis (non-negative by
definition). If a dot is located to the right of 0, it
means that we are highly confident that the BPC
gap is positive on that individual sequence. The
farther the dot is on the x-axis, the higher the cor-
responding BPC gap is. Likewise, the lower the
value on the y-axis, the lower is the variance of our
estimate of the marginalized probability and, conse-
quently, of the BPC gap. As can be seen, we obtain
low-variance predictions for most of the sequences,
and for almost all of them we can observe a positive
BPC gap. Moreover, we can note a distributional
difference between dataset/model pairs with a low
BPC gap (such as those on the right-hand side of
Figure 2, with points concentrated close to the 0
value) and those with high BPC gap (such as those
represented on the left-hand side of Figure 2, with
points spread up to the right).
4 Related Work
Stochastic tokenization or marginalisation over to-
kenizations were widely investigated in the context
of model training (Grave et al., 2019; van Mer-
riënboer et al., 2017; Buckman and Neubig, 2018;
Provilkov et al., 2020; Kudo, 2018) or learning bet-
ter tokenizers (He et al., 2020); in contrast, we eval-
uate the effect of marginalization at the inference
stage, when the tokenizer and the LM were trained
in the default, commonly-used way. The closest
study to ours is Cao and Rimell (2021), which relies
on the stochastic version of the UnigramLM tok-
enizer as their proposal Q, and thus their approach
is inapplicable to LMs with other tokenizers. They
also had to introduce a set of heuristics such as im-
posing consistent tokenization of repeated words
or enforcing the default tokenization to be included
among the sampled tokenizations, to make this pro-
posal closer to the posterior and to decrease the
variance of importance sampling.45 Conclusion
In this work, we have studied the effect of marginal-
ization over possible tokenizations in language
modeling. For this, we introduced a novel proposal
distribution over tokenizations, which is used in the
importance sampling algorithm to obtain estimates
of the marginalized probability, and that can be
applied to any tokenizer and language model. Our
results show that the overall effect of marginaliza-
tion over tokenizations is often smaller than 0.5%,
although it becomes more pronounced for data with
long complex words or distribution shift. We re-
lease our code to allow practitioners to check the
effect of marginalization for their models of inter-
est.
Limitations
The main limitation of the proposed approach is
that it would be relatively costly to apply at produc-
tion time, compared to the conventional LM eval-
uation. First, it requires drawing a number of tok-
enization samples, as defined by importance sam-
pling, in contrast to a single pass through the evalu-
ated sequence in the conventional approach. Sec-
ond, the conventional approach can be conducted
with teacher forcing and efficiently parallelized,
while the proposed approach relies on block-by-
block sequential processing. Nonetheless, the pro-
posed algorithm is designed for analysis purposes
rather than to be used in production systems, for
which it is feasible to run it in a reasonable time,
allowing users to evaluate the effect of marginal-
ization for any tokenizer and language.
Broader impact
As the work is dedicated to evaluating existing
models on publicly available datasets, we are not
aware of any potential negative impact.
Acknowledgements
We would like to thank Matthias Gallé for his valu-
able feedback.
References56A Data
We consider the following datasets:
•Wikitext ( h t t p s : / / h u g g i n g f a c
e . c o / d a t a s e t s / w i k i t e xt ,
wikitext-2-raw-v1 test subset, Merity
et al., 2016, CC BY-SA 4.0 license);
•Twitter posts ( https://huggingface.
co/datasets/tweet_eval ,emoji
test subset, Mohammad et al., 2018);
•CNN News ( https://www.kaggle.c
om/datasets/hadasu92/cnn-art
icles-after-basic-cleaning , CC0
license);
•The White House Speeches ( https://ww
w.kaggle.com/datasets/mohamedk
haledelsafty/the-white-house
-speeches-and-remarks-12102022 ,
CC0 license);
•Flores-200 ( https://github.com/fac
ebookresearch/flores/tree/main
/flores200 , Costa-jussà et al., 2022, CC
BY-SA 4.0 license);
•Python Code (all .py files from https:
//github.com/naver/disco , Cre-
ative Commons Attribution-NonCommercial-
ShareAlike 4.0 license);
•C++ Code (all .hand.cc files from https:
//github.com/microsoft/Tries
te, MIT license).
Wikitext and White House Speeches datasets con-
sist of paragraphs extracted from Wikipedia ar-
ticles ( wikipedia.org ) or from transcribed
speeches. Flores-200 is composed of sentences ex-
tracted from English Wikipedia and translated by
professional translators into 200 languages. Python
and C++ Code data consists of code files. Twitter /
News datasets consist of separate tweets / news arti-
cles. We compose sequences to evaluate an LM on,
by concatenating texts listed above into sequences
of 800 tokens according to the default tokenization
(concatenated texts are separated by \n\n ). The
sequence always begins with a new text. Code and
News data contains texts longer than 800 tokens,
these texts are considered as separate sequences
and clipped to 800 tokens. Table 3 reports statistics
of the data. Maximum 100 sequences per datasetDataset Av. / max
lengthTotal # of se-
quences
Wikitext 98 / 556 100
Twitter 20 / 159 100
News 833 / 2940 63
Tr. sp. 33 / 158 100
Flores (En) 27 / 69 37
Python 320 / 2623 6
C++ 2296 / 16324 12
are considered (Flores-200 dataset, News dataset
and code data are shorter).
We checked that the data we evaluate on was
not used in model training as follows. GPT-2 was
not trained on Wikipedia data, as reported in its
paper (Radford et al., 2019). BLOOM was trained
on Wikipedia data, so we do not evaluate it on
Wikipedia and English Flores data. At the same
time, data for other languages is based on trans-
lations, which makes it safe to use it for evalu-
ation. Twitter is not listed in data sources for
GPT-2 ( https://github.com/openai/
gpt-2/blob/master/domains.txt ) and
BLOOM ( https://huggingface.co/spa
ces/bigscience/BigScienceCorpus ).
For evaluation on code, we use the code of the
libraries created after the BLOOM’s training. Like-
wise, for evaluation on the news and White House
speech data, we selected only texts released af-
ter 11.03.2022 (after the beginning of the largest
BLOOM model’s training).
BAdditional information on experiments
The BLOOM model is released under the Respon-
sible AI License, and GPT-2 is released under the
Modified MIT License. Our code is based on the
transformers library (Wolf et al., 2020) which
is released under the Apache License 2.0 license.
All assets allow usage for research purposes. Eval-
uation of the GPT-2 model was conducted on a
single Tesla V-100 GPU (24–48 GPU hours per
dataset), and evaluation of the BLOOM model con-
ducted on a single Tesla A100 GPU (72–120 GPU
hours per dataset).7
C Segmentation into blocks
As discussed in Section 2.2, the proposal algorithm
splits the sequence into a sequence of blocks. In
our experiments, we split the sequence at white
spaces and new line characters, thus making blocks
roughly correspond to words. Because our algo-
rithm computes all possible tokenizations within a
block, this process can become prohibitively expen-
sive for long blocks, which can occur with complex
words or in languages that do not frequently use
the white space character, such as Chinese. For
this reason, we define a maximum block length hy-
perparameter, L. Words that have length lower or
equal to Lare denoted as type 0 (T0) blocks. If a
word has length larger than L, it is split into smaller
blocks, as follows. First, we compute the block’s
default tokenization and incrementally merge the
tokens while checking not to exceed L. Once the
limit is reached, a new block is started. The re-
sulting blocks are denoted as type 1 (T1) blocks.
Suppose at any point a token of length larger than
Lis encountered. In that case, this token is cropped
atL, and the remaining characters are then moved
to a new block. These blocks are denoted as type 2
(T2) blocks. Figure 3 illustrates these three block
types.
Table 4 illustrates the effect that the maximum
block length hyperparameter Lhas for BLOOM
on French (low-gap case) and Urdu (higher-gap
case). We experiment with three values of Lto rep-
resent various proportions of T1 and T2 blocks. For
low values of L(L= 17 andL= 19 for French
and Urdu, respectively), we observe some small or
even negative gap in BPC, and a large percentage
of sequences that have higher cross-entropy when
using the marginal than when using the default
tokenization. This result comes with a small but
non-negligible percentage of T2 blocks. Because
T2 splits a token that is selected by the default tok-
enization across different blocks, this prevents the
proposal from ever sampling the default tokeniza-
tion, resulting in a poor estimate. Higher values of
Lresult in the elimination of any T2 blocks with
also a moderate impact on T1 blocks. Yet, once
T2 blocks are eliminated, the number of T1 blocks
does not appear to have a sizeable effect. Overall,
these results provide the rule for selecting L: it
should be set to the maximum length of the tokens
in the default tokenization of the evaluation data in
order to avoid T2 blocks.
D Limiting the number of tokenizations
per block
The proposed importance sampling algorithm lim-
itsM, the number of tokenizations per block which
are scored with LM, for better efficiency. In this
section we motivate why it is not harmful for the
results. In the top plot of Figure 4 we show that
the proposal probability of a block’s tokenization8BPC−
BPCBPC−
BPCBPC−
BPCBPC−
BPC
N1: “runspiration from quotes”
.036271 -.000479 -.001765 .000529
N2: “Did organgatuangs fly”
.023346 .000675 -.000129 .001397
N3: “the Buffalo-Pitt road”
.000530 .000313 -.000293 .000773
N4: “It’s Friday today”
.000009 .000003 .000003 .000003
N5: “throughput of 600Mbit/s”
.000170 -.000001 -.000001 -.000001
N6: “Television morning show”
.000304 -.000024 -.000024 -.000024
N7: “snowboarding is cool”
-.000136 .000219 .000038 .0004613
strongly correlates with the number of subtokens
in the tokenization. This motivates selecting top-
Mtokenizations per block by sorting the block’s
tokenizations by the decreasing number of subto-
kens (we use M= 128 in our experiments). Now,
in the bottom plot of Figure 4 we present the 2d-
histogram of proposal probabilities of blocks’ tok-
enizations and their ranks in the sorting. It can be
seen that the proposal probabilities of tokenizations
with ranks higher than 10 have very low probabili-
ties, i.e. usually lower than 10. In fact, tokeniza-
tions with ranks greater than 70 were never sampled
(in 99.95% one of the first 10 tokenizations was
sampled, in 0.05% cases — one of tokenizations
with indices 11–40, and in 0.0004% cases — with
ranks 40–69.).
EAlgorithm validation on short sentences
To validate the proposed algorithm, we compare the
marginal BPC estimated with our algorithm to the
true marginal BPC, BPC, obtained by enumer-
ating all tokenizations of several relatively short
sentences ( ⩽25characters, <1M tokenizations).
From Table 5 we observe that for sentences with rel-
atively high BPC gap (N1–2), our estimate BPC
is close to BPC, with a thin confidence interval
which includes BPC. N3 shows the case with
lower BPC gap, for which our estimate BPCis
between BPCandBPC, and the confidence in-
terval is wider but still includes BPC. N4–6 show
the case of low BPC gap, in which our proposal al-
ways sampled the default tokenization hence there
is no variance. In all three cases the difference
between our estimate ( BPC) and the marginal
(BPC) is 3–100 times smaller than between de-
fault ( BPC) and marginal ( BPC). Finally, N7
shows the case with low BPC gap, in which our pro-
posal did sample some non-default tokenizations,
and the resulting estimate BPCwas larger than
BPC. However, this ordering almost never hap-
pens with long texts, which is the intended use-case
of our algorithm. To summarize, in almost all cases
our algorithm produces a quite precise estimate.
F Additional confidence interval plots
Figure 5 shows additional confidence interval plots.
The conclusions are the same as for plots in the
main text.9
G Additional analysis
The intuition why the impact of non-default tok-
enizations becomes more pronounced for complex
words, low-resource languages and data distribu-
tion shift is that all these cases are characterized
by the appearance of blocks which were rarely
or never seen during training. Roughly speaking,
frequent words are encoded with short token se-
quences (1-2 tokens) by design of the tokenizer.
Furthermore, the language model assigns high
probability to the default tokenizations of these
words because it saw them frequently during train-
ing. As a result, the effect of marginalization is
small. In contrast, rare words are encoded with
longer token sequences, and because they are not
frequently seen during training, the language model
can assign high probabilities to other tokenizations
than a default one.
To illustrate given reasoning, Table 6 reports
the distribution of number of tokens in sampled
block’s tokenization, for low-frequency and high-
frequency blocks, for GPT-2 on Twitter data. Low-
frequency blocks are split into more tokens and
have a higher portion of non-default tokenizations.10ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitations
/squareA2. Did you discuss any potential risks of your work?
Broader impact
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3 and Appendices A, B
/squareB1. Did you cite the creators of artifacts you used?
Section 3 and Appendices A, B
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Appendices A, B
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Appendix B
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Our research is devoted to evaluating log-likelihood of existing models, we do not release any new
models or textual artefacts. That is why we do not anticipate any harms from our work.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 3 and Appendix A
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A
C/squareDid you run computational experiments?
Section 3 and Appendices
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 3 and Appendix B11/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3 and Appendices C, D
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Not applicable. Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix B
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.12