
Zhigen Li, Yanmeng Wang, Rizhao Fan, Ye Wang
Jianfeng Liand Shaojun WangPing An Technology,University of Bologna
{lizhigen974,wangyanmeng219,wangye430 }@pingan.com.cn
rizhao.fan@unibo.it, {lijianfeng777,wangshaojun851 }@pingan.com.cn
Abstract
Paraphrase generation is a longstanding NLP
task and achieves great success with the aid of
large corpora. However, transferring a para-
phrasing model to another domain encounters
the problem of domain shifting especially when
the data is sparse. At the same time, widely us-
ing large pre-trained language models (PLMs)
faces the overfitting problem when training on
scarce labeled data. To mitigate these two is-
sues, we propose, LAPA, an effective adapter
for PLMs optimized by meta-learning. LAPA
has three-stage training on three types of related
resources to solve this problem: 1. pre-training
PLMs on unsupervised corpora, 2. inserting an
adapter layer and meta-training on source do-
main labeled data, and 3. fine-tuning adapters
on a small amount of target domain labeled
data. This method enables paraphrase gener-
ation models to learn basic language knowl-
edge first, then learn the paraphrasing task itself
later, and finally adapt to the target task. Our
experimental results demonstrate that LAPA
achieves state-of-the-art in supervised, unsu-
pervised, and low-resource settings on three
benchmark datasets. With only 2% of trainable
parameters and 1% labeled data of the target
task, our approach can achieve a competitive
performance with previous work.
1 Introduction
Paraphrase generation can comprehend a sentence
and generate another with the same semantics but
with variations in lexicon or syntax, which has
various applications on downstream tasks includ-
ing query rewriting (Dong et al., 2017), data aug-
mentation (Iyyer et al., 2018) and language model
pre-training (Lewis et al., 2020a). Conventional
approaches (Prakash et al., 2016; Chowdhury et al.,
2022) model the paraphrase generation as a su-
pervised encoding-decoding problem, inspired by
machine translation systems. However, the success
of these methods often relies on a large numberof parallel paraphrases, whose collection is time-
consuming and requires a lot of domain knowl-
edge. Therefore, in real scenarios with a small
amount of parallel data, the model suffers from
performance drops facing domain gaps. This phe-
nomenon, known as domain shift problem (Pan and
Yang, 2009), comes from the representation gap be-
tween training and testing domains with different
writing styles or forms.
To tackle this problem, unsupervised meth-
ods such as editing-based approaches (Bowman
et al., 2016; Miao et al., 2019) or reinforcement
learning (Li et al., 2018; Siddique et al., 2020),
and weakly-supervised methods such as retrieval-
enhanced (Ding et al., 2021; Yin et al., 2022) or
prompt-based (Wang et al., 2022) do not introduce
or only introduce a small number of supervised
signals, which limits their performance such that
underperforms supervised methods. In fact, large-
scale unlabeled corpus data (UCD) and labeled
source domain data (LSDD), as well as a few la-
beled target domain data (LTDD), can be easily
achieved. Therefore, we propose a new three-stage
learning paradigm: pre-training, meta-learning,
and fine-tuning, aiming to leverage the pre-trained
knowledge on UCD, source domain knowledge
on LSDD, and adapt to target domain on LSDD
to improve the performance of low-resource para-
phrase generation. In order to successfully imple-
ment this learning paradigm, we propose a simple
yet effective model which combined pre-trained
language model (PLM) and MAML (Finn et al.,
2017), named Learning to Adapt to low-resource
PAraphrase generation ( LAPA ). Specifically, be-
fore meta-learning, we insert an adapter layer
into each transformer layer of PLM. An adapter
layer is composed of a few parameters of feed-
forward layer and residual connection. During
meta-training and fine-tuning, only the adapter
layer and normalization layer are trainable. Param-
eter freezing and residual connection can retain the1014
prior knowledge of PLM to avoid negative trans-
fer effects. Smaller-scale parameter updating can
prevent MAML from gradient explosion or dimin-
ishing problems when the number of MAML inner
loop iterations and model depth increase (Antoniou
et al., 2019) or training data is extremely scarce.
Overall, we hold the idea that paraphrasing is
a fundamental ability of human beings. The para-
phrase model should not rely on domain and seen
data. Therefore, we are committed to characteriz-
ing the basic ability of the paraphrase model, ob-
taining gains from each domain, and applying it to
a specific domain. Our contributions are summa-
rized as follows:
•We define a novel three stages learning
paradigm for low-resource paraphrase genera-
tion in data scarcity scenarios.
•We propose that LAPA implement this learn-
ing paradigm, which transferred the PLM
knowledge and source domain knowledge to
complete the low-resource learning in the tar-
get domain quickly and with high quality.
•The supervised, unsupervised and weakly su-
pervised experimental results of LAPA on
three benchmark datasets achieve state-of-the-
art (SOTA). LAPA with only 2% of trainable
parameters and 1% target task labeled data
can achieve a competitive performance with
previous works.
2 Related Work
While the paraphrase generation performance is
greatly improved with various supervised tech-
niques (Zhao et al., 2008; Prakash et al., 2016;
Egonmwan and Chali, 2019; Cao and Wan, 2020;
Hosking and Lapata, 2021; Chowdhury et al.,
2022), there are few studies regarding the low-
resource setting. West et al. (2021) and Meng
et al. (2021) proposed novel unsupervised para-
phrasing strategies by data augmentation based on
reflective decoding or diverse decoding. Ding et al.(2021) and Yin et al. (2022) achieved improve-
ments on various low-resource datasets with re-
trieved data and meta reinforcement learning. How-
ever, these studies only use a single large corpus for
training the full PLM, which suffers from domain-
shifting problems (Wang et al., 2019). Besides,
under the extreme low-resource setting, directly
fine-tuning the full PLM will cause an over-fitting
problem (Antoniou et al., 2019).
Meta-learning helps improve low-resource per-
formance in various recent studies, such as im-
age classification (Soh et al., 2020), vehicle track-
ing (Song et al., 2020) and natural language pro-
cessing (Park et al., 2021; Chen and Shuai, 2021;
Hong and Jang, 2022). Finn et al. (2017) proposed
a meta learner named MAML, which uses other
example tasks to learn how to effectively initialize
a basic learner, which can be quickly generalized
to new tasks. Adapter modules have been mainly
used for parameter-efficient and quick fine-tuning
of a basic PLMs to new tasks (Houlsby et al., 2019;
Bapna and Firat, 2019; Pfeiffer et al., 2020, 2021;
He et al., 2021). Our paper proposes to incorporate
meta-learning approaches to realize multi-domain
migration and task adapter to realize parameter
effective transfer learning (i.e., limited trainable
parameters) to mitigate the above problems of para-
phrase generation.
3 The Approach
3.1 Learning Paradigm
As shown in Figure 1, the workflow of our learn-
ing paradigm including three stages: 1. Back-
bone model pre-training on large unlabeled corpora
2. Adapter model meta-training on large source
corpora using the meta-learning and 3. Adapter
model fine-tuning on target corpora and evaluate
model performance. The prior knowledge K
comes from first two stages: pre-training and
meta-learning. We denote our backbone model
byf(θ)with parameters θ. The first stage is pre-
training on unlabeled corpora D, and we get1015f(θ). The second stage is meta-training on
adapter model f[θ,Φ]with additional param-
etersΦand frozen θon related source corpora
D, and we got f[θ,Φ]. Finally, we initial-
ize the adapter model with [θ,Φ]and fine-
tuneΦon the target corpus Dto obtain a
target model f[θ,Φ]which are model param-
eters after target adapter, i.e., the posterior knowl-
edgeK.
3.2 Backbone Model
Because PLM is equipped with prior knowledge
Kand exhibits strong capabilities in a range
of different generative tasks, we choose the pre-
trained BART (Lewis et al., 2020b) as the back-
bone model for paraphrase generation. Specifi-
cally, given a labeled paraphrase pair i= (x,ˆ y),
where x= [x, . . . , x],ˆ y= [ˆy, . . . , ˆy], and
inputting x, the model has produced a predicted
segment sequence y= [y, . . . , y]before
timet, then the probability that the token generated
at time tisyisp(y|y,x, θ). The model is opti-
mized by minimizing the negative log-likelihood:
L(f(θ)) =−/summationtextlog p(ˆy|y,x, θ).
3.3 Adapter Model
The adapter model is obtained by inserting the
adapter layer into each transformer layer of the
backbone model. An adapter layer is a bottle-
necked feed-forward network consisting of a down-
project layer, a nonlinearity function and an up-
project layer. In addition, a skip connection layer
from input to output prevents the noised initial-
ization from interference with the training initially.
For the adapter in layer l, the function can be formu-
lated as: Adapter (z) =WReLU (Wz) +z
where zrepresents the inputs of the adapter in layer
l. Besides, the normalization layers are trainable
and initialized from the previous training stage.
3.4 Meta-Learning
The second stage is adapter model meta traning
based on MAML (Finn et al., 2017). The learning
process is shown in Algorithm 1. First, we freeze
the backbone model parameters θthat have been
pre-trained in the pre-training stage, then, add new
adapters with parameters Φto get adapter model
f[θ,Φ]. Based on Algorithm 1, we first com-
plete the meta-learning of the adapter model on
the source corpus Dto help the adapters Φfind
the initialization parameters Φsuitable for para-
phrase generation to adapt faster target task. At thisAlgorithm 1 Adapter Model Training with Model
Agnostic Meta-Learning
Require: p(T): distribution over tasks; stage (b)
overD, and stage (c) over D
Require: f[θ,Φ]: adapter model
Require: θ: pre-trained backbone model pa-
rameters
Require: Φ: initialization parameters of
adapters; stage (b) is the zero, and stage (c)
isΦlearned from D
Require: α, β: step size hyperparametersInitialize [θ,Φ]←[θ,Φ]Fixθin the training procedurewhile not done do Sample batch of tasks T∼p(T) forallTdo Evaluate gradient ∇L(f[θ,Φ])with re-
spect to Kexamples Compute adapted parameters with gra-
dient descent: [θ,ˆΦ] = [ θ,Φ]−
α∇L(f[θ,Φ]) end for Update [θ,Φ] ← [θ,Φ]−
β∇/summationtextL(f[θ,ˆΦ])end while
time, we obtain the model f[θ, ϕ]with knowl-
edge of the paraphrase generation task. In the third
stage, we initialize the parameters of adapter model
with[θ, ϕ]. Then, based on the Algorithm 1,
we fine-tune adapters ϕon target corpus Dto
quickly adapt to the target corpus. Finally, we get
the target model f[θ, ϕ].
4 Experimental Settings
4.1 Datasets
We conducted experiments on Quora, Twitter (Lan
et al., 2017) and MSCOCO (Lin et al., 2014) bench-
mark datasets, and followed the same setting in pre-
vious works (Lin et al., 2014; Liu et al., 2020; Ding
et al., 2021). For meta-learning, we choose a dif-
ferent source task’s labeld train-set from the target
task to randomly construct meta tasks. Appendix
Table 4 describes more details.
4.2 Baselines
Supervised methods are trained with all parallel
sentences of target task. Unsupervised baselines1016
do not use any labels of target task. Results of Con-
RPG and SGCP-R are from (Meng et al., 2021)
and (Kumar et al., 2020). Results for V AE are
copied from Meng et al. (2021). For others, we
use previously reported results in Ding et al. (2021)
and Yin et al. (2022). Low-resource methods
used a highly small amount training data of target
task. The baseline models compared include the
recent SOTA model LTSL (Ding et al., 2021) , MB-
RPG(contemporaneous with our work) (Yin et al.,
2022) and WS-BART with the full parameter fine-
tuning based on BART (Lewis et al., 2020b). Like
our work, they all used BART as PLM. To compare
the performance of our method against the previ-
ous works, we use BLEU (Papineni et al., 2002),
iBLEU (Sun and Zhou, 2012) and ROUGE (Hovy
et al., 2006) metrics. All metrics are computed be-
tween the generated and the reference paraphrases
in the test set (Kumar et al., 2020).
5 Experimental Results
Table 1 summarizes the performance of different
methods on Quora, Twitter and MSCOCO datasets.
The best score is shown in bold. Overall, our LAPA
method achieves SOTA performance on most met-rics across multiple datasets and different scene
settings, demonstrating the effectiveness of our pro-
posed framework. At the same time, low-resource
LAPA also approaches or even exceeds the super-
vised SOTA method (i.e SGCP-R) (Quora’s BLEU-
4 are comparable, and other metrics are exceeded).
Compared with supervised methods that need to
learn from a large amount of target task labeled
parallel paraphrases, our method makes full use of
other source domain paraphrases and can achieve
comparable results with very low-cost supervision
signals in target-domain.
6 Analyses
6.1 Parameter Study
We also separately analyze the impact of target task1017
labeled data scale under low-resource setting. Fig-
ure 2 shows the experimental results on the Quora
dataset. It can be conclused that LAPA has a signif-
icant effect compared with BART under the same
small data size. LAPA can achieve the effect of
89% to 93% of the full amount of data when not us-
ing any target task labeled data; when using a very
small amount of data such as 0.5k (i.e 0.5% of the
full data), it can be improved to 94% to 96%; when
the amount of data increases to 10k (i.e 10% of the
full data), the performance is almost the same as
the full amount of data 100k.
It should be pointed out that which dataset is se-
lected as the source data can not have a substantial
impact on the migration results, as shown in Figure
3. The results independent of the source dataset
prove that LAPA can learn the paraphrasing task
itself on any dataset, so it has strong adaptability to
the target task.
6.2 Ablation Study
We conduct an ablation study with three vari-
ants under the low-resource setting of the Quora
dataset to investigate the contribution of each com-
ponent in the proposed method. The experimental
results are shown in Table 3. We can get: first, us-
ing pre-trained BART can get good results; second,by adding the source task dataset for pre-trained
BART, the knowledge of the source domain can
be effectively learned, thereby improving the per-
formance of the model in the target domain; third,
adding our proposed meta-learning framework can
again effectively improve the speed and quality
of learning the source domain (LAPA only has
2.8% training parameters compared with BART)
and achieve the best performance.
6.3 Case Study
Table 2 lists some paraphrases generated by LAPA
and BART with different experimental settings. We
can observe that paraphrases produced by LAPA
are not only grammatically correct but preserve the
semantics of Input more completely, and the expres-
sion is closer to Reference than the other methods.
This benefits from the fact that our LAPA approach
can make full use of source domain data and task
features, and better preserve the prior knowledge
of PLM, so as to adapt to new target tasks quickly
and efficiently.
7 Conclusion
In this work, we investigate the problem of para-
phrase generation under the low-resource set-
ting and propose a simple yet effective approach
LAPA. We effectively combine transfer learning
and meta-learning by using adapter modules as the
bridge. Whether in supervised, unsupervised or
low-resource setting, the results that our approach
achieves the SOTA results on benchmark datasets.
In the future, we plan to explore how to choose a
smaller but suitable high-quality source corpus for
learning in the source domain to improve the effect
of transferring to the target domain, because not all
source domain data has a positive effect. Second,
we plan to extend this framework to other AI fields
to solve low-resource problems in other scenarios
and enable more industrial applications.1018Limitations
The major limitation of present study is the need
for source domain annotated data that can adapt
to the target domain. Because this is the source of
data for the knowledge of the learning task itself, it
cannot be avoided. In the real world, we can find
it from public free datasets, exchange it commer-
cially with other institutions, or annotate a batch
of raw data ourselves as a cold start to solve this
problem. Secondly, this study also has insufficient
research on related variables. Due to the limitation
of time and article length, we have not been able
to study. These findings provide the following in-
sights for future research: What is the lower bound
of the amount of source domain data that can be
well adapted to the target task? Whether we can
apply weak supervision, data augmentation and
other methods to create source domain data? How
to select high-quality source domain data to get a
better adapter model? We leave these questions to
future research.
References101910201021A Appendix
A.1 Datasets Details
Quora Quora includes 260K negative and 140 pos-
itive Quora question paraphrase pairs. We only
use positive pairs and follow the same setting in
Li et al. (2018); Kazemnejad et al. (2020); Ding
et al. (2021) and randomly sample 100K, 30K, 3K
parallel sentences for training, test, and validation,
respectively. Low-resource settings use the same
validation and test set, but the training set size is
reduced.
MSCOCO MSCOCO (Lin et al., 2014) contains
about 500K human annotated captions of over
120K images, i.e. each image contains five cap-
tions from five different annotators. We follow the
standard data split according to Lin et al. (2014);
Liu et al. (2020); Ding et al. (2021).
WikiAnswer WikiAnswer (Fader et al., 2013) con-
tains approximately 18 million paraphrases that
are word-aligned question pairs. We only use this
dataset as the source task of meta-training, and fol-
low the standard data split according to Li et al.
(2019); Liu et al. (2020); Siddique et al. (2020).
Twitter The twitter URL paraphrasing corpus is
built by Lan et al. (2017) for paraphrase identifica-
tion. We follow the setting in Li et al. (2018),
Kazemnejad et al. (2020) and Siddique et al.
(2020).
The detailed dataset statistics are summarized in
Table 4 .
A.2 Evaluation Details
To make a fair and comprehensive assessment, we
follow the same experiment setting of each com-
parison work (Li et al., 2018; Liu et al., 2020; Ding
et al., 2021) and conduct the comparison respec-
tively. For data preprocessing, all the sentences are
lower cased, and truncate all sentences to up to 20
words. <s> and </s> are spliced to the front and
back end of the sentence as start and end markers.
For evaluation metrics, we use BLEU, i-BLEU
and ROUGE that have been widely used in theprevious work to measure the quality of the para-
phrases. The i-BLUE aims to measure the diversity
of expression in the generated paraphrases by penal-
izing copying words from input sentences. Specifi-
cally, we follow the unsupervised paraphrase gen-
eration baselines and set the balancing parameter
α= 0.9.
A.3 Implementation
Our experiments were conducted with PyToch on
NVIDIA Tesla V100 16GB GPU. Following the
comparison methods, we used BART-large as the
pre-trained language model and use its pre-trained
parameters. For adapter modules, the hidden size is
128. For meta-training, unless otherwise specified,
a meta batch includes 3 tasks, and the batch size
of each task is 10. Both basic learners and meta
learners use the AdamW (Loshchilov and Hutter,
2019) optimizer for optimization, and the learning
rate is set by grid search in 1e-5, 5e-5, 1e-6 and
5e-6. The internal gradient step size is 4, and the
whole model has enough step size for training. For
meta verification, we use a corpus excluded from
the source task and the target task. For fine-tuning,
we use validation set to select the best model for
metrics calculation.1022