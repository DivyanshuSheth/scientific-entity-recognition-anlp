
Ting-Yao Hsu
Pennsylvania State University
txh357@psu.eduYoshi Suhara
Grammarly
yoshi.suhara@grammarly.comXiaolan Wang
Meta AI
xiaolan@meta.com
Abstract
Community-based Question Answering (CQA),
which allows users to acquire their desired in-
formation, has increasingly become an essen-
tial component of online services in various do-
mains such as E-commerce, travel, and dining.
However, an overwhelming number of CQA
pairs makes it difficult for users without par-
ticular intent to find useful information spread
over CQA pairs. To help users quickly digest
the key information, we propose the novel CQA
summarization task that aims to create a con-
cise summary from CQA pairs. To this end,
we first design a multi-stage data annotation
process and create a benchmark dataset, C-
QAS, based on the Amazon QA corpus.
We then compare a collection of extractive and
abstractive summarization methods and estab-
lish a strong baseline approach DedupLED for
the CQA summarization task. Our experiment
further confirms two key challenges, sentence-
type transfer and deduplication removal, to-
wards the CQA summarization task. Our data
and code are publicly available.
1 Introduction
Community-based Question Answering (CQA) en-
ables users to post their questions and obtain an-
swers from other users. With the increase in online
services, CQA has become essential for various pur-
poses, including online shopping, hotel/restaurant
booking, and job searching. Many online platforms
implement CQA features to help users acquire
additional information about entities (e.g., prod-
ucts, hotels, restaurants, and companies) of their
interests. CQA complements customer reviews—
another type of user-generated content, which pro-
vide additional information about the entity but
mostly focusing on user experiences and their opin-
ions.Figure 1: Example of the CQA summarization task.
The input contains a collection of QA pairs. Duplicated
information can be found in a single QA pair or across
multiple QA pairs. The output is a concise and coherent
summary written in declarative sentences.
While CQA greatly benefits users in decision-
making, digesting information from original ques-
tion and answer pairs (QA pairs) also has become
increasingly harder. Due to the community-based
nature, CQA tends to have a large number of heav-
ily repetitive QA pairs, which make it difficult for
users, especially those who do not have specific
intent (i.e., questions), to find and digest key infor-
mation.3798Existing summarization efforts for CQA (Liu
et al., 2008; Deng et al., 2020a; Fabbri et al., 2021b)
primarily focus on summarizing answers for a
given question, which still assumes that the user
has a certain intent. We believe that information
spread over QA pairs can be summarized into a
more concise text, which helps any users grasp
the key points of discussions about a target entity.
Therefore, we take a step beyond the scope of an-
swer summarization and propose a novel task of
CQA summarization, which aims to summarize a
collection of QA pairs about a single entity into a
concise summary in declarative sentences (shown
in Figure 1).
The CQA summarization task has the following
two unique challenges. First, CQA summarization
needs to solve sentence-type transfer as questions
in interrogative sentences have to be converted into
declarative sentences to make a concise summary.
This challenge is not trivial as existing summariza-
tion tasks assume that input and output are both
written in declarative sentences. Second, CQA con-
tains duplicated questions and answers. That is,
different users can post similar questions. A ques-
tion can have multiple answers, many of which con-
tain duplicate information. Also, unlike question-
answering forums (e.g., Quora), CQA in online
services is less incentivized to remove such redun-
dancy. Slightly different questions/answers can
provide detailed and useful information not men-
tioned in other questions/answers. Having more
similar answers supports the information is more
reliable.Those properties make existing summariza-
tion solutions unsuitable for CQA summarization.
To enable further study of the CQA summariza-
tion task, we create a corpus CQASby col-
lecting reference summaries on QA pairs from the
Amazon QA dataset (Wan and McAuley, 2016;
McAuley and Yang, 2016). Reference summary
annotation is challenging for CQA summarization,
as a single entity (i.e., a product for the dataset)
can have so many questions and answers that the
annotator cannot write a summary directly from
them. Furthermore, the sentence-type difference
(i.e., interrogative vs. declarative) obstructs sum-
mary writing. To make this annotation feasible, we
designed a multi-stage annotation framework. Sam-
pled seed QA pairs are given to the annotator to
convert into declarative sentences, which are then
rewritten into gold-standard summaries by expert
writers. At the last step, we collected semanticallysimilar QA pairs to to make the annotated corpus
more realistic.
We conduct a comprehensive experiment that
compares a collection of extractive and abstrac-
tive summarization solutions and establish a strong
baseline approach, DedupLED, for the CQA sum-
marization task. Specifically, DedupLED fine-
tunes the entire LED model for summary gener-
ation while additional classifier attached to the en-
coder is optimized to extract representative QA
pairs. Leveraging the strengths of both abstractive
and extractive summarization objectives, as well as
the pre-trained language model checkpoints, Dedu-
pLED significantly outperforms the other alterna-
tive methods. Our experiment also confirms that
DedupLED is suitable for CQA summarization, as
the model implements the functionality for both (1)
sentence-type transfer and (2) duplicate removal.
Our contributions of the paper are as follows:
•We propose the novel task of CQA summa-
rization, which takes QA pairs about a single
entity as input and make a summary written
in declarative sentences (Section 2).
•We designed a multi-stage annotation frame-
work and collected reference summaries to
build the first benchmark corpus for CQA
summarization. The corpus is based on
the Amazon QA corpus (Wan and McAuley,
2016) and consists of reference summaries for
1,440entities with 39,485QA pairs from 17
product categories. (Section 3).
•We conduct comprehensive experiments on
a collection of extractative and abstractive
summarization methods and develop a strong
baseline DedupLED, which implements key
characteristics of sentence-type transfer and
duplication removal functions. (Section 4 and
Section 5).
2 Problem definition
LetDdenote a dataset of questions and answers
on individual entities {e, e, ..., e}(e.g., prod-
ucts or hotels). For every entity e, we define a set
of question-answer pairs QA={(q, a)},
where the question qand the answer aare se-
quences of tokens q= (w, ..., w)anda=
(a, ..., a). Given a set of QA pairs for an entity3799e, the CQA summarization task is to generate a
natural language summary SfromQA.
3 The CQASCorpus
We first describe the multi-stage annotation frame-
work to collect gold-standard reference summaries
from input QA pairs and then describe our bench-
mark dataset CQAS.
3.1 A Multi-stage Annotation Framework
Reading and summarizing a set of QA pairs is chal-
lenging and error-prone for three reasons: (1) a
large number of QA pairs, (2) the heavy repetition
and noise in both questions answers, and (3) the
difficulty of converting questions and answers into
declarative summaries. Thus, it is infeasible to col-
lect high-quality reference summaries by simply
showing a set of QA pairs and asking annotators to
write a summary. In this paper, we design a multi-
stage annotation framework that first simplifies this
complex annotation task into more straightforward
annotation tasks and then enriches the collected
annotations.
Figure 2 depicts the schematic procedure of the
multi-stage annotation framework. For each entity
and its corresponding QA pairs in the original cor-
pus, we first select representative seed QA pairs
and ask annotators to rewrite them into declarative
sentences, which are then concatenated into a raw
summary. Next, we ask highly-skilled annotators
to polish the raw summary into a more fluent sum-
mary. In the last step, we enrich the seed QA pairs
by selecting semantically similar QA pairs from
the original corpus.
Step 1: QA Pair Selection and Rewriting
In this step, we use a drastic strategy to remove du-
plicate QA pairs and simplify the annotation task
for human annotators. A natural way to dedupli-
cate QA pairs is by manually comparing existing
QA pairs’ semantics and only keeping the unique
ones. However, we found this approach less prac-
tical because asking human annotators to perform
the comparison is extremely expensive. It is also
hard to validate the quality because selecting a rep-
resentative QA from a set of semantically similar
ones is a subjective process.
Thus, we use a heuristic-based strategy to select
representative QA pairs from the original corpus.
Specifically, we use the following two rules to filter
out QA pairs that are not suitable for creating refer-
ence summaries: (1) length rule : QA pairs with lessthan 5or more than 150tokens; (2) pronoun rule :
QA pairs that include first-person pronouns. We
found that long questions/answers tend to contain
their background information (e.g., personal sto-
ries), which is irrelevant to the entity. First-person
pronouns are also a strong indicator for such ques-
tions/answers. After the filtering, we randomly
sample kseed QA pairs from the remaining ones.
In addition, to avoid redundancy, we only sample
seed QA pairs of different questions.
For each of the kseed QA pairs, we ask hu-
man annotators to rewrite them into declarative
sentences. We recruited three crowd workers from
Amazon Mechanical Turkto annotate every QA
pair and chose the highest-quality annotation based
on 6 criteria: (1) length of LCS against the original
QA pair, (2) use of yes/no, (3) use of interroga-
tive determiner (e.g., What), (4) use of first-person
pronouns, (5) use of the item name at the begin-
ning, (6) the ignorance of the question information.
We also blocked MTurk workers with consistently
low-quality annotations to ensure the quality of
annotations.
Step 2: Summary Writing
We form a raw summary by concatenating annota-
tions (i.e., declarative sentences rewritten from QA
pairs) obtained in the first step for each entity. The
raw summaries are not necessarily fluent and coher-
ent as different pieces are annotated independently.
They may also contain redundant information. To
address these issues, we use another annotation task
to polish and write a summary from the raw sum-
mary. To ensure the quality, we hired highly-skilled
writers from Upworkby conducting screening in-
terviews for this annotation task. For each entity,
we show annotators the raw summary and ask them
to write a fluent and concise summary.
Step 3: Enriching Input QA Pairs
Recall that in Step 1, we select kseed QA pairs for
each entity. The seed QA pairs are less redundant
because of the filtering and sampling strategy. This
does not reflect the real-world scenario, where sim-
ilar questions are asked multiple times, and each
question often contains several answers.
To align the benchmark with more realistic set-
tings, we enrich the input QA pairs in Step 3. In3800
particular, we add all answers to every question
in the seed QA pairs. Besides, we retrieve ques-
tions that are semantically similar to seed questions
and add all the answers to the input QA pairs. For
semantic similarity calculation, we use BERT em-
beddings and word overlap to find the candidates,
followed by an additional crowd-sourcing task us-
ing Appenfor manual validation. The validation
step ensures that our reference summaries can be
created from the enriched input QA pairs.
3.2 Dataset Statistics
Using the multi-stage annotation framework, we
created the CQASbenchmark based on the
Amazon QA dataset (Wan and McAuley, 2016;
McAuley and Yang, 2016). We selected 1,440 enti-
ties from 17 product categories with 39,485 input
QA pairs and 1,440 reference summaries. Besides,
CQASalso contains rewritten QA pairs in
declarative sentences for the QA pair rewriting task
in Step 1, which consist of 3 annotations for each
of the 11,520 seed QA pairs ( k= 8seed QA pairs
for each entity).
Table 1 shows the statistics of CQAS. Weconfirm that the average word count of input QA
pairs/raw summaries/reference summaries is con-
sistent for different categories. The novel n-gram
distributions also confirm that CQASoffers a
fairly abstractive summarization task. Some prod-
uct categories such as “Office Products” and “Patio
Lawn and Garden” have lower novel n-gram ratios,
indicating that the task becomes relatively extrac-
tive. The word count difference between the raw
summary and the reference summary supports the
value and quality of the summary writing task in
Step 2, indicating that the raw summary still con-
tains some redundant information.
4 Models
To examine the feasibility and explore the chal-
lenges of CQA summarization, we tested several
summarization solutions on CQAS. The
models are grouped into Extractive ,Extractive-
Abstractive andAbstractive methods.
4.1 Extractive
Extractive methods extract salient sentences from
input QA pairs as the output summary. We consider
unsupervised (LexRank) and supervised (Bert-3801
SumExt) models in addition to a simple rule-based
baseline that filters the original seed input QA. We
evaluate those methods to understand how well
selecting sentences without sentence-type transfer
performs on the task.
SeedQAs: This method concatenates the seed
QA pairs used in the first annotation task of the
multi-stage annotation framework. This is an ora-
cle method as we cannot tell which QA pairs were
used as seed QA pairs for annotation. We use this
method to verify the performance of simply extract-
ing QA pairs.
LexRank (Erkan and Radev, 2004): This is an
unsupervised extractive method, which uses the
similarity between words to build a sentence graph
and compute the centrality of sentences for select-
ing top-ranked sentences as summary.
BertSumExt (Liu and Lapata, 2019): Bert-
SumExt is a supervised model, which fine-tunes
BERT (Devlin et al., 2019) to extract sentences
by solving multiple sentence-level classifications.
In our experiment, we use BertSumExt to extract
salient QA pairs from the input, where the gold-
standard labels are acquired by greedily select QA
pairs that maximize the ROUGE scores to the gold-
standard summary.
4.2 Extractive-Abstractive
While extractive methods can remove duplication
from the input, they cannot transfer interrogativesentences (i.e., questions) into declarative sen-
tences. To handle this better, we combine extrac-
tive and abstractive models to implement two-stage
solutions. We also test an existing two-stage algo-
rithm in addition to another summarization model
that learns to extract and rewrite in an end-to-end
manner.
LexRank+LED This method is a select-then-
rewrite hybrid model. Using a sentence-type trans-
fer model, the model rewrites each of the QA pairs
extracted by LexRank into declarative sentences,
which are then concatenated as an output sum-
mary. For the sentence-type transfer model, we
fine-tune the LED model (Beltagy et al., 2020) on
the seed QA pairs and their rewritten texts collected
in Step 1 of the multi-stage annotation pipeline
(Section 3.1).
LED+LexRank This method is a rewrite-then-
select hybrid model that swaps the steps of
LexRank+LED. It uses LexRank to extract salient
sentences from input QA pairs rewritten by the
same sentence-type transfer model.
Bert-SingPairMix (Lebanoff et al., 2019) Bert-
SingPairMix is a select-then-rewrite-style model
that first selects salient sentences from the input
and then summarizes the selected sentences into
the summary. In our experiment, we use our gold-
standard summaries to train both the content selec-
tion model and the abstractive summarizer.3802
FastAbstractiveSum (Chen and Bansal, 2018)
FastAbstractiveSum also implements select-then-
rewrite summarization via reinforcement learning.
The model learns to select representative sentences
with the extractor and rewrite the selected sentences
with the abstractor. We train a FastAbstractiveSum
model on the gold-standard summaries.
4.3 Abstractive
As the final group of models, we explore abstractive
models that directly summarize input QA pairs.
Specifically, we use LED and its variants, which
can take long-document as input. Our DedupLED
is a variant of LED and falls into this group.
LED (Beltagy et al., 2020) This model fine-
tunes Longformer Encoder-Decoder (LED) (Belt-
agy et al., 2020) on input QA pairs and the gold-
standard summaries in the training set.
HierLED (Zhu et al., 2020; Zhang et al., 2021)
Hierarchical LED (HierLED) is a variant of LED,
which has two encoders for token-level and QA-
level inputs to handle the structure of QA pairs
better. We use the same architecture as Hierarchical
T5 (Zhang et al., 2021), replacing T5 with LED. We
fine-tune the model in the same manner as LED.
DedupLED While pre-trained encoder-decoder
models, including LED, are known to be power-
ful summarization solutions, they do not explicitly
implement deduplication functionality. Inspired
by BertSumExt, we consider incorporating a clas-
sifier layer optimized to extract the original seed
QA pairs into an LED model and fine-tuning the
LED model via multi-task learning, which we re-
fer to as DedupLED. Figure 3 depicts the model
architecture. The classifier layer is trained to select
the original seed QA pairs, so the shared encoder
learns to detect duplicate information while the de-
coder is optimized to generate a summary. In the
training time, DedupLED uses the original seed QApair information in addition to the gold-standard
summaries in the training data. We would like to
note that DedupLED does not require any addi-
tional information other than input QA pairs in the
summary generation phase.
5 Evaluation
We conduct comparative experiments to evaluate
those models for the CQA summarization task
on the CQASdataset. We randomly split
the data into train/validation/test sets, which con-
sist of 1152/144/144 entities, respectively. For
LexRank, we limit the output length based on the
average reference summary length in the training
set. For LED and its variations, we fine-tuned the
allenai/led-base-16384 checkpoint using the
Hugging Face Transformers library.We report the
performance of the best epoch (based on ROUGE-1
F1) chosen on the validation set for all the super-
vised models.
5.1 Automatic Evaluation
For automatic evaluation, we use ROUGE (Lin,
2004) F1and BERTScore (Zhang et al., 2019)
F1with the default configuration. The perfor-
mance and required supervision of all models de-
scribed in Section 4 are shown in Table 2.
Extractive: SeedQAs, which simply selects the
original QA pairs, performs badly. This is expected
because while with high recall ( 88.45R1-recall),
the Oracle method suffers badly from low preci-
sion, largely due to the sentence-type inconsistency
(i.e., interrogative vs. declarative) and duplication
in input QA pairs. LexRank, the unsupervised sum-
marization baseline, performs slightly better than
SeedQAs thanks to its ability to select more concise
QAs for the output summary. BertSumExt, while
leveraging gold-standard summaries, achieves sim-
ilar performance with LexRank. We believe the
discrepancy between interrogative and declarative
sentences in input QA pairs and gold-standard sum-
maries is the primary cause of the performance.
Extractive-Abstractive: Extractive-abstractive
models achieve better performance than extrac-
tive models. The sentence-type transfer helps
LexRank+LED/LED+LexRank achieve a much
higher R1 score while comparative R2/RL/BS3803Performance Supervision
R1 R2 RL BS SeedQA Rewr. QA Gold Sum.
Extractive:
SeedQAs 18.96 10.22 12.57 83.26 - - -
LexRank 33.17 9.30 19.26 83.76 - - -
BertSumExt 31.81 11.10 19.38 84.57 No No Yes
Extractive-Abstractive:
LexRank+LED 35.92 8.97 18.37 84.37 No Yes No
LED+LexRank 38.01 10.71 19.98 84.01 No Yes No
BERT-SingPairMix 40.82 12.73 21.28 85.17 No No Yes
FastAbstractiveSum 42.51 15.21 22.53 84.47 No No Yes
Abstractive:
LED 45.82 19.34 26.01 87.55 No No Yes
HierLED 48.30 23.29 29.84 88.55 No No Yes
DedupLED 52.73 27.24 31.68 88.96 Yes No Yes
scores against the original LexRank. This implies
the limitation of sentence selection before/after
sentence-type transfer. Also, the sentence-type
transfer model was trained on seed QA pairs and
their corresponding declarative sentences, not the
gold-standard summaries. Thus, another factor
may be the difference between the rewritten QA
pairs and the gold-standard summaries.
Both FastAbstractiveSum and BERT-
SingPairMix, which are directly supervised
by the gold-standard summaries, show signif-
icantly better performance than the extractive
models. The results confirm that those models can
learn to perform both sentence-style transfer and
duplication removal directly from gold-standard
summaries.
Abstractive: All three models achieve strong
performance on the CQA summarization task.
The vanilla LED outperforms extractive/extractive-
abstractive models. By incorporating the hierarchi-
cal structure into the model, HierLED improves
the performance against the vanilla LED. Further-
more, DedupLED achieves the best performance
for all the evaluation metrics. This confirms that by
adding an auxiliary objective and using another su-
pervision (i.e., seed QA pair selection), DedupLED
appropriately learns to deduplicate while learning
to summarize input QA pairs.
Takeaway: From the results, we confirm that bothsentence-style transfer and duplication removal
are crucial for the CQA summarization task. In
addition, fine-tuning pre-trained language models
using the gold-standard summaries offers strong
performance, better than manually-crafted two-
stage summarization models. Finally, by incor-
porating the duplication removal functionality into
the model via multi-task learning, we show that
DedupLED establishes a strong baseline for the
CQA summarization task.
5.2 Human evaluation
We further conducted human evaluation to judge
the quality of generated summaries by different
models. For every entity in the test set, we showed
summaries generated by four models (LexRank,
FastAbstractiveSum, BERT-SinglePairMix, and
DedupLED) to three human judgesto choose the
best andworst summaries for three criteria: infor-
mativeness (Inf.), coherence (Coh.), and concise-
ness (Con.). Then, we computed the performance
of the models using the Best-Worst Scaling (Lou-
viere et al., 2015). Table 3 shows that DedupLED
consistently achieves the best performance in all
three criteria. On the other hand, LexRank, as
expected, performs the worst among all the meth-
ods we tested. The human evaluation performance
trend aligns with the automatic evaluation perfor-
mance, validating the quality of CQASas a3804
benchmark for the CQA summarization task.
6 Analysis
6.1 Choice of Pre-trained Language Models
To justify our observation that pre-trained lan-
guage models have strong abilities we test and com-
pare three additional pre-trained language models
onCQAS: PEGASUS (Zhang et al., 2020),
T5 (Raffel et al., 2020), and BART (Lewis et al.,
2019). We confirm that all models perform bet-
ter than the extractive and extractive-abstractive
models. While PEGASUS and T5 show similar
(24.81and24.61RL, respectively), they are less
effective than BART and LED ( 26.89and 26.01
RL, respectively).
6.2 Learning Curve Analysis
Since collecting reference summaries is costly and
time-consuming, we investigate the models’ per-
formance with limited training data. We tested the
models’ performance when trained with 20%, 40%,
. . . , 100% of the training data. Figure 4 shows the
ROUGE-L F1 scores of DedupLED and FastAb-
stractiveSum when trained on different size of train-
ing data. By leveraging a pre-trained checkpoint,
DedupLED performs consistently and substantially
better than FastAbstractiveSum, which is trained
from scratch. DedupLED also shows a faster learn-
ing curve and reaches the plateau in performance
when trained with 60% and more data. This sup-
ports that the annotation size of CQASis
sufficient for fine-tuning pre-trained language mod-
els, while it may be insufficient for non-pre-trained
models.
6.3 Cross-category Transfer Learning
CQAScontains 17 different categories and
varying amounts of entities within each category.
To investigate how different categories and num-
bers of training data affect the summarization per-
formance, we experiment DedupLED on the top
five categories in terms of entity count. We first
fine-tuned DedupLED on each category and tested
it on the five categories. For each category, we split
entities into train/dev/test sets in 0.8/0.1/0.1 ratios.
Table 4 shows ROUGE-1 F1 scores of the Dedu-
pLED models in a cross-category setting. We find
that more training data generally helps improve the
model quality even if not fine-tuned on training
data in the same category. Electronics and Home
& Kitchen are the top two categories with the most
training examples (243 and 209 entities, respec-
tively), and achieved the best performance across
all categories. From the results, we confirm that
summarization models based on pre-trained lan-
guage models have strong cross-category transfer
ability in CQA summarization.
7 Related Work
Opinion summarization (Amplayo et al., 2022)
aims to create a summary from multiple customer
reviews. While opinion summarization is relevant
to CQA summarization as it summarizes consumer-
generated text, customer reviews are significantly
different from QA pairs in CQA as they are self-
contained and tend to contain more subjective in-
formation. Recent opinion summarization models
have adopted pre-trained LMs (LED) for summa-
rizing multiple reviews (Oved and Levy, 2021; Iso
et al., 2022).
A line of work studies on summarizing answers
in CQA, which can be categorized into extrac-
tive models (Liu et al., 2008; Chan et al., 2012;
Deng et al., 2020a,b) and abstractive models (Fab-
bri et al., 2021b; Chowdhury et al., 2021). Among
them, Chowdhury and Chakraborty (2019) created
a benchmark by selecting the best answer as the ref-
erence summary, and Fabbri et al. (2021b) has col-
lected professionally written reference summaries
for answer summarization. Our CQA summariza-
tion differs from answer summarization as we con-3805Test category
Training category |Train| ELEC H&K S&O T&H AUTO Avg
ELEC 243 46.75 41.74 46.92 38.90 40.52 42.97
H&K 209 41.84 41.97 44.46 37.51 42.53 41.66
S&O 130 38.26 42.99 38.64 39.17 36.18 39.05
T&H 110 42.86 40.50 40.25 38.40 38.05 40.01
AUTO 76 41.71 38.96 40.34 38.75 37.53 39.46
sider multiple QAs as input, which offers unique
challenges not in answer summarization.
Another line of work in dialog summariza-
tion has created new benchmarks for E-mail
threads (Zhang et al., 2021), customer support con-
versations (Feigenblat et al., 2021), conversations
in multiple domains (Fabbri et al., 2021a), and
forum discussions (Khalman et al., 2021). CQA
summarization is similar to those tasks in cre-
ating abstractive summaries from multiple turn-
taking conversations between more than one user.
Meanwhile, we also found that CQA summariza-
tion tends to contain more duplication in the in-
put by nature as the compression ratio (i.e., input
length/summary length) of CQASis10.88%,
which is smaller than EmailSum ( 29.38%) and Fo-
rumSum ( 11.85%). We also tested HierLED, a
variant of the strongest baseline for E-mail thread
summarization, and confirmed that DedupLED per-
forms better than HierLED, indicating that CQA
summarization offers unique challenges that are
not in E-mail summarization.
8 Conclusion
We propose the CQA summarization task to sum-
marize QA pairs in Community-based Question
Answering. We develope a multi-stage annotation
framework and created a benchmark CQAS
for the CQA summarization task. Our multi-stage
annotation framework decomposes a complex an-
notation task into three much simpler ones, thus
allows higher annotation quality. We further com-
pare a collection of extractive and abstractive sum-
marization methods and establish a strong baseline
method DedupLED for the CQA summarization
task. Our experiment also confirms two key chal-
lenges, sentence-type transfer and duplication re-
moval, towards the CQA summarization task.Limitations
As we propose and tackle a challenging summariza-
tion task, the paper has certain limitations. First,
our benchmark is in a single domain (E-commerce)
in a single language (English), which not necessar-
ily ensuring the generalizability for other domains
and languages. Second, the quality of our anno-
tations relies on the initial selection of seed QA
pairs. As we discussed in the paper, we filtered
high-quality seed QA pairs to minimize the risk.
Nevertheless, it may not accurately replicate the
summarization procedure by experts. Third, we use
rules and heuristics to ensure the quality of the free-
text annotation. Despite being able to detect and
eliminate a significant ratio of low-quality annota-
tion, our rules and heuristics do not provide perfect
guarantee, meaning that CQASmay still con-
tain noisy and low-quality annotations. With those
limitations, we still believe that the paper and the
benchmark are benefitial for the community to take
a step beyond the scope of existing summarization
tasks.
Ethics Statement
For the annotation tasks, we paid $10 hourly wage
for the crowd workers on MTurk and Appen (Steps
1 and 3) and $15to$30hourly wage for the Up-
work contractors (Step 2), making sure to pay
higher than the minimum wage in the U.S. (i.e.,
$7.25 per hour). Our CQASis based on the
publicly available Amazon QA dataset. To our
knowledge, the dataset does not contain any harm-
ful content.3806References38073808