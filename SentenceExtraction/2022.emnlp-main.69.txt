
Yuxuan Chen David Harbecke Leonhard Hennig
German Research Center for Artificial Intelligence (DFKI)
Speech and Language Technology Lab
{yuxuan.chen, david.harbecke, leonhard.hennig }@dfki.de
Abstract
Prompting pre-trained language models has
achieved impressive performance on various
NLP tasks, especially in low data regimes. De-
spite the success of prompting in monolingual
settings, applying prompt-based methods in
multilingual scenarios has been limited to a
narrow set of tasks, due to the high cost of
handcrafting multilingual prompts. In this pa-
per, we present the first work on prompt-based
multilingual relation classification (RC), by in-
troducing an efficient and effective method that
constructs prompts from relation triples and in-
volves only minimal translation for the class
labels. We evaluate its performance in fully su-
pervised, few-shot and zero-shot scenarios, and
analyze its effectiveness across 14 languages,
prompt variants, and English-task training in
cross-lingual settings. We find that in both
fully supervised and few-shot scenarios, our
prompt method beats competitive baselines:
fine-tuning XLM-Rand null prompts. It
also outperforms the random baseline by a large
margin in zero-shot experiments. Our method
requires little in-language knowledge and can
be used as a strong baseline for similar multi-
lingual classification tasks.
1 Introduction
Relation classification (RC) is a crucial task in in-
formation extraction (IE), aiming to identify the
relation between entities in a text (Alt et al., 2019).
Extending RC to multilingual settings has recently
received increased interest (Zou et al., 2018; Kol-
luru et al., 2022), but the majority of prior work still
focuses on English (Baldini Soares et al., 2019; Lyu
and Chen, 2021). A main bottleneck for multilin-
gual RC is the lack of supervised resources, compa-
rable in size to large English datasets (Riedel et al.,
2010; Zhang et al., 2017). The SMiLER dataset
(Seganti et al., 2021) provides a starting point to
test fully supervised and more efficient approaches
due to different resource availability for different
languages.Previous studies have shown the promising per-
formance of prompting PLMs compared to the data-
hungry fine-tuning, especially in low-resource sce-
narios (Gao et al., 2021; Le Scao and Rush, 2021;
Lu et al., 2022). Multilingual pre-trained language
models (Conneau et al., 2020; Xue et al., 2021)
further enable multiple languages to be represented
in a shared semantic space, thus making prompting
in multilingual scenarios feasible. However, the
study of prompting for multilingual tasks so far
remains limited to a small range of tasks such as
text classification (Winata et al., 2021) and natural
language inference (Lin et al., 2022). To our knowl-
edge, the effectiveness of prompt-based methods
for multilingual RC is still unexplored.
To analyse this gap, we pose two research ques-
tions for multilingual RC with prompts:
RQ1. What is the most effective way to prompt?
We investigate whether prompting should be done
in English or the target language and whether to
use soft prompt tokens.
RQ2. How well do prompts perform in different
data regimes and languages? We investigate the
effectiveness of our prompting approach in three
scenarios: fully supervised, few-shot and zero-shot.
We explore to what extent the results are related to
the available language resources.
We present an efficient and effective prompt
method for multilingual RC (see Figure 1) that
derives prompts from relation triplets (see Sec-
tion 3.1). The derived prompts include the original
sentence and entities and are supposed to be filled
with the relation label. We evaluate the prompts
with three variants, two of which require no transla-
tion, and one of which requires minimal translation,
i.e., of the relation labels only. We find that our
method outperforms fine-tuning and a strong task-
agnostic prompt baseline in fully supervised and
few-shot scenarios, especially for relatively low-
resource languages. Our method also improves
over the random baseline in zero-shot settings, and1059
achieves promising cross-lingual performance. The
main contributions of this work hence are:
•We propose a simple but efficient prompt
method for multilingual RC, which is, to the
best of our knowledge, the first work to ap-
ply prompt-based methods to multilingual RC
(Section 3).
•We evaluate our method on the largest multi-
lingual RC dataset, SMiLER (Seganti et al.,
2021), and compare our method with strong
baselines in all three scenarios. We also inves-
tigate the effects of different prompt variants,
including insertion of soft tokens, prompt lan-
guage, and the word order of prompting (Sec-
tions 4 & 5).
2 Preliminaries
We first give a formal definition of the relation
classification task, and then introduce fine-tuning
and prompting paradigms to perform RC.
2.1 Relation Classification Task Definition
Relation classification is the task of classifying the
relationship such as date_of_birth ,founded_by or
parents between pairs of entities in a given context.
Formally, given a relation set Rand a text
x= [x, x, . . . , x](where x,···, xare to-
kens) with two disjoint spans eandedenot-
ing the head and tail entity, RC aims to predict
the relation r∈ R between eande, or give a
no_relation prediction if no relation in Rholds.RC is a multilingual task if the token sequences
come from different languages.
2.2 Fine-tuning for Relation Classification
In fine-tuning, a task-specific linear classifier is
added on top of the PLM. Fine-tuning hence intro-
duces a different scenario from pre-training, since
language model (LM) pre-training is usually for-
malized as a cloze-style task to predict target tokens
at[MASK] (Devlin et al., 2019; Liu et al., 2019) or
a corrupted span (Raffel et al., 2020; Lewis et al.,
2020). For the RC task, the classifier aims to pre-
dict the target class rat[CLS] or at the entity spans
denoted by M (Baldini Soares et al., 2019).
2.3 Prompting for Relation Classification
Prompting is proposed to bridge the gap between
pre-training and fine-tuning (Liu et al., 2022; Gu
et al., 2022). The essence of prompting is, by ap-
pending extra text to the original text according
to a task-specific template T(·), to reformulate the
downstream task to an LM pre-training task such
as masked language modeling (MLM), and apply
the same training objective during the task-specific
training. For the RC task, to identify the relation
between “Angela Merkel” and “Joachim Sauer” in
the text “Angela Merkel’s current husband is quan-
tum chemist Joachim Sauer,” an intuitive template
for prompting can be “The relation between An-
gela Merkel and Joachim Sauer is [MASK] ,” and the
LM is supposed to assign a higher likelihood to the
term couple than to e.g. friends orcolleagues at
[MASK] . This “fill-in the blank” paradigm is well1060
aligned with the pre-training scenario, and enables
prompting to better coax the PLMs for pre-trained
knowledge (Petroni et al., 2019).
3 Methods
We now present our method, as shown in Figure 1.
We introduce its template and verbalizer, and pro-
pose several variants of the prompt. Lastly, we
explain the training and inference process.
3.1 Template
For prompting (Liu et al., 2022), a prompt often
consists of a template T(·)and a verbalizer V.
Given a plain text x, the template Tadds task-
related instruction to xto yield the prompt input
x =T(x). (1)
Following Chen et al. (2022) and Han et al.
(2021), we treat relations as predicates and use
the cloze “ e{relation }e” for the LM to fill in.
Our template is formulated as
T(x) := “x.e____e”. (2)
In the template T(x),xis the original text and the
two entities eandecome from x. Therefore,
our template does not introduce extra tokens, thus
involves no translation at all.
3.2 Verbalizer
After being prompted by x , the PLM M
predicts the masked text yat the blank. To com-
plete an NLP classification task, a verbalizer ϕis
required to bridge the set of labels Yand the set of
predicted texts (verbalizations V). For the simplic-
ity of our prompt, we use the one-to-one verbalizer:
ϕ:Y → V , r∝⇕⊣√∫⊔≀→ϕ(r), (3)where ris a relation, and ϕ(r)is the simple ver-
balization of r.ϕ(·)normally only involves split-
tingrby “-” or “_” and replacing abbreviations
such as orgwith organization . E.g., the relation
org-has-member corresponds to the verbalization
“organization has member”. Then the prediction is
formalized as
p(r|x)∝p(y=ϕ(r)|x ;θ),(4)
where θdenotes the parameters of model M.
p(r|x)is normalized by the likelihood sum over
all relations.
3.3 Variants
To find the optimal way to prompt, we investigate
three variants as follows.
Hard prompt vs soft prompt (SP) Hard
prompts (a.k.a. discrete prompts) (Liu et al., 2022)
are entirely formulated in natural language. Soft
prompts (a.k.a. continuous prompts) consist of
learnable tokens (Lester et al., 2021) that are not
contained in the PLM vocabulary. Following Han
et al. (2021), we insert soft tokens before entities
and blanks as shown for SP in Table 1.
Code-switch (CS) vs in-language (IL) Re-
lation labels are in English across almost all RC
datasets. Given a text from a non-English input L
with a blank, the recovered text is code-mixed after
being completed with an English verbalization, cor-
responding to code-switch prompting. It is proba-
bly more reasonable for the PLM to fill in the blank
in language L. Inspired by Lin et al. (2022) and
Zhao and Schütze (2021), we machine-translate
the English verbalizers into the other languages.1061
Table 1 visualizes both code-switch (CS) and in-
language (IL) prompting. For English, CS- and IL-
prompting are equivalent, since Lis English itself.
Word order of prompting For the RC task,
head-relation-tail triples involve three elements.
Therefore, deriving natural language prompts from
them requires handling where to put the predicate
(relation). In the case of SOV languages, filling
in a relation that occurs between eandeseems
less intuitive. Therefore, to investigate if the word
order of prompting affects prediction accuracy,
we swap the entities and the blank in the SVO-
template “ x.e____e” and get “ x.ee____ ”
as the SOV-template.
3.4 Training and Inference
The training and inference setups depend on the em-
ployed model. Prompting autoencoding language
models requires the verbalizations to be of fixed
length, since the length of masks, which is identical
with verbalization length, is unknown during infer-
ence. Encoder-decoders can handle verbalizations
of varying length by nature (Han et al., 2022; Du
et al., 2022). Han et al. (2021) adjust all the ver-
balizations in TACRED to a length of 3, to enable
prompting with RoBERTa for RC. We argue that
for multilingual RC, this fix is largely infeasible,
because: (1) in case of in-language prompting on
SMiLER, the variance of the length of the verbal-
izations increases from 0.68 to 1.44 after translation
(see Table 2), and surpasses most of listed mono-
lingual RC datasets (SemEval, NYT and SERC ),
making it harder to unify the length; (2) manually
adjusting the translated prompts requires manualeffort per target language, making it much more
expensive than adjusting only English verbaliza-
tions. Therefore, we use an encoder-decoder PLM
for prompting (Han et al., 2022; Song et al., 2022).
Training objective For an encoder-decoder
PLMM, given the prompt input T(x)and the
target sequence ϕ(r)(i.e. label verbalization), we
denote the output sequence as y. The probability of
an exact-match decoding is calculated as follows:/productdisplayP(y=ϕ(r)|y, T(x)), (5)
where y,ϕ(r)denote the t-th token of yandϕ(r),
respectively. ydenotes the decoded sequence
on the left. θrepresents the set of all the learn-
able parameters, including those of the PLM θ,
and those of the soft tokens θin case of vari-
ant “soft prompt”. Hence, the final objective over
the training set Xis to minimize the negative log-
likelihood:
argmin−1
|X|/summationdisplay/summationdisplay
logP(y=ϕ(r)|y, T(x)).(6)
Inference We collect the output logits of the
decoder, L∈R, where |V|is the vocabulary
size of M, and Lis the maximum decode length.
For each relation r∈ R, its score is given by (Han
et al., 2022):
score(r) :=1
|ϕ(r)|/summationdisplayP(y=ϕ(r)),(7)1062
where we compute Pby looking up in the t-th
column of Land applying softmax at each time
stept. We aggregate Pby addition to encourage
partial matches as well, instead of enforcing exact
matches. The score is normalized by the length of
verbalization in order to avoid predictions favoring
longer relations. Finally, we select the relation with
the highest score as prediction.
4 Experiments
We implement our experiments using the Hug-
ging Face Transformers library (Wolf et al., 2020),
Hydra (Yadan, 2019) and PyTorch (Paszke et al.,
2019).We use micro-F1 as the evaluation met-
ric, as the SMiLER paper (Seganti et al., 2021)
suggests. To measure the overall performance
over multiple languages, we report the macro aver-
age across languages, following Zhao and Schütze
(2021) and Lin et al. (2022). We also group the
languages by their available resources in both pre-
training and fine-tuning datasets for additional ag-
gregate results. Details of the dataset, the models,
and the experimental setups are as follows. Further
experimental details are listed in Appendix A.
4.1 Dataset
We conduct an experimental evaluation of our mul-
tilingual prompt methods on the SMiLER (Seganti
et al., 2021) dataset, which contains 1.1M anno-
tated texts across 14 languages.Table 3 lists the
main statistics of the different languages in the
SMiLER dataset. Note that languages have varying
number of relations, mostly related to how many
samples are present. We do not evaluate other
datasets because the only prior multilingual RC
dataset that fits our task, RELX (Köksal and Özgür,
2020), contains only 502 parallel examples in 5
languages.
Grouping of the languages We visualize the
languages in Figure 2 based on the sizes of RC
training data, but include the pre-training data as
well, to give a more comprehensive overview of
the availability of resources for each language. We
divide the 14 languages into 4 groups, according
to the detectable clusters in Figure 2 and language
origins.
4.2 Model
For prompting, we use mT5(Xue et al.,
2021), an encoder-decoder PLM that supports 101
languages, including all languages in SMiLER.
mT5(Xue et al., 2021) has 220M parameters.
4.3 Baselines
EN(B) (Seganti et al., 2021) EN(B) is the base-
line proposed together with the SMiLER dataset.1063They fine-tune BERTon the English training
split and report the micro-F1 on the English test
split. BERThas 110M parameters.
XLM-R To provide a fine-tuning baseline,
we re-implement BERT(Baldini Soares et al.,
2019) with the E S variant.In this
method, the top-layer representations at the starts
of the two entities are concatenated for linear clas-
sification. To adapt BERTto multilingual tasks,
we change the PLM from BERT to a multilingual
autoencoder, XLM-R(Conneau et al., 2020),
and refer to this model as XLM-R. XLM-R
has 125M parameters.
Null prompts (Logan IV et al., 2022) To better
verify the effectiveness of our method, we imple-
ment null prompts as a strong task-agnostic prompt
baseline. Null prompts involve minimal prompt
engineering by directly asking the LM about the
relation, without giving any task instruction (see
Table 1). Logan IV et al. (2022) show that null
prompts surprisingly achieve on-par performance
with handcrafted prompts on many tasks. For best
comparability, we use the same PLM mT5.
4.4 Fully Supervised Setup
We evaluate the performance of XLM-R, null
prompts, and our method on each of the 14 lan-
guages, after training on the full train split from
that language. The prompt input and target of null
prompts and our prompts are listed in Table 1.
We employ the randomly generated seed 319
for all the evaluated methods. For XLM-R,
we follow Baldini Soares et al. (2019) and set the
batch size to be 64, the optimizer to be Adam
with the learning rate 3×10and the number
of epochs to be 5. For null prompts and ours, we
use AdamW as the optimizer with the learning rate
3×10, as Zhang et al. (2022) suggest for most
of the sequence-to-sequence tasks, the number of
epochs to 5, and batch size to 16. The maximum
sequence length is 256 for all methods.
4.5 Few-shot Setup
Few-shot learning is normally cast as a K-shot
problem, where Klabelled examples per class are
available. We follow Chen et al. (2022) and Han
et al. (2021), and evaluate on 8, 16 and 32 shots.
The few-shot training set D is generated by
randomly sampling Kinstances per relation fromthe training split. The test set Dis the original
test split from that language. We follow Gao et al.
(2021) and sample another K-shot set from the
English train split as validation set D. We tune
hyperparameters on Dfor the English task, and
apply these to all languages.
We evaluate the same methods as in the fully su-
pervised scenarios, but repeat 5 runs as suggested in
Gao et al. (2021), and report the mean and standard
deviation of micro-F1. We use a fixed set of ran-
dom seeds {13, 36, 121, 223, 319} for data genera-
tion and training across the 5 runs. For XLM-R,
we use the same hyperparameters as Baldini Soares
et al. (2019), a batch size of 256, and a learning
rate of 1×10. For null prompts and our prompts,
we set the learning rate to 3×10, batch size to
16, and the number of epochs to 20.
4.6 Zero-shot Setup
We consider two scenarios for zero-shot multilin-
gual relation classification.
Zero-shot in-context learning Following Ko-
jima et al. (2022), we investigate whether PLMs
are also decent zero-shot reasoners for RC. This
scenario does not require any samples or training.
We test the out-of-the-box performance of the PLM
by directly prompting it with x . Zero-shot
in-context learning does not specify further hyper-
parameters since it is training-free.
Zero-shot cross-lingual transfer In this sce-
nario, following Krishnan et al. (2021), we fine-
tune the model with in-language prompting on the
English train split, and then conduct zero-shot in-
context tests with this fine-tuned model on other
languages using code-switch prompting. Through
this setting, we want to verify if task-specific pre-
training in a high-resource language such as En-
glish helps in other languages. In zero-shot cross-
lingual transfer, we use the same hyperparameters
and random seed to fine-tune on the English task.
5 Results and Discussion
We first present the results in fully supervised, few-
shot and zero-shot scenarios, and then discuss the
main findings for answering the research questions
in Section 1.
5.1 Fully Supervised Results
Table 4 presents the experimental results in the
fully supervised scenario, for different methods,
languages, and language groups. We see that all1064
the three variants of our method beat the fine-
tuning baseline XLM-Rand the prompting
baseline null prompts, according to the macro-
averaged performance across 14 languages. In-
language prompting delivers the most promising
result, achieving an average Fof85.0, which is
higher than XLM-R(68.2) and null prompts
(66.2). The other two variants, code-switch prompt-
ing with and w/o soft tokens, achieve Fscores of
84.1 and 82.7, respectively, only 0.9 and 2.3 lower
than in-language. All three prompt variants are
hence effective in fully supervised scenarios.
On a per-group basis, we find that the lower-
resourced a language is, the greater an advantage
prompting enjoys against fine-tuning. In particular,
in-language prompts shows better robustness com-
pared to XLM-Rin low-resource languages.
They both yield 95.9-96.0 Fscores for English,
butXLM-Rdecreases to 54.3 and 3.7 Fin
Group-M and -L, while in-language prompting still
delivers 83.5 and 65.2 F.
5.2 Few-shot Results
Table 5 presents the per-group results in few-shot
experiments. All the methods benefit from larger
K. Similarly, in-language prompting still turns out
to be the best contender, performing 1st in 8- and
32-shot, and the 2nd in 16-shot. We see that in-
language outperforms XLM-Rin all K-shots,
while code-switch achieves comparable or even
lower FtoXLM-RforK= 8, suggesting
that the choice of prompt affects the few-shot per-
formance greatly, thus needs careful consideration.
On a per-group basis, we find that in-language
prompting outperforms other methods for middle-
and low-resourced languages. Similar observations
can also be drawn from fully supervised results.
We conclude that, with sufficient supervision, in-
language is the optimal variant to prompt rather
than code-switch. We hypothesize it is due to the
pre-training scenario, where the PLM rarely sees
code-mixed text (Santy et al., 2021).
5.3 Zero-shot Results
Table 6 presents the per-language results in zero-
shot scenarios. We consider the random baseline
for comparison (Zhao and Schütze, 2021; Winata
et al., 2021). We notice that performance of the
random baseline varies a lot across languages, since
the languages have different number of classes in
the dataset (cf. Table 3), with English being the
hardest task.
For zero-shot in-context, code-switch prompting
always outperforms the random baseline by a large1065
margin, in both word orders, while in-language
prompting performs worse than the random base-
line in 6 languages. Code-switch prompting out-
performs in-language prompting across all the 13
non-English languages, using SVO-template. We
assume that, without in-language training, the PLM
understands the task best when prompted in En-
glish. The impressive performance of code-switch
shows the PLM is able to transfer its pre-trained
knowledge in English to other languages. We also
find that the performance is also highly indicated
by the number of classes, with worst Fscores
achieved in EN, KO and PT (36, 28 and 22 classes),
and best scores in AR, RU and UK (9, 8 and 7
classes). In addition, we observe that word order
does not play a significant role for most languages,
except for FA, which is an SOV-language and has
54.5Fgain from in-language prompting with an
SOV-template.
For zero-shot cross-lingual transfer, we see that
non-English tasks benefit from English in-domain
prompt-based fine-tuning, and the Fgain im-
proves with the English data size. For 5 languages
(ES, FA, NL, SV , and UK), zero-shot transfer af-
ter training on 268k English examples delivers
even better results than in-language fully super-
vised training (cf. Table 4). Sanh et al. (2022) show
that including RC-specific prompt input in English
during pre-training can help in other languages.
5.4 Discussion
Based on the results above, we answer the research
questions from Section 1.
RQ1. Which is the most effective way to
prompt? In the fully-supervised and few-shot sce-
nario, in-language prompting displays the best re-sults. This appears to stem from a solid perfor-
mance across all languages in both settings. Its
worst performance is 31.8 Ffor Polish 8-shot (see
Table 7 in Appendix C). All other methods have
results lower than 15.0 Ffor some language. This
indicates that with little supervision mT5 is able to
perform the task when prompted in the language
of the original text. However, zero-shot results
strongly prefer code-switch prompting. It could
follow that, without fine-tuning, the model’s under-
standing of this task is much better in English.
RQ2. How well does our method perform in dif-
ferent data regimes and languages? Averaged over
all languages, all our variants outperform the base-
lines, except for 8-shot. For some high-resource
languages, XLM-Ris able to outperform our
method. On the other hand, for low-resource lan-
guages null prompts are a better baseline which
we consistently outperform. This could indicate
that prompting the underlying mT5 model is better
suited for multilingual RC on SMiLER. Overall,
the results suggest that minimal translation can be
very helpful for multilingual relation classification.
6 Related Work
Multilingual relation classification Previous
work in multilingual RC has primarily focused on
traditional methods rather than prompting PLMs.
Faruqui and Kumar (2015) machine-translate non-
English full text to English to deal with multilin-
guality. Akbik et al. (2016) employ a shared seman-
tic role labeler to get language-agnostic abstraction
and apply rule-based methods to classify the uni-
fied abstractions. Lin et al. (2017) employ convolu-
tional networks to extract relation embeddings from
texts, and propose cross-lingual attention between1066relation embeddings to model cross-lingual infor-
mation consistency. Sanh et al. (2019) leverage the
embeddings from BiLSTM, which is trained with a
set of selected semantic tasks to help (multilingual)
relation extraction. Köksal and Özgür (2020) fine-
tune (multilingual) BERT, classifying the embed-
ding at [CLS] . To take entity-related embeddings
into consideration as well, Nag et al. (2021) add an
extra summarization layer on top of a multilingual
BERT to collect and pool the embeddings at both
[CLS] and entity starts.
Multilingual prompting Multilingual prompt-
ing is a new yet fast-growing topic. Winata et al.
(2021) reduce handcrafting efforts by reformulat-
ing general classification tasks into binary classi-
fication with answers restricted to true or false for
all languages. Huang et al. (2022) propose a uni-
fied multilingual prompt by introducing a so-called
“two-tower” encoder, with the template tower pro-
ducing language-agnostic prompt representation,
and the context tower encoding text information.
Fu et al. (2022) manually translate prompts and
suggest multilingual multitask training to boost the
performance for a target downstream task.
7 Conclusion
In this paper, we present a first, simple yet effi-
cient and effective prompt method for multilingual
relation classification, by translating only the rela-
tion labels. Our prompting outperforms fine-tuning
and null prompts in fully supervised and few-shot
experiments. With supervised data, in-language
prompting enjoys the best performance, while in
the zero-shot scenarios prompting in English is
preferable. We attribute the good performance of
our method to its well-suitedness for RC, with the
derivation of entity-relation -entityprompts from
relation triples. We would like to see our method
extended to similar tasks, such as semantic role la-
beling, with a structure between concepts that can
be described in natural language.
Limitations
We acknowledge the main limitation of this work is
that we only experiment on one dataset with 14 lan-
guages. Multilingual RC datasets prior to SMiLER
are limited in the coverage of languages or in the
size of unique training examples. It would be in-
teresting to see how our method performs on other
multilingual RC datasets, especially for underrep-
resented languages (Winata et al., 2022).We restrict the target language to be supported
by the underlying PLM. The popular multilingual
PLMs, mT5 and mBART, include 101 and 25 lan-
guages during pre-training. We rely on these PLMs
and fail to study true low-resource languages that
are not represented in such PLMs (Aji et al., 2022).
It is noticeable that in the fully supervised sce-
nario, for 7 out of the 14 languages, at least one
method achieves over 0.95 micro- Fscore. We
hypothesize that is due to high homogeneity in and
between the train and test split. If so, the dataset
itself might not be challenging, which could indi-
cate that the results are mostly measuring how well
the model is able to fit a few indicators (quickly).
Like most other prompt methods, ours requires
the label names to be natural language which are in-
dicative of the class. Therefore, our method would
suffer from labels being non-descriptive.
Ethics Statement
We use automated machine translation by Google
Translate and DeepL for our method. These MT
systems contain biases regarding, e.g., gender
(“has-author”: “hat Autor”) where gender-neutral
English nouns are translated to gendered nouns in
target languages.
In this work we evaluate SMiLER (Seganti et al.,
2021), which is crawled from Wikipedia. In the
paper, they have not stated measures that prevent
collecting sensitive text. Therefore, we do not rule
out the possible risk of sensitive content in the data.
The PLMs involved in this paper are BERT
for EN(B), XLM-RforXLM-R, and
mT5for null prompts and ours. BERT
is pre-trained on the BooksCorpus (Zhu et al.,
2015) and English Wikipedia. XLM-Ris pre-
trained on a CommonCrawl corpus. mT5is
pre-trained on mC4, a filtered CommonCrawl cor-
pus. All our published models may have inherited
biases from these corpora.
Acknowledgments
We would like to thank Nils Feldhus and the anony-
mous reviewers for their valuable comments and
feedback on the paper. This work has been sup-
ported by the German Federal Ministry for Eco-
nomic Affairs and Climate Action as part of the
project PLASS (01MD19003E), and by the Ger-
man Federal Ministry of Education and Research
as part of the projects CORA4NLP (01IW20010)
and BBDC2 (01IS18025E).1067References106810691070
A Experimental Details
A.1 Hyperparameter Search
We investigated the following possible hyperparam-
eters for few-shot settings. For fully-supervised,
we take hyperparameters from literature (see Sec-
tion 4.4).
Number of epochs: [10,20]; Learning rate:
[1×10,3×10,1×10,3×10]. Batch
size: [16,64,256], not tuned but selected based on
available GPU VRAM.
We manually tune these hyperparameters, based
on the micro- Fscore on the validation set.
A.2 Computing Infrastructure
Fully supervised experiments are conducted on a
single A100-80GB GPU. Few-shot and zero-shot
experiments are conducted on a single A100 GPU.
A.3 Average Running Time
Fully supervised It takes 5 hours to train for 1 run
with mT5 and a prompt method (null prompts,
CS, SP and IL) on either English, or all other lan-
guages in total. With XLM-Rthe running time
is 3 hours.
Few-shot It takes 20 (8-shot), 26 (16-shot), and
36 minutes (32-shot) for 1 run with mT5 and1071a prompt method over all languages. With XLM-
Rthe running time is 8 minutes.
Zero-shot For zero-shot in-context experi-
ments, it takes 6 minutes with mT5 and a
prompt method over all languages. For zero-shot
cross-lingual transfer, the running time equals En-
glish training time (5 hours) plus inference-only
time (6 minutes).
B Verbalizers for SMiLER
•EN "birth-place": "birth place", "eats":
"eats", "event-year": "event year", "first-
product": "first product", "from-country":
"from country", "has-author": "has au-
thor", "has-child": "has child", "has-edu":
"has education", "has-genre": "has genre",
"has-height": "has height", "has-highest-
mountain": "has highest mountain", "has-
length": "has length", "has-lifespan": "has
lifespan", "has-nationality": "has national-
ity", "has-occupation": "has occupation",
"has-parent": "has parent", "has-population":
"has population", "has-sibling": "has sib-
ling", "has-spouse": "has spouse", "has-
tourist-attraction": "has tourist attraction",
"has-type": "has type", "has-weight": "has
weight", "headquarters": "headquarters",
"invented-by": "invented by", "invented-
when": "invented when", "is-member-of":
"is member of", "is-where": "located in",
"loc-leader": "location leader", "movie-has-
director": "movie has director", "no_relation":
"no relation", "org-has-founder": "organiza-
tion has founder", "org-has-member": "orga-
nization has member", "org-leader": "organi-
zation leader", "post-code": "post code", "star-
ring": "starring", "won-award": "won award";
Anonymous EMNLP submission
Abstract
1 Introduction
•AR "event-year": " ", "has-
edu": " ", "has-genre": " ", "has-occupation": " ",
"has-population": " ", "has-type":
" ", "is-member-of": " ",
"no_relation": " ", "won-award": " ";
•FA "event-year": ""," has-edu":
""," has-genre": ""," has-
occupation": ""," has-population":
""," has-type": ""," is-
member-of": ""," no_relation": "."
1
•DE "birth-place": "Geburtsort", "event-
year": "Veranstaltungsjahr", "from-country":
"vom Land", "has-author": "hat Autor", "has-
child": "hat Kind", "has-edu": "hat Bildung",
"has-genre": "hat Genre", "has-occupation":"hat Beruf", "has-parent": "hat Eltern-
teil", "has-population": "hat Bevölkerung",
"has-spouse": "hat Ehepartner", "has-type":
"hat Typ", "headquarters": "Hauptsitz", "is-
member-of": "ist Mitglied von", "is-where":
"gelegen in", "loc-leader": "Standortleiter",
"movie-has-director": "Film hat Regisseur",
"no_relation": "keine Beziehung", "org-has-
founder": "Organisation hat Gründer", "org-
has-member": "Organisation hat Mitglied",
"org-leader": "Organisationsleiter", "won-
award": "gewann eine Auszeichnung";
•ES "birth-place": "lugar de nacimiento",
"event-year": "año del evento", "from-
country": "del país", "has-author": "tiene
autor", "has-child": "tiene hijo", "has-
edu": "tiene educación", "has-genre": "tiene
género", "has-occupation": "tiene ocupación",
"has-parent": "tiene padre", "has-population":
"tiene población", "has-spouse": "tiene
cónyuge", "has-type": "tiene tipo", "head-
quarters": "sede central", "is-member-of":
"es miembro de", "is-where": "situado en",
"loc-leader": "líder de ubicación", "movie-
has-director": "película cuenta con el di-
rector", "no_relation": "sin relación", "org-
has-founder": "organización cuenta con el
fundador", "org-has-member": "organización
tiene miembro", "won-award": "ganó el pre-
mio";
Anonymous EMNLP submission
Abstract
1 Introduction
•AR "event-year": " ", "has-
edu": " ", "has-genre": " ", "has-occupation": " ",
"has-population": " ", "has-type":
" ", "is-member-of": " ",
"no_relation": " ", "won-award": " ";
•FA "event-year": ""," has-edu":
""," has-genre": ""," has-
occupation": ""," has-population":
""," has-type": ""," is-
member-of": ""," no_relation": ";"
1
•FR "birth-place": "lieu de naissance",
"event-year": "année de l’événement", "from-
country": "du pays", "has-author": "a un au-
teur", "has-child": "a un enfant", "has-edu":
"a une éducation", "has-genre": "a un genre",
"has-occupation": "a une profession", "has-
parent": "a un parent", "has-population": "a
de la population", "has-spouse": "a un con-
joint", "has-type": "a le type", "headquar-
ters": "siège social", "is-member-of": "est
membre de", "is-where": "situé à", "loc-
leader": "guide d’emplacement", "movie-
has-director": "le film a un réalisateur",1072"no_relation": "aucune relation", "org-has-
founder": "l’organisation a un fondateur",
"org-has-member": "l’organisation a un mem-
bre", "org-leader": "chef d’organisation",
"won-award": "a remporté le prix";
•IT "birth-place": "luogo di nascita",
"event-year": "anno dell’evento", "from-
country": "dal paese", "has-author": "ha au-
tore", "has-child": "ha un figlio", "has-edu":
"ha un’educazione", "has-genre": "ha genere",
"has-occupation": "ha occupazione", "has-
parent": "ha un genitore", "has-population":
"ha una popolazione", "has-spouse": "ha
un coniuge", "has-type": "ha il tipo",
"headquarters": "sede centrale", "is-member-
of": "è membro di", "is-where": "situato
in", "loc-leader": "leader della posizione",
"movie-has-director": "il film ha direttore",
"no_relation": "nessuna relazione", "org-
has-founder": "l’organizzazione ha fonda-
tore", "org-has-member": "l’organizzazione
ha un membro", "org-leader": "leader
dell’organizzazione", "won-award": "ha vinto
un premio";
•KO "birth-place": " 출생지", "event-
year": "이벤트연도", "first-product": "
첫번째제품", "from-country": " 나라에
서", "has-author": " 저자가있다", "has-
child": " 아이가있다", "has-edu": " 교
육이 있다", "has-genre": " 장르가있다",
"has-highest-mountain": " 가장높은산이
있다", "has-nationality": " 국적이 있다",
"has-occupation": " 직업이 있다", "has-
parent": " 부모가있다", "has-population":
"인구가있다", "has-sibling": " 형제가있
다", "has-spouse": " 배우자가있다", "has-
tourist-attraction": " 관광명소가있다", "has-
type": "유형이있습니다 ", "headquarters":
"본부", "invented-by": " 에의해발명",
"invented-when": " 언제발명", "is-member-
of": "의회원입니다 ", "is-where": " 어디
에", "movie-has-director": " 영화에감독이
있다", "no_relation": " 관계가없다", "org-
has-founder": "조직에는설립자가있습니
다", "org-has-member": " 조직에구성원이
있습니다 ", "org-leader": "조직리더", "won-
award": "수상";
•NL "birth-place": "geboorteplaats", "event-
year": "evenementenjaar", "from-country":
"van het land", "has-author": "heeft auteur","has-child": "heeft kind", "has-edu": "heeft
onderwijs", "has-genre": "heeft genre", "has-
occupation": "heeft beroep", "has-parent":
"heeft ouder", "has-population": "heeft
bevolking", "has-spouse": "heeft echtgenoot",
"has-type": "heeft type", "headquarters":
"hoofdkantoor", "is-member-of": "is lid van",
"is-where": "gevestigd in", "loc-leader":
"locatieleider", "movie-has-director": "film
had regisseur", "no_relation": "geen re-
latie", "org-has-founder": "organisatie heeft
oprichter", "org-has-member": "organisatie
heeft lid", "org-leader": "organisatieleider",
"won-award": "won prijs";
•PL "birth-place": "miejsce urodzenia",
"event-year": "rok imprezy", "from-country":
"z kraju", "has-author": "ma autor", "has-
child": "ma dziecko", "has-edu": "ma wyk-
ształcenie", "has-genre": "ma gatunek", "has-
occupation": "ma zawód", "has-parent": "ma
rodzica", "has-population": "ma ludno ´s´c",
"has-spouse": "ma współmał ˙zonka", "has-
type": "ma typ", "headquarters": "siedziba
główna", "is-member-of": "jest członkiem",
"is-where": "mieszcz ˛ acy si˛ e w", "loc-
leader": "lider lokalizacji", "movie-has-
director": "film ma re ˙zysera", "org-has-
founder": "organizacja ma zało ˙zyciela", "org-
has-member": "organizacja ma członków",
"org-leader": "lider organizacji", "won-
award": "otrzymał nagrod˛ e";
•PT "birth-place": "local de nasci-
mento", "event-year": "ano do evento", "from-
country": "do país", "has-author": "tem au-
tor", "has-child": "tem filho", "has-edu": "tem
educação", "has-genre": "tem género", "has-
occupation": "tem ocupação", "has-parent":
"tem pai", "has-population": "tem população",
"has-spouse": "tem cônjuge", "has-type":
"tem tipo", "headquarters": "sede", "is-
member-of": "é membro de", "is-where":
"localizado em", "loc-leader": "loc leader",
"movie-has-director": "filme tem realizador",
"no_relation": "sem relação", "org-has-
founder": "organização tem fundador", "org-
has-member": "organização tem membro",
"org-leader": "líder da organização", "won-
award": "ganhou prémio";
•RU "event-year": " год события ", "has-
edu": " имеет образование ", "has-genre":1073"имеет жанр ", "has-occupation": " имеет
профессию ", "has-population": " имеет
население ", "has-type": " имеет тип ",
"is-member-of": " является членом ",
"no_relation": " без связи ";
•SV "birth-place": "födelseort", "event-year":
"År för evenemanget", "from-country": "från
ett land", "has-author": "har en författare",
"has-child": "har chili", "has-edu": "har ut-
bildning", "has-genre": "har en genre", "has-
occupation": "har ockuperat", "has-parent":
"har en förälder", "has-population": "har en
befolkning", "has-spouse": "har make eller
maka", "has-type": "har typ", "headquarters":
"huvudkontor", "is-member-of": "är medlem
i", "is-where": "som ligger i", "loc-leader":
"platsansvarig", "movie-has-director": "fil-
men har regissör", "no_relation": "ingen rela-
tion", "org-has-founder": "organisationen har
en grundare", "org-has-member": "organisa-
tionen har en medlem", "org-leader": "ledare
för organisationen", "won-award": "vann ett
pris";
•UK "event-year": " рiк подiї ", "has-
edu": " має освiту ", "has-genre": " має
жанр ", "has-occupation": " має заняття ",
"has-population": " має населення ", "has-
type": " має тип ", "no_relation": " нiякого
вiдношення ".1074C Detailed Few-shot Results1075