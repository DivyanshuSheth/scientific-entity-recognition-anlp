
Zexuan ZhongTao LeiDanqi ChenPrinceton University
{zzhong, danqic}@cs.princeton.edu, taole@google.com
Abstract
Recent work has improved language models
(LMs) remarkably by equipping them with a
non-parametric memory component. However,
most existing approaches only introduce mem-
ories at testing time or represent them using a
separately trained encoder, resulting in subop-
timal training of the language model. In this
work, we present T , a novel yet simple
training approach designed for training LMs
with memory augmentation. Our approach
uses a training objective that directly takes in-
batch examples as accessible memory. We also
present new methods for memory construction
and data batching, which are used for adapt-
ing to different sets of memories—local, long-
term, and external memory—at testing time.
We evaluate T on multiple language mod-
eling and machine translation benchmarks and
show that it is able to achieve signiﬁcant im-
provements across all the settings. Concretely,
T reduces the perplexity from 18.70 to
15.37 on WT-103, by effectively lever-
aging a large memory set from the training
corpus. Compared to standard LM training,
T adds negligible computational over-
head and is compatible with different neural
architectures, making it a versatile solution for
training memory-augmented LMs.
1 Introduction
Memory augmentation has become a remarkable
approach to enhance language modeling perfor-
mance without signiﬁcantly increasing the amount
of parameters and computation. By accessing
memory units such as a neural cache of recent
inputs (Merity et al., 2017; Grave et al., 2017b)
and an external look-up table (Khandelwal et al.,
2020), a memory-augmented language model (LM)
enjoys increased memorization capacity and setsFigure 1: An illustration of our training objective. Our
objective aligns the hidden representation with both to-
ken embeddings and a set of in-batch contextualized
representations that are constructed during training.
new state-of-the-art records in various language
modeling benchmarks.
A major limitation of existing approaches, how-
ever, is that the memory units are either intro-
duced at testing time (Grave et al., 2017b,a; Khan-
delwal et al., 2020) or taken from a separately
trained model (Yogatama et al., 2021). As a con-
sequence, they are not directly optimized during
the training process, resulting in a missed oppor-
tunity to achieve even stronger results. In this pa-
per, we pioneer and present a novel yet simple
training approach T (Training with In-batch
Memories), that is well-suited for memory aug-
mentation in language modeling. Our approach
makes two major departures compared to standard
language model training:
Training objective Inspired by contrastive rep-
resentation learning, we propose a training objec-
tive that directly leverages in-batch examples as
accessible memory (Figure 1). Our training ob-5657jective is closely connected to neural cache mod-
els (Grave et al., 2017b; Merity et al., 2017) and
nearest-neighbor language models (Khandelwal
et al., 2020), where the next-token probabilities are
calculated by comparing encoder outputs against
static token embeddings andmemory representa-
tions. However, previous work only considers in-
corporating memories at testing time, while we do
for both training and testing.
In-batch memory construction With this train-
ing objective in mind, the key challenge is how
to construct memories effectively during training
while keeping it efﬁcient. We identify three types
of memories that can be leveraged at testing time
and have been explored in the literature: (a) local
memory denotes the words that appear in the re-
cent past and are modeled using attention (Vaswani
et al., 2017); (b) long-term memorydenotes long-
range context from the same document but cannot
be directly accessed due to the limit of input length;
(c)external memory is used to store the entire train-
ing set or any additional corpus (Khandelwal et al.,
2020; Borgeaud et al., 2021).
To better leverage these memories at testing time,
we devise new data batching strategies to improve
the construction of training memories (§4). By
packing consecutive segments from the same docu-
ment in one training batch, our model can access
long-term memories beyond the attention context.
We pack segments from other documents that have
high lexical overlap as a proxy to all external mem-
ory units. Importantly, these working memories are
generated on the ﬂy during training, allowing us to
back-propagate to all memory representations.
We instantiate T in three models by consid-
ering different sets of training and testing memories
(Table 1) and evaluate them on multiple language
modeling and machine translation benchmarks. We
highlight our results as follows:
•We ﬁrst show that we can simply optimize a
language model using our training objective with-
outlong-term and external memory. Without any
other modiﬁcations, we demonstrate that a 247M
Transformer-based model can achieve an improved
perplexity from 18.70 to 17.76 on WT-
103(Merity et al., 2017) with negligible overhead.
This model can be viewed as a simple replacement
for vanilla language models.
•By training with consecutive segments in the
same batch, our approach is capable of leveraging
very long context at testing time—up to 15k-25k to-
kens on WT-103 andE 8(Mahoney,
2009). Our approach achieves at least competitive
performance as previous works (Dai et al., 2019;
Martins et al., 2022; Ji et al., 2022) that modify
the Transformer architecture to incorporate mem-
ories from previous segments, yet our solution is
conceptually simpler and computationally cheaper.
•Finally, we train language models by incor-
porating all other segments in the same batch as
memories. Our model works better with a large
datastore at testing time and improves over the
kNN-LM model (Khandelwal et al., 2020) by re-
ducing the test perplexity from 16.23 to 15.41 on
WT-103 . We also demonstrate signiﬁcant
improvements over the kNN-MT baseline (Khan-
delwal et al., 2021) on an IWSLT’14 De-En ma-
chine translation task.
In summary, we propose a simple approach T
for optimizing language models with memory aug-
mentation and demonstrate consistent and signiﬁ-
cant gains in multiple experimental settings. Our
approach only uses memories at the ﬁnal predic-
tion step, and hence adds little computational over-
head and can be combined with different model
architectures such as recurrent networks and other
attention variants (Lei, 2021; Dai et al., 2019; Rae
et al., 2020). We hope that our work can encour-
age the research community to think about better
training objectives for language models, given their
signiﬁcant societal impacts (Brown et al., 2020;
Chowdhery et al., 2022; Zhang et al., 2022).56582 Preliminaries
2.1 Language Modeling
In this paper, we mainly focus on improving lan-
guage models, although our solutions may ex-
tend to most text generation tasks (see one exam-
ple of machine translation in §5.4). Neural lan-
guage models take a sequence of tokens as context
c=x,...,xand map it to a vector representa-
tionf(c)∈R, wheref(⋅)is parameterized by
a neural network. The next-token probability is:
P(w/divides.alt0c)∝exp(Ef(c)), (1)
whereE∈Rdenotes the output embedding of
tokenw∈V. The parameters are optimized to min-
imize the negative log-likelihood of ground truth
xduring training.
2.2 Memory Augmentation
We consider memory as a set of context-target pairs
{(c,x)}following Grave et al. (2017b); Khandel-
wal et al. (2020). These context-target pairs can
be aggregated to obtain the next-token probability
weighted by the similarity between hidden repre-
sentations.We formalize three types of context-
target memories as follows:
Local memory The local memory is simply the
preceding tokens in the same input. Speciﬁcally,
forc=x,...,x, it is deﬁned as:
M(c)={(c,x)}. (2)
Grave et al. (2017b) use the local memory at test-
ing time, denoted by the “continuous cache” model.
However, it has been argued less effective for
Transformer-based models because they can al-
ready learn to leverage recent tokens in the self-
attention layers (Khandelwal et al., 2020). Inter-
estingly, we show that using local memory is still
beneﬁcial if we consider it during training.
Long-term memory Long-term memory de-
notes long-range context from the same document,
but they cannot be directly accessed by attention.
For example, if a document contains 10K tokens,
only a short segment of text (e.g., 100-3K to-
kens) can be fed into a Transformer model because
the complexity scales quadratically with the inputlength. Formally, we divide a document into con-
secutive segments s,...,s, where a segment
scontainsLcontextss={c,...,c}. The
long-term memory for cis:
M(c)={(c,x)}.(3)
Previous works (Dai et al., 2019; Rae et al., 2020;
Martins et al., 2022; Ji et al., 2022; Wu et al., 2022;
Lei, 2021) leverage hidden representations from
previous segments with modiﬁed Transformer ar-
chitectures to learn long-range dependency. Our
approach does not modify the model architecture
and is compatible with these neural architectures.
External memory Finally, external memory as-
sumes a large corpus Dand the external memory
set can be deﬁned as:
M={(c,x)∈D}. (4)
Dcan be simply the training corpus, or a domain-
speciﬁc corpus when the testing domain shifts
(§5.3). Note that /divides.alt0M/divides.alt0is usually several or-
ders of magnitude larger than previous two types
(e.g., 10); accessing all the memories is computa-
tionally expensive and requires approximate near-
est neighbor search (Johnson et al., 2019).
3 Training with In-batch Memories
In this section, we propose a new training ap-
proach T for language model training. Com-
pared to standard language model training, our
training objective assumes a set of training memo-
riesM={(c,x)}. We differentiate training
memories from testing memories , as they are con-
structed on the ﬂy during training and may deviate
from the testing memories used during inference.
Importantly, the training memories are constructed
from the same training batch, which enables back-
propagating the training signal to the current hidden
representation as well as all the memory represen-
tations. We will discuss how to construct training
memories in the next section (§4) and only discuss
the training objective in a general form.
Our training objective is illustrated in Figure 1.
Given a memory set Mand a context c,T5659
deﬁnes the next-token probability distribution as:
P(w/divides.alt0c)∝exp(Ef(c))+
/summation.dispexp(sim(g(c),g(c))).(5)
Here,f(c)is the output representation of a Trans-
former model and Eis the token embedding.
g(⋅)denotes the representations that can be used
to compute similarity between cand all the con-
textscin the memory M. It is possible to
simply take g=f; however, we ﬁnd that tak-
inggto be the input of the ﬁnal feed-forward
layer in Transformer works better, which is con-
sistent with the observation in Khandelwal et al.
(2020). In addition, sim(⋅,⋅)is a similarity func-
tion and we found using the scaled dot-product
sim(q,k)=(Vaswani et al., 2017) leads to
stable training and better performance in our pre-
liminary experiments.
This training objective can be viewed as a con-
trastive loss (Hadsell et al., 2006): for a context-
target pair (c,w), the goal is to align the query rep-
resentationf(c)(andg(c)) with the static token
representation E, and contextualized represen-
tations that share the same next token i.e., g(c)
forx=w. Our objective handles rare words
nicely—ifwdoes not appear in the training mem-
ory, the objective will fall back to aligning f(c)
with only the word embedding E. Similar to
the vanilla training loss (Eq. 1), our T loss is
optimized to minimize the negative log-likelihood
of next token wand all the parameters θandE
are updated during training.
Our training objective is also inspired by
the success of contrastive learning in dense re-
trieval (Karpukhin et al., 2020). As we will show
in §6, it can help improve retrieving contexts that
share the same next token effectively when theset of testing memories is large. Our objective
is also closely connected to the objective used in
Grave et al. (2017b); Khandelwal et al. (2020),
which linearly interpolates the distribution of stan-
dard language modeling, and a distribution de-
ﬁned by cache/external datastore, e.g., P(w/divides.alt0c)=
(1−λ)P(w/divides.alt0c)+λP(w/divides.alt0c). Our work differs
from previous works that we use this objective dur-
ingtraining (and testing), while they only used it at
testing time —the key is how to construct training
memories that we will elaborate next.
4 Adaption to Different Memories
Inference We are interested in incorporating the
three types of memories deﬁned in §2.2 and their
combinations at testing time. The testing objec-
tive is basically the same as the training objective
(Eq. 5) except that we take testing memories as a
combination of M,MandM. AsM
can be very large, we approximate it by retrieving
the top-K closest terms to g(c). We tune a tem-
perature term τto adjust the weight of the memory
component (see Appendix A for details).
Notation Throughout this section, we use Lto
denote segment length, Bto denote the total num-
ber of segments used in the one training batch, and
mto denote the number of consecutive segments
from each document in the batch. Correspond-
ingly, each batch will contain b≈different doc-
uments.L,Bandmare hyper-parameters that5660
we will choose for training, and will vary as we
consider different memories during inference.
A key challenge is that the testing memories can
be very large (e.g., /divides.alt0M/divides.alt0∼10and/divides.alt0M/divides.alt0∼10
in our experiments) and it is computationally in-
feasible to keep training memories the same as
testing memories. In the following, we will dis-
cuss three ways of constructing training memories
and data batching, aiming to reduce the discrep-
ancy between training and testing. Along the way,
we will also present three major model instanti-
ations: T LM,T LM,T LM
(Table 1), which combine the training strategies
and different sets of testing memories.
4.1 Local Memory
Monly considers all the previous tokens in the
same segment. It is straightforward that we can
simply use M=M. As shown in Fig. 2(a),
we basically do not need to make anymodiﬁcations
compared to standard language model training. All
we need is to replace the training objective of Eq. 1
by our objective in Eq. 5, by incorporating (c,x),
∀j<tin the memory during both training and
testing. The computational overhead is also negli-
gible compared to running neural encoders on the
segmentx,...,xitself. We denote this model as
T LM, which can be viewed as a lightweight
replacement for vanilla language models. As we
will show in the experiments, simply incorporating
local memory provides a notable gain on multi-ple LM benchmarks, showing the effectiveness of
training with memories explicitly.
4.2 Long-term Memory
In order to enable long-term memory augmenta-
tion, we pack multiple consecutive segments from
the same document in a training batch (i.e., m>1).
For a context-target pair (c,w)in the training batch,
its accessible memory Mincludes tokens from
previous segments as well as the preceding tokens
in the same segment. Figure 2(b) illustrates the
training batch construction and the training mem-
ory for a given token. At testing time, we can use
a much longer context: we simply enumerate the
number of segments used in Mand choose the
optimum based on the development set.
We denote this model as T LM. It
shares a similar motivation with many previous
works which aim to leverage memory from pre-
vious segments through attention recurrence (Dai
et al., 2019; Ji et al., 2022), or memory compres-
sion (Rae et al., 2020; Martins et al., 2022; Wu
et al., 2022). However, our solution deviates signif-
icantly from previous approaches. First, previous
works need to store the hidden representations (of
every layer) from previous segments and modify
the self-attention layers to incorporate them. Our
approach does not modify the architecture and only
uses the outputs from the last layer. Additionally,
previous works use stale memory representations
and do not back-propagate gradients to the rep-5661resentations of previous segments, whereas our
batching method enables gradient propagation to
the memory and previous segments.As we will
show in the experiments, our approach is competi-
tive with previous works while being conceptually
simpler and computationally cheaper.
4.3 External Memory
Finally, we consider external memory M. Since
Mcontains the context-target pairs in a large
corpus such as the entire training set, we need
to retrieve top- Kpairs from Mmeasured by
sim(g(c),g(c))through (approximate) similar-
ity search (more details are given in §5.2).
Since the retrieved contexts at testing time are
expected to be similar to the query context, we
propose a simple heuristic for constructing train-
ing memories Mby packing segments that
have large lexical overlap into the same batch us-
ing BM25 scores (Robertson and Zaragoza, 2009).
Speciﬁcally, we start with a single segment and
repeatedly add segments with highest BM25 scores
into the same batch (Appendix B). A high BM25
score indicates that two segments have high lexical
overlap and can serve as a good proxy to nearest
neighbors in the external memory, which improves
our model predictions at testing time. Mcon-
tains all tokens from other segments as well as the
previous tokens in the same segment (Figure 2(c)).
We setm=1during training as many segments
from the same document tend to have high lexical
overlap and denote this model by T LM.
In practice, when considering tokens from both
the current segment and other segments in the
batch, we observe that the model tends to lever-
age local memory more and ignore other segments.
To encourage the use of information from other seg-
ments, we exclude the local memory from M
with a probability of pduring training (we ﬁnd that
p=90% works the best, see Appendix H). This sig-
niﬁcantly improves performance when the model
is evaluated with a large set of external memory.
5 Experiments
5.1 Datasets and Tasks
We evaluate our approach on two popular language
modeling benchmarks: WT-103 (Merityet al., 2017), E 8(Mahoney, 2009), and a ma-
chine translation benchmark: IWSLT’14 D e-En.
We also evaluate domain-adaptation performance
on the B C dataset (Zhu et al., 2015).
WT-103 is a word-level language mod-
eling dataset consisting of 103M training tokens.
We evaluate on two model conﬁgurations: one uses
a 247M Transformer model and a segment length
L=3,072and another one uses a 150M Trans-
former model with a segment length L=150.
E 8is a character-level language modeling
dataset that contains a total of 100M characters. We
use a 12-layer Transformer model with a hidden
dimension 512and segment length L=512.
B C is a word-level language mod-
eling dataset. We build our own train/dev/test splits
which consist of 100M/250K/250K tokens. On
this dataset, we evaluate the models trained on
WT-103 to study how our approach can
adapt to new domain without re-training.
IWSLT’14 De-En is a machine translation task,
which consists of 170K translation pairs. We use
a Transformer encoder-decoder model. See Ap-
pendix C for how we adapt our approach to the
machine translation task.
See Appendix C for data statistics and task se-
tups and Appendix D for model conﬁgurations.
5.2 Training and Inference Details
We implement our approach using the Fairseq li-
brary (Ott et al., 2019). For T LMand
T LM, we tune the number of segments
used in Mon the development set during evalu-
ation. Our T LMmodel requires building a
large datastore at testing time and we use the FAISS
library (Johnson et al., 2019) for approximate near-
est neighbor search (details in Appendix D).
We ﬁrst train our model with the standard LM
objective (Eq. 1) for the ﬁrst 5% updates. Without
this warmup stage, we observe the training process
to be unstable probably due to a large variance in
the estimated distributions. We use different mem-
ories when evaluating different instantiations of
T , as shown in Table 1. We ﬁnd that when a
large set of external memory Mis considered
during inference, the performance can be improved
by linearly interpolating the output distribution and
a distribution over the memory, similarly to kNN-
LM (Khandelwal et al., 2020). Thus, we apply an
additional linear interpolation to our output proba-
bility distribution when considering external mem-5662
oryM(see Appendix A for details).
5.3 Results: Language Modeling
T LM vs. vanilla LM We ﬁrst compare
ourT LMmodel which only uses local mem-
ory during training and testing. Table 2 shows that
adding a continuous cache during inference can
improve the performance of vanilla Transformer
from 18.70 to 18.26, and our T LM further
improves the perplexity to 17.76. These results sug-
gest that even though the attention mechanism can
“see” local context, using local memory during both
training and testing can still improve model perfor-
mance. T LMhas no computational overhead
compared to vanilla LM (indicated by the “speed”
column), making it a simple and better replacement
for vanilla language models. Similar trends can be
observed in Table 3 and Table 4 (25.87 vs. 25.60
and 1.16 vs. 1.12). The improvement is much
smaller though, due to a much smaller segment
lengthL. More analysis is given in Appendix G.
T LM leverages long contexts We
then examine our T LMmodel which
is trained with the data batching method de-
scribed in §4.2. As shown in Table 3 and Ta-
ble 4, T LMimproves vanilla Transformer
models substantially (i.e., 25.87→22.66on
WT-103 and1.16→1.05onE 8) by
leveraging long-range contexts at inference time.
We ﬁnd the model achieves its best results when
leveraging 15,000 tokens on WT-103 and
24,576 tokens on E 8, even though the seg-
ments used during training are much shorter ( L=
150 and 512 respectively). We also add continuous
cache to the vanilla Transformer model and ﬁnd
it to underperform our model, demonstrating the
importance of joint training using our approach.
Compared to previous methods which explic-
itly leverage hidden representations from previous
segments (Dai et al., 2019; Rae et al., 2020; Mar-
tins et al., 2022; Ji et al., 2022; Lei, 2021), our
approach achieves better or at least competitive per-
formance. Different from these approaches which
need to store all the hidden representations of ev-
ery layer and modify the model architecture, we
only incorporate the outputs from the last layer—
requiring less computations and GPU memory. Our
approach is orthogonal and can be applied on top
of these models. To verify this, we adapt our ap-
proach to SRU++ (Lei, 2021) (see details in Ap-
pendix E). As shown in the bottom block of Ta-
ble 4, T LMgains consistently improve-
ment over vanilla SRU++, outperforming previ-
ously reported results given the same model size.
T LMvs. kNN-LM Finally, our
model T LMoutperforms the kNN-LM
model (Khandelwal et al., 2020), which uses exter-
nal memory only at testing time—improving the
perplexity from 16.23 to 15.41 on WT-103
(Table 2). We also evaluate a model which does not
use long-term memory (denoted by T LM
w/oM) for a fair comparison with kNN-LM
with continuous cache and the difference is very
small (15.55 vs 15.41). Our results suggest that by
using contrastive loss and BM25 batching (§4.3),5663
the model learns to better retrieve and leverage
information from a large external memory.
Domain adaptation We evaluate the domain-
adaptation performance of T onB C- (Zhu et al., 2015). We take models that are
trained on WT-103 and evaluate them on
B C without any re-training or ﬁne-
tuning. As shown in Table 5, a vanilla Trans-
former model trained on WT-103 per-
forms poorly on B C .T LMand
T LMcan signiﬁcantly improve the per-
formance as they leverage local or long-term mem-
ory to adapt to the new domain. By building
the external memory using B C , both
kNN-LM and T LMperform much better
onB C compared to the vanilla Trans-
former model. T LMoutperforms kNN-
LM on domain adaptation. This indicates that al-
though the memory representations are optimized
on one domain, our approach does not overﬁt, and
building an external memory using the target do-
main dataset enables the model to perform well
with domain shifts.
5.4 Results: Machine Translation
To showcase the generality of our training approach
T to other generation tasks, we evaluate our
approach on the IWSLT’14 de-en translation task.
Since it is a sentence-level task, we do not use
any local or long-term memory ( M,M),
as there are few repetitive tokens. We denote our
model as T MT.
As shown in Table 6, our approach improves the
vanilla Transformer by 1.15BLEU score and out-
performs kNN-MT (Khandelwal et al., 2021). This
demonstrates that our approach is able to improve
the performance on other language generation tasks
with different memory access.
6 Analysis
We conduct ablation studies and analysis to further
understand individual components of our approach.
Due to the limited computation budget, some ex-
periments on WT-103 are conducted with
a small 7M Transformer model (8 layers, hidden di-
mension 128) in this section and the trends are gen-
erally similar for smaller models (see Appendix D
and Appendix F for details).
Memory construction We ﬁrst study how differ-
ent data batching and memory construction strate-
gies affect the performance when different testing
memories are used. We compare our three models
(T LM,T LM,T LM) in Ta-
ble 7. This ablation study clearly shows that pack-
ing consecutive segments and segments with high
BM25 scores in the same training batch and con-
structing memories properly can improve the per-
formance when the long-range and external memo-
ries are used. This demonstrates the importance of
closing the gap between training and inference.
Leveraging long-range contexts We study if
our model is able to handle large long-term mem-
ory. As Figure 3 shows, our model is able to ef-
fectively handle long-range context (more than 10k
tokens), which goes beyond typical attention con-
text. Compared to continuous cache (Grave et al.,5664
2017b,a), the improvement of our approach be-
comes larger when more long-term memory is in-
corporated. This suggests that our model is able to
leverage long-range context much more effectively.
Additional analysis We conduct more ablation
studies and analysis in Appendix G. We summa-
rize them as follows. (1) Our ablation studies
show using BM25 batching method and enabling
back-propagation to update memory representa-
tions are important for our approach (Table 11).
(2)T LM is able to leverage local memory
effectively to improve performance with different
segment lengths L(Table 12). (3) T LM
outperforms kNN-LM in terms of top-K retrieval
accuracy given the external memory set (Table 13).
(4) We study the perplexity of tokens in differ-
ent frequency groups and ﬁnd that T LM
andT LMachieve larger improvements
on rare words while T LMimproves results
across the board (Table 14).
7 Related Work
Memory-augmented language models We
have discussed continuous cache, kNN-LM
and models that leverage representations from
long-range context in the previous sections.
Yogatama et al. (2021) also aim to combine several
types of memories by learning an adaptive gating
function; however, their external memory uses a
pre-trained vanilla language model. Borgeaud et al.
(2021) demonstrate a remarkable performance
by augmenting LMs with an external datastore
of trillion of tokens and their datastore is built
based on chunks of text using off-the-shelfBERT embeddings (Devlin et al., 2019). Our
approach differs from prior works in the following
aspects, which help our model achieve superior
performance with little overhead: (1) we update the
memory representations through back-propagation
from the end loss; (2) our model does not modify
the base architecture; (3) we consider different
types of memories in a uniﬁed framework.
GNN-LM (Meng et al., 2022) augments LMs with
a graph neural network to aggregate information
of retrieved items from external memory, which
makes an orthogonal contribution to our paper.
Transformers for long inputs A large body of
research has investigated how to scale self-attention
mechanism to long contexts, either through sparse
attention (Liu et al., 2018; Child et al., 2019;
Beltagy et al., 2020; Zaheer et al., 2020) or sub-
quadratic-time attention (Wang et al., 2020; Choro-
manski et al., 2020; Peng et al., 2021; Katharopou-
los et al., 2020). See Tay et al. (2020) for a com-
prehensive survey of efﬁcient Transformers. Our
approach is orthogonal, as we only change the train-
ing objective and data batching to enable models
to use large contexts during inference.
Memory-augmented models for downstream
tasks While our paper focuses on improving lan-
guage models with memory augmentation, other
works improve models for downstream tasks with
a retrieval component, such as question answer-
ing (Kumar et al., 2016; de Masson D’Autume
et al., 2019; Karpukhin et al., 2020; Guu et al.,
2020; Zemlyanskiy et al., 2021; de Jong et al.,
2022; Chen et al., 2022; Izacard and Grave, 2021;
Singh et al., 2021), dialogue (Fan et al., 2021), and
other knowledge-intensive NLP tasks (Lewis et al.,
2020; Petroni et al., 2021).
8 Conclusion
In this work, we propose T , a training ap-
proach for language modeling. We present three
model instantiations T LM,T LM,
T LM: Through carefully-designed data
batching and memory construction during training,
we show that our models can leverage long-range
contexts and external memory effectively at test-
ing time. Our approach adds little computational
overhead and does not modify model architectures,
making it compatible with other neural models and
techniques. For future work, we are interested in
training T with large language models and
other text generation tasks.5665Limitations
We discuss limitations of our research as follows.
•Despite the strong performance achieved by
our approach when incorporating a large set
of external memory, it results in a reduced
inference efﬁciency at the same time due to
the nearest neighbor search. For example, the
model is 10×slower when incorporating ex-
ternal memory. This issue can be more crucial
when the external memory is even larger. Po-
tential solutions to this issue include (1) con-
structing the memory using a coarser granular-
ity (e.g., text blocks) (Borgeaud et al., 2021);
(2) compressing the external memory set and
reducing the dimension of memory represen-
tations (He et al., 2021).
•We mainly experiment with Transformer-
based models and additionally adapt our ap-
proach to SRU++ (Lei, 2021). We believe
our approach is compatible with other archi-
tectures or techniques such as Transformer-
XL (Dai et al., 2019) and Compressive Trans-
former (Rae et al., 2020). We plan to explore
them as future work.
•We evaluate our approach on machine trans-
lation to test the generality of T to other
generation tasks. However, due to compute
limitation, we only evaluate it on a small
dataset (i.e., IWSLT’14 ), which consists of
4M tokens in the external memory. We leave
the evaluation on larger machine translation
datasets as future work.
•Our paper mainly studies language model-
ing tasks and machine translation tasks. Al-
though we believe our approach is compati-
ble with all language generation tasks, how
to adapt T to natural language under-
standing tasks such as text classiﬁcation still
remains an open question.
•The biggest model we experimented with con-
sists of 247M parameters due to our com-
pute limit. The state-of-the-art auto-regressive
LMs contain hundreds of billions of param-
eters (Brown et al., 2020). We hope to see
future efforts in scaling up our approach and
evaluating the effectiveness on large LMs.Ethical Considerations
Our proposed approach leverages external mem-
ory to achieve strong results on multiple language
modeling benchmarks. In our experiments, we con-
struct the external memory using the corpus on
which the model is trained, while it can be con-
structed using any corpus. In general, we suggest
practitioners constructing external memory using
a public corpus, as retrieving from the external
datastore can cause information leakage from the
corpus. We acknowledge this ethical considera-
tion and caution those who apply our approach to
privacy-sensitive domains.
Acknowledgments
We thank Jane Pan, Howard Chen, Alexander Wet-
tig, Tianyu Gao, Kaiyu Yang, Mengzhou Xia, Jin-
hyuk Lee, and the members of Princeton NLP
group for helping with proofreading and providing
valuable feedback. This research is partially sup-
ported by the James Mi *91 Research Innovation
Fund for Data Science and a gift from Apple. ZZ
is also supported by a JP Morgan PhD fellowship.
References5666566756685669A Inference Method
Testing objective Formally speaking, our testing
objective is basically the same as the training ob-
jective (Eq. 5):
P(w/divides.alt0c)∝exp(Ef(c))+
/summation.dispexp(sim(g(c),g(c))
τ),(6)
except that we take Mas a combination of
M,MandM. AsMcan be very
large, we approximate it by retrieving the top-K
closest terms to g(c). Formally, Mof three
instantiations of T is constructed as follows,
(7)
where kNN(M,g(c))returns the top-K closest
terms tog(c)in the memory set M. Addi-
tionally, because Mmay be different from the
training memories, we tune a temperature term τto
adjust the weight of the memory component when
calibrating the distribution, based on the develop-
ment set.
Linear interpolation when using M We ﬁnd
that when a large set of external memory Mis
considered during inference, the performance can
be improved by calibrating a separated distribution
over the memory and interpolating the output dis-
tribution and the memory distribution, similarly
to kNN-LM (Khandelwal et al., 2020). We think
this is because the distribution of the similarity
values has been signiﬁcantly shifted during infer-
ence, while the relative ranking preserves. As a
result, having values from two different distribu-
tions in one softmax normalization is sub-optimal
compared to computing two separated probabilities
and interpolating them.
Thus, we apply an additional linear interpolation
to our output probability distribution. Speciﬁcally,
we ﬁrst use Eq. 6 to compute the distribution P(w/divides.alt0
c). Then, we compute a probability distribution
over the tokens in memory P(w/divides.alt0c)as follow,
(8)
We linearly interpolate these two probability distri-
butions with a coefﬁcient λand get the ﬁnal outputP(w/divides.alt0c):
P(w/divides.alt0c)=(1−λ)P(w/divides.alt0c)+λP(w/divides.alt0c).
(9)
We tune the temperature terms and λon the devel-
opment set.
Algorithm 1: Packing segments using
BM25 scores. SimSeg(I,c,k)returns the
top-kmost similar segments to cin the
BM25 indexer I. (k=20when packing
segments in our experiments.)
Data: training segments S={s,...,s}
BM25 Indexer: I
Hyper-parameters: k, batch sizeB
Output: training batches T
l←list();
c←None ;
while/divides.alt0S/divides.alt0≠0do
ifcisNone then
c←random_sample (S);
end
l.append(c);
S.remove(c);
n←None ;
forcinSimSeg(I,c,k)do
ifcinSthen
n←c;
break ;
end
end
c←n;
end
T←{[l,...,l],[l,...,l],...};
returnT;
B Packing Segments Using BM25 Scores
In §4.3, we construct training memories Mby
packing segments that have large lexical overlap
into the same batch using BM25 (Robertson and
Zaragoza, 2009). Algorithm 1 shows the process to
pack segments into training batches. We start with
a single segment and repeatedly add segments with
highest BM25 scores into the same batch.
C Dataset Statistics and Tasks
We evaluate our approach on three benchmarks:
WT-103 ,E 8, and IWSLT’14 . We
also evaluate our approach on B C for5670
domain adaptation (Appendix 5.3). Table 8 shows
the statistics.
WT-103 (Merity et al., 2017) is a word-
level language modeling dataset consisting of
103M training tokens. Following standard prac-
tice, we use adaptive softmax and adaptive to-
ken embeddings (Baevski and Auli, 2019) in our
model and report perplexity. In order to better
compare with previous work, we evaluate on two
model conﬁgurations—one uses a 247M Trans-
former model and a segment length L=3,072
following Baevski and Auli (2019); Khandelwal
et al. (2020) and another one uses a 150M Trans-
former model with segment length L=150follow-
ing Dai et al. (2019). More details are provided in
Appendix D.
E 8(Mahoney, 2009) is a character-level
language modeling dataset that contains a total of
100M characters. Following previous work, we
report bit-per-character (bpc) on this dataset. We
use a 12-layer Transformer model with a hidden
dimension 512and segment length L=512.
We also evaluate the IWSLT’14 D→Ema-
chine translation task, which consists of 170K trans-
lation pairs. Following Khandelwal et al. (2021),
we build an external memory by taking all the
translation contexts and the corresponding target
token((x,y),y)on the training set. We use
the output representation as f((x,y))and the in-
put representation of last FFN layer as g((x,y))
to compute the loss. Similarly, we use BM25 to
batch training data – we encourage two target sen-
tences with a high BM25 score to be in the same
training batch (see Algorithm 1). We use the de-
fault model conﬁguration in the Fairseq library (Ottet al., 2019), and sacrebleu (Post, 2018) to compute
BLEU scores (Papineni et al., 2002).
We evaluate our approach for domain adaptation
on the B C dataset (Zhu et al., 2015),
which is a word-level language modeling dataset.
The complete B C dataset consists of
0.7B tokens. We build our own train/dev/test splits
which consist of 100M/250K/250K tokens respec-
tively. The train set is only used to build external
memory. On this dataset, we evaluate the models
trained on WT-103 to study how our ap-
proach can adapt to new domain without re-training
or ﬁne-tuning. The model we used on this dataset
is the 247M Transformer model with a segment
lengthL=3,072.
D Model Conﬁgurations and
Hyperparameters
Table 9 shows the model conﬁgurations and hyper-
parameters that we used in our experiments. Fol-
lowing Baevski and Auli (2019), during training,
we train the model with ﬁxed-length segments; dur-
ing evaluation, we evaluate on the tokens at the
end of the segment (i.e., an evaluation segment can
overlap with others).
When evaluating with large external memory, we
always retrieve top- K(K=1,024) context-target
pairs for language modeling. For machine transla-
tion, we tune K={1,2,4,8,16,32,64}following
Zheng et al. (2021).
E Applying T LMto SRU++
We apply our approach to SRU++ (Lei, 2021) and
we believe our approach is also compatible with
other architectures such as Transformer-XL (Dai
et al., 2019). SRU++ is a language model which
combines recurrent units and the attention mecha-
nism. SRU++ use hidden representations from the
previous segment at attention layers to incorporate
long-range contexts, similarly to Dai et al. (2019).
To apply our approach to SRU++, we follow
their data-batching method as it is required due to
the recurrence of the model architecture. We con-
struct the training memory using all the contexts
in the current segment (i.e., local memory) and all
contexts in the previous segment (i.e., long mem-
ory). Note that the memory representations from
the previous segment will be stale, thus we do not
back-propagate to that part. During training, we up-
date the model with 400K steps and a batch size of5671
16. For other hyper-parameters and the optimizer,
we follow the default ones in their implementation.
During inference, we can use more contexts to
construct memory. We train with different segment
lengths, i.e., L=512orL=2048 . For the model
trained with L=512, it can leverage a long-term
memory of a size 6,144 during inference; for the
model trained with L=2048 , it can leverage a
long-term memory of a size 12,228.
F Performance of the 7M model on
WT-103
We conduct our ablation studies and analyses in
§6 with an 8-layer Transformer model due to theModel Dev (↓)
T LM 41.50
w/o BM25 batching 45.71
w/o back-prop to memory 45.155672
limited computation budget. The model consists of
7M parameters, 8 layers and 4 heads in each layer.
The embedding dimension is 128 and the interme-
diate dimension of FFN is 512. The model takes a
segment of 3072 tokens as input. We compare our
approach with baselines on this model architecture.
As shown in Table 10, our approach improves over
the baselines by a large margin. This shows that
modeling memory explicitly is essential when the
model capacity is limited.
G Additional Analysis
Ablation study on T LM We study the
importance of packing segments with high BM25
scores in the same training batch, as well as the ef-
fectiveness of enabling back-propagation to mem-
ory representations during training. As shown in
Table 11, when we random batch training segments
(instead of using BM25 scores), the perplexity in-
creases to 45.71 ( +4.21). Also, enabling back-
propagation to memory is crucial for our approach
— the performance is much worse if we disable it.
Effectiveness of using local memory We study
the effectiveness of our model T LMthat uses
only local memory with different segment lengths
L. As shown in Table 12, our model signiﬁcantly
outperforms the baselines in all the settings. This
suggests that our model can leverage local memory
very effectively to improve performance.
Retrieval performance on external memory
When external memory is used in our experiments,
we perform nearest-neighbor search over the entirememory set Mto retrieve the top Kkeys (we
useK=1024 ). Table 13 compares the retrieval
accuracy of our approach and kNN-LM (Khandel-
wal et al., 2020) for different K. Our approach
outperforms kNN-LM in terms of retrieval results;
this explains how our ﬁnal perplexity surpasses
kNN-LM when incorporating external memory.
Perplexity breakdown for different frequencies
We aim to understand which type of memories im-
proves perplexity of tokens in different frequency
groups. We group tokens into 5 buckets according
to their frequency on the development set. Table 14
shows the results for different models. T LM
andT LMimprove the perplexity of rare
words (i.e., frequency ≤1k) while achieving similar
or slightly worse results for frequent words com-
pared to the Transformer baseline. T LM
improves perplexity in all the buckets. Interestingly,
kNN-LM with continuous cache does not perform
signiﬁcantly better compared to T LM and
T LMalthough these two models do not
use external memory. This suggests that jointly
training memory representations and the language
model particularly help improve the performance
of rare words.
H Tuning pfor training with external
memory
When training the model with local and external
memory, to avoid the model to only relies on high-
quality local memory, we disable the local memory
with a probability of p. Here we study how pwill
affect the ﬁnal performance of our model. The re-
sults of using different pare shown in Table 15. We
ﬁnd that when p=0, the model performs poorly
with external memory as the model learns to only
leverage local memory and ignores external mem-
ory during training. By increasing p, this issue is
mitigated. We set p=0.9in our main experiments.5673