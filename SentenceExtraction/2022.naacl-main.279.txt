
Tingting Ma, Qianhui Wu, Zhiwei Yu, Tiejun Zhao, Chin-Yew LinHarbin Institute of Technology, Harbin, ChinaMicrosoft Research Asia
hittingtingma@gmail.com
{qianhuiwu, zhiwyu, cyl}@microsoft.com, tjzhao@hit.edu.cn
Abstract
Recent studies on few-shot intent detection
have attempted to formulate the task as a meta-
learning problem, where a meta-learning model
is trained with a certain capability to quickly
adapt to newly specified few-shot tasks with
potentially unseen intent categories. Prototypi-
cal networks have been commonly used in this
setting, with the hope that good prototypical
representations could be learned to capture the
semantic similarity between the query and a
few labeled instances. This intuition naturally
leaves a question of whether or not a good sen-
tence representation scheme could suffice for
the task without further domain-specific adap-
tation. In this paper, we conduct empirical
studies on a number of general-purpose sen-
tence embedding schemes, showing that good
sentence embeddings without any fine-tuning
on intent detection data could produce a non-
trivially strong performance. Inspired by the
results from our qualitative analysis, we pro-
pose a frustratingly easy modification, which
leads to consistent improvements over all sen-
tence encoding schemes, including those from
the state-of-the-art prototypical network vari-
ants with task-specific fine-tuning.
1 Introduction
The task of intent detection aims at classifying user
queries, typically in the form of short sentences,
into intent categories. It has been widely adopted
as one crucial component inside various applica-
tions such as dialogue systems, virtual assistants,
and search engines. The domain-specific nature of
those applications makes intent detection rather
challenging because of the difficulty to acquire
high-quality labeled data at scale. In particular,
the sets of intents could vary a lot in different real-
world scenarios. Such scenarios motivate the re-
search of few-shot intent detection, which aimsto classify utterances with new intent labels given
very few labeled examples.
Recently, there exists a popular stream of re-
search efforts (Yu et al., 2018; Geng et al., 2019;
Nguyen et al., 2020; Dopierre et al., 2021a,b) that
models the few-shot intent detection task as a meta-
learning problem - a general machine learning
paradigm which has already been successfully ap-
plied in other tasks of natural language processing
(Han et al., 2018; Gu et al., 2018; Chen et al., 2019;
Hou et al., 2020, inter alia ). Under this formu-
lation, the target is to train a good meta-learner
that could be used to quickly adapt to any few-
shot intent classification task with very few labeled
examples. One of the most popular methods of
meta-learning is the prototypical network (Snell
et al., 2017), which learns an embedding of the
input data, and then constructs a prototypical rep-
resentation for every class via averaging over the
input embeddings. Each query will be classified as
the class with the minimum distance between the
query embedding and the class prototype. Earlier
empirical findings (Dopierre et al., 2021a) suggest
that the prototypical networks could reach the state-
of-the-art performance on most intent detection
datasets when a text encoder based on fine-tuned
BERT (Devlin et al., 2019) is used for sentence
representation.
However, one notable challenge for prototypical
networks, or basically most of the current meta-
learning approaches is that the models could easily
overfit the sparse and biased data distribution from
merely a few training instances (Yang et al., 2021).
Given that the goal of prototypical networks is
essentially to learn proper encoding functions
for nearest-neighbor classification, one natural
question arises: is it possible to utilize other
general-purpose sentence representation schemes
without fine-tuning on intent detection data ? In
this way, the cost of collecting and fine-tuning on
domain-specific labeled data might be mitigated,3806while reducing the risk of domain-specific
overfitting.
In this paper, we conduct an empirical study to
verify the utility and the effectiveness of recent
popular sentence encoding schemes for intent de-
tection meta-learning. Specifically, we make the
following contributions:
•We empirically compare a number of pop-
ular sentence embedding methods on vari-
ous intent detection benchmarks and observe
non-trivial strong performance in the meta-
learning setup for few-shot intent detection.
•We quantitatively verify the better capability
for cross-dataset generalization from general-
purpose sentence encoders, and conduct qual-
itative analysis on the behaviors of different
sentence encoding schemes.
•Based on our analysis, we propose a frustrat-
ingly simple modification to utilize the label
name information, with the hope to yield sen-
tence representations more targeted at the in-
tent detection task. Follow-up experiments
show consistent and substantial improvements
over all sentence encoding methods, making
them stronger baselines for the task, while the
modification could also improve the state-of-
the-art system performance.
2 Preliminary
2.1 The meta-learning setup
The meta-learning setting aims at learning a good
“meta-learner” that could quickly adapt to newly
specified classification tasks with few labeled exam-
ples available. A few-shot task, called an episode ,
is denoted as T= (S,Q,Y), usually following
anN-way K-shot setting: given a support set
S={(x, y)} containing a small number
ofKlabeled examples for each class c∈ Y
(|Y|=N), the model is expected to assign a
label from Yto each instance in the query set
Q={(x, y)}. At the meta-training phase,
the meta-learner is trained from a series of episodes
sampled from training data with classes Y, via
updating based on the prediction loss on Q. At the
meta-testing phase, the meta-learner is evaluated
on many episodes T= (S,Q,Y)constructed
from test data, with each Y⊂ Yfrom a non-
overlapping label space to verify the meta-learning
capability, i.e., Y∩ Y =∅.2.2 Prototypical networks
Formally, a prototypical network (Snell et al., 2017,
ProtoNet) learns an encoding function E(param-
eterized by ϕ) to embed a sentence xinto a vector
E(x). The class prototype efor each class c
is obtained by taking the average embedding of
sentences with the label cin the support set S:
e=1
K/summationdisplayE(x).
With these prototype vectors, the predicted class
distribution in the label space Yfor a query xis
calculated by
p(y=c|x) =exp(−d(E(x), e))/summationtextexp(−d(E(x), e)),
where dis a distance metric, usually set to be the
Euclidean distance. The encoder is trained by min-
imizing the cross-entropy loss on the query set of
the episodes from Y.
3 Our Empirical Study
3.1 Adapting generic sentence embedding
The main goal of this work is to explore the effec-
tiveness of general-purpose sentence embedding
methods without fine-tuning on intent data. A high-
quality sentence embedding could be used to iden-
tify which instance in the few labeled examples is
semantically close to an input query and henceforth
expressing the same intent. This intuition makes
it natural to directly adapt sentence encoding to
ProtoNet. Specifically, for whatever pre-trained
encoder Eto produce sentence embedding, we
replace the encoder EwithEin the ProtoNet.
Note that we take a pre-trained Eas-is, in other
words, there is no meta-training phase .
We experiment with a number of modern popular
sentence embedding methods, covering sentence
embeddings pre-trained from either large-scale un-
labeled text data or with supervision from addi-
tional sentence pairs. Specifically, the following
four typical methods yielding five specific model
instances in total are used in our experiments:
Sentence-BERT Sentence-BERT (Reimers and
Gurevych, 2019) takes BERT / RoBERTa (Liu
et al., 2019b) as the basic encoder and uses Siamese
and triplet network (Schroff et al., 2015) structure
to derive sentence embeddings by comparing simi-
larities between sentence pairs. Here we consider3807two model instances pre-trained on different data: i)
SBERT-paraphrase , a DistillRoBERTa (Sanh et al.,
2019) based model trained on a broad range of para-
phrase corpora;ii)SBERT-NLI , a RoBERTa based
model trained on SNLI (Bowman et al., 2015) and
MNLI (Williams et al., 2018), using a three-way
classification objective to predict the relationship of
a pair of sentences, i.e.,entailment ,neural ,
orcontradiction . Both model instances uti-
lize mean pooling over token representations in a
sentence for sentence representation.
SimCSE SimCSE (Gao et al., 2021) learns sen-
tence embeddings by contrastive learning. Specif-
ically, it encodes a sentence with the RoBERTa
model and takes the representation of the [CLS]
token as the sentence representation. Given an an-
chor sentence, the model is trained to predict the
“positive” example, i.e., the most semantic similar
example, among the “negatives”. Here we consider
the situation that all anchors and their positive as
well as negative examples are constructed from
SNLI (Bowman et al., 2015) and MNLI (Williams
et al., 2018), denoted as SimCSE-NLI .
DeCLUTR DeCLUTR (Giorgi et al., 2021) is an
unsupervised sentence embedding method trained
on documents from OpenWebText (Gokaslan and
Cohen, 2019) and a subset of WebText (Radford
et al., 2019). The mean pooling of contextual word
representations obtained from RoBERTa is used as
the sentence embedding. The sentence encoder is
trained with a self-supervised contrastive loss to
minimize the distance between the embeddings of
textual segments randomly sampled from nearby
in the same document.
SP-paraphrase Different from afore-mentioned
models that are built upon large pre-trained lan-
guage models, Wieting et al. (2019, 2021) pro-
pose a lightweight paraphrastic sentence embed-
ding method, denoted as SP-paraphrase .SP-
paraphrase first tokenizes a sentence into subwords
with SentencePiece (Kudo and Richardson, 2018),
then learns context-independent embeddings for
sub-word tokens within a pre-defined vocabulary,
and finally averages over all sub-word embeddings
of a sentence for the sentence embedding. This
model is trained on ParaNMT (Wieting and Gim-
pel, 2018) with a margin loss and carefully selected
negative examples.3.2 Meta-learning methods for reference
To study the different behaviors between general
sentence encoding and meta-learning algorithms
trained on intent datasets, we also compare with
the representative and the start-of-art meta-learning
algorithms as following:
ProtoNet As described before.
ProtoNet+MLM The unlabeled data of the tar-
get dataset is used for finetuning the pretrained
language model with the masked language mod-
eling (MLM) objective. This step is intentionally
serving as a kind of domain adaptation, leading to
a finetuned encoder to be used as the initial base
encoder of ProtoNet (Dopierre et al., 2021a).
ProtAugment The current start-of-the-art frame-
work in intent detection meta-learning proposed
by Dopierre et al. (2021b). A paraphrasing model
is trained to produce multiple diverse paraphrases
for an unlabeled sentence from the training, de-
velopment, or testing instance. Based on Pro-
toNet+MLM, the prototypical network is trained
with an additional consistency loss to make the em-
bedding of a sentence to be closer to the unlabeled
prototype embedding of its paraphrases, and more
distant away from other unlabeled prototypes.
3.3 Datasets
We evaluate these methods on four datasets for a
comprehensive analysis and fair comparison with
Dopierre et al. (2021b).
Banking77 Banking77 (Casanueva et al., 2020)
is a single-domain dataset that contains 77 fine-
grained intents about banking. It consists of 13,083
customer service queries and many of the intents
are partially overlapped (e.g. verify top up, top up
limits, pending top up). 25 intent classes are used
for training, 25 for development and the remaining
classes are for testing.
HWU64 HWU64 (Liu et al., 2019a) contains
11,036 user-generated utterances about home
robots covering 64 intents from 21 different do-
mains such as alarm and calendar. This dataset is
class-balanced and each intent has 190 instances.
Intents are split into train, dev, and test by domains
to minimize the label semantic sharing amongst
splits.
Liu57 This is also a multi-domain intent dataset
from Liu et al. (2019a) which contains 25,478 user3808
utterances about home robots with a total number
of 54 intent classes. The class distribution is highly
imbalanced, with the most frequent intent ( query )
accounts for 23% of all utterances while the least
frequent intent ( volume_other ) only occupies
0.09%. Each of the train, dev, and test splits has 18
classes.
Clinc150 Larson et al. (2019) introduce a crowd-
sourced dataset containing 22,500 user queries cov-
ering 150 different intents in ten general domains.
We randomly select 50 classes for training, devel-
opment, and testing, respectively.
3.4 Evaluation
We evaluate all methods on the standard 5-way 1-
shot and 5-way 5-shot settings as in Dopierre et al.
(2021b). We compute the averaged accuracy on
600 episodes and there are five query examples in
each episode. To reduce the performance variation,
we run all experiments five times with five different
class splits and report the averaged accuracy.
4 Results and Analysis
Table 1 shows the performance of all approaches
on four benchmarks. We can see that general sen-
tence embeddings without any task-specific fine-
tuning achieve non-trivial performance . Com-
pared with meta-learning models elaborately de-
signed for few-shot learning, general sentence em-
bedding can reach a rather strong performance on
all datasets in the 5-way 5-shot setting, even out-
perform or on par with the representative ProtoNet
on HWU64, Liu57, and Clinic150. For the 5-way
1-shot setting, general sentence embeddings yield
less satisfactory results than meta-learning. We
suspect that the finetuned meta-learning modelscould be less sensitive to the distribution of support
examples, especially in the 1-shot setting.
4.1 Sentence embedding visualization
Here we try to obtain a more intuitive understand-
ing of the sentence embeddings obtained from dif-
ferent methods via low-dimensional projection us-
ing t-SNE (van der Maaten and Hinton, 2008). For
meta-learning based models, we use the sentence
encoders trained on the HUW64 training splitin
the 5-way 1-shot setting. Figure 1 shows the projec-
tion results of sentences from 10 randomly selected
classes in the HWU64 test split.
1) Meta-learning based methods generally pro-
duce sentence embeddings with larger between-
class distances and smaller within-class dis-
tances on most classes. Compared with gen-
eral pre-trained sentence embeddings, embeddings
of different classes derived from meta-learning
methods are more compact and distinguishable,
which enables these meta-learning based models to
achieve more robust test performance when given
different support examples for similarity compari-
son, especially in the 1-shot setting.
2) Meta-learning based methods are good at han-
dling test classes that share similar patterns with
training classes. As shown in Table 2, when
only testing on the class remove_calendar
and class query_calendar , all meta-learning
models significantly outperform sentence embed-
dings since two similar classes query_alarm
andremove_alarm exist in the training data.3809
3) General sentence embeddings may achieve
superior performance than meta-learning based
models on target tasks that require fine-grained
information but share little knowledge with the
training tasks. In Figure 1, all methods seem
to struggle in differentiating the fine-grained in-
tent classes volume_up ,volume_down , and
volume_mute , which are very dissimilar to
classes in training tasks. To quantify the ability
of different methods to discriminate them , we eval-
uate all methods only on these three classes in the
1-shot setting. Surprisingly, as shown in Table 2,
meta-learning models lag behind almost all gen-
eral sentence embedding methods. To investigate
whether this inferiority is caused by overfitting,
we take SBERT-NLI andSimCSE-NLIas base
encoders of ProtAugment , denoted as PA-SBERT-
NLI andPA-SimCSE-NLI . Table 2 indicates that
compared SBERT-NLI andSimCSE-NLI , finetun-
ing will lead to a performance drop of 7.10 points
and 7.23 points respectively, which verifies that
meta-learning based models may have a bias to-
wards categorizing intent classes similar to trainingtasks, which, on the debit side, restraints their capa-
bilities on distinguishing fine-grained test classes
that share little knowledge with training classes.
4.2 Cross-dataset generalization
To further verify our hypothesis that meta-learning
methods could fail when the target tasks require
fine-grained information but share little knowledge
with the training tasks, we test models on a more
challenging setting: directly transfer ProtAugment
trained on other datasets to the single-domain Bank-3810
ing77 dataset.From Table 3, we find:
1) General sentence embeddings have better
cross-dataset generalization performance. Pro-
tAugment, which shows better in-domain accu-
racy, lags behind most general sentence embed-
dings under the challenging cross-dataset gener-
alization test. ProtAugment trained on HWU64
and Liu57 performs the worst, even underperforms
the sentence embedding baselines since their in-
tents are about home robot, which is obviously
different from Banking77. ProtAugment trained
on Clinic150 achieves better performance since
Clinic150 has several intentsfrom bank domain,
but still inferior to SBERT-paraphrase on 5-way
1-shot setting, SBERT-paraphrase, SBERT-NLI,
SimCSE-NLI on 5-way 5-shot setting. This in-
dicates the meta-learning methods could overfit the
task distribution from training datasets. Benefiting
from the supervision signal and diverse data distri-
bution coverage provided by labeled sentence pairs
(paraphrase, or NLI), such tasks guided the gen-
eral sentence encoding to encode more fine-grained
information which might relevant to target intent
labels in any granularity, and moreover, reducing
the risk of overfitting to a small intent dataset.
2) Finetuning the strongest general sentence em-
bedding by meta-learning algorithm on intent
datasets struggles to bring significant improve-
ment compared with raw sentence embedding.
PA-SBERT-para. even drops performance com-
pared to SBERT-para. when finetuning on HWU64
and Liu57 datasets. This indicates that finetuning
sentence embedding on a small intent corpus dis-similar to test data may cause overfitting and harm
the cross-dataset generalization. When finetuning
on a more similar corpus Clinic 150, the limited im-
provement questions the necessity of current meta-
learning algorithms in practical large-domain gap
scenarios when a high-quality general sentence em-
bedding is available.
4.3 Case study
We conduct qualitative analysis to get a better un-
derstanding of the behaviors from different meth-
ods. A few examples have been listed in Table 4.
1) General sentence encoding captures semantic
relatedness instead of pure intent similarity be-
tween query and support, which may sometimes
be misleading. For the first example, all general
sentence embedding methods fail due to the shar-
ing part “list” between the query example and the
support example of the wrong label. This indicates
that the sentence embeddings actually capture the
relatedness between two sentences instead of in-
tents. So the general sentence embeddings tend
to be misled by irrelevant parts in the sentences
which do not convey the real intent. However, Pro-
tAugment can focus on the key parts such as verb
phrases for identifying intents by domain specific
finetuning.
2) Patterns learned by meta-learning mod-
els could overfit and fail in cross-dataset set-
tings. The second example from Banking77
shows an interesting case where ProtAugment
trained on HWU64 fails in the cross-dataset sce-
nario. The support example of the category
receiving_money and the query are seman-
tically close, therefore no surprise for our sentence
embedding baselines to get it correct. However,
ProtAugment makes the wrong prediction by incor-
rectly focusing on the same verb “transfer” between3811
the query and support example for the wrong label,
while ignoring the overall semantic meaning in the
sentence. Such shortcut leads to better accuracy
on the HWU64 dataset since most of its intents
contain only verbs and object nouns, however it
could lead to failure in Banking77 since intents in
Banking77 need capture more fine-grained seman-
tic information.
3) Uninformative support examples make all
methods fail. For the last instance, the support
example for meaning_of_life (asking what
is the meaning of life) is not a usual expression
for that intent, and all methods fail by predicting a
label with more content similarity.
5 An Easy Modification
5.1 Label names as support
Inspired by previous analysis revealing the need
for sentence encoders to capture more of the key
phrases, we propose the following frustratingly sim-
ple modification: adding the label names as in-
stances into the support set . Denoting the label
name of an intent category casl, then the proto-
type of class cafter adding the label name becomes
e=1
K+ 1(E(l) +/summationdisplayE(x)).
Note that the label names are always available for
freeat both meta-training and meta-testing phases.
The label name can be seen as a discriminative
andrepresentative example for the correspond-
ing intent. It is discriminative in the sense that
adding label names to the support set is equiv-
alent to putting more weights on words similar
to the intent phrase when calculating the proto-
types since words in the intent label usually are key
words. For the first example in Table 4, adding
the label name create_or_add_lists and
remove_lists could make the model payingmore attention to discriminate the act of “add” and
“delete”. The label name is also representative in
that it sometimes could directly convey a relatively
abstract concept. For the third example in Table
4, the label name meaning_of_life is more
informative than the support set examples.
5.2 Results and discussion
From Table 5, we can observe that the label name
support has consistently and substantially improved
all methods. Specifically, in the 5-way 1-shot set-
ting, the results improve about 3% to 11% on
Banking77, 6% to 10% on HWU64, 1% to 8%
on Clinc150. Moreover, adding the label name as
support could also improve the state-of-the-art Pro-
tAugment framework, as shown in the results of
L-ProtoNet+MLM and L-ProtAugment.
Meaningful labels improve baselines. As
shown in Table 6, for the first example, adding label
names correct the prediction of sentence embed-
ding baselines. Adding share_location as a
support example, the prototype for this class may
contain more information about “know” and “loca-
tion” compared to the irrelevant word “miranda”.
This illustrates the discriminative effectiveness of
label names. For the second example, all methods
incorrectly predict remove_lists because the
bad example given in the support set for the la-
belcreate_or_add_lists . Adding the label
names corrects all methods with better representa-
tion for the class create_or_add_lists .
Limitation: negative effect from misleading or
vague labels. We also observe slightly negative
effects in some cases. For example, when adding
the label name general_negate (this intent
means a person does not agree with something) for
the third example in Table 6, “negate” is not a usual
expression for the intent general_negate , and
the association between “not” in the query and3812
the word “don’t” in the label name general
don’t_care makes the model prefer this wrong
intent. We also find some label names in the Liu57
dataset to be rather vague or ambiguous, making
them difficult to bring any useful information. For
example, the label name likeness actually cor-
responds to utterances expressing the likeness to
music, while the label name music means listen-
ing to or playing music. Adding these label names
usually leads to confusion between these two in-
tents.
6 Related Work
6.1 Few-Shot Intent Detection
Studies on few-shot intent detection usually focus
on two settings: (1) only a handful of annotated
examples for each intent are available during train-
ing (Casanueva et al., 2020; Mehri and Eric, 2021;
Zhang et al., 2020, 2021b; Qu et al., 2021); (2) in
addition to the few-shot examples of target intents,
rich labeled examples of other intents are available
for training (Xia et al., 2021, 2020; Nguyen et al.,
2020; Li and Zhang, 2021; Dopierre et al., 2021b;
Yu et al., 2021; Zhang et al., 2021a). In this paper,
we focus on the second setting, where the problem
is typically formulated as the meta-learning prob-
lem and various approaches have been proposed
(Yu et al., 2018; Geng et al., 2019; Nguyen et al.,
2020; Li and Zhang, 2021; Dopierre et al., 2021b).
Dopierre et al. (2021b) propose to use diverse para-
phrases to improve the ProtoNet and achieve the
SoTA performance in the semi-supervised intent de-
tection meta-learning setting. Zhang et al. (2021a)
study the effectiveness of pre-training with labeled
intent data for this problem. Instead of developing
a new framework or a new algorithm, this work
conducts an empirical study on the effectiveness of
using general pre-trained sentence encodings forthis task.
6.2 Label Semantics for Low-Resource Text
Classification
There also exists more complex usage of label
names in the related literature on zero-shot or few-
shot text classification tasks (Yazdani and Hender-
son, 2015; Chen et al., 2016; Wang et al., 2018; Yan
et al., 2020; Luo et al., 2021; Hou et al., 2021), typ-
ically involving learnable label embeddings with
crafted encoder modules or a more clever usage of
pre-trained language models.
In zero-shot text classification, Chang et al.
(2008) embed label descriptions and texts into a
shared Wikipedia concept space and then measure
their similarities for classification. Similarly, Chen
et al. (2016) jointly embed label names and utter-
ances into one distributed embedding space. Yaz-
dani and Henderson (2015) leverage the structure
of intent labels to produce a classification hyper-
plane for zero-shot intent classification.
For few-shot text classification, prompt-based
methods (Schick and Schütze, 2021a,b) use la-
bel names to construct verbalizers to map each
label into cloze-style phrases, which enables the
utilization of powerful pretrained language mod-
els. More recently, Müller et al. (2022) propose
label tuning which only finetunes the label embed-
dings but freezes the sentence encoder for few-shot
text classification. Mueller et al. (2022) explore
label semantics by performing pre-training on a
mix of gold and weakly annotated sentence-label
pairs. Another line of works (Luo et al., 2021;
Hou et al., 2021) incorporate label names into the
meta-learning models. Luo et al. (2021) append
label names to the utterances to help the model ex-
tract discriminative features. Hou et al. (2021) take
a linear combination of the prototype calculated3813by support examples and the label embedding as
the anchored label representation to separate dif-
ferent labels in a multi-label intent classification
scenario. Different from previous works, our sim-
ple modification aims to enhance general sentence
embeddings towards key parts that express an in-
tent. Besides, the proposed modification doesn’t
introduce any parameters, so it can be adapted to
any pretrained sentence encoders with ease.
7 Conclusion and Future Work
Motivated by the nature of prototypical networks
for intent detection meta-learning, in this paper,
we empirically compare some modern popular sen-
tence encoders on multiple intent detection bench-
marks, observing non-trivially strong performance
with better cross-dataset generalization capability
than the fine-tuned sentence encoders. Inspired
by our follow-up analysis, we propose a simple
modification that has consistently and substantially
brought performance gain over all systems: adding
the intent label names into the support set. This
strategy not only improves over the performance
from general-purpose sentence encoding, but also
the state-of-the-art results from the fine-tuned Pro-
tAugment framework.
One limitation of our study for now is that the
sentence representation in use is mostly based on
BERT variants. It could be technically interest-
ing to experiment with models pre-trained in a
sequence-to-sequence fashion (Lewis et al., 2020;
Raffel et al., 2020), which might have better cap-
tured semantic paraphrase representations via de-
noising or other training objectives.
Acknowledgments
We would like to thank Jin-Ge Yao for helpful dis-
cussion and valuable feedback on this study. We
thank all the anonymous reviewers for their sugges-
tions to improve the initial draft.
References381438153816A Appendix
Extended Analysis for Label Semantics Since
we add label names as additional support examples,
the embedding space of sentences does not change
in general sentence embedding based methods. For
meta-learning based methods, we can take label
names as support examples only in the meta-testing
phase, or leverage them in both meta-training and
meta-testing stages. The latter will change the em-
bedding space. As shown in Table A.1, we find
that adding label names at both stages generally
yields slightly better performance than only using
it during meta-testing.
To further validate the effectiveness of using la-
bel names, we sample fifty 5-way 1-shot episodes
from five classes in Liu57 dataset. We visualize
the query instances and prototype representations
in the same embedding space. As shown in Fig-
ure A.1, in the 1-shot setting, the prototypes of
the same class spread out and it is difficult to dis-
tinguish them from other classes. This indicates
that the model struggles to extract the crucial in-
formation relevant to the intent class due to the
limited information contained in one support exam-
ple. However, adding label names helps centralize
the prototypes of the same class and separate them
away from those of other classes. This suggests that
label names may be regarded as high-quality free
examples which could help distinguish different
classes.
Implementation Details We use Euclidean dis-
tance as the distance metric in ProtoNet for all
sentence encoders, except for SP-paraphrase, in
which cosine distance leads to much better perfor-
mance. For meta-learning based methods, we use
RoBERTa-base from Huggingface (Wolf et al.,
2020) as the encoder. For the optimizer, we use
Adam (Kingma and Ba, 2015) with a learning rate
of 2e-5. We set the batch size to 32 and train the
model for 10,000 episodes. We use the validation
split to evaluate the model every 600 episodes and
select the checkpoint with the best performance.
The ProtAugment model has 123M parameters and
the training lasts around one hour on four Tesla
V100 GPUs.38173818