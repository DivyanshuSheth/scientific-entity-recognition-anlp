
Yongwei Zhou, Junwei Bao, Chaoqun Duan, Haipeng Sun, Jiahui Liang,
Yifan Wang, Jing Zhao, Youzheng Wu, Xiaodong He, Tiejun ZhaoHarbin Institute of TechnologyJD AI Research
ywzhou@hit-mtlab.net baojunwei@jd.com tjzhao@hit.edu.cn
Abstract
Machine reading comprehension (MRC) that
requires discrete reasoning involving symbolic
operations, e.g., addition, sorting, and count-
ing, is a challenging task. According to this
nature, semantic parsing-based methods pre-
dict interpretable but complex logical forms.
However, logical form generation is nontriv-
ial and even a little perturbation in a logical
form will lead to wrong answers. To alleviate
this issue, multi-predictor -based methods are
proposed to directly predict different types of
answers and achieve improvements. However,
they ignore the utilization of symbolic opera-
tions and encounter a lack of reasoning ability
and interpretability. To inherit the advantages
of these two types of methods, we propose
OPERA , an operation-pivoted discrete reason-
ing framework, where lightweight symbolic
operations (compared with logical forms) as
neural modules are utilized to facilitate the rea-
soning ability and interpretability. Specifically,
operations are first selected and then softly exe-
cuted to simulate the answer reasoning proce-
dure. Extensive experiments on both DROP
and RACENum datasets show the reasoning
ability of OPERA. Moreover, further analysis
verifies its interpretability.
1 Introduction
Machine reading comprehension (MRC) that re-
quires discrete reasoning is a valuable and chal-
lenging task (Dua et al., 2019), especially involv-
ing symbolic operations such as addition, sorting,
and counting. The examples in Table 1 illustrate
the task. To answer the question “ Who threw the
longest pass ”, it requires a model to choose the per-
son with the longest pass from all the people and
Table 1: An example of question-answer pair along
with a passage from DROP dataset (Dua et al., 2019).
Question words in color indicate the potential operations
for reasoning, i.e., ARGMAX andKEY_VALUE .
corresponding distance pairs based on the given
passage. This task has various application scenar-
ios in the real world, such as analyzing financial
reports and sports news.
Existing approaches for this task can be roughly
divided into two categories: the semantic-parsing-
based (Chen et al., 2020b; Gupta et al., 2020)
and the multi-predictor-based methods (Dua et al.,
2019; Ran et al., 2019; Hu et al., 2019; Chen et al.,
2020a; Zhou et al., 2021). The former maps the
natural language utterances into logical forms and
then executes them to derive the answers. For ex-
ample, Chen et al. (2020b) propose NeRd. It in-
cludes a reader to encode the passage and question,
and a programmer to generate a logical form for
multi-step reasoning. Intuitively, this method has
an advantage in interpretability. However, seman-
tic parsing over text is nontrivial and even a little
perturbation will result in wrong answers, which
hinders the MRC performance.
To alleviate the heavy dependence on logical
forms in the first category, the latter directly em-
ploys multiple predictors to derive different types
of answers. For example, Dua et al. (2019) and
Chen et al. (2020a) divide instances of the DROP
dataset into several types and design a model with
multi-predictors to deal with different answer types,
i.e., question/passage span(s), count, and arithmetic1655expression. It is the capability of deriving different
types of answers that improves the performance of
models. However, such methods are lack of the
necessary components to imitate discrete reason-
ing, which leads to inadequacy in reasoning ability
and interpretability.
To alleviate the shortcomings of the above meth-
ods and preserve their advantages, we attempt to
summarize reasoning steps into a set of operations
and adopt them as the pivot to connect the ques-
tion and the answer, which makes it possible to
perform discrete reasoning. For example, to an-
swer the question in Table 1, it needs two steps: (1)
finding all persons and the corresponding distance
of touchdown pass, and (2) choosing the one with
the longest pass among them. We attempt to con-
vert them into two operations, KEY_VALUE and
ARGMAX , respectively. We then use them to pro-
duce the answer. Specifically, we design a set of
lightweight symbolic operations (compared with
logical forms) to cover all of the questions in the
datasets and utilize them as neural modules to facil-
itate reasoning ability and interpretability. We de-
note this method as OPERA , an operation-pivoted
discrete reasoning MRC framework. To utilize
the operations, we propose an operation-pivoted
reasoning mechanism composed of an operation
selector and an operation executor. Specifically,
the operation selector automatically identifies rel-
evant operations based on the input. To enhance
the performance of this sub-mechanism, we further
design an auxiliary task to learn the alignment from
a question to operations according to a set of heuris-
tics rules. The operation executor softly integrates
the selected operations to perform discrete reason-
ing over text via an attention mechanism (Vaswani
et al., 2017).
To verify the effectiveness of the proposed
method, comprehensive experiments are conducted
on both the DROP and RACENum datasets, where
RACENum used in this paper is a subset of the
RACE dataset following (Chen et al., 2020a). Ex-
perimental results indicate that our method out-
performs strong baselines and achieves the state-
of-the-art on both datasets under the single model
setting. We further analyze the interpretability of
OPERA. Overall, this paper primarily makes the
following contributions:
(1) We propose OPERA, an operation-pivoted
discrete reasoning MRC framework, improving
both the reasoning ability and interpretability.(2) Extensive experiments on DROP and
RACENum dataset demonstrate the reasoning abil-
ity of OPERA. Moreover, statistic analysis and vi-
sualization indicate the interpretability of OPERA.
(3) We systematically design operations and
heuristic rules to map questions to operations, aim-
ing to facilitate research on symbolic reasoning.
2 Related Work
Recently, machine reading comprehension (MRC)
methods tend to deal with more practical prob-
lems (Yang et al., 2018; Dua et al., 2019; Zhao et al.,
2021), for example, answering complex questions
that require discrete reasoning (Dua et al., 2019)
such as arithmetic computing, sorting, and count-
ing. Intuitively, semantic parsing-based meth-
ods, which are well explored to deal with discrete
reasoning in question answering with structured
knowledge graphs (Bao et al., 2016) or tables, have
potential to address the discrete reasoning MRC
problem. Therefore, semantic parsing-based meth-
ods for discrete reasoning over text are proposed
to firstly convert the unstructured text into a table,
and then answer questions over the structured ta-
ble with a grammar-constrained semantic parser
(Krishnamurthy et al., 2017). NeRd (Chen et al.,
2020b) is a generative model that consists of a
reader and a programmer, which are responsible
for encoding the context into vector representation
and generating grammar-constrained logical forms,
respectively. NMNs (Gupta et al., 2020) learned to
parse compositional questions as executable logical
forms. However, it only adapts to limited question
types matched with a few pre-defined templates.
Multi-predictor-based methods employ multi-
ple predictors to derive different types of answers.
NAQANET (Dua et al., 2019), a number-aware
framework, employed multiple predictors to pro-
duce corresponding answer types, including a
span, count, and arithmetic expression. Based
on NAQANET, MTMSN(Hu et al., 2019) added a
negation predictor to solve the negative question
and re-ranked arithmetic expression candidates. To
aggregate the relative magnitude relation between
two numbers, NumNet (Ran et al., 2019) and Num-
Net+ leveraged a graph convolution network to
perform multi-step reasoning over a number graph.
QDGAT (Chen et al., 2020a) proposed a question-
directed graph attention network for reasoning over
a heterogeneous graph composed of entity and num-
ber nodes. EviDR (Zhou et al., 2021), an evidence-1656
emphasized MRC method, performed reasoning
over a multi-grained evidence graph. Compared
with these existing methods, our proposed OPERA
focuses on bridging the gap from questions to an-
swers with operations and integrating them to sim-
ulate discrete reasoning.
3 Approach
3.1 Task and Model Overview
Given a question Qand a passage P, MRC that
requires discrete reasoning aims to predict an an-
swer ˆAwith the maximum probability over the
candidate space Ωas follows:
ˆA= arg maxp(A|Q, P) (1)
where the answer ˆAin this task could not only
be span(s) extracted from context but also a num-
ber calculated with some numbers in context. To
handle this task, it generally requires not only nat-
ural language understanding but also performing
discrete reasoning over text, such as comparison,
sorting and arithmetic computing.
To address the aforementioned challenges in this
task, we propose OPERA, an operation-pivoted dis-
crete reasoning MRC framework and it is briefly
illustrated in Figure 1. In our framework, a set
of operations OP, defined in Table 2, are intro-
duced to support the modeling of answer probabil-
ityp(A|Q, P)as follows:
p(A|Q, P) =/summationdisplayp(A|Q, P, O )p(O|Q, P),
(2)
where O∈ OP represents one of the operations.
Concretely, in our framework, we first design an op-
eration selector p(O|P, Q)for choosing the correct
question-related operations. These selected opera-
tions are then softly executed over the given context.
Eventually, answer predictor p(A|Q, P, O )utilizes
the execution result to predict the final answer.3.2 Definition of Operations
To imitate discrete reasoning, we design a set of
operations OPas shown in Table 2. The set con-
tains 11 operations and each one represents a rea-
soning unit. Specifically, for questions that need
to be answered by calculation, we design opera-
tionsADDITION/DIFF to represent addition and
subtraction. For questions which need to be an-
swered by counting or sorting, we also design op-
erations COUNT ,MAX/MIN ,ARGMAX/ARGMIN ,
andARGMORE/ARGLESS . The rest operations
KEY_VALUE andSPAN are used to extract spans
from the question and the passage. To incor-
porate them into OPERA, each operation is de-
noted as a tuple. Formally, i-th operation OPis
⟨E, f(·)⟩, where i∈ {1,2, ..., n}andnis the
numbers of operations. E∈Rrepresents the
learnable embedding of the i-th operation. f(·)
is a neural executor parameterized with trainable
matrices W,WandW∈R. The
neural executor f(·)is capable of performing exe-
cution of OPon the given context. Specifically, it
takes the representation of context as input and out-
puts the executed representation as m(§ 3.3.2).
3.3 Architecture of OPERA
3.3.1 Context Encoder
The context encoder aims to learn the contextual
representation of the input. Formally, given a ques-
tionQand a passage P, we concatenate them into
a sequence and feed it into a pre-trained language
model (Liu et al., 2019; Clark et al., 2020; Lan
et al., 2020) to obtain their whole representation
H∈R. After that, we split Hinto the ques-
tion and passage representations, which are respec-
tively denoted as H∈RandH∈R.
l,l, and lare the number of tokens in question,
passage and concatenation of them. dis the di-
mension of the representations.1657
3.3.2 Operation-pivoted Discrete Reasoning
The operation-pivoted reasoning module is com-
posed of an operation selector and an operation
executor. The operation selector is adopted to se-
lect operations related to the given question. The
operation executor is responsible for imitating the
execution of the selected operations with an atten-
tion mechanism.
Operation Selector To imitate discrete reason-
ing, existing methods usually adopt a logical form
generated by a semantic parser to address this task.
However, these methods suffer severely from the
cascade error, where a little perturbation in the log-
ical form may result in wrong answers. Therefore,
we propose to map each question into an operation
set, instead of logical forms. Namely, we intend
to select relevant operations from the OP. To this
end, we adopt a bilinear function to compute the
similarity between each operation and the question
and normalize them with a softmax as follows:
p(O|Q, P) =softmax (EWh),(3)
where E∈Ris a learnable parameter,
which demotes the operation embedding matrix.
h∈Ris the representation of the question,
which is obtained by executing weighted pooling
on the H.W∈Ris a parameter matrix
andp(O|Q, P)is the distribution over operations.
Operation Executor The operation executor is
responsible for performing the execution of the se-
lected operations over the given context. Inspired
by previous studies (Andreas et al., 2016; Gupta
et al., 2020), we implement the operation executorbased on the neural module network, which takes
advantage of neural network in fitting and general-
ization, and the composition characteristics of sym-
bolic processing. Specifically, for each operation
OP=⟨E, f(·)⟩,i={1,2, ..., n}, we use a
multi-head cross attention mechanism (Vaswani
et al., 2017) to implement f(·). In detail, we lever-
age the embedding of each operation Eas query
and the representations of the whole input sequence
Has keys and values, respectively, to model f(·)
as follows:
α=softmax ((EW)(HW)
√d),
(4)
m=α(HW), (5)
where W,W,W∈Rare the pa-
rameter matrices in executor of operation OP.
m∈Ris the representation of the execution
result of the i-th operation.
Finally, we softly integrate all of the execution
results as the final output h∈Rwith the
distribution p(O|Q, P)as follows:
h=/summationdisplayp(O=OP|Q, P)m.(6)
The operation-aware semantic representation h
is further fed into the prediction module to reason
the final answer (§ 3.3.3).
As described above, OPERA introduces oper-
ations that assist in understanding questions and
integrates them into the model to perform discrete
reasoning. Therefore, it achieves an advantage in1658the reasoning capability and interpretability over
the previous multi-predictor-based methods (Hu
et al., 2019; Chen et al., 2020a; Zhou et al., 2021).
Moreover, soft execution and composition of opera-
tions in OPERA alleviate the cascaded error that the
semantic parsing methods (Ran et al., 2019; Chen
et al., 2020b) suffer from. More experiments and
analyses about reasoning ability and interpretability
are illustrated in § 4.4 and § 4.5.
3.3.3 Prediction Module
In this section, we introduce the prediction mod-
ule to derive different types of answers via multi-
predictors. Each predictor first reasons out a deriva-
tion and then performs execution to obtain the final
answer. This answer prediction procedure is for-
malized as follows:
p(A|Q, P, O ) =/summationdisplayI(g(D) =A)p(D|Q, P, O ),
(7)
where I(g(D) =A)is an indicator function with
value 1 if the answer Acan be derived from a
derivation executor g(·)based on D, and 0 oth-
erwise. p(D|Q, P, O )models the derivation pre-
diction. Specifically, a derivation D=⟨T, L⟩
includes an answer type Tand a corresponding
label L. For example, in Table 3, the textual an-
swerAof the question “ how many yards was the
longest field goals in the game ” is “ 80”. The pos-
sible derivations Dto this answer include a span
⟨Span,(100,102)⟩, and an arithmetic expression
⟨AE,(0∗29) + (1 ∗80)⟩. Inspired by previous
studies (Chen et al., 2020a; Zhou et al., 2021), the
derivation predictor
p(D|Q,P,O )=/summationdisplayp(L|Q,P,O )p(T|Q,P,O )
(8)
is decomposed into an answer type predictor
p(T|Q, P, O )and corresponding label predictors
p(L|Q, P, O )whereT={Question Span ,Pas-
sage Span ,Count ,Arithmetic Expression ,Multi-
spans}includes all the answer types defined in this
paper. Each label predictor takes question-passage
representation Hand the operation-pivo represen-
tation has input and calculates the probability
of label L. Specifically, these label predictors are
specified as follows and more details are shown in
Appendix A.3.
Question / Passage Span The probability of a ques-
tion/passage span is the product of the probabilities
of the start index and the end index. FollowingMTMSN (Hu et al., 2019), we use a question-aware
decoding strategy to predict the start and end index
across the input sequence, respectively.
Count As indicated in QDGAT (Chen et al.,
2020a), questions with 0-9 as answers account for
97% in all the count questions. Hence, such ques-
tions are modeled as a 10-class (0-9) classification
problem.
Arithmetic Expression Similar to NAQANet (Dua
et al., 2019), we first assign a sign (positive, neg-
ative, or zero) to each number in the context and
then compute the answer by summing them.
Multi-spans Inspired by Segal et al. (2020), the
multi-span answer (a set of non-contiguous spans)
is derived with a sequence labeling method, in
which each token of the input is tagged with BIO
labels. Finally, each span which is tagged with
continuous BandIis taken as a candidate span.
3.4 Learning with Weak Supervision
3.4.1 Training Instance Construction
Each training instance is originally composed of a
passage P, a question Q, and answer text A. Since
the derivations (i.e., labels for the spans, arithmetic
expressions, and count) are not provided, weak
supervision is adopted in OPERA. Specifically, for
each training instance, given the golden textual
answer A, we heuristically search all the possible
derivations Das supervision signals, each of which
can derive the correct answer A. Table 3 shows an
example of D.
In addition, we propose heuristic rules to map
a question to its related operations denoted as
O ⊆ OP . For example, to detect the operations
intimated by the question Qin Table 3, we design a
question template “ how many yards [Slot] longest
[Slot] ” which maps matched questions to the oper-
ationMAX. Overall, a training instance can be con-
structed as a tuple ⟨P, Q, A, O,D⟩. The one-shot
heuristic rules to obtain operation labels reduce the
cost of human annotations. Moreover, when apply-
ing OPERA to other discrete reasoning MRC tasks,
both the operations OPand the heuristic rules can
be extended and adjusted if necessary. Fortunately,
there is no need to construct strict logical forms in
our architecture, but only the set of lightweight op-
erations involved in the question. It tremendously
reduces the difficulty of adapting OPERA to other
discrete reasoning MRC tasks.
Meanwhile, we analyze the distribution of oper-
ations in the training set. More details about the1659
heuristic rules for mapping questions to operations
and the operation distribution in the dataset are
respectively given in the Appendix A.1 and A.2.
3.4.2 Joint Training
The training objective consists of two parts, includ-
ing the loss for answer prediction and operation
selection. The loss for answer prediction Lis
L=−logp(A|Q, P). (9)
Note that the calculation of loss Ltakes all possi-
ble derivations that can obtain the correct answer
Ainto account, which means that OPERA does
not require labeling answer types for training. In
addition, to learn better alignment from a question
to operations, we introduce auxiliary supervision
for the operation selector and calculate the loss
L=−/summationdisplaylogp(O|Q, P), (10)
where Oindicates the operations provided by the
heuristic rules. Finally, OPERA is optimized by
minimizing the loss L=L+λLwhere λis a
hyperparameter as a trade-off of the two objectives.
4 Experiment
4.1 Dataset and Evaluation
We conduct experiments on the following two
MRC datasets to examine the discrete reasoning
capability of our model. We employ Exact Match
(EM) and F1 score as the evaluation metrics.
DROP Question-answer pairs in DROP dataset
(Dua et al., 2019) are crowd-sourced based on
passages collected from Wikipedia. In detail,
it contains 96.6K question-answer pairs, where
77400/9536/9615 samples are for training/develop-
ment/test. Three kinds of answers are involved in
the raw dataset, i.e., NUMBER (60.69%), SPANS
(37.72%), and DATE (1.59%).RACENum To investigate the generalization ca-
pability of OPERA, we compare OPERA to other
strong baselines on samples of RACE (Lai et al.,
2017). Following Chen et al. (2020a), we sam-
ple instances from RACE, denoted as RACENum,
where the question of each instance starts with
“how many”. To conveniently evaluate the models
on RACENum, we convert the format of instances
in RACENum into the same as DROP, since RACE
is a multi-choice MRC dataset. RACENum is di-
vided into two categories, i.e., middle/high school
exam (RACENum-M/H). They respectively con-
tain 609 and 565 questions, where the scale is a bit
different from that reported in Chen et al. (2020a).
4.2 Baselines
We compare OPERA with various prior systems in
terms of reasoning capability and interpretability.
w/o Pre-trained Language Model: NAQANET
(Dua et al., 2019) leverages several answer predic-
tors to produce corresponding types of answers,
including a span, count, and arithmetic expression.
NumNet (Ran et al., 2019) leverages a graph con-
volution network to reason over a number graph
aggregated relative magnitude among numbers.
w/ Pre-trained Language Model: GenBERT
(Geva et al., 2020) injects reasoning capability into
BERT by pre-training with large-scale numerical
data. Based on NAQANET, MTMSN (Hu et al.,
2019) adds a negation predictor to solve the neg-
ative question and re-rank arithmetic expression
candidates. NeRd (Chen et al., 2020b) is essen-
tially a generative semantic parser that maps ques-
tions and passages into executable logical forms.
ALBERT-Calc was proposed for DROP by combin-
ing ALBERT with several predefined answer pre-
dictors (Andor et al., 2019). NumNet+ employs a
pre-trained model to further boost the performance
of NumNet. QDGAT (Chen et al., 2020a) builds
a heterogeneous graph composed of entity and
value nodes upon RoBERTa and utilizes a question-
directed graph attention network to reason over the
graph. EviDR (Zhou et al., 2021), an evidence-
emphasized MRC model, performs reasoning over
a multi-grained evidence graph based on ELEC-
TRA.
4.3 Implementation Details
We utilize adam optimizer (Kingma and Ba, 2015)
with a cosine warmup mechanism and set the1660
weight of loss λ= 0.3to train the model. The
hyper-parameters are listed in Table 4, where BLR,
LR, BWD, WD, BS, and drespectively represent
the learning rate of the encoder, the learning rate
of other parts of the model, the weight decay of
the encoder, the weight decay of other parts of the
model, batch size and hidden size of the model.
Each operation is neutralized with a multi-head
attention layer with nheads and ddimension.
4.4 Main Results
4.4.1 Results on DROP and Analysis
Table 5 shows the overall results of OPERA and
all the baselines on the DROP dataset. OPERA
achieves comparable and even higher performance
than the recently available methods. Specifically,
OPERA(RoBERTa) achieves comparable perfor-
mance to QDGAT with advantages of 0.32 EM
and 0.42 F1. OPERA(ELECTRA) exceeds EviDR
by 0.89 EM and 0.90 F1 and OPERA(ALBERT)
outperforms ALBERT-Calc by 4.84 EM and 4.24
F1. Moreover, the voting strategy is employed to
ensemble 7 OPERA(ALBERT) models with differ-
ent random seeds, achieving 86.26 EM and 89.12
F1 scores. We think the better performance comes
from the modeling of discrete reasoning over text
via operations, which mines more semantic infor-
mation of context and explicitly integrates them
into the answer prediction.
4.4.2 Results on RACENum
To investigate the generalization of OPERA for dis-
crete reasoning, we additionally compare OPERA
with QDGAT and NumNet+ on the RACENum
dataset. We directly evaluate the three models with-
out fine-tuning on RACENum due to its small scale.
As Table 6 shows, the scores of models on the
RACENum dataset are generally lower than that
on the DROP dataset, which is attributed to the
lack of in-domain training data. Nevertheless, the
performance of OPERA significantly outperforms
NumNet+ and QDGAT by a large margin of more
than 3.49 EM and 3.53 F1 score on average. It indi-
cates that OPERA has better generalization ability.
4.5 Interpretability Analysis
Interpretability is an essential property for evaluat-
ing an MRC model. We analyze the interpretability
of OPERA from the following two stages: (1) map-
ping from questions to operations, and (2) mapping
from operations to answers.
Mapping from Question to Operation To ex-
plicitly show the correlations between questions
and related operations, we manually evaluate the
performance of the operation selection on 50 sam-
ples on the development set of DROP. Specifically,
precision@n (P@n) is used as the evaluation met-
ric, i.e., judging whether the top npredicted oper-
ations contain the correct ones according to ques-
tions. We finally achieve 0.88 on P@1 and 0.98
on P@2 for our model OPERA, which indicates
that the operation selection module can accurately
predict interpretable operations.1661
Mapping from Operation to Answer We ex-
plore the relations between operations and final
answer types based on model predictions on the
development set of DROP. Specifically, for each
type of answer, the predicted operation distribu-
tions are summed over all samples and normal-
ized, which derives a relation matrix as shown
in Figure 2. We can observe that obvious cor-
relations between operations and answer types
exist. ADDITION ,DIFF ,MAX andMIN obvi-
ously correspond to Arithmetic Expression . The
top 3 answer types for KEY_VALUE andSPAN
arePassage Span ,Multiple Spans , and Question
Span .COUNT probably maps to Count answer type.
ARGMORE ,ARGLESS ,ARGMAX , andARGMIN are
usually used for both Passage Span andArithmetic
Expression .
4.6 Ablation Study
Effect of Operations for OPERA As shown in
Table 7, we first remove the loss of operation se-
lection (w/o L) by setting λ= 0, resulting in
the performance degradation of 0.74 EM / 0.52 F1
points and 0.14 EM / 0.18 F1 points for OPERA
based on RoBERTa and ALBERT, respectively. It
indicates that the supervision for explicitly learn-
ing the alignment from a question to operations
contributes to the reasoning capability of OPERA.
Yet our approach is somewhat limited by the fact
that the operation selector needs an auxiliary train-
ing task to work better, and heuristics rules are
required to map questions into an operation set as
training labels. Furthermore, we remove the total
operation-pivoted reasoning module (w/o OP), the
performance respectively declines by 1.06 EM /
0.79 F1 points and 0.85 EM / 0.75 F1 points for
OPERA(RoBERTa) and OPERA(ALBERT). We
also conduct the ablation study on the subsets con-
taining a specific operation. As shown in Figure 3,
OPERA achieves better performance than OPERA
w/o OP on the majority of subsets. Overall, it con-
firms that integrating the operation-pivoted discrete
reasoning mechanism contributes to the reasoning
ability of the model.
Probe on Answer Types and Language Models
As reported in Table 7, we observe that the perfor-
mance on the NUMBER(NUM) and SPANS ques-
tions, which together account for 98.4% of the total,
respectively declines by 0.55 EM / 0.61 F1 and 1.36
EM / 1.84 F1 when removing operation-pivoted rea-
soning mechanism from OPERA(RB). It demon-
strates that this mechanism contributes to various
answer types. Also, we respectively evaluate the
performance of OPERA based on RoBERTa and
ALBERT. We observe that OPERA(ALBERT) out-
performs OPERA(RoBERTa) due to the stronger
capability of semantic representation. Furthermore,
integrating this mechanism consistently contributes
to the performance of OPERA no matter it is based
on RoBERTa or ALBERT. It indicates that OPERA1662
could compensate for the discrete reasoning capa-
bility of language models.
4.7 Case Study
We show two examples from the development set
of DROP to illustrate the effectiveness of our model
by comparing the results of different models in Ta-
ble 8. The first example shows that operation is
essential for the prediction of answer type. Num-
Net+ and QDGAT fail to predict the correct an-
swer since the answer type of “how many” ques-
tions are wrongly predicted to Count . In contrast,
OPERA can capture the ADDITION operation,
which prompts the model to answer it with an arith-
metic expression predictor. The second example
shows that OPERA has stronger reasoning capabil-
ity. In the example, though NumNet+ and QDGAT
correctly predict the answer type, the final answer
is wrong. OPERA can utilize more semantic infor-
mation for answer prediction with the help of the
operation-pivoted discrete reasoning mechanism.
5 Conclusion
We propose a novel framework OPERA for ma-
chine reading comprehension requiring discrete
reasoning. Lightweight and one-shot operations
and heuristic rules to map questions to an operation
set are systematically designed. OPERA can lever-
age the operations to enhance the model’s reason-
ing capability and interpretability. Experiments on
DROP and RACENum demonstrate that OPERA
achieves remarkable performance. Further visual-
ization and analysis verify its interpretability.
6 Acknowledge
We would like to thank all the anonymous review-
ers for their useful feedback. This work is sup-
ported by the project of the National Natural Sci-
ence Foundation of China (No.U1908216) and theNational Key Research and Development Program
of China (No. 2020AAA0108600).
References16631664A Appendix
A.1 Template-Based Heuristic Rules
In this section, we propose some general
template-based heuristic rules to illustrate map-
ping from questions to operations. For example,
to detect the operations intimated by the question
"how many yards was the longest field goals " in
Table 9, we design a question template OP
"how many yards [Slot] longest/shortest [Slot] "
which maps matched questions to the operation
MAX. Meanwhile, we humanly evaluate the quality
of the heuristic rules. Specifically, we sample 50
instances from the training set and ask three an-
notators to label the required operations for each
question manually. The final average F1 score of
the three annotators is 86%, which indicates that
the heuristic rules can correctly predict most of the
operations, while still 14% to be noise for training.
A.2 Distribution of the Operations
We analysis the distribution of operations in the
training set of DROP, where ADDITION ,DIFF
andSPAN together accounts for more than 85%.
For other questions with span answers that requires
sorting or comparison, some specific operations are
involved, such as ARGMAX /ARGMIN /ARGMORE
/ARGLESS andKEY_VALUE .
A.3 Details of Prediction Module
In this section, we reveal the architecture details
of the prediction module, including a prediction
module for answer type and five label predictors
corresponding to different answer types. FFN(·)
means a feed-forward network that consists of two
linear projections with a GeLU activation function
(Hendrycks and Gimpel, 2016) and a layer normal-
ization mechanism (Ba et al., 2016).
Answer Type The probability distribution of an-
swer type choices p(T|Q, P, O )is derived by a
|T |-classifier with h,handhas input:
h=/summationdisplayp(O|Q, P)E,
p(T|Q, P, O )∝FFN([h;h;h]),(11)
where handh∈Ris the representation vec-
tor of question and passage calculated by weighted
pooling with HandH, respectively. Eis
the embedding matrix of operations.
Question/Passage Span Following MTMSN
(Hu et al., 2019), we use a question-aware decod-
ing strategy to predict the start and end indices of
the answer span. Specifically, we first compute
a question representation vector gvia weighted
pooling. Then derive the probabilities of the start
and end indices of the answer span denoted as p
andp:
M= [h;H;H⊙g],
p, p∝FFN(M),(12)
where ⊙denotes element-wised product. his
derived by Eq. 6 and His the representation of
input sequence from context encoder.1665Count The count predictor is a 10-classifier with
the operation-aware representation, all the men-
tioned number representation, question and pas-
sage representations as input. Specifically, when
Nnumbers exists, we gather the representation
of all numbers U= (u,u, ...,u)∈R
fromHand compute a global representation vector
of numbers as h. Then compute the probability
distribution of count answer p:
α∝UW,h=αU,
p∝FFN([h;h;h;h]),(13)
Arithmetic Expression Similar to NAQANet
(Dua et al., 2019), we perform addition and subtrac-
tion over all the numbers mentioned in the context
by assigning a sign (plus, minus, or zero) to each
number. The probability pof the i-th number’s
sign is derived as below:
p∝FFN([h;u;h;h]). (14)
Multi-Spans Inspired by Segal et al. (2020), the
multi-span answer is derived with a sequence role
labeling method over the input token sequence, de-
noted as SRL(·).pis the probability distribution
of token’s BIO tag:
p=SRL([H;h]). (15)1666