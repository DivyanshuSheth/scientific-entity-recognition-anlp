
Gowtham Ramesh, Makesh Sreedhar, and Junjie Hu
University of Wisconsin-Madison
{gramesh4,msreedhar,junjie.hu} @wisc.edu
Abstract
Recent generative approaches for multi-hop
question answering (QA) utilize the fusion-in-
decoder method (Izacard and Grave, 2021) to
generate a single sequence output which in-
cludes both a final answer and a reasoning path
taken to arrive at that answer, such as passage
titles and key facts from those passages. While
such models can lead to better interpretability
and high quantitative scores, they often have
difficulty accurately identifying the passages
corresponding to key entities in the context, re-
sulting in incorrect passage hops and a lack
of faithfulness in the reasoning path. To ad-
dress this, we propose a single-sequence pre-
diction method over a local reasoning graph
(SG )that integrates a graph structure
connecting key entities in each context passage
to relevant subsequent passages for each ques-
tion. We use a graph neural network to encode
this graph structure and fuse the resulting rep-
resentations into the entity representations of
the model. Our experiments show significant
improvements in answer exact-match/F1 scores
and faithfulness of grounding in the reasoning
path on the HotpotQA dataset and achieve state-
of-the-art numbers on the Musique dataset with
only up to a 4% increase in model parameters.
1 Introduction
Multi-hop Question Answering (QA) involves rea-
soning over multiple passages and understanding
the relationships between those pieces of informa-
tion to answer a question. Compared with single-
hop QA, which often extracts answers from a single
passage, multi-hop QA is more challenging as it
requires a model to determine the relevant facts
from multiple passages and connect those facts for
reasoning to infer the final answer.
To tackle multi-hop QA, recent works have inves-
tigated large pretrained generative models (LewisFigure 1: Localized graph construction connecting en-
tity spans to corresponding passages in the context. If
there are multiple passages with the same title, we con-
nect the entity span to all such passages.
et al., 2020b; Roberts et al., 2020; Brown et al.,
2020) and demonstrated their effectiveness over
traditional extractive models (Chen et al., 2017).
Compared with extractive models, the ability of
generative models to effectively aggregate and com-
bine evidence from multiple passages proves ad-
vantageous for multi-hop QA. In particular, Izacard
and Grave (2021) propose a method called FD
(Fusion-in-Decoder), which leverages passage re-
trieval with a generative model, such as T5 (Raf-
fel et al., 2020) or BART (Lewis et al., 2020a),
to achieve state-of-the-art performance on various
single-hop QA tasks. However, this approach does
not extend well to multi-hop QA tasks (Yavuz et al.,
2022), as it sorely relies on a black-box genera-
tive model to generate answers directly without
explicitly modeling the multi-hop reasoning pro-
cess. Additionally, FDencodes multiple context
passages independently for multi-hop QA, ignoring
the structural and semantic relationship between
these passages (Yu et al., 2022). Building on FD,
P-FD(Yavuz et al., 2022) addresses the in-
terpretability issue by training a model to gener-
ate a reasoning path that contains supporting pas-11466sage titles, facts, and the final answer. However,
our analysis of P-FDoutputs shows discon-
nected reasoning with incorrect passage hops in the
model’s reasoning path which affects final answer
generation. Recently, there have been multiple tech-
niques (Jiang and Bansal, 2019; Lee et al., 2021;
Ye et al., 2021) to counter disconnected reasoning
which operate at the dataset level, using adversarial
training, adding extra annotations or using dataset
rebalancing for training. While these approaches
optimize models to mitigate disconnected reason-
ing (Trivedi et al., 2020), the performance on the
original test set often suffers from a significant de-
crease.
In this paper, we propose a single- sequence
prediction method over a local reasoning graph
(SG ) that integrates a graph structure con-
necting key entities in each context passage to
relevant subsequent passages for each question.
Different from the prior works, our method not
only mitigates the disconnected reasoning issue
but also maintains robust performance on the orig-
inal dataset. Intuitively, for each multi-hop ques-
tion, our method leverages the structural relation-
ship between different passages to learn structured
representations through a graph neural network
(GNN) (Hamilton et al., 2017; Kipf and Welling,
2017). The structured representations are fused
to bias the generative model toward predicting
a faithful, connected reasoning path which im-
proves answer predictions. Our experiments on
theH -QA dataset (Yang et al., 2018) show
clear improvements in exact-match(EM)/F1 scores
compared to generative baselines in the distractor
setting while minimizing disconnected reasoning
quantified by the DRscore (Trivedi et al., 2020).
We also achieve the state-of-the-art performance
on the M -Answerable test dataset (Trivedi
et al., 2022a) with a 17-point improvement in an-
swer F1 over the current best-performing model in
the end-to-end (E2E) category.
To summarize, our contributions are as follows:
•We propose an interpretable single- sequence pre-
diction approach over local reasoning graph s,
SG , to bias the model representations
•SG achieves notable performance im-
provements on two multi-hop QA benchmarks,
H -QA andM (SOTA), with only
a minimal increase in the model size.
•SG reduces disconnected reasoning as
measured by DRscore while maintainingstrong performance gains on the original dataset.
2 Preliminaries
Problem Setup: In a multi-hop QA task, each
QA pair in a labeled dataset Dis given along with
a set of Npassages, P={p, p, ..., p},i.e.,
(q, a,P)∈ D, where a passage has its title and
content p= (t, c). The task is to learn a model
parameterized θto generate an answer string afor
the given question qandP. In this paper, we focus
on the distractor setting, where Pis given for
each question and contains mdistractors that are
not useful to the answer prediction. Thus, this task
requires a model to reason over multiple hops of the
remaining N−mrelevant passages. In addition to
predicting the final answer a, we also aim to train
a model to predict a reasoning path Rof important
elements ( e.g., relevant passage titles, supporting
facts in a passage) that lead to the final answer.
Multi-hop QA as Single Sequence Generation:
Recent generative question answering (QA) ap-
proaches (e.g., FD(Izacard and Grave, 2021),
P-FD(Yavuz et al., 2022)) utilize an encoder-
decoder model as the backbone to generate answers
in a single text sequence. In particular, FD is one
of the popular formulations. Specifically, for each
passage p= (t, c)∈ Pof a question q,FDen-
codes a combined sequence of the question, the pas-
sage title and contents into an embedding. These
embeddings for all passages are concatenated as in-
puts to the decoder for generating the final answer.
P-FDbuilds upon this by explicitly model-
ing a reasoning path as part of the generation output
in addition to the answer. Specifically, special in-
dex tokens [f]are added to demarcate all sentences
in each passage context. The sentences supporting
the prediction of a final answer are considered facts.
The decoder is then trained to generate the reason-
ing path Ras a linearized sequence consisting of
the passage titles and the index tokens of facts used
within those passages to obtain the final answer.
Figure 1 shows an example of a reasoning path.
Disconnected Reasoning in P-FD:Since
the model predictions now include the reasoning
path, we can analyze which facts in the passage
are utilized by the model to determine the next
passage to hop to and arrive at the final answer.
For a perfectly faithful model, all predictions with
correct answers should have correctly identified
passages and facts. However, due to the presence11467of shortcuts in the datasets as well as the model’s
predicted reasoning path not being faithful, we ob-
serve model predictions containing correct final an-
swers but incorrect identification of passage titles
or facts. This unfaithful prediction issue is referred
to as disconnected reasoning (Trivedi et al., 2020).
Different from P-FD, we use the presence of a
local graph structure between different passages in
the context to bias the representations of the model
and help alleviate this problem.
3 Method
In this section, we describe our proposed method
for solving disconnected reasoning for multi-hop
QA in the distractor setting.
Overview: Our method first constructs a local
graph over passage contexts for each question
(§3.1), and integrates the graph information with
the key entities to improve the generation of reason-
ing paths (§3.2). Different from prior works that
encode all the passages independently, we connect
the passages through the key pivot entities into a
local graph for a question, which allows us to en-
code structural representations across passages by
a graph neural network. These graph structured rep-
resentations are then fused with the contextualized
text representations from a text encoder, guiding
the model to leverage structural information to al-
leviate disconnected reasoning over passages.
3.1 Graph Construction
In contrast to the full-wiki setting where a model
must retrieve relevant passages from Wikipedia or
a large corpus, the distractor setting provides the
model with a list of Npassages Pconsisting of
N−mrelevant passages and mdistractors for
each question q. Conventionally, these passages
are collected from Wikipedia, as Wikipedia re-
mains one of the largest faithful knowledge sources
available for public usage. Even for text passages
out of Wikipedia, there are existing out-of-box en-
tity linkers (e.g., SLING (Ringgaard et al., 2017),
BLINK (Wu et al., 2020)) that can identify key en-
tities from texts and link them to their Wikipedia
pages. As a result, each provided passage may con-
tain pivot entities with hyperlinks connecting to
their corresponding Wikipedia pages. We exploit
such entity hyperlinks to construct a local directed
graphG= (N,L)containing two types of nodes
(i.e., entities and passage titles) and links between
these nodes. Specifically, for each pivot entity eina passage p, we create a link from eto the title t
of another passage p(denoted as l) whenever
the entity span epoints to a Wikipedia article that
contains the passage p.
For example, an entity span “David Noughton”
appears in the passage context: “An American
Werewolf in London is a 1981 horror comedy film
starring David Noughton, Jenny Agutter. ... ” This
entity would be connected to a passage with the
title of “David Walsh Noughton” , forming the
link (David Noughton[Entity] →David Walsh
Noughton[Passage]). If there are multiple passages
with the title “David Walsh Noughton” among the
Npassages, the entity span would be connected to
all of them with distinct links. Figure 1 shows an
example of an entity-passage graph.
3.2 Entity-to-Passage Fusion
Next, we describe how we encode such a local
directed graph into vector representations for all
nodes and fuse these node representations with
the contextualized text representations of the corre-
sponding entities from the language model.
We utilize the same model as P-FDwith
a pre-trained T5 model as our backbone architec-
ture. The input for this method consists of the N
sequences, where each sequence is a concatenation
of the question q, the title and contents of a passage
pfrom the collection p∈ Ptogether with their
indicator tokens, denoted as Sbelow:
S:= [Question ]q[Title ]t[Content ]c(1)
Given the T5’s encoder of Mtransformer layers,
we first encode Sthrough the first Llayers to ob-
tain the intermediate hidden representations Zin
Eq. (2), which capture the shallow contextualized
information of the input sequence.
Z=TextEncoder (S, L) (2)
We utilize these shallow representations to ini-
tialize the node embeddings for a graph neural
network. Specifically, we extract the represen-
tations of the entity spans or passage title spans
(i.e., nodes in the graph G) from Zaccording to
their span positions [a, b]inS. Next, for a text
spanSrepresenting either an entity or a title
inS, we average the extracted representations of
the text span to obtain an initial node embedding,
i.e.,n=avg(Z). Finally, we stack the initial
embeddings for all nodes denoted as Nand apply11468
a graph neural network (GNN) to further encode
the structural embeddings on the graph G:
Z=GraphEncoder (N,G) (3)
As we record the text span position [a, b]for each
node in G, we can leverage the node embeddings
Zto construct a new structured representation
Z(with the same size as Z) for each sequence
Swhere we fill in the node embeddings from Z
to their corresponding text span positions [a, b]in
Sand fill in 0to the other non-span positions.
Finally, we fuse the contextualized text represen-
tations Zfrom the text encoder and the structured
node representations Zby an aggregation opera-
tor⊕, and pass them to the remaining layers of the
text encoder to obtained the fused representations
Sfor each input sequence S:
S=TextEncoder (Z⊕Z, M−L)(4)
In this work, the aggregation operator used is a
simple addition. Complex aggregation mechanismssuch as learning a weighted combination of the
representations can be explored in future work.
We concatenate the fused representations S
from all of the Ncontext sequences to form
S= [S;S···;S]. Subsequently, Sis
passed as inputs to the T5 decoder that esti-
mates the conditional probability P(R|S)of
predicting a reasoning path R. Depending on
the annotations in different datasets, a reason-
ing path Rcan take various formats. For exam-
ple, the reasoning path takes the form “ R:=
[title ]t[facts ]f[answer ]a” for H -QA
and “ R:= [ title ]t[intermediate_answer ]
ans[answer ]a” for M . We also inves-
tigate variants of reasoning paths for M in
our experiments. As we can construct ground-truth
reasoning paths Rduring training, the model is
optimized using a cross-entropy loss between the
conditional probability P(R|S)andR.114694 Experimental Setting
In this section, we elaborate on the datasets, the
baseline models and the variants of SG
we consider for our experiment settings. We con-
sider two multi-hop QA datasets, H -QA and
M . Since SG is primarily focused
only on improving the efficacy of encoding, we con-
sider only the distractor setting for both datasets.
Table 4 shows the standard train/dev/test statistics.
H -QA :The final answer to each question
in the distractor setting is extracted from 10 pas-
sages. The dataset includes two main types of ques-
tions: bridge (80%) and comparison (20%). Bridge
questions often require identifying a bridge entity
in the first passage to correctly hop to the second
passage that contains the answer, while compari-
son questions do not have this requirement. Each
question is also provided with annotations of 2 sup-
porting passages (2-hop) and up to 5 corresponding
relevant sentences as their supporting facts.
M :M has questions that range
in difficulty from 2 to 4-hops and six types of rea-
soning chains. M uses a stringent filtering
process as well as a bottom-up technique to itera-
tively combine single-hop questions from several
datasets into a k-hop benchmark that is more diffi-
cult than each individual dataset and significantly
less susceptible to the disconnected-reasoning prob-
lem. Unlike H -QA ,M does not
provide annotations of relevant sentences but pro-
vides supporting passage titles, question decompo-
sition(decomposition of a multi-hop question into
simpler 1-hop sub-questions) and also intermedi-
ate answers to the decomposed questions. Given
this variety, we use the following reasoning path
variants to train the model to generate:
• DA: Question decomposition and final answer
• SA: Supporting titles and final answer
•SIA: Supporting titles, intermediate answers
and final answer
•DSIA: Question decomposition, supporting ti-
tles, intermediate answers and final answer
Table 6 shows an example of different reasoning
paths. While the last variant (predicting every de-
composition/intermediate answer or support title)
is more interpretable, it encounters the challenge
of producing a long sequence. SIA is our best-
performing reasoning path variant which is used
for all of our results and analysis.4.1 Models in Comparison
Our main baselines are generative approaches to
multi-hop QA that include and build upon the FD
approach. For all of the models, we use the pre-
trained T5 encoder-decoder as the backbone and
consider two sizes—base and large variants.
•FD: Model generation includes only the final
answer.
•P-FD: Model generation includes the rea-
soning path as well as the final answer.
•SG : Model that utilizes a fusion of rep-
resentations from the language model and the
Graph Neural Network. Similar to P-FD,
we train the model to generate the reasoning
path in addition to the final answer.
4.2 Evaluation Metrics
For both H -QA andM , we use the
standard quantitative metrics of exact-match and
F1 scores to evaluate the quality of predicted an-
swers. For models that predict the reasoning path
in addition to the final answer, we can quantify how
accurately they can identify the supporting facts (or
supporting titles for M ) using the Support-
EM and Support-F1 scores Yang et al. (2018).
To quantify the level of disconnected reasoning,
we compute dire F1 scores on the answer spans
(Answer ), supporting paragraphs ( Supp), support-
ing sentences ( Supp), joint metrics ( Ans+Supp,
Ans+Supp) of the Dire H -QA subset.
4.3 Implementation details
We train all models using an effective batch size of
64. We use an initial learning rate of 1e-4, a lin-
ear rate scheduler, a warmup of 2,000 steps (1,000
steps for M ), and finetune the models for 10
epochs. For SG , we use GAT (Veli ˇckovi ´c
et al., 2017) for our GNN layers. A maximum
sequence length of 256 tokens is used for construct-
ing the input. All experiments have been conducted
on a machine with either 4 ×40G A100 GPUs or
4×80G A100 GPUs. A detailed list of hyperparam-
eters can be found in Appendix E.
5 Results and Analysis
In this section, we present the main results of the
baselines and our proposed approach on H -
QAandM (§5.1), and then perform fine-
grained analysis thereafter.11470
5.1 Multi-hop Performance
The quantitative performance of the models in
terms of exact-match and F1 scores for both the
final answer and the predicted supports are shown
in Table 1. We find that across both model sizes
(B andL ), explicitly predicting the rea-
soning path helps P-FDin improving the an-
swer EM and F1 scores over the vanilla FDap-
proach. By biasing the model with graph represen-
tations, SG outperforms the baselines on
both the H -QA and the M datasets.
SG achieves a 2-point improvement in
both answer and support EM when considering the
base variant and 1.5 point improvement for the
large variant on the dev set of H -QA.
On the more challenging M dataset, we
observe stronger results from SG where
it records up to a 4-point improvement in both an-
swer and support scores across both model sizes
on the dev set. On the test set (in Table 8 of the
appendix), the current best performing approach
is a two stage RBERT/L -Large
model, Select-Answer, where the passage selec-
tion/ranking and answer generation stage is op-
timized separately using different models. S-
G -Large achieves state-of-the-art numbers
on Answer-F1 with a 5-point improvement over
the Select-Answer modeleven though it is a sin-
gle stage approach. When comparing with the top
score in the end-to-end (E2E) category which all of
our models belong to, SG gets a massive
17-point improvement in answer F1 and a 9-point
improvement in support F1 establishing the efficacy
of our approach. It should also be noted that all of
the current models on the leaderboard are discrim-
inative approaches with an encoder-only model(L -Large) encoding a very long con-
text length of 4,096, while all of our models are
generative in nature with a much smaller context
length of 256. M is also designed to be
more challenging than H -QA and explicitly
tackles the issue of disconnected reasoning during
dataset curation, making it harder for the model to
take shortcuts and cheat. The larger performance
improvements of SG onM com-
pared to H -QA showcases the advantage
of our proposed approach, providing promising
results for further research in this direction to miti-
gate disconnected reasoning.
5.2 Faithfulness of Reasoning Paths
We follow Yavuz et al. (2022) to perform analysis
at the passage and individual fact level to deter-
mine how faithful the generated reasoning paths
are across different models.
Predicted Answer in Predicted Titles/Support:
how often are the predicted answers found in one
of the predicted passages or in the predicted sup-
porting facts.
Gold Answer in Predicted Titles/Support: how
often are the gold answers found in one of the
predicted passages or in the predicted supporting
facts .
Predicted Answer in Gold Titles/Support: how
often are the predicted answers found in one of the
gold passages or in the gold supporting facts.
Figure 3 shows the described faithfulness metric
scores on H -QA . We find that SG11471
is more faithful with a 0.5-1.5% improvement over
P-FD across all considered categories.
5.3 Performance vs Number of hops
We break down the final answer exact-match and F1
scores based on how many supporting facts(or titles
for Musique) are required to answer the question.
Figure 5 shows this performance breakdown for
H -QA and Figure 6 shows it for M .
We observe that SG improves over P-
FDin the cases where the support includes two
or three supporting facts (or titles), but the answer
EM takes a hit when the number of supporting
facts(titles) ≥4. We notice that SG has a
higher support EM over P-FDin such cases
where shortcuts may exist in the dataset and P-
FDrelies on those shortcuts to get a higher answer
EM but a lower support EM. Section §5.4 quantifies
the extent to which P-FDsuffers from discon-
nected reasoning as compared to SG .
5.4 Probing Disconnected Reasoning
H -QA suffers from information leakage in
the form of reasoning shortcuts leading to discon-
nected reasoning . This affects the generalizationcapability of such models and inflates the perfor-
mance on the evaluation sets. Table 4 shows some
qualitative examples of disconnected reasoning in
P-FD that are avoided by SG
Trivedi et al. (2020) construct a probe of
H -QA by splitting the two supporting para-
graphs for the original question across two ques-
tions. If the model can answer modified questions
correctly without the complete context, it suggests
that the model uses disconnected reasoning for the
original question. By measuring the performance
of a model on such a dataset, we arrive at the DR
score with a higher value implying more discon-
nected reasoning. Table 2 shows the DRscores
for the various models. We see that SG re-
sorts to lower disconnected reasoning compared to
P-FDwhile maintaining strong performance
gains on the original evaluation set.
5.5 Comparison with PathFiD+
Yavuz et al. (2022) extend P-FDand intro-
duce P-FD+ to improve the cross-passage
interactions before feeding to the FiD decoder and
show an improvement of 7 EM points and achieve
state-of-the-art results on H -QA distractor11472
dataset. However, we find the following limitations
of the approach:
Hop-assumption :P-FD+ adds pairs of
contexts as input to the FID encoder, which as-
sumes a fixed number of hops (in case of H -
QA, two) and doubles the input sequence length,
leading to increased training time.
Multi-step : To efficiently encode pairs of pas-
sages (instead of inefficient/parenleftbig/parenrightbig
passages, where
N is the total number of passages), P-FD+
also needs to run the vanilla P-FDor train
another model to choose the first relevant context
P∗to jump to and then construct pairs ( P∗,P).
This makes it inefficient and not scalable to ques-
tions with higher hops or complex datasets like
M
In contrast, our approach does not make any as-
sumptions about the number of hops and is scalable.
It produces output in a single shot without requiring
multiple steps or increased sequence length. While
P-FD+ may achieve stronger performance in
2-hop H -QA , our proposed method is more
general, efficient and scalable, making it a morepractical solution for real-world applications and
also easily extendable to open-domain setting.
6 Related Works
Multihop question answering requires a model to
perform reasoning over multiple pieces of infor-
mation, utilizing multiple sources and inferring
relationships between them to provide a correct an-
swer to a given question. There have been various
approaches and datasets proposed for training QA
systems, such as HotpotQA (Yang et al., 2018),
IIRC(Ferguson et al., 2020) and Musique (Trivedi
et al., 2022b).
In the H -QA full-wiki setting, the task is
to find relevant facts from all Wikipedia articles and
then use them to complete the multi-hop QA task.
Retrieval models play an important role in this set-
ting, such as DPR (Karpukhin et al., 2020), which
focuses on retrieving relevant information in the
semantic space. Other methods, such as Entities-
centric (Das et al., 2019), and Golden Retriever (Qi
et al., 2019), use entities mentioned or reformu-
lated in query keywords to retrieve the next hop11473document. Additionally, PathRetriever (Asai et al.,
2020) and HopRetriever (Li et al., 2020) use RNN
to select documents to form a paragraph-level rea-
soning path iteratively. The above methods mainly
focus on the open-domain setting (full-wiki) and
improve the retriever’s performance and do not ad-
dress the disconnected reasoning problem.
Multiple techniques (Jiang and Bansal, 2019;
Lee et al., 2021; Ye et al., 2021) to counter discon-
nected reasoning operate at the dataset level, using
adversarial training, adding extra annotations or
using dataset augmentations to get a balanced train
set and prevent the model from cheating.
We highlight differences between our approach
and other related works on H -QA -distractor
and other works that combine language models
with graphs below :
Generative approaches : Our generative-FiD ap-
proach differs from others using KG/GNN (Ju et al.,
2022; Yu et al., 2022) as we use an entity-passage
graph with Wikipedia hyperlinks. Also, our focus
is primarily on the distractor setting of multi-hop
QA, while other baselines (Ju et al., 2022; Yu et al.,
2022) are either single-hop or improving retrieval
in open-domain setting
Pipeline vs single-stage : Other baselines (Tu
et al., 2019; Chen et al., 2019; Qiu et al., 2019;
Wang et al., 2021; Li et al., 2023) use a pipeline
approach with distinct encoder models in the rea-
soning process, while we use a single-stage, one-
shot prediction process without assumptions on the
number of hops.
Graph construction : Other methods (Tu et al.,
2019; Qiu et al., 2019) select relevant passages
heuristically from among distractors to construct
graphs. However, we construct our entity-passage
graph on all passages (including distractors) and
fuse the representations in the encoder.
While a direct comparison with pipeline-based
approaches is not possible or fair, we provide com-
parisons in Table 3 for completeness.7 Conclusion
In this paper, we propose SG , an approach
that utilizes the structured relationship between
passages in the context of multi-hop questions to
reduce disconnected reasoning. We construct a
localized entity-passage graph using Wikipedia hy-
perlinks, encode it using a GNN, and fuse the
structured representations with the text encoder
for predicting a reasoning path. Our approach re-
sults in strong performance gains in terms of both
answer and support EM/F1 on H -QA and
reduces disconnected reasoning measured using
DRscore. We also obtain state-of-the-art perfor-
mance on the more challenging M bench-
mark with a 17-point improvement in answer F1
over the current best end-to-end(E2E) model. Ex-
perimenting with sophisticated methods of encod-
ing the graph structure and fusing the text and graph
representations can be explored in future work.
Limitations
We identify the following limitations of our work:
Longer Output Sequences While outputting the
reasoning path as a single short sequence makes the
model more interpretable, it increases the challenge
of producing a long /coherent sequence when the
question is complex (more than 3 hops). Produc-
ing a longer sequence also increases the inference
time. Simplifying this output while not sacrificing
interpretability is a good future direction
Entity Identification Our method needs
wikipedia outlinks or a entity linker to construct a
localized graph for every question. Generalizing
this step by pretraining the model to do entity
linking (Févry et al., 2020; Sun et al., 2021; Verga
et al., 2020) might eliminate the need to use an
external module.
References114741147511476
A Breakdown of Performance by Question Type - H -QA
Model Bridge Comparison
FD-Base 60.8 65.97
P-FD-Base 61.19 65.37
SG -Base 63.6 66.51
P-FD-Large 63.72 71.68
SG -Large 65.21 71.69
B Reasoning Path variants in M
The different reasoning path variants that can be constructed based on ground truth can be found in
Table 6. Results of training baselines on these different variants can be found in Table 7
C Performance by Number of Hops - Graphs
We hypothesize that the answer F1 of SG on questions with ≥4hops gets impacted due to
presence of shortcuts since the support F1 score is higher than P-FD.
D Comparison of Musique-Answerable test F1 scores
Table 8 shows the comparison of our models with the current best performing ones on the M -
Answerable test set leaderboard. Our End-to-End single stage model SG -large trained to output
title + intermediate answers (SIA) outperforms the Longformer-Large(Beltagy et al., 2020) End-to-End
model by 17 points in answer F1 and by 9-points in support F1. Furthermore, we also outperform
the current state-of-the-art SA model which is a two stage model (Roberta Large(Liu et al., 2019) +
Longformer Large) by 5 points on Answer F1 and 3 points on Support F1.11477Model Answer-EM Answer-F1 Support-EM Support-F1
SA 32.02 41.76 47.04 76.23
DA* 31.61 41.4 XX XX
SIA 34.71 44.93 57.3 80.18
DSIA 33.35 43.08 53.5 78.7911478
E Hyperparameter Settings
Tables 9, 10, 11 detail the hyperparameters we use for FD,P-FDandSG forH -QA
and M .
The 2-layer GNN module is 17M parameters for the large model and 9.5M for the base, accounting for
only upto 4% increase in model parameters.11479ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Left blank.
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Left blank.
/squareA4. Have you used AI writing assistants when working on this paper?
General paraphrasing of content
B/squareDid you use or create scientiﬁc artifacts?
4
/squareB1. Did you cite the creators of artifacts you used?
4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
511480/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix E
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Left blank.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.11481