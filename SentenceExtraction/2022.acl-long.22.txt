
Maryam Davoodi
Purdue University
mdavoodi@purdue.eduEric Waltenburg
Purdue University
ewaltenb@purdue.eduDan Goldwasser
Purdue University
dgoldwas@purdue.edu
Abstract
Decisions on state-level policies have a deep
effect on many aspects of our everyday life,
such as health-care and education access.
However, there is little understanding of how
these policies and decisions are being formed
in the legislative process. We take a data-
driven approach by decoding the impact of leg-
islation on relevant stakeholders (e.g., teach-
ers in education bills) to understand legisla-
tors’ decision-making process and votes. We
build a new dataset for multiple US states that
interconnects multiple sources of data includ-
ing bills, stakeholders, legislators, and money
donors. Next, we develop a textual graph-
based model to embed and analyze state bills.
Our model predicts winners/losers of bills and
then utilizes them to better determine the leg-
islative body’s vote breakdown according to
demographic/ideological criteria, e.g., gender.
1 Introduction
State-level legislation is the cornerstone of national
policies and has long-lasting effects on residents
of US states. Thus, decoding the processes that
shape state bills is crucial yet involved. State leg-
islatures vote on 23 times more bills than Federal
legislatures, exceeding 120K bills per year (King,
2019). In addition, these state bills cover a broader
range of local problems as each state possesses
lawmaking power effective within its boundaries.
E.g., the State of Washington Health Care Com-
mittee addresses health service issues including li-
censing and regulation of health care facilities and
providers. Moreover, it regulates pharmacies, phar-
maceutical drugs, state public health programs, and
private/public insurance markets (House, 2021).
We argue that recent NLP architectures can
provide new insights into the state-level legislative
efforts. In particular, contextualized graph and text
embedding can better represent policies within
and across states via a shared political context.
However, most of the prior efforts are focusedon analyzing congressional bills with traditional
techniques, e.g., (Gerrish and Blei, 2011, 2012).
A few state-level studies (Eidelman et al., 2018;
Davoodi et al., 2020) took great steps in predicting
the progression of state bills towards a vote on
the ﬂoor and the breakdown of votes based on
demographic metrics (e.g., gender). But their main
downside is they evaluate policies in a limited
context and do not capture cross-state patterns.Figure 1: A health bill leading to voting cleavage
based on the party metric primarily due to its speciﬁc
losers (red) and winners (green).
Winners-Losers analysis . In this work, we take
a new data-driven approach to analyzing state leg-
islation. Our key insight is that each state bill
inevitably produces some winners andlosers to
provide practical solutions to speciﬁc in-state and
local problems. Thus, we argue that it is important
to examine state bills in the larger context of their
impact on different population segments as well as
commercial and professional stakeholders. To help
clarify this idea, consider the example in Figure 1.
This state bill makes it easier for patients ( winners)
to take legal actions against healthcare providers
(losers) . This analysis of winners and losers (WLs)
can foster transparency in legislative efforts in each
state, while interconnecting different states through
common stakeholders and revealing cross-state pat-
terns. In addition, the context of WLs can enable
a new category of NLP models for predicting the
roll-call behavior of legislators.
Downstream bill classiﬁcation tasks . For
instance, the political science community sees
tremendous value in predicting voting cleavages,270
based on the ideological and demographic identi-
ties of legislators (Section 2). Each of such metrics
(e.g., party, gender, district, ideology) splits legis-
lators into groups. Measuring lack of consensus
within and across these groups, which has political
and social beneﬁts, can be done using two classiﬁ-
cation tasks (Section 5): For a given metric, we say
a bill is competitive (Figure 2) if the majority vote
of legislators from a group (e.g., Democrat, male,
urban, liberal) is different from that of the opposite
group (e.g., Republican, female, rural, conserva-
tive). Similarly, a bill is inverse-competitive if there
is a tie in votes of members of the same group (e.g.,
liberals). For instance, the health bill in Figure 1 re-
sulted in a party competitive vote. Another example
is a state bill on abortion that “requires... physician
performing an abortion to admitting privileges at a
hospital in the county” resulted in a gender compet-
itive vote. We show the context of winners/losers
of these bills could hint at such cleavages prior to
voting (Sections 4, 6).
Framework overview. To achieve this goal,
we address multiple NLP challenges in our pro-
posed framework: (1) Data: The legislative pro-
cess in US states does not track the stakeholders
of bills and the impact of bills on them. Thus,
we design a reliable crowd-sourcing pipeline to
extract and analyze winners and losers of state
bills from their text and form a new annotated
dataset. (2) Modeling: To automate the WL analy-
sis, next, we provide a nationwide graph abstrac-
tion to model the state legislative process, as well
as a joint text and graph embedding architecture
for predicting winners and losers. Our model cap-
tures the interplay of different entities, e.g., bills,
stakeholders, legislators, and money donors, while
maintaining dependencies between their textual at-
tributes. We leverage RGCN (Schlichtkrull et al.,
2018), a relational graph convolutional network,
to represent diverse relations. We also adopt the
RoBERTa transformer (Liu et al., 2019) after per-
forming domain-adaptive pretraining on political
texts using the MLM (Masked Language Model)
task. (3) Application: Finally, we showcase the
ability of our WL analysis and prediction model indecoding the voting behavior of state legislators. In
summary, we make three technical contributions :
•We provide the ﬁrst deﬁnition and realization
of winners/losers analysis for state bills using
the latest NLP advances. (Sections 2, 3, 4).
•We developed a new joint graph and text
embedding model that both predicts win-
ners/losers of bills and legislators’ votes. In
particular, it incorporates the winners/losers
inference into the vote prediction task, to eval-
uate bills in a broader context (Section 5).
•We operationalized the winners/losers analy-
sis for several legislative topics (e.g., health)
and created a new dataset. The extensive eval-
uation shows our approach delivers a higher
F1, than existing models (Sections 3, 6)
2 Related Works
Our work is inspired by some promising studies:
Roll-call classiﬁcation. Eidelman et al. 2018
associate the bill’s text with partisan information
of its sponsors to predict the likelihood of a mem-
ber of the U.S. Congress voting in support of a
bill. Similarly, Gerrish and Blei 2011 embed the
combined topic and text of Congress bills in the
ideological space and develop ideal point models
for inferring votes. Peng et al., 2016; Kornilova
et al., 2018; Kraft et al., 2016; Patil et al., 2019;
Karimi et al., 2019; Pujari and Goldwasser, 2021
augment this model using data on social networks,
thus generating better embeddings.
Bill text classiﬁcation. Instead of leveraging
bill text in models to describe the behavior of each
legislator, Yano et al. 2012 include the bill’s text in
a model that directly predicts whether a bill comes
out from a standing committee. Particularly, they
develop features based on the urgency of the prob-
lem being solved by the bill and the set of legisla-
tors co-sponsoring the bill. Eidelman et al. 2018
conduct a similar study on US states.
Winners-losers analysis. Analyzing the impact
of bills on its stakeholders is a well-studied topic in
the political science literature. Gamm and Kousser,
2010 reveal state legislators are more likely to write
bills aimed at a particular local stakeholder when
the legislative body is dominated by one party. Sim-
ilarly, Bagashka and Clark, 2016 show state legisla-
tors are motivated to introduce particularistic bills
designed to help a speciﬁc geographical area within271their district. Pennock, 1979 analyzes legislation
based on its generalized and particularized impact
on different interest groups. By leveraging recent
NLP advances (e.g., contextualized language mod-
els, graph embedding, crowdsourcing), our work
extends these studies and provides the ﬁrst auto-
mated framework for the stakeholders analysis on
state bills.
Voting cleavages. Research has covered multi-
ple ways that the demographic background of leg-
islators can affect roll-call voting. Frederick 2010
demonstrates gender affects the roll-call vote in the
Senate by changing the inﬂuence of partisanship
for GOP women. Broach 1972 describes that urban-
rural voting cleavages happen in less partisan states
and on bills that separate urban and rural interests.
Similar to us, Davoodi et al. 2020 build a textual
graph to predict such cleavages. While our focus is
on a different problem, stakeholders analysis, we
outperform this prior study by representing bills in
a broader context containing their stakeholders.
Graph embedding in NLP. Our work uses
Graph convolutional networks (GCNs), which have
been applied to various NLP tasks, e.g., Semantic
role labeling (SRL) (Marcheggiani and Titov, 2017)
and relation classiﬁcation in clinical narratives (Li
et al., 2018). In these tasks, GCNs encode the
syntactic structure of sentences. Similarly, Deffer-
rard et al., 2016; Peng et al., 2018; Henaff et al.,
2015 use graph neural networks (GNNs) to repre-
sent a network of documents based on their refer-
ences. Similar to our work but for a different prob-
lem and objective, Sawhney et al., 2020 analyze
speech-level stance of members of the parliament,
by performing node classiﬁcation on graph atten-
tion networks (GATs), and Pujari and Goldwasser,
2021 analyze social media content generated by
politicians using a graph transformer model.
3 Modeling
We ﬁrst provide an overview of key players in the
state-level legislative process. Then, we model
them using an efﬁcient text-based graph abstrac-
tion (Figure 3), which will enable us to embed and
evaluate state policies in a broad context and per-
form the stakeholder and roll-call analysis on them.
3.1 Players in State Legislative Process
Our model, unlike prior works, fully captures the
interplay of main players in the lawmaking process:
1. Legislators . A state legislature typically con-
sists of two “chambers”: the House and the Sen-
ate. The legislative process starts with legislators
sponsoring a bill in a chamber. The idea of a bill
can come from different sources. Next, the bill
goes through multiple roll-call votes in the origin
chamber, where it can fail at any stage. It is ﬁrst
referred to the proper committee by the chamber
leader. Committee members, before casting their
votes, may set up a public hearing with the spon-
sors and interested parties. If the bill passes out
of the committee, it reaches the second reading,
where the full chamber debates, amends, and votes
on the bill. If the bill passes by a majority vote,
it is scheduled for the third reading and ﬁnal vote.
A bill must go through a similar procedure in the
other chamber before it is acted on by the governor.
2. Contributors . While legislators navigate
through bills, external contributors inﬂuence their
decisions. Individual and corporate money donors
aim at developing changes in the outcome and
theme of bills starting from the election times. Lob-
byists launch campaigns to persuade legislators
towards certain policies. Such efforts inevitably
lead to new bills or amendments to existing laws.
3. Stakeholders . A state bill cannot beneﬁt
everyone and it produces beneﬁcial or detrimental
effects on its stakeholders. Identifying winners and
losers of a bill from its text is crucial, which can
hint at the fate of a bill. Particularly, legislators
do not always write bills themselves. Corporations
and interest groups (e.g., ALEC) sell ﬁll-in-the-
blank bills to legislators. Thus, we can see voting
patterns on bills with the same winners and losers.2723.2 Nationwide, Multi-Relational, and
Heterogeneous Legislative Graph
To model these players and their interactions, we
design a legislative graph with three important prop-
erties (Figure 3). First, since each of the players
(e.g., stakeholders, legislators) has different textual
attributes, our proposed graph supports heteroge-
neous textual nodes. Second, we form a nation-
wide graph to capture cross-state patterns (abla-
tion study in Appendix A.2) by building common
entities (e.g., stakeholders in Section 4). Finally,
our abstraction supports multiple relations between
each pair of entities (e.g., legislators voting and
sponsoring a bill). With this overview, we present
the nodes and relations that we will realize based
on the real data:
Node types . The nodes in the legislative graph
contain a rich set of textual features: (1) Bill nodes
embed title, abstract, and body of state bills.
(2) Stakeholder nodes come with short texts on
political interests and constituent entities of stake-
holders of policies in bills (will be detailed shortly).
(3) Legislator nodes contain diverse textual infor-
mation on legislators, e.g., their biography, political
interests, committee assignments, and demographic
proﬁle (e.g., party, gender, ideology, and district).
(4) Contributors nodes have text-based attributes
on money donors covering their speciﬁc/general
business interests, party, and their type (individual
or non-individual).
Relation types . Based on the legislative process,
legislator and bill nodes participate in Bill Sponsor-
ship,’No‘ Vote , and ‘Yes’ Vote relations in the graph
(See Appendix A.4 for handling abstain votes.) A
stakeholder node forms Winner ,Loser , orNeutral
relations with a bill node, which we will extract
it based on the bill text. Similarly, we form two
types of relations between contributors and legisla-
tors: Positive Donation realized based on the real
donation data, and Negative Donation , which we
infer when a contributor shows a lack of interest in
a demographic of legislators (e.g., never donates to
women). We sample and connect such legislators
and the contributor via a negative relation.
4 Data-Driven Stakeholders Analysis
Next, we describe how we build up the legisla-
tive graph, by collecting data on legislators, bills,
and contributors. US states do not record the im-
pact of bills on relevant stakeholders. Thus, we
explain how to derive stakeholders from bill nodes,perform winners-losers analysis on them, and in-
terconnect different US states by forming common
stakeholder nodes. We highlight how our analy-
sis can be used (1) to inform the public about the
dynamic and direction of state policies, and (2) to
determine legislators’ roll-call behavior with differ-
ent demographic and ideological proﬁles.
4.1 Data Collection & Bootstrapping Graph
Bills. From the LegiScan website (LegiScan,
2019), we collected data on bills introduced in In-
diana, Oregon, and Wisconsin from 2011 through
2018 (details in Appendix 7). We developed a
crawler that uses the LegiScan API to fetch leg-
islative information on every bill, including: (1)
bill metadata, e.g., the bill type, title, description,
sponsors, and links to its texts; (2) vote metadata,
e.g., legislator’s roll-call vote; and (3) legislator
metadata, e.g., party and district info. Then, our
crawler converts bill texts in PDF format to text
ﬁles. In total, we collected 35k bills and sampled
58% of them that had both roll-call data and full
texts. Our focus is on the 2nd/3rd reading, in which
the full chambers vote, so we selected 32% of the
bills for building the legislative graph (Table 1). In
LegiScan, each bill is associated with a main topic
(e.g., health), used for referral to a proper commit-
tee. For the four most frequent topics (Table 2), we
will deﬁne a group of generic stakeholders for the
winners-losers analysis.
Legislators. Our crawler also used Ballotpedia
(Ballotpedia, 2019) to collect text information on
each legislator’s biography, political interests, and
committee assignments. Also, it consumed other
publicly available datasets to identify a legislator’s
demographic proﬁle, e.g., ideology, gender, and dis-
trict. The ideology scores for legislators (Shor and
McCarty, 2011) were grouped into conservatives,
moderates, and liberals. The district identiﬁer was
combined with GIS census data (Census, 2019) to
identify each legislator as representing an urban or273
rural district. Table 3 summarizes the distribution
of legislators’ demographic proﬁle.
Contributors : FollowTheMoney (FollowThe-
Money, 2019) records donations to state legislators
and candidates. Our crawler collected the infor-
mation of donors for each legislator in our dataset
(See Table 1). This includes multiple textual at-
tributes for each contributor: type (individual or
non-individual), general party, and economic and
business information. While the contributor data
can be utilized in more sophisticated ways, we fo-
cused on major contributors by setting a minimum
donation threshold and pruning donors who con-
tributed to a single legislator; We set the fraction
of negative donations (Section 3) to 30% of the
positive ones extracted from the data.
4.2 Stakeholders Extraction & Annotation
For each select bill topic, we (authors) randomly
sampled 10% of bills and carefully analyzed their
texts. We recorded entities discussed in the bill
texts as well as the detrimental or beneﬁcial im-
pact of the suggested policies on them (regardless
of the legislative outcome, i.e., passed in a vote
or not). To interconnect different states and opti-mize the legislative graph, we deduplicated entities
and clustered those whose interests align (e.g., sur-
geons, doctors, dentists, and etc.) into generic ones
(e.g., healthcare providers). Table 4 shows the ﬁ-
nal list of stakeholders for the select topics. With
detailed annotation guidelines, we leveraged Ama-
zon MTurk for labeling 4k bill texts from these
topics (Table 2), where 3-5 workers identiﬁed the
effect of the suggested policies in each bill on the
relevant stakeholders. As will be detailed in Ap-
pendix A.1, we ensured the accuracy of the labeled
data is 90%+.
4.3 Beneﬁts of Winners-Losers Analysis
Based on the outcomes of the previous two steps,
we formed a legislative graph for our target states.
We brieﬂy provide two results from the winners-
losers analysis on the graph to highlight its impor-
tance. First, we show the frequency distribution of
the stakeholders as a winner vs. a loser for each
topic in Table 4, which would inform the public
about the dynamics and directions of state-level
policies. E.g., under the education topic, students
were the largest winners, while educational insti-
tutions were the major losers. For law bills, law
enforcement agencies were the top losers given the
recent nationwide focus on police use of force.
Also, our winners-losers analysis captures the
policy preferences of different ideological and
demographic groups of legislators. For exam-
ple, Democrats are more likely to support legis-
lation beneﬁting teachers, compared to Republi-
cans (GOP). This fact is also reﬂected in our mod-
els predicting voting cleavages in Section 6 (e.g.,
our naive model, WL-Correlation, outperforming
other models in its category). Here, to motivate the
need for building such models, we are interested in274measuring the rate of ’Yes’ votes from each demo-
graphic and ideological group of legislators on bills
of a given topic, where a stakeholder is a loser and
a winner. E.g., on education bills beneﬁting a stake-
holder (e.g., Students) as a winner, we compute,
A =[# of yes votes ]=[total # of votes ]in the GOP
legislators. Similarly, on educations bills, where
this stakeholder is a loser, we calculate B = [# of
yes votes ]=[total # of votes ]for GOP. We then re-
port the difference, A-B, in Table 5, where a large
positive value indicates the stakeholder is being
advantaged by the respective group of legislators.
E.g., we see GOP has signiﬁcantly more Yes votes
when students are winners, compared to Yes votes
when students are losers. By running queries on
the legislative graph containing all players (e.g.,
donors), we were able to see the voting behavior
of GOP could be motivated by major donations to
this party from corporations representing students
(e.g., School Choice).
5 Embedding & Prediction Architecture
The stakeholder analysis based on human data an-
notation is expensive and time-consuming. To au-
tomate the analysis and better leverage its results
in other applications, we build up a contextualized
embedding architecture and deﬁne two classiﬁca-
tion tasks on the legislative graph:
5.1 Classiﬁcation Tasks on Legislative Graph
Task 1: winners-losers prediction. Our ﬁrst task
is to predict the relation between a bill node and
each relevant stakeholder node (based on its topic
in Table 4). Such predicted relations will bring
valuable insights into the bills, while also clarifying
legislators’ roll-call behavior (Section 6). Thus, we
deﬁne the next task to showcase these beneﬁts.
Task 2: bill cleavage and survival prediction.
For a bill, we predict if (1) it shows identiﬁable
voting cleavages and (2) it can advance by get-
ting a pass. We achieve these by predicting and
aggregating roll-call relations (between legislators
and bills) in the graph. In particular, we assign
9 labels to each bill: (1) Competitive labels : For
voting cleavages, we split legislators into groups
based on their demographic and ideological pro-
ﬁles (party, gender, ideology, and the urban/rural
nature of their district as deﬁned in Section 4).
For an attribute (e.g., gender), we say a voting
round is “competitive” if the majority of legislators
from one group (e.g., Women) and the majority ofthe opposite group (e.g., Men) cast different votes
(Figure 2a). (2) Inverse-competitive labels : Simi-
larly, for an attribute (e.g., gender), a voting round
is inverse-competitive if we identify a partial or
complete tie (Appendix A.4) in the vote of legis-
lators of the same group (e.g., Men in Figure 2b).
(3) Survival label : Finally, a bill passes its current
voting round by getting a majority vote.
5.2 Overview of Embedding & Training
At a high-level, we propose a uniﬁed model
to jointly embed and classify both roll-call and
winner-loser relations in the legislative graph (Fig-
ure 4a): (a) We ﬁrst train our model to predict
relations between bill nodes and their stakehold-
ers. One can use the result of this stage for further
analysis of state policies (e.g., Section ). (b) Our
key insight is that knowing winner-loser relations
enhances the embedding of nodes in the legislative
graph. Thus, we conduct inference on bills that lack
such relations (if any) using the pretrained model
from step (a) and add these predicted relations to
the graph. (c) Next, continue training on the up-
dated graph to ﬁne-tune the model for the roll-call
(vote) prediction task. Finally, we aggregate the
predicted votes for the bill cleavages/survival anal-
ysis. In all these steps, our model generates and
jointly optimizes both text and graph embeddings
for each node, and consumes them to classify the
two types of relations. Thanks to jointly optimizing275the tasks over the textual and graph information,
our architecture outperforms existing models (Sec-
tion 6). Hereafter, we detail the layers in our model
using a bottom-up approach:
5.3 Contextualized Text Embedding Layers
The lower half of our model generates a contex-
tual embedding for textual attributes of nodes in
the legislative graph. We leverage the RoBERTa
architecture (Liu et al., 2019). For improved per-
formance, one of our contributions is that we will
pretrain RoBERTa on unlabeled bill texts using the
MLM task (Section 6). In more detail, for each bill
node, we feed three pieces of textual information
to RoBERta: title, abstract, and body. RoBERTa
does not support input sequences longer than 512
tokens. Thus, we take the representation of each
of these components separately (the embedding of
their [CLS] token) and do average pooling to out-
put the ﬁnal representation of the bill. Similarly,
the text embedding of stakeholder ,legislator , and
contributor nodes are the average of the vectors
representing their key textual attributes (Section 4).
5.4 Relational Graph Convolutional Layers
On top of the text embedding layer, we place a
Relational Graph Convolutional Network (RGCN)
to create a graph embedding for each node. The
RGCN uses the text embedding of each node to
initialize its graph representation. In parallel, we
build a feed-forward neural network (FFNN), tak-
ing the text embeddings of nodes to a concatena-
tion layer for our joint text-graph optimization. The
(non-relational) GCN has multiple layers and each
layer performs two operations: propagation and
aggregation . In the propagation, nodes update their
neighbors by sharing their features or hidden states.
In the aggregation, each node adds up the messages
coming to it to update its representation. In GCN,
at layerl+ 1, the hidden representation of node i,
h, with neighbours Nis:
h= X1
cWh!
(1)
GCN uses the same weight matrix in each layer,
W, and normalization factor, c=jNj, for all
relation types in a graph. We choose RGCN as
it uses unique parameters for each relation type,
thus better handling our multi-relational graph. In
RGCN, the embedding of node iin layer (l+ 1) is:h= 
Wh+XX1
cWh!
(2)
A 3-layer RGCN turns out to be sufﬁcient in
our case to capture the 3rd order relations between
contributors and stakeholder nodes.
5.5 Relations Prediction Layers
By combining the outputs of the RGCN and FFNN,
we train a relation classiﬁcation layer by using
the DistMult scoring function (Schlichtkrull et al.,
2018; Yang et al., 2014). For each relation (s;r;d )
being predicted, this layer computes f(s;r;d ) =
eWe.eandeare the joint text and graph
embeddings of the source and destination nodes
andwis a diagonal relational weight matrix. Our
loss function is L=L+L+L en-
abling us to jointly optimize the text and graph em-
beddings as well as the relation prediction. L
is the cross-entropy loss of the relation classiﬁca-
tion;L andL are the L2 regularization
of RGCN’s and FFNN’s weights, optimizing the
graph and text representations, respectively.
6 Experiments
We ﬁrst evaluate the efﬁciency of our legislative
graph abstraction and text+graph embedding model
in the winners-losers prediction. Then, we show the
beneﬁts of our combined inference of stakeholders
and roll-calls in decoding state bills.
6.1 Experimental Setup
Data Split and metric. We split the legisla-
tive graph (Formed in Section 4) based on bill
nodes. We randomly select 20% of the bills
for testing and keep the rest for training and
validation. We study three settings in terms of
the winners-losers (stakeholders) information in
the graph: (a)Unknown winners-losers relations.
(b)Known relations based on our human-labeled
annotation. (c)Predicted: 30% of bills in the train
graph come with such relations and we predict
them for the rest of bills. In Appendix A.3, we will
report the results of state- and time-based splits.
Finally, given our data is highly skewed, we choose
Macro F1 as the main metric over accuracy.
Settings/parameters. We build our joint model
(Figure 4) on top of PyTorch, DGL, and spaCy. We
set the initial embedding dimension in RoBERTa
and RGCN to 1024. The FFNN and RGCN take276the embeddings to a 256-dimensional space. We
also used Adam optimizer, and for each observed
relation (Table 1), we sampled a negative example.
6.2 Baseline Models
We devise robust baselines for both of our tasks:
1. Text-based models. We build a logistic re-
gression (LR) classiﬁer that takes the text embed-
ding of a bill and predicts if it shows a certain
cleavage or passes/fails. A similar classiﬁer takes
the text embeddings of a bill and a stakeholder to
classify their relation. We evaluate three embed-
ding architectures: (a) BoW , where unigram and bi-
gram features (top 5K highest-scoring) are used to
represent textual information. (b) RoBERTa (Liu
et al., 2019). (c) Pretrained RoBERTa that we
adapted its domain by applying MLM on 10K unla-
beled state bills (39K sentences) (Gururangan et al.,
2020). We study two additional variations of these
models (only for winners-losers prediction due to
limited space): Sponsors , where the bill sponsors
are represented using a one-hot vector and con-
catenated to the bill text representation. Roll-Call ,
where we concatenate a vector containing cleav-
age/survival info. of each bill to its text embedding.
2. Graph-based models : We build a re-
lation classiﬁer over edge embeddings, gener-
ated by three widely-used graph models, to pre-
dict roll-call and winner-loser relations (for the
bill cleavages/survival task, we aggregate votes):
(a) DeepWalk (Perozzi et al., 2014) that gener-
ates embeddings for nodes and edges by run-
ning Skip-Gram on random walks on the graph.
(b) GCN (Kipf and Welling, 2016) is a basic 3-
layer GCN model with random node features in its
ﬁrst layer. (c) RGCN (Schlichtkrull et al., 2018)
is the relational GCN handling different relation
types in the legislative graph.
3. Naive models . We evaluate three naive classi-
ﬁers: (a) Majority : A baseline predicting the most
frequent class in the train data. (b) Sponsor : An LR
classiﬁer that uses the one-hot embedding of bill
sponsors to determine bill survival/cleavages (simi-
larly winner/loser relations). (c) WL-Correlation
(solely for the survival/cleavage task) predicts a
legislator’s vote on a test bill with known win-
ners/losers based on his historical votes on train
bills with the same winners/losers.
6.3 Exp 1: Winners-Losers Prediction
We compare these models in predicting relations be-
tween bills and their relevant stakeholders (Table 6).
(1) In the vanilla text-based category , RoBERTa
shows 2.9 higher F1 than BoW. Our pretrained
RoBERTa generates more efﬁcient contextual em-
bedding for text information of bills and stakehold-
ers (e.g., summary), and thus better determines the
impact of a bill on its stakeholders. Including the
sponsors’ info in the pretrained RoBERTa leads to
the best text model. (2) In the graph-based models ,
Deepwalk/GCN exhibits a sharp drop in F1, by
ignoring the heterogeneity of relations in the
graph and thus producing inefﬁcient representa-
tions for them. RGCN overcomes this issue
and approaches the best text model with F1 of
63.9. (3) Our joint text-graph model combines the
strengths of the graph and text models and delivers
3.3 points higher F1.
6.4 Exp 2: Impact on Bill Cleavage Prediction
Next, we focus on the performance of different
models in determining voting cleavages/survival,
with Unknown ,Known ,Predicted winners-losers
in the legislative graph. In Table 7, we report the
results for the bill survival and party-based voting
cleavages (results for the other cleavages in Ap-
pendix, Table 11). We can make a few observations:
First, our stakeholder analysis helps all models to
better decode state policies, when comparing the
same model in the Unknown and Known winners-
losers settings: (1) In the text-based models , pre-
diction on the textual information of both bills
and known winners-losers delivers a higher F1
than only on the text of bills (e.g., Pretrained
RoBERTa model gets a 5.4% boost in F1 in
predicting party competitive bills). Similarly,
(2) In the graph-based models : RGCN overcomes277
the limitations of Deepwalk in handling heteroge-
neous relation types (winner-loser vs. roll-call)
and delivers consistent gains in the setting Known .
(3) Our model has the best performance due to gen-
erating and optimizing a joint graph and text repre-
sentation for legislators, bills, money donors, and
stakeholders in the setting with known winners and
losers. Second, by focusing on the models with the
Predicted winners-losers information, we observe:
(5) Our model still beats the other baselines, due
to our uniﬁed model for roll-call and winner-loser
training as well as our text-based legislative graph
abstraction (Section 5). Of course, there is an ex-
pected drop in F1 across different models including
ours, when we consume predicted winner-loser re-
lations instead of human-labeled ones. This drop
could be tolerable in most cases, thus not hinder-
ing the automation of our stakeholder analysis and
leveraging its results in downstream vote analysis
tasks (ethical considerations in Section 7).
7 Conclusion
We took a new data-driven approach to analyze
state legislation in the US. We showed that identi-
fying the winners/losers of state bills can (1) inform
the public on the directions of state policies, and (2)
build a nationwide context for a better understand-
ing of legislators’ roll-call behaviors. Thus, we
proposed a text-based graph abstraction to model
the interplay of key players in the state legisla-tive process, e.g., bills, stakeholders, legislators,
and donors to legislators’ campaigns. Next, to au-
tomate our analysis, we developed a shared text
and graph embedding architecture to jointly pre-
dict winners/losers of bills and legislators’ votes on
them. We created a new dataset using different data
sources and human annotation and evaluated the
strength of our architecture against existing mod-
els. We hope this work will provide a starting point
for further studies examining the impact of policy
decisions on individuals and groups, an important
step towards making the democratic process more
transparent.
Ethical Considerations
Analyzing state legislation is a sensitive task, where
unexpected results of research and deployed ML
systems can create misguided beliefs on the gov-
ernment policies on important topics (e.g., health,
education). Thus, we would like to discuss some
ethical aspects related to our work in terms of data
and model (considering potential scenarios sug-
gested by Chandrabose et al., 2021):
1. Selection of data sources . While there can
be different inherent imbalances in the state legisla-
ture (e.g., gender and party distribution), we were
not able to identify that our data sources adding
systematic political and social biases to our study,
e.g., towards demographic populations of legis-
lators. All our data sources (e.g., LegiScan and
FollowTheMoney) are publicly available and have
been used by the political science community over
the years. LegiScan (LegiScan, 2019) is a nonpar-
tisan and impartial legislative tracking and report-
ing service for state bills. FollowTheMoney (Fol-
lowTheMoney, 2019) is a nonpartisan, nonproﬁt
organization revealing the inﬂuence of campaign
money on state-level elections and public policy
in all US states. Finally, Ballotpedia (Ballotpedia,
2019) is a nonpartisan, nonproﬁt organization pro-
viding a brief introduction, biography, committee
assignment, and general information on legislators
across different years. Our study combined these
data sources for analyzing state bills in a broad
context, thus contributing to reduced data bias for
all models evaluated in this paper.
2. Selection of states . In addition, to help mit-
igate the risk of data collection bias or topic pref-
erence that can be introduced through the choice
of speciﬁc state legislatures, we randomly picked
a “red”, a “blue”, and a “purple” state (indicating278a signiﬁcant majority for Republicans, Democrats
or more balanced state legislature, respectively).
There were some restrictions in terms of collecting
the data from the above sources (e.g., FollowThe-
Money and Ballotpedia). These data sources and
services often limit the number of API calls and
queries for retrieving the data for educational insti-
tutions. Besides this, annotating the data through
Amazon MTurk was expensive for us so we con-
ducted our study on four highly discussed topics
in state bills (i.e., health, education, agriculture,
and law). We will explore ways of expanding our
dataset to more states and topics over time.
3. Disguised winners and losers . In theory, the
authors of state bills (e.g., interest groups selling
ﬁll-in-the-blank bills to legislators) may try to re-
frame bills (disguise winners or losers) to further
their political aims. At the ﬁrst glance, this could
pose a challenge to our bill annotation, dataset, and
stakeholder analysis. As described in Section 3, the
state legislative process has a multi-stage review-
ing process in two chambers (e.g., ﬁrst reading,
second reading, and third reading). Thus, we have
observed that it is hard to hide the impact of bills on
their relevant stakeholders from our qualiﬁed anno-
tators, i.e., the authors and multiple vetted MTurk
workers for each example, in practice. In addition,
our work on MTurk maps the impact of policies
suggested by bills to winners and losers. Thus, it
already considers those stakeholders that are not
mentioned in the text explicitly (More details in
Appendix A.1).
4. Winners and losers analysis . The analysis,
aligning demographic cleavages with winners and
losers preferences, is done at an aggregate level
based on the data we annotated. These preferences
could be inﬂuenced by other factors beyond demo-
graphics. Deriving conclusions from this analysis
could require longitudinal studies, capturing the
change of these patterns over time, for example
when analyzing policies intended to help correct
inequities towards marginalized groups. Our goal
is to provide a tool for domain experts that would
point at nuanced, stakeholder speciﬁc, legislative
preferences that can be studied further in order to
determine their signiﬁcance.
5. Handling abstain votes . There are abstain
(absent and N/A) votes in our dataset. However, we
did not include them in our study due to their ex-
tremely low frequency (for our proposed model and
other baseline models). We leave this evaluation asa future work.
6. Handling other countries and languages.
While our dataset is speciﬁc to the US, the the
problem we studied, stakeholder analysis, can be
generalized to legislation from other countries and
in different languages. Although we have not eval-
uated such bills (due to lack of data sources), we
expect such legislation to produce winners or losers
to provide practical solutions to their local prob-
lems. In particular, our framework offers a multi-
relational graph abstraction and prediction models
to analyze stakeholders of bills (winners/losers)
and the voting behavior of legislators. These tech-
niques can support non-US national and state-level
legislative processes. To accommodate other lan-
guages, one could adopt cross-lingual embedding
models, e.g., XLM-R (Conneau et al., 2019) in-
stead of RoBERTa, in our architecture.
Acknowledgement
We would like to acknowledge the members of the
PurdueNLP lab. We also thank the reviewers for
their constructive feedback. The funding for the
use of mTurk was part of the Purdue University
Integrative Data Science Initiative: Data Science
for Ethics, Society, and Policy Focus Area. This
work was partially supported by an NSF CAREER
award IIS-2048001.
References279280281A Appendices
A.1 Data Annotation Pipeline
Our analysis on MTurk maps the policy described
in the bills to potential winners and losers, i.e.,
stakeholders that would be positively or negatively
impacted if the bill passes. The analysis is for the
proposed policy, regardless of the legislative out-
come (pass a vote or not). Due to lack of space,
we did not mention certain aspects of our bill an-
notation task on MTurk in Section 4. We referred
to it as a pipeline (in Section 4) because we fully
automated the whole process (e.g., selection of
MTurkers, publishing bills, collecting and analyz-
ing results, and etc.) in Python, based on MTurk
APIs and other open-source libraries. Annotation
of political bills, particularly our winners/losers
analysis, turned out to be a challenging task for
typical MTurk workers. Thus, we developed an au-
tomated quality assurance scheme to ensure high-
quality annotations for our study. In particular, we
built the following components in our pipeline:
1.We developed a Political Science Qualiﬁ-
cation test on MTurk to evaluate candidate
MTurk workers. Our test consists of 20 ques-
tions (e.g., Sentiment analysis on the US po-
litical text, identifying winners and losers of
US bills, basic political knowledge questions).
Table 8 shows the ﬁrst four questions in the
test.
2.Our pipeline selected 20 qualiﬁed English-
speaker annotators who successfully com-
pleted 80% of the tasks, assigned them ourqualiﬁcation label on Mturk, and added them
to our pool. We designed the test such that it
must be completed by candidates in 30 min-
utes and those who failed the test were not
allowed to take it again. While location was
not a determining metric for us in selectors
annotators (instead of focused on evaluating
their knowledge of the US policies and pol-
itics), most of our qualiﬁed annotators were
located in the US.
3.Next, for annotating each bill in our dataset,
our pipeline randomly chose 3 annotators
from the pool to determine the effect of the
bill on the relevant stakeholders (generated
based on the topic of the bill).
4.After collecting the result, for each bill, it
computed the ﬁnal winners and losers based
on the majority rule. For 5% of bills with no
agreement among annotators (each annotator
selected different winners/losers), we automat-
ically assigned these bills to two additional
MTurkers, and then recalculated the ﬁnal re-
sults/labels based on the majority rule.
5.For a small fraction of bills (around 1%)
adding new annotators was not sufﬁcient to
reach an agreement, and thus we automati-
cally rejected all the results and restarted the
process from Step 3 with a new group of an-
notators. Finally, for a handful of bills, the
authors performed the annotation manually.
6.To monitor the accuracy of our annotation,
our pipeline sampled labeled bills from each
batch of bills and we (authors) performed win-
ners/losers analysis on them to validate the re-
sults. We ended up observing that our pipeline
generated fully correct labels (all winners and
losers) for 90%+ of bills. Figure 5 shows the
distribution of winners and losers associated
with the bills in our crowd-sourced pipeline.
A.2 Ablation Study: Effect of Nationwide
Context
As discussed in Section 3, proposed policies and
legislative outcomes at the state level are inﬂuenced
by the nationwide context. Corporations and lobby-
ing groups coordinate their efforts across multiple
states to inﬂuence legislators in a similar way. We
capture this fact in our graph representation by in-
terconnecting states through common/shared nodes282
in Section 3. We conducted an ablation study to
show the beneﬁt of building a nationwide legisla-
tive graph. We split common nodes in the legisla-
tive graph that were shared across states (e.g., stake-
holders, money donors) into state-speciﬁc nodes.
Then, we repeated a handful of experiments in
Section 6. In our classiﬁcation tasks (both win-
ners/losers prediction and bill cleavage/survival),
we observed up to 4.3 points drop in the macro F1.
This indicated interconnecting states through com-
mon nodes (e.g., stakeholders, and money donors)
leads to better contextualized textual+graph em-
bedding. In addition, in another ablation study, we
measured the effect of different relation types andtextual attributes of nodes in the legislative graph.
For example, our evaluation showed the donors’ in-
formation (and relations with legislators) improves
the F1 score by at least 2.1 points across different
tasks (for graph models listed in Table 6).
A.3 Additional Results: State and
Time-based Data Splits
In Section 6, we evaluated all models using a ran-
dom split based on bill nodes. Here, We further
evaluate the best model in each category (from Ta-
ble 6, Table 7) with two different data splits: (1)
Time-based where test bills are selected from 20%
of most recently introduced bills. (2) State-based
where we choose the test bills from one speciﬁc
state and train bills from the other two states. In Ta-
ble 9, we look at the winners/losers prediction task
and the performance of the best model in each cat-
egory (i.e., naive, text-based, graph-based that we
discussed in Section 6). Similarly, in Table 10, we
study the best models (from Table 7) for classifying
party-based competitive bills.
Overall, we make multiple observations. First,
our results of the time- and state-based splits still
show that our pretrained textual+graph model out-
performs other models in both of the tasks (i.e.,
winners/losers prediction, and voting cleavage clas-
siﬁcation). Second, we can see a rather sharp drop
across all the models in terms of F1 score for these283
two new splits. The reason is that time-based and
state-based splits of bills lead to more unseen nodes
(e.g., legislators, money donors), challenging graph
models more than text models. For the time-based
data split, the performance degradation is slightly
less as the number of unseen nodes was fewer in
the test dataset. Third, when we use Oregon for
testing, we observe there is a higher drop in the
performance of models, compared to using Indiana
for testing; One potential reason is that Wisconsin
and Indiana tend to be Republican states, while OR
has been a Democratic one. Forth, our graph ab-
straction and stakeholders analysis (relations) help
even baseline models to better decode state poli-
cies, when we compare the performance of models
in the bill cleavage/survival tasks, with Unknown ,
Known ,Predicted winners and losers in the legisla-
tive graph.
A.4 Measuring Disagreement and Labeling
Competitive Roll-Calls
As discussed in Section 5, roll-call votes occur
when a state-level legislator votes “yea” or ”nay”
on a bill. In Sections 1 and 5, we deﬁned two
types of bill classiﬁcation tasks to characterize vot-
ing cleavages or disagreement across and within
different ideological and demographic groups of
legislators. Here, we discuss how we measure
the disagreement and label bills in each of these
tasks: (1) Inverse-Competitive bills : Consider a
bill where 55% of Men voting Yea and 45% of
them voting Nay. We deﬁne the disagreement as
the percentage of minority votes or 45%. Whenthe disagreement within a group of legislators (e.g,
men) is between 40-49%, we consider the bill as an
inverse-competitive bill with a partial tie in votes.
The disagreement of 50% is a complete tie. (2)
Competitive bills : Next, consider a bill with 60%
of Women voting Yea and 80% Men voting Nay
on a roll call. This bill is competitive because
the majority of female legislators voted differently
than the majority of male legislators (the cross-
group disagreement is 20% = 80%-60% in this
case.) Conceptually, we do not need to compute
the cross-group disagreement to identify competi-
tive bills.284