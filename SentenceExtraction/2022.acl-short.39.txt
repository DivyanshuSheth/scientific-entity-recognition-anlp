
Yiran Luo Pratyay Banerjee Tejas Gokhale Yezhou Yang Chitta Baral
Arizona State University, Tempe, AZ, USA
fyluo97, pbanerj6, tgokhale, yz.yang, chitta g@asu.edu
Abstract
We present a debiased dataset for the Person-
centric Visual Grounding (PCVG) task ﬁrst
proposed by Cui et al. (2021) in the Who’s
Waldo dataset. Given an image and a cap-
tion, PCVG requires pairing up a person’s
name mentioned in a caption with a bound-
ing box that points to the person in the im-
age. We ﬁnd that the original Who’s Waldo
dataset compiled for this task contains a large
number of biased samples that are solvable
simply by heuristic methods; for instance, in
many cases the ﬁrst name in the sentence cor-
responds to the largest bounding box, or the se-
quence of names in the sentence corresponds
to an exact left-to-right order in the image.
Naturally, models trained on these biased data
lead to over-estimation of performance on the
benchmark. To enforce models being correct
for the correct reasons, we design automated
tools to ﬁlter and debias the original dataset
by ruling out all examples of insufﬁcient con-
text, such as those with no verb or with a
long chain of conjunct names in their cap-
tions. Our experiments show that our new sub-
sampled datasetcontains less bias with much
lowered heuristic performances and widened
gaps between heuristic and supervised meth-
ods. We also demonstrate the same benchmark
model trained on our debiased training set out-
performs that trained on the original biased
(and larger) training set on our debiased test
set. We argue our debiased dataset offers the
PCVG task a more practical baseline for reli-
able benchmarking and future improvements.
1 Introduction
A newly released task called Person-centric Visual
Grounding (Cui et al., 2021) poses an interesting
angle into contextual reasoning in vision-language.
The task is motivated by humans’ reasoning ability.Figure 1: We ﬁnd many biased data from the origi-
nal Who’s Waldo dataset contain insufﬁcient contextual
cues and cannot be used to map names to persons in
an image. Left: An unsolvable example with no ac-
tions nor descriptions w.r.t the detected persons. Given
no background knowledge about the individuals, one
can only guess the masked [NAME] ’s based on heuris-
tic biases such as the locations of the bounding boxes.
Right: A qualifying example with clearly worded in-
teractions (e.g. detectable verbs such as ’watches’ &
’signs’) about each masked name - the very type of data
we incorporate into our debiased dataset.
Humans viewing an image with a caption as shown
in Figure 1 can reason (and if needed, speculate)
which name refers to which person in the image.
This reasoning task involves multiple abilities, such
as perceiving characteristics and behaviors of peo-
ple, understanding their actions in context, specu-
lating about their intentions and effects human of
actions (Fang et al., 2020), and connecting visually
perceived characteristics with grounded descrip-
tions in natural language (Kazemzadeh et al., 2014;
Yu et al., 2016; Zellers et al., 2019). In many cases,
this task can be performed without knowing the
names of the people; for instance in the example on
the right, one person is signing and the other is not,
as such it is possible to predict which person refers
to President and Secretary of State respectively.
However, in cases such as the example on the left,
if all persons are performing the same action (run-355ning on a track), then it is hard to match names with
these runners without any additional information.
Progress in the PCVG task can thus help better
capture what exact contextual cues are needed to
learn about a person’s characteristics in a scenario,
and can aid improvements in visual understanding
about human interactions and behaviors.
To support this task, Cui et al. (2021) offer
a large-scale dataset called Who’s Waldo which
consists of 272K annotated real life images. Ide-
ally, the dataset should consist of input-output pairs
(such as the example on the right in Figure 1) which
are ‘solvable’ as opposed to the one on the left
which is ambiguous. However, as we explore the
original Who’s Waldo dataset, we encounter a great
portion of cases that resemble the left example in
Figure 1, unsolvable data with insufﬁcient contex-
tual cues. Given such context, if we do not recog-
nize who exactly is in the picture, even we human
beings cannot tell which name is who. We can then
only make predictions with biased assumptions,
such as the ﬁrst named person would always be
on the leftmost, or the main subject would always
make up the largest area. Such biases in the orig-
inal dataset may explain why the heuristic meth-
ods perform very strongly, outperforming random
guessing by a big 27% increase in test accuracy and
trailing the top benchmark only by 6%. We believe
a fair dataset should not encourage approaches to
adopt biases to such an extent, and thus the original
baseline model overestimates its performance.
Inspired by dataset debiasing works such as
VQA-CP (Agrawal et al., 2018) and GQA-
OOD (Kervadec et al., 2021), we create a debi-
ased collection of 84K annotated image-captions
out of the Who’s Waldo dataset by ﬁltering out all
biased data with insufﬁcient context. We evaluate
the quality of our new dataset by applying the orig-
inal heuristic methods as well as Who’s Waldo ’s
benchmark model. Results show that our debiased
dataset greatly reduces the heuristic biases from
the original dataset and provides the PCVG task a
more practical baseline for future developments.
2 Related Work
Dataset Debiasing. We take many inspirations
from previous studies on uncurated datasets. A task
dataset if not curated properly could lead to meth-
ods that cheat their ways through without learning
generalized information. For example, VQAv2
(Goyal et al., 2017) addresses the imbalance be-tween language and images in VQAv1 (Antol et al.,
2015) which results in visual information being
ignored and inﬂated model performance. VQA-
CP (Agrawal et al., 2018) and GQA-OOD (Ker-
vadec et al., 2021) were designed to test model
performance if spurious correlations exist in the
training dataset. Cadene et al. (2019); Chen et al.
(2020a); Gokhale et al. (2020) are bias-aware tech-
niques that mitigate dataset bias with modeling and
data augmentation. Ye and Kovashka (2021) intro-
duce exploits by matching repeated texts in ques-
tions and answers to achieve high scores in Visual
Commonsense Reasoning (Zellers et al., 2019).
We also learn from various techniques to amend
priors, biases, or shortcuts in datasets. REPAIR
(Li and Vasconcelos, 2019) uses resampling to ﬁx
representation biases in image datasets. Dasgupta
et al. (2018) incorporate compositional information
into sentence embeddings for Natural Language
Inference. DQI (Mishra et al., 2020) offers quanti-
tative metrics to assess biases in automated dataset
creation in Natural Language Processing. Le Bras
et al. (2020) introduce adversarial measures to miti-
gate biases in various Natural Language Processing
and Computer Vision tasks.
Visual Grounding. The PCVG task adapts pre-
vious supervised Visual Grounding models as its
original baselines. The Visual Grounding task is de-
ﬁned as locating speciﬁc objects in an image from
a textual description. First established by Karpathy
et al. (2014), following researches have evolved
into extracting attention information such as works
by Deng et al. (2018) and Endo et al. (2017). A
huge variation of datasets for Visual Grounding
have also been created, including Flicker30k (Plum-
mer et al., 2015), Visual Genome (Krishna et al.,
2017), and RefCOCO (Yu et al., 2016).
Referring Expression Comprehension (REC).
An active branch from Visual Grounding, the Re-
ferring Expression Comprehension task (Rohrbach
et al., 2016) is no longer restricted to object cate-
gories. Instead its goal is to relate a free region in
an image to a sentence description. Mattnet (Yu
et al., 2018) is one prominent approach that lever-
ages both attention features and relation extraction
for the objects in the image. Qiao et al. (2020)
offers a comprehensive survey on this topic.
Human Detection. A specialized category under
Object Detection, detecting humans with bounding
boxes in images nowadays can easily use open
source toolboxes including MMDetection (Chen356
et al., 2019) or Detectron (Wu et al., 2019) that
are trained on large-scale real life image datasets
like COCO (Lin et al., 2014). Recent works such
as DarkPose (Zhang et al., 2020) also attempt to
utilize human pose information to better single out
human traits from complex background.
3 Method
In this section, we introduce the Person-centric Vi-
sual Grounding task, discuss the original Who’s
Waldo dataset, and provide our analysis of short-
cuts, biases, and other issues that we discovered in
the dataset. We describe the process via which we
curate, debias, and ﬁlter the dataset.
3.1 The Task
The Person-centric Visual Grounding task is de-
ﬁned as follows. The givens are an image I, a set
of m1 person detections B(in form of bound-
ing boxes), and a corresponding image caption T
where its tokens contain references to n 1 per-
sons. For each referred person, we look for the
best matching detection from the givens. We also
assume no two persons can be matched with the
same detection.
3.2 The Who’s Waldo Dataset
The dataset consists of 272K real-life captioned
images sourced from the free Wikimedia Com-
mons repository. Each image pictures individu-
als under the ’People by name’ category on Wiki-
media Commons, while its caption describes the
scene and explicitly mentions the featured people
in real names. Key dataset creation procedures,
text pre-processing, identifying person entities in
captions, detecting bounding boxes of people in im-
ages, and generating ground truths linking bound-
ing boxes and names, are all done with existing
automated tools such as FLAIR (Akbik et al., 2019)
and MMDetection (Chen et al., 2019). To prevent
misuse, in the publicly released version, all thereal names in the captions are replaced with the
[NAME] token, but references between bounding
boxes and token indices are given in individual an-
notation ﬁles. This is equivalent to masking each
name with indexed placeholders such as PERSON1 ,
PERSON2 , etc. Amongst the entirety of 272K an-
notated samples, 179K samples are used for train-
ing, 6.7K for validation, and 6.7K for testing. Each
test sample is supposed to either mention at least
two persons orchoose from at least two bounding
boxes . The original test set is further validated
manually on Amazon Mechanical Turk.
3.3 Biases in Who’s Waldo
The premise of the Person-centric Visual Ground-
ing task is to use ONLY the caption text and the
image as the cues to ﬁnd out the correct bounding
box from the image per mentioned name. However,
we observe a large portion of the original Who’s
Waldo dataset does not provide sufﬁcient contexts
and can only be solved by heuristic methods. We
discuss two major types of biases that we discover
in the following sections.
The ﬁrst type no-verb is that the caption text
contains zero detectable verbs. Since linguistically
a verb is the crucial part of an action that assigns
participants with semantic roles, we technically
have no way to tell who performs or who receives
an action without verbs. For example in Figure
2(a), we are unable to tell who is who from the
image and the no-verb caption alone, unless we
recognize Vladimir Putin or the Georgian President
with external knowledge.
The second type conjunct-names is that the
caption contains a long chain of conjunct referred
names. Shown in Figure 2(b), all the referred
names share the verb perform , joined together only
with conjunct words such as and oralong with .
With no indication of the order amongst these per-
sons, we can only resort to a naive positional order
such as left-to-right. But since we may also have
extra bounding boxes as choices, such naive as-
sumption is indeed unreliable. Figure 2(b) is such
an example that the ﬁrst mentioned name is not
always the one in the left-most bounding box.
3.4 Data Curation for De-biasing
In order to resolve the aforementioned limitations
of the original dataset, we utilize two pipelines
in SpaCy ver 3.0 (Honnibal et al., 2020) to ﬁlter
out the biased data. We apply the POS-Tagging
pipeline to ﬁnd out if sentences in an image cap-357
tion contain verbs in any form of conjugation. In
parallel, we use the Dependency Parsing pipeline
to examine if any [NAME] token conjuncts with
more than one [NAME] ’s from different referred
persons. We jointly ﬁlter out any example that
either (a) contains zero verbs, or (b) has at least
three conjunct referred person names in a sentence.
For both pipelines, we replace the [NAME] tokens
that refer to the same person in a caption with
a random popular ﬁrst name, so that the natural
language-based SpaCy pipelines can yield more
accurate results. Both pipelines use the state-of-
the-art en-web-core-trf model which is built
on RoBERTa (Liu et al., 2019).
Ultimately, our ﬁltering procedure produces 84K
qualifying image-caption pairs. Table 1 shows the
distribution of samples sourced from each split of
the original through our two debiasing pipelines.
We utilize data from the unused yet legitimately
annotated 79K samples of the original dataset. We
reorganize and split all the qualifying 84K samples
into 74K for training, 5K for validation, and 5K
for test. Our new test set does not overlap with the
original training set. Similarly to the design of the
original, we enforce that all samples in our new test
set involves no trivial case that contains exactly one
referred name and exactly one bounding box. We
also make sure that any test set sample always has
at least one name-to-bounding-box pair as ground
truth.4 Experiments and Baselines
Setup. We evaluate the quality of our debiased
dataset with the same heuristic and Transformer-
based methods from the original paper. We also
train the benchmark model on both the original
and our new training set. We report the accuracies
obtained from our new test set as the new baselines.
Heuristics. We inherit the original heuristic mea-
sures to study the potential biases of our debiased
dataset versus those of the original dataset. Along-
side Random guessing, we assign the names in the
caption to the bounding boxes sorted by: (a) de-
creasing area size (Big !Small), (b) left-to-right
upper-left coordinates (L !R (All)), and (c) left-to-
right upper-left coordinates of the largest dbound-
ing boxes, dbeing the larger between the number
of bounding boxes and the number of names in a
test case (L!R (Largest)).
Transformer-based Models. We adapt the origi-
nal benchmark Who’s Waldo model to our debiased
dataset and see how well it can perform under the
updated contexts. The benchmark model is a multi-
layer multi-modal Transformer (Vaswani et al.,
2017). Based on UNITER (Chen et al., 2020b),
it learns to maximize the similarities between the
corresponding person names and bounding boxes
while minimize the similarities between those that
do not match up. We ﬁne-tune the Who’s Waldo
model with pre-trained weights from UNITER.
Analysis of Results. Table 2 shows the test set
accuracies for the original dataset and our debi-358
ased dataset. We ﬁnd that the heuristic measures
have overall lower performance on our new dataset,
meaning we have successfully reduced the effects
of the positional and the size-based biases from
the original dataset. Most signiﬁcantly, we have
lowered L!R (All) from +7.5% to +1.4%, almost
equal to randomness. Even the strongest L !R
(Largest) heuristic has been lowered from +26.8%
all the way down to +13.3% as well. Our dataset is
thus proven less biased compared to the original.
We also show that our dataset has better prac-
ticality for the task. Measured with our new test
set, the performance of the Who’s Waldo bench-
mark model trained with the original training set
performs 3.8% lower than that trained with our new,
smaller training set. Meanwhile, the test accuracy
gap between the Transformer-based method and
the heuristic methods has become larger using our
debiased dataset, widened from 5.8% to 9.7%. In
addition, using the ﬁltered biased samples from the
original test set on our new trained model yields
an even lower performance at 48.2%, which indi-
cates our new baseline model now adopts fewer
biases during training compared to the original.
Altogether with the lowered new baseline accu-
racy of 54.0%, we argue that our debiased dataset
improves the quality of contextual cues that su-pervised models can learn from, and leaves more
applicable room for improvements in the future.
5 Conclusion
We present a reﬁned dataset for the PCVG task
with samples that contain contextual information
required for the task. We address prominent biases
that we identiﬁed in the original task dataset by
ﬁltering out a large number of unsolvable cases,
and report new baseline performances on the new
benchmark. Our reﬁned dataset can serve as a more
reliable benchmark to enable fair comparisons for
new modeling techniques and training protocols.
Acknowledgements
This research was supported by grants from
DARPA SAIL-ON, DARPA KAIROS, NSF
1816039, and NSF 2132724. The views and opin-
ions of the authors expressed herein do not neces-
sarily state or reﬂect those of the funding agencies
and employers.
Ethical Considerations
Our curated dataset is available at . We will
also follow the same licensing and data sharing
policy as the original Who’s Waldo dataset.359References360361