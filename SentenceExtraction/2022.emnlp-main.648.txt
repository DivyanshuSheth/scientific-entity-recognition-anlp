
Adam YaariJan DeWittHenry HuBennett Stankovits
Sue FelshinYevgeni BerzakHelena Aparicio
Boris KatzIgnacio Cases*Andrei Barbu*
Abstract
Treebanks have traditionally included only
text and were derived from written sources
such as newspapers or the web. We intro-
duce the Aligned Multimodal Movie Tree-
bank (AMMT) †, an English language tree-
bank derived from dialog in Hollywood movies
which includes transcriptions of the audio-
visual streams with word-level alignment, as
well as part of speech tags and dependency
parses in the Universal Dependencies (UD) for-
malism. AMMT consists of 31,264 sentences
and 218,090 words, that will amount to the 3rd
largest UD English treebank and the only multi-
modal treebank in UD. We find that parsers on
this dataset often have difficulty with conversa-
tional speech and that they often rely on punctu-
ation which is often not available from speech
recognizers. To help with the web-based an-
notation effort, we also introduce the Efficient
Audio Alignment Annotator (EAAA) ‡, a com-
panion tool that enables annotators to signifi-
cantly speed-up their annotation processes.
Keywords: multimodal, video, audio, tree-
bank, Universal Dependency parsing1 Introduction
Treebanks are fundamental resources in Natural
Language Processing (Nivre et al., 2016). Despite
their central role, most existing treebanks are de-
rived from single-modality texts such as newspa-
pers, blogs, and other online communities. The
vocabulary, syntax, and statistics of spoken and
written language can be quite different from one
another (Caines et al., 2017). To complement these
datasets and aid the advent of multimodal conver-
sational agents, we have created a new dataset, the
Aligned Multimodal Movie Treebank, AMMT, the
content of which is derived from language spoken
in Hollywood movies. AMMT is released pub-
licly under an open source license and will be con-
tributed to the Universal Dependencies (UD) (Nivre
et al., 2020) treebanks.
Speech based treebanks have proven to be a
resource of enormous importance to the NLP re-
search community (Ahrenberg, 2007; Nivre et al.,
2006). We find Treebank-3 of the Penn Treebank
(Marcus et al., 1993), which includes the Penn Tree-
bank Switchboard corpus (Godfrey et al., 1992), to
be the closest existing dataset to AMMT. This cor-
pus contains nearly one million transcribed words
from Switchboard annotated with part of speech
tags, dysfluencies, and parse trees, and it also in-Figure 1: An overview of AMMT, our novel multimodal dataset, consisting of transcriptions and parses for 21
movies aligned at the millisecond level. EAAA is a new transcription and alignment tool introduced below.9531cludes alignment between words and audio. How-
ever, there are several key differences between this
dataset and our own. AMMT provides alignment
to visual as well as audio data; it is annotated with
UD rather than Penn Treebank dependencies; and
conversations are much shorter (Switchboard was
designed to have long 10 minute conversations be-
tween strangers on the phone discussing one of a
preselected list of topics). While conversations in
AMMT can still be considered as prepared speech,
topics are way less constrained. AMMT also in-
cludes many more speakers and its audio quality
allowed us to recover almost all spoken words.
For practical experiments, AMMT is significantly
more entertaining for subjects, a key feature for
researchers aiming to study the neuroscience of
language via neural imaging. Finally, with this
contribution, AMMT is being made open to the
whole research community and not restricted to
LDC members.
Our contributions are: 1. AMMT is the first
large-scale treebank to include alignment to both
audio and video. 2. AMMT includes fine-grained
millisecond-level word boundaries. 3. AMMT is
parsed in the UD framework and is the 3rd largest
English UD treebank. 4. A new tool, Efficient
Audio Alignment Annotator (EAAA), for rapid
word boundary annotation in large corpora.
2 Dataset
The AMMT dataset is an English language tree-
bank based on 21 Hollywood movies that provides
transcriptions with word-level alignment to the
audio-visual stream, as well as part of speech tags
and dependency parses in the UD formalism. An-
notations for speaker identification will be included
at the time of release. Due to copyrighted source
material, AMMT provides multiple 1-second-long
audio-visual sample clips from every movie, and a
tool chain allowing users to obtain their own copies
and verify alignment with the dataset.
AMMT consists of 31,264 sentences, 218,090
words, 8,541 lemmas and 10,805 unique tokens.
The counts of POS tags and dependencies are
shown in appendix A. The 21 movies from which
the dataset is derived are listed in table 4 along with
their unique identifiers and relevant statistics.
Movies were chosen to be appropriate for many
ages, with the highest rating being PG-13. They be-
long to a variety of movie genres (including action,
adventure, animation, comedy, drama, fantasy, fam-ily, and sci-fi, according to IMDb’s categorization),
and their release dates range from 1995 to present.
They were selected to have verbose scripts, in the
top 50% of randomly sampled movies. Movies
which included extensive singing such as musicals
were omitted. Copies of the movies were obtained
and extracted in full including opening and closing
credits. Special features and after-credits scenes
were omitted.
2.1 Transcription pipeline
The audio track was originally transcribed using
the Google Cloud Speech-to-Text API (Google,
2020). It was then corrected by annotators, hired
from rev.com andhappyscribe.com depending on
the movie, and then further extensively corrected
by 7 expert annotators. Transcription followed a
set of guidelines to deal with problematic audio
segments and to enforce coherence. Manual tran-
scription was performed simultaneously with word-
boundary annotation using a new tool developed
for this purpose, EAAA (see section 4), which was
also subsequently used by annotators to perform
sentence segmentation and fixing capitalization.
Transcription was verbatim without any correc-
tions for dysfluencies or mistakes. Instructions
were provided to the annotators to standardize the
transcripts and eliminate problematic audio seg-
ments. Foreshortened words ( ’round vsaround )
were transcribed as they were said including the
foreshortening. Abbreviations were always ex-
panded ( dr.vsdoctor ). Cardinal and ordinal num-
bers were spelled out, while long numbers were
written as spoken including conjunctions such as
and(e.g., five hundred and five ).
Manual transcription was carried out simulta-
neously with word boundary annotation using a
purpose-built tool, EAAA (see section 4). EAAA
presented annotators with a spectrogram for 4 sec-
ond segments of a movie, along with the ability to
replay and slow down any sub-segment and seek
throughout the movie. As the audio was played,
a line marked the location of the audio sample in9532
the spectrogram in real time. In some cases, an-
notators could hear specific words but could not
clearly identify in the spectrogram where those
words occurred (e.g. short words like to). Anno-
tators were instructed to annotate what they heard
regardless of the spectrogram, sometimes leading
to such short words having zero-length intervals.
Foreign sentences (e.g., Elvish in the movie The
Lord Of The Rings ) were marked but not included
in the corpus, although one-off foreign words in
English sentences were transcribed. All cases of
singing, unintelligible speech, and multiple speak-
ers overlapping were noted and eliminated from the
dataset. Transcripts are as spoken, without correc-
tion, even when the speaker erred omitting a word
or using a word inappropriately.
After transcription and word boundary align-
ment, the text was segmented into sentences. An-
notators marked the end of each sentence manually
and fixed capitalization (of both proper nouns and
sentences as needed). Throughout this process,
some critical punctuation was introduced as anno-
tators saw fit.
2.2 Dependency parsing pipeline, annotation
and validating annotator performance
We parsed all transcriptions with Stanza (Qi et al.,
2020) using the standard English model.
The AMMT dataset was entirely annotated by an
in-house expert annotator over the course of a year.
Edge cases were discussed with other three team
members with strong background in linguistics and
Universal Dependencies in particular. In this period
of time, the expert annotator performed a total of
three sequential passes over the full dataset with
the idea of promoting internal consistency.
Separately, after this annotation process con-
cluded, a subset of AMMT consisting in 300 sen-
tences of length 5 through 20 uniformly sampled
across movies were reannotated by an expert an-
notator. This expert annotator has a strong back-
ground in linguistics and did not contribute to the
dataset otherwise. The length of these sentences
was selected to avoid the effect of very short or
very long sentences (see table 2).
The inter-annotator agreement of the annota-9533tions was with 99.53% on correct POS tagging,
98.95% on correctly placing dependencies (UAS),
and 98.31% on correctly identifying the type of
a dependency relation. MLAS ties together POS
and LAS into a single number, 96.72%, which mea-
sures the inter-annotator agreement of the annota-
tions (Straka, 2018).
Note that the inter-annotator score presented in
table 2 is thus a measure, for this particular subset
of the dataset, of the disagreement between the
original expert annotator and the external expert
annotator. As such it should only be considered as
a bound on the actual disagreement between the
two annotators.
We found word-boundary inter-annotator agree-
ment to be remarkably high, with less than 15ms
on average for all words in a single movie, Lord Of
The Rings , annotated by 5 annotators.
2.3 Performance of existing parsers
We compared our annotations against those pro-
duced by Stanza (Qi et al., 2020) in fig. 3. Stanza
was the original parser used to initialize the tree-
bank before extensive human correction. This
likely biases the results toward Stanza in subtle
ways (Berzak et al., 2016) which we do not investi-
gate here beyond section 2.2.
Note that performance on short sentences, fewer
than 3 words, and long sentences, with more than
20 words, is far worse than average-case perfor-
mance (see fig. 5 for the distribution of sentences
in AMMT). This trend is not observed in other cor-
pora such as the English Web Treebank (EWT) (Sil-
veira et al., 2014), where performance increases for
short sentences (although these are very infrequent)
while the performance drop for long sentences is
half or less than that seen in AMMT. While the
distributions of POS in both corpora are slightly
different (cf. appendix A), the performance drop
for short sentences appears to be driven by POS
tag errors, see the relative drop in POS accuracy
between fig. 3(a,b,c) — perhaps such sentences re-
quire more context to be correctly interpreted. The
performance drop for long sentences appears to be
driven by incorrectly identified relationships, see
the relative drop in UAS between fig. 3(a,b,c).
3 Multimodal feature analysis
Exploring the utility of the corpus as a multimodal
resource for grounded language and vision tasks,
we quantified the co-occurrence of nouns and their
corresponding objects (i.e. objects that are verbally
mentioned as they appear on screen). As an ap-
proximation, we considered the 80 object classes
of the Microsoft COCO dataset (Lin et al., 2014).
We extracted all nouns corresponding to a COCO
class (580 nouns across all movies) and manually
reviewed the middle frame of a word utterance. We
find an average of 36.5% noun-object agreement
rate (212 co-occurring objects) across all movies
(µ= 23.7%,σ≈17.5%per movie); see fig. 4.
Considering noun-object agreements across both
object classes and movie types reveals variable dis-
tributions. Some nouns are highly likely to appear
on screen as their corresponding noun is uttered,
like Person (94.4%), types of vehicles (Car: 59.7%,
Bicycle: 68.3%) and animals (Giraffe: 100%, Cow:
100%), while others have not co-occurred once
despite being uttered multiple times. Moreover, un-
ambiguous nouns (e.g. Laptop: 50%, TV: 42.8%,
Toilet: 33.3%) tend to have a significantly higher
agreement rate scores than words with multiple
POS (e.g. Bear: 2.5%, Orange: 0%, Remote: 0%).
Some movie categories are also more likely to have
high noun-object agreement, such as movies aimed
for a younger audience (educational and anima-
tion genres), perhaps to enable language learning9534through multimodality. For example Cars-2 and
Sesame Street present 79.2% and 74.3% agreement
rate respectively, while The Lord Of The Rings 1
and2, and Avengers Infinity War score only 17.6%,
14.2% and 5.9% respectively; see fig. 6.
4 Tools
To efficiently annotate the alignment between word
onsets and offsets and the audio stream, we cre-
ated a new tool, the Efficient Audio Alignment
Annotator (EAAA). EAAA enables annotators to
start with a rough transcript and approximate align-
ment between words and the audio track. An-
notators can simultaneously correct the transcriptwhile annotating new words. An overview of the
EAAA interface is shown in fig. 2. Tools such
as Praat (Boersma, 2001) also allow for annotat-
ing audio corpora with word boundaries. Unlike
Praat, EAAA is web-based making it easier for
annotators to use. Data such as spectrograms and
wave files seen by annotators is pre-processed on
the server-side, making browsing and accessing
movies with EAAA near real-time. Since EAAA
is a single-purpose tool meant for transcription and
fine-grained alignment, it provides custom features
which significantly speed up the annotation process
like keyboard shortcuts, the ability to handle au-
dio files of any length, and a streamlined interface.
EAAA also handles multiple concurrent annota-
tors, sharing and comparing multiple annotations
directly.
EAAA pre-processes movie files into 4 sec-
ond segments that overlap by 2 seconds and com-
putes spectrograms for each segment with Librosa
(McFee et al., 2015). Storage is provided by a local
Redis database which is not exposed to the web. In
addition, EAAA includes a telemetry server which
collects comprehensive information during the an-
notation process including every transcript change,
keyboard shortcut used, and mouse press.
5 Conclusion
AMMT and EAAA are open source and AMMT
will be contributed to the UD treebanks. In addition
to verbatim transcriptions and a treebank, AMMT
provides a tool chain to enable access and align-
ment to the source video and audio. Most datasets
for evaluating and training parsers are focused on
written rather than spoken language. With the rise
of conversational agents, AMMT can serve as a
more predictive benchmark in this domain.
At present, no end-to-end systems – from video-
and-audio to parses – exist, even if humans often
use visual information to disambiguate and contex-
tualize auditory information. We hope that AMMT
and its tooling will support further work on multi-
modal approaches to conversational agents, end-to-
end parsing, as well as psychophysics and neuro-
science with language in context.
Acknowledgements
This work was supported by the Center for Brains,
Minds and Machines, NSF STC award 1231216,
the NSF award 2124052, the MIT CSAIL Sys-
tems that Learn Initiative, the MIT CSAIL Ma-9535chine Learning Applications Initiative, the CBMM-
Siemens Graduate Fellowship, the DARPA Ar-
tificial Social Intelligence for Successful Teams
(ASIST) program, the DARPA Knowledge Man-
agement at Scale and Speed (KMASS) program,
the United States Air Force Research Laboratory
and United States Air Force Artificial Intelligence
Accelerator under Cooperative Agreement Num-
ber FA8750-19-2-1000, the Air Force Office of
Scientific Research (AFOSR) under award num-
ber FA9550-21-1-0014, and the Office of Naval
Research under award number N00014-20-1-2589
and award number N00014-20-1-2643. The views
and conclusions contained in this document are
those of the authors and should not be interpreted as
representing the official policies, either expressed
or implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation herein.
Ethics
The AMMT corpus was constructed using Holly-
wood movies. Many of these movies generated
by the US/Western film industry are unbalanced
in terms of cultural and sociolinguistic diversity
and oftentimes rely on stereotypes. As such, the
distributions of gender, race, age, socioeconomic
status, etc. appearing in this corpus are biased as
they are sampled from this pool.
Annotators, both in lab and online, contributed
significant effort to the development of this dataset.
The vast majority of the annotation effort was car-
ried out in lab, with only limited bootstrapping
from online services, due to both ethical and quality
concerns. Using state-of-the-art models like speech
recognizers significantly sped up every stage of the
annotation, for example, making transcription only
slightly slower than real time. In lab annotators
were paid over $18/hour.
Limitations
Movies in AMMT were selected to be appropriate
and entertaining for many ages with the highest
rating being PG-13. This selection criterion lim-
its the genres and topics covered. Also, speech in
movies is prepared speech. While prepared speech
is often meant to seem similar to natural speech, it
limits the applicability of the corpus. Similarly, the
relationships, social situations, and actions taken
by agents, are constructions designed with a pur-pose (e.g. discoursive, entertainment) rather than
examples of actual social dynamics, conflict, or
growth.
Effort was made to make transcriptions verba-
tim to maintain the regional or cultural variation
in speech present in the original movies, e.g., by
directly including foreshortened words. However,
such variation is generally selected against in the
creation of Hollywood movies and so is poorly
represented in this dataset.
The current version of the corpus is monolingual
(English) although two Spanish movies were par-
tially processed and may be included in a future
version.
References95369537A Appendix95389539