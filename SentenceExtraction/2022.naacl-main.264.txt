
Ameet Deshpande
Department of Computer Science
Princeton University, USAPartha Talukdar
Google Research
IndiaKarthik Narasimhan
Department of Computer Science
Princeton University, USA
Abstract
While recent work on multilingual language
models has demonstrated their capacity for
cross-lingual zero-shot transfer, there is a lack
of consensus in the community as to what
shared properties between languages enable
transfer on downstream tasks. Analyses in-
volving pairs of natural languages are often in-
conclusive and contradictory since languages
simultaneously differ in many linguistic as-
pects. In this paper, we perform a large-scale
empirical study to isolate the effects of var-
ious linguistic properties by measuring zero-
shot transfer between four diverse natural lan-
guages and their counterparts constructed by
modifying aspects such as the script, word or-
der, and syntax. Among other things, our ex-
periments show that the absence of sub-word
overlap signiﬁcantly affects zero-shot transfer
when languages differ in their word order, and
there is a strong correlation between transfer
performance and word embedding alignment
between languages (e.g., ρ= 0.94on the task
of NLI). Our results call for focus in multilin-
gual models on explicitly improving word em-
bedding alignment between languages rather
than relying on its implicit emergence.
1 Introduction
Multilingual language models like XLM (Con-
neau et al., 2020a) and Multilingual-BERT (Devlin,
2019) are trained with masked-language modeling
(MLM) objective on a combination of raw text from
multiple languages. Surprisingly, these models ex-
hibit decent cross-lingual zero-shot transfer, where
ﬁne-tuning on a task in a source language trans-
lates to good performance for a different language
(target).
Requirements for zero-shot transfer Recent
studies have provided inconsistent explanations for
properties required for zero-shot transfer (hereon,transfer). For example, while Wu and Dredze
(2019) conclude that sub-word overlap is vital for
transfer, K et al. (2020) demonstrate that it is not
crucial, although they consider only English as the
source language. While Pires et al. (2019) suggest
that typological similarity (e.g., similar SVO or-
der) is essential for transfer, other works (Kakwani
et al., 2020; Conneau et al., 2020a) successfully
build multilingual models for dissimilar languages.
Need for systematic analysis A major cause of
these discrepancies is a large number of varying
properties (e.g., syntax, script, and vocabulary size)
between languages, which make isolating crucial
ingredients for transfer difﬁcult. Some studies al-
leviate this issue by creating synthetic languages
which differ from natural ones only in speciﬁc lin-
guistic properties like script (K et al., 2020; Dufter
and Schütze, 2020). However, their focus is only
on English as a source language, and the scale of
their experiments is small (in number of tasks or
pre-training corpora size), thus limiting the scope
of their ﬁndings to their settings alone.
Our approach We perform a systematic study of
cross-lingual transfer on bilingual language mod-
els trained on a natural language and a systemati-
cally derived counterpart. We choose four diverse
natural languages (English, French, Arabic, and
Hindi) and create derived variants using four differ-
ent transformations on structural properties such as
inverting or permuting word order, altering scripts,
or varying syntax (Section 3.2). We train mod-
els on each of the resulting sixteen language pairs,
and evaluate zero-shot transfer on four downstream
tasks – natural language inference (), named-
entity recognition (), part-of-speech tagging
(), and question-answering ().
Our experiments reveal the following:
1. Contrary to previous belief, the absence of sub-
word overlap degrades transfer when languages3610differ in their word order (e.g., by more than 40
F1 points on POS tagging, (§ 4.1)).
2.There is a strong correlation between token em-
bedding alignment and zero-shot transfer across
different tasks (e.g., ρ= 0.94,p < . 005for
XNLI, Fig 4).
3.Using pre-training corpora from similar sources
for different languages (e.g., Wikipedia) boosts
transfer when compared to corpora from differ-
ent sources (e.g., 17F1 points on NER, Fig 3).
To our knowledge, we are the ﬁrst study to quan-
titatively show that zero-shot transfer between lan-
guages is strongly correlated with token embedding
alignment (ρ= 0.94for). We also show that
the current multilingual pre-training methods (Dod-
dapaneni et al., 2021) fall short of aligning em-
beddings even between simple natural and derived
language pairs, leading to failure in zero-shot trans-
fer. Our results call for training objectives that ex-
plicitly improve alignment using either supervised
(e.g., parallel corpora or bilingual dictionaries) or
unsupervised data.
2 Related work
Multilingual pre-training for Transformers
The success of monolingual Transformer lan-
guage models (Devlin et al., 2019; Radford et al.,
2018) has driven studies that learn a multilin-
gual language-model (LM) on several languages.
Multilingual-BERT (M-BERT) (Devlin, 2019) is a
single neural network pre-trained using the masked
language-modeling (MLM) objective on a corpus
of text from 104languages. XLM (Conneau and
Lample, 2019) introduced translation language-
modeling, which performs MLM on pairs of par-
allel sentences, thus encouraging alignment be-
tween their representations. These models exhibit
surprising zero-shot cross-lingual transfer perfor-
mance (Conneau and Lample, 2019; K et al., 2020),
a setup where the model is ﬁne-tuned on a source
language and evaluated on a different target lan-
guage.
Analysis of cross-lingual transfer While Pires
et al. (2019), Conneau et al. (2020b), and K et al.
(2020) showed that transfer works even without
a shared vocabulary between languages, Wu and
Dredze (2019) discovered a correlation between
sub-word overlap and zero-shot performance. Con-
neau et al. (2020b) and Artetxe et al. (2020a)
showed that shared parameters for languages withdifferent scripts were crucial for transfer. Pires et al.
(2019) and (Wu and Dredze, 2019) observed that
transfer for NER and POS tagging works better
between typologically similar languages. However,
a study conducted by Lin et al. (2019) showed
that there is no simple rule of thumb to gauge
when transfer works between languages. Hsu et al.
(2019) observed that changing the syntax (SOV)
order of the source to match that of the target does
not improve performance.
Transfer between real and synthetic Languages
K et al. (2020) create a synthetic language by
changing English’s script and ﬁnd that transfer be-
tween it and Spanish works even without common
sub-words. However, they use only English as their
source language, test only on two tasks, and use
a single natural-synthetic language pair. Dufter
and Schütze (2020) study transfer between English
andsynthetic English obtained by changing the
script, word order, or model delimiters. However,
they use a small corpus ( 228K words) compared to
current standards (we use 3orders more) and mea-
sure only embedding similarity and not zero-shot
transfer. A contemporary work (Wu et al., 2022)
uses synthetic transformations to modify the GLUE
dataset (Wang et al., 2018) and analyze properties
required for good zero-shot transfer, but they per-
form their experiments only on English and do not
perform token embedding alignment analysis. We
show that the latter is crucial for good transfer.
3 Approach
We ﬁrst provide some background on bilingual lan-
guage models (Section 3.1), followed by descrip-
tions of our transformations (Section 3.2), and our
training and evaluation setup (Section 3.3).
3.1 Background
Bilingual pre-training The standard setup (Con-
neau and Lample, 2019) trains a bilingual language
model ( Bi-LM ) on raw text corpora from two lan-
guages simultaneously. Bi-LM uses the masked
language-modeling loss ( L) on the corpora
from the two languages ( C,C) separately with
no explicit cross-lingual signal:
L(C+C) =L(C) +L(C)
A shared byte pair encoding tokenizer (Sennrich
et al., 2015) is trained on C+C. A single batch
contains instances from both languages, but each
instance belongs to a single language.3611Transformation Instance ( s) Transformed instance (T(s))
Inversion (T) Welcome to NAACL at Seattle Seattle at NAACL to Welcome
Permutation (T ) This is a conference a This conference is
Transliteration (T ) I am Sam . I am ♣♥♦♠♣♥
Syntax (T)Saraateapples Saraapplesate
Une tableronde Une rondetable
Zero-shot transfer evaluation Consider a bilin-
gual model ( Bi-LM ) pre-trained on two languages,
source andtarget . Zero-shot transfer involves ﬁne-
tuning Bi-LM on downstream task data from source
and evaluating on test data from target . This is con-
sidered zero-shot because Bi-LM is not ﬁne-tuned
on any data belonging to target .
3.2 Generating language variants with
systematic transformations
Natural languages typically differ in several ways,
like the script, word order, and syntax. To isolate
the affect of these properties on zero-shot transfer,
we obtain derived language corpora (hereon, de-
rived corpora) from original (natural) language cor-
pora by performing sentence level transformations
(T) which change particular properties. For exam-
ple, an “ inversion ” transformation could be used
to invert each sentence in the corpus ( Welcome
toNAACL⇒NAACLtoWelcome). Since the
transformation (T) is applied on each sentence of
theoriginal corpus, the size of the original and
thederived corpus is the same. In the following
sections, we will use the following notation:
C≡Original corpus
={s|i= 1 :N,s=sentence}
T ≡ Sentence-level transformation
C≡Derived corpus
={T(sent)|∀sent∈C}
Types of transformations We consider four
transformations which modify different aspects of
sentences (examples in Table 1):
1.Inversion (T): Invert the order of tokens
in the sentence, like in Dufter and Schütze(2020). The ﬁrst token becomes the last, and
vice versa.
2.Permutation (T ): Permute the order of
tokens in a sentence uniformly at random. For
a sentence of ntokens, we sample a random
ordering with probability.
3.Transliteration (T ): Change the script
of all tokens other than the special tokens
(like[CLS] ). This creates a derived vocab-
ulary (V) with a one-to-one correspon-
dence with the original vocabulary ( V).
4.Syntax (T): Modify a sentence to match
the syntactic properties of a different natu-
ral language by re-ordering the dependents
of nouns and verbs in the dependency parse.
These transformations are stochastic because
of the errors in parsing and sampling over pos-
sible re-orderings (Wang and Eisner, 2016).
These transformations allow us to systematically
evaluate the effect of corresponding properties on
zero-shot transfer. We also consider composed
transformations (§4.2) which consecutively apply
two transformations. We note that while real lan-
guages typically differ in more than one or two
properties considered in our transformations, our
methodology remains useful in isolating crucial
properties that enable good transfer and can be ex-
tended to more transformations.
Transformations for downstream tasks We ob-
tain the downstream corpus in the derived language
(D) by applying the same transformation ( T)
used during pre-training on the original down-
stream corpus (D). Unlike pre-training corpora
which contain raw sentences, instances in down-
stream tasks contain one or more sentences with
annotated labels. For text classiﬁcation tasks like3612
NLI, we apply the transformation on each sentence
in every dataset instance. For token classiﬁcation
tasks (e.g., NER, POS), any transformation which
changes the order of the tokens also changes the
order of the labels. We present the mathematical
speciﬁcation in Appendix A.
3.3 Model Training and Evaluation
We now describe our pre-training and zero-shot
transfer evaluation setup. Figure 1 provides an
overview of pre-training and ﬁne-tuning, and Ta-
ble 2 summarizes the evaluation metrics we use.
Pre-training LetCandC be the origi-
nalandderived language pre-training corpora. We
train two models for each original -derived pair:
1.Bilingual Model ( Bi-LM ): A bilingual model
pre-trained on the combined corpus ( C+C ) (Figure 1a).
2.Monolingual Model ( Mono-LM ): A mono-
lingual model trained only on C for the
same number of steps as Bi-LM ’s.Mono-
LMis used as a baseline to measure zero-shot
transfer of a model not pre-trained on derived .
Evaluation LetD andDbe the origi-
nallanguage training and development sets for a
downstream task, and D andD be the
corresponding derived language datasets. For eval-
uation, we ﬁrst ﬁne-tune the pre-trained models on
a downstream training set and evaluate the resulting
model on a development set (Figure 1b). Since our
goal is to investigate the extent of zero-shot transfer,
we require appropriate lower and upper bounds to
make informed conclusions. To this end, we com-
pute three metrics, all on the same development set
(summarized in Table 2):
•Bilingual zero-shot transfer ( BZ): This is
the standard zero-shot transfer score (Conneau
and Lample, 2019) which measures how well
a bilingual model ﬁne-tuned on D zero-
shot transfers to the other language ( D).
•Bilingual supervised synthetic ( BS): This is
the supervised learning performance on the
derived language obtained by ﬁne-tuning Bi-
LMonD and evaluating it on D.
•Monolingual zero-shot transfer ( MZ): This
measures the zero-shot performance of the
baseline Mono-LM , which is not pre-trained
on the derived language, by ﬁne-tuning Mono-
LMonD and evaluating it on D.
BSuses ﬁne-tuning train data from the derived lan-
guage and serves as an upper-bound on BZandMZ3613which don’t use it. MZdoesn’t pre-train on the de-
rived language and serves as a lower-bound on BZ
which does pre-train on it. For easier comparison
ofBZandMZwith BS(upper-bound), we report
the following score differences (Table 2), which
are both negative in our experiments.
∆= (BZ−BS) (1)
∆= (MZ−BS) (2)
BZalone cannot capture the quality of the zero-
shot transfer. A large and negative ∆im-
plies that bilingual zero-shot transfer is much worse
than supervised ﬁne-tuning on derived . Concur-
rently, ∆≈∆implies that Bi-LM
transfers as poorly as Mono-LM .Thus, good zero-
shot transfer is characterized by ∆≈0
and∆/greatermuch∆.
3.4 Experimental Setup
Languages We choose four diverse natural lan-
guages: English (Indo-European, Germanic),
French (Indo-European, Romance), Hindi (Indo-
European, Indo-Iranian), and Arabic (Afro-Asiatic,
Semitic), which are represented in the multilingual
XTREME benchmark (Hu et al., 2020). For each
language, we consider four transformations (Sec-
tion 3.2) to create derived counterparts, giving us
16different original-derived pairs in total. For the
Syntax transformation, we use Qi et al. (2020) for
parsing. We modify the syntax of,, and
to that of, and the syntax ofto that of.
Datasets For the pre-training corpus ( C), we
use a 500MB (uncompressed) subset of Wikipedia
(≈100M tokens) for each language. This matches
the size of WikiText-103 (Merity et al., 2016), a
standard language-modeling dataset. For down-
stream evaluation, we choose four tasks from the
XTREME benchmark (Hu et al., 2020). Table 4
lists all the datasets and their evaluation metrics.
Implementation Details We use a variant of
RoBERTa (Liu et al., 2019) which has 8layers,
8heads, and a hidden dimensionality of 512. We
train each model on 500K steps, a batch size of 128,
and a learning rate of 1e-4with a linear warmup
of10K steps. We use an original language vocab-
ulary size of 40000 for all the models and train
on8Cloud TPU v3 cores for 32-48hours. For
ﬁne-tuning, we use standard hyperparameters (Ap-
pendix F) from the XTREME benchmark and re-
port our scores on the development sets.4 Results
Our experiments reveal several interesting ﬁndings
for bilingual models including the situational im-
portance of sub-word overlap for zero-shot transfer
(§ 4.1, 4.2), the effect of domain mismatch between
languages (§ 4.3), and correlation of zero-shot per-
formance with embedding alignment (§ 4.4). We
connect our ﬁndings to zero-shot transfer results
between natural languages in Section 4.5.
4.1 Sub-word overlap is not strictly necessary
for strong zero-shot transfer
Sub-word overlap is the number of common tokens
between two different language corpora. If Eand
Eare sets of tokens which appear in the two cor-
pora, then: Sub-word overlap =|E∩E|/|E∪
E|(Pires et al., 2019). The Transliteration transfor-
mation (T ) creates original -derived language
pairs that have 0%sub-word overlap (equivalently,
different scripts), but follow the same word order.
Table 3 displays ∆scores forT , av-
eraged over four languages (Appendix B contains
a breakdown). We observe that ∆≈0for
all tasks while ∆is highly negative, im-
plying that zero-shot transfer is strong and on par
with supervised learning. This result indicates that
zero-shot transfer is possible even when languages
with different scripts have similar word orders (in
line with K et al. (2020)). However, it is unrealistic
for natural languages to differ only in their script
and not other properties (e.g., word order).
4.2 Absence of sub-word overlap signiﬁcantly
hurts zero-shot performance when
languages differ in their word-orders
To simulate a more realistic scenario, we create
original andderived language pairs which differ
both in their scripts ( 0%sub-word overlap) and in
word order. We achieve this by composing two
transformations on the original language corpus,
one of which is Transliteration (T ). We exper-
iment with three different compositions, (a) T
◦T, (b)T◦T , and (c)T◦T.
Here,α◦βmeans that transformation βis applied
before α. A composed transformation ( T◦3614
β) differs from its second constituent ( β) in that
the former produces a derived language which has
0%sub-word overlap with the original language
whereas the latter has a 100% sub-word overlap.
Results Our results (Figure 2, breakdown in Ap-
pendix C) show that zero-shot performance is sig-
niﬁcantly hurt for composed transformations when
compared to its constituents. |∆|is much
larger forT◦Twhen compared to T
orTindividually. For example, for XNLI,
|∆|= 19 for the composed transformation
and just 2and3forT andTindividually.
T◦T is worse by≈20points on XNLI
and NER, and over 40points on POS when com-
pared toT .T◦Tsuffers lesser than
the other two composed transformations, but it is
still worse thanTby3,6, and 1point on XNLI,
NER, and POS. In conclusion, the absence of sub-
word overlap signiﬁcantly degrades zero-shot per-formance in the realistic case of languages with
different word orders.
4.3 Data from the same domain boosts
bilingual performance
Previously, we considered transformations ( T) that
modiﬁed the original pre-training corpus to get a
parallel corpus,C =T(C), such that there
is a one-to-one correspondence between sentences
inCandC (we call this setting parallel ).
Since procuring large parallel corpora is expensive
in practice, we consider two other settings which
use different corpora for original andderived .
Setup Consider two text corpora of the same size,
CandC. We compare two settings: (1)
Theparallel setting pre-trains a bilingual model
onC+T(C), whereas the (2) non-parallel
corpus setting uses C+T(C). We con-
sider two variants of non-parallel , (1)non-parallel
(same) which uses different splits of Wikipedia
data (hence, same domain), and (2) non-parallel
(diff) which uses Wikipedia data for the original
and common crawl data (web text) for the derived
language (hence, different domain). We use the
Transliteration transformation (T ) to generate
thederived language corpus and report |∆|
averaged over all languages in Figure 3.
Results We observe consistently on all tasks that
theparallel setting (blue bar) performs better than
both the non-parallel settings. Non-parallel (same)3615
performs better than non-parallel (diff) , with gains
ranging between 2points on XQuAD to 17points
on NER. This result shows that even for original
andderived language pairs which differ only in
their script, having parallel pre-training corpora
leads to the best zero-shot transfer. Since large-
scale parallel unsupervised data is hard to procure,
the best alternative is to use corpora from simi-
lar domains (Wikipedia) rather than different ones
(Wikipedia v.s. web text).
4.4 Zero-shot performance is strongly
correlated with embedding alignment
Our previous results ( §4.2, 4.3) showed cases
where zero-shot transfer between languages is poor
when there is no sub-word overlap. To investigate
this further, we analyze the static word embeddings
learned by bilingual models and ﬁnd that zero-shot
transfer between languages is strongly correlated
with the alignment between word embeddings for
theoriginal andderived languages.
Setup The original and the derived languages
have a one-to-one correspondence between their
sub-word vocabularies when we use transliteration
(T ). For a token embedding in the original -
language embedding matrix, its alignment score is
100% if it retrieves the corresponding token em-
bedding in the derived language when a nearest-
neighbor search is performed, and 0%otherwise.We average the alignment score over all the tokens
and call it alignment .
Results We measure the alignment of bilin-
gual models pre-trained on different original -
derived language pairs created using translitera-
tion, namely the composed transformations (§ 4.2),
parallel , and non-parallel (§ 4.3). We plot the
alignment along with the corresponding ∆
scores for XNLI in Figure 4. Results for other tasks
are in Appendix E.
We observe that higher alignment is associ-
ated with lower ∆, implying better zero-
shot transfer. Alignment is lower for composed
transformations like T◦TandT◦
T which have large and negative ∆.
Alignment also explains the results in Section 4.3,
with non-parallel variants having lower alignment
scores than parallel , which is in line with their
lower ∆. Overall, we ﬁnd a strong and
signiﬁcant Spearman’s rank correlation between
alignment and∆, withρ= 0.94,p<. 005
for XNLI,ρ= 0.93,p < . 005 for NER, and
ρ= 0.89,p<. 01for POS, indicating that increas-
ing the embedding alignment between languages
helps improve zero-shot transfer.3616
4.5 Connections to results on natural
language pairs
Effect of sub-word overlap In § 4.2, we showed
that when languages have different scripts ( 0%sub-
word overlap), zero-shot transfer signiﬁcantly de-
grades when they additionally have different word
orders. However, the zero-shot transfer is good
when languages differ only in the script and have
similar or the same word order. This is in line with
anecdotal evidence in Pires et al. (2019), where
zero-shot transfer works well between English
andBulgarian (different script but same subject-
verb-object order – SVO), but is poor between
English andJapanese (different script andword
order – SVO v.s. SOV). Our result also corrobo-
rates ﬁndings in Conneau et al. (2020b) that artiﬁ-
cially increasing sub-word overlap between natural
languages (which have different word orders) im-
proves performance (e.g., 3points on XNLI).
Effect of token embedding alignment In § 4.4,
we showed that zero-shot transfer is strongly corre-
lated with word embedding alignment between lan-
guages. This explains the usefulness of recent stud-
ies which try to improve multilingual pre-training
with the help of auxiliary objectives, which im-
prove word or sentence embedding alignment.
DICT-MLM (Chaudhary et al., 2020) and Re-
lateLM (Khemchandani et al., 2021) require the
model to predict cross-lingual synonyms as an aux-
iliary objective, thus indirectly improving word-
embedding alignment and the zero-shot perfor-
mance on multiple tasks. Hu et al. (2021) add
an auxiliary objective that implicitly improves
word embedding alignment and show that they
can achieve performance similar to larger mod-
els. Cao et al. (2019) explicitly improve contextual
word embedding alignment with the help of word-
level alignment information in machine-translated
cross-lingual sentence pairs. Since they apply this
post hoc and not during pre-training, the improve-
ment, albeit signiﬁcant, is small ( 2points on XNLI).
While these studies do not fully utilize word and
sentence embedding alignment information, our re-
sults lead us to posit that they are a step in the right
direction and that baking alignment information
more explicitly into pre-training will be beneﬁcial.36175 Conclusion
Through a systematic study of zero-shot transfer
between four diverse natural languages and their
counterparts created by modifying speciﬁc prop-
erties like the script, word order, and syntax, we
showed that (1) absence of sub-word overlap hurts
zero-shot performance when languages differ in
their word order, and (2) zero-shot performance
is strongly correlated with word embedding align-
ment between languages. Some recent studies have
implicitly or unknowingly attempted to improve
alignment and have shown slight improvements
in zero-shot transfer performance. However, our
results lead us to posit that explicitly improving
word embedding alignment during pre-training by
using either supervised (e.g., parallel sentences and
translation dictionaries) or unsupervised data will
signiﬁcantly improve zero-shot transfer. Although
real languages typically differ in more ways than
the set of properties considered in our transforma-
tions, our methodology is still useful to help isolate
crucial properties for transfer. Future work can ex-
periment with more sophisticated transformations
and investigate closer connections with human lan-
guage pairs.
Acknowledgments
This work was funded through a grant from the
Chadha Center for Global India at Princeton Uni-
versity. We thank Shunyu Yao, Vishvak Murahari,
Tianyu Gao, Sadhika Malladi, and Jens Tuyls for
reading our draft and providing valuable comments.
We also thank the Google Cloud Research program
for computational support in running our experi-
ments.
References361836193620Appendices
A Mathematical Speciﬁcation for
Transformation of Downstream
Datasets
Text classiﬁcation Text classiﬁcation tasks like
news classiﬁcation or sentiment analysis typically
have instances which contain a single sentence and
a label. Instances in other classiﬁcation tasks like
natural language inference (NLI) (Bowman et al.,
2015) contain two sentences and one label. For
such tasks, we apply the transformation ( T) on
each sentence within every instance, and leave the
annotated label as is. Therefore, for a dataset of
sizenwhich contains msentences per instance, we
have:
D={(s,...,s,y)|i= 1 :N}
D={(T(s),...,T(s),y)|i= 1 :N}
Token-classiﬁcation tasks Tasks like named-
entity recognition (NER) and part-of-speech tag-
ging (POS tagging) have labels associated with
each token in the sentence. For these datasets, we
ensure that any transformation ( T) that changes the
order of the tokens also changes the order of the
corresponding labels.
We deﬁne a few quantities to express the trans-
formation mathematically. Let s= (w,...,w)
be a sentence comprised of ktokens and y=
(y,...,y)be labels corresponding to the tokens
in the sentence. We deﬁne a new transformation
(T) which operates on the label augmented sen-
tence,s= ((w,y),..., (w,y)). Let
s[j]correspond to the jelement in the se-
quence, and s[j][word ]ands[j][label ]
correspond to the word and label of the jele-
ment. LetT(s)[j][orig ]denote the index
of thejelement in the transformed sequence
with respect to the original sequence s. Then,
the new transformation Tis such that,
T(s)[j][orig ] =T(s)[j][orig ]
Letorig_j =T(s)[j][orig ]
T(s)[j][label ] =s[orig_j ][label ]
We transform the dataset using T:
D={s|i= 1 :N}
D={T(s)|i= 1 :N}B Zero-shot transfer results for different
transformations
Table 5 in the appendix is the extended version
of Table 3 in the main paper with a breakdown
for all languages. It reports ∆,∆,
andBZfor different languages and transformations
considered.
C Composed Transformations
Table 6 in the appendix presents the breakdown
of results in Figure 2 of the main paper. It reports
∆scores for composed transformations and
their constituents.
D Comparing different sources for
original andderived language corpora
Table 8 in the appendix contains the breakdown
of results in Figure 3 of the main paper. It reports
∆for different languages on different tasks
for the settings mentioned in Section 4.3.
E Alignment Correlation
We present alignment results (Section 4.4) for all
XNLI, NER, and POS in Figure 5. We observe
strong correlations between alignment and zero-
shot transfer, with ρ= 0.94,p<. 005on XNLI,
ρ= 0.93,p<. 005on NER, and ρ= 0.89,p<
.01on POS. We present the raw scores in Table 7.
F Hyperparameters for XTREME
•XNLI: Learning rate – 2e-5 , maximum se-
quence length – 128, epochs – 5, batch size –
32.
•NER: Learning rate – 2e-5 , maximum se-
quence length – 128, epochs – 10, batch size –
32.
•POS: Learning rate – 2e-5 , maximum se-
quence length – 128, epochs – 10, batch size –
32.
•Tatoeba: Maximum sequence length – 128,
pooling strategy – representations from the
middle layer/parenleftbig/parenrightbig
of the model.
•XQuAD: Learning rate – 3e-5 , maximum
sequence length – 384, epochs – 2, document
stride – 128, warmup steps – 500, batch size –
16, weight decay – 0.0001.36213622Task LanguageXNLI NER POS XQuAD
∆∆∆∆
ParallelEnglish -1.7 -2.1 -0.5 -4
French -1.6 -3.1 -0.2 -1
Hindi -0.1 -0.5 -0.1 10.6
Arabic -0.4 -1.9 -0.8 -0.5
Avg. -1.0 -1.9 -0.4 1.3
Non-parallel (Same)English -3.8 -4.1 -0.7 -6.9
French -1 -6.3 -0.5 -0.9
Hindi -0.4 -3.1 -0.2 4.5
Arabic -2 -6.1 -1.5 0.7
Avg. -1.8 -4.9 -0.7 -0.6
Non-parallel (Diff)English -5.7 -14.3 -1.5 -9.3
French -10.9 -30.3 -10.5 -5.2
Hindi -0.5 -8.6 -1 5
Arabic -6.3 -34.7 -3.7 -1.9
Avg. -5.9 -22.0 -4.2 -2.93623