
Avi CaciularuIdo DaganJacob GoldbergerArman CohanComputer Science Department, Bar-Ilan University, Ramat-Gan, IsraelFaculty of Engineering, Bar-Ilan University, Ramat-Gan, IsraelAllen Institute for AI, Seattle, WAPaul G. Allen School of Computer Science, University of Washington, Seattle, WA
Abstract
Long-context question answering (QA) tasks
require reasoning over a long document or
multiple documents. Addressing these tasks
often beneﬁts from identifying a set of evi-
dence spans (e.g., sentences), which provide
supporting evidence for answering the ques-
tion. In this work, we propose a novel method
for equipping long-context QA models with an
additional sequence-level objective for better
identiﬁcation of the supporting evidence. We
achieve this via an additional contrastive super-
vision signal in ﬁnetuning, where the model is
encouraged to explicitly discriminate support-
ing evidence sentences from negative ones by
maximizing question-evidence similarity. The
proposed additional loss exhibits consistent
improvements on three different strong long-
context transformer models, across two chal-
lenging question answering benchmarks – Hot-
potQA and QAsper.
1 Introduction
Answering questions that require reasoning over
a long sequence, such over long documents or
multiple documents, is a challenging task (Pang
et al., 2021). Research in this domain mostly in-
cludes tasks that involve multiple text segments,
over benchmarks like HotpotQA (Yang et al., 2018)
and QAsper (Dasigi et al., 2021). HotpotQA is a
multi-hop QA benchmark over multiple paragraphs
from Wikipedia, while QAsper involves reading
comprehension from long academic papers, where
relevant information on a question could be spread
across the paper.
Given the task complexity (Choi et al., 2017),
benchmarks often provide an additional set of evi-
dence spans, such as sentences or paragraphs, for
a given question answer pair. This breaks down
the long-context QA task, adding a preliminaryevidence span detection, which is crucial for suc-
cessfully ﬁnding the correct answer, and also poten-
tially helps in model interpretability. In this work,
we propose a method for improving long-context
QA via leveraging such evidence spans, by maxi-
mizing their similarity with the question.
Since identifying the evidences provides rele-
vant information for answering the question, prior
work showed that jointly training models to per-
form evidence span extraction in addition to answer
generation is important for achieving high perfor-
mance (Yang et al., 2018; Dasigi et al., 2021). To
jointly perform evidence extraction and question
answering, models utilize sentence representations
(marker tokens) in the input; the ﬁnal layer repre-
sentation corresponding to these markers is then
passed through a classiﬁcation layer and is opti-
mized using the cross-entropy loss in conjunction
with the answer extraction/generation loss. We
conjecture and demonstrate (see Table 1) that this
objective does not sufﬁciently capture relationships
between the question and the candidate evidence
spans. Thus, we propose a complementary objec-
tive for enforcing question-evidence similarity in
the model representation (see Fig. 1). Further, we
show that optimizing the question-evidence sim-
ilarity under a certain subspace by using linear
projection of the raw representations may softly
impose information encoding about their related-
ness. Since questions may be partitioned into sev-
eral types (e.g., yes/no, generative, non answerable,
etc.) in the common QA settings, we also inves-
tigate learning separate projections per question
type.
Driven by the intuition that questions should be
similar to their supporting evidences, under some
speciﬁc geometric sub-space, we propose a novel
supervised contrastive learning objective for the
ﬁnetuning stage, aiming to maximize similarity
of question-evidence representations. Contrastive
learning has been recently applied to a variety of2872
deep learning models in computer vision (Chen
et al., 2020; Chen and He, 2021) and NLP (Gao
et al., 2021; Gunel et al., 2021). Unlike prior NLP
related methods, our proposed loss term is model-
agnostic and operates in the sequence-level in a
supervised manner. This objective targets ques-
tion and evidence representations within input se-
quences, and unlike prior work, it is not based
on individually encoding sentences or paragraphs.
We show that our additional contrastive supervi-
sion provides consistent improvements on three
different models and two datasets, demonstrating
its effectiveness and versatility.
2 Setup
In this section, we deﬁne and elaborate the common
setup and notations for long-context QA.
Assume that we are given a question qand a con-
textS=/angbracketlefts,...,s/angbracketrightconsisting of Msentences (s
can be also a document/paragraph/passage, depend-
ing on the dataset). From S, the task is to iden-
tify the correct answer aand a set ofNevidence
spansS={s,...,s}whereiare indices of
the sentences that are the supporting evidence for
answering the question q.
As common in the input setup of long-context
transformer models (Beltagy et al., 2020; Zaheer
et al., 2020; Caciularu et al., 2021), which are thecurrent state-of-the-art models for solving long-
context QA tasks, the question and context sen-
tences are concatenated in a single long sequence
with special sentence tokens specifying sentence
boundaries. Then the input is passed to the long-
context transformer, which is trained to jointly iden-
tify the evidence sentences and extract or generate
the answer.
Concretely, for each example, we prepare the
following concatenated input sequence:
[/angbracketlefts/angbracketright,q,/angbracketleft/s/angbracketright,s,/angbracketleft/s/angbracketright,s,...,/angbracketleft/s/angbracketright,s]
where “,” is the string concatenation operation, q
andsare sequences of tokens corresponding to the
question and the jth sentence in the input context,
and/angbracketlefts/angbracketrightand/angbracketleft/s/angbracketrightare special tokens representing
the question and a context sentence, respectively
(See Fig. 1 for an example).
Then, a QA loss, which we denote by Lis
applied over the contextualized representation of
each sentence token, and is optimized using super-
vision.Ldepends on the dataset and can take
the form of a multi-task objective, representing
multiple tasks in the context of QA (Dasigi et al.,
2021) (particularly evidence extraction and answer
generation).28733 Question-Evidence Contrastive Loss
In this section, we elaborate on our novel proposed
contrastive loss term, combining question-evidence
similarity maximization (§3.1), and question-type
aware projections (§3.2).
3.1 Question-Evidence Similarity
To encourage the long-context transformer model
to capture relationships between the question and
evidence sentences, we introduce an additional
sequence-level loss that compares and contrasts
the question with context sentences.
The additional proposed loss Lis based on
the InfoNCE loss (Oord et al., 2018), and is ap-
plied over instances consisting of triplet of vectors
representing the question, an evidence sentence
and distractor sentences. The loss encourages the
question and evidence representations to become
closer to each other, while pushing the question
and distracting sentences away.
Formally, the contrastive loss is deﬁned as the
sum of negative log-likelihood losses over all input
examples, where each loss term discriminates the
positive units from negative ones:
L=−log/summationdisplay
e
/summationtexte
,(1)
wheres(q) is the sentence (question) marker vec-
tor representation (see /angbracketlefts/angbracketrightand/angbracketleft/s/angbracketrightin Fig. 1),τis
the conﬁgurable temperature hyperparameter, and
sim(·)is a similarity metric. Lserves a sin-
gle example, and the ﬁnal aggregated loss Lis
obtained by averaging over all the examples.
We incorporateLinto the main QA span ex-
traction/generation loss Lusing the augmented
loss:
L= (1−λ)·L+λ·L,
whereλis a weighting hyperparameter.
The underlying sim(·)can take the form of a
non-parametric similarity function, e.g., the dot
product (sim(s,q) =sq) or the cosine similarity
(sim(s,q) =). However, we show empiri-
cally that using such similarity over the raw rep-
resentations harms the performance results of the
model, since seemingly, it is hard to ﬁnd a shared
representation that should optimize the two loss
functions. Hence, we adopted linear projections,
per question type, to cast the similarity learning
objective into proxy linear spaces.3.2 Incorporating Question-Type Projections
Long-context QA benchmarks often provide a
question-type label per instance as an additional
challenge, such as {yes, no, span} for HotpotQA.
We hypothesize that maximizing question-evidence
similarity under a question-type-speciﬁc sub-space
can enable more ﬂexibility and inductive bias to
the model, for producing better representations and
further improving the performance. Following Iter
et al. (2020), we deﬁne the following similarity
function:
sim(s,q) =sWq, (2)
wherekis the expected question type and Wis the
corresponding learnable projection matrix. Such
linear projections ensure that a speciﬁc subspace
per question type exists. We additionally incorpo-
rate different temperature hyperparameters τper
question type in Eq. 1 (see the ablation in Table 4
for their effect). The dimensions of the proposed
Wtend to be large, in accordance with the dimen-
sions of the transformer’s hidden-layers.Hence,
following Barkan et al. (2020), we apply new non-
square linear projections instead of using W:
sim(s,q) =sq
/bardbls/bardbl/bardblq/bardbl, (3)
where we set
s:=Ws, q:=Wq,
andW(orW) is the matrix that projects s(or
q) into a lower dimension, in the kquestion-type
sub-space. In order to improve the separation be-
tween the different sub-spaces induced by different
question types, we generated additional negative
instances per sentence, as follows.
We projected every question-sentence pair using
all the mappings according to the available ques-
tion types, and computed their cosine similarity
(according to Eq. 3). Then, all the obtained scores
were considered as negative, except the ones that
belong to question-evidence pairs projected using
the correct question-type mapping.
4 Evaluation and Analysis
In this section, we provide details about the experi-
ments that we conducted and their outcomes.2874
4.1 Experimental Setup
In order to demonstrate the contribution of our
method, we evaluated it over the recent QAsper
dataset (Dasigi et al., 2021) and the well-known
HotpoQA dataset (Yang et al., 2018), which share
the same setup (§2).
QAsper (Dasigi et al., 2021) is a long-document
QA dataset which was built over academic papers,
where NLP practitioners were recruited to (abstract-
edly) generate questions following the title and the
abstract of a particular paper, as well as creating the
the correct evidence and answers to those questions
based on the entire paper. More than half of the
examples in QAsper require collecting evidences
from multiple evidences in the given paper. For this
benchmark,Lrepresents the sum of the teacher-
forced text generation and evidence classiﬁcation
loss functions, in a multi-task training setup.
HotpotQA (Yang et al., 2018) introduced the task
of multihop extractive question answering, in the
reading comprehension setting, where the inputs
are a question and multiple paragraphs from vari-
ous related and non-related documents. A model
is queried to extract answer spans and evidence
sentences, where it should handle challenging ques-
tions that require ﬁnding and reasoning over mul-
tiple supporting documents. For the models we
applied to this benchmark, Lrepresents the stan-
dard cross-entropy answer extraction loss.
To test the contribution of our Lloss, We
replicated the experiments described in Dasigi et al.
(2021) and Caciularu et al. (2021) for QAsper and
HotpotQA, respectively. For QAsper, we ﬁnetuned
the LED-base model,and evaluated it on the ques-
tion answering and evidence selection tasks. For
HotpotQA, we used the Longformer model and
CDLMas the backbone long-sequence language
models for this task. Since CDLM was providedonly as a base-sized model, we pretrained a larger
version of the CDLM model, and hence used both
the base and large versions of both Longformer
and CDLM. For further details see Appendix B.1
and B.2.
For both benchmarks, we performed a grid
search for determining the hyper-parameters of the
contrastive loss (more details in Appendix C).
4.2 Results and Analysis
Qustion-Evidence Similarity Analysis. As a
preliminary assessment, we ﬁrst investigate the
question and evidences representations of models
trained on the HopotQA dataset. We motivate the
use of our method by presenting the mean Average
Precision (mAP) ranking results produced accord-
ing to the question-sentence cosine similarities for
the marker tokens trained representations of the
CDLM-large model. From Table 1, we observe
that without applying our additional loss term, the
question representations are distant from the evi-
dence representations. Using a single learned pro-
jection increases this desirable similarity, and using
a learned projection per question type yields the
highest mAP scores. Hence, integrating question-
type aware linear projections can be a beneﬁcial
part of our contrastive loss, and overall it further
improves the QA results as we show next. An addi-
tional illustration of this effect, where we visualize
the marker representations, appears in Appendix A.
Main results. We adopted the F1 evaluation met-
rics corresponding to the original works (Dasigi
et al., 2021; Beltagy et al., 2020). Tables 2 and
3 present the evaluation results over the QAsper
and HotpoQA datasets, respectively. We show the
performance difference when adding our additional
loss term with “+L” (and question-type similar-
ity function from Eq. 3).
As shown in the table, the addition of Lex-
hibits the best performance across all examined
models and benchmarks, clearly demonstrating its
consistent advantage. Note that maximizing the
question-evidence similarity resulted also in evi-
dence detection improvement – see the “Evidence”
and the “Sup” metrics in Tables 2 and 3, respec-
tively. All the results are statistically signiﬁcant
using the bootstrap test with p<0.01(Dror et al.,
2018).
Ablations. Table 4 demonstrates ablation study
results for evaluating the effectiveness of our design2875
decisions. Using a constant temperature parameter
for all question types, as well as using different
degenerated similarity functions, exhibits lower
performance. Further, the last row in Table 4 shows
that treating correct answers that are projected to
the wrong question type as negatives also improves
the results. Overall, the ablation study shows the
advantage of using Eq. 3 as a similarity function
that provides ﬁne-grained expressive modeling for
each question type, in its own sub-space.
Discussion. An additional theoretical justiﬁcation
to our contrastive learning is provided in (Gao et al.,
2021), where we can imply that our loss term im-
proves the uniformity and therefore the expressive-
ness of the question and evidence representations.
Moreover, one can attribute the success of our con-
trastive loss to the fact that long-range transformer
models lack long-range signals during pretraining,and hence such explicit modeling as we suggest
is necessary. In fact, comparing CDLM’s results
to the Longformer illustrates that our cotrastive
term has higher impact on models without global
attention-based pretraining.
5 Conclusion
In this work, we proposed an effective sequence-
level contrastive loss for improving the perfor-
mance of long-range transformers in solving QA
tasks that require reasoning over long contexts. We
demonstrate consistent improvement when using
our approach on three different models over two
different benchmarks. For future work, we pro-
pose exploring variations of our proposed super-
vised loss on other long-context tasks, such as long-
document and multi-document summarization, and
integrating our method into information retrieval
re-ranker models.
Acknowledgments
We thank the BIU-NLP lab and the Semantic
Scholar research team at AI2 for fruitful discus-
sions and helpful feedback. The work described
herein was supported by the PBC fellowship for
outstanding PhD candidates in data science, in part
by by the Israel Science Foundation grant 2827/21,
and by a grant from the Israel Ministry of Science
and Technology.
Ethical Considerations
Our work in understanding the role of maximizing
the similarity between question and evidence pairs.
Therefore, there is a limited risk associated with
the quality of annotated evidence sentences in the
dataset, as there is no guarantee that our model
will always generate non-biased and factual con-
tent. Therefore, caution must be exercised when the
model is deployed in practical settings, where the
evidence quality is vague and cannot be veriﬁed.2876References2877Appendix
A Question-Evidence Similarity
Demonstration
In this section, we further interpret the outcome of
our contrastive loss.
We apply PCA over the relevant normalized to-
ken representations of the validation data of Hot-
potQA (i.e., the question and answers represen-
tations in Fig. 1), and depicted them in 2. The
projected representations of the correct and wrong
answers are equally distributed at the beginning
of the training (left ﬁgure). After several epochs
when the model converged (right ﬁgure), the an-
swer representations’ manifold got closer to the
questions’ representations (in terms of radial dis-
tance). Each beam in the ﬁgure corresponds to a
different question type (there are 3 in HotpotQA).
The correct evidence representations (green dots)
are the closest among the whole answer represen-
tations, conﬁrming that our additional contrastive
loss term generalizes and maximizes the question-
evidence similarity.
B Datasets and Finetuning Details
In this section, we provide details, regrading ﬁne-
tuning and hyper-parameter conﬁguration, over the
benchmarks we used during our experiments.
B.1 QAsper
Since some of the questions included in QAsper
are not answerable, we apply our contrastive loss
only over examples that are answerable and contain
at least one evidence sentence.
We train all models using the Adam opti-
mizer (Kingma and Ba, 2014) and a triangular
learning rate scheduler (Howard and Ruder, 2018)
with 10% warmup. To determine number of epochs,
peak learning rate, and batch size, we performed
manual hyperparameter search on a subset of the
training data. We searched over {1, 3, 5} epochs
with learning rates { 1e,3e,5e,9e}, and
found that smaller batch sizes generally work better
than larger ones. Our ﬁnal conﬁguration was 10
epochs, peak learning rate of 5e, and batch size
of 2, which we used for all reported experimental
settings. When handling full text, we use gradient
checkpointing (Chen et al., 2016) to reduce mem-
ory consumption. We run our experiments on a
single RTX 8000 GPU, and each experiment takes
30–60 minutes per epoch.B.2 HotpotQA
We used the HotpotQA-distractor dataset (Yang
et al., 2018). Each example in the dataset is in-
cludes a question and 10 paragraphs from differ-
ent documents, extracted from Wikipedia. Two
gold paragraphs include the relevant information
for properly answering the question, mixed and
shufﬂed with eight distractor paragraphs (for the
full dataset statistics, see Yang et al. (2018)). There
are two goals for this task: detecting the supporting
facts, i.e., evidence sentences, as well as extraction
of the correct answer span.
For preparing the data for training and eval-
uation, we follow the same ﬁnetuning scheme
of the CDLM (Caciularu et al., 2021) and
the Longformer (Beltagy et al., 2020); For
each example, we concatenate the question
and all the 10 paragraphs in one long con-
text. We particularly use the following input
format with special tokens and our document
separators: “ [CLS] [q] question [/q]
/angbracketleftdoc-s/angbracketright/angbracketleftt/angbracketrighttitle/angbracketleft/t/angbracketright/angbracketlefts/angbracketrightsent/angbracketleft/s/angbracketright
/angbracketlefts/angbracketrightsent/angbracketleft/s/angbracketright /angbracketleft/doc-s/angbracketright.../angbracketleftt/angbracketright
/angbracketleftdoc-s/angbracketrighttitle/angbracketleft/t/angbracketrightsent/angbracketleft/s/angbracketright /angbracketlefts/angbracketright
sent/angbracketleft/s/angbracketright/angbracketlefts/angbracketright...” where [q],[/q] ,/angbracketleftt/angbracketright,
/angbracketleft/t/angbracketright,/angbracketlefts/angbracketright,/angbracketleft/s/angbracketright,[p] are special tokens represent-
ing, question start and end, paragraph title start
and end, and sentence start and end, respectively.
The new special tokens were added to the models
vocabulary and randomly initialized before task
ﬁnetuning. We use global attention to question
tokens, paragraph title start tokens as well as sen-
tence tokens. The model’s structure is taken from
Beltagy et al. (2020).
As in Beltagy et al. (2020); Caciularu et al.
(2021), we ﬁnetune our models for 5 epochs, us-
ing a batch size of 32, learning rate of 1e, 100
warmup steps. Finetuning on our models took
∼6 hours per epoch, using four 48GB RTX8000
GPUs for ﬁnetuning our models. For generating the
CDLM-large results, we pretrined our version us-
ing the code from .
C Contrastive Loss Details
In this section, we provide the details for reproduc-
ing our contrastive term, which is relevant for both
QAsper and HotpotQA.
We searched over d×{d,,,}to determine
the linear projections’ dimensions, where dis the
model’s hidden layer representation dimension (it2878
depends on the size of the model). In order to
determine the temperature hyperparameter τ, we
searched over{0.2,0.4,0.6,0.8,1.0}per question
type (if applicable). We also applied dropout
with a rate of p= 0.1over the linear projections,
which consistently improved the results over all
the benchmarks. Finally, we searched for the best
performing λhyperparameter over the values of
{0.2,0.4,0.6,0.8,1.0}.2879