
Yiyang LiandHai ZhaoDepartment of Computer Science and Engineering, Shanghai Jiao Tong UniversityKey Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University
eric-lee@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn
Abstract
Dialogue response generation requires an agent
to generate a response according to the cur-
rent dialogue history, in terms of which two-
party dialogues have been well studied, but
leaving a great gap for multi-party dialogues
at the same time. Different from two-party di-
alogues where each response is a direct reply
to its previous utterance, the addressee of a re-
sponse utterance should be specified before it is
generated in the multi-party scenario. Thanks
to the huge amount of two-party conversational
data, various pre-trained language models for
two-party dialogue response generation have
been proposed. However, due to the lack of
annotated addressee labels in multi-party dia-
logue datasets, it is hard to use them to pre-train
a response generation model for multi-party di-
alogues. To tackle this obstacle, we propose
an Expectation-Maximization (EM) approach
that iteratively performs the expectation steps
to generate addressee labels, and the maximiza-
tion steps to optimize a response generation
model. Theoretical analyses and extensive ex-
periments have justified the feasibility and ef-
fectiveness of our proposed method. The offi-
cial implementation of this paper is available at
https://github.com/EricLee8/MPDRG .
1 Introduction
Inspired by the tremendous success in pre-training
large language models (PLMs) in general domains
(Devlin et al., 2019; Clark et al., 2020; Radford
et al., 2018), efforts have been made to train PLMs
for dialogue response generation (Zhang et al.,
2020; Bao et al., 2020; Chen et al., 2022). However,
they constrain the dialogues to be either two-party,
or sequential structured (i.e. each utterance replies
directly to its previous utterance). Different from
them, a multi-party dialogue can involve multiple
interlocutors, where each interlocutor can reply toany preceding utterances, making the response re-
lations of the dialogue tree-structured and much
more complicated (Zhang et al., 2018; Le et al.,
2019; Shi and Huang, 2019; Wang et al., 2020).
Besides, the speaker and addressee of a response
utterance should be specified before it is generated
in multi-party scenario, making the annotated data
for multi-party dialogue response generation (MP-
DRG) less available.
Figure 1 illustrates an example of MPDRG task
taken from the Ubuntu IRC benchmark (Hu et al.,
2019). The upper part shows the tree-structured ad-
dressee relations of the dialogue, where the arrows
point from addressees to speakers, and different
colors represent different interlocutors. The middle
part displays the content of the dialogue history,
where Uis the response to be generated. The ad-
dressee ( U) and the speaker (#4) of it are given,
and the content of this response is the target of our
model. The lower part gives the human response,
which is also called the ground truth reference.
Previous works on MPDRG fine-tune genera-
tive PLMs on small multi-party dialogue datasets
with explicit addressee annotations. They utilize
the response annotations to form a tree-structured
response graph, then encode the dialogue history
using either homogeneous or heterogeneous Graph
Neural Networks (GNNs) (Hu et al., 2019; Gu et al.,
2022). Nevertheless, none of them make attempts
to pre-train a response generation model for multi-
party dialogues due to the lack of large-scale cor-
pora with annotated addressee labels.
To solve the aforementioned problem of data
scarcity, we propose an EM approach that itera-
tively performs the expectation steps to generate
addressee labels, and the maximization steps to
optimize a response generation model. Specifi-
cally, we treat the addressee of each utterance in
the dialogue history as a discrete latent variable
z. During the E-steps, given the current dialogue
history cand the the response utterance r, we92model the distribution of the current addressee zas
p(z|c, r;θ), where θis the current model param-
eters. During the M-steps, we sample (c, r, z)
triplets from distribution p(z|c, r;θ)and opti-
mize the generative model p(r|c, z;θ)on these
samples. With the iteration number increasing, the
accuracy of latent variable prediction and the qual-
ity of generated responses will grow together. It is
worth noting that during these iterations, annotated
addressee labels are not required, which makes it
possible to leverage the huge amount of multi-party
dialogue corpora without addressee labels. We pro-
vide theoretical analyses to prove the feasibility of
our EM method, and conduct experiments on the
Ubuntu IRC benchmark, which is used in previous
works (Hu et al., 2019; Gu et al., 2022).
The contributions of our work can be summa-
rized as the following three folds:
•To the best of our knowledge, we are the first to
study the pre-training of multi-party dialogue re-
sponse generation, which is much more challeng-
ing and complicated than two-party dialogues.
•We put forward an EM approach to alleviate the
scarcity of multi-party dialogue data with ad-
dressee labels, making it possible to pre-train
a model with huge amount of unlabeled corpora.
•We provide theoretical analyses to prove the fea-
sibility of our EM pre-training method, and ex-
perimental results on the Ubuntu IRC benchmark
show our pre-trained model achieves state-of-the-
art performance compared with previous works.
2 Related Works
2.1 Pre-training for Response Generation
In recent years, researchers have gradually drawn
their attention from retrieval-based dialogue sys-
tems to generation-based ones. Thanks to the huge
amount of two-party dialogue corpora, various
PLMs for two-party dialogue response generation
have been proposed.
Zhang et al. (2020) propose DialoGPT, which
utilizes the sequential response chains in the Red-
dit Corpus to pre-train an auto-regressive response
generation model based on the architecture of GPT
(Radford et al., 2018). Different from their work,
which focuses on sequential dialogue history, our
work aims to solve the case where the agent can re-
spond to any previous utterance in a tree-structured
dialogue history.
Bao et al. (2020) propose PLATO, which mod-
els the conversational intents as Kdiscrete latent
variables, then utilizes response selection, bag-of-
words prediction, and language modeling objec-
tives to train the model. DialogVED (Chen et al.,
2022) further extends the discrete latent variables
to continuous ones, and models them with a multi-
variable Gaussian distribution. It utilizes KL di-
vergence reduction to optimize the parameters of
the latent distribution and applies masked language
modeling, response generation, and bag-of-words
prediction to train the whole model. PLATO and
DialogVED focus on two-party conversations, and
the conversational intents they put forward have
no corresponding concepts of actual entities (e.g.,
intent to argue, intent to end a conversation, and so
on). Distinct from their works, we lay emphasis on
multi-party dialogues, and the latent variables of
our method have actual meanings: variable z=j
indicates that the addressee of the response at the
tturn is the jutterance.
2.2 Multi-party Dialog Response Generation
Several previous works have studied the MPDRG
task. Hu et al. (2019) extract a subset of the
Ubuntu Dialogue Corpus (Lowe et al., 2015) with
explicit addressee labels to construct the Ubuntu
IRC benchmark, where they propose a Graph Struc-
tured Neural Network (GSN) for dialogue model-
ing. Specifically, they first treat each utterance93
of a dialogue as a node, and the addressee rela-
tions as edges to construct a dialogue graph, then
make use of GNNs to encode the dialogue his-
tory. Finally, they adopt a Gated Recurrent Unit
(GRU) with cross attention as the decoder to gen-
erate responses. Gu et al. (2022) put forward Het-
erMPC, which models the dialogue history as a
heterogeneous graph. In detail, they first design six
types of edges: reply and replied-by, address and
addressed-by, speak and spoken-by, among two
kinds of nodes: interlocutor nodes and utterance
nodes, and then encode the dialogue history using
Transformers (Vaswani et al., 2017) together with
heterogeneous GNNs. Finally, they utilize a Trans-
former Decoder to generate responses. Instead of
fine-tuning models on a small dataset with anno-
tated addressee labels as these existing work did,
our work focuses on the utilization of large unla-
beled corpora to pre-train a response generation
model for multi-party dialogues.
3 Methodology
To design a model for multi-party dialogue re-
sponse generation and make it compatible with
the EM training algorithm, there are two impor-
tant things to consider: how to model p(r|c, z;θ)
in the maximization step, and how to compute
p(z|c, r;θ)in the expectation step. In this sec-
tion, we will first address these two problems, then
mathematically derive the feasibility of our EM
pre-training algorithm.
3.1 Task Formulation
Given an input sequence of the dialogue history
and the speaker of the response at time step t,X=
{S: U[SEP]S: U[SEP] . . .S: U[SEP]S:},
together with the addressee of the response z=j,
our goal is to train a model that can generatean response Y= U. Here each Sis the
name of the speaker at time step i, which is
represented as Speaker # Slike those in Figure 1.
U={w, w, . . . , w}is the content of the i
utterance with nwords. z=jrepresents that S
speaks to S, who utters U, and [SEP] is a special
token that indicates the end of a dialogue turn.
3.2 Addressee Modeling
In this section, we answer the first question: how
to model p(r|c, z;θ), or in other words, how to
incorporate the addressee information z=jinto
the process of generating a response r. We de-
sign a straightforward method that adds addressee
embeddings to the positional encodings and word
embeddings, before they are further encoded by
a PLM. The left part of Figure 2 illustrates this
method, where we use an embedding look-up table
with 2entries to indicate whether a word belongs
to the addressee utterance or not. Specifically, if
a word is in the addressee utterance, it will get
its addressee embedding from entry 1, otherwise
from entry 0. Since addressee modeling is not the
key contribution of this work, we just adopt the
most straightforward and effective way. In our ex-
periments, we use BART (Lewis et al., 2020) as
the backbone PLM, following previous works (Gu
et al., 2022). Due to the page limit, the proverbial
architecture of Transformer and BART are omitted
here.
3.3 Latent Variable Prediction
In this section, we answer the second question: how
to compute p(z|c, r;θ)in the expectation step, or
in other words, how to predict the distribution of the
unlabeled addressee z, given the current dialogue
context c, response r, under parameters θ. The
solution to this question is essentially the most94important part of our method since it delicately
solves the problem of data scarcity in MPDRG.
Let’s consider what humans will do to participate
in a multi-party conversation. First, we will read
the dialogue history c, then choose an addressee
zto reply. Once candzare determined, we will
utter a response according to the content of the
whole dialogue and the addressee utterance. The
right part of Figure 2 gives the Bayesian Network
of the above process, where the joint distribution
of(c, z, r)can be factorized as:
p(c, z, r ) =p(c)·p(z|c)·p(r|c, z) (1)
Here we omit the subscript tand model parameters
θfor simplicity. Given Eq. (1), p(z|c, r;θ)can be
derived as:
p(z|c, r) =p(c, z, r )
p(c, r)
=p(c)·p(z|c)·p(r|c, z)
p(c)·p(r|c)
=p(z|c)·p(r|c, z)
p(r|c)(2)
We assume that the probability of choosing any
previous utterance as the addressee is the same
given the current dialogue history, which means
p(z|c)obeys a uniform distribution. Meanwhile,
the denominator p(r|c)is independent of z, leaving
only the term p(r|c, z). Now, we can induce that:
p(z|c, r)∝p(r|c, z) (3)
Therefore, for each z, i= 1,2, . . . , t −1, we have:
p(z|c, r) =p(r|c, z)/summationtextp(r|c, z)(4)
In practice, we can use the generative model
p(r|c, z;θ)to compute the probability distribu-
tion of p(z|c, r;θ)by Eq. (4).
3.4 Expectation-Maximization Process
Figure 3 illustrates the overview of our EM training
process. During the E-steps, we compute the prob-
ability distribution of the latent variable (the ad-
dressee z). During the M-steps, we sample (c, r, z )
triplets from this distribution and optimize the gen-
erative model by standard training algorithms.
The Expectation Step is to compute the condi-
tional distribution of the latent variable z, given
the observed data (c, r)and the current model
parameters θ, where Eq. (4) gives a reasonable ap-
proximation of this value. Specifically, for a sample
(c, r), with the model parameters θfixed, we first
calculate the un-normalized probability of each
of the i(i < t ) utterance being the addressee:
p(r|c, z;θ)using Eq. (3), then normalize them
to get the conditional distribution of zusing Eq.
(4). Once P(z|c, r;θ)is obtained, we sample
(c, r, z)triplets from this distribution, which is
further used in the maximization step.
The Maximization Step is analogical to the
normal training process. Given the sampled
{(c, r, z)}triplets, where Nis the total
number of samples, our goal is to minimize the
auto-regressive language modeling loss:
L=−/summationdisplay/summationdisplaylogp/parenleftig
w|w, c, z;θ/parenrightig
(5)
where wis the iword in the response of the k
sample: r={w}, andnis the length of this
response.
Compared with the vanilla EM algorithm , there
are several differences in our implementations.
First of all, we do not use the initial model to
generate the training data for the first round of
the maximization step. Instead, we utilize the dis-
course parser provided by Shi and Huang (2019)
to predict the addressee of each utterance in the
unlabeled corpus to get a coarse initial training
dataset. The reason for this initialization method
is that the initialization of training data (or model
parameters) is vital to the EM method, which helps
it converge to a better point. Second, rather than
sampling zfrom its conditional distribution, we
adopt a hard EM approach which takes the value
zwith highest probability as the predicted label,
where i= arg maxp(z|c, r;θ). This hard EM95approach is proved as more effective to boost the
performance (Min et al., 2019). Finally, to ensure
the quality of the generated training data in the max-
imization step, we set a hyper-parameter α∈[0,1]
to control the proportion of training data that is
actually used. Specifically, we first rank the predic-
tion confidence of each zaccording to the value
ofp(z|c, r;θ), then pick the top α×Nsamples
with the highest confidence scores. In our experi-
ments, αis dynamically set to ensure the addressee
prediction accuracy of the selected samples is over
80% in an annotated validation set.
3.5 Proof of Feasibility
In a multi-party dialogue corpus without annotated
addressee labels, a usual solution to train a response
generation model is to maximize the marginal log-
likelihood (or incomplete log-likelihood) over all
possible addressees:
ℓ(c, r;θ) = log p(r |c;θ) = log/summationdisplayp(r,z|c;θ)
(6)
However, this objective is hard to optimize since
the distribution of zis hard to obtain. Here, we
define an expected complete log-likelihood where
our estimation of p(z|c, r;θ)can come to rescue:
ˆℓ(c, r;θ) =q(z)/summationdisplaylog p(r ,z|c;θ)
q(z) =p(z|c, r;θ)(7)
Our new objective now becomes maximizing the
expected complete log-likelihood. The relation
between ℓandˆℓcan be derived as follows:
ℓ(c, r;θ) = log/summationdisplayp(r,z|c;θ)
= log/summationdisplayq(z)·p(r,z|c;θ)
q(z)
≥/summationdisplayq(z)·logp(r,z|c;θ)
q(z)
=/summationdisplayq(z)·log p(r ,z|c;θ)
−/summationdisplayq(z)·log q(z)
=ˆℓ(c, r;θ) +H(8)
where the third line is derived from the Jensen
Inequality , andHis the entropy of the distri-
bution of z. Since H≥0, we can derive that
ˆℓ(c, r;θ)≤ℓ(c, r;θ), which means ˆℓis the lowerbound of ℓ. By maximizing the lower bound ˆℓ, we
can indirectly maximize ℓ, which is originally hard
to optimize. Another important observation is hat
ˆℓ=ℓif and only if q(z) =p(z|c, r;θ), which is
exactly what we calculate during the E-steps in Eq.
(7). Though the derivation of the posterior distri-
bution of zis not exact since we assume uniform
prior in Eq. (2), it is still much closer to the real
distribution compared to random q(z).
It is worth noting that the global optimal point is
not guaranteed to be reached by this algorithm, and
it depends heavily on the initialization of model
parameters or the training data for the first round
of the maximization step. This explains the reason
why we utilize a discourse parser to get a coarse ini-
tial training dataset instead of using the expectation
step at the first iteration in Section 3.4.
4 Experiments
In this section, we first introduce the datasets to
pre-train and evaluate our model, then present the
experimental results and comparisons with previ-
ous methods.
4.1 Datasets and Experimental Setups
For pre-training, we adopt the second version
of Ubuntu Dialogue Corpus (Lowe et al., 2015),
which contains no annotated addressee labels. The
original dataset contains 1M dialogues for training,
and0.5M dialogues for validation and testing, re-
spectively. Dialogues that contain less than 4turns,
or have overlap with the dataset for the downstream
task (the Ubuntu IRC benchmark, Hu et al. 2019),
are excluded from the pre-training data. After fil-
tering, we eventually get a pre-training dataset that
contains 764,373 dialogues.
For fine-tuning, we follow previous works (Hu
et al., 2019; Gu et al., 2022) to adopt the Ubuntu
IRC benchmark, which is constructed by extracting
all utterances with response addressees indicated
by the “@" symbol in the Ubuntu Dialogue Corpus.
In total, this dataset consists of 311,725 dialogues
for training, and 5,000 dialogues for validation and
testing, respectively. It is worth noting that this
dataset contains addressee labels for every single
utterance in the dialogue history, which are utilized
by previous methods, yet not by ours.
For both pre-training and fine-tuning, BART
(Lewis et al., 2020) is used as the backbone model.
Before pre-training, we initialize the pre-trained
weights from BART-base. During the process of96
pre-training, we evaluate our model on the valida-
tion set of the Ubuntu IRC benchmark, and the best
checkpoint is saved for the fine-tuning process.
4.2 Baseline Models and Evaluation Metrics
Table 1 shows the results of our method and previ-
ous models, where GPT-2, GSN, and HeterMPC
(Radford et al., 2018; Hu et al., 2019; Gu et al.,
2022) are introduced in section 2.1 and 2.2, re-
spectively. BART is a sequence-to-sequence model
with encoder-decoder Transformer architecture and
is trained using denoising objectives. Following Hu
et al. (2019), we also adopt BLEU-1 to BLEU-4,
METEOR, and ROUGE-L as the automatic eval-
uation metrics, which can be calculated using the
pycocoevalcap package. Besides automatic evalua-
tion, human evaluation is also conducted and will
be introduced in Section 4.4.
4.3 Automatic Evaluation Results
Let’s firstly focus on the upper and middle part of
Table 1, where we present the results of previous
models and our methods. Three settings of our
method based on BART are experimented with:
pre-training only (PO), fine-tuning only (FO), and
pre-training-fine-tuning (PF). Results of PO are
obtained by directly using the pre-trained model
to generate the response for each dialogue. FO
means the checkpoint of BART is directly fine-
tuned on the Ubuntu IRC benchmark without pre-
training. PF follows a pre-training-fine-tuning
paradigm, where the best checkpoint of the pre-
training process is further fine-tuned on the down-
stream dataset.
Three observations can be seen from the ta-
ble. First of all, solely pre-training with our pro-
posed EM method with unlabeled corpus is already
able to achieve comparable results with the pre-
vious state-of-the-art (SOTA) models. It is sur-
prising since the pre-training requires no anno-
tated addressee labels, while previous models not
merely utilize the addressee information of the
response utterance, but also make use of the ad-
dressee labels of the dialogue history to form a
response graph. Second, fine-tuning our model on
the downstream dataset with the ground truth ad-
dressee labels yields better results compared with
pre-training only. Since it uses the ground truth
addressee labels of responses, the results of it can
be regarded as an upper bound of what the EM
training can achieve. Besides, FO outperforms
the previous SOTA model by large margins with
even simpler architecture and fewer annotations
(without addressee labels in the dialogue history),
demonstrating the effectiveness of our proposed ad-
dressee embeddings. Finally, by further fine-tuning
the pre-trained checkpoint with the ground truth
addressee labels, we achieve the best performance
on all metrics, which shows the transferability of
our pre-trained model.
4.4 Human Evaluation Results
For human evaluation, we recruit a team with 8
members who have at least a Bachelor’s degree in97Computer Science and are familiar with Ubuntu
and Linux. We randomly sample 100examples
from the testing set, then ask the team members to
score each prediction and select the best one. The
quality scores are considered in terms of three in-
dependent aspects: 1) relevance, 2) fluency and
3) informativeness. They are scored from 0-3
and the average values were reported. The eval-
uation results are shown in Table 2, where our
model (Pre-training + Fine-tuning) constantly out-
performs vanilla BART and the previous SOTA
model HeterMPC . We also report the Fleiss’s
Kappa to indicate the agreement between annota-
tors. Besides, the ratio of our predictions being
the best response is the same as that of human
responses, demonstrating the high quality of the
generated responses of our model.
5 Analysis
In order to get more insights into the proposed
EM pre-training method, we dive deeper into it by
conducting extensive analyses.
5.1 Ablation Study
We conduct ablation studies to investigate the con-
tribution of our different designs, whose results are
tabulated in the lower part of Table 1.
Firstly, let’s focus on the first line of the lower
part. To study whether other utterances that are
not in the reply chain of the current addressee can
help to generate a better response, we extract the
reply train by traversing from the current leave
utterance (the response) up to the root node (the
first utterance), then train a model by inputting
this chain only. We see a large performance drop
on all metrics in this setting, demonstrating the
significance of the side information provided by
the whole context.
Second, let’s pay attention to the second and
third lines of the lower part. In order to study the
effect of the EM pre-training process, which is the
key contribution of our work, we remove this pro-
cess and pre-train a model using only the addressee
labels obtained from the discourse parser (i.e. the
initial training data used in the first iteration of
our EM approach). A sharp performance drop is
observed compared with PO and PF with our pro-
posed EM pre-training strategy, demonstrating the
significance of our design. Without the iterative
EM procedure, the noisy addressee labels obtained
from the discourse parser can cause error propaga-
tion, which makes the model learn noisy features
to predict a response, and hurts the performance.
Finally, aiming at investigating whether the per-
formance gains come from seeing more in-domain
data in the pre-training process, we use the same
pre-training data to train another model with the
denoising objectives proposed in BART (Lewis
et al., 2020), then also fine-tune it on the Ubuntu
IRC benchmark. The last line of the lower part
presents the results, where we observe nearly the
same performance compared with FO. This obser-
vation indicates that simply performing domain
adaptation using the general pre-training objectives
is insufficient to benefit the MPDRG task.
5.2 Response Generation vs. Addressee
Prediction
In Section 3.3, we prove that p(z|c, r)∝p(r|c, z).
To verify the correctness of this equation and also
to investigate the training process of our EM strat-
egy, we draw the line chart of the BLEU-4 score
and addressee prediction accuracy of the top-30%
confidence samples on the validation set with the in-
creasing of pre-training iterations. The addressees
are predicted using Eq. (4), where we take the
zwith the highest conditional probability as the
predicted addressee.
Figure 4 illustrates the trending of the BLEU-4
score and addressee prediction accuracy. On the
one hand, we see that the trending of both metrics
is consistent, which means with a more powerful re-
sponse generation model comes a higher addressee
prediction accuracy. This observation verifies the
correctness of Eq. (3). On the other hand, with the
increasing of iterations, both metrics grow mutu-
ally, then reach their tops at around the 6iteration,
demonstrating the effectiveness of the EM process.98
5.3 Case Studies
To understand the effect of our method intuitively,
we sample two cases from the testing set and
present them in this section.
Figure 5 illustrates an example whose addressee
relations and dialogue history are shown in Figure
1. This conversation is about how to run the compiz
orberyl in a comp with 256MB RAM. Speaker
#2points that it’s the graphic card that is impor-
tant, but Speaker #4 seems unsatisfied by saying
that didn’t tell me much . After that, Speaker #5
suggests using the rdesktop andSpeaker #4 replies
him/her. Our model is able to capture the key in-
formation rdesktop andterminal in the addressee
utterance U, and generate a proper response Well,
how do I install rdesktop from the terminal , which
is very close to the human answer and even bet-
ter with more information from the terminal . On
the contrary, the baseline model (BART) fails to
capture the addressee information and just replies
with a safe response I tried but it didn’t work . This
case shows the great significance of modeling the
addressee information, and also demonstrates the
effectiveness of our model design.
Figure 6 presents another example sampled from
the testing set, where we investigate how different
addressee labels affect the generated responses. In
the figure, different colors represent different utter-
ances in the Dialogue History part, and different
responses generated by giving the corresponding ut-
terances as addressees in the Generated Responses
part. This conversation is about discussing the file
system in Ubuntu that can share on a network with
windows machines. When the addressee is given
asU, our model suggests using samba , which
is a solution to the question of U. Responses to
UandUare like safe responses, but they make
sense in their contexts: the former expresses its
confusion about a confusing utterance ( U), and
the latter expresses its gratitude to the suggestion in
U. Response to Ustates his/her understanding
towards U, and questions if his/her understanding
is right. Response to Uacknowledges the solution
gentoo inUby saying using gentoo on my com-
puter too . In general, this case demonstrates the
ability of our model to generate diverse responses
according to the specified addressees and contexts
of the dialogue history.
5.4 Response Parser: A Byproduct for Free
Another contribution of our EM pre-training is that
a response parser can be freely obtained. This
byproduct comes from Eq. (4), where given a re-
sponse generation model with addressee modeling,
we can predict the addressee for each utterance in
the dialogue. Previous literature has studied and
proved that explicitly modeling the structural in-
formation is beneficial to understanding specific
structured data. (Li et al., 2020, 2022a,b). In this
context, the response parser can be used to infer
the discourse structures, which contributes to boost-
ing the performance of some multi-party dialogue
comprehension tasks like response selection and
question answering. (Jia et al., 2020; Li and Zhao,
2021; Ma et al., 2022)996 Conclusion
Most multi-party dialogue corpora are not anno-
tated with addressee labels, making them unable
to support the pre-training of response generation
models. To solve this problem, we design a sim-
ple yet effective way to model the addressee of a
response as a latent variable and propose an EM
pre-training approach that iteratively performs the
expectation steps to generate addressee labels, and
the maximization steps to optimize a response gen-
eration model. Mathematical derivation, experi-
mental results on the Ubuntu IRC benchmark, and
extensive analyses have justified the theoretical fea-
sibility and actual effectiveness of our method.
Limitations
First, Due to the lack of datasets to evaluate the MP-
DRG task, we perform our experiments only on the
Ubuntu IRC benchmark and pre-train our model
only on the domain of Ubuntu chats. However, the
potential of our approach goes far beyond that since
it is applicable to any open-domain multi-party dia-
logue dataset. In the future work, we will consider
applying our method in more open-domain con-
versational datasets, such as the transcripts of TV
series or movies.
Additionally, the pre-training process solely re-
lies on the addressee information of individual
turns, disregarding the reply-to relations within the
dialogue history. This oversight prevents the model
from benefiting from valuable contextual cues nec-
essary for a comprehensive understanding of the
multi-party dialogue. In our future work, we will
explore the integration of discourse-level reply-to
relations into the pre-training process to further
enrich the capabilities of the model.
References100101ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
The last Section.
/squareA2. Did you discuss any potential risks of your work?
Not applicable. Left blank.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1.
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3.
/squareB1. Did you cite the creators of artifacts you used?
Section 4.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
They are publicly available and can be found on github.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 4.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Left blank.
C/squareDid you run computational experiments?
Section 4.
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
They can be found on our code.102/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Section 4.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 4.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
This will violate the double blind policy.103