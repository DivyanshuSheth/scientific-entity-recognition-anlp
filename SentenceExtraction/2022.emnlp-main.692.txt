
Haopeng Zhang, Xiao Liu, Jiawei Zhang
IFM Lab, Department of Computer Science, University of California, Davis, CA, USA
haopeng,xiao,jiawei@ifmlab.org
Abstract
Extractive summarization for long documents
is challenging due to the extended structured
input context. The long-distance sentence de-
pendency hinders cross-sentence relations mod-
eling, the critical step of extractive summa-
rization. This paper proposes H , a hy-
pergraph neural network for long document
summarization by capturing high-order cross-
sentence relations. H updates and learns
effective sentence representations with hyper-
graph transformer layers and fuses different
types of sentence dependencies, including la-
tent topics, keywords coreference, and section
structure. We validate H by conduct-
ing extensive experiments on two benchmark
datasets, and experimental results demonstrate
the effectiveness and efficiency of H .
1 Introduction
Extractive summarization aims to generate a
shorter version of a document while preserving the
most salient information by directly extracting rel-
evant sentences from the original document. With
recent advances in neural networks and large pre-
trained language models (Devlin et al., 2018; Lewis
et al., 2019), researchers have achieved promis-
ing results in news summarization (around 650
words/document) (Nallapati et al., 2016a; Cheng
and Lapata, 2016; See et al., 2017; Zhang et al.,
2022; Narayan et al., 2018; Liu and Lapata, 2019).
However, these models struggle when applied to
long documents like scientific papers. The input
length of a scientific paper can range from 2000 to
7,000words, and the expected summary (abstract)
is more than 200words compared to 40words in
news headlines.
Scientific paper extractive summarization is
highly challenging due to the long structured in-
put. The extended context hinders sequential mod-
els like RNN from capturing sentence-level long-
distance dependency and cross-sentence relations,Figure 1: An illustration of modeling cross-sentence re-
lations from section structure, latent topic, and keyword
coreference perspectives.
which are essential for extractive summarization.
In addition, the quadratic computation complexity
of attention with respect to the input tokens length
makes Transformer (Vaswani et al., 2017) based
models not applicable. Moreover, long documents
typically cover diverse topics and have richer struc-
tural information than short news, which is difficult
for sequential models to capture.
As a result, researchers have turned to graph
neural network (GNN) approaches to model cross-
sentence relations. They generally represent a doc-
ument with a sentence-level graph and turn extrac-
tive summarization into a node classification prob-
lem. These work construct graph from document
in different manners, such as inter-sentence cosine
similarity graph in (Erkan and Radev, 2004; Dong
et al., 2020), Rhetorical Structure Theory (RST)
tree relation graph in (Xu et al., 2019), approximate
discourse graph in (Yasunaga et al., 2017), topic-
sentence graph in (Cui and Hu, 2021) and word-
document heterogeneous graph in (Wang et al.,
2020). However, the usability of these approaches10167is limited by the following two aspects: (1)These
methods only model the pairwise interaction be-
tween sentences, while sentence interactions could
be triadic, tetradic, or of a higher-order in natu-
ral language (Ding et al., 2020). How to capture
high-order cross-sentence relations for extractive
summarization is still an open question. (2)These
graph-based approaches rely on either semantic
or discourses structure cross-sentence relation but
are incapable of fusing sentence interactions from
different perspectives. Sentences within a docu-
ment could have various types of interactions, such
as embedding similarity, keywords coreference,
topical modeling from the semantic perspective,
and section or rhetorical structure from the dis-
course perspective. Capturing multi-type cross-
sentence relations could benefit sentence repre-
sentation learning and sentence salience modeling.
Figure 1 is an illustration showing different types of
sentence interactions provide different connectiv-
ity for document graph construction, which covers
both local and global context information.
To address the above issues, we propose H
(HypErGraph transformer for Extractive Long doc-
ument summarization), a graph-based model de-
signed for summarizing long documents with rich
discourse information. To better model high-order
cross-sentence relations, we represent a document
as a hypergraph, a generalization of graph struc-
ture, in which an edge can join any number of ver-
tices. We then introduce three types of hyperedges
that model sentence relations from different per-
spectives, including section structure, latent topic,
and keywords coreference, respectively. We also
propose hypergraph transformer layers to update
and learn effective sentence embeddings on hyper-
graphs. We validate H by conducting exten-
sive experiments and analyses on two benchmark
datasets, and experimental results demonstrate the
effectiveness and efficiency of H . We high-
light our contributions as follows:
(i)We propose a hypergraph neural model,
H , for long document summarization. To
the best of our knowledge, we are the first to
model high-order cross-sentence relations with hy-
pergraphs for extractive document summarization.
(ii)We propose three types of hyperedges (sec-
tion, topic, and keyword) that capture sentence de-
pendency from different perspectives. Hypergraph
transformer layers are then designed to update and
learn effective sentence representations by messagepassing on the hypergraph.
(iii)We validate H on two benchmarked
datasets (arXiv and PubMed), and the experimental
results demonstrate its effectiveness over state-of-
the-art baselines. We also conduct ablation studies
and qualitative analysis to investigate the model
performance further.
2 Related Works
2.1 Scientific Paper Summarization
With the promising progress on short news summa-
rization, research interest in long-form documents
like academic papers has arisen. Cohan et al. (2018)
proposed benchmark datasets ArXiv and PubMed,
and employed pointer generator network with hi-
erarchical encoder and discourse-aware decoder.
Xiao and Carenini (2019) proposed an encoder-
decoder model by incorporating global and local
contexts. Ju et al. (2021) introduced an unsuper-
vised extractive approach to summarize long sci-
entific documents based on the Information Bottle-
neck principle. Dong et al. (2020) came up with
an unsupervised ranking model by incorporating
hierarchical graph representation and asymmetri-
cal positional cues. Recently, Ruan et al. (2022)
proposed to apply pre-trained language model with
hierarchical structure information.
2.2 Graph based summarization
Graph-based models have been exploited for ex-
tractive summarization to capture cross-sentence
dependencies. Unsupervised graph summarization
methods rely on graph connectivity to score and
rank sentences (Radev et al., 2004; Zheng and La-
pata, 2019; Dong et al., 2020). Researchers also
explore supervised graph neural networks for sum-
marization. Yasunaga et al. (2017) applied Graph
Convolutional Network (GCN) on the approximate
discourse graph. Xu et al. (2019) proposed to apply
GCN on structural discourse graphs based on RST
trees and coreference mentions. Cui et al. (2020)
leveraged topical information by building topic-
sentence graphs. Recently, Wang et al. (2020) pro-
posed to construct word-document heterogeneous
graphs and use word nodes as the intermediary be-
tween sentences. Jing et al. (2021) proposed to
use multiplex graph to consider different sentence
relations. Our paper follows this line of work on
developing novel graph neural networks for sin-
gle document extractive summarization. The main
difference is that we construct a hypergraph from10168
a document that could capture high-order cross-
sentence relations instead of pairwise relations, and
fuse different types of sentence dependencies, in-
cluding section structure, latent topics, and key-
words coreference.
3 Method
In this section, we introduce H in great detail.
We first present how to construct a hypergraph for
a given long document. After encoding sentences
into contextualized representations, we extract their
section, latent topic, and keyword coreference re-
lations and fuse them into a hypergraph. Then,
our hypergraph transformer layer will update and
learn sentence representations according to the hy-
pergraph. Finally, H will score the salience
of sentences based on the updated sentence repre-
sentations to determine if the sentence should be
included in the summary. The overall architecture
of our model is shown in Figure 2(a).
3.1 Document as a Hypergraph
A hypergraph is defined as a graph G= (V,E),
where V={v, . . . , v}represents the set of
nodes, and E={e, . . . , e}represents the set
of hyperedges in the graph. Here each hyperedge
econnects two or more nodes (i.e., σ(e)≥2).
Specifically, we use the notations v∈eandv /∈e
to denote node vis connected to hyperedge eor
not in the graph G, respectively. The topological
structure of hypergraph can also be represented by
its incidence matrix A∈R:A=/braceleftbigg1,ifv∈e
0,ifv/∈e(1)
Given a document D={s, s, ..., s}, each
sentence sis represented by a corresponding node
v∈ V. A Hyperedge ewill be created if a sub-
set of nodes V⊂ V share common semantic or
structural information.
3.1.1 Node Representation
We first adopt sentence-BERT (Reimers and
Gurevych, 2019) as sentence encoder to embed
the semantic meanings of sentences as X=
{x,x, ...,x}. Note that the sentence-BERT is
only used for initial sentence embedding, but not
updated in H .
To preserve the sequential information, we also
add positional encoding following Transformer
(Vaswani et al., 2017). We adopt the hierarchi-
cal position embedding (Ruan et al., 2022), where
position of each sentence scan be represented as
two parts: the section index of the sentence p,
and the sentence index in its corresponding section
p. The hierarchical position embedding (HPE)
of sentence scan be calculated as:
HPE(s) =γPE(p) +γPE(p),(2)
where γ, γare two hyperparameters to adjust the
scale of positional encoding and PE(·)refers to the
position encoding function:10169PE(pos,2i) = sin( pos/10000),
(3)
PE(pos,2i+ 1) = cos( pos/10000).
(4)
Then we can get the initial input node representa-
tionsH={h,h, ...,h}, with vector hde-
fined as:
h=x+HPE(s) (5)
3.1.2 Hyperedge Construction
To effectively model multi-type cross-sentence re-
lations in a long context, we propose the following
three hyperedges. These hyperedges could capture
high-order context information via the multi-node
connection and model both local and global con-
text through document structures from different
perspectives.
Section Hyperedges: Scientific papers mostly
follow a standard discourse structure describing the
problem, methodology, experiments/results, and fi-
nally conclusions, so sentences within the same sec-
tion tend to have the same semantic focus (Suppe,
1998). To capture the local sequential context, we
build section hyperedges that consider each section
as a hyperedge that connects all the sentences in
this section. Section hyperedges could also address
the incidence matrix sparsity issue and ensure all
nodes of the graph are connected by at least one
hyperedge. Assume a document has qsections,
section hyperedge efor the j-th section can be
represented formally in its corresponding incidence
matrix A∈Ras:
A=/braceleftbigg1,ifs∈e
0,ifs/∈e(6)
where Adenotes whether the i-th sentence is in
thej-th section.
Topic Hyperedges: Topical information has
been demonstrated to be effective in capturing im-
portant content (Cui et al., 2020). To leverage top-
ical information of the document, we first apply
the Latent Dirichlet Allocation (LDA) model (Blei
et al., 2003) to extract the latent topic relationships
between sentences and then construct the topic hy-
peredge. In addition, topic hyperedges could ad-
dress the long-distance dependency problem by
capturing global topical information of the doc-
ument. After extracting ptopics from LDA, weconstruct pcorresponding topic hyperedges e,
represented by the entry A in the incidence
matrix A∈Ras:
A=/braceleftigg
1,ifs∈e
0,ifs/∈e(7)
where A denotes whether the i-th sentence be-
longs to the j-th latent topic.
Keyword Hyperedges: Previous work finds that
keywords compose the main body of the sentence,
which are regarded as the indicators for impor-
tant sentence selection (Wang and Cardie, 2013;
Li et al., 2020). Keywords in the original sentence
provide significant clues for the main points of the
sentence. To utilize keyword information, we first
extract keywords for academic papers with Key-
BERT (Grootendorst, 2020) and construct keyword
hyperedges to link the sentences that contain the
same keyword regardless of their sequential dis-
tance. Like topic hyperedges, keyword hyperedges
also capture global context relations and thus, ad-
dress the long-distance dependency problem. After
extracting kkeywords for a document, we con-
struct kcorresponding keyword hyperedges e,
represented in the incidence matrix A∈R
as:
A=/braceleftbigg1,ifs∈e
0,ifs/∈e,(8)
where s∈emeans the i-th sentence contains
thej-th keyword.
We finally fuse the three hyperedges by con-
catenation ∥and get the overall incidence matrix
A∈Ras:
A=A∥A∥A, (9)
where dimension m=q+p+k
The initial input node representations H=
{h,h, ...,h}and the overall hyperedge inci-
dence matrix Awill be fed into hypergraph trans-
former layers to learn effective sentence embed-
dings.
3.2 Hypergraph Transformer Layer
The self-attention mechanism in Transformer
(Vaswani et al., 2017) has demonstrated its effec-
tiveness for learning text representation and graph
representations (Veli ˇckovi ´c et al., 2017; Ying et al.,
2021; Ding et al., 2020; Zhang and Zhang, 2020;10170Zhang et al., 2020). To model cross-sentence rela-
tions and learn effective sentence (node) represen-
tations in hypergraphs, we propose the Hypergraph
Transformer Layer as in Figure 2(b).
3.2.1 Hypergraph Attention
Given node representations H={h,h, ...,h}
and hyperedge incidence matrix A∈R, al-
layer hypergraph transformer computes hypergraph
attention (HGA) and updates node representations
Hin an iterative manner as shown in Algorithm 1.
Specifically, in each iteration, we first obtain all
mhyperedge representations {g,g, ...,g}as:
g=LeakyReLU
/summationdisplayαWh
,(10)
α=exp/parenleftbig
wu/parenrightbig
/summationtextexp/parenleftbig
wu/parenrightbig,
u= LeakyReLU/parenleftig
Wh/parenrightig
,(11)
where the superscript ldenotes the model layer,
matrices W,ware trainable weights and αis
the attention weight of node vin hyperedge e.
The second step is to update node representa-
tionsHbased on the updataed hyperedge repre-
sentations {g,g, ...,g}by:
h=LeakyReLU/parenleftigg/summationdisplayβWg/parenrightigg
,(12)
β=exp/parenleftbig
wz/parenrightbig
/summationtextexp (wz),
z= LeakyReLU/parenleftig/bracketleftig
Wg∥Wh/bracketrightig/parenrightig
,(13)
where his the representation of node v,W,w
are trainable weights, and βis the attention
weight of hyperedge ethat connects node v.∥
here is the concatenation operation. In this way,
information of different granularities and types can
be fully exploited through the hypergraph attention
message passing processes.
Multi-Head Hypergraph Attention As in Trans-
former, we also extend hypergraph attention (HGA)
into multi-head hypergraph attention (MH-HGA)
to expand the model’s representation subspaces,
represented as:MH-HGA (H,A) =σ(W∥head),
head=HGA(H,A),(14)
where HGA (·)denotes hypergraph attention, σ
is the activation function, Wis the multi-head
weight, and ∥denotes concatenation.
3.2.2 Hypergraph Transformer
After obtaining the multi-head attention, we also
introduce the feed-forward blocks (FFN) with resid-
ual connection and layer normalization (LN) like
in Transformer. We formally characterize the Hy-
pergraph Transformer layer as below:
H=LN(MH-HGA (H,A) +H)
H=LN(FFN(H) +H(15)
Algorithm 1: MH-HGA(H,A)
3.3 Training Objective
After passing Lhypergraph transformer layers,
we obtain the final sentence node representations
H={h,h, ...,h}. We then add a multi-
layer perceptron(MLP) followed by a sigmoid acti-
vation function indicating the confidence score for
selecting each sentence. Formally, the predicted
confidence score ˆyfor sentence sis:
z=LeakyReLU (Wh),
ˆy=sigmoid (Wz),(16)10171
where W,Ware trainable parameters.
Compared with the sentence ground truth label
y, we train H in an end-to-end manner and
optimize with binary cross-entropy loss as:
where Ndenotes the number of training instances
in the training set, and Ndenotes the number of
sentences in the document.
4 Experiment
This section presents experimental details on
two benchmarked academic paper summarization
datasets. We compare our proposed model with
state-of-the-art baselines and conduct detailed anal-
ysis to validate the effectiveness of H .
4.1 Experiment Setup
Datsasets Scientific papers are an example of
long documents with section discourse structure.
Here we validate H on two benchmark sci-
entific paper summarization datasets: ArXiv and
PubMed (Cohan et al., 2018). PubMed contains
academic papers from the biomedical domain,
while arXiv contains papers from different scien-
tific domains. We use the original train, validation,
and testing splits as in (Cohan et al., 2018). The
detailed statistics of datasets are shown in Table 1.
Compared Baselines We perform a system-
atic comparison with state-of-the-art baseline ap-
proaches as follows:
•Unsupervised methods: LEAD that selects
the first few sentences as summary; graph-
based methods LexRank (Erkan and Radev,
2004), PACSUM (Zheng and Lapata, 2019),
and HIPORANK (Dong et al., 2020).
•Neural extractive models: encoder-decoder
based model Cheng&Lapata (Cheng and La-
pata, 2016) and SummaRuNNer (Nallapati
et al., 2016a); local and global context model
ExtSum-LG (Xiao and Carenini, 2019) and
its variant RdLoss/MMR (Xiao and Carenini,2020); transformer-based models SentCLF,
SentPTR (Subramanian et al., 2019), and
HiStruct+ (Ruan et al., 2022).
•Neural abstractive models: pointer network
PGN (See et al., 2017), hierarchical attention
model DiscourseAware (Cohan et al., 2018),
transformer-based model TLM-I+E (Subra-
manian et al., 2019), and divide-and-conquer
method DANGER (Gidiotis et al., 2020).
4.2 Implementation Details
We use pre-trained sentence-BERT (Reimers and
Gurevych, 2019) checkpoint all-mpnet-base-v2 as
the encoder for initial sentence representations.
The embedding dimension is 768, and the input
layer dimension is 1024 . In our experiment, we
stack two layers of hypergraph transformer, and
each has 8attention heads with a hidden dimension
of128. The output layer’s hidden dimension is set
to4096 . We generate at most 100topics for each
document and filter out the topic and keyword hy-
peredges that connect less than 5sentence nodes
or greater than 25sentence nodes. For position
encodings, we set the rescale weights γandγto
0.001.
The model is optimized with Adam optimizer
(Loshchilov and Hutter, 2017) with a learning rate
of 0.0001 and a dropout rate of 0.3. We train the
model on an RTX A6000 GPU for 20 epochs and
validate after each epoch using ROUGE-1 F-score
to choose checkpoints. Early stopping is employed
to select the best model with the patience of 3.
Following the standard-setting, we use ROUGE
F-scores (Lin and Hovy, 2003) for performance
evaluation. Specifically, ROUGE-1/2 scores mea-
sure summary informativeness, and the ROUGE-L
score measures summary fluency. Following prior
work (Nallapati et al., 2016b), we construct ex-
tractive ground truth (ORACLE) by greedily op-
timizing the ROUGE score on the gold-standard
abstracts for extractive summary labeling.
4.3 Experiment Results
The performance of H and baseline methods
on arXiv and Pubmed datasets are shown in Ta-
ble 2. The first block lists the extractive ground
truth ORACLE and the unsupervised methods. The
second block includes recent extractive summariza-
tion models, and the third contains state-of-the-art
abstractive methods.
The LEAD method has limited performance
on scientific paper summarization compared to10172
its strong performance on short news summariza-
tion like CNN/Daily Mail (Hermann et al., 2015)
and New York Times (Sandhaus, 2008). The phe-
nomenon indicates that academic paper has less po-
sitional bias than news articles, and the ground truth
sentence distributes more evenly. For graph-based
unsupervised baselines, HIPORANK (Dong et al.,
2020) achieves state-of-the-art performance that
could even compete with some supervised methods.
This demonstrates the significance of incorporat-
ing discourse structural information when model-
ing cross-sentence relations for long documents.
In general, neural extractive methods perform bet-
ter than abstractive methods due to the extended
context. Among extractive baselines, transformer-
based methods like SentPTR and HiStruct+ show
substantial performance gain, demonstrating the ef-
fectiveness of the attention mechanism. HiStruct+
achieves strong performance by injecting inherent
hierarchical structures into large pre-trained lan-
guage models Longformer. In contrast, our model
H only relies on hypergraph transformer lay-
ers for sentence representation learning and re-
quires no pre-trained knowledge.
As shown in Table 2, H outperforms state-
of-the-art extractive and abstractive baselines on
both datasets. The supreme performance of H
shows hypergraphs’ capability of modeling high-
order cross-sentence relations and the importance
of fusing both semantic and structural information.
We conduct an extensive ablation study and perfor-
mance analysis next.
5 Analysis
5.1 Ablation Study
We first analyze the influence of different compo-
nents of H . Table 3 shows the experimental
results of removing hyperedges and the hierarchi-
cal position encoding of H on the PubMed
dataset. As shown in the second row, removing the
hierarchical position embedding hurts the model
performance, which indicates the importance of
injecting sequential order information. Regarding
hyperedges (row 3-5), we can see that all three
types of hyperedges (section, keyword, and topic)
help boost the overall model performance. Specifi-
cally, the performance drops most when the section
hyperedges are removed. The hypergraph becomes
sparse and hurts its connectivity. This indicates
that the section hyperedges, which contain local
context information, play an essential role in the in-
formation aggregation process. Note that although
we only discuss three types of hyperedges (sec-
tion, keyword, and topic) in this work, it is easy
to extend our model with hyperedges from other
perspectives like syntactic for future work.101735.2 Hyperedge Analysis
We also explore the hyperedge pattern to under-
stand the performance of H further. As shown
in Figure 3, we have the most topic hyperedges on
average, and section hyperedges have the largest
degree (number of connected nodes). In terms of
cross attention over the predicted sentence nodes,
H pays more than half of the attention to sec-
tion hyperedges and pays least to keywords edges.
The results are consistent with the earlier ablation
study that local section context information plays a
more critical role in long document summarization.
5.3 Embedding Analysis
To explore the sentence embedding learned by
H , we show a visualization of the output sen-
tence node embedding from the last hypergraph
transformer layer. We employ T-SNE (van der
Maaten and Hinton, 2008) and reduce each node’s
dimension to 2, as shown in Figure 4. The orangedots represent the ground truth sentences, and the
blue dots are the non-ground truth sentences. We
can see some clustering effects of the ground truth
nodes, which also tend to appear in the bottom left
zone of the plot. The results indicate that H
learns effective sentence embeddings as indicators
for salient sentence selection.
5.4 Case Study
Here we also provide an example output summary
from H in Table 4. We could see that the
selected sentences span a long distance in the origi-
nal document, but are triadically related according
to the latent topic and keyword coreference. As
a result, H effectively captures high-order
cross-sentence relations through multi-type hyper-
edges and selects these salient sentences according
to learned high-order representation.
6 Conclusion
This paper presents H for long document sum-
marization. H represents a document as a
hypergraph to address the long dependency issue
and captures higher-order cross-sentence relations
through multi-type hyperedges. The strong perfor-
mance of H demonstrates the importance of
modeling high-order sentence interactions and fus-
ing semantic and structural information for future
research in long document extractive summariza-
tion.10174Limitations
Despite the strong performance of H , its
design still has the following limitations. First,
H relies on existing keyword and topic mod-
els to pre-process the document and construct hy-
pergraphs. In addition, we only explore academic
paper datasets as a typical example for long docu-
ment summarization.
The above limitations may raise concerns about
the model’s performance. However, H is an
end-to-end model, so the pre-process steps do not
add the model computation complexity. Indeed,
H relies on hyperedge for cross-sentence at-
tention, so it is parameter-efficient and uses 50%
less parameters than heterogeneous graph model
(Wang et al., 2020) and 90% less parameters than
Longformer-base (Beltagy et al., 2020). On the
other hand, our experimental design follows a se-
ries of previous long document summarization
work (Xiao and Carenini, 2019, 2020; Subrama-
nian et al., 2019; Ruan et al., 2022; Dong et al.,
2020; Cohan et al., 2018) on benchmark datasets
ArXiv and PubMed. These two new datasets con-
tain much longer documents, richer discourse struc-
ture than all the news datasets and are therefore
ideal test-beds for long document summarization.
Acknowledgements
This work is supported by NSF through grants IIS-
1763365 and IIS-2106972. We thank the anony-
mous reviewers for the helpful feedback.
References1017510176