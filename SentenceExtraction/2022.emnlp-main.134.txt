
Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark
Allen Institute for AI, Seattle, WA
{oyvindt,bhavanad,peterc}@allenai.org
Abstract
Our goal is a question-answering (QA) system
that can show how its answers are implied by
its own internal beliefs via a systematic chain
of reasoning . Such a capability would allow
better understanding of whya model produced
the answer it did. Our approach is to recur-
sively combine a trained backward-chaining
model, capable of generating a set of premises
entailing an answer hypothesis, with a verifier
that checks that the model itself believes those
premises (and the entailment itself) through
self-querying. To our knowledge, this is the
first system to generate multistep chains that
are both faithful (the answer follows from the
reasoning) and truthful (the chain reflects the
system’s own internal beliefs). In evaluation
using two different datasets, users judge that
a majority (70%+) of generated chains clearly
show how an answer follows from a set of facts
- substantially better than a high-performance
baseline - while preserving answer accuracy.
By materializing model beliefs that systemat-
ically support an answer, new opportunities
arise for understanding the model’s system of
belief, and diagnosing and correcting its misun-
derstandings when an answer is wrong.
1 Introduction
Although pretrained language models (PTLMs)
have shown remarkable question-answering (QA)
performance, it is often unclear whytheir answers
follow from what they know. While there has been
substantial work on training models to also gener-
ate explanations for their answers (Wiegreffe and
Marasovi ´c, 2021), or produce them via few-shot
prompting, e.g., “chains of thought” (Wei et al.,
2022), those explanations may not be faithful (the
answer does not necessarily follow from them) and
may not be truthful , in the sense that the language
model itself does not believethe explanation state-Figure 1: Given a question, Entailer searches for an
answer hypothesis that is supported by an entailment
proof. First it over-generates candidate proofs, then it
removes those that the model itself does not “believe”
(i.e., confirms via self-querying that it considers all the
generated proof elements to be true). Finally it selects
the best verified proof. Multistep proofs are generated
by iteratively backward chaining on the premises (Sec-
tion 3.2).
ments that it generated. Rather, our goal is to gen-
erate answers that systematically follow from the
model’s own internal beliefs, materializing those
beliefs as explicit statements that can then be in-
spected. Such a capability offers new opportunities
for understanding, diagnosing, and ultimately cor-
recting errors in a language model’s behavior.
Our approach uses a combination of generation
and verification, implemented in a system called
Entailer. Chains are constructed by backward
chaining from candidate answers, recursively us-
ing a language model (LM) trained for a single
backward-chaining step. For each step, Entailer
over-generates candidate entailments, then filters2078
out those that do not conform to its own internal
knowledge (“beliefs”) by self-querying, asking it-
self whether (a) the generated premises (leaves of
the proof step) are true, and (b) each entailment step
is valid (Figure 1). It then recursively backward-
chains on premises until the overall proof confi-
dence cannot be further improved (or a depth limit
dis reached). Finally, the candidate answer sup-
ported by the highest-scoring chain of a reasoning
is returned. As a result, the system has material-
ized some of its latent knowledge from which the
selected answer follows. Most significantly, the
resulting proof is thus both faithful (the answer fol-
lows from the proof) and truthful (the proof reflects
the system’s beliefs), providing a previously un-
available window into the model’s beliefs about the
world and their implications, e.g., Figure 2.
To train the Entailer model, we use a combina-
tion of the existing EntailmentBank dataset (Dalvi
et al., 2021), plus a new crowd-annotated dataset
that we construct by bootstrapping (train an ini-
tial model, generate candidate entailment examples
with it, then annotate those examples as extra train-
ing data). The model is then frozen, and Entailer
is then applied zero-shot to new datasets, i.e., En-
tailer is a treated as a general-purpose, fixed model
specialized for reasoning, rather than requiring fine-
tuning for new tasks.
We evaluate Entailer on two existing datasets,
OBQA (Mihaylov et al., 2018) and QuaRTz
(Tafjord et al., 2019). We find that its reasoning-
based QA accuracy is similar to its direct QA ac-
curacy, with the advantage that a supporting chain
of reasoning is also produced. We also perform ahuman evaluation, and find that 70% of time users
judge the chains to clearly show how an answer
followed from their premises, substantially higher
than for explanations produced by a comparable
high-performance QA system, Macaw (Tafjord and
Clark, 2021). Our contributions are thus:
1.The first system to generate chains of reason-
ing showing how answers are systematically
implied by a model’s own internal beliefs ,
making relevant model beliefs explicit. The
chains are both faithful (the answer follows
from the reasoning) and truthful (the chain
reflects the system’s own beliefs).
2.A new, crowdsourced dataset of multi-premise
entailments, doubling the amount of data
available in EntailmentBank (Dalvi et al.,
2021), and including examples of both pos-
itive and negative entailments (Entailment-
Bank only includes positive examples).
2 Related Work
Systematic Reasoning: Several recent systems
have demonstrated the ability to perform system-
aticreasoning directly over natural language (Natu-
ral Language Inference (Manning and MacCartney,
2009)), namely deriving conclusions from known
facts via step-wise application of well-defined infer-
ence operations. One approach is to retrain a black-
box model end-to-end (Clark et al., 2020), but has
been limited to small rulebases. An alternative ap-
proach, which we follow here, is to have an outside
loop around a model, where the model generates
individual inference steps (i.e., rules), and a con-2079troller chains them together. SCSearch (Bostrom
et al., 2022), NLProofS (Yang et al., 2022), IRGR
(Ribeiro et al., 2022), ProofWriter (iterative ver-
sion) (Tafjord et al., 2020), and Selection-Inference
(Creswell et al., 2022) do this in a forward-chaining
fashion, MetGen (Hong et al., 2022) does this bidi-
rectionally, while Braid (Kalyanpur et al., 2020)
(like us) does this in a backward-chaining fashion.
In all these systems, the required facts were ex-
pected to be provided explicitly to the model. In
contrast, Entailer’s reasoning uses its own inter-
nal, latent knowledge, as well as (optionally) exter-
nally provided facts. LeapOfThought (Talmor et al.,
2020) demonstrated that reasoning with a combina-
tion of implicit and explicit knowledge was possi-
ble for simple 1-step inferences. We expand this for
multi-step inference, and (unlike LeapOfThought)
have the system also explicitly articulate the im-
plicit knowledge it uses, and its chain of reasoning.
Recent work has shown that generating a free-
form explanation (“chain of thought”) before an
answer can also improve performance on a variety
of tasks (Wei et al., 2022; Cobbe et al., 2021; Nye
et al., 2021). In these works, however, the expla-
nations are unstructured, and there are no claims
of faithfulness that the answer follows from the
generation, nor that the explanations themselves
represent model beliefs.
Materializing a Model’s Internal Knowledge:
Pretrained LMs contain a vast amount of knowl-
edge, and can be thought of as a kind of knowledge
base to tap into (Petroni et al., 2019). Recent work
has shown that this latent knowledge can be materi-
alized as explicit English sentences or a knowledge
graph using generative techniques, e.g., COMeT
(Bosselut et al., 2019), ParaCOMET (Gabriel et al.,
2021). Our work with Entailer similarly mate-
rializes its latent knowledge, but here in a goal-
directed way, namely by producing a faithful chain
of reasoning from facts it validates (“believes”) as
true to an answer. This articulation can be seen as
a kind of self-talk, where a self-generated context
can improve QA (Shwartz et al., 2020). However,
here our generations are not used as context for
opaque problem-solving, but are assembled into a
well-defined chain of reasoning.
Beliefs: We refer to the model’s factual opinions
as “beliefs” rather than “knowledge” because those
opinions may be wrong. In general, an agent can
be said to believe p if it acts as if p was true
(Schwitzgebel, 2019). Following (Kassner et al.,
2021), we take a simple, syntactic operationaliza-
tion of this, namely the agent answers “yes” to the
question “p?”, but also note that more semantic
versions could be used, e.g., the agent also answers
“yes” to paraphrases and implications of p. In gen-
eral, models can sometimes be inconsistent in their
beliefs (Elazar et al., 2021; Kassner and Schütze,
2020; Ribeiro et al., 2019). For our purposes here,
we simply note that such inconsistencies may oc-
casionally exist, and that techniques for inconsis-
tency resolution could be applied in future to re-
duce these, e.g., (Kassner et al., 2021; Li et al.,
2019).
3 Approach
Like several previous systems (Section 2), Entailer
treats reasoning as Natural Language Inference
(NLI). In NLI, the basic unit of knowledge is (rep-
resented as) a sentence rather than a structure, and
a proofis a tree of multi-step, multi-premise en-
tailments, e.g., Figures 2 and 3.
Within this framework, given a question, En-
tailer first generates candidate answers, then tries
to prove each one, selecting the answer with the
highest-scoring proof. We now describe these steps.
3.1 Hypothesis Generation
Given a question, Entailer first generates candi-
date answers and converts these into declarative
hypotheses (e.g., “Is the sky (A) blue (B) yellow”
→{H= “The sky is blue.”, H= “The sky is
yellow.”).AnN-way multiple choice question
yields Nhypotheses. A true/false question yields2080
2 hypotheses. For open-ended questions, Entailer
first collects Ncandidate answers generated by an
external source (Macaw (Tafjord and Clark, 2021)
using nucleus sampling (Holtzman et al., 2019))
then forms Nhypotheses from them.
3.2 Generating Entailment Trees
3.2.1 Generating a Backward-Chaining Step
Models:
The core of Entailer is generating and validating a
single entailment step that entails a hypothesis. We
define the following data types:
H:A hypothesis (English statement) to prove.
P:A set of premises { p,...,p} (sentences) that
together may entail the hypothesis H. To-
gether, P and H form a one-deep entailment
step, denoted by P ⊢H.
Q:A question posed to Entailer.
A:A candidate answer for consideration.
C:An optional context (set of sentences) relevant
to the problem. This allows Entailer to also
use external knowledge, if available, when
generating a tree.
We train a model (details in Section 4) with the
three input/output behaviors below (optional inputs
shown in parentheses):
(QAC )H→P:Given a hypothesis H, generate a
set of premises P that may entail H
(QAC )H→S:Score whether the model be-lieves that a hypothesis H (or premise p) is
true (S>0.5) or not, (i.e. perform yes/no
QA). We call this the direct score (range 0-1).
(QAC )PH→S:Score whether the model be-
lieves a candidate entailment (P ⊢H) is valid
(S>0.5) or not, i.e., P validly entails H
(range 0-1).
Examples of these three angles are in Table 1.
Algorithm:
To generate a single backward-chaining step we
adopt an overgenerate-and-filter approach, also
found useful elsewhere (Yang et al., 2022; Cobbe
et al., 2021; Li et al., 2022). First, given H, we use
the angle H→Pto generate a set of premises
Pthat may entail H. We then check that the
model believes all the premises p∈Pusing the
H(=p)→Sangle, and that it also believes the
inference step P⊢Hitself is valid (independent of
whether the pare true or not) using the PH→S
angle. The proof score, denoting how well the 1-
step proof supports the hypothesis, is the product
of the premises’ and entailment scores:
s(H) = (Πs(p)).s(P⊢H)
We repeat this ktimes using nucleus sampling to
obtain a diversity of alternative proof steps, and
then select the highest-scoring one P⊢H, as
illustrated in Figure 1.20813.2.2 Backward Chaining
This one-step backward chainer is embedded in
a larger control algorithm that grows a multi-step
entailment tree, searching for the best tree. This
algorithm is illustrated in Figure 4 and described
below. The full algorithm is in Appendix A.
Each node Nin the tree has two associated
scores, the direct (“fast”) score s, denoting the
model’s direct belief in N(in red in Figure 4), and
(for internal nodes) the proof (“slow”) score s,
denoting how confidently the model can prove N
(in blue), computed from its children. The overall
score sis the max of the two. The proof score sis
recursively defined as the product of its children’s
overall scores times the entailment score:
s(N) = (Πs(p)).s(P⊢N)
The algorithm starts with an initial hypothesis node
H, then iteratively expands each leaf node N, look-
ing for a proof that scores higher than the direct
score sof that node. In other words, can the sys-
tem prove Nwith a more confidence than simply
“guessing” if Nis true? If it can, the node’s overall
score s(max of the two) will increase, that increase
is be propagated up the tree, and the expansion is
retained, e.g., step 3 in Figure 4. If expansions
cannot improve a node’s score further (e.g., steps
2 and 4), the expansions are pruned and that node
becomes a leaf (red bars in Figure 4).
Note that because premises are self-generated
rather than externally provided, this stopping con-
dition has a different semantics to earlier work,
e.g., (Kalyanpur et al., 2020; Bostrom et al., 2021):
Rather than stopping when externally known facts
are reached, Entailer stops when strongly believed
facts are reached, and more backward chaining can
no longer improve a hypothesis’ confidence.
This whole procedure is repeated for each can-
didate answer hypothesis (Section 3.1). Finally
the system selects the answer corresponding to the
hypothesis with the top-scoring proof s(H).
4 Model Training
The core of Entailer is the model for one-step infer-
ence (Section 3.2.1), with the three functionalities
listed in Table 1. As Entailer is intended to be a
general-purpose system, the model is trained one-
time for these three functionalities, and then frozen.
It is then applied zero-shot to new datasets, e.g.,
the evaluation in Section 5.4.1 Data Sources
4.1.1 EntailmentBank
To train Entailer’s model, we leverage (the train-
ing partition of) the EntailmentBank dataset (Dalvi
et al., 2021). EntailmentBank contains 1840
multiple-choice science questions (1313 in the
training partition) along with their correct answer,
a hypothesis expressing the question and answer in
declarative form, and a multistep entailment tree
expressing how the hypothesis follows from a set
of facts drawn from a corpus of science facts (the
WorldTree corpus (Xie et al., 2020)). Using the
notation introduced earlier, each EntailmentBank
example is of the form:
< Q, A, H,{(P⊢H)∗ }>
where ( P⊢H)* denotes a setof entailment steps
forming a tree (with root H=H), describing
how the corpus facts entail the hypothesis H.
4.1.2 Crowdsourced Data
EntailmentBank only contains positive examples of
entailments. To obtain negative examples, we first
ran an earlier, positive-only-trained version of En-
tailer to generate 1-step proofs of (hypotheses for)
incorrect answer options in EntailmentBank’s ques-
tions (4-way multiple choice), resulting in (neces-
sarily bad) proofs for the three false hypotheses.
(This was done using just the H →P angle, without
verification). Note that Entailer will generate some
kind of proof even for false hypotheses, e.g.,
A rabbit has six legs because:
1. A rabbit is an animal
2. Animals have six legs
These invalid proofs will be incorrect either be-
cause a generated fact is false, and/or because
the inference itself is not a valid entailment. To
distinguish these, we use crowdworkers to assess
whether the generated facts were true, and if the en-
tailment itself was valid. The 1313 questions result
in 3939 proofs for false hypotheses. Dropping the
few with more than two premises (to simplify the
crowdsourcing interface), crowdworkers annotated
3673 proofs, using labels T/F/? for each premise
and T/F/? for the entailment itself. Each proof
was annotated by 3 crowdworkers, then an addi-
tional 3 workers provided additional annotations
for cases with no initial majority verdict. Drop-
ping premises/entailments without a final majority
verdict, we end up with 7013 additional labeled
premises for the H→Sangle, and 3391 addi-
tional labeled entailments for the PH→Sangle.2082The crowdworker interface is in Appendix B.
4.1.3 Optional Fields
We also augment the training data with duplicate
examples but with additional, optional input fields:
QA: The input QA question/answer-option pair,
as well as the hypothesis H
C:Acontext consisting of up to 5 relevant sen-
tences, allowing explicit external knowledge
to be provided if available.
To generate examples of C during training, we use
a mixture of sentences drawn from (a) the gold
(target) entailment (i.e., the gold premises), and (b)
sentences retrieved from a corpus of similar-styled
knowledge (namely all the leaf sentences used in
the EntailmentBank entailment trees), mixed in
different ratios so the model is exposed to a mixture
of high and medium relevance sentences.
In this work we do not use any context C at
test time, but it is utilized in concurrent work for
providing feedback to the overall system (Mishra
et al., 2022).
Further details of training are given in Ap-
pendix C1.
4.2 Model Details
We train a T5-11B multi-angle model, Entailer,
following the multi-task setup similar to Macaw
(Tafjord and Clark, 2021) for the three function-
alities described in Table 1.Details of hyperpa-
rameters and other settings are provided in Ap-
pendix C2.
5 Evaluation
Our goal is to generate answers supported by faith-
ful, truthful chains of reasoning, without signif-
icantly impacting performance. Our two corre-
sponding reserach questions are:
1.How does Entailer’s proof-based answer ac-
curacy compare with the direct QA accuracy
(both zero-shot)?
2.How good are the generated entailment-based
proofs? And are they better than those pro-
duced by a purely generative model?
For the first question, we evaluate using two exist-
ing multiple-choice datasets, namely OBQA (Mi-
haylov et al., 2018) and QuaRTz (Tafjord et al.,
2019). These datasets contain multiple-choice
questions that (typically) require multihop reason-
ing, rather than being simple lookup questions. We
use the test partitionswith sizes 500 questions
(OBQA) and 557 questions (QuaRTz).
For the second question, we collect human judge-
ments of whether the hypothesis clearly follows
from facts in the proof, and compare its proofs with
explanations generated by a high-quality baseline
model, Macaw (Tafjord and Clark, 2021).
5.1 QA Accuracy
We evaluate Entailer’s proof-based QA accura-
cies under various conditions, and compare them
against its direct QA accuracy:
1. Direct QA: Here we measure the model’s ability
to directly answer the test questions (zero shot) us-
ing the H→Sangle. One can think of this as the
“fast thinking” answer. Note that this capability of
the frozen model was trained on the same data as
the rest of Entailer, so is a fair comparison.
2. Entailer: Here we measure Entailer’s ability to
answer the test questions (again zero shot) by gen-
erating, scoring, and comparing entailment proofs
for each answer option. One can think of this as
the“slow thinking” answer. We vary:
•maximum proof depth d= 1 (blue in Fig-
ure 5) or 3 (green)
•degree of search: (a)greedy: use the first
(k= 1) generated entailment at each step, or2083(b)sampled top level: pick the best of k= 6
entailments for expanding the root hypothe-
sis node. ( ≈six times more computationally
expensive than (a)).
Note that these proofs are are faithful explanations
of why an answer was chosen, as the selected
answer by definition is always the one with the
highest-scoring proof.
3. Entailer + Direct: Here we combine the two
by selecting the overall most confident answer of
Direct and Entailer (using c(H)orc(H), Ap-
pendix A). Here, the proofs are not always faithful
explanations of an answer, as the chosen answer
may not be the one with the highest-scoring proof.
5.2 Results
The results are shown in Figure 5, and suggest that:
Proof Scores are competitive with direct an-
swer scores. In Entailer’s best configuration (sam-
pled top-level k= 6, max depth d= 3, last
bar), its reason-based answers have an accuracy of
75.2/75.4 for OBQA/QuaRTz respectively, not sig-
nificantly different to the Direct scores of 75.2/74.1.
This is important, as it suggests there is no signifi-
cant accuracy penalty for proof-based answers.
Sampling proofs helps: Using sampling for the
top-level proofs (last 4 bars) always outperforms
greedy proof selection by a small amount.
Allowing deeper proofs does not significantly af-
fect accuracy: Although deeper proofsslightly
help in the combination of Entailer+Direct, and
may provide more information to a user, the accu-
racy differences are not significant.
When Entailer + Direct are combined, by select-
ing the most confident answer (last two columns),
we lose the guarantee of faithfulness, as the se-
lected answer may not be the one with the highest-
scoring proof. In practice, this occurs for 16.8%
of the questions (OBQA), 14.2% (QuaRTz). In
addition, we find this combination does not have
significant performance gains, so has no obvious
advantage in these experiments.
Note that we are measuring zero-shot perfor-
mance on our test datasets, so our results are not
comparable with leaderboard entries.More im-portantly, though, our goal is not a state-of-the-art
zero-shot model, but rather a model that can show
how answers systematically follow from its own
internal beliefs . Our results suggest this is possible
with high reliability, and, as an additional bonus,
without loss of zero-shot accuracy. Thus, rather
than just answers, we now get answers supported
by faithful chains of reasoning.
5.3 Human Judgements
For our second question of evaluating the quality of
Entailer’s proofs, we compare against explanations
generated by Macaw,a public, state-of-the-art
QA system with explanation capability (Tafjord
and Clark, 2021). Examples of explanations from
both systems are in Appendix D. Six annotators
compared 1-deep Entailer proofs with explanations
from Macaw, scoring each along four dimensions,
and then comparing the two:
1.Does the conclusion clearly follow from the
premises?
2. Are all the premises correct?
3. Are all of the premises relevant?
4.Does the explanation introduce something
new and useful, i.e., does more than just re-
state the conclusion in different words?
5. Which explanation do you prefer?
Answer options for questions 1-4 were
yes/somewhat/no, and for question 5 were
first/similar/second. The ordering of explanations
were scrambled so the annotators did not know
which explanation was which, and in fact they
were unaware that the different explanations
had been generated by different systems. We
collected annotations for 100 pairs of explanations
for correct answers to 100 OBQA dev questions.
The annotators were recruited from our institute,
spanning a broad range of age (20-60) and
experience.
The results (Figure 6) suggest several findings:
1. In absolute terms, Entailer’s conclusions were2084
judged to clearly follow from the premises in the
large majority (over 70%) of the explanations,
and substantially more than Macaw’s explanations
(34%). This potentially contributes to system trust-
worthiness, where understanding howevidence sup-
ports a conclusion is critical.
2.≈90% of Entailer’s self-verified premises were
judged correct by users. Of the remainder, virtu-
ally all were labeled “unsure” (only 1 Entailer fact
was labeled not correct), indicating that the there
arefew false beliefs in proofs for correct answers.
Rather, vague facts (e.g., “Claws are used for crack-
ing open shells”) make up the remainder.
3.Entailer’s explanations were clearly preferred
(57% to 23%, last bar) over Macaw’s. In particular,
Entailer’s arrangement of facts into a tree discour-
ages irrelevant information (bar #3).
Finally we note Entailer’s proofs are faithful (show-
ing how an answer was derived) and truthful (re-
flecting the system’s own beliefs), while Macaw’s
explanations are post-hoc generated text. These all
suggest the value of entailment trees as trustable
explanations of a system’s behavior.
5.4 Analysis
5.4.1 Failure Analysis
If Entailer answers a question incorrectly, either a
model belief ( belief error ) and/or the reasoning
itself ( reasoning error ) must necessarily be
incorrect, unless the question itself is malformed
(dataset error ). To better understand these,
we classified the 51/500 cases in OBQA where
Entailer selected the wrong answer while the
Direct answer was correct, and found:
1.belief errors (33%) , where an incorrect belief
led to a wrong answer, for example:
Note that here the reasoning is correct but the
second is premise is false.
2.dataset errors (20%) . In several cases, the
question was ambiguous (e.g., does “animal”
include humans?) or more than one answer option
was valid (OBQA is a crowdsourced dataset).
3.reasoning errors (47%):
3a. Near-tautologies (20%): of the form “ X
because 1.X, 2. ...”, where Xis a near-repeat of
the hypothesis. In such cases, the proof offers little
new information.
3b. Basic reasoning errors (10%): , e.g.,
Here entailment is simply invalid.
3c. Incorrect abductive inferences (9%): of
the form Abecause (A→B)andB. While
sometimes useful, these inferences are not sound
and can produce incorrect conclusions, e.g.,
3d. Quantification and exceptions (8%):
where both beliefs and reasoning seem reason-
able, but the specific case does not hold, e.g.,
5.4.2 When do proofs do better?
At its best, Entailer decomposes an uncertain
hypothesis Hinto premises Pwhich it is very
confident about. For example, Entailer is unsure2085whether A suit of armor conducts electricity , but it
confidently believes the generated premises:
Thus an uncertain conclusion is replaced by more
certain premises, and we see Entailer performing
particularly well for such questions. However,
there are questions that are largely “fact lookup”,
where a proof may be less helpful. For example,
the model is already very confident about the
hypothesis Jupiter is the largest planet in the Solar
System ; decomposing this into 1. The largest
planet has the greatest mass plus 2. Jupiter has
the greatest mass in the solar system has not obvi-
ously made answering easier. In fact, Entailer’s
algorithm is specifically designed to account for
this, only growing a proof when the confidence in
the premises improves the confidence in H, thus
tailoring its degree of reasoning to the complexity
of the problem at hand.
6 Towards Teachable Systems
If a system can show how its answers systemat-
ically follows from its own beliefs, articulating
those beliefs in the process, then this opens a win-
dow into the model’s internal state for the user. As
a result, new opportunities arise for identifying and
correcting the model’s misunderstandings, so the
system gradually improves over time (sometimes
referred to as explanatory interactive machine learn-
ing (XIL) (Teso and Kersting, 2019)). One vehicle
for doing this is to use Entailer’s (currently un-
used) context field C at runtime: If the user asks
a question, receives a wrong answer, and sees an
incorrect belief in the proof, they would provide
the corrected belief, then re-ask the question with
the corrected belief in the context. This encourages
the model to use the corrected belief in its new an-
swer and proof, rather than just repeat the same bad
belief. Such overrides would then be stored and
retrieved from a persistent memory to use for future
questions also. A simple, hypothetical dialog illus-
trating this is shown in Figure 8. This is an exciting
avenue made possible by this work, currently used
by the TeachMe system(Mishra et al., 2022).
7 Conclusion
Our goal is a system that can show how its an-
swers systematically follow from its own internal
beliefs, and materialize those beliefs in the pro-
cess. Entailer is, to our knowledge, the first sys-
tem to demonstrate this capability, achieved using
an “over-generate and verify” approach for each
backward-chaining step. Our evaluation suggests
that Entailer’s proof-based answer accuracy is simi-
lar to the “direct” QA accuracy, with the additional
benefit of providing a faithful, truthful chain of
reasoning showing how its answers follow from
its internal beliefs. The significance of this is that
these chains provide a window into the model’s in-
ternal beliefs and their relationships, allowing users
to both verify a system’s answer, or if the system
provides an incorrect answer, to identify the incor-
rect belief leading to that error. This in turn offers
new opportunities for a user to correct the system’s
misunderstanding by overriding a faulty belief, e.g.,
by adding a memory of user corrections/overrides
(Tandon et al., 2022), or by editing the model pa-
rameters directly (Mitchell et al., 2021), a step to-
wards interactive systems that can both explain
to, and learn from, users over time (Mishra et al.,
2022). Entailer data and models are available at
https://allenai.org/data/entailer. We look forward
to future developments in these directions.2086Limitations
We have shown how a neural system can expose
how its answers systematically follow from its
own internal beliefs, providing a window into the
model’s system of beliefs. While exciting, there
are several limitations with the current work and
opportunities for the future.
First, the system is not perfect at generating co-
herent chains of reasoning, sometimes producing
entailments that are invalid or nearly tautologous
(Section 5.4.1 and Figure 7). Improved proof gen-
eration and scoring techniques would help address
this.
Second, like others, we use textual entailment
as the basic reasoning operation, but the definition
of entailment remains somewhat imprecise (a valid
entailment is one that “a person would typically
infer.” (Dagan et al., 2013)), contributing to noise
in the model’s training data. A more precise charac-
terization of reasoning validity would help in both
generation and evaluation of reasoning chains.
Third, we assume the model is generally consis-
tent about its beliefs, but in some cases the model
may verify contradictory statements, making it less
clear what the model actually believes in such cases.
We currently do not handle such situations. Use
of a global notion of belief (rather than per ques-
tion) would be a valuable avenue to explore, e.g.,
(Kassner et al., 2021).
Fourth, as a practical matter, recursive construc-
tion of proofs is computationally expensive ( ≈360
seconds/question for up to depth-3 proofs for 4-way
multiple-choice, Appendix A.2). Improvements to
the search algorithm would allow faster experimen-
tation, and also help deploy the model in a practical
setting.
Finally, we have speculated that showing users
faithful, truthful chains of reasoning might be a ba-
sis for a conversational system, where users could
correct and teach the system in cases where it was
wrong. However, this is currently just a conjecture -
futures explorations into how this might be realized
would be valuable.
Ethics Statement
Like any other large-scale language model, despite
the best intentions, there is a risk of our model
producing biased or offensive statements as part
of its explanations. We release our models for
research purposes only.Acknowledgements
This research was made possible, in part, by fund-
ing from Open Philanthropy. We also thank Google
for providing the TPUs for conducting experi-
ments.
References20872088Appendix A. Entailer’s Backward Chaining Algorithm
Algorithm 1 Entailer’s backchaining algorithm for searching for the best proof tree(H)with score s(H)
for a hypothesis H.
A.1 Generating One Backward-Chaining Step
The procedure to find a 1-step inference is called
1 (H) in Algorithm 1 (line 19). Given a hy-
pothesis H, we use the angle H→Pto over-
generate a set of kalternative backward-chaining
stepsP⊢Husing nucleus sampling (line 21). We
then check that the model believes all the premises
p∈Pusing the H(=p)→Sangle, and that it
believes the inference step P⊢Hitself is valid (in-
dependent of whether the pare true or not) using
thePH→Sangle (line 24). The overall score,
denoting how well the 1-step proof supports the
hypothesis, combines the premise end entailment
scores as follows (line 25):
s(H) = (Πs(p)).s(P⊢H)
The highest-scoring proof P⊢His returned.
A.2 Backward Chaining
Procedure (H) in Algorithm 1 generalizes
this to multi-step entailments by recursively gen-erating support for each premise pinP(line 10).
The stopping condition is when the model’s direct
confidenceinpis greater than the proof-derived
confidence in p, i.e., c(p)> c(p). When this
condition is met, the subtree P⊢pforpis dis-
carded and pbecomes a leaf node of the tree (line
17). We also impose a maximum depth don the
tree.
This whole procedure is repeated for each can-
didate answer hypothesis (Section 3.1). Finally
the system selects the answer corresponding to the
hypothesis with the top-scoring proof s(H).
On our datasets, the average runtime per ques-
tion (4-way multiple-choice) is ≈80 seconds (depth
1 proofs, sample size k=6) or ≈360 seconds (up
to depth 3 proofs, sample size k=6) on a 48GB
GPU, due to the large number of candidate infer-
ence steps generated during the search.2089Appendix B. Crowdsourcing Instructions for Verifying Entailments (Section 4.1.2)2090Appendix C: Model Training
C.1 Dataset Preparation
Here we describe in detail how Entailer’s training
data is assembled.
1.We start with the training partition of the
EntailmentBank dataset, containining 1313
multiple choice questions each with an entail-
ment tree for their correct answer choice.
2.We convert the question + each answer option
to a hypothesis using four different, alternative
methods:
•Entailer’s current QA2D model, a re-
construction of that by (Demszky et al.,
2018) (Section 3.1).
• An in-house, rule-based QA2D utility
•An earlier version of Entailer’s QA2D
model.
•The original hand-written hypothesis
supplied in the EntailmentBank dataset.
If a source generates the same hypothesis for
two different answer choices, we choose not
to trust it for that question and discard it.
3.We “shred” the EntailmentBank entailment
trees into individual 1-step P ⊢H entailments,
producing 4175 1-step (valid) entailments us-
ing≈9000 true premises.
4.The crowdsourced P ⊢H instances, with an-
notations on whether premises are true and en-
tailment is true, are also added (Section 4.1.2).
5.Each EntailmentBank proof also comes with
a set of associated relevant facts (sentences),
only some of which are used in the entailment
proofs. We use these to create a "relevant
context" containing these sentences, sorted
into two buckets:
high-relevance: sentences actually used in
the proof of the correct answer
medium-relevance: the remaining sentences
This context is the same across all answer
options at this point.
6.For every hypothesis and premise appearing in
this dataset, we run a simple BM25 IR search
against a larger science text corpus (about
1.5 million sentences from a science-filtered
Wikipedia) to obtain noisier sentences to use
as alow-relevance context.7.For the final training dataset, we create 4 dif-
ferent contexts, using 4 sampling strategies.
First we create a "full" context of sentences
sorted into three buckets, with added noise:
high: high-relevance facts + 10% chance of
a random sentence from medium/low
medium: medium-relevance facts + 20%
chance of random sentences from
high/low
low: 5 low-relevance facts + 20% chance of
random sentence from high/medium
Then, we use four sampling strategies to cre-
ate actual contexts for the Entailer training
data. Each strategy is defined by the per-
sentence chance of a sentence coming from
one of the high/medium/low buckets. For
example, a context sampled with 0.2/0.4/0.6
means that 20% of its sentences came from
high, 40% from medium, an 60% from low.
If no sentence is selectable (e.g., a bucket is
exhausted), we use one from low. The final
context is syntactically expressed as “[HIGH]
<high sentences> [MEDIUM] <medium sen-
tences> [LOW] <low sentences> ”. The 4
sampling strategies are:
1/1/1 (full context, all available sentences)
0.5/0.5/0.5 (half of sentences from each cate-
gory)
0.2/0.4/0.6 (more emphasis on lower cate-
gories)
0/0/1 (only low sentences)
For the training set we store these as a list of
contexts, to be sampled at random when gen-
erating the training instances. We do the same
for hypotheses, so these are also sampled at
random.
8.The model is trained across the following
angles, each sampled equally: {H →P, H→V ,
HP→I, QAH →P, QAH →V , QAHP →I,
HC→P, HC →V , HPC →I, QAHC →P,
QAHC →V , QAHPC →I}
This is just the core angles H →P, H→V ,
HP→I, with optional QA and/or C added. The
individual premise verification (leaves) uses
the H→V angle (where H is now a premise),
for those angles we don’t have associated
proofs P so we limit to the angles: H →V ,
QAH→V , HC→V , QAHC →V
The full dataset is provided in the supplemen-
tary material, and will be released publically.2091C.2 Model Details
We train a T5-11B multi-angle model, Entailer,
following the multi-task setup similar to (Tafjord
and Clark, 2021) for the three functionalities de-
scribed in Table 1. We used the default hyperpa-
rameters (including the Adafactor optimizer) in the
T5 library,fine-tune the model for 20K steps with
batch size of 8, selecting the checkpoint with high-
est validation score (usually the final step).
At inference time, Entailer’s primary function is
to generate candidate entailments for each answer
choice for a given question, which are then scored
by the same model using its two verification angles.
To generate multiple explanations for a single in-
put (answer hypothesis, IR retrieved context), in
addition to the greedy decoding we use nucleus
sampling (Holtzman et al., 2019). For the experi-
ments in this paper we set “temperature= 2.0”, and
“top_p= 0.95”.2092Appendix D. Examples of Macaw Explanations and Entailer Proofs
A random selection of explanations, both good and bad, from the two systems (Section 5.3).2093