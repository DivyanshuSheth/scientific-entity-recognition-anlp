
Dongqi Pu Yifan Wang Vera Demberg
Department of Computer Science
Department of Language Science and Technology
Saarland Informatics Campus, Saarland University, Germany
{dongqipu,yifwang,vera}@lst.uni-saarland.de
Abstract
For text summarization, the role of discourse
structure is pivotal in discerning the core con-
tent of a text. Regrettably, prior studies on in-
corporating Rhetorical Structure Theory (RST)
into transformer-based summarization models
only consider the nuclearity annotation, thereby
overlooking the variety of discourse relation
types. This paper introduces the ‘RSTformer’,
a novel summarization model that comprehen-
sively incorporates both the types and uncer-
tainty of rhetorical relations. Our RST-attention
mechanism, rooted in document-level rhetor-
ical structure, is an extension of the recently
devised Longformer framework. Through rig-
orous evaluation, the model proposed herein ex-
hibits significant superiority over state-of-the-
art models, as evidenced by its notable perfor-
mance on several automatic metrics and human
evaluation.
1 Introduction
For writing a good summary of a long docu-
ment, it is of paramount importance to discern
the salient information within the text and to com-
prehend the intricate interconnections among its
various components. Contemporary leading-edge
systems for abstractive (long) text summarization
employ Transformer (Vaswani et al., 2017) encoder-
decoder architecture (Zaheer et al., 2020; Guo et al.,
2022). These sequence-to-sequence (seq2seq) mod-
els first transform the source document into a high-
dimensional content representation and then de-
code the predicted summary conditioned on the
representation (Belinkov and Bisk, 2018; Xu and
Durrett, 2019; Cao and Wang, 2022; Balachandran
et al., 2021). It has been demonstrated in the past
that such an architecture does a poor job of digging
high-level discourse structure during the encoding
phase (Lin et al., 2019; Zhang et al., 2020; KotoFigure 1: An example of RST tree: [ Rhetorical structure
theory (RST) is a theory of text organization. ][Al-
though the RST structure is difficult to annotate, ]
[there are still many scholars who have studied it. ]
et al., 2021a; de Wynter et al., 2023). However,
discourse structure is very important for deciding
what to include vs. not to include in the summary
(Marcu, 1997, 1999, 1998; Zhong et al., 2020).
Given that previous work has indicated that the
performance of neural language models can be en-
hanced through the incorporation of latent structure
information (Ettinger, 2020; Miaschi et al., 2020;
Qian et al., 2021; Pu and Sima’an, 2022), we will
here explore the integration of discourse relation
structure into the Longformer model (Beltagy et al.,
2020); this architecture has been shown to be par-
ticularly suitable for encoding long input texts.
Rhetorical Structure Theory (RST) serves as a
discourse framework designed to articulate the in-
terrelationships among sentences at the document
level. This framework distinguishes a plethora
of coherence relations delineating the manner in
which two text segments are interconnected (e.g.,
one segment might give a reason for a claim made
in another segment, or alternatively, two segments
may contrast with one another). Moreover, RST
distinguishes between paratactic relations, where
both segments carry equivalent discourse impor-
tance, and hypotactic relations, which classify the
segment of greater centrality to the overarching dis-
course structure as the ‘nucleus’ and the less central
one as the ‘satellite’. Figure 1 shows a simple exam-
ple of an RST tree. In this instance, EDU1 serves5574as the nucleus of the elaboration relation, whereas
the combination of EDUs 2 and 3 constitutes the
satellite of said relation. Furthermore, we can see
that EDU3 assumes a more central role within the
concession relation, hence it is marked as its nu-
cleus, while EDU2 holds less important: if EDU2
was left out, the elaboration relation between EDUs
1 and 3 would still hold, but if EDU3 was removed,
an elaboration relation between EDU1 and EDU2
would not hold, and the coherence would be lost.
As has been recognized early on (Marcu, 1997,
1999), this discourse information can be effectively
used in summarization tasks.
While there have been some previous attempts at
integrating discourse structure into neural text sum-
marization models, as seen in Gabriel et al. (2021);
Dong et al. (2021); Xiao et al. (2020); Xu et al.
(2020); Cohan et al. (2018), these approaches do
not utilize relation labels and solely consider the
1-best RST tree obtained from preprocessing of a
discourse parser. We argue that this leads to two
significant issues: Firstly, information pertaining to
relation type is overlooked, despite its known rele-
vance to the summarization task. Secondly, there
may be benefits in considering distributions over co-
herence relation labels, rather than limiting analysis
to the 1-best results (Pu and Sima’an, 2022). One
reason is that external discourse parsers are known
to perform poorly on out-of-domain data (Atwell
et al., 2022; Liu et al., 2021b; Gessler et al., 2021;
Koto et al., 2021b; Liu et al., 2020; Nguyen et al.,
2021), and may hence propagate errors into the
summarization model. There is a subsequent risk
that these errors will be incrementally amplified
during back-propagation, thus potentially impair-
ing the model’s performance. A second reason is
that there might inherently be several coherence re-
lations holding at the same time (Yung et al., 2022),
which might be beneficial to represent through the
distributions of the discourse structure. Hence, we
posit that the output of the RST parser holds greater
significance when it not only provides the model
with the n-best results but also conveys the remain-
ing uncertainty associated with them.
In the remainder of the paper, we explore
whether incorporating the labeled discourse rela-
tion structure with uncertainty, which can be un-
derstood as the distributions of discourse structure,
into the attention mechanism can effectively aug-
ment the performance of neural summarization
models. Our main contributions are as follows:•We represent a generic approach for infus-
ing labeled discourse relations with uncer-
tainty into the encoder’s self-attention layer of
Longformer, wherein the self-attention heads
are made to specialize in specific discourse
categories. Additionally, our modules are
orthogonal to the choice of the underlying
encoder-decoder Transformer-based architec-
ture, thereby enabling them to be seamlessly
incorporated into other advanced models.
•We provide empirical evidence supporting
the notion that conveying uncertainty and in-
troducing labeled discourse relations to the
Transformer are complementary actions, both
significantly contributing to the enhancement
of the final performance. Our model also sur-
passes current state-of-the-art models across
multiple evaluation metrics.
•Quantitative and qualitative analyses show
that our model exceeds the baseline model
in both novel word generation and factual con-
sistency checking. Furthermore, our model
comes closer to human answers in terms of
sentence alignment and overall generation
quality.
2 Related Work
2.1 Text Summarization with RST
Rhetorical Structure Theory offers a structured
paradigm for describing how various discourse
units relate to one another in a text. The RST tree
structure, as illustrated in Marcu (1997) and Louis
et al. (2010), can serve as a valuable tool for content
selection in the process of summarization.
For instance, Kikuchi et al. (2014) characterize
the dependencies between sentences by construct-
ing RST trees and pruning the parts that are marked
as ‘satellites’ while preserving the important con-
tent (‘nucleus’) of the document as predicted sum-
maries. Although RNN-based models are some-
times argued to be sufficient in implicitly learning
discourse and semantic relations, Liu et al. (2019)’s
work underscores the value of explicitly integrating
RST trees into the summarization model, thereby
highlighting the significance of discourse relation
for the neural summarization network. It is also
worth noting that while the attention mechanism
can more effectively uncover discourse relations
without explicit training, it tends to unearth only
superficial discourse structure and is often prone to
mistakes (Vig and Belinkov, 2019; Sachan et al.,55752021; Xiao et al., 2021; Huber and Carenini, 2022;
Davis and van Schijndel, 2020).
Although attention-based models excel in exe-
cuting downstream tasks such as summarization,
the explicit incorporation of discourse relations can
yield additional benefits. Work highly related to
ours includes the model of Xiao et al. (2020), which
improves the performance of an extractive summa-
rization model by transmuting the RST structure
into a dependency tree and explicitly integrating it
into the computation of the attention mechanism.
Follow-up works Xu et al. (2020) and Dong et al.
(2021) further confirm the influence of RST struc-
ture on improving attention mechanism by incorpo-
rating discourse structure into a transformer-based
model and a graph neural network model for the
summarization task, respectively. However, all of
these neural strategies apply the one-best structure
derived from an external discourse parser.
2.2 Text Summarization with Longformer
The Longformer model (Beltagy et al., 2020),
based on a sparse attention mechanism, is consid-
ered to be an effective means for processing long
documents. Its essence is to make each token only
pay attention to a window of a certain size, so that
the time complexity of the model is reduced from a
quadratic correlation with the text length to a linear
correlation. Longformer-related models have since
been employed in several summarization tasks (e.g.,
Zhang et al., 2022; Otmakhova et al., 2022; Elaraby
and Litman, 2022; Xie et al., 2022; Pu et al., 2022).
At the same time, there have also been recent
attempts at integrating text structure information
with the Longformer model in summarization tasks.
Huang and Kurohashi (2021) first employ the Long-
former to encode input documents and propose an
extractive summarization model based on a hetero-
geneous graph of discourse and coreference rela-
tions. Liu et al. (2021a) extend the Longformer to
model different types of semantic nodes in the orig-
inal text as heterogeneous graphs and directly learn
relations between nodes. Specifically, they treated
tokens, entities, and sentences as different types of
nodes, and the multiple sparse masks as different
types of edges to represent relations (e.g., token-
to-token, token-to-sentence). Elaraby and Litman
(2022) improve the performance of the strong base-
line Longformer by integrating argument role label-
ing into the summarization process to capture the
argumentative structure of legal documents. Ruanet al. (2022) and Cao and Wang (2022) enhance
extractive and abstractive summarization tasks, re-
spectively, by introducing the text’s hierarchical
structure (e.g., section title) into the Longformer
model.
3 Proposed Approach
In the realm of document discourse parsing, the
performance of the RST parser leaves much to be
desired (Yu et al., 2022; Nguyen et al., 2021; Liu
et al., 2021b), with parsing performance deteri-
orating in conjunction with escalating document
complexity. Merely passing the 1-best RST tree
risks imparting misleading information to the sum-
marization model.
Inspired by Pu and Sima’an (2022), the approach
to alleviating the aforementioned problems is that
we retain uncertainty inside the parser, which can
convey the parser’s confidence in each discourse
relation. Furthermore, we contend that discourse re-
lation labels (types) can provide more fine-grained
labeled probability distributions that can assist at-
tention heads of the Transformer-based model to
capture the importance of different discourse units.
This in turn would contribute to a more precise es-
timation of the context vector and can enhance the
quality of source document encoding. Discourse
parsers tend to be more precise (and have more
peaked probability distributions) for local coher-
ence relations, which span only a short amount of
text, compared to global relations spanning large
portions of a text. This aligns well with the dilated
(yet still limited) sliding window attention mech-
anism of the Longformer (Beltagy et al., 2020).
We, therefore, integrate the probability distribu-
tions over local coherence relations into the atten-
tion window wof the Longformer.
3.1 RST Tensor with Labeled Distributions
The discourse-driven neural seq2seq summariza-
tion task can be modeled as follows:
P(t|s, d)≈/productdisplayP(t|t,encode(s , d)) (1)
In the above equation, s,t, and ddenote the
source, target sequence, and discourse representa-
tion, respectively. Tsignifies the target sequence
length and encode( ·)represents the encoder of the
summarization model. Previous research (Xu et al.,
2020; Cohan et al., 2018; Dong et al., 2021; Li5576
et al., 2020; Chen and Yang, 2021) has confirmed
that the probability of generating appropriate sum-
maries by incorporating dinto the model’s encoder
is significantly greater than the probability of gener-
ating proper summaries without the incorporation
ofd.
Our main idea is to find a better method to in-
corporate discourse structure d. To inject discourse
structure, we first apply a ‘matrixization’ approach
to represent the discourse structure and produce a
compact tensor representation appropriate for the
Longformer model (Pu and Sima’an, 2022).
Figure 2 illustrates by an example how we con-
vert the graph of all potential RST relations (in-
cluding the n-best RST trees present within the
graph) into a three-dimensional labeled discourse
distribution (LDD) tensor. The x-axis and y-
axis of the tensor represent the elementary dis-
course units (EDUs) in the source document, while
the z-axis represents the type of discourse rela-
tion. Each point represents a confidence value
p(edu, edu, l)∈[0,1]⊆Rof an elementary
discourse unit educonnecting to another elemen-
tary discourse unit edufrom source text via the
relation l. It should be noted that the generation
of the LDD tensor should meet the conditions: 1)
p(edu, edu) = 0 , as no unit is dependent on it-
self; 2) we only extract the relation probability of
nucleus units, since nucleus EDUs are more central
to the text and should be given more attention. Inthe example shown in Figure 1, we only extract
the discourse relation probabilities of EDU1 and
EDU3.
3.2 RST Sparse Attention
We propose a novel Longformer-Encoder-Decoder
Summarization model: RSTformer, which incor-
porates LDD into each layer of the Longformer
encoder in a discourse-aware manner. Given that
each encoder layer shares an identical configura-
tion, Figure 3 displays one layer architecture of our
proposed model.
The standard dilated sliding window attention
layer of Longformer employs a multi-head fixed-
size window attention mechanism. For a pre-
specified window size w, each token attends towtokens on either side. For an input sequence
of length T, the input of dilated sliding window
attention heads in the RSTformer layer comprises
the hidden representation tensor X∈R
and labeled discourse distribution tensor LDD∈
R, where d represents the size of
the hidden representation and hdenotes the number
of attention heads.
As usual in multi-head self-attention, we mul-
tiply the text feature representation tensor with
q,k,v∈Rto obtain the corresponding
Q∈R,K∈R, and V∈R
matrices, where d=d/h. Subsequently, the
attention weight matrix is obtained by:
S =Q·K
√
d(2)
Longformer utilizes two sets of projections, Q,
K,Vto compute the attention scores of sliding
window attention, and Q,K,Vto compute at-
tention scores for global attention. Notably, Q,
K,Vare all initialized with values that match Q,
K,Vrespectively. The dilated sliding window
attention operates by calculating a fixed number
of the diagonals of QKthrough sliding chunks
query-key multiplication. This process yields a re-
sulting tensor S∈R. Similarly, LDD
andVadopt the same chunk method as employed
by Longformer to acquire the sliding window at-
tention matrix.
It should be noted here that we inject the sliding
window attention tensor Sobtained from the pre-
ceding computation by element-wise multiplication
with the LDD tensor:
S⊙LDD (3)5577
The motivation behind employing element-wise
multiplication is to allow the learning parameters
of the attention mechanism ‘dynamically’ to opti-
mize the summarization objective but also diverge
the least from the parser probabilities in discourse
distribution (Pu and Sima’an, 2022). The estima-
tion of attention weights is adjusted to align with
the utility of discourse relations for the ultimate
summarization task.
Following, the obtained weights are further pro-
cessed using the softmax function to derive the final
tensor representing the discourse-infused distribu-
tion:
N = softmax(S ⊙LDD) (4)
It should be emphasized that each attention head
is assigned a different discourse matrix LDDfor
a specific relation l, This allocation enables heads
to concentrate on and learn different discourse la-
bels (Pu and Sima’an, 2022). In doing so, attention
heads can be specialized and acquire a deeper un-
derstanding of the impact of discourse labels.
Finally, the discourse-injected weights Nare
multiplied with the value matrix Vto obtain the
attention weights Mfor this layer and then transferMto the next Longformer encoder layer for further
computation.
M = N ·V (5)
4 Experiments and Analysis
4.1 Experimental Setup
Parser We employ an external RST parser called
DMRST (Liu et al., 2021b, 2020) to automatically
parse the source documents. The probability or
uncertainty of discourse relations is extracted from
the logits layer of the DMRSTmodel. In cases
where DMRST fails to parse the source document,
we simply skip the LDD generation process and
proceed with the normal Longformer procedure.
Datasets We conduct our experiments on three re-
cent long document summarization datasets: Book-
Sum Chapter (Kryscinski et al., 2022), eLife (Gold-
sack et al., 2022), and Multi-LexSum (Shen et al.,
2022). We choose these datasets because of their
high heterogeneity and we want to investigate
whether our approach can maintain adequate gen-
eralization performance across different data do-
mains. Table 1 shows the statistics of the datasets.5578
Coverage refers to the percentage of words in
the summary that are from the source document.
A higher coverage ratio indicates that a greater
proportion of summary words are derived directly
from the source text. It is mainly used to measure
the degree of derivation of the summary from the
text. Density is defined as the average length of the
extracted segments to which each summary word
belongs (Segarra Soriano et al., 2022). Compres-
sion ratio is defined as the ratio between the length
of the source document and summary (Scialom
et al., 2020).
Evaluation Metrics We evaluate the quality of
different summarization systems using Rouge-{1,
2, L} score (Lin, 2004), BERTscore (Zhang et al.,
2019), Meteor score (Banerjee and Lavie, 2005),
{1, 2, 3, 4}-gram novelty (Kry ´sci´nski et al., 2018),
SummaC (Laban et al., 2022) and sentence align-
ment (Liu and Liu, 2021) as criteria for the model’s
effectiveness.
In detail, Rouge-{1,2} is mainly evaluated based
on the co-occurrence of {1,2}-gram in summary,
while the calculation of Rouge-L uses the longest
common subsequence. BERTScore is used to com-
pute the semantic similarity score of candidate sen-
tences to reference sentences through contextual
embedding. Meteor is an improvement based on
BLEU (Papineni et al., 2002), which also consid-
ers the impact of sentence fluency and synonyms
on semantics. {1, 2, 3, 4}-gram novelty indicates
the capacity of the model to generate new words,
rather than merely extracting words from the origi-
nal text. SummaC detects semantic inconsistency
by segmenting documents into sentence units and
aggregating scores between sentence pairs.
Training and Inference Hyper-parameters for
the baseline, proposal models, and ablation models
are all kept identical. We adopt the same config-
uration as Longformer (Beltagy et al., 2020): All
experiments are optimized using Adam (Kingma
and Ba, 2014) ( β=0.9,β=0.999,ϵ=10,
and weight decay = 0.1) with Adafactor (Shazeer
and Stern, 2018), the number of warm-up steps is1500 , and the initial learning rate is set to 3ewith
cosine learning rate schedule. We also apply Noisy-
Tune (Noise lambda = 0.2) (Wu et al., 2022) for
efficient fine-tuning. The size of the local attention
window is w=1024 , and we choose cross-entropy
as loss function.
During the training phase, we save the check-
point with the highest Rouge-2 F1 score on the
validation set as the final model. The experiments
are all run for 30epochs using a batch size of 1
with early stopping implemented. In order to pre-
vent over-fitting, we set the dropout rate to 0.1in
all layers of the model. For model inference, we
adopt a beam size of 4with a length penalty of 2.0,
and we set the no-repeat n-gram size to 3.
4.2 Results
The experimental results for each model are pre-
sented in Table 2. To estimate a lower bound in
performance, we simply use the original document
as the summary. Further trivial models include the
Lead-3 model which simply picks the first three
sentences of the document as the summary. Lead-
K similarly extracts the first K sentences of the
document, until a similar length as the reference
summary is reached. Longformer and state-of-the-
art (SOTA) models serve as our baseline and com-
parison models, respectively. The remaining two
models are the models we proposed. RSTformer
(w/o relations) refers to the model that preserves
whether there are relations between EDUs and ig-
nores the type of relations by summing the third
dimension of LDD tensors. RSTformer (w rela-
tions) is the final model we propose, with the only
difference being the inclusion of the impact of RST
types.
Both RSTformer versions are found to outper-
form the baseline model on various measures. The
higher scores reflect an improved choice of words
(Rouge & Meteor scores), and also the semantics
of the text (BERTscore).The proposed model,5579RSTformer, demonstrates robust generalization ca-
pabilities across different datasets, highlighting its
promising potential in various summarization do-
mains.
In most of our summarization experiments, we
furthermore find that incorporating discourse struc-
ture with types provides better experimental re-
sults than the discourse distributions without types,
even beating the SOTA model on our experimental
datasets. This observation suggests that providing
more discourse information, especially type distri-
bution probabilities, is a promising approach.
Ablation Study We also define two additional
control conditions to examine the impact of RST
attention ( LDD ) on model performance:
•Without Attention Calculation (WAC) : We
skip the previous calculation of attention
weights, and directly replace attention weights
with LDD tensor.
•Random Identical Attention (RIA) : We as-
sign fixed random values to LDD tensor, re-
gardless of the probability of discourse rela-
tions.
Table 3 shows that the RST attention cannot
fully replace the calculation of the attention mech-
anism. Although the performance is significantly
lower than the baseline model, its main noteworthy
advantage is that it saves considerable computa-
tions and parameters. Experiments by introducing
random noise demonstrate that random values do
indeed negatively impact the model’s performance.
Furthermore, it also confirms the effectiveness of
incorporating the probability distributions of dis-
course structure.
Human Evaluation To better analyze the effec-
tiveness of our model, we randomly select 10 sam-
ples from the BookSum dataset and hire human
annotators to conduct the human evaluation. The
recruited annotators are all master’s students or
doctoral students with computer science-related
or computational linguistics-related backgrounds.
All annotators were compensated with the standard
hourly salary set by the university. At the time of
evaluation, we provide 3 candidate summaries for
each source document, namely outputs from our
final proposed model and baseline model, along
with the ground truth summary. Each instance is
assigned to 3 participants who are instructed to rate
the faithfulness, informativeness, readability, andconciseness of the candidate summaries on a scale
of 1 to 5. They are also supposed to give an overall
rank of three summaries and identify which one is
generated by humans. Detailed information regard-
ing the human evaluation process can be found in
Appendix B. Table 4 reports the human evaluation
results.
For each human evaluation indicator, we com-
pute the average value to represent whether the can-
didate system has good performance in that indica-
tor. Best and Worst indicate the proportion of times
a summary by a particular model is judged to be
best or worst among the three options. While neural
summarization models still exhibit a notable per-
formance gap when compared to human-generated
summaries, our proposed model consistently out-
performs the baseline model across all metrics.
4.3 Analysis
Sentence Alignment We examine the alignment
distributions of generated summaries to explore
whether the improved model can be closer to
human-summarized text (Liu and Liu, 2021). Our
results are depicted in Figure 4 and Appendix C.
From a broader perspective, the sentence align-
ment distribution of our proposed models is more
closely aligned with that of human summarizers.
In addition, the generated summaries produced by
our models demonstrate a greater emphasis on the
content of the second half of the document, result-
ing in summaries that are more comprehensive and5580
coherent in nature.
N-gram Novelty & Inconsistency Detection We
also study the level of abstractiveness and factual
consistency in the generated summaries. To evalu-
ate the abstractiveness, we employed N-gram nov-
elty as a measure to determine whether the modelcan generate words that are not present in the origi-
nal text, rather than solely extracting content from
the source document. For inconsistency detection,
we utilize the latest SummaC method (Laban et al.,
2022) for testing. Our results are shown in Figure
5 and Figure 6 respectively.
Compared with the baseline model, incorporat-
ing discourse information into the model does in-
crease the ability of the model to generate novel
words, especially evident in the context of 3-gram5581
and 4-gram, the gap becomes more prominent. In
addition, the proposed model also performs better
than the baseline model in terms of model con-
sistency checks. Due to the increased ability to
generate creative words, the semantic coherence
ability of the models incorporating typed discourse
relations is lower than that of models without typed
discourse relations.
5 Conclusion
This paper introduces a novel supervised discourse
enhanced Longformer model. This strategy mainly
improves the local attention mechanism in the
Longformer model by leveraging the rhetorical
structure as uncertainty distributions. The exper-
imental findings provide strong evidence that the
proposed approach is straightforward, and can ef-
fectively employ the discourse structure of source
documents to improve the summary performance
of Longformer. Furthermore, this strategy also has
a high potential capability for application in other
seq2seq natural language tasks.
6 Limitations
The present study has certain limitations that
should be acknowledged. Firstly, the RST parsing
task itself is known to be highly complex and chal-
lenging, and achieving high accuracy in this task is
not guaranteed. Although we have utilized the most
high-performing parser, there is still room for fur-
ther improvement in the RST parsing performance,
which could potentially enhance the downstream
summarization task.
Another limitation pertains to the size of the data
used for human evaluation. Due to the nature of
long document summarization and the length ofthe original texts (often spanning several pages),
scaling up the evaluation process, such as through
crowd-sourcing, becomes difficult. Consequently,
we are only able to evaluate a limited number of 10
documents, which may not be fully representative
of the entire dataset.
Furthermore, another potential risk in our study
is the limitation in obtaining an unlimited number
of training samples. The data samples investigated
are often small subsets of real-world data or may
exhibit certain biases, which may not accurately
reflect the distribution of real-world data. Although
we have verified the effectiveness of our model
using highly diverse and heterogeneous datasets
from different domains, it is important to note that
the model’s performance on the specific dataset of
interest may not be as robust as its performance on
unseen real-world data.
Finally, both training and evaluating the models
require significant computational resources. De-
spite our attempts to optimize the computation by
replacing the original attention calculation with
the RST attention tensor (as demonstrated in the
ablation experiment), we have not achieved satis-
factory results. The high computational costs pose
a challenge, as they result in increased human and
material resources required for the model.
7 Ethics Considerations
The datasets we use are all public, and our experi-
ment processes have no privacy disclosure issues.
As for human evaluation, all participants are vol-
untary and paid, and come from master or doctoral
students with a background in computer science
or computational linguistics, and all of them are
proficient in English. They first need to read the
instructions and evaluate without revealing which
model generates which summary.
Acknowledgements
This project has received funding from the Euro-
pean Research Council (ERC) under the European
Union’s Horizon 2020 Research and Innovation
Programme (Grant Agreement No. 948878). We
are grateful to the anonymous reviewers and area
chairs for their exceptionally detailed and helpful
feedback.5582References5583558455855586A Appendix: RST Relation Category
B Appendix: Questionnaire of Human
Evaluation
Here we provide a more detailed description of the
criterion in our human evaluation.
•Faithfulness
1. Completely hallucinated content
2.A lot of hallucinated content and factual
mistakes
3.Most content is supported by the source
document
4.Only one or two characters or events con-
tradicted or not mentioned in the source
5.All information in the summary is faith-
ful/supported by the source
•Informativeness
1.No important information in the source
is covered in the summary
2.Only covers a small fraction of the source
document information; one cannot learn
the main content of the story from only
the summary
3.Covers around half of the important
points from the source; one can learn
the main content of the story from only
the summary
4.Only a few important points are missing
in the summary
5.All important information is summarized
•Readability
1. Not understandable at all
2.Hard to understand the content of the
summary
3.The summary is overall readable, with
most sentences correct and fluent
4.Easy to understand, with only occasional
grammatical mistakes or incoherent sen-
tences5.Fluent, with minor or no grammatical
mistakes, coherent sentences, and clear
structure
•Conciseness
1.All information in the summary is redun-
dant or unimportant
2.Most of the information in the summary
is redundant or unimportant
3.Around half of the content in the sum-
mary is redundant
4.Only a few points in the summary are
redundant
5.No information in the summary is redun-
dant
User interface and instructions for rating and
ranking can be found in Figure 7 and Figure 8.
C Appendix: Sentence Alignment for
Other Datasets55875588ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7
/squareA2. Did you discuss any potential risks of your work?
Section 7
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract and Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3 and Section 4
/squareB1. Did you cite the creators of artifacts you used?
Section 3 and Section 4
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 3 and Section 4
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 3 and Section 4
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 45589/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
No response.
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.2
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Section 4.2
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Section 4.2
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Section 4.2
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Section 4 and Appendix
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.5590