
Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca J. Passonneau, Rui Zhang
Pennsylvania State University
{snigdha, arzoo, rjp49, rmz5227}@psu.edu
Abstract
Named Entity Recognition (NER) in Few-Shot
setting is imperative for entity tagging in low
resource domains. Existing approaches only
learn class-specific semantic features and inter-
mediate representations from source domains.
This affects generalizability to unseen target
domains, resulting in suboptimal performances.
To this end, we present CONTNER , a novel
contrastive learning technique that optimizes
the inter-token distribution distance for Few-
Shot NER. Instead of optimizing class-specific
attributes, CONTNER optimizes a general-
ized objective of differentiating between token
categories based on their Gaussian-distributed
embeddings. This effectively alleviates overfit-
ting issues originating from training domains.
Our experiments in several traditional test do-
mains (OntoNotes, CoNLL’03, WNUT ’17,
GUM) and a new large scale Few-Shot NER
dataset (Few-NERD) demonstrate that, on aver-
age,CONTNER outperforms previous meth-
ods by 3%-13% absolute F1 points while
showing consistent performance trends, even
in challenging scenarios where previous ap-
proaches could not achieve appreciable perfor-
mance. The source code of CONTNER will
be available at: https://github.com/
psunlpgroup/CONTaiNER .
1 Introduction
Named Entity Recognition (NER) is a fundamental
NLU task that recognizes mention spans in un-
structured text and categorizes them into a pre-
defined set of entity classes. In spite of its chal-
lenging nature, recent deep-learning based ap-
proaches (Huang et al., 2015; Ma and Hovy, 2016;
Lample et al., 2016; Peters et al., 2018; Devlin et al.,
2018) have achieved impressive performance. As
these supervised NER models require large-scale
human-annotated datasets, few-shot techniques that
can effectively perform NER in resource constraint
settings have recently garnered a lot of attention.Figure 1: Contrastive learning dynamics of a token ( Is-
lands ) with all other tokens in an example sentence from
GUM (Zeldes, 2017). CONTNER decreases the em-
bedding distance between tokens of the same category
(PLACE ) while increasing the distance between differ-
ent categories ( QTY . and O).
Few-shot learning involves learning unseen
classes from very few labeled examples (Fei-Fei
et al., 2006; Lake et al., 2011; Bao et al., 2020).
To avoid overfitting with the limited available data,
meta-learning has been introduced to focus on how
to learn (Vinyals et al., 2016; Bao et al., 2020).
Snell et al. (2017) proposed Prototypical Networks
to learn a metric space where the examples of a
specific unknown class cluster around a single pro-
totype. Although it was primarily deployed in com-
puter vision, Fritzler et al. (2019) and Hou et al.
(2020) also used Prototypical Networks for few-
shot NER. Yang and Katiyar (2020), on the other
hand, proposed a supervised NER model that learns
class-specific features and extends the intermediate
representations to unseen domains. Additionally,
they employed a Viterbi decoding variant of their
model as "StructShot".
Few-shot NER poses some unique challenges
that make it significantly more difficult than other
few-shot learning tasks. First, as a sequence label-
ing task, NER requires label assignment according
to the concordant context as well as the dependen-
cies within the labels (Lample et al., 2016; Yang
and Katiyar, 2020). Second, in NER, tokens that
do not refer to any defined set of entities are labeled
asOutside (O). Consequently, a token that is la-
beled as Oin training entity set may correspond
to a valid target entity in test set. For prototypical
networks, this challenges the notion of entity exam-6338ples being clustered around a single prototype. As
for Nearest Neighbor based methods such as Yang
and Katiyar (2020), they are initially “pretrained"
with the objective of source class-specific super-
vision. As a result, the trained weights will be
closely tied to the source classes and the network
will project training set O-tokens so that they
get clustered in embedding space. This will force
the embeddings to drop a lot of useful features per-
taining to its true target entity in the test set. Third,
in few-shot setting, there are not enough samples
from which we can select a validation set. This
reduces the capability of hyperparameter tuning,
which particularly affects template based methods
where prompt selection is crucial for good perfor-
mance (Cui et al., 2021). In fact, the absence of
held-out validation set puts a lot of earlier few-shot
works into question whether their strategy is truly
"Few-Shot" (Perez et al., 2021).
To deal with these challenges, we present a novel
approach , CONTNER that harnesses the power
of contrastive learning to solve Few-Shot NER.
CONTNER tries to decrease the distance of to-
ken embeddings of similar entities while increas-
ing it for dissimilar ones (Figure 1). This enables
CONTNER to better capture the label depen-
dencies. Also, since CONTNER is trained with
a generalized objective, it can effectively avoid the
pitfalls of O-tokens that the prior methods strug-
gle with. Lastly, CONTNER does not require
any dataset specific prompt or hyperparameter tun-
ing. Standard settings used in prior works (Yang
and Katiyar, 2020) works well across different do-
mains in different evaluation settings.
Unlike traditional contrastive learners (Chen
et al., 2020; Khosla et al., 2020) that optimize sim-
ilarity objective between point embeddings, CON-
TNER optimizes distributional divergence ef-
fectively modeling Gaussian Embeddings. While
point embedding simply optimizes sample dis-
tances, Gaussian Embedding faces an additional
constraint of maintaining class distribution through
the variance estimation. Thus Gaussian Embedding
explicitly models entity class distributions which
not only promotes generalized feature representa-
tion but also helps in few-sample target domain
adaptation. Previous works in Gaussian Embed-
ding has also shown that mapping to a density
captures representation uncertainties (Vilnis and
McCallum, 2014) and expresses natural asymme-
tries (Qian et al., 2021) while showing better gen-eralization requiring less data to achieve optimal
performance (Bojchevski and Günnemann, 2017).
Inspired by these unique qualities of Gaussian Em-
bedding, in this work we leverage Gaussian Em-
bedding in contrastive learning for Few-Shot NER.
A nearest neighbor classification scheme dur-
ing evaluation reveals that on average, CON-
TNER significantly outperforms previous SOTA
approaches in a wide range of tests by up to 13% ab-
solute F1-points. In particular, we extensively test
our model in both in-domain and out-of-domain ex-
periments as proposed in Yang and Katiyar (2020)
in various datasets (CoNLL ’03, OntoNotes 5.0,
WNUT ’17, I2B2) . We also test our model in
a large dataset recently proposed for Few-Shot
NER - Few-NERD (Ding et al., 2021) where CON-
TNER outperforms all other SOTA approaches
setting a new benchmark result in the leaderboard.
In summary, our contributions are as follows:
(1) We propose a novel Few-Shot NER approach
CONTNER that leverages contrastive learning
to infer distributional distance of their Gaussian
Embeddings. To the best of our knowledge we
are the first to leverage Gaussian Embedding in
contrastive learning for Named Entity Recognition.
(2) We demonstrate that CONTNER represen-
tations are better suited for adaptation to unseen
novel classes, even with a low number of support
samples. (3) We extensively test CONTNER in
a wide range of experiments using several datasets
and evaluation schemes. In almost every case, our
model largely outperforms present SOTAs estab-
lishing new benchmark results.
2 Task Formulation
Given a sequence of ntokens {x, x, . . . x},
NER aims to assign each token xto its correspond-
ing tag label y.
Few-shot Setting For Few-shot NER, a model
is trained in a source domain with a tag-set {C}
and tested in a data-scarce target domain with a
tag-set {C}where i, jare index of different tags.
Since{C} ∩ {C}=∅, it is very challenging
for models to generalize to unseen test tags. In an
N-way K-shot setting, there are Ntags in the target
domain |{C}|=N, and each tag is associated
with a support set with Kexamples.
Tagging Scheme For fair comparison of CON-
TNER against previous SOTA models, we fol-
low an IO tagging scheme where I-type repre-6339
sents that all of the tokens are inside an entity, and
O-type denotes all the other tokens (Yang and
Katiyar, 2020; Ding et al., 2021).
Evaluation Scheme To compare with SOTA
models in Few-NERD leaderboard (Ding et al.,
2021), we adpot episode evaluation as done by the
authors. Here, a model is assessed by calculating
the micro-F1 score over multiple number of test
episodes. Each episode consists of a K-shot sup-
port set and a K-shot unlabeled query (test) set to
make predictions . While Few-NERD is explicitly
designed for episode evaluation, traditional NER
datasets (e.g., OntoNotes, CoNLL’03, WNUT ’17,
GUM) have their distinctive tag-set distributions.
Thus, sampling test episodes from the actual test
data perturbs the true distribution that may not rep-
resent the actual performance. Consequently, Yang
and Katiyar (2020) proposed to sample multiple
support sets from the original development set and
use them for prediction in the original test set. We
also use this evaluation strategy for these traditional
NER datasets.
3 Method
CONTNER utilizes contrastive learning to op-
timize distributional divergence between different
token entity representations. Instead of focusing
on label specific attributes, this contradistinction
explicitly trains the model to distinguish between
different categories of tokens. Furthermore, mod-
eling Gaussian Embedding instead of traditionalpoint representation effectively lets CONTNER
model the entity class distribution, which incites
generalized representation of tokens. Finally, it
lets us carefully finetune our model even with a
small number of samples without overfitting which
is imperative for domain adaptation.
As demonstrated in Figure 2, we first train our
model in source domains. Next, we finetune model
representations using few-sample support sets to
adapt it to target domains. The training and finetun-
ing of CONTNER is illustrated in Algorithm 1.
Finally, we use an instance level nearest neighbor
classifier for inference in test sets.
3.1 Model
Figure 2 shows the key components of our model.
To generate contextualized representation of sen-
tence tokens, CONTNER incorporates a pre-
trained language model encoder PLM. For proper
comparison against existing approaches, we use
BERT (Devlin et al., 2018) as our PLM encoder.
Thus given a sequence of ntokens [x, x, . . . , x],
we take the final hidden layer output of the PLM as
the intermediate representations h∈R.
[h,h, . . . ,h] =PLM([x, x, . . . , x])(1)
These intermediate representations are then chan-
neled through simple projection layer for generat-
ing the embedding. Unlike SimCLR (Chen et al.,
2020) that uses projected point embedding for con-
trastive learning, we assume that token embeddings6340follow Gaussian distributions. Specifically, we em-
ploy projection network fandffor producing
Gaussian distribution parameters:
µ=f(h),Σ= ELU ( f(h))+(1+ ϵ)(2)
where µ∈R,Σ∈Rrepresents mean and
diagonal covariance (with nonzero elements only
along the diagonal of the matrix) of the Gaus-
sian Embedding respectively; fandfare imple-
mented as ReLU followed by single layer networks;
ELU for exponential linear unit; and ϵ≈efor
numerical stability.
3.2 Training in Source Domain
For calculating the contrastive loss, we consider
the KL-divergence between all valid token pairs in
the sampled batch. Two tokens xandxare con-
sidered as positive examples if they have the same
label y=y. Given their Gaussian Embeddings
N(µ,Σ)andN(µ,Σ), we can calculate their
KL-divergence as following:
D[N||N] =D[N(µ,Σ)||N(µ,Σ)]
=1
2
Tr(ΣΣ)
+ (µ−µ)Σ(µ−µ)
−l+ log|Σ|
|Σ|
(3)
Both directions of the KL-divergence are calculated
since it is not symmetric.
d(p, q) =1
2(D[N||N] +D[N||N])
(4)
We first train our model in resource rich source
domain having training data X. At each training
step, we randomly sample a batch of sequences
(without replacement) X ∈ Xfrom the training
set having batch size of b. For each (x, y)∈
X, we obtain its Gaussian Embedding N(µ,Σ)
by channeling the corresponding token sequence
through the model (Algorithm 1: Line 3-6). We
find in-batch positive samples Xfor sample pand
subsequently calculate the Gaussian embedding
loss of xwith respect to that of all other valid
tokens in the batch:
X={(x, y)∈ X | y=y, p̸=q} (5)ℓ(p) =−logPexp(−d(p, q))/|X|
Pexp(−d(p, q))(6)
In this way we can calculate the distributional diver-
gence of all the token pairs in the batch (Algorithm
1: Line 7-10 ). We do not scale the contrastive loss
by any normalization factor as proposed by Chen
et al. (2020) since we did not find it to be beneficial
for optimization.
3.3 Finetuning to Target Domain using
Support Set
After training in source domains, we finetune our
model using a small number of target domain sup-
port samples following a similar procedure as in
the training stage. As we have only a few sam-
ples for finetuning, we take them in a single batch.
When multiple few-shot samples (e.g., 5-shot) are
available for the target classes, the model can effec-
tively adapt to the new domain by optimizing KL-
divergence of Gaussian Embeddings as in Eq. 4.
In contrast, for 1-shot case, it turns out challenging
for models to adapt to the target class distribution.
If the model has no prior knowledge about target
classes (either from direct training or indirectly
from source domain training where the target class
entities are marked as O-type ), a single example
might not be sufficient to deduce the variance of the
target class distribution. Thus, for 1-shot scenario,
we optimize d(p, q) =||µ−µ||, the squared
euclidean distance between mean of the embedding
distributions. When the model has direct/indirect
prior knowledge about the target classes involved,
we still optimize the KL-divergence of the distribu-
tions similar to the 5-shot scenario.
We demonstrate in Table 7 that optimizing with
squared euclidean distance gives us slightly better
performance in 1-shot scenario. Nevertheless, in
all cases with 5-shot support set, optimizing the
KL-divergence between the Gaussian Embeddings
gives us the best result.
Early Stopping Finetuning with a small support
set runs the risk of overfitting and without access
to a held out validation set due to data scarcity in
the target domain, we cannot keep tabs on the satu-
ration point where we need to stop finetuning. To
alleviate this, we rely on the calculated contrastive
loss and use it as our early stopping criteria with a
patience of 1. (Algorithm 1: Line 16-17, 24 )6341Algorithm 1 Training and Finetuning of CON-
TNER
3.4 Instance Level Nearest Neighbor
Inference
After training and finetuning the network with
train and support data respectively, we extract the
pretrained language model encoder PLM for infer-
ence. Similar to SimCLR (Chen et al., 2020), we
found that representations before the projection lay-
ers actually contain more information than the final
output representation which contributes to better
performance, so fandfprojection heads are
not used for inference. We thus calculate the repre-
sentations of the test data from PLM and find near-
est neighbor support set representation for infer-
ence (Wang et al., 2019; Yang and Katiyar, 2020).
ThePLM representations hof each of the sup-
port token (x, y)∈ Xcan be calculated as
in Eq. 1. Similarly for test data X, we get the
PLM representations hwhere x∈ X. Here
we assign xthe same label as the support token
that is nearest in the PLM representation space:
y= arg min||h−h||(7)
Viterbi Decoding Most previous works (Hou
et al., 2020; Yang and Katiyar, 2020; Ding et al.,
2021) noticed a performance improvement by us-
ing CRFs (Lafferty et al., 2001) which removes
false predictions to improve performance. Thus
we also employ Viterbi decoding in the inference
stage with an abstract transition distribution as in
StructShot (Yang and Katiyar, 2020). For the tran-
sition probabilities , the transition between three
abstract tags O,I, and I-other is estimated by
counting their occurrences in the training set. Then
for the target domain tag-set, these transition prob-
abilities are evenly distributed into corresponding
target distributions. The emission probabilities
are calculated from Nearest Neighbor Inference
stage. Comparing domain transfer results (Table
3) against other tasks (Table 2,4,5) we find that,
interestingly, if there is no significant domain shift
involved in the test data, contrastive learning al-
lows CONTNER to automatically extract label
dependencies, obviating the requirement of extra
Viterbi decoding stage.
4 Experiment Setups
Datasets For evaluation, we use datasets across
different domains: General (OntoNotes 5.0
(Weischedel et al., 2013)), Medical (I2B2 (Stubbs
and Uzuner, 2015)), News (CoNLL’03 (Sang and
De Meulder, 2003)), Social (WNUT’17 (Derczyn-
ski et al., 2017)). We also test on GUM (Zeldes,
2017) that represents wide variety of texts: inter-
views, news articles, instrumental texts, and travel
guides. The miscellany of domains makes it a chal-
lenging dataset to work on. Ding et al. (2021) argue
that the distribution of these datasets may not be
suitable for proper representation of Few-Shot ca-
pability. Thus, they proposed a new large scale
dataset Few-NERD that contains 66 fine-grained
entities across 8 coarse grained entities, signifi-
cantly richer than previous datasets. A summary of
these datasets is given in Table 1.6342
Baselines We compare the performance of CON-
TNER with state-of-the-art Few-Shot NER mod-
els on different datasets across several settings. We
first measure the model performance in traditional
NER datasets in tag-set extension and domain trans-
fer tasks as proposed in Yang and Katiyar (2020).
We then evaluate our model in Few-NERD (Ding
et al., 2021) dataset that is explicitly designed for
Few-Shot NER, and compare it against the Few-
NERD leaderboard baselines. Similar to Ding et al.
(2021), we take Prototypical Network based Pro-
toBERT (Snell et al., 2017; Fritzler et al., 2019;
Hou et al., 2020), nearest neighbor based metric
method NNShot that leverages the locality of in-
class samples in embedding space, and additional
Viterbi decoding based Structshot (Yang and Kati-
yar, 2020) as the main SOTA baselines.
4.1 Tag-set Extension Setting
A common use-case of Few-Shot NER is that new
entity types may appear in the same existing text
domain. Thus (Yang and Katiyar, 2020) proposed
to experiment tag-set extension capability using
OntoNotes (Weischedel et al., 2013) dataset. The
eighteen existing entity classes are split in three
groups: A, B, and C, each having six classes. Mod-
els are tested in each of these groups having few
sample support set while being trained in the re-
maining two groups. During training, all test group
entities are replaced with O-tag. Since the source
and destination domains are the same, the train-
ing phase will induce some indirect information
about unseen target entities. So, during finetuningofCONTNER , we optimize the KL-divergence
between ouptut embeddings as in Eq. 4.
We use the same entity class splits as
used by Yang and Katiyar (2020) and used
bert-base-cased as the backbone encoder for
all models. Since they could not share the sampled
support set for licensing reasons, we sampled five
sets of support samples for each group and aver-
aged the results, as done by the authors. We show
these results in Table 2. We see that in different
entity groups, CONTNER outperforms present
SOTAs by upto 12.75 absolute F1 points, a substan-
tial improvement in performance.
4.2 Domain Transfer Setting
In this experiment a model trained on a source
domain is deployed to a previously unseen novel
text domain. Here we take OntoNotes (General) as
our source text domain, and evaluate the Few-Shot
performance in I2B2 (Medical), CoNLL (News),
WNUT (Social) domains as in (Yang and Katiyar,
2020). We also evaluate the performance in GUM
(Zeldes, 2017) dataset due to its particularly chal-
lenging nature. We show these results in Table 3.
While all the other domains have almost no inter-
section with OntoNotes, target entities in CoNLL
are fully contained within OntoNotes entities, that
makes it comparable to supervised learning.
4.3 Few-NERD Setting
For few-shot setting, Ding et al. (2021) pro-
posed two different settings: Few-NERD (IN-
TRA) andFew-NERD (INTER) . In Few-NERD6343
(INTRA) train, dev, and test sets are divided ac-
cording to coarse-grained types. As a result, fine-
grained entity types belonging to People, Art,
Product, MISC coarse grained types are put in
the train set, Event, Building coarse grained
types in dev set, and ORG, LOC in test set. So,
there is no overlap between train, dev, test set
classes in terms of coarse grained types. On the
other hand, in Few-NERD (INTER) coarse grained
types are shared, although all the fine grained types
are mutually disjoint. Because of the restrictions
of sharing coarse-grained types, Few-NERD (IN-
TRA) is more challenging. Since, few-shot perfor-
mance of any model relies on the sampled support
set, the authors also released train, dev, test split
for both Few-NERD (INTRA) andFew-NERD
(INTER) . We evaluate our model performance us-
ing these provided dataset splits and compare the
performance in Few-NERD leaderboard. All mod-
els use bert-base-uncased as the backbone
encoder. As shown in Table 4 and Table 5, CON-
TNER establishes new benchmark results in the
leaderboard in both of these tests.
5 Results and Analysis
We prudently analyze different components of our
model and justify the design choices made in the
scheming of CONTNER . We also examine the
results discussed in Section 4 that gives some intu-
itions about few-shot NER in general.
5.1 Overall Results
Table 2-5 demonstrates that overall, in every sce-
nario CONTNER convincingly outperforms all
other baseline approaches. This improvement is
particularly noticeable in challenging scenarios,
where all other baseline approaches perform poorly.
For example, FEW-NERD (intra) (Table 4) is achallenging scenario where the coarse grained en-
tity types corresponding to train and test sets do
not overlap. As a result, other baseline approaches
face a substantial performance hit, whereas CON-
TNER still performs well. In tag-set extension
(Table 2), we see a similar performance trend -
CONTNER performs consistently well across
the board. Likewise, in domain transfer to a very
challenging unseen text domain like GUM (Zeldes,
2017), baseline models performs miserably; yet
CONTNER manages to perform consistently
outperforming SOTA models by a significant mar-
gin. Analyzing these results more closely, we
notice that while CONTNER surpasses other
baselines in almost every tests, more prominently
in 5-shot cases. Evidently, CONTNER is able
to make better use of multiple few-shot samples
thanks to distribution modeling via contrastive
Gaussian Embedding optimization. In this con-
text, note that StructShot actually got marginally
higher F1-score in 1-shot CoNLL domain adapta-
tion and 1 ∼2 shot FEW-NERD (INTER) cases. In
CoNLL, the target classes are subsets of training
classes, so supervised learning based feature extrac-
tors are expected to get an advantage in prediction.
On the other hand, Ding et al. (2021) carefully
tuned the hyperparameters for baselines like Struct-
Shot for best performance. We could also improve
performance in a similar manner, however for uni-
formity of model across different few-shot settings,
we use the same model architecture in every test.
Nevertheless, CONTNER shows comparable
performance even in these cases while significantly
outperforming in every other test.
5.2 Training Objective
Traditional contrastive learners usually optimize
cosine similarity of point embeddings (Chen et al.,
2020). While this has proven to work well in im-
age data, in more challenging NLU tasks like Few-
Shot NER, it gives subpar performance. We com-
pare the performance of point embeddings with
euclidean distance and cosine similarity to that of
CONTNER using Gaussian Embedding and KL-
divergence in OntoNotes tag-set extension. We
report these performance in Table 8 in Appendix.
Basically, Gaussian Embedding leads to learning
generalized representation during training, which
is more suitable for finetuning to few sample target
domain. In Appendix C, we examine this aspect by
comparing the t-SNE representations from point6344embedding and Gaussian Embedding.
5.3 Effect of Model Fine-tuning
Being a contrastive learner, CONTNER can
take advantage of extremely small support set
to refine its representations through fine-tuning.
To closely examine the effects of fine-tuning,
we conduct a case study with OntoNotes tag-
extension task using PERSON, DATE, MONEY,
LOC, FAC, PRODUCT target entities.
As shown in Table 6, we see that finetuning in-
deed improves few-shot performance. Besides, the
effect of finetuning is even more marked in 5-shot
prediction indicating that CONTNER finetun-
ing process can make the best use of few-samples
available in target domain.
5.4 Modeling Label Dependencies
Analyzing the results, we observe that domain
transfer (Table 3) sees some good gains in perfor-
mance from using Viterbi decoding. In contrast,
tag-set extension (Table 2) and FEW-NERD (Table
4,5) gets almost no improvement from using Viterbi
decoding. This indicates an interesting property of
CONTNER . During domain transfer the text do-
mains have no overlap in train and test set. So, an
extra Viterbi decoding actually provides additional
information regarding the label dependencies, giv-
ing us some nice improvement. Otherwise, the train
and target domain have substantial overlap in both
tagset extension and FEW-NERD. Thus the model
can indirectly learn the label dependencies through
in-batch contrastive learning. Consequently, unless
there is a marked shift in the target text domain,
we can achieve the best performance even without
employing additional Viterbi decoding.
6 Related Works
Meta Learning The idea of Few-shot learning
was popularized in computer vision through Match-
ing Networks (Vinyals et al., 2016). Subsequently,
Prototypical Network (Snell et al., 2017) was pro-
posed where class prototypical representationswere learned. Test samples are given labels accord-
ing to the nearest prototype. Later this technique
was proven successful in other domains as well.
Wang et al. (2019), on the other hand found sim-
ple feature transformations to be quite effective in
few shot image recognition These metric learning
based approaches have also been deployed in differ-
ent NLP tasks (Geng et al., 2019; Bao et al., 2020;
Han et al., 2018; Fritzler et al., 2019).
Contrastive Learning Early progress was made
by contrasting positive against negative samples
(Hadsell et al., 2006; Dosovitskiy et al., 2014; Wu
et al., 2018). Chen et al. (2020) proposed SimCLR
by refining the idea of contrastive learning with the
help of modern image augmentation techniques to
learn robust sets of features. Khosla et al. (2020)
leveraged this to boost supervised learning perfor-
mance as well. In-batch negative sampling has also
been explored for learning representation (Doer-
sch and Zisserman, 2017; Ye et al., 2019). Storing
instance class representation vectors is another pop-
ular direction (Wu et al., 2018; Zhuang et al., 2019;
Misra and Maaten, 2020).
Gaussian Embedding Vilnis and McCallum
(2014) first explored the idea of learning word em-
beddings as Gaussian Distributions. Although the
authors used RANK-SVM based learning objec-
tive instead of modern deep contextual modeling,
they found that embedding densities in a Gaussian
space enables natural represenation of uncertainty
through variances. Later, Bojchevski and Günne-
mann (2017) leveraged Gaussian Embedding in
Graph representation. Besides state-of-the-art per-
formance, they found Gaussian Embedding to be
surprisingly effective in inductive learning, gen-
eralizing to unseen nodes with few training data.
Moreover, KL-divergence between Gaussian Em-
beddings allows explicit consideration of asym-
metric distance which better represents inclusion,
similarity or entailment (Qian et al., 2021) and
preserve the hierarchical structures among words
(Athiwaratkun and Wilson, 2018).
Few-Shot NER Established few-shot learning ap-
proaches have also been applied in Named Entity
Recognition. Fritzler et al. (2019) leveraged pro-
totypical network (Snell et al., 2017) for few shot
NER. Inspired by the potency of simple feature
extractors and nearest neighbor inference (Wang
et al., 2019; Wiseman and Stratos, 2019) in few-
Shot learning, Yang and Katiyar (2020) used super-6345vised learner based feature extractors for Few-Shot
NER. Pairing it with abstract transition tag Viterbi
decoding, they achieved current SOTA result in
Few-Shot NER tasks. Huang et al. (2020) proposed
noisy supervised pre-training for Few-Shot NER.
However, this method requires access to a large
scale noisy NER dataset such as WiNER (Ghaddar
and Langlais, 2017) for the supervised pretraining.
Acknowledging the shortcomings and evaluation
scheme disparity in Few-Shot NER, Ding et al.
(2021) proposed a large scale dataset specifically
designed for this task. Wang et al. (2021) explored
model distillation for Few-Shot NER. However,
this requires access to a large unlabelled dataset for
good performance. Very recently, prompt based
techniques have also surfaced in this domain (Cui
et al., 2021). However, the performance of these
methods rely heavily on the chosen prompt. As
denoted by the author, the performance delta can
be massive (upto 19% absolute F1 points) depend-
ing on the prompt. Thus, in the absence of a large
validation set, their applicability becomes limited
in true few-shot learning (Perez et al., 2021).
7 Conclusion
We propose a contrastive learning based frame-
work CONTNER that models Gaussian embed-
ding and optimizes inter token distribution distance.
This generalized objective helps us model a class
agnostic feature extractor that avoids the pitfalls
of prior Few-Shot NER methods. CONTNER
can also take advantage of few-sample support data
to adapt to new target domains. Extensive evalu-
ations in multiple traditional and recent few-shot
NER datasets reveal that, CONTNER consis-
tently outperforms prior SOTAs, even in challeng-
ing scenarios. While we investigate the efficacy of
distribution optimization based contrastive learning
in Few-Shot NER, it will be of particular interest
to investigate its potency in other domains as well.
Acknowledgement
We thank the ACL Rolling Review reviewers for
their helpful feedback. We also want to thank Nan
Zhang, Ranran Haoran Zhang, and Chandan Akiti
for their insightful comments on the paper.
Ethics Statement
With CONTNER , we have achieved state-of-the-
art Few-Shot NER performance leveraging Gaus-
sian Embedding based contrastive learning. How-ever, the overall performance is still quite low com-
pared to supervised NER that takes advantage of
the full training dataset. Consequently, it is still not
ready for deployment in high-stake domains (e.g.
Medical Domain, I2B2 dataset), leaving a lot of
room for improvement in future research.
References634663476348A Implementation Details
For all of our experiments in CONTNER . we
chose the same hyperparameters as in Yang and
Katiyar (2020). Across all our tests, we kept Gaus-
sian Embedding dimension fixed to l= 128 . In
order to guarantee proper comparison against prior
competitive approaches, we use the same back-
bone encoder for all methods in same tests, i.e.
bert-base-cased was used for all methods
in Tag-Set Extension and Domain Transfer tasks
while bert-base-uncased was used for Few-
NERD following the respective evaluation strate-
gies. Finally, to observe the effect of Viterbi de-
coding on CONTNER output, we set the re-
normalizing temperature τto 0.1.
Using an RTX A6000, we trained the network on
OntoNotes dataset for 30 minutes. The finetuning
stage requires less than a minute due to the small
number of samples.
B Fine-tuning Objective
During finetuning, if a model does not have any
prior knowledge about the target classes, directly or
indirectly, a 1-shot example may not give sufficient
information about the target class distribution (i.e.
the variance of the distribution). Consequently dur-
ing finetuning, for 1-shot adaptation to new classes,
optimizing euclidean distance of the mean embed-
ding gives better performance. Nevertheless, for
5-shot cases, KL-divergence of the Gaussian Em-
bedding always gives better performance indicating
that it takes better advantage of multiple samples.
We show this behavior in the best result of domain
transfer task with WNUT in Table 7. Since this
domain transfer task gives no prior information
about target embeddings during training, optimiz-
ing KL-divergence in 1-shot fineutuning actually
hurts performance a bit compared to euclidean fine-
tuning. However, in 5-shot, KL-finetuning again
gives superior performance as it can now adapt
better to the novel target class distributions.
C t-SNE Visualization: Point Embedding
vs. Gaussian Embedding
Figure 3 offers a deep dive into how Gaussian Em-
bedding improves generalization and takes better
advantage of few shot support set for target domain
adaptation. Here we compare the t-SNE visualiza-
tion of support set and test set of a sample few-
shot scenario in OntoNotes tag set extension task.
In Figure 3 (a) we can see that point embedding
paired with Euclidean distance metric has subopti-
mal clustering pattern in both support and test sets.
In fact, the support examples in different classes
are intermixed implying poor generalization. When
the point embedding model is finetuned with the
support set (Figure 3 (c)), Euclidean distance ag-
gressively optimizes them and tries to force the
same class support examples to collapse into essen-
tially a single point representation. In other words,
the model quickly overfits the small support data
which in fact hurts model performance. In compar-
sion, Gaussian Embedding offers a better t-SNE
representation prior to and after finetuning. Figure
3 (b) shows the representation of support and test
sets prior to finetuning with Gaussian Embedding
paired with KL-divergence. In both support and
test sets, we observe different class samples mostly
clustered together. This indicates that even before
finetuning it shows good generalization to unseen
classes. While finetuning, the KL-divergence opti-
mization objective maintains the class distribution
letting the model generate separate support clus-
ters (Figure 3(d)). After finetuning, the clusters
get cleaner offering even better separation between
different class clusters, which is also reflected in
the performance uplift of the model.
D Comparison of Different Training
Objectives
Table 8 compares the performance of Gaussian
Embedding (KL-divergence) with that of point em-
bedding (Euclidean distance of cosine similarity)
in OntoNotes tag extension task. Since Gaussian
Embedding utilizes ldimensional mean and ldi-
mensional diagonal covariance matrix, for a fair
comaparison we show the results for 2ldimen-
sional point embedding. As discussed in Section6349
5.2, Gaussian Embedding with KL-divergence ob-
jective largely outperforms point embedding irre-
spective of distance metric used.
E Embedding Quality: Before vs. After
Projection
As explained in Section 3.4, the representation be-
fore the projection layer contains more information
than that of after. In Table 9, we compare the per-
formance of representations before and after the
Gaussian projection layer. From the results it is
evident that, representation before the projection
indeed achieves higher performance, which also
supports the findings of (Chen et al., 2020). This
is because the representation after the projection
head is directly adjacent to the contrastive objec-
tive, which causes information loss in this layer.6350Consequently, the representation before projection
achieves better performance.
F NER Prediction Examples
Table 10 demonstrates some predictions with CON-
TNER and StructShot using PERSON, DATE,
MONEY, LOC, FAC, PRODUCT as target few-
shot entities while being trained on all other entity
types in OntoNotes dataset. A quick look at these
qualitative examples reveal that StructShot often
fails to distinguish between non-entity and entity
tokens. Moreover, it also misclassifies non-entity
tokens as one of the target classes. CONTNER
on the other hand has lower misclassifications and
better entity detection indicating its stability and
higher performance.635163526353