
Devamanyu Hazarika, Yingting Li,
Bo Cheng, Shuai Zhao, Roger Zimmermann , Soujanya PoriaNational University of Singapore, SingaporeBeijing University of Posts and Telecommunications
DeCLaRe Lab, Singapore University of Technology and Design, Singapore
{hazarika, rogerz}@comp.nus.edu.sg
{cindyyting, chengbo, zhaoshuaiby}@bupt.edu.cn
sporia@sutd.edu.sg
Abstract
Building robust multimodal models are cru-
cial for achieving reliable deployment in the
wild. Despite its importance, less attention
has been paid to identifying and improving
the robustness of Multimodal Sentiment Anal-
ysis (MSA) models. In this work, we hope
to address that by ( i) Proposing simple di-
agnostic checks for modality robustness in a
trained multimodal model. Using these checks,
we find MSA models to be highly sensitive
to a single modality, which creates issues in
their robustness; ( ii) We analyze well-known
robust training strategies to alleviate the is-
sues. Critically, we observe that robustness
can be achieved without compromising on the
original performance. We hope our extensive
study–performed across five models and two
benchmark datasets–and proposed procedures
would make robustness an integral component
in MSA research. Our diagnostic checks and
robust training solutions are simple to imple-
ment and available at
1 Introduction
Multimodal Sentiment Analysis (MSA) is a bur-
geoning field of research that has seen accelerated
developments in recent years. Numerous models
have been proposed that utilize multiple modalities
such as audio, visual, and language signals to pre-
dict sentiments, emotions, and other forms of affect.
While progress in MSA has been driven mainly by
improvements in multimodal performance, we call
for attention towards an equally important aspectFigure 1:
in multimodal systems – multimodal robustness .
Robustness is crucial when models are deployed
in the wild, where it is common to encounter inad-
vertent errors in the source modalities due to data
loss, data corruption, jitter, privacy issues, amongst
others.
A well-known fact in the MSA research is that
language modality tends to be the most effective,
which has prompted models to utilize language
as its core modality (Wu et al., 2021; Han et al.,
2021a; Zeng et al., 2021). In this work, we focus on
skewed dependence on language and try to under-
stand how it affects the robustness of MSA models.
Specifically, we ask,
RQ1: Are models in MSA over-reliant on a sub-
set of modalities, particularly language?
RQ2: If yes, what implications does it have on
modality robustness?
To answer RQ1 , we look at Fig. 1. The figure il-
lustrates a setup where we fully remove one modal-
ity during testing on the MISA model (Hazarika
et al., 2020). Here, we observe a sharp drop in per-
formance when language modality is removed but685do not see statistically significant drops when audio
or visual modalities are removed. This observation
aligns with recent findings in the MSA literature
highlighting the dominance of language.
This brings us to RQ2 where we try to under-
stand the robustness implications over this dom-
inance. We design an elaborate study in § 3—
over five state-of-the-art (SOTA) MSA models and
across two benchmark datasets—where we propose
diagnostic checks to understand modality robust-
ness, i.e., how robust are models against modality
errors such as missing or noisy modalities.
Based on our findings, we then proceed to ask,
RQ3: How can we improve the robustness of
these models?
RQ4: Does robust training lead to a perfor-
mance trade-off?
ForRQ3 , we study well-known robust training
methods, that act as a pre-emptive strategy to re-
duce the performance drops. Critically, our training
ismodel-agnostic and can be easily included in
any existing multimodal model (§ 4). For RQ4 ,
we observe that our method to improve robustness
does not trade-off with the final performance on
the clean testing set, thus achieving similar perfor-
mance as the original model.
2 Related Works
While MSA has received increased attention in re-
cent times, the topic of robustness has not taken
center stage. Fortunately, few works have started
changing this trend. (Gat et al., 2020) reveals how
multimodal classifiers often utilize a subset of
modalities, which they addressed by inducing uni-
form contribution from all input modalities. In
MSA, multiple works over-rely on language modal-
ity to improve the performance. (Wu et al., 2021)
constructs a text-centered shared-private frame-
work for multimodal fusion, and (Han et al., 2021a)
obtains two text-related modal pairs and iteratively
push the interaction between modalities to supple-
ment information for better performance. While
this has enabled performance boosts, our goal is
to explore the double-edged nature of this feature
and how it impacts robustness. Our motivation for
diagnostics is similar to (Frank et al., 2021), but un-
like them, we do not perturb the raw data (such as
image patches). Instead, we intervene on modality
representations, which is easier to integrate with
existing models and do not require prior knowledge
of the modality structure.To address robustness in MSA, (Tsai et al., 2018)
proposes a factored model that can accommodate
modality drops. Also, (Ma et al., 2021) introduces
modality drops during training and testing and uses
meta-learning to make models robust. However, our
work comprises some crucial distinctions: i)Unlike
these works, our diagnostics and robust training
do not require sophisticated architecture and can
be easily integrated into existing models. ii)We
perform an exhaustive analysis of robustness across
multiple models, which is previously not done in
the MSA literature.
3 Testing Robustness via Diagnostic
Checks
In this section we perform an elaborate study on
modality robustness by simulating potential issues
with modality signals during testing (or deploy-
ment) of MSA models.
3.1 Experiment Setup
Models. In order to fully verify the universal-
ity of our experiments, we select a series of di-
verse SOTA models, ranging from RNN-based
to Transformer-based architectures. These models
work across different granularities from word-level
to sentence-level variants:
(i)MISA (Hazarika et al., 2020) is a popu-
lar model that generates modality-invariant and
-specific features of multimodal data, to learn both
shared and unique characteristics of each modal-
ity.(ii)BBFN (Han et al., 2021a) in a similar
vein performs fusion and separation to increase
cross-modal relevances and differences. This work
acknowledges the dominance of text modality
in MSA and proposes two text-centric bi-modal
transformers to increase performance. (iii)Self-
MM (Yu et al., 2021) focuses on the relationship
between multi- and uni-modal predictions by multi-
tasking consistencies and differences between them.
(iv)MMIM (Han et al., 2021b) incorporates mu-
tual information (MI) into MSA by maximizing
MI at the input and fusion level. (v)MulT (Tsai
et al., 2019) merges multimodal time series through
multiple sets of directional pairwise cross-modal
transformers. It accounts for long-range dependen-
cies across modality elements to create a strong
baseline (see Appendix B).
Datasets. We consider two benchmark datasets
widely used in the field of multimodal sentiment
analysis, CMU-MOSI (Zadeh et al., 2016), which686
is a popular dataset for studying the intensity of
multimodal sentiment in the MSA field and CMU-
MOSEI (Bagher Zadeh et al., 2018) which is a
larger counterpart of MOSI with richer annotations
and more diverse samples. Both these datasets con-
tain short utterance videos and provide language,
audio, and visual modality features.
3.2 Proposed Diagnostic Checks
We propose two diagnostic checks that introduce
i)Missing Modalities , which drops (or nullifies) a
modality from the input and ii)Noisy Modalities
which include random changes to the modality rep-
resentations, introduced via white Gaussian noise
to the respective modality representations. To sim-
ulate a realistic scenario, we apply these checks to
30% of the testing data set. Given the increased
dependence on language modality in MSA mod-
els, we limit our study to errors introduced only in
language modality without loss of generality.
Procedure. We aim to intervene on modality
representations to simulate modality errors. For
the language modality l, all models map the se-
quence of tokens U∈Rto its low-level em-
bedding U∈RwithTtokens and dem-
bedding dimension. This low-level sequence isthen encoded into hidden representations using an
encoder of choice, such as BERT (Devlin et al.,
2019), to achieve the language representation vec-
toru=enc(U)∈R. We intervene on this rep-
resentation and apply our diagnostics as follows.
We sample 30% of ufrom the testing set and
modify them as ˆ u=f(u), where f(x)is defined
as either f(x) =x⊙0for modality dropping
(nulling the vector to 0s by element-wise multi-
plication) or f(x) =x+N(0,1)to add white
noise. The modified ˆ uis then fed to the rest of the
network as usual.
In the selected models, we apply diagnostics at
different network locations. These include the rep-
resentations before the hidden projection, such as
in MISA, or fusion operation, such as in Self-MM.
For MulT and BBFN, we apply the interventions
right after the word embeddings. Detailed discus-
sion on the location of interventions is provided in
Appendix A.
Observations. Figs. 2 and 3 presents the results,
where across both MOSI and MOSEI datasets, we
find that language modality is highly sensitive to
modality errors in the language source (across all
models). This trend is observed for both missing
and noisy modality checks, thus highlighting the
concerns over robustness of these SOTA models.
These diagnostic checks are easy to analyze, and
we hope they will become an integrated part of the
model-development pipeline in MSA.687
4 Robust Training
In this section, we explore how to reduce the sen-
sitivity of the models to the dominant modality,
i.e., language. One of the popular ways to alleviate
such issues is to teach the model such scenarios
during training. We dub this approach as modality-
perturbation , which is conceptually similar to re-
moving or masking modalities in (Ma et al., 2021;
Georgiou et al., 2021) or adding noise in (Miyato
et al., 2018). It simulates the modality errors dur-
ing training so that the model learns to expect such
events during testing/deployment. The procedure
is as follows,
1.Training:
(a)For a particular batch of data, sample a
proportion of the data to be perturbed.
(b)Similar to the diagnostic checks in § 3, per-
turb the dominant modality (in our case,
language) of half of this data with missing
and the other half with noisy perturbation.
Repeat both these steps for the next batch.
2.Testing: Apply the diagnostic checks as in § 3.
This simple approach can be interpreted as reg-
ularization akin to dropouts or noising strategies
used in de-noising auto-encoders.
4.1 Results
Robustness. Table 1 presents the results, where
we perform balanced perturbation between missingand noisy modalities. For the 30% perturbable data
in training, we drop the language modality on 15%
and for the other 15%, we add noise. This setting
improves the diagnostics in both kinds of errors.
Appendix C presents results on other proportions
of the training data.
With balanced perturbation, (BBFN-MOSI) re-
duces the relative drop on missing language diag-
nostic by 31% (in F1) and 98% on noise . Also, miss-
ingdrop reduces by 11% in Corr and by 99% for
noisy diagnostic. (Self-MM, MOSI) increases the
relative drop in Corr slightly on missing diagnostic,
but in all other cases, it is significantly reduced. For
example, the F1 drop on MOSI for noisy diagnostic
reduces significantly by 93%. Table 1 also shows
that our method performs well on both RNN-based
and Transformer-based models, demonstrating the
wide applicability of our method.
Performance Trade-off. While alleviating ro-
bustness via regularization is well-known in the
literature, there is often a trade-off with absolute
performance in the original testing setup. Most
approaches that achieve robustness take a hit at
their best performance on clean input (Zhang et al.,
2022) (Nakkiran, 2019) (Su et al., 2018) (Tsipras
et al., 2019). This raises the question of whether
introducing modality-perturbation reduces the per-
formance of the model on the original testing set.
We find the answer to this is No. Surprisingly,
our robust training procedure does not degrade in688
its original performance and can perform similar to
the original model variants. This is highly ideal as
we achieve robustness without compromising on
performance in clean data.
5 Conclusion
In this work, we performed a systematic study that
demonstrate the double-edged nature of dominant
modality in SOTA MSA models. Our analysis us-
ing diagnostic checks reveal high susceptibility to
performance drops when presented with unwanted
errors in their representations.
To alleviate the issues, we also study robust train-
ing methods that uses modality perturbations. Criti-
cally, we find that robustness and performance can
co-exist without an explicit trade-off. These im-
provements demonstrate a positive nudge in the
effort to achieve robustness and we believe there re-
mains significant room for improvement. With this
work, by proposing simple and easy-to-integrate
diagnostic checks and training methods, we hope
to permeate discussions on robustness into main-
stream MSA research.Acknowledgements
This research is supported by (i)T2MOE2008
awarded by Singapore’s MoE under its Tier-2 grant
scheme, and (ii)SUTD SRG grant #T1SRIS19149.
We also thank the anonymous reviewers for their
valuable comments.
References689690A Model Details
MISA: We get the MISA model from its official
repository. In this model, we apply the interven-
tions at the following encoded language represen-
tation from the original paper:
u= Bert/parenleftig
U;θ/parenrightig
(1)
ˆ u=f(ˆ u)(2)
h=E(ˆ u;θ),h=E/parenleftbig
ˆ u;θ/parenrightbig
(3)
That is, the interventions are applied before the
language representation is projected to its shared
and private subspaces.
BBFN: We get the BBFN model from its official
repository. In this model, we execute the interven-
tions after the following language embedding from
the original paper.
M= (m, m, . . . , m) (4)
ˆM=f(M) (5)
Self-MM: We get the Self-MM model from its
official repository. In this model, we set the inter-
ventions after the language features encoded below
from the original paper.
F=BERT/parenleftig
I;θ/parenrightig
∈R (6)
ˆF=f(F) (7)
MMIM: We get the MMIM model from its of-
ficial repository. For this model, we perform the
interventions after the following encoded language
representation from the original paper.
x= BERT/parenleftbig
X;θ/parenrightbig
(8)
ˆ x=f(x) (9)
MulT: We get the MulT model from its official
repository. We intervene in this model after the
following encoded language representation:.
x= Conv1D( X, k)∈R(10)
ˆ x=f(x) (11)
B Reproducing Results
For each model we train the models to achieve
performances close to reported in the respective
papers. Table 2 presents the hyper-parameters we
used to reproduce their results.
C Additional Results on
Modality-Perturbation
We also analyze with varying proportions of per-
turbations in the training and testing phase, respec-
tively. As seen in Table 3, as the noise gradually in-
creases from 5% to 15%, the drop of Corr in MOSI
is gradually reduced , which shows the robustness
is getting better, until it reaches the optimum at 30%
perturbation (15% missing + 15% noise). In other
models, 30% perturbation is also advantageous. For
example, in Table 7, (Mult, MOSEI) reduces Corr
drop while improving F1 performance in 30% per-
turbation. Although it is only a small improvement
at present, we believe that there will be more mean-
ingful improvements in the future.691692693694695696