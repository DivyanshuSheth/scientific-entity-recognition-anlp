
Huaao Zhang, Qiang Wang, Bo Qin, Zelin Shi, Haibo Wang, Ming ChenRoyalFlush AI Research Institute, Hangzhou, ChinaZhejiang University, Hangzhou, China
{zhanghuaao, wangqiang3, qinbo, shizelin}@myhexin.com, chm@zju.edu.cn
Abstract
In this work, we study the robustness of two
typical terminology translation methods: Place-
holder (PH) and Code-Switch (CS), concern-
ing (1) the number of constraints and (2) the
target constraint length. We identify that ex-
isting terminology constraint test sets, such as
IATE, Wiktionary, and TICO, are blind to this
issue due to oversimplified constraint settings.
To solve it, we create a new challenging test
set of English-German, increasing the average
constraint count per sentence from 1.1 ∼1.7 to
6.1 and the length per target constraint from
1.1∼1.2 words to 3.4 words. Then we find that
PH and CS methods degrade as the number
of constraints increases, but they have comple-
mentary strengths. Specifically, PH is better
at retaining high constraint accuracy but lower
translation quality as measured by BLEU and
COMET scores. In contrast, CS has the oppo-
site results. Based on these observations, we
propose a simple but effective method com-
bining the advantages of PH and CS. This ap-
proach involves training a model like PH to pre-
dict the term labels, and then during inference
replacing those labels with target terminology
text like CS, so that the subsequent generation
is aware of the target term content. Extensive
experimental results show that this approach
can achieve high constraint accuracy and trans-
lation quality simultaneously, regardless of the
number or length of constraints.
1 Introduction
Although Neural Machine Translation (NMT) has
achieved expressive performance improvement
with the increase of model and data scale, it still
struggles when involved in mismatched domains
and rare entities (Koehn and Knowles, 2017). Ter-
minology constraints (TC) is a popular solution
that requires the model to generate the translation
following the pre-provided terminology pairs andhas been widely applied in commercial translation
systems, such as Google, DeepL, etc.
Perhaps the most popular approach for TC is
learning the constraint-aware model through data
augmentation (Song et al., 2019; Dinu et al., 2019;
Ailem et al., 2021; Bergmanis and Pinnis, 2021).
Early data augmentation is based on placeholder
(PH). During training, PH methods replace the
terminology terms in both source and target sen-
tences with ordered labels (e.g., “ T”, “T”), while
the model predicts labels rather than the concrete
terms at inference (Crego et al., 2016; Michon et al.,
2020). The main drawback of PH methods is that
the term labels lose the original semantic informa-
tion, resulting in incoherent translation. Unlike
PH methods, Code-Switch (CS) methods follow
the standard model and generate term translations
word by word by injecting target constraints in the
source sequence (Song et al., 2019; Dinu et al.,
2019; Ailem et al., 2021).
In this work, we focus on understanding the ro-
bustness of existing terminology constraint meth-
ods in challenging constraint settings in practice.
Our contributions are four-fold:
•We point out that the widely used terminol-
ogy constraint test sets (IATE, Wiktionary,
TICO) are too oversimplified to evaluate the
robustness. To address this, we have created
a new, challenging English-German terminol-
ogy constraint test set containing 500 sentence
pairs with multiple long constraints. This
proposed test set significantly increases the
average number of constraints from 1.1 ∼1.7
to 6.1, and the target constraint length from60291.1∼1.2 words to 3.4 words. We will release
this benchmark to promote the development
of robust terminology translation.
•Through the proposed test set, we reveal
that the performance of both Placeholder and
Code-Switch degrades seriously with the in-
crease of constraint count/length. However,
it shows a strong complementarity in terms
of constraint accuracy and translation quality;
Placeholder is better at preserving accurate
constraint, while Code-Switch yields higher
translation quality as measured by COMET.
•Inspired by our findings, we propose a simple
yet effective method for robust terminology
translation (RTT), combining PH and CS’s ad-
vantages. RTT learns to predict the term label
and achieves a high constraint accuracy (like
PH). Once a term label is generated, RTT ap-
pends the constraint counterpart in the decod-
ing sequence to make the consequence gener-
ation aware of the semantic constraints (like
CS).
•The experimental results of IATE, Wiktionary,
and the proposed test set demonstrate that our
approach can attain higher constraint accuracy
and translation quality compared to using PH
or CS alone, regardless of the number and
length of the constraints. In addition, RTT
maintains a slightly faster inference speed
than the vanilla Transformer.
2 Background
Letx={x, . . . , x}be the source sentence,
y={y, . . . , y}be the target sentence, and
C={⟨s,t⟩, . . . ,⟨s,t⟩}be the constraint set
aboutxandy, where sandtare the i-th source
and target constraint respectively. Each constraint
could be multi-word, e.g., |s|>= 1,|t|>= 1.
Then TC asks the system must translate sintot.
In this section, we briefly introduce two typical TC
methods based on data augmentation: Placeholder
(PH) (Crego et al., 2016) and Code-Switch (CS)
(Song et al., 2019; Dinu et al., 2019). We also de-
scribe some variants of them. Figure 1 explains the
differences between these methods.
Placeholder. Placeholder is an early method for
incorporating terminology constraints into machine
translation. During training, the raw bitext is
pre-processed by replacing source and target con-
straints with corresponding ordered labels T. At
inference time, source constraints are marked as
ordered labels, and the model predicts the labels au-
tonomously. The translation result is then obtained
by replacing the labels with their corresponding
target constraints in a post-processing step.
Code-Switch. Instead of using ordered labels,
Code-Switch directly substitutes the source con-
straints with the corresponding target constraints in
the input sentence. This allows the model to learn
to copy the pre-specified target constraints from
the input, so the decoder only needs to generate the
target token step-by-step, like a standard system.
Variants. Considering the source side, vanilla
PH and CS lose the source constraints’ semantics
due to direct replacement by labels or target con-
straints. A simple yet efficient solution is to retain
the source constraints but use a tag to distinguish
them from the replacement marks, as proposed
by Dinu et al. (2019). We refer to this variant as
the source-enhanced model (SE). For PH, we can
further additionally tag target constraint informa-
tion in the input sentence, denoted as the target-
enhanced model (TE). Since CS has already in-
jected target constraints into the input sentence, TE
is not available for it.
3 On the robustness of terminology
constraint
In this section, we explore the robustness of exist-
ing TC solutions from two aspects: (1) number of
constraints and (2) target constraint length. We first
point out the oversimplified problem in existing TC
test sets in Section 3.1. We describe our proposed6030
challenging TC test set in Section 3.2. Then we
conduct comprehensive experiments to analyze the
robustness of prior TC solutions in Section 3.3.
3.1 Oversimplified problem
As summarized in Table 2, oversimplified termi-
nology constraint setups are widely present in pub-
lished test sets, such as IATE, Wiktionary (Dinu
et al., 2019), and TICO (Barrault et al., 2021), as
well as extracted from alignment data (called EFA)
(Wang et al., 2022; Guanhua et al., 2021). Typ-
ically, most open-source test sets have only one
constraint per sentence, and the target constraint
is also short, usually consisting of a single word.
We suspect that this easy test set may lead to a
misunderstanding of the practical performance of
different methods. Intuitively, PH/PH+SE may suf-
fer from poor translation fluency due to more target
constraints, as the contents of these constraints are
invisible during the generation of the decoder. How-
ever, this is not a severe problem for CS/CS+SE.
On the other hand, PH should be insensitive to the
constraint length, as it uses a single label as an al-
ternative. In contrast, it is more difficult for CS to
generate a long constraint due to more decoding
steps required.
3.2 Proposed test set
To shed light on this issue, we made up a challeng-
ing TC test set. We notice that previous TC test
sets generally are made by matching pre-build term
database (e.g. IATE, Wiktionary) on existing bitext
data sets. Since the term set is not strongly related
to bitext, the number of matched constraints is not
controlled. Instead, we first decide on the bitext
data and then ask the linguistics expert to pick suit-
able sentence pairs to label constraints satisfying
the requirement.
Specifically, we first collect WMT 13-18 test
sets on English-German news translation task as
the bitext data (14585 sentence pairs); The linguis-tic expert artificially hand-picks 500 sentence pairs
for the study. These pairs are designed to include a
minimum of 6 constraints each, drawn from a care-
fully curated set of noun phrases (such as the names
of organizations, persons, movies and brands) and
common expressions. By focusing on these types
of constraints, the expert aims to replicate the lin-
guistic conditions found in industrial systems as
closely as possible. Table 10 in Appendix shows
some samples in the proposed test set.
3.3 Experiments
Setup. We conduct experiments on the WMT16
En-De task (4.5M). We replicate the same data pro-
cessing as Vaswani et al. (2017) with 32k joined
BPE codes. We use the standard transformer-base
model setting: 6-layer encoder/decoder, 8 attention
heads, hidden size of 512, and FFN hidden size of
2048. We train all models with 65536 batch tokens
for 120k updates and use checkpoint average of the
last 5 checkpoints. To apply constraints on training
data, we extract terminologies from two publicly
available term databases, Wiktionary and IATE. In
order to avoid spurious matches, we filtered out
the top 10k frequent words in term databases. Ac-
cording to previous work (Dinu et al., 2019), the
augmented data size is about 10% of the original
data. We compare five TC models from two fami-
lies, including PH, PH+SE, PH+SE+TE, CS, and
CS+SE. The difference lies in augmented data is
shown in Figure 1.
Metrics. We use several metrics to study the per-
formance of different methods comprehensively.
Specifically, in addition to reporting detokenized
BLEU scores with sacrebleu(Post, 2018), we also
use COMET(Rei et al., 2020) to evaluate the trans-
lation quality, inspired by the inconsistent trend in
recent study (Helcl et al., 2022). Besides, we use
strict sentence-level constraint accuracy (SCA) as
the metric for terminology constraint. That is to say,
only translations that satisfy all constraints in the
sentence are considered correct. In contrast, most
previous studies consider term-level constraint ac-
curacy (TCA). Compared to TCA, SCA is more
desired in the practical system because the transla-
tion may be severely misunderstood even if only
one constraint is wrong.6031
Results on various constraint counts. To simu-
late the case of various constraint counts, suppose
there are N constraints for each sentence pair in
the proposed test set, we randomly pick up 1,. . . , N
constraints. As a result, we conduct kTC test sets
with constraint count ranges from 1 to k, denoted
byT, . . . , T, where every pair in Thas exactly i
constraints. Table 3 shows the results of three met-
rics (BLEU, COMET, SCA) along with the number
of constraint counts ( k= 6). We can see that:
(i)The SE variants based on either PH or CS sig-
nificantly improve translation quality in terms
of BLEU and COMET, which indicates that
it is necessary to make the model aware of
source terminology semantics. The excep-
tions are the SCA results when increasing T.
The possible reason is that injecting too much
non-source information (e.g., label, target con-
straints) in the input confuses the model, de-
creasing the copying success rate.
(ii)The PH family performs better in SCA than
the CS family, especially for larger T. For
example, the gap between PH and CS is 8.8%
inT, extending to 26.8% in T. To our best
knowledge, it is the first time to reveal that
dramatic SCA degradation in CS models.
(iii) According to COMET, the family of CS has
a superior translation quality compared to
the PH family. We contend that COMET
is a crucial supplement to BLEU for assess-
ing terminology constraints. We observe
that PH+SE and CS+SE have similar aver-
age BLEU scores, yet there is a substantial
performance gap in COMET. This is due
to BLEU’s insensitivity to syntactic errors,
whereas COMET imposes a hefty penalty,
which is in line with earlier finding (Helcl
et al., 2022).
Results on various target constraint lengths.
To study the impact of target constraint lengths,
we report the TCA on different constraint length in
the proposed test set as shown in Table 4. Like the
trend of SCA in various constraint counts, we find
that the PH family is again significantly superior to
the CS family, especially when the length becomes
longer. This result also proves that the benefits
of label prediction in terms of constraint accuracy
exist widely in different situations.
4 Our approach
4.1 Basic idea
The above experiments empirically show the solid
complementarity between PH and CS, and here we
analyze the reason behind it (see Figure 1). We
suppose there are two sequences impacting the de-
coding process: prediction sequence andcontext
sequence , where the former is the realistic predic-
tion by the model, and the latter decides the target
context exposed to the model. For both PH-like and
CS-like methods, the common problem is that they
share the two sequences. Specifically, using place-
holders in PH simplifies the prediction sequence
but leads to the loss of constraint information (Fig-
ure 1a). In contrast, CS can observe the completed
context but is redundant in the prediction sequence6032
(Figure 1b). Thus, we propose to decouple the two
sequences, which is the basis of our approach, re-
ferred to as RTT. As illustrated in Figure 1c, we
still use placeholders to simplify the prediction se-
quence, but expose their semantics to future tokens
by replacing the placeholder with its text. We ex-
plain how to efficiently implement RTT in both
training and inference in the following section.
4.2 Training
RTT is agnostic to model architecture, and here we
use the vanilla Transformer due to its wide applica-
tion. Figure 2 illustrates the overall architecture.
Data augmentation. Since RTT behaves the
same as PH for the source side, we only describe
the data augmentation on the target side. Consider
the target sentence y={y, . . . , y}and con-
straints C={⟨s,t⟩, . . . ,⟨s,t⟩}, then we
construct a new target sequence yby prepending
an ordered term label Tbefore the beginning of
constraint t. For instance, in Figure 2, we aug-ment the original target input “ a, b, c, d, e, f, g ” by
“a, b,T, c, d, e, T, f, g ”. We do not use any tags to
distinguish term labels from normal tokens further
to minimize the target sequence length.
Input embedding. In addition to the word em-
bedding and sinusoidal positional embedding uti-
lized in the standard Transformer, we introduce an
additional learnable term embedding at the input
layer. This term embedding provides information
to the model about the number of constraints gen-
erated up to position i, thereby reducing the likeli-
hood of generating repetitive constraints. Then the
three embeddings are element-wise added to serve
as the input to the Transformer layer. We note that
the increase in the parameter size, K×d, due to
the inclusion of the term embedding is negligible
compared to the overall network parameters. Here,
Krepresents the maximum number of constraints
in a sentence, and dcorresponds to the hidden size.
In our work, we set Kto be 64.
Control visible context. In the Code-Switch
method, term labels Tare not present during the
translation generation. To replicate this behavior,
we suggest using a mask matrix in the self-attention
layer of the RTT’s decoder to make Tinvisible for
subsequent tokens. Let Mbe the mask matrix
of the decoder self-attention layer, where M= 1
implies that the j-th target token is visible for the i-
th target token. In the standard Transformer, Mis a
lower triangular matrix, which means that M= 1
ifi≤j. However, RTT additionally requires that
y̸=Tandy̸=T, thus preventing term labels
from being exposed to regular tokens.6033Loss masking. In the context of RTT, we aim
to encourage the model to focus more on learning
to predict the term label Trather than the corre-
sponding constraint tokens t. This is because once
Tis predicted, the corresponding constraint to-
kenstwill be automatically appended. To achieve
this goal, we propose "Loss Masking" to guide the
model’s attention. Specifically, for each token y
in the target sequence, we introduce a weight w
to modify the original log-likelihood log(P(y))
byw×log(P(y)). Then, we assign w= 1 to
normal tokens or term labels in the target sequence.
However, we set the weight wto 0 for tokens that
correspond to the target constraint. This is also
equivalent to treating the target constraint tokens as
padding symbols. It is important to note that even
though the target constraint tokens are masked, they
can still be learned from the raw training data.
4.3 Inference
RTT follows the autoregressive translation
paradigm. At decoding step i, if the prediction ˆy
is a normal token, it is appended to the decoding
sequence and the next step is taken. However, if ˆy
is a term label, the sequence will also contain its
corresponding target constraint retrieved from the
input term base. The use of beam search in RTT
can complicate this process, as other translation
candidates must add several PADs (padding
symbols) to compensate for the increased sequence
length when a term label is generated. This can
lead to a larger footprint and higher computational
costs at inference, especially when the number of
constraints or beam size is larger. To address this
issue, we propose a dynamic padding strategy that
reduces the number of redundant PADs. As shown
in Figure 3, we append PADs at the beginning of
the sequence rather than the end of a term label.
This allows us to truncate the longest portion of
common PADs once all candidates have some
PADs at the beginning of the sequence, resulting
in a shorter sequence. The effectiveness of this
implementation trick is shown in Figure 5.
5 Experimental results
We first validate the effectiveness of proposed ap-
proach on the same setup as Section 3.3. Then, for
fair comparisons to existing work, we also conduct
experiments with WMT18 En-De training data (Eu-
roparl, News Commentary) and common test sets
(IATE, Wiktionary).
5.1 Results on proposed test set
We compared the performance of our proposed
RTT model with two types of baseline methods:
Placeholder approaches (PH) and Code-Switch ap-
proaches (CS). We also included the Transformer
model as a baseline for comparison. Table 5 shows
the average results of BLEU, COMET and SCA on
our proposed test sets ( T, . . . , T). Unlike the PH
family or CS family, which are either proficient in
BLEU/COMET or SCA, our proposed RTT model
achieves high translation quality and constraint ac-
curacy at the same time. Specifically, the proposed
RTT model with source enhancement (RTT+SE)
achieved the highest BLEU score, with an average
score of 40.2. It also achieved the highest COMET
score, with an average of 0.4866. In terms of SCA,
although RTT+SE slightly falls behind the best sys-
tem (PH), it outperforms CS+SE in a significant
performance gap (about 20%). Similar to PH, we
note that additionally applying TE to RTT+SE is
not consistently optimal. Therefore, unless other-
wise stated, we take RTT+SE as our primary model
in the following experiments. We note that the use
of source enhancement is critical. Otherwise, the
pure RTT model degrades severely due to asymmet-
ric constraint information between the source and
target side. That is, the constraints on the source
side are term labels, while those on the target side
are constraint text. To make the improvement of
RTT clear, we also draw performance curves along
with the change in the number of constraints, as6034
illustrated in Figure 4.
5.2 Comparisons to existing methods
To compare RTT fairly with existing methods, we
perform additional experiments on WMT18 En-De
task and replicate Dinu et al. (2019)’s setup. We
use Europarl and News Commentary data as train-
ing data (2.2M), and report BLEU (sacrebleu) and
TERM accuracy (TCA) on two easy TC test sets
(IATE, Wiktionary). We consider several systems
as our baselines, such as Transformer (Vaswani
et al., 2017), Const. Dec. (Post and Vilar, 2018),
Source. Fact. (Dinu et al., 2019) and TADA (Ailem
et al., 2021). The results of our experiments are
shown in Table 6. Our proposed RTT model with
source enhancement (RTT + SE) achieved the high-
est BLEU score on both test sets, with 27.2 on
IATE and 27.8 on Wiktionary. It also achieved the
highest TCA on the IATE test set, with a score
of 99.6%. On the Wiktionary test set, the RTT
model achieved a TCA score of 98.3%, which was
slightly lower than the constraint decoding method
but still significantly higher than the other meth-
ods. Overall, the results indicate that our proposed
RTT model is not only capable of handling difficult
constraints, but also works well on such easy test
sets.
6 Analysis
6.1 Inference speed
As illustrated in Figure 5, we compared the de-
coding step size and inference speed between our
model and the vanilla Transformer. We also study
the effect when our model decodes with naive
padding (NP) and dynamic padding (DP). It is clear
that the decoding step of NP is linearly increasing
along with the number of constraints. Instead, the
DP strategy successfully reduces an average of 52%
decoding step and is very close to the baseline. As
a bonus, the shorter decoding step in DP leads to a
faster inference speed than NP. We note that RTT
with DP can also run faster than the Transformer
baseline when the constraint count is large because
the corresponding target constraints in RTT are di-
rectly substituted to avoid costly model generation.
6.2 RTT without training
Without training, RTT can also be regarded as a
modified Placeholder method. That is, the replace-
ment of term labels transformers from the end of
generation (as post-process) to the generation pe-
riod. We are interested in whether the performance
of Placeholder methods can be improved by plug-
and-play the inference part of RTT. To this end,
we tested it on two pre-trained models: PH and
PH+SE, and Table 7 listed the results. We can
see that the impact of RTT inference is different:
PH+SE benefits in COMET (+0.0061) and SCA
(+1.4%), while all metrics degrade in the vanilla
PH model. We attribute it to asymmetry constraint
information between the source and target like RTT
and RTT+SE. Specifically, RTT inference makes
the model aware of the semantics of constraint,
while the source side of PH loses information.
Even so, the improvement in PH+SE indicates that
RTT inference can be used directly on the existing
PH+SE model without further training.
6.3 Ablation study
In Table 8, we demonstrate the effects of two train-
ing components: term embedding (TermE) and loss
masking (LM). As expected, using TermE and LM
yields the best performance, as indicated by the6035
highest scores on all three evaluation metrics. Not
utilizing either component leads to a decrease in
performance. Notably, LM has a greater effect than
TermE, suggesting that allowing the model to focus
on learning the desired targets is essential. The
model appears less sensitive to TermE, likely be-
cause the word embedding of the introduced term
label implicitly informs the model of the state of
constraints.
7 Related work
There have been several approaches to addressing
the issue of translating specialized terminology in
the field of machine translation. One branch of
approaches focuses on the decoding process, such
as extending the search space (Hokamp and Liu,
2017; Post and Vilar, 2018; Hu et al., 2019) or us-
ing a finite-state acceptor (Hasler et al., 2018), to
enforce terminology translation strictly. However,
these methods can incur high calculation costs and
often result in poor translation quality (Guanhua
et al., 2021). Another branch of approaches aims
to modify the network architecture to better inte-
grate with external terminologies through the use of
alignment information (Song et al., 2020; Guanhua
et al., 2021), vectorized terminology representation
(Wang et al., 2022), or non-autoregressive trans-
lation (Susanto et al., 2020). These methods can
potentially improve the integration of terminolo-
gies, but the big changes in network architecture
greatly reduce their usability.
Data augmentation perhaps be the most widely
used approach for terminology translation in ma-
chine translation. The placeholder method is an
early solution for terminology translation by in-
troducing special term labels (Crego et al., 2016).
Michon et al. (2020) add linguistic information in
the label to compensate for the semantic loss. Al-
though effective, Placeholder techniques have dif-
ficulties producing smooth translations. Recently,
Code-Switch methods have become popular as it
overcomes this problem by allowing the model
to generate word-by-word constraint translation,
like standard neural machine translation. Song
et al. (2019) directly replaces the source constraint
with its translation in the input sequence; Dinu
et al. (2019) uses some tags to distinguish be-6036tween source constraints and target constraints;
Ailem et al. (2021) further improves performance
by masking the source constraints; Bergmanis and
Pinnis (2021) uses target lemma to make the model
learn morphology knowledge. As observed in our
experiments, Code-Switch methods are fluent in
translation but degrade in constraint accuracy. In
contrast, our approach attempts to combine the
strengths of Placeholder and Code-Switch, achiev-
ing high translation quality and constraint accuracy
simultaneously.
8 Conclusion
Our study has highlighted the importance of taking
robustness into account when comparing different
methods of terminology constraint translation. We
have found that the Placeholder and Code-Switch
families are superior in different metrics, and the
gap between them increases when dealing with
more and longer terms. Additionally, we have ob-
served that current TC test sets are inadequate for
testing the robustness of different methods. To ad-
dress this problem, we have created a new, more dif-
ficult terminology constraint test set. Moreover, we
have proposed the RTT model, which merges the
best features of the Placeholder and Code-Switch
approaches and is capable of delivering both high
translation quality and constraint accuracy regard-
less of the number of constraints and their length.
Limitations
While our proposed method demonstrates high
translation quality and constraint accuracy, it is
important to acknowledge that the hard copy mech-
anism may not be suitable for certain morphologi-
cally complex languages, such as Arabic. In Arabic,
phrases or terminologies often involve conjunctions
or prepositions and exhibit varying morphological
forms. Unfortunately, our proposed method is not
capable of effectively handling such cases, and ad-
dressing this challenge remains an open area for
future research.
Acknowledgements
We would like to thank the anonymous reviewers
for their helpful comments. We also thank Shuqin
Pan for the writing suggestions.References6037
A Appendix
A.1 Samples of different terminology
constraint test sets
We pick samples from IATE, Wiktionary, and our
proposed test set randomly and show them in Ta-
ble 10.
A.2 Detailed settings
We take two different settings for proposed test set
and previous public test set, the detailed settings
are listed in Table 9.
A.3 Samples of translation results
Table 11 shows the translation result of different
systerms.603860396040ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section Limitations
/squareA2. Did you discuss any potential risks of your work?
Our work attend to a new method for terminology constraint and hope to beneﬁt terminology
translation in the future.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section Abstract and Introduction
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 3
/squareB1. Did you cite the creators of artifacts you used?
Section 3
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Not applicable. Left blank.
C/squareDid you run computational experiments?
Section 3,5,6
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section Appendix6041/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3,5,6
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 3
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
section 3.2
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.6042