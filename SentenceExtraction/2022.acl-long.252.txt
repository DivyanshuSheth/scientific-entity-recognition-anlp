
Verna Dankers,Christopher G. Lucas, and Ivan TitovILCC, University of EdinburghILLC, University of Amsterdam
vernadankers@gmail.com ,{clucas2, ititov}@inf.ed.ac.uk
Abstract
Unlike literal expressions, idioms’ meanings
do not directly follow from their parts, pos-
ing a challenge for neural machine translation
(NMT). NMT models are often unable to trans-
late idioms accurately and over-generate com-
positional, literal translations. In this work, we
investigate whether the non-compositionality
of idioms is reﬂected in the mechanics of
the dominant NMT model, Transformer, by
analysing the hidden states and attention pat-
terns for models with English as source lan-
guage and one of seven European languages
as target language. When Transformer emits
a non-literal translation – i.e. identiﬁes the ex-
pression as idiomatic – the encoder processes
idioms more strongly as single lexical units
compared to literal expressions. This mani-
fests in idioms’ parts being grouped through
attention and in reduced interaction between
idioms and their context. In the decoder’s
cross-attention, ﬁgurative inputs result in re-
duced attention on source-side tokens. These
results suggest that Transformer’s tendency to
process idioms as compositional expressions
contributes to literal translations of idioms.
1 Introduction
An idiom is a group of words of which the ﬁgura-
tive meaning differs from the literal reading, such
as “kick the bucket,” which means to die, instead
of physically kicking a bucket. An idiom’s ﬁgu-
rative meaning is established by convention and
is typically non-compositional – i.e. the meaning
cannot be computed from the meanings of the id-
iom’s parts. Idioms are challenging for the task of
neural machine translation (NMT) (Barreiro et al.,
2013; Isabelle et al., 2017; Constant et al., 2017;
Avramidis et al., 2019). On the one hand, ﬁgures of
speech are ubiquitous in natural language (Colson,
2019). On the other hand, idioms occur much less
frequently than their parts, their meanings need
to be memorised due to the non-compositionality,Figure 1: How do attention patterns of ﬁgurative PIEs
that are paraphrased by the model compare to atten-
tion patterns of literal PIEs that are translated word for
word? We ﬁnd (1) decreased interaction between the
PIE and its context, (2) increased attention within the
PIE, (3) decreased cross-attention between the PIE and
its paraphrase, (4) increased cross-attention from the
paraphrase to </s> .
and they require disambiguation before translation.
After all, not all potentially idiomatic expressions
(PIEs) are ﬁgurative – e.g. consider “When I kicked
the bucket, it fell over”. Whether PIEs should re-
ceive a ﬁgurative or literal translation depends on
the context. Yet, little is known about neural mecha-
nisms enabling idiomatic translations and methods
for improving them, other than data annotation (Za-
ninello and Birch, 2020). Related work studies
how idioms are represented by Transformer-based
language models (e.g. García et al., 2021a,b), but
those models are not required to output a discrete
representation of the idiom’s meaning, which is a
complicating factor for NMT models.
In this work, we analyse idiom processing for
pre-trained NMT Transformer models (Vaswani
et al., 2017) for seven European languages by com-
paring literal and ﬁgurative occurrences of PIEs.
The comparison can help identify mechanics that
underlie neural idiom processing to pave the way
for methods that improve idiomatic translations.
Large-scale analyses of idiom translations suffer3608from a lack of parallel corpora (Fadaee et al., 2018).
We, therefore, use a monolingual corpus, heuris-
tically label Transformer’s translations, and ver-
ify the heuristic works as intended through human
evaluation, as described in §3. To understand how
idioms are represented in Transformer, we ﬁrstly
apply interpretability techniques to measure the im-
pact of PIEs on the encoder’s self-attention and the
cross-attention mechanisms (§4), as well as the en-
coder’s hidden representations (§5). Afterwards, in
§6, we intervene in the models while they process
idiomatic expressions to show that one can change
non-compositional translations into compositional
ones.
The results indicate that Transformer typically
translates idioms in a too compositional manner,
providing a word-for-word translation. Analyses
of attention patterns – summarised in Figure 1 –
and hidden representations point to the encoder
as the mechanism grouping components of ﬁgura-
tive PIEs. Increased attention within the PIE is ac-
companied by reduced attention to context. When
translating ﬁgurative PIEs, the decoder relies less
on the encoder’s output than for literal PIEs. These
patterns are stronger for ﬁgurative PIEs that the
model paraphrases than for sentences that receive
an overly compositional translation and hold across
the seven European languages. Considering that
a recent trend in NLP is to encourage even more
compositional processing in NMT (Raunak et al.,
2019; Chaabouni et al., 2021; Li et al., 2021, i.a.),
we recommend caution. It may be beneﬁcial to
evaluate the effect of compositionality-favouring
techniques on non-compositional phenomena like
idioms to ensure their effect is not detrimental.
2 Related Work
This section summarises work discussing human
idiom comprehension, interpretability studies for
NMT, and literature about ﬁgurative language pro-
cessing in Transformer.
Idiom comprehension Historically, idioms were
considered non-compositional units (Swinney and
Cutler, 1979). Two main views ( literal ﬁrst and
direct access ) existed for how humans interpreted
them. The former suggests humans attempt a com-
positional interpretation before considering the ﬁg-
urative interpretation in case of a contextual dis-
crepancy (Bobrow and Bell, 1973; Grice, 1975,
1989). The latter view suggests one can imme-
diately retrieve the non-compositional meaning(Gibbs Jr et al., 1994). The more recent hybrid view
posits that idioms are simultaneously processed as
a whole – primed by a superlemma (Kuiper et al.,
2007) – and word for word (Caillies and Butcher,
2007). The processing speed and retrieval of the
ﬁgurative meaning depend on the idiom’s seman-
tic properties and the context (Cain et al., 2009;
Vulchanova et al., 2019). Examples of semantic
properties are the conventionality and decompos-
ability of idioms (Nunberg et al., 1994). We do
not expect processes in Transformer to resemble id-
iom processing in humans. Nonetheless, this work
helps us determine our focus of study on the role
of the surrounding context and the extent to which
idioms’ parts are processed as a whole.
Translating PIEs that are used ﬁguratively is not
always straightforward. Baker et al. (1992) discuss
strategies for human translators: (i) Using an idiom
from the target language of similar meaning and
form, (ii) using an idiom from the target language
with a similar meaning and a different form, (iii)
copying the idiom to the translation, (iv) paraphras-
ing the idiom or (v) omitting it. In the absence
of idioms with similar meanings across languages,
(iv) is the most common strategy. Our main focus
is on literal translations ( word-for-word transla-
tions), and paraphrases .
Interpreting Transformer Analyses of Trans-
former for NMT studied the encoder’s hidden rep-
resentations and self-attention mechanism (e.g. Ra-
ganato and Tiedemann, 2018; Tang et al., 2019b;
V oita et al., 2019), the cross-attention (e.g. Tang
et al., 2019a) and the decoder (e.g. Yang et al.,
2020). The encoder is particularly important for
the contextualisation of tokens from the source sen-
tence; it acts as a feature extractor (Tang et al.,
2019b). The encoder’s bottom three layers better
represent low-level syntactic features, whereas the
top three layers better capture semantic features
(Raganato and Tiedemann, 2018). As a result, one
would expect the representations in higher layers
to be more representative of idiomaticity.
Idioms are a speciﬁc kind of ambiguity, and
whether a word is ambiguous can accurately be pre-
dicted from the encoder’s hidden representations,
as shown by Tang et al. (2019a) for ambiguous
nouns. Transformer’s cross-attention is not cru-
cial for disambiguating word senses (Tang et al.,
2018), but the encoder’s self-attention does reﬂect
ambiguity through more distributed attention for
ambiguous nouns (Tang et al., 2019a).3609Tropes in Transformer Various studies exam-
ine the Transformer-based language model BERT’s
(Devlin et al., 2019) ability to capture tropes like
metonyms (Pedinotti and Lenci, 2020), idioms
(Kurfalı and Östling, 2020), and multiple types
of ﬁgurative language (Shwartz and Dagan, 2019).
Kurfalı and Östling (2020) detect idioms based on
the dissimilarity of BERT’s representations of a
PIE and its context, assuming that contextual dis-
crepancies indicate ﬁgurative usage. Pedinotti and
Lenci (2020) measure whether BERT detects mean-
ing shift for metonymic expressions but ﬁnd cloze
probabilities more indicative than vector similari-
ties. Shwartz and Dagan (2019) ﬁnd that BERT is
better at detecting ﬁgurative meaning shift than at
predicting implicit meaning – e.g. predicting that
“a hot argument” does not involve temperature.
The most recent work studies properties of
hidden representations of noun-noun compounds
(NCs) and verb-noun compounds (VCs): García
et al. (2021b) examine (contextualised) word em-
beddings, including BERT, to compare ﬁgurative
and literal NC types . They investigate the simi-
larities between (1) NCs and their synonyms, (2)
NCs and their components, (3) in-context and out-
of-context representations, and (4) the impact of
replacing one component in the NC. Surprisingly,
idiomatic NCs are quite similar to their components
and are less similar to their synonym compared to
literal NCs. Moreover, the context of the NC hardly
contributes to how indicative its representation is of
idiomaticity, which was also shown by García et al.
(2021a), who measured the correlation between to-
ken-level idiomaticity scores and NCs’ similarity
in- and out-of-context.
In search of the idiomatic key of VCs (the part of
the input that cues idiomatic usage), Nedumpozhi-
mana and Kelleher (2021) train a probing classiﬁer
to distinguish literal usage from ﬁgurative usage.
They then compare the impact of masking the PIE
to masking the context on the classiﬁer’s perfor-
mance and conclude that the idiomatic key mainly
lies within the PIE itself, although there is some
information coming from the surrounding context.
3 Method
We use Transformer models (Vaswani et al., 2017)
with English as the source language and one of
seven languages as the target language (Dutch,
German, Swedish, Danish, French, Italian, Span-ish).Transformer contains encoder and decoder
networks with six self-attention layers each and
eight heads per attention mechanism. The mod-
els are pre-trained by Tiedemann and Thottingal
(2020) with the Marian-MT framework (Junczys-
Dowmunt et al., 2018) on a collection of corpora
(OPUS) (Tiedemann and Thottingal, 2020).We
extract hidden states and attention patterns for sen-
tences with PIEs. The analyses presented are de-
tailed for Dutch, after which we explain how the
results for the other languages compare to Dutch.
Parallel PIE corpora are rare, exist for a handful
of languages only, and are limited in size (Fadaee
et al., 2018). Rather than rely on a small paral-
lel corpus, we use the largest corpus of English
PIEs to date and annotate the translations heuris-
tically. This section provides corpus statistics and
discusses the heuristic annotation method.
MAGPIE corpus The MAGPIE corpus pre-
sented by Haagsma et al. (2020) contains 1756
English idioms from the Oxford Dictionary of En-
glish with 57k occurrences. MAGPIE contains
identical PIE matches and morphological and syn-
tactic variants, through the inclusion of common
modiﬁcations of PIEs, such as passivisation (“the
beans were spilled”) and word insertions (“spill
all the beans”).We use 37k samples annotated as
fully ﬁgurative orliteral , for 1482 idioms that con-
tain nouns, numerals or adjectives that are colours
(which we refer to as keywords ). Because idioms
show syntactic and morphological variability, we
focus mostly on the nouns. Verbs and their trans-
lation are harder to identify due to the variability.
Moreover, idiom indexes are also typically organ-
ised based on the nominal constituents, instead of
the verbs (Piirainen, 2013). Only the PIE and its
sentential context are presented to the model. We
distinguish between PIEs and their context using
the corpus’s word-level annotations.
Heuristic annotation method The MAGPIE
sentences are translated by the models with beam
search and a beam size of ﬁve. The translations are
labelled heuristically. In the presence of a literal
translation of at least one of the idiom’s keywords,3610
the entire translation is labelled as a word-for-
word translation, where the literal translations of
keywords are extracted from the model and Google
translate. When a literally translated keyword is not
present, it is considered a paraphrase .Shao et al.
(2018) previously analysed NMT translations of 50
Chinese idioms using a similar method and man-
ually curated lists of literal translations of idioms’
words to detect literal translation errors. Dankers
et al. (2022) use a similar method for 20 English
idioms, to track when a word-for-word translation
changes into a paraphrased one during training for
an English-Dutch ( En-Nl ) NMT model.
Table 1 summarises the distribution of these cat-
egories for all languages, for the subsets of ﬁgu-
rative and literal examples from MAGPIE. Gen-
erally, paraphrased translations of ﬁgurative PIEs
are more appropriate than word-for-word transla-
tions, whereas literal PIEs can be translated word
for word (Baker et al., 1992). The vast major-
ity of literal PIEs indeed result in word-for-word
translations. The subset of ﬁgurative samples re-
sults in more paraphrases, but 76% is still a
word-for-word translation, dependent on the lan-
guage. Although the statistics are similar across
languages, there are differences in which examples
are paraphrased. Figure 2 illustrates the agreement
by computing the F-score when using the predic-
tions for ﬁgurative instances of one language as
the target, and comparing them to predictions from
another language. The agreement positively corre-
lates with genetic similarity as computed using the
Uriel database (Littell et al., 2017).
To assess the quality of the heuristic method, one
(near) native speaker per target language annotated
350 samples, where they were instructed to focus
on one PIE keyword in the English sentence. An-
notators were asked whether (1) the English word
was present in the translation (initially referred to as
“copy”), (2) whether there was a literal translation
for the word, or (3) whether neither of those options
were suited, referred to as the “paraphrase”.Due
to the presence of cognates in the “copy” category,
that category was merged with the “word for word”
category after the annotation. Table 2 summarises
the accuracies obtained. Of particular interest are
samples that are ﬁgurative and paraphrased, since
they represent the translations that are treated non-
compositionally by the model, as well as instances
that are literal and translated word for word, since
they represent the compositional translations for
non-idiomatic PIE occurrences. These categories
have annotation accuracies of 75% and89%,
respectively. During preliminary analyses, an anno-
tation study was conducted for Dutch by annotators
from the crowd-sourcing platform Proliﬁc. The an-
notators and the heuristic method agreed in 83% of
the annotated examples, and for 77% of the sam-
ples an average of 4 annotators agreed on the label
unanimously (see Appendix A for more details).
Sentences containing idioms typically yield
lower BLEU scores (Fadaee et al., 2018). MAG-
PIE is a monolingual corpus and does not allow us
to compute BLEU scores, but we refer the reader
to Appendix G for an exploratory investigation for
MAGPIE’s idioms using the En-Nl training corpus.3611
4 Attention
We now turn to comparing how literal and ﬁgura-
tive PIEs are processed by Transformer. Whether a
PIE is ﬁgurative depends on the context – e.g. com-
pare “in culinary school, I felt at sea ” to “the sailors
were at sea ”. Within Transformer, contextualisa-
tion of input tokens is achieved through the atten-
tion mechanisms, which is why they are expected to
combine the representations of the idioms’ tokens
and embed the idiom in its context. This section
discusses the impact of PIEs on the encoder’s self-
attention and the encoder-decoder cross-attention.
To assert that the conclusions drawn in this sec-
tion are not simply explained by shallow statistics
of the data used, we recompute the results in Ap-
pendix C for (1) a data subset excluding variations
of PIEs’ standard surface forms, (2) a data subset
that includes PIEs that appear in both ﬁgurative and
literal contexts, (3) a data subset that controls for
the number of tokens within a PIE. Qualitatively,
these results lead to the same ﬁndings.
Attention within the PIE For the En-Nl Trans-
former, Figure 3a visualises the distribution of
attention weights in the encoder’s self-attention
mechanism for incoming weights to one noun con-
tained in the PIE from the remaining PIE tokens.
Throughout the ﬁgures in the paper, we refer to the
subset of sentences that have a ﬁgurative PIE and a
paraphrased translation as ‘ ﬁg-par ’. The subset of
sentences with a literal PIE and a word-for-word
translation are indicated by ‘ lit-wfw ’. We compare
those two subsets, as well as all instances of ﬁg-
urative PIEs (‘ ﬁg’) to all instances of literal PIEs
(‘lit’) using the labels from the MAGPIE dataset.
Overall, there is increased attention in ﬁgurative
occurrences of PIEs compared to literal instances.
This difference is ampliﬁed for the subset of ﬁgu-
rative PIEs yielding paraphrased translations. This
pattern is consistent for all languages, as is dis-
played in Figure 3d that presents the difference
between the mean attention weights of the ﬁgura-
tive, paraphrased instances, and the mean weights
of the literal instances translated word for word.
In other words, ﬁgurative PIEs are grouped more
strongly than their literal counterparts.
Attention between PIEs and context To exam-
ine the interaction between a PIE and its context,
we obtain the attention weights from tokens within
the PIE to nouns in the surrounding context of size
10 (Figure 3b).Similarly, the attention from the
surrounding context to PIE nouns is measured (Fig-
ure 3c). There is reduced attention from PIEs to
context for ﬁgurative instances, which mirrors the
effect observed in Figure 3a: increased attention3612within the PIE is accompanied by reduced atten-
tion to the context. This pattern is consistent across
languages (Figure 3d). From the context to the
PIE, the average weight is slightly higher for literal
PIEs, but the effect size is small, indicating only
a minor impact of ﬁgurativeness on the context’s
attention weights. This will be further investigated
in §5.
Cross-attention To analyse the encoder-decoder
interaction, we decode translations with beam size
ﬁve, and extract the cross-attention weights for
those translations. Afterwards, alignments are com-
puted for the models’ predictions by, together with
1M sentences from the OPUS corpus per target
language, aligning them using the eflomal toolkit
(Östling et al., 2016). The alignment is used to
measure attention from a token aligned to a PIE’s
noun to that noun on the source side.
Figure 4a presents the attention distribution for
the weights that go from the noun’s translation to
that PIE noun on the source side, for the En-Nl
model. There is a stark difference between ﬁgu-
rative and literal PIEs, through reduced attention
on the source-side noun for ﬁgurative PIEs. This
difference is particularly strong for the ﬁgurative
sentences that are paraphrased during the transla-
tion: when paraphrasing the model appears to rely
less on the source-side noun than when translat-
ing word for word. Where does the attention ﬂow,
instead? To some extent, to the remaining PIE to-
kens (Figure 4b). A more pronounced pattern of
increased attention on the </s> token is shown in
Figure 4c. Similar behaviour has been observed by
Clark et al. (2019) for BERT’s [SEP] token, who
suggest that this indicates a no-operation . In Trans-
former’s cross-attention mechanism, this would
mean that the decoder collects little information
from the source side. Figure 4d compares the mean
attention weights of the seven languages for the
ﬁgurative inputs that are paraphrased to the literal
samples that are translated word for word, conﬁrm-
ing that these patterns are not speciﬁc to En-Nl
translation.
Collectively, the results provide the observations
depicted in Figure 1. When paraphrasing a ﬁgu-rative PIE, the model groups idioms’ parts more
strongly than it would otherwise – i.e. it captures
the PIE more as one unit. A lack of grouping all ﬁg-
urative PIEs could be a cause of too compositional
translations. Increased attention within the PIE is
accompanied by reduced interaction with context,
indicating that the PIE is translated in a stand-alone
manner, contrary to what is expected, namely that
contextualisation can resolve the ﬁgurative versus
literal ambiguity. There is less cross-attention on
the source-side PIE and more attention on the </s>
token when the model emits the translation of ﬁgu-
rative (paraphrased) PIEs. This suggests that even
though the encoder cues ﬁgurative usage, the de-
coder retrieves a PIE’s paraphrase and generates its
translation more as a language model would.
5 Hidden representations
Within Transformer, the encoder’s upper layers
have previously been found to encode semantic
information (e.g. Raganato and Tiedemann, 2018).
PIEs’ hidden states are expected to transform over
layers due to contextualisation, and become increas-
ingly more indicative of ﬁgurativeness. This sec-
tion focuses on the impact of PIEs on the hidden
states of Transformer’s encoder. We ﬁrstly discuss
how much these hidden states change between lay-
ers. Secondly, we measure the inﬂuence of a token
by masking it out in the attention and analysing the
degree of change in the hidden representations of
its neighbouring tokens. This analysis is performed
to consolidate ﬁndings from §4, since the extent to
which attention can explain model behaviour is a
topic of debate (Jain and Wallace, 2019; Wiegreffe
and Pinter, 2019).
5.1 PIE changes over layers
To compare representations from different layers,
we apply canonical correlation analysis (CCA)
(Hotelling, 1936), using an implementation from
Raghu et al. (2017). Assume matrices A2R
andB2R, that are representations for N
data points, drawn from two different sources with
dimensionalities dandd– e.g. different layers
of one network. CCA linearly transforms these
subspacesA=WA,B=VB such as to max-
imise the correlations f;:::;gof the
transformed subspaces. We perform CCA using
>60k random token vectors for a previously un-
used subset of the MAGPIE corpus – the subset of
sentences that did not contain nouns in the PIEs –3613
to compute the CCA projection matrices WandV.
WandVare then used to project new data points
before measuring the data points’ correlation. The
CCA similarity reported in the graphs is the av-
erage correlation of projected data points. We do
not perform CCA separately per data subset due to
the small subset sizes and the impact of vocabulary
sizes on CCA correlations for small datasets (see
Appendix E).
We compute the CCA similarity for hidden states
from adjacent layers for PIE and non-PIE nouns.
Figurative PIEs in layer lare typically less similar
to their representation in layer l 1compared to
literal instances (shown in Figures 5b and 5c). The
results for non-PIE nouns (Figure 5a for the En-Nl
Transformer) do not differ across data subsets, sug-
gesting that changes observed for ﬁgurative PIEs
are indeed due to ﬁgurativeness.
5.2 Intercepting in attention
We now compute similarities of representations for
the model in two setups: with and without one
token masked in the attention mechanism, as sug-
gested by V oita et al. (2019). Masking a token
means that other tokens are forbidden to attend to
the chosen one. This can reveal whether the atten-
tion patterns discussed in §4 are indicative of the
inﬂuence tokens have on each other’s hidden rep-
resentations. The ﬁrst representation is the hidden
representation from layer lfor a token encoded as
usual. The second one is the hidden representa-
tion of layer lwhen applying the ﬁrst l 1layers
as usual and masking one token in the lth layer.
CCA is again performed on separate data, where a
non-PIE noun is masked, to provide the projection
matrices applied before computing similarities in
the remainder of this subsection.
Masking a PIE token To estimate the inﬂuence
of PIE nouns, we ﬁrst compute the CCA similar-
ity between two representations of tokens from the
PIE’s context while masking one PIE noun in the at-
tention for one of those representations. Similarly,
we measure the inﬂuence on other tokens within the
PIE when masking one PIE noun. Within the PIE,
the impact is the largest for ﬁgurative instances
(see Figure 6a for En-Nl and 6e for averages over
layers for all languages). This is in line with the
attention pattern observed. However, whether the
impact is the largest on context tokens from ﬁgu-
rative or literal instances is dependent on the layer3614
(Figure 6b), suggesting that the slight difference in
attention from the context to the PIE observed in §4
need not represent a difference in impact between
ﬁgurative and literal PIEs.
Masking a context token Lastly, we measure
the inﬂuence of masking a noun in the context of
the PIE on PIE tokens and non-PIE tokens. Within
the PIE, as shown in Figures 6c and 6e, ﬁgura-
tive instances are less affected by the masked con-
text noun compared to literal occurrences of PIEs.
Again, this mirrors the patterns observed for atten-
tion where there was less attention on the context
for ﬁgurative PIEs. When masking a non-PIE noun
and measuring the impact on non-PIE tokens, one
would hardly expect any differences between data
subsets, as is conﬁrmed in Figures 6d and 6e.
In summary, these analyses conﬁrm most of the
trends noted for attention patterns. Intercepting
in the attention through masking indicated that for
PIE tokens, there is less interaction with the context.
However, this does not necessarily mean that the
context interacts less with ﬁgurative PIEs compared
to literal PIEs, even if there was a slight difference
in attention (see §4). The CCA analyses further-
more showed that ﬁgurative PIEs are distinct from
typical tokens in how they change over layers.6 (Amnesic) probing for ﬁgurativeness
The previous analyses compared the hidden states
for ﬁgurative and literal PIEs, but do not use these
labels, otherwise. We now train logistic regression
probing classiﬁers (Conneau et al., 2018) to predict
the label from hidden representations. The probes’
inputs are the hidden states of PIE tokens, and the
F-scores are averaged over ﬁve folds. All samples
from one PIE are in the same fold, such that the
classiﬁer is evaluated on PIEs that were absent from
its training data. The results (Figure 7) indicate ﬁg-
urativeness can be predicted from these encodings,
with performance increasing until the top layer for
all languages. F-scores for the embeddings al-
ready exceed a random baseline, indicating some
idioms are recognisable independent of context.
Finally, we use probing classiﬁers to change
models’ PIE translations through amnesic prob-
ing(Elazar et al., 2021): removing features from
hidden states with iterative null-space projection
(INLP) (Ravfogel et al., 2020) and measuring the
inﬂuence of these interventions. INLP trains k
classiﬁers to predict a property from vectors. Af-
ter training probe i, parametrised by W, the vec-
tors are projected onto the nullspace of W. The
projection matrix of the intersection of all knull
spaces can then remove features found by these
classiﬁers. Using INLP, we train 50classiﬁers to
distinguish ﬁgurative PIEs that will be paraphrased
from those to be translated word for word. Af-
terwards, we run the previously paraphrased PIE
occurrences through the model while removing in-
formation from the PIE’s hidden states using INLP
– i.e. information that could be captured by linear
classiﬁers, which need not be the only features rele-
vant to idiomatic translations. Per idiom, we record
the percentage of translations that are no longer
paraphrased. We report the scores for idioms from
four folds and BLEU scores comparing translations
that changed label before and after INLP. A ﬁfth
fold is used for parameter estimation (Appendix F).
Table 3 presents the results. When intervening
in the hidden states for all layers l2f0;1;2;3;4g,
the average success rate per PIE ranges from 27%
(for Swedish) to 40% (for Spanish). The interven-
tions yield reduced attention within the PIE and
increased interaction with the context (see Table 3b
for Dutch). Table 3 also provides results for a base-
line probe predicting whether the half-harmonic
mean of the zipf-frequency of PIE tokens is below
or above average. This probe is successful too,3615
emphasising how brittle idiomatic translations are:
when removing information from the hidden states,
the model reverts to compositional translations.
Figure 8 provides example translations before
and after the application of INLP, while indicating
how the attention on the underlined noun changes.
Generally, the attention on that noun reduces for
tokens other than itself.
In summary, when applying INLP to hidden states,
the attention patterns resemble patterns for literal
tokens more, conﬁrming a causal connection be-
tween the model paraphrasing ﬁgurative PIEs and
the attention. However, amnesic probing cannot
change the paraphrases for all idioms; thus, ﬁgura-
tiveness is not merely linearly encoded in the hid-
den states. The probing accuracies differed across
layers and suggested ﬁgurativeness is more easily
detectable in higher layers, which is in line with
the changes across layers observed in §5.
7 Conclusion
Idioms are challenging for NMT models that often
generate overly compositional idiom translations.
To understand why this occurs, we analysed idiom
processing in Transformer, using an English id-
iom corpus and heuristically labelled translations inseven target languages. We compared hidden states
and attention patterns for ﬁgurative and literal PIEs.
In the encoder, ﬁgurative PIEs are grouped more
strongly as one lexical unit than literal instances
and interact less with their context. The effect is
stronger for paraphrased translations, suggesting
that capturing idioms as single units and translating
them in a stand-alone manner aids idiom process-
ing. This ﬁnding agrees with results from Zaninello
and Birch (2020), who ascertain that encoding an
idiom as one word improves translations. It also
agrees with the INLP application causing more
compositional translations whilst changing the at-
tention. By relying less on the encoder’s output,
the decoder determines the meaning of ﬁgurative
PIEs more independently than for literal ones. To
improve idiomatic translations, future work could
use these insights to make architectural changes to
improve the grouping of idioms as single units by
training speciﬁc attention heads to capture multi-
word expressions or by penalising overly composi-
tional translations in the training objective.
Although we learnt about mechanics involved in
idiomatic translations, the vast majority of transla-
tions was still word for word, indicating that non-
compositional processing does not emerge well
(enough) in Transformer. Paradoxically, a recent
trend is to encourage more compositional process-
ing in NMT (Chaabouni et al., 2021; Li et al., 2021;
Raunak et al., 2019, i.a.). We recommend caution
since this inductive bias may harm idiom transla-
tions. It may be beneﬁcial to evaluate the effect
of compositionality-favouring techniques on non-
compositional phenomena to ensure their effect is
not detrimental.
Acknowledgements
We are grateful to Rico Sennrich for providing
feedback on an earlier version of the paper. Many
thanks to Agostina Calabrese, Matthias Lindemann,
Gautier Dagan, Irene Winther, Ronald Cardenas,
Helena Fabricius-Vieira and Emelie van de Vreken
for data annotation and assistance with queries
about their native languages. VD is supported
by the UKRI Centre for Doctoral Training in Nat-
ural Language Processing, funded by the UKRI
(grant EP/S022481/1) and the University of Edin-
burgh, School of Informatics and School of Phi-
losophy, Psychology & Language Sciences. IT ac-
knowledges the support of the European Research
Council (ERC StG BroadSem 678254) and the3616Dutch National Science Foundation (NWO Vidi
639.022.518).
References361736183619Appendix A Survey details
A.1 Crowd-sourcing annotations for Dutch
In an early phase of the research, the quality of the
heuristic annotation method was estimated through
a survey conducted using the Qualtrics platform by
annotators from Proliﬁc. The heuristic annotation
method labelled a translation as ‘word for word’
if the literal translation of a keyword was present,
where the keyword was elicited from MarianMT,
and from the translation tool DeepL. These anno-
tators were native speakers of Dutch, and ﬂuent
in English. To guard the quality of the data col-
lection, participants went through a pre-screening
process that consisted of a shorter version of the
survey with three practice questions and seven regu-
lar questions. Participants were selected for the full
study if they correctly answered practice questions,
used all three of the labels (paraphrase, word for
word, copy), and did not choose ‘copy’ if the key-
word was clearly absent from the translation. The
main survey consisted of three parts: (1) An expla-
nation of what an idiom is, of potential literal and
ﬁgurative usage of PIEs, the meaning of the three la-
bels, and the format to be used in the study. (2) One
practice exercise where three potential translations
of one sentence had to be connected to the correct
label. (3) Lastly, 38 questions were ﬁlled out: 12
instances that were ﬁgurative and were paraphrased
by the model, 4 literal instances paraphrased by the
model, 8 literal instances that were translated word
for word, 8 ﬁgurative instances that were translated
word for word, 6 copies (3 ﬁgurative, 3 literal).
If the participant indicated that it was a word-
by-word translation, the follow-up question would
be asked, where the participant indicated the lit-
eral translation of the keyword. We repeated the
instruction of what constitutes a word-by-word
translation since participants would often select
the (conventionalised) idiomatic translation in the
pre-screening phase – e.g. ‘handbereik’ for ‘ﬁn-
gertips’, for which a literal translation would be
‘vingertoppen’.
Table 4 summarises the survey outcomes. The
annotators and the heuristic method agreed in 83%
of the cases. For 77% of the samples, the annota-
tions agreed on the label unanimously.
A.2 Collecting annotations for 7 languages
Later on, the analyses were applied to heuristically
annotated data for all seven languages. The proce-
dure to elicit the translations of keywords from Mar-
ianMT and an online translation tool were adapted
to improve the recall of keywords for languages
other than Dutch. Afterwards, postgraduate stu-
dents from the local university were invited to an-
notate the data in exchange for payment, where one
annotator annotated all 350 samples for a language.
To reduce the cognitive load of the experiment,
only sentences with 40 tokens were shown to
the participants. The annotators were native in the
target language and ﬂuent in English, with the ex-
ception of the Swedish speaker, that was native in
Norwegian and Finnish, and ﬂuent in Swedish and
English. The annotators participated in a similar
pre-screening test with language-speciﬁc explana-
tions and examples, and seven practice questions.
If the annotators’ answers differed from what was
expected, the instructions were discussed with the
annotator before they proceeded with the full sur-
vey, and they ﬁlled out the remainder of the survey
without intermediate help or instructions. Table 5
shows an example question for Dutch.
A.3 Ethical considerations
The surveys referred to in §3 were both approved
through to the university’s research ethics process,3620where an independent committee assessed the setup
of the survey, the research’s potential harmful im-
pacts and the compensation for the participants.
In collecting data annotations, participants were
shown data from the MAGPIE corpus, available
under the CC-BY-4.0 License. All other informa-
tion shown to them was either collected from the
computational model, or written up by the authors.
Any identiﬁable information about the participants
was stored separately from the participants’ anno-
tations, for the purposes of compensation. Partic-
ipants were able to provide informed consent to
data collection and anonymised data being used in
academic publications. They were given the oppor-
tunity to withdraw at any time. Participants were
compensated above the minimum hourly wage of
the country in which they were a resident at the
time of participating in the study.Appendix B Aligning PIEs and
paraphrases
When automatically aligning sentences with PIEs
to translations that are labelled as a paraphrase
by the heuristic, how does the automated aligner
(theeflomal toolkit of Östling et al., 2016) handle
paraphrases? For many PIEs ( 34% of the ﬁg-par
sentences for all languages), the paraphrases do not
have a word in the translation aligned to the PIE
keyword on the source side using eflomal . These
examples are excluded. However, for a subset that
appears more well-known, there are common para-
phrases that the PIE keyword aligns to. We provide
examples for Dutch in Table 6. The examples pro-
vided in the table together cover 48% of all aligned
sentences used in the cross-attention analysis for
theﬁg-par category, and all are reasonable align-
ments.3621Appendix C Attention for data subsets
The attention weight distributions in the main paper
included all data. To further investigate whether the
differences in attention patterns observed are due to
factors other than ﬁgurativeness, we recompute the
attention patterns for three additional data subsets.PIE identical matches We ﬁrst use a subset that
only includes samples for which MAGPIE reports
anidentical match between the PIE and the En-
glish sentence, that includes 17k samples. This
subset excludes sentences with modiﬁcations to the
typical surface form of a PIE, such as upper-cased
tokens or insertions of a token into the PIE (e.g.
“That gossip of a man spilled allof the beans.”).
Figure 9 shows the three attention patterns pre-
viously discussed for the encoder’s self-attention
– i.e. attention from the PIE to the PIE, attention
from the PIE to the context, and from the context
to the PIE. Overall, the patterns resemble those
discussed in the main text, apart from Figure 9a,
where ﬁgurative instances do not display consis-
tently higher attention weights compared to literal
instances, although the ﬁg-par subset does.
This procedure is repeated for the cross-attention
distributions. Figure 10 depicts the three patterns
discussed in the main paper – i.e. from the aligned
target-side tokens to the PIE noun, to another PIE
token, and to </s> – for this data subset, providing
the same qualitative ﬁndings.
Intersection of PIEs The second subset (re-
ferred to as intersection ) considered is one that
only contains idioms that are among all of the sub-
sets of ﬁgurative, literal, paraphrased and word for
word instances, covering 11k examples from the
dataset. The results for the encoder’s self-attention3622
patterns are shown in Figure 11. Figure 12 sum-
marises the results for the cross-attention mecha-
nisms. These results lead to the same qualitative
ﬁndings as mentioned in the main paper, and, in
the encoder, the PIE to PIE attention patterns for
ﬁgurative and literal PIEs are even more distinct.
Controlling PIE length To investigate the im-
pact of the length of a PIE and the length of its
context on the results, we now report additional
measures over sentences, namely:
•theaverage number of MarianMT tokens
labelled as being part of the PIE (in MAGPIE
words like prepositions and determiners are
not counted as part of the PIE, so the annota-
tion can be discontinuous);
•thedistance between the ﬁrst and the last
token of the PIE (two tokens right next to
each other have a distance of 1);
•therelative position of the tokens that are
annotated as belonging to the PIE, which im-
pacts the potential context size, but could also
impact how a PIE ‘behaves’;
•the average distance of the ﬁrst position of
the PIE’s context tokens (PIE - 10) to the last
position (PIE + 10) ( context length ).
Figure 13 summarises these statistics for the
MAGPIE PIEs. The last two metrics are very stable
across categories, with an average relative position
of 0.57 for PIEs (0.58 for ﬁgurative, 0.56 for lit-
eral), and context lengths of 17.0 (17.0 for ﬁgura-
tive, 17.1 for literal). The ﬁrst two metrics indicate
that ﬁgurative PIEs are a bit longer than literal PIEs
(0.69 words), and that the distance between the ﬁrst
and the last word is slightly larger (0.46 positions).
To assert that these differences do not substan-
tially impact our qualitative ﬁndings, we compute
the attention analyses over a data subset that only
uses sentences where there are three tokens an-
notated for the PIE, for which the start and end
are three positions apart. This covers a subset of
approximately 7k samples, with small variations
between languages due to slightly different tokeni-
sation of the English words. Figures 14 and 15
present the results for the encoder’s self-attention
and the encoder-decoder cross-attention analyses,
respectively. Qualitatively, our ﬁndings for this
subset do not differ from the previous ﬁndings.3623
Appendix D Results for 7 languages, per
layer
Figures 16 and 17 present the results per layer, for
the (cross-)attention graphs from §4. Figure 18
present the results per layer, for the CCA similarity
graphs from §5.3624Appendix E Two-step CCA
CCA can be used to compare representations over
different layers of the same network or different
networks in a way that is invariant to afﬁne trans-
formations (Raghu et al., 2017). The CCA similar-
ity expresses the extent to which two representa-
tions contain the same information while account-
ing for transformations in these two views of the
data. Nonetheless, the similarity depends on the
data used to perform CCA. Even with a dataset
that is at least an order of magnitude larger than
the number of dimensions in the hidden represen-
tations, the composition of the dataset impacts the
outcome. Particularly relevant in the context of
our work is the vocabulary size that impacts CCA
computations.
We illustrate this by measuring how hidden rep-
resentations change over layers, randomly sam-
pling tokens and considering multiple dataset com-
positions, varying from 64 occurrences of 80
unique tokens, to 4 occurrences of 1280 unique
tokens. Recomputing CCA per subset yields the
similarities shown in Figure 19a. Although the
overall pattern of lower similarity between lower
layers and higher similarity between higher layers
is present for all subsets, the absolute similarity
measures differ between subsets. In Figure 19b,
however, where the projection matrix is computed
on a separate dataset, subsets show comparable
similarities. The differences between the methods
decrease as the number of hidden representations
used to perform CCA grows.
Performing CCA separately per (relatively
small) subset of the MAGPIE corpus could thus re-
ﬂect vocabulary differences rather than systematic
differences due to ﬁgurativeness. We merely want
to apply CCA to account for differences between
layers and differences with and without masking
attention, and thus apply two-step CCA, computing
projection matrices on a separate dataset.Appendix F Amnesic probing
Amnesic probing (Elazar et al., 2021) evaluates
the behavioural inﬂuence of information recovered
from hidden representations Hby probes, by re-
moving that information from the representation
and measuring the change in behaviour on the main
task. INLP, proposed by Ravfogel et al. (2020),
is used to remove this information from the rep-
resentations, by training kclassiﬁers to predict a
property from input vectors. After training probe
i, parametrised by W, the vectors are projected
onto the nullspace of W, using projection matrix
P, such thatWPH= 0. The projec-
tion matrix of the intersection of all knull spaces
can then remove features found by the kclassiﬁers.
Using INLP, we train 50classiﬁers to detect ﬁgu-
rative, paraphrased PIEs from ﬁgurative PIEs trans-
lated word for word from the hidden state. After-
wards, we apply the projection matrices while the
model processes previously paraphrased transla-
tions. We separate the PIEs into ﬁve folds, using
one for parameter estimation. For every foldis
used to train INLP’s probes,is used to measure
whether the performance of the kprobes decreases
andis used to measure the changed percentage.
Dependent on where one intervenes in the model,
amnesic probing may be more or less successful,
since not every layer encodes the linguistic prop-
erty and higher layers could recover information
removed from lower layers (Elazar et al., 2021).
The parameter estimation performed measures the
impact of different combinations of layers as the
average success rate per PIE type (success means
achieving a word-for-word translation). As shown
in Figure 20, there is quite some variation among
languages, but generally intervening in the lower
layers of Transformer is the most successful, and
including the sixth layer is quite detrimental. The
results in the main body of the paper are computed3625by intervening on the hidden states of PIE tokens
inl2f0;1;2;3;4g.
Appendix G Idioms in OPUS
To understand whether the model’s translations re-
ﬂect target translations from its training corpus,
we extract up to 500 identical matches per idiom
from OPUS for the En-Nl model. These target
translations are labelled heuristically, resulting in
54% of paraphrased instances, which is substan-
tially higher than the percentage of paraphrased
instances in the model’s translations. This may be
the result of infrequent idioms contained in OPUS,
for which the model fails to learn the correct im-
plicit meaning, even though the corpus does pro-
vide paraphrases. Table 7 illustrates how the pre-
dicted translations’ labels relate to the labels of
target translations and provides BLEU scores per
subset. Samples with a paraphrased target transla-
tion score substantially lower compared to those
with a word-for-word or copied target translation,
emphasising the negative impact of idioms on trans-
lation quality.3626