
Liming Wang, Mark Hasegawa-Johnsonand Chang D. YooUniversity of Illinois Urbana-ChampaignKorea Advanced Institute of Science Technology
Abstract
Unsupervised speech recognition (ASR-U) is
the problem of learning automatic speech
recognition (ASR) systems from unpaired
speech-only and text-only corpora. While var-
ious algorithms exist to solve this problem, a
theoretical framework is missing to study their
properties and address such issues as sensitivity
to hyperparameters and training instability. In
this paper, we proposed a general theoretical
framework to study the properties of ASR-U
systems based on random matrix theory and
the theory of neural tangent kernels. Such a
framework allows us to prove various learnabil-
ity conditions and sample complexity bounds
of ASR-U. Extensive ASR-U experiments on
synthetic languages with three classes of transi-
tion graphs provide strong empirical evidence
for our theory (code available at cactuswith-
thoughts/UnsupASRTheory.git).
1 Introduction
Unsupervised speech recognition (ASR-U) is the
problem of learning automatic speech recognition
(ASR) systems from unpaired speech-only and text-
only corpora. Such a system can not only signifi-
cantly reduce the amount of annotation resources
required for training state-of-the-art ASR system,
but serve as a bridge between spoken and written
language understanding tasks in the low-resource
setting. Since its first proposal (Liu et al., 2018), it
has seen remarkable progress and the current best
system (Baevski et al., 2021) has achieved compara-
ble performance to systems trained with paired data
on various languages. However, there are several
mysteries surrounding ASR-U, which potentially
hinder the future development of such systems.
In particular, prior experiments have found that
training the current state-of-the-art ASR-U model,
wav2vec-U (Baevski et al., 2021), requires care-
ful tuning over the weights of various regulariza-
tion losses to avoid converging to bad local op-
tima and that even despite extensive regularizationweight tuning, wav2vec-U may still fail to converge
(Ni et al., 2022). Therefore, it remains a mystery
whether or when unpaired speech and text data in-
deed provide sufficient information for learning an
ASR system. Another mystery is whether the suc-
cess of existing ASR-U models based on generative
adversarial net (GAN) (Goodfellow et al., 2014) is
sufficiently explained by the GAN objective func-
tion per se, or also requires other factors, such as
randomness in training, quirks in the data used and
careful domain-specific hyper-parameter settings,
etc.
In this paper, we provide a theoretical analysis
of ASR-U to investigate the mysteries surrounding
ASR-U. First, we prove learnability conditions and
sample complexity bounds that crucially depend
on the eigenvalue spacings of the transition prob-
ability matrix of the spoken language. Random
matrix theory shows that such learnability condi-
tions are achievable with high probability. Next, we
study the gradient flow of GAN-based ASR-U and
provide conditions under which the generator min-
imizing the GAN objective converges to the true
generator. Finally, to verify our theory empirically,
we perform GAN-based ASR-U experiments on
three classes of synthetic languages. Not only do
we observe phase transition phenomena predicted
by our theory, but we achieve stable training with
lower test word error rate by several modifications
of the existing state-of-the-art ASR-U system in-
spired by our theory.
2 Problem formulation
General formulation The training data comprise
a set of sequences of quantized speech vectors, and
a set of sequences of phoneme labels. The data are
unpaired: there is no label sequence that matches
any one of the speech sequences. The data are,
however, matched in distribution. Let P(x)and
P(y)be the probability mass functions (pmfs) of
theispeech vector in a sequence, x∈X, and the1192
jphoneme in a sequence, y∈Y, respectively:
the requirement that they are matched in distri-
bution is the requirement that there exists some
generator function O: (X,Y)→ {0,1}such that
/summationdisplayP(x)O(x, y) =P(y) (1)
The problem of ASR-U is to find the generator
function O.
GAN-based ASR-U Eq. (1) leverages sequence
information to remove ambiguity: Omust be
an optimal generator not only for the position-
independent distributions of XandY, but
also for their position-dependent distributions
P, P∀i∈N. In reality we cannot observe
every possible sequence of speech vectors, or every
possible sequence of phonemes, but instead must
estimate Ofrom samples. To address this issue, a
GAN can be used to reproduce the empirical distri-
bution of the training dataset with minimum error,
subject to the generator’s inductive bias, e.g., sub-
ject to the constraint that the function Ois a matrix
of the form O∈ {0,1}, where |X|and|Y|
are the sizes of the alphabets XandY, respectively.
As shown in Figure 1, a GAN achieves this goal
by computing Oas the output of a neural network,
O=G(x, y;θ), and by requiring Gto play a zero-
sum game with another neural network called the
discriminator Dwith the following general utility
function:
minmaxJ(G, D) :=E[a(D(Y))]−
E[b(D(G(X)))].(2)
For the original GAN (Goodfellow et al., 2014),
a(D) = log( σ(D))andb(D) =−log(1−σ(D)),
where σis the sigmoid function. For the Wasser-
stein GAN (Arjovsky et al., 2017), D(Y)is aLipschitz-continuous scalar function, and a(D) =
b(D) = D. A maximum mean discrepancy
(MMD) GAN (Li et al., 2017) minimizes the squred
norm of Eq. (2), where D(Y)is an embedding
into a reproducing kernel Hilbert space (RKHS).
In this paper we take the RKHS embedding to be
the probability mass function of a scalar random
variable D(Y), and assume that the discriminator
is trained well enough to maintain Eq. (2). In this
situation, the MMD GAN minimizes Eq. (2) with
a(D) =b(D) =Y. In practice, Eq. (2) is opti-
mized by alternatively updating the parameters of
the discriminator and the generator using gradient
descent/ascent:
ϕ=ϕ+η∇J(G, D) (3)
θ=θ−ν∇J(G, D). (4)
Theoretical questions of ASR-U The aforemen-
tioned formulation of ASR-U is ill-posed. Intu-
itively, the function Ohas finite degrees of freedom
(O∈ {0,1}), while Eq. (1) must be valid for
an infinite number of distributions ( PandP
fori∈N), so there is no guarantee that a solution
exists. On the other hand, if the sequence is unim-
portant ( P=P∀i, j∈N), then the solution
may not be unique. One important question is then:
what are the necessary and sufficient conditions for
Eq. (1) to have a unique solution?
Further, it is well-known that gradient-based
training of GAN can be unstable and prior works
on ASR-U (Yeh et al., 2019; Baevski et al., 2021)
have used various regularization losses to stabilize
training. Therefore, another question of practical
significance is: what are the necessary and suffi-
cient conditions for the alternate gradient method
as described by Eq. (3)-(4) to converge to the true
generator for ASR-U? In the subsequent sections,
we set out to answer these questions.
3 Theoretical analysis of ASR-U
3.1 Learnability of ASR-U: a sufficient
condition
A key assumption of our theory is that the distribu-
tion of the speech and the text units can be modeled
by a single hidden Markov model whose hidden
states are N-grams of speech units and whose out-
puts are N-grams of text units, as shown in Fig-
ure 1.
The parameters of this HMM are its initial prob-
ability vector, π, which specifies the distribution1193of the first Nspeech vectors X∈X, its
transition probability matrix A, which specifies the
probability of any given sequence of Nspeech vec-
tors given the preceding Nspeech vectors, and its
observation probability matrix, which specifies the
distribution of one phone symbol given one speech
vector:
π:=P∈∆
A:=P∈∆
O:=P∈∆,
where ∆is the k-dimensional probability simplex.
The first-order Markov assumption is made plau-
sible by the use of N-gram states, X, rather
than unigram states; with sufficiently long N, nat-
ural language may be considered to be approx-
imately first-order Markov. The connection be-
tween the N-gram states and the unigram obser-
vations requires the use of a selector matrix, E=
1⊗I, where ⊗denotes the Kronecker
product, thus P=πAE, and for multiples
ofN, Eq. (1) can be written P=πAEO.
It turns out that a crucial feature for a spoken lan-
guage to be learnable in an unsupervised fashion is
that it needs to be “complex” enough such that a
simple, symmetric and repetitive graph is not suf-
ficient to generate the language. This is captured
by the following assumptions on the parameters A
andπ.
Assumption 1. There exists an invertible matrix
U∈R= [U|U|···|U], where
the columns of each matrix U= [u|···|u]
are eigenvectors with the same eigenvalue and a di-
agonal matrix Λ = blkdiag(Λ,···,Λ), where
each matrix Λis a diagonal matrix with all diag-
onal elements equal to the same scalar λ, such
thatA=UΛUwith|X|≥K≥ |X|nonzero
eigenvalues λ> λ>···> λ.
Assumption 2. For at least |X|values of j, there
is at least one ks.t.πu̸= 0.
With Assumptions 1 and 2, we can consider the
following algorithm: First, we construct the follow-
ing matrices
P:=
P
P...
P
, P:=
P
P...
P
,(5)Then, Osatisfies the following matrix equation
PO=P. (6)
The binary matrix Oin Eq. (6) is unique if and
only if Phas full column rank. The following
theorem proves that this is indeed the case under
our assumptions.
Theorem 1. Under Assumptions 1 and 2, Phas
full column rank and perfect ASR-U is possible.
Further, the true phoneme assignment function is
O=PP, where P= (PP)P
is the left-inverse of P.
Further, if we measure how far the matrix Pis
from being singular by its smallest singular value
defined as
σ(P) := min∥Pv∥
∥v∥,
we can see that Pbecomes further and further
away from being singular as the sequence length
Lgets larger. An equivalent result for a different
purpose has appeared in the Theorem 1 of (Bazán,
2000).
Lemma 1. Under Assumptions 1 and 2 and for sim-
plicity assuming the number of distinct eigenvalues
K=|X|forT, then we have
σ(P)≥
δ/summationtextλ(A)
κ(V(λ))min∥ˆr∥(7)
where δ:= min|λ(A)−λ(A)|,λ(A)
is the smallest eigenvalue of square matrix A,
κ(V(λ))is the condition number of the
square Vandermonde matrix created from eigenval-
uesλ(A), . . . , λ(A),r=πUΩE, andΩ
is the set of rows of Ucorresponding to eigen-
value λ(A), after orthogonalizing them from every
other block of rows, i.e., U=L[Ω|···|Ω]
such that Lis lower-triangular, and the blocks Ω
andΩare orthogonal.
Next, we will show that Assumption 1 can be
easily met using random matrix arguments.
3.2 Finite-sample learnability of ASR-U
Matched setup Now we show that the require-
ment for distinct eigenvalues is a mild one as it can
easily be satisfied with random transition matri-
ces. According to such a result, ASR-U is feasible1194with high probability in the (empirically) matched
setting commonly used in the ASR-U literature,
where the empirical generated and true distribu-
tions can be matched exactly by some generator
in the function class (Liu et al., 2018). Our proof
relies crucially on the seminal work of (Nguyen
et al., 2017) on eigenvalue gaps of symmetric ran-
dom matrices with independent entries.
In the context of ASR-U, it is of particular inter-
est to study the eigenvalue gaps of a Markov ran-
dom matrix, which unlike the symmetric case, is
asymmetric with correlated entries. Fortunately, by
modifying the proof for Theorem 2.6 of (Nguyen
et al., 2017), we can show that if the language
model belongs to a special but rather broad class
of Markov random matrices defined below and the
states are non-overlapping N-gram instead of the
more common overlapping ones, it should have at
least|X|distinct eigenvalues with minimum spac-
ing depending on |X|and the Nfor the N-gram.
Definition 1. (symmetric Markov random matrix)
A symmetric Markov random matrix is a matrix of
the form A:=DW, where the adjacency matrix
Wis a real, symmetric random matrix with positive
entries and bounded variance and Dis a diagonal
matrix with d=/summationtextW>0.
Intuitively, a symmetric Markov random matrix
is the transition matrix for a reversible Markov
chain formed by normalizing edge weights of a
weighted, undirected graph.
Theorem 2. (simple spectrum of symmetric
Markov random matrix) Let A=DW∈
Rbe a real symmetric Markov random ma-
trix with adjacency matrix W. Further, suppose
W=F+X, where Fis a deterministic
symmetric matrix with eigenvalues of order n
andXis a symmetric random matrix of zero-
mean, unit variance sub-Gaussian random vari-
ables. Then we have for any C > 0, there exists
B >4γC+ 7γ+ 1such that
maxPr[|λ−λ| ≤n]≤n,
with probability at least 1−O(exp(−αn))for
some α>0dependent on Bandγ=
max{γ,1/2}.
Corollary 1. Suppose the speech feature tran-
sition probability is a symmetric Markov ran-
dom matrix A:=DWwith entries W∼
Uniform(0 ,2√
3)andDis a diagonal matrix withd=/summationtextW. Then for any ϵ > 0, there ex-
istsα>0such that with probability at least
1−O/parenleftbig
|X|+ exp/parenleftbig
−α|X|/parenrightbig/parenrightbig
, the transition
probability matrix Ahas|X|distinct eigenvalues
with minimum gap |X|>0.
The proof of Theorem 2 and Corollary 1 are
presented in detail in the Appendix A.2.
Unmatched setup In the finite-sample, un-
matched setup, the empirical distribution of the
fake text data generated by the GAN does not nec-
essarily match the empirical distribution of the true
text data. Assuming the discriminator is perfect in
the sense that it maintains Eq. (2) non-negative, and
assuming D(Y)is a scalar random variable, then
minimizing Eq. (2) is equivalent to minimizing a
divergence measure d(·,·), between the empirical
text distribution, P, and the text distribution gen-
erated by O(y) =ˆP(y|x):
mind(P, PO), (8)
where γ >0. For example, for the original GAN,
d(·,·)is the Jensen-Shannon distance and for the
MMD GAN, d(·,·)is the Ldistance between the
expectations E[D(Y)]under the distributions P
andPO. In both cases, however, Eq. (8) can
be minimized using a decomposable discriminator
defined to be:
E[a(D(Y))] =/summationdisplayE[a(D(Y))] (9)
E[b(D(G(X)))] =/summationdisplayE[b(D(G(X))],
(10)
with components D:|Y| ∝⇕⊣√∫⊔≀→R, l= 1,···, L.
Under the assumption that Dis decomposable and
that the MMD GAN is used, we have the following
sample complexity bound on perfect ASR-U.
Theorem 3. The empirical risk minimizer (ERM)
of Eq. (8) recovers the true assignment Operfectly
fromnspeech frames and ntext characters with
probability 1−2δif
σ(P)≥/radicalbigg
4L|Y|(n+n) +L|X|n
nn+
10/radicaligg
Llog
n∧n,
where n∧n:= min {n, n}.1195
3.3 Training dynamic of GAN-based ASR-U
So far, we have assumed the GAN training is able
to find the optimal parameters for the discriminator
and the generator. However, there is no guarantee
that this is indeed the case with gradient updates
such as Eq. (3). To analyze the behaviour of the
GAN training dynamic for ASR-U, we follow prior
works on neural tangent kernel (NTK) (Jacot et al.,
2018) to focus on the infinite-width, continuous-
time regime, or NTK regime, where the generator
and the discriminator are assumed to be neural
networks with an infinite number of hidden neurons
trained with gradient descent at an infinitely small
learning rate. Though highly idealized, studying
such a regime is practically useful as results from
this regime can often be converted to finite-width,
discrete-time settings (See, e.g., (Du et al., 2019)).
For simplicity, denote f:=Dandg:=G
and define L(f) := J(g, f), then in the NTK
regime, between each generator step, the training
dynamic of the discriminator can be described by
the following partial differential equation (PDE):
∂ϕ=∇L(f). (11)
Letf= limfbe the limit of Eq. (11). If
the limit exists and is unique, the generator loss is
well-defined as C(g) :=J(g, f). Note that the
output of the ASR-U generator is discrete, which
is not a differentiable function per se, but we can
instead directly parameterize the generated text
distribution asP:=P◦Ofor some softmax
posterior distribution O:
O(y) :=/productdisplayexp(h(x))/summationtextexp(h(x)), (12)
where his a neural network, and is assumed to be
one layer in our analysis, though it can be extended
to multiple layers with slight modifications using
techniques similar to those in (Du et al., 2019).Using such a generator, the generator dynamic
can be then described by the following PDE:
∂θ=/summationdisplayb(f(y))∇P(y), (13)
where the right-hand side is the term in the gradient
ofCwith respect to θignoring the dependency
of the discriminator f. Define the NTKs of the
discriminator and the generator (distribution) as
K(y, y) =E/bracketleftigg
∂f(y)
∂ϕ∂f(y)
∂ϕ/bracketrightigg
(14)
K(y, y) =E/bracketleftigg
∂P(y)
∂θ∂P(y)
∂θ/bracketrightigg
,
(15)
where Wis the initialization distribution (usually
Gaussian).
Note that the NTKs are |Y|×|Y|matrices for
ASR-U due to the discrete nature of the generator.
A key result in (Jacot et al., 2018) states that as the
widths of the hidden layers of the discriminator and
generator go to infinity, K→K, K→K
stay constant during gradient descent/ascent and
we have
∂f=K(diag( P)∇a
−diag( P)∇b), (16)
∂P=Kb, (17)
where ∇{a, b}=/bracketleftig/bracketrightigandb=
(b(y)).
However, Eq. (16)-(17) is in general highly non-
linear and it remains an open problem as to their
convergence properties. Instead, we focus on the
case when the discriminator fis decomposable
with components f, l= 1,···, L, and simplify1196Eq. (16) and Eq. (17) into PDEs involving only
samples at a particular time step:
∂f=K/parenleftbig
diag( P)∇a
−diag( P)∇b/parenrightbig
, (18)
∂O=/summationdisplayP(x)Kb, (19)
for all l= 1,···, L, x ∈Xin terms of the step-
wise NTKs defined as:
K(y, y) :=E/bracketleftigg
∂f(y)
∂ϕ∂f(y)
∂ϕ/bracketrightigg
K(y, y) :=E/bracketleftigg
∂O(y)
∂θ∂O(y)
∂θ/bracketrightigg
.
We further focus on the special case that fis
parameterized by a two-layer neural network with
ReLU activation, though the framework can be
extended to network of arbitrary depths:
f(y) = lim1√m/summationdisplayvmax{W,0}.(20)
In this case, under mild regularity conditions, we
can show that the generator trained with the al-
ternate gradient method minimizes Eq. (8), which
under the same condition as in Section 3.2, implies
ASR-U is feasible.
Theorem 4. Suppose the following assumptions
hold:
1.The discriminator is decomposable and pa-
rameterized by Eq. (20), whose parameters
are all initialized by standard Gaussian vari-
ables;
2.The generator is linear before the softmax
layer;
3. The GAN objective is MMD;
4.The linear equation PO=Phas at least
one solution.
Then we have for any solution Oof Eq. (19),
limPO=P.
4 Experiments
Synthetic language dataset To allow easy con-
trol of the eigenvalue spacings of the transitionmatrix Tand thus observe the phase transition phe-
nomena predicted by our theory, we design six
synthetic languages with HMM language models
as follows. First, we create the HMM transition
graph by treating non-overlapping bigrams as hid-
den states of the HMM. The hidden state of the
HMM will henceforth be referred to as the “speech
unit”, while the observation emitted by the HMM
will be referred to as the “text unit”. For the asymp-
totic ASR-U, we control the number of eigenvalues
of the Markov transition graph by varying the num-
ber of disjoint, identical subgraphs. The number of
distinct eigenvalues of the whole graph will then
be equal to the number of eigenvalues of each sub-
graph. For the finite sample setting, we instead
select only Hamiltonian graphs and either gradu-
ally decrease the degrees of the original graph to its
Hamiltonian cycle or interpolate between the graph
adjacency matrix and that of its Hamiltonian cycle.
Thus, we can increase σ(P)by increasing w.
For both the subgraph in the former case and the
Hamiltonian graph in the latter, we experiments
with circulant, de Bruijn graphs (de Bruijn, 1946)
and hypercubes, as illustrated in Figure 2. Next, we
randomly permute the hidden state symbols to form
the true generator mapping from the speech units
to text units. To create matched speech-text data,
we simply sample matched speech and text unit
sequences using a single HMM. For unmatched
datasets, we sample the speech and text data inde-
pendently with two HMMs with the same parame-
ters. Please refer to Appendix B for more details.
Model architecture For finite-sample ASR-U,
we use wav2vec-U (Baevski et al., 2021) with
several modifications. In particular, we experi-
ment with various training objectives other than
the Jensen-Shannon (JS) GAN used in the original
wav2vec-U, including the Wasserstein GAN (Liu
et al., 2018) and the MMD GAN. All additional
regularization losses are disabled . Moreover, we
experimentally manipulate two hyperparameters:
(1) the averaging strategy used by the generator,
and (2) whether to reset the discriminator weights
to zero at the beginning of each discriminator train-
ing loop. More details can be found in Appendix B.
Phase transition of PER vs. eigenvalue gaps:
asymptotic case The phoneme error rate (PER)
as a function of the number of eigenvalues of Afor
the asymptotic ASR-U on the synthetic datasets are
shown in Figure 3. For all three graphs, we observe1197
clear phase transitions as the number of eigenval-
ues exceeds the number of speech units, and an
increase of the number of distinct, nonzero eigen-
values required for perfect ASR-U as the number
of speech units increases.
Phase transition of PER vs. eigenvalue gaps:
finite-sample case The PER as a function of the
least singular value σ(P)for the finite-sample
ASR-U on the synthetic datasets are shown in Fig-
ure 4. As we can see, the ASR-U exhibit the phase
transition phenomena in all three graphs, albeit
with differences in the critical point and their rate of
approaching the perfect ASR-U regime. While the
PER generally decreases as σ(P)gets larger,
we found a dip in PER in the circulant graph case
asσ(P)moves from 10to10. Though
unexpected, this observation is not contradictory
to our theory since our theory does not make ex-
plicit predictions about the rate of phase transi-
tion for ASR-U. Across different GAN models, we
found that JSD generally approaches perfect ASR-
U at a faster rate than MMD in all three graphs,
suggesting the use of nonlinear dynamic may be
beneficial. Nevertheless, the overall trends for dif-
ferent GANs remain in large part homogeneous.1198Between Wasserstein and MMD, we observe very
little difference in performance, suggesting the reg-
ularization effect of NTK is sufficient to control the
Lipschitz coefficient of the network. Finally, for
the MMD GAN in the matched setting, we found
the network is able to achieve perfect ASR-U re-
gardless of the spectral properties of the Markov
transition graphs, which confirms our theory that
a symmetric Markov random matrix tends to have
simple eigenvalue spectrum suitable for ASR-U.
Effect of discriminator reset As pointed out by
(Franceschi et al., 2021), a discriminator may suffer
from residual noise from previous updates and fail
to approximate the target divergence measure. We
analyze such effects for MMD and JSD as shown in
Figure 5. We observed consistent trends that mod-
els whose weights are reset to the initial weights
every discriminator loop outperform those without
resetting. The effect is more pronounced for JSD
GAN than MMD GAN and for smaller σ(P).
Effect of generator averaging strategy The
original wav2vec-U (Baevski et al., 2021) directly
feeds the text posterior probabilities Ointo the dis-
criminator, which we refer to as the “soft input”
approach. Alternatively, we can instead calculate a
weighted average of the gradient form over the sam-
plesy∈Yas in Eq. (13), which we refer to as the
“outside cost” approach. The comparison between
the two approaches are shown in Figure 6. We
observed mixed results: for MMD GANs, the soft-
input approach outperforms the outside-cost ap-
proach and performs best among the models in the
high-σ(P)setting; for JSD GANs, we found
that the outside-cost approach performs slightly
better than the soft-input approach. Such inconsis-
tencies may be another consequence of the regular-
ization effect predicted by the GANTK. We leave
the theoretical explanation as future work.
5 Related works
(Glass, 2012) first proposed the challenging task of
ASR-U as a key step toward unsupervised speech
processing, and framed it as a decipherment prob-
lem. (Liu et al., 2018) takes on the challenge by
developing the first ASR-U system with ground-
truth phoneme boundaries and quantized speech
features as inputs, by training a GAN to match the
speech-generated and real text distributions. (Chen
et al., 2019) later replaced the ground truth bound-
aries with unsupervised ones refined iteratively byan HMM, which also incorporates language model
information into the system. (Yeh et al., 2019)
explored the cross entropy loss for matching the
generated and real text distribution, but it is prone
to mode collapse and needs the help of additional
regularization losses such as smoothness weight.
More recently, (Baevski et al., 2021; Liu et al.,
2022) proposed another GAN-based model using
continuous features from the last hidden layer of
the wav2vec 2.0 (Baevski et al., 2020) model and
additional regularization losses to stabilize training.
Their approach achieves ASR error rates compara-
ble to the supervised system on multiple languages,
making it the current state-of-the-art system.
To better understand the learning behavior of
ASR-U systems, (Lin et al., 2022) analyze the ro-
bustness of wav2vec-U against empirical distribu-
tion mismatch between the speech and text, and
found that N-gram language model is predictive
of the success of ASR-U. Inspired by the original
framework in (Glass, 2012), (Klejch et al., 2022)
proposed a decipher-based cross-lingual ASR sys-
tem by mapping IPA symbols extracted from a
small amount of speech data with unpaired pho-
netic transcripts in the target language.
Our analysis on the sufficient condition of ASR-
U is based on previous work on the asymptotic
behaviour of GAN objective functions (Goodfel-
low et al., 2014; Arjovsky et al., 2017). Our finite-
sample analysis takes inspiration from later work
extending the asymptotic analysis to the finite-
sample regimes (Arora et al., 2017; Bai et al.,
2019). Such frameworks, however, do not account
for the alternate gradient optimization method of
GANs and inevitably lead to various inconsisten-
cies between the theory and empirical observa-
tions of GAN training (Franceschi et al., 2021).
Building upon prior works (Mescheder et al., 2017,
2018; Domingo-Enrich et al., 2020; Mroueh and
Nguyen, 2021; Balaji et al., 2021), (Franceschi
et al., 2021) proposed a unified framework called
GANTK based on NTK (Jacot et al., 2018) to de-
scribe the training dynamic of anyGAN objectives
and network architectures. Our analysis on the
training dynamic of ASR-U adopts and extends the
GANTK framework to handle discrete, sequential
data such as natural languages.
6 Conclusion
In this paper, we develop a theoretical framework
to study the fundamental limits of ASR-U as well1199as the convergence properties of GAN-based ASR-
U algorithms. In doing so, our theory sheds light
on the underlying causes of training instability for
such algorithms, as well as several new directions
for more reliable ASR-U training.
7 Limitations
Our theory currently assumes that input speech fea-
tures are quantized into discrete units, as in (Chen
et al., 2019), while preserving allthe linguistic in-
formation in the speech. As a result, our theory
does not account for the loss of linguistic infor-
mation during the quantization process, as often
occurred in realistic speech datasets. Further, more
recent works (Baevski et al., 2021; Liu et al., 2022)
have shown that continuous features, with the help
of additional regularization losses, can achieve al-
most perfect ASR-U. Such phenomena is beyond
explanations based on our current theory and re-
quire generalizing our theory to continuous speech
features. Further, our model assumes that suffi-
ciently reliable phoneme boundaries are fed to the
ASR-U system, and kept fixed during training. It
will be interesting to extend our framework to sys-
tems with trainable phoneme boundaries, such as
the wav2vec-U systems, to better understand its
effect on training stability.
Acknowledgements
This work was supported by Institute of Informa-
tion & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea gov-
ernment(MSIT) (No.2022-0-00184, Development
and Study of AI Technologies to Inexpensively
Conform to Evolving Policy on Ethics)
References1200
A Proofs of theoretical results
A.1 Learnability of ASR-U: a sufficient
condition
Proof. (Theorem 1)
For simplicity, we assume that the eigenvalues
ofAare real though a similar argument applies to
complex eigenvalues as well. By Assumptions 1
and 2, it can be verified that
P=πAE
=πUΛUE,
where E=1⊗I, where ⊗denotes the
Kronecker product. Define c=πu. Define
rto be the krow of the jblock of the matrix
UE, i.e., UUE=/summationtext/summationtextur. De-
fine the matrix RasR= [r,···, r], where
r=/summationtextcr. Then we have:
P=/summationdisplayλr
P=V(λ)R,
where V(λ)is the Vandermonde matrix
formed by nonzero eigenvalues λ,···, λand
withLcolumns, K≥ |X|by Assumption 1. R
has full column rank of K≥ |X|by Assumption 2,
therefore it is possible to write R=ˆRL, where
ˆR= ˆr, . . . , ˆr]is a matrix with orthogonal
columns, and Lis lower-triangular. As a result, we
havePis full rank iff V(λ)has full row rank
of at least |X|, which holds by Assumption 1.
Proof. (Lemma 1)
Use the Rayleigh-characterization of eigenvalues1201of the matrix PP, we have
σ(P)
=/radicalig
λ(PP)
=/radicalbigg
minwPPw
=/radicalbigg
minwRVVRw
≥/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplayλminwRVVRw
=σ(P)/radicaltp/radicalvertex/radicalvertex/radicalbt/summationdisplayλ,
where λis the eigenvalue of Awith minimum
absolute value, and Pis the first |X|rows of
P. Therefore, to lower bound σ(P), it suf-
fices to lower bound σ(P). But note that
σ(P)
= min∥VRw∥
≥σ(V) min∥Rw∥
≥σ(V)
κ(V)min∥ˆr∥
≥|det(V)|
κ(V)min∥ˆr∥
=|/producttext|λ−λ|
κ(V)min∥ˆr∥
≥δ
κ(V)min∥ˆr∥
where the last equality uses the closed-form for-
mula of the determinant of a square Vandermonde
matrix, and where the behaviour of κ(V), the
condition number of the Vandermonde matrix, has
been studied in depth in (Bazán, 2000).
A.2 Finite-sample learnability of ASR-U:
matched setup
Theory of small ball probability The proof of
Theorem 2 makes extensive use of the theory of
small ball probability. Therefore, we briefly pro-
vide some background on the subject. First, we
define the small ball probability of a vector xas
follows.Definition 2. (Small ball probability) Given a fixed
vector x= (x,···, x), and i.i.d random vari-
ables ξ= (ξ,···, ξ), the small ball probability
is defined as
ρ(x) := supPr[|ξx−a| ≤δ].
Intuitively, small ball probability is the amount
of “additive structure” in x: for example, if the
coordinates of xare integer multiples of each other
andξ’s are symmetric Bernoulli variables, the
product ξxtends to have small magnitude as
terms cancel out each other very often. Since
sparser vectors tend to have less additive structure,
small ball probability can also be used to measure
how sparse the weights of xare. Another way to
look at this is that, if the L2 norm of xis fixed and
most of the weight of xis gathered in a few coordi-
nates, the product ξxhas higher variance and is
thus less likely to settle in any fixed-length intervals.
This is quantitatively captured by the celebrated
Offord-Littlewood-Erdös (OLE) anti-concentration
inequality (and its inverse) for general subgaussian
random variables:
Lemma 2. (Erdös, 1945; Rudelson and Vershynin,
2008; Tao and Vu, 2009) Let ϵ >0be fixed, let
δ >0, and let v∈Rbe a unit vector with
ρ(v)≥m.
Then all but at most ϵmof the coefficients of vhave
magnitude at most δ.
Note that here we use a slight generalization of
the notion of sparsity called compressibility defined
as follows.
Definition 3. ((α, δ)-compressible) A vector v∈
Ris(α, δ)-compressible if at most ⌊αn⌋of its
coefficients have magnitude above δ.
Note that a sparse vector with a support of size
at most ⌊αn⌋is(α,0)-compressible.
A more generally applicable anti-concentration
inequality requires the following definition of gen-
eralized arithmetic progression, which is used to
quantify the amount of additive structure of a vec-
tor.
Definition 4. (Generalized arithmetic progression)
A generalized arithmetic progression (GAP) is a
set of the form
Q={aw:a∈Z,|a| ≤N,1≤i≤r},1202where r≥0is called the rank of the GAP and
w,···, w∈Rare called generators of the GAP .
Further, the quantity
vol(Q) :=/productdisplay(2N+ 1)
is called the volume of the GAP .
Lemma 3. (Continuous inverse Littlewood-Offord
theorem, Theorem 2.9 of (Nguyen and Vu, 2011))
Letϵ >0be fixed, let δ >0and let v∈Rbe a
unit vector whose small ball probability ρ:=ρ(v)
obeys the lower bound
ρ≫n.
Then there exists a generalized arithmetic progres-
sionQof volume
vol(Q)≤max/parenleftbigg
O/parenleftbigg1
ρ√αn/parenrightbigg
,1/parenrightbigg
such that all but at most αnof the coefficients
v,···, vofvlie within δof Q. Furthermore, if
rdenotes the rank of Q, then r=O(1)and all
the generators w,···, wofQhave magnitude
O(1).
While applicable for any ρ≫nrather than
only those with ρ(v)≥nas required by
Lemma 2, Lemma 3 is weaker than Lemma 2 in
the sense that rather than showing that the vector
is compressible with high probability and thus cov-
ered by the set of compressible vectors, it proves
that the vector is covered by a small set with high
probability.
A related notion that is often more convenient for
our analysis is the segmental small ball probability,
which is simply small ball probability computed on
a segment of the vector:
ρ(x) = infρ(x),
From the definition, it is not hard to see that
ρ(x)≥ρ(x).
Eigen-gaps of symmetric Markov random ma-
trix Armed with tools from the theory of small
ball probability, we will establish guarantees of
eigenvalue gaps for a symmetric Markov random
matrix. First, we shall show that Theorem 2 implies
Corollary 1.Proof. (Proof of Corollary 1) Using Theorem 2
and union bound, the probability that a symmetric
Markov random matrix has at least |X|distinct
eigenvalues can be bounded as
Pr/bracketleftbigg
min|λ−λ| ≤ |X|/bracketrightbigg
≤
|X|maxPr/bracketleftbig
|λ−λ| ≤ |X|/bracketrightbig
=O(|X|),
with probability at least 1−O(exp(−α|X|)).
It turns out that a symmetric Markov random ma-
trix enjoys various properties analogous to a sym-
metric matrix. First, we can show that its eigenval-
ues are real. This can be proved by noting that for a
symmetric Markov random matrix A:=DW
and for any of its eigenvalues λwith eigenvector v,
DWv=λv
⇐⇒DWD(Dv) =λDv,(21)
which implies Ahas the same spectrum as
DWD, which is symmetric and thus has
a real spectrum. Further, we can prove a variant of
Cauchy’s interlace theorem for symmetric Markov
random matrix.
Lemma 4. Suppose A=DW∈Ris a
symmetric Markov random matrix with adjacency
matrix Wand eigenvalues λ≥ ··· ≥ λand
A=DWwith adjacency matrix Wand
eigenvalues ν≥ ··· ≥ ν, m < n is formed by
successively deleting i-rows and i-columns, then
λ≤ν≤λ.
Proof. Using the previous observation in Eq. 21,
we can apply the standard Cauchy’s interlacing
theorem on A:=DWD andA:=
DWD, then we have
λ(A) =λ(A)≤λ(A) =λ(A)
≤λ(A) =λ(A).
Next, we can show that the eigenvalues of a
symmetric Markov random matrix and its adja-
cency matrix are simultaneously distributed within
the bounded intervals [−10n,10n]and
[−10n,10n]with high probability. For this and
subsequent proofs, we will assume γ=γ >1/2.1203Lemma 5. LetA=DWbe a symmetric
Markov random matrix with adjacency matrix W
and properties defined in Theorem 2, then we have
with probability at least 1−O(exp(−αn)),
λ(A)∈[−10n,10n],
λ(W)∈[−10n,10n],
for any 1≤i≤nand some α>0.
Proof. First, by definition, we can let W=F+
X, where Fis a deterministic matrix with eigen-
values of order nandXis a symmetric matrix
whose elements are independent zero-mean unit-
variance subgaussian random variables. Using stan-
dard results from random matrix theory (Anderson
et al., 2009), we have
{λ(X),···, λ(X)} ⊂[−10n,10n],
with probability at least 1−O(exp(−αn)). There-
fore, Weyl’s matrix perturbation inequality then
ensures that
{λ(W),···, λ(W)} ∈[−10n,10n],
with probability at least 1−O(exp(−αn)). Sup-
pose this event occurs and use Lemma 4 and the
variational characterization of eigenvalues, we have
λ(A) = minmaxvDWDv
= minmaxvWv
= minmaxvWv
vDv,
where Vis a subspace of dimension i−1. Com-
bining the two results, we have with probability at
least1−O(exp(−αn)),
max/vextendsingle/vextendsingle/vextendsingle/vextendsinglevWv
vDv/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤max|vWv|
min|vDv|
=λ(W)
min|d|
Recall that d=/summationtextw=/summationtext(f+x),
where w,f, and xare the (i, j)elements
ofW, F, and Xrespectively. Since A=
DWis a Markov matrix we assume that f
and the distribution of xare selected to guarantee
thatw≥0, e.g., it must be true that f≥0.We also know that xis a zero-mean unit-variance
sub-Gaussian random variable, therefore
Pr{w< δ}= Pr{x<−f+δ}
≤2 exp/parenleftbigg
−1
2(f−δ)/parenrightbigg
Pr{d< nδ}= Pr

/summationdisplayw< nδ


≤2 exp ( −αn)
where α=−(¯f−δ), and ¯f=/summationtextf. Therefore, with probability at least
1−O(exp(−αn))where α=α+α,
λ(A)∈[−10n,10n],1≤i≤n(22)
Remark. Lemma 5 ensures that for any symmet-
ric Markov random matrix A=DWwith
properties defined in Theorem 2, we can focus our
attention on any eigenvector vwhose eigenvalue is
no greater than O(n)and whose ∥Wv∥is of
order nwith high probability. Therefore, we will
assume such conditions in later proofs.
Using Lemmas 4-5, we can reduce Theorem 2 to
the following statement on small ball probability of
theeigenvectors ofX, analogous to the arguments
for symmetric random matrices in (Nguyen et al.,
2017).
Lemma 6. LetA=DW∈Rbe a sym-
metric Markov random matrix with adjacency ma-
trixW. Let λ(A)andw= [u, b]∈Rbe
thei-th eigenvalue and eigenvector of the matrix
A, respectively, where u∈Randb∈R.
Then we have
Pr[|λ(A)−λ(A)| ≤δ]≤
nPr[ρ(v)≥cnδ] +cnδ
+O(exp(−αn)),
for some c, α>0.
Proof. LetWandDbe the (n−1)-
dimensional minors of WandD, respectively,
then
/bracketleftbiggWw
ww/bracketrightbigg/bracketleftbiggu
b/bracketrightbigg
=λ/bracketleftbiggD0
0d/bracketrightbigg/bracketleftbiggu
b/bracketrightbigg
,1204where wis the last column of W. Let vbe the
i-th eigenvector of matrix A:=DW,
we have
vWu+vWb=λ(A)vDu
=⇒|(λ(X)−λ(X))|maxd≥
|(λ(A)−λ(A))vDu|=|vwb|.
Therefore,
Pr[|λ(A)−λ(A)| ≤δ]
≤Pr/bracketleftbigg|vw|
maxd≤δ
b/bracketrightbigg
.
By Lemma 4, λ(A)≤λ(A)≤λ(A)
and we have
Pr[|λ(A)−λ(A)| ≤δ]≤
Pr[|λ(A)−λ(A)| ≤δ]≤
Pr/bracketleftbigg|vw|
maxd≤δ
b/bracketrightbigg
.
dis typically O(n), but we have been unable to
prove that it is necessarily O(n). Consider that
w=f+x, where Fis a symmetric matrix
with eigenvalues λ(F) =O(n), therefore/summationdisplayf= (F1)≤ ∥F1∥=∥F∥
≤n∥F∥=O/parenleftig
n/parenrightig
.
W=F+X, therefore
Pr/braceleftig
d̸=O/parenleftig
n/parenrightig/bracerightig
≤Pr

/summationdisplayx>/summationdisplayf−nδ


≤O(exp(−αn))
Now, by the law of total probability,
Pr/bracketleftbigg|vw|
maxd≤δ
b/bracketrightbigg
≤Pr/bracketleftbigg|vw|
maxd≤δ
b,maxd≤O(n)/bracketrightbigg
+ Pr/bracketleftbigg
maxd̸=O/parenleftig
n/parenrightig/bracketrightbigg
≤Pr/bracketleftigg
|vw|=O/parenleftigg
δn
b/parenrightigg/bracketrightigg
+O(exp(−αn)).By symmetry, we can choose any row and the
corresponding column to split the matrix and de-
rive inequality of the same form. Further, sup-
pose for some b>0, with probability at least
1−exp(−cn), there are at least ncoordinates
ofwthat are at least band suppose we choose the
split index Juniformly at random. Let the J-th
column of WbeWand the J-th coefficient of the
eigenvector of Wbew, then we have
Pr[|λ(A)−λ(A)| ≤δ]
≤Pr/bracketleftigg
|vW| ̸=O/parenleftigg
δn
w/parenrightigg
|N≥n/bracketrightigg
+O(exp(−cn)) +O(exp(−αn))
≤n
nPr/bracketleftigg
|vW| ̸=O/parenleftigg
δn
b/parenrightigg
|N≥n/bracketrightigg
+O(exp(−cn)) +O(exp(−αn)),
where the second inequality can be proved as fol-
lows. Define
E={N≥n},
F={w≥b},
G=/braceleftigg
|vW| ̸=O/parenleftigg
δn
w/parenrightigg/bracerightigg
,
H=/braceleftigg
|vW| ̸=O/parenleftigg
δn
b/parenrightigg/bracerightigg
.
Then use the above definitions and the fact that F
andGare conditionally independent given N, we
have
Pr/bracketleftigg
|vW| ̸=O/parenleftigg
δn
b/parenrightigg
|N≥n/bracketrightigg
= Pr(H|E)≥Pr(F ∩ G|E )≥n
nPr(G|E)
=n
nPr/bracketleftigg
|vW| ̸=O/parenleftigg
δn
w/parenrightigg
|N≥n/bracketrightigg
.
Further, to remove the dependency on N, notice
that
Pr(H|E)≤Pr(H)
Pr(E)= Pr(H) +O(exp(−cn)).
Next, by the pigeonhole principle, at least one
coordinate of the unit eigenvector wis at least
n, and thus we can let c=∞,n= 1 and1205b=nand arrive at
Pr [|λ(A)−λ(A)| ≤δ]
≤nPr/bracketleftig
|vW| ̸=O/parenleftbig
δn/parenrightbig/bracketrightig
+O(e)
≤nρ(v) +O(exp(−αn)), (23)
where α=c+α. Finally, recall the definition
of small ball probability, we have
Pr/bracketleftig
|vW| ≤δ/bracketrightig
≤Pr/bracketleftig
|vW| ≤δ|ρ(v)≤ϵ/bracketrightig
+ Pr[ ρ(v)> ϵ]
≤Pr[ρ(v)> ϵ] +ϵ,
and thus applying this inequality with δ:=
cδnon Eq. (23) yields the result.
Remark. We can sharpen the bound in Lemma 6
by extending the delocalization theorem for a
symmetric Wigner matrix (see Theorem 4.2 of
(Nguyen et al., 2017)) to a symmetric Markov ran-
dom matrix and using it to choose a larger nin
the proof. This will be left as future work.
With the help of Lemma 6, we can reduce Theo-
rem 2 to the following theorem.
Theorem 5. LetA∈Rbe a symmetric
Markov random matrix matrix and vbe an eigen-
vector with eigenvalue λ=O(n), then for any
fixed C > 0, there exists some B > max{4γC+
3γ,4γ+ 1}such that
ρ(v)≤n,
with probability at least 1−O(exp(−αn))for
some αdepending on B.
Similar to the proof for the perturbed symmet-
ric matrices in (Nguyen et al., 2017), we reduce
Theorem 5 to the following.
Theorem 6. Letvbe the eigenvector and Bbe
the constant defined in Theorem 5. Then for any
n≤δ≤n, we have with probability
O(exp(−αn)),
n≤ρ(v)≤nρ(v). (24)
To show that Theorem 6 implies Theorem 5, we
prove the contrapositive of the statement, that is,
ifρ(v)> n, then there exists n≤δ≤
nsuch that Eq. 24 holds with probability at
least1−O(exp(−αn)). To construct such δ, let
δ:=n
δ:=nδ,forj= 0,···, J−1with J=⌊B/2γ⌋. By
construction, we have
n=δ≤δ≤δ≤n
ρ(v)≥ρ(v)≥n.
Suppose Eq. 24 does not hold for any δ:=δ, or
otherwise the result follows, we have
ρ(v)≥nρ(v)≥n>1,
ifB≥4γC+ 3γ, which contradicts the fact that
ρ(v)≤1. As a result, there has to exist some j
such that Eq. 24 holds.
Again similar to the perturbed symmetric matrix
case in (Nguyen et al., 2017), we divide the proof
of Theorem 6 into the compressible case and the
non-compressible case. For the compressible case,
we first prove the following lemma.
Lemma 7. Suppose vis an eigenvector of a sym-
metric Markov random matrix A:=DWwith
adjacency matrix Wand the same properties de-
fined in Theorem 2, and suppose there exists δ∈
[n, n]such that ρ(v)≥(αn), we
have with probability O(exp(−αn)),
n≤ρ(v)≤nρ(v).
Proof. Using concentration inequalities, we have
with probability at least 1−O(exp(−αn))for
some α>0,
d=O/parenleftig
n/parenrightig
,1≤i≤n (25)
Further, since ρ(v)≥(αn), by Lemma 2,
we have vis(O(α), δ)compressible, and thus
there exists Iof of size O(αn)such that v> δ
only if i∈I. Without loss of generality, let I=
{n−k,···, n}fork=O(αn)andE[A] = 1 .
Further, split v= [v, v], then by definition
of eigenvalues and eigenvectors,
/bracketleftbiggWF
FW/bracketrightbigg/bracketleftbiggv
v/bracketrightbigg
=λ/bracketleftbiggD0
0D/bracketrightbigg/bracketleftbiggv
v/bracketrightbigg
.
Reading off the first line of the matrix equation, we
have
∥Fv∥=∥(W−λD)v∥
≤ ∥Wv∥+∥λDv∥.
Notice that assuming Eq. 25 and Eq. 22 occur, we
have that all elements vofvhave|v|< δ, there-
fore∥v∥≤δn, therefore
∥Wv∥≤δnmax∥Wv∥
=O(n)1206Furthermore, if we assume that Eq. (22) and
Eq. (25) occur, then
∥λDv∥=O(n·n·δn)
=O(n).
Thus, using the fact that B≥4γ+ 1,
∥Fv∥=O(n) =O(n).
On the other hand, using a standard epsilon-
net argument, with probability at least 1−
O(exp(−αn)),
inf∥Fw∥≥n.
Now, define the events
E:={vis an eigenvector of A}
E:={vis(O(α), δ)-compressible }
E:={∥Wv∥≫O(n)},
then by the previous discussion, we have
Pr(E|E ∩ E) =O(exp(−αn))
Pr(E|E) =O(exp(−αn)).
Note that to prove the lemma, it suffices to
show that the eigenvector vis not (O(α), δ)-
compressible with high probability, or Pr(E|E)
is small, since that will lead to ρ(v)<
(αn)with high probability and thus a con-
tradiction with high probability. Indeed, we have
Pr(E|E)≤Pr(E∩ E|E) + Pr( E∩ E|E)
≤Pr(E|E ∩ E) + Pr( E|E)
=O(exp(−αn))
for some α>0.
For the incompressible case, we apply the con-
tinuous inverse Offord-Littlewood theorem to dis-
cretize the set of eigenvectors, and prove the fol-
lowing result analogous to the symmetric case in
(Nguyen and Vu, 2011).
Lemma 8. Suppose vis an eigenvector of a sym-
metric Markov random matrix A:=DW
with adjacency matrix Wand the same prop-
erties defined in Theorem 2, and suppose there
exists δ∈[n, n]such that q:=
ρ(v)<(αn), we have with probability
O(exp(−αn)),
n≤ρ(v)≤nρ(v).To prove this result, we need the following useful
lemmas.
Lemma 9. For any eigenvector-eigenvalue pair
(v, λ)andα > 0with|λ|=O(n), sup-
pose n< ρ(v) =: q≤(αn),
then with probability at least 1−O(exp(−αn))
there exists a subset NofR×Rof size
O(nq)such that, there exists
(˜v,˜λ)∈ N with the properties:
1.|v−˜v| ≤δfor1≤j≤n;
2.|λ−˜λ| ≤nδ.
Proof. Split{1,···, n}into sets of length differ-
ing by at most 1,I,···, I,m=/floorleftbig/floorrightbig
+ 1, then
we have the length of each set is greater than or
equal to ⌊αn⌋, and its small ball probability is
ρ(v)≥ρ(v) =q,1≤i≤m.
Therefore, since q≤(αn)andn< q,
there exists a GAP
Q=

/summationdisplayaw:a∈Z,
|a| ≤N,
1≤j≤r


such that
supinf|v−˜v| ≤δ,
with volume
vol(Q)≤O((αn)/q),1≤i≤m,
for all except at most O(αn)indices from some
exceptional set S. Further, for each Q, we can
quantize its generators w,···, wto the closest
multiple of qδ,˜w,···,˜w. This introduces an
additional approximation error of at most
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/summationdisplayaw−/summationdisplaya˜w/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤vol(Q)·qδ≤(αn)/q·qδ
=(αn)δ=O(δ).
Next, for the coefficients from the exceptional set
S, we also round them to the closest multiple of qδ
and let the set of such values be R, which ensures
that
supinf|v−v|=O(δ).1207Therefore, for fixed generators w’s and a given S,
we can construct a finite set of vectors
{˜v: ˜v∈ ∪Q,∀j̸∈Sandv∈R,∀j∈S}
of size at most
/parenleftbigg
msupvol(Q)/parenrightbigg
|R|
≤O/parenleftigg
1
α(αn)
q/parenrightigg
·O((1/qδ))
≤O/parenleftig
nq/parenrightig
O/parenleftbig
n/parenrightbig
=O(nq),
=O(nq),
that approximates vwithin O(δ)for every coeffi-
cients. The third line uses δ > nandα=O(1);
the fourth line assumes ϵ=O(α). Further, if we
allow the generators to be variable and assume S
to be unknown, the quantization mentioned previ-
ously and the crude bound of the number of possi-
bleSby2enlarges the set of vectors by a factor
of
O/parenleftig
(1/qδ)/parenrightig
·O(2) =O(n)·O(2)
=O(n)·O(2) =O(n).
For the eigenvalue, we also have there exists a set
that covers its domain to be within δnwith a set
of size
O/parenleftbiggn
nδ/parenrightbigg
=O(n)≤O(n).
with probability at least 1−O(exp(−αn)). Com-
posing the sets, we find the set Nhas size
O(nq).
Lemma 10. For any eigenvector-eigenvalue pair
(v, λ)of an symmetric Markov random matrix
A=DWwith adjacency matrix Wand
the same properties defined in Theorem 2 and let
(˜v,˜λ)∈ N be the tuple that well approximates it
as defined in Lemma 9, we have
∥A˜v−u∥=O(δn),
where Ais the matrix formed by row indices
fromIand column indices from Jandu:= (˜λ−
A)˜v.Proof. By symmetry, we can let I={1,···, k}
fork=⌊αn⌋. Notice by definition we can split A
as
/bracketleftbiggA G
FA/bracketrightbigg/bracketleftbiggw
v/bracketrightbigg
=λ/bracketleftbiggw
v/bracketrightbigg
,
where v= [w, v], and as a result,
∥F˜v−(˜λ−A)˜v∥
≤∥Fw−(λ−A)v∥+
∥F(˜v−w)∥+∥(˜λ−λ)˜v∥+
∥(λ−A)(˜v−v)∥
=∥F(˜v−w)∥+|(˜λ−λ)˜v∥
+∥(λ−A)(˜v−v)∥
=O(n·δn) +O(nδ)
+O(n·δn) =O(nδ).
Now we are ready to prove Lemma 8.
Proof. LetEbe the event that there exists some
δ∈[n, n]such that
n≤ρ(v)≤nρ(v) =:nq
withq:=ρ(v)andGbe the event that
∥A˜v−u∥=O(δn),
where u:= (˜λ−A)˜vand(˜v, λ)well ap-
proximates (v, λ)as defined in Lemma 10. Let
k:=|I|=O(αn), from Lemma 9, we have
Pr(G) =O(exp(−αn)).
On the other hand, if Eoccurs, define A=
[a, . . . , a],u= [u, . . . , u], then we
have
Pr(G|E)≤/summationdisplay
Pr/bracketleftigg/summationdisplay|aw−u|=O(δn)/bracketrightigg
≤ |N| (ρ(v))≤ |N| (nq)
=O(n),
which is O(exp(−αn))ifαis chosen small
enough. As a result, we have
Pr(E)≤Pr(G|E) + Pr( G) =O(exp(−αn)).1208A.3 Finite-sample learnability of ASR-U:
Unmatched setup
Proof. (Theorem 3) Under the assumptions that the
discriminator is perfect and decomposable and the
GAN objective is MMD with a linear kernel over
the embeddings D(Y) =ˆP, Eq. (8) becomes the
following least squares regression problem
min∥ˆPO−ˆP∥. (26)
LetˆObe the ERM of Eq. (26) and Obe the true
assignment matrix, by definition and triangle in-
equality,
∥ˆPˆO−ˆP∥
≤ ∥ˆPO−ˆP∥
≤ ∥ˆPO−P∥+∥ˆP−P∥.
Apply the triangle inequality again, we have
∥ˆP(ˆO−O)∥
≤ ∥ˆPˆO−ˆP∥+∥ˆPO−ˆP∥
≤2∥ˆPO−P∥+ 2∥ˆP−P∥
Note that if we replace any X→Xand let
the resulting empirical distribution be ˆP,
/vextendsingle/vextendsingle/vextendsingle∥ˆPO−P∥− ∥ˆPO−P∥/vextendsingle/vextendsingle/vextendsingle
≤∥(ˆP−ˆP)O∥≤√
2L
n,
and similarly for ˆPandˆP,
/vextendsingle/vextendsingle/vextendsingle∥ˆP−P∥− ∥ˆP−P∥/vextendsingle/vextendsingle/vextendsingle≤√
2L
n
/vextendsingle/vextendsingle/vextendsingle∥ˆP−P∥− ∥ˆP−P∥/vextendsingle/vextendsingle/vextendsingle≤√
2L
n.
Therefore, we can apply McDiarmid’s inequality
to obtain
Pr/bracketleftigg
∥ˆP−P∥≥/radicalbig
L|X|√
n+ϵ/bracketrightigg
≤e
Pr/bracketleftigg
∥ˆPO−P∥≥/radicalbig
L|Y|√
n+ϵ/bracketrightigg
≤e
Pr/bracketleftigg
∥ˆP−P∥≥/radicalbig
L|Y|√
n+ϵ/bracketrightigg
≤e.
Moreover, let ϵ:=√+ϵ,ϵ:=√+
ϵ,ϵ=√+ϵ, then by a union bound, wehave
Pr/bracketleftig
∥ˆP(ˆO−O)∥≥ϵ+ϵ/bracketrightig
≤
Pr/bracketleftig
∥ˆPˆO−P∥+∥ˆP−P∥≥
ϵ+ϵ
2/bracketrightbigg
≤Pr/bracketleftbigg
∥ˆPˆO−P∥≥ϵ
2/bracketrightbigg
+
Pr/bracketleftbigg
∥ˆP−P∥≥ϵ
2/bracketrightbigg
≤e+e.
Therefore, we have with probability at least 1−
e−e,
ϵ+ϵ≥ ∥ˆP(ˆO−O)∥
≥ ∥P(ˆO−O)∥− ∥ˆP−P∥∥ˆO−O∥
≥(σ(P)− ∥ˆP−P∥)∥ˆO−O∥,
and combined with the bound on ∥ˆP−P∥,
we obtain with probability at least (1−e−
e)(1−e),
∥ˆO−O∥≤ϵ+ϵ
σ(P)−ϵ.
Assume the correct mapping is deterministic, so
thatO∈ {0,1}and each row has only one
nonzero element, then to achieve perfect ASR-U,
we need for any x∈Xandy̸=G(x),
|ˆO−ˆO|>0
⇐= 1− |ˆO−O| − |ˆO−O|>0
⇐= 1−2∥ˆO−O∥>0⇐⇒ ∥ ˆO−O∥<1
2,
which occurs if
σ(P)> ϵ+ 2ϵ+ 2ϵ.
A.4 Training dynamic of ASR-U
To prove Theorem 4, we need the following lemma
on the properties of the gradient of the softmax
function based on (Gao and Pavel, 2017).
Lemma 11. LetH(x)be the Jacobian matrix of
the softmax function σ:R∝⇕⊣√∫⊔≀→Rwithσ(x) =, then we have H(x) = diag( σ(x))−
σ(x)σ(x)andH(x)is positive semi-definite
(PSD) with the null space span{1}.1209Proof. Apply product rule of calculus, we have
H(x) =∂σ(x)
∂x
=δσ(x)−ee
(/summationtexte)
=δσ(x)−σ(x)σ(x),
and therefore H(x) = diag( σ(x))−σ(x)σ(x).
To show that H(x)is PSD, notice that
vH(x)v=vdiag( σ(x))v−(vσ(x))
=E[v]−E[v]
= Var( v)≥0,
where by Jensen’s inequality, achieves “ =” if and
only if v=σv=C,∀ifor some constant C.
Next, we shall establish explicit formula for
NTKs of the discriminator and the generator. For
clarity, we will copy the formula for the discrimi-
nator and the generator used in our analysis:
f(y) = lim1√m/summationdisplayvmax{W,0},
(27)
P(y) =E[O(y|X)]
:=E/bracketleftigg
exp(Ux)/summationtextexp(Ux)/bracketrightigg
.(28)
Lemma 12. For the NTKs of the discriminators
defined by Eq. (27), we have K≡K,1≤
l≤Land1is an eigenvector of K.
Proof. For simplicity, we ignore the dependency
onτfor the terms in the proof. First, by definition,
we have
∂f(y)
∂W= lim1√m/summationdisplayve 1[W≥0],
∂f(y)
∂v= lim=1√mmax{W,0}and therefore
E/bracketleftigg
∂f(y)
∂W∂f(y)
∂W/bracketrightigg
=
lim1
mE/summationdisplayδv 1[W≥0]
=δ1
m/summationdisplayE[ 1[W≥0]]
=1
2δ.
On the other hand,
E/bracketleftigg
∂f(y)
∂v∂f(y)
∂v/bracketrightigg
=1
mE/bracketleftigg/summationdisplaymax{W,0}max{W,0}/bracketrightigg
=/braceleftigg
E/bracketleftbig
max{W,0}/bracketrightbig
ify=y,
E/bracketleftbig
max{W,0}/bracketrightbigotherwise .
Therefore,
K(y, y) =

/parenleftig+E/bracketleftbig
max{W,0}/bracketrightbig/parenrightig
if y=y’,
E/bracketleftbig
max{W,0}/bracketrightbigotherwise .
Notice that the sum of every row in Kis
/parenleftbigg1
2+E/bracketleftbig
max{W,0}/bracketrightbig/parenrightbigg
+
(|Y| −1)E/bracketleftbig
max{W,0}/bracketrightbig,
and thus 1is an eigenvector of K.
Lemma 13. For the generator defined by Eq. (28),
we have
K=
E/bracketleftig
(diag( O)−OO)/bracketrightig
.(29)
Further, the null space of Kisspan{1}.
Proof. For simplicity, we ignore the dependency
ontfor the terms in the proof. By chain rule,
∂O(y)
∂U=∂h(x)
∂U∂O(y)
∂h(x)
=δ(O(y|x)δ−O(y|x)O(y|x))1210As a result,
/summationdisplay∂O(y)
∂U∂O(y)
∂U
=/summationdisplay(O(y)δ−O(y)O(y))
(O(y)δ−O(y)O(y))·
= ((diag( O)−OO))
Take the expectation over Uand put everything in
matrix form, we obtain
K=E/bracketleftig
(diag( O)−OO)/bracketrightig
.
Next we shall study the null space of K. From
Lemma 11, we have H:= diag( O)−OOis
PSD with null space span{1}, and thus
vKv=E/bracketleftbig
∥Hv∥/bracketrightbig
≥0,
with equality achieved if and only if
Hv= 0,∀x∈X⇔v∈span(1).
We are now ready to prove Theorem 4.
Proof. (Theorem 4) When the objective is MMD,
the discriminator can be decomposed as
a(y) =f(y) =/summationdisplayf(y),
we have
L(f) =/summationdisplayE[f(Y)]−E[f(Y)],
(30)
and the discriminator dynamic PDE Eq. (18) be-
comes:
∂f=K(P−PO).
Without much loss of generality, suppose we initial-
izef(y)≡0and stop training the discriminator
afterτsteps. The solution for the discriminator
PDE is then simply
f=τK(P−PO). (31)Plug this expression into the generator loss and
apply Lemma 12, we obtain
C(g) :=τ/summationdisplay∥P−PO∥
=τ∥P−PO∥,
where ∥A∥=/radicalbig
Tr(AKA)is the kernelized
norm of Aby kernel K.
Further, plug Eq. (31) into the generator PDE
Eq. (19), we obtain
∂O=K/summationdisplayP(x)K(P−PO)
=KK(P−PO)˜P,
where ˜Pis the x-th column of P. Next, notice
that
∂C
∂O
=2τK(y,·)(PO−P)˜P
=⇒∂C
∂O=P(PO−P)K.
Then apply the chain rule,
∂C= Tr/parenleftigg
∂C
∂O∂O
∂t/parenrightigg
=/summationdisplayTr/parenleftigg
∂C
∂O∂O
∂t/parenrightigg
=
−τ/summationdisplay∥˜P(P−PO)∥.
Now, apply Lemma 12, we have
∂f1
=(P−PO)K1
=λ(P−PO)1= 1−1 = 0
=⇒1⊥K(P−PO),
where λis the eigenvalue of Kassociated with
1, and thus
K(P−PO)˜P⊥1.
As a result, using Lemma 13, we conclude that the
kernelized residual vector ∂fis always perpen-
dicular to the null space of the stepwise generator1211NTK Kfor all 1≤l≤L, x∈X, and thus
∥K(P−PO)˜P∥
≥λ∥K(P−PO)˜P∥
≥λλ∥P−PO∥,
where
λ≥minλ(K)>0,
λ≥λ(K)>0.
Summing over x, we obtain
∂C≤ −τλλ∥P(P−PO)∥.
Under the assumption that PO=Phas at least
one solution, we have P−POis in the range
space of P, which implies
∥P(P−PO)∥≥
λ∥P−PO∥,
for some λ>0. Put together the results, we can
bound the convergence rate of the generator loss
by
∂C≤ −τλλλC
=⇒C≤ Ce− − − → 0,
which implies that limPO=P.
B Reproducibility checklist
Synthetic language creation To create a syn-
thetic HMM language, we need to specify the ini-
tial probability vector π, the transition probability
matrix T, the generator matrix Oand the maximal
length of the utterances L.
Initial probability : we create πby first uniformly
randomly sampling each coefficient between [0,1]
and then normalizing the resulting vector by its
sum.
Transition probability : for the asymptotic set-
ting, for all three languages, we control the num-
ber of eigenvalues mof its transition matrix using
a disjoint union of identical sub-graphs with m
eigenvalues, with the remainder of the nodes being
self-loops. The parameters and the procedure used
to determine them are as follows:•Circulant graph : only undirected cycles or
equivalently, circulant graph with the action
set{−1,1}, are used. Since the distinct
eigenvalues of an undirected n-cycle Care
−cos/parenleftbig/parenrightbig
, k= 0,···,⌊⌋+ 1, we can
create a Markov graph with |X|nodes and
n±1eigenvalues by a disjoint union of
⌊⌋Cgraphs. In our phase transi-
tion experiment, we fix N= 2 and vary
10≤ |X| ≤14and2≤n≤20;
•De Bruijn graph : an undirected de Bruijn
graph DB(k, m)is a graph with knodes
such that node iconnects to any node j
whose k-ary numerals v(i)andv(j)satis-
fiesv(i) =v(j). Clearly, mis the
in/out-degree of the graph. The eigenvalues
ofDB(k, m)are known to be cos/parenleftig/parenrightig
,0≤
i < j ≤m+ 1 (Delorme and Tillich,
1998). Therefore, we can create a Markov
graph with |X|nodes and at most n,n≤
(⌊log|X|⌋+ 1)/2distinct eigenvalues by
a disjoint union ofDB(k,√
2n−1)
graphs. For the phase transition experiment,
we set the in/out-degree of the de Bruijn sub-
graphs to be 2and the N-gram size N= 3,
and we vary 8≤ |X| ≤11and2≤n≤32
with a step size of 2for the latter.
•Hypercube : an n-cube Qis a graph with
2nodes such that node iconnects to any
nodejwith Hamming distance between their
binary numerals d(b(i), b(j)) = 1 . The
eigenvalues of the adjacency matrix of Q
is1−, k= 0,···, n. Therefore, we can
create a Markov graph with |X|nodes and
n≤ ⌊Nlog|X|⌋eigenvalues by a disjoint
union of ⌊⌋n-cubes. For the phase tran-
sition experiment, we fix N= 4, and vary
5≤ |X| ≤8and2≤n≤9.
In the finite-sample setting, we create transition
matrices for phase transition experiments using two
different setups:
•For the circulant graph, we vary its action set
to be{1,···, d}, where dtakes values from
2to81with a step size of 8;
•For the other two graphs, we linearly interpo-
late between the underlying graph Tand its
Hamiltonian cycle Tas
T= (1−w)T+wT, (32)1212with a weight w∈[0,1]. In particular, for
the de Bruijn graph, the weight for the cycle
wtakes 10 different values equally spaced
between [0,1]; for the n-cube, the weight w
takes 10 different values equally spaced be-
tween [0.98,1].
Generator matrix O: set by assuming |X|=|Y|
and randomly permuting the rows of the |X| × |X|
identity matrix.
Sampling : in the asymptotic case, no sampling is
needed and we simply set maximal length L= 20
for cycle graph and 10 for the other two graphs.
For the finite-sample case, the synthetic speech and
text datasets are created independently by sampling
from the same HMM twice. For all three graphs,
we sample n=n= 2560 utterances for both
text and speech with L= 40 for the de Bruijn
graph and L= 80 for the other two graphs.
Model architecture We use a one-layer linear
generator with |X|input nodes and |Y|output
nodes, with no bias. Next, for all experiments
except the experiment on different generator av-
eraging strategies, we use a one-layer CNN with
|Y|input channels, 1 output channel and a 1×L
kernel with no bias. For the experiment on different
averaging strategies, we use instead a sequence of
2-layer MLPs with 128 hidden nodes and ReLU
activation function, one at each time step, as the
discriminators. For all experiments, we disable
the logits for special tokens and silences during
training and testing.
Training setting SGD with a learning rate of 1.0
is used to train the discriminator, while Adam with
a learning rate of 0.005 is used to train the gener-
ator. The dataset is used as a single batch for all
experiments, though we do not observe any signifi-
cant drop in performance using smaller batch sizes.
No weight decays or dropouts is used. Further, we
alternatively train the generator and discriminator
1 epoch each, and reset the discriminator weight
to 0 for the linear case and to random Gaussian
weights using Xavier initialization in the nonlinear
case. All experiments are conducted on a single
12GB NVIDIA GeForce GTX 1080Ti GPU.1213ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 7
/squareA2. Did you discuss any potential risks of your work?
It is theoretical paper and has no signiﬁcant risks per se as far as the authors concern
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 1,2,3,4
/squareB1. Did you cite the creators of artifacts you used?
Section 5, Appendix B
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
It uses open source and publicly available toolkit/data and the license and terms are listed in their
websites
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
We believe we use all the artifacts with their intended purposes
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Our data are synthetic and do not contain personal information
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
The documentations are available on the ofﬁcial websites of the artifacts
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4, Appendix B
C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix B1214/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 4, Appendix B
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Section 4, Appendix B
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.1215