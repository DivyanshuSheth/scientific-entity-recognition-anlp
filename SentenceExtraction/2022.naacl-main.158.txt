
Chenhao Tan
Department of Computer Science & Harris School of Public Policy
The University of Chicago
chenhao@uchicago.edu
Abstract
A growing effort in NLP aims to build datasets
of human explanations. However, it remains
unclear whether these datasets serve their in-
tended goals. This problem is exacerbated by
the fact that the term explanation isoverloaded
and refers to a broad range of notions with dif-
ferent properties and ramiﬁcations. Our goal
is to provide an overview of the diversity of
explanations, discuss human limitations in pro-
viding explanations, and ultimately provide
implications for collecting and using human
explanations in NLP.
Inspired by prior work in psychology and cog-
nitive sciences, we group existing human ex-
planations in NLP into three categories: prox-
imal mechanism, evidence, and procedure.
These three types differ in nature and have im-
plications for the resultant explanations. For
instance, procedure is not considered explana-
tion in psychology and connects with a rich
body of work on learning from instructions.
The diversity of explanations is further evi-
denced by proxy questions that are needed
for annotators to interpret and answer “why is
[input] assigned [label]”. Finally, giving ex-
planations may require different, often deeper,
understandings than predictions, which casts
doubt on whether humans can provide valid ex-
planations in some tasks.
1 Introduction
With the growing interest in explainable NLP sys-
tems, the NLP community have become increas-
ingly interested in building datasets of human ex-
planations. These human explanations can ideally
capture human reasoning of why a (correct) label is
chosen. If this is indeed the case, they are hypoth-
esized to aid models with additional supervision,
train models that explain their own predictions, and
evaluate machine-generated explanations (Wiegr-
effe and Marasovi ´c, 2021). In fact, DeYoung et al.
(2020) already developed a leaderboard, where theimplicit assumption is that humans can provide
valid explanations and these explanations can in
turn be uniformly considered as groundtruths.
However, are these assumptions satisﬁed and
can human explanations serve these goals? In this
work, we aim to introduce prior relevant literature
in psychology to the NLP community and argue
against abusing the term explanations and prema-
turely assuming that human explanations provide
valid reasoning for inferring a label.
First, we point out the rich diversity in what the
NLP community refer to as explanations and how
researchers collect them. The term “explanation” is
overloaded in the NLP and AI community: it often
refers to many distinct concepts and outcomes.
For example, procedural instructions are different
from explanations that attempt to convey proximal
causal mechanisms. The diversity of explanations
is further evidenced by the variety of proxy ques-
tions that researchers ask to collect explanations,
e.g., “highlight the important words that would tell
someone to see the movie” vs. “highlight ALL
words that reﬂect a sentiment”. These proxy ques-
tions are necessary because the question of “why is
[input] assigned [label]” is too open-ended. It fol-
lows that these “human explanations” are supposed
to answer different questions in the ﬁrst place and
may not all be used for the same goals, e.g., serving
as groundtruth labels.
In addition to the diversity, we highlight two in-
sights from psychology on whether humans can
provide valid explanations: 1) prediction does not
entail explanation (Wilson and Keil, 1998), i.e., al-
though humans may be able to provide valid labels,
they may not be able to provide explanations that
capture the reasoning process needed or used to
infer a label; 2) everyday explanations are neces-
sarily incomplete (Keil, 2006; Lombrozo, 2006),
because they seldom capture the complete deduc-2173tive processes from a set of axioms to a statement.
In summary, notall explanations are equal and
humans may notalways be able to provide valid
explanations. We encourage the NLP community
to embrace the complex and intriguing phenomena
behind human explanations instead of simply view-
ing explanations as another set of uniform labels.
A better understanding and characterization of hu-
man explanations will inform how to collect and
use human explanations in NLP.
2 Types of Human Explanations in NLP
To understand whether datasets of human expla-
nations can serve their intended goals, we ﬁrst
connect current human explanations in NLP with
existing psychology literature to examine the use
of the term “explanation” in NLP. We adapt the
categorization in Lombrozo (2006) and group hu-
man explanations in NLP into the following three
categories based on the conveyed information:
•Proximal mechanisms . This type of expla-
nation attempts to provide the mechanism be-
hind the predicted label, i.e., how to infer the
label from the text, and match efﬁcient cause
in Lombrozo (2006). We created E1 in Table 1
to illustrate this type of explanation. Note that
E1 does not provide the complete mechanism.
For instance, it does not deﬁne “year” or “tem-
poral modiﬁer”, or make clear that “1997” is
a “year”. Neither does it cover the axioms
of logic. This is a common property of hu-
man explanations: they are known to cover
partial/proximal mechanisms rather than the
complete deduction from natural laws and em-
pirical conditions (Hempel and Oppenheim,
1948).
•Evidence . This type of explanation includes
the relevant tokens in the input (e.g., E2 in
Table 1) and directly maps to highlights in
Wiegreffe and Marasovi ´c (2021). However,
it does not map to any existing deﬁnitions of
explanations in the psychology literature since
the evidence does not provide any information
onhow evidence leads to the label. In other
words, evidence alone does not explain .
•Procedure . Unlike proximal mechanisms,
this type of explanation provides step-by-step
rules or procedures that one can directly fol-
low, e.g., E3 in Table 1. They are more ex-plicit and unambiguous than proximal mecha-
nisms. In fact, one can write a rule based on
E3 to ﬁnd marriage relation, but one cannot
easily do that with E1. Furthermore, the proce-
dures are grounded to the input, so it is related
toformal cause , “the form or properties that
make something what it is” (Lombrozo, 2006),
which is deﬁnitional and does not convey the
underlying mechanisms. Procedural instruc-
tions are only possible for some tasks, while
proximal mechanisms are the most common
form of everyday explanations.
These three categories empirically capture all
the explanations discussed in NLP literature. Lom-
brozo (2006) also discuss two other categories, ﬁ-
nal causes (the goal) and material causes (the con-
stituting substance). For instance, a ﬁnal cause to
“why [input] is assigned [label]” can be that “this
label is provided to train a classiﬁer”. These two
categories have been less relevant for NLP.
Implications. This categorization allows us to
think about what kind of explanations are desired
for NLP systems and help clarify how to use them
appropriately. First, proximal mechanisms are best
aligned with human intuitions of explanations, es-
pecially in terms of hinting at causal mechanisms.
However, they can be difﬁcult to collect for NLP
tasks. For example, Table 2 shows example expla-
nations in E-SNLI that fail to convey any proximal
mechanisms: they either repeat the hypothesis or
express invalid mechanisms (“the bike competition”
does not entail “bikes on a stone road”). See further
discussions on the challenges in collecting expla-
nations in §4. Furthermore, they may be difﬁcult
to use for supervising or evaluating a model.
Second, evidence by deﬁnition provides little
information about the mechanisms behind a label,
but it can be potentially useful as additional super-
vision or groundtruths. We will further elaborate
on the nature of evidence in different tasks in §3.
However, it may be useful to the community to use
clear terminology (e.g., evidence or rationale (Lei
et al., 2020; Carton et al., 2020)) to avoid lumping
everything into “explanation”.
Finally, procedures are essentially instructions,
and Keil (2006) explicitly distinguishes explana-
tions from simple procedural knowledge: “Know-
ing how to operate an automated teller machine
or make an international phone call might not en-
tail having any understanding of how either system2174
works”. Another reason to clarify the procedure
category is that it would be useful to engage with
a rich body of work on learning from instructions
when human explanations are procedural (Gold-
wasser and Roth, 2014; Matuszek et al., 2013).
We would like to emphasize that procedures or
instructions are powerful and can potentially bene-
ﬁt many NLP problems (e.g., relation extraction).
At the same time, it is useful to point out that pro-
cedures are different from proximal mechanisms.
3 Proxy Questions Used to Collect
Human Explanations
Although explanations are supposed to answer
“why is [input] assigned [label]” (Wiegreffe and
Marasovi ´c, 2021), this literal form is too open-
ended and may not induce “useful” human expla-
nations. As a result, proxy questions are often nec-
essary for collecting human explanations. These
proxy questions further demonstrate the diversity
of human explanations beyond the types of expla-
nation. Here we discuss these proxy questions for
collecting evidence. See the appendix for discus-
sions on proximal mechanisms and procedures.
To collect evidence (highlights), researchers
adopt diverse questions for relatively simple single-
text classiﬁcation tasks (see Table 3). Considerthe seemingly straightforward case of sentiment
analysis, “why is the sentiment of a review posi-
tive/negative”. A review can present both positive
and negative sentiments (Aithal and Tan, 2021), so
the label often comes from one sentiment outweigh-
ing the other. However, in practice, researchers of-
ten ask annotators to identify only words support-
ing the label. Critical wording differences remain
in their questions: Zaidan et al. (2007) ask for the
most important words and phrases that would tell
someone to see the movie , while Sen et al. (2020)
requires allwords reﬂecting the sentiment . Two
key differences arise: 1) “the most important” vs.
“all”; 2) “telling someone to see the movie” vs. “re-
ﬂecting the sentiment”.
In contrast, personal attack detection poses a
task where the negative class (“no personal attack”)
by deﬁnition points to the lack of evidence in the
text. It follows that the questions that researchers
can ask almost exclusively apply to the positive
class (i.e., “highlight sections of comments that
they considered to constitute personal attacks”).
In comparison, researchers approach evidence
more uniformly for document-query classiﬁcation
tasks. They generally use similar proxy questions
(e.g., Thorne et al. (2018) and Hanselowski et al.
(2019) ask almost the same questions) and ask peo-
ple to select sentences instead of words. That said,
intriguing differences still exist: 1) Lehman et al.
(2019) simply ask annotators to provide accom-
panying rationales; 2) Thorne et al. (2018) aim
for “strong” reasons, which likely induces differ-
ent interpretations among annotators; 3) Khashabi
et al. (2018) collect questions, answer, and sentence
indices at the same time, among which sentence
indices can be used to ﬁnd the corresponding sen-
tences as evidence. It remains unclear how these2175
differences in annotation processes and question
phrasings affect the collected human explanations.
Implications. Our observation on proxy ques-
tions aligns with dataset-speciﬁc designs discussed
in Wiegreffe and Marasovi ´c (2021). We emphasize
that these different forms of questions entail differ-
ent properties of the collected human explanations,
as evidenced by Carton et al. (2020). For exam-
ple, the lack of evidence in the negative class in
personal attack classiﬁcation likely requires special
strategies in using human explanations to train a
model and evaluate machine rationales. Sentence-
level and token-level annotations also lead to sub-
stantially different outcomes, at least in the forms
of explanations. We believe that it is important
for the NLP community to investigate the effect of
proxy questions and use the collected explanations
with care, rather than lumping all datasets under
the umbrella of explanations.
We also recommend all researchers to provide
detailed annotation guidelines used to collect hu-
man explanations. As the area of collecting human
explanation is nascent, the goal is not to promote
consistent and uniform annotation guidelines but
to encourage the community to pay attention to the
different underlying questions and characterize the
resultant diverse properties of human explanations.
4 Can Humans Provide Explanations?
In order for human explanations to serve as ad-
ditional supervision in training models and eval-
uate machine-generated explanations, human ex-
planations need to provide valid mechanisms fora correct label. Finally, we discuss challenges for
humans to provide explanations of such qualities.
Conceptual framework. We situate our discus-
sion in the psychological framework provided by
Wilson and Keil (1998) to highlight what may be
required to explain. Wilson and Keil (1998) exam-
ines where explanation falls in three central notions:
prediction, understanding, and theories. They ar-
gue that these three notions “form a progression of
increasing sophistication and depth with explana-
tions falling between understanding and theories”.
For instance, we may be able to predict that a car
will start when we turn the ignition switch, but few
of us are able to explain in detail why this is so. In
contrast, if a person is able to explain in detail why
a car starts when you turn on the ignition switch,
they can likely predict what will happen if various
parts of the engine are damaged or removed.
These three central notions are also essential in
machine learning. Traditional label annotation is
concerned with prediction, however, being able to
predict does not entail being able to explain.
Emulation vs. discovery. Next, we gradually
unfold the practical challenges in collecting valid
explanations from humans. The ﬁrst challenge lies
in whether humans can predict, i.e., assign the cor-
rect label. We highlight two types of tasks for
AI: emulation vs. discovery (Lai et al., 2020). In
emulation tasks, models are trained to emulate hu-
man intelligence and labels are often crowdsourced.
Labels, however, can also derive from external (so-
cial/biological) processes, e.g., the popularity of a
tweet and the effect of a medical treatment. Mod-2176els can thus discover patterns that humans may
not recognize in these discovery tasks. While most
NLP tasks such as NLI and QA are emulation tasks,
many NLP problems, especially when concerned
with social interaction, are discovery tasks, rang-
ing from identifying memorable movie quotes to
predicting the popularity of messages (Danescu-
Niculescu-Mizil et al., 2012; Tan et al., 2014).
Aligning with our discussion on explanation and
prediction, most datasets of human explanations in
NLP assume that humans are able to predict and are
on emulation tasks. However, we note exceptions
such as explanations of actions in gaming (Ehsan
et al., 2019), where humans may often choose sub-
optimal actions (labels).
Cognitive challenges in providing valid explana-
tions. Even conditioned on that humans can pre-
dict the label, humans may not be able to provide
valid explanations for at least two reasons. First, as
Wilson and Keil (1998) suggests, explanation re-
quires more depth than prediction. For instance, we
may possess some notions of common sense (e.g.,
one should not slap a stranger), but it is unclear
whether we can explain common sense in detail
(e.g., why one should not slap a stranger through
theory of morality), similar to the car ignition ex-
ample. One may argue that theory of morality may
not be what NLP researchers seek, but it is critical
to consider the desiderata of human explanations,
with the limits in mind.
Second, explanation often requires people to re-
port their subjective mental processes, i.e., how our
minds arrive at a particular judgement, rather than
following objective consensual guidelines such as
annotating logical entailment. However, classic
work by Nisbett and Wilson (1977) suggests that
our verbal reports on our mental processes can be
highly inaccurate. For instance, in admission deci-
sions, legitimate information can be used to justify
preferences based on illegitimate factors such as
race (Norton et al., 2006). Many studies on im-
plicit bias also reinforces that we are not aware of
our biases and thus cannot include them (i.e., the
actual reasoning in our mind) in our explanations
(Greenwald et al., 1998).
Explanations are necessarily incomplete. Fi-
nally, there are indeed cases where we believe that
humans can provide valid mechanisms. For in-
stance, some question answering tasks boil down to
logical inference from evidence to query. In thesecases, NLP researchers need to recognize that hu-
man explanations are necessarily incomplete: peo-
ple do not start from a set of axioms and present all
the deductive steps (Keil, 2006; Lombrozo, 2006).
Therefore, even for simple tasks such as natural lan-
guage inference, we may simply give explanations
such as repeating the hypothesis without presenting
any axiom or deduction required to infer the label.
Implications. We cannot assume that humans
are capable of providing explanations that con-
tain valuable proximal mechanisms. The very fact
that humans can still provide explanations for in-
correct labels and tasks where they do not per-
form well suggests that one should be skeptical
about whether human explanations can be used to
train models as additional supervision or evaluate
machine-generated explanations as groundtruths.
Note that incomplete explanations can still be
very useful for NLP. We believe that recognizing
and characterizing this incompleteness (e.g., which
proximal mechanism is more salient to humans) is
critical for understanding and leveraging human
explanations for the intended goals in NLP. To
summarize, we argue that human explanations are
necessarily incomplete and it is important to under-
stand and characterize this incompleteness, which
can inform how we can leverage it for the intended
goals in NLP.
5 Conclusion
Explanations represent a fascinating phenomenon
and are actively studied in psychology, cognitive
science, and other social sciences. While the grow-
ing interest in explanations from the NLP com-
munity is exciting, we encourage the community
to view this as an opportunity to understand how
humans approach explanations and contribute to
understanding and exploring the explanation pro-
cesses. This will in turn inform how to collect
and use human explanations in NLP. A modest
proposal is that it is useful to examine and charac-
terize human explanations before assuming that all
explanations are equal and chasing a leaderboard.
Acknowledgments
We thank anonymous reviewers for their feedback,
and members of the Chicago Human+AI Lab for
their insightful suggestions. This work is sup-
ported in part by research awards from Amazon,
IBM, Salesforce, and NSF IIS-2040989, 2125116,
2126602.2177References21782179A Proxy Questions for Proximal
Mechanisms and Procedure
Proximal mechanisms. In collecting proximal
mechanisms, studies are more likely to ask explic-
itly versions of “why is [input] assigned [label]”,
compared to the case of evidence. However, they
often need to provide structured guidelines. For
example, Camburu et al. (2018) and Rajani et al.
(2019) discussed the need to enforce word over-
lap as a way to improve the quality of human ra-
tionales. The speciﬁc requirements are quite dif-
ferent (see Table 6 and Table 7). There are also
speciﬁc formulations of explanations, e.g., “What
aspect/stereotype/characteristic of this group (often
un-fairly assumed) is referenced or implied by this
post?” in Sap et al. (2020). Finally, it is common
that we cannot infer the exact questions asked (8/18
papers that collect explanations in free text).
Procedures. We cannot identify the exact ques-
tions in three of ﬁve papers for explicitly step-by-
step procedures, which reﬂects the importance of
reporting detailed annotation guidelines. As re-
searchers collect step-by-step guidelines, Ye et al.
(2020) and Geva et al. (2021) adopt very different
decomposition for their problems (see Table 12).
B Detailed Proxy Questions
Table 4-12 show the instructions we ﬁnd in prior
work that detail the proxy questions. Camburu et al.
(2018) and Rajani et al. (2019) collect both evi-
dence and proximal mechanism. We include them
in the tables for proximal mechanisms. Also, for
question answering tasks, the difference between
procedure and proximal mechanism can be subtle.
We consider the collected explanations procedure
if they aim to explicitly provide step-by-step guides
directly grounded in the input.2180Reference Task Questions and guidelines
Zaidan et al. (2007) sentiment anal-
ysisEach review was intended to give either a positive or a
negative overall recommendation. You will be asked to
justify why a review is positive or negative. To justify
why a review is positive, highlight the most important
words and phrases that would tell someone to see the
movie. To justify why a review is negative, highlight
words and phrases that would tell someone not to see
the movie . These words and phrases are called rationales.
You can highlight the rationales as you notice them, which
should result in several rationales per review. Do your best
to mark enough rationales to provide convincing support
for the class of interest.
You do not need to go out of your way to mark everything.
You are probably doing too much work if you ﬁnd yourself
go- ing back to a paragraph to look for even more ratio-
nales in it. Furthermore, it is perfectly acceptable to skim
through sections that you feel would not contain many
rationales, such as a re- viewer’s plot summary, even if
that might cause you to miss a rationale here and there.
Sen et al. (2020) sentiment anal-
ysis1. Read the review and decide the sentiment of this review
(positive or negative). Mark your selection.
2. Highlight ALL words that reﬂect this sentiment. Click
on a word to highlight it. Click again to undo.
3. If multiple words refect this sentiment, please highlight
them all.
Carton et al. (2018) Personal attack
detection40 undergraduate students used Brat (Stenetorp et al.,
2012) to highlight sections of comments that they con-
sidered to constitute personal attacks.
Lehman et al. (2019) Question
answeringPrompt Generation: Question answering & Prompt cre-
ators were instructed to identify a snippet, in a given full-
text article, that reports a relationship between an inter-
vention, comparator, and outcome. Generators were also
asked to provide answers and accompanying rationales to
the prompts that they provided; such supporting evidence
is important for this task and domain.
The annotator was also asked to mark a snippet of text
supporting their response. Annotators also had the option
to mark prompts as invalid, e.g., if the prompt did not seem
answerable on the basis of the article.
Thorne et al. (2018) fact veriﬁcation
(QA)If I was given only the selected sentences, do I have strong
reason to believe the claim is true (supported) or stronger
reason to believe the claim is false (refuted). If I’m not
certain, what additional information (dictionary) do I have
to add to reach this conclusion.
In the annotation interface, all sentences from the introduc-
tory section of the page for the main entity of the claim and
of every linked entity in those sentences were provided as
a default source of evidence (left-hand side in Fig. 2).
We did not set a hard time limit for the task, but the anno-
tators were advised not to spend more than 2-3 minutes
per claim.2181Reference Task Questions and guidelines
Khashabi et al. (2018) Question
answeringWe show each paragraph to 5 turkers and ask them to
write 3-5 questions such that: (1) the question is answer-
able from the pas- sage, and (2) only those questions are
allowed whose answer cannot be determined from a sin-
gle sentence. We clarify this point by providing example
paragraphs and questions. In order to encourage turkers
to write meaningful questions that ﬁt our criteria, we addi-
tionally ask them for a correct answer and for the sentence
indices required to answer the question.
Yang et al. (2018) Question
answeringWorkers provide the supporting facts
(cannot infer the exact question )
Hanselowski et al. (2019) Fact veriﬁca-
tionStance annotation. We asked crowd workers on Amazon
Mechanical Turk to annotate whether an ETS (evidence
text snippets) agrees with the claim, refutes it, or has no
stance towards the claim. An ETS was only con- sidered
to express a stance if it explicitly referred to the claim and
either expressed support for it or refuted it. In all other
cases, the ETS was consid- ered as having no stance.
FGE annotation. We ﬁltered out ETSs with no stance, as
they do not contain supporting or refut- ing FGE. If an ETS
was annotated as supporting the claim, the crowd workers
selected only sup- porting sentences; if the ETS was anno-
tated as refuting the claim, only refuting sentences were
selected.
Kwiatkowski et al. (2019) Question
answeringLong Answer Identiﬁcation: For good ques- tions only,
annotators select the earliest HTML bounding box con-
taining enough information for a reader to completely
infer the answer to the ques- tion. Bounding boxes can
be paragraphs, tables, list items, or whole lists. Alterna-
tively, annotators mark “no answer” if the page does not
answer the question, or if the information is present but
not contained in a single one of the allowed elements.
Wadden et al. (2020) Fact veriﬁca-
tionAn evidence set is a collection of sentences from the ab-
stract that provide support or contradiction for the given
claim. To decide whether a collection of sentences is an
evidence set, ask yourself, “If I were shown only these
sentences, could I reasonably conclude that the claim
is true (or false)”? 1) Evidence sets should be mini-
mal. If you can remove a sentence from the evidence
set and the remaining sentences are sufﬁcient for sup-
port / contradiction, you should remove it. 2) There
may be multiple evidence sets in a given abstract. See
more at
Kutlu et al. (2020) relevance
assessmentPlease copy and paste text 2-3 sentences from the webpage
which you believe support your decision. For instance, if
you selected Highly Relevant, paste some text that you feel
clearly satisﬁes the given query. If you selected Deﬁnitely
not relevant, copy and paste some text that shows that the
page has nothing to do with the query. If there is no text on
the page or images led you to your decision, please type
“The text did not help me with my decision”.2182Reference Task Questions and guidelines
Jansen et al. (2016) science QA For each question, we create gold explanations that describe
the inference needed to arrive at the correct answer. Our
goal is to derive an explanation corpus that is grounded in
grade-appropriate resources. Accordingly, we use two ele-
mentary study guides, a science dictionary for elementary
students, and the Simple English Wiktionary as relevant cor-
pora. For each question, we retrieve relevant sentences from
these corpora and use them directly, or use small variations
when necessary. If relevant sentences were not located, then
these were constructed using simple, straightforward, and
grade-level appropriate language. Approximately 18% of
questions required specialized domain knowledge (e.g. spa-
tial, mathematical, or other abstract forms) that did not easily
lend itself to simple verbal description, which we removed
from consideration. This resulted in a total of 363 gold expla-
nations.
Rajani et al. (2019) Question
answeringTurkers are prompted with the following question: “Why
is the predicted output the most appropriate answer?” An-
notators were in- structed to highlight relevant words in the
question that justiﬁes the ground-truth answer choice and to
provide a brief open-ended explanation based on the high-
lighted justiﬁcation could serve as the commonsense reason-
ing behind the question.
Annotators cannot move forward if they do not highlight any
relevant words in the question or if the length of explanations
is less than 4 words. We also check that the explanation is
not a sub- string of the question or the answer choices with-
out any other extra words. We collect these ex- planations
from only one annotator per example, so we also perform
some post-collection checks to catch examples that are not
caught by our previ- ous ﬁlters. We ﬁlter out explanations
that could be classiﬁed as a template. For example, expla-
nations of the form “ <answer >is the only option that is
[correct—obvious]” are deleted and then reannotated.
Sap et al. (2020) social bias What aspect/stereotype/characteristic of this group (often un-
fairly assumed) is referenced or implied by this post? — Use
simple phrases and do not copy paste from the post.2183Reference Task Questions and guidelines
Camburu et al. (2018) Natual lan-
guage inferenceWe encouraged the annotators to focus on the non-obvious
elements that induce the given relation, and not on the
parts of the premise that are repeated identically in the
hypothesis. For entailment, we required justiﬁcations of
all the parts of the hypothesis that do not appear in the
premise. For neutral and contradictory pairs, while we
encouraged stating all the elements that contribute to the
relation, we consider an explanation correct, if at least one
element is stated. Finally, we asked the annotators to pro-
vide self-contained explanations, as opposed to sentences
that would make sense only after reading the premise and
hypothesis.
We did in-browser checks to ensure that each explanation
contained at least three tokens and that it was not a copy
of the premise or hypothesis. We further guided the an-
notators to provide adequate answers by asking them to
proceed in two steps. First, we require them to highlight
words from the premise and/or hypothesis that they con-
sider essential for the given relation. Secondly, annotators
had to formulate the explanation using the words that they
highlighted. However, using exact spelling might push
annotators to formulate grammatically incorrect sentences,
therefore we only required half of the highlighted words to
be used with the same spelling. For entailment pairs, we
required at least one word in the premise to be highlighted.
For contradiction pairs, we required highlighting at least
one word in both the premise and the hypothesis. For
neutral pairs, we only allowed highlighting words in the
hypothesis, in order to strongly emphasize the asymme-
try in this relation and to prevent workers from confusing
the premise with the hypothesis. We believe these label-
speciﬁc constraints helped in putting the annotator into the
correct mindset, and additionally gave us a means to ﬁlter
incorrect explanations. Finally, we also checked that the
annotators used other words that were not highlighted, as
we believe a correct explanation would need to articulate
a link between the keywords.
Do et al. (2020) visual NLI similar to Camburu et al. (2018)
Kim et al. (2018) self-driving
carsWe provide a driving video and ask a human annotator
in Amazon Mechanical Turk to imagine herself being a
driving instructor. Note that we speciﬁcally select human
annotators who are familiar with US driving rules. The an-
notator has to describe what the driver is doing (especially
when the behavior changes) and why, from a point of view
of a driving instructor. Each described action has to be
accompanied with a start and end time-stamp. The annota-
tor may stop the video, forward and backward through it
while searching for the activities that are interesting and
justiﬁable.2184Reference Task Questions and guidelines
Zhang et al. (2020) coreference res-
olutionGiven a context and a pronoun reference relationship, write
how you would decide the selected candidate is more likely
to be referred than the other candidate using natural lan-
guage. Don’t try to be overly formal, simply write what
you think. In the ﬁrst phase, we ask annotators to pro-
vide reasons for all WSC questions. Detailed instructions
are provided such that annotators can fully understand
the task1. As each question may have multiple plausible
reasons, for each question, we invite ﬁve annotators to pro-
vide reasons based on their own judgments. A screenshot
of the survey is shown in Figure 3. As a result, we collect
1,365 reasons. As the quality of some given reasons might
not be satisfying, we introduce the second round annota-
tion to evaluate the quality of collected reasons. In the
second phase, for each reason, we invite ﬁve annotators to
verify whether they think the reason is reasonable or not2.
If at least four annotators think the reason is plausible, we
will accept that reason. As a result, we identify 992 valid
reasons.
Lei et al. (2020) future event pre-
dictionwe also require them to provide a rationale as to why it is
more or less likely
Da et al. (2020) harm of manip-
ulated imagesFor each question we require annotators to provide both
an answer to the question and a rationale (e.g. the physical
change in the image edit that alludes to their answer). This
is critical, as the rationales prevent models from guessing
a response such as “would be harmful” without providing
the proper reasoning for their response. We ask annotators
to explicitly separate the rationale from the response by
using the word “because” or “since” (however, we ﬁnd that
the vast majority of annotators naturally do this, without
being explicitly prompted).
Ehsan et al. (2019) gaming “Please explain your action”. During this time, the player’s
microphone automatically turns on and the player is asked
to explain their most recent action while a speech-to-text
library automatically transcribes the explanation real-time.2185Reference Task Questions and guidelines
Ling et al. (2017) algebraic prob-
lemscannot infer the exact question
Alhindi et al. (2018) fact veriﬁcation we cannot infer the exact question automatically extract-
ing for each claim the justiﬁcation that humans have pro-
vided in the fact-checking article associated with the claim.
Most of the articles end with a summary that has a headline
“our ruling” or “summing up”
Kotonya and Toni (2020) fact veriﬁcation automatically scraped from the website,
we cannot infer the exact question
Wang et al. (2020) sentiment anal-
ysis & relation
extractionTurkers are prompted with a list of selected predicates
(see Appendix) and several examples of NL explanations.
We cannot infer the exact question
Brahman et al. (2020) natural lan-
guage inferenceautomatically generated.
We cannot infer the exact question
Li et al. (2018) visual QA automatically generated,
We cannot infer the exact question
Park et al. (2018) visual QA During data annotation, we ask the annotators to complete
the sentence “I can tell the person is doing (action) be-
cause..” where the action is the ground truth activity label.
However, We cannot infer the exact question in VQA-X.
Rajani et al. (2020) physics reason-
ingWe cannot infer the exact question2186Reference Task Questions and guidelines
Jansen et al. (2018) science QA Speciﬁc interfaces were designed. For a given question,
annotators identiﬁed the central concept the question was
testing, as well as the inference required to correctly an-
swer the question, then began progressively constructing
the explanation graph. Sentences in the graph were added
by querying the tablestore based on key- words, which re-
trieved both single sentences/table rows, as well as entire
explanations that had been previously annotated. If any
knowledge required to build an explanation did not exist
in the table store, this was added to an appropriate table,
then added to the explanation.
Xie et al. (2020) science QA similar to Jansen et al. (2018)
Khot et al. (2020) question an-
sweringThe HIT here is to write a test question that requires
CHAINING two facts (a science fact and some other fact)
to be combined.
Jhamtani and Clark
(2020)question an-
sweringWe then use (Amazon Turk) crowdworkers to annotate
each chain. Workers were shown the question, correct
answer, and reasoning chain. They were then asked if fact
1 and fact 2 together were a reasonable chain of reasoning
for the answer, and to promote thought were offered sev-
eral categories of “no” answer: fact 1 alone, or fact 2 alone,
or either alone, justiﬁed the answer; or the answer was
not justiﬁed; or the question/answer did not make sense.
(Detailed instructions in the appendix)
Inoue et al. (2020) question an-
swering1. Read a given question and related articles. 2. An-
swer to the question solely based on the information from
each article. 3. Describe your reasoning on how to reach
the answer. Each reasoning step needs to be in a sim-
ple subject-verb-object form (see example below). Your
reasoning must include sentences containing your answer.
Reference Task Questions and guidelines
Srivastava et al. (2017) concept learn-
ingThe screenshot includes both “explanations” and “instruc-
tions”, however, we cannot infer the exact question
Hancock et al. (2018) relation extrac-
tionwe cannot infer the exact question2187Reference Task Questions and guidelines
Lamm et al. (2020) question an-
sweringreferential equality, we cannot infer the exact question
Ye et al. (2020) question an-
sweringPlease read carefully to get accepted! (1) You’re not re-
quired to answer the question. The answer is already
provided and marked in red. Read examples below care-
fully to learn about what we want! (2) Identify important
short phrases that appear both in the question and in the
context. Important: The two appearances of the phrase
should be exactly the same (trivial differences like plural
form or past tense are still acceptable). Important: Write
sentences like Y is ”Switzerland”. Make sure there is no
typo in what you quote. (3) Explain how you locate the an-
swer with the phrases you marked; Only use the suggested
expressions in the table in the bottom.
Geva et al. (2021) question an-
swering1) Creative question writing: Given a term (e.g., silk),
a description of the term, and an expected answer (yes
or no), the task is to write a strategy question about the
term with the expected answer, and the facts required to
answer the question. 2) Strategy question decomposition:
Given a strategy question, a yes/no answer, and a set of
facts, the task is to write the steps needed to answer the
question. 3) Evidence matching: Given a question and
its de- composition (a list of single-step questions), the
task is to ﬁnd evidence paragraphs on Wikipedia for each
retrieval step. Operation steps that do not require retrieval
are marked as operation.2188