
Brielen Madureira David Schlangen
Computational Linguistics
Department of Linguistics
University of Potsdam, Germany
{madureiralasota, david.schlangen}@uni-potsdam.de
Abstract
Cognitively plausible visual dialogue models
should keep a mental scoreboard of shared es-
tablished facts in the dialogue context. We pro-
pose a theory-based evaluation method for in-
vestigating to what degree models pretrained
on the VisDial dataset incrementally build rep-
resentations that appropriately do scorekeeping .
Our conclusion is that the ability to make the
distinction between shared and privately known
statements along the dialogue is moderately
present in the analysed models, but not always
incrementally consistent, which may partially
be due to the limited need for grounding inter-
actions in the original task.
1 Introduction
“There’s a cute dog outside!” you say on the phone
to your friend. “Sweet. What colour is the dog?”,
they say. “What dog?” you reply – and your friend
is rightfully confused. With your first utterance,
you have committed yourself to there being a dog;
a commitment you can’t just simply ignore later on.
Models of dialogue from linguistics and psycholin-
guistics take this process of grounding orscorekeep-
ing—making propositions mutual knowledge—to
be an elementary fact about dialogue (Lewis, 1979;
Clark and Brennan, 1991).
In this short paper, we investigate whether recent
NLP models of visual dialogue capture this pro-
cess. Specifically, we use the VisDial dataset (Das
et al., 2017a), which consists of dialogues in En-
glish about an image in an asymmetric setting simi-
lar to that from the first paragraph, and derive from
it diagnostic propositions that should be considered
mutual knowledge at a given point in the dialogue,
and others whose truth value is only known to one
participant at the given time. We then probe dia-
logue representations built by models pretrained on
the VisDial task for whether they correctly track
the participants’ knowledge and commitments.2 Related Literature
Representing dialogue context implicitly as the con-
tinuous hidden states of neural networks trained in
an end-to-end fashion has been a prevailing prac-
tice since the works of Vinyals and Le (2015), Sor-
doni et al. (2015) and Serban et al. (2016). This
paradigm also enables multimodal input like im-
ages to be easily integrated (Shekhar et al., 2019b).
However, there is evidence that the human ability
ofcollaborative grounding still lacks in such mod-
els, in part due to the limitations of training regimes
and datasets (Benotti and Blackburn, 2021).
We witness extensive efforts to look into how
these models encode and make use of dialogue
history, capture salient information and produce
visually grounded representations (Sankar et al.,
2019; Agarwal et al., 2020; Greco et al., 2020a,b).
The analysis and evaluation of current dialogue
models (as Hupkes et al. (2018a), Shekhar et al.
(2019a), Parthasarathi et al. (2020), Saleh et al.
(2020), Wu and Xiong (2020), inter alia ) often rely
on diagnostic classifiers (Hupkes et al., 2018b) and
probing tasks (Belinkov and Glass, 2019), common
tools to examine whether representations built by
neural networks encode linguistic information.
Another purposeful area of research on dialogue
revolves around inference. Zhang and Chai (2009,
2010) discuss conversation entailment ,i.e.deter-
mining whether a conversation discourse entails a
hypothesis. Annotating or generating entailments,
contradictions and neutral statements in dialogue
datasets is usual in recent works (Welleck et al.,
2019; Dziri et al., 2019; Galetzka et al., 2021).
With insights from these three pillars, we pro-
pose a probing task for scorekeeping (Lewis, 1979)
on visual dialogues, formalised in the next section.
3 Problem Statement
Based on the premise that humans keep a mental
scoreboard of presupposed propositions and per-651
missible courses of action as a function of what has
been stated in a conversation (Lewis, 1979) and on
the public/private dichotomy discussed in Ginzburg
(2012), we propose a formalisation for the “kine-
matics of scorekeeping” (Lewis, 1979) on VisDial.
Each dialogue in the VisDial dataset is a tuple
D= (I, Q, A, T, P )representing an interaction
between a questioner Qand an answerer A. They
exchange turns T, which establish propositions P,
about a scene depicted in an image I.AseesI, but
Qdoes not. Both are provided with a caption K,
which for simplicity we take to be the first turn of
A,t=K; other turns comprise a question and
an answer, t= (q, a), so that T= (t)(as
dialogues have 10 turns).
We assume that: i) Adoes not lie about their in-
terpretation of the image; ii) Qdoes not ask redun-
dant questions; and iii) a fact disclosed by Aimme-
diately becomes a shared commitment, even though
in reality this is not always the case ( e.g.when a
misunderstanding happens). Under these assump-
tions, each tdiscloses a new fact p(and its im-
plications) about A’s judgement of the image that
was unknown to Quntilt.Pis then defined
as a set of Npropositions {p, p,···, p}. Each
pis either the direct entailment of t(that is, the
expressed proposition), which is established by A
to be true, or its negation, which is established by
Ato be false . The truth value of pis known to A
throughout the dialogue, but only privately so for
allk < i . It becomes shared between AandQat
k=iand remains so until the end of the dialogue.
With this in place, A’s scoreboard of a dialoguecan be represented by a matrix Swith dimensions
|T| × |P|. Each element sis a tuple c∈C=
{(true to A, private), (true to A, shared), (false
toA, private), (false to A, shared )}representing
the ‘score’ of proposition pat turn tas a class,
like the example in Figure 1. Hence, the negation
of a fact that Aconsiders true but has not been
mentioned yet is labelled as (false to A, private).
That way, the scoreboard at a given turn tis given
by the t-th row in Sand the whole matrix helps
visualising how the scoreboard is incrementally
updated throughout D.
Probing Task and Model . We design a classifi-
cation task to examine whether the continuous rep-
resentations of pretrained visual dialogue models
incrementally encode information about the score-
board represented by S. The probing classifier
is a function f:P×R→C, where P
is the set of propositions in a dialogue D,Ris
the space of hidden representations of a visual di-
alogue encoder and Care the scoreboard classes.
Based on the probing classifier architecture in He-
witt and Liang (2019), we approximate fas a neu-
ral network which maps a dialogue representation r
concatenated to a continuous representation zof a
proposition to a vector vwith a probability distribu-
tion over classes, v=softmax (Wσ(W[r;z]))
(bias term omitted), as illustrated in Figure 1. The
class is then predicted with the argmax function.
4 Data
Visual Dialogues and Encoders . We use the Vis-
Dial dataset v.1.0 (Das et al., 2017a) and the three
QandAencoders (RL_DIV , SL and ICCV_RL)652from Das et al. (2017b) and Murahari et al. (2019).
The first work implemented an end-to-end model
to train AandQusing reinforcement learning. The
latter is a follow-up study that adds an auxiliary ob-
jective function to encourage Qto ask more diverse
questions.The VisDial training set contains im-
ages from the MS COCO dataset (Lin et al., 2014).
Proposition embeddings zare built with Sentence-
Transformers (Reimers and Gurevych, 2019).
Generating Probes . The sets Pare program-
matically generated by manipulating QA pairs us-
ing rules that identify common lexical and syntactic
patterns in VisDial, in a similar fashion as Demszky
et al. (2018) and Ribeiro et al. (2019). Whenever
the pattern of a QA pair matches a rule, a direct
entailment and a direct contradiction are generated,
as those shown in Figure 1.
Dataset Construction . We retrieve the pre-
trained dialogue context representations R=
{r|0≤l≤10}, where ris the hidden state
of the encoder after it processed the dialogue up to
turnlinT(and the image and next question for A).
We then pair elements in Rwith the embeddings
of the generated propositions pinP, forming
tuples {(r, p)|0≤l≤10,1≤j≤N}which
are mapped to the corresponding class c∈C. The
true to Aorfalse to Astatus of a proposition pre-
mains fixed for all turns in D, since it refers to a fact
(according to A’s beliefs) about the image, while
theprivate status holds for (r, p), . . . , (r, p)
and shifts to shared for(r, p), . . . , (r, p). The
probing dataset is thus composed of datapoints
(r, p, c )for all D, for all turns’ representations
r∈R, for all p∈P. Propositions gener-
ated from captions are downsampled because they
outnumber the other turns, resulting in too many
propositions that are always shared. In order to
avoid bias with respect to the true/false dimension,
we sample the training set of propositions enforc-
ing that each type appears as true to Aexactly the
same number of times as it does as false to Ain
different dialogues. Table 1 presents a summary
(see Appendix for details).
5 Experiments
We train and test the classifier varying three as-
pects: i) AorQ, ii) main task with all classes in C
(TFxPS), plus three variations with reduced dimen-
sions: Only true/false (TF), only private/shared
(PS) and merging true/false on the private cases
only (PxTSFS) and iii) control tasks (Hewitt and
Liang, 2019) (a) replacing rby a random vector (b)
replacing rby a null vector, both only on the train-
ing set, to quantify how much information can be
extracted from propositions alone during training.
Evaluation . Results are evaluated with accu-
racy on class predictions. To avoid any influence
that knowing the position in the dialogue could
have (early in the dialogue, propositions have a
greater chance of being private , and vice versa),
we evaluate the results at turn 5 (at which there
is a more balanced chance of a fact having been
mentioned or not). For the error analysis, we recon-
struct complete predicted scoreboards and evaluate
incremental aspects: In each column, only one shift
from private to shared should occur at the right turn
(except for caption propositions, which are always
shared) and the true/false status should not change.
Implementation . The classifier is implemented
with PyTorch (Paszke et al., 2019) and trained with
gradient descent using Adam optimizer (Kingma
and Ba, 2014) to minimize cross entropy.
6 Results
Table 2 presents the accuracy of all models and
tasks at turn 5. The performance on the main task
is very similar across encoders, with differences
lower than 1.5%. Qoutperforms Ain all models
in the main task. While this is expected, since
Q’s representations must only keep track of the
dialogue whereas Amust interpret the image, the
difference is only marginal.653
For the TF task, the performance on the con-
trol tasks is close to random, as expected, but it
is higher than random for other tasks. We notice
that, while the training dataset is constructed to be
balanced in the true/false dimension, information
on the private/shared dimension has an inherent
bias that is more complex to counterbalance on the
training set. Despite the fact that datapoints in the
private class do not substantially outnumber the
shared class, we observe that each proposition type
can have a tendency to occur either early or late in
the dialogue (examples in Figure 2), causing them
to have an individual skewed distribution towards
shared or private at turn 5. This information leak
can be used as a shortcut by the classifier.Still,
AandQ’s representations lead to performances
between 8% and 32% higher than the control tasks
in all cases.
Human Performance . Table 3 shows the human
performance, estimated as the average accuracy of
3 annotators (0.86 Fleiss’ κon TFxPS) on a sample
of 94 datapoints, each from a different dialogue in
the test set (not only at turn 5). We observe that hu-
mans agree most of the times on their judgements
and all models perform well below human level.
Error Analysis . We conduct an error analysis
onA, main task, TFxPS. The confusion matrix
in Figure 3 shows that it is easier to distinguish
between true/false to Ain the shared dimension,
which can be a sign that dialogue information is
more salient in the representations than the image.
The accuracy on all datapoints with proposition
types that occur on the training set is 67.69, higher
than for those that do not, which is 53.11.
When we reconstruct full predicted scoreboards,
some qualitative shortcomings become evident. A
shift from private to shared is predicted at the cor-
rect turn for 60.32% of the propositions but only65438.24% shifts only at the correct turn. Besides, only
44.50% of the propositions have stable predictions
regarding the true/false to Adimension.
Figure 4 shows types of errors in the predictions
(the Appendix has more examples). We see the
same truth value assigned to opposite propositions,
the same proposition classified both as true and
false at different turns, as well as an occasional
oscillation between private/shared throughout the
dialogue. These are indications that, although accu-
racy per label is generally high, the representations
do not seem to always allow incrementally stable
and consistent predictions throughout the dialogue.
7 Scope and Limitations
The results on this paper comprise three visual di-
alogue models trained using a similar setting on
the same dataset. The preprocessing steps used by
these models replace some tokens by a UNK token
and truncate long captions, which prevents some
information to become shared as assumed. Further
investigation with other models and data is neces-
sary in future research in order to support more
general conclusions. The results also rely on the ca-
pabilities of the classifier. Although we performed
hyperparameter search, the probing classifier does
not completely overfit the full training dataset, thus
other architectures and hyperparatemeters can be
further investigated.
The rule-based generation of propositions has
limitations. It cannot generate propositions for all
QA pairs and some rules end up not always yield-
ing grammatically valid sentences, for instance be-
cause of countable/uncountable nouns, detection of
singular/plural forms and mistakes and typos deriv-
ing from the dialogues themselves. Besides, spuri-ous patterns deriving from the implemented rules
or other confounds and inherent biases ( e.g.Fig-
ure 2) may exist and be predictive of the classes,
which could be captured by the probing classifier
and influence (likely overestimating) the results.
Enforcing a balance on the training set in terms of
true/false to Asolves one source of bias but causes
its distribution to differ from the validation and test
set. The test set also has a different distribution
because of its varying number of turns.
Finally, while the assumptions proposed in Sec-
tion 3 are necessary idealizations for using VisDial
for this task, they simplify essential aspects of di-
alogues, e.g.the uncertainty about a fact actually
being shared, memory limitations and the many
kinds of inference that are used in the accommoda-
tion of shared knowledge, such as presuppositions,
implicatures, entailments and implicit information.
Our method cannot capture background knowledge
not explicitly stated in dialogue turns.
8 Conclusion
We have proposed a novel way to do theory-based
evaluation of visual dialogue models. Using diag-
nostic propositions, we investigated to what degree
neural network visual dialogue models incremen-
tally build up representations that are appropriate to
doscorekeeping of shared commitments through-
out a dialogue. The evaluated models trained on
VisDial capture part of this process, but not always
consistently, possibly because this ability is not
an elementary component of the training regime.
The relatively impoverished nature of the original
task in terms of coordination phenomena can also
limit the capability of models to build good dia-
logue representations (Schlangen, 2019). Future
work should extend the evaluation to other models
and reflect on how better and ecologically valid
diagnostic datasets for visual dialogues can be con-
structed.
9 Ethical Considerations
Propositions are direct manipulations of QA pairs
and thus reflect the subjective judgments of Vis-
Dial crowdworkers. Therefore, they are not per se
necessarily trueorfalse with respect to the image,
but with respect to A’s interpretation expressed as
answers. Inappropriate content on images, captions
and dialogues can be replicated by the rule-based655proposition generation. To try to remedy this, we
filtered out dialogues containing words that could
be used for sensitive content. Despite our efforts,
we cannot guarantee that we could remove every-
thing, given the size of the dataset and the inherent
bias of how humans interpret images. As a result,
the only purpose of the propositions is performing
the evaluation as proposed here.
Acknowledgements
We are thankful to the anonymous reviewers for
their feedback and suggestions, to Wencke Lier-
mann for implementing the interface for the human
evaluation and to the student assistants of the Com-
putational Linguistics Lab who contributed on the
experiment.
References656657Appendix
A Generating Propositions and
Constructing the Datasets
This section presents details about the procedure
to turn QA pairs from the VisDial datasetinto
propositions.
Solving Pronouns . Coreference resolution is
specially challenging on visual dialogues, as dis-
cussed in Loáiciga et al. (2021). Despite the limi-
tations, we used the model proposed in Lee et al.
(2018) to replace pronouns (those that were de-
tected and solved) by their corresponding entity as
follows:
1.Merged caption and QA pairs into a single
string.
2.Passed string to coreference resolution model
to get coreference clusters.
3.Assumed that the first element in the cluster
was the entity (its first mention).
4.For each dialogue, checked which questions
and answers contained pronouns of interest
(he, she, it, they, his, her, its, their, him, them,
hers, theirs, this, that, these, those) and re-
placed them with their corresponding cluster
entity, if detected. Assumed the pronoun her
was always possessive.
5.If the entity comprised more than N=5 to-
kens, we did not replace it (because entities
spanning over many tokens are very likely to
be long portions of the caption that result in
wrong propositions).
6.With postprocessing steps, put string back into
VisDial format.
On average, 2.24 pronouns were replaced per
dialogue on the training set, 2.43 on the validation
set and 1.15 on the test set.
Generating Propositions . Automatic genera-
tion of diagnostic datasets or adversarial examples
via programmatic manipulation rules or templates
is a usual step in probing studies, e.g.Johnson et al.
(2017), Shekhar et al. (2017), Ribeiro et al. (2018)
and Bitton et al. (2021). The main steps to turn
QA pairs into propositions were to some extent
based on Ribeiro et al. (2019) and Demszky et al.
(2018). We analysed common patterns of questions658and answers on VisDial and implemented 34 rules
that create entailments and contradictions. Some
rules are lexical ( e.g.questions starting with ‘ what
color is ’ and whose answer has a color name) and
others depend on POS tag patterns extracted using
SpaCy v.3.0.5.Most rules work for polar ques-
tions, some work for other types of questions. We
noticed that some images and dialogues on VisDial
contain inappropriate content. To avoid replicating
this on the propositions, we filtered out dialogues
that contain words that may be sensitive (see code
documentation for details). Propositions were then
generated as follows:
1.Parsed the caption to extract nouns and adjec-
tives and generated caption propositions.
2.For each turn, checked whether it matched a
manipulation rule.
3.Every rule, when they were applied, generated
a direct entailment and a direct contradiction
(negation of the entailment).
4.Propositions that contained pronouns (for
cases in which coreference resolution did not
work), except for it, or that were too long
(more then 15 tokens) were excluded.
The code documentation has a more detailed
description of the rules. The next sections present
details of the resulting proposition sets. Note that
the number of dialogues in each set is smaller than
in the VisDial original splits, because some were
filtered out and others had no propositions.
Propositions have four attributes: i) kind of ma-
nipulation rule; ii) dialogue and turn from which
it derives; iii) a true/false status with respect to
what Athinks about the image; iv) the polarity
(positive/negative) of the answer, if applicable.
Downsampling and de-biasing . We noticed
that the proportion of caption propositions was
much larger than propositions deriving from other
turns, which would cause a considerable imbalance
towards facts that are always shared in the score-
board. Therefore, we sampled 15% of the caption
pairs (entailment and contradiction) on all datasets
to make the distribution over manipulated turns be
closer to uniform.
Furthermore, in preliminary experiments we ob-
served that propositions could give away informa-
tion on the true/false to Astatus. For instance,
‘there is a zebra. ’ can appear very often as an en-
tailment (on the many photos showing zebras) butrarely as a contradiction (dialogues where Qspon-
taneously asks ‘ is there a zebra? ’ and the answer is
‘no’). Besides, on rules that manipulate questions
that are not polar ( what color is the dog? black. ),
negation is always a contradiction. So the classi-
fier could make predictions based on the lexical
form alone. To counter this bias, we constructed
a balanced training dataset by sampling from the
original set while making sure that, for each p
thatAestablished to be true with respect to an
image/dialogue, we also included an equal ppaired
with an image/dialogue in which it is established
to be false. While this procedure reduced the size
of the training set, we ensured that predictions on
the true/false dimension would need to use the dia-
logue representations. We also limited the number
ofpof the same kind to 2,000 (1,000 as entailment,
1,000 as contradiction), to avoid having very com-
mon propositions like ‘ the photo is in color ’ or ‘ it
is sunny ’ occurring too often.
Datasets used in the experiments. The fol-
lowing paragraphs discuss the final datasets used
in the experiments ( i.e.after downsampling cap-
tions and balancing the training set). The frequency
over which turn was manipulated is shown in Fig-
ure 5. Although there is an imbalance towards
later turns on the training set, the proportion of pri-
vate/shared classes at turn 5 is relatively balanced
(around 44.5/55.5), partially due to the fact that, at
the last turn, no proposition is assigned a private
class. Figure 6 shows the frequency of the number
of turns that have been turned into propositions in
a dialogue. Table 4 show the proportion of each
type of proposition on the datasets. The training set
has less propositions that do not derive from polar
questions due to the balancing.
The propositions, paired to dialogue representa-
tions on each dialogue turn, with the class assigned
to each tuple can be seen as a layer of annotation
which is not predicted but constructed.
37.20% of the validation proposition types and
31.58% of the test proposition types appear among659
the training propositions. 82.68% of the validation
propositions and 79.63% of the test propositions
occur in only one dialogue. On average, a propo-
sition appears in 12.77 dialogues in the training
set, 1.91 dialogues in the validation set and 2.34
dialogues in the test set. 72.73% of the word types
in the validation set and 63.00% of the word types
in the test set occur in the training set.
Examples . Figure 10 shows dialogues from the
training set and the propositions generated for each
turn, after downsampling the caption propositions
(but before balancing). Propositions can inherit
grammatical or spelling problems from the dia-
logues themselves. Figure 1 in the main section
contains all propositions, before downsampling.
Collecting dialogue representations . To collect
the dialogue state representations, we adapted the
original train.py andevaluate.py scripts.To get
the representation at turn 10 for A, we needed to
feed a dummy next question made of the start and
the end symbols with a question mark token in
between.
Human Judgement . We randomly sampled 100
dialogues and one proposition on each of them.
Then we sampled a random turn up to which the
corresponding dialogue would be shown. The an-
notators were non-native English speakers who
worked as student assistants at the Computational
Linguistics Lab of the University of Potsdam. The
task was explained to the annotators verbally and
then again in written form at the beginning of the
annotation. All participants saw the same data-
points at a different random order, presented in a
setting as shown in Figure 7, and had to select one
of the four alternatives (which correspond to the
main task TFxPS).
B Reproducibility
In this section, we present further details of the
implementation and additional results to support re-
producibility. More information can also be found
in the code documentation.
Hyperparameters . We used comet.ml’sim-
plementation of the Bayes algorithm for hyperpa-660rameter search on A, main task, TFxPS, RL_DIV ,
aiming at maximizing accuracy on the validation
set, as well as some manual selections. The (non-
exhaustive) search space is shown in Table 6. The
optimal configuration was then used in all experi-
ments, with a maximum of 30 epochs and no early-
stopping. A preliminary test with an even larger
hidden dimension showed a very minor improve-
ment. For each experiment, we used the config-
uration that led to the best performance on the
validation set to get results on the test set. Each
experiment took between 50 and 60 minutes.
The sentence encoder models listed on Table 6
are available at HuggingFace’s Model Hub.
Classifier architecture . The neural network was
implemented using Pytorch 1.7.1. The proposition
embeddings have 768 dimensions and the dialogue
context embeddings have 512 dimensions. We used
a sequential model from PyTorch with the follow-
ing layers and dimensions:
1.linear layer (in features=768+512, out fea-
tures=1024, bias=True)
2. sigmoid function
3. dropout layer (p=0.1)
4.linear layer (in features=1024, out features=n
labels in {2,3,4}, bias=True)
5. softmax function + cross entropy loss
The models have 1,315,844, 1,314,819 and
1,313,794 trainable parameters for the classification
tasks with 4, 3 and 2 labels, respectively.
Infrastructure . The operating system used to
run experiments was Linux, release 5.4.0-99-
generic, processor x86_64. We had two GPUs
available (NVIDIA GeForce GTX 1080 Ti), but
each individual experiment used only one of them.
C Detailed Results
Table 7 shows the overall accuracy on all datapoints
(comprising all turns in the test set). Table 8 and
Table 9 show all results on the validation set.
On Figure 8 we split the accuracy per type of
proposition. Propositions that derive from negative
facts about the image ( ‘is there a dog? no. ’ ) seem
to be harder than positive ones when they derive
from earlier turns, but they are easier to correctly
classify when they derive from later turns. Propo-
sitions deriving from questions that are not polar
are harder (which may be a consequence of the
balanced dataset selection that results in few propo-
sitions of this type for training). We also see that
propositions derived from manipulating later turns
are, in general, harder to classify.
When we consider each row of the scoreboard
(representing the scoreboard at a given turn), we
can inspect how accuracy evolves over turns, illus-
trated in Figure 9.
For the error analysis on captions, a right shift
from private to shared means that the class at turn 0
is shared. Shifting only at the right turn means that
it starts as shared and does not shift at any turn.661662663664