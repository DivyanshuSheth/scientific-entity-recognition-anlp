
Saiteja KosgiSarath SivaprasadNiranjan Pedanekar
Anil NelakantiVineet GandhiKohli Centre on Intelligent Systems, IIIT HyderabadTCS Research, PunePrime Video, Amazon Bengaluru
Abstract
We present a method to control the emotional
prosody of Text to Speech (TTS) systems by us-
ing phoneme-level intermediate features (pitch,
energy, and duration) as levers. As a key idea,
we propose Differential Scaling (DS) to dis-
entangle features relating to affective prosody
from those arising due to acoustics conditions
and speaker identity. With thorough exper-
imental studies, we show that the proposed
method improves over the prior art in accu-
rately emulating the desired emotions while
retaining the naturalness of speech. We ex-
tend the traditional evaluation of using indi-
vidual sentences for a more complete evalua-
tion of HCI systems. We present a novel ex-
perimental setup by replacing an actor with
a TTS system in offline and live conversa-
tions. The emotion to be rendered is either
predicted or manually assigned. The results
show that the proposed method is strongly pre-
ferred over the state-of-the-art TTS system and
adds the much-coveted “human touch” in ma-
chine dialogue. Audio samples for our exper-
iments and the code are available at: https:
//emtts.github.io/tts-demo/
1 Introduction
“The text is like a canoe, and the river on which
it sits is the emotion. It all depends on the flow
of the river, which is your emotion. The text
takes on the character of your emotion.”
— Sanford Meisner
In natural language processing, vocabulary and
grammar tend to take center stage, but those ele-
ments of speech only tell half the story. Affective
prosody provides context and gives meaning to
words, and keeps listeners engaged. Understand-
ing emotional prosody is central to language and
social development. Studies suggest that we show
remarkable sensitivity to prosody "even as infants"
(Nazzi et al., 1998; Massicotte-Laforge and Shi,
2015). Recently Kraus (2017) shows that voice-
only communication likely elicits higher empathicFigure 1: Dialogues can have different meanings de-
spite having the same text. Also, starting with the same
emotion, Juliet has different emotions post Romeo’s
response.
accuracy than even multi-sense modes including
facial expressions.
Buchholz (2016) shows that any meaningful spo-
ken dialogue cannot happen without some amount
of prosodic matching. As humans, we naturally an-
ticipate and adapt with emotional cues in convers-
ing with others, see Figure 1 for an example. Cele-
brated trainer Sanford Meisner employed this to de-
velop Meisner technique for theatre actors to react
naturally to others in the environment as opposed
tomethod acting . The importance of emotional
prosody in conversations cannot be overstated and
TTS models need to fill this gap to make human-
like conversations possible in HCI systems.
Mitchell and Xu (2015) study the value of emo-
tional prosody in HCI and emphasize its role in
healthcare dialogue systems, improving social in-
teraction skills in people with autism, augmentative
and alternative communication devices and gaming
narratives. They explain that successfully incorpo-
rating expressive speech into HCI, involves two as-
pects: (a) prosodic emotion recognition and (b) ex-
pression of emotional prosody. Considerable effort
has been made towards recognizing and predicting
the emotional nuances in human dialogues (Kim
and V ossen, 2021; Poria et al., 2019b; Zhu et al.,
2021; Li et al., 2017; Poria et al., 2021; Vinyals
and Le, 2015). However, current TTS systems are336yet to improve on rendering emotive or expressive
speech for real-world HCI systems.
State-of-the-art TTS systems (Ren et al., 2020;
Wang et al., 2017) tend to exhibit average emo-
tions for a given phoneme sequence by taking the
mean of utterances from training data. Some ef-
forts towards improving expressiveness (like Bat-
tenberg et al., 2019; Karlapati et al., 2020) pro-
vide prosody control using a reference clip. Oth-
ers like Sivaprasad et al. (2021) and Habib et al.
(2019) further focused on controllability exposing
levers that can be manipulated at inference-time
to derive the intended expression. However, the
quality and stability of synthesized speech heavily
depends on various modeling choices. Emotion or
prosody modeling, for example, could pick from
numerous available discrete or continuous space
representations. The encoder network module cho-
sen might vary in its ability to disentangle prosody
from other acoustic features like speaker identity
and adaptability to content. For example, those
relying on reference clip to replicate prosody might
perform poorly when input text is unsuitable for
rendering with prosody of reference. Some models
feed prosody features with phoneme embeddings
directly into the decoder while others use them to
predict intermediate features that are used in condi-
tioning the decoder. It is empirically verified (like
in Sivaprasad et al., 2021) that intermediate fea-
tures could be suitably manipulated to bring about
the desired change in expression.
We take this direction forward to endow the in-
termediate feature prediction module with affective
state control over the final rendering. We propose
Differential Scaling (DS) of the predicted intermedi-
ates to bring about the required change in emotion.
TheDSmodule is aimed to effect only emotion as
intended while remaining agnostic to all other fea-
tures like speakers identities or acoustic conditions
as seen in train data. We show that this significantly
improves the naturalness of the generated speech,
while allowing finer control over prosody.
In addition to comparing our model’s renderings
against various others’ from literature for natural-
ness and emotion control on conventional single
utterances drawn from disconnected contexts, we
also evaluate them in conversations. We curate data
with conversational theatre dialogues and replace
an actor with a TTS system. We use its response
as a proxy to evaluate the empathic accuracy. In
another experiment, we had a theatre director con-trol the emotion levers of our TTS model in a live
conversation with the actor to evaluate controllabil-
ity. As demonstrated in the results, our proposed
method significantly improves over existing meth-
ods in producing suitable prosodic variation lend-
ing closer to human-like conversations. The rest of
this paper will elaborate on the following contribu-
tions of this work.
•We propose a simple technique of using
aDSmodule to better emulate emotions
in TTS rendered speech. This works as
plug-and-play with both autoregressive and
non-autoregressive TTS models that predict
prosodic features as an intermediate step.
•Our work extends the literature of training
controllable and expressive TTS models with
improved empathic accuracy and without spe-
cific studio recorded data.
•Finally, we present novel methods and data
for evaluating TTS models in real conversa-
tions with human subjects. The method of
evaluation is a useful step towards filling the
gap of emulating emotional speech that needs
more work.
2 Related Work
Prosody and conversational speech. Unlike in
written text, spoken words contain additional non-
verbal information. These cues are collectively
termed prosody (Leentjens et al., 1998) that include
variations in tone, pitch, energy, duration, accents,
intonation, stress, etc. Buchholz (2016) showed
that prosodic exchange is unavoidable in human
dialogue. Various machine learning methods have
been proposed to predict emotion in speech from its
prosody variations (Asgari et al., 2014; Kamarud-
din and Abdul Rahman, 2013). Variations in pitch
accents (Nielsen et al., 2020), for example, lead to a
significant difference in how the receiver perceives
the content. A sentence (like I said unlock
the door, not lock it from (Rosenberg
and Hirschberg, 2009)) could be delivered both as
a statement and a command by merely changing
prosody.
Emotion recognition in conversations has gained
increasing attention for developing empathetic ma-
chines with emotion-tagged multi-modal data pub-
licly available for modeling like (Li et al., 2017;
Poria et al., 2019a; Busso et al., 2008). While most337methods like (Majumder et al., 2019; Jiao et al.,
2019) use a combination of text and speech infor-
mation, some leverage additional side-information
from broader context (Ghosal et al., 2020) and the
topic of conversation (Zhu et al., 2021).
In such labeled data, emotion is often rep-
resented as a categorical variable over a dis-
crete space following models like Ekman’s ba-
sic emotions (Ekman, 1992) or the wheel of
Plutchik (Plutchik, 1980). This choice is largely
owing to the ease of annotating data. Russell (1980)
proposed a continuous two-dimensional space as
an alternative called valence-arousal model for hu-
man emotions. Arousal signifies the intensity of
the emotion while valence captures its polarity. It
has been extended to add a third dimension of dom-
inance, making it the valence-arousal-dominance
(V AD) model. V AD has since been widely used in
modelling emotion in music (Grekow, 2016; Rach-
man et al., 2019), speech (Asgari et al., 2014; Ka-
maruddin and Abdul Rahman, 2013) and other con-
tent (Joshi et al., 2019; Buechel and Hahn, 2017).
We use the continuous space representation as it is
richer and more convenient to handle in our model.
For instance, a continuous space allows the user
to change the level of emotion like happy to de-
lighted, sad to depressed, etc., superlatively during
synthesis.
Expressive and controllable TTS. Neural
TTS systems are now increasingly popular, im-
proving upon older concatenative statistical sys-
tems (Michelle and Georgia, 2020) in synthesized
speech naturalness. These are broadly sequence-
to-sequence networks with an encoder processing
the input text or phoneme sequence followed by a
decoder that generates the sequence of Mel frames
for output speech. Mel frames are then projected
into the time domain by a vocoder (van den Oord
et al., 2016; Griffin and Lim, 1984) to generate
the speech. Decoding could be autoregressive with
Tacotron-like models (Wang et al., 2017) or non-
autoregressive with Fastspeech-like models (Ren
et al., 2019).
Non-autoregressive models are faster at infer-
ence than autoregressive models with about com-
parable naturalness of speech quality (Ren et al.,
2020). The trick non-autoregressive models use
to generate Mel frames in parallel is to predict the
relevant features as an intermediate step and con-
dition the independent decoding of Mels on them.
This technique is now increasingly adopted for au-toregressive models as well (Wang et al., 2021)
to predict features like phoneme duration that im-
prove decoding stability avoiding alignment issues.
Our method is compatible with any architecture
that predicts prosodic features of pitch, energy, and
duration as an intermediate step before decoding.
Going beyond the naturalness of speech, there
has been considerable effort to improve the expres-
siveness of the renderings. Some focused on learn-
ing a linear space of variations in speech expres-
sions for selecting a suitable variation at inference
time. Wang et al. (2018) learn this space unsuper-
vised by encouraging it to explain all variations in
training data not captured in content embedding.
A reference encoder maps an input utterance to a
style embedding as a linear combination of basis
style vectors. Manual analysis is required to un-
derstand the prosody feature learned into a basis
vector that could include variations like vocal depth
or pitch, speaking rate, or even background noise
as available in training data. While this offers style
control, it does not explicitly learn the prosody vari-
ations of interest into the style space. Our work
focuses on the same level of control but specifically
over the affective state as labeled in some data for
supervision.
Sivaprasad et al. (2021) propose a model similar
to Wang et al. (2018) with style tokens restricted
to valence and arousal. However, the absolute
(pitch, energy, duration) feature predictions restrict
prosody control, leading to unnatural distortions.
Specifically, it skews more towards retaining the
speaker’s voice identity than the emotion and en-
tangles emotion with other acoustic features. Kar-
lapati et al. (2020) replace the linear style space
with a variational reference encoder to generate
prosody embedding to condition the decoder. Bat-
tenberg et al. (2019) use a similar variational model
but instead force its posterior to match that of the
reference utterance to copy prosody with a con-
trollable parameter determining the closeness of
the match. This trick alleviates certain issues like
in pitch-range (Younggun and Taesu, 2019) and
transfer to unrelated sentences but exposes a lower
degree of control with no explicit levers to operate,
as possible in our work.
Habib et al. (2019) propose to learn an explicit
latent representation for various prosodic variables,
segregating them into explicitly controlable (like
affect, speaking rate, etc) and implicit (like intona-
tion, rhythm, stress, etc). While the model offers a338
higher degree of explicit control, it requires using
a proprietary studio recorded data with utterances
reflecting prompted emotions at specified arousal.
Dependence on explicit supervision from studio
recorded data makes it harder scale this model
across languages and other prosodic variations. In
contrast, we use publicly available data with emo-
tion labels to train our models.
There are other methods that try to predict suit-
able prosody features from text content. Raitio
et al. (2020) add a prosody encoder module to
standard TTS network that predicts certain hand-
crafted prosody features from text embedding of
input. This prosody encoder is used with a small op-
tional bias for affect variations at inference. Hodari
et al. (2021) extend this to replace hand-crafting
prosody features with explicit training followed by
their prediction from text. Karlapati et al. (2021)
further enrich the textual context using BERT em-
beddings and parse-trees. These methods are lim-
ited in expressiveness offering no control over ren-
dering emotion that our work focuses on.
3 Model
Our network uses a backbone TTS that can be bor-
rowed from any model which predicts pitch, energy
and duration as intermediates features from input
phoneme sequence. This network learns to predict
the average features for given phonemes. Follow-
ing the convention in earlier works, we refer to
the intermediate features as variances and the mod-
ule that predicts them as variance adaptor. Prior
work improves standard variance adaptors in, say
FastSpeech2, by conditioning on emotion variablesof valence-arousal in addition to the phoneme se-
quence to generate expressive speech. We refer to
it as Emotional Variance Adaptor (EV A) for which
we propose an alternative. Our proposed Differ-
ential Scaler (DS) module determines how best to
vary the output of the EV A to bring the desired
change in emotion. We describe the details of these
network choices in this section; specifically, the
broader backbone network architecture and the dif-
ferent variance adaptor modules from non-emotive
baseline, emotive baseline and our proposal.
3.1 Backbone
We present experiments with two suitable choices
for our backbone systems, FastSpeech2 and FCL-
taco2. The backbone has three modules; an en-
coder, variance adaptor and decoder. The encoder
maps an input phoneme sequence to its embedding.
Given this representation, the variance adaptor pre-
dicts the pitch, energy and duration for each of
the phonemes. These intermediate features are pro-
cessed by the decoder module downstream to return
Mel-spectrogram frames. We reuse the encoder and
decoder modules as designed in their original archi-
tectures without any changes. We refer readers to
the respective papers for details of these networks.
Wavenet (van den Oord et al., 2016) vocoder is used
to map Mel-spectrogram outputs of the decoder to
time-domain raw audio.
3.2 Variance adaptor module
Non-emotive baselines. Our baseline models of
FastSpeech2 and FCL-taco2 are trained with the
variance adaptors as described by their authors. We
also train a derivative of the FastSpeech2 with the
variance adaptor modified to make predictions at
the phoneme-level and not at frame-level. This is
to facilitate the phoneme level control of variances.
A duration dis predicted for each phoneme π,
following which the length regulator repeats the
hidden state of that phoneme.πtimes. Also unlike
FastSpeech2, we use this length regulator after the
predicted pitch and energy are added to the encoder
output. We refer to this derivative as FastSpeech2 π.
Emotive baseline. Sivaprasad et al. (2021) con-
ditioned the variance adaptor of FastSpeech2 on
additional emotion embedding that gives the model
control over prosody of the rendered speech. It gen-
erates the emotion embedding as a linear weighted
combination of the valence and arousal vectors that
are learned from data during training. The weights
are valence and arousal values as annotated for339
training and can be used as control levers to modify
emotion during inference. This emotion variance
adaptor (EV A) module generates suitable interme-
diate features of pitch and energy at frame-level
and duration at phoneme-level. These features are
consumed by the decoder along with the encoder
output in generating Mel frames. While this helps
control emotional prosody rendered speech, it leads
to a significant drop in perceptual quality and natu-
ralness relative to the baselines. Our contribution is
an alternative design of the variance adaptor mod-
ule that improves upon Sivaprasad et al. (2021)’s
FastSpeech2 + EV A model in emotion control and
expressiveness and upon the baselines in terms of
naturalness.
Differential Scaler. We extend the emotion rep-
resentation from EV A to include dominance in ad-
dition to valence and arousal values. Dominance
is the degree of control exerted by an emotion. In-
cluding dominance dimension to the emotion space
expands the range of emotions the TTS model can
express. For example, by introducing this dimen-
sion, we can better distinguish outputs for emotions
like ‘anger and fear’ or ‘sad and contempt’.
TheDifferential Scaler module further extends
EV A to estimate the change in variances neces-
sary for a pronounced effect of the target emotion
relative to its neutral counterpart. As shown in Fig-
ure 3(b), the variances are estimated using the EV A
module for a given phoneme sequence at two dif-
ferent triplets of V AD values. One prediction cor-
responds to the neutral emotion with V AD values
all set to zeros. The other prediction corresponds
to the chosen V AD values of target emotion. Wetake the difference of these two estimates as the
direction along which the variances can be varied
for the desired change in emotion without affecting
other acoustic features. We are implicitly making
two assumptions here. Emotion variations are cap-
tured as linear transformations in this space and
that there is a strong disentangling of emotional
prosody with other acoustic features in this space.
Results from our empirical evaluation favorably
support the above assumptions.
4 Training
Modelling with intermediate features facilitates
training the backbone and the variance adaptors
independently on different data. We exploit this
to train our variance adaptor on scarcely available
V AD annotated data while reusing backbone mod-
els trained on abundant transcribed speech data.
Backbone. We train two backbone networks
FastSpeech2 π(non-autoregressive) and FCL-taco2
(autoregressive) on Blizzard 2013 dataset (King
and Karaiskos, 2014). It contains 147 hours of
Catherine Bayers’s speech, reading books in Ameri-
can English. Due to the style of reading, the dataset
is rich in expressiveness and spans different combi-
nations of pitch, energy and duration. Both models
are trained with Mel loss (mean absolute error be-
tween predicted and ground truth Mels), pitch loss,
energy loss and duration loss (mean square error
between predicted and ground truth features). Both
models are trained for 200K iterations using Adam
optimizer with warm-up learning rate scheduler
and batch size of 16.
EV A. We train EV A on MSP-Podcast corpus340(Lotfian and Busso, 2019) annotated with arousal,
valance and dominance values. The corpus consists
of around 100 hours speech data but their transcrip-
tions are not available. We generate transcripts
using a speech-to-text model. We use Montreal-
Forced-Aligner (MFA) (McAuliffe and Sondereg-
ger, 2017) for phoneme alignments. Those tran-
scripts that MFA fails to find a good alignment for
are filtered out. The remaining utterances add up
to about 71 hours of emotive speech data which
we use to train our EV A. We train pitch, energy
and duration predictors conditioned on V AD values
minimizing only the sum of variance losses. For
all the experiments, text transcripts are converted
to phonemes using Sun et al. (2019). We generate
Mel spectrogram from the audio files similar to
Wang et al. (2017). Pitch and energy are computed
from the Mel spectrogram and we use MFA for
aligning phonemes to train the duration predictor.
5 Experiments and user study
We present three experiments; comparison with
prior-art using conventional evaluation metrics,
those for emotional consistency with pre-recorded
audio, and finally, live conversations with humans.
5.1 Comparisons with prior-art
We compare the proposed approach against four
state of the art TTS models. The list includes
two non-emotive TTS models (FastSpeech2 and
FCL-taco2), one reference-based method (Cai et al.,
2021) and one A V conditioned model (FastSpeech2
+ EV A). We also compare our method with the mod-
ified backbone, FastSpeech2 π.
To show the efficacy of DS over EV A module
independent of the effect of other interventions,
we perform two more comparisons. The first com-
parison evaluates our model against Fastspeech2 +
EV A trained with ‘dominance’ (on arousal, valence
and dominance). The second comparison is made
against Fastspeech2 π+ EV A with the backbone
trained on Blizzard (both backbones trained on the
same dataset). The first comparison is made on the
perceptual-quality and emotional expressiveness
while the second comparison is made only for their
perceptual-quality.
To evaluate the perceptual-quality/naturalness
we compare Mean Opinion Score (MOS) (Chu and
Peng, 2006) averaged across forty subjects profi-
cient in English. We synthesize twenty different
sentences from the test set using each of the sevenmodels. We prepare user study by picking five
samples rendered by each model to make a survey.
Annotator rates each sample on a Likert scale of
one for ‘completely unnatural’ to five for ‘com-
pletely natural’.
To evaluate the emotional expressiveness of the
proposed model, we perform two surveys. In the
first survey, given a sample, we ask the user to
choose the best perceived emotion from a set of
four, namely, ‘Happy’, ‘Sad’, ‘Angry’ and ‘Fear’.
We ask the raters to not judge the textual content
and annotate the emotion for each sample based on
the rendering alone. In the second survey we evalu-
ate the efficacy of the models to bring about finer
control over emotion. We generate two samples
with same broader emotion category but with two
levels of intensity. The subject now has to identify
the sample with higher intensity. For both surveys
we generate five samples per emotion and twenty
samples for each model. We aggregate the rating
across forty proficient English language speakers.
5.2 Emotional consistency in dialogues
Previous efforts in prosody controlled TTS have
been evaluated on individual sentences without con-
text. We propose a novel evaluation strategy us-
ing excerpts from theater recordings. We replace
the audio of one of the actors in the conversation
with renderings from a TTS model and have a hu-
man subject evaluate it for emotional consistency.
The emotion for TTS renderings are chosen man-
ually by a theater director. We compare this with
TTS rendered with emotion predicted using Tod-
Kat (Zhu et al., 2021) from the dialogues spoken
so far. This study consolidates the two aspects of
HCI we mentioned in the introduction; prosodic
emotion recognition and its expression in TTS ut-
terances.
The dataset is curated using segments from four
popular plays, namely, ‘Speed-the-Plow’, ‘Night,
Mother’, ‘Bobby Gould in Hell’ and ‘Death of a
Salesman’. We select 30dialogue segments collec-
tively from the four plays with an average dialogue
length of 90seconds per segment. Timestamps of
segments selected from each play is given in sup-
plementary material. We replace the female voice
in the segment with (a) non-emotive TTS model
(FastSpeech2 π) (b) our model with emotion pre-
dicted for each utterance using TodKat and (c) our
model with a senior theatre director picking the
emotion for each utterance. We randomly pick five341
dialogues from the 30 samples in all three settings
for each of our surveys. We ask forty raters to rank
the three setting in terms of the emotional consis-
tency of the dialogue i.e., to judge the naturalness
and aptness of the emotional prosody in the given
context.
5.3 Conversation with Meisner trained actor
A Meisner trained actor responds to another actor
taking into account his/her behavior. In this experi-
ment, we observe how a Meisner trained actor (Ac-
tor M) reacts in a live dialogue initiated by (a) an-
other trained human actor, (b) a non-emotive TTS
(FastSpeech2 π) and (c) our model (FastSpeech2 π
+ DS). We use the same neutral script with 18 lines
in all three cases. We use the behavior of Actor
M during interaction with the human as reference.
The closeness of Actor M’s behavior to this refer-
ence while interacting with the two TTS models
is used as a measure of the latter’s effectiveness in
rendering speech expressive enough to evoke an
emotive response.
For each of the three scenarios, the conversation
is initiated with two different emotional states, viz.
(a) highly positive and (b) highly negative. The
emotion for our TTS model is chosen live on-the-
fly by a theatre director from fourteen bins in the
discretized arousal-valence space. The bins arechosen to span the V-shape around high-arousal-
high-valence and low-arousal-neutral-valence (Di-
etz and Lang, 1999). We take majority vote of three
listener ratings for each utterance of Actor M on
the same discretized arousal-valence space to allow
quantitative comparisons.
6 Results
6.1 Comparing with prior art
Naturalness. Table 1 compares the audio quality
of the TTS models listed in Section 5.1. It can be
seen that the proposed model achieves affective
control, without drop in perceived audio quality.
In contrast, previous SOTA emotive models ( Cai
et al. (2021) and FastSpeech2 + EV A) achieve con-
trol over emotion at the cost of naturalness (MOS
of3.08and3.01respectively). This result demon-
strates the efficacy of using DS module over EV A
and validates its ability to disentangle affective fea-
tures from the acoustic ones. The MOS score of
FastSpeech2 πimproves with addition of DS, as
some samples appear more natural when rendered
in intended emotions.
Coarse affective control. Results correspond-
ing to emotion detection are presented in Table 1.
For each sample, the raters were asked to choose
one among the four discrete emotions. On an av-342erage, the FastSpeech2 π+ DS gives best results,
outperforming the other models by a significant
margin. We observe about 17and25.5improve-
ment in percentage points (pp) over FastSpeech2 +
EV A and (Cai et al., 2021) respectively. Figure 4
shows the confusion matrix for this survey. Our
models are better at differentiating positive valence
emotions from the negative ones. There is still a
scope of improvement in distinctly expressing low
valence emotions.
Finer affective control. When asked raters to
pick the sample from a pair that expresses a par-
ticular emotion better, 85% of the times they were
able to pick the sample that was actually rendered
with a higher arousal value (Table 1). Our best
performing model scores 3.8pp over FastSpeech2
+ EV A and 5.0pp over (Cai et al., 2021).
Efficacy of DS. To further validate the effi-
cacy of DS (over the EV A), we present evalua-
tions to show that the performance gains occur pri-
marily due to the DS module and not the other
interventions. We observe that adding ‘domi-
nance’ to Fastspeech2 + EV A does not improve its
MOS and affective controllability as shown in Ta-
ble 1. Furthermore, we observe performance drop
on Fastspeech2 π+ EV A when compared against
Fastspeech2 π+ DS when both have their back-
bones trained on Blizzard dataset (Table 1). The
lack of improvement from (Sivaprasad et al., 2021)
further highlights that the performance gains by our
model does not come from the choice of dataset on
which the backbone is trained. Overall, the two ex-
periments conclusively show that DS module is the
decisive component that brings the improvements
in naturalness and controllability to the proposed
TTS system.
6.2 Emotional consistency in dialogues
As described in Section 5.2, we evaluate the emo-
tional consistency of a dialogue when a TTS model
replaces an actor in excerpts from a play. Figure 5
shows that emotive models bring significant im-
provement in emphatic quality of conversations and
are picked 80% of the times as the first preference.
This result reiterates the hypothesis (Wang et al.,
2018) that prosody averaging as in non-emotive
TTS is insufficient for emulating emotionally con-
sistent conversations.
Another important observation is how emphatic
quality measured as user’s first preference falls
from 52% to 27% in moving away from hand-
picked to model-predicted emotions. This suggests
a scope for improvement for emotion prediction
models. Nonetheless the results present clear evi-
dence that tying together emotion prediction mod-
els to expressive TTS is significantly more prefer-
able to a non-emotive TTS.
This proposed evaluation methodology is more
comprehensive and enables assessment of a con-
solidated conversational system as required in ex-
pressive HCI that includes various moving parts
like causal emotion recognition in conversation
and expressive TTS. This is not feasible with the
traditional approach of evaluating on individual
sentences drawn from distinct contexts. We argue
that this evaluation with contextual dialogues from
a conversation is more coherent to humans as re-
flected in inter-annotator agreement measured by
Fleiss’s Kappa Score (FKS). FKS goes up by 34%
from 0.43 in traditional coarse affective control (Ta-
ble 1) to 0.58 for our evaluation strategy (Figure 5).
We hope this will be useful in a more thorough
evaluation of expressive HCI systems.
6.3 Conversation with Meisner trained actor
As mentioned in Section 5.3, we gather the be-
havioural response of a Meisner trained human
actor to TTS systems (emotive and non-emotive)
and compare it against his/her reference response
to another human actor. We use Pearson’s corre-
lation ρwith reference for valence and compare
mean-std (µ, σ)for arousal values.
When the conversation was triggered with
a positive initial emotion, we had a high
ρ(FastSpeech2 π+DS, human) of 0.702 for our
model compared to negative correlation for non-
emotive TTS at ρ(FastSpeech2 π, human) of343−0.282. Similarly for a negative initial emotion
ρ(FastSpeech2 π+DS, human) was high 0.838 rela-
tive to low ρ(FastSpeech2 π, human) of 0.158.
We find that the average arousal for the human
response to our TTS ( µ=3.5,σ=1.06) is comparable
to a human-human conversation ( µ=3.94, σ=0.97),
as opposed to the response to a non-emotive TTS
(µ=2.55, σ=0.49). This indicates that the range of
arousal response elicited from a human actor by our
TTS is comparable to a human-human conversation
as opposed to that of a prosody unaware TTS.
We also interviewed the human actor about the
experience of conversing with the TTS systems.
He reported that our TTS gave him "an emotional
structure". He felt that the TTS could "dictate the
neutral part of the script to change it". He could "re-
member specific utterances" by our TTS and their
emotional content which "drove him" to respond
in an emotional manner. In contrast, he reported
that the prosody unaware TTS gave "dry answers",
made him feel that it was "disinterested", "auto
generated" and "did not evoke excitement". He
expressed that he "could not have a longer conver-
sation with it".
7 Conclusion
This work presents a novel method that leverages
prosodic features (pitch, energy and duration) to
modify emotions in the output of a TTS system.
Our method is model agnostic and can be used with
any TTS backbone that predicts prosodic features
in an intermediate step. This method outperforms
existing approaches by a significant margin in its
ability to accurately render desired emotions, while
preserving the naturalness of speech. We curated
theatre conversation data to evaluate and show that
our prosody-aware TTS better maintains the natu-
ral flow of emotions in conversations. Our work
shows promise in consolidation of prosodic emo-
tional recognition and expression, a coveted pursuit
in the field of HCI. We present further qualitative
experiments involving professional theatre artists
and demonstrate that the proposed TTS method
leads to more human-like conversations. While
exposing valence, arousal and dominance values as
model levers improves control over the final ren-
dering, in reality it is overwhelming for the user to
choose them correctly for a desired output. This
is further aggravated by the fact that some sen-
tences cannot be suitably spoken with a chosen
set of values, degrading output quality. These arelimitations that need to be addressed and appropri-
ately deriving these values from semantics of text
input or reference clips could be relevant future
directions. Affective control is incomplete without
explicit levers on the intonations, which is another
limitation to be looked upon in the future work.
8 Ethical concerns
This work shares the same concerns as with oth-
ers in the domain of TTS systems as discussed
by Habib et al. (2019). With TTS outputs getting
closer to actual human speech, there could be a
potential misuse. The threat of abuse of fake voices
is particularly high with similar developments in
conjugate areas like computer vision. However, the
benefits of improvements to emotive TTS technol-
ogy could significantly benefit HCI and the cor-
responding applications to problems in healthcare
and other domains. Example applications include
healthcare dialogue systems, improving social in-
teraction skills in people with autism and augmen-
tative communication devices. TTS systems syn-
thesizing speech with empathy can ease machine
interaction in many touchpoint applications. While
the benefits seem to outweigh the concerns at this
point, we believe the research community should
proactively continue to identify methods for detec-
tion and prevention of misuse.
References344345346347