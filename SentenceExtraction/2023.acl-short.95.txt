
Panuthep Tasawong, Wuttikorn Ponwitayarat, Peerat Limkonchotiwat,
Can Udomcharoenchaikit,Ekapol Chuangsuwanich,Sarana NutanongSchool of Information Science and Technology, VISTEC, ThailandDepartment of Computer Engineering, Chulalongkorn University, Thailand
{panuthep.t_s20,wuttikorn.p_s22,peerat.l_s19
,canu_pro,snutanon}@vistec.ac.th,
ekapolc@cp.eng.chula.ac.th
Abstract
Dense retrieval is a basic building block of in-
formation retrieval applications. One of the
main challenges of dense retrieval in real-world
settings is the handling of queries contain-
ing misspelled words. A popular approach
for handling misspelled queries is minimizing
the representations discrepancy between mis-
spelled queries and their pristine ones. Un-
like the existing approaches, which only fo-
cus on the alignment between misspelled and
pristine queries, our method also improves the
contrast between each misspelled query and
its surrounding queries. To assess the effec-
tiveness of our proposed method, we compare
it against the existing competitors using two
benchmark datasets and two base encoders.
Our method outperforms the competitors in
all cases with misspelled queries. Our code
and models are available at .
1 Introduction
Dense retrieval is a fundamental component in
many information retrieval applications, such as
open-domain question answering and ad-hoc re-
trieval. The objective is to score and rank a large
collection of candidate passages based on their sim-
ilarity to a given query. The performance of dense
retrieval relies on representation learning. A popu-
lar approach is to finetune a pre-trained language
model to create an embedding space that puts each
query closer to its corresponding passages (Zhan
et al., 2020; Khattab and Zaharia, 2020; Xiong
et al., 2021; Qu et al., 2021; Ren et al., 2021a,b).
One of the major challenges of dense retrieval
is the handling of misspelled queries which in-
duces representations of the misspelled queries to
be closer to irrelevant passages than their corre-
sponding passages. Several studies have demon-
strated that misspellings in search queries can sub-
stantially degrade retrieval performance (Zhuang
and Zuccon, 2021; Penha et al., 2022), specificallywhen informative terms, such as entity mentions,
are misspelled (Sidiropoulos and Kanoulas, 2022).
To create a retrieval model that is capable of
handling misspelled queries, researchers have pro-
posed different training methods to align repre-
sentations of misspelled queries with their pris-
tine ones. Zhuang and Zuccon (2021, 2022) de-
vise augmentation methods to generate misspelled
queries and propose training methods, Typos-aware
Training and Self-Teaching (ST), to encourage con-
sistency between outputs of misspelled queries
and their non-misspelled counterparts. Alterna-
tively, Sidiropoulos and Kanoulas (2022) apply
contrastive loss to enforce representations of mis-
spelled queries to be closer to their corresponding
non-misspelled queries. Although these methods
can improve the performance of retrieval models
for misspelled queries, there is still a substantial
performance drop for misspelled queries.
In this paper, we propose a training method to
improve dense retrieval for handling misspelled
queries based on the following desired properties:
•Alignment : the method should be able to align
queries with their corresponding passages.
•Robustness : the method should be able to align
misspelled queries with their pristine queries.
•Contrast : the method should be able to sepa-
rate queries that refer to different passages and
passages that correspond to different queries.
In contrast to the existing methods for handling
misspelled queries that only satisfy the Alignment
andRobustness properties, our method also aims
to satisfy the Contrast property. Increasing the
distance between dissimilar queries should help
distinguish misspelled queries from other distinct
queries. We design the following components for
our training method: (i) Dual Self-Teaching (DST)
incorporates the ideas of Dual Learning (Xia et al.,
2017; Li et al., 2021) and Self-Teaching (Zhuang
and Zuccon, 2022) to train robust dense retrieval
in a bidirectional manner: passage retrieval and1106query retrieval. (ii) Query Augmentation generates
a numerous number of misspelling variations for
each query to supply our training objective.
Experimental studies were conducted to assess
the efficiency of the proposed method in compar-
ison to existing approaches. We conduct experi-
ments based on two different pre-trained language
models. We evaluate using two passage retrieval
benchmark datasets, a standard one and a special-
ized one for misspellings robustness evaluation.
For each dataset, we measure performance on both
misspelled and non-misspelled queries, where the
misspelled queries are both generated and real-
world queries. The experimental results show that
the proposed method outperforms the best exist-
ing methods for enhancing the robustness of dense
retrieval against misspellings without sacrificing
performance for non-misspelled queries.
We summarize our contributions as follows:
•We propose a novel training method to enhance
the robustness of dense retrieval against mis-
spellings by incorporating three desired proper-
ties: Alignment ,Robustness , and Contrast .
•We introduce Dual Self-Teaching (DST) which
adopts the idea of Dual Learning and Self-
Teaching to learn robust representations. In ad-
dition, we propose Query Augmentation to gen-
erate multiple views of a particular query under
different misspelling scenarios.
•We evaluate our method on misspelled and non-
misspelled queries from two passage retrieval
datasets. The results show that our method out-
performs the previous state-of-the-art methods
by a significant margin on misspelled queries.
2 Methodology
We propose a training pipeline to enhance the dense
retrieval capability for handling spelling variations
and mistakes in queries. As shown in Figure 1, the
training pipeline comprises three steps. (i) Query
Augmentation : we augment each query in the train-
ing set into multiple misspelled queries using the
typo generators provided by Zhuang and Zuccon
(2021). (ii) Similarity Score Calculation : we com-
pute similarity score distributions between queries
and passages for passage retrieval and query re-
trieval tasks using in-batch negative queries and
passages, with additional hard negative passages.
(iii)Dual Self-Teaching Loss Calculation : we com-
pute the DST loss using the similarity score distri-
butions to achieve all three desired properties.2.1 Query Augmentation
The purpose of this step is to guide the learn-
ing with a broad array of possible misspelling
patterns. Let Qdenote a set {q, q, ..., q}
ofNqueries. From all queries in Q, we
generate a set of K×Nmisspelled queries
Q={⟨q, q, ..., q⟩}, where Kis the
misspelling variations. We use five typo generators
proposed by Zhuang and Zuccon (2021), including:
RandInsert, RandDelete, RandSub, SwapNeighbor,
and SwapAdjacent. Please refer to Appendix A.2
for examples of the misspelled queries.
2.2 Similarity Score Calculation
LetS(a,B)denote a function that computes a sim-
ilarity score distribution of any vector aover any
set of vectors B:
S(a,B) =/braceleftigg
b∈B/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleexp(a·b)/summationtextexp(a·b)/bracerightigg
(1)
GivenP={p, p, ..., p}to be a set of Mpas-
sages and Q={q, q, ..., q}to be the k
set of misspelled queries in Q, we compute two
groups of score distributions as follows:
•Passage retrieval : we calculate score distribu-
tions in a query-to-passages direction for each
original query s=S(q,P)and misspelled
query s=S(q,P).
•Query retrieval : we calculate score distribu-
tions in a passage-to-queries direction for orig-
inal queries s=S(p,Q)and each set of mis-
spelled queries s=S(p,Q).
This way, we produce four different score distribu-
tions ( s,s,s,s) for our training objective.
2.3 Dual Self-Teaching Loss Calculation
We design the Dual Self-Teaching loss (L) to
capture the three desired properties: Alignment ,
Robustness , and Contrast .
L= (1−β)L/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright+ βL/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright(2)
Dual Cross-Entropy loss (L) satisfies the
Alignment andContrast properties by utilizing
cross-entropy losses to learn score distributions of
the original queries for passage retrieval ( s) and
query retrieval ( s) given labels yandy.
L= (1−γ)L(s, y)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright+γL(s, y)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(3)1107
Minimizing the Lterm will increase the sim-
ilarity scores between queries and their relevant
passages to be higher than other irrelevant passages
by separating the relevant and irrelevant passages
from one another. Minimizing the Lterm will
increase the similarity scores between passages
and their relevant queries to be higher than other
irrelevant queries by separating the relevant and
irrelevant queries from one another. In this man-
ner, minimizing one of the two terms will align
queries with their corresponding passages, satisfy-
ing the Alignment property. Moreover, minimizing
both terms will separate queries that refer to differ-
ent passages and passages that belong to different
queries, satisfying the Contrast property.
Dual KL-Divergence loss (L) aims to ful-
fill the Robustness property by using KL losses
to match score distributions of misspelled queries
{s, s, ..., s}and{s, s, ..., s}to the
score distributions of the original query sands.
L=1
K/summationdisplay(1−σ)L(s, s)
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
+ σL(s, s)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright(4)
Minimizing LandLwill reduce the dis-
crepancy between misspelled and non-misspelled
queries for both query-to-passages and passage-to-
queries score distributions. This way, we implicitly
align representations of the misspelled queries to
the original queries, satisfying the Robustness prop-
erty. To stabilize training, we apply stop-gradient
to the score distributions of the original queries ( s
ands) in the L. The β,γ, andσare the balanc-
ing coefficients selected by hyper-parameter tuningon a development set. With this loss combination,
we achieve all three desired properties.
3 Experimental Settings
3.1 Training Details
We experiment on two pre-trained language mod-
els, BERT (Devlin et al., 2019) and Character-
BERT (El Boukkouri et al., 2020). We train
models only on the training set of MS MARCO
dataset (Nguyen et al., 2016). Moreover, the train-
ing data provided by the Tevatron toolkit (Gao et al.,
2022) also contains hard negative passages. We in-
clude the training set details and hyper-parameter
settings in Appendix A.1.
3.2 Competitive Methods
To show the effectiveness of our method, we com-
pare our work with the following baseline and com-
petitive training methods.
•DPR (Karpukhin et al., 2020) is a baseline train-
ing method that trains dense retrieval merely on
non-misspelled queries using Lloss.
•DPR+Aug (Zhuang and Zuccon, 2021) is the
Typos-aware Training method which trains dense
retrieval on both misspelled and non-misspelled
queries using Lloss.
•DPR+Aug+CL (Sidiropoulos and Kanoulas,
2022) employs additional contrastive loss to train
the misspelled queries.
•DPR+ST (Zhuang and Zuccon, 2022) is the Self-
Teaching method that trains dense retrieval on
both misspelled and non-misspelled queries us-
ingLandLlosses.
Note that their query augmentation method is iden-
tical to the Query Augmentation with K=1. We
retrain all models using the same setting described
in the previous section.1108
3.3 Dataset and Evaluation
Datasets. We evaluate the effectiveness of DST on
two passage retrieval datasets, MS MARCO and
DL-typo (Zhuang and Zuccon, 2022), each with
misspelled and non-misspelled queries. There are
8.8 million candidate passages for both datasets.
The development set of MS MARCO contains
6,980 non-misspelled queries. To obtain misspelled
queries, we use the typos generator method pro-
posed by Zhuang and Zuccon (2021) to generate 10
misspelled variations for each original query. The
DL-typo provides 60 real-world misspelled queries
and 60 corresponding non-misspelled queries that
are corrected manually.
Evaluation. We use the standard metrics originally
used by each dataset’s creators. For MS MARCO,
each misspelled query performance is the average
of 10 measurements. We employ Ranx evaluation
library (Bassani, 2022) to measure performance
and statistical significance. Specifically, we use a
two-tailed paired t-test with Bonferroni correction
to measure the statistical significance (p <0.05).
4 Experimental Results
4.1 Main Results
As shown in Table 1, the results indicate that DST
outperforms competitive methods for misspelled
queries in every case without sacrificing perfor-
mance for non-misspelled queries in eight out of
ten cases. We observe some performance trade-offs
for the BERT-based model in the DL-typo dataset’s
non-misspelling scores (nDCG@10 and MRR).
Aside from that, there is no performance trade-off
for the CharacterBERT-based model. These out-
comes conform with the observation in Figure 2
(Section 4.4) that DST improves the Robustness
andContrast of misspelled queries.4.2 Query Augmentation Size Study
To study the benefit of query augmentation and find
the optimal augmentation size, we measure the per-
formance of BERT-based dense retrieval models
trained with DST using the query augmentation
sizeKof 1, 10, 20, 40, and 60. Note that the query
augmentation method used in previous works is a
special case of Query Augmentation when K=1.
We report the results using MRR@10 for the devel-
opment set of the MS MARCO dataset. We also
report training time to show trade-offs between
performance and computation.
As shown in Table 2, the results indicate that
increasing Kimproves the performance of both
misspelled and non-misspelled queries, but only
up to a certain point, after which the performance
begins to decline. We observe that setting K=40
produces the best results, and there is no further
performance improvement after this point.
4.3 Loss Ablation Study
In this experiment, we study the benefit of each
term in DST by training BERT-based dense re-
trieval models on variant loss combinations with
K=40. The results in Table 3 reveal that L
andLterms positively contribute to the perfor-
mance of misspelled and non-misspelled queries,
with the Lbeing more significant. The L
term is crucial for retrieval performance, whereas
theLterm indirectly improves the performance1109
of misspelled queries by separating their pristine
queries from the surrounding queries. Disabling
query retrieval terms ( LandL) greatly re-
duces performances for misspelled queries. The
passage retrieval terms ( LandL) are indis-
pensable and cannot be substituted.
4.4 Query Distributions
The purpose of this section is to study the impact
of our training method on the Robustness andCon-
trast of misspelled queries. We also compare our
method against the baseline and competitive meth-
ods to show its effectiveness. The Robustness and
Contrast of misspelled queries are illustrated using
the following kernel density graphs:
•Original-to-Misspell: the cosine similarity distri-
bution between original and misspelled queries.
•Original-to-Neighbor: the cosine similarity dis-
tribution between original and neighbor queries.
The Robustness property is emphasized by the
Original-to-Misspell distribution having high co-
sine similarity. On the other hand, the Con-
trast property is emphasized by the small over-
lapping between Original-to-Misspell and Original-
to-Neighbor distributions. The results in Figure 2
show that our method (c) produces the best Robust-
ness andContrast properties for misspelled queries
in comparison to other methods.
5 Conclusion
This paper aims to address the misspelling prob-
lem in dense retrieval. We formulate three desired
properties for making dense retrieval robust to mis-
spellings: Alignment ,Robustness , and Contrast .
Unlike previous methods, which only focus on the
Alignment andRobustness properties, our method
considers all the desired properties. The empirical
results show that our method performs best against
misspelled queries, revealing the importance of the
Contrast property for handling misspellings.
6 Limitations
We list the limitations of our work as follows:
•The Query Augmentation is designed for the En-
glish alphabet; therefore, other languages with
different alphabets will require further work.
•Since the training strategy relies on fine-tuning
a pre-trained language model using a large pas-
sage retrieval dataset, it may not be suitable for
languages with limited resources1110References11111112A Appendix
A.1 Training Setup and Hyperparameters
The MS MARCO is a large-scale English language
dataset for machine reading comprehension (MRC).
The dataset consists of anonymized queries sam-
pled from Bing’s search query logs, each with hu-
man generated answers. The training set we used
contains 400,782 training samples, each consisting
of a query, positive passage, and a set of hard nega-
tive passages, which we randomly select 7 hard neg-
ative passages for each training sample. We set a
batch size to 16 and use in-batch negative sampling
for each training sample. Therefore, we obtain 7
+ 8 * 15 = 127 negative passages for each training
sample. We use the AdamW optimizer and learn-
ing rate of 1e−5for 150,000 steps with a linear
learning rate warm-up over the first 10,000 steps
and a linear learning rate decay over the rest of the
training steps. For our training method, we set the
hyper-parameters β=0.5,γ=0.5,σ=0.2, and the
query augmentation size K=40. Using one V100
32G GPU, the BERT-based model training time is
around 31 hours, while the CharacterBERT-based
model training time is roughly 56 hours.
A.2 Query Augmentation Examples
Table 4 provides examples of misspelled queries
generated by the Query Augmentation for each
original query.A.3 Licenses
Datasets : The MS MARCO dataset is available
under the MIT license, and the DL-typo dataset
is available under the Apache license 2.0. These
licenses allow users to use the datasets under non-
restrictive agreements.
Softwares : We employ Hugging Face (Wolf et al.,
2020) and Tevatron (Gao et al., 2022) libraries
to train dense retrieval models. We utilize Ranx
library (Bassani, 2022) to evaluate retrieval per-
formance. These libraries are available under the
Apache license 2.0 which allows both academic
and commercial usages. For this reason, we release
our code under the Apache license 2.0 to make our
code fully accessible and compatible with the other
codes we use.1113ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 6
/squareA2. Did you discuss any potential risks of your work?
There is no potential risk associated with increasing the robustness of information retrieval applica-
tions to question containing misspellings.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
We use Grammarly to check grammatical errors and QuillBot to polish writing quality. These tools
are applied to a certain number of sentences in each section, which are then reviewed by humans.
B/squareDid you use or create scientiﬁc artifacts?
Section 3.1 for pre-trained language models, training dataset, and training toolkit. Section 3.2 for
competitive methods. Section 3.3 for evaluation datasets and evaluation toolkit.
/squareB1. Did you cite the creators of artifacts you used?
Section 3.1 for pre-trained language models, training dataset, and training toolkit. Section 3.2 for
competitive methods. Section 3.3 for evaluation datasets and evaluation toolkit.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Appendix A.3
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Appendix A.3
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
We did not collect any data. The datasets we used are publicly available and widely used in
information retrieval literature. The data is already anonymized by the creators of the datasets.
Therefore we do not need to anonymize the data.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Appendix A.1
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 3.3 for the evaluation set Appendix A.1 for the training set1114C/squareDid you run computational experiments?
Section 4
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Appendix A.1
/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix A.1
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 4.1 for Main Results Section 4.2 for Query Augmentation Size Study Section 4.3 for Loss
Ablation Study Section 4.4 for Query Distributions
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Our evaluation is parameter free, therefore there is no parameter settings.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.1115