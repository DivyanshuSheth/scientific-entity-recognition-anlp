
Xiang Zhou Shiyue Zhang Mohit Bansal
Department of Computer Science
University of North Carolina at Chapel Hill
{xzh, shiyue, mbansal}@cs.unc.edu
Abstract
Previous Part-Of-Speech (POS) induction
models usually assume certain independence
assumptions (e.g., Markov, unidirectional, lo-
cal dependency) that do not hold in real lan-
guages. For example, the subject-verb agree-
ment can be both long-term and bidirectional.
To facilitate ﬂexible dependency modeling,
we propose a Masked Part-of-Speech Model
(MPoSM), inspired by the recent success of
Masked Language Models (MLM). MPoSM
can model arbitrary tag dependency and per-
form POS induction through the objective of
masked POS reconstruction. We achieve com-
petitive results on both the English Penn WSJ
dataset as well as the universal treebank con-
taining 10 diverse languages. Though model-
ing the long-term dependency should ideally
help this task, our ablation study shows mixed
trends in different languages. To better un-
derstand this phenomenon, we design a novel
synthetic experiment that can speciﬁcally diag-
nose the model’s ability to learn tag agreement.
Surprisingly, we ﬁnd that even strong base-
lines fail to solve this problem consistently in
a very simpliﬁed setting: the agreement be-
tween adjacent words. Nonetheless, MPoSM
achieves overall better performance. Lastly,
we conduct a detailed error analysis to shed
light on other remaining challenges.
1 Introduction
Unsupervised Part-Of-Speech (POS) tagging is the
task of discovering POS tags from text without
any supervision. These unsupervised syntax induc-
tion approaches can reduce the effort needed for
collecting expensive syntactic annotation, and can
bring us insights about what inductive bias leads
to the emergence of syntax. Recent POS induction
models have made great progress using different
frameworks (Christodoulopoulos et al., 2010; Berg-
Kirkpatrick et al., 2010; He et al., 2018; StratosFigure 1: Two long-term tag dependency examples in
English.
et al., 2016; Shi et al., 2020). However, most of
them assume certain independence assumptions
among POS tags, e.g., Markov (Merialdo, 1994;
Berg-Kirkpatrick et al., 2010; Ammar et al., 2014;
He et al., 2018), unidirectional (Tran et al., 2016),
local dependency (Stratos et al., 2016; Gupta et al.,
2022), etc. On the contrary, complex and long-term
dependency appear in many real languages and
plays an important role in deﬁning the POS tags.
For example, in Figure 1, the VBP tag of areand
the NNS tag of areas depend on each other, and so
do the VBZ tag of isand the NN tag of news .So in
this case, models only conditioning the immediate
preceding tag (Markov) or 1-2 neighboring words
(local) cannot explain the distinction between NNS
and NN, or between VBZ and VBP. While unidi-
rectional (e.g., using a unidirectional LSTM (Tran
et al., 2016)) models are in theory capable of mod-
eling long-term dependency through optimizing the
joint probability of tags, bidirectional architectures
still show clear advantage in language modeling
literature (Bahdanau et al., 2015; Vaswani et al.,
2017; Devlin et al., 2019).
In this work, we present a novel framework for
POS induction that is capable of modeling arbitrary
long-term bidirectional dependencies: Masked
Part-Of-Speech Model ( MPoSM ), inspired by re-
cent success of Masked Language Models (MLM)
(Devlin et al., 2019). Speciﬁcally, MPoSM con-
sists of two modules (see Figure 2): a local POS1099prediction module that maps each word to its POS
tag and a masked POS reconstruction module that
masks a certain portion of tags produced from the
previous step, and then learns to ﬁrst predict the
masked tags as latent variables and then reconstruct
the corresponding words. We use a bidirectional
LSTM (Bi-LSTM) to predict the mask tags con-
ditioned on the remaining tags, which grants our
model the ability to model complex long-term and
bidirectional dependencies among tags. Through
the training signal back-propagated from this mod-
ule, the tags predicted from the local POS predic-
tionmodule will also be encouraged to have global
inter-dependency, which leads to better tags. Since
we do not have gold POS tags, at the masked po-
sitions, we marginalize over all possible tags and
optimize word reconstruction probabilities. Intu-
itively, the correct induction of POS tags is beneﬁ-
cial for the prediction of the correct masked words.
For example, in Figure 1, if we mask the second
positions of the two sentences (corresponding to
areas andnews ), inducing two different tags (i.e.,
NNS for areas and NN for news ) correctly will
make the word prediction easier than inducing the
same tag. From a probabilistic view, our model
is conceptually similar to approximately modeling
the probability of generating the whole sentence
from latent tags using masked loss as a surrogate.
MPoSM achieves competitive performance on
both the 45-tag English Penn WSJ dataset (Mar-
cus et al., 1993) and the 12-tag universal tree-
bank (McDonald et al., 2013) containing 10 di-
verse different languages. It achieves comparable
oracle M-1 compared to the SOTA (state-of-the-
art) models (Stratos, 2019; Gupta et al., 2022) on
Penn WSJ dataset and achieves higher performance
than Stratos (2019) on 4 out of 10 languages on the
universal treebank. We also show that substantial
improvements can be made with the help of con-
textualized representations in mBERT, similar to
Gupta et al. (2022). We conduct an ablation study
on multiple languages by replacing the Bi-LSTM
architecture with a window-based MLP that models
the local dependency of tags. Surprisingly, while
modeling the full-sentence context can improve the
performance of English and German, modeling lo-
cal context is better for Indonesian and Korean. Our
mutual information analysis indicates that this dif-
ference may be resulted from the different degrees
of gold-tag dependency of different languages.
Since real-life datasets can contain many con-founding factors, we next design a suite of well-
controlled synthetic experiments from the angle
of agreement-learning to examine how well the
model can take advantage of the long-term depen-
dency. Our synthetic datasets can guarantee enough
training signals for the model to capture the agree-
ments. However, we show that all current models
fail to consistently solve the agreement learning
problems, even with the most basic agreement hap-
pening between adjacent words. Nonetheless, our
model shows the best performance with the highest
percentage of solving these problems in multiple
runs. We conjecture that this is relate to the general
optimization problems of latent variable inference
problems (Jin et al., 2016) (see more discussions
in Section 7). Such obstacles prevent models from
gaining additional beneﬁts from modeling long-
term dependency. Finally, we did error analysis on
the predicted clusters for English and Portuguese
and identify remaining challenges both from imper-
fect modeling and lack of data diversity.
In summary, our main contributions are: (1)
a novel POS induction architecture with MLM-
inspired loss that allows learning arbitrary tag
dependencies and reaches close-to-SOTA perfor-
mance; and (2) examining the effectiveness of us-
ing long-term context and providing a suite of syn-
thetic datasets to expose the challenges in agree-
ment learning and pointing out future challenges.
2 Background
POS Induction. A POS tag is a category of
words that share the same grammatical property.
A simpliﬁed form of these tags will involve com-
monly known categories such as nouns, verbs, etc.
Formally, given a sentence with lwords x=
{x}, the corresponding POS tags z={z},
then the goal of the POS induction task is to infer
zfromxwithout supervision from gold tags.
Limitations of Existing POS Induction Models.
From the perspective of probabilistic graphical
models, POS tags can be viewed as latent vari-
ables related to all the observed words. Each tag z
is a latent variable that generates the corresponding
wordx. Hence, inducing the POS tag sequence
becomes the problem of performing MAP infer-
ence of the latent variables. This is a popular and
effective view adopted by many previous works.
To make such inference tractable, previous works
have to add certain assumptions, including adding
Markov assumption to the latent variables z(i.e.,1100the current tag only depends on the immediate pre-
vious tag) (Merialdo, 1994; Berg-Kirkpatrick et al.,
2010; Ammar et al., 2014), only considering local
dependencies (Stratos, 2019; Gupta et al., 2022),
unidirectional dependencies (Tran et al., 2016), etc.
However, dependencies in real language are not
constrained by length or direction, as we discussed
in Sec. 1. Hence, simplifying and ruling the capa-
bility out in the model design is suboptimal. To
mitigate this problem, in Sec. 3, we will describe
our approach to model long-term dependency.
Why are the Learned Latent Variables Corre-
lated with POS-Tags? Before introducing our
method, we discuss why latent variable models can
induce POS-tags well. Take the vanilla HMM as
an example, the latent variables in the model can
be viewed as being optimized towards two objec-
tives: the transition probability p(z|z)and the
emission probability p(x|z). They characterize
two properties respectively: (1) strong ordering
dependencies among latent variables; and (2) the
strong correlation between latent variables and the
observed word. In short, the success of previous
latent variable models implies: A word’s inherent
category that has strong ordering constraints will
highly resemble the POS tag. In this work, we
follow this assumption, but propose a model that
is able to learn arbitrary bidirectional long-term
dependencies p(z|{z})instead ofp(z|z).
3 Masked Part-Of-Speech Model
Inspired by the recent success in masked language
modeling (MLM) (Devlin et al., 2019), we present
Masked Part-Of-Speech Model ( MPoSM ). Next,
we will ﬁrst describe the model architecture and
then introduce several useful additional techniques.
3.1 Model Architecture
As is shown in Figure 2, our model consists of
two parts: a local POS prediction module and a
masked POS reconstruction module. The local
POS prediction module predicts a POS tag for each
word, and the masked POS reconstruction module
encourages strong dependencies among these tags.
Local POS Prediction. Given the input word
sequence x={x}with length l, we ﬁrst
get the word embeddings. As morphological
features are shown to be useful for POS induc-
tion (Christodoulopoulos et al., 2010) to capture
inﬂection (e.g., the ‘-s’ sufﬁx for English plu-
rals), we follow Stratos (2019) to extract character-
level representations using a Bi-LSTM. We con-
catenate word embeddings and char representa-
tions to form the ﬁnal representations for each
word, w={w}. Then, we use a sin-
gle context-independent feed-forward network to
predict the POS tags zout of w, i.e.z=
arg max(Softmax(FF( w))). Essentially, this
module models P(z|x)for every position and
predicts the POS tag only conditioned on the word
itself without considering its context. We make
this design choice as POS tags are the syntactic
property of each individual word , so it should be
able to be predicted as an attribute of the word.
Importantly, in order to make the whole model
end-to-end differentiable, we replace the arg max
with a straight-through Gumbel-Softmax estima-
tor (Jang et al., 2017; Maddison et al., 2017) (see
Appendix A for more details).
Masked POS Reconstruction. After we get all
the predicted POS tags z={z}for the previ-
ous module, we conduct masked POS reconstruc-
tion to encourage modeling strong dependencies
among z. Speciﬁcally, we follow Devlin et al.
(2019) to mask 15% of the predicted POS tags
and replace them with a placeholder MASK tag.
Then we map them into POS embeddings and use a
Bi-LSTM (Hochreiter and Schmidhuber, 1997) as
the dependency-modeling network.This grants
ﬂexibility of modeling the long-term and bidirec-
tional dependency among tags without any assump-1101tions and thus brings us an advantage over tradi-
tional HMM-based models. Then, we predict the
masked POS tags out of the contextualized rep-
resentations from the Bi-LSTM, so, essentially, it
modelsP(ˆz|C), where ˆzis the reconstructed tag
at positionjandC={z}is the context. We
treat the predicted ˆzas latent variables and maxi-
mize the probability of the corresponding word x,
which can be written out by marginalizing over ˆz:
P(x|C) =/summationdisplayP(x|ˆz)P(ˆz|C) (1)
The conditional probability P(x|ˆz)can be mod-
eled through another feed-forward network with the
POS embeddings as the input. Intuitively, predict-
ing theP(ˆz|C)objective encourages strong de-
pendency among the tags and predicting P(x|ˆz)
reinforces the connection between the words and
the tags. Hence, the total loss is the sum of all the
log-probabilities at masked positions:
L =/summationdisplaylogP(x|C)(2)
Importantly, the supervision from this module will
back-propagate to the local POS prediction mod-
ule. Therefore, even though it produces POS tags
independently, the supervision helps it to capture
the interdependency among all the tags.
During testing, we use the output of the local
prediction module as the output tags.
3.2 Additional Techniques
Below, we introduce several additional techniques
used in our model to achieve good performance.
Careful Initialization. Similar to many other un-
supervised learning models (Gimpel and Smith,
2012; Meila and Heckerman, 1998; He et al., 2018),
we found our model to be sensitive to initialization
in our preliminary experiments. Below, we propose
Masked Language Modeling Pretraining (MLMP ).
We use a two-stage training procedure: (1) we re-
move the modeling architecture for P(x|ˆz)and
P(ˆz|C), and directly apply an MLP to model
P(x|C)without explicitly predicting the masked
tag; (2) we initialize our MPoSM with the pre-
trained model in (1) and continue training with the
loss in Eqn. 2. This procedure trains the bottom
layers with a smoother objective and provides a bet-
ter starting point for optimizing the MPoSM loss.Besides, the MPoSM model can leverage knowl-
edge from both pretrained embeddings similar to
He et al. (2018) and Zhu et al. (2020), or pretrained
language models similar to Gupta et al. (2022).
Connecting P(x|z) and P(z|x). While the local
POS prediction module models P(z|x), themasked
POS reconstruction module has a part that mod-
elsP(x|z)(Eqn. 1). These two probabilities can
be connected using the Bayes’ rule: P(x|z) =. If we assume the training set is rep-
resentative enough of the language, we can approx-
imateP(x)by the word frequency in the dataset,
and then we can compute P(x|z)directly follow-
ing the Bayes’ rule instead of using a neural net-
work to model it. We notice that binding these two
probabilities can usually make the training more
stable and improve the performance when training
from scratch. Note that we do not adopt this change
when using pretrained word embeddings because
we can use the pretrained embedding weights at the
output layer (Press and Wolf, 2017), which brings
additional knowledge for the ﬁnal word prediction.
Dataset Rechunking. One potential problem of
using the full sentence context is the position bias
of POS tags . For example, since a large number of
English sentences start with the word ‘the‘, position
0 will have a strong bias towards predicting the
‘DT’ tag. In our experiments, we concatenate all the
sentences in the original dataset and re-chunk them
randomly. Then we combine the rechunked dataset
and the original dataset as our training set. In our
preliminary experiments, we ﬁnd it can improve
the stability and the performance of the model.
4 Connections to Related Works
The HMM-based POS induction model (Merialdo,
1994) has many extensions, including using hand-
engineered linguistic features (Berg-Kirkpatrick
et al., 2010), pretrained embeddings (Lin et al.,
2015), task-speciﬁc modiﬁcations (Blunsom and
Cohn, 2011; Stratos et al., 2016), ﬂow-based
transformations (He et al., 2018), external re-
sources (Haghighi and Klein, 2006; Snyder et al.,
2008; Das and Petrov, 2011), etc. They all optimize
the probability of the sequence, P(x). However,
it requires certain dependency assumptions to be
tractable. Our model instead optimizes the sum of
conditional word probabilities given the remaining
context/summationtextlogP(x|x,x), i.e., MLM
loss (Devlin et al., 2019). While being different1102
fromP(x), this objective is an effective surrogate
and makes modeling complex dependencies possi-
ble. There also exist some earlier methods that do
not require the Markov assumption. For example,
Abend et al. (2010) design a method to directly
cluster the embeddings containing distributional
and morphological information of the word, and
then identify prototype words to facilitate the ﬁ-
nal POS induction. Tran et al. (2016) propose a
neural HMM model. Similar to our model, it can
also model long-term dependency (due to the use
of LSTM), however, they still constrain the de-
pendence to be uni-directional (due to the HMM
nature). Our model does not have such constraints
and empirically works better.
Architecture-wise, our model is conceptually
similar to a denoising auto-encoder (Vincent et al.,
2008), where the masking step can be viewed as
adding noises to the POS tags. The idea of using
auto-encoder models for unsupervised learning has
been explored with CRFs in Ammar et al. (2014).
However, they still require Markov independence
assumption to make inference on CRF tractable,
while our model has the ability to model complex
long-term dependencies. Plus, we use an MLM-
inspired loss instead of reconstructing Brown clus-
ters (Brown et al., 1992) as Ammar et al. (2014).
Our model also provide additional insight on
the relation between MLM and syntax emergence.
Such connection has also been explored in previous
works. Pretrained transformers using MLM (De-
vlin et al., 2019; Clark et al., 2020; Raffel et al.,
2020) have shown strong syntactic abilities (Tenney
et al., 2019; Jawahar et al., 2019; Goldberg, 2019).
CBOW and skip-gram embeddings (Mikolov et al.,
2013) can be viewed as an MLM with a limited
context window (i.e., local context), and have been
shown to be useful for syntax induction, espe-
cially with small window sizes (Bansal et al., 2014;
Lin et al., 2015; He et al., 2018). Some recent
POS induction works explore CBOW-style objec-
tives (Stratos, 2019; Gupta et al., 2022). However,
using the sentence-level MLM objective for syntax
induction is under-explored. The only exception isthe recent work of Shen et al. (2021), which focuses
on a different task: unsupervised parsing. The dif-
ferent tasks lead to substantially different focuses
and designs in the architecture. They use MLM
with a dependency-constrained self-attention mech-
anism to extract parses, while we extend MLM
to the POS-tag level (MPoSM) and explicitly dis-
cretizes the latent variables to extract tags.
5 Experimental Setup
5.1 Datasets and Metrics
We evaluate our model on two datasets: the 45-tag
English Penn WSJ dataset (Marcus et al., 1993)
and the 12-tag universal treebank (McDonald et al.,
2013). Following Ammar et al. (2014) and Stratos
(2019), we use the v2.0 versioncontaining 10 dif-
ferent languages. Detailed statistics are in Table 1.
Following recent works (Stratos, 2019; Gupta
et al., 2022), we use the Many-to-One accuracy
(M-1) (Johnson, 2007) as our metric, and train and
evaluate our model on the whole dataset. Follow-
ing Shi et al. (2020), we distinguish between the
oracle performance that selects the model with the
best M-1 metric (M-1), and the fully unsuper-
vised performance that selects the model with the
best loss (M-1). However, many previous works
used different or unspeciﬁed model selection set-
tings. For a fair comparison, we get results under
our setting using their ofﬁcial code if possible.
5.2 Implementation Details
For the English Penn WSJ dataset, we use the pre-
trained embedding provided in He et al. (2018). For
the main results on the universal treebank, we do
not use any external resources and use MLMP ini-
tialization. Additionally, we also report the results
with mBERT contextualized representations on the
universal treebank following Gupta et al. (2022),
where they show mBERT representations empiri-
cally outperforms English BERT representations
on POS-tag induction. Same to the implementa-
tion in Gupta et al. (2022), we also use the average1103
representation over all the subwords and layers as
the representation for each word. For all models,
we train our model using Adam (Kingma and Ba,
2015) with an initial learning rate 0.001. The batch
size is set to 80. The Gumbel softmax temperature
is set to 2.0. The results on the Penn WSJ dataset
are the mean of 5 runs, and the results on the uni-
versal treebank are the mean of 3 runs (see more
details in Appendix C).
6 Results and Ablations
6.1 45-tag English Penn WSJ dataset.
The results are shown in Table 3. We reported
two variants: the MPoSM model that does not
use any external resource, and the MPoSM + emb
model that uses the same pretrained word embed-
dings as He et al. (2018). Using pretrained embed-
dings does provide substantial improvements to our
model. Overall, our model achieves competitive
performance compared to SOTA models (Stratos,
2019; Gupta et al., 2022), reaching 78.6 oracle
M-1. The oracle performance is 0.5 points higherthan the model in Stratos (2019) using a mutual
information-based loss. Our fully unsupervised
performance reaches 77.9 M-1, which is also sim-
ilar to SOTA models (Stratos, 2019; Gupta et al.,
2022), and is higher compared to previous models
using the same pretrained embeddings (He et al.,
2018) (75.6), models not using the Markov assump-
tion (Abend et al., 2010) (75.1) or models using
uni-directional long-term dependency (Tran et al.,
2016) (75.0). Concurrent to our work, Gupta et al.
(2022) achieve a higher M-1 of 79.5, but they use
more additional resources, including mBERT rep-
resentations and fastText (Joulin et al., 2017)
morphological features.
6.2 12-tag Multilingual Results on Universal
Treebank.
We also report results on all 10 languages on the
universal treebank in Table 2 (the full table with
standard deviations can be found in Table 6 of Ap-
pendix D). To make the settings practical to low-
resource languages, we do not use any pretrained
word embeddings on this dataset. Compared to the
SOTA model (Stratos, 2019) that also does not use
any external resources, our model achieves com-
petitive performance, outperforming it on 4 out of
10 languages (es, fr, it, pt-br). Together with the
English Penn WSJ dataset, we notice that MPoSM
perform well on most of the languages, but may un-
derperform the previous model on languages with
weaker tag-level dependency (e.g., ko and id, stat-
stics are in Table 1, more detailed analyses and dis-
cussions are in Appendix E and F) and on smaller
datasets (e.g., ko and sv).
Concurrently, Gupta et al. (2022) showed sub-
stantial improvement on the universal treebank by
leveraging knowledge in the pretrained mBERT
representations. Inspired by their success, we also
report the result using mBERT in MPoSM (as de-
noted by MPoSM + mBERT) in Table 2. Similarly,1104
using mBERT substantially improves MPoSM’s
performance on the universal treebank. While
on languages with weak tag-level dependency or
smaller datasets, MPoSM + mBERT still does not
perform most effectively (similar to the trend in
MPoSM), MPoSM + mBERT achieves substan-
tially higher results on most of the languages com-
pared to MPoSM, and achieves results higher than
Gupta et al. (2022) on French and Italian. On
average, Gupta et al. (2022) still achieves higher
results. This trend may imply that other factors
(e.g. the clustering methods used in Gupta et al.
(2022)) are important for their good performance.
We have also tried using mBERT on the English
WSJ dataset, but do not see a substantial improve-
ment. We leave how to combine their method with
MPoSM as a promising future direction.
6.3 How does modeling long context
inﬂuence the results?
Taking advantage of the ﬂexibility of our model,
we analyze whether modeling long-term context is
always better than modeling local context. We com-
pare two models: the MPoSM (full) model is the
default model described in Sec. 3, and the MPoSM
(width=2) model that replaces the Bi-LSTM net-
work with an MLP and only takes in the neighbor-
ing 4 predicted POS tags as the input (i.e., local con-
text). We test our model on 4 languages: English,
German, Indonesian, and Korean. These languages
are selected to have representative statistics among
all the languages in the universal treebank in terms
of dataset size and average word frequency (see
Table 1).The results are in Table 4. On English
and German, the default model is better than the
MPoSM (width=2) variant by 1.3 and 3.3 points
respectively. However, on Indonesian and Korean,
the trend is reversed with the MPoSM (width=2)
variant showing 0.6 and 1.4 point advantage re-
spectively. We notice that the languages do not
beneﬁt from using a longer context also correlates
well with the languages with weak tag-level depen-
dencies. Such property prevent the MPoSM from
beneﬁt from the advantage of dependence model-
ing on those languages, and consequently using
a longer context does not provide additional help.
More detailed analysis is in Appendix E.
7 Analysis and Challenges
7.1 Agreement Learning Experiments
Inducing good POS tags requires models to un-
derstand what “agreement” is. To match the gold
45-tag set of Penn Treebank, the model needs to dis-
tinguish between VBP (Verb, non-3rd person sin-
gular present) and VBZ (Verb, 3rd person singular
present) tags (see examples in Figure 1). Though
local morphological features do provide useful cues
for such classiﬁcation, models should achieve bet-
ter performance by observing the full picture of
agreement in the long context. From the results
in Sec. 6, we notice evaluation in real-life datasets
may contain many confounding factors. Hence,
we design a well-controlled synthetic dataset to
examine exactly how well the model learns these
agreements. Surprisingly, we ﬁnd that the limita-
tion of current models is not about leveraging long
context, but a common fundamental limitation on
using co-dependency to distinguish tags.
Controlled Dataset Design. To provide a sim-
pliﬁed and well-controlled environment, our syn-
thetic datasets consist of a small set of words and
tags, and simple sentences. Speciﬁcally, we use
6 different tags, with 5 unique words correspond
to each tag. Our 6 tags are named after nouns
(n1,n2), verbs ( v1,v2), and other unimportant
tokens ( o1,o2). In every sentence, n1always
appear before v1, and n2before v2, analogous
to subject-verb agreement in English. We create
the synthetic data by ﬁrst sampling a tag sequence
(illustrated in Figure 3) and then randomly select
words of each tag in the sequence. We also make
sure the two agreements ( n1-v1,n2-v2) have ex-1105
actly symmetric data. We use the “distance”, i.e.,
the number of tokens, between nandvto control
the agreement-learning difﬁculty. The larger the
distance is, the harder the example is. Therefore,
we create two subsets with different levels of difﬁ-
culty, and each contains 40,000 sentences. In the
ﬁrst simpler subset D(0), nandvare adjacent. In
the second harder subset D(2-4), nandvare sepa-
rated by 2-4 words. Complete illustrations, regexes
and additional results are in the Appendix H.
Surprising Difﬁculty of Learning Agreement.
Model performances on our synthetic datasets are
in Table 5. We report the mean M-1 of 20 runs and
the percentage of perfect runs (achieving 100 M-1),
as models are expected to consistently achieve the
perfect score if they really acquire agreement. We
include three variants of MPoSM using different
contexts (from the width=1 model only consider-
ing the immediate neighboring tokens to the full
model considering the whole sentence). We com-
pare with two representative baselines: the SOTA
model Stratos (2019) which uses the context with
width=2, and the neural HMM model (Tran et al.,
2016) which leverages unidirectional full context.
In Table 5, we ﬁrst notice the surprising difﬁculty
of learning agreement even in the simple D(0) set-
ting, where the nandvare already adjacent. None
of the models can consistently produce the per-
fect tags in this setting. The best results are from
MPoSM with a speciﬁc inductive bias of only us-
ing the width=1 local context, but it still fails to
achieve the perfect score consistently. Other mod-
els using a larger context show substantially lower
results. On the harder D(2-4) setting, we see sim-
ilar observations. Due to the architecture limita-
tion, none of the models using local context can
achieve the perfect score even once. Models using
the long context also fail to perform well consis-
tently. MPoSM (full) is the single best model that
successfully acquires agreement, albeit only 30%
of the time. These observations demonstrate the
difﬁculty of learning agreement in POS induction.
As reﬂected by the results on D(0), such difﬁculty
cannot be fully attributed to the long-term issue.
We suspect the latent variable-based loss function
used in current models can contain many bad local
minimums, similar to the optimization difﬁculty
observed in Gaussian Mixture Models (Jin et al.,
2016). Models are likely to stuck in one local min-
imum (e.g., viewing n1andn2as the same tag)
and never reach the global optimum.
Finally, we want to point out that our ﬁndings
are not contradictory with recent studies that show
the derivation of agreements from MLM-style mod-
els (Jawahar et al., 2019; Goldberg, 2019; Lin et al.,
2019). One key difference is that they directly mea-
sure the word-level agreement, e.g., areshould fol-
lowareas in Figure 1. However, POS induction fo-
cuses on the tag-level agreement, i.e., VBP should
follow NNS. Our MPoSM can also be viewed as
adding an explicit discretization step in a normal
MLM so that we can predict discrete tags. If we
remove the POS-factorization step in Eqn 1, and di-
rectly predict the word from the word context, our
model can also capture the word-level agreement.
7.2 Error Analysis of Predicted Clusters
In Table 3, we notice that performances of different
models are saturating around 78 M-1 on the English
Penn WSJ dataset. To examine the limitations of
current models point out future directions, we man-
ually investigated the clusters learned by our model.
Below, we list our main ﬁndings on English (see
similar ﬁndings of Portuguese in Appendix I):
The sizes of predicted clusters are more uni-
form than gold clusters. Only 1 predicted clus-
ter contains very few (less than 3000) words, while
the scale of gold clusters showing a much larger
variance, with 29% of the 45 clusters containing
less than 3000 words. A bar plot illustration is in
Figure 4. We can see that the size of gold clusters
has a much larger range than the predicted clusters.
Under the current losses, assigning a small number
of words to one tag is likely to make the loss worse,
but it hard to match the skewed distribution of nat-
ural tags. Johnson (2007) show similar ﬁndings on1106HMM models trained with EM. These consistent
ﬁndings may hint at a common limitation of current
objectives. Future work should explore different
objectives with more suitable inductive biases.
Agreements are not learned well. Similar to
the observation in Sec. 7.1, agreements are not
learned well in the predicted clusters. For exam-
ple, the VBP tag (Verb, non-3rd person singular
present) is an important tag in the subject-verb
agreement. While this tag has 15377 occurrences
in the gold annotations, it is not the major tag in
any of the predicted clusters. Most VBP words are
either mixed with the VB or the VBZ words. We
consistently observe models fail to separate these
verbs, showing a large room for improvement.
Difﬁculty in mapping one word to multiple tags.
Without using mBERT representations, MPoSM
(also applies to many other models, e.g., He et al.
(2018); Stratos (2019), etc.) predicts the same tag
for one word. However, the same word can have
different tags in different contexts. For example,
the word ‘that’ can have gold tags IN, RB, and
WDT. Future works should explore directions on
capturing the multi-sense phenomenon.
Dataset biases inﬂuence predicted clusters.
For example, the English WSJ dataset contains
many ﬁnancial news reports, so numerical words
(e.g., ‘million’, ‘billion’) and related symbols like
‘%’ are very frequent. Since these words always ap-
pear in a distinctive context, models will naturally
cluster these tokens together. Hence, we encourage
future research to explore more diverse datasets.
8 Conclusion
We propose MPoSM, a POS induction model in-
spired by MLM and can model complex long-term
dependencies between POS tags. Our model shows
competitive performance on both English and mul-
tilingual datasets. We analyze the effectiveness
of using long context compared to local context.
Finally, we use synthetic datasets and analyses to
point out remaining challenges.
9 Ethical Considerations
The model proposed in this paper is intended to
study how syntax emerge from unsupervised learn-
ing objectives. It can also help understand lan-
guages with limited annotations. However, as we
showed in this paper, the syntax predicted by cur-
rent models can contain errors and be inﬂuenced bythe choice of datasets, so the model’s output should
be used with caution and examined by experts. Our
model has been tested on 10 diverse languages. Our
ﬁndings on these languages should generalize to
languages with similar linguistic properties, but we
suggest careful empirical studies before applying
our approach to languages distant from those we
study in this paper.
Acknowledgments
We thank the reviewers for their helpful comments.
We thank Chao Lou for the help in data prepro-
cessing steps. This work was supported by ONR
Grant N00014-18-1-2871, NSF-CAREER Award
1846185, DARPA MCS Grant N66001-19-2-4031,
and a Bloomberg Data Science Ph.D. Fellowship.
The views are those of the authors and not of the
funding agency.
References110711081109
A Straight-Through Gubmbel-Softmax
Estimator
To allow the gradients from the masked POS con-
struction module to back-propagate to the local
POS prediction module, we replace the standard
argmax in the local POS prediction module with
thestraight-through Gumbel-Softmax estimator.
Speciﬁcally, we follow Jang et al. (2017); Mad-
dison et al. (2017) to calculate the one-hot POS-tag
vector using one_hot(softmax (g+logit,τ)).
In this equation, τis the softmax temperature and
gare i.i.d. samples drawn from a standard Gum-
bel distribution, i.e., g=−log(−log(U(0,1))),
whereU(0,1)is a uniform distribution over the
range [0,1]. Following Jang et al. (2017), we
usearg max to discretize the distribution to a one-
hot vector in the forward pass, but back-propagate
through the continuous Gumbel-softmax. With this
technique, the whole MPoSM becomes end-to-end
differentiable.
Alternatively, instead of operating on the single
POS embedding of the Gumbel-Softmax output,
we can also use the weighted sum of all the POS
embeddings with weight P(z|x). However, em-
pirically, we notice the weighted sum approachdoes not perform well when the number of tags is
large (e.g., 45 in the Penn WSJ dataset).
B Dataset Links
The universal treebank dataset is from https://
github.com/ryanmcd/uni-dep-tb . The
English Penn WSJ dataset can be obtained through
LDC.
C Implementation and Hyper-Parameter
Details
For initialization, for the English 45-tag Penn WSJ
dataset, we use the pretrained word embedding
provided in He et al. (2018). For the main results on
the universal treebank, we do not use any external
resources and always initialize our models using
MLM pretraining. Additionally, we also report the
results with mBERT contextualized representations
on the universal treebank following Gupta et al.
(2022). Same to the implementation in Gupta et al.
(2022), we also use the average representation over
all the subwords and layers as the representation
for each word. We apply the “connecting P(x|z)
and P(z|x)” technique for all our models not using
pretrained word embeddings or pretrained language
models. We apply the dataset rechunking technique
to all our experiments.
For the hyper-parameters, we train all our model
using Adam (Kingma and Ba, 2015) with an ini-
tial learning rate 0.001. The batch size is set to 80
and we decay the learning rate with a factor 0.1
the loss stagnates. We set the word embedding di-
mension to 100, POS embedding dimension to 200,
the character embedding dimension to 100, and the
hidden vector dimension to 128. We use one layer
of MLP in the local POS prediction module and
one layer of Bi-LSTM in the masked POS recon-
struction module. The masking rate is set to 15%
and the Gumbel softmax temperature is set to 2.0.
We set the dropout rate (Srivastava et al., 2014) to
0.5. Specially, with pretraining word embeddings,
we tie the input and output embeddings follow-
ing (Press and Wolf, 2017) and add one more layer
in the local POS prediction layer to more effectively
convert the pretrained embedding to POS tags fol-
lowing (He et al., 2018). And for our synthetic
experiments, since the vocabulary size is small, we
use a smaller character embedding dimension of 8.
We use the loss as the metric to judge if our model
has been converged. In this work, the results on the
Penn WSJ dataset are the mean of 5 different runs,1110
and the results on the universal treebank are the
mean of 3 different runs. The experiments using
mBERT can be run on a single RTX A6000 GPU,
and all other experiments can be conducted on a sin-
gle TITAN Xp GPU. The time of experiments can
take from several hours to several days, depending
on the size of the dataset and the models.
D Results and Standard Deviations on
the Universal Treebank
The full means and standard deviations (for our
model and for previous works that reported this
number) are shown in Table 6. We also include
the fully unsupervised performance (evaluating the
model with the best loss) in the MPoSM row. Our
fully unsupervised model is slightly worse than the
oracle version of both our model and Stratos (2019),
but show comparable or higher performance to
other results (Stratos et al., 2016; Berg-Kirkpatrick
et al., 2010; Brown et al., 1992).
E Analysis on the Dependency among
Gold Tags
In Sec. 6, we notice that MPoSM does not work
equally well on all the languages. For example,
in Table 4, we can see that out of 4 different lan-
guages, using full context instead of local contextonly improve 2 of them: English and German. In
this section, we provide evidence that these differ-
ent trends can result from the different strength of
dependencies among tags in different languages.
Assuming a tag sequence is z,z,...,z, we
compute the mutual information between a tag-
level context of z(denoted as C) and the tag z.
A larger mutual information value can represent
stronger dependencies among the gold tags.The
results are shown in Table 7. We can see that for
all kinds of mutual information calculated in the ta-
ble, Korean and Indonesian has the two lowest val-
ues, both substantially lower than the value of Ger-
man and English. Notably, Korean and Indonesian
are also the worst two languages of the MPoSM
model, while German and English and two of the
languages with better performances. By its design,
our model will induce tags that have strong depen-
dencies among each other (see Sec. 2). Hence, it
is not strange that on Korean and Indonesian, the
MPoSM model could produce tags different from1111
the gold tags and become less effective. And due
to the difference between the predicted tags and
the gold tags, it is not surprising to see that using a
larger context in these two languages does not help
the MPoSM model in these two languages.
F Discussion on uni-Korean Results
In Sec. 6, we notice that MPoSM does not work
well on the Korean dataset in the universal tree-
bank. Inspired by Stratos (2019) that achieves de-
cent performance on Korean and the observation
in Appendix E, we study another modiﬁcation of
MPoSM for the Korean language, MPoSM-word.
Instead of using the local POS prediction module to
predict a POS tag, the MPoSM-word directly feeds
the word embedding to an MLP, and use the out-
put as the input of the masked POS reconstruction
module. Finally, we use the predicted tags after the
Bi-LSTM layer as the induced tags. The results are
shown in Table 9. We do observe that the MPoSM-
word (width=2) variant, which is most similar to
Stratos (2019), achieves the best result, demonstrat-
ing the effectiveness of such inductive biases on
this Korean dataset. Nonetheless, this preference
is not consistent over languages. In our prelimi-
nary study, we notice many other languages still
prefer our default model. We show the result on
the 45-tag English dataset in Table 9, where the de-
fault MPoSM show substantial advantages. These
preferences together with the observation in Ap-
pendix E suggest different languages (or datasets)
can prefer different types of dependency modeling
(e.g. tag-tag dependency vs. word-tag dependency)
and we encourage further study on this topic.G Using Inducted Tags for Unsupervised
Dependency Parsing
In this section, as a side study, we test whether the
performance trends on POS induction can transfer
to unsupervised dependency parsing. We choose
to use the Neural E-DMV model from Jiang et al.
(2016), a commonly used baseline model that uses
gold POS tags in the training. In our experiments,
we replace the gold tags with the inducted tags
from different models to see if the parsing perfor-
mance correlates with the tag quality measured by
M-1 accuracy. Following the convention in unsu-
pervised parsing experiment setups, we train all
the models on sections 2–21 of the English Penn
WSJ dataset, and use section 22 for validation and
section 23 for testing. We remove all the punc-
tuation and only train and test on sentences with
length≤10 (i.e., following the WSJ10 setting).
We compared three different models, our MPoSM
(78.6 M-1), the model from Stratos (2019) (78.1
M-1), and the model from Tran et al. (2016) (75.0
M-1). We notice the models are highly sensitive
to initialization. Hence, to remove the inﬂuence of
bad initialization, we train each model ten different
times and compare the best run. Using the gold
tags, the E-DMV model can reach over 70 DDA
(directed dependency accuracy). However, none
of the models trained using predicted tags achieve
DDA over 45, showing a substantial performance
gap between the gold tags and the predicted tags.
Surprisingly, while the tags from the Neural HMM
model Tran et al. (2016) have lower M-1 accuracy
than the other two models, it shows a small advan-
tage in the parsing performance over the over two
models. We suspect the different trends may result
from a mismatch between the objective optimized
in parsing models and tagging models. The DMV
objective explicitly models the transition probabil-
ity between different nodes, hence the neural HMM
model may have a slight advantage by using a more
similar HMM-style objective.1112
H Additional Agreement Learning
Experiment Design and Results
H.1 Additional Experiment Design
Besides the D(0) and D(2-4) subsets introduced in
Sec. 7.1, we add another variant: MORPH. The
MORPH setting is a variant of D(0) with additional
morphological features. While characters in ev-
ery word in D(0) are all randomly generated, in
MORPH, words with the n1 tags always end with
-n1, words with the n2 tags always end with -n2, etc.
The tag-level regular expressions of all the subsets
are shown in Table 8.
H.2 Illustrations for Each Subset
We provide illustrations of the tag-level regular
expression for each subset. The illustration for D(2-
4) is in Figure 3. The tag-level regular expressions
D(0) and MORPH are the same and the illustration
can be seen in Figure 5.
H.3 Additional Results
We show additional results on the synthetic datasets
in Table 10. Besides the results of the default
MPoSM, we also include an ablation of removing
the “Connecting P(x|z)andP(z|x)” trick intro-
duced in Sec. 3.2. We can see connecting these
two probabilities does bring substantial improve-
ment on this agreement learning task. Surprisingly,
adding the morphological features (MORPH) does
not help the models learn the agreement. Instead,
nearly all models perform slightly worse on this
variant. We suspect the problem may lies in the
speciﬁc design of the morphological feature. The
current setting provides an additional feature to ﬁrst
cluster n1 and n2 words, v1 and v2 words together
since they have a common character ‘n’ or ‘v’ in
the word, whereas normally we randomly sample
characters to form the word. Hence, it can be easier
for the models to enter the unideal local minimum.
I Predicted Clusters Analysis on
Brazilian Portuguese
We provide additional analysis on the pt-br dataset
in the universal treebank and check if our ﬁndings
on the English dataset can generalize to the other
language. Due to the 12-tag annotations on the
universal treebank do not contain ﬁne-grained tags,
it is difﬁcult to single out an agreement type to
conduct a well-controlled analysis (like the subject-
verb agreement analysis on English in the main
paper), but below we verify all the other ﬁndings.
The sizes of predicted clusters are more uni-
form than gold clusters. Similar to the ﬁndings
on the English 45-tag dataset, the sizes of predicted
clusters are much more uniform than the gold clus-
ters. A bar plot is shown in Figure 6.
Difﬁculty in mapping one word to multiple tags.
Brazilian Portuguese also has words with different
senses and POS-tags. For example, the word ‘pare-
cido’, which means ‘similar’ in English, has three1113possible gold tags in the annotated data, including
ADJ, VERB, and ADV . But again, in the model
predictions, this word is always paired with the
same tag.
Dataset biases inﬂuence predicted clusters.
Since models on the universal treebank are only
required to predict 12 tags, the inﬂuence of dataset
biases is smaller than the English 45-tag data. How-
ever, we still can ﬁnd some hints about the negative
effect of the lack of linguistic diversity in the data.
In the predicted clusters, nouns are separated into
a number of clusters. Possibly due to the special
domains of the data, the corpus includes lots of
nouns representing locations and events. These
nouns usually appear in a similar context after the
ADP tag, hence models are likely to use a single
cluster for these nouns, which is not ideal. While
one can argue models should learn to separate the
correct syntactic property from other spurious sta-
tistical properties, small datasets may not contain
enough data to represent the complete picture of
grammar. Hence, models are more likely to cap-
ture the statistical properties that are more common
in the presented corpus and unlikely to induce the
POS tags that ares more well-suited for the general
language.
J Additional Experiment Results
We brieﬂy describe several variants we have tried
in our preliminary experiments but do not observe
signiﬁcant improvement.
•For the dependency modeling network in the
masked POS reconstruction module in the
MPoSM model, we have also explored using
a Transformer (Vaswani et al., 2017) architec-
ture or adding self-attention to the Bi-LSTM.
However, we do not see substantial improve-
ments. On the 45-tag English Penn Treebank
dataset, our best Transformer result reaches
77.2 M-1, which is still lower than the av-
erage M-1 of the Bi-LSTM counterparts in
Table 3. We suspect this trend is due to two
reasons: (1) we notice that the Transformer
models are more sensitive to initialization and
hyper-parameter settings than the LSTM coun-
terparts. Additionally, POS induction datasets
are relatively small, which makes it harder
to train a good Transformer model. (2) com-
pared to Transformer models, LSTM models
have the advantage of a preference of learningshort-term dependencies ﬁrst while learning
long-term dependencies is still possible. This
inductive bias could be useful for the POS
induction task.
•Instead of directly training our model using
gradient descent, another way to optimize our
model is to view the reconstructed POS as la-
tent variables and use EM-based algorithms to
optimize the objective, similar to the method
used in Yang et al. (2019). However, in our
experiments, we do not observe substantial im-
provement by using EM-based optimization
methods over SGD-based methods.1114