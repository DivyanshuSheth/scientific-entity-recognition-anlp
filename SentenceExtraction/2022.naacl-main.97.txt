
Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, Tristan Thrush, Scott A. HaleUniversity of Oxford,The Alan Turing Institute,Hugging Face,Meedanhannah.kirk@oii.ox.ac.uk
Abstract
Detecting online hate is a complex task, and
low-performing models have harmful conse-
quences when used for sensitive applications
such as content moderation. Emoji-based hate
is an emerging challenge for automated detec-
tion. We present H C , a test suite
of3,930short-form statements that allows us
to evaluate performance on hateful language ex-
pressed with emoji. Using the test suite, we ex-
pose weaknesses in existing hate detection mod-
els. To address these weaknesses, we create the
H B dataset using a human-and-
model-in-the-loop approach. Models built with
these 5,912adversarial examples perform sub-
stantially better at detecting emoji-based hate,
while retaining strong performance on text-only
hate. Both H C andH -
B are made publicly available.
Content Warning This article contains exam-
ples of hateful language from H C
to illustrate its composition. Examples are quoted
verbatim, except for slurs and profanity in text, for
which the first vowel is replaced with an asterisk.
The authors oppose the use of hateful language.
1 Introduction
Online hate harms its targets, disrupts online com-
munities, pollutes civic discourse and reinforces
social power imbalances (Gelber and McNamara,
2016). The sheer scale of hateful content online
has led to widespread use of automated detection
systems to find, monitor and stop it (Vidgen et al.,
2019; Gillespie, 2020). However, hateful content is
complex and diverse, which makes it challenging
to detection systems. One particular challenge is
the use of emoji for expressing hate. Emoji are pic-
torial representations that can be embedded in text,allowing complex emotions, actions and intentions
to be displayed concisely (Rodrigues et al., 2018).
Over 95% of internet users have used an emoji and
10 million are sent every day (Brandwatch, 2018).
Following England’s defeat in the Euro 2020 foot-
ball final, there was widespread racist use of emoji
such as ,and (Jamieson, 2020). This pa-
per focuses on emoji-based hate, answering two
research questions:
RQ1 What are the weaknesses of current detection
systems for hate expressed with emoji?
RQ2 To what extent can human-and-model-in-the-
loop training improve the performance of de-
tection systems for emoji-based hate?
To answer RQ1, we present H C ,
a suite of functional tests for emoji-based hate. We
provide 3,930test cases for seven functionalities,
covering six identities. 2,126original test cases
are matched with three types of challenging per-
turbations to enable accurate evaluation of model
decision boundaries (Gardner et al., 2020). We use
H C to assess Google Jigsaw’s Per-
spective API as well as models trained on academic
datasets, exposing critical model weaknesses.
To answer RQ2 and address the model weak-
nesses identified by H C , we im-
plement a human-and-model-in-the-loop dynamic
training scheme. We build on the work of Vidgen
et al. (2021b), who used this approach for textual
hate. Our work begins where their study ends. We
conduct three rounds of adversarial data generation
focused explicitly on emoji-based hate, tasking an-
notators to generate sentences that the model-in-
the-loop misclassifies. This process yields a dataset
of5,912entries, half of which are challenging con-
trasts. We call this dataset H B . The
dataset is evenly split between hate and non-hate,
and each hateful entry has labels for the type and
target. Between each round, the model-in-the-loop1352is re-trained so that annotators are trying to trick
a progressively stronger and more ‘emoji-aware’
model. Relative to existing commercial and aca-
demic models, our models improve performance
on the detection of emoji-based hate, without sacri-
ficing performance on text-only hate.
We make several contributions: (1) we construct
H C , which tests key types of emoji-
based hate as separate functionalities, (2) we eval-
uate the performance of existing academic and
commercial models at detecting emoji-based hate,
(3) we present H B , a labeled emoji-
based hate speech dataset that is adversarially-
generated for model training and (4) we train mod-
els that can accurately detect emoji-based hate.
These data-centric contributions demonstrate the
benefits of systematic and granular evaluation, and
the need to diversify how hate detection systems
are trained. We make both H C and
H B publicly available.
Definition of Hate We use the United Nations
definition of hate speech: “any kind of communi-
cation in speech, writing or behavior, that attacks
or uses pejorative or discriminatory language with
reference to a person or a group on the basis of who
they are, in other words, based on their religion,
ethnicity, nationality, race, color, descent, gender or
other identity factor” (United Nations, 2019, p.2).
2 H C : Functional Tests
for Emoji-Based Hate
The concept of functional testing, an expected
input-output behavior (Beizer, 1995), has been
adapted from software engineering for NLP tasks
(Ribeiro et al., 2020), including hate speech detec-
tion (Röttger et al., 2021). These diagnostic tests
identify model vulnerabilities to intentionally sim-
ple and non-ambiguous constructions of hate so
comprise a minimal performance standard.
2.1 Identifying Functionalities
A functionality describes the ability of a model
to provide a correct classification when presented
with hateful or non-hateful content (Röttger et al.,
2021). Each functionality has a set of correspond-
ing test cases that share one gold-standard label.
We select the functionalities in H C
to be (1) realistic: they capture real-world uses ofemoji-based hate, (2) unique: each covers a dis-
tinct aspect of emoji-based hate, without overlaps
between functionalities, and (3) unambiguous: they
have clear gold-standard labels. The functionalities
are motivated from two perspectives:
Previous Literature We identify distinct uses of
emoji in online communications, particularly abuse.
This includes appending emoji to the end of oth-
erwise neutral statements to reveal emotive intent
(Wiegand and Ruppenhofer, 2021), using emoji as
a ‘pictorial slur’ to attack or dehumanize groups
(Bick, 2020; Wiegand and Ruppenhofer, 2021) and
substituting emoji for identity terms or threatening
verbs (Samghabadi et al., 2019; Bick, 2020; Wie-
gand and Ruppenhofer, 2021). We also review a
range of literature on online hate, identifying dis-
tinct types of hateful language (Talat et al., 2017;
Zampieri et al., 2019; Vidgen et al., 2021a).
Empirical Investigation Based on the literature,
we define a list of potentially hateful emoji and
words, and use Twitter’s Streaming API to search
for the Cartesian products of emoji–emoji and
emoji–word pairs over a two week period. To iden-
tify different forms of emoji-based hate, we apply
a grounded theory approach (Corbin and Strauss,
1990) on a sample of 3,295tweets, splitting out dis-
tinctive categories, and recursively selecting sub-
categories until all key parts of the data are captured
and the framework is ‘saturated’.
2.2 Functionalities in H C
H C has seven functionalities (see
Tab. 1). F1 Verb Swap tests threatening hate where
the verb is replaced with the equivalent emoji: ,,,,. It includes both direct threats and
normative statements of threat. F2 Identity Swap
tests derogatory hate where the identity term is re-
placed with an emoji representation: woman ( ),
trans people ( ), gay people ( ), disabled peo-
ple ( ), Black people ( ) and Muslims ( ).F3
Descriptor Swap tests hate where the negative de-
scriptor is replaced with the equivalent emoji. It
includes both dehumanizing language and expres-
sions of disgust. For dehumanizing language, five
descriptors are used: ,,,,. Based on
our Twitter analysis, target-specific descriptors are1353
used for Black people: ( ,) and Muslims: ( ,). For expressions of disgust, three descriptors
are used: ,,.F4 Double Swap tests a com-
bination of F1 with either F2 or F3, where emoji are
swapped for both the identity and the verb, or both
the identity and the negative descriptor. Some tem-
plates contain only emoji e.g., “ ”, while others
have some text e.g., “ make me ”.F5 Append
tests statements which would be non-hateful on
their own but become hateful when appended with
a negative emoji such as ,,. For instance,
“My new manager is a woman” is neutral but “My
new manager is a woman ” is hateful. F6 Posi-
tive Confounder tests clearly hateful statements
appended with a positive emoji such as ,,.
For instance, “Kill all women ” or “All women
deserve to die ”.F7 Emoji Leetspeak tests state-
ments where parts of the hateful language have
been obfuscated by replacing characters with emoji.
The character substitution can occur in the term ex-
pressing negative sentiment, e.g., “I h te gays”,
the identity, e.g., “I hate g ys” or the slur, e.g., “I
hate f gs”. There are three emoji character substi-
tutions: (1) a numeric substitution which mimics
leetspeak (a: , i: , e: , o: ), (2) a star mimick-
ing * for vowels in slurs and profanities (vowel: ),
and (3) a heart mimicking starring out vowels but
in a more adversarial manner (vowel: ). To reflect
specific cases on Twitter of anti-Muslim hate, two
additional slurs are included where a word-piece is
substituted with an emoji ( -f*cker, -f*cker).
2.3 Test Cases in H C
H C is strongly inspired by empiri-
cal examples of emoji-based hate but we want it to
comprise a clear, minimum performance standard.
Thus, for each functionality we hand-craft test
cases, i.e., short English-language text documents
that clearly correspond to just one gold-standard
label. The test cases are varied in their vocabulary
and syntax to ensure they assess model robustnessto diverse language (Zhou et al., 2020). To generate
test cases at scale and ensure balanced coverage
across targeted groups, we use a template-based
approach (Dixon et al., 2018; Ribeiro et al., 2020;
Röttger et al., 2021). First, each template is given
an identity placeholder e.g., “I hate [IDENTITY]”
or a slur placeholder e.g., “You are just a [SLUR] to
me”. Six protected identities populate these place-
holders: women, trans people, gay people, Black
people, disabled people and Muslims. For tem-
plates with slurs, we use the 16 slurs compiled by
Röttger et al. (2021) (see Appendix C.1). Second,
each template is given an emoji placeholder that is
populated by emoji that share semantic and syntac-
tic properties, so that the test suite can be scaled
across different emoji. For example, the template “I
will [EMOJI] all [IDENTITY]” could be populated
with {‘ ’, ‘trans people’}. H C
comprises 3,930entries. There are 2,126original
test cases and 1,804perturbations (see Tab. 1).
2.3.1 Perturbations
Each set of test cases is matched to a set of per-
turbations which are constructed through minimal
edits to the original hateful template.Test cases
generated from these perturbed templates share vo-
cabulary and syntax with the original test cases,
enabling fine-grained assessment of model weak-
nesses (Gardner et al., 2020). We provide three
types of perturbations. For identity perturba-
tions , the identity targeted by the original hateful
statement is substituted for a non-protected entity,
making it non-hateful. Possible identity perturba-
tions include non-protected groups (e.g., “I hate
accountants”), non-human entities (e.g., “I hate spi-
ders”), and inanimate objects (e.g., “I hate pizza”).
Forpolarity perturbations , the negative sentiment
of the original hateful statement is reversed to be
positive, making it non-hateful, without changing
the target of the statement. Possible polarity per-1354turbations include changing threatening language
to supportive language, e.g., “kill” to “respect”,
changing expressions of hate to expressions of love,
e.g., “I hate. . . ” to “I love. . . ”, or replacing hateful
use of slurs with counterspeech against the slurs,
e.g., “[IDENTITY] are [SLUR]” to “[IDENTITY]
should never be called [SLUR]”. For no emoji per-
turbations , emoji are removed or replaced with
their equivalent text to preserve the semantic ex-
pression (e.g., “ ” becomes “shoot” and “ ” be-
comes “love”). For most functionalities this pertur-
bation preserves the original label of the test case,
e.g., “[IDENTITY] makes me ” is hateful, and its
perturbation “[IDENTITY] makes me sick” is still
hateful. However, for F5 Append the label changes
because when the negative emoji appended to the
neutral statement is removed, the part of the state-
ment that remains is non-hateful.
2.4 Validating Test Cases
To validate the gold-standard labels assigned to
each test case, we recruited three annotators with
prior experience on hate speech projects.Anno-
tators were given extensive and prescriptive guide-
lines (Röttger et al., 2022), as well as test tasks
and training sessions, which included examining
real-world examples of emoji-based hate from Twit-
ter. We followed guidance for protecting annotator
well-being (Vidgen et al., 2019). There were two it-
erative rounds of annotation. In the first round, each
annotator labeled all 3,930test cases as hateful or
non-hateful, and had the option to flag unrealistic
entries. Test cases with any disagreement or un-
realistic flags were reviewed by the study authors
(n= 289 ). One-on-one interviews were conducted
with annotators to identify dataset issues versus an-
notator error. From 289test cases, 119were iden-
tified as ambiguous or unrealistic, replaced with
alternatives and re-issued to annotators for label-
ing. No further issues were raised. We measured
inter-annotator agreement using Randolph’s Kappa
(Randolph, 2005), obtaining a value of 0.85 for
the final set of test cases, which indicates “almost
perfect agreement” (Landis and Koch, 1977).
3 Building Better Models with
H B
As reported in §4, we find existing models per-
form poorly on emoji-based hate as measured with
H C . We address those failings byimplementing a human-and-model-in-the-loop ap-
proach using the Dynabench interface in order to
train a model that better detects emoji-based hate.
Dataset Generation We implemented three suc-
cessive rounds of data generation and model re-
training to create the H B dataset.
In each round we tasked a team of 10 trained
annotators with entering content that the model-
in-the-loop would misclassify.We refer to this
model as the target model . Annotators were in-
structed to generate linguistically diverse entries
while ensuring each entry was (1) realistic, (2)
clearly hateful or non-hateful and (3) contained
at least one emoji. Each entry was first given a
binary label of hateful or non-hateful, and hate-
ful content was assigned secondary labels for the
type and target of hate (Zampieri et al., 2019; Vid-
gen et al., 2021a,b). Each entry was validated by
two additional annotators, and an expert resolved
disagreements. After validation, annotators cre-
ated a perturbation for each entry that flipped the
label. To maximize similarity between originals
and perturbations, annotators could either make
an emoji substitution while fixing the text or fix
the emoji and minimally change the surrounding
text. Including a hateful and non-hateful version of
each sentence in the dataset prevents overfitting to
certain emoji or identity references. Each perturba-
tion received two additional label annotations, and
disagreements were resolved by the expert. This
weekly cadence of annotator tasks was repeated
in three consecutive weeks. The dataset composi-
tion and inter-annotator agreement is described in
Appendix E.
Model Implementation Our work follows di-
rectly from Vidgen et al. (2021b), who collect four
rounds (R1–4) of dynamically-generated textual
data as well as 468,928 entries compiled from 11
English-language hate speech datasets (R0). Our
rounds of data collection are referred to as R5–7.
At the end of each round, the data for that round
is assigned a 80/10/10 train/dev/test split. The test
split is never used for any future training. The train
split is upsampled to improve performance with
multipliers of 1,5,10,100, with the optimum ratio
taken forward to subsequent rounds. The target1355model is then re-trained on the training data from
all prior rounds as well as the current round. For R5
data collection, the target model (R5-T) is the De-
BERTa model released by Ma et al. (2021), trained
on R0–R4 from Vidgen et al. (2021b). This model
performs well on text-only hate, but has seen lim-
ited emoji in training. For subsequent target mod-
els, we evaluate two architectures: DeBERTA (He
et al., 2021) and BERTweet (Nguyen et al., 2020).
We select the best models and upsampling ratios
at the end of each round. The criteria to select the
best model must satisfy two requirements: (1) the
target model must still perform well on prior rounds
of text-based examples because emoji-based hate
represents just one construction of textual hate, and
(2) the target model must successfully handle the
most recent round data because iterative data collec-
tion produces more challenging examples in succes-
sive rounds. To balance these requirements, we use
a weighted accuracy metric with 50% weight on the
test sets from all prior rounds and 50% weight on
the current round test set, which enforces a recency-
based discount factor (Kiela et al., 2021). This en-
sures we can assess performance against the latest
(emoji-specific and most-adversarial) round with-
out overfitting and reducing performance on the
previous test sets.
Model Error Rate For each emoji-based round
of data generation (R5–7) as well as prior text-only
rounds from Vidgen et al. (2021b), we calculate
model error rate (MER) as the proportion of anno-
tators’ original entries that fool the model (Fig. 1).
Between R4 and R5, MER increases by 35 percent-
age points (pp) to over 60%, higher than the R1
MER. From R5 to R6, once the model has been
trained on emoji-based hate, there is a steep reduc-
tion in MER of 24pp. From R6 to R7, there is
a smaller reduction in MER of 1pp. Overall, themodel is easily tricked by emoji content at first but
then becomes much harder to trick after our first
round of re-training.
Performance on Adversarial Test Sets To evalu-
ate model performance across rounds, we calculate
F1-score and accuracy for each test set (Tab. 2).
The baseline R5-T model is the highest performing
model on the text-only test set (R1–4), with an F1
of 0.85, but only scores 0.49 on the newly generated
emoji-containing test sets from H B
(R5–7). R6-T, R7-T and R8-T perform better on
R5–7, with F1 between 0.76 and 0.77. They per-
form similarly well on the R1–4 test set, with F1 of
0.84, suggesting no trade-off for performance on
text-only hate speech. The best performing model
across all R1–7 test sets is R8-T, with an F1 of 0.83.
The greatest performance gain in the adversarially-
trained models is from R5-T to R6-T, with an in-
crease in F1 of 0.28 on the emoji test set. This is
achieved with only 2,000emoji-containing exam-
ples. By comparison, R7-T and R8-T yield very
small improvements.
4 Evaluating Models with
H C
We present results for two existing models as
baselines, with additional baselines shown in Ap-
pendix G. The first baseline is Google Jigsaw’s Per-
spective API, a widely-used commercial tool for
content moderation. We use Perspective’s “Identity
Attack” attribute, which is defined as “negative or
hateful comments targeting someone because of
their identity” and thus closely matches our defini-
tion of hate. The returned score is converted to a bi-
nary label with a 50% cutoff. We refer to this model
as P-IA. The second baseline is the R5-T model
from Ma et al. (2021), introduced above. To com-
pare model performance on H C , we
use accuracy because three sets of test cases (orig-
inals, polarity perturbations and identity perturba-1356
tions) have one class label, making F1-score incom-
patible. To measure emoji-specific weaknesses, we
also calculate emoji difference , the difference be-
tween averaged model accuracy on the original
emoji test cases compared with averaged accuracy
on the no emoji perturbations.
OnH C as a whole, our newly
trained models R6-T, R7-T and R8-T perform best,
with overall accuracy from 0.87 to 0.88 (Tab. 3).
They substantially outperform P-IA, with an accu-
racy of 0.69, and R5-T, with an accuracy of 0.78.
Our newly trained models have the smallest emoji
difference, between 0.03 and 0.08. In contrast,
P-IA and R5-T have emoji differences of 0.16 and
0.22, respectively. Comparing the three models
trained on H B , the first round of ad-
versarial data generation yields the largest relative
improvement, and in many ways R6-T is at least as
good a model, if not better, than R8-T.
4.1 Model Performance by Functionality
Our models trained on H B perform
better than the two baseline models on nearly every
functionality (Tab. 4). They also perform far more
consistently across all perturbation types.
For F1 Verb Swap, P-IA and R5-T perform well
on original statements, but then perform poorly on
the polarity perturbations (0.20 and 0.42 accuracy,
respectively). Our models have much stronger per-
formance on polarity perturbations (between 0.73
and 0.77), and comparably high performance on the
other sets of test cases. For F2 Identity Swap, P-IA
and R5-T perform very poorly on original state-
ments (0.14 and 0.33 accuracy, respectively) but
then perform well on the polarity perturbations and
no emoji perturbations. This vulnerability is carried
forward to performance on F4 Double Swap. R5-T
only achieves 0.03 and P-IA makes zero correctpredictions. In contrast, our models achieve accu-
racy of 0.83 on F2 and accuracies between 0.70 and
0.79 on F4. For F3 Descriptor Swap, our models
improve over P-IA and R5-T on the original state-
ments. The relative improvement is particularly
large for the polarity perturbations (0.25 for P-IA
compared with 0.93 for R8-T). For F5 Append, P-
IA and R5-T perform moderately with accuracies
of 0.73 and 0.69. Our models perform far better,
with accuracies from 0.87 to 0.99. However, they
do not show a substantial increase in performance
for the no emoji perturbations in F5.3. For F6 Posi-
tive Confounder, our models perform worse than
P-IA and R5-T on the original statements, but then
have far more consistent performance across the
perturbations. For instance, R8-T achieves accu-
racies between 0.89 to 0.93 across all sets of test
cases in this functionality, compared with a range
of 0.44 to 0.93 for P-IA. P-IA and R5-T perform
well on original statements and poorly on pertur-
bations precisely because they ignore the effect of
the emoji confounder. F7 Emoji Leetspeak is a
successful adversarial strategy. R5-T achieves 0.85
accuracy on the original statements but just 0.67
on the identity perturbations, while P-IA does even
worse, with only 0.59 on the original statements
and 0.57 on the polarity perturbations. Our mod-
els perform better on the original statements, but
still struggle with identity perturbations, showing
how challenging this functionality is. However,
non-hateful leetspeak constructions are likely less
prevalent on social media than slur-based leetspeak.
Overall, all models trained on H B
perform well on emoji-based hate, but there is no
clear ‘best’ model among them. After the large
improvement from R5-T to R6-T, subsequent mod-
els produce minimal performance gains across the
functionalities. This suggests the gains, as mea-
sured on H C ’s intentionally short
and simple statements, are quickly saturated.
4.2 Model Performance by Target Group
H C contains target group labels for
hateful and non-hateful entries, allowing models to
be compared by their subgroup fairness. However,
defining fairness is a non-trivial task, and there is
substantial debate in the machine learning commu-
nity as to which metrics are most appropriate. We
consider six metrics calculated across subgroups:
accuracy, precision, recall, false positive rate, false13571358negative rate and selection rate.Fig. 2 shows
subgroup fairness metrics compared across three
models: P-IA, R5-T and R8-T. It is concerning that
P-IA has such differential performance, especially
regarding the unbalanced false positive and nega-
tive rates for women and disabled people, as both
of these error types are societally harmful. R8-T
has more balanced accuracy, precision, recall and
selection rate across subgroups, driven by stable
performance in false negatives and positives. In
Appendix H we provide two further between-group
fairness metrics: demographic parity ratio (Agar-
wal et al., 2019), and equalized odds ratio (Hardt
et al., 2016; Agarwal et al., 2018). There is also
substantial improvement in these metrics from ad-
versarial training schemes relative to commercial
solutions or statically-trained models.
5 Discussion
H C reveals critical model weak-
nesses in detecting emoji-based hate. Existing
commercial and academic models perform poorly
at identifying hate where the identity term has
been replaced with an emoji representation (F2 and
F4), even though they perform well at identifying
the equivalent textual statements. These models
have better performance on Verb Swap (F1) but
then struggle with the polarity perturbations (F1.2).
This suggests that the models are overfitting to the
identity term and ignore the sentiment from the
emoji, leading to false positive predictions. Our
newly trained models substantially improve per-
formance on original hateful statements from F2,
F4 and F5, indicating they have a better semantic
grasp of emoji substitutions and appends. They
also make large performance gains on the polarity
perturbations in F1.2, F3.2, F5.2 and F6.2, sug-
gesting they better incorporate information on how
different emoji condition the likelihood of hateful-
ness. Despite improving on existing models, our
models still perform relatively more poorly on F4,
as well as the F1.2, F5.3 and F7.1 perturbations.
These weaknesses could potentially be addressed
in future work through additional data generation.
Training on just one round of adversarial data
yields the biggest improvement on H -
C . Thereafter, performance plateaus. This
aligns with the sharp increase and then fall in MERacross emoji-based rounds of data generation (in
Fig. 1). On the adversarial test sets, R8-T only
marginally outperforms R6-T and R7-T on all R1–7
test sets. It slightly underperforms on the R5–7 test
sets from H B . This suggests that
while training on a relatively small number of en-
tries can substantially improve performance, the
gains quickly saturate. Future work could investi-
gate performance differences in more detail using
the labels for type and target of hate that we provide
for all three rounds of data.
Due to practical constraints, H C
has several limitations, which could be addressed
in future work. First, it includes relatively sim-
plistic statements, and so offers negative predictive
power: high performance only indicates the ab-
sence of specific weaknesses (Gardner et al., 2020;
Röttger et al., 2021). Second, it is inspired by real-
world hate but synthetically generated. Future work
could evaluate against more complex and diverse
forms of emoji-based hate. Third, it is limited in
scope. It only considers short English-language
statements with one binary label. Only six identi-
ties are included, none of which are intersectional,
and the set of emoji covers a fraction of the full
Unicode Standard. In-scope identities and emoji us-
age are culturally-dependent (Barbieri et al., 2016;
Ljubeši ´c and Fišer, 2016) so future work could as-
sess the generalizability of H C to
other cultures and languages. These limitations
do not diminish H C ’s utility as a
tool for effectively identifying model weaknesses
to emoji-based hate. By publicly releasing the test
suite, practitioners and academics can scrutinize
their models prior to deployment or publication.
Adversarial data generation is a powerful tech-
nique for creating diverse, complex and informative
datasets. However, it also introduces challenges.
First, the entries are ‘synthetic’ rather than sampled
from ‘real-world’ examples. Substantial training
and time are required to ensure that annotators un-
derstand real online hate and can imitate it. Second,
annotators can exhaust their creativity and start
producing unrealistic, simplistic or non-adversarial
examples. Third, because of the need for train-
ing, supervision and support, only a small pool
of annotators is feasible, which can introduce ad-
ditional idiosyncratic biases. Given these issues,
quality control is a key consideration throughout
the data generation process. Encouragingly, we
find carefully-curated and adversarially-generated1359training datasets can significantly improve perfor-
mance on emoji-based hate as a particular type of
challenging content, and that this approach is effec-
tive with relatively few training examples. Thus,
substantial model improvements can be realized
with minimal financial and computational cost.
6 Related Work
Emoji-based hate has received limited attention
in prior work on hate and abusive language de-
tection. Studies that attend to emoji often do so
as a potential input feature to aid classifier perfor-
mance. For example, Samghabadi et al. (2019)
improve offensive language classification with an
‘emotion-aware’ mechanism built on emoji embed-
dings. Ibrohim et al. (2019) find that adding emoji
features to Continuous Bag of Words and word un-
igram models marginally improves performance
for abusive language detection on Indonesian Twit-
ter. Bick (2020) identifies examples of subtle and
non-direct hate speech in German–Danish Twitter
conveyed through ‘winking’ or ‘skeptical’ emoji
that flag irony or non-literal meaning in their ac-
companying text. Corazza et al. (2020) train an
emoji-based Masked Language Model (MLM) for
zero-shot abuse detection. They show this method
improves performance on classifying abuse in Ger-
man, Italian and Spanish tweets compared to an
MLM which does not attend to emoji. Wiegand and
Ruppenhofer (2021) use abusive emoji as a proxy
for learning a lexicon of abusive words. Their find-
ings indicate that emoji can disambiguate abusive
and profane usages of words such as f*ck and b*tch.
By contrast, our work focuses on emoji-based hate
as a challenge for hate detection models. With
H C , we enable a systematic eval-
uation of how well models handle different types
of emoji-based hate. Rather than adjusting the
model architecture, we account for emoji-based
hate in our iterative data generation process and
show that models trained on such data perform
better on emoji-based hate, while retaining strong
performance on text-only hate.
As a suite of functional tests for evaluation,
H C directly builds on previous
work by Ribeiro et al. (2020) and Röttger et al.
(2021). Ribeiro et al. (2020) introduced functional
tests as a framework for NLP model evaluation with
C L, showing that their approach can iden-
tify granular model strengths and weaknesses that
are obscured by high-level metrics like accuracyand F1-score. Röttger et al. (2021) adapted this
framework to hate detection with HC ,
which covers 29 model functionalities motivated
by interviews with civil society stakeholders and
a review of previous hate speech literature. Like
HC , we pair hateful test cases with con-
trasting perturbations that are particularly challeng-
ing to models relying on overly simplistic decision
rules and thus reveal granular decision boundaries.
HC did not consider emoji-based hate,
which is the main focus of our work.
Our approach to training better models for emoji-
based hate builds directly on work by Vidgen et al.
(2021b), who apply iterative human-and-model-in-
the-loop training to hate detection models. Like
us, they used the Dynabench interface (Kiela et al.,
2021) to implement their training system, which
has also been used to improve model performance
for other tasks such as reading comprehension (Bar-
tolo et al., 2020) and sentiment analysis (Potts et al.,
2021). Earlier work by Dinan et al. (2019) intro-
duced a similar ‘build it, break it, fix it’ system of
repeated interactions between a hate classifier and
crowdworkers to develop safe-by-design chatbots.
Unlike previous work, we focus data generation on
a particular type of hateful content, emoji-based
hate, and show that the training scheme can address
specific model weaknesses on such content without
sacrificing performance on text-only hate.
7 Conclusion
Online hate is a pervasive, harmful phenomenon,
and hate detection models are a crucial tool for
tackling it at scale. We showed that emoji pose
a particular challenge for such models, and pre-
sented H C , a first-of-its-kind eval-
uation suite for emoji-based hate. It covers seven
functionalities with 3,930test cases. Using this
test suite, we exposed clear weaknesses in the per-
formance of commercial and academic models on
emoji-based hate. To address these weaknesses,
we created the H B dataset using an
innovative human-and-model-in-the-loop approach.
We showed that models trained on this adversar-
ial data are substantially better at detecting emoji-
based hate, while retaining strong performance on
text-only hate. Our approach of first identifying
granular model weaknesses, and then generating
an adversarial dataset to address them presents a
promising direction for building models to detect
other diverse and emerging forms of online harm.1360Acknowledgments
We are thankful for support that the Oxford au-
thors received to facilitate annotation and computa-
tional resources from the V olkswagen Foundation,
Meedan, Keble College, the Oxford Internet Insti-
tute, Rewire and The Alan Turing Institute. We
owe a debt of gratitude to all our annotators, to
Dynabench and to our anonymous reviewers. We
are also grateful to Zeerak Talat and Douwe Kiela
for their helpful advice on this research, as well as
support from Devin Gaffney and Darius Kazemi.
Hannah Rose Kirk was supported by the Economic
and Social Research Council grant ES/P000649/1.
Paul Röttger was supported by the German Aca-
demic Scholarship Foundation.
References136113621363A Ethical Considerations
Misuse: We release two datasets of challenging
emoji examples on which commercial solutions
and state-of-the-art transformer models have been
proven to fail. Malicious actors could take inspi-
ration for bypassing current detection systems on
internet platforms, or in principal train a generative
hate speech model. This concern is also raised by
Vidgen et al. (2021b), but the conclusion is reached
that such risk of misuse is small and outweighed
by the considerable scientific and social benefits.
Harm Statement: Following the advice of Der-
czynski et al. (2022), we describe the risks to well-
being during the production and publication of
this research and the steps taken to mitigate them.
There is a risk of harm to data subjects i.e., the
targets of hate, from reinforcing hateful, dehuman-
izing or derogatory statements, and to readers view-
ing this content in the paper. To mitigate these
harms, we include a content warning directly after
the abstract, at least a page before any harmful con-
tent is displayed and colored in red for maximum
visibility. We also include a section-specific con-
tent warning before §2.2, which includes a number
of hateful examples. Hateful examples in §2.2 are
consistently formatted and visually distanced with
gray text. All other examples (including Tab. 1,
Tab. 4) are presented with placeholders for the
[IDENTITY]. Hateful slurs are starred out with
an asterisk where possible (e.g., Tab. 5) but we can-
not star out emoji without hindering interpretation.
There is also a risk to researchers and annotators
from labeling and viewing harmful content. As
authors, we oppose the use of hateful language.
We follow protocols for protecting annotator well-
being, including briefing sessions, regular check-
ins and provision of mental health support.
B Data Statement for H C
We provide a data statement (Bender and Friedman,
2018) to document the generation and provenance
of H C .
B.1 Curation Rationale
To construct H C , we hand-crafted
3,930short-form English-language texts using a
template-based method for group identities and
slurs. Each test case exemplifies one functionality
and is associated with a binary gold standard label
(hate versus not hate). All 3,930cases were labeledby a trained team of three annotators, who could
also flag examples that were unrealistic. Any test
cases with multiple disagreements or flags were
replaced with alternative templates and re-issued
for annotation to improve the quality of exam-
ples in the final set of test cases. The purpose of
H C is to evaluate the performance
of black-box models against varied constructions
of emoji-based hate.
B.2 Language Variety
The test cases are in English. This choice was mo-
tivated by the researchers’ and annotators’ exper-
tise, and to maximize H C ’s appli-
cability to previous hate speech detection studies,
which are predominantly conducted on English-
language data. We discuss the limitations of re-
stricting H C to one language and
suggest that future work should prioritize expand-
ing the test suite to other languages.
B.3 Speaker Demographics
All test cases were hand-crafted by the lead author,
who is a native English-speaking researcher at a UK
university with extensive subject matter expertise
in online harms.
B.4 Annotator Demographics
We recruited a team of three annotators who
worked for two weeks in May 2021 and were paid
£16/hour. All annotators were female and between
30–39 years old. One had an undergraduate de-
gree, one a taught graduate degree and one a post-
graduate research degree. There were three nation-
alities: Argentinian, British and Iraqi, two ethnic-
ities: White and Arab, and three religious affilia-
tions: Catholic, Muslim and None. One annotator
was a native English speaker and the others were
non-native but fluent. All annotators used emoji
and social media more than once per day. All an-
notators had seen others targeted by abuse online,
and one had been targeted personally.
B.5 Speech Situation
The modality of all test cases is written text embed-
ded with emoji. The empirical investigation using
Twitter’s streaming API was conducted between
1st–14th April 2021. The first set of test cases was
created between 26th April–7th May 2021. The
first round of annotation ran between 7th–14th May
2021. The second round of cases was created and
re-issued between 14th–21st May 2021.1364B.6 Text Characteristics
The genre of texts is hateful and non-hateful state-
ments using emoji constructions. Renderings
of emoji vary by operating system and browser
providers. The renderings in this paper are from
WhatsApp. The composition of the dataset by la-
bels (hate versus not hate) and by set (originals
versus perturbations) is described in Tab. 3 of the
main paper. 68% of the test cases are hateful.
H C has 644 cases for Muslims, 608
cases each for gay people, disabled people and
women, 582 cases for Black people and 566 cases
for trans people. identity perturbations switch a
protected identity for a non-protected identity so
314 cases have no protected identity tag.
C Constructing H C
C.1 List of Hateful Slurs in
H C
The slurs used in H C are selected in
the same way as for HC (Röttger et al.,
2021). For each of the six identities in H -
C , the most common slurs are taken from
hatebase.org , an online crowd-sourced repos-
itory of hateful terms and slurs. The top three most
common slurs logged by users of hatebase are used,
unless the 3rd ranked slur is significantly less com-
mon than the 2nd ranked. Each identity thus has
two or three associated slurs (Tab. 5).
C.2 Defining Perturbations
Each template has three perturbations but the map-
ping of original test cases to perturbations is not
one-to-one. Instead, there is a set of original tem-
plates contrasted by three sets of perturbation tem-
plates. The perturbation templates have fewer de-
grees of freedom than the original templates so test
cases generated from them are fewer in number.
Within each perturbed template we use the same
emoji for substitutions to ensure consistency, but
across templates, we use a range of emoji to ensure
linguistic variety.
The perturbed templates are constructed as fol-
lows. (1) Identity perturbations : the protected
identity (which could be an emoji or a word depend-
ing on the functionality) in each original template
is substituted with onenon-protected entity (which
could be an emoji or a word). (2) Polarity pertur-
bations : the negative term (which could be a word
or an emoji) in each original template is substituted
foronepositive term (which could be a word or
an emoji). (3) No emoji perturbations : all emoji
elements of the original template are replaced by
equivalent text where oneword is used to cover all
versions e.g. and are both substituted for the
verb ‘harm’. For F5 and F6, the appended emoji is
removed not replaced.
D Data Statement for H B
We provide a data statement (Bender and Friedman,
2018) to document the generation and provenance
of H B .
D.1 Curation Rationale
We use an online interface designed for dynamic
dataset generation and model benchmarking (Dyn-
abench) to collect synthetic adversarial examples
in three successive rounds, running between 24th
May–11th June. Each round contains ∼2,000 en-
tries, where each original entry inputed to the in-
terface is paired with an offline perturbation. Data
was synthetically-generated by a team of trained
annotators, i.e., not sampled from social media.
D.2 Language Variety
All entries are in English. Language choice was dic-
tated by the expertise of researchers and annotators.
Furthermore, English is used for a wide number of
benchmark hate speech datasets (Davidson et al.,
2017; Founta et al., 2018) and was also used in the
adversarial dataset for textual hate speech (Vidgen
et al., 2021b). The method be could be adapted for
other languages in future work.
D.3 Speaker Demographics
All entries are synthetically-created by annotators
so the speaker demographics match the annotator
demographics.
D.4 Annotator Demographics
Ten annotators were recruited to work for three
weeks, and paid £16/hour. An expert annotator
was recruited for quality control purposes and paid
£20/hour. In total, there were 11 annotators. All1365annotators received a training session prior to data
collection and had previous experience working on
hate speech projects. A daily ‘stand-up’ meeting
was held every morning to communicate feedback
and update guidelines as rounds progressed. An-
notators were able to contact the research team at
any point using a messaging platform. Of 11 an-
notators, 8 were between 18–29 years old and 3
between 30–39 years old. The completed education
level was high school for 3 annotators, undergradu-
ate degree for 1 annotator, taught graduate degree
for 4 annotators and post-graduate research degree
for 3 annotators. 6 annotators were female, and
5 were male. Annotators came from a variety of
nationalities, with 7 British, as well as Jordanian,
Irish, Polish and Spanish. 7 annotators identified
as ethnically White and the remaining annotators
came from various ethnicities including Turkish,
Middle Eastern, and Mixed White and South Asian.
4 annotators were Muslim, and others identified
as Atheist or as having no religious affiliation. 9
annotators were native English speakers and 2 were
non-native but fluent. The majority of annotators
(9) used emoji and social media more than once
per day. 10 annotators had seen others targeted by
abuse online, and 7 had been personally targeted.
D.5 Speech Situation
Entries were created from 24th May–11th June
2021. Their modality is short-form written texts
embedded with emoji. Entries are synthetically-
generated but annotators were trained on real-world
examples of emoji-based hate from Twitter.
D.6 Text Characteristics
The genre of texts is hateful and non-hateful state-
ments using emoji constructions. Annotators in-
putted emoji into the platform using a custom emoji
picker.The composition of the final dataset is
described in Tab. 6. 50% of the 5,912test cases
are hateful. 50% of the entries in the dataset are
original content and 50% are perturbations.
E Constructing H B
Types of Hate We adopt the same categorization
used by Vidgen et al. (2021b, p.3).There are
four types of hate. Derogation: Language which
explicitly derogates, demonizes, demeans or insults
a group. Animosity: Expressions of abuse through
implicit statements or mockery, where a logical
step must be taken between the sentence and its
intended negativity. Threatening language: State-
ments of intent to take action against a group, with
the potential to inflict serious or imminent harm on
its members. Dehumanizing language: Comparing
groups to insects, animals, germs or trash.
Targets of Hate Annotators were provided with
a non-exhaustive list of high-priority identities to
focus on which included categorizations by gen-
der identity (e.g., women, trans), sexual orientation
(e.g., gay), ethnicity (e.g., Hispanic people), reli-
gion (e.g., Sikh), nationality (e.g., Polish), disabil-
ity and class, alongside intersections (e.g., Muslim
women). Hate directed towards majority groups
(e.g., men, white people and heterosexuals) is out-
side the remit of this work. The explicit decision
not to focus on issues such as ‘reverse racism’ (Bax,
2018) is made due to the complex debate on its in-
clusion in hate speech definitions.
Composition Tab. 7 reports the inter-annotator
agreement and number of final entries per round.
Each round has approximately the same number of
entries with slightly fewer in R7 due to more qual-
ity control issues. Labels are equally distributed
in each round (see Tab. 6). For 75 pairs of origi-
nals and perturbations, the perturbation unsuccess-
fully flipped the label given by majority agreement
between three annotators. All other pairs have
opposite labels. Derogation is always the most-1366
commonly inputted form of hate. From R5 to R7,
there is a rise in animosity entries paired with a
decline in threatening and dehumanizing language
entries. Annotators were given substantial freedom
in the targets of hate resulting in 54unique tar-
gets, and 126unique intersections of these. The
entries from R5–R7 contain 1,082unique emoji
out of 3,521defined in the Unicode Standard as of
September 2020. The mode of emoji per entries
is1and mean is approximately 1.5in each round.
The frequency of targets and emoji follow a long-
tailed distribution, similar to a Zipf curve. These
distributions match those found online for targets
(Silva et al., 2016) and for emoji (Felbo et al., 2017;
Cappallo et al., 2019; Bick, 2020).
In the first round, annotators commonly em-
ployed a strategy of substituting identities for emoji
so three identity emoji were frequently used ( ,,). Using emoji character substitutions in slurs
was also a successful strategy (A: ), as was sub-
stituting the entire slur for an emoji homonym (f*g:). In the final round, the model was wise to
such strategies, and so annotators changed their
adversarial techniques to irony, satire or mockery,
as reflected in the top emoji ( ,,,). For
a qualitative understanding of annotators’ experi-
ences, we conducted a post-study survey.
F Target Models
In each round we assessed two candidate model
architectures. The first is an uncased DeBERTa
base model (134M params) with a sequence clas-
sification head (He et al., 2021). DeBERTa has
been shown to improve on BERT (Devlin et al.,
2019) and RoBERTa (Liu et al., 2019) models by
incorporating a disentangled attention mechanism
and enhanced mask decoder. However, emoji are
likely relatively sparse in DeBERTa’s pre-trainingmaterial which includes English Wikipedia, Book
Corpus (Zhu et al., 2015) and a subset of Common-
Crawl. DeBERTa uses the BPE-encoding method
so the tokenizer can encode emoji as unique tokens.
The second candidate is an uncased BERTweet
model (135M params) with a sequence classifi-
cation head where the RoBERTa training proce-
dure was repeated on 850M English Tweets us-
ing a custom vocabulary (Nguyen et al., 2020).
BERTweet tokenizes emoji by first translating them
to text representations using the emoji package.
These word representations are derived from the
Unicode Standard, so for example becomes
:crying_face: .
Upsampling ratios are evaluated for each
new round of training data with multipliers of
1,5,10,100. For the R1–4 data, we carry forward
the upsampling from Vidgen et al. (2021b): R1 is
upsampled five times, R2 is upsampled 100 times,
R3 is upsampled once, and R4 is upsampled once.
Combining the model architectures and upsampling
ratios gives 8 candidate models for each round’s
target model. All models were implemented using
thetransformers library (Wolf et al., 2020).
All models were trained for 3 epochs with early
stopping based on the dev set loss, a learning rate
of2e−5and a weighted Adam optimizer. Other
hyperparameters were set to HuggingFace defaults.
We train and evaluate each model once and report
results for this single run. Training took approxi-
mately 7 hours for each BERTweet model and 15
hours for each DeBERTa model using 8 NVIDIA
Tesla V100 GPU on the JADE2 supercomputer.
The best target model for each round is selected
by weighted accuracy between all prior rounds and
the current round. For R6-T, a DeBERTa model
with 100x upsampling on R5 performs best. Given
the dearth of emoji in R1–R4, it is unsurprising
a large upsample improves performance on emoji-
based hate. For R7-T, DeBERTa with one upsample
on R6 performs best. For R8-T, DeBERTa with five
upsamples on R7 performs best. In all rounds, De-
BERTa significantly outperforms BERTweet, while
the upsampling ratio less substantially affects per-
formance. This suggests first converting emoji to
their text representations does not substantially aid
performance, though there may be other differences
between the models driving results. We upload
models to Dynalab for model-in-the-loop evalua-
tion and data collection (Ma et al., 2021).1367
G Robustness Analysis of Baselines
In addition to the models analyzed in the main pa-
per, we evaluate three further models. The first is
the ‘toxicity’ attribute returned by Perspective API
(P-TX). We test this model because toxicity rat-
ings are the most popular attributes.The second
and third models are two uncased BERT models
(Devlin et al., 2019) trained on publicly-available
academic datasets. B-D is trained on the Davidson
et al. (2017) dataset of 24,783tweets, labeled as
hateful ,offensive andneither . B-F is trained on
the Founta et al. (2018) dataset of 99,996tweets,
labeled as hateful ,abusive ,spam andnormal . Any
labels besides hateful are binarized into a single
non-hateful label. Two factors motivated the test-
ing of these models. (1) There is a lack of emoji
in the training data: the Davidson et al. dataset
has 5.8% hateful cases, but only 7.4% of these con-
tain emoji, and the Founta et al. dataset has 5.0%
hateful cases, of which 14.7% contain emoji. (2)
Despite BERT being a commonly-used architec-
ture for hate speech detection, it encodes emoji
as<UNK> tokens by default. We compare perfor-
mance of the full set of pre-emoji models including
those analyzed in the main paper (P-IA, P-TX, B-
D, B-F, R5-T) versus our ‘emoji-aware’ models
from each round of data collection (R6-T, R7-T,
R8-T). Tab. 8 shows performance against the ad-
versarially produced datasets in the emoji rounds
ofH B and text rounds from Vidgen
et al. (2021b), alongside two benchmark evaluation
setsH C andHC (Röttger
et al., 2021). P-TX has comparable performance to
P-IA. B-D and B-F perform poorly on H -
C (F1 = 0.64, 0.61), and even more poorly on
the adversarial test sets of emoji (F1 = 0.27, 0.32).H Fairness Considerations
As well as the six metrics discussed in the main pa-
per, Tab. 9 shows two further fairness metrics: (1)
the demographic parity ratio (Agarwal et al., 2019),
which is equal to one when the selection rate is
balanced across subgroups, and (2) the equalized
odds ratio (Hardt et al., 2016; Agarwal et al., 2018),
which is equal to one when the true positive, true
negative, false positive, and false negative rates are
balanced across subgroups. The academic BERT
models perform worst (0.14 to 0.28 across the met-
rics), followed by the Perspective models (0.41 to
0.67). The most fair model is R5-T with Demo-
graphic Parity Ratio of 0.90 and Equalized Odds
Ratio of 0.77. However, the overall performance of
the emoji-aware models is higher (see Fig. 2): for
every subgroup, R8-T has higher accuracy, preci-
sion and recall, and lower false positive and nega-
tive rates. Thus, the marginally worse performance
by between-group fairness metrics is paired with a
better ability to protect all these subgroups against
emoji-based hate.1368