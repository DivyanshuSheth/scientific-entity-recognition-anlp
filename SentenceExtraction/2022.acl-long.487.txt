
Shuo WangZhixing TanYang LiuDept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China
Beijing National Research Center for Information Science and TechnologyInstitute for AI Industry Research, Tsinghua University, Beijing, ChinaInternational Innovation Center of Tsinghua University, Shanghai, ChinaQuan Cheng LaboratoryInstitute for Guo Qiang, Tsinghua University, Beijing, China
wangshuo.thu@gmail.com {zxtan, liuyang2011 }@tsinghua.edu.cn
Abstract
Lexically constrained neural machine transla-
tion (NMT), which controls the generation of
NMT models with pre-specified constraints, is
important in many practical scenarios. Due
to the representation gap between discrete con-
straints and continuous vectors in NMT models,
most existing works choose to construct syn-
thetic data or modify the decoding algorithm
to impose lexical constraints, treating the NMT
model as a black box. In this work, we propose
to open this black box by directly integrating
the constraints into NMT models. Specifically,
we vectorize source and target constraints into
continuous keys and values, which can be uti-
lized by the attention modules of NMT mod-
els. The proposed integration method is based
on the assumption that the correspondence be-
tween keys and values in attention modules is
naturally suitable for modeling constraint pairs.
Experimental results show that our method
consistently outperforms several representative
baselines on four language pairs, demonstrating
the superiority of integrating vectorized lexical
constraints.
1 Introduction
Controlling the lexical choice of the translation
is important in a wide range of settings, such as
interactive machine translation (Koehn, 2009), en-
tity translation (Li et al., 2018), and translation in
safety-critical domains (Wang et al., 2020). How-
ever, different from the case of statistical machine
translation (Koehn et al., 2007), it is non-trivial to
directly integrate discrete lexical constraints into
neural machine translation (NMT) models (Bah-
danau et al., 2015; Vaswani et al., 2017), whose
hidden states are all continuous vectors that are
difficult for humans to understand.
In accordance with this problem, one branch
of studies directs its attention to designing ad-Figure 1: An example of the integration of vectorized
lexical constraints into attention proposed in this work.
We omit queries for simplicity. Blue and green squares
denote the continuous representation of the source sen-
tence and the constraints, respectively. The provided
constraints are "Beatles →Beatles" and "band →乐团".
vanced decoding algorithms (Hokamp and Liu,
2017; Hasler et al., 2018; Post and Vilar, 2018)
to impose hard constraints and leave NMT models
unchanged. For instance, Hu et al. (2019) propose
avectorized dynamic beam allocation (VDBA) al-
gorithm, which devotes part of the beam to can-
didates that have met some constraints. Although
this kind of method can guarantee the presence of
target constraints in the output, they are found to
potentially result in poor translation quality (Chen
et al., 2021b; Zhang et al., 2021), such as repeated
translation or source phrase omission.
Another branch of works proposes to learn
constraint-aware NMT models through data aug-
mentation. They construct synthetic data by replac-
ing source constraints with their target-language
correspondents (Song et al., 2019) or appending tar-
get constraints right after the corresponding source
phrases (Dinu et al., 2019). During inference, the
input sentence is edited in advance and then pro-
vided to the NMT model. The major drawback of
data augmentation based methods is that they may
suffer from a low success rate of generating target
constraints in some cases, indicating that only ad-
justing the training data is sub-optimal for lexical
constrained translation (Chen et al., 2021b).
To make NMT models better learn from and
cope with lexical constraints, we propose to lever-7063age attention modules (Bahdanau et al., 2015) to
explicitly integrate vectorized lexical constraints.
As illustrated in Figure 1, we use vectorized source
constraints as additional keys and vectorized tar-
get constraints as additional values. Intuitively, the
additional keys are used to estimate the relevance
between the current query and the source phrases
while the additional values are used to integrate the
information of the target phrases. In this way, each
revised attention is aware of the guidance to trans-
latewhich source phrase intowhat target phrase .
Experiments show that our method can signifi-
cantly improve the ability of NMT models to trans-
late with constraints, indicating that the correspon-
dence between attention keys and values is suitable
for modeling constraint pairs. Inspired by recent
progress in controlled text generation (Dathathri
et al., 2020; Pascual et al., 2021), we also intro-
duce a plug-in to the output layer that can further
improve the success rate of generating constrained
tokens. We conduct experiments on four language
pairs and find that our model can consistently out-
perform several representative baselines.
2 Neural Machine Translation
Training The goal of machine translation is
to translate a source-language sentence x=
x. . . xinto a target-language sentence y=
y. . . y. We use P(y|x;θ)to denote an NMT
model (Vaswani et al., 2017) parameterized by θ.
Modern NMT models are usually trained by maxi-
mum likelihood estimation (Bahdanau et al., 2015;
Vaswani et al., 2017), where the log-likelihood is
defined as
logP(y|x;θ) =XlogP(y|y,x;θ),(1)
in which yis a partial translation.
Inference The inference of NMT models can be
divided into two sub-processes:
•probability estimation : the model estimates
the token-level probability distribution for
each partial hypothesis within the beam;
•candidate selection : the decoding algorithm
selects some candidates based on the proba-
bility estimated by the NMT model.
These two sub-processes are performed alterna-
tively until reaching the maximum length or gener-
ating the end-of-sentence token.3 Approach
This section explains how we integrate lexical con-
straints into NMT models. Section 3.1 illustrates
the way we encode discrete constraints into contin-
uous vectors, Section 3.2 details how we integrate
the vectorized constraints into NMT models, and
Section 3.3 describes our training strategy.
3.1 Vectorizing Lexical Constraints
Lets=s, . . . ,sbe the source constraints
andt=t, . . . ,tbe the target constraints.
Given a constraint pair ⟨s,t⟩, lexically con-
strained translation requires that the system must
translate the source phrase sinto the target
phrase t. Since the inner states of NMT mod-
els are all continuous vectors rather than discrete
tokens, we need to vectorize the constraints before
integrating them into NMT models.
For the n-th constraint pair ⟨s,t⟩, let|s|
and|t|be the lengths of sandt, respec-
tively. We use S∈Rto denote the vector
representation of the k-th token in s, which is
the sum of word embedding and positional embed-
ding (Vaswani et al., 2017). Therefore, the matrix
representation of sis given by:
S=h
S;. . .;Si
, (2)
where S∈Ris the concatenation of all
vector representations of tokens in s. Similarly,
the matrix representation of the target constraint
tisT∈R. Note that the positional
embedding for each constraint is calculated inde-
pendently, which is also independent of the posi-
tional embeddings of the source sentence xand the
target sentence y.
3.2 Integrating Vectorized Constraints
We adopt Transformer (Vaswani et al., 2017) as
our NMT model, which is nowadays one of the
most popular and effective NMT models (Liu et al.,
2020). Typically, a Transformer consists of an en-
coder, a decoder, and an output layer, of which
the encoder and decoder map discrete tokens into
vectorized representations and the output layer con-
verts such representations into token-level proba-
bility distributions. We propose to utilize the at-
tention modules to integrate the constraints into
the encoder and decoder and use a plug-in mod-
ule to integrate constraints into the output layer.
We change the formal representation of our model7064
from P(y|x;θ)toP(y|x,s,t;θ)to indicate that
the model explicitly considers lexical constraints
when estimating probability.
Constraint-Related Keys and Values We pro-
pose to map source and target constraints into addi-
tional keys and values, which are called constraint-
related keys and values , in order to distinguish from
the original keys and values in vanilla attention
modules. In practice, source and target constraints
may have different lengths and they are usually not
monotonically aligned (Du et al., 2021), making it
challenging to directly convert the constraints into
keys and values. To fix this problem, We adopt a
multi-head attention layer (Vaswani et al., 2017)
to align the bilingual constraints. The constraint-
related keys and values for the n-th constraint pair
are given by
K=S,
V= attn
S,T,T
,(3)
where K∈RandV∈R.
attn(Q,K,V)denotes the multi-head attention
function. Note that the resulting KandV
are of the same shape. Vcan be seen as a re-
distributed version of the representation of target
constraints. The constraint-related keys and values
of each constraint pair are calculated separately and
then concatenated together:
K= [K;. . .;K],
V= [V;. . .;V],(4)
where K∈RandV∈R.|s|is the
total length of all the Nsource constraints.Integration into the Encoder The encoder of
Transformer is a stack of Iidentical layers, each
layer contains a self-attention module to learn
context-aware representations. For the i-th layer,
the self-attention module can be represented as
attn
H,H,H
, (5)
where H∈Ris the output of the (i−1)-
th layer, and His initialized as the sum of word
embedding and positional embedding (Vaswani
et al., 2017). For different layers, H may lay
in various manifolds, containing different levels
of information (V oita et al., 2019). Therefore, we
should adapt the constraint-related keys and val-
ues for each layer before the integration. We use a
two-layer adaptation network to do this:
K = [adapt( K);H],
V = [adapt( V);H],(6)
where adapt( ·)denotes the adaptation network,
which consists of two linear transformations with
shape d×dand a ReLU activation in between.
The adaptation networks across all layers are in-
dependent of each other. K∈R
andV∈Rare the constraint-aware
keys and values for the i-th encoder layer, respec-
tively. The vanilla self-attention module illustrated
in Eq. (5) is revised into the following form:
attn
H,K,V
. (7)
Figure 2b plots an example of the integration
into the encoder self-attention.7065
Integration into the Decoder The integration
into the decoder is similar to that into the en-
coder, the major difference is that we use the cross-
attention module to model constraints for the de-
coder. The decoder of the Transformer is a stack of
Jidentical layers, each of which is composed of a
self-attention, a cross-attention, and a feed-forward
module. We integrate vectorized constraints into
the cross-attention module for the decoder. For-
mally, the vanilla cross-attention is given by
attn
S,H,H
, (8)
where S∈Ris the output of the self-
attention module in the j-th decoder layer, and
H∈Ris the output of the last encoder
layer. We adapt the constraint-related keys and val-
ues to match the manifold in the j-th decoder layer:
K= [adapt( K);H],
V= [adapt( V);H].(9)
Then we revise the vanilla cross-attention
(Eq. (8)) into the following form:
attn
S,K,V
. (10)
Figure 2c plots an example of the integration
into the decoder cross-attention.
Integration into the Output Layer In vanilla
Transformer, an output layer is employed to convert
the output of the last decoder layer into token-level
probabilities. Let h∈Rbe the decoder output
at the t-th time step, the output probability of the
Transformer model is defined as
P (y|y,x,s,t;θ) = softmax
hW
,
(11)where W∈Ris the output embedding ma-
trix and |V|is the vocabulary size. Inspired by the
plug-and-play method (Pascual et al., 2021) in the
field of controlled text generation (Dathathri et al.,
2020; Pascual et al., 2021), we introduce an addi-
tional probability distribution over the vocabulary
to better generate constrained tokens:
P(y|y,x,s,t;θ)
=

0 y /∈t
max
0,cosw
|w|,h
|h|
y∈t,
(12)
where w∈Ris the word embedding of token
yandtis the sequence of all the target-side con-
strained tokens. We also use a gating sub-layer to
control the strength of the additional probability:
g(y,h)
= sigmoid
tanhh
wW;hWi
W
,
(13)
where W∈R,W∈R, and W∈
Rare three trainable linear transformations.
The final output probability is given by
P(y|y,x,s,t;θ)
= (1−g(y,h))P (y|y,x,s,t;θ)
+g(y,h)P(y|y,x,s,t;θ).
(14)
3.3 Training and Inference
Training The proposed constraint-aware NMT
model should not only generate pre-specified con-
straints but also maintain or improve the translation
quality compared with vanilla NMT models. We
thus propose to distinguish between constraint to-
kens and constraint-unrelated tokens during train-
ing. Formally, the training objective is given by
L(y|x,s,t;θ)
=αXlogP(y|y,x,s,t;θ)
+βXlogP(y|y,x,s,t;θ),(15)
where αandβare hyperparameters to balance the
learning of constraint generation and translation.
We can divide the parameter set of the whole
model into two subsets: θ=θ∪θ, where θ7066is a set of original vanilla model parameters and
θis a set of newly-introduced parameters that are
used to vectorize and integrate lexical constraints.
Since θis significantly smaller than θ, it requires
much less training iterations. Therefore, we adopt
the strategy of two-stage training (Tu et al., 2018;
Zhang et al., 2018) for model optimization. Specif-
ically, we optimize θusing the standard NMT
training objective (Bahdanau et al., 2015; Vaswani
et al., 2017) at the first stage and then learn the
whole model θat the second stage. The second
stage is significantly shorter than the first stage, we
will give more details in Section 4.1.
Inference As discussed in Section 2, the infer-
ence process is composed of two sub-processes:
probability estimation and candidate selection. In
this work, we aim to improve the probability esti-
mation sub-process and our method is orthogonal
to constrained decoding algorithms (Hokamp and
Liu, 2017; Post and Vilar, 2018; Hu et al., 2019),
which instead focus on candidate selection. There-
fore, we can employ not only beam search but also
constrained decoding algorithms at inference time.
We use VDBA (Hu et al., 2019) as the default
constrained decoding algorithm, which supports
batched inputs and is significantly faster than most
other counterparts (Hokamp and Liu, 2017; Post
and Vilar, 2018; Hasler et al., 2018).
4 Experiments
4.1 Setup
Training Data In this work, we conduct ex-
periments on Chinese ⇔English (Zh ⇔En) and
German ⇔English (De ⇔En) translation tasks. For
Zh⇔En, the training set contains 1.25M sentence
pairs from LDC. For De ⇔En, the training set is
from the WMT 2014 German ⇔English translation
task, which consists of 4.47M sentence pairs. We
apply BPE (Sennrich et al., 2016b) with 32K joint
merge operations for both Zh ⇔En and De ⇔En.
Evaluation Data Following Chen et al. (2021b),
we evaluate our approach on the test sets with
human-annotated alignments, which are widely
used in related studies (Chen et al., 2020b, 2021a).We find the alignment test sets have significant over-
laps with the corresponding training sets, which is
not explicitly stated in previous works. In this work,
we remove the training examples that are covered
by the alignment test sets. For Zh ⇔En, we use
the alignment datasets from Liu et al. (2005), in
which the validation and test sets both contain 450
sentence pairs. For De ⇔En, we use the alignment
dataset from Zenkel et al. (2020)as the test set,
which consists of 508 sentence pairs. Since there is
no human-annotated alignment validation sets for
De⇔En, we use fast-alignto annotate the
newstest 2013 as the validation set for De ⇔En.
Lexical Constraints In real-world applications,
lexical constraints are usually provided by human
translators. We follow Chen et al. (2021b) to simu-
late the practical scenario by sampling constraints
from the phrase pairs that are extracted from par-
allel data using alignments. The script for phrase
pair extraction is publicly available.For the val-
idation and test sets of Zh ⇔En and the test set of
De⇔En, we use human-annotated alignments to
extract phrase pairs. For the training corpora in
both Zh ⇔En and De ⇔En, we use fast-align
to firstly learn an alignment model and then use
the model to automatically annotate the alignments.
The validation set of De ⇔En is also annotated by
the alignment model learned on the correspond-
ing training corpus. We use the same strategy as
Chen et al. (2021b) to sample constraints from the
extracted phrase pairs. More concretely, the num-
ber of constraints in each sentence is up to 3. The
length of each constrained phrase is uniformly sam-
pled among 1 and 3. For each sentence pair, all the
constraint pairs are shuffled and then supplied to
the model in an unordered manner.
Model Configuration We use the base set-
ting (Vaswani et al., 2017) for our model. Specif-
ically, the hidden size dis 512 and the depths of
both the encoder and the decoder are 6. Each multi-
head attention module has 8 individual attention
heads. Since our method introduces additional pa-
rameters, we use a larger model with an 8-layer
encoder and an 8-layer decoder to assimilate the7067MethodBLEU CSR (%)
Z→E E→Z D→E E→D Avg. Z →E E→Z D→E E→D Avg.
Vanilla 30.4 56.1 31.7 24.5 35.7 26.9 30.5 19.2 13.8 22.6
VDBA 31.6 56.4 35.0 27.9 37.7 99.4 98.9 100.0 100.0 99.6
Replace 33.6 58.3 34.9 28.1 38.7 89.7 90.2 93.2 90.7 91.0
CDAlign 32.1 58.0 35.2 28.338.4 87.4 90.0 96.8 95.7 92.5
Ours 34.4 59.1 35.9 28.8 39.6 99.4 98.9 100.0 100.0 99.6
parameter count for the baselines. For Zh ⇔En, we
optimize θfor 50K iterations at the first stage and
then optimize θfor 10K iterations at the second
stage. For a fair comparison, we train the base-
lines for 60K iterations in total. For De ⇔En, we
optimize θfor 90K iterations then optimize θfor
10K iterations. The baselines are trained for 100K
iterations. All the involved models are optimized
by Adam (Kingma and Ba, 2015), with β= 0.9,
β= 0.98andϵ= 10. The dropout rate is
set to 0.3 for Zh ⇔En and 0.1 for De ⇔En. Label
smoothing is employed and the smoothing penalty
is set to 0.1 for all language pairs. We use the same
learning rate schedule as Vaswani et al. (2017).
All models are trained on 4 NVIDIA V100 GPUs
and evaluated on 1 NVIDIA V100 GPU. During
training, each mini batch contains roughly 32K to-
kens in total across all GPUs. We set the values
ofαandβbased on the results on the validation
set. Specifically, for models using VDBA, we set
α=β= 0.5, while for models using beam search,
we set α= 0.8andβ= 0.2. The beam size is set
to4during inference.
Baselines We compare our approach with three
representative baselines:
•VDBA (Hu et al., 2019): dynamically devot-
ing part of the beam for constraint-related hy-
potheses at inference time;
•Replace (Song et al., 2019): directly replacing
source constraints in the training data with
their corresponding target constraints. The
model is also improved with pointer network;
•CDAlign (Chen et al., 2021b): explicitly using
an alignment model to decide the position to
insert target constraints during inference.Evaluation Metrics We evaluate the involved
methods using the following two metrics:
•BLEU : we use sacreBLEU(Post, 2018) to
report the BLEU score;
•Copying Success Rate (CSR) : We follow Chen
et al. (2021b) to use the percentage of con-
straints that are successfully generated in the
translation as the CSR, which is calculated at
word level after removing the BPE separator.
We use compare-mt (Neubig et al., 2019) for
significance testing, with bootstrap = 1000 and
prob _thresh = 0 .05.
4.2 Main Results
Table 1 shows the results of lexically constrained
translation on test sets of all four translation tasks.
All the investigated methods can effectively im-
prove the CSR over the vanilla Transformer. The
CSR of VDBA on Zh ⇔En is not 100.0% for the
reason that some target constraints contain out-of-
vocabulary tokens. Replace (Song et al., 2019)
achieves better BLEU scores on three translation
directions (i.e., Zh ⇔En and En →De) than VDBA,
but its CSR is much lower. CDAlign (Chen et al.,
2021b) also performs better than Replace on aver-
age regarding CSR. Our method consistently out-
performs all the three baselines across the four
translation directions in terms of BLEU, demon-
strating the necessity of integrating vectorized con-
straints into NMT models. Decoding with VDBA,
we also achieve the highest CSR. To disentangle
the effect of integrating vectorized constraints and7068
VDBA, we also report the result of our model us-
ing beam search in Table 2. Decoding with beam
search, our model can also achieve a better BLEU
score than the baselines and the CSR is higher than
both Replace and CDAlign on average.
4.3 Ablation Study
We investigate the effect of different components
through an ablation study, the results are shown
in Table 3. We find that only integrating lexical
constraints into attention can significantly improve
the CSR over the vanilla model (91.5% vs. 25.5%),
which is consistent with our motivation that the cor-
respondence between keys and values is naturally
suitable for modeling the relation between source
and target constraints. Plugging target constraints
into the output layer can further improve the per-
formance, but the output plug-in itself can only
generate 61.9% of constraints. When decoding
with VDBA, combining both the two types of inte-
gration achieves the best BLEU score, indicating
that every component is important for the model to
translate with constraints.
4.4 Code-Switched Translation
Task Description and Data Preparation An in-
teresting application of lexically constrained ma-
chine translation is code-switched translation, of
which the output contains terms across different
languages. Figure 1 shows an example of code-
switched translation, where the output Chinese sen-
tence should include the English token "Beatles".
Code-switched machine translation is important
in many scenarios, such as entity translation (Li
et al., 2018) and the translation of sentences con-
taining product prices or web URLs (Chen et al.,
2021b). In this work, we evaluate the performance
of several approaches on code-switched machine
translation. The parallel data and extracted con-
straint pairs for each language pair are the same as
those used in the lexically constrained translation
task. To construct the training and evaluation data
for code-switched translation, we randomly replace
50% of the target constraints with their correspond-
ing source constraints. The target sentence is also
switched if it contains switched target constraints.
Results Table 4 gives the results of the code-
switched translation task. The CSR of Re-
place (Song et al., 2019) is lower than 50% across
all the four translation directions, indicating that
simply replacing the training data can not handle
the code-switched translation. A potential reason
is that it is difficult for the NMT model to decide
whether to translate or copy some source phrases in
the input sentence. Surprisingly, VDBA, CDAlign,7069MethodBatch Size (# Sent.)
1 128
Vanilla 1.0 × 43.2×
VDBA 0.5 × 2.1×
Replace 0.9 × 40.5×
CDAlign 0.7 × n/a
Ours 0.5 × 2.3×
w/o VDBA 0.9 × 39.2×
Model DAProb.ECE
All Const.
VanillaB0.69 0.74 8.03
Ours 0.70 0.80 7.19
VanillaV0.64 0.42 10.73
Ours 0.67 0.61 7.72
and our method all perform well in this scenario,
and our method outperforms the two baselines.
These results suggest the capability of our method
to cope with flexible types of lexical constraints.
5 Discussion
5.1 Inference Speed
We report the inference speed of each involved ap-
proach in Table 5. The speed of Replace is close
to that of the vanilla Transformer, but its CSR is
much lower than other methods. Since the open-
sourced implementation of CDAligndoes not sup-
port batched decoding, we compare our method
with CDAlign with batch _size = 1 . The speed of
our method using beam search is faster than that
of CDAlign (0.9 ×vs. 0.7 ×). When provided with
batched inputs, our method can slightly speed up
VDBA (2.3 ×vs. 2.1 ×). A potential reason is that
the probability estimated by our model is more
closely related to the correctness of the candidates,
making target constraints easier to find.
5.2 Calibration
To validate whether the probability of our model
is more accurate than vanilla models, we followZ→E E →Z D →E E→D Avg.
38.9% 43.4% 27.7% 32.2% 35.6%
Wang et al. (2020) to investigate the gap between
the probability and the correctness of model out-
puts, which is measured by the inference expected
calibration error (ECE). As shown in Table 6, the
inference ECE of our method is much lower than
that of the vanilla model, indicating that the prob-
ability of our model is more accurate than vanilla
models. To better understand the calibration of our
model and the baseline model, we also estimate the
average probability of all the predicted tokens and
the constrained tokens. The results show that our
model assigns higher probabilities to constrained
tokens, which are already known to be correct.
5.3 Memory vs. Extrapolation
To address the concern that the proposed model
may only memorize the constraints seen in the train-
ing set, we calculate the overlap ratio of constraints
between training and test sets. As shown in Table 7,
we find that only 35.6% of the test constraints are
seen in the training data, while the CSR of our
model decoding with beam search is 94.4%. The
results indicate that our method extrapolates well
to constraints unseen during training.
5.4 Case Study
Table 8 shows some example translations of differ-
ent methods. We find Replace tends to omit some
constraints. Although VDBA and CDAlign can
successfully generate constrained tokens, the trans-
lation quality of the two methods is not satisfying.
Our result not only contains constrained tokens
but also maintains the translation quality compared
with the unconstrained model, confirming the ne-
cessity of integrating vectorized constraints into
NMT models.
6 Related Work
6.1 Lexically Constrained NMT
One line of approaches to lexically constrained
NMT focuses on designing advanced decoding al-
gorithms (Hasler et al., 2018). Hokamp and Liu
(2017) propose grid beam search (GBS), which en-
forces target constraints to appear in the output by7070Constraints Zielsetzung →objectives ,Fiorella →Fiorella
SourceMit der Zielsetzung des Berichtes von Fiorella Ghilardotti allerdings sind
wir einverstanden .
Reference Even so , we do agree with the objectives ofFiorella Ghilardotti’s report .
Vanilla However , we agree with the aims of the Ghilardotti report .
VDBAFiorella ’s Ghilardotti report , however , has our objectives of being one
which we agree with .
Replace However , we agree with the objectives of the Ghilardotti report .
CDAlign However , we agree with objectives of Fi Fiorella Ghilardotti’s report .
Ours We agree with the objectives ofFiorella Ghilardotti’s report , however .
enumerating constraints at each decoding step. The
beam size required by GBS varies with the number
of constraints. Post and Vilar (2018) propose dy-
namic beam allocation (DBA) to fix the problem
of varying beam size for GBS, which is then ex-
tended by Hu et al. (2019) into VDBA that supports
batched decoding. There are also some other con-
strained decoding algorithms that leverage word
alignments to impose constraints (Song et al., 2020;
Chen et al., 2021b). Although the alignment-based
decoding methods are faster than VDBA, they may
be negatively affected by noisy alignments, result-
ing in low CSR. Recently, Susanto et al. (2020)
adopt Levenshtein Transformer (Gu et al., 2019)
to insert target constraints in a non-autoregressive
manner, for which the constraints must be provided
with the same order as that in the reference.
Another branch of studies proposes to edit the
training data to induce constraints (Sennrich et al.,
2016a). Song et al. (2019) directly replace source
constraints with their target translations and Dinu
et al. (2019) insert target constraints into the source
sentence without removing source constraints. Sim-
ilarly, Chen et al. (2020a) propose to append target
constraints after the source sentence.
In this work, we propose to integrate vectorized
lexical constraints into NMT models. Our work
is orthogonal to both constrained decoding and
constraint-oriented data augmentation. A similar
work to us is that Li et al. (2020) propose to use
continuous memory to store only the target con-
straint, which is then integrated into NMT models
through the decoder self-attention. However, Li
et al. (2020) did not exploit the correspondence
between keys and values to model both source and
target constraints.6.2 Controlled Text Generation
Recent years have witnessed rapid progress in con-
trolled text generation. Dathathri et al. (2020) pro-
pose to use the gradients of a discriminator to con-
trol a pre-trained language model to generate to-
wards a specific topic. Liu et al. (2021) propose a
decoding-time method that employs experts to con-
trol the generation of pre-trained language models.
We borrow the idea presented in Pascual et al.
(2021) to insert a plug-in into the output layer. The
difference between our plug-in network and Pas-
cual et al. (2021) is that we use an input-dependent
gate to control the effect of the plugged probability.
7 Conclusion
In this work, we propose to vectorize and integrate
lexical constraints into NMT models. Our basic
idea is to use the correspondence between keys and
values in attention modules to model constraint
pairs. Experiments show that our approach can
outperform several representative baselines across
four different translation directions. In the future,
we plan to vectorize other attributes, such as the
topic, the style, and the sentiment, to better control
the generation of NMT models.
Acknowledgments
This work is supported by the National Key R&D
Program of China (No. 2018YFB1005103), the
National Natural Science Foundation of China (No.
61925601, No. 62006138), and the Tencent AI
Lab Rhino-Bird Focused Research Program (No.
JR202031). We sincerely thank Guanhua Chen and
Chi Chen for their constructive advice on technical
details, and all the reviewers for their valuable and
insightful comments.7071References70727073