
Hongyi YuanZheng YuanSheng Yu
Tsinghua University
{yuanhy20,yuanz17}@mails.tsinghua.edu.cn
syu@tsinghua.edu.cn
Abstract
Entities lie in the heart of biomedical natu-
ral language understanding, and the biomed-
ical entity linking (EL) task remains chal-
lenging due to the ﬁne-grained and diversi-
form concept names. Generative methods
achieve remarkable performances in general
domain EL with less memory usage while
requiring expensive pre-training. Previous
biomedical EL methods leverage synonyms
from knowledge bases (KB) which is not triv-
ial to inject into a generative method. In
this work, we use a generative approach to
model biomedical EL and propose to inject
synonyms knowledge in it. We propose KB-
guided pre-training by constructing synthetic
samples with synonyms and deﬁnitions from
KB and require the model to recover con-
cept names. We also propose synonyms-aware
ﬁne-tuning to select concept names for train-
ing, and propose decoder prompt and multi-
synonyms constrained preﬁx tree for inference.
Our method achieves state-of-the-art results
on several biomedical EL tasks without can-
didate selection which displays the effective-
ness of proposed pre-training and ﬁne-tuning
strategies. The source code is available at
Github.com/Yuanhy1997/GenBioEL.
1 Introduction
Biomedical entity linking (EL) refers to mapping
a biomedical mention (i.e., entity) in free texts to
its concept in a biomedical knowledge base (KB)
(e.g., UMLS (Bodenreider, 2004)). This is one of
the most concerned tasks of research in medical
natural language processing which is highly related
to high-throughput phenotyping (Yu et al., 2015),
relation extraction (Li et al., 2016), and automatic
diagnosis (Yuan and Yu, 2021).
Recent methods in biomedical EL mainly used
neural networks to encode mentions and each con-
cept name into the same dense space, then linkedmentions to corresponding concepts depending
on embedding similarities (Sung et al., 2020; Lai
et al., 2021; Bhowmik et al., 2021; Ujiie et al.,
2021; Agarwal et al., 2021). Synonyms knowl-
edge has been injected into these similarity-based
methods by contrastive learning (Liu et al., 2021a;
Yuan et al., 2022). For example in UMLS, con-
cept C0085435 has synonyms: Reiter syndrome ,
Reactive arthritis andReA which help models to
learn different names of a concept entity. However,
similarity-based methods requires large memory
footprints to store representation for each concept
which are hard to deploy.
In general domain, GENRE (Cao et al., 2021a)
viewed EL as a seq2seq task which inputs mentions
with contexts and outputs concept names token-
by-token. Mentions, contexts and concepts can
be mingled due to the power of transformers, and
the model does not need to store representations
for each concept during inference. GENRE pre-
trained on Wikipedia EL datasets to boost perfor-
mances. However, directly implementing GENRE
on biomedical EL cannot harvest satisfying re-
sults. The gap occurs in two aspects: (1) There
are no such large-scale human-labeled biomedical
EL datasets for pre-training. (2) Biomedical con-
cepts may have multiple synonyms. We ﬁnd the
results are sensitive to the synonyms selection for
training, and simply using a 1-to-1 mapping be-
tween names and concepts as Cao et al. (2021a,b)
may hurt performances.
To address the above issues, we propose KB-
guided pre-training and synonyms-aware ﬁne-
tuning to improve generative EL. For pre-training ,
we construct pre-training samples using synonyms
and deﬁnitions collected from KBs and sentence
templates. KB-guided pre-training has the same
format as seq2seq EL, which ﬁlls the gap of loss of
pre-training corpus. Compared to the method intro-
duced in Cao et al. (2021a), ours performs better
in biomedical EL with fewer resources. For ﬁne-4038tuning , we propose decoder prompts to highlight
mentions. We ﬁnd the model tends to generate
textually similar names to mentions. Hence tex-
tual similar criterion is proposed for selecting con-
cept names during ﬁne-tuning. During inference,
we propose a multi-synonyms constrained preﬁx
tree, which results in signiﬁcantly improved perfor-
mance. The overview of our approach is illustrated
in Figure 1.
We conduct experiments on various biomed-
ical EL datasets and achieve SOTA results on
COMETA, BC5CDR, and AskAPatient (AAP)
even without candidate selection. Extensive studies
show the effectiveness of our proposed pre-training
and ﬁne-tuning schemes.
2 Approach
Deﬁne a set of concepts Eas target concepts (i.e.
concepts from target KBs). For each concept e∈E,
we have a set of synonyms names f(e) ={s|i∈
{1,...,n}}. All the synonyms forms a name
setS=/uniontext{s|i∈{1,...,n}}. Names-to-
concept mappings can be deﬁned by: σ(s) =e
whereσ=f. For mention mwith left and right
contextscandcwhich gold label is e∈E, we
need to ﬁnd the target concept ˆe∈E.m,cands
comprise a sequence of tokens.
2.1 Seq2seq EL
Our model applies an encoder-decoder transformer
architecture following Cao et al. (2021a). The en-
coder input is: [BOS]c[ST]m[ET]c[EOS],
where [ST] and[ET] are the special tokens mark-
ingm. For the decoder side, unlike GENRE de-
coding target names directly, we use simple preﬁx
promptsP=<mis> to strengthen the inter-
action between mentions and make the decoder
side output resemble a natural language sentence:
[BOS]miss,wheresis a target name be-
long to label concept e. The training objective of
Seq2Seq EL is to maximize the likelihood:
p(s|P,c,m ) =/productdisplayp(y|y,P,c,m ),
whereNis the number of tokens of sandyindi-
cates theith token. The inference of Seq2seq EL
applies beam search (Sutskever et al., 2014) with
targets constrained to the name set Sby a preﬁxtree (constructed by name set S). Unlike mGENRE
Cao et al. (2021b) using provided candidates to de-
crease the size of the preﬁx tree, we use the whole
name setSinstead.
2.2 KB-Guided Pre-training
As the training data of EL is tiny compared to
the vast number of concepts in KB, thus it makes
EL for some mentions zero-shot problems. We
want to leverage synonyms knowledge from KB to
enhance the model’s performance. Injecting syn-
onyms knowledge to the encoder-only models can
be done by contrastive learning (Liu et al., 2021a;
Yuan et al., 2022). However, such a paradigm can-
not directly apply to encoder-decoder architecture
as entities are not represented by dense embed-
dings. To mitigate this problem, we construct a
pre-training task that shares a similar form as Sec-
tion 2.1. We manually deﬁne a set of clause tem-
plates to splice with synonyms and deﬁnitions in
KB to form input synthetic language discourses.
Concretely, we select two synonyms sandsand
deﬁnitioncof a concept e∈E. Then we ran-
domly pick a template to concatenate them to form
the encoder input, here we give two examples:
[BOS][ST] s[ET] is deﬁned as c[EOS]
[BOS]cdescribes [ST]s[ET][EOS]
For the decoder: [BOS]siss.cis the sim-
ulated context and sis for model to predict. If
deﬁnitions are absent in KB, we will use other syn-
onyms to construct c. All templates we used can
be found in Appendix E.2.
2.3 Synonyms-Aware Fine-tuning
We propose and validate by experiments in Sec-
tion 4 that seq2seq EL is profoundly inﬂuenced by
the textual similarity between mentions and con-
cept names. It tends to generate textually similar
names. We select the target name by calculating
the character 3-gram TF-IDF similarity (Neumann
et al., 2019) between mention mand all synonyms
{s}and choosing the most similar one as
s= arg maxcos(TFIDF( m),TFIDF(s)).
By the textual similarity criterion, we manually
reduce the difﬁculty of ﬁne-tuning. We do not use
this criterion for pre-training since we want it to
learn various synonyms to improve generalization.4039
Different from GENRE using only one canonical
name for each concept for preﬁx tree, we use multi-
synonym names (i.e. S) to construct preﬁx tree.
During inference, we apply preﬁx tree constrained
beam search to decode the name ˆs, and map to the
concept ˆe=σ(ˆs)via N-to-1 names-to-concept
mappingσ.
3 Experiments
3.1 Datasets and KBs
Pre-training We use a subset of UMLS st21pv
(Mohan and Li, 2019) as Efor pre-training. It con-
tains 2.37M concepts, where 160K concepts con-
tain deﬁnitions and 1.11M concepts have multiple
synonyms. While pre-training, we iterate concepts
and synonyms to construct inputs and outputs.
Fine-tuning We evaluate our model on BC5CDR
(Li et al., 2016), NCBI (Do ˘gan et al., 2014),
COMETA (Basaldella et al., 2020) and AAP (Lim-
sopatham and Collier, 2016). These benchmarks
focus on different entity types, including disease,
chemicals, and colloquial terms. For fair compar-
ison, we follow Varma et al. (2021) in BC5CDR,
Lai et al. (2021) in NCBI and COMETA, and Lim-
sopatham and Collier (2016) in AAP to construct
dataset splits and target KB concepts E. Name set
Sis constructed by synonyms from UMLS and
original KB which is detailed in Appendix E.1.
Datasets summaries are shown in Appendix A.
We use recall @1/@5as metrics for performance
illustration, and recall @5results are listed in Ap-
pendix C.
3.2 Implementation Details
We use BART-large (Lewis et al., 2020) as the
model backbone. We pre-train and ﬁne-tune our
model using teacher forcing (Williams and Zipser,
1989) and label smoothing (Szegedy et al., 2016)
which are standard in seq2seq training. The hyper-
parameters can be found in Appendix E.3.
3.3 Main Results
Table 1 and 2 compare the recall @1with state-
of-the-art(SOTA) methods. Our method with KB-
guided pre-training exceeds the previous SOTA on
BC5CDR, COMETA and AAP, which is up to 1.4
on BC5CDR, 1.3 on COMETA and 0.3 on AAP.
Our method also shows superiority over previous
SOTA on BC5CDR and COMETA without pre-
training. Besides, our method shows competitive
results on NCBI compared to SOTA, and we further
analyse the results of NCBI through case studies in
Appendix D.
4 Discussion
Does pre-training help? On all datasets, KB-
guided pre-training improves ﬁne-tuning consis-4040
tently, which is 0.7 on BC5CDR, 0.3 on NCBI,
0.7 on COMETA, and 0.5 on AAP. To better un-
derstand KB-guided pre-training, we conduct ab-
lation studies. We compare different pre-trained
models without ﬁne-tuning in Table 3. BART fails
to link mentions due to the mismatch of the pre-
training task. GENRE has been pre-trained on
the large-scale BLINK dataset (Wu et al., 2020),
and it obtains a decent ability to disambiguate
biomedical mentions. Our pre-trained model shows
improvement on BC5CDR and COMETA com-
pared to GENRE with fewer pre-training resources
(6 GPU days vs. 32 GPU days). We then con-
duct synonyms-aware ﬁne-tuning on different pre-
trained models in Table 3. Our pre-trained model
outperforms BART (+0.8 on BC5CDR, +0.5 on
COMETA) and GENRE (+0.4 on BC5CDR, +0.6
on COMETA) which proves the effectiveness of
pre-training.
Selection of Names We ablate the selection of
target names sfor ﬁne-tuning: (a) proposed TF-
IDF similarity; (b) the shortest name in a concept;
(c) randomly sampled name in a concept. We also
compare how to construct Sfor inference: (i) us-
ing all synonyms from UMLS and target KB; (ii)
using the shortest name for a concept; (iii) using
randomly sampled name for a concept. We note (i)
establish an N-to-1 mapping from synonym name
to concept, while (ii) and (iii) establish a 1-to-1
mapping. We conduct experiments on available
combinations. From Table 4, we conclude that (1)
N-to-1 mapping performs better than 1-to-1 map-
pings during inference, which means synonyms
can boost performances. Using one synonym like
GENRE degrades performances. (2) Textual sim-
ilarity criterion performs better than shortest or
sampled names when training.
We also check the accuracies of different TF-IDF
similarity sample groups on COMETA trained with
using all synonyms as Sand TF-IDF for selecting
target names s. From Figure 2, we ﬁnd the distri-
bution of TF-IDF similarity between mentions and
selected names is polarized, and the accuracy in-
creases along with textual similarities which prove
textually similar targets are easy to generate. This
phenomenon validates the advantage of selecting
textually similar names for ﬁne-tuning.
Decoder Prompting One difference between
GENRE and ours is using prompt tokens on the de-
coder side. Prompting has shown improvement on
various NLP tasks (Liu et al., 2021b). Here, prompt
tokens serve as informative hints by providing addi-
tional decoder attention queries and making the out-
puts resemble language models’ pre-training tasks.
We test dropping the prompt tokens, and Table 4
shows degraded performances (-0.3 on BC5CDR
and -0.5 on COMETA). The results illustrate the
improvement brought by decoder prompting.
Sub-population Analysis We list several sub-
populations of the BC5CDR benchmark to illus-
trate the model’s performance on different ﬁne-
grained categories of mentions. The details of sub-
populations are shown in Appendix C.2
Our model’s performance on different sub-
populations of BC5CDR is shown in Table 9.
Through the results, we have several ﬁndings:
1.Compared with Varma et al. (2021), our
method shows superiority over most of the
sub-populations of BC5CDR. Our method
without pre-training outperforms the data-
augmented version of Varma et al. (2021).
2.Our method surpasses Varma et al. (2021)
by the largest margin on Unseen Concepts .
One possible explanation is that our genera-
tive method learns the linkage between men-
tions and contextual information better, thus
gaining superior zero-shot performance.4041
3.Our KB-guided pre-training gains improve-
ment universally on most subsets. This re-
ﬂects the effectiveness of the pre-training task.
4.Popular concepts and single-word mentions
are more easily resolved compared to unseen
mentions or concepts and multi-word men-
tions, respectively. Unsurprisingly, mentions
with less training resources and longer tex-
tual forms are more difﬁcult. The lengths of
mentions may challenge the generative model.
5 Related Work
Biomedical EL is an important task in biomedi-
cal NLP. Classiﬁcation-based methods used a soft-
max layer for classiﬁcation (Limsopatham and Col-
lier, 2016; Miftahutdinov and Tutubalina, 2019)
which consider concepts as category factors and
lost information of concept names. Recent methods
(Sung et al., 2020; Liu et al., 2021a; Lai et al., 2021;
Bhowmik et al., 2021; Ujiie et al., 2021; Agarwal
et al., 2021; Yuan et al., 2022) encoded mentions
and names into a common space and disambiguated
mentions by nearest neighbors. Angell et al. (2021)
and Varma et al. (2021) adopted a retrieve-and-
rerank framework to boost performances. Varma
et al. (2021) emphasized the lack of training sam-
ples in EL and augmented data using Wikipedia
and PubMed, while our pre-training corpus con-structed by KB and templates can serve as good
supplementary training data.
Generative EL Cao et al. (2021a) proposed to
view EL as a seq2seq problem that got rid of hard
negative sampling during training and required less
memory at inference. The shortage is it demanded
vast training sources (11 GB training data and 32
GPU days) to achieve competitive performance.
Cao et al. (2021b) explored dealing synonyms in
multilingual generative EL by adding language
identiﬁers that cannot be directly implemented in
biomedical EL.
6 Conclusion
To the best of our knowledge, our work is the
ﬁrst to explore generative EL in the biomedical
domain. We inject synonyms and deﬁnition knowl-
edge into the generative language model by KG-
guided pre-training. We emphasize the synonym
selection issue and propose synonyms-aware ﬁne-
tuning by considering the textual similarity. De-
coding prompts are also introduced to improve the
model’s performance. Our model sets new state-
of-the-art on different biomedical EL benchmarks.
GENRE shows that well-selected candidate sets
can improve seq2seq EL, and we believe this will
further boost our performances.
Acknowledgements
We thank Shengxuan Luo and Chuanqi Tan for
fruitful discussions and advises, and Xixi Mo for
drawing the overview ﬁgure. We appreciate the
anonymous reviewers for their helpful comments
and suggestions. This work was supported by
the National Natural Science Foundation of China
(Grant No. 12171270), the Natural Science Founda-
tion of Beijing Municipality (Grant No. Z190024),
and the International Digital Economy Academy.4042References4043
A Dataset Summary and Statistics
We pre-process the datasets by the following pro-
cedures: (1) the abbreviations in the texts are ex-
panded using AB3P (Sohn et al., 2008); (2) the
texts are lower-cased, and the beginning and end-
ing of a mention are marked by two special tokens
[ST] and[ET] ; (3) the overlapping mentions and
mentions absent from the target KB are discarded.BC5CDR (Li et al., 2016) is a benchmark for
biomedical entity recognition and disambiguation.
The dataset annotates 1500 PubMed article ab-
stracts with 4409 chemicals, 5818 diseases entities,
and 3116 chemical-disease interactions. All the
annotated entities are mapped to MeSH ontology,
which is a smaller medical vocabulary that com-
prises a subset of UMLS (Bodenreider, 2004). In
this work, in consideration of fairness, we follow
two most recent works (Angell et al., 2021; Varma
et al., 2021) that use MeSH contained in UMLS
2017 AA release to construct the target knowledge
base.
NCBI (Do˘gan et al., 2014) contains a corpus of
793 PubMed abstracts. It consists of 6892 anno-
tated disease mentions of 790 unique disease con-
cepts. All the mentions are labeled with concepts
in MELIC ontology (Davis et al., 2012). MELIC
is a medical dictionary that merges the diseases
concepts, synonyms, and deﬁnitions in MeSH and
OMIM and is composed of 9700 unique diseases.
In our work, we used the processed data and the
target ontology provided by BioSyn (Sung et al.,
2020) and ResCNN (Lai et al., 2021). We followed
their works to construct our training, developing,
and testing data.
COMETA (Basaldella et al., 2020) consists of
20k English biomedical entity mentions from pub-
licly available and anonymous health discussions
on Reddit. All the mentions are expert-annotated
with concepts from SNOMEL CT. We use the
“stratiﬁed (general)” split and follow the evalua-
tion protocol of SapBert (Liu et al., 2021a) and
ResCNN (Lai et al., 2021).
AskAPatient (Limsopatham and Collier, 2016)
is a dataset containing 8,662 phrases of social me-
dia language. Each phrase can be mapped to one
of the 1,036 medical concepts from SNOMEL-CT
and AMT (the Australian Medicines Terminology).
The samples in AskAPatient do not include contex-
tual information and mentions can only be disam-
biguated by phrases per se. We follow the exper-
imental settings from works of Sung et al. (2020)
and Limsopatham and Collier (2016) and apply the
10-fold evaluation protocol.
Statistics of above-mentioned datasets are listed
in Table 6.4044
B License and Availability of Resources
BC5CDR, NCBI, COMETA, and AskAPatients
are all publicly available datasets on the Internet.
Their target KBs MeSH, MELIC, and SNOMEL
CT are covered by UMLS Metathesaurus License.
One can require such a license by signing up for
a UMLS terminology services account to access
KBs mentioned above.
C Additional Experiment Results
C.1 Recall@5 Results
We show the Recall@5 result of our method with
and without pre-training in Table 7.
C.2 Sub-population Analysis
Following Varma et al. (2021), we split test samples
into different sub-populations. The details of dif-
ferent sub-population categories we use is shown
in Table 8.
D Case Study
We provide case studies on the NCBI-disease
benchmark to give an insight and justiﬁcation of the
performance of our method. For the case of men-
tion colorectal adenomas which is annotated with
D018256 adenomatous polyp , the mention exists
for 6 times in the test set (account for 0.63 score
of Recall@1). Our model fails to correctly disam-
biguate all such mentions while linking the mention
to other concepts D000236 colorectal adenomas
orD003123 hereditary nonpolyposis colorectal
cancer . In the training set, the mention colorectal
adenomas exists for two times and is annotated
with D003132 andD000236 respectively. Thus,
through this case, we can see (1) our model learnsthe information contained in the training set; (2)
such inconsistent test samples are hard to disam-
biguate correctly.
E Implementation Details
In this section, we provide more details of our ex-
periments.
E.1 Knowledge Base Pre-processing
Given different knowledge bases, we pre-process
their content by the following procedure:
1.For each concept, we include all its syn-
onyms from the original target KB. We also
expand the synonym set for each concept us-
ing UMLS. We use the 2017 AA Active Re-
lease of UMLS.
2.For synonyms in the expanded name set S, we
lowercase them and remove the symbols (e.g.,
dash line - or comma ,).
3.There may exist a name as a synonym for mul-
tiple concepts. We de-duplicate these over-
lapped synonyms by removing the synonym
from the concept with more other synonyms
to avoid the unbalanced number of synonyms
in each concept.
It is worth noticing that we do not de-duplicate the
synonyms in the target KB of NCBI in considera-
tion of comparison fairness. In the previous works,
a mention link to multiple concepts, and correct
disambiguation is claimed if the target concept is
hit by one of the predicted concepts in NCBI.
E.2 Pre-training Clause Templates
We list pre-training clause templates we used in
Table 10. For those concepts containing only 2
synonyms,sandsare the two synonyms respec-
tively andcis the same as s. For those concepts
containing only 1 sole synonym, s,sandcare
the same.40454046E.3 Experiment Parameters
Our model contains 406M parameters with 12-
layer transformer encoders and 12-layer trans-
former decoders. We list the hyper-parameters
of our model for KB-guided pre-training and
synonyms-aware ﬁne-tuning on different bench-
marks in Table 11. For pre-training, we heuristi-
cally select our parameters. For ﬁne-tuning, we
tune training steps among {20000,30000,40000}
on development set. For BC5CDR and COMETA,
we use learning rate as 1e−5and warmup steps as
500. For AskAPatient and NCBI, we search learn-
ing rate among{5e−6,8e−7,3e−7}, and do
not use warmup. We only evaluate our model at the
end of training. For each benchmark, we run three
times to calculate means and standard deviations.
E.4 Computational Resource
For our KB-guided pre-training, we implement our
model on 6 A100 GPU with 40 GB memory with
the help of DeepSpeed ZeRO 2 (Rajbhandari et al.,
2020) and train for 1 day. For ﬁne-tuning on dif-
ferent benchmarks, we implement our model on 1
A100 GPU.40474048