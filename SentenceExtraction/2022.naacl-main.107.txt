
Miguel Arana-Catania, Elena Kochkina, Arkaitz Zubiaga, Maria Liakata,
Rob Procter, Yulan HeDepartment of Computer Science, University of Warwick, UKThe Alan Turing Institute, UKQueen-Mary University of London, UK
Abstract
We present a comprehensive work on auto-
mated veracity assessment from dataset cre-
ation to developing novel methods based on
Natural Language Inference (NLI), focusing on
misinformation related to the COVID-19 pan-
demic. We first describe the construction of the
novel PANACEA dataset consisting of hetero-
geneous claims on COVID-19 and their respec-
tive information sources. The dataset construc-
tion includes work on retrieval techniques and
similarity measurements to ensure a unique set
of claims. We then propose novel techniques
for automated veracity assessment based on
Natural Language Inference including graph
convolutional networks and attention based ap-
proaches. We have carried out experiments on
evidence retrieval and veracity assessment on
the dataset using the proposed techniques and
found them competitive with SOTA methods,
and provided a detailed discussion.
1 Introduction
In recent years, and particularly with the emer-
gence of the COVID-19 pandemic, significant ef-
forts have been made to detect misinformation on-
line with the aim of mitigating its impact. With
this objective, researchers have proposed numerous
approaches and released datasets that can help with
the advancement of research in this direction.
Most existing datasets (D’Ulizia et al., 2021) fo-
cus on a single medium (e.g., Twitter, Facebook,
or specific websites), a unique information domain
(e.g., health information, general news, or scholarly
papers), a type of information (e.g., general claims
or news), or a specific application (e.g., verifying
claims, or retrieving useful information). This in-
evitably results in a limited focus on what is a com-
plex, multi-faceted phenomenon. With the aim of
furthering research in this direction, the contribu-
tions of our work are twofold: (1) creating a new
comprehensive dataset of misinformation claims,
and (2) introducing two novel approaches to verac-
ity assessment.In the first part of our work, we contribute to
the global effort on addressing misinformation in
the context of COVID-19 by creating a dataset for
PANdemic Ai Claim vEracity Assessment, called
thePANACEA dataset . It is a new dataset that
combines different data sources with different foci,
thus enabling a comprehensive approach that com-
bines different media, domains and information
types. To this effect our dataset brings together a
heterogeneous set of True andFalse COVID claims
and online sources of information for each claim.
The collected claims have been obtained from on-
line fact-checking sources, existing datasets and re-
search challenges. We have identified a large over-
lap of claims between different sources and even
within each source or dataset. Thus, given the chal-
lenges of aggregating multiple data sources, much
of our efforts in dataset construction has focused
on eliminating repeated claims. Distinguishing be-
tween different formulations of the same claim and
nuanced variations that include additional infor-
mation is a challenging task. Our dataset is pre-
sented in a large and a small version, accounting
for different degrees of such similarity. Finally, the
homogenisation of datasets and information me-
dia has presented an additional challenge, since
fact-checkers use different criteria for labelling the
claims, requiring a specific review of the different
kinds of labels in order to combine them.
In the second part of our work, we propose
NLI-SAN and NLI-graph, two novel veracity as-
sessment approaches for automated fact-checking
of the claims. Our proposed approaches are cen-
tred around the use of Natural Language Inference
(NLI) and contextualised representations of the
claims and evidence. NLI-SAN combines the in-
ference relation between claims and evidence with
attention techniques, while NLI-graph builds on
graphs considering the relationship between all the
different pieces of evidence and the claim.
Specifically we make the following contribu-
tions:1496•We describe the development of a com-
prehensive COVID fact-checking dataset,
PANACEA, as a result of aggregating and
de-duplicating a set of heterogeneous data
sources. The dataset is available in the project
website, as well as a fully operational search
platform to find and verify COVID-19 claims
implementing the proposed approaches.
•We propose two novel approaches to claim
verification, NLI-SAN and NLI-graph.
•We perform an evaluation of both evidence re-
trieval and the application of our proposed ve-
racity assessment methods on our constructed
dataset. Our experiments show that NLI-SAN
and NLI-graph have state-of-the-art perfor-
mance on our dataset, beating GEAR (Zhou
et al., 2019) and matching KGAT (Liu et al.,
2020). We discuss challenging cases and pro-
vide ideas for future research directions.
2 Related Work
COVID-19 and misinformation datasets. Com-
prehensive information on COVID-19 datasets is
provided in Appendix A. Such datasets include
the CoronaVirusFacts/DatosCoronaVirus Alliance
Database, the largest existing collection of COVID
claims and the largest existing network of journal-
ists working together on COVID misinformation,
an essential reference for our work; COVID-19-
TweetIDs (Chen et al., 2020) the widest dataset
of COVID tweets with more than 1 billion tweets;
Cord-19: The COVID-19 open research dataset
(Wang et al., 2020a), the largest downloadable set
of scholarly articles on the pandemic with nearly
200,000 articles. General misinformation datasets
linked to our verification work include: Emer-
gent (Ferreira and Vlachos, 2016) collection of
300 labeled claims by journalists; LIAR (Wang,
2017) with 12,836 statements from PolitiFact with
detailed justifications; FakeNewsNet (Shu et al.,
2020) collecting not only claims from news con-
tent, but also social context and spatio-temporal
information; NELA-GT-2018 (Nørregaard et al.,
2019) with 713,534 articles from 194 news out-
lets; FakeHealth (Dai et al., 2020) collecting in-
formation from HealthNewsReview, a project criti-
cally analysing claims about health care interven-
tions; PUBHEALTH (Kotonya and Toni, 2020)
with 11,832 claims related to health topics; FEVER
(Thorne et al., 2018a) as well as its later versionsFEVER 2.0 (Thorne et al., 2018b) and FEVER-
OUS (Aly et al., 2021), containing claims based on
Wikipedia and therefore constituting a well-defined,
informative and non-duplicated information cor-
pus; SciFact (Wadden et al., 2020) also from a
very different domain, containing 1,409 scientific
claims. Our dataset is a real-world dataset bring-
ing together heterogeneous sources, domains and
information types.
Approaches to claim veracity assessment. We
employ our dataset for automated fact-checking
and veracity assessment (Zeng et al., 2021). Re-
searchers such as Hanselowski et al. (2018);
Yoneda et al. (2018); Luken et al. (2018); Soleimani
et al. (2020); Pradeep et al. (2021) analysed the ve-
racity relation between the claim and each piece
of evidence independently, combining this infor-
mation later. Other authors considered multiple
pieces of evidence together (Thorne et al., 2018a;
Nie et al., 2019; Stammbach and Neumann, 2019).
Different pieces of evidence have been previously
combined using graph neural networks (Zhou et al.,
2019; Liu et al., 2020; Zhong et al., 2020). Many
of these authors have centred their techniques on
the use of NLI (Chen et al., 2017; Ghaeini et al.,
2018; Parikh et al., 2016; Li et al., 2019) to verify
the claim. In our work we also make use of NLI
results of claim-evidence pairs, but propose alter-
native approaches built on a self-attention network
and a graph convolutional network for veracity as-
sessment.
3 Dataset Construction
This section describes our dataset construction by
selecting COVID-19 related data sources (§3.1),
and applying information retrieval and re-ranking
techniques to remove duplicate claims (§3.2).
3.1 Data Sources
We first identified a set of COVID-19 related data
sources to build our dataset. Our aim is to have the
largest compilation of non-overlapping, labelled
and verified claims from different media and infor-
mation domains (Twitter, Facebook, general web-
sites, academia), and used for different applications
(media reporting, veracity evaluation, information
retrieval challenges, etc.). We have included any
large dataset or media, to our knowledge, related
to that objective that includes claims together with
their information sources. The data sources iden-
tified are shown in Table 1. More details and pre-
processing steps are presented in Appendix A. By1497
processing and combining these sources we ob-
tained 20,689 initial claims.
3.2 Claim De-duplication
We processed claims and removed: exact dupli-
cates; claims making only a direct reference to
existing content in other media (audio, video, pho-
tos); automatically obtained content not represent-
ing claims; entries with claims or fact-checking
sources in languages other than English.
The similarity of claims was then analysed
using: BM25 (Robertson et al., 1995; Crestani
et al., 1998; Robertson and Zaragoza, 2009) and
BM25 with MonoT5 re-ranking (Nogueira et al.,
2020). BM25 is a commonly-used ranking func-
tion that estimates the relevance of documents to
a given query. MonoT5 uses a T5 model trained
using as input the template ‘ Query:[query]
Document:[doc] Relevant: ’, fine-tuned
to produce as output the token ‘ True’ or ‘ False ’. A
softmax layer applied to those tokens gives the re-
spective relevance probabilities. These methods are
used to identify not only claims similar in content,
but also distinct claims that are sufficiently relevant
when searching for information about them. This
ensures that the claims presented are unique, and
avoids overlap between training and testing cases
when using the data to train veracity assessment
models. These methods were carried out usingPyseriniand PyGaggle. The set of claims was
indexed and a search was performed for each of
the claims to detect similar claims. We created
two versions of the dataset by varying the similar-
ity threshold between claims. The L dataset
excludes claims with a 90% probability of being
similar, while in the S dataset the probabil-
ity is increased to 99%, as obtained through the
MonoT5 model. These thresholds were chosen em-
pirically by manual inspection of the results with
simultaneous consideration of the efficiency of the
method.
As a further assessment of the uniqueness of the
claims, we evaluated the de-duplication process
using BERTScore(Zhang et al., 2019) on the re-
sulting datasets. We used the linked code with a
RoBERTa-large model with baseline rescaling. We
compared each claim with all the other claims in
the dataset and kept the score of the most similar
match. The mean and standard deviation, and the
90th percentile of claim similarity values are shown
in the upper part of Table 3. The average claim sim-
ilarity has been drastically reduced in the L
dataset compared to the original dataset and further
reduced in the S dataset.
To illustrate the difference between the two ver-1498
sions of the dataset, we present some examples of
claims in Table 2. For Claim 1, the semantically
similar claim ‘ Loss of smell may suggest milder
COVID-19 ’ is identified and excluded from both
L andS datasets. But the claim ‘ Loss
of smell and taste validated as COVID-19 symp-
toms in patients with high recovery rate ’, which
includes mentions of another symptom and the
recovery rate, is only excluded from the S
dataset. For Claim 2, the rephrased claim ‘ The
African American community is being hit hard by
COVID-19 ’ is excluded from both datasets. But the
claim ‘ COVID-19 impacts in African-Americans
are different from the rest of the U.S. population ’,
which refers specifically to the U.S. population, is
only excluded from the S dataset.
3.3 Dataset Statistics
Our final dataset statistics are shown in the lower
part of Table 3, where the original and the two
reduced versions are presented. After the steps de-
scribed in Section 3.2 the L dataset contains
5,143 claims, and the S version 1,709 claims.
Example claims contained in the dataset are
shown in Table 4. Each of the entries in the datasetcontains the following information:
•Claim . Text of the claim.
•Claim label . The labels are: False , and True.
•Claim source . The sources include mostly
fact-checking websites, health information
websites, health clinics, public institutions
sites, and peer-reviewed scientific journals.
•Original information source . Information
about which general information source was
used to obtain the claim.
•Claim type . The different types, explained in
Section A.2, are: Multimodal, Social Media,
Questions, Numerical , and Named Entities .
4 Claim Veracity Assessment
We develop a pipeline approach consisting of three
steps: document retrieval, sentence retrieval and
veracity assessment for claim veracity evaluation.
Given a claim, we first retrieve the most relevant
documents from COVID-19 related sources and
then further retrieve the top Nmost relevant sen-
tences. Considering each retrieved sentence as ev-
idence, we train a veracity assessment model to
assign a True orFalse label to the claim.
4.1 Document Retrieval
Document Dataset. In order to retrieve docu-
ments relevant to the claims, we first construct an
additional dataset containing documents obtained
from reliable COVID-19 related websites. These
information sources represent a real-world com-
prehensive database about COVID-19 that can be
used as a primary source of information on the
pandemic. We have selected four organisations
from which to collect the information: (1) Cen-
ters for Disease Control and Prevention (CDC) ,
national public health agency of the United States;
(2)European Centre for Disease Prevention and
Control (ECDC) , EU agency aimed at strengthen-
ing Europe’s defenses against infectious diseases;
(3)WebMD , online publisher of news and informa-
tion on health; and (4) World Health Organization
(WHO) , agency of the United Nations responsible
for international public health.
All pages corresponding to the COVID-19 sub-
domains of each site have been downloaded. The1499
web content was downloaded using the Beautiful-
Soupand Scrapypackages. Social networking
sites and non-textual content were discarded. In
total 19,954 web pages have been collected. The
list of websites and the full content of each website
constitute this additional dataset used for document
retrieval. This dataset is enhanced with some addi-
tional websites used only in the document retrieval
experiments, detailed in Section 5.1.
Method. Information sources were indexed by
creating a Pyserini Lucene index and PyGaggle
was used to implement a re-ranker model on the
results. The documents were split into paragraphs
of 300 tokens segmented with a BERT tokenizer.
To retrieve the information we first used a BM25
score. Additionally, we tested the effect of multi-
stage retrieval by re-ranking the initial results using
MonoBERT (Nogueira et al., 2019) and MonoT5
models, and query expansion using RM3 pseudo-
relevance feedback (Abdul-Jaleel et al., 2004) on
the BM25 results (Lin, 2019; Yang et al., 2019).
MonoBERT uses a BERT model trained us-
ing as inputs the query and each of the
documents to be re-ranked encoded together
([CLS]query[SEP]doc[SEP] ), and then the
[CLS] output token is passed to a single layer
fully-connected network that produces the proba-
bility of the document being relevant to the query.
4.2 Sentence Retrieval
For each claim, once documents are retrieved us-
ing BM25 and MonoT5 re-ranking of the top 100
BM25 results, we then further retrieve the Nmost
similar sentences obtained from the 10 most rele-
vant documents. The relevance of the sentences iscalculated using cosine similarity in relation to the
original claim. The similarity is obtained with the
pre-trained model MiniLM-L12-v2 (Wang et al.,
2020b), using Sentence-Transformers(Reimers
and Gurevych, 2019) to encode the sentences.
4.3 Veracity Assessment
We propose two veracity assessment approaches
built on the NLI results of claim-evidence pairs.
For each of the most similar sentences (pieces of
evidence) retrieved for a claim, we apply the pre-
trained NLI model RoBERTa-large-MNLI(Liu
et al., 2019). This model acts as a cross-encoder
on pairs of sentences, trained to detect the rela-
tionship between the two sentences: contradiction ,
neutrality , or entailment . The model is trained
on the Multi-Genre Natural Language Inference
(MultiNLI) dataset (Williams et al., 2018). The
inference results are then used in our proposed ap-
proaches described below.
NLI-SAN. The first approach, named NLI-SAN ,
incorporates the inference results of claim-evidence
pairs into a Self-Attention Network (SAN) (See
Figure 1a). First, a claim is paired with each piece
of retrieved relevant evidence. Each pair (c, e)
is fed into a RoBERTa-largemodel, and the last
hidden layer output Sis used as its representa-
tion. Additionally, each pair is also fed to the men-
tioned RoBERTa-large-MNLImodel obtaining I,
a triplet containing the probability of contradiction ,
neutrality , orentailment .
S=RoBERTa (c, e)
I=RoBERTa(c, e)(1)1500
The sentence representation is combined with the
NLI output through a Self Attention Network
(SAN) (Galassi et al., 2020; Bahdanau et al., 2015).
The RoBERTa-encoded claim-evidence repre-
sentation Swith length n=n=nis
mapped onto a Key K∈Rand a Value
V∈R, while the NLI output Iof each
claim-evidence pair is mapped onto a Query Q∈
R. The representation dimensionality is
d=d=d= 1024 . The attention function is
defined as:
Att(Q,K,V) =softmax (QK/√
d)V (2)
While standard attention mechanisms use only the
sentence representation information for the Key,
Value and Query, here the inference information
is used in the Query. This attention mechanism is
applied to each of the claim-evidence pairs, and
the outputs are concatenated into an output O
that is passed through a Multi-Layer Perceptron
(MLP) with hidden size dand a Softmax layer to
generate the veracity classification output.
ˆy=softmax (MLP(O)) (3)
NLI-graph. We propose an alternative approach
based on Graph Convolutional Networks (GCN).
First, for each claim-evidence pair, we derive
RoBERTa-encoded representations for the claims
and evidence separately (using the pooled output
of the last layer) and obtain NLI results of the pairsas before.
C=RoBERTa (c);E=RoBERTa (e)(4)
I=RoBERTa(c, e) (5)
Next, we build an evidence network in which the
central node is the claim and the rest of the nodes
are the evidence. Two nodes are linked if their simi-
larity value exceeds a pre-defined threshold, which
is empirically set to 0.9 by comparing the results of
the experimental evaluation described in the follow-
ing section using different thresholds. The similar-
ity is considered between claim and evidence, but
also between pieces of evidence. Similarity calcu-
lation is performed following the same approach as
in Section 4.2. The features considered in each evi-
dence node are the concatenation of EandI. For
the claim node we use its representation Cand a
unity vector (0,0,1)for the inference. The network
is implemented with the package PyTorch Geomet-
ric (Fey and Lenssen, 2019), using in the first layer
the GCNConv operator (Kipf and Welling, 2016)
with 50 output channels and self-loops to the nodes,
represented by:
X=ˆDˆAˆDXW, (6)
whereXis the matrix of node feature vectors, ˆA=
A+Idenotes the adjacency matrix with inserted
self-loops, ˆD=/summationtextˆAits diagonal degree
matrix, and Wis a trainable weight matrix.
Once the node representation is updated via
GCN, all the node representations are averaged1501and passed to the MLP and the Softmax layer to
generate the final veracity classification output.
ˆy=softmax (MLP(O)) (7)
5 Experiments
In this section, we perform a twofold evaluation:
We first evaluate our document retrieval methods
(presented in §4.1) on obtaining information rel-
evant to the dataset claims from a database of
COVID-19 related websites. We subsequently
present an evaluation of the veracity assessment
approaches for the claims (described in §4.3).
5.1 Document Retrieval
In order to evaluate our document retrieval meth-
ods, we need the gold-standard relevant document
for each claim. Therefore, in the documents dataset
described in section 4.1 we additionally include the
web content referenced in each of the information
sources used to compile our claim dataset:
The CoronaVirus Alliance Database . All web
pages from the websites referenced as fact-
checking sources for the claims have been down-
loaded from 151 different domains.
CoAID dataset . We downloaded the websites used
as fact-checking sources of false claims and the
websites where correct information on true claims
is gathered from 68 different domains.
MM-COVID . We collected both fact-checking
sources and reliable information related to the
claims of this dataset from 58 web domains.
CovidLies dataset . We include the web content
used as fact-checking sources of the misconcep-
tions from 39 domains.
We have not included web content from the
TREC Challenges, as each of them is performed
on a very large dataset specific to each challenge
(CORD19 and Common Crawl corpus), as ex-
plained previously. Note that in our subsequent
experiments, we have excluded all fact-checking
websites to avoid finding directly the claim refer-
ences. The results of the document retrieval are pre-
sented in Table 5. For each claim, the precision@ k
is defined as 1 if the relevant result is retrieved in
the top klist and 0 otherwise.
We can see that by using BM25, it is possible
in many cases to retrieve the relevant results at the
very top of our searches. Combining BM25 with
MonoBERT did not offer any improvement. It even
introduced noise to the retrieval results, leading
to inferior performance compared to using BM25
only on AP@5 and AP@10. MonoT5 appears
to be more effective, consistently improving the
retrieval results across all metrics. Moreover for
this dataset the use of query expansion using RM3
pseudo-relevance feedback on the BM25 results
does not improve the results.
5.2 Veracity Assessment Evaluation
Here we evaluate our proposed NLI-SAN and
NLI-graph veracity assessment approaches. To
gain a better insight into the benefits of the pro-
posed architectures, we conducted additional ex-
periments on the variants of the models including:
•NLI, using only the NLI outputs of the claim-
evidence pairs. The outputs are concatenated
and then passed through the final classifica-
tion layer to generate veracity classification
results.
•NLI+sent , this is the ablated version of
NLI-SAN without the self-attention layer.
Here, the RoBERTa-encoded claim-evidence
representations are concatenated with the NLI
results and then fed to the classification layer
to produce the veracity classification output.
•NLI+PSent , this is similar to the previous
ablated version, but using the pooled represen-
tation of the claim-evidence pair to concate-
nate with the NLI result.
•NLI-graph, this is the ablated version
ofNLI-graph in which the node represen-
tation is the NLI result of the correspond-
ing claim-evidence pair without its RoBERTa-
encoded representation.
ForNLI,NLI+sent andNLI-SAN , we con-
sider the 5 most similar sentences for each claim,
obtained from the 10 most relevant documents of
the information source database. Those documents
are retrieved using BM25 and MonoT5 re-ranking
of the top 100 BM25 results. For NLI-graph ,
NLI-graphandNLI+PSent , in order to
have enough nodes to benefit from the network
structure, the number of retrieved sentences is in-
creased to 30 for each claim, selected as the 31502
most similar sentences from the top 10 retrieved
documents. The retrieval procedure is as in sec-
tions 4.1 and 4.2. Details of parameter settings
can be found in Appendix B. We compare against
the SOTA methods GEAR(Zhou et al., 2019) and
KGAT(Liu et al., 2020), with settings as de-
scribed by the authors.
For all approaches we perform 5-fold cross-
validation and report the averaged results on the
S dataset in Table 6. By using the NLI
information alone it is possible to obtain reason-
able results for the True claims, however, this is
not the case for the most relevant False claims.
Once we add sentence representations the effi-
ciency of the method increases significantly. Using
NLI-SAN instead of simply concatenating contex-
tualised claim-evidence representations and NLI
outputs further improves the results. A similar
observation can be made in the results generated
byNLI-graph and its variants; the contextu-
alised representations of claim-evidence pairs are
much more important than merely using the corre-
sponding NLI values. We also note that using the
graph version NLI-graph obtains better scoresthan a non-graph model with the same information
NLI+PSent , however the scores are still lower
than the NLI-SAN method. Our method performs
on a par with KGAT, while being simpler, and out-
performs GEAR.
Complementing the results for the S
dataset, Table 7 presents the results for the L
dataset. In general, we observe improved perfor-
mance for all models across all metrics for both
classes compared to the results on the S
dataset. The previous results in the S dataset
constitute a more challenging case, since the
uniqueness of the claims is increased and there-
fore the veracity assessment models are not able
to learn from similar claims when performing the
assessment.
5.3 Discussion
Our results show that in document retrieval, we
have obtained values of around 0.6 from a simple
term scoring and re-ranking retrieval model. How-
ever, this baseline represents only a rough measure
of quality using this technique, since we have only
evaluated the retrieval of a single document specific
to each claim; we have not evaluated the quality of
other retrieved documents.1503The distinction into True andFalse claims can
be rather coarse-grained. We note that initially
we considered a larger number of veracity labels,
including more nuanced cases that could be inter-
esting to analyse (see A.1). However, we have not
found a clear separation between complex cases
and it would seem that different fact checkers do
not follow the same conventions when labelling
such cases. The development of datasets especially
focused on such nuanced cases may be therefore
an important line of work in the future, together
with the development of techniques for these more
complex situations.
In analysing misclassified claims, we note some
interesting cases. The scope and globality of the
pandemic imply that similar issues are mentioned
repeatedly on multiple occasions, yet claims to be
verified may include nuances or specificities. This
is challenging as it is easy to retrieve information
that omits relevant nuances. E.g. The claim “ Bar-
ron Trump had COVID-19, Melania Trump says "
retrieves sentences such as “ Rudy Giuliani has
tested positive for COVID-19, Trump says. " with a
similar structure and mentions but missing the key
name. This type of situation could be addressed
by using Named Entity Recognition (NER) meth-
ods that prioritise matching between the entities
involved in the claim and the information sources.
See e.g. (Taniguchi et al., 2018; Nooralahzadeh
and Øvrelid, 2018).
Other interesting cases involve claims for which
documents with adequate information are retrieved,
but the sentences containing evidence cannot be
identified because they are too different from the
original claim. E.g. The claim “ Vice President of
Bharat Biotech got a shot of the indigenous COV-
AXIN vaccine " retrieves correct documents on the
issue. Similar sentences are retrieved such as “ Co-
vaxin which is being developed by Bharat Biotech
is the only indigenous vaccine that is approved
for emergency use. ". Despite being similar such
retrieved sentences give no information about the
claimed situation. In the retrieved document, the
sentence “ The pharmaceutical company, has in a
statement, denied the claim and said the image
shows a routine blood test. " contains the essen-
tial information to debunk the original claim but is
missed by the sentence retrieval engine as it is very
different from the claim (See Table A1 in Appendix
C for other examples).
Such cases are more difficult to deal with, as
the similarity between claim and evidence is cer-tainly a good indicator of relevance. Nevertheless,
these cases are very interesting for future work us-
ing more complex approaches. We have made an
initial attempt to address this problem by represent-
ing claims and retrieved documents using Abstract
Meaning Representation (Banarescu et al., 2013)
in order to better select relevant information. Al-
though the results were not satisfactory, it may be
an interesting avenue for future exploration. An-
other line of future work is the design of strategies
against adversarial attacks to mitigate possible risks
to our system.
6 Conclusions
We have presented a novel dataset that aggregates a
heterogeneous set of COVID-19 claims categorised
asTrue orFalse . Aggregation of heterogeneous
sources involved a careful deduplication process
to ensure dataset quality. Fact-checking sources
are provided for veracity assessment, as well as
additional information sources for True claims. Ad-
ditionally, claims are labelled with sub-types (Mul-
timodal, Social Media, Questions, Numerical, and
Named Entities).
We have performed a series of experiments using
our dataset for information retrieval through direct
retrieval and using a multi-stage re-ranker approach.
We have proposed new NLI methods for claim ve-
racity assessment, attention-based NLI-SAN and
graph-based NLI-graph , achieving in our dataset
competitive results with the GEAR and KGAT
state-of-the-art models. We have also discussed
challenging cases and provided ideas for future
research directions.
Acknowledgements
This work was supported by the UK Engineering
and Physical Sciences Research Council (grant
no. EP/V048597/1, EP/T017112/1). ML and YH
are supported by Turing AI Fellowships funded
by the UK Research and Innovation (grant no.
EP/V030302/1, EP/V020579/1).
References15041505150615071508A Data Sources
Here we present detailed information of the data
sources introduced in section 3.1.
It is worth noting that for the construction of
our dataset, we have only included sources or
datasets that contain explicit veracity labels of spe-
cific claims, thus we have not included collections
of tweets related to COVID that do not have verac-
ity labels (Chen et al., 2020; Lamsal, 2021; Abdul-
Mageed et al., 2021; Huang et al., 2020; Dimitrov
et al., 2020; Kerchner and Wrubel, 2020; Qazi et al.,
2020). We have not included claims without inde-
pendent fact-checking sources (Memon and Carley,
2020; Shahi et al., 2021) and information sources
without formulated claims such as the collections
of scholarly articles (Wang et al., 2020a; Chen et al.,
2021), news articles (Zhou et al., 2020), or articles
obtained through specific repositories as (COVID-
19 Data Portal , EU; WHO database of publica-
tions on coronavirus; Elsevier journals Novel Coro-
navirus Information Center; Cambridge journals
Coronavirus Free Access Collection; The Lancet
COVID-19 content collection; Oxford journals re-
sources on COVID-19; MedRN medical research
network SSRN Coronavirus Infectious Disease Re-
search Hub).
The data sources that we have used for the con-
struction of our dataset are:
•The CoronaVirusFacts/DatosCoronaVirus
Alliance Database. Published by Poyn-
ter, this online publication combines fact-
checking articles from more than 100 fact-
checkers from all over the world, being the
largest journalist fact-checking collaboration
on the topic worldwide. The publication
is presented as an online portal, thus we had
to develop scripts to crawl the content and
extract the relevant claims, categories, and
information sources.
•CoAID dataset. The dataset (Cui and Lee,
2020) contains fake news from fact-checking
websites and real news from health informa-
tion websites, health clinics, and public insti-
tutions. Unlike most other datasets, it contains
a wide selection of true claims.•MM-COVID. The multilingual dataset (Li
et al., 2020) contains fake and true news col-
lected from Poynter and Snopes, being a
good complement to the first data source.
•CovidLies dataset. The dataset (Hossain
et al., 2020) contains a curated list of common
misconceptions about COVID appearing in
social media, carefully reviewed to contain
very relevant and unique claims unlike other
automatically collected datasets.
•TREC Health Misinformation track. Re-
search challenge using claims on the health
domain focused on information retrieval from
general websites through the Common Crawl
corpus. This dataset is specialized in a very
specific domain, and has been used for a very
different application than the previous data
sources.
•TREC COVID challenge. Research chal-
lenge (V oorhees et al., 2021; Roberts et al.,
2020) using claims on the health domain fo-
cused on information retrieval from scholarly
peer-reviewed journals through the CORD19
dataset (Wang et al., 2020a), the largest exist-
ing compilation of such articles. Similar to the
last source, but focused on scholarly papers
unlike the other sources.
A.1 Pre-processing
A separate pre-processing step was carried out for
each of the selected data sources:
The CoronaVirusFacts/CoronaVirus Alliance
Database. The data was downloaded on 13 Febru-
ary 2021. From the 11,647 entries initially ob-
tained, entries with no fact-checking source and cat-
egories with less than 10 entries were removed. The
different fact-checkers used different categories to
label the claims, although in most of the cases the
difference was mainly in terms of spelling. Initially
we identified the following common categories:
False (including FALSE, FALSO, Fake, false, false
and misleading, Two Pinocchios, Misinformation1509/ Conspiracy theory, Not true, false headline, MA-
NIPULATED, Unproven), Misleading (MIslead-
ing, MISLEADING, mislEADING, MiSLEAD-
ING, misleading, Misleading/False, Misleading),
Missing Context (Missing context, Needs Context,
missing context), No Evidence (NO EVIDENCE,
No evidence, No Evidence), Mostly False (Mostly
False, Mostly false, MOSTLY FALSE, mostly
false, Mainly false), Partially False (Partially False,
Partly false, Partially false, partially false, partly
false), Partially True (PARTLY TRUE, Partially
correct, Partially true, Partly true, HALF TRUE,
HALF TRUTH, half true), and Mostly True (Mostly
true, MOSTLY TRUE, mainly correct). Next, we
conducted a manual inspection of the different cat-
egories. We found that the categories Misleading,
Missing Context, No Evidence, Mostly False , and
Partially False had no homogeneous and clear def-
inition through the different fact-checking media.
Each group contains claims fitting the definition
mixed with claims that are simply false (e.g. of
false claims under other labels: “Misleading: Only
people from South Korea have Covid-19 antibod-
ies"; “Partially False: The vaccines contain sub-
stances such as arsenic or uranium according to
scientific studies"; “Mostly False: Pope Francis
contracted coronavirus"). Therefore, we decided to
not use these nuanced categories but group them
in the general False category. Additionally, we re-
moved the 25 claims from the categories Partially
True andMostly True , since they contained both
True andFalse claims.
CoAID dataset. The datasets
NewsRealCOVID-19 ,NewsFakeCOVID-19 ,
andClaimFakeCOVID-19 were selected. The
additional available dataset contains claims already
existing in other datasets, formulated in this case as
questions, and thus was not included. The selected
datasets contain True andFalse claims.
MM-COVID. The claims were obtained from the
English_news part of the dataset since we are
only interested in English claims. 3,409 claims
were collected. Claims in other languages appeared
in the file, therefore we did a language filtering
using polyglot. Additionally, claims without fact-
checking sources were deleted. It contains True
andFalse claims.
CovidLies dataset. The available claims have been
manually revised by eliminating duplicates, result-
ing in a total of 62 misconception claims. It con-
tains False claims.TREC Health Misinformation track. The claims
used in the track were obtained and reformulated
manually by us as affirmative claims (e.g., “ Can
vitamin D cure COVID-19? " was changed to “ Vita-
min D cures COVID-19 ") for consistency with the
rest of the data sources and to allow claim veracity
assessment. True andFalse claims are used.
TREC COVID challenge. The claims used in the
challenge were obtained and reformulated manu-
ally by us as full sentences using the explanations
related to each query (e.g., for a given query “ coro-
navirus immunity ", and its explanation “ will SARS-
CoV2 infected people develop immunity? ", we form
the following claim, “ coronavirus infected people
develop immunity ").True andFalse claims are
used.
The above processed data sources were com-
bined to provide 20,689 initial claims.
A.2 Claim Categorisation
The claims were analysed to identify types of
claims that may be of particular interest, either
for inclusion or exclusion depending on the type
of analysis. The following types were identified:
(1) Multimodal; (2) Social media references; (3)
Claims including questions; (4) Claims including
numerical content; (5) Named entities, including:
PERSON −People, including fictional; ORGA-
NIZATION −Companies, agencies, institutions,
etc.; GPE −Countries, cities, states; FACILITY −
Buildings, highways, etc. These entities have been
detected using a RoBERTa base English model (Liu
et al., 2019) trained on the OntoNotes Release 5.0
dataset (Weischedel et al., 2013) using Spacy.
B Parameter Setting
In our veracity assessment experiments, the param-
eters of the initial RoBERTa models are frozen
during the training. The inputs are padded and
truncated to the longest sequence, and a ReLU
function is used as the activation function for the
hidden layer. The GCNConv outputs are padded
to the longest graph size. The loss function used
is cross-entropy. The size of the hidden layer is
50, the batch size is 30, and the training is per-
formed for 100 epochs for NLI-SAN and its vari-
ants, and 200 epochs for NLI-graph and its vari-
ants. The optimizer used is AdamW (Loshchilov
and Hutter, 2019) with β= 0.9,β= 0.999, a
weight decay of 0.01, and a learning rate of 10
forNLI,10forNLI+Sent andNLI-SAN , and1510
a learning rates of 10forNLI-graph ,10
forNLI-graph, and 10forNLI+PSent ,
these last three with a step size of 0.1after 100
epochs.
C Additional Examples of Document or
Sentence Retrieval Errors
Here we expand on the examples mentioned in
Section 5.3 related to difficulties in the document
or sentence retrieval parts of the process. Table
A1 presents in more detail the cases previously
mentioned, and includes new examples.1511