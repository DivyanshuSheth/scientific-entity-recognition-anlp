
Mohammad AkbarTajari, Sara Rajaee,and Mohammad Taher PilehvarSharif University of Technology, IranUniversity of Amsterdam, NetherlandsTehran Institute for Advanced Studies, Khatam University, Iran
m.akbarTajari@gmail.com s.rajaee@uva.nl mp792@cam.ac.uk
Abstract
Parameter-efficient fine-tuning approaches
have recently garnered a lot of attention.
Having considerably lower number of train-
able weights, these methods can bring about
scalability and computational effectiveness.
In this paper, we look for optimal sub-
networks and investigate the capability of
different transformer modules in transferring
knowledge from a pre-trained model to a
downstream task. Our empirical results
suggest that every transformer module in
BERT can act as a winning ticket : fine-tuning
each specific module while keeping the rest
of the network frozen can lead to comparable
performance to the full fine-tuning. Among
different modules, LayerNorms exhibit the
best capacity for knowledge transfer with
limited trainable weights, to the extent that,
with only 0.003% of all parameters in the
layer-wise analysis, they show acceptable
performance on various target tasks. On
the reasons behind their effectiveness, we
argue that their notable performance could
be attributed to their high-magnitude weights
compared to that of the other modules in the
pre-trained BERT. The code for this paper
is freely available at .
1 Introduction
Fine-tuning is widely used as a procedure to em-
ploy the knowledge learned during pre-training of
language models for specific tasks (Howard and
Ruder, 2018; Peters et al., 2019; Merchant et al.,
2020; Zhou and Srikumar, 2022). However, fine-
tuning can be a computationally expensive pro-
cess, given that it usually involves updating all
the parameters in transformer-based models which
are often massive in size. Parameter-efficient fine-
tuning methods try to ameliorate this by reducing
the number of updatable parameters during fine-
tuning.Adapters (Houlsby et al., 2019; Pfeiffer et al.,
2020; Wang et al., 2021; Rücklé et al., 2021;
Karimi Mahabadi et al., 2021; Hu et al., 2021)
try to circumvent this issue by inserting light-
weight modules in the transformer blocks, tun-
ing of which usually results in comparable per-
formance to the full fine-tuning (while the num-
ber of updatable parameters is significantly lower).
Nevertheless, introducing new parameters to an
already-large model can be considered a draw-
back. Another category of parameter-efficient
fine-tuning methods is based on the Lottery Ticket
Hypothesis (Prasanna et al., 2020), where the goal
is to find a small subset of parameters that can
compete with the full fine-tuning setting. Vari-
ous subsets of network parameters have been sug-
gested as the winning ticket , including the con-
nections with high magnitudes (Han et al., 2015),
identity mappings (Lin et al., 2020), and dominant
dimensions (Guo et al., 2021).
In this paper, we study the ability of differ-
ent modules of a transformer block in knowl-
edge transfer. Our experiments provide a more
comprehensive analysis than the existing work,
which usually suggests specific modules as the
winning ticket , such as the bias terms (Ben Zaken
et al., 2022). Through module-wise fine-tuning,
we check if the winning ticket is a property that
can be associated only with some specific mod-
ules in the transformer block. Our results sug-
gest that allindividual modules possess this prop-
erty to some extent. Among these, LayerNorms
prove to be highly reliable for knowledge trans-
fer: fine-tuning only 37kLayerNorm weights (out
of110Mparameters in BERT-base) is often on
par with full fine-tuning on various downstream
tasks. Extending this analysis, we show that tun-
ing even only one LayerNorm can yield compara-
ble performance and that the middle layers are the
best in terms of transferability. We also investigate
the reasons behind the effectiveness of LayerNorm10617tuning. Our experiments suggest that this could
be due to the relatively high-magnitude weights in
these modules. In fact, we show that tuning just a
tiny fraction of high-magnitude dimensions (usu-
ally referred to as outliers ) can lead to competitive
performance on various tasks.
2 Winning Modules
According to the Lottery Ticket Hypothesis, there
are small sub-networks whose performance is
comparable to the over-parameterized model on
different tasks (Frankle and Carbin, 2019). Sev-
eral studies have been carried out to identify sub-
networks across the model that can provide the
best transferability (Gale et al., 2019; Evci et al.,
2020; Lee et al., 2021; Guo et al., 2021; Hu
et al., 2021). Nonetheless, finding the winning
sub-network usually requires extra computation,
which is costly in terms of time and memory. In
this section, we take another look at the trans-
former block of BERT and focus on the ability
of its different modules to transfer knowledge to
various downstream tasks. More specifically, we
aim to find the winning module among the differ-
ent modules in the transformer-based architecture
of the pre-trained BERT.
2.1 Experimental Setup
Datasets. We fine-tune our models on the GLUE
benchmark (Wang et al., 2018). We leave out
the WNLI (the Winograd Schema Challenge) task
(Levesque et al., 2012)), given that BERT’s per-
formance on this benchmark is not much better
than a random classifier. Instead, we test the
models on the Corpus of Linguistic Acceptabil-
ity (Warstadt et al., 2019, CoLA), the Stanford
Sentiment Treebank (Socher et al., 2013, SST-2),
the Microsoft Research Paraphrase Corpus (Dolan
and Brockett, 2005, MRPC), the Semantic Textual
Similarity (Cer et al., 2017, STS-B), the Quora
Question Pairs (Wang et al., 2018, QQP), the
Multi-Genre Natural Language Inference Corpus
(Williams et al., 2018, MNLI), the Stanford Ques-
tion Answering Dataset (Rajpurkar et al., 2016,
QNLI), and the Recognizing Textual Entailment
(Dagan et al., 2005, RTE). All the reported re-
sults are obtained on the corresponding develop-
ment sets.
Models. We opt for bert-base-uncased, imple-
mented by the HuggingFace library in TensorFlow(Wolf et al., 2020; Abadi et al., 2015). The maxi-
mum sequence length is set to 128. Except for the
fully fine-tuned model (Full-FT), where we train
the models for five epochs, the number of epochs
is chosen based on the size of the tasks: 10 epochs
for SST-2, QQP, MNLI, and QNLI and 20 epochs
otherwise. We use the Adam optimizer with an
epsilon set to 1e-6, a warmup ratio of 10%, and
a batch size of 16. The only hyperparameter tun-
ing we do is on choosing the learning rate from
{1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2} to draw
a fair comparison with previous work. We report
the average and standard deviation of the results of
three models trained with different random seeds.
All the models are trained on four NVIDIA Tesla
V100S-32G GPUs.
Module Settings. To find out the potential of
transformer modules in transfer learning, we pick
similar modules across all layers and fine-tune
them while keeping the rest of the network frozen.
The aim of this setup is to broaden our insights
on the distribution of knowledge across the model
and the adaptability of different modules to target
tasks.
In every transformer block, we check for the
role played by the Multi-head attention, Feedfor-
ward layer, and LayerNorms in knowledge trans-
fer. Since every transformer block has two Layer-
Norms (attention and feedforward), we also con-
sider fine-tuning them separately ( LayerNorms
and LayerNorms). We also compare against
the replicated results of BitFit (Ben Zaken et al.,
2022), in which consistent bias terms across the
transformer blocks are employed for fine-tuning.
To verify if consistency in selecting parameters
matters, we also show the results of fine-tuning
only a small randomly selected subset of all the pa-
rameters with the same size as LayerNorms (Ran-
dom). In the experiments, the full fine-tuning
(Full-FT ) and Frozen modes are considered as the
upper and lower bounds, respectively.
2.2 Results
Table 1 shows our experimental results on eight
tasks from the GLUE benchmark.For each set-
ting, we also report the corresponding ratio of up-
datable parameters (compared to full fine-tuning).10618
As can be observed, individual modules of BERT
can be considered as winning tickets because they
can achieve comparable performance to the Full-
FTsetting, despite involving significantly smaller
numbers of trainable parameters. In particular,
LayerNorms prove to have a high potential in
transferability and adaptability to various down-
stream tasks with a very limited set of trainable
parameters ( 0.034% ). The performance is mostly
preserved even when only one of the two Layer-
Norms is set to be trainable, reducing the number
of effective parameters to 0.017% of that in the
full fine-tuning. Moreover, our results also reveal
that selecting consistent weights (similar modules
across layers) has a key role in fine-tuning qual-
ity, given that the random subset of a comparable
number of parameters does not lead to the same
performance levels.
2.3 Token-level Classification
In addition to the sentence-level tasks of the
GLUE benchmark, we also conduct experiments
on two different token-level datasets to broaden
our insights on the capacity of individual mod-
ules: Penn Treebank Part-of-speech tagging (Mar-
cus et al., 1993) and CoNLL-2003 Named Entity
Recognition (Tjong Kim Sang and De Meulder,
2003). For part-of-speech tagging, we use the sub-
set of the Wall Street Journal (WSJ) portion of
PTB which is freely available in the Natural Lan-
guage Toolkit (Bird et al., 2009, NLTK). In this ex-
periment, we adhere to the convention of using the
cased version of BERT, given the case-sensitive
nature of these token-level tasks.
Table 2 summarizes the results. Similarly to
what is observed on the sentence-level tasks, Ley-
erNorms can attain competitive performance on
the two token-level tasks, despite involving just a
small fraction of all the model parameters. More-
over, in comparison with the equal number of ran-
domly selected weights, they demonstrate remark-
ably better performance.
2.4 Single Norms Tuning
Previous studies have reported that different lay-
ers do not contribute equally to the ultimate per-
formance in transfer learning (Zhou and Sriku-
mar, 2021; Rogers et al., 2020; Kovaleva et al.,
2019; Mehrafarin et al., 2022). We are interested
in studying the extent to which individual Layer-
Norms in different transformer blocks are adapt-
able to downstream tasks. To this end, we per-
form a layer-wise analysis in which the only train-
able parameters are the two LayerNorms in each
block and the final classifier. Therefore, the total
number of fine-tuning parameters is less than 5K10619
(3,072 and 1,538 for LayerNorms and the classi-
fier, respectively), which is about 0.003% of all
the parameters. Due to our limited computational
resources, we restrict our experiments to CoLA,
MRPC, STS-B, and RTE.
Table 3 presents the results for the layer-wise
analysis. According to fine-tuning results, tuning
a single LayerNorm may be sufficient to achieve
performance comparable to fine-tuning all Lay-
erNorms. Furthermore, the middle-layer Layer-
Norms exhibit the best results across all layers,
which can be attributed to the high transferability
of the middle layers in BERT, corroborating previ-
ous findings on the concentration of task-specific
features in these subsets of the network (Liu et al.,
2019).
3 Analysis
In the previous section, we have shown that differ-
ent modules of a transformer block can play as the
winning tickets , since they all have the potential
for transferring knowledge to the selected down-
stream tasks. Among different modules, Layer-
Norms have proven to be the most reliable in fine-
tuning. In this section, we search for the reasons
behind the effectiveness of these modules. To this
end, we focus on the magnitude of every weight
and how they change during full fine-tuning across
all layers.
As a first step, in Figure 1, we visualize the
distribution of weights for different BERT mod-
ules on RTE and STS-B (more tasks can be found
in the Appendix). In general, the distribution
of weights is similar across Feed-Forward and
Multi-Head modules. Nevertheless, LayerNorms
tend to have a bimodal distribution, with one of
the modes having significantly higher magnitudes.
The pattern is consistent across LayerNormsand
LayerNorms. We hypothesize that these high-
magnitude weights are the reason behind the ef-
fectiveness of LayerNorms and, in what follows,check our hypothesis by restricting our experi-
ments to only high-magnitude dimensions of Lay-
erNorms.
3.1 Outlier Tuning
Outliers are high-magnitude weights in Layer-
Norms appearing early in the pre-training process
(Kovaleva et al., 2021). Transformer-based mod-
els perform significantly worse on downstream
tasks when their outliers are disabled after the fine-
tuning process (Kovaleva et al., 2021).
In this experiment, we choose outliers as the
set of nweights whose values are farthest from
the mean. Except for the outliers, all the param-
eters are frozen during fine-tuning. It should be
considered that the specific dimensions where the
outliers appear may not necessarily be the same
across different layers.
Table 4 presents the performance of fine-tuned
BERT in two different settings and for four differ-
ent values of n: 4, 16, 64, 256. We also report the
results for the corresponding sets of nrandomly
selected weights. As can be observed, outliers tun-
ing leads to competitive performance on most tar-
get tasks, despite using less than 0.0056% of all
the model parameters. Interestingly, tuning in the
extremely constrained setting of n= 4 still out-
performs the frozen model, sometimes by signifi-
cant margins (e.g., on STS-B). Setting nto higher
values gives the model more capacity, bringing
about higher performance.
Overall, we can conclude that the high-
magnitude weights in LayerNorms play an impor-
tant role in the effectiveness of these modules in
parameter-efficient fine-tuning.
4 Conclusions
In this work, we study the efficiency of differ-
ent modules in the transformer block of BERT to
transfer knowledge from the pre-trained model to
various downstream tasks. Our experimental re-
sults demonstrate that, contrary to what was sug-10620
gested by previous work, every module can be a
winning ticket , achieving comparable performance
to the full fine-tuning scenario. Among all mod-
ules, LayerNorms prove to be the most reliable for
transferability with a limited number of trainable
weights, such that tuning them in only one layer
can be sufficient for attaining performance on a par
with that of the full fine-tuning. We find that the
weights in these modules have notably high mag-
nitudes compared to other modules, which could
be the reason for their effectiveness. We exam-
ine this hypothesis through outlier tuning (tuning
only the nweights in a LayerNorm whose values
are farthest from the mean), limiting the number
of tunable parameters to a significantly small frac-
tion.
Our results pave the way for better parameter-
efficient fine-tuning of large language models
without the need for costly algorithms to deter-
mine the optimum sub-network or introduce ad-
ditional parameters for knowledge transfer.5 Acknowledgment
We thank the anonymous reviewers for the con-
structive comments and suggestions that helped
improve the paper. Sara Rajaee is funded in
part by the Netherlands Organization for Sci-
entific Research (NWO) under project number
VI.C.192.080.
6 Limitations
We were subject to the constraints of computa-
tional resources; as a consequence, we reported re-
sults only for bert-base and chose the four smallest
tasks of the GLUE benchmark in the tuning sin-
gle norms (Section 2.4) as well as outliers tuning
(Section 3.1). Obviously, the more trainable pa-
rameters a model has, the more accurate its results
will be. Since our Outlier Tuning technique fine-
tunes just a tiny portion of parameters, less than
0.006% of the model weights, there is an upper
bound on its learning capability.10621References1062210623
A Weight Distribution
Figure 2 demonstrates the distribution of weights
for different BERT modules on MRPC and CoLA.1062410625