
Ilias ChalkidisAbhik JanaDirk HartungMichael Bommarito
Ion AndroutsopoulosDaniel Martin KatzNikolaos AletrasUniversity of Copenhagen, DenmarkUniversit ¬®at Hamburg, GermanyBucerius Law School, Hamburg, GermanyCodeX, Stanford Law School, United StatesAthens University of Economics and Business, GreeceUniversity of She eld, UKIllinois Tech ‚Äì Chicago Kent College of Law, United States
Abstract
Laws and their interpretations, legal arguments
and agreements are typically expressed in writ-
ing, leading to the production of vast corpora
of legal text. Their analysis, which is at the
center of legal practice, becomes increasingly
elaborate as these collections grow in size.
Natural language understanding (NLU) tech-
nologies can be a valuable tool to support le-
gal practitioners in these endeavors. Their use-
fulness, however, largely depends on whether
current state-of-the-art models can generalize
across various tasks in the legal domain. To
answer this currently open question, we intro-
duce the Legal General Language Understand-
ing Evaluation (LexGLUE) benchmark, a col-
lection of datasets for evaluating model perfor-
mance across a diverse set of legal NLU tasks
in a standardized way. We also provide an
evaluation and analysis of several generic and
legal-oriented models demonstrating that the
latter consistently o er performance improve-
ments across multiple tasks.
1 Introduction
Law is a Ô¨Åeld of human endeavor dominated by
the use of language. As part of their professional
training, law students consume large bodies of
text as they seek to tune their understanding of
the law and its application to help manage human
behavior. Virtually every modern legal system
produces massive volumes of textual data (Katz
et al., 2020). Lawyers, judges, and regulators con-
tinuously author legal documents such as briefs,
memos, statutes, regulations, contracts, patents and
judicial decisions (Coupette et al., 2021). Beyond
the consumption and production of language, law
and the art of lawyering is also an exercise centered
around the analysis and interpretation of text.
Natural language understanding (NLU) technolo-
gies can assist legal practitioners in a variety of
legal tasks (Chalkidis and Kampas, 2018; AletrasFigure 1: LexGLUE: A new benchmark dataset to eval-
uate the capabilities of NLU models on legal text.
et al., 2019, 2020; Zhong et al., 2020b; Bommarito
et al., 2021), from judgment prediction (Aletras
et al., 2016; Sim et al., 2016; Katz et al., 2017;
Zhong et al., 2018; Chalkidis et al., 2019a; Malik
et al., 2021), information extraction from legal doc-
uments (Chalkidis et al., 2018, 2019c; Chen et al.,
2020; Hendrycks et al., 2021) and case summariza-
tion (Bhattacharya et al., 2019) to legal question an-
swering (Ravichander et al., 2019; Kien et al., 2020;
Zhong et al., 2020a,c) and text classiÔ¨Åcation (Nal-
lapati and Manning, 2008; Chalkidis et al., 2019b,
2020a). Transformer models (Vaswani et al., 2017)
pre-trained on legal, rather than generic, corpora
have also been studied (Chalkidis et al., 2020b;
Zheng et al., 2021; Xiao et al., 2021).
Pre-trained Transformers, including BERT (De-
vlin et al., 2019), GPT-3 (Brown et al., 2020), T5
(Rael et al., 2020), BART (Lewis et al., 2020),
DeBERTa (He et al., 2021) and numerous variants,
are currently the state of the art in most natural lan-
guage processing (NLP) tasks. Rapid performance
improvements have been witnessed, to the extent
that ambitious multi-task benchmarks (Wang et al.,
2018, 2019b) are considered almost ‚Äòsolved‚Äô a few
years after their release and need to be made more
challenging (Wang et al., 2019a).4310Recently, Bommasani et al. (2021) named these
pre-trained models (e.g., BERT, DALL-E, GPT-3)
foundation models . The term may be controversial,
but it emphasizes the paradigm shift these models
have caused and their interdisciplinary potential.
Studying the latter includes the question of how
to adapt these models to legal text (Bommarito
et al., 2021). As discussed by Zhong et al. (2020b)
and Chalkidis et al. (2020b), legal text has dis-
tinct characteristics, such as terms that are uncom-
mon in generic corpora (e.g., ‚Äòrestrictive covenant‚Äô,
‚Äòpromissory estoppel‚Äô, ‚Äòtort‚Äô, ‚Äònovation‚Äô), terms that
have di erent meanings than in everyday language
(e.g., an ‚Äòexecuted‚Äô contract is signed and e ec-
tive, a ‚Äòparty‚Äô is a legal entity), older expressions
(e.g., pronominal adverbs like ‚Äòherein‚Äô, ‚Äòhereto‚Äô,
‚Äòwherefore‚Äô), uncommon expressions from other
languages (e.g., ‚Äòlaches‚Äô, ‚Äòvoir dire‚Äô, ‚Äòcertiorari‚Äô,
‚Äòsub judice‚Äô), and long sentences with unusual word
order (e.g., ‚Äúthe provisions for termination here-
inafter appearing or will at the cost of the borrower
forthwith comply with the same‚Äù) to the extent
that legal language is often classiÔ¨Åed as a ‚Äòsub-
language‚Äô (Tiersma, 1999; Williams, 2007; Haigh,
2018). Furthermore, legal documents are often
much longer than the maximum length state-of-
the-art deep learning models can handle, including
those designed to handle long text (Beltagy et al.,
2020; Zaheer et al., 2020; Yang et al., 2020).
Inspired by the recent widespread use of the
GLUE multi-task benchmark NLP dataset (Wang
et al., 2018, 2019b), the subsequent more di cult
SuperGLUE (Wang et al., 2019a), other previous
multi-task NLP benchmarks (Conneau and Kiela,
2018; McCann et al., 2018), and similar initiatives
in other domains (Peng et al., 2019), we intro-
duce LexGLUE, a benchmark dataset to evaluate
the performance of NLP methods in legal tasks.
LexGLUE is based on seven English existing legal
NLP datasets, selected using criteria largely from
SuperGLUE (discussed in Section 3.1).
We anticipate that more datasets, tasks, and
languages will be added in later versions of
LexGLUE.As more legal NLP datasets become
available, we also plan to favor datasets checked
thoroughly for validity (scores reÔ¨Çecting real-life
performance), annotation quality, statistical power,
and social bias (Bowman and Dahl, 2021).
As in GLUE and SuperGLUE (Wang et al.,2019b,a), one of our goals is to push towards
generic (or ‚Äòfoundation‚Äô) models that can cope with
multiple NLP tasks, in our case legal NLP tasks,
possibly with limited task-speciÔ¨Åc Ô¨Åne-tuning. An-
other goal is to provide a convenient and informa-
tive entry point for NLP researchers and practition-
ers wishing to explore or develop methods for legal
NLP. Having these goals in mind, the datasets we
include in LexGLUE and the tasks they address
have been simpliÔ¨Åed in several ways, discussed be-
low, to make it easier for newcomers and generic
models to address all tasks. We provide Python
APIs integrated with Hugging Face (Wolf et al.,
2020; Lhoest et al., 2021) to easily import all the
datasets we experiment with and evaluate the per-
formance of di erent models (Section 4.4).
By unifying and facilitating the access to a set of
law-related datasets and tasks, we hope to attract
not only more NLP experts, but also more interdis-
ciplinary researchers (e.g., law doctoral students
willing to take NLP courses). More broadly, we
hope LexGLUE will speed up the adoption and
transparent evaluation of new legal NLP methods
and approaches in the commercial sector, too. In-
deed, there have been many commercial press re-
leases in the legal tech industry on high-performing
systems, but almost no independent evaluation of
the performance of machine learning and NLP-
based tools. A standard publicly available bench-
mark would also allay concerns of undue inÔ¨Çuence
in predictive models, including the use of metadata
which the relevant law expressly disregards.
2 Related Work
The rapid growth of the legal text processing Ô¨Åeld
is demonstrated by numerous papers presented
in top-tier conferences in NLP and artiÔ¨Åcial in-
telligence (Luo et al., 2017; Zhong et al., 2018;
Chalkidis et al., 2019a; Valvoda et al., 2021) as well
as surveys (Chalkidis and Kampas, 2018; Zhong
et al., 2020b; Bommarito et al., 2021). Moreover,
specialized workshops on NLP for legal text (Ale-
tras et al., 2019; Di Fatta et al., 2020; Aletras et al.,
2020) are regularly organized.
A core task in this area has been legal judgment
prediction (forecasting), where the goal is to pre-
dict the outcome (verdict) of a court case. In this
direction, there have been at least three lines of
work. The Ô¨Årst one (Aletras et al., 2016; Chalkidis
et al., 2019a; Medvedeva et al., 2020, 2021) pre-
dicts violations of human rights in cases of the4311European Court of Human Rights (ECtHR). The
second line of work (Luo et al., 2017; Zhong et al.,
2018; Yang et al., 2019) considers Chinese criminal
cases where the goal is to predict relevant law arti-
cles, criminal charges, and the term of the penalty.
The third line of work (Ruger et al., 2004; Katz
et al., 2017; Kaufman et al., 2019) includes meth-
ods for predicting the outcomes of cases of the
Supreme Court of the United States (SCOTUS).
The same or similar tasks have also been studied
with court cases in many other jurisdictions includ-
ing France ( S ¬∏ulea et al., 2017), Philippines (Virtu-
cio et al., 2018), Turkey (Mumcuo Àòglu et al., 2021),
Thailand (Kowsrihawat et al., 2018), United King-
dom (Strickson and De La Iglesia, 2020), Germany
(Urchs et al., 2021), and Switzerland (Niklaus et al.,
2021). Apart from predicting court decisions, there
is also work aiming to interpret (explain) the deci-
sions of particular courts (Ye et al., 2018; Chalkidis
et al., 2021c; Branting et al., 2021).
Another popular task is legal topic classiÔ¨Åca-
tion. Nallapati and Manning (2008) highlighted the
challenges of legal document classiÔ¨Åcation com-
pared to more generic text classiÔ¨Åcation by using a
dataset including docket entries of US court cases.
Chalkidis et al. (2020a) classify EU laws into Eu-
roV oc concepts, a task earlier introduced by Mencia
and F ¬®urnkranzand (2007), with a special interest
in few- and zero-shot learning. Luz de Araujo
et al. (2020) also studied topic classiÔ¨Åcation us-
ing a dataset of Brazilian Supreme Court cases.
There are similar interesting applications in con-
tract law (Lippi et al., 2019; Tuggener et al., 2020).
Several studies (Chalkidis et al., 2018, 2019c;
Hendrycks et al., 2021) explored information ex-
traction from contracts, to extract important infor-
mation such as the contracting parties, agreed pay-
ment amount, start and end dates, applicable law,
etc. Other studies focus on extracting information
from legislation (Cardellino et al., 2017; Angelidis
et al., 2018) or court cases (Leitner et al., 2019).
Legal Question Answering (QA) is another task
of interest in legal NLP, where the goal is to train
models for answering legal questions (Kim et al.,
2015; Ravichander et al., 2019; Kien et al., 2020;
Zhong et al., 2020a,c; Louis and Spanakis, 2022).
Not only is this task interesting for researchers
but it could support e orts to help laypeople better
understand their legal rights. In the general task set-
ting, this requires identifying relevant legislation,
case law, or other legal documents, and extractingelements of those documents that answer a partic-
ular question. A notable venue for legal QA has
been the Competition on Legal Information Extrac-
tion and Entailment (COLIEE) (Kim et al., 2016;
Kano et al., 2017, 2018).
More recently, there have also been e orts to
pre-train Transformer-based language models on
legal corpora (Chalkidis et al., 2020b; Zheng et al.,
2021; Xiao et al., 2021), leading to state-of-the-
art results in several legal NLP tasks, compared to
models pre-trained on generic corpora.
Overall, the legal NLP literature is overwhelm-
ing, and the resources are scattered. Documenta-
tion is often not available, and evaluation measures
vary across articles studying the same task. Our
goal is to create the Ô¨Årst uniÔ¨Åed benchmark to ac-
cess the performance of NLP models on legal NLU.
As a Ô¨Årst step, we selected a representative group
of tasks, using datasets in English that are also pub-
licly available, adequately documented and have
an appropriate size for developing modern NLP
methods. We also introduce several simpliÔ¨Åcations
to make the new benchmark more standardized and
easily accessible, as already noted.
3 LexGLUE Tasks and Datasets
We present the Legal General Language Under-
standingEvaluation (LexGLUE) benchmark, a
collection of datasets for evaluating model perfor-
mance across a diverse set of legal NLU tasks.
3.1 Dataset Desiderata
The datasets of LexGLUE were selected to satisfy
the following desiderata:
Language: In this Ô¨Årst version of LexGLUE, we
only consider English datasets, which also makes
experimentation easier for researchers across the
globe. We hope to include other languages in
future versions of LexGLUE.
Substance:The datasets should check the abil-
ity of systems to understand and reason about
legal text to a certain extent in order to perform
tasks that are meaningful for legal practitioners.
Diculty: The performance of state-of-the-art
methods on the datasets should leave large scope
for improvements (cf. GLUE and SuperGLUE,4312
where top-ranked models now achieve average
scores higher than 90%). Unlike SuperGLUE
(Wang et al., 2019a), we did not rule out, but
rather favored, datasets requiring domain (in our
case legal) expertise.
Availability & Size: We consider only publicly
available datasets, documented by published arti-
cles, avoiding proprietary, untested, poorly doc-
umented datasets. We also excluded very small
datasets, e.g., with fewer than 5K documents. Al-
though large pre-trained models often perform
well with relatively few task-speciÔ¨Åc training in-
stances, newcomers may wish to experiment with
simpler models that may perform disappointingly
with small training sets. Small test sets may also
lead to unstable and unreliable results.
3.2 Tasks and Datasets
LexGLUE comprises seven datasets. Table 1 shows
core information for each of the LexGLUE datasets
and tasks, described in detail below.
ECtHR Tasks A & B The European Court of
Human Rights (ECtHR) hears allegations that a
state has breached human rights provisions of the
European Convention of Human Rights (ECHR).
We use the dataset of Chalkidis et al. (2019a,
2021c), which contains approx. 11K cases from
the ECtHR public database. The cases are chrono-
logically split into training (9k, 2001‚Äì2016), devel-
opment (1k, 2016‚Äì2017), and test (1k, 2017‚Äì2019).
For each case, the dataset provides a list of factual
paragraphs (facts) from the case description. Each
case is mapped to articles of the ECHR that were
violated (if any). In Task A, the input to a model is
the list of facts of a case, and the output is the set of
violated articles. In the most recent version of the
dataset (Chalkidis et al., 2021c), each case is also
mapped to articles of ECHR that were allegedly
violated (considered by the court). In Task B, the
input is again the list of facts of a case, but the
output is the set of allegedly violated articles.The total number of ECHR articles is currently
66. Several articles, however, cannot be violated,
are rarely (or never) discussed in practice, or do not
depend on the facts of a case and concern procedu-
ral technicalities. Thus, we use a simpliÔ¨Åed version
of the label set (ECHR articles) in both Task A and
B, including only 10 ECHR articles that can be
violated and depend on the case‚Äôs facts.
SCOTUS The US Supreme Court (SCOTUS)
is the highest federal court in the United States of
America and generally hears only the most con-
troversial or otherwise complex cases which have
not been su ciently well solved by lower courts.
We release a new dataset combining information
from SCOTUS opinionswith the Supreme Court
DataBase (SCDB)(Spaeth et al., 2020). SCDB
provides metadata (e.g., decisions, issues, decision
directions) for all cases (from 1946 up to 2020). We
opted to use SCDB to classify the court opinions in
the available 14 issue areas (e.g., Criminal Proce-
dure, Civil Rights, Economic Activity, etc.). This
is a single-label multi-class classiÔ¨Åcation task (Ta-
ble 1). The 14 issue areas cluster 278 issues whose
focus is on the subject matter of the controversy
(dispute). The SCOTUS cases are chronologically
split into training (5k, 1946‚Äì1982), development
(1.4k, 1982‚Äì1991), test (1.4k, 1991‚Äì2016) sets.
EUR-LEX European Union (EU) legislation is
published in the EUR-Lex portal.All EU laws
are annotated by EU‚Äôs Publications O ce with
multiple concepts from EuroV oc, a multilingual
thesaurus maintained by the Publications O ce.
The current version of EuroV oc contains more than
7k concepts referring to various activities of the
EU and its Member States (e.g., economics, health-
care, trade). We use the English part of the dataset
of Chalkidis et al. (2021a), which comprises 65k
EU laws (documents) from EUR-Lex. Given a4313
document, the task is to predict its EuroV oc labels
(concepts). The dataset is chronologically split in
training (55k, 1958‚Äì2010), development (5k, 2010‚Äì
2012), test (5k, 2012‚Äì2016) subsets. It supports
four di erent label granularities, comprising 21,
127, 567, 7390 EuroV oc concepts, respectively. We
use the 100 most frequent concepts from level 2,
which has a highly skewed label distribution and
temporal concept drift (Chalkidis et al., 2021a),
making it su ciently di cult for an entry point.
LEDGAR Tuggener et al. (2020) introduced
LEDGAR (Labeled EDGAR), a dataset for contract
provision (paragraph) classiÔ¨Åcation. The contract
provisions come from contracts obtained from the
US Securities and Exchange Commission (SEC) Ô¨Ål-
ings, which are publicly available from EDGAR
(Electronic Data Gathering, Analysis, and Retrieval
system). The original dataset includes approx. 850k
contract provisions labeled with 12.5k categories.
Each label represents the single main topic (theme)
of the corresponding contract provision, i.e., this
is a single-label multi-class classiÔ¨Åcation task. In
LexGLUE, we use a subset of the original dataset
with 80k contract provisions, considering only the
100 most frequent categories as a simpliÔ¨Åcation.
We split the new dataset chronologically into train-
ing (60k, 2016‚Äì2017), development (10k, 2018),
and test (10k, 2019) sets.
UNFAIR-ToS The UNFAIR-ToS dataset (Lippi
et al., 2019) contains 50 Terms of Service (ToS)
from on-line platforms (e.g., YouTube, Ebay, Face-
book, etc.). The dataset has been annotated on the
sentence-level with 8 types of unfair contractual
terms , meaning terms (sentences) that potentially
violate user rights according to EU consumer law.
The input to a model is a sentence, the output is
the set of unfair types (if any). We split the dataset
chronologically into training (5.5k, 2006‚Äì2016),
development (2.3k, 2017), test (1.6k, 2017) sets.CaseHOLD The CaseHOLD (Case Holdings on
Legal Decisions) dataset (Zheng et al., 2021) con-
tains approx. 53k multiple choice questions about
holdings of US court cases from the Harvard Law
Library case law corpus. Holdings are short sum-
maries of legal rulings that accompany referenced
decisions relevant for the present case, e.g.:
‚Äú:::to act pursuant to City policy, re d 503, 506-07
(3d Cir.l985)( holding that for purposes of a class
certiÔ¨Åcation motion the court must accept as true
all factual allegations in the complaint and may
draw reasonable inferences therefrom ). ‚Äù
The input consists of an excerpt (or prompt) from
a court decision , containing a reference to a partic-
ular case, where the holding statement (in boldface)
is masked out. The model must identify the cor-
rect (masked) holding statement from a selection of
Ô¨Åve choices. We split the dataset in training (45k),
development (3.9k), test (3.9k) sets, excluding sam-
ples that are shorter than 256 tokens. Chronological
information is missing from CaseHOLD, thus we
cannot perform a chronological re-split.
4 Models Considered
4.1 Linear SVM
Our Ô¨Årst baseline model is a linear Support Vector
Machine (SVM) (Cortes and Vapnik, 1995) with
TF-IDF features for the top- Kfrequent n-grams of
the training set, where n2[1;2;3].
4.2 Pre-trained Transformer Models
We experiment with Transformer-based (Vaswani
et al., 2017) pre-trained language models, which
achieve state of the art performance in most NLP
tasks (Bommasani et al., 2021) and NLU bench-
marks (Wang et al., 2019a). These models are pre-
trained on very large unlabeled corpora to predict
masked tokens (masked language modeling) and
typically also to perform other pre-training tasks
that still do not require any manual annotation (e.g.,
predicting if two sentences were adjacent in the
corpus or not, dubbed next sentence prediction).4314
The pre-trained models are then Ô¨Åne-tuned (further
trained) on task-speciÔ¨Åc (typically much smaller)
annotated datasets, after adding task-speciÔ¨Åc layers.
We Ô¨Åne-tune and evaluate the performance of the
following publicly available models (Table 2).
BERT (Devlin et al., 2019) is the best-known pre-
trained Transformer-based language model. It is
pre-trained to perform masked language modeling
and next sentence prediction.
RoBERTa (Liu et al., 2019) is also a pre-trained
Transformer-based language model. Unlike BERT,
RoBERTa uses dynamic masking, it eliminates the
next sentence prediction pre-training task, uses
a larger vocabulary, and has been pre-trained
on much larger corpora. Liu et al. (2019) re-
ported improved results on NLU benchmarks using
RoBERTa, compared to BERT.
DeBERTa (He et al., 2021) is another improved
BERT model that uses disentangled attention, i.e.,
four separate attention mechanisms considering
the content and the relative position of each token,
and an enhanced mask decoder, which explicitly
considers the absolute position of the tokens. De-
BERTa has been reported to outperform BERT and
RoBERTa in several NLP tasks (He et al., 2021).
Longformer (Beltagy et al., 2020) extends
Transformer-based models to support longer se-
quences, using sparse-attention. The latter is a
combination of local (window-based) attention and
global (dilated) attention that reduces the compu-
tational complexity of the model and thus can be
deployed in longer documents (up to 4096 tokens).
Longformer outperforms RoBERTa on long docu-
ment tasks and QA benchmarks.
BigBird (Zaheer et al., 2020) is another sparse-
attention based transformer that uses a combina-
tion of a local (window-based) attention, global
(dilated), and random attention, i.e., all tokens also
attend a number of random tokens on top of thosein the same neighborhood (window) and the global
ones. BigBird has been reported to outperform
Longformer on QA and summarization tasks.
Legal-BERT (Chalkidis et al., 2020b) is a BERT
model pre-trained on English legal corpora, con-
sisting of legislation, contracts, and court cases. It
uses the original pre-training BERT conÔ¨Åguration.
The sub-word vocabulary of Legal-BERT is built
from scratch, to better support legal terminology.
CaseLaw-BERT (Zheng et al., 2021) is another
law-speciÔ¨Åc BERT model. It also uses the origi-
nal pre-training BERT conÔ¨Åguration and has been
pre-trained from scratch on the Harvard Law case
corpus,which comprises 3.4M legal decisions
from US federal and state courts. This model is
called Custom Legal-BERT by Zheng et al. (2021).
We call it CaseLaw-BERT to distinguish it from
the previously published Legal-BERT of Chalkidis
et al. (2020b) and to better signal that it is trained
exclusively on case law (court opinions).
Hierarchical Variants Legal documents are usu-
ally much longer (i.e., consisting of thousands of
words) than other text types (e.g., tweets, customer
reviews, news articles) often considered in vari-
ous NLP tasks. Thus, standard Transformer-based
models that can typically process up to 512 sub-
word units cannot be directly applied across all
LexGLUE datasets, unless documents are severely
truncated to the model‚Äôs limit. Figure 2 shows
the distribution of text input length across all
LexGLUE datasets. Even for Transformer-based
models speciÔ¨Åcally designed to handle long text
(e.g., Longformer, BigBird), handling longer legal
documents remains a challenge.
Given the length of the text input in three of the
seven LexGLUE tasks, i.e., ECtHR (A and B) and
SCOTUS, we employ a hierarchical variant of each
pre-trained Transformer-based model that has not
been designed for longer text (BERT, RoBERTa,4315DeBERTa, Legal-BERT, CaseLaw-BERT) dur-
ing Ô¨Åne-tuning and inference. The hierarchical
variants are similar to those of Chalkidis et al.
(2021c). They use the corresponding pre-trained
Transformer-based model to encode each para-
graph of the input text independently and ob-
tain the top-level representation h of each
paragraph. A second-level shallow (2-layered)
Transformer encoder with always the same (across
BERT, RoBERTa, DeBERTa etc.) speciÔ¨Åcations
(e.g., hidden units, number of attention heads) is
fed with the paragraph representations to make
them context-aware (aware of the surrounding para-
graphs). We then max-pool over the context-aware
paragraph representations to obtain a document rep-
resentation, which is fed to a classiÔ¨Åcation layer.
4.3 Task-SpeciÔ¨Åc Fine-Tuning
Text ClassiÔ¨Åcation Tasks For EUR-LEX,
LEDGAR and UNFAIR-ToS tasks, we feed
each document to the pre-trained model (e.g.,
BERT) and obtain the top-level representation
h of the special token as the document
representation, following Devlin et al. (2019).
The latter goes through a dense layer of Loutput
units, one per label, followed by a sigmoid
(in EUR-LEX, UNFAIR-ToS) or softmax (in
LEDGAR) activation, respectively. For the two
ECtHR tasks (A and B) and SCOTUS, where
the hierarchical variants are employed, we feed
the max-pooled (over paragraphs) document
representation to a classiÔ¨Åcation linear layer. The
linear layer is again followed by a sigmoid (EctHR)
or softmax (SCOTUS) activation.
Multiple-Choice QA Task For CaseHOLD, we
convert each training (or test) instance (the prompt
and the Ô¨Åve candidate answers) into Ô¨Åve input pairs
following Zheng et al. (2021). Each pair consists of
the prompt and one of the Ô¨Åve candidate answers,
separated by the special delimiter token . The
top-level representation h of each pair is fed
to a linear layer to obtain a logit, and the Ô¨Åve logits
are then passed through a softmax yielding a prob-
ability distribution over the Ô¨Åve candidate answers.
4.4 Data Repository and Code
For reproducibility purposes and to facilitate future
experimentation with other models, we pre-processand release all datasets on Hugging Face Datasets
(Lhoest et al., 2021).We also release the code
of our experiments, which relies on the Hugging
Face Transformers (Wolf et al., 2020) library.
Appendix A explains how to load the datasets and
run experiments with our code.
5 Experiments
5.1 Experimental Set Up
For TFIDF-based linear SVM models, we use the
implementation of Scikit-learn (Pedregosa et al.,
2011) and grid-search for hyper parameters (num-
ber of features, C, and loss function). For all the
pre-trained models, we use publicly available Hug-
ging Face checkpoints.We use the *-base con-
Ô¨Åguration of each pre-trained model, i.e., 12 Trans-
former blocks, 768 hidden units, and 12 attention
heads. We train models with the Adam optimizer
(Kingma and Ba, 2015) and an initial learning rate
of 3e-5 up to 20 epochs using early stopping on de-
velopment data. We use mixed precision (fp16) to
decrease the memory footprint in training and gra-
dient accumulation for all hierarchical models. The
hierarchical models can read up to 64 paragraphs of
128 tokens each. We use Longformer and BigBird
in default settings, i.e., Longformer uses windows
of 512 tokens and a single global token ( ),
while BigBird uses blocks of 64 tokens (windows:
3block, random: 3 block, global: 2initial
block; each token attends 512 tokens in total). The
batch size is 8 in all experiments. We run Ô¨Åve repe-
titions with di erent random seeds and report the
test scores based on the seed with the best scores on
development data. We evaluate performance using
micro-F1 (-F) and macro-F1 (m-F) across all
datasets to take into account class imbalance. For
completeness, we also report the arithmetic, har-
monic, and geometric mean across tasks following
Shavrina and Malykh (2021).
5.2 Experimental Results
Main Results Table 3 presents the test results for
all models across all LexGLUE tasks, while Table 44316
presents the aggregated (averaged) results. We ob-
serve that the two legal-oriented pre-trained mod-
els (Legal-BERT, CaseLaw-BERT) perform overall
better, especially considering m- Fthat accounts
for class imbalance (considers all classes equally
important). Their in-domain (legal) knowledge
seems to be more critical in the two datasets rely-
ing on US case law data (SCOTUS, CaseHOLD)
with an improvement of approx. +2-4% p.p. (m-
F) over equally sized Transformer-based models,
which are pre-trained on generic corpora. These
results are explained by the fact that these tasks are
more domain-speciÔ¨Åc in terms of language, com-
pared to the rest. No single model performs best in
all tasks, and the results of Table 3 show that there
is still large scope for improvement (Section 6).
An exceptional case of the dominance of the pre-
trained Transformer models is the SCOTUS dataset,
where the TFIDF-based linear SVM performs best.
We suspect the large size of the SCOTUS opin-
ions (Figure 2) to be the main reason, i.e., in many
cases full paragraphs or parts of them are not con-
sidered by the hierarchical models (limited to 64
paragraphs of 128 tokens each).
Legal-oriented Models Interestingly, the per-
formance of Legal-BERT and CaseLaw-BERT,
the two legal-oriented pre-trained models, is al-
most identical on CaseHOLD, despite the fact thatCaseLaw-BERT is solely trained on US case law.
On the other hand, Legal-BERT has been exposed
to a wider variety of legal corpora, including EU
and UK legislation, ECtHR, ECJ and US court
cases, and US contracts. Legal-BERT performs
as well as or better than CaseLaw-BERT on all
datasets. These results suggest that domain-speciÔ¨Åc
pre-training (and learning a domain-speciÔ¨Åc sub-
word vocabulary) is beneÔ¨Åcial, but over-Ô¨Åtting a
speciÔ¨Åc (niche) sub-domain (e.g., US case law),
similarly to Zheng et al. (2021), has no beneÔ¨Åts.
6 Vision ‚Äì Future Considerations
Beyond the scope of this work and the examined
baseline models, we identify four major factors
that could potentially advance the state of the art in
LexGLUE and legal NLP more generally:
Long Documents: Several Transformer-based
models (Beltagy et al., 2020; Zaheer et al., 2020;
Liu et al., 2022) have been proposed to handle long
documents by exploring sparse attention mecha-
nisms. These models can handle sequences up to
4096 sub-words, which is largely exceeded in three
out of seven LexGLUE tasks (Figure 2). Contrary,
the hierarchical model of Section 4.2 can handle se-
quences up to 8192 sub-words in our experiments,
but a part of the model (the additional Transformer
blocks that make the paragraph embeddings aware
of the other paragraphs) is not pre-trained, which
possibly negatively a ects performance.
Structured Text: Current models for long docu-
ments, like Longformer and BigBird, do not con-
sider the document structure (e.g., sentences, para-
graphs, sections). For example, window-based
attention may consider a sequence of sentences
across paragraph boundaries or even consider trun-
cated sentences. To exploit the document structure,
Yang et al. (2020) proposed SMITH, a hierarchi-4317cal Transformer model that hierarchically encodes
increasingly larger blocks (e.g., words, sentences,
documents). SMITH is very similar to the hierarchi-
cal model of Section 4.2, but it is pre-trained end-
to-end with two objectives: token-level masked and
sentence block language modeling.
Large-scale Legal Pre-training: Recent stud-
ies (Chalkidis et al., 2020b; Zheng et al., 2021;
Bambroo and Awasthi, 2021; Xiao et al., 2021) in-
troduced language models pre-trained on legal cor-
pora, but of relatively small sizes, i.e., 12‚Äì36 GB.
In the work of Zheng et al. (2021), the pre-training
corpus covered only a narrowly deÔ¨Åned area of
legal documents, US court opinions. The same
applies to Lawformer (Xiao et al., 2021), which
was pre-trained on Chinese court opinions. Future
work could curate and release a legal version of the
C4 corpus (Ra el et al., 2020), containing multi-
jurisdictional legislation, court decisions, contracts
and legal literature at a size of hundreds of GBs.
Given such a corpus, a large language model ca-
pable of processing long structured text could be
pre-trained and it might excel in LexGLUE.
Even Larger Language Models: Scaling up the
capacity of pre-trained models has led to increas-
ingly better results in general NLU benchmarks
(Kaplan et al., 2020), and models have been scaled
up to billions of parameters (Brown et al., 2020;
Rael et al., 2020; He et al., 2021). In Ap-
pendix E, we observe that using the large version
of RoBERTa leads to substantial performance im-
provements compared to the base version. The
results are comparable or better - in some cases-
compared to the legal-oriented language models
(Legal-BERT, CaseLaw-BERT). Considering that
the two legal-oriented models are much smaller
and have been pre-trained with (5  10) less data
(Section 2), we have a strong indication for perfor-
mance gains by pre-training larger legal-oriented
models using larger legal corpora.
7 Limitations and Future Work
Although, our benchmark inevitably cannot cover
‚Äúeverything in the whole wide (legal) world ‚Äù (Raji
et al., 2021), we include a representative collection
of English datasets that also ground to a certain
degree in practically interesting applications.
In its current version, LexGLUE can only be
used to evaluate English models. As legal docu-
ments are typically written in the o cial languageof the particular country of origin, there is an in-
creasing need for developing models for other lan-
guages. The current scarcity of datasets in other
languages (with the exception of Chinese) makes a
multilingual extension of LexGLUE challenging,
but an interesting avenue for future research.
Beyond language barriers, legal restrictions cur-
rently inhibit the creation of more datasets. Impor-
tant document types, such as contracts and schol-
arly publications are protected by copyright or con-
sidered trade secrets. As a result, their owners are
concerned with data-leakage when they are used for
model training and evaluation. Providing both legal
and technical solutions, e.g., using privacy-aware
infrastructure and models (Downie, 2004; Feyise-
tan et al., 2020) is a challenge to be addressed.
Access to court decisions can also be hindered by
bureaucratic inertia, outdated technology and data
protection concerns, which collectively result in
these otherwise public decisions not being publicly
available (Pah et al., 2020). While the anonymiza-
tion of personal data provides a solution to this
problem, it is itself an open challenge for legal
NLP (Jana and Biemann, 2021). In lack of suit-
able datasets and benchmarks, we have refrained
from including anonymization in this version of
LexGLUE, but plan to do so at a later stage.
Another limitation of the current version of
LexGLUE is that human evaluation is missing. All
datasets rely on ground truth labels automatically
extracted from data (e.g., court decisions) produced
as part of o cial judicial or archival procedures.
These resources should be highly reliable (valid),
but we cannot statistically assess their quality. In
the future, re-annotating part of the datasets with
multiple legal experts would provide an estimation
of human level performance and inter-annotator
agreement, though the cost would be high, because
of the required legal expertise.
While LexGLUE o ers a much needed uniÔ¨Åed
testbed for legal NLU, there are several other criti-
cal aspects that need to be studied carefully. These
include multi-disciplinary research to better under-
stand the limitations and challenges of applying
NLP to law (Binns, 2020), while also consider-
ing fairness and robustness (Angwin et al., 2016;
Dressel and Farid, 2018; Baker Gillis, 2021; Wang
et al., 2021; Chalkidis et al., 2022), and broader
legal considerations of AI technologies in general
(Schwemer et al., 2021; Tsarapatsanis and Aletras,
2021; Delacroix, 2022).4318Acknowledgments
This work was partly funded by the Innovation
Fund Denmark (IFD)under File No. 0175-
00011A and by the German Federal Ministry of
Education and Research (BMBF) kmu-innovativ
program under funding code 01IS18085. We would
like to thank Desmond Elliott for providing valu-
able feedback (baselines for truncated documents
presented in Appendix D), Xiang Dai and Joel
Niklaus for reviewing and pointing out issues in
the new resources (code, datasets).
Ethics Statement
Original Work Attribution
All datasets included in LexGLUE, except SCO-
TUS, are publicly available and have been previ-
ously published. If datasets or the papers that in-
troduced them were not compiled or written by
ourselves, we referenced the original work and en-
courage LexGLUE users to do so as well. In fact,
we believe this work should only be referenced,
in addition to citing the original work, when ex-
perimenting with multiple LexGLUE datasets and
using the LexGLUE evaluation infrastructure. Oth-
erwise only the original work should be cited.
Social Impact
We believe that this work does not contain any
grounds for ethical concerns. A transparent and
rigorous benchmark for NLP in the legal domain
might serve as an orientation for scholars and in-
dustry researchers. As a result, the capabilities of
tools that are trained using natural language data
from the legal domain will become clearer, thereby
helping their users to better understand them. This
increased certainty would also raise the awareness
within research and industry communities to poten-
tial risks associated with the use of these tools. We
regard this contribution to a more realistic, more
informed discussion as an important use case of the
work presented. Ideally, it could help both begin-
ners and seasoned professionals to understand the
limitations of using NLP tools in the legal domain
and thereby prevent exaggerated expectations and
potential applications that might risk endangering
fundamental rights or the rule of law. We currently
cannot imagine use cases of this particular work
that would lead to ethical concerns or potential
harm (Tsarapatsanis and Aletras, 2021).Licensing & Personal Information
LexGLUE comprises seven datasets: ECtHR
Task A and B, SCOTUS, EUR-LEX, LEDGAR,
UNFAIR-ToS, and CaseHOLD that are available
for re-use and re-share with appropriate attribution.
The data is in general partially anonymized in ac-
cordance with the applicable national law. The data
is considered to be in the public sphere from a pri-
vacy perspective. This is a very sensitive matter,
as the courts try to keep a balance between trans-
parency (the public‚Äôs right to know) and privacy
(respect for private and family life).
ECtHR contains personal data of the parties and
other people involved in the legal proceedings. Its
data is processed and made public in accordance
with the European data protection laws. This in-
cludes either implied consent or legitimate interest
to process the data for research purposes. As a
result, their processing by us or other future users
of the benchmark is not likely to raise ethical con-
cerns.
SCOTUS contains personal data of a similar na-
ture. Again, the data is processed and made avail-
able by the US Supreme Court, whose proceedings
are public. While this ensures compliance with US
law, it is very likely that similarly to the ECtHR
any processing could be justiÔ¨Åed by either implied
consent or legitimate interest under European law.
EUR-LEX by contrast is merely a collection of
legislation material and therefore not likely to con-
tain personal data, except signatory information
(e.g., president of EC). It is openly published by
the European Union and processed by the EU‚Äôs
Publication O ce. In addition, since our work
qualiÔ¨Åes as research, it is privileged pursuant to
Art. 6 (1) (f) GDPR.
LEDGAR contains publicly available contract
provisions published in the EDGAR database of the
US Securities and Exchange Commission (SEC).
As far as personal information might be contained,
it should equally fall into the public sphere and
be covered by research privilege. Our processing
does not focus on personal information at all, rather
attributing content labels to provisions.
UNFAIR-ToS contains Terms of Services from
business entities such as YouTube, Ebay, Facebook,
etc., which makes it unlikely for the data to include
personal information. These companies keep user
data separate from contractual provisions, so to the
best of our knowledge not contained in this dataset.
CaseHOLD contains parts of legal decisions4319from US Court decisions, obtained from the Har-
vard library case law corpus. All of the decisions
were previously published in compliance with US
law. In addition, most instances (case snippets)
are too short to contain identiÔ¨Åable information.
Should such data be contained, their processing
would equally be covered either by implicit consent
or a public interest exception. We use all datasets
in accordance with copyright terms and under the
licenses set forth by their creators.
Limitations & Potential Harms
We have not employed any crowd-workers or anno-
tators for this work. The paper outlines the main
limitations with regard to speaker population (En-
glish) and generalizability in a dedicated section
(Section 7). As a benchmark paper, our claims nat-
urally match the results of the experiments, which
‚Äì given the current detail of instructions ‚Äì should
be easily reproduced. We provide several ways of
accessing the datasets and running the experiments
both with and without Hugging Face infrastructure.
We do not currently foresee any potential harms
for vulnerable or marginalized populations and we
do not use, to the best of our knowledge, any identi-
fying characteristics for populations of these kinds.
References432043214322432343244325
A Datasets, Code, and Participation
Where are the datasets? We provide access
to LexGLUE on Hugging Face Datasets (Lhoest
et al., 2021) at . For example, to load the SCOTUS
dataset, you Ô¨Årst simply install the datasets
Python library and then make the following call:
How do I run experiments? To make repro-
ducing the results of the already examined mod-
els or future models even easier, we release our
code on GitHub ( ). In that repository (in the folder /exper -
iments ), there are Python scripts, relying on the
Hugging Face Transformers library (Wolf et al.,
2020), to run and evaluate any Transformer-based
model (e.g., BERT, RoBERTa, LegalBERT, and
their hierarchical variants, as well as, Longformer,
and BigBird). We also provide bash scripts to repli-
cate the experiments for each dataset with 5 random
seeds, as we did for the reported results.
B No labeling as an additional class
In ECtHR Tasks A & B and UNFAIR-ToS, there
are unlabeled samples. Concretely, in ECtHR Task
A, a possible event is no violation , i.e., the court
ruled that the defendant did not violate any ECHR
article. Contrary, no violation is not a possible
event in the original ECtHR Task B dataset, i.e.,
at least a single ECHR article is allegedly violated(considered by the court) in every case; however,
there is such a rare scenario after the simpliÔ¨Åcations
we introduced, i.e., some cases were originally la-
beled only with rare labels that were excluded from
our benchmark (Section 3.2). In UNFAIR-ToS,
the vast majority of sentences are not labeled with
any type of unfairness (unfair term against users),
i.e., most sentences do not raise any questions of
possible violations of the European consumer law.
In multi-label classiÔ¨Åcation, the set of labels
per instance is represented as a one-hot vector
Y=[y;y;:::; y], where y=1 if the instance
is labeled with the i-th class, and y=0 oth-
erwise. If an instance is not labeled with any
class, its Yincludes only zeros. During training,
binary cross-entropy correctly penalizes such in-
stances, if the predictions ( ÀÜY=[ÀÜy;ÀÜy:::;ÀÜy])
diverge from zeros. During evaluation, however,
the F1-score ( F1=) ignores instances
with Y=ÀÜY=[0;0;:::; 0], because it consid-
ers only the true positives (TP), false positives
(FP), and false negatives (FN), and instances where
Y=ÀÜY=[0;0;:::; 0] contribute no TPs, FPs, FNs.
In order to make F1 sensitive to the correct labeling
of such examples, during evaluation (not training)
we include an additional label ( yorÀÜy) in both
targets ( Y) and predictions ( ÀÜY), whose value is 1
(positive) if the original (without y,ÀÜy)Yand ÀÜY
areY=[0;0;:::; 0] or ÀÜY=[0;0;:::; 0], respec-
tively, and 0 (negative) otherwise. This is partic-
ularly important for proper evaluation, as across
three datasets a considerable portion of the exam-
ples are unlabeled (11.5% in ECtHR Task A, 1.6%
in ECtHR Task B, and 95.5% in UNFAIR-ToS).
C Additional Results
Tables 5 and 6 show development results for all
examined models across datasets. We report the
mean and standard deviations ( ) for the three
seeds (among the Ô¨Åve used) with the best devel-
opment scores per model to exclude catastrophic
failures, i.e., runs with severely low performance.
The standard deviations are relatively low across
models and datasets (up to 0.5% for -Fand up to
1% for m- F). The development results are gener-
ally higher compared to the test ones (cf. Table 3)
in many cases, as one would expect.
Table 7 reports training times per dataset and
model; both the time per epoch ( T=e), and the to-
tal training time ( T) across all epochs. All full-
attention BERT models, except Longformer and4326
Big-Bird, have comparable times with the excep-
tion of DeBERTa that has four separate attention
mechanisms. We observe that when the hierarchi-
cal variant of these models is deployed, i.e., in EC-
tHR tasks and SCOTUS, it is approximately twice
(2) as fast compared to Longformer and BigBird.D Use of 512-token BERT models
In Figure 3, we show results for the standard BERT
model of Devlin et al. (2019), which can process
up to 512 tokens, compared to its hierarchical vari-
ant (Section 4.2), which can process up to 64 128
tokens. We observe that across all datasets that
contain long documents (ECtHR A & B, SCOTUS,
cf. Fig. 2(a)), the hierarchical variant clearly out-
performs the standard model fed with truncated
documents (ECtHR A: +10.2% p.p., ECtHR B:
7.5% p.p., SCOTUS: 4.9% p.p.). Compared to the
ECtHR tasks, the gains are lower in SCOTUS, a
topic classiÔ¨Åcation task where long-range reason-
ing is not needed; by contrast, for ECtHR multiple
distant facts need to be combined. Based on these
results, we conclude that using severely truncated
documents is not a plausible option for LexGLUE,
and other directions for processing long documents
should be considered in the future, ideally fully
pre-trained hierarchical models, contrary to our
semi-pre-trained hierarchical models (Section 6).
E Use of Roberta Large
We additionally evaluate RoBERTa-large, i.e., 24
Transformer blocks, 1024 hidden units, and 18 at-
tention heads, to better understand the dynamics
between domain speciÔ¨Åcity and model size. In this4327
case, we use the AdamW optimizer with a 1e-5
maximum learning rate, warm-up ratio of 0.1, and
a weight decay rate of 0.06, and we use a similar
mini-batch size of 8 examples.
Table 8 reports the development and test results
using the seed (run) with the best development
scores. We observe that using the large version of
RoBERTa, dubbed RoBERTa (L), with more than
2parameters (355M), leads to substantial perfor-
mance improvements compared to the base version
of RoBERTa, dubbed RoBERTa (B), in many tasks.
The results are comparable, or better in some cases,
compared to the legal-oriented language models
(Legal-BERT, CaseLaw-BERT). Considering that
the two legal-oriented models are much smaller
and have been pre-trained with (5  10) less data
(Section 2), we have a strong indication for perfor-
mance gains by pre-training larger legal-oriented
models using larger legal corpora (Section 6).F Other Tasks and Datasets Considered
We considered including the Contract Understand-
ing Atticus Dataset (CUAD) (Hendrycks et al.,
2021), an expertly curated dataset that comprises
510 contracts annotated with 41 valuable contrac-
tual insights (e.g., agreement date, parties, govern-
ing law). The task is formulated as a SQUAD-like
question answering task, where given a question
(the name of an insight) and a paragraph from the
contract, the model has to identify the answer span
in the paragraph.The original dataset follows the
SQUAD v2.0 setting, including unanswerable ques-
tions. Following SQUAD v1.1 (Rajpurkar et al.,
2016), we simpliÔ¨Åed the task by removing all unan-
swerable pairs (question, paragraph), which are the
majority in the original dataset. We also excluded
pairs whose answers exceeded 128 full words to
alleviate the imbalance between short and long an-
swers. We then re-split the dataset chronologically
into training (5.2k, 1994‚Äì2019), development (572,
2019‚Äì2020), and test (604, 2020) sets.
Following Devlin et al. (2019), and similarly to
Hendrycks et al. (2021), for each training (or test)4328instance, we consider pairs that consist of a ques-
tion and a paragraph, separated by the special de-
limiter token . The top-level representations
[h;:::; h] of the tokens of the paragraph are fed
into a linear layer to obtain two logits per token (for
the token being the start or end of the answer span),
which are then passed through a softmax activation
(separately for start and end) to obtain probabil-
ity distributions. The tokens with the highest start
and end probabilities are selected as boundaries of
the answer span. We evaluated performance with
token-level F1 score, similarly to SQUAD.
We trained all the models of Table 2, which
scored approx. 10-20% in token-level F1, with
Legal-BERT performing slightly better than the rest
(+5% F1).In the paper that introduced CUAD
(Hendrycks et al., 2021), several other measures
(Precision@ N% Recall, AUPR, Jaccard similar-
ity) are used to more leniently estimate a model‚Äôs
ability to approximately locate answers in context
paragraphs. Through careful manual inspection of
the dataset, we noticed the following points that
seem to require more careful consideration.
Contractual insights (categories, shown in ital-
ics below) include both entity-level (short) an-
swers (e.g., ‚ÄúSERVICE AGREEMENT‚Äù for Doc-
ument Name , and ‚ÄúImprimis Pharmaceuticals,
Inc.‚Äù for Parties ) and paragraph-level (long) an-
swers (e.g., ‚ÄúIf any of the conditions speciÔ¨Åed in
Section 8 shall not have been fulÔ¨Ålled when and
as required by this Agreement, or by the Closing
Date, or waived in writing by Capital Resources,
this Agreement and all of Capital Resources obli-
gations hereunder may be canceled [...] except
as otherwise provided in Sections 2, 7, 9 and
10 hereof.‚Äù for Termination for Convenience ).
These two di erent types of answers (short and
paragraph-long) seem to require di erent mod-
els and di erent evaluation measures, unlike how
they are treated in the original CUAD paper.
Some contractual insights (categories), e.g., Par-
ties, have been annotated with both short (e.g.,
‚ÄúImprimis Pharmaceuticals, Inc.‚Äù) and long (e.g.,
‚Äútogether, Blackwell and Munksgaard shall be
referred to as ‚Äòthe Publishers‚Äô.‚Äù) answers. Anno-
tations of this kind introduce noise during both
training and evaluation. For example, it becomes
unclear when a short (Ô¨Åner /strict) or a long (loose)
annotation should be taken to be the correct one.Annotations may include indirect mentions, e.g.,
‚ÄòFranchisee‚Äô, ‚ÄòService Provider‚Äô for Parties , in-
stead of the actual entities (the company name).
Annotations may include semi-redacted text (e.g.,
‚Äú , 1996‚Äù for Agreement Date ), or even fully
redacted text (e.g., ‚Äú ‚Äù for Parties ).
This practice may be necessary to hide sensitive
information, but for the purposes of a benchmark
dataset such cases could have been excluded.
The points above, which seem to require revisit-
ing the annotations of CUAD, and the very low F1
scores of all models led us to exclude CUAD from
LexGLUE. We also note that there is related work
covering similar topics, such as Contract Element
Extraction (Chalkidis and Androutsopoulos, 2017),
Contractual Obligation Extraction (Chalkidis et al.,
2018), and Contractual Provision ClassiÔ¨Åcation
(Tuggener et al., 2020), where models perform
much better (in terms of accuracy), relying on sim-
pler (separate) more carefully designed tasks and
much bigger datasets. Thus we believe that the
points mentioned above, which blur the task deÔ¨Åni-
tion of CUAD and introduce noise, and the limited
(compared to larger datasets) number of annota-
tions strongly a ect the performance of the models
on CUAD, underestimating their true potential.
We also initially considered some very interest-
ing legal Information Retrieval (IR) datasets (Locke
and Zuccon, 2018; Chalkidis et al., 2021b) that aim
to examine crucial real-life tasks (relevant case law
retrieval, regulatory compliance). However, we
decided to exclude them from the Ô¨Årst version of
LexGLUE, because they rely on processing multi-
ple long documents and require more task-speciÔ¨Åc
neural network architectures (e.g., siamese net-
works), and di erent evaluation measures. Hence,
they would make LexGLUE more complex and a
less attractive entry point for newcomers to legal
NLP. We plan, however, to include more demand-
ing tasks in future LexGLUE versions, as the legal
NLP community will be growing.
G Dataset Examples
In Table 9, we present training examples, i.e., pairs
of input(s), output(s), for LeXGLUE datasets and
tasks. More examples can be inspected using the
dataset preview functionality provided in the online
dataset card of Hugging Face.43294330