
Zhiming Mao, Huimin Wang, Yiming Du, Kam-Fai WongThe Chinese University of Hong Kong, Hong Kong, ChinaMoE Key Laboratory of High Confidence Software Technologies, ChinaJarvis Lab, Tencent, Shenzhen, China
{zmmao,ydu,kfwong}@se.cuhk.edu.hk
hmmmwang@tencent.com
Abstract
Prior study has shown that pretrained language
models (PLM) can boost the performance of
text-based recommendation. In contrast to pre-
vious works that either use PLM to encode user
history as a whole input text, or impose an
additional aggregation network to fuse multi-
turn history representations, we propose a uni-
fied local- and global-attention Transformer en-
coder to better model two-level contexts of user
history. Moreover, conditioned on user history
encoded by Transformer encoders, our frame-
work leverages Transformer decoders to esti-
mate the language perplexity of candidate text
items, which can serve as a straightforward
yet significant contrastive signal for user-item
text matching. Based on this, our framework,
UniTRec, unifies the contrastive objectives of
discriminative matching scores and candidate
text perplexity to jointly enhance text-based
recommendation. Extensive evaluation shows
that UniTRec delivers SOTA performance on
three text-based recommendation tasks.
1 Introduction
Text-based recommendation (Li et al., 2010; Gu
et al., 2016; Okura et al., 2017; Malkiel et al., 2020)
aims to recommend relevant textual content (e.g.,
news articles, Twitter posts) to people based on
their behaviors as represented in historical log texts.
For instance, engagement recommendation (Cheng
et al., 2022) on social media (e.g., Twitter and Red-
dit) helps users discover and engage with interested
threads by modeling their browsing history.
Pretrained language models (Devlin et al., 2019;
Brown et al., 2020) have made waves in recent
text-based recommendation research (Zhang et al.,
2021; Qi et al., 2022; Geng et al., 2022). The
most common practice is using PLM encoders
(BERT family) to learn representations of user his-
tory and candidate item texts. Recommendationmatching scores are computed over the user and
item representations and finally optimized by noise
contrastive estimation (NCE) loss (Gutmann and
Hyvärinen, 2010) for ranking multiple candidates.
Unlike encoding single text, using PLM to en-
code multi-turn texts of user history is nontrivial.
Existing works (Malkiel et al., 2020; Qi et al., 2022;
Geng et al., 2022) concatenate multi-turn history
texts as a whole input text, then use one PLM en-
coder to learn the holistic user representation. This
is a standard PLM encoding manner but ignores
the relation among history turns, as all word tokens
from different history turns are equally attended.
In contrast, previous studies point out that learn-
ing the relation among user history turns is also
beneficial (Zeng et al., 2020; Qi et al., 2021). An-
other approach is using PLM encoders to learn
representations from multi-turn history texts, fol-
lowed by an additional aggregation network to fuse
the multi-turn representations (Wu et al., 2021;
Li et al., 2022). However, the imposed aggrega-
tion networks (with newly initialized parameters)
weaken the representation power of PLM encoders
which are already pretrained on large-scale corpora.
This work introduces UniTRec, a Unified text-to-
textTransformer framework for text-based Recom-
mendation. In the encoder component of UniTRec,
we design local- and global-attention to learn user
history representations through tailored attention
masking, which aims to jointly model word-level
and turn-level relations of user history. UniTRec
can utilize the full power of PLM encoders because
it preserves the intact structure of PLM encoders
without newly imposed parameters.
Different from most previous works that predict
user-candidate matching scores solely based on the
representations learned by Transformer encoders,
we argue that conditioned on user representations1160
learned by Transformer encoders, candidate text
perplexity (PPL) estimated by pretrained Trans-
former decoders is also a straightforward yet sig-
nificant signal for text-based recommendation. As
shown in Figure 1, we hypothesize that the can-
didate text perplexity estimated by pretrained LM
decoders can directly measure the text matching
degree between user history and candidate texts. It
is because the perplexity estimates the likelihood
of candidate texts based on encoder outputs, which
naturally indicates the probabilities of candidate
texts given the user history. Besides, UniTRec can
use the last hidden states of Transformer decoders
to directly predict matching scores. Hence, this
work unifies the contrastive objectives of discrimi-
native matching scores and candidate text perplex-
ity to jointly enhance text-based recommendation.
The contributions of this work are: ( 1) We pro-
pose local- and global-attention to model two-level
relation of user history without additional parame-
ters, which enjoys the full power of PLM encoders.
(2) We introduce PLM perplexity to measure user-
candidate text matching and unify the objectives of
discriminative matching scores and candidate text
perplexity to enhance text-based recommendation.
(3) Experiments on three text-based recommenda-
tion datasets validate the effectiveness of UniTRec.
2 Approach
2.1 Unified User-history Modeling
Formally, multi-turn history of a user is represented
asH= [t, t, ..., t], and each turn text tcon-
tains|t|words as t= [x, x, ..., x]. UniTRec
aims to unify learning word- and turn-level context
representations in one Transformer encoder.
Local attention on word-level context. We firstconcatenate the multi-turn history texts as the input
tokens X= [x, x, ..., x, ..., x, x, ..., x].
Inspired by Dong et al. (2019), we tailor the atten-
tion masking in Transformer self-attention to learn
the word-level context of each turn. Specifically,
we allow word tokens from the same turn to attend
to each other, while tokens from different turns are
excluded from self-attention computation:
M=/braceleftigg
0, token xandxin the same turn
−∞,otherwise
Attention( Q, K, V ) = softmax(QK
√d+M)V
(1)
, where Q, K, V are self-attention query, key, and
value in Vaswani et al. (2017), Mis the mask ma-
trix to achieve local-attention inside each turn text.
The local self-attention blocks consist of Llayers,
by which original PLM encoders can be adapted to
learn word-level context representations of turns.
Global attention on turn-level context. Over
the local self-attention layers, we leverage global
self-attention to model the relation among history
turns. Specifically, tokens from all turns attend to
each other in self-attention computation (by setting
the mask matrix M=0). In this way, Transformer
encoders can perform global interaction among
each token (and turn) to learn turn-level context
representations of user history. There are Llayers
in the global self-attention blocks, which can also
be inherited from PLM encoders directly.
2.2 Joint Contrastive Ranking Objectives
Conditioned on the history representation, we in-
put the candidate text to Transformer decoders to
predict how likely it should be recommended. It is
worth noting that Transformer decoders can natu-
rally perform effective cross-attention interaction
between history and candidate hidden states.
2.2.1 Objective on Discriminative Scores
Motivated by Lewis et al. (2020), we feed the last
hidden state of decoder output hto an MLP score-
head which predicts the user-candidate matching
score S= ScoreHead( h). The matching score
is discriminative, as higher scores indicate higher
user-candidate matching probabilities.
Following previous works (Li et al., 2022; Qi
et al., 2022), we adopt negative sampling with NCE
loss to optimize matching score prediction. Given
the user history and its ground truth matched can-
didate C, UniTRec predicts the matching score1161
asS. In addition, Kunmatched negative candi-
dates{C}are sampled from the candidate set,
and their matching scores are {S}. The NCE
loss is represented in a contrastive form:
L=−logexp(S)
exp(S) +/summationtextexp(S)(2)
2.2.2 Objective on Candidate Text Perplexity
As aforementioned, UniTRec leverages perplexity
to rank candidate texts. Since lower perplexity in-
dicates higher user-candidate matching probability,
regarding the candidate text Y= [y, y, ..., y],
we define the perplexity-based matching score S
as its negative perplexity:
S=−PPL( Y) =1
T/summationdisplaylogp(y|y)(3)
, where p(·)denotes the target probability output
from the UniTRec Transformer decoder. Similar to
Eq. (2), we optimize the perplexity-based match-
ing score Sin the NCE loss form. As perplexity
empirically varies in a wide range, we introduce a
temperature parameter τto balance the joint NCE
loss gradients following Radford et al. (2021).
L=−logexp(τ·S)
exp(τ·S) +/summationtextexp(τ·S)
(4)
, where τis learnable and initialized to 1. On the
training dataset D, the joint contrastive learning
objective is formulated as:
L=/summationdisplay/parenleftbig
L+L/parenrightbig
(5)2.3 Model Initialization and Inference
As UniTRec is a standard text-to-text Transformer,
we initialize the parameters from pretrained BART
(Lewis et al., 2020). In inference, UniTRec predicts
the discriminative and perplexity-based scores for
each candidate item, respectively. The two sepa-
rate scores SandSare normalized, averaged,
and finally ranked as the output. Detailed ranking
process is provided in Appendix B.
3 Experiments
We evaluate UniTRec on three text-based recom-
mendation tasks: 1)NewsRec , to recommend news
articles to users based on their browsing history.
We use the MIND-small dataset (Wu et al., 2020)
for experiments. 2) QuoteRec , to recommend quo-
tations to users based on their conversation history.
We use the Reddit-quotation dataset (Wang et al.,
2021) for experiments. 3) EngageRec , to recom-
mend social media posts for users to engage with
based on their comment history. We use the dataset
released by Zeng et al. (2020) for experiments. De-
tailed dataset statistics is provided in Appendix A.
Implementation Details. The UniTRec encoder
and decoder both consist of 6Transformer layers
with 768-dimensional hidden states and 12atten-
tion heads. We set L= 3 andL= 3. We use
AdamW optimizer (Loshchilov and Hutter, 2019)
to train UniTRec with cosine learning rate decay.
Baselines. We compare UniTRec with compet-
itive baselines: 1)GRU4Rec (Balázs et al., 2016)
utilizes a GRU network to learn multi-turn history.
2)SASRec (Kang and McAuley, 2018) encodes
user history with a self-attention based sequential
model. 3)BERT4Rec (Sun et al., 2019) employs
bidirectional self-attention to model user history. 4)
RoBERTa-Sim , a simple yet strong baseline men-1162
tioned in Qi et al. (2022), uses the hidden states of
[CLS] tokens to measure user-candidate similarity.
5)UNBERT , implemented as Zhang et al. (2021),
concatenates history and candidate texts as the in-
put to BERT and predicts matching scores from the
final hidden states of [CLS] tokens.
Note that we do not consider other methods that
use non-text inputs (e.g., user profile, text topic
labels). For fair comparison, all baseline models
use pretrained 12-layer RoBERTa-base (Liu et al.,
2019) as text encoders to learn embeddings of texts.
3.1 Main Results
Table 1 shows the performance of experiment mod-
els. From the results of NewsRec andQuoteRec ,
we can see that UniTRec outperforms all baseline
models by a clear margin. Also, RoBERTa-Sim
and UNBERT that directly use the [CLS] hidden
states to represent user history, surpass other base-
lines that build additional aggregation networks
upon the whole RoBERTa outputs. As displayed
in the results, EngageRec is the most difficult task.
We inspect the dataset and find that the texts on so-
cial media contain too much noise (e.g., URL and
emoji), and the user history contains less number
of turns. Nevertheless, UniTRec achieves better
overall performance than other baseline models,
validating its robustness on noisy text inputs and
limited user history.
3.2 Ablation Studies and Analyses
We further conduct ablation studies on UniTRec.
The experiment results are reported in Table 2.Initialization of UniTRec. We train UniTRec
from scratch without initialization from pretrained
BART (refer to w/o BART Init). The recommen-
dation performance significantly drops in all three
tasks, which indicates that acquiring effective text
understanding ability from PLM is a necessary key
to UniTRec performance.
Local and global attention. We investigate the
function of two-level attention modules of the Uni-
TRec history encoder. Concretely, we set L= 0
in w/o Local-Att and L= 0 in w/o Global-Att,
where L+L= 6. We can observe that remov-
ing local and global attention from the original
UniTRec history encoder both lead to suboptimal
performance, while the performance drop is more
significant in w/o Global-Att. The results justify
the effectiveness of jointly modeling two-level his-
tory contexts through adapted Transformer atten-
tion masking without additional parameters.
Discriminative and perplexity-based objectives.
We probe into training UniTRec with standalone
discriminative (Disc-Score only) and perplexity-
based (PPL-Score only) contrastive objectives, re-
spectively. We can see that the discriminative objec-
tive yields better performance than the perplexity-
based objective. Besides, the model performance
on both standalone objectives declines compared to
the original joint objective. The results indicate that
the discriminative and perplexity-based matching
scores are complementary and can jointly provide
more accurate signals of user history and candidate
text matching for text-based recommendation.11634 Conclusion
We present a unified Transformer UniTRec for text-
based recommendation. UniTRec learns two-level
contexts of multi-turn user history and jointly ex-
ploits discriminative matching scores and candidate
text perplexity as matching objectives. Empirical
experiments on three text-based recommendation
datasets corroborate the effectiveness of UniTRec.
5 Limitations
Our model only focuses on utilizing text informa-
tion for recommendation, which is a key limitation
of this work. In real-world settings, recommender
systems are usually required to handle heteroge-
neous information inputs. UniTRec is a pure text-
based recommender modeling user history and can-
didate texts as inputs. However, incorporating addi-
tional side information (e.g., user profile, text topic
labels, and dwell time of user behaviors) could
further improve the recommendation performance
and alleviate the cold start problem. Furthermore,
UniTRec only models two-level relations of user
behavior history. Nonetheless, incorporating more
user behavior information, such as implicit and
negative feedback, could further enhance the rec-
ommendation performance.
Acknowledgements
We appreciate constructive comments from anony-
mous reviewers. The research described in this pa-
per is partially supported by CUHK under Project
No. 3230366.
References11641165
A Dataset Statistics
The detailed statistics of the three text-based recom-
mendation datasets are displayed in Table 3. Note
that we use news titles as the text inputs for News-
Recfollowing Qi et al. (2021). NewsRec regards
the user clicked and non-clicked news as candidate
texts, while QuoteRec andEngageRec regard all po-
tential quotation texts and post texts as candidates.
Different from Zeng et al. (2020) that formulates
the task as recommending candidate users to given
posts based on post content, we formulate the task
as recommending candidate posts to given users
based on user history.
Algorithm 1 Candidate Ranking Processs
B Inference Ranking
Given the user history and Mcandidate texts,
UniTRec first predicts the discriminative rank-
ing scores S={S, S, ..., S}and perplexity-
based ranking scores S={S, S, ..., S}of the
candidates. Algorithm 1 outlines an approach to ag-
gregate the final ranking based on SandS. Note
that the function Rank( S)denotes outputting the
sorted order of elements in a score list S. There
exist other ways to average the ranking of Sand
S, which we leave for future work to explore.C Qualitative Analysis
We show randomly sampled outputs of UniTRec,
for instance, demonstrated on the news recommen-
dation and quote recommendation tasks. Table 4
and 5 showcase the qualitative samples.116611671168ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Section 5
/squareA2. Did you discuss any potential risks of your work?
We see no concern about potential risks.
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Section 1
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
The Abstract provides the link to our code.
/squareB1. Did you cite the creators of artifacts you used?
Not applicable. Left blank.
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Not applicable. Left blank.
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Not applicable. Left blank.
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
In the Abstract, a Github repository with documentation is released.
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Appendix A
C/squareDid you run computational experiments?
Section 3
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
Section 31169/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Section 3
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
Section 3
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.1170