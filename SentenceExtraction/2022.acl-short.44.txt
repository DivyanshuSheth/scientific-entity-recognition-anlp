
Tassilo Klein
SAP AI Research
tassilo.klein@sap.comMoin Nabi
SAP AI Research
m.nabi@sap.com
Abstract
In this paper, we propose Self-Contrastive
Decorrelation (SCD), a self-supervised ap-
proach. Given an input sentence, it optimizes
a joint self-contrastive and decorrelation objec-
tive. Learning a representation is facilitated by
leveraging the contrast arising from the instan-
tiation of standard dropout at different rates.
The proposed method is conceptually simple
yet empirically powerful. It achieves compa-
rable results with state-of-the-art methods on
multiple benchmarks without using contrastive
pairs. This study opens up avenues for efficient
self-supervised learning methods that are more
robust than current contrastive methods.
1 Introduction
Unsupervised learning of representation (a.k.a. em-
bedding) is a fundamental problem in NLP and has
been studied extensively in the literature (Mikolov
et al., 2013; Pennington et al., 2014; McCann et al.,
2017; Peters et al., 2018). Sentence embeddings
are essential for numerous language processing
applications, such as machine translation, senti-
ment analysis, information retrieval, and seman-
tic search. Recently, self-supervised pre-training
schemes have been successfully used in the context
of transformer architectures, leading to a paradigm
shift in natural language processing and understand-
ing (Devlin et al., 2018; Liu et al., 2019; Radford
et al., 2018) The idea here is to employ an auxil-
iary task, which enforces an additional objective
during training. Typically, this entails predictions
based on a subset of information from the context.
Most objectives found effective in practice are quite
simple. Some successful examples of such pretext
tasks are Masked Language Model (MLM), Next
Sentence Prediction (NSP), Sentence Order Pre-
diction (SOP), etc. (Devlin et al., 2019; Liu et al.,2019; Lan et al., 2019). When working with unla-
beled data, contrastive learning is among the most
powerful approaches in self-supervised learning.
The goal of contrastive representation learning is
to learn an embedding space in such a manner that
similar sample pairs (i.e., positive pairs ) stay close
to each other. Simultaneously, dissimilar sample
pairs (i.e., negative pairs ) are far pushed apart. To
this end, different augmented views of the same
sample and the augmented views from different
samples are used as positive and negative pairs.
These methods have shown impressive results over
a wide variety of tasks from visual to textual repre-
sentation learning (Chen et al., 2020a,b; Gao et al.,
2021; Grill et al., 2020; Chen and He, 2021).
Different techniques have been proposed for the
augmentation and selection of positive and negative
pairs. For example, DeCLUTR (Giorgi et al., 2021)
proposes to take different spans from the same doc-
ument as positive pairs, while CT (Carlsson et al.,
2020) aligns embeddings of the same sentence from
two different encoders. CERT (Fang et al., 2020)
applies the back-translation to create augmenta-
tions of original sentences, and IS-BERT (Zhang
et al., 2020) maximizes the agreement between
global and local features. Finally, CLEAR (Wu
et al., 2020) employs multiple sentence-level aug-
mentation strategies to learn a sentence represen-
tation. Despite the simplicity of these methods,
they require careful treatment of negative pairs, re-
lying on large batch sizes (Chen et al., 2020a) or
sophisticated memory strategies. These include
memory banks (Chen et al., 2020b; He et al., 2020)
or customized mining strategies (Klein and Nabi,
2020) to retrieve negative pairs efficiently. In NLP
specifically, the endeavor of “hard negative mining”
becomes particularly challenging in the unsuper-
vised scenario. Increasing training batch size or the
memory bank size implicitly introduces more hard
negative samples, coming along with the heavy
burden of large memory requirements.394In this paper, we introduce SCD, a novel algo-
rithm for self-supervised learning of sentence em-
bedding. SCD achieves comparable performance in
terms of sentence similarity-based tasks compared
with state-of-the-art contrastive methods without ,
e.g., employing explicit contrastive pairs. Rather,
in order to learn sentence representations, the pro-
posed approach leverages the self-contrast imposed
on the augmentations of a single sample. In this
regard, the approach builds upon the idea that suf-
ficiently strong perturbation of the sentence em-
bedding reflects the semantic variations of the sen-
tence. However, it is unclear which perturbation
is simply a slight variation of the sentence without
changing the semantic (positive pair) and which
perturbation sufficiently modifies the semantic to
create a negative sample. Such ambiguity mani-
fests itself in the augmented sample sharing the
characteristics of both negative and positive sam-
ples. To accommodate this, we propose an ob-
jective function consisting of two opposing terms,
which acts on augmentations pairs of a sample: i)
self-contrastive divergence ( repulsion ), and ii)fea-
ture decorrelation ( attraction ). The first term treats
the two augmentations as a negative pair pushing
apart the different views. In contrast to that, the
second term attends to the augmentations as a posi-
tive pair. Thus, it maximizes the correlation of the
same feature across the views, learning invariance
w.r.t. the augmentation. Given the opposing na-
ture of the objectives, integrating them in a joint
loss yields a min-max optimization scheme. The
proposed approach avoids degenerated embeddings
by framing the representation learning objective as
an attraction-repulsion trade-off. Simultaneously,
it learns to improve the semantic expressiveness
of the representation. Due to the difficulty of aug-
mentation in NLP, the proposed approach generates
augmentation “on-the-fly” for each sample in the
batch. To this end, multiple augmentations are pro-
duced by varying dropout rates for each sample.
We empirically observed that SCD is more robust
to the choice of augmentations than pairwise con-
trastive methods; we believe that not relying on
contrastive pairs is one of the main reasons for this,
an observation also made in self-supervised learn-
ing literature such as BYOL (Grill et al., 2020).
While other methods take different augmentation
or different copies of models, we utilized the dif-
ferent outputs of the same sentence from standard
dropout.Most related to our paper is (Gao et al., 2021),
which considers using dropout as data augmenta-
tion in the context of contrastive learning. A key
novelty of our approach is that we use the dropout
for creating the self-contrastive pairs, which can
be utilized as both positive and negative. At last,
we note that our model is different from the pair-
wise feature decorrelation or whitening in (Zbontar
et al., 2021; Su et al., 2021; Ermolov et al., 2021),
which encourage similar representations between
augmented views of a sample while minimizing
the redundancy within the representation vector. A
key difference compared to these methods is that
they ignore the contrastive objective completely.
In contrast, our method takes it into account and
provides the means to treat self-contrastive views
as positive and negative pairs simultaneously.
Our contribution: i) generation of sentence em-
beddings by leverage multi-dropout ii)elimination
of reliance on negative pairs using self-contrast,
iii)proposing feature decorrelation objective for
non-contrastive self-supervised learning in NLP.
2 Method
Our approach relies on the generation of two views
AandBof samples. To this end, augmentations
are generated in embedding space for each sample
xin batch X. Batches are created from samples
of setD={(x)}, where Ndenotes the num-
ber of sample (sentences). Augmentations are pro-
duced by an encoder f, parametrized by θ. The
output of the encoder is the embeddings of sam-
ples in Xdenoted as H∈ T andH∈ T .
HereTdenotes the embedding space. Next, we
let,h∈ T denote the associated representation
of the sentence. The augmentation embeddings
produced per sample are then denoted handh.
To obtain the different embedding, we leverage a
transformer language model as an encoder in com-
bination with varying dropout rates. Specifically,
one augmentation is generated with high dropout
and one with lowdropout. This entails employing
different random masks during the encoding phase.
The random masks are associated with different
ratios, randr, with r< r. Integrating the
distinct dropout rates into the encoder, we yield
h=f(x, r)andh=f(x, r). Given the
embeddings, we leverage a joint loss, consisting of
two objectives:
minL(f) +αL(f, p) (1)395
Here α∈Rdenotes a hyperparameter and p:
T → P is a projector (MLP) parameterized by θ,
which maps the embedding to P, with|P| ≫ |T | .
The objective of Lis to increase the contrast of
the augmented embedding, pushing apart the em-
beddings handh. The objective of Lis to
reduce the redundancy and promote invariance w.r.t.
augmentation in a high-dimensional space P. See
Fig. 1 for a schematic illustration of the method.
2.1 Self-Contrastive Divergence:
Self-contrast seeks to create a contrast between the
embeddings arising from different dropouts. Hence,
Lconsists of the cosine similarity of the samples
in the batch as:
L=1
NXh·(h) 
∥h∥∥h∥(2)
2.2 Feature Decorrelation:
Lseeks to make the embeddings invariant to aug-
mentation while at the same time reducing the re-
dundancy in feature representation. To this end,
the embedding his projected up from Tto a
high-dimensional space P, where decorrelation is
performed. To avoid clutter in notation, we let
p=p(h)and∗ ∈ { A, B}, denote the aug-
mented embedding vectors of sample xafter ap-
plying a projection with p(.). Then, a correlation
matrix is computed from the projected embeddings.
Its entries Care:
C=Xp·p X(p)(p)!
(3)Here, p∈Rdenotes the jcomponent in the
projected embedding vector. Then the loss objec-
tive for feature decorrelation is defined as:
L=−X(1−C)+λXXC (4)
The first term seeks to achieve augmentation in-
variance by maximization of the cross-correlation
along the diagonal. The second term seeks to re-
duce redundancy in feature representation by mini-
mizing correlation beyond the diagonal. Given that
these objectives are opposing, λ∈Ris a hyperpa-
rameter, controlling the trade-off.
3 Experiments & Results
3.1 Training Setup:
Training is started from a pre-trained trans-
former LM. Specifically, we employ the Hugging
Face (Wolf et al., 2020) implementation of BERT
and RoBERTa. For sentence representation, we
take the embedding of the [CLS] token. Then
similar to (Gao et al., 2021), we train the model
in an unsupervised fashion on 10randomly sam-
ples sentences from Wikipedia. The LM is trained
with a learning rate of 3.0e−5for1epoch at batch-
size of 192. The projector MLP qhas three linear
layers, each with 4096 output units in conjunction
with ReLU and BatchNorm in between. For BERT
hyperparameters are α= 0.005,λ= 0.013, and
dropout rates are r= 5.0%andr= 15 .0%.
For RoBERTa hyperparameters are α= 0.0033 ,
λ= 0.028, and dropout rates are r= 6.5%and396
r= 24.0%. The values were obtained by grid-
search. First a coarse-grid was put in place with
a step-size of 0.1forα,10% for the dropout rates
r, r. Forλthe coarse-grid consisted of different
magnitudes {0.1,0.01,0.001}. Second, on a fine-
grid with step-size of 0.01and1%, respectively.
3.2 Evaluation Setup:
Experiments are conducted on 7 standard seman-
tic textual similarity (STS) tasks. In addition to
that, we also evaluate on 7 transfer tasks. Specif-
ically, we employ the SentEval toolkit (Conneau
and Kiela, 2018) for evaluation. As proposed by
(Reimers and Gurevych, 2019; Gao et al., 2021),
we take STS results as the main comparison of
sentence embedding methods and transfer task re-
sults for reference. For the sake of comparability,
we follow the evaluation protocol of (Gao et al.,
2021), employing Spearman’s rank correlation and
aggregation on all the topic subsets.
3.3 Main Results
3.3.1 Semantic Textual Similarity:
We evaluate on 7 STS tasks: (Agirre et al., 2012,
2013, 2014, 2015, 2016), STS Benchmark (Cer
et al., 2017) and SICK-Relatedness (Marelli et al.,
2014). These datasets come in sentence pairs to-
gether with correlation labels in the range of 0 and
5, indicating the semantic relatedness of the pairs.
Results for the sentence similarity experiment can
be seen in Tab. 1. The proposed approach is on-
par with state-of-the-art approaches. Using BERT-
LM, we outperform the next-best approach on STS-
B(+1.19 )and on SICK-R (+3.81 )points. UsingRoBERTa-LM, we outperform the next best compa-
rable approach (SimCSE-RoBERTA) on STS-
15(+0.55% )and SICK-R (+3.8% ).
3.3.2 Transfer task:
We evaluate our models on the following trans-
fer tasks: MR (Pang and Lee, 2005), CR (Hu
and Liu, 2004), SUBJ (Pang and Lee, 2004),
MPQA (Wiebe et al., 2005), SST-2 (Socher et al.,
2013), TREC (V oorhees and Tice, 2000) and
MRPC (Dolan and Brockett, 2005). To this end,
a logistic regression classifier is trained on top of
(frozen) sentence embeddings produced by differ-
ent methods. We follow default configurations
from SentEval. Results for the transfer task ex-
periment can be seen in Tab. 2. SCD is on-par397
with state-of-the-art approaches. Using BERT-LM,
we outperform the next best approach on SUBJ
(+4.6% )and MRPC (+2.2% ). Using RoBERTa-
LM, we outperform the next best comparable ap-
proach (SimCSE-RoBERTA) on almost all
benchmarks, with an average margin of (+2.61% ).
3.4 Analysis
3.4.1 Ablation Study:
We evaluated each component’s performance by
removing them individually from the loss to assess
both loss terms’ contributions. It should be noted
thatLof Eq. 2 and Lof Eq. 4 both interact
in a competitive fashion. Hence, only the equi-
librium of these terms yields an optimal solution.
Changes - such as eliminating a term - have detri-
mental effects, as they prevent achieving such an
equilibrium, resulting in a significant drop in per-
formance. See Tab. 3 for the ablation study on mul-
tiple benchmarks. Best performance is achieved in
the presence of all loss terms.
3.4.2 Uniformity and Alignment Analysis:
To better understand the strong performance of
SCD, we borrow the analysis tool from (Wangand Isola, 2020), which takes alignment between
semantically-related positive pairs and uniformity
of the whole representation space to measure the
quality of learned embeddings. Figure 2 shows
uniformity andalignment of different methods and
their results on the STS. SCD achieves the best
in terms of uniformity , reaching to the supervised
counterparts ( -3.83 ), which can be related to the
strong effect of the self-contrastive divergence ob-
jective. It shows the self-contrastive pairs can ef-
fectively compensate for the absence of contrastive
pairs. In terms of alignment , SCD is inferior to
other counterparts ( 0.84), which can be attributed
to the fact that our repulsion objective mainly fo-
cuses on the feature decorrelation aiming to learn
a more effective and efficient representation. This
is reflected in the final results on the STS where
SCD obtains significantly higher correlation even
compared to the method with lower alignment such
as BERT-whitening or BERT-flow.
4 Conclusion & Future Work
We proposed a self-supervised representation learn-
ing approach, which leverages the self-contrast of
augmented samples obtained by dropout. Despite
its simplicity, it achieves comparable results with
state-of-the-arts on multiple benchmarks. Future
work will deal with sample-specific augmentation
to improve the embeddings and, particularly, the
representation alignment.
Acknowledgement: We would like to thank
Mahdyar Ravanbakhsh for valuable feedback on
the manuscript.398References399400