
Chun Hei Lo Wai Lam Hong Cheng
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
{chlo, wlam, hcheng}@se.cuhk.edu.hk
Abstract
We introduce a data-driven approach to gener-
ating derivation trees from meaning represen-
tation graphs with probabilistic synchronous
hyperedge replacement grammar (PSHRG).
SHRG has been used to produce meaning rep-
resentation graphs from texts and syntax trees,
but little is known about its viability on the
reverse. In particular, we experiment on Depen-
dency Minimal Recursion Semantics (DMRS)
and adapt PSHRG as a formalism that approx-
imates the semantic composition of DMRS
graphs and simultaneously recovers the deriva-
tions that license the DMRS graphs. Consistent
results are obtained as evaluated on a collec-
tion of annotated corpora. This work reveals
the ability of PSHRG in formalizing a syntax–
semantics interface, modelling compositional
graph-to-tree translations, and channelling ex-
plainability to surface realization.
1 Introduction
General graph-based meaning representations
(MRs) that model sentence-level semantics aim
to provide interpretable intermediate representa-
tions that are application- and domain-independent
(Koller et al., 2019). Recently, graph gram-
mars and algebras that formalize semantic con-
structions were introduced to MR processing (for
example: (Koller, 2015; Drewes and Jonsson,
2017; Groschwitz et al., 2017; Chen et al., 2018;
Groschwitz et al., 2018; Lindemann et al., 2019;
Donatelli et al., 2019; Chen and Sun, 2020)). These
formal grammars bridge between linguistic assump-
tions and data-driven parsing, and offer the bene-
fit of cross-framework adaptability. For instance,
the Apply–Modify algebra was adopted in pars-
ing across 5 MR frameworks (Oepen et al., 2019)
(Lindemann et al., 2019; Donatelli et al., 2019).
Another formalism that was adopted in generating
semantic graphs from syntax trees is synchronous
hyperedge replacement grammar (SHRG) (Peng
et al., 2015; Chen et al., 2018; Chen and Sun, 2020).The use of SHRG in recovering syntax trees from
MRs has however received scant research coverage.
Empirical results of PSHRG’s application are lim-
ited to Jones et al. (2012)’s work in semantic-based
machine translation.
An immediate application of MR-to-tree pars-
ing is surface realization. Previous data-driven ap-
proaches to it include rule-based (Flanigan et al.,
2016; Song et al., 2017; Horvat, 2017; Ye et al.,
2018) and neural methods (Song et al., 2018; Da-
monte and Cohen, 2019; Hajdik et al., 2019). All
these methods do not generate syntactic analyses.
In contrast, the Answer Constraint Engine (ACE;
Carroll et al., 1999; Carroll and Oepen, 2005; Vell-
dal and Oepen, 2006), an HPSG grammar-based
parser, generates both derivations and sentences
from Minimal Recursion Semantics (MRS; Copes-
take et al., 2005). If we can induce an SHRG from
data, MRs can be translated into derivation trees
without relying on a hand-engineered grammar, and
natural language texts can be obtained by realizing
the terminals. Combining the strengths of rule-
based systems and the data-driven paradigm, such
an approach gives both linguistically-informed real-
ization processes and explainable results, removes
syntactic ambiguities that would otherwise exist
in flattened surface strings, and provides potential
usage for downstream tasks such as chunking.
Among the different MR frameworks, we inves-
tigate Dependency Minimal Recursion Semantics
(DMRS; Copestake, 2009). DMRS are directed
graphs derived losslessly from MRS, whereas an
MRS structure with respect to a reading of an En-
glish sentence is composed along with a derivation
tree using the English Resource Grammar (ERG;
Flickinger, 2000, 2011), a broad-coverage hand-
engineered HPSG grammar of English. DMRS en-
codes logical formulae with underspecified scopes
(for an introduction, see: Copestake, 2009). Fig. 1
shows an example of an ERG analysis.
Copestake et al. (2001) gave a compositional se-5425
mantic algebra on the constructions of MRS, where
the introduced constraints allow the semantics of
complex sentences to be derived from simple com-
position rules. The compositionality exhibited in
the MRS (for a discussion, see: Bender et al., 2015)
is not obvious in some other MRs, e.g., Abstract
Meaning Representation (AMR; Banarescu et al.,
2013) (Bender et al., 2015), and we suggest that
HRG be particularly suitable to simulate the DMRS
algebra if equipped with adequate adaptations.
In this paper, we shed light on the applicability
of a succinct grammar formalism in approximating
the semantic compositions of a graph-based MR.
Essentially, we capture the syntax–semantics inter-
face of the ERG by inducing a PSHRG from an
annotated treebank to provide generative models
for both DMRS graphs and derivation trees. With
the induced PSHRG, derivation trees that license
the DMRS graphs can be reconstructed through
graph parsing. We describe the procedures and
the relevant adaptations involved, from PSHRG
induction, parsing to generating derivation trees.
Finally, we present the empirical results on deriva-
tion trees and surface strings reconstruction from
DMRS graphs under different configurations.
2 Probabilistic Synchronous Hyperedge
Replacement Grammar
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al., 1997). A synchronous HRG
(SHRG) defines mappings between languages of
graphs, which in our context are an HRG and a
context-free grammar (CFG) for strings. We give
the definitions in this section.
Hypergraph and Hypergraph Fragments. A
hypergraph is specified by a tuple 𝐻=⟨𝑉,𝐸,𝑙⟩,
where𝑉is a finite set of nodes, 𝐸⊆𝑉is a finite
set of hyperedges, each of which connects one or
more distinct nodes, 𝑙:𝐸→𝐿assigns a label
from the finite set 𝐿to each hyperedge. A hyper-
graph fragment is a tuple 𝑅=⟨𝑉,𝐸,𝑙,𝑋⟩, where
⟨𝑉,𝐸,𝑙⟩is a hypergraph and 𝑋∈𝑉is an ordered
list of distinct nodes called the external nodes.
HRG. An HRG is a tuple 𝐺=⟨𝑁,𝑇,𝑃,𝑆⟩,
where𝑁and𝑇are disjoint finite sets of nonter-
minal and terminal symbols respectively, 𝑃is a5426finite set of productions of the form 𝐴→𝑅, where
𝐴∈𝑁and𝑅is a hypergraph fragment where hy-
peredge labels are over 𝑁∪𝑇, and𝑆∈𝑁is the start
symbol. In a step of rewriting a hyperedge 𝑒by a
production𝐴→𝑅,𝑒is replaced with a copy of
𝑅by identifying each of the nodes connected by 𝑒
with a distinct external node of 𝑅, whose mapping
is specified in the production. Fig. 2 illustrates the
HRG rewriting process.
Synchronous HRG. An SHRG is a tuple 𝐺=
⟨𝑁,𝑇,𝑇,𝑃,𝑆⟩, where𝑁is a finite set of nonter-
minal symbols in both the CFG and the HRG, 𝑇
and𝑇are finite sets of terminal symbols in HRG
and CFG respectively, 𝑆∈𝑁is the start symbol,
and𝑃is a finite set of productions of the form
𝐴→ ⟨𝑅,𝑅,∼⟩, where𝑅is a hypergraph frag-
ment with hyperedge labels over 𝑁∪𝑇,𝑅is a
symbol sequence over 𝑁∪𝑇, and∼is a bijection
between the nonterminal hyperedges of the same
labels in𝑅and𝑅. When applying a production
𝐴→ ⟨𝑅,𝑅,∼⟩,𝐴→𝑅rewrites the graph as
described in the HRG and the synchronous opera-
tion on the CFG counterpart is a string rewrite by
𝐴→𝑅(see Fig. 3).
Probabilistic SHRG. A probabilistic SHRG is
obtained by assigning a constant probability to each
production in the SHRG, where probabilities of
the productions that rewrite the same nonterminal
add up to one. In this work, the probability of
𝐴→⟨𝑅,𝑅,∼⟩is simply modelled as the fraction
of times it appears among all 𝐴→∗ in the training
data. The probability of a derivation is the prod-
uct of the probabilities of the context-free SHRG
productions applied.
3 PSHRG Induction and Parsing
In this section, we describe how a PSHRG is in-
duced from training data and how a derivation tree
is reconstructed from a DMRS of the test data with
the induced grammar. We also describe two meth-
ods for modelling the semantics of lexical items.
DMRS as Hypergraph. We first establish the
connections between DMRS and HRG. A DMRS
graph is modelled as a hypergraph 𝐻with terminal
hyperedges. In 𝐻, each terminal hyperedge corre-
sponds to a DMRS node or edge: the former can
connect to an arbitrary number of nodes in 𝐻, and
the latter connects to only two nodes in 𝐻.
3.1 PSHRG Induction
We describe how to induce a PSHRG from pairs of
aligned trees and graphs. The PSHRG induction
procedure generally follows Chen et al. (2018)’s
SHRG extraction algorithm, which operates based
on the surface string alignment information be-
tween DMRS graphs and their derivation trees (for
expositions, see: Chen et al., 2018). Fig. 3 illus-
trates the grammar induction process. For each
nonterminal in the tree, i.e., a node labelled by an
ERG syntactic construction (a label of the form *_c,
e.g., hd-cmp_u_c ), a production 𝐴→⟨𝑅,𝑅,∼⟩
is extracted, where 𝐴,𝑅and𝑅are the ERG syn-
tactic construction, connected DMRS hypergraph
fragment and daughter ERG rule(s) respectively.
For a binary ERG construction, if any of its daugh-
ters is a semantically empty terminal (e.g., for the
lower hd-cmp_u_c in Fig. 3), no productions are
extracted (discussed in §3.3.3). The DMRS hyper-
graph fragment specified by 𝑅is then rewritten to a5427nonterminal hyperedge labelled by 𝐴. We perform
the same rule extraction procedure for each training
instance to induce an SHRG from the training data,
thus a PSHRG when we consider the frequency
information.
3.2 PSHRG Parsing
To recover the derivation tree of a DMRS, we
can translate the semantic compositions to the
corresponding syntactic operations by parsing the
DMRS with the induced PSHRG (see Fig. 3). We
aim at recognizing the best derivation according to
a PSHRG model, which requires exact graph pars-
ing. Chiang et al. (2013); Groschwitz et al. (2015);
Ye and Sun (2020) studied HRG parsing and pro-
posed various techniques on improving the effi-
ciency, but no evaluation on accuracy is performed
on the parsed results with respect to a gold-standard
grammar. Although efficient algorithms are de-
veloped for HRG parsing, existing parsers do not
provide convenient adaptations to the extensions
introduced in this work. Rather than efficiency,
our work focuses on the correctness of derivations
as measured against the original derivations that
license each DMRS. Therefore, we implement a
parser that returns the best PSHRG derivation of
DMRS via bottom–up passive chart parsing (for
details of the parsing algorithms, see Appendix A).
3.3 PSHRG Adaptations
We introduce two adaptations to align PSHRG with
the semantics introduced by the ERG lexical items.
3.3.1 Semantics of Lexical Items
Complex semantics. While most lexical items
introduce just one DMRS predicate each, some
introduce more complex semantics. As an exam-
ple, Fig. 4 shows that somebody provides both the
_some_q quantifier and the person predicate. There-
fore, the R.H.S. hypergraph fragment of an SHRG
production is not confined to a hypergraph frag-
ment with one or two terminal hyperedges, but
one with more than two terminal hyperedges that
corresponds to a connected DMRS subgraph.
Empty semantics. There are semantically empty
lexical items that do not contribute predicates to
the DMRS, e.g., auxiliary verbs and particles. This
poses another challenge for derivation reconstruc-
tion because the syntactic properties of these lexical
items are highly language-dependent, yet they are
not captured by general semantic representations
as they are not semantically functional.
3.3.2 Canonization of Small Subgraphs
To recognize complex semantics, we borrow the
idea of the graph canonization method described by
Horvat (2017) that isomorphic DMRS subgraphs
can be identified by comparing if their canonical
representations are the same. Graph canonization
is achieved in two steps: first, each node in the
DMRS subgraph is given a canonical node repre-
sentation by encoding its 1- and 2-hop neighbours;
then the final canonical form is obtained by con-
catenating the sorted node representations based on
a canonical ordering. Most subgraphs introduced
contain fewer than seven DMRS nodes, for which
the canonization method is sound (Horvat, 2017).
Fig. 4 exemplifies the idea.
During grammar induction, the canonical forms
of all small subgraphs that correspond to ERG lexi-
cal items are extracted from the training data. Then,
given a DMRS from the test data, we first identify
its subgraphs that are isomorphic to any of the
extracted ones before parsing. This is achieved
by first enumerating all small subgraphs from the
DMRS, then computing the canonical form for
each of them, and finally comparing the canonical
forms with those of the subgraphs extracted. The
process can be sped up by computing the canonical
form of a subgraph only if its collection of DMRS
predicates is present in the set of those extracted
from the training data.
3.3.3 Semantically Empty Lexical Items
We devise a semi-automatic method to extract (dur-
ing grammar induction) and recover (during pars-
ing) the syntax of common semantically empty
words. To this set of words with empty seman-5428tics, we define a collection of linguistic signals that
can serve as their cues, and match each signal to
the set of lexical items it can recover. For exam-
ple, the tense and aspects of verbs and predicative
adjectives are signals for auxiliary verbs.
During SHRG induction, the signals are first
identified from the DMRS node attributes and are
then generally passed up from the syntactic head
daughter. When extracting a binary construction, if
the unextracted subtree (described in §3.1) contains
a semantically empty lexical item that matches a
signal of any of the daughters, a hypergraph frag-
ment is extracted together with the subtree asso-
ciated with that signal. During parsing, the same
bottom–up signal passing procedures apply. If the
R.H.S. hypergraph fragment of a binary production
is recognized and any signal passed up matches
that of an extracted subtree, the HRG production is
applied and the CFG subtree is added on top of the
two daughters in the derivation tree (see Fig. 3).
4 Towards Practical Grammar
Approximation and Modelling
To approximate complex grammars or implicit re-
lations established between trees and graphs by
PSHRG, we introduce extensions for improving
on both the precision and generalizability of mod-
elling. The application of most of the proposed
techniques is not limited to the DMRS, but to gen-
eral MR parsing by PSHRG.
4.1 Annotations for Refining Compositions
Three techniques are introduced to impose restric-
tions to semantic compositions, allow probability
to be estimated on more fine-grained SHRG pro-
ductions, and prevent overgeneration.
Typed HRG. Chen and Sun (2020) introduced
typed HRG, where each node of a hypergraph (frag-
ment) and hyperedge is assigned a label chosen
from a finite set. In typed HRG, an R.H.S. hyper-
graph fragment is recognized only if the type of its
every node matches that of the corresponding node
in the input graph. In our case, we propose to type
a node by the major sense tag of the correspond-
ing DMRS predicate. For example, in Fig. 5, the
nodes on the R.H.S. correspond to _want_v_1 and
_go_v_1 respectively, so both are typed v.
Annotation and Normalization of Derivation.
An HPSG derivation tree merely records the recipe
of a derivation, where the non-atomic rule symbols
only convey highly generalized linguistic princi-
ples. Each rule represents a unification of typed
feature structures. Inspired by Zhang and Krieger
(2011), we annotate each syntactic construction
with that of their immediate parent (order-2 verti-
cal markovization) and the syntactic category of
the phrase (see Fig. 5). This adds extra contextual
information to the constituents. We further normal-
ize the derivations by substituting chains of unary
lexical rules, affixation rules for punctuation marks,
and the preterminals by the canonical form of a
DMRS node or subgraph (see Fig. 4).
Framework-Specific Constraints. PSHRG Pars-
ing on DMRS without regard for the MRS semantic
algebra leads to inefficiency and overgeneration. In
particular, the features and in MRS
specify the semantic materials of a phrase that are
accessible during composition. In the ERG, their
values are determined by the type of composition.
Hence, when parsing a DMRS, every composition
should ensure that subsequent compositions can
only happen to the two variables of the newly com-
posed item. This procedure resembles Carroll and
Oepen (2005)’s proposal on index accessibility fil-
tering. The checks can be easily incorporated into
SHRG parsing (for the details, see Appendix B).
Nevertheless, and are not the only fea-
tures that permit compositions in the ERG. There-
fore, the constraints introduced prevent overgener-
ation to a large extent but lead to undergeneration.
In this work, we examine the two most prominent
features, and how we should further integrate the
MRS algebra to HRG is an open question.
4.2 Underspecification for Generalization
Two underspecification methods are developed to
alleviate the rule sparsity and out-of-vocabulary
(OOV) problems, which are the main challenges
faced by general rule-based systems.5429Extended HRG Productions. When recogniz-
ing an HRG production 𝐴→𝑅on a hypergraph
𝐻, we suggest that the external nodes of 𝑅be not
distinguished from the non-external nodes. Con-
sequently, the rewriting hyperedge 𝐴connects to
a variable number of nodes, and such number de-
pends on𝐻. To motivate such decision, consider
H1of Fig. 3. The fact that the hyperedge _boy_n_1
connects to an external node is not significant to
the characterization of the sp-hd_n_c (the specifier–
head construction where the specifier is the seman-
tic head). This effectively creates more SHRG
productions that share the same probabilities if
their R.H.S. hypergraphs (minus external nodes)
are identical.
Delexicalization. The PSHRG models are es-
timated based on delexicalized productions: the
lexeme stems of verbs, adjectives, adverbs, and
nouns (whose DMRS predicates are in the form of
‘_*_v_* ’, ‘_*_a_* ’ and ‘ _*_n_* ’ respectively) are
underspecified (see Fig. 5). This is a significant dis-
tinction between our grammar and the ERG since
the ERG is highly lexicalized.Hence, delexical-
ization trades lexical preciseness for OOV coverage.
Furthermore, since an approximation grammar is
assumed to have no access to the lexical informa-
tion of the underlying grammar, the results of our
experiments would reflect the viability of PSHRG
as a general approach to grammar approximation.
5 Experiments
The main objective of the experiments is to assess
the performance of PSHRG models on simulating
DMRS compositions and producing approximat-
ing derivation trees. Specifically, we reconstruct
a derivation tree for each DMRS whose nontermi-
nals are aligned to DMRS subgraphs and labelled
by an ERG syntactic construction; the ERG 1214
contains more than 210 fine-grained syntactic con-
structions that reflect the distinguishing properties
of different syntactic constructions.
As a secondary evaluation, we analyze our per-
formance on the task of surface realization. The
purpose of this is twofold: first, assessing the qual-
ity of the surface strings produced from the recon-structed derivation trees gives additional perspec-
tives on the evaluation of our models; and secondly,
there are existing works on surface realization from
DMRS, so our models can be benchmarked against.
Finally, we evaluate the significance of the
two proposed adaptations, namely recovering
words with empty semantics and incorporating
framework-specific constraints to PSHRG parsing.
An instance of sample input and sample output are
provided in Appendix C.
5.1 Data
The main data set we experiment on is the Ninth
Growth of the Redwoods Treebank (Oepen et al.,
2002).It contains English sentences from a range
of domains including Wall Street Journal (WSJ)
and the Brown corpus, each paired with the analy-
ses of the 1214 version of the ERG. Each MRS is
converted into a DMRS using Pydelphin (Copes-
take et al., 2016).We discard the instances with
ambiguous analysis, disconnected DMRS, and un-
parsable MRS by Pydelphin.
To assess the scalability of our models, we fur-
ther sampled sentences from the Gigaword v.5 cor-
pus (Parker et al., 2011) for model training, where
extra training instances are obtained by parsing sen-
tences with the ACE and choosing the best ERG
analysis for each sentence as ranked by the ACE.
After preprocessing, the total number of in-
stances in the training and test sets are 70,774 and
10,042 respectively under the standard Redwoods
data split. 70,774 extra training instances are cre-
ated from the Gigaword corpus.
5.2 Experimental Configurations
We removed the mostly uninformative syntactically
covert quantifiers (e.g., udef_q ,proper_q ) in all
DMRS graphs. The numbers of DMRS nodes re-
ported below are counted after the removal. Sub-
graph canonization (§3.3.2) was performed only on
the DMRS subgraphs of fewer than seven nodes.
The maximum length of the unary chains in the
generated derivation trees was set to be three. The
parser was implemented in PyPy3.6 and ran under
one Intel Xeon E5-2697 CPU on x86_64 Linux.
Our implementation is available online.5430
5.3 Results
5.3.1 On Derivation Tree Reconstruction
If a DMRS is parsed correctly, the synchronously
reconstructed derivation should not deviate too
much from the original derivation. Nevertheless,
equivalence is a sufficient, but not necessary con-
dition for a parse to be correct since syntactic dif-
ferences do not necessarily contribute to semantic
differences.
We devise a modified version of the ParsEval
(Black et al., 1991) measure, ParsEval-Graph, to
assess the quality of the generated trees, as the con-
stituents of the trees are not aligned to a surface
string but on the input semantics. In ParsEval-
Graph, the alignment of a constituent refers to the
DMRS nodes covered instead of the characters
covered in the surface string. Following ParsEval,
ParsEval-Graph only accounts for binary rules, and
preterminals are disregarded. All ParsEval-Graph
scores are evaluated on the parsable instances of
the respective models on unannotated derivation
trees.
As introduced in §4.1, we experiment with the
typed variant of PSHRG, PSTHRG, and two con-
figurations of tree annotation, namely order-2 ver-
tical markovization with syntactic category anno-
tation (M2C), and only syntactic category anno-
tation (M1C). Since no existing works on data-
driven DMRS parsing or surface realization pro-
duce syntactic derivations, we develop a baseline
model SHRG–PCFG for benchmarking. SHRG–
PCFG parses a DMRS with the SHRG induced and
the probability of each parse is given by a PCFG
model where the probability of each CFG produc-
tion𝐴→𝑅is modelled as the fraction of times
𝐴→𝑅appears among all 𝐴→∗ in the training
data. The ACE is not evaluated because it always
generates derivations faithful to the ERG.
As reported in Table 1, all PSHRG models attain
Fscores of over 80 consistently across settings
and outperform the baseline under the same annota-
tion configurations. More extensive annotation and
typing the grammar respectively improve the F
score by about 2, at the expense of coverage reduc-
tion caused by rule sparsity when the Redwoods
training set does not provide adequate data for the
over-specific annotation. It is insightful to note
that the baseline under M2C performs at a level be-
tween the PSHRG model and PSTHRG model on
M1C, which conveys that the contextual informa-
tion of a nonterminal node could already provide
ample information on the semantic compositions.
To study models’ performance with respect to
the size of the training data, we add the Gigaword
instances on top of the Redwoods training set. This
doubles the amount of training data. As reported in
Table 2, the coverages of the two models increase
by 4.50% and 6.31% respectively with more data.
Nevertheless, the accuracy of derivations does not
improve further, as frequency-based context-free
probability models have low learning capacities.
Despite all DMRS being generated by the ERG,
the ACE does not parse every DMRS–parsing fails
when a DMRS predicate is OOV . In contrast, our
models parse more instances than the ACE when
given more training data, since they generalize to
OOV with delexicalization. Although delexicaliza-
tion removes much lexical information, we suggest
that SHRG-based parsing and the incorporation of
MRS-specific constraints can restrict compositions
outside of the ERG to a large extent.
5.3.2 On Surface Realization
To produce a surface string from a DMRS, we real-
ize the most frequently recorded surface form for
each preterminal from the reconstructed derivation
tree. We compare our work with the Neural MRS
(Hajdik et al., 2019) and the ACE. The Neural MRS
generates in an end-to-end manner without interme-
diate syntactic derivations. For more comparable
results, we evaluate the models under M1C anno-
tation configuration since they have similar parse5431
coverages to the ACE.
We evaluate the generation quality with BLEU
(Papineni et al., 2002) using SacreBLEU (Post,
2018). Following (Hajdik et al., 2019)’s eval-
uation on in- and out-of-domain performances,
we experiment on the different training–test data
splits, namely the WSJ–WSJ and WSJ–Brown
splits. WSJ contains 34,751 training instances and
1,442 test instances, and Brown contains 2,181 test
instances.All BLEU scores are evaluated on the
parsable instances of the respective models.
Table 3 shows that our models perform consis-
tently across data splits. Under the Redwoods stan-
dard data split, our models are worse than the neu-
ral model. With similar parse coverages to the ACE,
our performance is also close to the ACE. Under
the WSJ-Brown split, our PSHRG models outper-
form the Neural MRS.The PSHRG (M1C) parses
74.90% of the test set under the WSJ–WSJ split.
When the model is typed and when switching from
in- to out-of-domain, the models parse about 7%
less data respectively. The coverages of M1C mod-
els reported here are lower than those in Table 1
since the amount of training data is halved. There-
fore, we consider the relative decreases in coverage
from in- to out-of-domain of our respective mod-
els to be more insightful on models’ transferability
between domains than the absolute coverage.
Apart from the automatic evaluation, we also
value qualitative details and seek linguistically in-
teresting phenomena that result from a grammar-
based approach. To this end, we observe that our
models identify different realization possibilities
from the original sentence to the same semantics
that conform to the ERG. Some syntactic variations
are reported and explained in Table 4.
Compared to neural approaches, PSHRG is a
shallow statistical model with a high inductive bias.
It encodes a syntax–semantics interface effectively
through tree- and graph- rewriting. Even though
our models are not engineered towards the task of
surface realization, and with limited morphological
analyses and no language modelling, our approach
is still competitive as evaluated quantitatively and
qualitatively. We suggest that PSHRG-based ap-
proaches and neural models be decent alternatives
to each other for general surface realization from
MR: neural models provide full-coverage and high-
quality generation when substantial training data
is available, whereas PSHRG-based solutions ex-
trapolate from limited data and in out-of-domain
scenarios, and produce interpretable derivations.
5.3.3 Ablation Studies
We conduct a few more experiments to test against
the significance of two proposed adaptations,
namely the solution to semantically empty words
(§3.3.3) and framework-specific constraints (§4.1).
We implement two models, PSTHRG- ∅(M1C) and
PSTHRG-λ(M1C), which parse DMRS without
regard for semantically empty lexical items and
MRS constraints respectively.
As reported in Table 5, The treatment of empty
semantics not only adds 15.51% more parsable in-
stances but also corrects some parses in the 58.80%
that require analyses of empty semantics, thus pro-5432
ducing more accurate surface strings.
Fig. 6 shows the importance of MRS constraints
on parsing efficiency. We set a time limit of 300
seconds on parsing, and the parsing of 16.71% of
the WSJ test set exceeds the time limit. When the
DMRS contains 24 or more nodes, timeouts occur
on at least one DMRS graph of each size. Table 6
shows that when the constraints are enforced, all
parses are completed in much shorter times and the
derivations reconstructed are more accurate.
6 Discussion
The proposed frequency-based PSHRG models are
simple yet competitive data-driven baselines for
recovering derivations of DMRS. They can be fur-
ther combined with sophisticated machine learning
methods for a more accurate parse ranking. More
extensive features can also be included to enhance
grammar approximation. For instance, the feature-
paths of ERG signs are shown to be helpful for
PCFG approximation (Zhang and Krieger, 2011).
Language-specific knowledge about words with
empty semantics is also critical for syntactic pur-
poses. In terms of efficiency, Ye and Sun (2020)showed that exact parsing can be very practical on
Elementary Dependency Structures (EDS; Oepen
and Lønning, 2006), a close equivalent to DMRS
that excludes scopal information. Different from
Ye and Sun (2020)’s implementation, we retain
more than 210 ERG syntactic constructions for pre-
cision and adopt delexicalization for generalization,
both of which increase the search space of parsing
and trade efficiency. We suggest that the described
PSHRG-based approach can be a potential alter-
native to the unification-based ACE generator for
surface realization from DMRS, whilst improving
parsing coverage, accuracy and efficiency without
sacrificing one another would be a critical problem
for future research. In this paper, we report our
findings with respect to the engineering decisions
we investigated based on the data at hand.
In principle, the application of the described
PSHRG-based approach is not limited to DMRS,
but also to generic graph-to-tree translations that
exhibit compositionality, if suitable data of aligned
trees and graphs is available. If explicit associa-
tions do not exist between the trees and graphs, the
induced grammar formalizes the underlying rela-
tions and patterns; otherwise, the induced grammar
provides an approximation to such relations, which
can be desirable for computation purposes.
7 Conclusion
Based on the experimental results, we can assess
the contributions of this work from three perspec-
tives: (1) PSHRG with framework-specific adap-
tions as a formalism that approximates the semantic
composition process of DMRS, (2) PSHRG graph
parsing with framework-independent extensions
as a general approach to modelling compositional
graph-to-tree translation, and (3) derivation recon-
struction with a PSHRG induced from data as a
solution to surface realization from MRs that pro-
vides explainability, syntactic disambiguation, and
syntactic variations. We hope that this work pro-
vides relevant and substantial empirical insights to
stimulate more research on approaching MR pro-
cessing with linguistically-motivated methods.
Acknowledgements
The work was supported by grants from the Re-
search Grant Council of the Hong Kong Special
Administrative Region, China [Project No.: CUHK
14205618], and CUHK Direct Grant No. 4055159.5433References54345435
A A Passive Chart Parser
Although it is not the intent of this work to sug-
gest a new HRG parsing algorithm, we provide
more information on the parser implemented so
that it provides more context for comprehending
the absolute parsing time in Fig. 6.
A.1 Rewriting DMRS Hypergraph
As described in §3, a hypergraph can be instan-
tiated corresponding to a DMRS. During SHRG
parsing, the hypergraph undergoes graph-rewriting,
where its subgraphs are rewritten by nonterminal
hyperedges, and produces other hypergraphs.
For every hypergraph 𝐻, we categorize every
hyperedge into one of the two types, s-hyperedge
or c-hyperedge. An s-hyperedge is a terminal hy-
peredge that is identified and labelled by a DMRS
edge. It connects to exactly two nodes of 𝐻. A
c-hyperedge corresponds to a node or a connected
DMRS subgraph. A terminal c-hyperedge is la-
belled by a DMRS predicate. A nonterminal c-
hyperedge is labelled by the canonical form of a
DMRS subgraph (see §3.3.2) or an ERG syntactic
construction.
A.2 Representation of Chart Items
In our chart parser, a passive item is essentially a
c-hyperedge packed with additional information. It
is represented by 𝑀=⟨𝑌,𝐴,Σ⟩, where𝑌is the set
of DMRS nodes covered by the item, 𝐴is the c-
hyperedge label and Σis the set of s-hyperedges in-
cident to the item. In a passive item 𝑀=⟨𝑌,𝐴,Σ⟩,
each of the s-hyperedges 𝜎∈Σis represented by
𝜎=⟨𝑠,𝑡,𝑘,𝑑,𝑥⟩, where𝑠,𝑡,𝑘represent the source
and target DMRS node, and the key of the edge.
These three variables together specify a unique
edge in a DMRS multigraph; 𝑑is a boolean vari-
able indicating if the DMRS edge ⟨𝑠,𝑡,𝑘⟩is an
outgoing edge from the c-hyperedge and 𝑥∈Z
specifies the order of the node of the c-hyperedge
this s-hyperedge connects to. Fig. 7 illustrates three
example passive items.
Algorithm 1 shows the high-level procedures of
a minimal PSHRG parsing algorithm for DMRS. It
only includes the application of unary and binary
rules without constructional contentand semanti-
cally empty words. The synchronous CFG part is
also omitted for simplicity. It parses a DMRS 𝐷
given a PSHRG 𝐺with production probabilities 𝜋
and returns the best parse. 𝑍is the set of the small
subgraphs extracted from training data.
A.3 Bottom–Up Parsing
Line 3 of Algorithm 1. We first initialize the ad-
jacency information of each DMRS node. 𝛼maps
a set of DMRS nodes to the set of their adjacent
nodes. A DMRS node set of size 𝑛is represented
by an𝑛-hot bit vector. 𝑁returns the neighbour-5436Algorithm 1: A minimal PSHRG parsing
algorithm for DMRS.
hood of a set of nodes in 𝐷.
Line 4 to 16 of Algorithm 1. In this section,
we instantiate passive items for the terminal c-
hyperedges which correspond to a DMRS node,
and for the nonterminal c-hyperedges which corre-
spond to a connected DMRS subgraph of fewer
than seven nodes. 𝑉(𝐷)and𝐸(𝐷)denote the
sets of DMRS nodes and edges in 𝐷respectively.
𝜑(𝑌)returns the DMRS predicate of the unit set
𝑌in𝐷. For a recognized terminal, we compute
the incident s-hyperedges Σof it. C re-
turns𝐴, the canonical form, and Σ, the incident
s-hyperedges of 𝐷[𝑌], where𝐷[𝑌]is the node-
induced subgraph formed by the node set 𝑌. The
node order of the s-hyperedges in Σfollows the
order of the corresponding node representation in
the canonical form. The procedures of obtaining
Σhere is similar to those described at line 7 to 10
and are omitted. Line 13 updates the adjacency
information of 𝑌(for the detailed procedure, refer
to line 16 of Algorithm 2). Then, a passive item
𝑀=⟨𝑌,𝐴,Σ⟩is created and added to the chart.𝑝(𝑀)maps the passive item 𝑀to the highest
possible log probability of 𝑀at that instant.
Line 20 to 27 of Algorithm 1. Given the ex-
tracted SHRG productions 𝑃, new items are created
by applying unary rules to the passive items just cre-
ated ( A U ), then binary rules between
all the new passive items and their respective ad-
jacent passive items ( A B ). These two
steps are iterated repeatedly until no more passive
items are created. Line 22 records the successful
parses, i.e. the items that cover all DMRS nodes
and whose c-hyperedge label equals the start sym-
bol𝑆of𝐺. Finally, the successful parse with the
highest log probability is returned.
A.4 Grammar Intersection
The grammar intersection of an HRG is similar to
that of a CFG for strings in the CYK algorithm.
where neighbouring constituents are combined to
form new passive items. Algorithm 2 describes the
generation of new passive items via binary rules
(Line 24 of Algorithm 1).
For a set of passive items, we further define the
following: the common incident s-hyperedges are
called interior s-hyperedges; and the remaining s-
hyperedges are called exterior s-hyperedges, which
connect to other c-hyperedges in the subsequent
rewriting steps (for an illustration, see Fig. 7).
Line 3 of Algorithm 2. Grammar intersection is
performed only on the adjacent passive items (there
exists at least one s-hyperedge that connects them)
that cover disjoint sets of DMRS nodes. For effi-
ciency, all passive items are indexed by 𝑌, so that
validating adjacency and disjointness of DMRS
subgraphs can be manipulated by quick bitwise
operations to return the pairs of items possible for
grammar intersection. All items covering 𝑌are
further indexed by 𝐴so that we only consider the
pairs of items whose labels exist in the set of the
CFG daughter sequences of productions, 𝑅.
Line 4 to 7 of Algorithm 2. For each pair of
passive items in 𝐶, we try to generate possible
new passive items. Line 5 first computes the union
of set of DMRS nodes covered by the two items.
Then, the interior s-hyperedges of the new passive
item are computed at line 6. As the notion of exter-
nal nodes is dropped when we match an R.H.S.
hypergraph fragment 𝑅(see §4.2), the recogni-
tion only requires the identification of the set of
c-hyperedges in 𝑅plus the interior s-hyperedges.5437Algorithm 2: Generation of passive items with binary rules.
Thus, the hypergraph 𝐻is obtained at line 7
simply by gluing the two c-hyperedges with the
interior s-hyperedges.
Line 8 to 18 of Algorithm 2. If the hypergraph
obtained at line 8 appeared on the R.H.S of any
production in 𝑃(line 8), we proceed to compute
the exterior s-hyperedges of the new passive item
(line 9). Line 10 searches for the productions of
𝐺whose (1) R.H.S. hypergraph is 𝐻and (2)
rewriting syntactic construction 𝐴licenses a valid
composition (see §4.1; for the detailed procedures,
see Appendix B). Line 11 to 13 factor local am-
biguities; if we wish to extend the algorithm to
obtain k-best derivations, we just keep the k-best
hypotheses instead of the 1-best. Line 16 updates
the adjacency information of 𝑌through bitwise
operations. Finally, the newly generated passive
items, the updated log probabilities, and the adja-
cency map are returned.
B MRS-Specific Constraints in HRG
parsing
We describe how the MRS-specific constraints in-
troduced in §4.1 can be incorporated into HRG
parsing. In general, the accessibility check amounts
to validating the following conditions for each c-
hyperedge𝑒in the R.H.S. hypergraph fragment
when composing a new c-hyperedge 𝑎: (1) if𝑒
is not the of𝑎, it is not the of the
DMRS, and no s-hyperedges of type /or/outside𝑎are connected to 𝑒, and (2) if the scope
of𝑒,𝑙, is not the of𝑎,𝑙is not the of
the DMRS, and no s-hyperedges of type /,/
or/outside𝑎are connected to 𝑒. By scan-
ning the interior s-hyperedges once, we can decide
on the new , and by scanning the exterior s-
hyperedges once, we can verify the accessibility
of both variables. Our implementation excludes
the check of / edges due to undergeneration as
described in §4.1.
C Sample Input and Output
Fig. 8 shows an instance of sample input and out-
put compared against the original derivation. The
model used to produce the results is PSTHRG
(M1C) trained on the Redwoods training data.54385439