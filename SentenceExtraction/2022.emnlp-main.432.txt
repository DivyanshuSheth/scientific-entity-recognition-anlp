
Yaoyao Zhong, Junbin Xiao, Wei Ji,
Yicong Li, Weihong Deng, Tat-Seng ChuaNational University of Singapore, SingaporeSea-NExT Joint Lab, SingaporeBeijing University of Posts and Telecommunications, Beijing, China , ,
Abstract
This survey aims to organize the recent ad-
vances in video question answering (VideoQA)
and point towards future directions. We
firstly categorize the datasets into: 1) nor-
mal VideoQA, multi-modal VideoQA and
knowledge-based VideoQA, according to the
modalities invoked in the question-answer
pairs, and 2) factoid VideoQA and inference
VideoQA, according to the technical challenges
in comprehending the questions and deriving
the correct answers. We then summarize the
VideoQA techniques, including those mainly
designed for Factoid QA (such as the early
spatio-temporal attention-based methods and
the recent Transformer-based ones) and those
targeted at explicit relation and logic infer-
ence (such as neural modular networks, neu-
ral symbolic methods, and graph-structured
methods). Aside from the backbone tech-
niques, we also delve into specific models
and derive some common and useful insights
either for video modeling, question answer-
ing, or for cross-modal correspondence learn-
ing. Finally, we present the research trends of
studying beyond factoid VideoQA to inference
VideoQA, as well as towards the robustness and
interpretability. Additionally, we maintain a
repository, https://github.com/VRU-NExT/
VideoQA , to keep trace of the latest VideoQA
papers, datasets, and their open-source imple-
mentations if available. With these efforts, we
strongly hope this survey could shed light on
the follow-up VideoQA research.
1 Introduction
Recent years have witnessed a flourish of research
in vision-language understanding (Xu et al., 2016;
Chen et al., 2017; Antol et al., 2015; Chen et al.,
2018; Jang et al., 2017), of which, video Ques-
tion Answering (VideoQA) is one of the most
prominent, given its promise to develop interac-
tive AI to communicate with the dynamic visualworld via natural languages. Despite the popularity,
VideoQA remains one of the greatest challenges,
because it demands the models to comprehensively
understand the videos to correctly answer questions.
The questions involve not only the recognition of
visual objects, actions, activities and events, but
also the inference of their semantic, spatial, tempo-
ral, and causal relationships (Xu et al., 2017; Jang
et al., 2017; Shang et al., 2019, 2021; Yang et al.,
2021b; Xiao et al., 2021, 2022a).
To tackle the challenges, techniques such as
spatio-temporal attention (Jang et al., 2017),
motion-appearance memory (Gao et al., 2018),
and spatio-temporal or hierarchical graph models
(Cherian et al., 2022; Xiao et al., 2022a) have been
proposed and demonstrated their effectiveness on
different VideoQA datasets. However, we find that
the datasets, the defined challenges, and the cor-
responding algorithms are varied and a bit messy.
There is a lack of a meaningful survey to categorize
the datasets and to organize the technique devel-
oped, which seriously impedes the research.
Although a handful of recent works (Sun et al.,
2021; Khurana and Deshpande, 2021; Patel et al.,
2021) have tried to review VideoQA, they mostly
follow an old-to-new fashion to summarize the lit-
erature and lack an effective taxonomy to classify
them. In terms of the contents, these works fo-
cus merely on factoid questions and neglect the
inference questions (see Fig. 1 for the difference).
Furthermore, lots of recent new techniques ( e.g.,
pre-training and Transformer) are missing.
This paper thus gives a more comprehensive
and meaningful survey to VideoQA, in the hope
of learning from the past and shaping the future.
Our contributions are as follows. (1) We provide
a clear taxonomy to VideoQA. We can either clas-
sify existing VideoQA tasks into Factoid VideoQA
and Inference VideoQA according to the fundamen-
tal challenges embodied in QAs, or classify them
into normal VideoQA, Multi-modal VideoQA, and6439Knowledge-based VideoQA according to the multi-
modal information invoked in the QAs. (2) We
categorize existing VideoQA techniques as Mem-
ory, Transformer, Graph, Neural Modular Network,
and Neural-Symbolic method. Along with the tech-
niques, some meaningful insights are also included:
attention modeling, cross-modal pre-training, hi-
erarchical learning, multi-granular ensemble, and
progressive reasoning. (3) We analyze existing
methods from the perspective of the challenges
encountered in the various VideoQA tasks and pro-
vide our prospects for future research.
2 VideoQA Task and Datasets
2.1 Problem Formulation
VideoQA is a task to predict the correct answer
𝑎based on a question 𝑞and a video 𝑉. There
are mainly two types of tasks in VideoQA: multi-
choice QA and open-ended QA.
Formulti-choice QA, the models are presented
with several candidate answers Afor each ques-
tion and are required to pick the correct one 𝑎=
F(𝑎|𝑞,V,A). For open-ended QA, the prob-
lem can be classification (the most popular), gener-
ation (word-by-word) and regression (for counting)
depending on the specific datasets. Specifically,
open-ended QA is popularly set as a multi-class
classification problem which requires the mod-
els to classify a video-question pair into a pre-
defined global answer set A:𝑎=F(𝑎|𝑞,V)
where 𝑎∈A. Open-ended QA can also be for-
mulated as a generation problem, which might
have more practical use and receiving increas-
ing attention. Usually the answer is denoted as
𝑎=(𝑎, 𝑎, ..., 𝑎, ..., 𝑎)of length 𝑀, where
𝑎is the 𝑡-th word; and the model is required
to predict the next word 𝑎in the vocabulary set
W:𝑎=F(𝑎|𝑞,V,(𝑎, 𝑎, ..., 𝑎)), where
𝑎∈W . For the counting task, which is defined as
an open-ended question about counting the number
of repetitions of an action (Jang et al., 2017), it is
formulated as an regression problem, requiring the
model to compute an integer-valued answer to be
close to the ground truth.
Compared with open-ended QA, multi-choice
QA is typically defined to study beyond factoid
QA to inference QA (Xiao et al., 2021; Wu et al.,
2021a), as it dispenses with the generation and
evaluation of natural languages.64402.2 Evaluation Metrics
Accuracy. For multi-choice QA and open-ended
QA (classification), accuracy is defined based on
the entire testing question set Q, given by:
𝑎𝑐𝑐=1
|Q|∑︁I[𝑎=𝑎], (1)
whereQrepresents the number of QA pairs, and
I[·]is an indicator function (1 only if 𝑎=𝑎and
0 otherwise). Similarly, for open-ended QA (word-
by-word generation) (Zhao et al., 2017b, 2018),
accuracy is defined as:
𝑎𝑐𝑐=1
|Q|∑︁1
𝑀∑︁I/b√︂acketleftbig
𝑎=𝑎/b√︂acket√︂ightbig
, (2)
where 𝐿denotes the length of the shorter answer.
WUPS. The WUPS is the soft measure of accu-
racy by taking into account word synonyms. It is
based on the WUP score (Wu and Palmer, 1994) to
evaluate the quality of the generated answer (Zhao
et al., 2017b, 2018; Xiao et al., 2021). The WUP
measures word similarity based on WordNet (Fell-
baum, 1998). WUPS score with the threshold 𝛾is
defined as,
(3)
where WUP score is given by,
𝑊𝑈𝑃(𝑎, 𝑎)=/b√︂aceleftig.
(4)
where the parameter 𝛾is dataset-specific.
MeanLloss. For the repetition count
task (Jang et al., 2017), the mean Lloss is de-
fined based on the entire testing question set Q:
L=1
|Q|∑︁(𝑎−𝑎), (5)
in which a and a* are predicted and ground-truth
numbers respectively.
The evaluation metrics mainly serve for differ-
ent task settings, while there are also some novel
and diagnostic ones (Gandhi et al., 2022; Li et al.,
2022b; Castro et al., 2022a) that may be helpful for
robustness and interpretation of VideoQA models.
2.3 Datasets
VideoQA can be understood from different per-
spectives, since the aim is to gain multi-view andmulti-grained understanding of videos under the
guidance of specific questions.
Modality-based Taxonomy. According to the
data modality invoked in the questions and an-
swers, VideoQA can be classified into normal
VideoQA, multi-modal VideoQA (MM VideoQA),
and knowledge VideoQA (KB VideoQA). Normal
VideoQA only invokes visual resources to under-
stand the question and to derive the correct answer.
It emphasizes visual understanding of the video
elements and reasoning of their relations. Usu-
ally, the videos are short and are typically user-
generated on social platforms. Different from nor-
mal VideoQA, MM VideoQA often involves other
resources aside from visual contents, such as subti-
tles/transcripts and text plots of movies (Tapaswi
et al., 2016) and TV shows (Lei et al., 2018). MM
VideoQA mainly challenges multi-modal informa-
tion fusion and long video story understanding.
Finally, KB VideoQA (Garcia et al., 2020) de-
mands external knowledge distillation from explicit
knowledge bases or commonsense reasoning (Fang
et al., 2020). Different from MM VideoQA, KB
VideoQA provides a global knowledge base for the
whole dataset, instead of giving paired “knowledge”
for each question. For better understanding of the
three kinds of VideoQA, we show typical examples
in Figure 1 (right).
Question-based Taxonomy. According to the
type of question (or the challenges posted in the
questions), VideoQA can be classified into factoid
VideoQA and inference VideoQA. A factoid ques-
tion directly asks about the visual fact, such as the
location ( where is ), objects/attributes ( who/what
(color) is ), and invokes little relations to under-
stand the questions and infer the correct answers.
Factoid QA emphasizes the holistic understanding
of the questions and the recognition of the visual
elements. In contrast, inference VideoQA aims to
explore the logic and knowledge reasoning abil-
ity in dynamic scenarios. It features various rela-
tionships between the visual facts. Though rich
in relation types, VideoQA emphasizes temporal
(before/after ) and causal ( why/How/what if )
relationships that feature temporal dynamics, as
emphasized by recent works (Zadeh et al., 2019;
Yi et al., 2020; Xiao et al., 2021; Li et al., 2022b).
Datasets Analysis. The timeline of some es-
tablished VideoQA datasets is shown in Figure 2.
We categorize all the datasets according to our de-
fined taxonomy in Table 1 and their details are6441
listed in Table A1 (see Appendix). VideoQA and
MM VideoQA almost appear simultaneously, and
have been studied separately by the community.
Despite the unique challenges of MM VideoQA in
reasoning on multiple modalities (Kim et al., 2020),
algorithms targeting VideoQA and MM VideoQA
share similar spirits. Modality-based taxonomy
stems from research preference for video domains.
While question-based taxonomy is affected more
by the methodological considerations, since the
recently proposed Inference VideoQA brings new
technical challenges, which is driving artificial in-
telligence towards new heights, not just limited to
learning the correlations in data.
2.4 Main Framework
As shown in Figure 3, a common framework com-
prises four parts: video encoder, question encoder,
cross-modal interaction, and answer decoder. The
video encoder often encodes raw videos by jointly
extracting frame appearance and clip motion fea-
tures. Recent works also show that object-level
visual and semantic features ( e.g., category and
attribute labels) are important. These features are
usually extracted with pre-trained 2D or 3D neural
networks, as summarized in Table 2. Question en-
coder extracts token-level representation, such as
GloVe and BERT features (Kenton and Toutanova,
2019). Then, the sequential data of vision and
language can be further processed by sequential
models ( e.g., RNN, CNN, and Transformer) for theconvenience of cross-modal interaction, which will
be detailed further. For multi-choice QA, the an-
swer decoder can be a 1-way classifier to select the
correct answer from the provided multiple choices.
For open-ended QA, it can be either an n-way clas-
sifier to select an answer from a pre-defined global
answer set, or a language generator to generate an
answer word by word. The video and language
encoders can be pre-trained or more recently end-
to-end fine-tuned (Lei et al., 2021).
2.5 Challenges and Meaningful Insights
Unique Challenges. Compared with Im-
ageQA (Lu et al., 2016; Anderson et al., 2018),
VideoQA is much more challenging because of the
spatio-temporal nature of videos (Xiao et al., 2020,
2021). Thus, a simple extension of existing Im-
ageQA techniques to answer queries of videos will
lead to sub-optimal results. Compared with other
video tasks, question-answering requires a compre-
hensive understanding of videos in different aspects
and granularity, such as from fine-grained to coarse-
grained in both temporal and spatial domains, and
from factoid questions to inference questions. To
tackle the challenges, a lot of research efforts have
been developed on cross-modal interaction, which
aims to gain understanding of videos under the
guidance of questions. We summarize some com-
mon and meaningful insights as follows:
Attention. Attention is a human-inspired mech-
anism that locates the important part of the input6442
and selectively focuses on useful information. In
VideoQA, to attend to a specific part of videos in
both spatial and temporal dimensions, temporal at-
tention andspatial attention are widely used. Self-
attention has a good ability to model long-range
dependencies, and can be used in intra-modal mod-
eling, such as temporal information in the video
and global dependencies of questions. Co-attention
(Cross-modal attention ) can attend to both relevant
and critical multi-modal information, such as the
question-guided video representation and video-
guided question representation.
Cross-modal Pre-training and Fine-tuning.
With the development of unified network architec-
tures ( e.g., Transformer (Vaswani et al., 2017)) that
can well handle visual and linguistic data, cross-
modal pre-training can make full use of the seman-
tic information from noisy but web-scale vision-
text data (Radford et al., 2021). The learned model
can be transferred to downstream vision-language
tasks by fine-tuning on small-scale manually anno-
tated datasets with strong supervision. Currently,
the research in this paradigm lies in four aspects:
large-scale data collection, proxy task definition,
Transformer-style model design, and downstream
adaption. We recommend the readers to read the
latest survey (Chen et al., 2022) for details.
Multi-Granularity Ensemble. Questions are
diverse and unconstrained, and may demand
video information of different granularities for an-
swers (Xiao et al., 2022a). To gain rich informa-
tion and answer the varied questions, the multi-
granularity ensemble is essential. Specifically, the
multi-granularity ensemble exists in both the text
domain and the vision domain of both spatial and
temporal dimensions. In the text domain, word-,
phrase- and sentence-level feature representations
are coordinated to achieve both fine- and coarse-
grained information modeling. In the vision do-main, region-, trajectory-, frame- and clip-level
feature representations can complement each other
to achieve comprehensive video understanding.
Hierarchical Learning. Considering that the
video elements and their textual correspondences
in QA pairs are in different abstraction levels, hi-
erarchical learning aims to organize multi-modal
representation from low-level to high-level, and
from local to global (Le et al., 2020; Dang et al.,
2021; Xiao et al., 2022a). Specifically, linguistic
concepts are analyzed from word to sentence. Sim-
ilarly, video elements are processed from objects
to actions, activities, and global events. Compared
with the multi-granularity ensemble, hierarchical
learning processes the multi-granular information
progressively. It gradually reasons and aggregates
the low-level, local visual information into the high-
level, global video representation. Thus, hierarchi-
cal learning can better reflect the structure and rela-
tionship of video elements and accomplish question
answering hierarchically.
Others. Aside from the above, multi-step rea-
soning (Wang et al., 2021; Mao et al., 2022) and
causal discovery (Li et al., 2022d) also demonstrate
the effectiveness. Most importantly, these insights
are not mutually exclusive; they can be coordinated
in a single model for good performance.
3 Algorithms
3.1 Methods
Early Attention-based Works. (Zeng et al.,
2017) try to directly apply element-wise multipli-
cation to fuse the global video and question repre-
sentations for answer prediction. Additionally, it
demonstrates the advantage of a simple temporal at-
tention. Attention is also explored in more complex
scenarios in conjunction with various other ideas,
such as multi-granularity ensemble (Xu et al., 2017)
and hierarchical learning (Zhao et al., 2017a). In
particular, (Jang et al., 2017) propose a dual-LSTM
based approach with spatial and temporal atten-
tion mechanisms, which can focus better on critical
frames in a video and critical regions in a frame.
(Xu et al., 2017) refine attention over both frame-
level and clip-level visual features, conditioned
with both the coarse-grained question feature and
fine-grained word feature. (Zhao et al., 2017a)
propose hierarchical dual-level attention networks
(DLAN) to learn the question-aware video repre-
sentations with word-level and question-level atten-
tion based on appearance and motion.6443Despite the ability to attend to video frames and
clips, these works rely on RNN for history infor-
mation modeling, which has later been shown to
be weak in capturing long-term dependency.
Memory Networks. Memory networks can
cache sequential inputs in memory slots and ex-
plicitly utilize even far early information. Memory
especially receives attention in long video story
understanding , such as movies and TV-Shows. Be-
cause the QAs in these VideoQA tasks not only
involve the understanding of visual contents, but
also the long stories they convey.
(Tapaswi et al., 2016) first incorporate and mod-
ify the memory network (Sukhbaatar et al., 2015)
into VideoQA, to store video and subtitle features
in the memory bank. To enable memory read and
write operations with high capacity and flexibil-
ity, (Na et al., 2017) design a memory network
with multiple convolution layers. Considering dual-
modal information in the movie story, (Kim et al.,
2019) introduce a progressive attention mechanism
to progressively prune out irrelevant temporal parts
in the memory bank for each modality, and adap-
tively integrate outputs of each memory.
Memory has also been explored in normal
VideoQA . (Gao et al., 2018) propose a two-stream
framework (CoMem) to deal with motion and ap-
pearance information with a co-memory attention
module, introducing multi-level contextual infor-
mation and producing dynamic fact ensembles for
diverse questions. Considering that CoMem syn-
chronizes the attentions detected by appearance
and motion features, it could thus generate incor-
rect attention, (Fan et al., 2019) further introduce
a heterogeneous external memory module (HME)
with attentional read and write operations to inte-
grate the motion and appearance features and learn
the spatio-temporal attention simultaneously.
Transformer. Transformer (Vaswani et al.,
2017) has a good ability to model long-term re-
lationships and has demonstrated promising perfor-
mance for modeling multi-modal vision-language
tasks such as VideoQA, with pre-training on large-
scale datasets (Zhu and Yang, 2020). Motivated by
the success of Transformer, (Li et al., 2019) first
introduce the architecture of Transformer without
pre-training to VideoQA (PSAC), which consists
of two positional self-attention blocks to replace
LSTM, and a video-question co-attention block to
simultaneously attend both visual and textual infor-
mation. (Yang et al., 2020) and (Urooj et al., 2020)incorporate the pre-trained language-based Trans-
former (BERT) (Kenton and Toutanova, 2019) to
movie and story understanding, which requires
more modeling on languages like subtitles and di-
alogues. Both works process each of the input
modalities such as video and subtitles, with ques-
tion and candidate answer, respectively, and lately
fuse several streams for the final answer.
More recently, (Lei et al., 2021) apply the image-
text pre-trained Transformer for cross-modal pre-
training and fine-tune it for downstream video-text
tasks, such as VideoQA. (Yang et al., 2021a) train
a VideoQA model, based on a large-scale dataset,
with 69M video-question-answer triplets, using
contrastive learning between a multi-modal video-
question Transformer and an answer Transformer.
This video-text pre-trained Transformer can be
further fine-tuned on other downstream VideoQA
tasks, which shows the benefits of task-specific pre-
training for the target VideoQA task. Furthermore,
(Zellers et al., 2021) train a cross-modal Trans-
former (MERLOT) in a label-free, self-supervised
manner, based on 180M video segments with im-
age frames and words. Similar to MERLOT, VIO-
LET (Fu et al., 2021) is another end-to-end video-
text pre-trained Transformer model but with more
advanced video encoder and proxy tasks.
While the aforementioned Transformer-style
models have demonstrated strong performances
on popular Factoid VideoQA datasets (refer to our
analysis in Sec. 3.2), recent works (Buch et al.,
2022; Xiao et al., 2022b) reveal that their perfor-
mance are weak in answering questions that em-
phasize visual relation reasoning, especially the
temporal and causal relations which feature video
dynamics. Furthermore, their demands on large-
scale video data for pre-training and the lack of
explanability largely prevent their popularity. Such
weaknesses call for more future efforts in develop-
ing foundation models for fine-grained video rea-
soning, and simultaneously, with less computation
resources and better interpretability.
Graph Neural Networks. Graph-structured
techniques (Kipf and Welling, 2017; Zhang et al.,
2022) are recently more favoured for improving
the reasoning ability of VideoQA models, espe-
cially when Inference VideoQA draws attention
to the community (Jang et al., 2017; Xiao et al.,
2021). HGA (Jiang and Han, 2020), and more
recent works, B2A (Park et al., 2021) and Du-
alVGR (Wang et al., 2021) build the graphs based6444on coarse-grained video segments. Yet, they in-
corporate both intra- and inter-modal relationship
learning and achieve good performances. To gain
object-level information, (Huang et al., 2020) build
the graph (LGCN) based on objects represented by
their appearance and location features. They model
the interaction between objects related to questions
with GNN (Kipf and Welling, 2017).
Considering that the video elements are hierar-
chical in semantic space, (Liu et al., 2021a), (Peng
et al., 2021) and (Xiao et al., 2022a) incorporate
hierarchical learning into graph networks. Specifi-
cally, (Liu et al., 2021a) propose a graph memory
mechanism (HAIR), to perform relational vision-
semantic reasoning from object level to frame
level; (Peng et al., 2021) concatenate different-level
graphs, that is, object-level, frame-level, and clip-
level, progressively to learn the visual relations
(PGAT). (Xiao et al., 2022a) propose a hierarchi-
cal conditional graph model (HQGA) to weave to-
gether visual facts from low-level entities to higher-
level video elements through graph aggregation and
pooling, to enable vision-text matching at multi-
granularity levels. To leverage the semantics of
the 3D scene, (Cherian et al., 2022) transfer the
video frames to a 2.5D (pseudo-3D) scene graph
and then split it into static and dynamic sub-graphs,
allowing the pruning of redundant detections.
With a good ability for information communi-
cation, graph architectures have shown promising
results on inference VideoQA. Nonetheless, the em-
phasis and difficulty lie in how to skillfully design
the graph structure for video representation.
Modular Networks. (Le et al., 2020) find that
most VideoQA models design tailor-made network
architectures. They point out such hand-crafted ar-
chitectures are inflexible in dealing with varied data
modality, video length and question types. There-
fore, they design a reusable neural unit - Condi-
tional Relation Network (CRN), which captures
the relations of input features given the global
context and encapsulates them hierarchically to
form networks. Such a constituted architecture has
shown better generalization ability and flexibility
in handling different types of questions. Follow-
ing similar design philosophy, (Dang et al., 2021)
and (Xiao et al., 2022a) design the spatio-temporal
graph and conditional graph respectively as neural
building blocks. The neural building blocks are hi-
erarchically stacked to achieve good reasoning per-
formances. While the above works aim for repeat-ing a single module for videoQA. Recently, (Qian
et al., 2022) design multiple modules tailored for
compositional video question-answering (Grunde-
McLaughlin et al., 2021), and has also demon-
strated success. Overall, modular networks are of
improved flexibility and transparency. Nonetheless,
they either lack explicit logic for reasoning (Le
et al., 2020; Dang et al., 2021; Xiao et al., 2022a),
or can only handle questions that can be parsed into
pre-defined subtasks of limited scope.
Neural-Symbolic. (Yi et al., 2020) point out two
essential elements for causal reasoning in VideoQA
are object-centric video representation that is aware
of the temporal and causal relations between the
objects and events, and a dynamics model that is
able to predict the object dynamics under unob-
served or counterfactual scenarios. Motivated by
the neural-symbolic method in ImageQA (Yi et al.,
2018), (Yi et al., 2020) propose the NS-DR model,
which extracts object-level representation with a
video parser, turns a question into a functional pro-
gram, extracts and predicts the dynamic scene of
the video with a dynamics predictor, and runs the
program on the dynamic scene to obtain an answer.
NS-DR aims to combine neural nets for pattern
recognition and dynamics prediction, and symbolic
logic for causal reasoning. It achieves significant
gain on the explanatory, predictive, and counterfac-
tual questions on the synthetic object dataset (Yi
et al., 2020). (Chen et al., 2021) and (Ding et al.,
2021) promote further progress.
Despite the good reasoning ability of Neural-
Symbolic methods on synthetic datasets, they are
currently hard to be applied in unconstrained video
with open-form natural questions.
Others. There are also flexibly designed net-
works to address specific problems. For example,
(Kim et al., 2020) propose a framework that first
detects a specific temporal moment from moments
of interest candidates for temporally-aligned video
and subtitle using pre-defined sliding windows, and
then fuses information based on the localized mo-
ment using intra-modal and cross-modal attention
mechanisms. Due to their focuses on specific pur-
poses, the question remains on whether these net-
works can be generalized to other VideoQA tasks.
Studies are also conducted in terms of input in-
formation . (Falcon et al., 2020) explore several
data augmentation techniques to prevent overfit-
ting with only small-scale datasets. (Kim et al.,
2021a) point out existing works suffer from signifi-6445
cant computational complexity and insufficient rep-
resentation capability and they introduce VideoQA
features obtained from coded video bit-stream to
address the problem. To overcome spurious visual-
linguistic correlations, (Li et al., 2022d,c) explore
robust and trustworthy grounding framework from
causal theory, which is promising to enhance the
SOTA models’ accuracy and trustability.
3.2 Performance Analysis
We analyze the advanced methods for Factoid
VideoQA in Table 2 and Inference VideoQA in
Table 3 based on the results reported on popu-
lar VideoQA benchmarks. Apart from normal
VideoQA, advanced methods for MM VideoQA
and KB VideoQA are also summarized in Table 4.
Table 2 reveals that the cross-modal pre-trained
Transformer-style models can achieve superior per-formance for factiod QA than others. By focusing
on methods without pre-training, graph-structured
techniques are the most popular and have also
shown great potential. It would be interesting to
explore cross-modal pre-training of graph models
for VideoQA. Besides, hierarchical learning and
fine-grained object features usually help to improve
performances. In addition to the datasets given
in Table 2, the recent iVQA (Yang et al., 2021a)
dataset has also received increasing attention, and
we believe it could be a more effective dataset to-
wards open-ended VideoQA for its high quality.
Inference VideoQA is a nascent task that chal-
lenges mainly visual relation reasoning of video
information. It also receives increasing attention.
Graph-structured techniques, causal discovery, and
hierarchical learning have shown promising per-
formance (see Table 3). Notably, we find that
cross-modal pre-training and fine-tuning not only
achieves good performance on factoid VideoQA,
but also significantly improves the results on in-
ference VideoQA. Particularly, the accuracies of
reasoning tasks on TGIF-QA reach unprecedent-
edly high. This dataset is likely not challenging
enough and has serious language bias as revealed
by recent studies (Peng et al., 2021; Piergiovanni
et al., 2022; Xiao et al., 2022b). In contrast, NExT-
QA is much more challenging; it emphasizes causal
and temporal relation reasoning between multiple
objects in real-world videos. Table 3 shows that
SOTA methods still struggle on NExT-QA. As such,6446
NExT-QA could be a more effective benchmark for
visual reasoning of realistic video contents under
natural language instructions. Additionally, NExT-
QA also contains open-ended QA task that provide
ample challenge for existing research.
MM and KB VideoQA require models to locate
and perform reasoning in all heterogeneous modal-
ities for answering the question. Similar to nor-
mal VideoQA, MM VideoQA also benefits from
advanced networks and large-scale datasets. How-
ever, it is worth noting that modality shifting ability
is essential (Kim et al., 2020; Engin et al., 2021).
4 Future Direction
From Recognition to Reasoning. Advanced neu-
ral network models excel at recognizing objects,
attributes and even actions in visual data. Thus, an-
swering the questions like "what is" is no longer the
core of VideoQA. To enable more meaningful and
in-depth human-machine interaction, it is urgent to
study the casual and temporal relations between ob-
jects, actions, and events (Xiao et al., 2021). Such
problems feature video -level understanding and de-
mand inference ability for question answering. The
focus on inference questions promotes research to-
wards the core of human intelligence, which could
be one of the “north stars” towards groundbreaking
works (Fei-Fei and Krishna, 2022).
Knowledge VideoQA. To answer the questions
that are beyond the visual scene, it is of crucial
importance to inject knowledge into the reasoning
stage (Jin et al., 2019; Garcia et al., 2020; Zhuang
et al., 2020). Knowledge incorporation can not
only greatly extend the scope of questions that can
be asked about videos, but also enable the explo-
ration of more human-like inference. Because we
humans are natural to answer questions that may in-
volve commonsense (Fang et al., 2020) or domain-
specific knowledge (Xu et al., 2021; Gao et al.,
2021). Reasoning with knowledge and diagnosing
the retrieved knowledge for a specific question willhelp to enhance the model’s interpretability and
trustability. It will also serve as important ground-
work for the future multi-modal conversation sys-
tems (Nie et al., 2019; Li et al., 2022e).
Cross-modal Pre-training and Fine-tuning.
Cross-modal pre-trained representations (Zellers
et al., 2021; Fu et al., 2021) have shown great bene-
fit for VideoQA (see Table 2 and 3). However, most
models only demonstrate their good performance
on VideoQA tasks that challenge the recognition or
shallow description of the video contents. Also, it
demands a lot of computation and other resources
to handle large-scale video-text data. Therefore,
how to pre-train vision-language models more effi-
ciently and how to adapt them to reasoning type of
VideoQA tasks deserve more attention.
Interpretability, Robustness and Generaliza-
tion. Despite the strong power of the advanced
pre-training models, it is still unknown how they
work, to what extent they can generalize, when they
will fail, and how to gain further technical improve-
ment. Recent works towards interpretability and
logical robustness (Li et al., 2021b; Sheng et al.,
2021) have achieved initial success. (Gandhi et al.,
2022) design a benchmark to diagnose whether
models can gain true understanding by examining
compositional consistency. However, there is a still
long way to go towards model interpretability, ro-
bustness and generalization. We believe this is of
great significance towards practical QA systems.
5 Conclusion
This paper gives a quick overview to the broad
aspect of video question answering. We mainly cat-
egorized the related datasets and techniques. Also,
we discussed some meaningful insights and ana-
lyzed the performances of different techniques on
different type of datasets. We finally concluded
several promising future directions. With these ef-
forts, we hope this survey can shed light and attract
more research to VideoQA, and eventually, foster
more efforts towards strong AI systems that can
demonstrate their understanding of the dynamic
visual world by making meaningful responses to
our natural language instructions or queries.
Acknowledgements
The research is supported by the Sea-NExT Joint
Lab. The research is also supported by the
National Natural Science Foundation of China
(No.62236003 and No.62276030), and China
Scholarships Council (No.202106470037).6447Limitations
Although we have tried to comprehensively ana-
lyze the literature of VideoQA research, we real-
ize that we fail to cover and detail all the datasets
and algorithms due to the thriving VideoQA re-
search and the limited space. Hence, we com-
plement the survey by maintaining a repository
https://github.com/VRU-NExT/VideoQA . The
repository contains the latest VideoQA papers,
datasets, and their open-source implementations.
We will periodly update the repository to trace the
progress of the latest research.
References644864496450645164526453AAppendix: Details of VideoQA datasets
in the literature
Due to limited space, details of VideoQA datasets
are listed in Table A1.
B Appendix: Timeline of VideoQA
techniques
In the literature, the VideoQA datasets and tech-
niques jointly evolve in time (as shown in Fig-
ure A1). Some of the datasets and techniques
influence each other. As the cross-modal pre-
training and fine-tuning technique develops, the
performance of early-stage datasets like TGIF-
QA (Jang et al., 2017) and TVQA+ (Lei et al.,
2020) reaches unprecedentedly high (close to hu-
man performance, refer to Table 3 and Table 4).
The new research focus turns to the more challeng-
ing VideoQA datasets like NExT-QA (Xiao et al.,
2021), which invokes complicated inference among
multiple objects and relations. In turn, the infer-
ence QA datasets motivate new research interests
in new techniques. CLEVRER (Yi et al., 2020) has
inspired new works using neuro-symbolic learn-
ing (Yi et al., 2020; Chen et al., 2021; Ding et al.,
2021), and NExT-QA has promoted a lot of recent
works on graph models (Xiao et al., 2022a,b). Di-
agnostic datasets like AGQA (Grunde-McLaughlin
et al., 2021) and AGQA 2.0 (Gandhi et al., 2022)
analyze existing methods by checking composi-
tional consistency to examine whether they gain
true understanding. These diagnostic datasets are
promising to find the existing defects and motivate
new methods (Qian et al., 2022).64546455