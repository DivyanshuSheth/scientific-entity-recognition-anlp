
Yongliang Shen, Kaitao Song, Xu Tan,
Dongsheng Li, Weiming Lu, Yueting Zhuang
Zhejiang University, Microsoft Research Asia
{syl, luwm}@zju.edu.cn ,{kaitaosong, xuta}@microsoft.com
Abstract
In this paper, we propose D NER ,
which formulates the named entity recognition
task as a boundary-denoising diffusion process
and thus generates named entities from noisy
spans. During training, D NER grad-
ually adds noises to the golden entity bound-
aries by a fixed forward diffusion process and
learns a reverse diffusion process to recover
the entity boundaries. In inference, D - NER first randomly samples some noisy
spans from a standard Gaussian distribution
and then generates the named entities by de-
noising them with the learned reverse diffusion
process. The proposed boundary-denoising dif-
fusion process allows progressive refinement
and dynamic sampling of entities, empower-
ingD NER with efficient and flexible
entity generation capability. Experiments on
multiple flat and nested NER datasets demon-
strate that D NER achieves compara-
ble or even better performance than previous
state-of-the-art models.
1 Introduction
Named Entity Recognition (NER) is a basic task
of information extraction (Tjong Kim Sang and
De Meulder, 2003), which aims to locate entity
mentions and label specific entity types such as
person, location, and organization. It is fundamen-
tal to many structured information extraction tasks,
such as relation extraction (Li and Ji, 2014; Miwa
and Bansal, 2016) and event extraction (McClosky
et al., 2011; Wadden et al., 2019).
Most traditional methods (Chiu and Nichols,
2016) formulate the NER task into a sequence la-
beling task by assigning a single label to each token.
To accommodate the nested structure between en-
tities, some methods (Ju et al., 2018; Wang et al.,Figure 1: Boundary diffusion in named entity recogni-
tion. The fixed forward diffusion process adds Gaussian
noise to the entity boundaries at each timestep, and the
noisy boundaries recover their original state by denois-
ing with the learnable reverse diffusion process. For
inference, the reverse diffusion process generates en-
tity boundaries and performs entity typing based on the
noisy spans sampled from the Gaussian distribution.
2020) further devise cascaded or stacked tagging
strategies. Another class of methods treat NER as a
classification task on text spans (Sohrab and Miwa,
2018; Eberts and Ulges, 2020), and assign labels
to word pairs (Yu et al., 2020; Li et al., 2022a)
or potential spans (Lin et al., 2019; Shen et al.,
2021a). In contrast to the above works, some pio-
neer works (Paolini et al., 2021; Yan et al., 2021b;
Lu et al., 2022) propose generative NER methods
that formulate NER as a sequence generation task
by translating structured entities into a linearized
text sequence. However, due to the autoregressive
manner, the generation-based methods suffer from
inefficient decoding. In addition, the discrepancy
between training and evaluation leads to exposure
bias that impairs the model performance.
We move to another powerful generative model
for NER, namely the diffusion model. As a class
of deep latent generative models, diffusion models
have achieved impressive results on image, audio
and text generation (Rombach et al., 2022; Ramesh
et al., 2022; Kong et al., 2021; Li et al., 2022b;
Gong et al., 2022). The core idea of diffusion mod-
els is to systematically perturb the data through3875a forward diffusion process, and then recover the
data by learning a reverse diffusion process.
Inspired by this, we present D NER , a
new generative framework for named entity recog-
nition, which formulates NER as a denoising diffu-
sion process (Sohl-Dickstein et al., 2015; Ho et al.,
2020) on entity boundaries and generates entities
from noisy spans. As shown in Figure 1, during
training, we add Gaussian noise to the entity bound-
aries step by step in the forward diffusion process,
and the noisy spans are progressively denoised by
a reverse diffusion process to recover the original
entity boundaries. The forward process is fixed
and determined by the variance schedule of the
Gaussian Markov chains, while the reverse process
requires learning a denoising network that progres-
sively refines the entity boundaries. For inference,
we first sample noisy spans from a prior Gaussian
distribution and then generate entity boundaries
using the learned reverse diffusion process.
Empowered by the diffusion model, D -
NER presents three advantages. First, the itera-
tive denoising process of the diffusion model gives
D NER the ability to progressively re-
fine the entity boundaries, thus improve perfor-
mance. Second, independent of the predefined
number of noisy spans in the training stage, D- NER can sample a different number of
noisy spans to decode entities during evaluation.
Such dynamic entity sampling makes more sense
in real scenarios where the number of entities is
arbitrary. Third, different from the autoregressive
manner in generation-based methods, D -
NER can generate all entities in parallel within
several denoising timesteps. In addition, the shared
encoder across timesteps can further speed up infer-
ence. We will further analyze these advantages of
D NER in § 6.2. In summary, our main
contributions are as follows:
•D NER is the first to use the diffu-
sion model for NER, an extractive task on
discrete text sequences. Our exploration pro-
vides a new perspective on diffusion models
in natural language understanding tasks.
•D NER formulates named entity
recognition as a boundary denoising diffusion
process from the noisy spans. D -
NER is a novel generative NER method that
generates entities by progressive boundary re-
finement over the noisy spans.•We conduct experiments on both nested and
flatNER to show the generality of D - NER . Experimental results show that our
model achieves better or competitive perfor-
mance against the previous SOTA models.
2 Related Work
2.1 Named Entity Recognition
Named entity recognition is a long-standing study
in natural language processing. Traditional meth-
ods can be divided into two folders: tagging-based
and span-based. For tagging-based methods (Chiu
and Nichols, 2016; Ju et al., 2018; Wang et al.,
2020), they usually perform sequence labeling at
the token level and then translate into predictions
at the span level. Meanwhile, the span-based meth-
ods (Sohrab and Miwa, 2018; Eberts and Ulges,
2020; Shen et al., 2021a,b; Li et al., 2022a) di-
rectly perform entity classification on potential
spans for prediction. Besides, some methods at-
tempt to formulate NER as sequence-to-set (Tan
et al., 2021, 2022; Wu et al., 2022) or reading
comprehension (Li et al., 2020; Shen et al., 2022)
tasks for prediction. In addition, autoregressive
generative NER works (Athiwaratkun et al., 2020;
De Cao et al., 2021; Yan et al., 2021b; Lu et al.,
2022) linearize structured named entities into a se-
quence, relying on sequence-to-sequence language
models (Lewis et al., 2020; Raffel et al., 2020)
to decode entities. These works designed various
translation schemas, including from word index
sequence to entities (Yan et al., 2021b) and from
label-enhanced sequence to entities (Paolini et al.,
2021), to unify NER to the text generation task
and achieved promising performance and general-
izability. Other works (Zhang et al., 2022) focus on
the disorder of the entities and mitigate incorrect
decoding bias from a causal inference perspective.
Different from previous works, our proposed
D NER is the first one to explore the
utilization of the generative diffusion model on
NER, which enables progressive refinement and
dynamic sampling of entities. Furthermore, com-
pared with previous generation-based methods, our
D NER can also decode entities in a non-
autoregressive manner, and thus result in a faster
inference speed with better performance.
2.2 Diffusion Model
Diffusion model is a deep latent generative model
proposed by (Sohl-Dickstein et al., 2015). With3876the development of recent work (Ho et al., 2020),
diffusion model has achieved impressive results on
image and audio generation (Rombach et al., 2022;
Ramesh et al., 2022; Kong et al., 2021). Diffu-
sion model consists of the forward diffusion pro-
cess and the reverse diffusion process. The for-
mer progressively disturbs the data distribution by
adding noise with a fixed variance schedule (Ho
et al., 2020), and the latter learns to recover the
data structure. Despite the success of the diffu-
sion model in continuous state spaces (image or
waveform), the application to natural language still
remains some open challenges due to the discrete
nature of text (Austin et al., 2021; Hoogeboom
et al., 2022; Strudel et al., 2022; He et al., 2022).
Diffusion-LM (Li et al., 2022b) models discrete
text in continuous space through embedding and
rounding operations and proposes an extra classifier
as a guidance to impose constraints on controllable
text generation. DiffuSeq (Gong et al., 2022) and
SeqDiffuSeq (Yuan et al., 2022a) extend diffusion-
based text generation to a more generalized setting.
They propose classifier-free sequence-to-sequence
diffusion frameworks based on encoder-only and
encoder-decoder architectures, respectively.
Although diffusion models have shown their gen-
erative capability on images and audio, its potential
on discriminative tasks has not been explored thor-
oughly. Several pioneer works (Amit et al., 2021;
Baranchuk et al., 2022; Chen et al., 2022) have
made some attempts on diffusion models for ob-
ject detection and semantic segmentation. Our pro-
posed D NER aims to solve an extractive
task on discrete text sequences.
3 Preliminary
In diffusion models, both the forward and reverse
processes can be considered a Markov chain with
progressive Gaussian transitions. Formally, given
a data distribution x∼q(x)and a predefined
variance schedule {β, . . . , β}, the forward pro-
cessqgradually adds Gaussian noise with variance
β∈(0,1)at timestep tto produce latent variables
x,x, . . . ,xas follows:
q(x, . . . ,x|x) =/productdisplayq(x|x) (1)
q(x|x) =N/parenleftig
x;/radicalbig
1−βx, βI/parenrightig
(2)
An important property of the forward process is
that we can sample the noisy latents at an arbitrarytimestep conditioned on the data x. With the nota-
tionα:= 1−βand¯α:=/producttextα, we have:
q(x|x) =N/parenleftbig
x;√¯αx,(1−¯α)I/parenrightbig
(3)
As¯αapproximates 0, xfollows the standard
Gaussian distribution: p(x)≈ N (x;0,I). Un-
like the fixed forward process, the reverse pro-
cessp(x)is defined as a Markov chain with
learnable Gaussian transitions starting at a prior
p(x) =N(x;0,I):
p(x) =p(x)/productdisplayp(x|x)
p(x|x) =N(x;µ(x, t),Σ(x, t))
where θdenotes the parameters of the model and
µandΣare the predicted covariance and mean
ofq(x|x). We set Σ(x, t) =σIand build
a neural network fto predict the data x, denoted
asˆx=f(x, t). Then we have µ(x, t) =
˜µ(x,ˆx) = ˜µ(x, f(x, t)), where ˜µdenotes
the mean of posterior q(x|x,ˆx). The re-
verse process is trained by optimizing a variational
upper bound of −log (p(x)). According to the
derivation in Ho et al. (2020), we can simplify the
training objective of the diffusion model by training
the model f(·)to predict the data x.
4 Method
In this section, we first present the formulation of
diffusion model for NER (i.e., the boundary denois-
ing diffusion process) in § 4.1. Then, we detail the
architecture of the denoising network for boundary
reverse process in § 4.2. Finally, we describe the
inference procedure of D NER in § 4.3.
4.1 Boundary Denoising Diffusion Model
Given a sentence Swith length M, the named
entity recognition task is to extract the entities
E={(l, r, t)}contained in the sentence,
where Nis the number of entities and l, r, tde-
note the left and right boundary indices and type of
thei-th entity. We formulate NER as a boundary
denoising diffusion process, as shown in Figure 2.
We regard entity boundaries as data samples, then
the boundary forward diffusion is to add Gaussian
noise to the entity boundaries while the reverse
diffusion process is to progressively recover the
original entity boundaries from the noisy spans.3877
Boundary Forward Diffusion Boundary for-
ward diffusion is the process of adding noise to
the entity boundary in a stepwise manner. In order
to align the number of entities in different instances,
we first expand the entity set to a fixed number K
(> N ). There are two ways to expand the entities,
repetition strategy andrandom strategy , which add
K−Nentities by duplicating entities or sampling
random spans from a Gaussian distribution. For
convenience, we use B∈Rto denote the
boundaries of the Kexpanded entities, with all of
them normalized by the sentence length Mand
scaled to (−λ, λ)interval.
Formally, given the entity boundaries as data
samples x=B, we can obtain the noisy spans
at timestep tusing the forward diffusion process.
According to Equation (3), we have:
x=√¯αx+√
1−¯αϵ (4)
where ϵ∼ N (0,I)is the noise sampled from
the standard Gaussian. At each timestep, the
noisy spans have the same shape as x, i.e.,
x,x, . . . ,x∈R.
Boundary Reverse Diffusion Starting from the
noisy spans xsampled from the Gaussian distri-
bution, boundary reverse diffusion adopts a non-
Markovian denoising practice used in DDIM (Song
et al., 2021) to recover entities boundaries. Assum-
ingτis an arithmetic subsequence of the com-
plete timestep sequence [1, . . . , T ]of length γwith
τ=T. Then we refine the noisy spans xtoxas follows:
ˆ x=f(x, S, τ) (5)
ˆϵ=x−√αˆ x√1−α(6)
x=√αˆ x+/radicalbig
1−αˆϵ (7)
where ˆ xandˆϵare the predicted entity boundary
and noise at timestep τ.f(x, S, t)is a learnable
denoising network and we will cover the network
architecture in the next section (§ 4.2). After γiter-
ations of DDIM, the noisy spans are progressively
refined to the entity boundaries.
4.2 Network Architecture
Denoising network f(x, S, t)accepts the noisy
spans xand the sentence Sas inputs and predicts
the corresponding entity boundaries ˆ x. As shown
in Figure 2, we parameterize the denoising network
with a sentence encoder and an entity decoder.
Sentence Encoder consists of a BERT (Devlin
et al., 2019) plus a stacked bi-directional LSTM.
The whole span encoder takes the sentence Sas
input and outputs the sentence encoding H∈
R. The sentence encoding Hwill be calcu-
lated only once and reused across all timesteps to
save computations.
Entity Decoder uses the sentence encoding H
to first compute the representations of Knoisy
spans xand then predicts the corresponding entity
boundaries. Specifically, we discretize the noisy
spans into word indexes by rescaling, multiplying
and rounding, then perform mean pooling over the3878Algorithm 1: Training
inner-span tokens. The extracted span representa-
tions can be denoted as H∈R. To further
encode the spans, we design a span encoder that
consists of a self-attention and a cross-attention
layer. The former enhances the interaction between
spans with key, query, and value as H. And the
latter fuses the sentence encoding to the span rep-
resentation with key, value as H, and query as
H. We further add the sinusoidal embedding E
(Vaswani et al., 2017) of timestep tto the span rep-
resentations. Thus the new representations ¯Hof
the noisy spans can be computed:
¯H= SpanEncoder( H,H) +E,
Then we use two boundary pointers to predict
the entity boundaries. For boundary δ∈ {l, r},
we compute the fusion representation H∈
Rof the noisy spans and the words, and
then the probability of the word as the left or right
boundaries P∈Rcan be computed as:
H=HW+¯HW
P= sigmoid(MLP( H))
where W,W∈Rare two learnable ma-
trixes and MLP is a two-layer perceptron. Based
on the boundary probabilities, we can predict the
boundary indices of the Knoisy spans. If the cur-
rent step is not the last denoising step, we compute
ˆ xby normalizing the indices with sentence length
Mand scaling to (−λ, λ)intervals. Then we con-
duct the next iteration of the reverse diffusion pro-
cess according to Equations (5) to (7).
It is worth noting that we should not only lo-
cate entities but also classify them in named entity
recognition. Therefore, we use an entity classi-
fier to classify the noisy spans. The classification
probability P∈Ris calculated as follows:
P= Classifier( ¯H)Algorithm 2: Inference
whereCis the number of entity types and Classifier
is a two-layer perceptron with a softmax layer.
Training Objective With Kentities predicted
from the noisy spans and Nground-truth entities,
we first use the Hungarian algorithm (Kuhn, 1955)
to solve the optimal matching ˆπbetween the two
setsas in Carion et al. (2020). ˆπ(i)denotes the
ground-truth entity corresponding to the i-th noisy
span. Then, we train the boundary reverse process
by maximizing the likelihood of the prediction:
L=−/summationdisplay/summationdisplaylogP/parenleftig
ˆπ(i)/parenrightig
where ˆπ(i),ˆπ(i)andˆπ(i)denote the left and
right boundary indexes and type of the ˆπ(i)entity.
Overall, Algorithm 1 displays the whole training
procedure of our model for an explanation.
4.3 Inference
During inference, D NER first samples
Knoisy spans from a Gaussian distribution,
then performs iterative denoising with the learned
boundary reverse diffusion process based on the
denoising timestep sequence τ. Then with the pre-
dicted probabilities on the boundaries and type, we
can decode Kcandidate entities (l, r, c),
where δ= argmax P, δ∈ {l, r, c}. After that,
we employ two simple post-processing operations
on these candidates: de-duplication and filtering.
For spans with identical boundaries, we keep the
one with the maximum type probability. For spans
with the sum of prediction probabilities less than
the threshold φ, we discard them. The inference
procedure is shown in Algorithm 2.3879
5 Experimental Settings
5.1 Datasets
For nested NER, we choose three widely used
datasets for evaluation: ACE04 (Doddington et al.,
2004), ACE05 (Walker et al., 2006), and GE-
NIA (Ohta et al., 2002). ACE04 and ACE05 belong
to the news domain and GENIA is in the biologi-
cal domain. For flat NER, we use three common
datasets to validate: CoNLL03 (Tjong Kim Sang
and De Meulder, 2003), OntoNotes (Pradhan et al.,
2013), and MSRA (Levow, 2006). More details
about datasets can be found in Appendix B.
5.2 Baselines
We choose a variety of recent advanced methods
as our baseline, which include: 1) Tagging-based
methods (Straková et al., 2019; Ju et al., 2018;
Wang et al., 2020); 2) Span-based methods (Yu
et al., 2020; Li et al., 2020; Wan et al., 2022; Lou
et al., 2022; Zhu and Li, 2022; Yuan et al., 2022b);
3) Generation-based methods (Tan et al., 2021; Yan
et al., 2021b; Lu et al., 2022). More details about
baselines can be found in Appendix D.
5.3 Implementation Details
For a fair comparison, we use bert-large (Devlin
et al., 2019) on ACE04, ACE05, CoNLL03 and
OntoNotes, biobert-large (Chiu et al., 2016) on
GENIA and chinese-bert-wwm (Cui et al., 2020)
on MSRA. We adopt the Adam (Kingma and Ba,2015) as the default optimizer with a linear warmup
and linear decay learning rate schedule. The peak
learning rate is set as 2e−5and the batch size is 8.
For diffusion model, the number of noisy spans K
(K) is set as 60, the timestep Tis 1000, and the
sampling timestep γis 5 with a filtering threshold
φ= 2.5. The scale factor λfor noisy spans is 1.0.
Please see Appendix C for more details.
6 Results and Analysis
6.1 Performance
Table 1 illustrates the performance of D -
NER as well as baselines on the nested NER
datasets. Our results in Table 1 demonstrate that
D NER is a competitive NER method,
achieving comparable or superior performance
compared to state-of-the-art models on the nested
NER. Specifically, on ACE04 and GENIA datasets,
D NER achieves F1 scores of 88.39%
and 81.53% respectively, with an improvement of
+0.77% and +0.41%. And on ACE05, our method
achieves comparable results. Meanwhile, D - NER also shows excellent performance on flat
NER, just as shown in Table 2. We find that D- NER outperforms the baselines on OntoNotes
with +0.16% improvement achieves a comparable
F1-score on both the English CoNLL03 and Chi-
nese MSRA. These improvements demonstrate that
ourD NER can locate entities more ac-
curately due to the benefits of progressive bound-3880
ary refinement, and thus obtain better performance.
The results also validate that our D NER
can recover entity boundaries from noisy spans via
boundary denoising diffusion.
6.2 Analysis
Inference Efficiency To further validate whether
ourD NER requires more inference com-
putations, we also conduct experiments to compare
the inference efficiency between D NER
and other generation-based models (Lu et al., 2022;
Yan et al., 2021a). Just as shown in Table 3, we find
thatD NER could achieve better perfor-
mance while maintaining a faster inference speed
with minimal parameter scale. Even with a denois-
ing timestep of γ= 10 ,D NER is 18×
and 3×faster than them. This is because D - NER generates all entities in parallel within
several denoising timesteps, which avoids generat-
ing the linearized entity sequence in an autoregres-
sive manner. In addition, D NER shares
sentence encoder across timesteps, which further
accelerates inference speed.
Denoising Timesteps We also conduct experi-
ments to analyze the effect of different denois-
ing timesteps on model performance and inference
speed of D NER under various numbers
of noisy spans. Just as shown in Figure 3, we
find that, with an increase of denoising steps, the
model obtains incremental performance improve-
ment while sacrificing inference speed. Consid-
ering the trade-off between performance and ef-
ficiency, we set γ= 5 as the default setting. In
addition, when the noisy spans are smaller, the
improvement brought by increasing the denoising
timesteps is more obvious. This study indicates that
our DiffusionNER can effectively counterbalance
the negative impact of undersampling noise spans
on performance by utilizing additional timesteps.
Sampling Number As a generative latent model,
D NER can decouple training and eval-3881uation, and dynamically sample noisy spans dur-
ing evaluation. To manifest this advantage, we
train D NER on ACE04 with K= 60
noisy spans and evaluate it with different sam-
pling numbers K. The results are shown in
Figure 4. Overall, the model performance becomes
better as the sampling number of noisy spans in-
creases. Specifically, we find that D NER
performs worse when K<30. We guess this is
because fewer noisy spans may not cover all poten-
tial entities. When sampling number K>60,
we find it could also slightly improve model per-
formance. Overall, the dynamic sampling of noisy
spans in D NER has the following advan-
tages: 1) we can improve model performance by
controlling it to sample more noisy spans; 2) dy-
namic sampling strategy also allows the model to
predict an arbitrary number of entities in any real-
world application, avoiding the limitations of the
sampling number at the training stage.
6.3 Ablation Study
Network Architecture As shown in Table 4, we
conduct experiments to investigate the network ar-
chitecture of the boundary reverse diffusion pro-
cess. We found that D NER performs
better with a stronger pre-trained language model
(PLM), as evidenced by an improvement of +0.53%
on ACE04 and +0.11% on CoNLL03 when us-
ingroberta-large . Additionally, for the span en-
coder, we find that directly removing self-attention
between noisy spans or cross-attention of spans
to the sentence can significantly impair perfor-
mance. When both are ablated, model performance
decreases by 1.37% and 1.15% on ACE04 and
CoNLL03. These results indicate that the inter-
action between the spans or noisy spans and the
sentence is necessary.
Variance Scheduler The variance scheduler
plays a crucial role in controlling the intensity of
the added noise at each timestep during boundary
forward diffusion process. Therefore, we analyze
the performance of D NER on different
variance schedulers with different noise timesteps
T. The results on ACE04 and CoNLL03 are shown
in Table 5. We find that the cosine scheduler gen-
erally yields superior results on the ACE04, while
the linear scheduler proves to be more effective on
CoNLL03. In addition, the performance of D- NER varies with the choice of noise timestep,
with the best performance achieved at T= 1000
for ACE04 and T= 1500 for CoNLL03.
Expansion Stratagy The expansion stratagy of
the entity set can make the number of Knoisy
spans consistent across instances during training.
We conduct experiments to analyze the perfor-
mance of D NER for different expansion
strategies with various numbers of noisy spans. The
experimental results are shown in Table 6. Gener-
ally, we find that the random strategy could achieve
similar or better performance than the repetitive
strategy. In addition, Table 6 shows that D - NER is insensitive to the number of noisy
spans during training. Considering that using more
noisy spans brings more computation and memory
usage, we set K= 60 as the default setting.
7 Conclusion
In this paper, we present D NER , a novel
generative approach for NER that converts the task
into a boundary denoising diffusion process. Our
evaluations on six nested and flat NER datasets3882show that D NER achieves comparable
or better performance compared to previous state-
of-the-art models. Additionally, our additional anal-
yses reveal the advantages of D NER in
terms of inference speed, progressive boundary re-
finement, and dynamic entity sampling. Overall,
this study is a pioneering effort of diffusion models
for extractive tasks on discrete text sequences, and
we hope it may serve as a catalyst for more research
about the potential of diffusion models in natural
language understanding tasks.
Limitations
We discuss here the limitations of the proposed D- NER . First, as a latent generative model,
D NER relies on sampling from a Gaus-
sian distribution to produce noisy spans, which
leads to a random characteristic of entity genera-
tion. Second, D NER converges slowly
due to the denoising training and matching-based
loss over a large noise timestep. Finally, since
discontinuous named entities often contain multi-
ple fragments, D NER currently lacks the
ability to generate such entities. We can design a
simple classifier on top of D NER , which
is used to combine entity fragments and thus solve
the problem of discontinuous NER.
Acknowledgments
This work is supported by the Key Research and
Development Program of Zhejiang Province, China
(No. 2023C01152), the Fundamental Research
Funds for the Central Universities (No. 226-2023-
00060), and MOE Engineering Research Center of
Digital Library.
References3883388438853886A Optimal Matching ˆπ
Given a fixed-size set of Knoisy spans, D - NER infers Kpredictions, where Kis larger
than the number of Nentities in a sentence. One
of the main difficulties of training is to assign the
ground truth to the prediction. Thus we first pro-
duce an optimal bipartite matching between pre-
dicted and ground truth entities and then optimize
the likelihood-based loss.
Assuming that ˆY={ˆY}are the set of K
predictions, where ˆY=/parenleftbig
P,P,P/parenrightbig
. We de-
note the ground truth set of Nentities as Y=
{(l, r, c)}, where l, r, care the boundary
indices and type for the i-th entity. Since Kis
larger than the number of Nentities, we pad Y
with∅(no entity). To find a bipartite matching
between these two sets we search for a permutation
ofKelements π∈S(K)with the lowest cost:
ˆπ= arg min/summationdisplayL/parenleftig
ˆY, Y/parenrightig
where L/parenleftig
ˆY, Y/parenrightig
is a pair-wise matching
cost between the prediction ˆYand ground truth
Ywith index π(i). We define it as −1(Y̸=
∅)/summationtextP/parenleftig
Y/parenrightig
, where 1(·)denotes an
indicator function. Finally, the optimal assignment
ˆπcan be computed with the Hungarian algorithm.
B Datasets
We conduct experiments on six widely used NER
datasets, including three nested and three flat
datasets. Table 7 reports detailed statistics about
the datasets.
ACE04 and ACE05 (Doddington et al., 2004;
Walker et al., 2006) are two nested NER datasets
and contain 7 entity categories, including PER,ORG,
LOC,GPE,WEA,FACandVEHcategories. We fol-
low the same setup as previous works Katiyar and
Cardie (2018); Lin et al. (2019).
GENIA (Ohta et al., 2002) is a biology nested
NER dataset and contains 5 entity types, including
DNA,RNA,protein ,cell line andcell type
categories. Follow Huang et al. (2022); Shen et al.
(2021a), we train the model on the concatenation
of the train and dev sets.
CoNLL03 (Tjong Kim Sang and De Meulder,
2003) is a flat dataset with 4 types of named entities:LOC,ORG,PERandMISC . Follow Yu et al. (2020);
Yan et al. (2021c); Shen et al. (2021a), we train our
model on the combination of the train and dev sets.
OntoNotes (Pradhan et al., 2013) is a flat dataset
with 18 types of named entities, including 11 entity
types and 7 value types. We use the same train,
development, and test splits as Li et al. (2020);
Shen et al. (2022).
MSRA (Levow, 2006) is a Chinese flat dataset
with 3 entity types, including ORG,PER,LOC. We
keep the same dataset splits and pre-processing
with Li et al. (2022a); Shen et al. (2021a).
C Detailed Parameter Settings
Entity boundaries are predicted at the word level,
and we use max-pooling to aggregate subwords
into word representations. We use the multi-headed
attention with 8 heads in the span encoder, and add
a feedforward network layer after the self-attention
and cross-attention layer. During training, we first
fix the parameters of BERT and train the model
for5epochs to warm up the parameters of the
entity decoder. We tune the learning rate from
{1e−5,2e−5,3e−5}and the threshold φfrom
range [2.5,2.7]with a step 0.05, and select the
best hyperparameter setting according to the per-
formance of the development set. The detailed
parameter settings are shown in Table 8.
D Baselines
We use the following models as baselines:
•LinearedCRF (Straková et al., 2019) concate-
nates the nested entity multiple labels into one
multilabel, and uses CRF-based tagger to de-
code flat or nested entities.
•CascadedCRF (Ju et al., 2018) stacks the flat
NER layers and identifies nested entities in an
inside-to-outside way.
•Pyramid (Wang et al., 2020) constructs the
representations of mentions from the bottom
up by stacking flat NER layers in a pyramid,
and allows bidirectional interaction between
layers by an inverse pyramid.
•Seq2seq (Straková et al., 2019) converts the
labels of nested entities into a sequence and
then uses a seq2seq model to decode entities.3887
•BARTNER (Yan et al., 2021b) is also a
sequence-to-sequence framework that trans-
forms entity labels into word index sequences
and decodes entities in a word-pointer manner.
•Seq2Set (Tan et al., 2021)treats NER as a
sequence-to-set task and constructs learnable
entity queries to generate entities.
•UIE (Lu et al., 2022) designs a special schema
for the conversion of structured information to
sequences, and adopts a generative model to
generate linearized sequences to unify variousinformation extraction tasks.
•Biaffine (Yu et al., 2020) reformulates NER
as a structured prediction task and adopts a
dependency parsing approach for NER.
•MRC (Li et al., 2020) reformulates NER as
a reading comprehension task and extracts
entities to answer the type-specific questions.
•Locate&label (Shen et al., 2021a) is a two-
stage method that first regresses boundaries to
locate entities and then performs entity typing.
•SpanGraph (Wan et al., 2022) utilizes a
retrieval-based span-level graph to improve
the span representation, which can connect
spans and entities in the training data.
•LLCP (Lou et al., 2022) treat NER as latent
lexicalized constituency parsing and resort to
constituency trees to model nested entities.
•BoundarySmooth (Zhu and Li, 2022), in-
spired by label smoothing, proposes boundary
smoothing for span-based NER methods.
•Triffine (Yuan et al., 2022b) proposes a tri-
affine mechanism to integrate heterogeneous
factors to enhance the span representation, in-
cluding inside tokens, boundaries, labels, and
related spans.
•Word2Word (Li et al., 2022a) treats NER
as word-word relation classification and uses
multi-granularity 2D convolutions to con-
struct the 2D word-word grid representations.3888ACL 2023 Responsible NLP Checklist
A For every submission:
/squareA1. Did you describe the limitations of your work?
Limitation Section
/squareA2. Did you discuss any potential risks of your work?
Limitation Section
/squareA3. Do the abstract and introduction summarize the paper’s main claims?
Abstract Section and Introduction Section
/squareA4. Have you used AI writing assistants when working on this paper?
Left blank.
B/squareDid you use or create scientiﬁc artifacts?
Section 4.1 and Appendix B
/squareB1. Did you cite the creators of artifacts you used?
Section 4.2 and Appendix B
/squareB2. Did you discuss the license or terms for use and / or distribution of any artifacts?
Section 4.2 and Appendix B
/squareB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
Section 4.2 and Appendix B
/squareB4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
/squareB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Section 4.2 and Appendix B
/squareB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
Section 4.2 and Appendix B
C/squareDid you run computational experiments?
Section 5
/squareC1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
No response.3889/squareC2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
Appendix C
/squareC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
No response.
/squareC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Appendix B
D/squareDid you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
/squareD1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
No response.
/squareD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
No response.
/squareD3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
No response.
/squareD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
No response.
/squareD5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
No response.3890