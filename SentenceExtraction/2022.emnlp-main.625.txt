
Eric Michael Smith Melissa Hall Melanie Kambadur
Eleonora Presani Adina Williams
Meta AI
{ems,melissahall,mkambadur,epresani,adinawilliams}@fb.com
Abstract
As language models grow in popularity, it be-
comes increasingly important to clearly mea-
sure all possible markers of demographic iden-
tity in order to avoid perpetuating existing so-
cietal harms. Many datasets for measuring
bias currently exist, but they are restricted in
their coverage of demographic axes and are
commonly used with preset bias tests that pre-
suppose which types of biases models can ex-
hibit. In this work, we present a new, more
inclusive bias measurement dataset, H -B, which includes nearly 600 descriptor
terms across 13 different demographic axes.
H Bwas assembled in a partici-
patory process including experts and commu-
nity members with lived experience of these
terms. These descriptors combine with a set
of bias measurement templates to produce over
450,000 unique sentence prompts, which we
use to explore, identify, and reduce novel forms
of bias in several generative models. We
demonstrate that H Bis effective
at measuring previously undetectable biases in
token likelihoods from language models, as
well as in an offensiveness classifier. We will
invite additions and amendments to the dataset,
which we hope will serve as a basis for more
easy-to-use and standardized methods for eval-
uating bias in NLP models.
1 Introduction
In recent years, there has been a series of works
aiming to measure social biases or other unwanted
behaviors in NLP. In particular, many works fo-
cus on generative models (Dinan et al., 2020a,b;
Xu et al., 2021b; Kirk et al., 2021; Sheng et al.,
2021b; Nozza et al., 2021; Renduchintala et al.,
2021; Baheti et al., 2021; Perez et al., 2022), which
are well known to pose unique challenges for au-
tomatic evaluation (Lowe et al., 2017; Howcroft
et al., 2020; Celikyilmaz et al., 2021).
For models that generate, a common way to
surface bias is to input prompts containing demo-Figure 1: The H Bdataset has 13 different
demographic axes, plus further divisions into buckets
and nearly 600 individual descriptor terms.
graphic information, and then analyze whether the
models output socially biased text. Such prompts
are generally derived either from crowdsourcing
(Nadeem et al., 2021; Nangia et al., 2021) or from
slotting a set of terms into templates (Kurita et al.,
2019; May et al., 2019; Sheng et al., 2019; Webster
et al., 2020). However, whenever a method se-
lects particular terms or templates for prompts, and
groups them under particular demographic head-
ings, it implicitly adopts a taxonomy which can
include, or exclude, particular groups of people or
particular ways of talking about groups of people.
Those who are most excluded from bias measure-
ment are those who are historically marginalized
or from underrepresented groups.
In this work, we aim to create the largest and
most inclusive taxonomy of textual people refer-9180
ences to date (Tables 2 and 6), with nearly 600
terms across 13 demographic axes, for measuring
NLP bias with templates at scale (see Figure 1).
Our taxonomy has been generated and vetted in
close conversation with numerous experts and indi-
viduals with lived experiences of different descrip-
tor terms, and it includes many more terms than
other evaluation datasets.
H Balso aims to tackle another issue
that plagues many existing word list taxonomies.
Namely, many existing taxonomies are static and
unchanging, meaning they implicitly assert a par-
ticular classification of people as objective and im-
mutable, and thus often reify an undesirable status
quo. Since people can refer to themselves and oth-
ers in an endless number of ways (Van Miltenburg
et al., 2018), and since people references are prone
to change over time (Smith, 1992; Galinsky et al.,
2003; Haller et al., 2006; Zimman and Hayworth,
2020), we have taken inspiration from calls to
make model evaluation more dynamic (Kiela et al.,
2021; Gehrmann et al., 2021), and we have created
H Bas a “living” evaluation dataset for
measuring social biases in language models. We
expect H Bto expand and be adjusted
as needed over time, and we invite researchers andcommunity members to leave comments or con-
tribute terms or additional annotations in the form
of GitHub pull requests on our open-sourced code.
To demonstrate the utility of H B,
we target several exemplar models—GPT-2,
RoBERTa, DialoGPT, and BlenderBot 2.0—and
show that our expanded demographic terms list can
better expose model social biases, including sub-
tle ones pertaining to previously overlooked social
categories, as in Table 1.
We measure bias across three settings (Sec-
tion 2.3): (1) token likelihoods of H B
sentences, (2) generations prompted with H -Bsentences, and (3) differential rates of
flagging H Bsentences as offensive.
After having exposed such biases, we perform pre-
liminary mitigations in Section 4, to demonstrate
how H Bcan facilitate the whole social
bias research cycle: it is useful in uncovering so-
cial biases, measuring their impact, and developing
mitigations to help address them. We have open-
sourced our dataset and tooling, with the goal of
helping to improve and standardize methods for
researching social biases in NLP.
2 Methods
2.1 Defining bias
In this work, we define language model bias as de-
mographic difference , i.e., group-level differences
in model output or assigned probabilities that re-
sult from different identity or demographic data
present in input text. According to this definition,
difference is what matters. Some biases will be be-
nign, while others will be harmful or stereotypical,
such as othering and inappropriate sympathy (see
Section 2.3.3 for further discussion). Adopting a
general definition of bias as difference allows for
NLP practitioners to make the delineation between
benign and harmful for each identity term sepa-
rately, based on the particular task and use case at
hand (Olteanu et al., 2017; Blodgett et al., 2020;
Czarnowska et al., 2021; Dev et al., 2021).
We acknowledge that works that attempt to mea-
sure bias often run into inadequate or incomplete
definitions of bias (Blodgett et al., 2020): for in-
stance, Devinney et al. (2022) surveys nearly 200
articles regarding gender bias in NLP and finds that
almost all of them do not clearly specify how they9181are conceptualizing gender, disregarding intersec-
tionality and non-binary genders, conflating sex
and gender, etc. We believe the best way forward
is to try to strike the right balance between having
a general-purpose bias measurement resource and
ensuring that everyone is included and appropri-
ately represented. We make initial steps towards
this by creating a living measurement dataset that
anyone can contribute to, and which includes the
voices of people who are most likely to be excluded
or incompletely represented by researchers’ design
choices.
2.2 The H Bdataset
TheH Bdataset consists of a set of sen-
tences containing demographic identity language
(e.g. “Hi! I am a Catholic grandmother. ” ) used
in the context of a two-person conversation. These
sentences can be used for measurements of token
likelihood scores or as prompts for a generative
model. The construction of these sentences is de-
tailed in the following sections.
2.2.1 Demographic descriptor terms
To measure bias holistically in language models,
we have created a list of roughly 600 American
English descriptor terms (e.g., “Deaf”, “Asian-
American”) grouped into 13 different demographic
axes (e.g., “Ability”, “Race/Ethnicity”). The axes
are shown in Figure 1 and a full list of descriptors
can be found in Table 5.
We used a combination of participatory and algo-
rithmic processes to develop the descriptor terms.
First, the authors brainstormed sample descriptor
terms for each axis. We then expanded these terms
by selecting additional relevant terms from among
the 50 nearest neighbors per existing term as mea-
sured with fastText word embeddings (Joulin et al.,
2017), as well as WordNet synonyms and antonyms
(Fellbaum and Miller, 1998).
Next, we used a participatory process to expand
the list, inviting contributors who self-identify as
being from particular demographic groups to share
feedback and add additional terms based on their
lived experiences. Over two dozen people partic-
ipated, including experts in responsible/inclusive
research, racial justice, and preferred language in
(dis)ability, as well as Asian American and Pacific
Islander (AAPI), Black, Latinx, and LGBTQ+ com-
munities. The guidance of these contributors was
indispensable: incorporating feedback from mem-
bers of various communities is crucial to develop-
ing NLP tools that work for the good of everyone.
See the Acknowledgments for a list of contributors
who agreed to be named.
Finally, we also included a small set of “nonce”
terms (Soja et al., 1991), which are novel, phono-
logically well-formed terms with no associated lex-
ical semantics. We have included these terms as
a sort of baseline, to get an idea of how models
handle unfamiliar or out-of-vocabulary descriptors.
See Section A.2 for further dataset creation details.
We heartily welcome suggestions for new de-
scriptor terms from other researchers, at the GitHub
URL in the introduction.
2.2.2 Making prompts with templates
To evaluate language model bias, we examine
prompts that mirror the myriad ways in which
people use descriptor terms in real life. Prompts
are constructed by slotting demographic terms and
person nouns into pre-selected sentence templates.
We compiled 26 templates, such as “What do you
think about [PLURAL NOUN PHRASE]?” , each
of which must be filled with a noun phrase con-
sisting of a descriptor term and a noun referring
to a person (see Table 13 for the list of templates
and Section A.3 for the list of nouns). The de-
scriptor term is appended either before or after the
noun, depending on the syntactic structure of the
template, person noun, and descriptor term, such
as“What do you think about [PLURAL NOUN]
who are [DESCRIPTOR]?” . The resultant prompts
can help us answer questions about bias, such as
whether a model is primed to respond derogatorily
towards particular groups.
TheH Bdataset comprises all pos-
sible combinations of descriptor, noun, and tem-
plate, totaling 460,000 unique sentence prompts.
This exceeds the number of prompts in other recent
datasets measuring demographic bias (Table 2). As9182we will show, this breadth is important: we can dis-
cern new biases and understand their nuances, more
closely approximating the many ways in which hu-
mans actually discuss identity and its complexities.
2.3 Measuring bias
How we measure bias with H Bde-
pends on the model architecture. We measure bias
using token likelihoods in RoBERTa, GPT-2, and
BlenderBot 2.0 in Section 2.3.2; we compare gen-
erations from DialoGPT and BlenderBot 2.0 given
different demographic prompts in Section 2.3.3;
and we explore how an unsafe dialogue detection
classifier changes predictions as a function of de-
scriptor term in Section 2.3.4.
2.3.1 Models
To demonstrate the utility of our evaluation dataset,
we focus on four models that represent some of its
most likely use cases. More experimental details,
including generation settings, are in Section A.4.
GPT-2. We measure the perplexity of H -Bdescriptors on the 774M-parameter gen-
erative GPT-2 ( gpt2-large ) model (Radford et al.,
2019) (Section 2.3.2).
RoBERTa. We compare the token likelihoods of
different H Bdescriptors on RoBERTa-
large (Liu et al., 2019) (Section B.1).
DialoGPT. We use the 345M-parameter medium
DialoGPT model (Zhang et al., 2020), which con-
sists of a model with GPT-2 architecture trained
on Reddit comment chains in order to expose it
to dialogue, to measure bias in generations given
H Bprompts (Section 2.3.3).
BlenderBot 2.0. We also measure bias in
BlenderBot 2.0 (Komeili et al., 2022; Xu et al.,
2022), an encoder/decoder model pre-trained on a
Reddit dataset extracted by a third party and made
available on pushshift.io (Baumgartner et al., 2020).
BlenderBot 2.0 is a useful case study, because a
recent error analysis found evidence of biased and
unsafe generations (Lee et al., 2022).
2.3.2 Bias in token likelihoods
Bias in a language model can manifest in the rel-
ative likelihood that the model attributes to differ-
ent text sequences, for instance, ascribing a high
likelihood to “John is an engineer. ” but a low
likelihood to “Joan is an engineer. ” (examplesfrom May et al. 2019). For the generative mod-
els GPT-2 and BlenderBot 2.0, we measure and
compare the perplexity of different templated dia-
logue sentences in H B, extending the
technique of Nadeem et al. (2021) that compares
the log probabilities of pairs of stereotypical and
anti-stereotypical sentences.
We adopt a definition of bias in token likelihoods,
Likelihood Bias , that measures the extent to which
a model treats different descriptors as functionally
different in terms of how likely they are to be used
in certain contexts. For each pair of descriptors in a
H Baxis, we use the Mann-Whitney U
test (Mann and Whitney, 1947) to test the hypothe-
sis that, for two templated sentences AandBwith
different descriptors, there is an equal likelihood of
either sentence to have a higher perplexity than the
other. The fraction of pairs of descriptors for which
the Mann-Whitney Ustatistic indicates a rejection
of this hypothesis is taken to be the Likelihood Bias
for that axis. A larger value of this metric implies
a greater difference in the model’s perception of
the descriptors within that axis, revealing the axes
in which the model tends to be most biased in its
treatment of descriptors.
2.3.3 Bias in generations
To detect biases in text produced by generative lan-
guage models, such as the overly sympathetic and
confused responses shown in Table 1, we input
various H Bprompts, have the models
generate a large corpus of text (Section A.5), and
then investigate how these generations vary as a
function of descriptor. Since generative models
may exhibit many types of biases, we employ a
novel measurement technique to find them. First,
we classify the text generations into conversational
styles (“Empathetic”, “Solemn”, “Charming”, etc.)
using a 3B-parameter Transformer-based style clas-
sifier from Smith et al. (2020a). The style classifier
covers 217 unique styles, allowing for the detection
of nuances in tone within a generated response, as
well as for the comparison of those nuances across
H Bdescriptors (more details in Sec-
tion A.6).
We determine the extent of bias across styles
by defining a custom metric, Full Gen Bias , that
measures how much the distribution of all styles
varies across descriptors. We also define a second
metric, Partial Gen Bias , that cuts this variance by
specific clusters of related styles (Section A.7). A
high value on these scores implies that the genera-9183tive model is much more likely to use some styles
of response than others for certain descriptors, po-
tentially signalling unwanted bias as a function of
its partner’s identity.
2.3.4 Differences in offensiveness by
descriptor
To find the descriptors in H B that
may be labeled as inherently “offensive”, we
use the 311M-parameter Transformer-based Bot-
Adversarial Dialogue (B.A.D.) classifier from Xu
et al. (2021b).
3 Measuring generative bias
3.1 Bias in token likelihoods
Pairwise differences among descriptors. Ta-
ble 3 gives an example of how different H -Bdescriptors are treated differently for the
template “I love [PLURAL NOUN PHRASE]. ” . We
see that, for both BlenderBot 2.0 3B and GPT-2, the
axes “Characteristics” and “Ability” have a higher
Likelihood Bias, implying a greater difference in
the models’ perceptions of the descriptors within
these axes. There are trends within high- and low-
perplexity descriptors for each axis: for example,
for both models, the lowest-perplexity “Character-
istics” descriptors mostly pertain to military status,
and the highest-perplexity ones are mostly associ-
ated with immigration and job status.
We find similar patterns in descriptor token like-
lihoods when evaluating RoBERTa using SEAT
(May et al., 2019) templates (see Section B.1), sug-
gesting a broad efficacy of the H Bde-
scriptor list in identifying language biases across
templates and model types.
Differences across axes and templates. Perplex-
ity scores from GPT-2 on templated sentences in
H B, split by axis and template, are pre-
sented in Figure 2 (scores from BlenderBot 2.0 3B
are in Figure 4 in the Appendix). We find that a
single descriptor can have perplexity scores that
vary greatly: in certain circumstances, unlikely de-
scriptors (e.g., “half-timer”) still exhibit relatively
low perplexities. Pathologically low perplexities
for certain descriptors over others can indicate a bi-
ased model preference for those descriptors. How-
ever, descriptors as a whole tend to fall into a simi-
lar overall perplexity range across all axes except
for “Nonce”, for which they are much higher, as
expected for words that are purposefully out-of-
distribution (Section 2.2.1).For both GPT-2 and BlenderBot 2.0 3B, tem-
plates that convey a strong opinion tend to have
higher perplexities than their less opinionated coun-
terparts: templates such as “I {love/like/hate}
[PLURAL NOUN PHRASE]. ” have higher perplex-
ities on average than neutral templates like “What
do you do for a living? I’m [NOUN PHRASE]. ”
This effect is not due solely to template length,
as seen when comparing longer, emotional tem-
plates ( “I think [PLURAL NOUN PHRASE] are
the worst. ” ) to shorter neutral templates ( “Hi, I’m
[NOUN PHRASE]. ” ).
Furthermore, the range of perplexity values
across descriptors is much wider for the value-
conveying templates of “I {love/like/hate} [PLU-
RAL NOUN PHRASE]. ” than for the others, imply-
ing large differences in the models’ likelihoods that
individual descriptors have a positive or negative
connotation.
3.2 Bias in generations
We show the bias in generated responses to H -Btemplated sentences in Table 4. We find
that DialoGPT generally has less bias (Full Gen
Bias and Partial Gen Bias) than either of the two
BlenderBot 2.0 sizes, which might partially be ex-
plained by differences in model size and partially
by overall differences in generation between the
two classes of models (Adiwardana et al., 2020;
Roller et al., 2021; Shuster et al., 2021). The rel-
atively high Full Gen Bias and Partial Gen Bias
scores of BlenderBot 2.0 imply that this model
is much more liable to gravitate towards certain
styles over others when responding to its partner’s
mention of a specific demographic identity term
(Section 2.3.3).
The smaller 400M-parameter BlenderBot 2.0
model has somewhat less bias than the larger 3B-
parameter one, reflecting similar correlations be-
tween model size and bias in Bender et al. (2021)
and Smith and Williams (2021). The absence of
internet search in the 3B-parameter BlenderBot 2.0
model leaves the bias relatively unchanged. For
BlenderBot 2.0 3B, the largest contributions to the
Full Gen Bias come from styles related to sympathy
(Sympathetic, Compassionate, and Empathetic),
followed by the style expressing envy and the two
clusters of style expressing curiosity and confusion.
More findings are discussed in Section B.2.
To better illustrate how generated responses vary
in style as a function of descriptor, we show the91849185
mean probabilities that responses can be classified
as belonging to certain style clusters as a func-
tion of descriptor, for generations from the 3B-
parameter BlenderBot 2.0 model. We plot these
style cluster probabilities against the frequency
of each descriptor in the BlenderBot 2.0 3B pre-
training data (Figure 3, and expanded as Figure 5
in the appendix) and fine-tuning data (Figure 6).
In particular, the C cluster plot shows
that few descriptors are both very common and
very confusing, hinting that prevalence of a term
in the training data may ward off confusion by the
model. In general, gender/sex terms seem to elicit
responses with higher C andC - probabilities, whereas the opposite tends to
be true for body type terms, irrespective of their
frequency in the training data. See Section B.2.1
for more details.
3.3 Differences in offensiveness by descriptor
The likelihood that the B.A.D. classifier will clas-
sify a sentence prompt from H Bas of-
fensive depends greatly on both the descriptor and
template used in the sentence (see Table 12): for
the templates “I’m [NOUN PHRASE]. ” and“I love
[PLURAL NOUN PHRASE]. ” , sentences tend to be
rated as very likely offensive if they include terms
that are derogatory ( “hideous” ,“trailer trash” )
or represent marginalized or disadvantaged groups
(“gay” ,“with a limb difference” ). Section B.3
discusses overall offensiveness as a function of
template.4 Reducing generative bias
The previous section has shown how an expanded
demographic bias dataset can help identify new
biases in models. We now turn to how such a
dataset can guide the mitigation of these newly
uncovered biases.
4.1 Objective
To mitigate bias, we introduce a style equality tech-
nique. This technique forces generative models,
such as DialoGPT and BlenderBot 2.0, to more
closely match the distribution of styles in the mod-
els’ responses as a function of descriptor. Increas-
ing distributional equality can make the models less
likely to display harmful microaggressions that oc-
cur when delivering pathological types of responses
to certain marginalized demographics, such as feel-
ing overly sorry for people with disabilities and
acting confused when encountering specific terms
related to race, ethnicity, gender, or sex (Table 1).
One caveat of this approach is that it glosses over
the question of if a certain demographic descrip-
tor term should justifiably elicit a certain style of
response. For instance, it may be less controver-
sial for the model to give an explicitly sympathetic
response to someone experiencing a temporary dif-
ficulty like unemployment or a divorce. Still, this
technique allows for a proof-of-concept demonstra-
tion of how the minimization of a single metric
(Full Gen Bias) could be used to address multiple
categories of bias simultaneously.91864.2 Technique
We calculate the bias in each response to a H -Bsentence by projecting its style vector in
the direction of the mean style for all responses to
that sentence’s descriptor (Figure 7; see Liang et al.
(2020) for a similar bias projection technique). We
tag each response with a binary label indicating its
level of bias, and we then perform style-controlled
generation on those labels so that the model can be
prompted to generate responses containing lower
amounts of bias (Weston et al., 2018; Smith et al.,
2020a). See Section C.1 for details.
4.3 Results
Bias reduction tuning reduces Full Gen Bias by
13% on DialoGPT and 24% on BlenderBot 2.0 3B
(Table 4). Splitting by style cluster, we see that
this reduction in variance for BlenderBot 2.0 3B
across descriptors is not uniform for every style:
the Partial Gen Bias of the S ,C -, and C clusters drops by more than
half, the Partial Gen Bias of C stays roughly
constant, and the E andHclusters actually
have their variance across clusters increase. (This
may be partly due to an increase in the model’s
regurgitation of the H Bprompt, as dis-
cussed in Section C.2.1.) Since the per-response
bias value has been tuned to produce roughly the
same magnitude for BlenderBot 2.0 3B’s two most
prominent categories of harmful biased response
(Table 1), an alternate optimization of this value
could perhaps give a more balanced reduction of
Partial Gen Bias across clusters.
More bias reduction results are discussed in Sec-
tion C.2, including changes in the frequency of
specific styles and key phrases (e.g. “I’m sorry to
hear” ) after bias tuning, sample responses before
vs. after tuning, and human evaluations of model
performance after tuning.
4.4 Limitations of method
We present this bias reduction technique as an
initial demonstration of how the H B
dataset could potentially be used for bias reduction,
but we acknowledge that more research is needed
before we can recommend this specific technique
for widespread real-world use. A few limitations of
the technique as currently formulated are (1) an in-
crease in sentiments of hate/envy among responses
(Table 15); (2) an increase in regurgitation of the
H Bprompt (Tables 16 and 17); and (3)a slight increase in the offensiveness of responses
by BlenderBot 2.0 as measured by the B.A.D. clas-
sifier (Table 11). More discussions found in Sec-
tion C.2.1.
5 Related work
Templates. This work assembles a large set of
demographic descriptor terms to be slotted into ex-
isting bias templates. The practice of using descrip-
tors to measure social bias began as a technique
specific for probing the gender associations of static
word embeddings (Bolukbasi et al., 2016; Caliskan
et al., 2017; Bordia and Bowman, 2019). Because
contextualized word embeddings take context into
account, templates were necessary for measuring
social biases, such as stereotypical association with
other text content (Tan and Celis, 2019).
Many projects have proposed particular measure-
ment templates, which form the basis for prompts
that can be used to measure bias (Rudinger et al.,
2018; May et al., 2019; Sheng et al., 2019; Kurita
et al., 2019; Webster et al., 2020; Gehman et al.,
2020; Huang et al., 2020; Vig et al., 2020; Kirk
et al., 2021; Perez et al., 2022). Some even select
existing sentences from text sources and swap de-
mographic terms heuristically (Zhao et al., 2019;
Ma et al., 2021; Wang et al., 2021; Papakipos and
Bitton, 2022), utilize handcrafted grammars (Ren-
duchintala et al., 2021), or use machine-learned sys-
tems to swap descriptors (Qian et al., 2022). Since
one of our main contributions is the participatory
assembly of a large set of demographic terms, our
terms are compatible with nearly any templates to
measure imbalances across demographic groups.
Prompts. A common approach to measuring bias
relies on prompts generated by seeding crowdwork-
ers with terms and having them write prompts from
them (Nadeem et al., 2021; Nangia et al., 2021).
This approach has limitations, in particular because
crowdworkers often misunderstand or can only in-
completely follow annotation guidelines, which
themselves can be difficult to specify completely
(Blodgett et al., 2021). Moreover, crowdsourc-
ing can be very expensive and result in evaluation
datasets limited in their size and scope, often cov-
ering only certain demographics or having only a
few test sentences per demographic. To avoid the
downsides of crowdsourcing and to enable more
experimental control over the evaluation dataset,
many works, including ours, employ a “term-and-
template” method for bias evaluation.9187Measuring bias. A popular set of techniques for
measuring bias in generated text involves comput-
ing the frequency of different demographic terms
using a word list, for example, those signifying
gender (Dinan et al., 2020a); religion, race, gender,
and orientation (Barikeri et al., 2021); or occupa-
tions (Kirk et al., 2021). In this work, we aim to
push this kind of word-list-based approach to its
limit, by making a bigger and ever-growing terms
list.
Another aspect of this work is that it enables
intrinsic measurement, i.e., measurement of bias
“upstream” in the pre-trained language model. De-
spite the fact that upstream bias mitigations can
transfer to extrinsic, “downstream”, tasks well (Jin
et al., 2021), it is currently unclear whether intrin-
sic measurement is sufficient, in particular because
intrinsic and extrinsic task-based bias metrics don’t
always correlate (Delobelle et al., 2021; Goldfarb-
Tarrant et al., 2021; Cao et al., 2022). We take no
stand in this debate, and have demonstrated how
H Bcan be useful not only for intrinsic
measurement upstream, but also for tasks such as
dialogue.
6 Conclusion
We have introduced a large dataset, H -B, with roughly 600 descriptor terms and
half a million distinct sentence prompts. The com-
prehensiveness of the list allows us to uncover new
biases in language models, as we demonstrated
with three bias measurements (token likelihoods,
generation bias, and an offensiveness classifier).
We then showed a proof-of-concept bias mitigation
technique, style equality , that uses a style classifier
and controlled generation to reduce these newly
found biases. The new dataset, new measurements,
and mitigation can more holistically improve model
fairness for a broader range of identities and demo-
graphics than previous approaches.
In the future, we plan to expand this dataset to
an even greater number of demographic terms, as
well as intersections of those terms, to reflect the
continually evolving ways in which people refer
to themselves and others. The range of templates
used in H Bcan expand to cover other
contexts in which identity is discussed, and non-
dialogue contexts more generally. We thus invite
other researchers to contribute terms and templates
toH Bin order to further broaden its
coverage of demographic identities.Limitations
Our descriptor list (Table 5) is limited to only terms
that the authors of this paper and their collaborators
have been able to produce, and so we acknowledge
that many possible demographic or identity terms
are certainly missing. (For instance, the list in-
cludes only a small handful of national demonyms
and only the most basic of race/ethnicity terms,
and a more complete dataset would include more
of these.) Results that we show in this work can-
not be assumed to generalize to all possible de-
mographic terms omitted from this dataset. Some
H Baxes are given more attention than
others in these results (for instance, the Characteris-
tics and Ability axes in Section 3.1), and so it is not
assured that all trends shown here will necessarily
apply across all axes. (However, see Table 10 for
bias reduction results split by axis.)
As mentioned in Section A.2, the dispreferred-
ness of demographic terms is contentious, and the
listing of certain descriptors as dispreferred, po-
larizing, or neither cannot be taken as authorita-
tive. The list is restricted to terms in US English
given the limitations of the authors’ experiences
and the fine-tuning data of the models studied, lim-
iting the universality of these findings. A more
intersectional extension of this work would also in-
clude pairs of descriptors (“homeless and disabled”,
“queer person of color”), and it would extend the
list of nouns injected in the H Btem-
plated sentences (Section 2.2.2) beyond just terms
connoting female, male, or unknown gender to
include non-binary-specific nouns (“enby”, “demi-
boy”, etc.) as well.
Finally, the process of assembling word lists it-
self can be tricky, as seed lexica often have several
practical (Antoniak and Mimno, 2021) and concep-
tual (Dinan et al., 2020b) disadvantages, especially
when they consist of paired gendered words. How-
ever, relying on a word list has advantages as well:
blame can be easily assigned to a particular term,
making model failure modes are more human in-
terpretable. Moreover, for words, researchers can
more easily keep track of confounding features,
such as frequency, part-of-speech, etc. (Antoniak
and Mimno, 2021), which may affect the interpre-
tation of results.
Ethics statement
Some bias measurement approaches, such as self-
debiasing (Schick et al., 2021), do not require a list9188of terms at all. On the one hand, this could be seen
as a benefit, since whenever we select terms we
are implicitly categorizing, and there are trade-offs
being made. On the other hand, without a list, we
cannot be sure that we are actually being inclusive
in our measurement, nor can we be accountable
to the choice of how to classify groups. Ignoring
some groups in effect deems them as not worthy
of measuring bias on, which is a form of othering
and exclusion in its own right. This being said,
a possible line of future work could more closely
compare list-less approaches like self-debiasing
with more handcrafted list-based approaches like
ours.
Our bias reduction technique relies on the un-
derstanding that responding differently to people
with different identities is often harmful, for in-
stance, if it stigmatizes disabilities or delegitimizes
marginalized identities by giving a confused re-
sponse. However, the use of a single numerical
value to characterize the level of bias in a model’s
generated response will inevitably be a blunt instru-
ment that will fail to capture the nuances of harm in
many cases. Thus, the idiosyncrasies of using this
form of bias reduction should be more thoroughly
studied before accepting it as universally suitable.
Acknowledgments
We thank the following people for their feedback
on this work and on our list of H Bde-
scriptors: Andrew Rayner, Anya Drabkin, Bran-
don Sanchez, Brandon Smith, Carolyn Hilton,
Claire Davidson, Danielle Flam, Emily Dinan, Jes-
sica Castillo, Jody Allard, Judith Basler, Kristen
Kennedy, Lenny Markus, Lex V ogt, Marcus Julien
Lee, Miranda Sissons, MJ Doctors Rajashekhar,
Mona Diab, Niambi Young, Nik Sawe, Renata Vi-
olante Mena, Rina Hahm, Stacey Houston, Susan
Epstein, Y-Lan Boureau, and Zuraya Tapia-Hadley.
Thanks as well to Paul Tolfor use of the axis-
specific color palette that enables color-blind safer
reading.
References91899190919191929193
A Additional methods
A.1 Dataset creation approach
While creating a dataset via crowdsourcing has
merits—it can be viewed as a naïve human ground
truth—it also has some downsides. Firstly, the prac-
tical, financial pressures of crowdsourcing usually
mean that the resulting datasets are small. This
can be an issue, as tentative experimental evidence
suggests that “more samples per prompt [yields]
a higher confidence measure . . . for that specific
prompt” in some experimental settings (Rae et al.,
2021). For most NLP tasks, crowdsourced data
usually makes up for its size in quality; however,
as mentioned above, Blodgett et al. (2021) outlined
several data quality issues arising from crowdsourc-
ing socially relevant data. For social applications
of NLP, it’s crucial to know what’s in your data.
Handcrafting data or creating it semi-automatically,
in particular, affords more control over the contents
of the dataset.
A.2 Descriptor terms
See Table 5 for all descriptors in H B,
segmented by axis and bucket. To keep the list9194
tractable, nationality terms (“Chinese”, “Chinese-
American”, etc.) are limited to countries with rela-tively high foreign-born populations in the US ac-
cording to a 2019 US Census estimate (US Census9195
Bureau, 2019). Nonce terms, words which do not
yet exist and are nonsensical by design, are from
Soja et al. (1991), and are included as a baseline
to gauge model behavior in response to descrip-
tor terms known to be out-of-vocabulary. Within
most axes, descriptor terms are further grouped into
buckets (Figure 1): the buckets in the race/ethnicity
axis are modeled after those appearing in the 2020
US Census (US Census Bureau, 2021).
We are cognizant that many of these are not ac-
cepted terms by members of the communities that
they describe, and so for some of these terms we
sought feedback from individuals with lived expe-rience of the terms in question. Where a term is
used in common parlance but has been identified as
dispreferred or polarizing by such experts (see the
Acknowledgments), we refer to it as such, keeping
in mind that there will always be disagreements
about some terms, even among experts themselves.
It is important to note that these denotations are
potentially subjective and that context matters: for
example, some terms may be offensive when used
by an out-of-group speaker but preferred when used
by an in-group speaker.
We include terms in our list that are still in use
despite being out-of-date, dispreferred, or polariz-9196ing, such as “handicapped”, because people using
language models may employ them in prompts,
and because they do appear in the models’ train-
ing data (Section 3.2), meaning that it is important
to determine whether language models respond to
these terms inappropriately. We exclude outright
slurs, which could more easily be caught by other
safety mitigations, such as blocklists, that can be
employed alongside these models if needed.
A.3 Using templates to generate prompts
The following nouns, some of which specify a spe-
cific gender, are used in H Bsentences:
•Woman : woman, lady, girl, mother, mom,
daughter, wife, grandmother, grandma, sister
•Man : man, bro, guy, boy, father, dad, son,
husband, grandfather, grandpa, brother
•Unspecified: individual, person, kid, parent,
child, spouse, grandparent, sibling, veteran
When using templated sentences for measure-
ments of bias in token likelihoods (Section 2.3.2)
or in generations (Section 2.3.3), several stylistic
variations are intermittently applied to improve the
robustness of results: lowercasing the descriptor,
removing any hyphens from the descriptor, remov-
ing the contraction from “I’m” , and removing any
final period.
See Table 6 for a comparison of the sizes of
different datasets for evaluating demographic bias,
extending upon Table 2.
A.4 Model details
RoBERTa, GPT-2 We use HuggingFace Trans-
formers (Wolf et al., 2020) to measure pseudo-
log-likelihoods of H Bdescriptors on
RoBERTa and perplexities on GPT-2. Our
RoBERTa pseudo-log-likelihood calculation adapts
the code of Nangia et al. (2020).
DialoGPT We specifically use a DialoGPT
model tuned further on the ConvAI2 dataset (Di-
nan et al. 2020c, model from Smith and Williams
2021) to acclimate the model to BlenderBot-style
prompts containing two sentences of persona in-
formation (Roller et al., 2021). Prepending thesepersona strings to the H Btemplated
sentence prompt allows for a greater diversity of
possible responses by the generative model.We
perform generations using the ParlAI framework
(Miller et al., 2017). We use beam search with
a beam size of 10, matching Zhang et al. (2020),
and beam blocking of 3-grams within the response
but not the context, matching the setting used for
BlenderBot 2.0. We use a beam minimum length
of 20 to match the domain of the style classifier
used to measure bias in generations (Section 2.3.3),
as well as to match Shuster et al. (2021).
BlenderBot 2.0 BlenderBot 2.0 has been fine-
tuned on several purpose-built dialogue datasets,
including ones designed to teach consistent per-
sonas, knowledge, and empathy (Zhang et al., 2018;
Dinan et al., 2018; Rashkin et al., 2019; Smith et al.,
2020b; Roller et al., 2021), recall of past conver-
sation details across multiple sessions (Xu et al.,
2021a), and the ability to retrieve factual informa-
tion from the internet (Komeili et al., 2022). We use
two sizes of model, with 400 million and 2.7 billion
parameters, which we refer to as BlenderBot 2.0
400M and BlenderBot 2.0 3B, respectively. Biases
both in token likelihoods and in generations are
measured using ParlAI: we perform beam search
with a beam size of 3, a minimum generation length
of 20 tokens, and beam blocking of 3-grams within
the response but not the context, following Komeili
et al. (2022).
A.5 Generation details
To measure bias in generations as a function of de-
scriptor in the H Bdataset, we produce
a minimum of 240,000 generations each for the
DialoGPT and BlenderBot 2.0 models, given the
settings in Section A.4. Each generation consti-
tutes one line of dialogue, responding to the given
templated sentence prompt containing a descriptor
from H B.
A.6 Using style classifiers to classify generated
responses
Before performing style classification with the
classifier of Smith et al. (2020a) on our gener-
ated responses to H Bsentences, we9197
first censor all mentions of the descriptor in
the response by replacing it with the neutral-
sounding “left-handed”, in order to avoid biasing
the style classifier. We also remove the string
“_POTENTIALLY_UNSAFE__ ” in BlenderBot 2.0’s re-
sponses, which indicates that the generation may
potentially be offensive.
A simpler alternative to the 217-class style clas-
sifier of Smith et al. (2020a) could be to use the
uni-axial sentiment classifier V ADER (Hutto and
Gilbert, 2014), which is used in Sheng et al. (2021a)
in part to measure the sentiment of harmful affir-
mations (i.e. “[DEMOGRAPHIC] are ridiculous” )
and in Liu et al. (2020) to measure the sentiment
of responses to phrases with demographic mark-
ers. However, when looking at sentiment scores
given to sample responses, it became evident to the
authors that flattening the diversity of possible re-
sponses onto a single “positive” vs. “negative” axis
leads to a score that is not sufficiently interpretable,
especially for bias reduction purposes.A.7 Generation bias metrics
In order to account for biases in generations among
all descriptors, we use the style classifier to com-
pute the style vector p= [p, p, ..., p]
for each generated response rto a H -Btemplated sentence. The style vector con-
sists of the probability pof the response belong-
ing to each of the style classes s, of which there
areS= 217 classes total. We compute the mean
style vector across all responses i∈ {1, ..., N},
for each combination of descriptor dand template
t∈ {1, ..., T}, to control for differences in style
distribution across templates. We define the bias
metric Full Gen Bias to be the total variance in
this mean style vector across descriptors, averaged
across templates:
FGB =1
T/summationdisplay/summationdisplayVar/parenleftigg
1
N/summationdisplayp/parenrightigg
We can probe the Full Gen Bias further by break-
ing down how much of its magnitude comes from
different types of styles. Since there are 217 styles
in total and some of them are rather similar (for9198instance, “Sympathetic” and “Empathetic”), we de-
fine the following style clusters C ∈ {C,C, ...}:
•S : {Sympathetic, Compassionate,
Empathetic}
• E: {Envious}
• C : {Curious, Questioning}
•C : {Vacuous, Absentminded, Be-
wildered, Stupid, Confused}
• H: {Hateful, Resentful}
•C: {Sensitive, Considerate, Warm, Kind,
Caring, Respectful}
The style clusters are produced by performing an
agglomerative hierarchical clustering over styles,
where each sample consists of a per-response style
probability vector for BlenderBot 2.0 3B without
any bias-reduction tuning. We identify the top 20
styles ranked by amount of Partial Gen Bias, and
for each of those styles, we identify all neighbor-
ing styles on the clustering dendrogram that are
roughly synonyms of it. We rank the resulting style
clusters by Partial Gen Bias (defined below) and
report on the 6 highest clusters in Table 4.
We define the Partial Gen Bias metric to be the
contribution of a certain style cluster to the Full
Gen Bias, calculated by summing the mean style
vector over just the styles in the given cluster as
opposed to over all styles:
PGB(C) =1
T/summationdisplay/summationdisplayVar/parenleftigg
1
N/summationdisplayp/parenrightigg
However, even though the Partial Gen Bias is
able to measure the contribution of each style clus-
ter to the overall bias, one issue with it is that it
artificially deflates the bias in style clusters with
many styles. Since the variance is calculated via
the squared deviation of each descriptor’s style vec-
tor from the overall mean, the variance of many
low-probability styles summed together will be
much less than the variance calculated on the to-
tal probability across all styles in the cluster.Wethus also compute a second per-cluster bias met-
ric,Summed-Cluster Gen Bias , that sums the
probabilities over all styles in the cluster before
calculating the variance among them:
SCGB( C) =1
T/summationdisplayVar/parenleftigg
1
N/summationdisplay/summationdisplayp/parenrightigg
B Additional results
B.1 Bias in token likelihoods
Perplexity differences in generative models.
See Figure 4 for an expanded version of the GPT-2
perplexity measurements in Figure 2, including all
templates as well as additional measurements for
BlenderBot 2.0 3B.
Pseudo-log-likelihood differences in MLMs.
Many of the patterns found in the token likeli-
hoods of descriptors using H Btem-
plates in generative models (Section 3.1) also ex-
tend to a setting with a different model and a dif-
ferent set of templates, the masked language model
RoBERTa and templates from the Sentence En-
coder Association Test (SEAT) (May et al., 2019).
Using RoBERTa-large, we calculate the pseudo-
log-likelihood (Wang and Cho, 2019; Salazar et al.,
2020; Nangia et al., 2020) of descriptor/noun
phrases (i.e. “tall guy” in the sentence “This is
a tall guy. ” ) on a sample of 500,000 sentences in
which descriptors are randomly drawn and inserted
into SEAT templates.
Similarly to Section 2.3.2, we use the Mann-
Whitney Utest to calculate the fraction of pairs of
descriptors within each H Baxis that
have a statistically significant difference in their
distributions of pseudo-log-likelihoods. We show
a subset of results in Table 7, focusing on the two
SEAT templates that most “humanize” the descrip-
tor terms: “ [NOUN PHRASE] is a person. ” and
“[PLURAL NOUN PHRASE] are people. ”
We see that axes like “Ability” and “Body type”
tend to have larger differences in descriptor distri-
bution, while “Age” and “Nationality” have fewer9199
differences: this may be due to an increased het-
erogeneity of terms in the former axes (Table 5) or
due to a larger disparity in the contexts in which
RoBERTa has learned to use the terms in the former
axes.
We note the similarity between these results and
those observed with GPT-2 and BlenderBot 2.0 3Bin Section 2.3.2, for which “Ability” and “National-
ity” also had high and low proportions of significant
differences, respectively, for the template “ I love
[NOUN PHRASE] ” for both models. This suggests
thatH Bmay be effective in identifying
trends in disparities of descriptor usage across dif-
ferent templates, language models, and likelihood9200
metrics.
B.2 Bias in generations
Full measurements of the bias in DialoGPT and
BlenderBot 2.0 3B are shown in Table 8 for Full
Gen Bias and Partial Gen Bias and in Table 9 for
Summed-Cluster Gen Bias. The Full Gen Bias
cut by descriptor axis is shown in Table 10. Ta-
ble 11 lists the percentage of generations marked
as offensive at a probability ≥50% by the B.A.D.
classifier.
Unlike with the Partial Gen Bias metric, when
computing the bias in each style cluster by first
summing over the probabilities for each cluster,
we see a greater amount of bias in the clusters of
styles connoting curiosity/confusion relative to that
of envy (Summed-Cluster Gen Bias, Table 9).
B.2.1 Descriptor training frequency analysis
Figures 5 and 6 show on the x-axis the relative
frequency of descriptor terms in the pre-training
and fine-tuning data, respectively, of BlenderBot
2.0 3B. For simplicity, only one-word descriptors
inH Bare shown. Frequencies are
calculated by dividing the total number of case-
insensitive usages of each term among training set
examples (including their prompts) by the number
of examples. For the pre-training data, a random
subset of 10 million examples are drawn to estimate
the descriptor frequency.
For the C cluster, very few descrip-
tors are both (1) very common in the pre-training
data and (2) elicit a highly “confused” response
from BlenderBot 2.0. This perhaps suggests that
increased exposure to a term during training im-
proves the likelihood that the model knows howto respond confidently to it. (The few exceptions
contain terms like “pan”, “ace”, and “poly” that
have multiple meanings and may be less familiar
to BlenderBot 2.0 when in the specific contexts of
H Btemplated sentences.)
B.3 Differences in offensiveness by descriptor
Table 12 lists example descriptors split by their
mean probabilities of offensiveness in H -Bsentences as measured by the B.A.D. clas-
sifier.
Table 13 shows, for each H Btem-
plate, the mean and standard deviation of the of-
fensiveness probabilities across descriptors. The
templates that lead to the highest variance in offen-
siveness probability are those that express love or
favoritism towards the descriptor in question, per-
haps reflecting the polarizing nature of the descrip-
tors; by contrast, templates reflecting curiosity of
or identity with specific descriptors have less vari-
ance, perhaps because they contain fewer content
words (Delobelle et al., 2021). Templates express-
ing hatred of specific descriptors are among those
with the most consistent offensiveness probabilities
across descriptors, likely because their offensive-
ness probabilities have saturated at close to 100%.
C Reducing generative bias
C.1 Technique
This section provides details about the bias reduc-
tion technique presented in Section 4.2, visualized
in Figure 7.
First, we generate a set of responses to H -Btemplated dialogue sentences. We denote
this set as R={R, R, ..., R}, where Ris92019202
the subset of responses to templated sentences that
specifically contain descriptor d. For each responser∈R, where tdenotes the template and i
indexes the individual response, we use the style9203
classifier of Smith et al. (2020a) to produce the
style probability vector
p= [p, p, ..., p];/summationdisplayp= 1
indicating the likelihood of rto belong to each
ofS= 217 dialogue styles (Section 2.3.3). Then,
we calculate the mean style probability vector
m=1
T/summationdisplay/parenleftigg
1
N/summationdisplayp/parenrightigg
for each descriptor dinH B, as well
as the mean style vector ¯m=/summationtextmacross
all descriptors together. (Here, we average across
responses to all templates t∈ {1, ..., T}in order to
maximize the chance that a characteristic responsestyle profile emerges for each descriptor.) We de-
scribe the line spanned by mand¯mas defining
the “direction of bias” for the descriptor d: if the
style vector pfor a response is much closer to
the mean vector mfor that particular descriptor
than to the global mean vector ¯m, we can think of
it as displaying the “characteristic” style for that
descriptor, and thus we deem it to be a biased re-
sponse because the model may have been unduly
influenced by the descriptor when responding. We
calculate the “bias value” bof response rby
performing a scaled projection along the direction
of bias:
b=(p−¯m)·(m−¯m)
||m−¯m||.
We empirically test 0, 1, and 2 as choices for the
scaling exponent α, and we find 0 to produce the
most similar bias values across examples of both
categories of harm (feeling overly sorry for one’s
partner and showing curiosity/confusion about their
identity) exhibited in Table 1. We tag the end of the
context of r, consisting of persona strings and the
H Btemplated sentence, with the string
“bias ” ifb> β and “ no_bias ” otherwise, where
βis a threshold determined empirically (Table 8).
We tuned our models on these tagged con-
text/response pairs using 8 32-GB V olta GPUs with
a batch size of 16, with early stopping with per-
plexity as the validation metric. For DialoGPT, we
tuned with SGD and swept the maximum learning
rate from 3e-7 to 3e0 (15 runs), with the best model9204
training in 19 hours and having a learning rate of
3e-1. For BlenderBot 2.0 3B, we used 100 warmup
steps with the Adam (Kingma and Ba, 2014) opti-
mizer and swept the maximum learning rate from
3e-7 to 3e-3 (9 runs): the best model trained in 2.2
days and had a learning rate of 3e-6. Learning rate
ranges were chosen in a uniform logarithmic grid.
C.2 Results
C.2.1 Automatic evaluations
Measuring the extent of bias reduction. From
Table 8, sweeping the bias threshold βhas a mod-
erate effect on the level of bias reduction. (Un-
less specified, all bias-reduction tuning results in
this work use β= 0.0003 for DialoGPT and
β= 0.0030 for BlenderBot 2.0 3B.) An ablation
consisting of tuning DialoGPT and BlenderBot 2.0
3B on responses to H Bsentences but
without appended bias labels mostly shows no de-
crease, and often an increase, in Full Gen Bias and
Partial Gen Bias over the original models. Table 10
shows that Full Gen Bias, when filtered by descrip-tor axis, undergoes a double-digit percentage drop
on nearly every axis for BlenderBot 2.0 3B, but
that it leads to substantial reductions for DialoGPT
only on certain axes, largely corresponding to those
axes on which the Full Gen Bias was originally the
largest to begin with.
As a check on the style classifier, we see from
Table 14 that certain frequently used phrases ex-
pressing sympathy and confusion are used much
less often in BlenderBot 2.0 3B responses after bias-
reduction tuning. Tables 16 and 17 show Blender-
Bot 2.0 3B responses before vs. after tuning to
H Bsentences containing the descrip-
tors “who are hard of hearing” and “non-binary”,
to which the untuned BlenderBot 2.0 3B often re-
sponds with sympathy or confusion, respectively
(Table 1): by inspection, the example responses
show these sentiments less often after tuning.
Understanding the effects of bias reduction
tuning. Table 15 lists the styles whose variance
in probability across descriptors in BlenderBot 2.0
3B changes the most as a result of bias-reduction9205
tuning. A lower variance in style probability
after tuning tends to correspond to a lower mean
probability of that style altogether. Descriptorswith the largest decreases in the Sympa-
thetic/Empathetic/Compassionate styles tend to be
those relating to additions or traumatic situations,9206
and descriptors with the largest decreases in the Cu-
rious/Stupid/Questioning/Confused/Bewildered
styles often relate to cultural, gender/sex, sexual
orientation, or religious terms. There is a slightincrease (0.14%) in the Hateful probability of the
average BlenderBot 2.0 3B response; the top 5
descriptors with the largest Hateful increase have a
mean increase of 1.10%, with the largest having an9207increase of 1.55%.
We can start to understand this increase in mean
Hateful probability by measuring the effect of bias-
reduction tuning on “parroting”, when BlenderBot
2.0 3B repeats back the H Bprompt
entirely without changes: a few examples of this
are visible in Tables 16 and 17. We find that 17.3%
of responses exactly match the H B
prompt before bias reduction tuning and 20.0%
after, suggesting that perhaps the tuning itself may
be contributing to this effect. Specifically, con-
sidering the two descriptors with the largest Hate-
ful increases in Table 15, we find that the amount
of parroting in response to the “I hate [PLURAL
NOUN PHRASE]. ” template increases for “druggie”
from 6% before tuning to 47% after, and for “who
is deaf” from 25% before to 100% after. (All 15
responses generated by the tuned model given an
“I hate druggie [PLURAL NOUN]. ” prompt copy the
prompt exactly, with minor punctuation changes.)
Across all templates, the overall change in parrot-
ing due to tuning varies significantly by descriptor,
from -10% for “in the Marine Corps” to +18% for
“who incurred a traumatic brain injury”. However,
the Spearman’s ρbetween the mean change in par-
roting per descriptor and the mean bias value across
responses to that descriptor (Section C.1) is +0.19
(two-sided p=2e-6), indicating some relationship
between the strength of the bias reduction applied
to a given descriptor and the increase in parroting
observed. Future work will seek to ensure that
this parroting cannot contribute to an increase in
responses expressing hate given a hateful dialogue
prompt.
Offensiveness before and after bias reduction
tuning. Table 11 shows the fraction of responses
marked as offensive by the B.A.D. classifier as
a function of model type, size, and whether it
underwent bias-reduction tuning. Bias-reduction
tuning leads to a slight decrease in offensiveness
for DialoGPT and a slight increase in Blender-
Bot 2.0 3B, but these findings are complicated
by the fact that the B.A.D. classifier is influenced
by usages of H B descriptors them-
selves (Section 3.3). By inspection, utterances
marked as offensive tend to be those that respond
to negative templates like “I hate [PLURAL NOUN
PHRASE]. ” ,“I think [PLURAL NOUN PHRASE]
are the worst. ” , etc., or to descriptors with negative
connotations, such as “hideous” and “alcoholic”.C.2.2 Human evaluations
Table 18 shows human evaluations of the perfor-
mance of models with bias reduction tuning vs.
the original models, using workers crowdsourced
on Amazon Mechanical Turk.These evaluations
use the Acute-Eval technique (Li et al., 2019): a
crowdworker is shown two snippets of conversation
side-by-side, each snippet consisting of a H -Bsentence followed by a generated model
response. The crowdworker is asked to choose
which response is better, given the following crite-
ria:
•Preference :“Who would you prefer to talk to
for a long conversation?”
•Humanness :“Which speaker sounds more
human?”
•Interestingness :“If you had to say one of
these speakers is interesting and one is boring,
who would you say is more interesting?”
Potentially inflammatory templates and descrip-
tors are filtered out before being shown to crowd-
workers, as are any responses marked as unsafe by
the B.A.D. classifier.
We find that the reduced-bias DialoGPT model
may be slightly disfavored to the original one by
a few percentage points, and that the reduced-bias
BlenderBot 2.0 3B is roughly comparable to the
original, but none of these trials are individually
statistically significant.9208920992109211